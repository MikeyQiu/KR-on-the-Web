Approximation Complexity of
Maximum A Posteriori Inference
in Sum-Product Networks∗

Diarmaid Conaty
Queen’s University Belfast, UK

Denis D. Mau´a
Universidade de S˜ao Paulo, Brazil

Cassio P. de Campos
Queen’s University Belfast, UK

We discuss the computational complexity of approximating maximum a
posteriori inference in sum-product networks. We ﬁrst show np-hardness in
trees of height two by a reduction from maximum independent set; this im-
plies non-approximability within a sublinear factor. We show that this is a
tight bound, as we can ﬁnd an approximation within a linear factor in net-
works of height two. We then show that, in trees of height three, it is np-hard
to approximate the problem within a factor 2f (n) for any sublinear function
f of the size of the input n. Again, this bound is tight, as we prove that the
usual max-product algorithm ﬁnds (in any network) approximations within
factor 2c·n for some constant c < 1. Last, we present a simple algorithm,
and show that it provably produces solutions at least as good as, and poten-
tially much better than, the max-product algorithm. We empirically analyze
the proposed algorithm against max-product using synthetic and realistic
networks.

7
1
0
2
 
p
e
S
 
5
 
 
]
I

A
.
s
c
[
 
 
5
v
5
4
0
6
0
.
3
0
7
1
:
v
i
X
r
a

∗

A similar version of this manuscript appeared in [8].

1

1 Introduction

Finding the maximum probability conﬁguration is a key step of many solutions to prob-
lems in artiﬁcial intelligence such as image segmentation [12], 3D image reconstruction
[3], natural language processing [15], speech recognition [21], sentiment analysis [30],
protein design [27] and multicomponent fault diagnosis [26], to name but a few. This
problem is often called (full) maximum a posteriori (map) inference, or most likely ex-
planation (mpe).

Sum-Product Networks (spns) are a relatively new class of graphical models that allow
marginal inference in linear time in their size [23]. This is therefore in sharp diﬀerence
with other graphical models such as Bayesian networks and Markov Random Fields that
require #p-hard eﬀort to produce marginal inferences [10]. Intuitively, an spn encodes
an arithmetic circuit whose evaluation produces a marginal inference [9]. spns have
received increasing popularity in applications of machine learning due to their ability to
represent complex and highly multidimensional distributions [23, 1, 21, 6, 17, 2].

In his PhD thesis, Peharz showed a direct proof of np-hardness of map in spns by
a reduction from maximum satisﬁability [18, Theorem 5.3].1 Later, Peharz et al. [20]
noted that np-hardness can be proved by transforming a Bayesian network with a naive
Bayes structure into a distribution-equivalent spn of height two (this is done by adding a
sum node to represent the latent root variable and its marginal distribution and product
nodes as children to represent the conditional distributions). As map inference in the
former is np-hard [11], the result follows.

In this paper, we show a direct proof of np-hardness of map inference by a reduction
from maximum independent set, the problem of deciding whether there is a subset of
vertices of a certain size in an undirected graph such that no two vertices in the set
are connected. This new proof is quite simple, and (as with the reduction from naive
Bayesian networks) uses a sum-product network of height two. An advantage of the new
proof is that, as a corollary, we obtain the non-approximability of map inference within
a sublinear factor in networks of height two. This is a tight bound, as we show that there
is a polynomial-time algorithm that produces approximations within a linear factor in
networks of height two. Note that an spn of height two is equivalent to a ﬁnite mixture of
tractable distributions (e.g. an ensemble of bounded-treewidth Bayesian networks) [24].
We prove that, in networks of height three, it is np-hard to approximate the problem
within any factor 2f (n) for any sublinear function f of the input size n, even if the spn
is a tree. This a tight bound, as we show that the usual max-product algorithm [9, 23],
which replaces sums with maximizations, ﬁnds an approximation within a factor 2c·n for
some constant c < 1. Table 1 summarizes these results. As far as we are concerned,
these are the ﬁrst results about the complexity of approximating map in spns.

We also show that a simple modiﬁcation to the max-product algorithm leads to an
algorithm that produces solutions which are never worse and potentially signiﬁcantly
better than the solutions produced by max-product. We compare the performance of

1The original proof was incorrect, as it encoded clauses by products; a corrected proof was provided by

the author in an erratum note [19].

2

Height Lower bound Upper bound

1
2
≥ 3

1
(m − 1)ε
2sε

1
m − 1
2s

Table 1: Lower and upper bounds on the approximation threshold for a polynomial-time
algorithm: s denotes the size of the instance, m is the number of internal nodes,
ε is a nonnegative number less than 1.

the proposed algorithm against max-product in several structured prediction tasks using
both synthetic networks and spns learned from real-world data. The synthetic networks
encode instances of maximum independent set problems. The purpose of these networks
is to evaluate the quality of solutions produced by both algorithms on shallow spns which
(possibly) encode hard to approximate map problems. Deeper networks are learned
using the LearnSPN algorithm by Gens and Domingos [13]. The purpose of these
experiments is to assess the relative quality of the algorithms on spns from realistic
datasets, and their sensitivity to evidence. The empirical results show that the proposed
algorithm often ﬁnds signiﬁcantly better solutions than max-product does, but that this
improvement is less pronounced in networks learned from real data. We expect these
results to foster research in new approximation algorithms for map in spns.

Before presenting the complexity results in Section 3, we ﬁrst review the deﬁnition
of sum-product networks, and comment on a few selected results from the literature
in Section 2. The experiments with the proposed modiﬁed algorithm and max-product
appear in Section 4. We conclude the paper with a review of the main results in Section 5.

2 Sum-Product Networks

We use capital letters without subscripts to denote random vectors (e.g. X), and capital
letters with subscripts to denote random variables (e.g., X1). If X is a random vector,
we call the set X composed of the random variables Xi in X its scope. The scope of a
function of a random vector is the scope of the respective random vector. In this work,
we constrain our discussion to random variables with ﬁnite domains.

Poon and Domingos [23] originally deﬁned spns as multilinear functions of indicator
variables that allow for space and time eﬃcient representation and inference.
In its
original deﬁnition spns were not constrained to represent valid distributions; this was
achieved by imposing properties of consistency and completeness. This deﬁnition more
closely resembles Darwiche’s arithmetic circuits which represent the network polynomial
of a Bayesian network [9], and also allow eﬃcient inference (in the size of the circuit).
See [7] for a recent discussion on the (dis)similarities between arithmetic circuits and
sum-product networks.

Later, Gens and Domingos [13] re-stated spns as complex mixture distributions as

follows.

3

0.2

0.3

×

×

0.5

+

×

X2

0.7

X1

0.4

X2

0.2

X1

0.9

Figure 1: A sum-product network over binary variables X1 and X2. Only the probabil-

ities P(Xi = 1) are shown.

• Any univariate distribution is an spn.

• Any weighted sum of spns with the same scope and nonnegative weights is an spn.

• Any product of spns with disjoint scopes is an spn.

This alternative deﬁnition (called generalized spns by Peharz [18]) implies decomposabil-
ity, a stricter requirement than consistency. Peharz et al. [22] showed that any consistent
spn over discrete random variables can be transformed in an equivalent decomposable
spn with a polynomial increase in size, and that weighted sums can be restricted to the
probability simplex without loss of expressivity. Hence, we assume in the following that
spns are normalized: the weights of a weighted sum add up to one. This implies that
spns specify (normalized) distributions. A similar result was obtained by Zhao et al. [29].
We note that the base of the inductive deﬁnition can also be extended to accommodate
any class of tractable distributions (e.g., Chow-Liu trees) [25, 28]. For the purposes of
this work, however, it suﬃces to consider only univariate distributions.

An spn is usually represented graphically as a weighted rooted graph where each
internal node is associated with an operation + or ×, and leaves are associated with
variables and distributions. The arcs from a sum node to its children are weighted
according to the corresponding convex combinations. The remaining arcs have implicitly
weight 1. The height of an spn is deﬁned as the maximum distance, counted as number
of arcs, from the root to a leaf of its graphical representation. Figure 1 shows an example
of an spn with scope {X1, X2} and height two. Unit weights are omitted in the ﬁgure.
Note that by deﬁnition every node represents an spn (hence a distribution) on its own;
we refer to nodes and their corresponding spns interchangeably.

Consider an spn S(X) over a random vector X = (X1, . . . , Xn). The value of S
at a point x = (x1, . . . , xn) in its domain is denoted by S(x) and deﬁned recursively as
follows. The value of a leaf node is the value of its corresponding distribution at the point
obtained by projecting x onto the scope of the node. The value of a product node is the
product of the values of its children at x. Finally, the value of a sum node is the weighted
average of its children’s values at x. For example, the value of the spn S(X1, X2) in

4

Figure 1 at the point (1, 0) is S(1, 0) = 0.2 · 0.3 · 0.4 + 0.5 · 0.4 · 0.8 + 0.3 · 0.8 · 0.9 = 0.4.
Note that since we assumed spns to be normalized, we have that

x S(x) = 1.

P

Let E ⊆ {1, . . . , n} and consider a random vector XE with scope {Xi : i ∈ E}, and an
assignment e = {Xi = ei : i ∈ E}. We write x ∼ e to denote a value of X consistent with
e (i.e., the projection of x on E is e). Given an spn S(X) representing a distribution
P(X), we denote the marginal probability P(e) =
x∼e S(x) by S(e). This value can
be computed by ﬁrst marginalizing the variables {Xj : j 6∈ E} from every (distribution
in a) leaf and then propagating values as before. Thus marginal probabilities can be
computed in time linear in the network size (considering univariate distributions are
represented as tables). The marginal probability P(X2 = 0) = 0.7 induced by the spn
in Figure 1 can be obtained by ﬁrst marginalizing leaves without {X2} (thus producing
values 1 at respective leaves), and then propagating values as before.

P

In this work, we are interested in the following computational problem with spns:

Deﬁnition 1 (Functional map inference problem). Given an spn S speciﬁed with ra-
tional weights and an assignment e, ﬁnd x∗ such that S(x∗) = maxx∼e S(x).

A more general version of the problem would be to allow some of the variables to be
summed out, while others are maximized. However, the marginalization (i.e., summing
out) of variables can performed in polynomial time as a preprocessing step, the result of
which is a map problem as stated above. We stick with the above deﬁnition for simplicity
(bearing in mind that complexity is not changed).

To prove np-completeness, we use the decision variant of the problem:

Deﬁnition 2 (Decision map inference problem). Given an spn S speciﬁed with rational
weights, an assignment e and a rational γ, decide whether maxx∼e S(x) ≥ γ.

We denote both problems by map, as the distinction to which particular (functional or
decision) version we refer should be clear from context. Clearly, np-completeness of the
decision version establishes np-hardness of the functional version. Also, approximation
complexity always refers to the functional version.

The support of an spn S is the set of conﬁgurations of its domain with positive values:
supp(S) = {x : S(x) > 0}. An spn is selective if for every sub-spn T corresponding to
a sum node in S it follows that the supports of any two children are disjoint. Peharz et
al. [20] recently showed that map is tractable in selective spns.

Here, we discuss the complexity of (approximately) solving map in general spns. We
assume that instances of the map problem are represented as bitstrings hS, ei using a rea-
sonable encoding; for instance, weights and probabilities are rational values represented
by two integers in binary notation, and graphs are represented by (a binary encoding of
their) adjacency lists.

3 Complexity Results

As we show in this section, there is a strong connection between the height of an spn
and the complexity of map inferences. First, note that an spn of height 0 is just a

5

marginal distribution. So consider an spn of height 1. If the root is a sum node, then
the network encodes a sum of univariate distributions (over the same variable), and map
can be solved trivially by enumerating all values of that variable. If on the other hand
the root is a product node, then the network encodes a distribution of fully independent
variables. Also in this case, we can solve map easily by optimizing independently for
each variable. So map in networks of height 1 or less is solvable in polynomial time.

Let us now consider spns of height 2. As already discussed in the introduction, Peharz
et al. [20] brieﬂy observed that the map problem is np-hard even for tree-shaped networks
of height 2. Here, we give the following alternative, direct proof of np-hardness of map
in spns, that allows us to obtain results on non-approximability.
Theorem 1. map in sum-product networks is np-complete even if there is no evidence,
and the underlying graph is a tree of height 2.

Proof. Membership is straightforward as we can evaluate the probability of a conﬁgura-
tion in polynomial time.

We show hardness by reduction from the np-hard problem maximum independent set
(see e.g. [31]): Given an undirected graph G = (V, E) with vertices {1, . . . , n} and an
integer v, decide whether there is an independent set of size v. An independent set is a
subset V ′ ⊆ V such that no two vertices are connected by an edge in E.

Let Ni denote the neighbors of i in V . For each i ∈ V , build a product node Si
whose children are leaf nodes Si1, . . . , Sin with scopes X1, . . . , Xn, respectively. If j ∈ Ni
then associate Sij with distribution P(Xi = 1) = 0; if j /∈ Ni ∪ {i} associate Sij with
P(Xj = 1) = 1/2; ﬁnally, associate Sii with distribution P(Xi = 1) = 1. See Figure 2 for
an example. Let ni = |Ni| be the number of neighbors of i. Then Si(x) = 1/2n−ni−1 if
xi = 1 and xj = 0 for all j ∈ Ni; and Si(x) = 0 otherwise. That is, Si(x) > 0 if there
is a set V ′ which contains i and does not contain any of its neighbors. Now connect
all product nodes Si with a root sum node parent S; specify the weight from S to Si
as wi = 2n−ni−1/c, where c =
i 2n−ni−1. Suppose there is an independent set I of
size v. Take x such that xi = 1 if i ∈ I and xi = 0 otherwise. Then S(x) = v/c.
For any conﬁguration x of the variables, let I(x) = {i : Si(x) > 0}. Then I(x) is an
independent set of size c·S(x). So suppose that there is no independent set of size v. Then
maxx S(x) < v/c. Thus, there is an independent set if and only if maxx S(x) ≥ v/c.

P

Consider a real-valued function f (hS, ei) of the encoded network S and evidence e.
An algorithm for map in spns is a f (hS, ei)-approximation if it runs in time polynomial
in the size of its input (which speciﬁes the graph, the weights, the distributions, the
evidence) and outputs a conﬁguration ˜x such that S(˜x) · f (hS, ei) ≥ maxx∼e S(x). That
is, a f (hS, ei)-approximation algorithm provides, for every instance hS, ei of the map
problem, a solution whose value is at most a factor f (hS, ei) from the optimum value.
The value f (hS, ei) is called the approximation factor. We have the following consequence
of Theorem 1:
Corollary 1. Unless p equals np, there is no (m − 1)ε-approximation algorithm for
map in spns for any 0 ≤ ε < 1, where m is the number of internal nodes of the spn,
even if there is no evidence and the underlying graph is a tree of height 2.

6

S

+

1 / 6

1/3

1
/
6

1/3

4

1

3

2

×S1

×S2

×S3

×S4

X1

1

X2

0

X3

0

X4

0

X1

0

X2

1

X3

0

X4

1/2

X1

0

X2

0

X3

1

X4

0

X1

0

X2

1/2

X3

0

X4

1

Figure 2: A sum-product network encoding the maximum independent set problem for
the graph on the right. Only the values for P(Xi = 1) are shown.

0

X1

1/2

X2

0

X3

0

X4

1

X1

1/2

X2

1

X3

0

X4

1

X1

1/2

X2

0

X3

1

X4

0

X1

1/2

X2

1

X3

0

X4

0

X1

1/2

X2

0

X3

1

X4

1

X1

1/2

X2

1

X3

1

X4

0

X1

1/2

X2

1

X3

1

X4

×

×

×

×

×

×

×

×

×

×

×

×

X1

0

X2

0

X3

1

X4

1/2

X1

1

X2

1

X3

1

X4

1/2

X1

1

X2

0

X3

0

X4

1/2

X1

0

X2

1

X3

1

X4

1/2

X1

0

X2

0

X3

0

X4

1/2

X1

1

X2

1

X3

0

X4

1/2

X1

0

X2

1

X3

0

X4

1/2

Figure 3: A sum-product network encoding the Boolean formula (¬X1 ∨ X2 ∨ ¬X3) ∧
(¬X1 ∨ X3 ∨ X4). We represent only the probabilities P(Xi = 1), and omit the
uniform weights 1/14 of the root sum node.

Proof. The proof of Theorem 1 encodes a maximum independent set problem and the
reduction is a weighted reduction (see Deﬁnition 1 in [5]), which suﬃces to prove the
result. To dispense with weighted reductions, we now give a direct proof. So suppose
that there is a (m − 1)ε-approximation algorithm for map with 0 ≤ ε < 1. Let ˜x be the
conﬁguration returned by this algorithm when applied to the spn S created in the proof
of Theorem 1 for a graph G given as input of the maximum independent set problem.
We have that

S(˜x) · c · (m − 1)ε ≥ c · max

S(x) = max
I∈I(G)

|I| ,

x

i 2n−ni−1, I(G) is the collection of independent sets of G, n is the number
where c =
of vertices in G and ni is the number of neighbors of vertex i in G. Consequently, this
algorithm is a nε-approximation for maximum independent set (note that n = m − 1 by
construction). We know that there is no nε-approximation for maximum independent
set with 0 ≤ ε < 1 unless p equals np [31], so the result follows.

P

The above result shows that mixtures (or ensembles) of selective spns are np-hard
to approximate. Note that mixtures of (selective) spns are typically obtained when

×

+

×

7

using bagging to reduce variance in complex models [4]. The result can also be used to
prove complexity for similar models such as mixture of trees [16], mixture of naive Bayes
models [14] and mixture of arithmetic circuits [24]. Corollary 1 can be read as stating
that there is probably no approximation algorithm for map with sublinear approximation
factor in the size of the input. The following result shows that this lower bound is tight:

Theorem 2. There exists a (m − 1)-approximation algorithm for map in sum-product
networks whose underlying graph has height at most 2, where m is the number of internal
nodes.

Proof. Consider a sum-product network of height 2. If the root is a product node then
the problem decomposes into independent map problems in spns of height 1; each of
those problems can be solved exactly. So assume that the root S is a sum node connected
to either leaf nodes or to nodes which are connected to leaf nodes. Solve the respective
map problem for each child Si independently (which is exact, as the corresponding spn
has height at most 1); denote by xi the corresponding solution. Note that Si(xi) is
an upper bound on the value Si(x∗), where x∗ is a (global) map conﬁguration. Let
w1, . . . , wm−1 denote the weights from the root to children S1, . . . , Sm−1. Return ˜x =
arg maxi wi · Si(xi). It follows that (m − 1)S(˜x) ≥ maxx∼e S(x). Note that this is the
same value returned by the max-product algorithm.

Thus, for networks of height 2, we have a clear divide: there is an approximation
algorithm with linear approximation factor in the number of internal nodes, and no
approximation algorithm with sublinear approximation factor in the number of inter-
nal nodes. Allowing an additional level of nodes reduces drastically the quality of the
approximations in the worst case:

Theorem 3. Unless p equals np, there is no 2sε
-approximation algorithm for map in
spns for any 0 ≤ ε < 1, where s is the size of the input, even if there is no evidence and
the underlying graph is a tree of height 3.

Proof. First, we show how to build an spn for deciding satisﬁability: Given a Boolean
formula φ in conjunctive normal form, decide if there is a satisfying truth-value assign-
ment. We assume that each clause contains exactly 3 distinct variables (np-completeness
is not altered by this assumption, but if one would like to drop it, then the weights of
the sum node we deﬁne below could be easily adjusted to account for clauses with less
than 3 variables).

Let X1, . . . , Xn denote the Boolean variables and φ1, . . . , φm denote the clauses in
the formula. For i = 1, . . . , m, consider the conjunctions φi1, . . . , φi7 over the variables
of clause φi, representing all the satisfying assignments of that clause. For each such
assignment, introduce a product node Sij encoding the respective assignment: there is
a leaf node with scope Xk whose distribution assigns all mass to value 1 (resp., 0) if
and only if Xk appears nonnegated (resp., negated) in φij; and there is a leaf node with
uniform distribution over Xk if and only if Xk does not appear on φij. See Figure 3 for
an example. For a ﬁxed conﬁguration of the random variables, the clause φi is true if and
only if one of the product nodes Si1, . . . , Si7 evaluates to 1. And since these products

8

P

P

ij Sij(x) = m/2n−3 if φ(x) is true, and

encode disjoint assignments, at most one such product is nonzero for each conﬁguration.
ij Sij(x) < m/2n−3 if φ(x)
We thus have that
is false. So introduce a sum node S with all product nodes as children and with uniform
weights 1/7m. There is a satisfying assignment for φ if and only if maxx S(x) ≥ 23−n/7.
Now take the spn above and make q copies of it with disjoint scopes: each copy contains
diﬀerent random variables X t
k, t = 1, . . . , q, at the leaves, but otherwise represents the
very same distribution/satisﬁability problem. Name each copy St, t = 1, . . . q, and let
its size be st. Denote by s′ = maxt st (note that, since they are copies, their size is
the same, apart from possible indexing, etc). Connect these copies using a product
q
node S with networks S1, . . . , Sq as children, so that S(x) =
t=1 St(x). Note that
maxx St(x) ≥ 23−n/7 if there is a satisfying assignment to the Boolean formula, and
maxx St(x) ≤ m−1
m · 23−n/7 if there is no satisfying assignment. Hence, maxx S(x) ≥
(23−n/7)q if there is a satisfying assignment and maxx S(x) ≤ ((m − 1)23−n/(7m))q if
there is no satisfying assignment. Specify

Q

q = 1 +

(ln(2) · m · (s′ + 2)ε)

1
1−ε

,

k

which is polynomial in s′, so the spn S can be constructed in polynomial time and space
and has size s < q(s′ + 2). From the deﬁnition of q, we have that

j

q >

ln(2) · m · (s′ + 2)ε

1
1−ε .

(cid:0)
Raising both sides to 1 − ε yields

(cid:1)

q > qε ln(2) · m · (s′ + 2)ε = m ln(2(q(s′+2))ε

) > m ln 2sε

.

Since 1

m ≤ ln m

m−1 for any integer m > 1, it follows that

q ln

m
m − 1

(cid:18)

(cid:19)

> ln 2sε

.

By exponentiating both sides, we arrive at

q

m
m − 1

> 2sε

hence 2sε

m − 1
m

q

< 1 ,

(cid:19)
Finally, by multiplying both sides by (23−n/7)q, we obtain

(cid:19)

(cid:18)

(cid:18)

2sε

23−n(m − 1)
7m

q

<

23−n
7

q

.

(cid:18)
Hence, if we can obtain an 2sε
-approximation for maxx S(x), then we can decide sat-
isﬁability: there is a satisfying assignment to the Boolean formula if and only if the
approximation returns a value strictly greater than (23−n(m − 1)/(7m))q .

(cid:19)

(cid:19)

(cid:18)

According to Theorem 3, there is no 2f (s)-approximation (unless p equals np) for any
sublinear function f of the input size s. The following result is used to show that this
lower bound is tight.

9

Theorem 4. Let S+ denote the sum nodes in spn S, and di be the number of children
Si∈S+ di)-approximation algorithm for map
of sum node Si ∈ S+. Then there exists a (
with input S and evidence e.

Q
Proof. There are two cases to consider, based on the value of S(e), which can be checked
If S(e) = 0, then we can return any assignment consistent with
in polynomial time.
e, as the result will be exact (and equal to zero).
If S(e) > 0, then take the max-
product algorithm [23], which consists of an upward pass where sums are replaced by
maximizations in the evaluation of an spn, and a downward pass which selects the
maximizers of the previous step. Deﬁne pd(S, e) recursively as follows. If S is a leaf
then pd(S, e) = maxx∼e S(x).
If S is a sum node, then pd(S, e) = maxj=1,...,t wj ·
pd(Sj, e), where S1, . . . , St are the children of S. Finally, if S is a product node with
t
children S1, . . . , St, then pd(S, e) =
j=1 pd(Sj, e). Note that pd(S, e) corresponds to
the upward pass of the max-product algorithm; hence it is a lower bound on the value of
the conﬁguration obtained by such algorithm. We prove that the max-product algorithm
is a (

Si∈S+ di)-approximation by proving by induction in the height of the spn that

Q

Q

pd(S, e) ≥



1
di 

max
x∼e

S(x) .

YSi∈S+
To show the base of the induction, take a network S of height 0 (i.e., containing a sin-
gle node). Then pd(S, e) = maxx∼e S(x) trivially. So take a network S with children
S1, . . . , St, and suppose (by inductive hypothesis) that pd(Sj, e) ≥ (
for every child Sj. If S is a product node, then

Si∈S+
j

1
di





) maxx∼e Sj(x)

Q

pd(S, e) =

pd(Sj, e) ≥

max
x∼e

Sj(x)

t

Yj=1

t






Yj=1

YSi∈S+

j

1
di





t

=



1
di 

Yj=1
where the last two equalities follow as the scopes of products are disjoints, which implies
that the children do not share any node. If S is a sum node, then

YSi∈S+

YSi∈S+









max
x∼e

Sj(x) =



1
di 

max
x∼e

S(x) ,

pd(S, e) = max
j=1,...,t

wj · pd(Sj, e)

1
di






≥ max

j=1,...,t 



= max

j=1,...,t  

t

j

YSi∈S+
t

wj · max
x∼e

Sj(x)

wj · max
x∼e

Sj(x)

di !

≥ max
j=1,...,t

· wj · max
x∼e

Sj(x)

Si∈S+
j
t
Q
Si∈S+ di

Q

10

=

≥

t · maxj=1,...,t wj maxx∼e Sj(x)
Si∈S+ di

Q

1
di 







YSi∈S+

max
x∼e

S(x) .

The ﬁrst inequality uses the induction hypothesis. The second inequality follows since
1/(t ·
Si∈S+ di. The last inequality follows as
maxj wj · maxx∼e Sj(x) is an upper bound on the value of any child of S. This concludes
the proof.

Si∈S+,Si6=S di) = 1/

di) ≥ 1/(t ·

Si∈S+
j

Q

Q

Q

We have the following immediate consequence, showing the tightness of Theorem 3.
Corollary 2. There exists a 2ε·s-approximation algorithm for map for some 0 < ε < 1,
where s is the size of the spn.

Proof. Assume the network has at least one sum node (otherwise we can ﬁnd an exact
solution in polynomial time). Given the result of Theorem 4, we only need to show that
Si∈S+ di < 2ε·s, with S+ the sum nodes in spn S and di be the
there is ε < 1 such that
number of children of sum node Si ∈ S+. Because s is strictly greater than the number
of nodes and arcs in the network (as we must somehow encode the graph of S), we know
that s >

Si∈S+ di. One can show that 3x/3 > x for any positive integer. Hence,

Q

P
di ≤

3di/3 =

2di log2(3)/3 = 2log2(3)/3·PSi∈S+ di < 2s log2(3)/3 < 2ε·s ,

YSi∈S+

YSi∈S+
for some ε < 0.5284.

YSi∈S+

The previous result shows that the max-product algorithm achieves tight upper bounds
on the approximation factor. This however does not rule out the existence of approxima-
tion algorithms that achieve the same (worst-case) upper bound but perform signiﬁcantly
better on average. For instance, consider the following algorithm that takes an spn S and
evidence e, and returns amap(S, e) as follows, where amap is short for argmax-product
algorithm.

Argmax-Product Algorithm

• If S is a sum node with children S1, . . . , St, then compute

amap(S, e) = arg max

wj · Sj(x) ,

x∈{x1,...,xt}

t

Xj=1

where xk = amap(Sk, e), that is, xk is the solution of the map problem ob-
tained by argmax-product for network Sk (argmax-product is run bottom-up).

• Else if S is a product node with children S1, . . . , St, then amap(S, e) is the

concatenation of amap(S1, e), . . . , amap(St, e).

11

5/16

11/48

11/48

11/48

Xi

1

Xi

0

Xi

0

Xi

0

Figure 4: Fragment of the sum-product network used to prove Theorem 5.

• Else, S is a leaf, so return amap(S, e) = arg maxx∼e S(x).

Argmax-product has a worst-case time complexity quadratic in the size of the net-
work; that is because the evaluation of all the children of a sum node with the argument
which maximizes each of the children takes linear time (with a smart implementation,
it might be possible to achieve subquadratic time). For comparison, the max-product
(with a smart implementation to keep track of solutions and evaluations) takes linear
time. While this is a drawback of the argmax-product algorithm, worst-case quadratic
time is still quite eﬃcient. More importantly, argmax-product always produces an ap-
proximation at least as good as that of max-product, and possibly exponentially better:
Theorem 5. For any spn S and evidence e, we have that S(amap(S, e)) ≥ S(PD(S, e)),
where PD(S, e) is the conﬁguration returned by the max-product algorithm. Moreover,
there exists S and e such that S(amap(S, e)) > 2mS(PD(S, e)), where m is the number
of sum nodes in S.

Proof. It is not diﬃcult to see that S(amap(S, e)) ≥ S(PD(S, e)), because the conﬁg-
uration that is selected by max-product at each sum node is one of the conﬁgurations
that are tried by the maximization of argmax-product (and both algorithms perform
the same operation on leaves and product nodes). To see that this improvement can be
exponentially better, consider the spn Si in Figure 4. Let pd(Si, e) be deﬁned as in the
proof of Theorem 4. One can verify that pd(Si, e) = 5/16, while

Si(amap(Si, e)) = 3 · 11/48 = 11/16 > 2 · 5/16 .
Now, create an spn S with a product root node connected to children S1, . . . , Sm as
described (note that the scope of S is X1, . . . , Xm). Then,

S(amap(S, e)) = (11/16)m > 2m(5/16)m = 2m · pd(S, e) .

The result follows as (for this network) pd(S, e) = S(PD(S, e)).

As an immediate result, the solutions produced by argmax-product achieve the up-
per bound on the complexity of approximating map. We hope that this simple result
motivates researchers to seek for more sophisticated algorithms that exhibit the time
performance of max-product while achieving the accuracy of argmax-product.

Si

+

12

4 Experiments

We perform two sets of experiments to verify empirically the diﬀerence between argmax-
product and max-product. We emphasize that our main motivation is to understand the
complexity of the problem and how much it can be approximated eﬃciently in practice.
In the light of Theorem 5, one may suggest that a map problem instance is easy to
approximate when argmax-product and max-product produce similar approximations.
In the ﬁrst set of evaluations, we build spns from random instances for the maximum
independent set problem, that is, we generate random undirected graphs with number of
vertices given in the ﬁrst column of Table 2 and number of edges presented as percentage
of the maximum number of edges (that is, the number of edges in a complete graph).
For each graph, we apply the transformation presented in the proof of Theorem 1 to
obtain an spn. The number of nodes of such spn is given in the third column of the
table. The fourth column shows the ratio of the values of the conﬁgurations found by
argmax-product and max-product (that is, S(amap(S, e))/S(PD(S, e))), averaged over
100 random repetitions. Take the ﬁrst row: On average, the result of argmax-product is
1.58 times better than the result of max-product in spns encoding maximum independent
set problems with graphs of 5 vertices and 10% of edges (which creates spns of 31 nodes:
5 · 5 = 25 leaves, 5 product nodes and one sum node, same structure as exempliﬁed in
Figure 2). The standard deviation for these ratios are also presented (last column in
the table). It is clear that argmax-product obtains results that are signiﬁcantly better
than max-product, often surpassing the 2m = 2 ratio lower bound in Theorem 5. While
in theory argmax-product can be signiﬁcantly slower than max-product, we did not
observe signiﬁcant diﬀerences in running time for these spns with up to 6481 nodes
(either algorithm terminated almost instantaneously).

In the second set of evaluations, we use realistic spns that model datasets from the UCI
Repository.2 The spns are learned using an implementation of the ideas presented in [13].
For each dataset, we use a random sample of 90% of the dataset for learning the model
and save the remaining 10% to be used as test. Then we perform two types of queries.
The ﬁrst type of query consists in using the learned network to compute the mode of the
corresponding distribution, that is, running both algorithms with no evidence. In the
second type, we split the set of variables in half; for each instance (row) in the test set,
we set the respective evidence for the variables in the ﬁrst half, and compute the map
conﬁguration/value for the other half. This allows us to assess the eﬀect of evidence in
the approximation complexity. The results are summarized in Table 3. The ﬁrst three
columns display information about the dataset (name, number of variables and number
of samples); the middle four columns display information about the learned spn (total
number of nodes, number of sum nodes, number of product nodes and height); the last
three columns show the approximation results: the ratio of probabilities of solutions
found by argmax-product and max-product in the test cases with no evidence, the same
ratio in test cases with 50% of variables given as evidence, and the standard deviation
for the latter (as it is run over diﬀerent evidences corresponding to 10% of the data). We

2Obtained at http://archive.ics.uci.edu/ml/.

13

Vertices % Edges Nodes Ratio Std. Dev.

5
5
5
5
10
10
10
10
20
20
20
20
40
40
40
40
80
80
80
80

10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60

31
31
31
31
111
111
111
111
421
421
421
421
1641
1641
1641
1641
6481
6481
6481
6481

1.58
1.72
1.61
1.57
1.89
2.16
2.12
2.04
1.94
2.89
3.02
2.60
2.64
3.37
2.33
2.27
3.96
2.10
1.07
1.04

0.76
0.78
0.75
0.60
0.95
1.09
1.01
0.89
0.88
1.72
1.24
1.04
1.42
1.45
0.73
0.76
1.81
0.49
0.26
0.20

14

Table 2: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns encoding randomly generated instances of
the maximum independent set problem. Vertices and (percentage of) edges are
related to the maximum independent set problem; while nodes is the number
of nodes in the corresponding spn. Each row summarizes 100 repetitions. The
last two columns report the ratio of the probabilities of conﬁgurations found by
argmax-product and max-product, together with its standard deviation.

Dataset

audiology
breast-cancer
car
cylinder-bands
ﬂags
ionosphere
nursery
primary-tumor
sonar
vowel

No. of
variables

No. of No. of
nodes
samples

No. of
No. of
nodes + nodes ×

Height

No Evidence
Ratio

50% Evidence
StDev
Ratio

70
10
7
33
29
34
9
18
61
14

204
258
1556
487
175
316
11665
306
188
892

13513
1357
16
3129
787
603
20
804
1057
23

28
23
2
61
25
43
2
113
101
2

27
24
3
62
26
44
3
114
102
3

12
24
3
62
26
42
3
114
26
3

1.0000
1.1572
1.1028
1.1154
1.3568
1.1176
1.6225
1.0882
1.2380
1.0751

1.0029
1.1977
1.0514
1.1185
1.3654
1.1109
1.2060
1.0828
1.2314
1.0666

0.0133
0.1923
0.0514
0.0220
0.0363
0.0273
0.2926
0.0210
0.0261
0.0229

Table 3: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns learned from UCI datasets with height at
least 2. The columns indicate the number of variables and samples of each
dataset; total number of nodes, sum nodes, product nodes and height of the
learned spn; ratio between argmax-product and max-product solutions for test
cases witouth evidence, for test cases with 50% of variabels as evidence, and
the standard deviation of the latter.

only show datasets where the learned spn has at least one sum node, since in the other
cases the map can be trivially found and a comparison would be pointless. The results
suggest that in these spns learned from real data, the diﬀerence between argmax-product
and max-product is less prominent, yet non negligible. We also see that the complexity
of approximation is not considerably aﬀected by the presence of evidence.

5 Conclusion

We analyzed the complexity of maximum a posteriori inference in sum-product networks
and showed that it relates with the height of the underlying graph. We ﬁrst provided an
alternative (and more direct) proof of np-hardness of maximum a posteriori inference
in sum-product networks. Our proof uses a reduction from maximum independent set
in undirected graphs, from which we obtain the non-approximability for any sublinear
factor in the size of input, even in networks of height 2 and no evidence. We then showed
that this limit is tight, that is, that there is a polynomial-time algorithm that produces
solutions which are at most a linear factor for networks of height 2. We also showed that
in networks of height 3 or more, complexity of approximation increases considerably:
there is no approximation within a factor 2f (n), for any sublinear function f of the input
size n. This is also a tight bound, as we showed that the usual max-product algorithm
ﬁnds an approximation within factor 2c·n for some constant c < 1. Last, we showed
that a simple modiﬁcation to max-product results in an algorithm that is at least as
good, and possibly greatly superior to max-product. We compared both algorithms
in two diﬀerent types of networks: shallow sum-product networks that encode random
instances of the maximum independent set problem and deeper sum-product networks

15

learned from real-world datasets. The empirical results show that while the proposed
algorithm produces better solutions than max-product does, this improvement is less
pronounced in the deeper realistic networks than in the shallower synthetic networks.
This suggests that characteristics other than the height of the network might be equally
important in determining the hardness of approximating maximum a posteriori inference,
and that further (theoretical and empirical) investigations are required. We hope that
these results foster research on approximation algorithms for maximum a posteriori
inference in sum-product networks.

Acknowledgements

Our implementation of the argmax-product algorithm was built on top of the GoSPN
library (https://github.com/RenatoGeh/gospn). We thank Jun Mei for spotting a
typo in the proof of Theorem 3. The second author received ﬁnancial support from the
S˜ao Paulo Research Foundation (FAPESP) grant #2016/01055-1 and the CNPq grants
#420669/2016-7 and PQ #303920/2016-5.

References

[1] M. R. Amer. Sum-product networks for modeling activities with stochastic struc-
In Proceedings of the IEEE Conference on Computer Vision and Pattern

ture.
Recognition, pages 1314–1321, 2012.

[2] M. R. Amer and S. Todorovic. Sum product networks for activity recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(4):800–813, 2016.

[3] Y. Boykov, O. Veksler, and R. Zabih. Markov random ﬁelds with eﬃcient approx-
imations. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, pages 648–655, 1998.

[4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.

[5] A. Bulatov, M. Dyer, L. A. Goldberg, M. Jalsenius, M. Jerrum, and D. Richerby.
The complexity of weighted and unweighted #CSP. Journal of Computer and
System Sciences, 78(2):681–688, 2012.

[6] W.-C. Cheng, S. Kok, H. V. Pham, H. L. Chieu, and K. M. A. Chai. Language
modeling with sum-product networks. In Proceedings of the Fifteenth Annual Con-
ference of the International Speech Communication Association, 2014.

[7] A. Choi and A. ˜Darwiche. On relaxing determinism in arithmetic circuits. In Pro-
ceedings of the Thirty-Fourth International Conference on Machine Learning, pages
825–833, 2017.

16

[8] D. Conaty, D. D. Mau´a, and C. P. de Campos. Approximation complexity of
maximum a posteriori in sum-product networks. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artiﬁcial Intelligence, pages 322–331, 2017.

[9] A. Darwiche. A diﬀerential approach to inference in Bayesian networks. Journal of

the ACM, 50(3):280–305, 2003.

[10] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge Uni-

versity Press, 2009.

[11] C. P. de Campos. New complexity results for MAP in Bayesian networks. In Proceed-
ings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence,
pages 2100–2106, 2011.

[12] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6(6):721–741, 1984.

[13] R. Gens and P. Domingos. Learning the structure of sum-product networks.

In
Proceedings of Thirtieth International Conference on Machine Learning, pages 873–
880, 2013.

[14] D. Lowd and P. Domingos. Naive Bayes models for probability estimation.

In
Proceedings of the Twenty-Second International Conference on Machine Learning,
pages 529–536, 2005.

[15] T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for
parsing with non-projective head automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing, pages 1288–1298, 2010.

[16] M. Meil˘a and M. I. Jordan. Learning with mixtures of trees. Journal of Machine

Learning Research 1:1–48, 2000.

[17] A. Nath and P. Domingos. Learning tractable probabilistic models for fault local-
ization. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
pages 1294–1301, 2016.

[18] R. Peharz. Foundations of sum-product networks for probabilistic modeling. PhD

thesis, 2015.

Addendum. April 6, 2017.

[19] R. Peharz. Foundations of sum-product networks for probabilistic modeling - Errata.

[20] R. Peharz, R. Gens, F. Pernkopf, and P. Domingos. On the latent variable inter-
pretation in sum-product networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1–14, 2016.

17

[21] R. Peharz, G. Kapeller, P. Mowlaee, and F. Pernkopf. Modeling speech with sum-
In IEEE International

product networks: Application to bandwidth extension.
Conference on Acoustics, Speech and Signal Processing, pages 3699–3703, 2014.

[22] R. Peharz, S. Tschiatschek, F. Pernkopf, and P. Domingos. On theoretical properties
of sum-product networks. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, pages 744–752, 2015.

[23] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In
Proceedings of Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence,
pages 337–346, 2011.

[24] A. Rooshenas and D. Lowd. Learning tractable graphical models using mixture of
arithmetic circuits. In AAAI Workshop Late-Breaking Developments in the Field
of Artiﬁcial Intelligence, pages 104–106, 2013.

[25] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect
variable interactions. In Proceedings of the Thirty-First International Conference
on Machine Learning, pages 710–718, 2014.

[26] M. Steinder and A. Sethi. Probabilistic fault localization in communication systems
IEEE/ACM Transactions on Networking, 12(5):809–822,

using belief networks.
2004.

[27] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala,
M. Tappen, and C. Rother. A comparative study of energy minimization methods
for Markov random ﬁelds with smoothness-based priors.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 30(6):1068–1080, 2008.

[28] A. Vergari, N. Di Mauro, and F. Esposito. Simplifying, regularizing and strengthen-
ing sum-product network structure learning. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery in Databases, pages 343–358,
2015.

[29] H. Zhao, M. Melibari, and P. Poupart. On the relationship between sum-product
networks and Bayesian networks. In Proceedings of the 32nd International Confer-
ence on Machine Learning, pages 116–124, 2015.

[30] C. Zirn, M. Niepert, M. Strube, and H. Stuckenschmidt. Fine-grained sentiment
analysis with structural features. Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing, pages 336–344, 2011.

[31] D. Zuckerman. Linear degree extractors and the inapproximability of max clique

and chromatic number. Theory of Computing, 3:103–128, 2007.

18

Approximation Complexity of
Maximum A Posteriori Inference
in Sum-Product Networks∗

Diarmaid Conaty
Queen’s University Belfast, UK

Denis D. Mau´a
Universidade de S˜ao Paulo, Brazil

Cassio P. de Campos
Queen’s University Belfast, UK

We discuss the computational complexity of approximating maximum a
posteriori inference in sum-product networks. We ﬁrst show np-hardness in
trees of height two by a reduction from maximum independent set; this im-
plies non-approximability within a sublinear factor. We show that this is a
tight bound, as we can ﬁnd an approximation within a linear factor in net-
works of height two. We then show that, in trees of height three, it is np-hard
to approximate the problem within a factor 2f (n) for any sublinear function
f of the size of the input n. Again, this bound is tight, as we prove that the
usual max-product algorithm ﬁnds (in any network) approximations within
factor 2c·n for some constant c < 1. Last, we present a simple algorithm,
and show that it provably produces solutions at least as good as, and poten-
tially much better than, the max-product algorithm. We empirically analyze
the proposed algorithm against max-product using synthetic and realistic
networks.

7
1
0
2
 
p
e
S
 
5
 
 
]
I

A
.
s
c
[
 
 
5
v
5
4
0
6
0
.
3
0
7
1
:
v
i
X
r
a

∗

A similar version of this manuscript appeared in [8].

1

1 Introduction

Finding the maximum probability conﬁguration is a key step of many solutions to prob-
lems in artiﬁcial intelligence such as image segmentation [12], 3D image reconstruction
[3], natural language processing [15], speech recognition [21], sentiment analysis [30],
protein design [27] and multicomponent fault diagnosis [26], to name but a few. This
problem is often called (full) maximum a posteriori (map) inference, or most likely ex-
planation (mpe).

Sum-Product Networks (spns) are a relatively new class of graphical models that allow
marginal inference in linear time in their size [23]. This is therefore in sharp diﬀerence
with other graphical models such as Bayesian networks and Markov Random Fields that
require #p-hard eﬀort to produce marginal inferences [10]. Intuitively, an spn encodes
an arithmetic circuit whose evaluation produces a marginal inference [9]. spns have
received increasing popularity in applications of machine learning due to their ability to
represent complex and highly multidimensional distributions [23, 1, 21, 6, 17, 2].

In his PhD thesis, Peharz showed a direct proof of np-hardness of map in spns by
a reduction from maximum satisﬁability [18, Theorem 5.3].1 Later, Peharz et al. [20]
noted that np-hardness can be proved by transforming a Bayesian network with a naive
Bayes structure into a distribution-equivalent spn of height two (this is done by adding a
sum node to represent the latent root variable and its marginal distribution and product
nodes as children to represent the conditional distributions). As map inference in the
former is np-hard [11], the result follows.

In this paper, we show a direct proof of np-hardness of map inference by a reduction
from maximum independent set, the problem of deciding whether there is a subset of
vertices of a certain size in an undirected graph such that no two vertices in the set
are connected. This new proof is quite simple, and (as with the reduction from naive
Bayesian networks) uses a sum-product network of height two. An advantage of the new
proof is that, as a corollary, we obtain the non-approximability of map inference within
a sublinear factor in networks of height two. This is a tight bound, as we show that there
is a polynomial-time algorithm that produces approximations within a linear factor in
networks of height two. Note that an spn of height two is equivalent to a ﬁnite mixture of
tractable distributions (e.g. an ensemble of bounded-treewidth Bayesian networks) [24].
We prove that, in networks of height three, it is np-hard to approximate the problem
within any factor 2f (n) for any sublinear function f of the input size n, even if the spn
is a tree. This a tight bound, as we show that the usual max-product algorithm [9, 23],
which replaces sums with maximizations, ﬁnds an approximation within a factor 2c·n for
some constant c < 1. Table 1 summarizes these results. As far as we are concerned,
these are the ﬁrst results about the complexity of approximating map in spns.

We also show that a simple modiﬁcation to the max-product algorithm leads to an
algorithm that produces solutions which are never worse and potentially signiﬁcantly
better than the solutions produced by max-product. We compare the performance of

1The original proof was incorrect, as it encoded clauses by products; a corrected proof was provided by

the author in an erratum note [19].

2

Height Lower bound Upper bound

1
2
≥ 3

1
(m − 1)ε
2sε

1
m − 1
2s

Table 1: Lower and upper bounds on the approximation threshold for a polynomial-time
algorithm: s denotes the size of the instance, m is the number of internal nodes,
ε is a nonnegative number less than 1.

the proposed algorithm against max-product in several structured prediction tasks using
both synthetic networks and spns learned from real-world data. The synthetic networks
encode instances of maximum independent set problems. The purpose of these networks
is to evaluate the quality of solutions produced by both algorithms on shallow spns which
(possibly) encode hard to approximate map problems. Deeper networks are learned
using the LearnSPN algorithm by Gens and Domingos [13]. The purpose of these
experiments is to assess the relative quality of the algorithms on spns from realistic
datasets, and their sensitivity to evidence. The empirical results show that the proposed
algorithm often ﬁnds signiﬁcantly better solutions than max-product does, but that this
improvement is less pronounced in networks learned from real data. We expect these
results to foster research in new approximation algorithms for map in spns.

Before presenting the complexity results in Section 3, we ﬁrst review the deﬁnition
of sum-product networks, and comment on a few selected results from the literature
in Section 2. The experiments with the proposed modiﬁed algorithm and max-product
appear in Section 4. We conclude the paper with a review of the main results in Section 5.

2 Sum-Product Networks

We use capital letters without subscripts to denote random vectors (e.g. X), and capital
letters with subscripts to denote random variables (e.g., X1). If X is a random vector,
we call the set X composed of the random variables Xi in X its scope. The scope of a
function of a random vector is the scope of the respective random vector. In this work,
we constrain our discussion to random variables with ﬁnite domains.

Poon and Domingos [23] originally deﬁned spns as multilinear functions of indicator
variables that allow for space and time eﬃcient representation and inference.
In its
original deﬁnition spns were not constrained to represent valid distributions; this was
achieved by imposing properties of consistency and completeness. This deﬁnition more
closely resembles Darwiche’s arithmetic circuits which represent the network polynomial
of a Bayesian network [9], and also allow eﬃcient inference (in the size of the circuit).
See [7] for a recent discussion on the (dis)similarities between arithmetic circuits and
sum-product networks.

Later, Gens and Domingos [13] re-stated spns as complex mixture distributions as

follows.

3

0.2

0.3

×

×

0.5

+

×

X2

0.7

X1

0.4

X2

0.2

X1

0.9

Figure 1: A sum-product network over binary variables X1 and X2. Only the probabil-

ities P(Xi = 1) are shown.

• Any univariate distribution is an spn.

• Any weighted sum of spns with the same scope and nonnegative weights is an spn.

• Any product of spns with disjoint scopes is an spn.

This alternative deﬁnition (called generalized spns by Peharz [18]) implies decomposabil-
ity, a stricter requirement than consistency. Peharz et al. [22] showed that any consistent
spn over discrete random variables can be transformed in an equivalent decomposable
spn with a polynomial increase in size, and that weighted sums can be restricted to the
probability simplex without loss of expressivity. Hence, we assume in the following that
spns are normalized: the weights of a weighted sum add up to one. This implies that
spns specify (normalized) distributions. A similar result was obtained by Zhao et al. [29].
We note that the base of the inductive deﬁnition can also be extended to accommodate
any class of tractable distributions (e.g., Chow-Liu trees) [25, 28]. For the purposes of
this work, however, it suﬃces to consider only univariate distributions.

An spn is usually represented graphically as a weighted rooted graph where each
internal node is associated with an operation + or ×, and leaves are associated with
variables and distributions. The arcs from a sum node to its children are weighted
according to the corresponding convex combinations. The remaining arcs have implicitly
weight 1. The height of an spn is deﬁned as the maximum distance, counted as number
of arcs, from the root to a leaf of its graphical representation. Figure 1 shows an example
of an spn with scope {X1, X2} and height two. Unit weights are omitted in the ﬁgure.
Note that by deﬁnition every node represents an spn (hence a distribution) on its own;
we refer to nodes and their corresponding spns interchangeably.

Consider an spn S(X) over a random vector X = (X1, . . . , Xn). The value of S
at a point x = (x1, . . . , xn) in its domain is denoted by S(x) and deﬁned recursively as
follows. The value of a leaf node is the value of its corresponding distribution at the point
obtained by projecting x onto the scope of the node. The value of a product node is the
product of the values of its children at x. Finally, the value of a sum node is the weighted
average of its children’s values at x. For example, the value of the spn S(X1, X2) in

4

Figure 1 at the point (1, 0) is S(1, 0) = 0.2 · 0.3 · 0.4 + 0.5 · 0.4 · 0.8 + 0.3 · 0.8 · 0.9 = 0.4.
Note that since we assumed spns to be normalized, we have that

x S(x) = 1.

P

Let E ⊆ {1, . . . , n} and consider a random vector XE with scope {Xi : i ∈ E}, and an
assignment e = {Xi = ei : i ∈ E}. We write x ∼ e to denote a value of X consistent with
e (i.e., the projection of x on E is e). Given an spn S(X) representing a distribution
P(X), we denote the marginal probability P(e) =
x∼e S(x) by S(e). This value can
be computed by ﬁrst marginalizing the variables {Xj : j 6∈ E} from every (distribution
in a) leaf and then propagating values as before. Thus marginal probabilities can be
computed in time linear in the network size (considering univariate distributions are
represented as tables). The marginal probability P(X2 = 0) = 0.7 induced by the spn
in Figure 1 can be obtained by ﬁrst marginalizing leaves without {X2} (thus producing
values 1 at respective leaves), and then propagating values as before.

P

In this work, we are interested in the following computational problem with spns:

Deﬁnition 1 (Functional map inference problem). Given an spn S speciﬁed with ra-
tional weights and an assignment e, ﬁnd x∗ such that S(x∗) = maxx∼e S(x).

A more general version of the problem would be to allow some of the variables to be
summed out, while others are maximized. However, the marginalization (i.e., summing
out) of variables can performed in polynomial time as a preprocessing step, the result of
which is a map problem as stated above. We stick with the above deﬁnition for simplicity
(bearing in mind that complexity is not changed).

To prove np-completeness, we use the decision variant of the problem:

Deﬁnition 2 (Decision map inference problem). Given an spn S speciﬁed with rational
weights, an assignment e and a rational γ, decide whether maxx∼e S(x) ≥ γ.

We denote both problems by map, as the distinction to which particular (functional or
decision) version we refer should be clear from context. Clearly, np-completeness of the
decision version establishes np-hardness of the functional version. Also, approximation
complexity always refers to the functional version.

The support of an spn S is the set of conﬁgurations of its domain with positive values:
supp(S) = {x : S(x) > 0}. An spn is selective if for every sub-spn T corresponding to
a sum node in S it follows that the supports of any two children are disjoint. Peharz et
al. [20] recently showed that map is tractable in selective spns.

Here, we discuss the complexity of (approximately) solving map in general spns. We
assume that instances of the map problem are represented as bitstrings hS, ei using a rea-
sonable encoding; for instance, weights and probabilities are rational values represented
by two integers in binary notation, and graphs are represented by (a binary encoding of
their) adjacency lists.

3 Complexity Results

As we show in this section, there is a strong connection between the height of an spn
and the complexity of map inferences. First, note that an spn of height 0 is just a

5

marginal distribution. So consider an spn of height 1. If the root is a sum node, then
the network encodes a sum of univariate distributions (over the same variable), and map
can be solved trivially by enumerating all values of that variable. If on the other hand
the root is a product node, then the network encodes a distribution of fully independent
variables. Also in this case, we can solve map easily by optimizing independently for
each variable. So map in networks of height 1 or less is solvable in polynomial time.

Let us now consider spns of height 2. As already discussed in the introduction, Peharz
et al. [20] brieﬂy observed that the map problem is np-hard even for tree-shaped networks
of height 2. Here, we give the following alternative, direct proof of np-hardness of map
in spns, that allows us to obtain results on non-approximability.
Theorem 1. map in sum-product networks is np-complete even if there is no evidence,
and the underlying graph is a tree of height 2.

Proof. Membership is straightforward as we can evaluate the probability of a conﬁgura-
tion in polynomial time.

We show hardness by reduction from the np-hard problem maximum independent set
(see e.g. [31]): Given an undirected graph G = (V, E) with vertices {1, . . . , n} and an
integer v, decide whether there is an independent set of size v. An independent set is a
subset V ′ ⊆ V such that no two vertices are connected by an edge in E.

Let Ni denote the neighbors of i in V . For each i ∈ V , build a product node Si
whose children are leaf nodes Si1, . . . , Sin with scopes X1, . . . , Xn, respectively. If j ∈ Ni
then associate Sij with distribution P(Xi = 1) = 0; if j /∈ Ni ∪ {i} associate Sij with
P(Xj = 1) = 1/2; ﬁnally, associate Sii with distribution P(Xi = 1) = 1. See Figure 2 for
an example. Let ni = |Ni| be the number of neighbors of i. Then Si(x) = 1/2n−ni−1 if
xi = 1 and xj = 0 for all j ∈ Ni; and Si(x) = 0 otherwise. That is, Si(x) > 0 if there
is a set V ′ which contains i and does not contain any of its neighbors. Now connect
all product nodes Si with a root sum node parent S; specify the weight from S to Si
as wi = 2n−ni−1/c, where c =
i 2n−ni−1. Suppose there is an independent set I of
size v. Take x such that xi = 1 if i ∈ I and xi = 0 otherwise. Then S(x) = v/c.
For any conﬁguration x of the variables, let I(x) = {i : Si(x) > 0}. Then I(x) is an
independent set of size c·S(x). So suppose that there is no independent set of size v. Then
maxx S(x) < v/c. Thus, there is an independent set if and only if maxx S(x) ≥ v/c.

P

Consider a real-valued function f (hS, ei) of the encoded network S and evidence e.
An algorithm for map in spns is a f (hS, ei)-approximation if it runs in time polynomial
in the size of its input (which speciﬁes the graph, the weights, the distributions, the
evidence) and outputs a conﬁguration ˜x such that S(˜x) · f (hS, ei) ≥ maxx∼e S(x). That
is, a f (hS, ei)-approximation algorithm provides, for every instance hS, ei of the map
problem, a solution whose value is at most a factor f (hS, ei) from the optimum value.
The value f (hS, ei) is called the approximation factor. We have the following consequence
of Theorem 1:
Corollary 1. Unless p equals np, there is no (m − 1)ε-approximation algorithm for
map in spns for any 0 ≤ ε < 1, where m is the number of internal nodes of the spn,
even if there is no evidence and the underlying graph is a tree of height 2.

6

S

+

1 / 6

1/3

1
/
6

1/3

4

1

3

2

×S1

×S2

×S3

×S4

X1

1

X2

0

X3

0

X4

0

X1

0

X2

1

X3

0

X4

1/2

X1

0

X2

0

X3

1

X4

0

X1

0

X2

1/2

X3

0

X4

1

Figure 2: A sum-product network encoding the maximum independent set problem for
the graph on the right. Only the values for P(Xi = 1) are shown.

0

X1

1/2

X2

0

X3

0

X4

1

X1

1/2

X2

1

X3

0

X4

1

X1

1/2

X2

0

X3

1

X4

0

X1

1/2

X2

1

X3

0

X4

0

X1

1/2

X2

0

X3

1

X4

1

X1

1/2

X2

1

X3

1

X4

0

X1

1/2

X2

1

X3

1

X4

×

×

×

×

×

×

×

×

×

×

×

×

X1

0

X2

0

X3

1

X4

1/2

X1

1

X2

1

X3

1

X4

1/2

X1

1

X2

0

X3

0

X4

1/2

X1

0

X2

1

X3

1

X4

1/2

X1

0

X2

0

X3

0

X4

1/2

X1

1

X2

1

X3

0

X4

1/2

X1

0

X2

1

X3

0

X4

1/2

Figure 3: A sum-product network encoding the Boolean formula (¬X1 ∨ X2 ∨ ¬X3) ∧
(¬X1 ∨ X3 ∨ X4). We represent only the probabilities P(Xi = 1), and omit the
uniform weights 1/14 of the root sum node.

Proof. The proof of Theorem 1 encodes a maximum independent set problem and the
reduction is a weighted reduction (see Deﬁnition 1 in [5]), which suﬃces to prove the
result. To dispense with weighted reductions, we now give a direct proof. So suppose
that there is a (m − 1)ε-approximation algorithm for map with 0 ≤ ε < 1. Let ˜x be the
conﬁguration returned by this algorithm when applied to the spn S created in the proof
of Theorem 1 for a graph G given as input of the maximum independent set problem.
We have that

S(˜x) · c · (m − 1)ε ≥ c · max

S(x) = max
I∈I(G)

|I| ,

x

i 2n−ni−1, I(G) is the collection of independent sets of G, n is the number
where c =
of vertices in G and ni is the number of neighbors of vertex i in G. Consequently, this
algorithm is a nε-approximation for maximum independent set (note that n = m − 1 by
construction). We know that there is no nε-approximation for maximum independent
set with 0 ≤ ε < 1 unless p equals np [31], so the result follows.

P

The above result shows that mixtures (or ensembles) of selective spns are np-hard
to approximate. Note that mixtures of (selective) spns are typically obtained when

×

+

×

7

using bagging to reduce variance in complex models [4]. The result can also be used to
prove complexity for similar models such as mixture of trees [16], mixture of naive Bayes
models [14] and mixture of arithmetic circuits [24]. Corollary 1 can be read as stating
that there is probably no approximation algorithm for map with sublinear approximation
factor in the size of the input. The following result shows that this lower bound is tight:

Theorem 2. There exists a (m − 1)-approximation algorithm for map in sum-product
networks whose underlying graph has height at most 2, where m is the number of internal
nodes.

Proof. Consider a sum-product network of height 2. If the root is a product node then
the problem decomposes into independent map problems in spns of height 1; each of
those problems can be solved exactly. So assume that the root S is a sum node connected
to either leaf nodes or to nodes which are connected to leaf nodes. Solve the respective
map problem for each child Si independently (which is exact, as the corresponding spn
has height at most 1); denote by xi the corresponding solution. Note that Si(xi) is
an upper bound on the value Si(x∗), where x∗ is a (global) map conﬁguration. Let
w1, . . . , wm−1 denote the weights from the root to children S1, . . . , Sm−1. Return ˜x =
arg maxi wi · Si(xi). It follows that (m − 1)S(˜x) ≥ maxx∼e S(x). Note that this is the
same value returned by the max-product algorithm.

Thus, for networks of height 2, we have a clear divide: there is an approximation
algorithm with linear approximation factor in the number of internal nodes, and no
approximation algorithm with sublinear approximation factor in the number of inter-
nal nodes. Allowing an additional level of nodes reduces drastically the quality of the
approximations in the worst case:

Theorem 3. Unless p equals np, there is no 2sε
-approximation algorithm for map in
spns for any 0 ≤ ε < 1, where s is the size of the input, even if there is no evidence and
the underlying graph is a tree of height 3.

Proof. First, we show how to build an spn for deciding satisﬁability: Given a Boolean
formula φ in conjunctive normal form, decide if there is a satisfying truth-value assign-
ment. We assume that each clause contains exactly 3 distinct variables (np-completeness
is not altered by this assumption, but if one would like to drop it, then the weights of
the sum node we deﬁne below could be easily adjusted to account for clauses with less
than 3 variables).

Let X1, . . . , Xn denote the Boolean variables and φ1, . . . , φm denote the clauses in
the formula. For i = 1, . . . , m, consider the conjunctions φi1, . . . , φi7 over the variables
of clause φi, representing all the satisfying assignments of that clause. For each such
assignment, introduce a product node Sij encoding the respective assignment: there is
a leaf node with scope Xk whose distribution assigns all mass to value 1 (resp., 0) if
and only if Xk appears nonnegated (resp., negated) in φij; and there is a leaf node with
uniform distribution over Xk if and only if Xk does not appear on φij. See Figure 3 for
an example. For a ﬁxed conﬁguration of the random variables, the clause φi is true if and
only if one of the product nodes Si1, . . . , Si7 evaluates to 1. And since these products

8

P

P

ij Sij(x) = m/2n−3 if φ(x) is true, and

encode disjoint assignments, at most one such product is nonzero for each conﬁguration.
ij Sij(x) < m/2n−3 if φ(x)
We thus have that
is false. So introduce a sum node S with all product nodes as children and with uniform
weights 1/7m. There is a satisfying assignment for φ if and only if maxx S(x) ≥ 23−n/7.
Now take the spn above and make q copies of it with disjoint scopes: each copy contains
diﬀerent random variables X t
k, t = 1, . . . , q, at the leaves, but otherwise represents the
very same distribution/satisﬁability problem. Name each copy St, t = 1, . . . q, and let
its size be st. Denote by s′ = maxt st (note that, since they are copies, their size is
the same, apart from possible indexing, etc). Connect these copies using a product
q
node S with networks S1, . . . , Sq as children, so that S(x) =
t=1 St(x). Note that
maxx St(x) ≥ 23−n/7 if there is a satisfying assignment to the Boolean formula, and
maxx St(x) ≤ m−1
m · 23−n/7 if there is no satisfying assignment. Hence, maxx S(x) ≥
(23−n/7)q if there is a satisfying assignment and maxx S(x) ≤ ((m − 1)23−n/(7m))q if
there is no satisfying assignment. Specify

Q

q = 1 +

(ln(2) · m · (s′ + 2)ε)

1
1−ε

,

k

which is polynomial in s′, so the spn S can be constructed in polynomial time and space
and has size s < q(s′ + 2). From the deﬁnition of q, we have that

j

q >

ln(2) · m · (s′ + 2)ε

1
1−ε .

(cid:0)
Raising both sides to 1 − ε yields

(cid:1)

q > qε ln(2) · m · (s′ + 2)ε = m ln(2(q(s′+2))ε

) > m ln 2sε

.

Since 1

m ≤ ln m

m−1 for any integer m > 1, it follows that

q ln

m
m − 1

(cid:18)

(cid:19)

> ln 2sε

.

By exponentiating both sides, we arrive at

q

m
m − 1

> 2sε

hence 2sε

m − 1
m

q

< 1 ,

(cid:19)
Finally, by multiplying both sides by (23−n/7)q, we obtain

(cid:18)

(cid:19)

(cid:18)

2sε

23−n(m − 1)
7m

q

<

23−n
7

q

.

(cid:18)
Hence, if we can obtain an 2sε
-approximation for maxx S(x), then we can decide sat-
isﬁability: there is a satisfying assignment to the Boolean formula if and only if the
approximation returns a value strictly greater than (23−n(m − 1)/(7m))q .

(cid:19)

(cid:19)

(cid:18)

According to Theorem 3, there is no 2f (s)-approximation (unless p equals np) for any
sublinear function f of the input size s. The following result is used to show that this
lower bound is tight.

9

Theorem 4. Let S+ denote the sum nodes in spn S, and di be the number of children
Si∈S+ di)-approximation algorithm for map
of sum node Si ∈ S+. Then there exists a (
with input S and evidence e.

Q
Proof. There are two cases to consider, based on the value of S(e), which can be checked
If S(e) = 0, then we can return any assignment consistent with
in polynomial time.
e, as the result will be exact (and equal to zero).
If S(e) > 0, then take the max-
product algorithm [23], which consists of an upward pass where sums are replaced by
maximizations in the evaluation of an spn, and a downward pass which selects the
maximizers of the previous step. Deﬁne pd(S, e) recursively as follows. If S is a leaf
then pd(S, e) = maxx∼e S(x).
If S is a sum node, then pd(S, e) = maxj=1,...,t wj ·
pd(Sj, e), where S1, . . . , St are the children of S. Finally, if S is a product node with
t
children S1, . . . , St, then pd(S, e) =
j=1 pd(Sj, e). Note that pd(S, e) corresponds to
the upward pass of the max-product algorithm; hence it is a lower bound on the value of
the conﬁguration obtained by such algorithm. We prove that the max-product algorithm
is a (

Si∈S+ di)-approximation by proving by induction in the height of the spn that

Q

Q

pd(S, e) ≥



1
di 

max
x∼e

S(x) .

YSi∈S+
To show the base of the induction, take a network S of height 0 (i.e., containing a sin-
gle node). Then pd(S, e) = maxx∼e S(x) trivially. So take a network S with children
S1, . . . , St, and suppose (by inductive hypothesis) that pd(Sj, e) ≥ (
for every child Sj. If S is a product node, then

Si∈S+
j

1
di





) maxx∼e Sj(x)

Q

pd(S, e) =

pd(Sj, e) ≥

max
x∼e

Sj(x)

t

Yj=1

t






Yj=1

YSi∈S+

j

1
di





t

=



1
di 

Yj=1
where the last two equalities follow as the scopes of products are disjoints, which implies
that the children do not share any node. If S is a sum node, then

YSi∈S+

YSi∈S+









max
x∼e

Sj(x) =



1
di 

max
x∼e

S(x) ,

pd(S, e) = max
j=1,...,t

wj · pd(Sj, e)

1
di






≥ max

j=1,...,t 



= max

j=1,...,t  

t

j

YSi∈S+
t

wj · max
x∼e

Sj(x)

wj · max
x∼e

Sj(x)

di !

≥ max
j=1,...,t

· wj · max
x∼e

Sj(x)

Si∈S+
j
t
Q
Si∈S+ di

Q

10

=

≥

t · maxj=1,...,t wj maxx∼e Sj(x)
Si∈S+ di

Q

1
di 







YSi∈S+

max
x∼e

S(x) .

The ﬁrst inequality uses the induction hypothesis. The second inequality follows since
1/(t ·
Si∈S+ di. The last inequality follows as
maxj wj · maxx∼e Sj(x) is an upper bound on the value of any child of S. This concludes
the proof.

Si∈S+,Si6=S di) = 1/

di) ≥ 1/(t ·

Si∈S+
j

Q

Q

Q

We have the following immediate consequence, showing the tightness of Theorem 3.
Corollary 2. There exists a 2ε·s-approximation algorithm for map for some 0 < ε < 1,
where s is the size of the spn.

Proof. Assume the network has at least one sum node (otherwise we can ﬁnd an exact
solution in polynomial time). Given the result of Theorem 4, we only need to show that
Si∈S+ di < 2ε·s, with S+ the sum nodes in spn S and di be the
there is ε < 1 such that
number of children of sum node Si ∈ S+. Because s is strictly greater than the number
of nodes and arcs in the network (as we must somehow encode the graph of S), we know
that s >

Si∈S+ di. One can show that 3x/3 > x for any positive integer. Hence,

Q

P
di ≤

3di/3 =

2di log2(3)/3 = 2log2(3)/3·PSi∈S+ di < 2s log2(3)/3 < 2ε·s ,

YSi∈S+

YSi∈S+
for some ε < 0.5284.

YSi∈S+

The previous result shows that the max-product algorithm achieves tight upper bounds
on the approximation factor. This however does not rule out the existence of approxima-
tion algorithms that achieve the same (worst-case) upper bound but perform signiﬁcantly
better on average. For instance, consider the following algorithm that takes an spn S and
evidence e, and returns amap(S, e) as follows, where amap is short for argmax-product
algorithm.

Argmax-Product Algorithm

• If S is a sum node with children S1, . . . , St, then compute

amap(S, e) = arg max

wj · Sj(x) ,

x∈{x1,...,xt}

t

Xj=1

where xk = amap(Sk, e), that is, xk is the solution of the map problem ob-
tained by argmax-product for network Sk (argmax-product is run bottom-up).

• Else if S is a product node with children S1, . . . , St, then amap(S, e) is the

concatenation of amap(S1, e), . . . , amap(St, e).

11

5/16

11/48

11/48

11/48

Xi

1

Xi

0

Xi

0

Xi

0

Figure 4: Fragment of the sum-product network used to prove Theorem 5.

• Else, S is a leaf, so return amap(S, e) = arg maxx∼e S(x).

Argmax-product has a worst-case time complexity quadratic in the size of the net-
work; that is because the evaluation of all the children of a sum node with the argument
which maximizes each of the children takes linear time (with a smart implementation,
it might be possible to achieve subquadratic time). For comparison, the max-product
(with a smart implementation to keep track of solutions and evaluations) takes linear
time. While this is a drawback of the argmax-product algorithm, worst-case quadratic
time is still quite eﬃcient. More importantly, argmax-product always produces an ap-
proximation at least as good as that of max-product, and possibly exponentially better:
Theorem 5. For any spn S and evidence e, we have that S(amap(S, e)) ≥ S(PD(S, e)),
where PD(S, e) is the conﬁguration returned by the max-product algorithm. Moreover,
there exists S and e such that S(amap(S, e)) > 2mS(PD(S, e)), where m is the number
of sum nodes in S.

Proof. It is not diﬃcult to see that S(amap(S, e)) ≥ S(PD(S, e)), because the conﬁg-
uration that is selected by max-product at each sum node is one of the conﬁgurations
that are tried by the maximization of argmax-product (and both algorithms perform
the same operation on leaves and product nodes). To see that this improvement can be
exponentially better, consider the spn Si in Figure 4. Let pd(Si, e) be deﬁned as in the
proof of Theorem 4. One can verify that pd(Si, e) = 5/16, while

Si(amap(Si, e)) = 3 · 11/48 = 11/16 > 2 · 5/16 .
Now, create an spn S with a product root node connected to children S1, . . . , Sm as
described (note that the scope of S is X1, . . . , Xm). Then,

S(amap(S, e)) = (11/16)m > 2m(5/16)m = 2m · pd(S, e) .

The result follows as (for this network) pd(S, e) = S(PD(S, e)).

As an immediate result, the solutions produced by argmax-product achieve the up-
per bound on the complexity of approximating map. We hope that this simple result
motivates researchers to seek for more sophisticated algorithms that exhibit the time
performance of max-product while achieving the accuracy of argmax-product.

Si

+

12

4 Experiments

We perform two sets of experiments to verify empirically the diﬀerence between argmax-
product and max-product. We emphasize that our main motivation is to understand the
complexity of the problem and how much it can be approximated eﬃciently in practice.
In the light of Theorem 5, one may suggest that a map problem instance is easy to
approximate when argmax-product and max-product produce similar approximations.
In the ﬁrst set of evaluations, we build spns from random instances for the maximum
independent set problem, that is, we generate random undirected graphs with number of
vertices given in the ﬁrst column of Table 2 and number of edges presented as percentage
of the maximum number of edges (that is, the number of edges in a complete graph).
For each graph, we apply the transformation presented in the proof of Theorem 1 to
obtain an spn. The number of nodes of such spn is given in the third column of the
table. The fourth column shows the ratio of the values of the conﬁgurations found by
argmax-product and max-product (that is, S(amap(S, e))/S(PD(S, e))), averaged over
100 random repetitions. Take the ﬁrst row: On average, the result of argmax-product is
1.58 times better than the result of max-product in spns encoding maximum independent
set problems with graphs of 5 vertices and 10% of edges (which creates spns of 31 nodes:
5 · 5 = 25 leaves, 5 product nodes and one sum node, same structure as exempliﬁed in
Figure 2). The standard deviation for these ratios are also presented (last column in
the table). It is clear that argmax-product obtains results that are signiﬁcantly better
than max-product, often surpassing the 2m = 2 ratio lower bound in Theorem 5. While
in theory argmax-product can be signiﬁcantly slower than max-product, we did not
observe signiﬁcant diﬀerences in running time for these spns with up to 6481 nodes
(either algorithm terminated almost instantaneously).

In the second set of evaluations, we use realistic spns that model datasets from the UCI
Repository.2 The spns are learned using an implementation of the ideas presented in [13].
For each dataset, we use a random sample of 90% of the dataset for learning the model
and save the remaining 10% to be used as test. Then we perform two types of queries.
The ﬁrst type of query consists in using the learned network to compute the mode of the
corresponding distribution, that is, running both algorithms with no evidence. In the
second type, we split the set of variables in half; for each instance (row) in the test set,
we set the respective evidence for the variables in the ﬁrst half, and compute the map
conﬁguration/value for the other half. This allows us to assess the eﬀect of evidence in
the approximation complexity. The results are summarized in Table 3. The ﬁrst three
columns display information about the dataset (name, number of variables and number
of samples); the middle four columns display information about the learned spn (total
number of nodes, number of sum nodes, number of product nodes and height); the last
three columns show the approximation results: the ratio of probabilities of solutions
found by argmax-product and max-product in the test cases with no evidence, the same
ratio in test cases with 50% of variables given as evidence, and the standard deviation
for the latter (as it is run over diﬀerent evidences corresponding to 10% of the data). We

2Obtained at http://archive.ics.uci.edu/ml/.

13

Vertices % Edges Nodes Ratio Std. Dev.

5
5
5
5
10
10
10
10
20
20
20
20
40
40
40
40
80
80
80
80

10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60

31
31
31
31
111
111
111
111
421
421
421
421
1641
1641
1641
1641
6481
6481
6481
6481

1.58
1.72
1.61
1.57
1.89
2.16
2.12
2.04
1.94
2.89
3.02
2.60
2.64
3.37
2.33
2.27
3.96
2.10
1.07
1.04

0.76
0.78
0.75
0.60
0.95
1.09
1.01
0.89
0.88
1.72
1.24
1.04
1.42
1.45
0.73
0.76
1.81
0.49
0.26
0.20

14

Table 2: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns encoding randomly generated instances of
the maximum independent set problem. Vertices and (percentage of) edges are
related to the maximum independent set problem; while nodes is the number
of nodes in the corresponding spn. Each row summarizes 100 repetitions. The
last two columns report the ratio of the probabilities of conﬁgurations found by
argmax-product and max-product, together with its standard deviation.

Dataset

audiology
breast-cancer
car
cylinder-bands
ﬂags
ionosphere
nursery
primary-tumor
sonar
vowel

No. of
variables

No. of No. of
nodes
samples

No. of
No. of
nodes + nodes ×

Height

No Evidence
Ratio

50% Evidence
StDev
Ratio

70
10
7
33
29
34
9
18
61
14

204
258
1556
487
175
316
11665
306
188
892

13513
1357
16
3129
787
603
20
804
1057
23

28
23
2
61
25
43
2
113
101
2

27
24
3
62
26
44
3
114
102
3

12
24
3
62
26
42
3
114
26
3

1.0000
1.1572
1.1028
1.1154
1.3568
1.1176
1.6225
1.0882
1.2380
1.0751

1.0029
1.1977
1.0514
1.1185
1.3654
1.1109
1.2060
1.0828
1.2314
1.0666

0.0133
0.1923
0.0514
0.0220
0.0363
0.0273
0.2926
0.0210
0.0261
0.0229

Table 3: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns learned from UCI datasets with height at
least 2. The columns indicate the number of variables and samples of each
dataset; total number of nodes, sum nodes, product nodes and height of the
learned spn; ratio between argmax-product and max-product solutions for test
cases witouth evidence, for test cases with 50% of variabels as evidence, and
the standard deviation of the latter.

only show datasets where the learned spn has at least one sum node, since in the other
cases the map can be trivially found and a comparison would be pointless. The results
suggest that in these spns learned from real data, the diﬀerence between argmax-product
and max-product is less prominent, yet non negligible. We also see that the complexity
of approximation is not considerably aﬀected by the presence of evidence.

5 Conclusion

We analyzed the complexity of maximum a posteriori inference in sum-product networks
and showed that it relates with the height of the underlying graph. We ﬁrst provided an
alternative (and more direct) proof of np-hardness of maximum a posteriori inference
in sum-product networks. Our proof uses a reduction from maximum independent set
in undirected graphs, from which we obtain the non-approximability for any sublinear
factor in the size of input, even in networks of height 2 and no evidence. We then showed
that this limit is tight, that is, that there is a polynomial-time algorithm that produces
solutions which are at most a linear factor for networks of height 2. We also showed that
in networks of height 3 or more, complexity of approximation increases considerably:
there is no approximation within a factor 2f (n), for any sublinear function f of the input
size n. This is also a tight bound, as we showed that the usual max-product algorithm
ﬁnds an approximation within factor 2c·n for some constant c < 1. Last, we showed
that a simple modiﬁcation to max-product results in an algorithm that is at least as
good, and possibly greatly superior to max-product. We compared both algorithms
in two diﬀerent types of networks: shallow sum-product networks that encode random
instances of the maximum independent set problem and deeper sum-product networks

15

learned from real-world datasets. The empirical results show that while the proposed
algorithm produces better solutions than max-product does, this improvement is less
pronounced in the deeper realistic networks than in the shallower synthetic networks.
This suggests that characteristics other than the height of the network might be equally
important in determining the hardness of approximating maximum a posteriori inference,
and that further (theoretical and empirical) investigations are required. We hope that
these results foster research on approximation algorithms for maximum a posteriori
inference in sum-product networks.

Acknowledgements

Our implementation of the argmax-product algorithm was built on top of the GoSPN
library (https://github.com/RenatoGeh/gospn). We thank Jun Mei for spotting a
typo in the proof of Theorem 3. The second author received ﬁnancial support from the
S˜ao Paulo Research Foundation (FAPESP) grant #2016/01055-1 and the CNPq grants
#420669/2016-7 and PQ #303920/2016-5.

References

[1] M. R. Amer. Sum-product networks for modeling activities with stochastic struc-
In Proceedings of the IEEE Conference on Computer Vision and Pattern

ture.
Recognition, pages 1314–1321, 2012.

[2] M. R. Amer and S. Todorovic. Sum product networks for activity recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(4):800–813, 2016.

[3] Y. Boykov, O. Veksler, and R. Zabih. Markov random ﬁelds with eﬃcient approx-
imations. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, pages 648–655, 1998.

[4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.

[5] A. Bulatov, M. Dyer, L. A. Goldberg, M. Jalsenius, M. Jerrum, and D. Richerby.
The complexity of weighted and unweighted #CSP. Journal of Computer and
System Sciences, 78(2):681–688, 2012.

[6] W.-C. Cheng, S. Kok, H. V. Pham, H. L. Chieu, and K. M. A. Chai. Language
modeling with sum-product networks. In Proceedings of the Fifteenth Annual Con-
ference of the International Speech Communication Association, 2014.

[7] A. Choi and A. ˜Darwiche. On relaxing determinism in arithmetic circuits. In Pro-
ceedings of the Thirty-Fourth International Conference on Machine Learning, pages
825–833, 2017.

16

[8] D. Conaty, D. D. Mau´a, and C. P. de Campos. Approximation complexity of
maximum a posteriori in sum-product networks. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artiﬁcial Intelligence, pages 322–331, 2017.

[9] A. Darwiche. A diﬀerential approach to inference in Bayesian networks. Journal of

the ACM, 50(3):280–305, 2003.

[10] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge Uni-

versity Press, 2009.

[11] C. P. de Campos. New complexity results for MAP in Bayesian networks. In Proceed-
ings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence,
pages 2100–2106, 2011.

[12] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6(6):721–741, 1984.

[13] R. Gens and P. Domingos. Learning the structure of sum-product networks.

In
Proceedings of Thirtieth International Conference on Machine Learning, pages 873–
880, 2013.

[14] D. Lowd and P. Domingos. Naive Bayes models for probability estimation.

In
Proceedings of the Twenty-Second International Conference on Machine Learning,
pages 529–536, 2005.

[15] T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for
parsing with non-projective head automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing, pages 1288–1298, 2010.

[16] M. Meil˘a and M. I. Jordan. Learning with mixtures of trees. Journal of Machine

Learning Research 1:1–48, 2000.

[17] A. Nath and P. Domingos. Learning tractable probabilistic models for fault local-
ization. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
pages 1294–1301, 2016.

[18] R. Peharz. Foundations of sum-product networks for probabilistic modeling. PhD

thesis, 2015.

Addendum. April 6, 2017.

[19] R. Peharz. Foundations of sum-product networks for probabilistic modeling - Errata.

[20] R. Peharz, R. Gens, F. Pernkopf, and P. Domingos. On the latent variable inter-
pretation in sum-product networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1–14, 2016.

17

[21] R. Peharz, G. Kapeller, P. Mowlaee, and F. Pernkopf. Modeling speech with sum-
In IEEE International

product networks: Application to bandwidth extension.
Conference on Acoustics, Speech and Signal Processing, pages 3699–3703, 2014.

[22] R. Peharz, S. Tschiatschek, F. Pernkopf, and P. Domingos. On theoretical properties
of sum-product networks. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, pages 744–752, 2015.

[23] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In
Proceedings of Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence,
pages 337–346, 2011.

[24] A. Rooshenas and D. Lowd. Learning tractable graphical models using mixture of
arithmetic circuits. In AAAI Workshop Late-Breaking Developments in the Field
of Artiﬁcial Intelligence, pages 104–106, 2013.

[25] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect
variable interactions. In Proceedings of the Thirty-First International Conference
on Machine Learning, pages 710–718, 2014.

[26] M. Steinder and A. Sethi. Probabilistic fault localization in communication systems
IEEE/ACM Transactions on Networking, 12(5):809–822,

using belief networks.
2004.

[27] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala,
M. Tappen, and C. Rother. A comparative study of energy minimization methods
for Markov random ﬁelds with smoothness-based priors.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 30(6):1068–1080, 2008.

[28] A. Vergari, N. Di Mauro, and F. Esposito. Simplifying, regularizing and strengthen-
ing sum-product network structure learning. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery in Databases, pages 343–358,
2015.

[29] H. Zhao, M. Melibari, and P. Poupart. On the relationship between sum-product
networks and Bayesian networks. In Proceedings of the 32nd International Confer-
ence on Machine Learning, pages 116–124, 2015.

[30] C. Zirn, M. Niepert, M. Strube, and H. Stuckenschmidt. Fine-grained sentiment
analysis with structural features. Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing, pages 336–344, 2011.

[31] D. Zuckerman. Linear degree extractors and the inapproximability of max clique

and chromatic number. Theory of Computing, 3:103–128, 2007.

18

Approximation Complexity of
Maximum A Posteriori Inference
in Sum-Product Networks∗

Diarmaid Conaty
Queen’s University Belfast, UK

Denis D. Mau´a
Universidade de S˜ao Paulo, Brazil

Cassio P. de Campos
Queen’s University Belfast, UK

We discuss the computational complexity of approximating maximum a
posteriori inference in sum-product networks. We ﬁrst show np-hardness in
trees of height two by a reduction from maximum independent set; this im-
plies non-approximability within a sublinear factor. We show that this is a
tight bound, as we can ﬁnd an approximation within a linear factor in net-
works of height two. We then show that, in trees of height three, it is np-hard
to approximate the problem within a factor 2f (n) for any sublinear function
f of the size of the input n. Again, this bound is tight, as we prove that the
usual max-product algorithm ﬁnds (in any network) approximations within
factor 2c·n for some constant c < 1. Last, we present a simple algorithm,
and show that it provably produces solutions at least as good as, and poten-
tially much better than, the max-product algorithm. We empirically analyze
the proposed algorithm against max-product using synthetic and realistic
networks.

7
1
0
2
 
p
e
S
 
5
 
 
]
I

A
.
s
c
[
 
 
5
v
5
4
0
6
0
.
3
0
7
1
:
v
i
X
r
a

∗

A similar version of this manuscript appeared in [8].

1

1 Introduction

Finding the maximum probability conﬁguration is a key step of many solutions to prob-
lems in artiﬁcial intelligence such as image segmentation [12], 3D image reconstruction
[3], natural language processing [15], speech recognition [21], sentiment analysis [30],
protein design [27] and multicomponent fault diagnosis [26], to name but a few. This
problem is often called (full) maximum a posteriori (map) inference, or most likely ex-
planation (mpe).

Sum-Product Networks (spns) are a relatively new class of graphical models that allow
marginal inference in linear time in their size [23]. This is therefore in sharp diﬀerence
with other graphical models such as Bayesian networks and Markov Random Fields that
require #p-hard eﬀort to produce marginal inferences [10]. Intuitively, an spn encodes
an arithmetic circuit whose evaluation produces a marginal inference [9]. spns have
received increasing popularity in applications of machine learning due to their ability to
represent complex and highly multidimensional distributions [23, 1, 21, 6, 17, 2].

In his PhD thesis, Peharz showed a direct proof of np-hardness of map in spns by
a reduction from maximum satisﬁability [18, Theorem 5.3].1 Later, Peharz et al. [20]
noted that np-hardness can be proved by transforming a Bayesian network with a naive
Bayes structure into a distribution-equivalent spn of height two (this is done by adding a
sum node to represent the latent root variable and its marginal distribution and product
nodes as children to represent the conditional distributions). As map inference in the
former is np-hard [11], the result follows.

In this paper, we show a direct proof of np-hardness of map inference by a reduction
from maximum independent set, the problem of deciding whether there is a subset of
vertices of a certain size in an undirected graph such that no two vertices in the set
are connected. This new proof is quite simple, and (as with the reduction from naive
Bayesian networks) uses a sum-product network of height two. An advantage of the new
proof is that, as a corollary, we obtain the non-approximability of map inference within
a sublinear factor in networks of height two. This is a tight bound, as we show that there
is a polynomial-time algorithm that produces approximations within a linear factor in
networks of height two. Note that an spn of height two is equivalent to a ﬁnite mixture of
tractable distributions (e.g. an ensemble of bounded-treewidth Bayesian networks) [24].
We prove that, in networks of height three, it is np-hard to approximate the problem
within any factor 2f (n) for any sublinear function f of the input size n, even if the spn
is a tree. This a tight bound, as we show that the usual max-product algorithm [9, 23],
which replaces sums with maximizations, ﬁnds an approximation within a factor 2c·n for
some constant c < 1. Table 1 summarizes these results. As far as we are concerned,
these are the ﬁrst results about the complexity of approximating map in spns.

We also show that a simple modiﬁcation to the max-product algorithm leads to an
algorithm that produces solutions which are never worse and potentially signiﬁcantly
better than the solutions produced by max-product. We compare the performance of

1The original proof was incorrect, as it encoded clauses by products; a corrected proof was provided by

the author in an erratum note [19].

2

Height Lower bound Upper bound

1
2
≥ 3

1
(m − 1)ε
2sε

1
m − 1
2s

Table 1: Lower and upper bounds on the approximation threshold for a polynomial-time
algorithm: s denotes the size of the instance, m is the number of internal nodes,
ε is a nonnegative number less than 1.

the proposed algorithm against max-product in several structured prediction tasks using
both synthetic networks and spns learned from real-world data. The synthetic networks
encode instances of maximum independent set problems. The purpose of these networks
is to evaluate the quality of solutions produced by both algorithms on shallow spns which
(possibly) encode hard to approximate map problems. Deeper networks are learned
using the LearnSPN algorithm by Gens and Domingos [13]. The purpose of these
experiments is to assess the relative quality of the algorithms on spns from realistic
datasets, and their sensitivity to evidence. The empirical results show that the proposed
algorithm often ﬁnds signiﬁcantly better solutions than max-product does, but that this
improvement is less pronounced in networks learned from real data. We expect these
results to foster research in new approximation algorithms for map in spns.

Before presenting the complexity results in Section 3, we ﬁrst review the deﬁnition
of sum-product networks, and comment on a few selected results from the literature
in Section 2. The experiments with the proposed modiﬁed algorithm and max-product
appear in Section 4. We conclude the paper with a review of the main results in Section 5.

2 Sum-Product Networks

We use capital letters without subscripts to denote random vectors (e.g. X), and capital
letters with subscripts to denote random variables (e.g., X1). If X is a random vector,
we call the set X composed of the random variables Xi in X its scope. The scope of a
function of a random vector is the scope of the respective random vector. In this work,
we constrain our discussion to random variables with ﬁnite domains.

Poon and Domingos [23] originally deﬁned spns as multilinear functions of indicator
variables that allow for space and time eﬃcient representation and inference.
In its
original deﬁnition spns were not constrained to represent valid distributions; this was
achieved by imposing properties of consistency and completeness. This deﬁnition more
closely resembles Darwiche’s arithmetic circuits which represent the network polynomial
of a Bayesian network [9], and also allow eﬃcient inference (in the size of the circuit).
See [7] for a recent discussion on the (dis)similarities between arithmetic circuits and
sum-product networks.

Later, Gens and Domingos [13] re-stated spns as complex mixture distributions as

follows.

3

0.2

0.3

×

×

0.5

+

×

X2

0.7

X1

0.4

X2

0.2

X1

0.9

Figure 1: A sum-product network over binary variables X1 and X2. Only the probabil-

ities P(Xi = 1) are shown.

• Any univariate distribution is an spn.

• Any weighted sum of spns with the same scope and nonnegative weights is an spn.

• Any product of spns with disjoint scopes is an spn.

This alternative deﬁnition (called generalized spns by Peharz [18]) implies decomposabil-
ity, a stricter requirement than consistency. Peharz et al. [22] showed that any consistent
spn over discrete random variables can be transformed in an equivalent decomposable
spn with a polynomial increase in size, and that weighted sums can be restricted to the
probability simplex without loss of expressivity. Hence, we assume in the following that
spns are normalized: the weights of a weighted sum add up to one. This implies that
spns specify (normalized) distributions. A similar result was obtained by Zhao et al. [29].
We note that the base of the inductive deﬁnition can also be extended to accommodate
any class of tractable distributions (e.g., Chow-Liu trees) [25, 28]. For the purposes of
this work, however, it suﬃces to consider only univariate distributions.

An spn is usually represented graphically as a weighted rooted graph where each
internal node is associated with an operation + or ×, and leaves are associated with
variables and distributions. The arcs from a sum node to its children are weighted
according to the corresponding convex combinations. The remaining arcs have implicitly
weight 1. The height of an spn is deﬁned as the maximum distance, counted as number
of arcs, from the root to a leaf of its graphical representation. Figure 1 shows an example
of an spn with scope {X1, X2} and height two. Unit weights are omitted in the ﬁgure.
Note that by deﬁnition every node represents an spn (hence a distribution) on its own;
we refer to nodes and their corresponding spns interchangeably.

Consider an spn S(X) over a random vector X = (X1, . . . , Xn). The value of S
at a point x = (x1, . . . , xn) in its domain is denoted by S(x) and deﬁned recursively as
follows. The value of a leaf node is the value of its corresponding distribution at the point
obtained by projecting x onto the scope of the node. The value of a product node is the
product of the values of its children at x. Finally, the value of a sum node is the weighted
average of its children’s values at x. For example, the value of the spn S(X1, X2) in

4

Figure 1 at the point (1, 0) is S(1, 0) = 0.2 · 0.3 · 0.4 + 0.5 · 0.4 · 0.8 + 0.3 · 0.8 · 0.9 = 0.4.
Note that since we assumed spns to be normalized, we have that

x S(x) = 1.

P

Let E ⊆ {1, . . . , n} and consider a random vector XE with scope {Xi : i ∈ E}, and an
assignment e = {Xi = ei : i ∈ E}. We write x ∼ e to denote a value of X consistent with
e (i.e., the projection of x on E is e). Given an spn S(X) representing a distribution
P(X), we denote the marginal probability P(e) =
x∼e S(x) by S(e). This value can
be computed by ﬁrst marginalizing the variables {Xj : j 6∈ E} from every (distribution
in a) leaf and then propagating values as before. Thus marginal probabilities can be
computed in time linear in the network size (considering univariate distributions are
represented as tables). The marginal probability P(X2 = 0) = 0.7 induced by the spn
in Figure 1 can be obtained by ﬁrst marginalizing leaves without {X2} (thus producing
values 1 at respective leaves), and then propagating values as before.

P

In this work, we are interested in the following computational problem with spns:

Deﬁnition 1 (Functional map inference problem). Given an spn S speciﬁed with ra-
tional weights and an assignment e, ﬁnd x∗ such that S(x∗) = maxx∼e S(x).

A more general version of the problem would be to allow some of the variables to be
summed out, while others are maximized. However, the marginalization (i.e., summing
out) of variables can performed in polynomial time as a preprocessing step, the result of
which is a map problem as stated above. We stick with the above deﬁnition for simplicity
(bearing in mind that complexity is not changed).

To prove np-completeness, we use the decision variant of the problem:

Deﬁnition 2 (Decision map inference problem). Given an spn S speciﬁed with rational
weights, an assignment e and a rational γ, decide whether maxx∼e S(x) ≥ γ.

We denote both problems by map, as the distinction to which particular (functional or
decision) version we refer should be clear from context. Clearly, np-completeness of the
decision version establishes np-hardness of the functional version. Also, approximation
complexity always refers to the functional version.

The support of an spn S is the set of conﬁgurations of its domain with positive values:
supp(S) = {x : S(x) > 0}. An spn is selective if for every sub-spn T corresponding to
a sum node in S it follows that the supports of any two children are disjoint. Peharz et
al. [20] recently showed that map is tractable in selective spns.

Here, we discuss the complexity of (approximately) solving map in general spns. We
assume that instances of the map problem are represented as bitstrings hS, ei using a rea-
sonable encoding; for instance, weights and probabilities are rational values represented
by two integers in binary notation, and graphs are represented by (a binary encoding of
their) adjacency lists.

3 Complexity Results

As we show in this section, there is a strong connection between the height of an spn
and the complexity of map inferences. First, note that an spn of height 0 is just a

5

marginal distribution. So consider an spn of height 1. If the root is a sum node, then
the network encodes a sum of univariate distributions (over the same variable), and map
can be solved trivially by enumerating all values of that variable. If on the other hand
the root is a product node, then the network encodes a distribution of fully independent
variables. Also in this case, we can solve map easily by optimizing independently for
each variable. So map in networks of height 1 or less is solvable in polynomial time.

Let us now consider spns of height 2. As already discussed in the introduction, Peharz
et al. [20] brieﬂy observed that the map problem is np-hard even for tree-shaped networks
of height 2. Here, we give the following alternative, direct proof of np-hardness of map
in spns, that allows us to obtain results on non-approximability.
Theorem 1. map in sum-product networks is np-complete even if there is no evidence,
and the underlying graph is a tree of height 2.

Proof. Membership is straightforward as we can evaluate the probability of a conﬁgura-
tion in polynomial time.

We show hardness by reduction from the np-hard problem maximum independent set
(see e.g. [31]): Given an undirected graph G = (V, E) with vertices {1, . . . , n} and an
integer v, decide whether there is an independent set of size v. An independent set is a
subset V ′ ⊆ V such that no two vertices are connected by an edge in E.

Let Ni denote the neighbors of i in V . For each i ∈ V , build a product node Si
whose children are leaf nodes Si1, . . . , Sin with scopes X1, . . . , Xn, respectively. If j ∈ Ni
then associate Sij with distribution P(Xi = 1) = 0; if j /∈ Ni ∪ {i} associate Sij with
P(Xj = 1) = 1/2; ﬁnally, associate Sii with distribution P(Xi = 1) = 1. See Figure 2 for
an example. Let ni = |Ni| be the number of neighbors of i. Then Si(x) = 1/2n−ni−1 if
xi = 1 and xj = 0 for all j ∈ Ni; and Si(x) = 0 otherwise. That is, Si(x) > 0 if there
is a set V ′ which contains i and does not contain any of its neighbors. Now connect
all product nodes Si with a root sum node parent S; specify the weight from S to Si
as wi = 2n−ni−1/c, where c =
i 2n−ni−1. Suppose there is an independent set I of
size v. Take x such that xi = 1 if i ∈ I and xi = 0 otherwise. Then S(x) = v/c.
For any conﬁguration x of the variables, let I(x) = {i : Si(x) > 0}. Then I(x) is an
independent set of size c·S(x). So suppose that there is no independent set of size v. Then
maxx S(x) < v/c. Thus, there is an independent set if and only if maxx S(x) ≥ v/c.

P

Consider a real-valued function f (hS, ei) of the encoded network S and evidence e.
An algorithm for map in spns is a f (hS, ei)-approximation if it runs in time polynomial
in the size of its input (which speciﬁes the graph, the weights, the distributions, the
evidence) and outputs a conﬁguration ˜x such that S(˜x) · f (hS, ei) ≥ maxx∼e S(x). That
is, a f (hS, ei)-approximation algorithm provides, for every instance hS, ei of the map
problem, a solution whose value is at most a factor f (hS, ei) from the optimum value.
The value f (hS, ei) is called the approximation factor. We have the following consequence
of Theorem 1:
Corollary 1. Unless p equals np, there is no (m − 1)ε-approximation algorithm for
map in spns for any 0 ≤ ε < 1, where m is the number of internal nodes of the spn,
even if there is no evidence and the underlying graph is a tree of height 2.

6

S

+

1 / 6

1/3

1
/
6

1/3

4

1

3

2

×S1

×S2

×S3

×S4

X1

1

X2

0

X3

0

X4

0

X1

0

X2

1

X3

0

X4

1/2

X1

0

X2

0

X3

1

X4

0

X1

0

X2

1/2

X3

0

X4

1

Figure 2: A sum-product network encoding the maximum independent set problem for
the graph on the right. Only the values for P(Xi = 1) are shown.

0

X1

1/2

X2

0

X3

0

X4

1

X1

1/2

X2

1

X3

0

X4

1

X1

1/2

X2

0

X3

1

X4

0

X1

1/2

X2

1

X3

0

X4

0

X1

1/2

X2

0

X3

1

X4

1

X1

1/2

X2

1

X3

1

X4

0

X1

1/2

X2

1

X3

1

X4

×

×

×

×

×

×

×

×

×

×

×

×

X1

0

X2

0

X3

1

X4

1/2

X1

1

X2

1

X3

1

X4

1/2

X1

1

X2

0

X3

0

X4

1/2

X1

0

X2

1

X3

1

X4

1/2

X1

0

X2

0

X3

0

X4

1/2

X1

1

X2

1

X3

0

X4

1/2

X1

0

X2

1

X3

0

X4

1/2

Figure 3: A sum-product network encoding the Boolean formula (¬X1 ∨ X2 ∨ ¬X3) ∧
(¬X1 ∨ X3 ∨ X4). We represent only the probabilities P(Xi = 1), and omit the
uniform weights 1/14 of the root sum node.

Proof. The proof of Theorem 1 encodes a maximum independent set problem and the
reduction is a weighted reduction (see Deﬁnition 1 in [5]), which suﬃces to prove the
result. To dispense with weighted reductions, we now give a direct proof. So suppose
that there is a (m − 1)ε-approximation algorithm for map with 0 ≤ ε < 1. Let ˜x be the
conﬁguration returned by this algorithm when applied to the spn S created in the proof
of Theorem 1 for a graph G given as input of the maximum independent set problem.
We have that

S(˜x) · c · (m − 1)ε ≥ c · max

S(x) = max
I∈I(G)

|I| ,

x

i 2n−ni−1, I(G) is the collection of independent sets of G, n is the number
where c =
of vertices in G and ni is the number of neighbors of vertex i in G. Consequently, this
algorithm is a nε-approximation for maximum independent set (note that n = m − 1 by
construction). We know that there is no nε-approximation for maximum independent
set with 0 ≤ ε < 1 unless p equals np [31], so the result follows.

P

The above result shows that mixtures (or ensembles) of selective spns are np-hard
to approximate. Note that mixtures of (selective) spns are typically obtained when

×

+

×

7

using bagging to reduce variance in complex models [4]. The result can also be used to
prove complexity for similar models such as mixture of trees [16], mixture of naive Bayes
models [14] and mixture of arithmetic circuits [24]. Corollary 1 can be read as stating
that there is probably no approximation algorithm for map with sublinear approximation
factor in the size of the input. The following result shows that this lower bound is tight:

Theorem 2. There exists a (m − 1)-approximation algorithm for map in sum-product
networks whose underlying graph has height at most 2, where m is the number of internal
nodes.

Proof. Consider a sum-product network of height 2. If the root is a product node then
the problem decomposes into independent map problems in spns of height 1; each of
those problems can be solved exactly. So assume that the root S is a sum node connected
to either leaf nodes or to nodes which are connected to leaf nodes. Solve the respective
map problem for each child Si independently (which is exact, as the corresponding spn
has height at most 1); denote by xi the corresponding solution. Note that Si(xi) is
an upper bound on the value Si(x∗), where x∗ is a (global) map conﬁguration. Let
w1, . . . , wm−1 denote the weights from the root to children S1, . . . , Sm−1. Return ˜x =
arg maxi wi · Si(xi). It follows that (m − 1)S(˜x) ≥ maxx∼e S(x). Note that this is the
same value returned by the max-product algorithm.

Thus, for networks of height 2, we have a clear divide: there is an approximation
algorithm with linear approximation factor in the number of internal nodes, and no
approximation algorithm with sublinear approximation factor in the number of inter-
nal nodes. Allowing an additional level of nodes reduces drastically the quality of the
approximations in the worst case:

Theorem 3. Unless p equals np, there is no 2sε
-approximation algorithm for map in
spns for any 0 ≤ ε < 1, where s is the size of the input, even if there is no evidence and
the underlying graph is a tree of height 3.

Proof. First, we show how to build an spn for deciding satisﬁability: Given a Boolean
formula φ in conjunctive normal form, decide if there is a satisfying truth-value assign-
ment. We assume that each clause contains exactly 3 distinct variables (np-completeness
is not altered by this assumption, but if one would like to drop it, then the weights of
the sum node we deﬁne below could be easily adjusted to account for clauses with less
than 3 variables).

Let X1, . . . , Xn denote the Boolean variables and φ1, . . . , φm denote the clauses in
the formula. For i = 1, . . . , m, consider the conjunctions φi1, . . . , φi7 over the variables
of clause φi, representing all the satisfying assignments of that clause. For each such
assignment, introduce a product node Sij encoding the respective assignment: there is
a leaf node with scope Xk whose distribution assigns all mass to value 1 (resp., 0) if
and only if Xk appears nonnegated (resp., negated) in φij; and there is a leaf node with
uniform distribution over Xk if and only if Xk does not appear on φij. See Figure 3 for
an example. For a ﬁxed conﬁguration of the random variables, the clause φi is true if and
only if one of the product nodes Si1, . . . , Si7 evaluates to 1. And since these products

8

P

P

ij Sij(x) = m/2n−3 if φ(x) is true, and

encode disjoint assignments, at most one such product is nonzero for each conﬁguration.
ij Sij(x) < m/2n−3 if φ(x)
We thus have that
is false. So introduce a sum node S with all product nodes as children and with uniform
weights 1/7m. There is a satisfying assignment for φ if and only if maxx S(x) ≥ 23−n/7.
Now take the spn above and make q copies of it with disjoint scopes: each copy contains
diﬀerent random variables X t
k, t = 1, . . . , q, at the leaves, but otherwise represents the
very same distribution/satisﬁability problem. Name each copy St, t = 1, . . . q, and let
its size be st. Denote by s′ = maxt st (note that, since they are copies, their size is
the same, apart from possible indexing, etc). Connect these copies using a product
q
node S with networks S1, . . . , Sq as children, so that S(x) =
t=1 St(x). Note that
maxx St(x) ≥ 23−n/7 if there is a satisfying assignment to the Boolean formula, and
maxx St(x) ≤ m−1
m · 23−n/7 if there is no satisfying assignment. Hence, maxx S(x) ≥
(23−n/7)q if there is a satisfying assignment and maxx S(x) ≤ ((m − 1)23−n/(7m))q if
there is no satisfying assignment. Specify

Q

q = 1 +

(ln(2) · m · (s′ + 2)ε)

1
1−ε

,

k

which is polynomial in s′, so the spn S can be constructed in polynomial time and space
and has size s < q(s′ + 2). From the deﬁnition of q, we have that

j

q >

ln(2) · m · (s′ + 2)ε

1
1−ε .

(cid:0)
Raising both sides to 1 − ε yields

(cid:1)

q > qε ln(2) · m · (s′ + 2)ε = m ln(2(q(s′+2))ε

) > m ln 2sε

.

Since 1

m ≤ ln m

m−1 for any integer m > 1, it follows that

q ln

m
m − 1

(cid:18)

(cid:19)

> ln 2sε

.

By exponentiating both sides, we arrive at

q

m
m − 1

> 2sε

hence 2sε

m − 1
m

q

< 1 ,

(cid:19)
Finally, by multiplying both sides by (23−n/7)q, we obtain

(cid:18)

(cid:19)

(cid:18)

2sε

23−n(m − 1)
7m

q

<

23−n
7

q

.

(cid:18)
Hence, if we can obtain an 2sε
-approximation for maxx S(x), then we can decide sat-
isﬁability: there is a satisfying assignment to the Boolean formula if and only if the
approximation returns a value strictly greater than (23−n(m − 1)/(7m))q .

(cid:19)

(cid:19)

(cid:18)

According to Theorem 3, there is no 2f (s)-approximation (unless p equals np) for any
sublinear function f of the input size s. The following result is used to show that this
lower bound is tight.

9

Theorem 4. Let S+ denote the sum nodes in spn S, and di be the number of children
Si∈S+ di)-approximation algorithm for map
of sum node Si ∈ S+. Then there exists a (
with input S and evidence e.

Q
Proof. There are two cases to consider, based on the value of S(e), which can be checked
If S(e) = 0, then we can return any assignment consistent with
in polynomial time.
e, as the result will be exact (and equal to zero).
If S(e) > 0, then take the max-
product algorithm [23], which consists of an upward pass where sums are replaced by
maximizations in the evaluation of an spn, and a downward pass which selects the
maximizers of the previous step. Deﬁne pd(S, e) recursively as follows. If S is a leaf
then pd(S, e) = maxx∼e S(x).
If S is a sum node, then pd(S, e) = maxj=1,...,t wj ·
pd(Sj, e), where S1, . . . , St are the children of S. Finally, if S is a product node with
t
children S1, . . . , St, then pd(S, e) =
j=1 pd(Sj, e). Note that pd(S, e) corresponds to
the upward pass of the max-product algorithm; hence it is a lower bound on the value of
the conﬁguration obtained by such algorithm. We prove that the max-product algorithm
is a (

Si∈S+ di)-approximation by proving by induction in the height of the spn that

Q

Q

pd(S, e) ≥



1
di 

max
x∼e

S(x) .

YSi∈S+
To show the base of the induction, take a network S of height 0 (i.e., containing a sin-
gle node). Then pd(S, e) = maxx∼e S(x) trivially. So take a network S with children
S1, . . . , St, and suppose (by inductive hypothesis) that pd(Sj, e) ≥ (
for every child Sj. If S is a product node, then

Si∈S+
j

1
di





) maxx∼e Sj(x)

Q

pd(S, e) =

pd(Sj, e) ≥

max
x∼e

Sj(x)

t

Yj=1

t






Yj=1

YSi∈S+

j

1
di





t

=



1
di 

Yj=1
where the last two equalities follow as the scopes of products are disjoints, which implies
that the children do not share any node. If S is a sum node, then

YSi∈S+

YSi∈S+









max
x∼e

Sj(x) =



1
di 

max
x∼e

S(x) ,

pd(S, e) = max
j=1,...,t

wj · pd(Sj, e)

1
di






≥ max

j=1,...,t 



= max

j=1,...,t  

t

j

YSi∈S+
t

wj · max
x∼e

Sj(x)

wj · max
x∼e

Sj(x)

di !

≥ max
j=1,...,t

· wj · max
x∼e

Sj(x)

Si∈S+
j
t
Q
Si∈S+ di

Q

10

=

≥

t · maxj=1,...,t wj maxx∼e Sj(x)
Si∈S+ di

Q

1
di 







YSi∈S+

max
x∼e

S(x) .

The ﬁrst inequality uses the induction hypothesis. The second inequality follows since
1/(t ·
Si∈S+ di. The last inequality follows as
maxj wj · maxx∼e Sj(x) is an upper bound on the value of any child of S. This concludes
the proof.

Si∈S+,Si6=S di) = 1/

di) ≥ 1/(t ·

Si∈S+
j

Q

Q

Q

We have the following immediate consequence, showing the tightness of Theorem 3.
Corollary 2. There exists a 2ε·s-approximation algorithm for map for some 0 < ε < 1,
where s is the size of the spn.

Proof. Assume the network has at least one sum node (otherwise we can ﬁnd an exact
solution in polynomial time). Given the result of Theorem 4, we only need to show that
Si∈S+ di < 2ε·s, with S+ the sum nodes in spn S and di be the
there is ε < 1 such that
number of children of sum node Si ∈ S+. Because s is strictly greater than the number
of nodes and arcs in the network (as we must somehow encode the graph of S), we know
that s >

Si∈S+ di. One can show that 3x/3 > x for any positive integer. Hence,

Q

P
di ≤

3di/3 =

2di log2(3)/3 = 2log2(3)/3·PSi∈S+ di < 2s log2(3)/3 < 2ε·s ,

YSi∈S+

YSi∈S+
for some ε < 0.5284.

YSi∈S+

The previous result shows that the max-product algorithm achieves tight upper bounds
on the approximation factor. This however does not rule out the existence of approxima-
tion algorithms that achieve the same (worst-case) upper bound but perform signiﬁcantly
better on average. For instance, consider the following algorithm that takes an spn S and
evidence e, and returns amap(S, e) as follows, where amap is short for argmax-product
algorithm.

Argmax-Product Algorithm

• If S is a sum node with children S1, . . . , St, then compute

amap(S, e) = arg max

wj · Sj(x) ,

x∈{x1,...,xt}

t

Xj=1

where xk = amap(Sk, e), that is, xk is the solution of the map problem ob-
tained by argmax-product for network Sk (argmax-product is run bottom-up).

• Else if S is a product node with children S1, . . . , St, then amap(S, e) is the

concatenation of amap(S1, e), . . . , amap(St, e).

11

5/16

11/48

11/48

11/48

Xi

1

Xi

0

Xi

0

Xi

0

Figure 4: Fragment of the sum-product network used to prove Theorem 5.

• Else, S is a leaf, so return amap(S, e) = arg maxx∼e S(x).

Argmax-product has a worst-case time complexity quadratic in the size of the net-
work; that is because the evaluation of all the children of a sum node with the argument
which maximizes each of the children takes linear time (with a smart implementation,
it might be possible to achieve subquadratic time). For comparison, the max-product
(with a smart implementation to keep track of solutions and evaluations) takes linear
time. While this is a drawback of the argmax-product algorithm, worst-case quadratic
time is still quite eﬃcient. More importantly, argmax-product always produces an ap-
proximation at least as good as that of max-product, and possibly exponentially better:
Theorem 5. For any spn S and evidence e, we have that S(amap(S, e)) ≥ S(PD(S, e)),
where PD(S, e) is the conﬁguration returned by the max-product algorithm. Moreover,
there exists S and e such that S(amap(S, e)) > 2mS(PD(S, e)), where m is the number
of sum nodes in S.

Proof. It is not diﬃcult to see that S(amap(S, e)) ≥ S(PD(S, e)), because the conﬁg-
uration that is selected by max-product at each sum node is one of the conﬁgurations
that are tried by the maximization of argmax-product (and both algorithms perform
the same operation on leaves and product nodes). To see that this improvement can be
exponentially better, consider the spn Si in Figure 4. Let pd(Si, e) be deﬁned as in the
proof of Theorem 4. One can verify that pd(Si, e) = 5/16, while

Si(amap(Si, e)) = 3 · 11/48 = 11/16 > 2 · 5/16 .
Now, create an spn S with a product root node connected to children S1, . . . , Sm as
described (note that the scope of S is X1, . . . , Xm). Then,

S(amap(S, e)) = (11/16)m > 2m(5/16)m = 2m · pd(S, e) .

The result follows as (for this network) pd(S, e) = S(PD(S, e)).

As an immediate result, the solutions produced by argmax-product achieve the up-
per bound on the complexity of approximating map. We hope that this simple result
motivates researchers to seek for more sophisticated algorithms that exhibit the time
performance of max-product while achieving the accuracy of argmax-product.

Si

+

12

4 Experiments

We perform two sets of experiments to verify empirically the diﬀerence between argmax-
product and max-product. We emphasize that our main motivation is to understand the
complexity of the problem and how much it can be approximated eﬃciently in practice.
In the light of Theorem 5, one may suggest that a map problem instance is easy to
approximate when argmax-product and max-product produce similar approximations.
In the ﬁrst set of evaluations, we build spns from random instances for the maximum
independent set problem, that is, we generate random undirected graphs with number of
vertices given in the ﬁrst column of Table 2 and number of edges presented as percentage
of the maximum number of edges (that is, the number of edges in a complete graph).
For each graph, we apply the transformation presented in the proof of Theorem 1 to
obtain an spn. The number of nodes of such spn is given in the third column of the
table. The fourth column shows the ratio of the values of the conﬁgurations found by
argmax-product and max-product (that is, S(amap(S, e))/S(PD(S, e))), averaged over
100 random repetitions. Take the ﬁrst row: On average, the result of argmax-product is
1.58 times better than the result of max-product in spns encoding maximum independent
set problems with graphs of 5 vertices and 10% of edges (which creates spns of 31 nodes:
5 · 5 = 25 leaves, 5 product nodes and one sum node, same structure as exempliﬁed in
Figure 2). The standard deviation for these ratios are also presented (last column in
the table). It is clear that argmax-product obtains results that are signiﬁcantly better
than max-product, often surpassing the 2m = 2 ratio lower bound in Theorem 5. While
in theory argmax-product can be signiﬁcantly slower than max-product, we did not
observe signiﬁcant diﬀerences in running time for these spns with up to 6481 nodes
(either algorithm terminated almost instantaneously).

In the second set of evaluations, we use realistic spns that model datasets from the UCI
Repository.2 The spns are learned using an implementation of the ideas presented in [13].
For each dataset, we use a random sample of 90% of the dataset for learning the model
and save the remaining 10% to be used as test. Then we perform two types of queries.
The ﬁrst type of query consists in using the learned network to compute the mode of the
corresponding distribution, that is, running both algorithms with no evidence. In the
second type, we split the set of variables in half; for each instance (row) in the test set,
we set the respective evidence for the variables in the ﬁrst half, and compute the map
conﬁguration/value for the other half. This allows us to assess the eﬀect of evidence in
the approximation complexity. The results are summarized in Table 3. The ﬁrst three
columns display information about the dataset (name, number of variables and number
of samples); the middle four columns display information about the learned spn (total
number of nodes, number of sum nodes, number of product nodes and height); the last
three columns show the approximation results: the ratio of probabilities of solutions
found by argmax-product and max-product in the test cases with no evidence, the same
ratio in test cases with 50% of variables given as evidence, and the standard deviation
for the latter (as it is run over diﬀerent evidences corresponding to 10% of the data). We

2Obtained at http://archive.ics.uci.edu/ml/.

13

Vertices % Edges Nodes Ratio Std. Dev.

5
5
5
5
10
10
10
10
20
20
20
20
40
40
40
40
80
80
80
80

10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60

31
31
31
31
111
111
111
111
421
421
421
421
1641
1641
1641
1641
6481
6481
6481
6481

1.58
1.72
1.61
1.57
1.89
2.16
2.12
2.04
1.94
2.89
3.02
2.60
2.64
3.37
2.33
2.27
3.96
2.10
1.07
1.04

0.76
0.78
0.75
0.60
0.95
1.09
1.01
0.89
0.88
1.72
1.24
1.04
1.42
1.45
0.73
0.76
1.81
0.49
0.26
0.20

14

Table 2: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns encoding randomly generated instances of
the maximum independent set problem. Vertices and (percentage of) edges are
related to the maximum independent set problem; while nodes is the number
of nodes in the corresponding spn. Each row summarizes 100 repetitions. The
last two columns report the ratio of the probabilities of conﬁgurations found by
argmax-product and max-product, together with its standard deviation.

Dataset

audiology
breast-cancer
car
cylinder-bands
ﬂags
ionosphere
nursery
primary-tumor
sonar
vowel

No. of
variables

No. of No. of
nodes
samples

No. of
No. of
nodes + nodes ×

Height

No Evidence
Ratio

50% Evidence
StDev
Ratio

70
10
7
33
29
34
9
18
61
14

204
258
1556
487
175
316
11665
306
188
892

13513
1357
16
3129
787
603
20
804
1057
23

28
23
2
61
25
43
2
113
101
2

27
24
3
62
26
44
3
114
102
3

12
24
3
62
26
42
3
114
26
3

1.0000
1.1572
1.1028
1.1154
1.3568
1.1176
1.6225
1.0882
1.2380
1.0751

1.0029
1.1977
1.0514
1.1185
1.3654
1.1109
1.2060
1.0828
1.2314
1.0666

0.0133
0.1923
0.0514
0.0220
0.0363
0.0273
0.2926
0.0210
0.0261
0.0229

Table 3: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns learned from UCI datasets with height at
least 2. The columns indicate the number of variables and samples of each
dataset; total number of nodes, sum nodes, product nodes and height of the
learned spn; ratio between argmax-product and max-product solutions for test
cases witouth evidence, for test cases with 50% of variabels as evidence, and
the standard deviation of the latter.

only show datasets where the learned spn has at least one sum node, since in the other
cases the map can be trivially found and a comparison would be pointless. The results
suggest that in these spns learned from real data, the diﬀerence between argmax-product
and max-product is less prominent, yet non negligible. We also see that the complexity
of approximation is not considerably aﬀected by the presence of evidence.

5 Conclusion

We analyzed the complexity of maximum a posteriori inference in sum-product networks
and showed that it relates with the height of the underlying graph. We ﬁrst provided an
alternative (and more direct) proof of np-hardness of maximum a posteriori inference
in sum-product networks. Our proof uses a reduction from maximum independent set
in undirected graphs, from which we obtain the non-approximability for any sublinear
factor in the size of input, even in networks of height 2 and no evidence. We then showed
that this limit is tight, that is, that there is a polynomial-time algorithm that produces
solutions which are at most a linear factor for networks of height 2. We also showed that
in networks of height 3 or more, complexity of approximation increases considerably:
there is no approximation within a factor 2f (n), for any sublinear function f of the input
size n. This is also a tight bound, as we showed that the usual max-product algorithm
ﬁnds an approximation within factor 2c·n for some constant c < 1. Last, we showed
that a simple modiﬁcation to max-product results in an algorithm that is at least as
good, and possibly greatly superior to max-product. We compared both algorithms
in two diﬀerent types of networks: shallow sum-product networks that encode random
instances of the maximum independent set problem and deeper sum-product networks

15

learned from real-world datasets. The empirical results show that while the proposed
algorithm produces better solutions than max-product does, this improvement is less
pronounced in the deeper realistic networks than in the shallower synthetic networks.
This suggests that characteristics other than the height of the network might be equally
important in determining the hardness of approximating maximum a posteriori inference,
and that further (theoretical and empirical) investigations are required. We hope that
these results foster research on approximation algorithms for maximum a posteriori
inference in sum-product networks.

Acknowledgements

Our implementation of the argmax-product algorithm was built on top of the GoSPN
library (https://github.com/RenatoGeh/gospn). We thank Jun Mei for spotting a
typo in the proof of Theorem 3. The second author received ﬁnancial support from the
S˜ao Paulo Research Foundation (FAPESP) grant #2016/01055-1 and the CNPq grants
#420669/2016-7 and PQ #303920/2016-5.

References

[1] M. R. Amer. Sum-product networks for modeling activities with stochastic struc-
In Proceedings of the IEEE Conference on Computer Vision and Pattern

ture.
Recognition, pages 1314–1321, 2012.

[2] M. R. Amer and S. Todorovic. Sum product networks for activity recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(4):800–813, 2016.

[3] Y. Boykov, O. Veksler, and R. Zabih. Markov random ﬁelds with eﬃcient approx-
imations. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, pages 648–655, 1998.

[4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.

[5] A. Bulatov, M. Dyer, L. A. Goldberg, M. Jalsenius, M. Jerrum, and D. Richerby.
The complexity of weighted and unweighted #CSP. Journal of Computer and
System Sciences, 78(2):681–688, 2012.

[6] W.-C. Cheng, S. Kok, H. V. Pham, H. L. Chieu, and K. M. A. Chai. Language
modeling with sum-product networks. In Proceedings of the Fifteenth Annual Con-
ference of the International Speech Communication Association, 2014.

[7] A. Choi and A. ˜Darwiche. On relaxing determinism in arithmetic circuits. In Pro-
ceedings of the Thirty-Fourth International Conference on Machine Learning, pages
825–833, 2017.

16

[8] D. Conaty, D. D. Mau´a, and C. P. de Campos. Approximation complexity of
maximum a posteriori in sum-product networks. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artiﬁcial Intelligence, pages 322–331, 2017.

[9] A. Darwiche. A diﬀerential approach to inference in Bayesian networks. Journal of

the ACM, 50(3):280–305, 2003.

[10] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge Uni-

versity Press, 2009.

[11] C. P. de Campos. New complexity results for MAP in Bayesian networks. In Proceed-
ings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence,
pages 2100–2106, 2011.

[12] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6(6):721–741, 1984.

[13] R. Gens and P. Domingos. Learning the structure of sum-product networks.

In
Proceedings of Thirtieth International Conference on Machine Learning, pages 873–
880, 2013.

[14] D. Lowd and P. Domingos. Naive Bayes models for probability estimation.

In
Proceedings of the Twenty-Second International Conference on Machine Learning,
pages 529–536, 2005.

[15] T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for
parsing with non-projective head automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing, pages 1288–1298, 2010.

[16] M. Meil˘a and M. I. Jordan. Learning with mixtures of trees. Journal of Machine

Learning Research 1:1–48, 2000.

[17] A. Nath and P. Domingos. Learning tractable probabilistic models for fault local-
ization. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
pages 1294–1301, 2016.

[18] R. Peharz. Foundations of sum-product networks for probabilistic modeling. PhD

thesis, 2015.

Addendum. April 6, 2017.

[19] R. Peharz. Foundations of sum-product networks for probabilistic modeling - Errata.

[20] R. Peharz, R. Gens, F. Pernkopf, and P. Domingos. On the latent variable inter-
pretation in sum-product networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1–14, 2016.

17

[21] R. Peharz, G. Kapeller, P. Mowlaee, and F. Pernkopf. Modeling speech with sum-
In IEEE International

product networks: Application to bandwidth extension.
Conference on Acoustics, Speech and Signal Processing, pages 3699–3703, 2014.

[22] R. Peharz, S. Tschiatschek, F. Pernkopf, and P. Domingos. On theoretical properties
of sum-product networks. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, pages 744–752, 2015.

[23] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In
Proceedings of Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence,
pages 337–346, 2011.

[24] A. Rooshenas and D. Lowd. Learning tractable graphical models using mixture of
arithmetic circuits. In AAAI Workshop Late-Breaking Developments in the Field
of Artiﬁcial Intelligence, pages 104–106, 2013.

[25] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect
variable interactions. In Proceedings of the Thirty-First International Conference
on Machine Learning, pages 710–718, 2014.

[26] M. Steinder and A. Sethi. Probabilistic fault localization in communication systems
IEEE/ACM Transactions on Networking, 12(5):809–822,

using belief networks.
2004.

[27] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala,
M. Tappen, and C. Rother. A comparative study of energy minimization methods
for Markov random ﬁelds with smoothness-based priors.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 30(6):1068–1080, 2008.

[28] A. Vergari, N. Di Mauro, and F. Esposito. Simplifying, regularizing and strengthen-
ing sum-product network structure learning. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery in Databases, pages 343–358,
2015.

[29] H. Zhao, M. Melibari, and P. Poupart. On the relationship between sum-product
networks and Bayesian networks. In Proceedings of the 32nd International Confer-
ence on Machine Learning, pages 116–124, 2015.

[30] C. Zirn, M. Niepert, M. Strube, and H. Stuckenschmidt. Fine-grained sentiment
analysis with structural features. Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing, pages 336–344, 2011.

[31] D. Zuckerman. Linear degree extractors and the inapproximability of max clique

and chromatic number. Theory of Computing, 3:103–128, 2007.

18

Approximation Complexity of
Maximum A Posteriori Inference
in Sum-Product Networks∗

Diarmaid Conaty
Queen’s University Belfast, UK

Denis D. Mau´a
Universidade de S˜ao Paulo, Brazil

Cassio P. de Campos
Queen’s University Belfast, UK

We discuss the computational complexity of approximating maximum a
posteriori inference in sum-product networks. We ﬁrst show np-hardness in
trees of height two by a reduction from maximum independent set; this im-
plies non-approximability within a sublinear factor. We show that this is a
tight bound, as we can ﬁnd an approximation within a linear factor in net-
works of height two. We then show that, in trees of height three, it is np-hard
to approximate the problem within a factor 2f (n) for any sublinear function
f of the size of the input n. Again, this bound is tight, as we prove that the
usual max-product algorithm ﬁnds (in any network) approximations within
factor 2c·n for some constant c < 1. Last, we present a simple algorithm,
and show that it provably produces solutions at least as good as, and poten-
tially much better than, the max-product algorithm. We empirically analyze
the proposed algorithm against max-product using synthetic and realistic
networks.

7
1
0
2
 
p
e
S
 
5
 
 
]
I

A
.
s
c
[
 
 
5
v
5
4
0
6
0
.
3
0
7
1
:
v
i
X
r
a

∗

A similar version of this manuscript appeared in [8].

1

1 Introduction

Finding the maximum probability conﬁguration is a key step of many solutions to prob-
lems in artiﬁcial intelligence such as image segmentation [12], 3D image reconstruction
[3], natural language processing [15], speech recognition [21], sentiment analysis [30],
protein design [27] and multicomponent fault diagnosis [26], to name but a few. This
problem is often called (full) maximum a posteriori (map) inference, or most likely ex-
planation (mpe).

Sum-Product Networks (spns) are a relatively new class of graphical models that allow
marginal inference in linear time in their size [23]. This is therefore in sharp diﬀerence
with other graphical models such as Bayesian networks and Markov Random Fields that
require #p-hard eﬀort to produce marginal inferences [10]. Intuitively, an spn encodes
an arithmetic circuit whose evaluation produces a marginal inference [9]. spns have
received increasing popularity in applications of machine learning due to their ability to
represent complex and highly multidimensional distributions [23, 1, 21, 6, 17, 2].

In his PhD thesis, Peharz showed a direct proof of np-hardness of map in spns by
a reduction from maximum satisﬁability [18, Theorem 5.3].1 Later, Peharz et al. [20]
noted that np-hardness can be proved by transforming a Bayesian network with a naive
Bayes structure into a distribution-equivalent spn of height two (this is done by adding a
sum node to represent the latent root variable and its marginal distribution and product
nodes as children to represent the conditional distributions). As map inference in the
former is np-hard [11], the result follows.

In this paper, we show a direct proof of np-hardness of map inference by a reduction
from maximum independent set, the problem of deciding whether there is a subset of
vertices of a certain size in an undirected graph such that no two vertices in the set
are connected. This new proof is quite simple, and (as with the reduction from naive
Bayesian networks) uses a sum-product network of height two. An advantage of the new
proof is that, as a corollary, we obtain the non-approximability of map inference within
a sublinear factor in networks of height two. This is a tight bound, as we show that there
is a polynomial-time algorithm that produces approximations within a linear factor in
networks of height two. Note that an spn of height two is equivalent to a ﬁnite mixture of
tractable distributions (e.g. an ensemble of bounded-treewidth Bayesian networks) [24].
We prove that, in networks of height three, it is np-hard to approximate the problem
within any factor 2f (n) for any sublinear function f of the input size n, even if the spn
is a tree. This a tight bound, as we show that the usual max-product algorithm [9, 23],
which replaces sums with maximizations, ﬁnds an approximation within a factor 2c·n for
some constant c < 1. Table 1 summarizes these results. As far as we are concerned,
these are the ﬁrst results about the complexity of approximating map in spns.

We also show that a simple modiﬁcation to the max-product algorithm leads to an
algorithm that produces solutions which are never worse and potentially signiﬁcantly
better than the solutions produced by max-product. We compare the performance of

1The original proof was incorrect, as it encoded clauses by products; a corrected proof was provided by

the author in an erratum note [19].

2

Height Lower bound Upper bound

1
2
≥ 3

1
(m − 1)ε
2sε

1
m − 1
2s

Table 1: Lower and upper bounds on the approximation threshold for a polynomial-time
algorithm: s denotes the size of the instance, m is the number of internal nodes,
ε is a nonnegative number less than 1.

the proposed algorithm against max-product in several structured prediction tasks using
both synthetic networks and spns learned from real-world data. The synthetic networks
encode instances of maximum independent set problems. The purpose of these networks
is to evaluate the quality of solutions produced by both algorithms on shallow spns which
(possibly) encode hard to approximate map problems. Deeper networks are learned
using the LearnSPN algorithm by Gens and Domingos [13]. The purpose of these
experiments is to assess the relative quality of the algorithms on spns from realistic
datasets, and their sensitivity to evidence. The empirical results show that the proposed
algorithm often ﬁnds signiﬁcantly better solutions than max-product does, but that this
improvement is less pronounced in networks learned from real data. We expect these
results to foster research in new approximation algorithms for map in spns.

Before presenting the complexity results in Section 3, we ﬁrst review the deﬁnition
of sum-product networks, and comment on a few selected results from the literature
in Section 2. The experiments with the proposed modiﬁed algorithm and max-product
appear in Section 4. We conclude the paper with a review of the main results in Section 5.

2 Sum-Product Networks

We use capital letters without subscripts to denote random vectors (e.g. X), and capital
letters with subscripts to denote random variables (e.g., X1). If X is a random vector,
we call the set X composed of the random variables Xi in X its scope. The scope of a
function of a random vector is the scope of the respective random vector. In this work,
we constrain our discussion to random variables with ﬁnite domains.

Poon and Domingos [23] originally deﬁned spns as multilinear functions of indicator
variables that allow for space and time eﬃcient representation and inference.
In its
original deﬁnition spns were not constrained to represent valid distributions; this was
achieved by imposing properties of consistency and completeness. This deﬁnition more
closely resembles Darwiche’s arithmetic circuits which represent the network polynomial
of a Bayesian network [9], and also allow eﬃcient inference (in the size of the circuit).
See [7] for a recent discussion on the (dis)similarities between arithmetic circuits and
sum-product networks.

Later, Gens and Domingos [13] re-stated spns as complex mixture distributions as

follows.

3

0.2

0.3

×

×

0.5

+

×

X2

0.7

X1

0.4

X2

0.2

X1

0.9

Figure 1: A sum-product network over binary variables X1 and X2. Only the probabil-

ities P(Xi = 1) are shown.

• Any univariate distribution is an spn.

• Any weighted sum of spns with the same scope and nonnegative weights is an spn.

• Any product of spns with disjoint scopes is an spn.

This alternative deﬁnition (called generalized spns by Peharz [18]) implies decomposabil-
ity, a stricter requirement than consistency. Peharz et al. [22] showed that any consistent
spn over discrete random variables can be transformed in an equivalent decomposable
spn with a polynomial increase in size, and that weighted sums can be restricted to the
probability simplex without loss of expressivity. Hence, we assume in the following that
spns are normalized: the weights of a weighted sum add up to one. This implies that
spns specify (normalized) distributions. A similar result was obtained by Zhao et al. [29].
We note that the base of the inductive deﬁnition can also be extended to accommodate
any class of tractable distributions (e.g., Chow-Liu trees) [25, 28]. For the purposes of
this work, however, it suﬃces to consider only univariate distributions.

An spn is usually represented graphically as a weighted rooted graph where each
internal node is associated with an operation + or ×, and leaves are associated with
variables and distributions. The arcs from a sum node to its children are weighted
according to the corresponding convex combinations. The remaining arcs have implicitly
weight 1. The height of an spn is deﬁned as the maximum distance, counted as number
of arcs, from the root to a leaf of its graphical representation. Figure 1 shows an example
of an spn with scope {X1, X2} and height two. Unit weights are omitted in the ﬁgure.
Note that by deﬁnition every node represents an spn (hence a distribution) on its own;
we refer to nodes and their corresponding spns interchangeably.

Consider an spn S(X) over a random vector X = (X1, . . . , Xn). The value of S
at a point x = (x1, . . . , xn) in its domain is denoted by S(x) and deﬁned recursively as
follows. The value of a leaf node is the value of its corresponding distribution at the point
obtained by projecting x onto the scope of the node. The value of a product node is the
product of the values of its children at x. Finally, the value of a sum node is the weighted
average of its children’s values at x. For example, the value of the spn S(X1, X2) in

4

Figure 1 at the point (1, 0) is S(1, 0) = 0.2 · 0.3 · 0.4 + 0.5 · 0.4 · 0.8 + 0.3 · 0.8 · 0.9 = 0.4.
Note that since we assumed spns to be normalized, we have that

x S(x) = 1.

P

Let E ⊆ {1, . . . , n} and consider a random vector XE with scope {Xi : i ∈ E}, and an
assignment e = {Xi = ei : i ∈ E}. We write x ∼ e to denote a value of X consistent with
e (i.e., the projection of x on E is e). Given an spn S(X) representing a distribution
P(X), we denote the marginal probability P(e) =
x∼e S(x) by S(e). This value can
be computed by ﬁrst marginalizing the variables {Xj : j 6∈ E} from every (distribution
in a) leaf and then propagating values as before. Thus marginal probabilities can be
computed in time linear in the network size (considering univariate distributions are
represented as tables). The marginal probability P(X2 = 0) = 0.7 induced by the spn
in Figure 1 can be obtained by ﬁrst marginalizing leaves without {X2} (thus producing
values 1 at respective leaves), and then propagating values as before.

P

In this work, we are interested in the following computational problem with spns:

Deﬁnition 1 (Functional map inference problem). Given an spn S speciﬁed with ra-
tional weights and an assignment e, ﬁnd x∗ such that S(x∗) = maxx∼e S(x).

A more general version of the problem would be to allow some of the variables to be
summed out, while others are maximized. However, the marginalization (i.e., summing
out) of variables can performed in polynomial time as a preprocessing step, the result of
which is a map problem as stated above. We stick with the above deﬁnition for simplicity
(bearing in mind that complexity is not changed).

To prove np-completeness, we use the decision variant of the problem:

Deﬁnition 2 (Decision map inference problem). Given an spn S speciﬁed with rational
weights, an assignment e and a rational γ, decide whether maxx∼e S(x) ≥ γ.

We denote both problems by map, as the distinction to which particular (functional or
decision) version we refer should be clear from context. Clearly, np-completeness of the
decision version establishes np-hardness of the functional version. Also, approximation
complexity always refers to the functional version.

The support of an spn S is the set of conﬁgurations of its domain with positive values:
supp(S) = {x : S(x) > 0}. An spn is selective if for every sub-spn T corresponding to
a sum node in S it follows that the supports of any two children are disjoint. Peharz et
al. [20] recently showed that map is tractable in selective spns.

Here, we discuss the complexity of (approximately) solving map in general spns. We
assume that instances of the map problem are represented as bitstrings hS, ei using a rea-
sonable encoding; for instance, weights and probabilities are rational values represented
by two integers in binary notation, and graphs are represented by (a binary encoding of
their) adjacency lists.

3 Complexity Results

As we show in this section, there is a strong connection between the height of an spn
and the complexity of map inferences. First, note that an spn of height 0 is just a

5

marginal distribution. So consider an spn of height 1. If the root is a sum node, then
the network encodes a sum of univariate distributions (over the same variable), and map
can be solved trivially by enumerating all values of that variable. If on the other hand
the root is a product node, then the network encodes a distribution of fully independent
variables. Also in this case, we can solve map easily by optimizing independently for
each variable. So map in networks of height 1 or less is solvable in polynomial time.

Let us now consider spns of height 2. As already discussed in the introduction, Peharz
et al. [20] brieﬂy observed that the map problem is np-hard even for tree-shaped networks
of height 2. Here, we give the following alternative, direct proof of np-hardness of map
in spns, that allows us to obtain results on non-approximability.
Theorem 1. map in sum-product networks is np-complete even if there is no evidence,
and the underlying graph is a tree of height 2.

Proof. Membership is straightforward as we can evaluate the probability of a conﬁgura-
tion in polynomial time.

We show hardness by reduction from the np-hard problem maximum independent set
(see e.g. [31]): Given an undirected graph G = (V, E) with vertices {1, . . . , n} and an
integer v, decide whether there is an independent set of size v. An independent set is a
subset V ′ ⊆ V such that no two vertices are connected by an edge in E.

Let Ni denote the neighbors of i in V . For each i ∈ V , build a product node Si
whose children are leaf nodes Si1, . . . , Sin with scopes X1, . . . , Xn, respectively. If j ∈ Ni
then associate Sij with distribution P(Xi = 1) = 0; if j /∈ Ni ∪ {i} associate Sij with
P(Xj = 1) = 1/2; ﬁnally, associate Sii with distribution P(Xi = 1) = 1. See Figure 2 for
an example. Let ni = |Ni| be the number of neighbors of i. Then Si(x) = 1/2n−ni−1 if
xi = 1 and xj = 0 for all j ∈ Ni; and Si(x) = 0 otherwise. That is, Si(x) > 0 if there
is a set V ′ which contains i and does not contain any of its neighbors. Now connect
all product nodes Si with a root sum node parent S; specify the weight from S to Si
as wi = 2n−ni−1/c, where c =
i 2n−ni−1. Suppose there is an independent set I of
size v. Take x such that xi = 1 if i ∈ I and xi = 0 otherwise. Then S(x) = v/c.
For any conﬁguration x of the variables, let I(x) = {i : Si(x) > 0}. Then I(x) is an
independent set of size c·S(x). So suppose that there is no independent set of size v. Then
maxx S(x) < v/c. Thus, there is an independent set if and only if maxx S(x) ≥ v/c.

P

Consider a real-valued function f (hS, ei) of the encoded network S and evidence e.
An algorithm for map in spns is a f (hS, ei)-approximation if it runs in time polynomial
in the size of its input (which speciﬁes the graph, the weights, the distributions, the
evidence) and outputs a conﬁguration ˜x such that S(˜x) · f (hS, ei) ≥ maxx∼e S(x). That
is, a f (hS, ei)-approximation algorithm provides, for every instance hS, ei of the map
problem, a solution whose value is at most a factor f (hS, ei) from the optimum value.
The value f (hS, ei) is called the approximation factor. We have the following consequence
of Theorem 1:
Corollary 1. Unless p equals np, there is no (m − 1)ε-approximation algorithm for
map in spns for any 0 ≤ ε < 1, where m is the number of internal nodes of the spn,
even if there is no evidence and the underlying graph is a tree of height 2.

6

S

+

1 / 6

1/3

1
/
6

1/3

4

1

3

2

×S1

×S2

×S3

×S4

X1

1

X2

0

X3

0

X4

0

X1

0

X2

1

X3

0

X4

1/2

X1

0

X2

0

X3

1

X4

0

X1

0

X2

1/2

X3

0

X4

1

Figure 2: A sum-product network encoding the maximum independent set problem for
the graph on the right. Only the values for P(Xi = 1) are shown.

0

X1

1/2

X2

0

X3

0

X4

1

X1

1/2

X2

1

X3

0

X4

1

X1

1/2

X2

0

X3

1

X4

0

X1

1/2

X2

1

X3

0

X4

0

X1

1/2

X2

0

X3

1

X4

1

X1

1/2

X2

1

X3

1

X4

0

X1

1/2

X2

1

X3

1

X4

×

×

×

×

×

×

×

×

×

×

×

×

X1

0

X2

0

X3

1

X4

1/2

X1

1

X2

1

X3

1

X4

1/2

X1

1

X2

0

X3

0

X4

1/2

X1

0

X2

1

X3

1

X4

1/2

X1

0

X2

0

X3

0

X4

1/2

X1

1

X2

1

X3

0

X4

1/2

X1

0

X2

1

X3

0

X4

1/2

Figure 3: A sum-product network encoding the Boolean formula (¬X1 ∨ X2 ∨ ¬X3) ∧
(¬X1 ∨ X3 ∨ X4). We represent only the probabilities P(Xi = 1), and omit the
uniform weights 1/14 of the root sum node.

Proof. The proof of Theorem 1 encodes a maximum independent set problem and the
reduction is a weighted reduction (see Deﬁnition 1 in [5]), which suﬃces to prove the
result. To dispense with weighted reductions, we now give a direct proof. So suppose
that there is a (m − 1)ε-approximation algorithm for map with 0 ≤ ε < 1. Let ˜x be the
conﬁguration returned by this algorithm when applied to the spn S created in the proof
of Theorem 1 for a graph G given as input of the maximum independent set problem.
We have that

S(˜x) · c · (m − 1)ε ≥ c · max

S(x) = max
I∈I(G)

|I| ,

x

i 2n−ni−1, I(G) is the collection of independent sets of G, n is the number
where c =
of vertices in G and ni is the number of neighbors of vertex i in G. Consequently, this
algorithm is a nε-approximation for maximum independent set (note that n = m − 1 by
construction). We know that there is no nε-approximation for maximum independent
set with 0 ≤ ε < 1 unless p equals np [31], so the result follows.

P

The above result shows that mixtures (or ensembles) of selective spns are np-hard
to approximate. Note that mixtures of (selective) spns are typically obtained when

×

+

×

7

using bagging to reduce variance in complex models [4]. The result can also be used to
prove complexity for similar models such as mixture of trees [16], mixture of naive Bayes
models [14] and mixture of arithmetic circuits [24]. Corollary 1 can be read as stating
that there is probably no approximation algorithm for map with sublinear approximation
factor in the size of the input. The following result shows that this lower bound is tight:

Theorem 2. There exists a (m − 1)-approximation algorithm for map in sum-product
networks whose underlying graph has height at most 2, where m is the number of internal
nodes.

Proof. Consider a sum-product network of height 2. If the root is a product node then
the problem decomposes into independent map problems in spns of height 1; each of
those problems can be solved exactly. So assume that the root S is a sum node connected
to either leaf nodes or to nodes which are connected to leaf nodes. Solve the respective
map problem for each child Si independently (which is exact, as the corresponding spn
has height at most 1); denote by xi the corresponding solution. Note that Si(xi) is
an upper bound on the value Si(x∗), where x∗ is a (global) map conﬁguration. Let
w1, . . . , wm−1 denote the weights from the root to children S1, . . . , Sm−1. Return ˜x =
arg maxi wi · Si(xi). It follows that (m − 1)S(˜x) ≥ maxx∼e S(x). Note that this is the
same value returned by the max-product algorithm.

Thus, for networks of height 2, we have a clear divide: there is an approximation
algorithm with linear approximation factor in the number of internal nodes, and no
approximation algorithm with sublinear approximation factor in the number of inter-
nal nodes. Allowing an additional level of nodes reduces drastically the quality of the
approximations in the worst case:

Theorem 3. Unless p equals np, there is no 2sε
-approximation algorithm for map in
spns for any 0 ≤ ε < 1, where s is the size of the input, even if there is no evidence and
the underlying graph is a tree of height 3.

Proof. First, we show how to build an spn for deciding satisﬁability: Given a Boolean
formula φ in conjunctive normal form, decide if there is a satisfying truth-value assign-
ment. We assume that each clause contains exactly 3 distinct variables (np-completeness
is not altered by this assumption, but if one would like to drop it, then the weights of
the sum node we deﬁne below could be easily adjusted to account for clauses with less
than 3 variables).

Let X1, . . . , Xn denote the Boolean variables and φ1, . . . , φm denote the clauses in
the formula. For i = 1, . . . , m, consider the conjunctions φi1, . . . , φi7 over the variables
of clause φi, representing all the satisfying assignments of that clause. For each such
assignment, introduce a product node Sij encoding the respective assignment: there is
a leaf node with scope Xk whose distribution assigns all mass to value 1 (resp., 0) if
and only if Xk appears nonnegated (resp., negated) in φij; and there is a leaf node with
uniform distribution over Xk if and only if Xk does not appear on φij. See Figure 3 for
an example. For a ﬁxed conﬁguration of the random variables, the clause φi is true if and
only if one of the product nodes Si1, . . . , Si7 evaluates to 1. And since these products

8

P

P

ij Sij(x) = m/2n−3 if φ(x) is true, and

encode disjoint assignments, at most one such product is nonzero for each conﬁguration.
ij Sij(x) < m/2n−3 if φ(x)
We thus have that
is false. So introduce a sum node S with all product nodes as children and with uniform
weights 1/7m. There is a satisfying assignment for φ if and only if maxx S(x) ≥ 23−n/7.
Now take the spn above and make q copies of it with disjoint scopes: each copy contains
diﬀerent random variables X t
k, t = 1, . . . , q, at the leaves, but otherwise represents the
very same distribution/satisﬁability problem. Name each copy St, t = 1, . . . q, and let
its size be st. Denote by s′ = maxt st (note that, since they are copies, their size is
the same, apart from possible indexing, etc). Connect these copies using a product
q
node S with networks S1, . . . , Sq as children, so that S(x) =
t=1 St(x). Note that
maxx St(x) ≥ 23−n/7 if there is a satisfying assignment to the Boolean formula, and
maxx St(x) ≤ m−1
m · 23−n/7 if there is no satisfying assignment. Hence, maxx S(x) ≥
(23−n/7)q if there is a satisfying assignment and maxx S(x) ≤ ((m − 1)23−n/(7m))q if
there is no satisfying assignment. Specify

Q

q = 1 +

(ln(2) · m · (s′ + 2)ε)

1
1−ε

,

k

which is polynomial in s′, so the spn S can be constructed in polynomial time and space
and has size s < q(s′ + 2). From the deﬁnition of q, we have that

j

q >

ln(2) · m · (s′ + 2)ε

1
1−ε .

(cid:0)
Raising both sides to 1 − ε yields

(cid:1)

q > qε ln(2) · m · (s′ + 2)ε = m ln(2(q(s′+2))ε

) > m ln 2sε

.

Since 1

m ≤ ln m

m−1 for any integer m > 1, it follows that

q ln

m
m − 1

(cid:18)

(cid:19)

> ln 2sε

.

By exponentiating both sides, we arrive at

q

m
m − 1

> 2sε

hence 2sε

m − 1
m

q

< 1 ,

(cid:19)
Finally, by multiplying both sides by (23−n/7)q, we obtain

(cid:19)

(cid:18)

(cid:18)

2sε

23−n(m − 1)
7m

q

<

23−n
7

q

.

(cid:18)
Hence, if we can obtain an 2sε
-approximation for maxx S(x), then we can decide sat-
isﬁability: there is a satisfying assignment to the Boolean formula if and only if the
approximation returns a value strictly greater than (23−n(m − 1)/(7m))q .

(cid:19)

(cid:19)

(cid:18)

According to Theorem 3, there is no 2f (s)-approximation (unless p equals np) for any
sublinear function f of the input size s. The following result is used to show that this
lower bound is tight.

9

Theorem 4. Let S+ denote the sum nodes in spn S, and di be the number of children
Si∈S+ di)-approximation algorithm for map
of sum node Si ∈ S+. Then there exists a (
with input S and evidence e.

Q
Proof. There are two cases to consider, based on the value of S(e), which can be checked
If S(e) = 0, then we can return any assignment consistent with
in polynomial time.
e, as the result will be exact (and equal to zero).
If S(e) > 0, then take the max-
product algorithm [23], which consists of an upward pass where sums are replaced by
maximizations in the evaluation of an spn, and a downward pass which selects the
maximizers of the previous step. Deﬁne pd(S, e) recursively as follows. If S is a leaf
then pd(S, e) = maxx∼e S(x).
If S is a sum node, then pd(S, e) = maxj=1,...,t wj ·
pd(Sj, e), where S1, . . . , St are the children of S. Finally, if S is a product node with
t
children S1, . . . , St, then pd(S, e) =
j=1 pd(Sj, e). Note that pd(S, e) corresponds to
the upward pass of the max-product algorithm; hence it is a lower bound on the value of
the conﬁguration obtained by such algorithm. We prove that the max-product algorithm
is a (

Si∈S+ di)-approximation by proving by induction in the height of the spn that

Q

Q

pd(S, e) ≥



1
di 

max
x∼e

S(x) .

YSi∈S+
To show the base of the induction, take a network S of height 0 (i.e., containing a sin-
gle node). Then pd(S, e) = maxx∼e S(x) trivially. So take a network S with children
S1, . . . , St, and suppose (by inductive hypothesis) that pd(Sj, e) ≥ (
for every child Sj. If S is a product node, then

Si∈S+
j

1
di





) maxx∼e Sj(x)

Q

pd(S, e) =

pd(Sj, e) ≥

max
x∼e

Sj(x)

t

Yj=1

t






Yj=1

YSi∈S+

j

1
di





t

=



1
di 

Yj=1
where the last two equalities follow as the scopes of products are disjoints, which implies
that the children do not share any node. If S is a sum node, then

YSi∈S+

YSi∈S+









max
x∼e

Sj(x) =



1
di 

max
x∼e

S(x) ,

pd(S, e) = max
j=1,...,t

wj · pd(Sj, e)

1
di






≥ max

j=1,...,t 



= max

j=1,...,t  

t

j

YSi∈S+
t

wj · max
x∼e

Sj(x)

wj · max
x∼e

Sj(x)

di !

≥ max
j=1,...,t

· wj · max
x∼e

Sj(x)

Si∈S+
j
t
Q
Si∈S+ di

Q

10

=

≥

t · maxj=1,...,t wj maxx∼e Sj(x)
Si∈S+ di

Q

1
di 







YSi∈S+

max
x∼e

S(x) .

The ﬁrst inequality uses the induction hypothesis. The second inequality follows since
1/(t ·
Si∈S+ di. The last inequality follows as
maxj wj · maxx∼e Sj(x) is an upper bound on the value of any child of S. This concludes
the proof.

Si∈S+,Si6=S di) = 1/

di) ≥ 1/(t ·

Si∈S+
j

Q

Q

Q

We have the following immediate consequence, showing the tightness of Theorem 3.
Corollary 2. There exists a 2ε·s-approximation algorithm for map for some 0 < ε < 1,
where s is the size of the spn.

Proof. Assume the network has at least one sum node (otherwise we can ﬁnd an exact
solution in polynomial time). Given the result of Theorem 4, we only need to show that
Si∈S+ di < 2ε·s, with S+ the sum nodes in spn S and di be the
there is ε < 1 such that
number of children of sum node Si ∈ S+. Because s is strictly greater than the number
of nodes and arcs in the network (as we must somehow encode the graph of S), we know
that s >

Si∈S+ di. One can show that 3x/3 > x for any positive integer. Hence,

Q

P
di ≤

3di/3 =

2di log2(3)/3 = 2log2(3)/3·PSi∈S+ di < 2s log2(3)/3 < 2ε·s ,

YSi∈S+

YSi∈S+
for some ε < 0.5284.

YSi∈S+

The previous result shows that the max-product algorithm achieves tight upper bounds
on the approximation factor. This however does not rule out the existence of approxima-
tion algorithms that achieve the same (worst-case) upper bound but perform signiﬁcantly
better on average. For instance, consider the following algorithm that takes an spn S and
evidence e, and returns amap(S, e) as follows, where amap is short for argmax-product
algorithm.

Argmax-Product Algorithm

• If S is a sum node with children S1, . . . , St, then compute

amap(S, e) = arg max

wj · Sj(x) ,

x∈{x1,...,xt}

t

Xj=1

where xk = amap(Sk, e), that is, xk is the solution of the map problem ob-
tained by argmax-product for network Sk (argmax-product is run bottom-up).

• Else if S is a product node with children S1, . . . , St, then amap(S, e) is the

concatenation of amap(S1, e), . . . , amap(St, e).

11

5/16

11/48

11/48

11/48

Xi

1

Xi

0

Xi

0

Xi

0

Figure 4: Fragment of the sum-product network used to prove Theorem 5.

• Else, S is a leaf, so return amap(S, e) = arg maxx∼e S(x).

Argmax-product has a worst-case time complexity quadratic in the size of the net-
work; that is because the evaluation of all the children of a sum node with the argument
which maximizes each of the children takes linear time (with a smart implementation,
it might be possible to achieve subquadratic time). For comparison, the max-product
(with a smart implementation to keep track of solutions and evaluations) takes linear
time. While this is a drawback of the argmax-product algorithm, worst-case quadratic
time is still quite eﬃcient. More importantly, argmax-product always produces an ap-
proximation at least as good as that of max-product, and possibly exponentially better:
Theorem 5. For any spn S and evidence e, we have that S(amap(S, e)) ≥ S(PD(S, e)),
where PD(S, e) is the conﬁguration returned by the max-product algorithm. Moreover,
there exists S and e such that S(amap(S, e)) > 2mS(PD(S, e)), where m is the number
of sum nodes in S.

Proof. It is not diﬃcult to see that S(amap(S, e)) ≥ S(PD(S, e)), because the conﬁg-
uration that is selected by max-product at each sum node is one of the conﬁgurations
that are tried by the maximization of argmax-product (and both algorithms perform
the same operation on leaves and product nodes). To see that this improvement can be
exponentially better, consider the spn Si in Figure 4. Let pd(Si, e) be deﬁned as in the
proof of Theorem 4. One can verify that pd(Si, e) = 5/16, while

Si(amap(Si, e)) = 3 · 11/48 = 11/16 > 2 · 5/16 .
Now, create an spn S with a product root node connected to children S1, . . . , Sm as
described (note that the scope of S is X1, . . . , Xm). Then,

S(amap(S, e)) = (11/16)m > 2m(5/16)m = 2m · pd(S, e) .

The result follows as (for this network) pd(S, e) = S(PD(S, e)).

As an immediate result, the solutions produced by argmax-product achieve the up-
per bound on the complexity of approximating map. We hope that this simple result
motivates researchers to seek for more sophisticated algorithms that exhibit the time
performance of max-product while achieving the accuracy of argmax-product.

Si

+

12

4 Experiments

We perform two sets of experiments to verify empirically the diﬀerence between argmax-
product and max-product. We emphasize that our main motivation is to understand the
complexity of the problem and how much it can be approximated eﬃciently in practice.
In the light of Theorem 5, one may suggest that a map problem instance is easy to
approximate when argmax-product and max-product produce similar approximations.
In the ﬁrst set of evaluations, we build spns from random instances for the maximum
independent set problem, that is, we generate random undirected graphs with number of
vertices given in the ﬁrst column of Table 2 and number of edges presented as percentage
of the maximum number of edges (that is, the number of edges in a complete graph).
For each graph, we apply the transformation presented in the proof of Theorem 1 to
obtain an spn. The number of nodes of such spn is given in the third column of the
table. The fourth column shows the ratio of the values of the conﬁgurations found by
argmax-product and max-product (that is, S(amap(S, e))/S(PD(S, e))), averaged over
100 random repetitions. Take the ﬁrst row: On average, the result of argmax-product is
1.58 times better than the result of max-product in spns encoding maximum independent
set problems with graphs of 5 vertices and 10% of edges (which creates spns of 31 nodes:
5 · 5 = 25 leaves, 5 product nodes and one sum node, same structure as exempliﬁed in
Figure 2). The standard deviation for these ratios are also presented (last column in
the table). It is clear that argmax-product obtains results that are signiﬁcantly better
than max-product, often surpassing the 2m = 2 ratio lower bound in Theorem 5. While
in theory argmax-product can be signiﬁcantly slower than max-product, we did not
observe signiﬁcant diﬀerences in running time for these spns with up to 6481 nodes
(either algorithm terminated almost instantaneously).

In the second set of evaluations, we use realistic spns that model datasets from the UCI
Repository.2 The spns are learned using an implementation of the ideas presented in [13].
For each dataset, we use a random sample of 90% of the dataset for learning the model
and save the remaining 10% to be used as test. Then we perform two types of queries.
The ﬁrst type of query consists in using the learned network to compute the mode of the
corresponding distribution, that is, running both algorithms with no evidence. In the
second type, we split the set of variables in half; for each instance (row) in the test set,
we set the respective evidence for the variables in the ﬁrst half, and compute the map
conﬁguration/value for the other half. This allows us to assess the eﬀect of evidence in
the approximation complexity. The results are summarized in Table 3. The ﬁrst three
columns display information about the dataset (name, number of variables and number
of samples); the middle four columns display information about the learned spn (total
number of nodes, number of sum nodes, number of product nodes and height); the last
three columns show the approximation results: the ratio of probabilities of solutions
found by argmax-product and max-product in the test cases with no evidence, the same
ratio in test cases with 50% of variables given as evidence, and the standard deviation
for the latter (as it is run over diﬀerent evidences corresponding to 10% of the data). We

2Obtained at http://archive.ics.uci.edu/ml/.

13

Vertices % Edges Nodes Ratio Std. Dev.

5
5
5
5
10
10
10
10
20
20
20
20
40
40
40
40
80
80
80
80

10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60
10
20
40
60

31
31
31
31
111
111
111
111
421
421
421
421
1641
1641
1641
1641
6481
6481
6481
6481

1.58
1.72
1.61
1.57
1.89
2.16
2.12
2.04
1.94
2.89
3.02
2.60
2.64
3.37
2.33
2.27
3.96
2.10
1.07
1.04

0.76
0.78
0.75
0.60
0.95
1.09
1.01
0.89
0.88
1.72
1.24
1.04
1.42
1.45
0.73
0.76
1.81
0.49
0.26
0.20

14

Table 2: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns encoding randomly generated instances of
the maximum independent set problem. Vertices and (percentage of) edges are
related to the maximum independent set problem; while nodes is the number
of nodes in the corresponding spn. Each row summarizes 100 repetitions. The
last two columns report the ratio of the probabilities of conﬁgurations found by
argmax-product and max-product, together with its standard deviation.

Dataset

audiology
breast-cancer
car
cylinder-bands
ﬂags
ionosphere
nursery
primary-tumor
sonar
vowel

No. of
variables

No. of No. of
nodes
samples

No. of
No. of
nodes + nodes ×

Height

No Evidence
Ratio

50% Evidence
StDev
Ratio

70
10
7
33
29
34
9
18
61
14

204
258
1556
487
175
316
11665
306
188
892

13513
1357
16
3129
787
603
20
804
1057
23

28
23
2
61
25
43
2
113
101
2

27
24
3
62
26
44
3
114
102
3

12
24
3
62
26
42
3
114
26
3

1.0000
1.1572
1.1028
1.1154
1.3568
1.1176
1.6225
1.0882
1.2380
1.0751

1.0029
1.1977
1.0514
1.1185
1.3654
1.1109
1.2060
1.0828
1.2314
1.0666

0.0133
0.1923
0.0514
0.0220
0.0363
0.0273
0.2926
0.0210
0.0261
0.0229

Table 3: Empirical comparison of the quality of approximations produced by argmax-
product and max-product in spns learned from UCI datasets with height at
least 2. The columns indicate the number of variables and samples of each
dataset; total number of nodes, sum nodes, product nodes and height of the
learned spn; ratio between argmax-product and max-product solutions for test
cases witouth evidence, for test cases with 50% of variabels as evidence, and
the standard deviation of the latter.

only show datasets where the learned spn has at least one sum node, since in the other
cases the map can be trivially found and a comparison would be pointless. The results
suggest that in these spns learned from real data, the diﬀerence between argmax-product
and max-product is less prominent, yet non negligible. We also see that the complexity
of approximation is not considerably aﬀected by the presence of evidence.

5 Conclusion

We analyzed the complexity of maximum a posteriori inference in sum-product networks
and showed that it relates with the height of the underlying graph. We ﬁrst provided an
alternative (and more direct) proof of np-hardness of maximum a posteriori inference
in sum-product networks. Our proof uses a reduction from maximum independent set
in undirected graphs, from which we obtain the non-approximability for any sublinear
factor in the size of input, even in networks of height 2 and no evidence. We then showed
that this limit is tight, that is, that there is a polynomial-time algorithm that produces
solutions which are at most a linear factor for networks of height 2. We also showed that
in networks of height 3 or more, complexity of approximation increases considerably:
there is no approximation within a factor 2f (n), for any sublinear function f of the input
size n. This is also a tight bound, as we showed that the usual max-product algorithm
ﬁnds an approximation within factor 2c·n for some constant c < 1. Last, we showed
that a simple modiﬁcation to max-product results in an algorithm that is at least as
good, and possibly greatly superior to max-product. We compared both algorithms
in two diﬀerent types of networks: shallow sum-product networks that encode random
instances of the maximum independent set problem and deeper sum-product networks

15

learned from real-world datasets. The empirical results show that while the proposed
algorithm produces better solutions than max-product does, this improvement is less
pronounced in the deeper realistic networks than in the shallower synthetic networks.
This suggests that characteristics other than the height of the network might be equally
important in determining the hardness of approximating maximum a posteriori inference,
and that further (theoretical and empirical) investigations are required. We hope that
these results foster research on approximation algorithms for maximum a posteriori
inference in sum-product networks.

Acknowledgements

Our implementation of the argmax-product algorithm was built on top of the GoSPN
library (https://github.com/RenatoGeh/gospn). We thank Jun Mei for spotting a
typo in the proof of Theorem 3. The second author received ﬁnancial support from the
S˜ao Paulo Research Foundation (FAPESP) grant #2016/01055-1 and the CNPq grants
#420669/2016-7 and PQ #303920/2016-5.

References

[1] M. R. Amer. Sum-product networks for modeling activities with stochastic struc-
In Proceedings of the IEEE Conference on Computer Vision and Pattern

ture.
Recognition, pages 1314–1321, 2012.

[2] M. R. Amer and S. Todorovic. Sum product networks for activity recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(4):800–813, 2016.

[3] Y. Boykov, O. Veksler, and R. Zabih. Markov random ﬁelds with eﬃcient approx-
imations. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, pages 648–655, 1998.

[4] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.

[5] A. Bulatov, M. Dyer, L. A. Goldberg, M. Jalsenius, M. Jerrum, and D. Richerby.
The complexity of weighted and unweighted #CSP. Journal of Computer and
System Sciences, 78(2):681–688, 2012.

[6] W.-C. Cheng, S. Kok, H. V. Pham, H. L. Chieu, and K. M. A. Chai. Language
modeling with sum-product networks. In Proceedings of the Fifteenth Annual Con-
ference of the International Speech Communication Association, 2014.

[7] A. Choi and A. ˜Darwiche. On relaxing determinism in arithmetic circuits. In Pro-
ceedings of the Thirty-Fourth International Conference on Machine Learning, pages
825–833, 2017.

16

[8] D. Conaty, D. D. Mau´a, and C. P. de Campos. Approximation complexity of
maximum a posteriori in sum-product networks. In Proceedings of the Thirty-Third
Conference on Uncertainty in Artiﬁcial Intelligence, pages 322–331, 2017.

[9] A. Darwiche. A diﬀerential approach to inference in Bayesian networks. Journal of

the ACM, 50(3):280–305, 2003.

[10] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge Uni-

versity Press, 2009.

[11] C. P. de Campos. New complexity results for MAP in Bayesian networks. In Proceed-
ings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence,
pages 2100–2106, 2011.

[12] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6(6):721–741, 1984.

[13] R. Gens and P. Domingos. Learning the structure of sum-product networks.

In
Proceedings of Thirtieth International Conference on Machine Learning, pages 873–
880, 2013.

[14] D. Lowd and P. Domingos. Naive Bayes models for probability estimation.

In
Proceedings of the Twenty-Second International Conference on Machine Learning,
pages 529–536, 2005.

[15] T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. Dual decomposition for
parsing with non-projective head automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing, pages 1288–1298, 2010.

[16] M. Meil˘a and M. I. Jordan. Learning with mixtures of trees. Journal of Machine

Learning Research 1:1–48, 2000.

[17] A. Nath and P. Domingos. Learning tractable probabilistic models for fault local-
ization. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
pages 1294–1301, 2016.

[18] R. Peharz. Foundations of sum-product networks for probabilistic modeling. PhD

thesis, 2015.

Addendum. April 6, 2017.

[19] R. Peharz. Foundations of sum-product networks for probabilistic modeling - Errata.

[20] R. Peharz, R. Gens, F. Pernkopf, and P. Domingos. On the latent variable inter-
pretation in sum-product networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1–14, 2016.

17

[21] R. Peharz, G. Kapeller, P. Mowlaee, and F. Pernkopf. Modeling speech with sum-
In IEEE International

product networks: Application to bandwidth extension.
Conference on Acoustics, Speech and Signal Processing, pages 3699–3703, 2014.

[22] R. Peharz, S. Tschiatschek, F. Pernkopf, and P. Domingos. On theoretical properties
of sum-product networks. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, pages 744–752, 2015.

[23] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In
Proceedings of Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence,
pages 337–346, 2011.

[24] A. Rooshenas and D. Lowd. Learning tractable graphical models using mixture of
arithmetic circuits. In AAAI Workshop Late-Breaking Developments in the Field
of Artiﬁcial Intelligence, pages 104–106, 2013.

[25] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect
variable interactions. In Proceedings of the Thirty-First International Conference
on Machine Learning, pages 710–718, 2014.

[26] M. Steinder and A. Sethi. Probabilistic fault localization in communication systems
IEEE/ACM Transactions on Networking, 12(5):809–822,

using belief networks.
2004.

[27] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala,
M. Tappen, and C. Rother. A comparative study of energy minimization methods
for Markov random ﬁelds with smoothness-based priors.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 30(6):1068–1080, 2008.

[28] A. Vergari, N. Di Mauro, and F. Esposito. Simplifying, regularizing and strengthen-
ing sum-product network structure learning. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery in Databases, pages 343–358,
2015.

[29] H. Zhao, M. Melibari, and P. Poupart. On the relationship between sum-product
networks and Bayesian networks. In Proceedings of the 32nd International Confer-
ence on Machine Learning, pages 116–124, 2015.

[30] C. Zirn, M. Niepert, M. Strube, and H. Stuckenschmidt. Fine-grained sentiment
analysis with structural features. Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing, pages 336–344, 2011.

[31] D. Zuckerman. Linear degree extractors and the inapproximability of max clique

and chromatic number. Theory of Computing, 3:103–128, 2007.

18

