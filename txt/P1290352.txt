On better training the infinite restricted Boltzmann
machines

Xuan Peng, Xunzhang Gao, Xiang Li

College of Electronic Science, National University of Defense Technology

Changsha, China

E-Mails: pengxuan@nudt.edu.cn; gaoxunzhang@nudt.edu.cn; lixiang01@vip.sina.com

Abstract The infinite restricted Boltzmann machine (iRBM) is an extension of the
classic RBM. It enjoys a good property of automatically deciding the size of the
hidden layer according to specific training data. With sufficient training, the iRBM
can achieve a competitive performance with that of the classic RBM. However, the
convergence of learning the iRBM is slow, due to the fact that the iRBM is sensitive
to the ordering of its hidden units, the learned filters change slowly from the left-most
hidden unit to right. To break this dependency between neighboring hidden units and
speed up the convergence of training, a novel training strategy is proposed. The key
idea of the proposed training strategy is randomly regrouping the hidden units before
each gradient descent step. Potentially, a mixing of infinite many iRBMs with
different permutations of the hidden units can be achieved by this learning method,
which has a similar effect of preventing the model from over-fitting as the dropout.
The original iRBM is also modified to be capable of carrying out discriminative
training. To evaluate the impact of our method on convergence speed of learning and
the model's generalization ability, several experiments have been performed on the
binarized MNIST and CalTech101 Silhouettes datasets. Experimental results indicate
that
the proposed training strategy can greatly accelerate learning and enhance
generalization ability of iRBMs.

Keywords Infinite restricted Boltzmann machines; Model averaging; Regularization;
Discriminative and generative training objective

1 Introduction

are

neural

networks

stochastic

consisting

Boltzmann machines

of
symmetrically-coupled binary stochastic units [1]. They are proposed to find
statistical regularities in the data. One of the most popular subset of Boltzmann
machines is the restricted Boltzmann machine (RBM) [2]. The RBM and its various
extensions have enjoyed much popularity for pattern analysis and generation. The
generality and flexibility of RBMs enable them to be used in a wide range of
applications, e.g., image and audio classification [3,4], generation [5], collaborative
filtering [6,7], motion modeling [8] etc. More specifically, an RBM is a bipartite
graphical model that uses a layer of hidden binary variables or units to model the

probability distribution of a layer of visible variables. With enough hidden units, an
RBM is able to represent a binary probability distribution fitting the training data as
well as possible.

In general, adding more hidden units can improve the representational power of
the model [9]. However, as the number of hidden units increases, many learned
features become strongly correlated [10,11], which increases the risk of over fitting.
Choosing a proper number of hidden units requires a procedure of model selection,
which is time-consuming. To deal with this issue, Welling et al. [12] propose a
boosting algorithm in the feature space of RBM, and at each iteration, a new feature is
added and greedily learned. Nair，et al. [13] conceptually tie the weights of an infinite
number of binary hidden units, and connect these sigmoid units with noisy rectified
linear units (ReLUs) for better feature learning. More recently, Côté and Larochelle
[14] have proposed a non-parametric model called the iRBM. By making the effective
number of hidden units participating in the energy function change freely during
training, the iRBM can automatically adjust the effective number of hidden units
according to the data. The implicitly infinite capability of the iRBM also makes it
capable of lifelong learning. Despite that, there is a major drawback for the iRBM, the
slow convergence of learning, which offsets the advantage it brings to a certain degree.
The reason for this drawback is that, the hidden units are correlated with each other
given the visible variables. The learned filters or feature detectors change slowly from
the left-most hidden unit to right, which is also called "ordering effect" in [14].

The fixed order of hidden units is the reason of strong dependency between filters
learned by neighboring hidden units. The newly added hidden unit always begins to
learns features jointly with the previous hidden units. In fact, the hidden units does not
have to be constrained a fixed order, all possible permutations can be considered and
evaluated. To achieve this, a random permutation of the hidden units is sampled from
a certain distribution before each gradient descent step. Thus, the neighbors of each
hidden unit are continuously changing as training progresses, which encourages the
hidden units to learn features depending on themselves. By doing this, a different
iRBM is trained at each gradient descent step. As there are infinite many hidden units
in the iRBM, a mixture of infinite many iRBMs with different permutation of hidden
units but shared parameters can be achieved theoretically. From this point of view, the
proposed training strategy provides an effective way of preventing the model from
the
over-fitting, as averaging of different models nearly always
generalization performance [15]. A similar effect can be achieved by dropout [16],
which randomly drops units of the neural network during training, and is equivalent to
sub networks, and also serves as
combining exponentially many different
regularization by an adaptive weight decay and sparse representation [17]. In fact, a
more generalized iRBM is defined by treating the permutation of hidden units as a
model parameter. Besides the new training strategy, another contribution of our work
is extending the iRBM to being capable of performing supervised learning, which

improves

allows us to compare the generalization performance of our training strategy to other
training methods and models on discrimination tasks.

Related work It needs to be mentioned here that, Ping, et al. [18] have proposed
an alternative definition of the infinite RBM, and accordingly the Frank-Wolfe
algorithm to learn it. We name their model FW-iRBM to avoid confusing with the
model studied in this paper. The definition of FW-iRBM is motivated by the marginal
log-likelihood of Classic RBMs. The weight matrix W is treated as N samples of
weight vector  . The marginal
log-likelihood of RBM is thus sampling
approximation to marginal log-likelihood of the so-called "fractional RBM". Adding a
hidden unit is equivalent to draw a new sample of  of fractional RBM. The


q ω .

training objective of fractional RBM tries to learning the distribution of  ,

They proposes a greedy training procedure which adds a hidden unit at each iteration
and updates the weight of newly added hidden unit. The advantage of FW-iRBM is
that it is a more generalized model that can be extended to an RBM with uncountable



q ω is a continuous distribution. However, even

number of hidden units, as long as

though the order of hidden units invariant to model's marginal log-likelihood, the
training procedure of FW-iRBM indicates that the order of hidden units still has an
effect on the final performance. The reason is that at each step only the parameter of
newly added hidden unit is updated based on all the previous added hidden units. And
this greedy optimization algorithm is more likely to result in a sub-optimal solution,
that is the reason that the performance is not monotonically improving as the model
size gets larger. The iRBM doesn't encounter this problem as it simultaneously
updates all the non-zero parameters and automatically decides whether adding a new
hidden unit or not.

The remainder of this paper is organized in the following order: Sect.2 gives a
brief review of the iRBM model, after which the discriminative iRBM is introduced,
and the cause of ordering effect is briefly analyzed. In Sect. 3, the proposed training
strategy is formally presented, and an condition under which the model is invariant to
the order of hidden units is proposed with a proof in the appendix. And in Sect. 4,
several experiments are performed to empirically evaluate our training strategy.
Finally, we conclude our work in Sect. 5.

2 Infinite restricted Boltzmann machines

In this section, the original iRBM is first introduced briefly. After that, some
modifications to the energy function of the iRBM are made, which leads to the
discriminative iRBMs. Finally, we briefly analyze cause of the ordering effect in
iRBMs.

2.1 iRBM

The iRBM [14] is proposed to address the difficulty of deciding a proper size of
its capacity as training

it can effectively adapt

the hidden layer for the RBM,
progresses.

The iRBM is an extension of the classic RBM, which mixes infinitely many
RBMs with different number of hidden units from 1 to  , and all the RBMs choose
the hidden units from the same sequence starting from the first one. It defines a

probability

p



v h
,

,

z



with the energy function given as follows:

E



v h
,

,

z



 

v

T
v b



h
i



W v

i




h

b 
i
i

,

(1)

z



i


1

where,

v

0,1 D


is the D dimensional visible vector representing the observable

data.

ih 

0,1


is the

thi

element of the infinite-dimensional hidden vector h . The

random variable z  can be understood as the total number of hidden units being
i is the penalty for each selected
selected to participate in the energy function.

hidden unit

ih .

iW  is the

thi

row of the weight matrix W connecting the visible

units and the hidden units.

vb is the visible units bias vector.

h

ib is the

thi

hidden

unit bias.

where

The joint probability over v , h and z

p



v h
,

,

z







E



v h
,

,

z



,

is
1
Z

e



Z

   v h

e

E



,







,

z





,

z




1


v h




H

z



(2)

(3)

and

H

z


 

h H h
k

|

  

0

k


z

, where H is the set of all possible values h takes.

Thus

zH defines the legal values of h given z . The graphical model of the iRBM

is shown in Fig. 1.

zh

1zh  

W

h

1h

v

Fig.1. Graphical model of the iRBM.

irrelevant for the dimensions of h from

It should be noticed that, for a given z , the value of the energy function is
1z 
ih where
z will never be activated. Thus, (1) has the same form with the energy function of

to , which means that

i

have.

follows:

where

the classic RBM with z hidden units except the penalty

i which the latter does not

By marginalizing out h and z , the marginal distribution


p v is derived as



(4)

(5)

(6)

p



v





1
Z



 

z

h
 
1

H

z

e



E



v h
,

,

z





1
Z





z


1

e



F



v

,

z



,

F



v

,

z



 

v

T
v b




ln 1 exp





W v

i



h
b
i








i

.

z



i


1

The iRBM is trained by maximizing the likelihood of the training examples, the

objective function is defined below:

f




,

D

train





1
D

train

D

train



n


1

ln

p



v

n



,

where

trainD

is the set of training examples.

The gradient of this objective function has a simple form given below:





f




,

D

train





1
D

train

D

train



n


1








p z

|

v

n











F



v

where







f x






 
p x

represents the expectation of

n

,

z






p z













f x over distribution  

p x .










F

v





z

,

v



,

,

(7)

For nearly all types of BMs, computing the partition function Z is intractable,

therefore, the gradients of

f



D
,

train



cannot be exactly computed. This is also the

case for the iRBMs, as there are infinite many hidden units. Côté and Larochelle [14]
suggest using the Contrastive Divergence (CD) and the Persistent Contrastive
Divergence (PCD) [19,20] algorithms to approximately calculate the gradients, i.e. a
Gibbs sampling is used to sample 


v , and then the second expectation in


p z

~

v



z

,

,

order:

(11) is estimated with these samples. The
1

v


p z
of learning is provided in [14].

 
h

h v
|



~

~

p

z

|

k

k

k

k











thk Gibbs step is done in the following


 
k

k

k

k









~

p

v h
|

,

z

.The detail



 
v







1

,

z

, and if the penalty

The hidden units of iRBM are selected in sequence as z takes the value from 1 to
i is chosen properly, an infinite pool of hidden units can be

This will ensure the infinite summing in partition function Z is convergent as long
and the number of hidden units having non-zero weights and biases is
as

i is suggested in [14], which is

achieved. A way to parameterize


ln 1

 

1 



e

h
ib

.

i

always finite.

2.2 Discriminative iRBM

To apply iRBM to discrimination tasks, it is possible to modify the energy
function to make it target dependent just as the case of Classification RBM [21]. And
we call
the new model discriminative infinite restricted Boltzmann machine
(Dis-iRBM). The energy function of the Dis-iRBM is defined below:

E



v h
,

,

y z
,



T

 

v b e d




T

y

W v U e





i

i

y





c 
i
i

，

(8)

z



i


1

h
i



is the “one hot” representation of the label

y

  . d is
,


1, 2,


C

iU  is the

thi

row of the weight matrix U connecting h and

With the energy function defined above, the joint probability over v , h , y and

C

e
y


1

i y

where,

 1
the label bias vector.



i

ye .

z is given below:

p



v h
,

,

y z
,







E



v h
,

,

y z
,



,

1
Z

e



where

Z

   v h

e

E



,







,


z y
,





.

z




1

y




v h




H


z

By marginalizing out h and z , we get the marginal distribution


,p

yv



as

follows:

where

function (8):

p



v

,

y





1
Z



 

z

h
 
1

H

z

e



E



v h
,

,

y z
,





1
Z





z


1

e



F



v

,

y z
,



,

F



v

,

y z
,



T

 

v b e d




T

y


ln 1 exp

W v U e





i

i

y



c
i










i

.

(11)

z



i


1



The following conditional distributions can be easily derived from the energy


p h
i



1|

v

,

z y
,



w v U e





i

i

y



c
i


0


s

 


,

i



z

,

i



z

,


p v

j



1|

h
,

z



s





b
i


z



i


1

hW
i
ij







,


p y

|

h
,

z



exp



d

y







z



i


1

h
i



U e

i

y









y







exp

d

y





z



i


1

h
i



U e

i

y









,

where

s

 
x




1 1



x

e



is the sigmoid function.

The graphical model of Dis-iRBM is shown in Fig. 2.

(9)

(10)

(12)

(13)

(14)

h

1h

zh

1zh  

W

U

v

y e



0,



,0,1,0,



0



Fig. 2. Graphical model of the Dis-iRBM.

2.3 The ordering effect in iRBMs

The reason for slow convergence of learning for iRBMs is that newly added
hidden units are correlated to previous hidden units, it takes a long time for the filters
to become diverse from each other.

To briefly explain this phenomenon, let's firstly take a look at the conditional

probability of the

hidden unit:

thi


p h
i



1|

v






p h
i



1|

v

,



z p z

|

v





s



W v

i





c p z
i

|

v



,

(15)






z i






z i
s





W v

i





c p z
i



i

|

v



and


p z



i

|

v






p z

|

v










z i





F



v

,

z





exp



F





v

,

z







exp







z i



z


1

.

(16)

It can be seen from (16) that computing


p z

i v
|



involves summing up all

possible states of the other hidden units h . The conditional probability


ip h 

1|

v



is a multiply of two terms, one of which is the sigmoid function identical to that of the
classic RBM, the other reflects the influence of the other hidden units. The hidden
units are co-adapted to represent features. This co-adaption sometimes is harmful to
generalization.

Suppose

i

l which corresponds to the newly added hidden unit at step t . Now,

t


p h
lt



1|

v



s



W v

lt








c p z
i



p z

c
i



l
t

|

v


v





s

W v

lt





l
t

|



constant

.



(17)

The constant comes from the infinite sum of all the remaining hidden units with

zero parameters.

Equation (17) indicates that, the newly added hidden unit is always influenced by
all the previous added hidden units. It starts to learn the feature jointly with the

previous features not independently by itself.

3 The proposed training strategy

In this section,

the proposed training strategy is formally presented, which
consists of two parts, the dynamic training objective and the approximated gradient
descent algorithm for optimizing the objective.

3.1 The dynamic training objective

The basic training objective studied in this paper is a hybrid training objective
combining the generative and the discriminative training objective similar to [21], and
is given below:

f

hybrid





,

D

train



 


1






ln


p y

n

|

v

n







ln

p



v

n



1
D

train






D

train



n


1

D

train



n


1






,

(18)



1
D

train




1






f

dis





,

D

train







f

gen





,

D

train





where,

D

train






v

,n

y

n




is

the

set

of

training

examples.

f

dis



D
,

train



ln


p y

n

|

v

n



corresponds to the discriminative part modeling

D

train

  

n


1


p y v
|n

n



, and

f

gen



D
,

train



ln

p



v

n



corresponds to the generative part

D

train

  

n


1

modeling the inputs


p v

n

only.

0 controls the proportion of each part. The

second part can be thought of as a model regularization term.

Stochastic gradient descent or mini-batch gradient descent method can be used to
minimize the objective function (18). However, as mentioned above, convergence of
learning is slow for iRBMs.

An interesting fact can be derived from (15) that, if


p z

i

|


v

0

, equation (15)

becomes


p h
i



1|

v





s



W v
i



c
i



.

(19)

In this case, the

thi

hidden unit is independent with the other hidden units. This

indicates that, if


p z M

|


v

0

, then the first M hidden units are independent

with each other and act like a classic RBM, and any order of the first M hidden
units does not influence the performance of the model.

In [14] the iRBM is trained in a fixed order, which leads to an order biased model.

If we assume the order of hidden units is changeable, and jointly train iRBMs with all
possible orders, the bias might be alleviated, and the learned features will become
closer to (19). This inspires us to propose an alternative training objective as follows.

At each gradient descent step t , we first draw a sample of permutation

to

of the

left-most

tM hidden units from a distribution



p o :
t
t

o
t





o o
,
1
2

,


,
o
Mt



~



p o ,
t
t

where,


p o
t
t



is a discrete distribution as there are in total

!tM permutations of

tM numbers. In this paper, we use a uniform distribution

p
t



Mo
1



t




!

t

, which

gives equal chance to each permutation. Other distributions can also be used. We note
tl as the maximal number of activated hidden units at gradient descent step t . To
stabilize the growing of selected hidden units, only part of them are regrouped
M l ).
(

t

t

Then, the left-most

tM hidden units are reordered according to

to :

t
t
h h
,
1
2




,



,

t
h
Mt

 





h h
,


o
o
1
2

,



,

h

o
Mt

,




and the parameters with each hidden unit are also reordered:

t

t


 
1 ,

Mt

,

 


t




o
1

,



t

,

o
Mt

.




Now, all probabilities are conditioned on permutation

to :


p y v o
|
n
t

;

n



and

Finally, the gradient of objective function (18) with


p y v
|n

n



and

np v


replaced by


p y v o
|
n
t

;

n



and

is computed:




p v o
|n
t

t




1





1
D

train

f

hybrid





t

,

D

train

,

t








f

dis





t

,

D

train

,

t







f

gen





t

,

D

train

,

t





,(20)


p v o
|n
t



.


t



where,

and

f

dis





t

,

D

train

,

t



  

ln


p y

n

|

v o
;
n

t



,

f

gen





t

,

D

train

,

t



  

ln

p



v o
|
n

t



.

D

train

n


1

D

train

n


1

f

hybrid




t

,

D t
,
train



is the objective function at step t , its gradient (20) is in fact a

sampling approximation to the gradient of the following marginalized objective
function:

F

hybrid





t

,

D

train

,

t






1






F

dis





t

,

D

train

,

t






F

gen





t

,

D

train

,

t





,

(21)

1
D

train



where,

and

F

dis





t

,

D

train

,

t



   

ln


p y

n

|

v o
;
n

t



p
t




o ,
t

F

gen





t

,

D

train

,

t



   

ln

p



v o
|
n

t



p
t




o .
t

The word "dynamic" is used to emphasize that, the training objective changes
accordingly as the number of regrouping hidden units changes. The proposed training
procedure is briefly illustrated Fig. 3.

D

train

n


1

o
t

D

train

n


1

o

t

tM 

4

 

1h

2h

3h

4h



t o




3, 2, 4,1

3h

2h

4h

1h

tW



tU

p



v

,

y

|

o

;t



t



1v

2v

3v

4v

5v

6v

y

0,



, 0,1, 0,



0





tM  

1

5

 

1h

2h

3h

4h

5h



2h

1h

3h

5h

4h

o

t  

1



2,1,3,5, 4





1t U

p



v

,

y

|

o

;



t

1



t


1

1t W

1v

2v

3v

4v

5v

6v

y

0,



, 0,1, 0,



0





Fig. 3 An illustration of RP training at gradient descent step t and t+1.

From the dynamic objective function (21), we can see that a mixture of

!tM
iRBMs is been trained at gradient descent step t . All the iRBMs share the same set of
hidden units but with different permutations. When the number of hidden units with

non-zero weights

grows during training, the number of mixed iRBMs grows

tl

accordingly. Theoretically,

is allowed to take arbitrary positive integer value. Thus,

tl

the proposed training objective can potentially allow a mixture of infinite many
iRBMs. For convenience, we name our training strategy as "random permutation
(RP)".

tM controls the proportion of hidden units being regrouped. If

tM is too large,

the model will grow explosively. If

tM is too small, the boosting effect is minor. We
prefer as many hidden units to be regrouped as possible, in the meanwhile, the model
tM will be

would not growing too rapidly. The strategy of choosing a proper

specifically discussed in Sec. 4.

3.2 The approximated gradient descent algorithm

In the previous subsection we have defined the objective function (21) together
with the approximated gradient (20) for parameter learning. In this subsection, we will
provide a way of computing the gradient (20).

At each step t , the dynamic gradient (20) is identical to the gradient of the

objective function (18) except the probability


p y v
|n

n



and

np v


are replaced

by


p y v o
|
n
t

;

n



and


p v o
|n
t



. Once

to

is sampled, the training objective aims

at learning an iRBM with permutation

of the hidden units. The learning of the

to

generative part

f

gen




t

,

D t
,
train



is identical to the learning of the iRBM, which is

briefly introduced in Sect.2. CD or PCD can be directly used to compute the gradients.
The approximated gradient for the generative part is given below:


t



f

gen





t

,

D

train

,

t





1
D

train

D

train



n


1









F



pos

v

n

,

z


o

t





F



v

neg

neg

,

z


o

t

,

(22)











where,

posz

is sampled from


p z v o
|

,n

t



, and

neg

v

, z

neg

are sampled from

p



zv
,

o

t



by K-step Gibbs sampling.

The pseudo code of model parameter update for generative training objective is

summarized in Algorithm 1，which is shown in the Appendix.

In order to calculate the gradient of the discriminative part

f

dis




t

,

D t
,
train



, the

conditional probability


p y v o
|
; t



is derived as follows:


p y

|


v o
,



t






p y

p

t


|
,
v o


v o
|

t







F



v

,

y z
,


o
|



t




G y z
,


v o
;

|



t



F



v

,


y z
,


o
|



t




G y z
,



|


v o
;



t

,

(23)





z


1

e







z


1

y



e

where


G y z

,

|


v o
;



t

 

d

y




ln 1 exp

W v U e



t

i

t
i

y



c
i










i

.

(24)

By taking equations (23) and (24) into the discriminative part of (20), the gradient

of

f

dis




t

,

D t
,
train



is derived as follows:





z


1

e

 

z


1

y



e

z



i


1




t



f

dis





t

,

D

train

,

t





1

D

train

D

train



n


1







and


p z

|

v

,

y
n


o
;

t

n




G y z
n

,

|


v o
;
n

t






p z y
|
,


v o
;
n

t




G y z

,

|


v o
;
n

t


























p z

|

v

,

y

;


o



t



e




G y z
,

|


p z y
,

|


v o
;



t



e




G y z
, |


v o
;

t





t

;




v o

e

z


1




G y z
,


|


v o
;
t



,





z




1

y



e




G y z
,




|


v o
;



t

.

(25)









,

(26)

(27)

used to approximate (25). Sampling


p z y  v o
;

For the same reason stated for training the iRBM, the (P)CD algorithm can also be

p z y v o
,
|
; t

p y z
is given below:



p y z v o
|
; t
,

is done by repeating the two

already given in (26) and

and then

z y
, ~

steps:


p z

v o
;
t

yv
,

o
; t

is



~

~

y







z


1

,

,

.

|

|

|

k

k

k

k













t


p y

|

v

,

z

;


o



t



e




G y z
,

|

t

;




v o

e

y







G y z
,


v o
;

|



t

.

(28)

By taking sampling approximations, the gradient of the discriminative part can be

replaced by the following equation:


t



f

dis





t

,

D

train

,

t





1
D

train

D

train



n


1










G y z
n

,

pos


v o
;
n

t






G y

neg

neg

,

z


v o
;
n

t

,(29)











Where

posz

is a sample from


p z

|

yv
,
n
n

,

o
t



,

negy

and negz

are samples from


p z y v o by K-step Gibbs sampling.

,n



,

|

t

However, the approximated gradient (29) causes high variance during learning in
practice. To alleviate this problem, an alternative derivation of the discriminative part
is proposed as follows:


t



where,

and

f

dis




,

D

train

,

t





1
D

train

D

train



n


1



F y



|


v o
;
n
n
t



t





y




p y



|


v o
;
n

t





F y



|




v o
;
n
t



t

,(30)


F y

|


v o
;



  
ln

t



z


1



e




G y z
,

|


v o
;



t



,


p y

|


v o
;



t



e




F y

|

t

;




v o

e




F y


|


v o
;
t



.

y



(31)

(32)

As there are

tl

computing (32) is

hidden units with non-zero weights at step t, the complexity of

O t

2
l D l C



2
t

.

The gradients



F y
t

|

v o
;
t

t



t



can be exactly computed, which are shown in

this involves computing gradients for

infinite many
the Appendix. However,
parameters. To avoid this issue, we only compute the gradients for parameters of first
hidden units, and leave all the remaining parameters to be 0. This operation is
tl
equivalent to using (30) to compute the gradients for non-zero parameters and (29) for
the remaining parameters.

The maximum number of activated hidden units

changes gradually during

tl

training. Practically, if the Gibbs sampling chain ever samples a value of
than tl , we clamp it to
value of z .

z larger
1tl  . This avoids filling large memory for a large (but rare)

3.3 Evaluation of the models

After the training is done (with T steps), we can simply treat

Tl

as the number

of hidden units been trained. However this results in many “redundant” hidden units,
as many of weight vectors is fairly small.

As


P z v

|

n



reflects how strong the filters respond to an input. We can use this

information to estimate the size of efficiently trained hidden layer size as training
progresses, which is given below

N

h



1
M

bN



b

batch


1

max arg max


P z

|

v

n



,

v

n



D

batch

z





,

(33)

where,

bN is the number of mini-batches of the training set.

batchD

is the current

mini-batch. In (33) we take an average between mini-batches of the maximal z giving

the highest


P z v

|

n



in each mini-batch.

Principally,

the likelihood of a new data point

v ,

p



v



for

iRBM

or y conditioned on

v ,


p y

|

v



for Dis-iRBM are computed as follows:

p



v









p



o

T



p




v o
|

T






,


p y

|

v









p



o
T




p y

|


v o
;

T






.







(34)

(35)

However, computing (34) and (35) are much more expensive than simply

computing

p



v o
|
T



and


p y

|

v o
; T



, as there are

!TM different permutations. A

way to deal with it

is using samples

To

:

p



v





N

1
 
N 
1
n

p



v o

|

n
T



,


p y

|

v



N

1
 
N 
1
n


p y

|

v o

;

n
T



.

But if


p z M

T

|


v o
,

0

for an arbitrary order of the first

TM hidden units

is satisfied, the likelihood


p v



is invariant to the order of the first

TM hidden

units.


p v



p

the first

Any

order


T

M


1:



v o
|


TM hidden units,


o

,

T

0



M

T



o

0



M

T

gives


N

1,

h


N
1,



h

the

result,

then

, where

o
T


1:

is an arbitrary order of

same


TM

is the original order of the remaining

hidden units. This conclusion is formally represented in the following form:

Proposition 3.1 For any permutation of the first

TM hidden units

o
T


1:

TM



,

if



p z M

T

|

v o
,
T


1:

M

T







0

,

 v

0,1 D


is satisfied, then

p



v





p

1

v o
|
T


1:

M

T



,


o

0



M

T



1:

N







h



p




v o
|

2
T


1:

M

T



,


o

0



M

T



1:

N





,

h

where


1 1:
o
T

TM



and


2 1:
o
T

TM



are two arbitrary permutations of the first

TM hidden units, and

o

0



M

T

N
1:



h

is the original permutation of the remaining

hidden units. The proof is given in the Appendix.

Experimental results have shown that RP training can successfully make the
condition in Proposition 3.1 approximately satisfied, any ordering gives a nearly
identical result, thus a small N (e.g. N=5) is enough to give a good estimate of (34)
and (35).

4 Experiments and discussions

In this section, we evaluate our training strategy empirically according to the
convergence speed and the final generalization performance. The datasets used for
evaluation are binarized MNIST [22] and CalTech101 Silhouettes [23].The MNIST
dataset is composed of 70,000 images of size 28  28 pixels representing handwritten
digits (0-9), among which 60,000 images are used for training, and 10,000 images for
testing. Each pixel of the image has been stochastically binarized according to its
intensity as in [22]. The CalTech101 Silhouettes dataset is composed of 8,671 images
of size 28  28 binary pixels, representing object silhouettes of 101 classes. The
dataset is divided into three parts: 4,100 examples for training, 2,264 for validation
and 2,307 for testing.We reshape each image of both datasets into a 784-dimensional
vector by concatenating the adjacent rows one by one.

principle of choosing a proper regrouping rate

We have designed several experiments for different purposes. In Subsect. 4.1, the
tM was experimentally investigated.
In Subsect. 4.2, we evaluated the generalization performance of the iRBM trained
with RP according to its log-likelihood on the test sets of binarized MNIST and
CalTech101 Silhouettes. In Subsect. 4.3, we evaluated the generalization performance
of Dis-iRBMs trained with RP on classification tasks. For all the experiments, the
mini-batch size is 100 and (P)CD is used to compute the gradients. Max-norm
regularization [16] was also used to suppress very large weights, the bounds for each
iU  were 10 and 5 respectively. Côté and Larochelle [14] claims that
iW 

and

results of learning are robust to the value of the hidden unit penalty

i. We have tried

several different

i and find that smaller

i enables the model to grow to proper
size faster at the beginning of learning. However, due to the ordering effect, it takes a
long time for the hidden units to learn filters diverse from each other. RP training also
i to allow as many hidden units to be mixed as possible. But a too

prefers a small

small

i (coupled with a large

tM ) is more likely to cause the model grow
explosively. Based on the above arguments, and for convenience of comparing, we

used the same

i 


1.01 ln 2

for all the models in this paper, which is identical to

that in [14]. We also used L1 regularization and L2 regularization to regularize the
models. The code to reproduce the results of this paper is available on GitHub1.

We would like to mention that there exist a number of sophisticated techniques
that improve performance of classic RBMs on sampling strategies [24 ， 25], model

1https://github.com/Boltzxuann/RP-iRBM

architectures [26，27], etc. However, the aim of this paper is to propose a alternative

training strategy for faster convergence and better generalization of the original
iRBMs in general. Combining these techniques can benefit for both training strategies,
here we focus on comparing on basic settings of parameters.

4.1 The principle of choosing the regrouping rate

tM

As mentioned in Sect. 3, choosing a proper

tM is essential for RP training. In

this subsection, we experimentally investigated the influence of

tM on the growing
of the model, and a principle of choosing it is proposed based on the experimental
results.

In order to have a preliminary understanding on how different regrouping rates
tM influence the growing of the model, we have tried several different settings of

tM from 0 to 0.9 tl . The model was trained for 10 epochs on binarized MNIST and
60 epochs on CalTech101 Silhouettes for each setting, and the training was repeated
for 5 times. The mean results of the 5-time trials are illustrated in Fig. 4. The results

of

tM 

0

illustrate the growing of iRBMs without RP training, which are the

baselines for comparison. As shown in Fig. 4, the growing of the model is not
tM is smaller than 0.8 tl . The growing is even

remarkably influenced as long as

more stable when using RP for CalTech101 Silhouettes.

(a)

(b)

Fig.4. Growing of iRBMs when using different regrouping rates

tM , (a) binarized MNIST; (b)

CalTech101 Silhouettes.

Fig. 5 illustrates the results of


p z v o
|
T

;n



for all training examples after 50

epochs of training on binarized MNIST and 300 epochs of training on CalTech101
Silhouettes. An interesting fact of RP trained iRBMs is that, RP training pushes all the

z

that maximize


p z v o
|
T

;n



above

TM . Similar numbers of hidden units are

activated even though the inputs are quite different. The number of activated hidden
units ranges from 500~520 on binarized MNIST and 700~750 on CalTech101
Silhouettes.

Fig. 5. A illustration of


p z v
|

n



for all training examples after 50 epochs of training on

binarized MNIST (left column) and 300 epochs of training on CalTech101 Silhouettes (right
column). The top row are results without RP training, while the bottom row are those with RP
training.

Proposition 3.1 states that if


p z M

T

|


v o
,
T

0

for all

v

0,1 D


is

fulfilled,


p v



is invariant

to the order of

the first

TM .

In that case,


p z M

T

|

v o
,
n

T





0

for all observed data

nv . This can be easily checked by

computing


p z M

T

|

v o
,
n
T



for all training examples on an arbitrary permutation

To . After 50 epochs of training on binarized MNIST and 300 epochs on CalTech101

Silhouettes, the average value of

ln


p z M

T

|

v o
,
n
T



on the two training sets are

-28.06 and -37.46 respectively, which are fairly small. The condition in Proposition

tl for all
p z

3.1 is approximately satisfied. Plots of

i v o
|

from

1i 

to

,n



T

training examples on the two datasets are shown in Fig. 6.

Fig. 6. A illustration of


p z

i

|

v



n

for all training examples after 50 epochs of training on

binarized MNIST (left)

and

300

epochs

of

training

on CalTech101 Silhouettes

(right).


p z M

t

|


v

n

0

are approximately satisfied on the whole training sets.

As


p z v
|

n



is influenced by the value of

tM , it can be used as "feedback" to

guide the change of

tM during training, which lead to a more adaptable

tM proposed

as follows:

M

t



1
epoch

epoch


1



0.8



epoch

0.2





M epoch

z





10

,

where


zM epoch



is the average number of activated hidden units defined below:


M epoch

z





1
D

train

D

train



n


1

arg max

z


p z

|

v o
;
n


t epoch



.

Based on the above analysis, a principle of choosing

tM is proposed here:

At the beginning of training,

M

t



l
0.7
t



l
0.8
t

to allow a greedy mixing of

iRBMs. After that, a more adaptable

tM defined by (32) is used to stabilize the

(32)

(33)

growing.

models

4.2 Evaluating the generalization performance of RP trained iRBMs as density

This subsection aims at evaluating the generalization performance of the iRBM
trained with RP as a density model. Firstly, we investigated the property of RP
training with different learning rates and regularizations. We have also trained an
iRBM without RP training, as a comparison to our method. All the iRBMs were
trained on binarized MNIST. Two different learning rates, a decaying learning rate

(

lr

t
1

) and an adaptive learning rate method ADAGRAD [28], were used. And the

global learning rate for ADAGRAD was 0.05, which gives the best results in [14].

. Each model was
The weights of the regularizations (L1 and L2) were set as
trained up to 1000 epochs and we performed the Annealed Importance Sampling [22]
to estimate the log likelihood of the test set every 50 epochs．The results are shown in

1 10
4

Fig. 7. For ordinarily trained iRBMs, we only show the results using ADAGRAD, as
the decaying learning rate leads to fairly poor convergence. After 1000 epochs of
training, the effective activated number of hidden units of the ordinary trained iRBMs
using CD are 805 (L2 regularization) and 566 (L1 regularization), which are larger
than those of RP trained iRBMs (588 for L2 regularization and 501 with L1
regularization). And their convergence is much slower than the later. Due to the
"ordering effect", many filters of the former are inadequately trained, which results in
more "redundant" hidden units than the later.
is also noticeable that L1
regularization gives better results than L2 regularization. This may indicate that
sparsity can lead to better generalization for iRBMs.

It

Fig. 7. Log-likelihood on test set of binarized MNIST for iRBMs with different training strategies.
Where "L1R" and "L2R" stand for "L1 regularization" and "L2 regularization" respectively,
"ADA" stands for "ADAGRAD", "DCL" stands for "decaying learning rate".

Fig. 8 illustrates the filters learned by the hidden units of the two iRBMs after 10
epochs of training, where the left-most 100 filters are illustrated. As shown in Fig. 8,
the filters learned by hidden units with RP training are more diverse from each other
than those learned without RP training. The former contains various kinds of local
features such as strokes and specific character parts. While the ordering effect in the
later is obvious, the left filters look like mixtures of different characters, and the right

ones are just different shapes of the character "1".

(a)

(b)

Fig. 8. Comparison of learned filters between iRBMs with different learning strategies after 10
epochs of training, (a) with RP; (b) without RP. The left-most100 filters are shown starting from
the top-left corner and incrementing across rows first.

Based on the above experimental results, we used PCD with 10 steps of Gibbs
sampling and CD with 25 steps of Gibbs sampling to train the models on binarized
MNIST and CalTech101 Silhouettes respectively. And L1 regularization was used all
models. Each model was trained up to 1000 epochs. We have also performed a grid
search on all the hyper parameters (global learning rate: 0.001~1, L1 regularization
weight: 0~0.01).The best results of different models are illustrated in Table 1.

Table 1. The best results of average log-likelihood on test sets of binarized MNIST and
CalTech101 Silhouettes of different models.

Binarized MNIST

CalTech101 Silhouettes

Model
RBM [22]
iRBM [14]

FW-iRBM [18]

iRBM, RP

Size
500
1208

460

674

Avg. LL
-86.34
-85.65
 -85

-85.81

Model
RBM [14]
iRBM [14]

FW-iRBM [18]

iRBM, RP

Size
500
915

550

695

Avg. LL
-119.05
-121.47

 -127

-114.09

The best model trained on binarized MNIST has 674 effective hidden units. The

global learning rate is 0.05, and L1 regularization weight is
. Its average
log-likelihood on the test set is -85.81, which is similar to-85.65 (3000 epochs of
training) in [14]. The size of our model is also smaller than that in [14], which has
1208 hidden units with non-zero parameters. The best model trained on CalTech101
Silhouettes has 695 effective hidden units. The global learning rate is 0.02, and L1

1 10
4

. Its average log-likelihood on the test set is -114.09,
regularization weight is
which is better than -121.47 in [14]. The results of FW-iRBM [18] are also listed in

1 10
3

the table, the best results are achieved by RBMs with CD training initiated by
FW-iRBMs. Samples from the best models are illustrated in Fig. 9.

5

n


1

m
z

(a)

(b)

Fig. 9. Random samples drawn from the best models by randomly initializing visible units and
running10,000 Gibbs steps, examples from the test set are also illustrated, (a) binarized MNIST;
(b) CalTech101 Silhouettes.

To show how RP training affects the dependency between z and v . We computed

the histogram of

z

m



arg max

z


p z

|


 v

t


p z

|

v o
;
t

n
T



on the two test sets. The

results are shown in Fig. 10, which reveals two facts: (a) All

mz

are larger than

tM ;

(b) All the inputs have similar numbers of activated hidden units, as all

mz

are close

to each other. The number of example-specific filters has been greatly reduced.

m
z

MT

MT

(a)

hi

mz

hi

(b)

Fig. 10. The histogram of

on the two test sets, (a) binarized MNIST; (b) CalTech101

Silhouettes.

To further validate that the filters are independent from each other, we just use all
is clamped to

to compose a classic RBM,

the learned filters

i.e.

z

hN ,

p




v

p



v

|

N



h

. The average log-likelihood on the two test sets are

-88.13(binarized MNIST) and -115.90 (CalTech101 Silhouettes) for the converted
RBMs.

4.3 Evaluating the generalization performance of RP trained Dis-iRBMs on

0.1

lr 

0 ) was optimized. Two different learning rates (

classification tasks
In this subsection, we aim at evaluating the generalization performance of
Dis-iRBMs trained with RP on binarized MNIST and CalTech101 Silhouettes. Firstly
we compared the performance of RP with ordinary training to validate its boosting on
learning speed and as guidance for selecting hyper-parameters. The discriminative
training objective (
, 1)
together with ADAGRAD (global learning rate is 0.1), were used. For each learning
rate, we trained the model with or without our training strategy for 5 epochs. At the
end of every epoch, miss-classification rate on the test set was computed. We repeated
the training procedure for each parameter setting 5 times. The results are shown in Fig.
10. As shown in Fig. 11, RP accelerates learning for all the learning rates. ADAGRAD
coupled with RP training achieves the best performance in this experiment. The best
)
result with RP training at epoch 5 is 3.10%. High learning rate (e.g.,
encourages the model to quickly explore different regions of the weight space, but it
is also more likely to cause oscillating unless proper learning rate decay is used.
ADAGRAD gives smaller learning rates to parameters close to convergence, thus it is
more stable than the fixed learning rate. Another fact can be observed from Fig. 11 is
that, RP training makes the learning more stable, the variance of learning is much
smaller. As the additional label information makes the filters prefer some classes
instead of others, the "ordering effect" is more significant.

lr 

1

Fig. 11. Comparison of convergence speed between Dis-iRBMs trained with or without RP. The
test errors are illustrated in log-scale.

According to the results of the above experiment, we used RP to train a
Dis-iRBM on binarized MNIST and CalTech101 Silhouettes, and evaluated their
generalization performance
test miss-classification errors.
ADAGRAD were used in all experiments. The validation sets were used to search the
hyper parameters on a log-scale (the generative proportion : 0~1, global learning
rate: 0.001~1, L1 regularization weight: 0~0.01). The best results for different models
and method are illustrated in Table 2.

according to the

Table 2. The best results of classification error on the test set of MNIST and CalTech101
Silhouettes achieved by different models.

Model

Dis-iRBM

CalTech101 Silhouettes
Training
objective
Dis.
Hybrid

Size

235

373

(



0.01

)

Test
error
37.10%

34.59%

2.20% FW-iRBM+SR [18]

600

--

34.5%

(



0.005

)

1.71%

Dis-iRBM, RP

165

Dis.

34.55%

742

Hybrid

(



0.01

)

30.95%

Binarized MNIST

Model

ClassRBM [19]

Dis-iRBM

Test
error
1.81%

1.28%

Training
objective
Dis.
Hybrid

(



0.01

)

Dis.
Hybrid

Size

500

1500

382

416

FW-iRBM+SR [18]

600

--

2.2%

Dis-iRBM, RP

489

621

Dis.

Hybrid

(



0.005

)

1.92%

1.41%

The best Dis-iRBM trained using hybrid training objective (

) achieves
a test error of 1.42% on MNIST, which is better than 1.71% achieved by normally
trained Dis-iRBM. The global learning rate for it is 0.1, and the L1 regularization

0.005



1 10
4

. Dis-iRBM performs slightly worse than the ClassRBM on MNIST ,
weight is
but the difference between the two best results is smaller than 0.2%, which is
commonly regarded as statistical
insignificant for MNIST. The best result on
CalTech101 Silhouettes is also achieved by a Dis-iRBM trained using hybrid training
objective (
) and RP training. The test error is 30.95%, which is better than
34.59% achieved by normally trained Dis-iRBM. The global learning rate for the best

0.01



. An interesting fact about
model is 0.01, and the L1 regularization weight is
the model size of iRBMs trained with different training objective is that size is smaller
when merely the discriminative training objective is used. This makes sense as less
features are often needed if the model only needs to discriminate objects from each
other, instead of modeling all the examples well.

1 10
3

The FW-iRBM [18] have also been used for classification by taking the hidden
units' activation vectors and using them as input for a softmax regression (SR). As
FW-iRBM cannot perform discriminative training directly due to its training objective,

learning the FW-iRBM and learning the SR are two separate procedures. i.e., after
training the FW-iRBM with T iterations, fix the parameters and using its hidden
units' activation vectors to train a SR. The results of FW-iRBM are also listed in the
table.

A "trick" to make the learning a bit faster is using the momentum, we use
different momentum values for parameters of different hidden units according to the
time they have been trained. When the hidden unit is added to training, the starting
momentum value is 0.5, and then it gradually increases to 0.9.

5 Conclusion and future work

In this paper, we have proposed a novel training strategy (RP training) for the
infinite RBMs, which aims at achieving better convergence and generalization. The
core concept of the RP training is a dynamic training objective that allows a different
model to be optimized at each gradient descent step. More specifically, an iRBM with
an random grouping of hidden units is sampled before doing gradient descent. An
implicit mixture of infinite many iRBMs with different permutations of hidden units
is achieved with RP training. Experiments on binarized MNIST and CalTech101
Silhouettes have shown that, RP can train the hidden units more efficiently, thus
results in smaller hidden layer size and better generalization performance. Compared
with the FW-iRBM, the iRBM trains all the hidden units jointly not one unit greedily
each update step, thus the former is more likely to reach sub-optimal solution. In the
future, more datasets especially some real-valued datasets, will be used to give a
further evaluation of the performance of our training strategy. Meanwhile, we are
exploring a multi-layer extension of the iRBM, the idea of RP training can be also
applied to this new architecture, combined with a greedy layer-wise pre-training.

Appendix

A.1

Algorithm 1
RP training
Notation: ~ p

x

means x is sampled from p

x

y means x is set to y

Initiation of parameters: 0
for t,2,T do

l  ;
1

0
  


,

t

0




0

Parameter update of the iRBM for generative training objective with

Input:

training example 

nyv
,n



, learning rate

 
lr t

, re-permutating length

tM

#Re-permutating the hidden units and the corresponding parameters

#Sampling a permutation of length



o
,
Mt


p o
t



2,
o o
1

o
t

~





,

t

tM

t
t
h h
,
1
2




,



,

t
h
Mt

 





h h
,


o
o
1
2

,



,

h

o
Mt




t



1


1

,




1
t

,
Mt

 



1

t




o
1

,




1

t

,

o
Mt




#Positive phase of CD
ˆ
pos
h


p z v o
|
;n

pos ~



z

,

t

p



h v
|

,

z

n

pos

;

o

t



neg

v

~

p

v h
|

neg

pos

,

z



;

o

t



,

neg

z

~


p z v
|

neg

;

o

t



#Negative phase of CD


h v
|

h

~

o



p

neg

pos

z

;

,

n

t

,

ˆ
neg
h

p



h v
|

neg

neg

,

z

;

o

t



#Update
for t

   do

t

end for
#Adding a hidden unit
> 1tl  and negz
l  
1 1

posz

if

l
t

t

> 1tl  then

t

t

1
 
 

 
lr t

F



pos

v

n

,

z


o

t





F



v

neg

neg

,

z


o

t



















t

0

lt 
else then
l 

l
t

t

1

end if
end for

A.2


t



where,

Derivation of gradients (30) w.r.t.

t is presented as follows. Without loss of

generality, we only show the results related to the binary Dis-iRBM.

Recall the equation (30):

f

dis




,

D

train

,

t





1
D

train

D

train



n


1



F y



|


v o
;
n
n
t



t





y




p y



|


v o
;
n

t





F y

|




v o
;
n
t



t



,



F y

|


v o
;
n

t














ln







e




G y z
,

|


v o
;
n

t













z


1



e




G y z
, |


v o
;
t
n






G y z

,

|


v o
;
n

t




1
z
1

 





z


1



e




G y z
,

|


v o
;
n

t








G y z
,

|


v o
;
n

t






G y z
,

|


v o
;
n

t



e


e





z


1







z


1







z


1












P z

|

v

n

,

y

;


o



t


G y z

,

|


v o
;
n

t



.

(38)

In order to compute (38), we need to compute


G y z

,

|

v o
;n

t



first, which





are shown as follows:


t
W

i




G y z

,

|


v o
;



t

 


ln 1 exp

W v U e







t

i

t
i

y



t
c

i









i









i


W

i



z



i


1


t
W


i

z



i


1









 


ln 1 exp

W v U e







t

i

t
i

y



t
c

i

 


H z



i




ln 1 exp

W v U e



t

i

t
i

y



t
c
i










t
W


i

 


H z



i

 


H z



i



c
i

y

t
i



t

i

exp



1 exp
 
s

W v U e



W v U e



W v U e




c
i



t

i

t

i

t
i

t
i

y

y



v


c
i

v



.

Similarly,


d


t

c
i

where,



G y z

,


v o
;

t



|
t
i



U


 


H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



e

y

,


t G y z
,

|


v o
;



t

 


H z



i



e

y

,


G y z

,

|


v o
;



t

 


H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



,


H z

i

0,


  
1,


z
z




i
i

.

(39)

(40)

(41)

(42)

Substitute (39) ~ (42) into (38), we get




t



v o
|

F y
;
t
W


i

 



z


1


P z

|

v

,



y H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



v

,

(43)




P z



i

|

v

,

y

 
s

W v U e



t

i

t
i

y



t
c
i



v



t



F y
|
U



v o
;
t
i







z


1

 


P z

|

v

,



y H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



e

y




P z



i

|

v

,

y

 
s

W v U e



t

i

t
i

y



t
c
i



e

y


|
F y

d





v o
;
t

t

 





z


1


P z

|

v

,

y

;



o e
t

y

 

e

y

,



t



v o

F y
|
;
t

c
i

 





z


1


P z

|

v

,

y

;


o




H z

t



i

 
s

W v U e



t

i

t
i

y



t
c
i






P z



i

|

v

,

y

;

t

o W v U e


i



t
i

t

 
s

y



t
c
i



,

,

(44)

(45)

(46)

where,


P z



i

|

v

,

y

;


o

t



  
1


P z



|

v

,

y

;


o



.

t

i

z


1

A.3

The proof of proposition 3.1 is given as follows:

Suppose the permutation of the first

TM hidden units is


1 1:
o
T

TM



. The

likelihood of v conditioned on this permutation is given below:

From the condition given in Proposition 3.1 we can derive that,



p z M

T

|

1
v o
,
T


1:

M

T







0

,

 v

0,1 D



p z



i

|

1
v o
,
T


1:

M

T







0

, for

i

 
1, 2,

,

M

T

M

T



z




1

exp



F





v

,

z



|

1

o
T


1:

M

T









exp



F

v

,

z

|

1

o
T


1:

M

T


















z M

T


1

exp



F



i

 
1, 2,

,

M

T

v

,

z



|


o



M

T

0



1:

N

h









0

, for



ln 1 exp


F





v

,

exp






z M

T


1



W v

i



c
i





z



|


o



M

T

0



1:

N

h









0

, for

i

 
1, 2,

,

M

T









exp

F

v

,

z

|

2

o
T


1:

M

T







exp

F

v

,

z



|


o



M

T

0



1:

N

h





















z M

T


1



0

,

(47)

for

i

 
1, 2,

,

M

and any permutation

T

2

o
T


1:

M

T





1

o
T


1:

M

T



and

 v

0,1 D





p z



i

|

v o
,

2
T


1:

M

T







0

, for

i

 
1, 2,

,

M

T

The above conclusion indicates that if any permutation

o
T


1:

TM



satisfies that



i

|

v o
,


1:

M

T

T



0

, then all the other permutations

o
T


1:

TM



also satisfy


p z

p z



i

|

v o
,


T


1:

M

T



0

.

Now, consider the likelihood

p



v o
|

T


1:

M

T





for an arbitrary permutation













o
T


1:

TM



:

p




v o
|

T


1:

M

T







z


1





z




1

v



M

T



z


1



M

T



z




1

v







exp



F



v

,

z

|


o


1:

M

T

T







exp



F



v


,

z



|


o


1:

M

T

T







(48)

.







exp



F

v

,

z

|




o
T


1:

M

T









exp



F





v

,

z

|


o



M

T

0



1:

N

h







exp



F

v


,

z



|


o
T


1:

M

T













 



z M


1

T

v



exp



F





v


,

z



|


o



M

T

0



1:

N

h





T



z M


1

Take (47) into account, we have

p




v o
|
T


1:

M

T








1

T

z M



 



z M

T


1

v







exp



F





v

,

z

|


o



M

T

0



1:

N

h







exp



F





v


,

z



|


o



M

T

0



1:

N

h







.

(49)

Equation (49) indicates that

p



v o
|

T


1:

M

T

is invariant to the permutation




now becomes:

o
T


1:

TM



. The marginalized likelihood


p v



p



v








o
T

p




v o
|

T


1:



M p
T




o

T


1:

M

T









exp

F





v

,

z

|


o



M

T

0



1:

N

h







exp

F





v


,

z



|


o



M

T

0



1:

N

h







p




o

T


1:

M

T





exp

F

v

,

z

|


o



M

T

0



1:

N

h

exp

F

v


,

z



|


o



M

T

0



1:

N

h

p




o

T


1:

M

T








o
T



















exp

F

v

,

z

|


o



M

T

0



1:

N

h

exp

F

v


,

z



|


o



M

T

0



1:

N

h












o
T


1

T


z M


 



z M

T


1

v














1

T

z M



 



z M


1

T

v






1

T

z M



 



z M


1

T

v





















p




v o
|

0


1:

M

T





References

[1] Ackley, D.H., Hinton, G.E., Sejnowski, T.J.. A learning algorithm for Boltzmann machines.

Cognitive Science vol.9, pp. 147–169, 1985.

[2] Smolensky, P.. Information processing in dynamical systems: foundations of harmony theory.
In: Parallel Distributed processing: Explorations in the Microstructure of Cognition,
Foundations, vol. 1, USA, pp. 194–281, 1986.

[3] G. E. Hinton, R. R. Salakhutdinov. Reducing the Dimensionality of Data with Neural

Networks. Science, vol. 313, pp.504-507, 2006.

[4] A. Mohamed and G. E. Hinton. Phone recognition using restricted Boltzmann machines. In
IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), pp.
4354–4357, 2010.

[5] Mohamed A, Dahl G E, Hinton G. Acoustic modeling using deep belief networks. IEEE

Transactions on Audio Speech & Language Processing, vol. 20, pp. 14-22, 2011.

[6] R. Salakhutdinov, A. Mnih, and G. E. Hinton. Restricted Boltzmann machines for
collaborative filtering. In Z. Ghahramani, editor, Proceedings of the 24th International
Conference on Machine Learning (ICML), pp. 791–798. ACM, 2007.

[7] Truyen Tran, Dinh Phung, Svetha Venkatesh. Mixed-Variate Restricted Boltzmann Machines.

Eprint Arxiv, 1408.1160v1, 2014.

[8] G. W. Taylor, G. E. Hinton, and S. T. Roweis. Modeling human motion using binary latent
variables.In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information
Processing Systems (NIPS 19), pp. 1345–1352. MIT Press, 2007.

[9] Fischer A, Igel C. Empirical Analysis of the Divergence of Gibbs Sampling Based Learning
Algorithms for Restricted Boltzmann Machines. Artificial Neural Networks – ICANN 2010.
Springer Berlin Heidelberg, pp. 208--217, 2010.

[10] Nitish Srivastava, Georey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.
Dropout: A Simple Way to Prevent Neural Networks from Over fitting. Journal of Machine
Learning Research, vol. 15, pp. 1929-1958, 2014.

[11] Tomczak J M, Gonczarek A. Sparse hidden units activation in Restricted Boltzmann Machine.
Progress in Systems Engineering. Springer International Publishing, pp. 181-185, 2015.

[12] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised boosting. In NIPS, 2002.
[13] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In

[14] Côté M A, Larochelle H. An infinite restricted Boltzmann machine. Neural Computation,

[15]T. G. Dietterich. Ensemble methods in machine learning. In Multiple classifier systems, pp.

ICML, 2010.

vol.28, pp. 1265-1289, 2016.

1–15. Springer, 2000.

[16]Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine Learning Research, vol. 15, pp. 1929-1958,
2014.

[17] Baldi P, Sadowski P. The dropout learning algorithm. Artificial Intelligence, vol. 210, pp.

78-122, 2014.

[18]W Ping, Q Liu, AT Ihler. Learning infinite RBMs with Frank-Wolfe. In NIPS, 2016.
[19] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation. Vol. 14, pp. 1771–1800, 2002

[20] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood
gradient. In International Conference on Machine learning(ICML), pp. 1064–1071, 2008.
[21] Hugo Larochelle, et al. Learning Algorithms for the Classiﬁcation Restricted Boltzmann

Machine. Journal of Machine Learning Research vol. 13, pp.

643-669, 2012.

[22] Salakhutdinov, R. and Murray, I. On the quantitative analysis of Deep Belief Networks. In
Proceedings of the 25th Annual International Conference on Machine Learning (ICML), pp.
872-879, 2008.

[23] Marlin, B. M., Swersky, K., Chen, B., and de Freitas, N. Inductive Principles for Restricted
Boltzmann Machine Learning. Proc. Intl. Conference on Artificial Intelligence and Statistics,
pages 305–306.

[24] K. Cho, T. Raiko, and A. Ilin. Parallel tempering is efficient for learning restricted Boltzmann
machines. In Proceedings of the International Joint Conference on Neural Networks (IJCNN),
pages 3246–3253. IEEEPress, 2010.

[25] Sohldickstein J, Battaglino P, Deweese M R. Minimum probability flow learning, in ICML.

2009.

[26] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an

application to information retrieval. In NIPS, 2005.

[27] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
[28] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and

stochastic optimization. J. Mach. Learn. Res., vol. 12, pp. 2121-2159, 2011.

On better training the infinite restricted Boltzmann
machines

Xuan Peng, Xunzhang Gao, Xiang Li

College of Electronic Science, National University of Defense Technology

Changsha, China

E-Mails: pengxuan@nudt.edu.cn; gaoxunzhang@nudt.edu.cn; lixiang01@vip.sina.com

Abstract The infinite restricted Boltzmann machine (iRBM) is an extension of the
classic RBM. It enjoys a good property of automatically deciding the size of the
hidden layer according to specific training data. With sufficient training, the iRBM
can achieve a competitive performance with that of the classic RBM. However, the
convergence of learning the iRBM is slow, due to the fact that the iRBM is sensitive
to the ordering of its hidden units, the learned filters change slowly from the left-most
hidden unit to right. To break this dependency between neighboring hidden units and
speed up the convergence of training, a novel training strategy is proposed. The key
idea of the proposed training strategy is randomly regrouping the hidden units before
each gradient descent step. Potentially, a mixing of infinite many iRBMs with
different permutations of the hidden units can be achieved by this learning method,
which has a similar effect of preventing the model from over-fitting as the dropout.
The original iRBM is also modified to be capable of carrying out discriminative
training. To evaluate the impact of our method on convergence speed of learning and
the model's generalization ability, several experiments have been performed on the
binarized MNIST and CalTech101 Silhouettes datasets. Experimental results indicate
that
the proposed training strategy can greatly accelerate learning and enhance
generalization ability of iRBMs.

Keywords Infinite restricted Boltzmann machines; Model averaging; Regularization;
Discriminative and generative training objective

1 Introduction

are

neural

networks

stochastic

consisting

Boltzmann machines

of
symmetrically-coupled binary stochastic units [1]. They are proposed to find
statistical regularities in the data. One of the most popular subset of Boltzmann
machines is the restricted Boltzmann machine (RBM) [2]. The RBM and its various
extensions have enjoyed much popularity for pattern analysis and generation. The
generality and flexibility of RBMs enable them to be used in a wide range of
applications, e.g., image and audio classification [3,4], generation [5], collaborative
filtering [6,7], motion modeling [8] etc. More specifically, an RBM is a bipartite
graphical model that uses a layer of hidden binary variables or units to model the

probability distribution of a layer of visible variables. With enough hidden units, an
RBM is able to represent a binary probability distribution fitting the training data as
well as possible.

In general, adding more hidden units can improve the representational power of
the model [9]. However, as the number of hidden units increases, many learned
features become strongly correlated [10,11], which increases the risk of over fitting.
Choosing a proper number of hidden units requires a procedure of model selection,
which is time-consuming. To deal with this issue, Welling et al. [12] propose a
boosting algorithm in the feature space of RBM, and at each iteration, a new feature is
added and greedily learned. Nair，et al. [13] conceptually tie the weights of an infinite
number of binary hidden units, and connect these sigmoid units with noisy rectified
linear units (ReLUs) for better feature learning. More recently, Côté and Larochelle
[14] have proposed a non-parametric model called the iRBM. By making the effective
number of hidden units participating in the energy function change freely during
training, the iRBM can automatically adjust the effective number of hidden units
according to the data. The implicitly infinite capability of the iRBM also makes it
capable of lifelong learning. Despite that, there is a major drawback for the iRBM, the
slow convergence of learning, which offsets the advantage it brings to a certain degree.
The reason for this drawback is that, the hidden units are correlated with each other
given the visible variables. The learned filters or feature detectors change slowly from
the left-most hidden unit to right, which is also called "ordering effect" in [14].

The fixed order of hidden units is the reason of strong dependency between filters
learned by neighboring hidden units. The newly added hidden unit always begins to
learns features jointly with the previous hidden units. In fact, the hidden units does not
have to be constrained a fixed order, all possible permutations can be considered and
evaluated. To achieve this, a random permutation of the hidden units is sampled from
a certain distribution before each gradient descent step. Thus, the neighbors of each
hidden unit are continuously changing as training progresses, which encourages the
hidden units to learn features depending on themselves. By doing this, a different
iRBM is trained at each gradient descent step. As there are infinite many hidden units
in the iRBM, a mixture of infinite many iRBMs with different permutation of hidden
units but shared parameters can be achieved theoretically. From this point of view, the
proposed training strategy provides an effective way of preventing the model from
the
over-fitting, as averaging of different models nearly always
generalization performance [15]. A similar effect can be achieved by dropout [16],
which randomly drops units of the neural network during training, and is equivalent to
sub networks, and also serves as
combining exponentially many different
regularization by an adaptive weight decay and sparse representation [17]. In fact, a
more generalized iRBM is defined by treating the permutation of hidden units as a
model parameter. Besides the new training strategy, another contribution of our work
is extending the iRBM to being capable of performing supervised learning, which

improves

allows us to compare the generalization performance of our training strategy to other
training methods and models on discrimination tasks.

Related work It needs to be mentioned here that, Ping, et al. [18] have proposed
an alternative definition of the infinite RBM, and accordingly the Frank-Wolfe
algorithm to learn it. We name their model FW-iRBM to avoid confusing with the
model studied in this paper. The definition of FW-iRBM is motivated by the marginal
log-likelihood of Classic RBMs. The weight matrix W is treated as N samples of
weight vector  . The marginal
log-likelihood of RBM is thus sampling
approximation to marginal log-likelihood of the so-called "fractional RBM". Adding a
hidden unit is equivalent to draw a new sample of  of fractional RBM. The


q ω .

training objective of fractional RBM tries to learning the distribution of  ,

They proposes a greedy training procedure which adds a hidden unit at each iteration
and updates the weight of newly added hidden unit. The advantage of FW-iRBM is
that it is a more generalized model that can be extended to an RBM with uncountable



q ω is a continuous distribution. However, even

number of hidden units, as long as

though the order of hidden units invariant to model's marginal log-likelihood, the
training procedure of FW-iRBM indicates that the order of hidden units still has an
effect on the final performance. The reason is that at each step only the parameter of
newly added hidden unit is updated based on all the previous added hidden units. And
this greedy optimization algorithm is more likely to result in a sub-optimal solution,
that is the reason that the performance is not monotonically improving as the model
size gets larger. The iRBM doesn't encounter this problem as it simultaneously
updates all the non-zero parameters and automatically decides whether adding a new
hidden unit or not.

The remainder of this paper is organized in the following order: Sect.2 gives a
brief review of the iRBM model, after which the discriminative iRBM is introduced,
and the cause of ordering effect is briefly analyzed. In Sect. 3, the proposed training
strategy is formally presented, and an condition under which the model is invariant to
the order of hidden units is proposed with a proof in the appendix. And in Sect. 4,
several experiments are performed to empirically evaluate our training strategy.
Finally, we conclude our work in Sect. 5.

2 Infinite restricted Boltzmann machines

In this section, the original iRBM is first introduced briefly. After that, some
modifications to the energy function of the iRBM are made, which leads to the
discriminative iRBMs. Finally, we briefly analyze cause of the ordering effect in
iRBMs.

2.1 iRBM

The iRBM [14] is proposed to address the difficulty of deciding a proper size of
its capacity as training

it can effectively adapt

the hidden layer for the RBM,
progresses.

The iRBM is an extension of the classic RBM, which mixes infinitely many
RBMs with different number of hidden units from 1 to  , and all the RBMs choose
the hidden units from the same sequence starting from the first one. It defines a

probability

p



v h
,

,

z



with the energy function given as follows:

E



v h
,

,

z



 

v

T
v b



h
i



W v

i




h

b 
i
i

,

(1)

z



i


1

where,

v

0,1 D


is the D dimensional visible vector representing the observable

data.

ih 

0,1


is the

thi

element of the infinite-dimensional hidden vector h . The

random variable z  can be understood as the total number of hidden units being
i is the penalty for each selected
selected to participate in the energy function.

hidden unit

ih .

iW  is the

thi

row of the weight matrix W connecting the visible

units and the hidden units.

vb is the visible units bias vector.

h

ib is the

thi

hidden

unit bias.

where

The joint probability over v , h and z

p



v h
,

,

z







E



v h
,

,

z



,

is
1
Z

e



Z

   v h

e

E



,







,

z





,

z




1


v h




H

z



(2)

(3)

and

H

z


 

h H h
k

|

  

0

k


z

, where H is the set of all possible values h takes.

Thus

zH defines the legal values of h given z . The graphical model of the iRBM

is shown in Fig. 1.

zh

1zh  

W

h

1h

v

Fig.1. Graphical model of the iRBM.

irrelevant for the dimensions of h from

It should be noticed that, for a given z , the value of the energy function is
1z 
ih where
z will never be activated. Thus, (1) has the same form with the energy function of

to , which means that

i

have.

follows:

where

the classic RBM with z hidden units except the penalty

i which the latter does not

By marginalizing out h and z , the marginal distribution


p v is derived as



(4)

(5)

(6)

p



v





1
Z



 

z

h
 
1

H

z

e



E



v h
,

,

z





1
Z





z


1

e



F



v

,

z



,

F



v

,

z



 

v

T
v b




ln 1 exp





W v

i



h
b
i








i

.

z



i


1

The iRBM is trained by maximizing the likelihood of the training examples, the

objective function is defined below:

f




,

D

train





1
D

train

D

train



n


1

ln

p



v

n



,

where

trainD

is the set of training examples.

The gradient of this objective function has a simple form given below:





f




,

D

train





1
D

train

D

train



n


1








p z

|

v

n











F



v

where







f x






 
p x

represents the expectation of

n

,

z






p z













f x over distribution  

p x .










F

v





z

,

v



,

,

(7)

For nearly all types of BMs, computing the partition function Z is intractable,

therefore, the gradients of

f



D
,

train



cannot be exactly computed. This is also the

case for the iRBMs, as there are infinite many hidden units. Côté and Larochelle [14]
suggest using the Contrastive Divergence (CD) and the Persistent Contrastive
Divergence (PCD) [19,20] algorithms to approximately calculate the gradients, i.e. a
Gibbs sampling is used to sample 


v , and then the second expectation in


p z

~

v



z

,

,

order:

(11) is estimated with these samples. The
1

v


p z
of learning is provided in [14].

 
h

h v
|



~

~

p

z

|

k

k

k

k











thk Gibbs step is done in the following


 
k

k

k

k









~

p

v h
|

,

z

.The detail



 
v







1

,

z

, and if the penalty

The hidden units of iRBM are selected in sequence as z takes the value from 1 to
i is chosen properly, an infinite pool of hidden units can be

This will ensure the infinite summing in partition function Z is convergent as long
and the number of hidden units having non-zero weights and biases is
as

i is suggested in [14], which is

achieved. A way to parameterize


ln 1

 

1 



e

h
ib

.

i

always finite.

2.2 Discriminative iRBM

To apply iRBM to discrimination tasks, it is possible to modify the energy
function to make it target dependent just as the case of Classification RBM [21]. And
we call
the new model discriminative infinite restricted Boltzmann machine
(Dis-iRBM). The energy function of the Dis-iRBM is defined below:

E



v h
,

,

y z
,



T

 

v b e d




T

y

W v U e





i

i

y





c 
i
i

，

(8)

z



i


1

h
i



is the “one hot” representation of the label

y

  . d is
,


1, 2,


C

iU  is the

thi

row of the weight matrix U connecting h and

With the energy function defined above, the joint probability over v , h , y and

C

e
y


1

i y

where,

 1
the label bias vector.



i

ye .

z is given below:

p



v h
,

,

y z
,







E



v h
,

,

y z
,



,

1
Z

e



where

Z

   v h

e

E



,







,


z y
,





.

z




1

y




v h




H


z

By marginalizing out h and z , we get the marginal distribution


,p

yv



as

follows:

where

function (8):

p



v

,

y





1
Z



 

z

h
 
1

H

z

e



E



v h
,

,

y z
,





1
Z





z


1

e



F



v

,

y z
,



,

F



v

,

y z
,



T

 

v b e d




T

y


ln 1 exp

W v U e





i

i

y



c
i










i

.

(11)

z



i


1



The following conditional distributions can be easily derived from the energy


p h
i



1|

v

,

z y
,



w v U e





i

i

y



c
i


0


s

 


,

i



z

,

i



z

,


p v

j



1|

h
,

z



s





b
i


z



i


1

hW
i
ij







,


p y

|

h
,

z



exp



d

y







z



i


1

h
i



U e

i

y









y







exp

d

y





z



i


1

h
i



U e

i

y









,

where

s

 
x




1 1



x

e



is the sigmoid function.

The graphical model of Dis-iRBM is shown in Fig. 2.

(9)

(10)

(12)

(13)

(14)

h

1h

zh

1zh  

W

U

v

y e



0,



,0,1,0,



0



Fig. 2. Graphical model of the Dis-iRBM.

2.3 The ordering effect in iRBMs

The reason for slow convergence of learning for iRBMs is that newly added
hidden units are correlated to previous hidden units, it takes a long time for the filters
to become diverse from each other.

To briefly explain this phenomenon, let's firstly take a look at the conditional

probability of the

hidden unit:

thi


p h
i



1|

v






p h
i



1|

v

,



z p z

|

v





s



W v

i





c p z
i

|

v



,

(15)






z i






z i
s





W v

i





c p z
i



i

|

v



and


p z



i

|

v






p z

|

v










z i





F



v

,

z





exp



F





v

,

z







exp







z i



z


1

.

(16)

It can be seen from (16) that computing


p z

i v
|



involves summing up all

possible states of the other hidden units h . The conditional probability


ip h 

1|

v



is a multiply of two terms, one of which is the sigmoid function identical to that of the
classic RBM, the other reflects the influence of the other hidden units. The hidden
units are co-adapted to represent features. This co-adaption sometimes is harmful to
generalization.

Suppose

i

l which corresponds to the newly added hidden unit at step t . Now,

t


p h
lt



1|

v



s



W v

lt








c p z
i



p z

c
i



l
t

|

v


v





s

W v

lt





l
t

|



constant

.



(17)

The constant comes from the infinite sum of all the remaining hidden units with

zero parameters.

Equation (17) indicates that, the newly added hidden unit is always influenced by
all the previous added hidden units. It starts to learn the feature jointly with the

previous features not independently by itself.

3 The proposed training strategy

In this section,

the proposed training strategy is formally presented, which
consists of two parts, the dynamic training objective and the approximated gradient
descent algorithm for optimizing the objective.

3.1 The dynamic training objective

The basic training objective studied in this paper is a hybrid training objective
combining the generative and the discriminative training objective similar to [21], and
is given below:

f

hybrid





,

D

train



 


1






ln


p y

n

|

v

n







ln

p



v

n



1
D

train






D

train



n


1

D

train



n


1






,

(18)



1
D

train




1






f

dis





,

D

train







f

gen





,

D

train





where,

D

train






v

,n

y

n




is

the

set

of

training

examples.

f

dis



D
,

train



ln


p y

n

|

v

n



corresponds to the discriminative part modeling

D

train

  

n


1


p y v
|n

n



, and

f

gen



D
,

train



ln

p



v

n



corresponds to the generative part

D

train

  

n


1

modeling the inputs


p v

n

only.

0 controls the proportion of each part. The

second part can be thought of as a model regularization term.

Stochastic gradient descent or mini-batch gradient descent method can be used to
minimize the objective function (18). However, as mentioned above, convergence of
learning is slow for iRBMs.

An interesting fact can be derived from (15) that, if


p z

i

|


v

0

, equation (15)

becomes


p h
i



1|

v





s



W v
i



c
i



.

(19)

In this case, the

thi

hidden unit is independent with the other hidden units. This

indicates that, if


p z M

|


v

0

, then the first M hidden units are independent

with each other and act like a classic RBM, and any order of the first M hidden
units does not influence the performance of the model.

In [14] the iRBM is trained in a fixed order, which leads to an order biased model.

If we assume the order of hidden units is changeable, and jointly train iRBMs with all
possible orders, the bias might be alleviated, and the learned features will become
closer to (19). This inspires us to propose an alternative training objective as follows.

At each gradient descent step t , we first draw a sample of permutation

to

of the

left-most

tM hidden units from a distribution



p o :
t
t

o
t





o o
,
1
2

,


,
o
Mt



~



p o ,
t
t

where,


p o
t
t



is a discrete distribution as there are in total

!tM permutations of

tM numbers. In this paper, we use a uniform distribution

p
t



Mo
1



t




!

t

, which

gives equal chance to each permutation. Other distributions can also be used. We note
tl as the maximal number of activated hidden units at gradient descent step t . To
stabilize the growing of selected hidden units, only part of them are regrouped
M l ).
(

t

t

Then, the left-most

tM hidden units are reordered according to

to :

t
t
h h
,
1
2




,



,

t
h
Mt

 





h h
,


o
o
1
2

,



,

h

o
Mt

,




and the parameters with each hidden unit are also reordered:

t

t


 
1 ,

Mt

,

 


t




o
1

,



t

,

o
Mt

.




Now, all probabilities are conditioned on permutation

to :


p y v o
|
n
t

;

n



and

Finally, the gradient of objective function (18) with


p y v
|n

n



and

np v


replaced by


p y v o
|
n
t

;

n



and

is computed:




p v o
|n
t

t




1





1
D

train

f

hybrid





t

,

D

train

,

t








f

dis





t

,

D

train

,

t







f

gen





t

,

D

train

,

t





,(20)


p v o
|n
t



.


t



where,

and

f

dis





t

,

D

train

,

t



  

ln


p y

n

|

v o
;
n

t



,

f

gen





t

,

D

train

,

t



  

ln

p



v o
|
n

t



.

D

train

n


1

D

train

n


1

f

hybrid




t

,

D t
,
train



is the objective function at step t , its gradient (20) is in fact a

sampling approximation to the gradient of the following marginalized objective
function:

F

hybrid





t

,

D

train

,

t






1






F

dis





t

,

D

train

,

t






F

gen





t

,

D

train

,

t





,

(21)

1
D

train



where,

and

F

dis





t

,

D

train

,

t



   

ln


p y

n

|

v o
;
n

t



p
t




o ,
t

F

gen





t

,

D

train

,

t



   

ln

p



v o
|
n

t



p
t




o .
t

The word "dynamic" is used to emphasize that, the training objective changes
accordingly as the number of regrouping hidden units changes. The proposed training
procedure is briefly illustrated Fig. 3.

D

train

n


1

o
t

D

train

n


1

o

t

tM 

4

 

1h

2h

3h

4h



t o




3, 2, 4,1

3h

2h

4h

1h

tW



tU

p



v

,

y

|

o

;t



t



1v

2v

3v

4v

5v

6v

y

0,



, 0,1, 0,



0





tM  

1

5

 

1h

2h

3h

4h

5h



2h

1h

3h

5h

4h

o

t  

1



2,1,3,5, 4





1t U

p



v

,

y

|

o

;



t

1



t


1

1t W

1v

2v

3v

4v

5v

6v

y

0,



, 0,1, 0,



0





Fig. 3 An illustration of RP training at gradient descent step t and t+1.

From the dynamic objective function (21), we can see that a mixture of

!tM
iRBMs is been trained at gradient descent step t . All the iRBMs share the same set of
hidden units but with different permutations. When the number of hidden units with

non-zero weights

grows during training, the number of mixed iRBMs grows

tl

accordingly. Theoretically,

is allowed to take arbitrary positive integer value. Thus,

tl

the proposed training objective can potentially allow a mixture of infinite many
iRBMs. For convenience, we name our training strategy as "random permutation
(RP)".

tM controls the proportion of hidden units being regrouped. If

tM is too large,

the model will grow explosively. If

tM is too small, the boosting effect is minor. We
prefer as many hidden units to be regrouped as possible, in the meanwhile, the model
tM will be

would not growing too rapidly. The strategy of choosing a proper

specifically discussed in Sec. 4.

3.2 The approximated gradient descent algorithm

In the previous subsection we have defined the objective function (21) together
with the approximated gradient (20) for parameter learning. In this subsection, we will
provide a way of computing the gradient (20).

At each step t , the dynamic gradient (20) is identical to the gradient of the

objective function (18) except the probability


p y v
|n

n



and

np v


are replaced

by


p y v o
|
n
t

;

n



and


p v o
|n
t



. Once

to

is sampled, the training objective aims

at learning an iRBM with permutation

of the hidden units. The learning of the

to

generative part

f

gen




t

,

D t
,
train



is identical to the learning of the iRBM, which is

briefly introduced in Sect.2. CD or PCD can be directly used to compute the gradients.
The approximated gradient for the generative part is given below:


t



f

gen





t

,

D

train

,

t





1
D

train

D

train



n


1









F



pos

v

n

,

z


o

t





F



v

neg

neg

,

z


o

t

,

(22)











where,

posz

is sampled from


p z v o
|

,n

t



, and

neg

v

, z

neg

are sampled from

p



zv
,

o

t



by K-step Gibbs sampling.

The pseudo code of model parameter update for generative training objective is

summarized in Algorithm 1，which is shown in the Appendix.

In order to calculate the gradient of the discriminative part

f

dis




t

,

D t
,
train



, the

conditional probability


p y v o
|
; t



is derived as follows:


p y

|


v o
,



t






p y

p

t


|
,
v o


v o
|

t







F



v

,

y z
,


o
|



t




G y z
,


v o
;

|



t



F



v

,


y z
,


o
|



t




G y z
,



|


v o
;



t

,

(23)





z


1

e







z


1

y



e

where


G y z

,

|


v o
;



t

 

d

y




ln 1 exp

W v U e



t

i

t
i

y



c
i










i

.

(24)

By taking equations (23) and (24) into the discriminative part of (20), the gradient

of

f

dis




t

,

D t
,
train



is derived as follows:





z


1

e

 

z


1

y



e

z



i


1




t



f

dis





t

,

D

train

,

t





1

D

train

D

train



n


1







and


p z

|

v

,

y
n


o
;

t

n




G y z
n

,

|


v o
;
n

t






p z y
|
,


v o
;
n

t




G y z

,

|


v o
;
n

t


























p z

|

v

,

y

;


o



t



e




G y z
,

|


p z y
,

|


v o
;



t



e




G y z
, |


v o
;

t





t

;




v o

e

z


1




G y z
,


|


v o
;
t



,





z




1

y



e




G y z
,




|


v o
;



t

.

(25)









,

(26)

(27)

used to approximate (25). Sampling


p z y  v o
;

For the same reason stated for training the iRBM, the (P)CD algorithm can also be

p z y v o
,
|
; t

p y z
is given below:



p y z v o
|
; t
,

is done by repeating the two

already given in (26) and

and then

z y
, ~

steps:


p z

v o
;
t

yv
,

o
; t

is



~

~

y







z


1

,

,

.

|

|

|

k

k

k

k













t


p y

|

v

,

z

;


o



t



e




G y z
,

|

t

;




v o

e

y







G y z
,


v o
;

|



t

.

(28)

By taking sampling approximations, the gradient of the discriminative part can be

replaced by the following equation:


t



f

dis





t

,

D

train

,

t





1
D

train

D

train



n


1










G y z
n

,

pos


v o
;
n

t






G y

neg

neg

,

z


v o
;
n

t

,(29)











Where

posz

is a sample from


p z

|

yv
,
n
n

,

o
t



,

negy

and negz

are samples from


p z y v o by K-step Gibbs sampling.

,n



,

|

t

However, the approximated gradient (29) causes high variance during learning in
practice. To alleviate this problem, an alternative derivation of the discriminative part
is proposed as follows:


t



where,

and

f

dis




,

D

train

,

t





1
D

train

D

train



n


1



F y



|


v o
;
n
n
t



t





y




p y



|


v o
;
n

t





F y



|




v o
;
n
t



t

,(30)


F y

|


v o
;



  
ln

t



z


1



e




G y z
,

|


v o
;



t



,


p y

|


v o
;



t



e




F y

|

t

;




v o

e




F y


|


v o
;
t



.

y



(31)

(32)

As there are

tl

computing (32) is

hidden units with non-zero weights at step t, the complexity of

O t

2
l D l C



2
t

.

The gradients



F y
t

|

v o
;
t

t



t



can be exactly computed, which are shown in

this involves computing gradients for

infinite many
the Appendix. However,
parameters. To avoid this issue, we only compute the gradients for parameters of first
hidden units, and leave all the remaining parameters to be 0. This operation is
tl
equivalent to using (30) to compute the gradients for non-zero parameters and (29) for
the remaining parameters.

The maximum number of activated hidden units

changes gradually during

tl

training. Practically, if the Gibbs sampling chain ever samples a value of
than tl , we clamp it to
value of z .

z larger
1tl  . This avoids filling large memory for a large (but rare)

3.3 Evaluation of the models

After the training is done (with T steps), we can simply treat

Tl

as the number

of hidden units been trained. However this results in many “redundant” hidden units,
as many of weight vectors is fairly small.

As


P z v

|

n



reflects how strong the filters respond to an input. We can use this

information to estimate the size of efficiently trained hidden layer size as training
progresses, which is given below

N

h



1
M

bN



b

batch


1

max arg max


P z

|

v

n



,

v

n



D

batch

z





,

(33)

where,

bN is the number of mini-batches of the training set.

batchD

is the current

mini-batch. In (33) we take an average between mini-batches of the maximal z giving

the highest


P z v

|

n



in each mini-batch.

Principally,

the likelihood of a new data point

v ,

p



v



for

iRBM

or y conditioned on

v ,


p y

|

v



for Dis-iRBM are computed as follows:

p



v









p



o

T



p




v o
|

T






,


p y

|

v









p



o
T




p y

|


v o
;

T






.







(34)

(35)

However, computing (34) and (35) are much more expensive than simply

computing

p



v o
|
T



and


p y

|

v o
; T



, as there are

!TM different permutations. A

way to deal with it

is using samples

To

:

p



v





N

1
 
N 
1
n

p



v o

|

n
T



,


p y

|

v



N

1
 
N 
1
n


p y

|

v o

;

n
T



.

But if


p z M

T

|


v o
,

0

for an arbitrary order of the first

TM hidden units

is satisfied, the likelihood


p v



is invariant to the order of the first

TM hidden

units.


p v



p

the first

Any

order



o

T

M


1:



v o
|


TM hidden units,

,

T

0



M

T



o

0



M

T

gives


N

1,

h


N
1,



h

the

result,

then

, where

o
T


1:

is an arbitrary order of

same


TM

is the original order of the remaining

hidden units. This conclusion is formally represented in the following form:

Proposition 3.1 For any permutation of the first

TM hidden units

o
T


1:

TM



,

if



p z M

T

|

v o
,
T


1:

M

T







0

,

 v

0,1 D


is satisfied, then

p



v





p

1

v o
|
T


1:

M

T



,


o

0



M

T



1:

N







h



p




v o
|

2
T


1:

M

T



,


o

0



M

T



1:

N





,

h

where


1 1:
o
T

TM



and


2 1:
o
T

TM



are two arbitrary permutations of the first

TM hidden units, and

o

0



M

T

N
1:



h

is the original permutation of the remaining

hidden units. The proof is given in the Appendix.

Experimental results have shown that RP training can successfully make the
condition in Proposition 3.1 approximately satisfied, any ordering gives a nearly
identical result, thus a small N (e.g. N=5) is enough to give a good estimate of (34)
and (35).

4 Experiments and discussions

In this section, we evaluate our training strategy empirically according to the
convergence speed and the final generalization performance. The datasets used for
evaluation are binarized MNIST [22] and CalTech101 Silhouettes [23].The MNIST
dataset is composed of 70,000 images of size 28  28 pixels representing handwritten
digits (0-9), among which 60,000 images are used for training, and 10,000 images for
testing. Each pixel of the image has been stochastically binarized according to its
intensity as in [22]. The CalTech101 Silhouettes dataset is composed of 8,671 images
of size 28  28 binary pixels, representing object silhouettes of 101 classes. The
dataset is divided into three parts: 4,100 examples for training, 2,264 for validation
and 2,307 for testing.We reshape each image of both datasets into a 784-dimensional
vector by concatenating the adjacent rows one by one.

principle of choosing a proper regrouping rate

We have designed several experiments for different purposes. In Subsect. 4.1, the
tM was experimentally investigated.
In Subsect. 4.2, we evaluated the generalization performance of the iRBM trained
with RP according to its log-likelihood on the test sets of binarized MNIST and
CalTech101 Silhouettes. In Subsect. 4.3, we evaluated the generalization performance
of Dis-iRBMs trained with RP on classification tasks. For all the experiments, the
mini-batch size is 100 and (P)CD is used to compute the gradients. Max-norm
regularization [16] was also used to suppress very large weights, the bounds for each
iU  were 10 and 5 respectively. Côté and Larochelle [14] claims that
iW 

and

results of learning are robust to the value of the hidden unit penalty

i. We have tried

several different

i and find that smaller

i enables the model to grow to proper
size faster at the beginning of learning. However, due to the ordering effect, it takes a
long time for the hidden units to learn filters diverse from each other. RP training also
i to allow as many hidden units to be mixed as possible. But a too

prefers a small

small

i (coupled with a large

tM ) is more likely to cause the model grow
explosively. Based on the above arguments, and for convenience of comparing, we

used the same

i 


1.01 ln 2

for all the models in this paper, which is identical to

that in [14]. We also used L1 regularization and L2 regularization to regularize the
models. The code to reproduce the results of this paper is available on GitHub1.

We would like to mention that there exist a number of sophisticated techniques
that improve performance of classic RBMs on sampling strategies [24 ， 25], model

1https://github.com/Boltzxuann/RP-iRBM

architectures [26，27], etc. However, the aim of this paper is to propose a alternative

training strategy for faster convergence and better generalization of the original
iRBMs in general. Combining these techniques can benefit for both training strategies,
here we focus on comparing on basic settings of parameters.

4.1 The principle of choosing the regrouping rate

tM

As mentioned in Sect. 3, choosing a proper

tM is essential for RP training. In

this subsection, we experimentally investigated the influence of

tM on the growing
of the model, and a principle of choosing it is proposed based on the experimental
results.

In order to have a preliminary understanding on how different regrouping rates
tM influence the growing of the model, we have tried several different settings of

tM from 0 to 0.9 tl . The model was trained for 10 epochs on binarized MNIST and
60 epochs on CalTech101 Silhouettes for each setting, and the training was repeated
for 5 times. The mean results of the 5-time trials are illustrated in Fig. 4. The results

of

tM 

0

illustrate the growing of iRBMs without RP training, which are the

baselines for comparison. As shown in Fig. 4, the growing of the model is not
tM is smaller than 0.8 tl . The growing is even

remarkably influenced as long as

more stable when using RP for CalTech101 Silhouettes.

(a)

(b)

Fig.4. Growing of iRBMs when using different regrouping rates

tM , (a) binarized MNIST; (b)

CalTech101 Silhouettes.

Fig. 5 illustrates the results of


p z v o
|
T

;n



for all training examples after 50

epochs of training on binarized MNIST and 300 epochs of training on CalTech101
Silhouettes. An interesting fact of RP trained iRBMs is that, RP training pushes all the

z

that maximize


p z v o
|
T

;n



above

TM . Similar numbers of hidden units are

activated even though the inputs are quite different. The number of activated hidden
units ranges from 500~520 on binarized MNIST and 700~750 on CalTech101
Silhouettes.

Fig. 5. A illustration of


p z v
|

n



for all training examples after 50 epochs of training on

binarized MNIST (left column) and 300 epochs of training on CalTech101 Silhouettes (right
column). The top row are results without RP training, while the bottom row are those with RP
training.

Proposition 3.1 states that if


p z M

T

|


v o
,
T

0

for all

v

0,1 D


is

fulfilled,


p v



is invariant

to the order of

the first

TM .

In that case,


p z M

T

|

v o
,
n

T





0

for all observed data

nv . This can be easily checked by

computing


p z M

T

|

v o
,
n
T



for all training examples on an arbitrary permutation

To . After 50 epochs of training on binarized MNIST and 300 epochs on CalTech101

Silhouettes, the average value of

ln


p z M

T

|

v o
,
n
T



on the two training sets are

-28.06 and -37.46 respectively, which are fairly small. The condition in Proposition

tl for all
p z

3.1 is approximately satisfied. Plots of

i v o
|

from

1i 

to

,n



T

training examples on the two datasets are shown in Fig. 6.

Fig. 6. A illustration of


p z

i

|

v



n

for all training examples after 50 epochs of training on

binarized MNIST (left)

and

300

epochs

of

training

on CalTech101 Silhouettes

(right).


p z M

t

|


v

n

0

are approximately satisfied on the whole training sets.

As


p z v
|

n



is influenced by the value of

tM , it can be used as "feedback" to

guide the change of

tM during training, which lead to a more adaptable

tM proposed

as follows:

M

t



1
epoch

epoch


1



0.8



epoch

0.2





M epoch

z





10

,

where


zM epoch



is the average number of activated hidden units defined below:


M epoch

z





1
D

train

D

train



n


1

arg max

z


p z

|

v o
;
n


t epoch



.

Based on the above analysis, a principle of choosing

tM is proposed here:

At the beginning of training,

M

t



l
0.7
t



l
0.8
t

to allow a greedy mixing of

iRBMs. After that, a more adaptable

tM defined by (32) is used to stabilize the

(32)

(33)

growing.

models

4.2 Evaluating the generalization performance of RP trained iRBMs as density

This subsection aims at evaluating the generalization performance of the iRBM
trained with RP as a density model. Firstly, we investigated the property of RP
training with different learning rates and regularizations. We have also trained an
iRBM without RP training, as a comparison to our method. All the iRBMs were
trained on binarized MNIST. Two different learning rates, a decaying learning rate

(

lr

t
1

) and an adaptive learning rate method ADAGRAD [28], were used. And the

global learning rate for ADAGRAD was 0.05, which gives the best results in [14].

. Each model was
The weights of the regularizations (L1 and L2) were set as
trained up to 1000 epochs and we performed the Annealed Importance Sampling [22]
to estimate the log likelihood of the test set every 50 epochs．The results are shown in

1 10
4

Fig. 7. For ordinarily trained iRBMs, we only show the results using ADAGRAD, as
the decaying learning rate leads to fairly poor convergence. After 1000 epochs of
training, the effective activated number of hidden units of the ordinary trained iRBMs
using CD are 805 (L2 regularization) and 566 (L1 regularization), which are larger
than those of RP trained iRBMs (588 for L2 regularization and 501 with L1
regularization). And their convergence is much slower than the later. Due to the
"ordering effect", many filters of the former are inadequately trained, which results in
more "redundant" hidden units than the later.
is also noticeable that L1
regularization gives better results than L2 regularization. This may indicate that
sparsity can lead to better generalization for iRBMs.

It

Fig. 7. Log-likelihood on test set of binarized MNIST for iRBMs with different training strategies.
Where "L1R" and "L2R" stand for "L1 regularization" and "L2 regularization" respectively,
"ADA" stands for "ADAGRAD", "DCL" stands for "decaying learning rate".

Fig. 8 illustrates the filters learned by the hidden units of the two iRBMs after 10
epochs of training, where the left-most 100 filters are illustrated. As shown in Fig. 8,
the filters learned by hidden units with RP training are more diverse from each other
than those learned without RP training. The former contains various kinds of local
features such as strokes and specific character parts. While the ordering effect in the
later is obvious, the left filters look like mixtures of different characters, and the right

ones are just different shapes of the character "1".

(a)

(b)

Fig. 8. Comparison of learned filters between iRBMs with different learning strategies after 10
epochs of training, (a) with RP; (b) without RP. The left-most100 filters are shown starting from
the top-left corner and incrementing across rows first.

Based on the above experimental results, we used PCD with 10 steps of Gibbs
sampling and CD with 25 steps of Gibbs sampling to train the models on binarized
MNIST and CalTech101 Silhouettes respectively. And L1 regularization was used all
models. Each model was trained up to 1000 epochs. We have also performed a grid
search on all the hyper parameters (global learning rate: 0.001~1, L1 regularization
weight: 0~0.01).The best results of different models are illustrated in Table 1.

Table 1. The best results of average log-likelihood on test sets of binarized MNIST and
CalTech101 Silhouettes of different models.

Binarized MNIST

CalTech101 Silhouettes

Model
RBM [22]
iRBM [14]

FW-iRBM [18]

iRBM, RP

Size
500
1208

460

674

Avg. LL
-86.34
-85.65
 -85

-85.81

Model
RBM [14]
iRBM [14]

FW-iRBM [18]

iRBM, RP

Size
500
915

550

695

Avg. LL
-119.05
-121.47

 -127

-114.09

The best model trained on binarized MNIST has 674 effective hidden units. The

global learning rate is 0.05, and L1 regularization weight is
. Its average
log-likelihood on the test set is -85.81, which is similar to-85.65 (3000 epochs of
training) in [14]. The size of our model is also smaller than that in [14], which has
1208 hidden units with non-zero parameters. The best model trained on CalTech101
Silhouettes has 695 effective hidden units. The global learning rate is 0.02, and L1

1 10
4

. Its average log-likelihood on the test set is -114.09,
regularization weight is
which is better than -121.47 in [14]. The results of FW-iRBM [18] are also listed in

1 10
3

the table, the best results are achieved by RBMs with CD training initiated by
FW-iRBMs. Samples from the best models are illustrated in Fig. 9.

5

n


1

m
z

(a)

(b)

Fig. 9. Random samples drawn from the best models by randomly initializing visible units and
running10,000 Gibbs steps, examples from the test set are also illustrated, (a) binarized MNIST;
(b) CalTech101 Silhouettes.

To show how RP training affects the dependency between z and v . We computed

the histogram of

z

m



arg max

z


p z

|


 v

t


p z

|

v o
;
t

n
T



on the two test sets. The

results are shown in Fig. 10, which reveals two facts: (a) All

mz

are larger than

tM ;

(b) All the inputs have similar numbers of activated hidden units, as all

mz

are close

to each other. The number of example-specific filters has been greatly reduced.

m
z

MT

MT

(a)

hi

mz

hi

(b)

Fig. 10. The histogram of

on the two test sets, (a) binarized MNIST; (b) CalTech101

Silhouettes.

To further validate that the filters are independent from each other, we just use all
is clamped to

to compose a classic RBM,

the learned filters

i.e.

z

hN ,

p




v

p



v

|

N



h

. The average log-likelihood on the two test sets are

-88.13(binarized MNIST) and -115.90 (CalTech101 Silhouettes) for the converted
RBMs.

4.3 Evaluating the generalization performance of RP trained Dis-iRBMs on

0.1

lr 

0 ) was optimized. Two different learning rates (

classification tasks
In this subsection, we aim at evaluating the generalization performance of
Dis-iRBMs trained with RP on binarized MNIST and CalTech101 Silhouettes. Firstly
we compared the performance of RP with ordinary training to validate its boosting on
learning speed and as guidance for selecting hyper-parameters. The discriminative
training objective (
, 1)
together with ADAGRAD (global learning rate is 0.1), were used. For each learning
rate, we trained the model with or without our training strategy for 5 epochs. At the
end of every epoch, miss-classification rate on the test set was computed. We repeated
the training procedure for each parameter setting 5 times. The results are shown in Fig.
10. As shown in Fig. 11, RP accelerates learning for all the learning rates. ADAGRAD
coupled with RP training achieves the best performance in this experiment. The best
)
result with RP training at epoch 5 is 3.10%. High learning rate (e.g.,
encourages the model to quickly explore different regions of the weight space, but it
is also more likely to cause oscillating unless proper learning rate decay is used.
ADAGRAD gives smaller learning rates to parameters close to convergence, thus it is
more stable than the fixed learning rate. Another fact can be observed from Fig. 11 is
that, RP training makes the learning more stable, the variance of learning is much
smaller. As the additional label information makes the filters prefer some classes
instead of others, the "ordering effect" is more significant.

lr 

1

Fig. 11. Comparison of convergence speed between Dis-iRBMs trained with or without RP. The
test errors are illustrated in log-scale.

According to the results of the above experiment, we used RP to train a
Dis-iRBM on binarized MNIST and CalTech101 Silhouettes, and evaluated their
generalization performance
test miss-classification errors.
ADAGRAD were used in all experiments. The validation sets were used to search the
hyper parameters on a log-scale (the generative proportion : 0~1, global learning
rate: 0.001~1, L1 regularization weight: 0~0.01). The best results for different models
and method are illustrated in Table 2.

according to the

Table 2. The best results of classification error on the test set of MNIST and CalTech101
Silhouettes achieved by different models.

Model

Dis-iRBM

CalTech101 Silhouettes
Training
objective
Dis.
Hybrid

Size

235

373

(



0.01

)

Test
error
37.10%

34.59%

2.20% FW-iRBM+SR [18]

600

--

34.5%

(



0.005

)

1.71%

Dis-iRBM, RP

165

Dis.

34.55%

742

Hybrid

(



0.01

)

30.95%

Binarized MNIST

Model

ClassRBM [19]

Dis-iRBM

Test
error
1.81%

1.28%

Training
objective
Dis.
Hybrid

(



0.01

)

Dis.
Hybrid

Size

500

1500

382

416

FW-iRBM+SR [18]

600

--

2.2%

Dis-iRBM, RP

489

621

Dis.

Hybrid

(



0.005

)

1.92%

1.41%

The best Dis-iRBM trained using hybrid training objective (

) achieves
a test error of 1.42% on MNIST, which is better than 1.71% achieved by normally
trained Dis-iRBM. The global learning rate for it is 0.1, and the L1 regularization

0.005



1 10
4

. Dis-iRBM performs slightly worse than the ClassRBM on MNIST ,
weight is
but the difference between the two best results is smaller than 0.2%, which is
commonly regarded as statistical
insignificant for MNIST. The best result on
CalTech101 Silhouettes is also achieved by a Dis-iRBM trained using hybrid training
objective (
) and RP training. The test error is 30.95%, which is better than
34.59% achieved by normally trained Dis-iRBM. The global learning rate for the best

0.01



. An interesting fact about
model is 0.01, and the L1 regularization weight is
the model size of iRBMs trained with different training objective is that size is smaller
when merely the discriminative training objective is used. This makes sense as less
features are often needed if the model only needs to discriminate objects from each
other, instead of modeling all the examples well.

1 10
3

The FW-iRBM [18] have also been used for classification by taking the hidden
units' activation vectors and using them as input for a softmax regression (SR). As
FW-iRBM cannot perform discriminative training directly due to its training objective,

learning the FW-iRBM and learning the SR are two separate procedures. i.e., after
training the FW-iRBM with T iterations, fix the parameters and using its hidden
units' activation vectors to train a SR. The results of FW-iRBM are also listed in the
table.

A "trick" to make the learning a bit faster is using the momentum, we use
different momentum values for parameters of different hidden units according to the
time they have been trained. When the hidden unit is added to training, the starting
momentum value is 0.5, and then it gradually increases to 0.9.

5 Conclusion and future work

In this paper, we have proposed a novel training strategy (RP training) for the
infinite RBMs, which aims at achieving better convergence and generalization. The
core concept of the RP training is a dynamic training objective that allows a different
model to be optimized at each gradient descent step. More specifically, an iRBM with
an random grouping of hidden units is sampled before doing gradient descent. An
implicit mixture of infinite many iRBMs with different permutations of hidden units
is achieved with RP training. Experiments on binarized MNIST and CalTech101
Silhouettes have shown that, RP can train the hidden units more efficiently, thus
results in smaller hidden layer size and better generalization performance. Compared
with the FW-iRBM, the iRBM trains all the hidden units jointly not one unit greedily
each update step, thus the former is more likely to reach sub-optimal solution. In the
future, more datasets especially some real-valued datasets, will be used to give a
further evaluation of the performance of our training strategy. Meanwhile, we are
exploring a multi-layer extension of the iRBM, the idea of RP training can be also
applied to this new architecture, combined with a greedy layer-wise pre-training.

Appendix

A.1

Algorithm 1
RP training
Notation: ~ p

x

means x is sampled from p

x

y means x is set to y

Initiation of parameters: 0
for t,2,T do

l  ;
1

0
  


,

t

0




0

Parameter update of the iRBM for generative training objective with

Input:

training example 

nyv
,n



, learning rate

 
lr t

, re-permutating length

tM

#Re-permutating the hidden units and the corresponding parameters

#Sampling a permutation of length



o
,
Mt


p o
t



2,
o o
1

o
t

~





,

t

tM

t
t
h h
,
1
2




,



,

t
h
Mt

 





h h
,


o
o
1
2

,



,

h

o
Mt




t



1


1

,




1
t

,
Mt

 



1

t




o
1

,




1

t

,

o
Mt




#Positive phase of CD
ˆ
pos
h


p z v o
|
;n

pos ~



z

,

t

p



h v
|

,

z

n

pos

;

o

t



neg

v

~

p

v h
|

neg

pos

,

z



;

o

t



,

neg

z

~


p z v
|

neg

;

o

t



#Negative phase of CD


h v
|

h

~

o



p

neg

pos

z

;

,

n

t

,

ˆ
neg
h

p



h v
|

neg

neg

,

z

;

o

t



#Update
for t

   do

t

end for
#Adding a hidden unit
> 1tl  and negz
l  
1 1

posz

if

l
t

t

> 1tl  then

t

t

1
 
 

 
lr t

F



pos

v

n

,

z


o

t





F



v

neg

neg

,

z


o

t



















t

0

lt 
else then
l 

l
t

t

1

end if
end for

A.2


t



where,

Derivation of gradients (30) w.r.t.

t is presented as follows. Without loss of

generality, we only show the results related to the binary Dis-iRBM.

Recall the equation (30):

f

dis




,

D

train

,

t





1
D

train

D

train



n


1



F y



|


v o
;
n
n
t



t





y




p y



|


v o
;
n

t





F y

|




v o
;
n
t



t



,



F y

|


v o
;
n

t














ln







e




G y z
,

|


v o
;
n

t













z


1



e




G y z
, |


v o
;
t
n






G y z

,

|


v o
;
n

t




1
z
1

 





z


1



e




G y z
,

|


v o
;
n

t








G y z
,

|


v o
;
n

t






G y z
,

|


v o
;
n

t



e


e





z


1







z


1







z


1












P z

|

v

n

,

y

;


o



t


G y z

,

|


v o
;
n

t



.

(38)

In order to compute (38), we need to compute


G y z

,

|

v o
;n

t



first, which





are shown as follows:


t
W

i




G y z

,

|


v o
;



t

 


ln 1 exp

W v U e







t

i

t
i

y



t
c

i









i









i


W

i



z



i


1


t
W


i

z



i


1









 


ln 1 exp

W v U e







t

i

t
i

y



t
c

i

 


H z



i




ln 1 exp

W v U e



t

i

t
i

y



t
c
i










t
W


i

 


H z



i

 


H z



i



c
i

y

t
i



t

i

exp



1 exp
 
s

W v U e



W v U e



W v U e




c
i



t

i

t

i

t
i

t
i

y

y



v


c
i

v



.

Similarly,


d


t

c
i

where,



G y z

,


v o
;

t



|
t
i



U


 


H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



e

y

,


t G y z
,

|


v o
;



t

 


H z



i



e

y

,


G y z

,

|


v o
;



t

 


H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



,


H z

i

0,


  
1,


z
z




i
i

.

(39)

(40)

(41)

(42)

Substitute (39) ~ (42) into (38), we get




t



v o
|

F y
;
t
W


i

 



z


1


P z

|

v

,



y H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



v

,

(43)




P z



i

|

v

,

y

 
s

W v U e



t

i

t
i

y



t
c
i



v



t



F y
|
U



v o
;
t
i







z


1

 


P z

|

v

,



y H z



i

 
s

W v U e



t

i

t
i

y



t
c
i



e

y




P z



i

|

v

,

y

 
s

W v U e



t

i

t
i

y



t
c
i



e

y


|
F y

d





v o
;
t

t

 





z


1


P z

|

v

,

y

;



o e
t

y

 

e

y

,



t



v o

F y
|
;
t

c
i

 





z


1


P z

|

v

,

y

;


o




H z

t



i

 
s

W v U e



t

i

t
i

y



t
c
i






P z



i

|

v

,

y

;

t

o W v U e


i



t
i

t

 
s

y



t
c
i



,

,

(44)

(45)

(46)

where,


P z



i

|

v

,

y

;


o

t



  
1


P z



|

v

,

y

;


o



.

t

i

z


1

A.3

The proof of proposition 3.1 is given as follows:

Suppose the permutation of the first

TM hidden units is


1 1:
o
T

TM



. The

likelihood of v conditioned on this permutation is given below:

From the condition given in Proposition 3.1 we can derive that,



p z M

T

|

1
v o
,
T


1:

M

T







0

,

 v

0,1 D



p z



i

|

1
v o
,
T


1:

M

T







0

, for

i

 
1, 2,

,

M

T

M

T



z




1

exp



F





v

,

z



|

1

o
T


1:

M

T









exp



F

v

,

z

|

1

o
T


1:

M

T


















z M

T


1

exp



F



i

 
1, 2,

,

M

T

v

,

z



|


o



M

T

0



1:

N

h









0

, for



ln 1 exp


F





v

,

exp






z M

T


1



W v

i



c
i





z



|


o



M

T

0



1:

N

h









0

, for

i

 
1, 2,

,

M

T









exp

F

v

,

z

|

2

o
T


1:

M

T







exp

F

v

,

z



|


o



M

T

0



1:

N

h





















z M

T


1



0

,

(47)

for

i

 
1, 2,

,

M

and any permutation

T

2

o
T


1:

M

T





1

o
T


1:

M

T



and

 v

0,1 D





p z



i

|

v o
,

2
T


1:

M

T







0

, for

i

 
1, 2,

,

M

T

The above conclusion indicates that if any permutation

o
T


1:

TM



satisfies that



i

|

v o
,


1:

M

T

T



0

, then all the other permutations

o
T


1:

TM



also satisfy


p z

p z



i

|

v o
,


T


1:

M

T



0

.

Now, consider the likelihood

p



v o
|

T


1:

M

T





for an arbitrary permutation













o
T


1:

TM



:

p




v o
|

T


1:

M

T







z


1





z




1

v



M

T



z


1



M

T



z




1

v







exp



F



v

,

z

|


o


1:

M

T

T







exp



F



v


,

z



|


o


1:

M

T

T







(48)

.







exp



F

v

,

z

|




o
T


1:

M

T









exp



F





v

,

z

|


o



M

T

0



1:

N

h







exp



F

v


,

z



|


o
T


1:

M

T













 



z M


1

T

v



exp



F





v


,

z



|


o



M

T

0



1:

N

h





T



z M


1

Take (47) into account, we have

p




v o
|
T


1:

M

T








1

T

z M



 



z M

T


1

v







exp



F





v

,

z

|


o



M

T

0



1:

N

h







exp



F





v


,

z



|


o



M

T

0



1:

N

h







.

(49)

Equation (49) indicates that

p



v o
|

T


1:

M

T

is invariant to the permutation




now becomes:

o
T


1:

TM



. The marginalized likelihood


p v



p



v








o
T

p




v o
|

T


1:



M p
T




o

T


1:

M

T









exp

F





v

,

z

|


o



M

T

0



1:

N

h







exp

F





v


,

z



|


o



M

T

0



1:

N

h







p




o

T


1:

M

T





exp

F

v

,

z

|


o



M

T

0



1:

N

h

exp

F

v


,

z



|


o



M

T

0



1:

N

h

p




o

T


1:

M

T








o
T



















exp

F

v

,

z

|


o



M

T

0



1:

N

h

exp

F

v


,

z



|


o



M

T

0



1:

N

h












o
T


1

T


z M


 



z M

T


1

v














1

T

z M



 



z M


1

T

v






1

T

z M



 



z M


1

T

v





















p




v o
|

0


1:

M

T





References

[1] Ackley, D.H., Hinton, G.E., Sejnowski, T.J.. A learning algorithm for Boltzmann machines.

Cognitive Science vol.9, pp. 147–169, 1985.

[2] Smolensky, P.. Information processing in dynamical systems: foundations of harmony theory.
In: Parallel Distributed processing: Explorations in the Microstructure of Cognition,
Foundations, vol. 1, USA, pp. 194–281, 1986.

[3] G. E. Hinton, R. R. Salakhutdinov. Reducing the Dimensionality of Data with Neural

Networks. Science, vol. 313, pp.504-507, 2006.

[4] A. Mohamed and G. E. Hinton. Phone recognition using restricted Boltzmann machines. In
IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), pp.
4354–4357, 2010.

[5] Mohamed A, Dahl G E, Hinton G. Acoustic modeling using deep belief networks. IEEE

Transactions on Audio Speech & Language Processing, vol. 20, pp. 14-22, 2011.

[6] R. Salakhutdinov, A. Mnih, and G. E. Hinton. Restricted Boltzmann machines for
collaborative filtering. In Z. Ghahramani, editor, Proceedings of the 24th International
Conference on Machine Learning (ICML), pp. 791–798. ACM, 2007.

[7] Truyen Tran, Dinh Phung, Svetha Venkatesh. Mixed-Variate Restricted Boltzmann Machines.

Eprint Arxiv, 1408.1160v1, 2014.

[8] G. W. Taylor, G. E. Hinton, and S. T. Roweis. Modeling human motion using binary latent
variables.In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information
Processing Systems (NIPS 19), pp. 1345–1352. MIT Press, 2007.

[9] Fischer A, Igel C. Empirical Analysis of the Divergence of Gibbs Sampling Based Learning
Algorithms for Restricted Boltzmann Machines. Artificial Neural Networks – ICANN 2010.
Springer Berlin Heidelberg, pp. 208--217, 2010.

[10] Nitish Srivastava, Georey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.
Dropout: A Simple Way to Prevent Neural Networks from Over fitting. Journal of Machine
Learning Research, vol. 15, pp. 1929-1958, 2014.

[11] Tomczak J M, Gonczarek A. Sparse hidden units activation in Restricted Boltzmann Machine.
Progress in Systems Engineering. Springer International Publishing, pp. 181-185, 2015.

[12] M. Welling, R. S. Zemel, and G. E. Hinton. Self supervised boosting. In NIPS, 2002.
[13] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In

[14] Côté M A, Larochelle H. An infinite restricted Boltzmann machine. Neural Computation,

[15]T. G. Dietterich. Ensemble methods in machine learning. In Multiple classifier systems, pp.

ICML, 2010.

vol.28, pp. 1265-1289, 2016.

1–15. Springer, 2000.

[16]Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine Learning Research, vol. 15, pp. 1929-1958,
2014.

[17] Baldi P, Sadowski P. The dropout learning algorithm. Artificial Intelligence, vol. 210, pp.

78-122, 2014.

[18]W Ping, Q Liu, AT Ihler. Learning infinite RBMs with Frank-Wolfe. In NIPS, 2016.
[19] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation. Vol. 14, pp. 1771–1800, 2002

[20] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood
gradient. In International Conference on Machine learning(ICML), pp. 1064–1071, 2008.
[21] Hugo Larochelle, et al. Learning Algorithms for the Classiﬁcation Restricted Boltzmann

Machine. Journal of Machine Learning Research vol. 13, pp.

643-669, 2012.

[22] Salakhutdinov, R. and Murray, I. On the quantitative analysis of Deep Belief Networks. In
Proceedings of the 25th Annual International Conference on Machine Learning (ICML), pp.
872-879, 2008.

[23] Marlin, B. M., Swersky, K., Chen, B., and de Freitas, N. Inductive Principles for Restricted
Boltzmann Machine Learning. Proc. Intl. Conference on Artificial Intelligence and Statistics,
pages 305–306.

[24] K. Cho, T. Raiko, and A. Ilin. Parallel tempering is efficient for learning restricted Boltzmann
machines. In Proceedings of the International Joint Conference on Neural Networks (IJCNN),
pages 3246–3253. IEEEPress, 2010.

[25] Sohldickstein J, Battaglino P, Deweese M R. Minimum probability flow learning, in ICML.

2009.

[26] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an

application to information retrieval. In NIPS, 2005.

[27] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
[28] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and

stochastic optimization. J. Mach. Learn. Res., vol. 12, pp. 2121-2159, 2011.

