Neural Relation Extraction with Selective Attention over Instances

Yankai Lin1, Shiqi Shen1, Zhiyuan Liu1,2∗, Huanbo Luan1, Maosong Sun1,2
1 Department of Computer Science and Technology,
State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China

Abstract

Distant supervised relation extraction has
been widely used to ﬁnd novel relational
facts from text. However, distant su-
pervision inevitably accompanies with the
wrong labelling problem, and these noisy
data will substantially hurt
the perfor-
mance of relation extraction. To allevi-
ate this issue, we propose a sentence-level
attention-based model for relation extrac-
tion. In this model, we employ convolu-
tional neural networks to embed the se-
mantics of sentences. Afterwards, we
build sentence-level attention over multi-
ple instances, which is expected to dy-
namically reduce the weights of those
noisy instances. Experimental results on
real-world datasets show that, our model
can make full use of all informative sen-
tences and effectively reduce the inﬂuence
of wrong labelled instances. Our model
im-
achieves signiﬁcant and consistent
provements on relation extraction as com-
pared with baselines. The source code of
this paper can be obtained from https:
//github.com/thunlp/NRE.

1

Introduction

In recent years, various large-scale knowledge
bases (KBs) such as Freebase (Bollacker et al.,
2008), DBpedia (Auer et al., 2007) and YAGO
(Suchanek et al., 2007) have been built and widely
used in many natural language processing (NLP)
tasks, including web search and question answer-
ing. These KBs mostly compose of relational facts
with triple format, e.g., (Microsoft, founder,
Bill Gates). Although existing KBs contain a

∗ Corresponding
uzy@tsinghua.edu.cn).

author:

Zhiyuan

Liu

(li-

massive amount of facts, they are still far from
complete compared to the inﬁnite real-world facts.
To enrich KBs, many efforts have been invested
in automatically ﬁnding unknown relational facts.
Therefore, relation extraction (RE), the process of
generating relational data from plain text, is a cru-
cial task in NLP.

Most existing supervised RE systems require a
large amount of labelled relation-speciﬁc training
data, which is very time consuming and labor in-
tensive. (Mintz et al., 2009) proposes distant su-
pervision to automatically generate training data
via aligning KBs and texts. They assume that if
two entities have a relation in KBs, then all sen-
tences that contain these two entities will express
this relation. For example, (Microsoft, founder,
Bill Gates) is a relational fact in KB. Distant su-
pervision will regard all sentences that contain
these two entities as active instances for relation
founder. Although distant supervision is an
effective strategy to automatically label training
data, it always suffers from wrong labelling prob-
lem. For example, the sentence “Bill Gates ’s turn
to philanthropy was linked to the antitrust prob-
lems Microsoft had in the U.S. and the European
union.” does not express the relation founder
but will still be regarded as an active instance.
Hence, (Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012) adopt multi-instance learn-
ing to alleviate the wrong labelling problem. The
main weakness of these conventional methods is
that most features are explicitly derived from NLP
tools such as POS tagging and the errors generated
by NLP tools will propagate in these methods.

Some recent works (Socher et al., 2012; Zeng
et al., 2014; dos Santos et al., 2015) attempt to
use deep neural networks in relation classiﬁca-
tion without handcrafted features. These meth-
ods build classiﬁer based on sentence-level anno-
tated data, which cannot be applied in large-scale

rized as follows:

• As compared to existing neural relation ex-
traction model, our model can make full use
of all informative sentences of each entity
pair.

• To address the wrong labelling problem in
distant supervision, we propose selective
attention to de-emphasize those noisy in-
stances.

• In the experiments, we show that selective
attention is beneﬁcial to two kinds of CNN
models in the task of relation extraction.

2 Related Work

Relation extraction is one of the most impor-
tant tasks in NLP. Many efforts have been invested
in relation extraction, especially in supervised re-
lation extraction. Most of these methods need a
great deal of annotated data, which is time con-
suming and labor intensive. To address this issue,
(Mintz et al., 2009) aligns plain text with Free-
base by distant supervision. However, distant su-
pervision inevitably accompanies with the wrong
labelling problem. To alleviate the wrong la-
belling problem, (Riedel et al., 2010) models dis-
tant supervision for relation extraction as a multi-
instance single-label problem, and (Hoffmann et
al., 2011; Surdeanu et al., 2012) adopt multi-
instance multi-label learning in relation extraction.
Multi-instance learning was originally proposed to
address the issue of ambiguously-labelled training
data when predicting the activity of drugs (Diet-
terich et al., 1997). Multi-instance learning con-
siders the reliability of the labels for each instance.
(Bunescu and Mooney, 2007) connects weak su-
pervision with multi-instance learning and extends
it to relation extraction. But all the feature-based
methods depend strongly on the quality of the fea-
tures generated by NLP tools, which will suffer
from error propagation problem.

Recently, deep learning (Bengio, 2009) has
been widely used for various areas, including com-
puter vision, speech recognition and so on. It has
also been successfully applied to different NLP
tasks such as part-of-speech tagging (Collobert
et al., 2011), sentiment analysis (dos Santos and
Gatti, 2014), parsing (Socher et al., 2013), and
machine translation (Sutskever et al., 2014). Due
to the recent success in deep learning, many re-

Figure 1: The architecture of sentence-level
attention-based CNN, where xi and xi indicate the
original sentence for an entity pair and its corre-
sponding sentence representation, αi is the weight
given by sentence-level attention, and s indicates
the representation of the sentence set.

KBs due to the lack of human-annotated train-
ing data. Therefore, (Zeng et al., 2015) incor-
porates multi-instance learning with neural net-
work model, which can build relation extractor
based on distant supervision data. Although the
method achieves signiﬁcant improvement in re-
lation extraction, it is still far from satisfactory.
The method assumes that at least one sentence that
mentions these two entities will express their rela-
tion, and only selects the most likely sentence for
each entity pair in training and prediction. It’s ap-
parent that the method will lose a large amount
of rich information containing in neglected sen-
tences.

In this paper, we propose a sentence-level
attention-based convolutional neural network
(CNN) for distant supervised relation extraction.
As illustrated in Fig. 1, we employ a CNN to
embed the semantics of sentences. Afterwards, to
utilize all informative sentences, we represent the
relation as semantic composition of sentence em-
beddings. To address the wrong labelling prob-
lem, we build sentence-level attention over mul-
tiple instances, which is expected to dynamically
reduce the weights of those noisy instances. Fi-
nally, we extract relation with the relation vector
weighted by sentence-level attention. We evaluate
our model on a real-world dataset in the task of
relation extraction. The experimental results show
that our model achieves signiﬁcant and consistent
improvements in relation extraction as compared
with the state-of-the-art methods.

The contributions of this paper can be summa-

searchers have investigated the possibility of us-
ing neural networks to automatically learn features
for relation extraction. (Socher et al., 2012) uses
a recursive neural network in relation extraction.
They parse the sentences ﬁrst and then represent
each node in the parsing tree as a vector. More-
over, (Zeng et al., 2014; dos Santos et al., 2015)
adopt an end-to-end convolutional neural network
for relation extraction. Besides, (Xie et al., 2016)
attempts to incorporate the text information of en-
tities for relation extraction.

Although these methods achieve great success,
they still extract relations on sentence-level and
suffer from a lack of sufﬁcient training data.
In
addition, the multi-instance learning strategy of
conventional methods cannot be easily applied in
neural network models. Therefore, (Zeng et al.,
2015) combines at-least-one multi-instance learn-
ing with neural network model to extract relations
on distant supervision data. However, they assume
that only one sentence is active for each entity pair.
Hence, it will lose a large amount of rich informa-
tion containing in those neglected sentences. Dif-
ferent from their methods, we propose sentence-
level attention over multiple instances, which can
utilize all informative sentences.

The attention-based models have attracted a lot
of interests of researchers recently. The selectiv-
ity of attention-based models allows them to learn
alignments between different modalities.
It has
been applied to various areas such as image clas-
siﬁcation (Mnih et al., 2014), speech recognition
(Chorowski et al., 2014), image caption generation
(Xu et al., 2015) and machine translation (Bah-
danau et al., 2014). To the best of our knowl-
edge, this is the ﬁrst effort to adopt attention-based
model in distant supervised relation extraction.

3 Methodology

Given a set of sentences {x1, x2, · · · , xn} and
two corresponding entities, our model measures
the probability of each relation r. In this section,
we will introduce our model in two main parts:

• Sentence Encoder. Given a sentence x and
two target entities, a convolutional neutral
network (CNN) is used to construct a dis-
tributed representation x of the sentence.

• Selective Attention over Instances. When
the distributed vector representations of all
sentences are learnt, we use sentence-level at-

tention to select the sentences which really
express the corresponding relation.

3.1 Sentence Encoder

Figure 2: The architecture of CNN/PCNN used for
sentence encoder.

As shown in Fig. 2, we transform the sentence
x into its distributed representation x by a CNN.
First, words in the sentence are transformed into
dense real-valued feature vectors. Next, convo-
lutional layer, max-pooling layer and non-linear
transformation layer are used to construct a dis-
tributed representation of the sentence, i.e., x.

3.1.1

Input Representation

The inputs of the CNN are raw words of the
sentence x. We ﬁrst transform words into low-
dimensional vectors. Here, each input word is
transformed into a vector via word embedding ma-
trix. In addition, to specify the position of each en-
tity pair, we also use position embeddings for all
words in the sentence.

Word Embeddings. Word embeddings aim to
transform words into distributed representations
which capture syntactic and semantic meanings
of the words. Given a sentence x consisting of
m words x = {w1, w2, · · · , wm}, every word
wi is represented by a real-valued vector. Word
representations are encoded by column vectors in
an embedding matrix V ∈ Rda×|V |where V is a
ﬁxed-sized vocabulary.

Position Embeddings. In the task of relation
extraction, the words close to the target entities are
usually informative to determine the relation be-
tween entities. Similar to (Zeng et al., 2014), we

use position embeddings speciﬁed by entity pairs.
It can help the CNN to keep track of how close
each word is to head or tail entities. It is deﬁned
as the combination of the relative distances from
the current word to head or tail entities. For ex-
ample, in the sentence “Bill Gates is the founder
of Microsoft.”, the relative distance from the word
“founder” to head entity Bill Gates is 3 and tail
entity Microsoft is 2.

In the example shown in Fig. 2, it is assumed
that the dimension da of the word embedding is 3
and the dimension db of the position embedding is
1. Finally, we concatenate the word embeddings
and position embeddings of all words and denote
it as a vector sequence w = {w1, w2, · · · , wm},
where wi ∈ Rd(d = da + db × 2).

3.1.2 Convolution, Max-pooling and

Non-linear Layers

In relation extraction, the main challenges are
that the length of the sentences is variable and the
important information can appear in any area of
the sentences. Hence, we should utilize all lo-
cal features and perform relation prediction glob-
ally. Here, we use a convolutional layer to merge
all these features. The convolutional layer ﬁrst
extracts local features with a sliding window of
length l over the sentence. In the example shown
in Fig. 2, we assume that the length of the sliding
window l is 3. Then, it combines all local features
via a max-pooling operation to obtain a ﬁxed-sized
vector for the input sentence.

Here, convolution is deﬁned as an operation be-
tween a vector sequence w and a convolution ma-
trix W ∈ Rdc×(l×d), where dc is the sentence em-
bedding size. Let us deﬁne the vector qi ∈ Rl×d
as the concatenation of a sequence of w word em-
beddings within the i-th window:

qi = wi−l+1:i

(1 ≤ i ≤ m + l − 1).

(1)

Since the window may be outside of the sen-
tence boundaries when it slides near the boundary,
we set special padding tokens for the sentence. It
means that we regard all out-of-range input vec-
tors wi(i < 1 or i > m) as zero vector.

Hence, the i-th ﬁlter of convolutional layer is

computed as:

where b is bias vector. And the i-th element of the
vector x ∈ Rdc

as follows:

pi = [Wq + b]i

[x]i = max(pi),

(2)

(3)

Further, PCNN (Zeng et al., 2015), which is a
variation of CNN, adopts piecewise max pooling
in relation extraction. Each convolutional ﬁlter pi
is divided into three segments (pi1, pi2, pi3) by
head and tail entities. And the max pooling pro-
cedure is performed in three segments separately,
which is deﬁned as:

[x]ij = max(pij),

(4)

And [x]i is set as the concatenation of [x]ij.

Finally, we apply a non-linear function at the

output, such as the hyperbolic tangent.

3.2 Selective Attention over Instances

Suppose there is a set S contains n sen-
i.e., S =

tences for entity pair (head, tail),
{x1, x2, · · · , xn}.

To exploit the information of all sentences, our
model represents the set S with a real-valued vec-
tor s when predicting relation r. It is straightfor-
ward that the representation of the set S depends
on all sentences’ representations x1, x2, · · · , xn.
Each sentence representation xi contains informa-
tion about whether entity pair (head, tail) con-
tains relation r for input sentence xi.

The set vector s is,

then, computed as a

weighted sum of these sentence vector xi:

where αi is the weight of each sentence vector xi.
In this paper, we deﬁne αi in two ways:

Average: We assume that all sentences in the
set X have the same contribution to the represen-
tation of the set. It means the embedding of the set
S is the average of all the sentence vectors:

(cid:88)

s =

αixi,

i

(cid:88)

s =

1
n

xi,

i

(5)

(6)

It’s a naive baseline of our selective attention.

Selective Attention: However, the wrong la-
belling problem inevitably occurs. Thus, if we
regard each sentence equally, the wrong labelling
sentences will bring in massive of noise during
training and testing. Hence, we use a selec-
tive attention to de-emphasize the noisy sentence.
Hence, αi is further deﬁned as:

αi =

exp(ei)
k exp(ek)

,

(cid:80)

(7)

where ei is referred as a query-based function
which scores how well the input sentence xi and
the predict relation r matches. We select the bilin-
ear form which achieves best performance in dif-
ferent alternatives:

ei = xiAr,

(8)

where A is a weighted diagonal matrix, and r is
the query vector associated with relation r which
indicates the representation of relation r.

Finally, we deﬁne the conditional probability

p(r|S, θ) through a softmax layer as follows:

p(r|S, θ) =

exp(or)
k=1 exp(ok)

,

(cid:80)nr

(9)

where nr is the total number of relations and o is
the ﬁnal output of the neural network which cor-
responds to the scores associated to all relation
types, which is deﬁned as follows:

o = Ms + d,

(10)

where d ∈ Rnr is a bias vector and M is the rep-
resentation matrix of relations.

(Zeng et al., 2015) follows the assumption that
at least one mention of the entity pair will reﬂect
their relation, and only uses the sentence with the
highest probability in each set for training. Hence,
the method which they adopted for multi-instance
learning can be regarded as a special case as our
selective attention when the weight of the sentence
with the highest probability is set to 1 and others
to 0.

3.3 Optimization and Implementation Details

Here we introduce the learning and optimiza-
tion details of our model. We deﬁne the objective
function using cross-entropy at the set level as fol-
lows:

J(θ) =

log p(ri|Si, θ),

(11)

s
(cid:88)

i=1

where s indicates the number of sentence sets and
θ indicates all parameters of our model. To solve
the optimization problem, we adopt stochastic gra-
dient descent (SGD) to minimize the objective
function. For learning, we iterate by randomly
selecting a mini-batch from the training set until
converge.

In the implementation, we employ dropout (Sri-
vastava et al., 2014) on the output layer to pre-
vent overﬁtting. The dropout layer is deﬁned as

an element-wise multiplication with a a vector h
of Bernoulli random variables with probability p.
Then equation (10) is rewritten as:

o = M(s ◦ h) + d.

(12)

In the test phase, the learnt set representations
are scaled by p, i.e., ˆsi = psi. And the scaled set
vector ˆri is ﬁnally used to predict relations.

4 Experiments

Our experiments are intended to demonstrate
that our neural models with sentence-level selec-
tive attention can alleviate the wrong labelling
problem and take full advantage of informative
sentences for distant supervised relation extrac-
tion. To this end, we ﬁrst introduce the dataset and
evaluation metrics used in the experiments. Next,
we use cross-validation to determine the parame-
ters of our model. And then we evaluate the ef-
fects of our selective attention and show its per-
formance on the data with different set size. Fi-
nally, we compare the performance of our method
to several state-of-the-art feature-based methods.

4.1 Dataset and Evaluation Metrics

We evaluate our model on a widely used
dataset1 which is developed by (Riedel et al.,
2010) and has also been used by (Hoffmann et
al., 2011; Surdeanu et al., 2012). This dataset was
generated by aligning Freebase relations with the
New York Times corpus (NYT). Entity mentions
are found using the Stanford named entity tagger
(Finkel et al., 2005), and are further matched to the
names of Freebase entities. The Freebase relations
are divided into two parts, one for training and one
for testing.
It aligns the the sentences from the
corpus of the years 2005-2006 and regards them
as training instances. And the testing instances
are the aligned sentences from 2007. There are
53 possible relationships including a special rela-
tion NA which indicates there is no relation be-
tween head and tail entities. The training data con-
tains 522,611 sentences, 281,270 entity pairs and
18,252 relational facts. The testing set contains
172,448 sentences, 96,678 entity pairs and 1,950
relational facts.

Similar to previous work (Mintz et al., 2009),
we evaluate our model in the held-out evaluation.
It evaluates our model by comparing the relation

1http://iesl.cs.umass.edu/riedel/ecml/

facts discovered from the test articles with those
It assumes that the testing systems
in Freebase.
have similar performances in relation facts inside
and outside Freebase. Hence, the held-out evalua-
tion provides an approximate measure of precision
without time consumed human evaluation. We
report both the aggregate curves precision/recall
curves and Precision@N (P@N) in our experi-
ments.

et al., 2015) as our sentence encoders and imple-
ment them by ourselves which achieve compara-
ble results as the authors reported. And we com-
pare the performance of the two different kinds
of CNN with sentence-level attention (ATT) , its
naive version (AVE) which represents each sen-
tence set as the average vector of sentences inside
the set and the at-least-one multi-instance learning
(ONE) used in (Zeng et al., 2015).

4.2 Experimental Settings

4.2.1 Word Embeddings

In this paper, we use the word2vec tool 2 to train
the word embeddings on NYT corpus. We keep
the words which appear more than 100 times in
the corpus as vocabulary. Besides, we concatenate
the words of an entity when it has multiple words.

4.2.2 Parameter Settings

Following previous work, we tune our mod-
els using three-fold validation on the training set.
We use a grid search to determine the optimal
parameters and select learning rate λ for SGD
among {0.1, 0.01, 0.001, 0.0001}, the sliding win-
dow size l ∈ {1, 2, 3, · · · , 8}, the sentence embed-
ding size n ∈ {50, 60, · · · , 300}, and the batch
size B among {40, 160, 640, 1280}. For other pa-
rameters, since they have little effect on the results,
we follow the settings used in (Zeng et al., 2014).
For training, we set the iteration number over all
the training data as 25.
In Table 1 we show all
parameters used in the experiments.

Table 1: Parameter settings

Window size l
Sentence embedding size dc
Word dimension da
Position dimension db
Batch size B
Learning rate λ
Dropout probability p

3
230
50
5
160
0.01
0.5

4.3 Effect of Sentence-level Selective

Attention

To demonstrate the effects of the sentence-level
selective attention, we empirically compare dif-
ferent methods through held-out evaluation. We
select the CNN model proposed in (Zeng et al.,
2014) and the PCNN model proposed in (Zeng

2https://code.google.com/p/word2vec/

Figure 3: Top: Aggregate precion/recall curves of
CNN, CNN+ONE, CNN+AVE, CNN+ATT. Bot-
tom: Aggregate precion/recall curves of PCNN,
PCNN+ONE, PCNN+AVE, PCNN+ATT

From Fig. 3, we have the following observa-
(1) For both CNN and PCNN, the ONE
tion:
method brings better performance as compared to
CNN/PCNN. The reason is that the original distant
supervision training data contains a lot of noise
and the noisy data will damage the performance of
relation extraction. (2) For both CNN and PCNN,
the AVE method is useful for relation extraction
as compared to CNN/PCNN. It indicates that con-
sidering more sentences is beneﬁcial to relation
extraction since the noise can be reduced by mu-
tual complementation of information. (3) For both

CNN and PCNN, the AVE method has a similar
performance compared to the ONE method. It in-
dicates that, although the AVE method brings in
information of more sentences, since it regards
each sentence equally, it also brings in the noise
from the wrong labelling sentences which may
hurt the performance of relation extraction. (4) For
both CNN and PCNN, the ATT method achieves
the highest precision over the entire range of re-
call compared to other methods including the AVE
method. It indicates that the proposed selective at-
tention is beneﬁcial.
It can effectively ﬁlter out
meaningless sentences and alleviate the wrong la-
belling problem in distant supervised relation ex-
traction.

4.4 Effect of Sentence Number

In the original testing data set, there are 74,857
entity pairs that correspond to only one sen-
Since
tence, nearly 3/4 over all entity pairs.
the superiority of our selective attention lies in
the entity pairs containing multiple sentences, we
compare the performance of CNN/PCNN+ONE,
CNN/PCNN+AVE and CNN/PCNN+ATT on the
entity pairs which have more than one sentence.
And then we examine these three methods in three
test settings:

• One: For each testing entity pair, we ran-
domly select one sentence and use this sen-
tence to predict relation.

• Two: For each testing entity pair, we ran-
domly select two sentences and proceed re-
lation extraction.

• All: We use all sentences of each entity pair

for relation extraction.

Note that, we use all the sentences in training. We
will report the P@100, P@200, P@300 and the
mean of them for each model in held-out evalua-
tion.

Table 2 shows the P@N for compared models in
three test settings. From the table, we can see that:
(1) For both CNN and PCNN, the ATT method
achieves the best performance in all test settings.
It demonstrates the effectiveness of sentence-level
selective attention for multi-instance learning. (2)
For both CNN and PCNN, the AVE method is
comparable to the ATT method in the One test set-
ting. However, when the number of testing sen-
tences per entity pair grows, the performance of

the AVE methods has almost no improvement. It
even drops gradually in P@100, P@200 as the
sentence number increases. The reason is that,
since we regard each sentence equally, the noise
contained in the sentences that do not express any
relation will have negative inﬂuence in the perfor-
mance of relation extraction. (3) CNN+AVE and
CNN+ATT have 5% to 8% improvements com-
pared to CNN+ONE in the ONE test setting. Since
each entity pair has only one sentence in this test
setting, the only difference of these methods is
from training. Hence, it shows that utilizing all
sentences will bring in more information although
it may also bring in some extra noises.
(4) For
both CNN and PCNN, the ATT method outper-
forms other two baselines over 5% and 9% in the
Two and All test settings. It indicates that by tak-
ing more useful information into account, the re-
lational facts which CNN+ATT ranks higher are
more reliable and beneﬁcial to relation extraction.

4.5 Comparison with Feature-based

Approaches

Figure 4: Performance comparison of proposed
model and traditional methods

To evaluate the proposed method, we select the
following three feature-based methods for com-
parison through held-out evaluation:

Mintz (Mintz et al., 2009) is a traditional distant

supervised model.

MultiR (Hoffmann et al., 2011) proposes a
probabilistic, graphical model of multi-instance
learning which handles overlapping relations.

MIML (Surdeanu et al., 2012) jointly models

both multiple instances and multiple relations.

We implement them with the source codes re-

leased by the authors.

Table 2: P@N for relation extraction in the entity pairs with different number of sentences

Test Settings
100
P@N(%)
68.3
CNN+ONE
75.2
+AVE
76.2
+ATT
PCNN+ONE 73.3
71.3
+AVE
+ATT
73.3

One

200
60.7
67.2
65.2
64.8
63.7
69.2

300 Mean
60.9
53.8
67.1
58.8
60.8
67.4
65.0
56.8
64.3
57.8
67.8
60.8

Two

All

100
70.3
68.3
76.2
70.3
73.3
77.2

200
62.7
63.2
65.7
67.2
65.2
71.6

300 Mean
62.9
55.8
64.0
60.5
68.0
62.1
66.9
63.1
66.9
62.1
71.6
66.1

100
67.3
64.4
76.2
72.3
73.3
76.2

200
64.7
60.2
68.6
69.7
66.7
73.1

300 Mean
63.4
58.1
60.4
60.1
68.2
59.8
68.7
64.1
67.6
62.8
72.2
67.4

Fig.

4 shows the precision/recall curves
for each method. We can observe that:
(1)
CNN/PCNN+ATT signiﬁcantly outperforms all
feature-based methods over the entire range of re-
call. When the recall is greater than 0.1, the perfor-
mance of feature-based method drop out quickly.
In contrast, our model has a reasonable preci-
sion until the recall approximately reaches 0.3.
It demonstrates that the human-designed feature
cannot concisely express the semantic meaning of
the sentences, and the inevitable error brought by
NLP tools will hurt the performance of relation
extraction. In contrast, CNN/PCNN+ATT which
learns the representation of each sentences auto-
matically can express each sentence well.
(2)
PCNN+ATT performs much better as compared
to CNN+ATT over the entire range of recall.
It
means that the selective attention considers the
global information of all sentences except the in-
formation inside each sentence. Hence, the perfor-
mance of our model can be further improved if we
have a better sentence encoder.

4.6 Case Study

Table 3 shows two examples of selective at-
tention from the testing data. For each relation,
we show the corresponding sentences with high-
est and lowest attention weight respectively. And
we highlight the entity pairs with bold formatting.

From the table we ﬁnd that: The former exam-
ple is related to the relation employer of. The
sentence with low attention weight does not ex-
press the relation between two entities, while the
high one shows that Mel Karmazin is the chief ex-
ecutive of Sirius Satellite Radio. The later exam-
ple is related to the relation place of birth.
The sentence with low attention weight expresses
where Ernst Haeﬂiger is died in, while the high
one expresses where he is born in.

Table 3: Some examples of selective attention in
NYT corpus

Relation

employer of

Low When Howard Stern was prepar-
ing to take his talk show to Sirius
Satellite Radio, following his for-
mer boss, Mel Karmazin, Mr. Hol-
lander argued that ...

Relation
Low

High Mel Karmazin, the chief executive
of Sirius Satellite Radio, made a
lot of phone calls ...
place of birth
Ernst Haeﬂiger, a Swiss tenor
who ...
roles , died on Saturday
in Davos, Switzerland, where he
maintained a second home.
Ernst Haeﬂiger was born in Davos
on July 6, 1919, and studied at the
Wettinger Seminary ...

High

5 Conclusion and Future Works

In this paper, we develop CNN with sentence-
level selective attention. Our model can make full
use of all informative sentences and alleviate the
wrong labelling problem for distant supervised re-
lation extraction. In experiments, we evaluate our
model on relation extraction task. The experimen-
tal results show that our model signiﬁcantly and
consistently outperforms state-of-the-art feature-
based methods and neural network methods.

In the future, we will explore the following di-

rections:

• Our model incorporates multi-instance learn-
ing with neural network via instance-level se-
lective attention. It can be used in not only
distant supervised relation extraction but also
other multi-instance learning tasks. We will
explore our model in other area such as text

categorization.

• CNN is one of the effective neural net-
works for neural relation extraction. Re-
searchers also propose many other neural net-
In the
work models for relation extraction.
future, we will incorporate our instance-level
selective attention technique with those mod-
els for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61303075) and the Tsinghua University Initiative
Scientiﬁc Research Program (20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
In Proceedings of ACL, volume 45,
supervision.
page 576.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In Pro-
ceedings of NIPS, pages 2204–2212.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with compo-
In Proceedings of ACL.
sitional vector grammars.
Citeseer.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
nn: ﬁrst results. arXiv preprint arXiv:1412.1602.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artiﬁcial in-
telligence, 89(1):31–71.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image
caption generation with visual attention. Proceed-
ings of ICML.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

Neural Relation Extraction with Selective Attention over Instances

Yankai Lin1, Shiqi Shen1, Zhiyuan Liu1,2∗, Huanbo Luan1, Maosong Sun1,2
1 Department of Computer Science and Technology,
State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China

Abstract

Distant supervised relation extraction has
been widely used to ﬁnd novel relational
facts from text. However, distant su-
pervision inevitably accompanies with the
wrong labelling problem, and these noisy
data will substantially hurt
the perfor-
mance of relation extraction. To allevi-
ate this issue, we propose a sentence-level
attention-based model for relation extrac-
tion. In this model, we employ convolu-
tional neural networks to embed the se-
mantics of sentences. Afterwards, we
build sentence-level attention over multi-
ple instances, which is expected to dy-
namically reduce the weights of those
noisy instances. Experimental results on
real-world datasets show that, our model
can make full use of all informative sen-
tences and effectively reduce the inﬂuence
of wrong labelled instances. Our model
im-
achieves signiﬁcant and consistent
provements on relation extraction as com-
pared with baselines. The source code of
this paper can be obtained from https:
//github.com/thunlp/NRE.

1

Introduction

In recent years, various large-scale knowledge
bases (KBs) such as Freebase (Bollacker et al.,
2008), DBpedia (Auer et al., 2007) and YAGO
(Suchanek et al., 2007) have been built and widely
used in many natural language processing (NLP)
tasks, including web search and question answer-
ing. These KBs mostly compose of relational facts
with triple format, e.g., (Microsoft, founder,
Bill Gates). Although existing KBs contain a

∗ Corresponding
uzy@tsinghua.edu.cn).

author:

Zhiyuan

Liu

(li-

massive amount of facts, they are still far from
complete compared to the inﬁnite real-world facts.
To enrich KBs, many efforts have been invested
in automatically ﬁnding unknown relational facts.
Therefore, relation extraction (RE), the process of
generating relational data from plain text, is a cru-
cial task in NLP.

Most existing supervised RE systems require a
large amount of labelled relation-speciﬁc training
data, which is very time consuming and labor in-
tensive. (Mintz et al., 2009) proposes distant su-
pervision to automatically generate training data
via aligning KBs and texts. They assume that if
two entities have a relation in KBs, then all sen-
tences that contain these two entities will express
this relation. For example, (Microsoft, founder,
Bill Gates) is a relational fact in KB. Distant su-
pervision will regard all sentences that contain
these two entities as active instances for relation
founder. Although distant supervision is an
effective strategy to automatically label training
data, it always suffers from wrong labelling prob-
lem. For example, the sentence “Bill Gates ’s turn
to philanthropy was linked to the antitrust prob-
lems Microsoft had in the U.S. and the European
union.” does not express the relation founder
but will still be regarded as an active instance.
Hence, (Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012) adopt multi-instance learn-
ing to alleviate the wrong labelling problem. The
main weakness of these conventional methods is
that most features are explicitly derived from NLP
tools such as POS tagging and the errors generated
by NLP tools will propagate in these methods.

Some recent works (Socher et al., 2012; Zeng
et al., 2014; dos Santos et al., 2015) attempt to
use deep neural networks in relation classiﬁca-
tion without handcrafted features. These meth-
ods build classiﬁer based on sentence-level anno-
tated data, which cannot be applied in large-scale

rized as follows:

• As compared to existing neural relation ex-
traction model, our model can make full use
of all informative sentences of each entity
pair.

• To address the wrong labelling problem in
distant supervision, we propose selective
attention to de-emphasize those noisy in-
stances.

• In the experiments, we show that selective
attention is beneﬁcial to two kinds of CNN
models in the task of relation extraction.

2 Related Work

Relation extraction is one of the most impor-
tant tasks in NLP. Many efforts have been invested
in relation extraction, especially in supervised re-
lation extraction. Most of these methods need a
great deal of annotated data, which is time con-
suming and labor intensive. To address this issue,
(Mintz et al., 2009) aligns plain text with Free-
base by distant supervision. However, distant su-
pervision inevitably accompanies with the wrong
labelling problem. To alleviate the wrong la-
belling problem, (Riedel et al., 2010) models dis-
tant supervision for relation extraction as a multi-
instance single-label problem, and (Hoffmann et
al., 2011; Surdeanu et al., 2012) adopt multi-
instance multi-label learning in relation extraction.
Multi-instance learning was originally proposed to
address the issue of ambiguously-labelled training
data when predicting the activity of drugs (Diet-
terich et al., 1997). Multi-instance learning con-
siders the reliability of the labels for each instance.
(Bunescu and Mooney, 2007) connects weak su-
pervision with multi-instance learning and extends
it to relation extraction. But all the feature-based
methods depend strongly on the quality of the fea-
tures generated by NLP tools, which will suffer
from error propagation problem.

Recently, deep learning (Bengio, 2009) has
been widely used for various areas, including com-
puter vision, speech recognition and so on. It has
also been successfully applied to different NLP
tasks such as part-of-speech tagging (Collobert
et al., 2011), sentiment analysis (dos Santos and
Gatti, 2014), parsing (Socher et al., 2013), and
machine translation (Sutskever et al., 2014). Due
to the recent success in deep learning, many re-

Figure 1: The architecture of sentence-level
attention-based CNN, where xi and xi indicate the
original sentence for an entity pair and its corre-
sponding sentence representation, αi is the weight
given by sentence-level attention, and s indicates
the representation of the sentence set.

KBs due to the lack of human-annotated train-
ing data. Therefore, (Zeng et al., 2015) incor-
porates multi-instance learning with neural net-
work model, which can build relation extractor
based on distant supervision data. Although the
method achieves signiﬁcant improvement in re-
lation extraction, it is still far from satisfactory.
The method assumes that at least one sentence that
mentions these two entities will express their rela-
tion, and only selects the most likely sentence for
each entity pair in training and prediction. It’s ap-
parent that the method will lose a large amount
of rich information containing in neglected sen-
tences.

In this paper, we propose a sentence-level
attention-based convolutional neural network
(CNN) for distant supervised relation extraction.
As illustrated in Fig. 1, we employ a CNN to
embed the semantics of sentences. Afterwards, to
utilize all informative sentences, we represent the
relation as semantic composition of sentence em-
beddings. To address the wrong labelling prob-
lem, we build sentence-level attention over mul-
tiple instances, which is expected to dynamically
reduce the weights of those noisy instances. Fi-
nally, we extract relation with the relation vector
weighted by sentence-level attention. We evaluate
our model on a real-world dataset in the task of
relation extraction. The experimental results show
that our model achieves signiﬁcant and consistent
improvements in relation extraction as compared
with the state-of-the-art methods.

The contributions of this paper can be summa-

searchers have investigated the possibility of us-
ing neural networks to automatically learn features
for relation extraction. (Socher et al., 2012) uses
a recursive neural network in relation extraction.
They parse the sentences ﬁrst and then represent
each node in the parsing tree as a vector. More-
over, (Zeng et al., 2014; dos Santos et al., 2015)
adopt an end-to-end convolutional neural network
for relation extraction. Besides, (Xie et al., 2016)
attempts to incorporate the text information of en-
tities for relation extraction.

Although these methods achieve great success,
they still extract relations on sentence-level and
suffer from a lack of sufﬁcient training data.
In
addition, the multi-instance learning strategy of
conventional methods cannot be easily applied in
neural network models. Therefore, (Zeng et al.,
2015) combines at-least-one multi-instance learn-
ing with neural network model to extract relations
on distant supervision data. However, they assume
that only one sentence is active for each entity pair.
Hence, it will lose a large amount of rich informa-
tion containing in those neglected sentences. Dif-
ferent from their methods, we propose sentence-
level attention over multiple instances, which can
utilize all informative sentences.

The attention-based models have attracted a lot
of interests of researchers recently. The selectiv-
ity of attention-based models allows them to learn
alignments between different modalities.
It has
been applied to various areas such as image clas-
siﬁcation (Mnih et al., 2014), speech recognition
(Chorowski et al., 2014), image caption generation
(Xu et al., 2015) and machine translation (Bah-
danau et al., 2014). To the best of our knowl-
edge, this is the ﬁrst effort to adopt attention-based
model in distant supervised relation extraction.

3 Methodology

Given a set of sentences {x1, x2, · · · , xn} and
two corresponding entities, our model measures
the probability of each relation r. In this section,
we will introduce our model in two main parts:

• Sentence Encoder. Given a sentence x and
two target entities, a convolutional neutral
network (CNN) is used to construct a dis-
tributed representation x of the sentence.

• Selective Attention over Instances. When
the distributed vector representations of all
sentences are learnt, we use sentence-level at-

tention to select the sentences which really
express the corresponding relation.

3.1 Sentence Encoder

Figure 2: The architecture of CNN/PCNN used for
sentence encoder.

As shown in Fig. 2, we transform the sentence
x into its distributed representation x by a CNN.
First, words in the sentence are transformed into
dense real-valued feature vectors. Next, convo-
lutional layer, max-pooling layer and non-linear
transformation layer are used to construct a dis-
tributed representation of the sentence, i.e., x.

3.1.1

Input Representation

The inputs of the CNN are raw words of the
sentence x. We ﬁrst transform words into low-
dimensional vectors. Here, each input word is
transformed into a vector via word embedding ma-
trix. In addition, to specify the position of each en-
tity pair, we also use position embeddings for all
words in the sentence.

Word Embeddings. Word embeddings aim to
transform words into distributed representations
which capture syntactic and semantic meanings
of the words. Given a sentence x consisting of
m words x = {w1, w2, · · · , wm}, every word
wi is represented by a real-valued vector. Word
representations are encoded by column vectors in
an embedding matrix V ∈ Rda×|V |where V is a
ﬁxed-sized vocabulary.

Position Embeddings. In the task of relation
extraction, the words close to the target entities are
usually informative to determine the relation be-
tween entities. Similar to (Zeng et al., 2014), we

use position embeddings speciﬁed by entity pairs.
It can help the CNN to keep track of how close
each word is to head or tail entities. It is deﬁned
as the combination of the relative distances from
the current word to head or tail entities. For ex-
ample, in the sentence “Bill Gates is the founder
of Microsoft.”, the relative distance from the word
“founder” to head entity Bill Gates is 3 and tail
entity Microsoft is 2.

In the example shown in Fig. 2, it is assumed
that the dimension da of the word embedding is 3
and the dimension db of the position embedding is
1. Finally, we concatenate the word embeddings
and position embeddings of all words and denote
it as a vector sequence w = {w1, w2, · · · , wm},
where wi ∈ Rd(d = da + db × 2).

3.1.2 Convolution, Max-pooling and

Non-linear Layers

In relation extraction, the main challenges are
that the length of the sentences is variable and the
important information can appear in any area of
the sentences. Hence, we should utilize all lo-
cal features and perform relation prediction glob-
ally. Here, we use a convolutional layer to merge
all these features. The convolutional layer ﬁrst
extracts local features with a sliding window of
length l over the sentence. In the example shown
in Fig. 2, we assume that the length of the sliding
window l is 3. Then, it combines all local features
via a max-pooling operation to obtain a ﬁxed-sized
vector for the input sentence.

Here, convolution is deﬁned as an operation be-
tween a vector sequence w and a convolution ma-
trix W ∈ Rdc×(l×d), where dc is the sentence em-
bedding size. Let us deﬁne the vector qi ∈ Rl×d
as the concatenation of a sequence of w word em-
beddings within the i-th window:

qi = wi−l+1:i

(1 ≤ i ≤ m + l − 1).

(1)

Since the window may be outside of the sen-
tence boundaries when it slides near the boundary,
we set special padding tokens for the sentence. It
means that we regard all out-of-range input vec-
tors wi(i < 1 or i > m) as zero vector.

Hence, the i-th ﬁlter of convolutional layer is

computed as:

where b is bias vector. And the i-th element of the
vector x ∈ Rdc

as follows:

pi = [Wq + b]i

[x]i = max(pi),

(2)

(3)

Further, PCNN (Zeng et al., 2015), which is a
variation of CNN, adopts piecewise max pooling
in relation extraction. Each convolutional ﬁlter pi
is divided into three segments (pi1, pi2, pi3) by
head and tail entities. And the max pooling pro-
cedure is performed in three segments separately,
which is deﬁned as:

[x]ij = max(pij),

(4)

And [x]i is set as the concatenation of [x]ij.

Finally, we apply a non-linear function at the

output, such as the hyperbolic tangent.

3.2 Selective Attention over Instances

Suppose there is a set S contains n sen-
i.e., S =

tences for entity pair (head, tail),
{x1, x2, · · · , xn}.

To exploit the information of all sentences, our
model represents the set S with a real-valued vec-
tor s when predicting relation r. It is straightfor-
ward that the representation of the set S depends
on all sentences’ representations x1, x2, · · · , xn.
Each sentence representation xi contains informa-
tion about whether entity pair (head, tail) con-
tains relation r for input sentence xi.

The set vector s is,

then, computed as a

weighted sum of these sentence vector xi:

where αi is the weight of each sentence vector xi.
In this paper, we deﬁne αi in two ways:

Average: We assume that all sentences in the
set X have the same contribution to the represen-
tation of the set. It means the embedding of the set
S is the average of all the sentence vectors:

(cid:88)

s =

αixi,

i

(cid:88)

s =

1
n

xi,

i

(5)

(6)

It’s a naive baseline of our selective attention.

Selective Attention: However, the wrong la-
belling problem inevitably occurs. Thus, if we
regard each sentence equally, the wrong labelling
sentences will bring in massive of noise during
training and testing. Hence, we use a selec-
tive attention to de-emphasize the noisy sentence.
Hence, αi is further deﬁned as:

αi =

exp(ei)
k exp(ek)

,

(cid:80)

(7)

where ei is referred as a query-based function
which scores how well the input sentence xi and
the predict relation r matches. We select the bilin-
ear form which achieves best performance in dif-
ferent alternatives:

ei = xiAr,

(8)

where A is a weighted diagonal matrix, and r is
the query vector associated with relation r which
indicates the representation of relation r.

Finally, we deﬁne the conditional probability

p(r|S, θ) through a softmax layer as follows:

p(r|S, θ) =

exp(or)
k=1 exp(ok)

,

(cid:80)nr

(9)

where nr is the total number of relations and o is
the ﬁnal output of the neural network which cor-
responds to the scores associated to all relation
types, which is deﬁned as follows:

o = Ms + d,

(10)

where d ∈ Rnr is a bias vector and M is the rep-
resentation matrix of relations.

(Zeng et al., 2015) follows the assumption that
at least one mention of the entity pair will reﬂect
their relation, and only uses the sentence with the
highest probability in each set for training. Hence,
the method which they adopted for multi-instance
learning can be regarded as a special case as our
selective attention when the weight of the sentence
with the highest probability is set to 1 and others
to 0.

3.3 Optimization and Implementation Details

Here we introduce the learning and optimiza-
tion details of our model. We deﬁne the objective
function using cross-entropy at the set level as fol-
lows:

J(θ) =

log p(ri|Si, θ),

(11)

s
(cid:88)

i=1

where s indicates the number of sentence sets and
θ indicates all parameters of our model. To solve
the optimization problem, we adopt stochastic gra-
dient descent (SGD) to minimize the objective
function. For learning, we iterate by randomly
selecting a mini-batch from the training set until
converge.

In the implementation, we employ dropout (Sri-
vastava et al., 2014) on the output layer to pre-
vent overﬁtting. The dropout layer is deﬁned as

an element-wise multiplication with a a vector h
of Bernoulli random variables with probability p.
Then equation (10) is rewritten as:

o = M(s ◦ h) + d.

(12)

In the test phase, the learnt set representations
are scaled by p, i.e., ˆsi = psi. And the scaled set
vector ˆri is ﬁnally used to predict relations.

4 Experiments

Our experiments are intended to demonstrate
that our neural models with sentence-level selec-
tive attention can alleviate the wrong labelling
problem and take full advantage of informative
sentences for distant supervised relation extrac-
tion. To this end, we ﬁrst introduce the dataset and
evaluation metrics used in the experiments. Next,
we use cross-validation to determine the parame-
ters of our model. And then we evaluate the ef-
fects of our selective attention and show its per-
formance on the data with different set size. Fi-
nally, we compare the performance of our method
to several state-of-the-art feature-based methods.

4.1 Dataset and Evaluation Metrics

We evaluate our model on a widely used
dataset1 which is developed by (Riedel et al.,
2010) and has also been used by (Hoffmann et
al., 2011; Surdeanu et al., 2012). This dataset was
generated by aligning Freebase relations with the
New York Times corpus (NYT). Entity mentions
are found using the Stanford named entity tagger
(Finkel et al., 2005), and are further matched to the
names of Freebase entities. The Freebase relations
are divided into two parts, one for training and one
for testing.
It aligns the the sentences from the
corpus of the years 2005-2006 and regards them
as training instances. And the testing instances
are the aligned sentences from 2007. There are
53 possible relationships including a special rela-
tion NA which indicates there is no relation be-
tween head and tail entities. The training data con-
tains 522,611 sentences, 281,270 entity pairs and
18,252 relational facts. The testing set contains
172,448 sentences, 96,678 entity pairs and 1,950
relational facts.

Similar to previous work (Mintz et al., 2009),
we evaluate our model in the held-out evaluation.
It evaluates our model by comparing the relation

1http://iesl.cs.umass.edu/riedel/ecml/

facts discovered from the test articles with those
It assumes that the testing systems
in Freebase.
have similar performances in relation facts inside
and outside Freebase. Hence, the held-out evalua-
tion provides an approximate measure of precision
without time consumed human evaluation. We
report both the aggregate curves precision/recall
curves and Precision@N (P@N) in our experi-
ments.

et al., 2015) as our sentence encoders and imple-
ment them by ourselves which achieve compara-
ble results as the authors reported. And we com-
pare the performance of the two different kinds
of CNN with sentence-level attention (ATT) , its
naive version (AVE) which represents each sen-
tence set as the average vector of sentences inside
the set and the at-least-one multi-instance learning
(ONE) used in (Zeng et al., 2015).

4.2 Experimental Settings

4.2.1 Word Embeddings

In this paper, we use the word2vec tool 2 to train
the word embeddings on NYT corpus. We keep
the words which appear more than 100 times in
the corpus as vocabulary. Besides, we concatenate
the words of an entity when it has multiple words.

4.2.2 Parameter Settings

Following previous work, we tune our mod-
els using three-fold validation on the training set.
We use a grid search to determine the optimal
parameters and select learning rate λ for SGD
among {0.1, 0.01, 0.001, 0.0001}, the sliding win-
dow size l ∈ {1, 2, 3, · · · , 8}, the sentence embed-
ding size n ∈ {50, 60, · · · , 300}, and the batch
size B among {40, 160, 640, 1280}. For other pa-
rameters, since they have little effect on the results,
we follow the settings used in (Zeng et al., 2014).
For training, we set the iteration number over all
the training data as 25.
In Table 1 we show all
parameters used in the experiments.

Table 1: Parameter settings

Window size l
Sentence embedding size dc
Word dimension da
Position dimension db
Batch size B
Learning rate λ
Dropout probability p

3
230
50
5
160
0.01
0.5

4.3 Effect of Sentence-level Selective

Attention

To demonstrate the effects of the sentence-level
selective attention, we empirically compare dif-
ferent methods through held-out evaluation. We
select the CNN model proposed in (Zeng et al.,
2014) and the PCNN model proposed in (Zeng

2https://code.google.com/p/word2vec/

Figure 3: Top: Aggregate precion/recall curves of
CNN, CNN+ONE, CNN+AVE, CNN+ATT. Bot-
tom: Aggregate precion/recall curves of PCNN,
PCNN+ONE, PCNN+AVE, PCNN+ATT

From Fig. 3, we have the following observa-
(1) For both CNN and PCNN, the ONE
tion:
method brings better performance as compared to
CNN/PCNN. The reason is that the original distant
supervision training data contains a lot of noise
and the noisy data will damage the performance of
relation extraction. (2) For both CNN and PCNN,
the AVE method is useful for relation extraction
as compared to CNN/PCNN. It indicates that con-
sidering more sentences is beneﬁcial to relation
extraction since the noise can be reduced by mu-
tual complementation of information. (3) For both

CNN and PCNN, the AVE method has a similar
performance compared to the ONE method. It in-
dicates that, although the AVE method brings in
information of more sentences, since it regards
each sentence equally, it also brings in the noise
from the wrong labelling sentences which may
hurt the performance of relation extraction. (4) For
both CNN and PCNN, the ATT method achieves
the highest precision over the entire range of re-
call compared to other methods including the AVE
method. It indicates that the proposed selective at-
tention is beneﬁcial.
It can effectively ﬁlter out
meaningless sentences and alleviate the wrong la-
belling problem in distant supervised relation ex-
traction.

4.4 Effect of Sentence Number

In the original testing data set, there are 74,857
entity pairs that correspond to only one sen-
Since
tence, nearly 3/4 over all entity pairs.
the superiority of our selective attention lies in
the entity pairs containing multiple sentences, we
compare the performance of CNN/PCNN+ONE,
CNN/PCNN+AVE and CNN/PCNN+ATT on the
entity pairs which have more than one sentence.
And then we examine these three methods in three
test settings:

• One: For each testing entity pair, we ran-
domly select one sentence and use this sen-
tence to predict relation.

• Two: For each testing entity pair, we ran-
domly select two sentences and proceed re-
lation extraction.

• All: We use all sentences of each entity pair

for relation extraction.

Note that, we use all the sentences in training. We
will report the P@100, P@200, P@300 and the
mean of them for each model in held-out evalua-
tion.

Table 2 shows the P@N for compared models in
three test settings. From the table, we can see that:
(1) For both CNN and PCNN, the ATT method
achieves the best performance in all test settings.
It demonstrates the effectiveness of sentence-level
selective attention for multi-instance learning. (2)
For both CNN and PCNN, the AVE method is
comparable to the ATT method in the One test set-
ting. However, when the number of testing sen-
tences per entity pair grows, the performance of

the AVE methods has almost no improvement. It
even drops gradually in P@100, P@200 as the
sentence number increases. The reason is that,
since we regard each sentence equally, the noise
contained in the sentences that do not express any
relation will have negative inﬂuence in the perfor-
mance of relation extraction. (3) CNN+AVE and
CNN+ATT have 5% to 8% improvements com-
pared to CNN+ONE in the ONE test setting. Since
each entity pair has only one sentence in this test
setting, the only difference of these methods is
from training. Hence, it shows that utilizing all
sentences will bring in more information although
it may also bring in some extra noises.
(4) For
both CNN and PCNN, the ATT method outper-
forms other two baselines over 5% and 9% in the
Two and All test settings. It indicates that by tak-
ing more useful information into account, the re-
lational facts which CNN+ATT ranks higher are
more reliable and beneﬁcial to relation extraction.

4.5 Comparison with Feature-based

Approaches

Figure 4: Performance comparison of proposed
model and traditional methods

To evaluate the proposed method, we select the
following three feature-based methods for com-
parison through held-out evaluation:

Mintz (Mintz et al., 2009) is a traditional distant

supervised model.

MultiR (Hoffmann et al., 2011) proposes a
probabilistic, graphical model of multi-instance
learning which handles overlapping relations.

MIML (Surdeanu et al., 2012) jointly models

both multiple instances and multiple relations.

We implement them with the source codes re-

leased by the authors.

Table 2: P@N for relation extraction in the entity pairs with different number of sentences

Test Settings
100
P@N(%)
68.3
CNN+ONE
75.2
+AVE
76.2
+ATT
PCNN+ONE 73.3
71.3
+AVE
+ATT
73.3

One

200
60.7
67.2
65.2
64.8
63.7
69.2

300 Mean
60.9
53.8
67.1
58.8
60.8
67.4
65.0
56.8
64.3
57.8
67.8
60.8

Two

All

100
70.3
68.3
76.2
70.3
73.3
77.2

200
62.7
63.2
65.7
67.2
65.2
71.6

300 Mean
62.9
55.8
64.0
60.5
68.0
62.1
66.9
63.1
66.9
62.1
71.6
66.1

100
67.3
64.4
76.2
72.3
73.3
76.2

200
64.7
60.2
68.6
69.7
66.7
73.1

300 Mean
63.4
58.1
60.4
60.1
68.2
59.8
68.7
64.1
67.6
62.8
72.2
67.4

Fig.

4 shows the precision/recall curves
for each method. We can observe that:
(1)
CNN/PCNN+ATT signiﬁcantly outperforms all
feature-based methods over the entire range of re-
call. When the recall is greater than 0.1, the perfor-
mance of feature-based method drop out quickly.
In contrast, our model has a reasonable preci-
sion until the recall approximately reaches 0.3.
It demonstrates that the human-designed feature
cannot concisely express the semantic meaning of
the sentences, and the inevitable error brought by
NLP tools will hurt the performance of relation
extraction. In contrast, CNN/PCNN+ATT which
learns the representation of each sentences auto-
matically can express each sentence well.
(2)
PCNN+ATT performs much better as compared
to CNN+ATT over the entire range of recall.
It
means that the selective attention considers the
global information of all sentences except the in-
formation inside each sentence. Hence, the perfor-
mance of our model can be further improved if we
have a better sentence encoder.

4.6 Case Study

Table 3 shows two examples of selective at-
tention from the testing data. For each relation,
we show the corresponding sentences with high-
est and lowest attention weight respectively. And
we highlight the entity pairs with bold formatting.

From the table we ﬁnd that: The former exam-
ple is related to the relation employer of. The
sentence with low attention weight does not ex-
press the relation between two entities, while the
high one shows that Mel Karmazin is the chief ex-
ecutive of Sirius Satellite Radio. The later exam-
ple is related to the relation place of birth.
The sentence with low attention weight expresses
where Ernst Haeﬂiger is died in, while the high
one expresses where he is born in.

Table 3: Some examples of selective attention in
NYT corpus

Relation

employer of

Low When Howard Stern was prepar-
ing to take his talk show to Sirius
Satellite Radio, following his for-
mer boss, Mel Karmazin, Mr. Hol-
lander argued that ...

Relation
Low

High Mel Karmazin, the chief executive
of Sirius Satellite Radio, made a
lot of phone calls ...
place of birth
Ernst Haeﬂiger, a Swiss tenor
who ...
roles , died on Saturday
in Davos, Switzerland, where he
maintained a second home.
Ernst Haeﬂiger was born in Davos
on July 6, 1919, and studied at the
Wettinger Seminary ...

High

5 Conclusion and Future Works

In this paper, we develop CNN with sentence-
level selective attention. Our model can make full
use of all informative sentences and alleviate the
wrong labelling problem for distant supervised re-
lation extraction. In experiments, we evaluate our
model on relation extraction task. The experimen-
tal results show that our model signiﬁcantly and
consistently outperforms state-of-the-art feature-
based methods and neural network methods.

In the future, we will explore the following di-

rections:

• Our model incorporates multi-instance learn-
ing with neural network via instance-level se-
lective attention. It can be used in not only
distant supervised relation extraction but also
other multi-instance learning tasks. We will
explore our model in other area such as text

categorization.

• CNN is one of the effective neural net-
works for neural relation extraction. Re-
searchers also propose many other neural net-
In the
work models for relation extraction.
future, we will incorporate our instance-level
selective attention technique with those mod-
els for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61303075) and the Tsinghua University Initiative
Scientiﬁc Research Program (20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
In Proceedings of ACL, volume 45,
supervision.
page 576.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In Pro-
ceedings of NIPS, pages 2204–2212.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with compo-
In Proceedings of ACL.
sitional vector grammars.
Citeseer.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
nn: ﬁrst results. arXiv preprint arXiv:1412.1602.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artiﬁcial in-
telligence, 89(1):31–71.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image
caption generation with visual attention. Proceed-
ings of ICML.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

Neural Relation Extraction with Selective Attention over Instances

Yankai Lin1, Shiqi Shen1, Zhiyuan Liu1,2∗, Huanbo Luan1, Maosong Sun1,2
1 Department of Computer Science and Technology,
State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China

Abstract

Distant supervised relation extraction has
been widely used to ﬁnd novel relational
facts from text. However, distant su-
pervision inevitably accompanies with the
wrong labelling problem, and these noisy
data will substantially hurt
the perfor-
mance of relation extraction. To allevi-
ate this issue, we propose a sentence-level
attention-based model for relation extrac-
tion. In this model, we employ convolu-
tional neural networks to embed the se-
mantics of sentences. Afterwards, we
build sentence-level attention over multi-
ple instances, which is expected to dy-
namically reduce the weights of those
noisy instances. Experimental results on
real-world datasets show that, our model
can make full use of all informative sen-
tences and effectively reduce the inﬂuence
of wrong labelled instances. Our model
im-
achieves signiﬁcant and consistent
provements on relation extraction as com-
pared with baselines. The source code of
this paper can be obtained from https:
//github.com/thunlp/NRE.

1

Introduction

In recent years, various large-scale knowledge
bases (KBs) such as Freebase (Bollacker et al.,
2008), DBpedia (Auer et al., 2007) and YAGO
(Suchanek et al., 2007) have been built and widely
used in many natural language processing (NLP)
tasks, including web search and question answer-
ing. These KBs mostly compose of relational facts
with triple format, e.g., (Microsoft, founder,
Bill Gates). Although existing KBs contain a

∗ Corresponding
uzy@tsinghua.edu.cn).

author:

Zhiyuan

Liu

(li-

massive amount of facts, they are still far from
complete compared to the inﬁnite real-world facts.
To enrich KBs, many efforts have been invested
in automatically ﬁnding unknown relational facts.
Therefore, relation extraction (RE), the process of
generating relational data from plain text, is a cru-
cial task in NLP.

Most existing supervised RE systems require a
large amount of labelled relation-speciﬁc training
data, which is very time consuming and labor in-
tensive. (Mintz et al., 2009) proposes distant su-
pervision to automatically generate training data
via aligning KBs and texts. They assume that if
two entities have a relation in KBs, then all sen-
tences that contain these two entities will express
this relation. For example, (Microsoft, founder,
Bill Gates) is a relational fact in KB. Distant su-
pervision will regard all sentences that contain
these two entities as active instances for relation
founder. Although distant supervision is an
effective strategy to automatically label training
data, it always suffers from wrong labelling prob-
lem. For example, the sentence “Bill Gates ’s turn
to philanthropy was linked to the antitrust prob-
lems Microsoft had in the U.S. and the European
union.” does not express the relation founder
but will still be regarded as an active instance.
Hence, (Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012) adopt multi-instance learn-
ing to alleviate the wrong labelling problem. The
main weakness of these conventional methods is
that most features are explicitly derived from NLP
tools such as POS tagging and the errors generated
by NLP tools will propagate in these methods.

Some recent works (Socher et al., 2012; Zeng
et al., 2014; dos Santos et al., 2015) attempt to
use deep neural networks in relation classiﬁca-
tion without handcrafted features. These meth-
ods build classiﬁer based on sentence-level anno-
tated data, which cannot be applied in large-scale

rized as follows:

• As compared to existing neural relation ex-
traction model, our model can make full use
of all informative sentences of each entity
pair.

• To address the wrong labelling problem in
distant supervision, we propose selective
attention to de-emphasize those noisy in-
stances.

• In the experiments, we show that selective
attention is beneﬁcial to two kinds of CNN
models in the task of relation extraction.

2 Related Work

Relation extraction is one of the most impor-
tant tasks in NLP. Many efforts have been invested
in relation extraction, especially in supervised re-
lation extraction. Most of these methods need a
great deal of annotated data, which is time con-
suming and labor intensive. To address this issue,
(Mintz et al., 2009) aligns plain text with Free-
base by distant supervision. However, distant su-
pervision inevitably accompanies with the wrong
labelling problem. To alleviate the wrong la-
belling problem, (Riedel et al., 2010) models dis-
tant supervision for relation extraction as a multi-
instance single-label problem, and (Hoffmann et
al., 2011; Surdeanu et al., 2012) adopt multi-
instance multi-label learning in relation extraction.
Multi-instance learning was originally proposed to
address the issue of ambiguously-labelled training
data when predicting the activity of drugs (Diet-
terich et al., 1997). Multi-instance learning con-
siders the reliability of the labels for each instance.
(Bunescu and Mooney, 2007) connects weak su-
pervision with multi-instance learning and extends
it to relation extraction. But all the feature-based
methods depend strongly on the quality of the fea-
tures generated by NLP tools, which will suffer
from error propagation problem.

Recently, deep learning (Bengio, 2009) has
been widely used for various areas, including com-
puter vision, speech recognition and so on. It has
also been successfully applied to different NLP
tasks such as part-of-speech tagging (Collobert
et al., 2011), sentiment analysis (dos Santos and
Gatti, 2014), parsing (Socher et al., 2013), and
machine translation (Sutskever et al., 2014). Due
to the recent success in deep learning, many re-

Figure 1: The architecture of sentence-level
attention-based CNN, where xi and xi indicate the
original sentence for an entity pair and its corre-
sponding sentence representation, αi is the weight
given by sentence-level attention, and s indicates
the representation of the sentence set.

KBs due to the lack of human-annotated train-
ing data. Therefore, (Zeng et al., 2015) incor-
porates multi-instance learning with neural net-
work model, which can build relation extractor
based on distant supervision data. Although the
method achieves signiﬁcant improvement in re-
lation extraction, it is still far from satisfactory.
The method assumes that at least one sentence that
mentions these two entities will express their rela-
tion, and only selects the most likely sentence for
each entity pair in training and prediction. It’s ap-
parent that the method will lose a large amount
of rich information containing in neglected sen-
tences.

In this paper, we propose a sentence-level
attention-based convolutional neural network
(CNN) for distant supervised relation extraction.
As illustrated in Fig. 1, we employ a CNN to
embed the semantics of sentences. Afterwards, to
utilize all informative sentences, we represent the
relation as semantic composition of sentence em-
beddings. To address the wrong labelling prob-
lem, we build sentence-level attention over mul-
tiple instances, which is expected to dynamically
reduce the weights of those noisy instances. Fi-
nally, we extract relation with the relation vector
weighted by sentence-level attention. We evaluate
our model on a real-world dataset in the task of
relation extraction. The experimental results show
that our model achieves signiﬁcant and consistent
improvements in relation extraction as compared
with the state-of-the-art methods.

The contributions of this paper can be summa-

searchers have investigated the possibility of us-
ing neural networks to automatically learn features
for relation extraction. (Socher et al., 2012) uses
a recursive neural network in relation extraction.
They parse the sentences ﬁrst and then represent
each node in the parsing tree as a vector. More-
over, (Zeng et al., 2014; dos Santos et al., 2015)
adopt an end-to-end convolutional neural network
for relation extraction. Besides, (Xie et al., 2016)
attempts to incorporate the text information of en-
tities for relation extraction.

Although these methods achieve great success,
they still extract relations on sentence-level and
suffer from a lack of sufﬁcient training data.
In
addition, the multi-instance learning strategy of
conventional methods cannot be easily applied in
neural network models. Therefore, (Zeng et al.,
2015) combines at-least-one multi-instance learn-
ing with neural network model to extract relations
on distant supervision data. However, they assume
that only one sentence is active for each entity pair.
Hence, it will lose a large amount of rich informa-
tion containing in those neglected sentences. Dif-
ferent from their methods, we propose sentence-
level attention over multiple instances, which can
utilize all informative sentences.

The attention-based models have attracted a lot
of interests of researchers recently. The selectiv-
ity of attention-based models allows them to learn
alignments between different modalities.
It has
been applied to various areas such as image clas-
siﬁcation (Mnih et al., 2014), speech recognition
(Chorowski et al., 2014), image caption generation
(Xu et al., 2015) and machine translation (Bah-
danau et al., 2014). To the best of our knowl-
edge, this is the ﬁrst effort to adopt attention-based
model in distant supervised relation extraction.

3 Methodology

Given a set of sentences {x1, x2, · · · , xn} and
two corresponding entities, our model measures
the probability of each relation r. In this section,
we will introduce our model in two main parts:

• Sentence Encoder. Given a sentence x and
two target entities, a convolutional neutral
network (CNN) is used to construct a dis-
tributed representation x of the sentence.

• Selective Attention over Instances. When
the distributed vector representations of all
sentences are learnt, we use sentence-level at-

tention to select the sentences which really
express the corresponding relation.

3.1 Sentence Encoder

Figure 2: The architecture of CNN/PCNN used for
sentence encoder.

As shown in Fig. 2, we transform the sentence
x into its distributed representation x by a CNN.
First, words in the sentence are transformed into
dense real-valued feature vectors. Next, convo-
lutional layer, max-pooling layer and non-linear
transformation layer are used to construct a dis-
tributed representation of the sentence, i.e., x.

3.1.1

Input Representation

The inputs of the CNN are raw words of the
sentence x. We ﬁrst transform words into low-
dimensional vectors. Here, each input word is
transformed into a vector via word embedding ma-
trix. In addition, to specify the position of each en-
tity pair, we also use position embeddings for all
words in the sentence.

Word Embeddings. Word embeddings aim to
transform words into distributed representations
which capture syntactic and semantic meanings
of the words. Given a sentence x consisting of
m words x = {w1, w2, · · · , wm}, every word
wi is represented by a real-valued vector. Word
representations are encoded by column vectors in
an embedding matrix V ∈ Rda×|V |where V is a
ﬁxed-sized vocabulary.

Position Embeddings. In the task of relation
extraction, the words close to the target entities are
usually informative to determine the relation be-
tween entities. Similar to (Zeng et al., 2014), we

use position embeddings speciﬁed by entity pairs.
It can help the CNN to keep track of how close
each word is to head or tail entities. It is deﬁned
as the combination of the relative distances from
the current word to head or tail entities. For ex-
ample, in the sentence “Bill Gates is the founder
of Microsoft.”, the relative distance from the word
“founder” to head entity Bill Gates is 3 and tail
entity Microsoft is 2.

In the example shown in Fig. 2, it is assumed
that the dimension da of the word embedding is 3
and the dimension db of the position embedding is
1. Finally, we concatenate the word embeddings
and position embeddings of all words and denote
it as a vector sequence w = {w1, w2, · · · , wm},
where wi ∈ Rd(d = da + db × 2).

3.1.2 Convolution, Max-pooling and

Non-linear Layers

In relation extraction, the main challenges are
that the length of the sentences is variable and the
important information can appear in any area of
the sentences. Hence, we should utilize all lo-
cal features and perform relation prediction glob-
ally. Here, we use a convolutional layer to merge
all these features. The convolutional layer ﬁrst
extracts local features with a sliding window of
length l over the sentence. In the example shown
in Fig. 2, we assume that the length of the sliding
window l is 3. Then, it combines all local features
via a max-pooling operation to obtain a ﬁxed-sized
vector for the input sentence.

Here, convolution is deﬁned as an operation be-
tween a vector sequence w and a convolution ma-
trix W ∈ Rdc×(l×d), where dc is the sentence em-
bedding size. Let us deﬁne the vector qi ∈ Rl×d
as the concatenation of a sequence of w word em-
beddings within the i-th window:

qi = wi−l+1:i

(1 ≤ i ≤ m + l − 1).

(1)

Since the window may be outside of the sen-
tence boundaries when it slides near the boundary,
we set special padding tokens for the sentence. It
means that we regard all out-of-range input vec-
tors wi(i < 1 or i > m) as zero vector.

Hence, the i-th ﬁlter of convolutional layer is

computed as:

where b is bias vector. And the i-th element of the
vector x ∈ Rdc

as follows:

pi = [Wq + b]i

[x]i = max(pi),

(2)

(3)

Further, PCNN (Zeng et al., 2015), which is a
variation of CNN, adopts piecewise max pooling
in relation extraction. Each convolutional ﬁlter pi
is divided into three segments (pi1, pi2, pi3) by
head and tail entities. And the max pooling pro-
cedure is performed in three segments separately,
which is deﬁned as:

[x]ij = max(pij),

(4)

And [x]i is set as the concatenation of [x]ij.

Finally, we apply a non-linear function at the

output, such as the hyperbolic tangent.

3.2 Selective Attention over Instances

Suppose there is a set S contains n sen-
i.e., S =

tences for entity pair (head, tail),
{x1, x2, · · · , xn}.

To exploit the information of all sentences, our
model represents the set S with a real-valued vec-
tor s when predicting relation r. It is straightfor-
ward that the representation of the set S depends
on all sentences’ representations x1, x2, · · · , xn.
Each sentence representation xi contains informa-
tion about whether entity pair (head, tail) con-
tains relation r for input sentence xi.

The set vector s is,

then, computed as a

weighted sum of these sentence vector xi:

where αi is the weight of each sentence vector xi.
In this paper, we deﬁne αi in two ways:

Average: We assume that all sentences in the
set X have the same contribution to the represen-
tation of the set. It means the embedding of the set
S is the average of all the sentence vectors:

(cid:88)

s =

αixi,

i

(cid:88)

s =

1
n

xi,

i

(5)

(6)

It’s a naive baseline of our selective attention.

Selective Attention: However, the wrong la-
belling problem inevitably occurs. Thus, if we
regard each sentence equally, the wrong labelling
sentences will bring in massive of noise during
training and testing. Hence, we use a selec-
tive attention to de-emphasize the noisy sentence.
Hence, αi is further deﬁned as:

αi =

exp(ei)
k exp(ek)

,

(cid:80)

(7)

where ei is referred as a query-based function
which scores how well the input sentence xi and
the predict relation r matches. We select the bilin-
ear form which achieves best performance in dif-
ferent alternatives:

ei = xiAr,

(8)

where A is a weighted diagonal matrix, and r is
the query vector associated with relation r which
indicates the representation of relation r.

Finally, we deﬁne the conditional probability

p(r|S, θ) through a softmax layer as follows:

p(r|S, θ) =

exp(or)
k=1 exp(ok)

,

(cid:80)nr

(9)

where nr is the total number of relations and o is
the ﬁnal output of the neural network which cor-
responds to the scores associated to all relation
types, which is deﬁned as follows:

o = Ms + d,

(10)

where d ∈ Rnr is a bias vector and M is the rep-
resentation matrix of relations.

(Zeng et al., 2015) follows the assumption that
at least one mention of the entity pair will reﬂect
their relation, and only uses the sentence with the
highest probability in each set for training. Hence,
the method which they adopted for multi-instance
learning can be regarded as a special case as our
selective attention when the weight of the sentence
with the highest probability is set to 1 and others
to 0.

3.3 Optimization and Implementation Details

Here we introduce the learning and optimiza-
tion details of our model. We deﬁne the objective
function using cross-entropy at the set level as fol-
lows:

J(θ) =

log p(ri|Si, θ),

(11)

s
(cid:88)

i=1

where s indicates the number of sentence sets and
θ indicates all parameters of our model. To solve
the optimization problem, we adopt stochastic gra-
dient descent (SGD) to minimize the objective
function. For learning, we iterate by randomly
selecting a mini-batch from the training set until
converge.

In the implementation, we employ dropout (Sri-
vastava et al., 2014) on the output layer to pre-
vent overﬁtting. The dropout layer is deﬁned as

an element-wise multiplication with a a vector h
of Bernoulli random variables with probability p.
Then equation (10) is rewritten as:

o = M(s ◦ h) + d.

(12)

In the test phase, the learnt set representations
are scaled by p, i.e., ˆsi = psi. And the scaled set
vector ˆri is ﬁnally used to predict relations.

4 Experiments

Our experiments are intended to demonstrate
that our neural models with sentence-level selec-
tive attention can alleviate the wrong labelling
problem and take full advantage of informative
sentences for distant supervised relation extrac-
tion. To this end, we ﬁrst introduce the dataset and
evaluation metrics used in the experiments. Next,
we use cross-validation to determine the parame-
ters of our model. And then we evaluate the ef-
fects of our selective attention and show its per-
formance on the data with different set size. Fi-
nally, we compare the performance of our method
to several state-of-the-art feature-based methods.

4.1 Dataset and Evaluation Metrics

We evaluate our model on a widely used
dataset1 which is developed by (Riedel et al.,
2010) and has also been used by (Hoffmann et
al., 2011; Surdeanu et al., 2012). This dataset was
generated by aligning Freebase relations with the
New York Times corpus (NYT). Entity mentions
are found using the Stanford named entity tagger
(Finkel et al., 2005), and are further matched to the
names of Freebase entities. The Freebase relations
are divided into two parts, one for training and one
for testing.
It aligns the the sentences from the
corpus of the years 2005-2006 and regards them
as training instances. And the testing instances
are the aligned sentences from 2007. There are
53 possible relationships including a special rela-
tion NA which indicates there is no relation be-
tween head and tail entities. The training data con-
tains 522,611 sentences, 281,270 entity pairs and
18,252 relational facts. The testing set contains
172,448 sentences, 96,678 entity pairs and 1,950
relational facts.

Similar to previous work (Mintz et al., 2009),
we evaluate our model in the held-out evaluation.
It evaluates our model by comparing the relation

1http://iesl.cs.umass.edu/riedel/ecml/

facts discovered from the test articles with those
It assumes that the testing systems
in Freebase.
have similar performances in relation facts inside
and outside Freebase. Hence, the held-out evalua-
tion provides an approximate measure of precision
without time consumed human evaluation. We
report both the aggregate curves precision/recall
curves and Precision@N (P@N) in our experi-
ments.

et al., 2015) as our sentence encoders and imple-
ment them by ourselves which achieve compara-
ble results as the authors reported. And we com-
pare the performance of the two different kinds
of CNN with sentence-level attention (ATT) , its
naive version (AVE) which represents each sen-
tence set as the average vector of sentences inside
the set and the at-least-one multi-instance learning
(ONE) used in (Zeng et al., 2015).

4.2 Experimental Settings

4.2.1 Word Embeddings

In this paper, we use the word2vec tool 2 to train
the word embeddings on NYT corpus. We keep
the words which appear more than 100 times in
the corpus as vocabulary. Besides, we concatenate
the words of an entity when it has multiple words.

4.2.2 Parameter Settings

Following previous work, we tune our mod-
els using three-fold validation on the training set.
We use a grid search to determine the optimal
parameters and select learning rate λ for SGD
among {0.1, 0.01, 0.001, 0.0001}, the sliding win-
dow size l ∈ {1, 2, 3, · · · , 8}, the sentence embed-
ding size n ∈ {50, 60, · · · , 300}, and the batch
size B among {40, 160, 640, 1280}. For other pa-
rameters, since they have little effect on the results,
we follow the settings used in (Zeng et al., 2014).
For training, we set the iteration number over all
the training data as 25.
In Table 1 we show all
parameters used in the experiments.

Table 1: Parameter settings

Window size l
Sentence embedding size dc
Word dimension da
Position dimension db
Batch size B
Learning rate λ
Dropout probability p

3
230
50
5
160
0.01
0.5

4.3 Effect of Sentence-level Selective

Attention

To demonstrate the effects of the sentence-level
selective attention, we empirically compare dif-
ferent methods through held-out evaluation. We
select the CNN model proposed in (Zeng et al.,
2014) and the PCNN model proposed in (Zeng

2https://code.google.com/p/word2vec/

Figure 3: Top: Aggregate precion/recall curves of
CNN, CNN+ONE, CNN+AVE, CNN+ATT. Bot-
tom: Aggregate precion/recall curves of PCNN,
PCNN+ONE, PCNN+AVE, PCNN+ATT

From Fig. 3, we have the following observa-
(1) For both CNN and PCNN, the ONE
tion:
method brings better performance as compared to
CNN/PCNN. The reason is that the original distant
supervision training data contains a lot of noise
and the noisy data will damage the performance of
relation extraction. (2) For both CNN and PCNN,
the AVE method is useful for relation extraction
as compared to CNN/PCNN. It indicates that con-
sidering more sentences is beneﬁcial to relation
extraction since the noise can be reduced by mu-
tual complementation of information. (3) For both

CNN and PCNN, the AVE method has a similar
performance compared to the ONE method. It in-
dicates that, although the AVE method brings in
information of more sentences, since it regards
each sentence equally, it also brings in the noise
from the wrong labelling sentences which may
hurt the performance of relation extraction. (4) For
both CNN and PCNN, the ATT method achieves
the highest precision over the entire range of re-
call compared to other methods including the AVE
method. It indicates that the proposed selective at-
tention is beneﬁcial.
It can effectively ﬁlter out
meaningless sentences and alleviate the wrong la-
belling problem in distant supervised relation ex-
traction.

4.4 Effect of Sentence Number

In the original testing data set, there are 74,857
entity pairs that correspond to only one sen-
Since
tence, nearly 3/4 over all entity pairs.
the superiority of our selective attention lies in
the entity pairs containing multiple sentences, we
compare the performance of CNN/PCNN+ONE,
CNN/PCNN+AVE and CNN/PCNN+ATT on the
entity pairs which have more than one sentence.
And then we examine these three methods in three
test settings:

• One: For each testing entity pair, we ran-
domly select one sentence and use this sen-
tence to predict relation.

• Two: For each testing entity pair, we ran-
domly select two sentences and proceed re-
lation extraction.

• All: We use all sentences of each entity pair

for relation extraction.

Note that, we use all the sentences in training. We
will report the P@100, P@200, P@300 and the
mean of them for each model in held-out evalua-
tion.

Table 2 shows the P@N for compared models in
three test settings. From the table, we can see that:
(1) For both CNN and PCNN, the ATT method
achieves the best performance in all test settings.
It demonstrates the effectiveness of sentence-level
selective attention for multi-instance learning. (2)
For both CNN and PCNN, the AVE method is
comparable to the ATT method in the One test set-
ting. However, when the number of testing sen-
tences per entity pair grows, the performance of

the AVE methods has almost no improvement. It
even drops gradually in P@100, P@200 as the
sentence number increases. The reason is that,
since we regard each sentence equally, the noise
contained in the sentences that do not express any
relation will have negative inﬂuence in the perfor-
mance of relation extraction. (3) CNN+AVE and
CNN+ATT have 5% to 8% improvements com-
pared to CNN+ONE in the ONE test setting. Since
each entity pair has only one sentence in this test
setting, the only difference of these methods is
from training. Hence, it shows that utilizing all
sentences will bring in more information although
it may also bring in some extra noises.
(4) For
both CNN and PCNN, the ATT method outper-
forms other two baselines over 5% and 9% in the
Two and All test settings. It indicates that by tak-
ing more useful information into account, the re-
lational facts which CNN+ATT ranks higher are
more reliable and beneﬁcial to relation extraction.

4.5 Comparison with Feature-based

Approaches

Figure 4: Performance comparison of proposed
model and traditional methods

To evaluate the proposed method, we select the
following three feature-based methods for com-
parison through held-out evaluation:

Mintz (Mintz et al., 2009) is a traditional distant

supervised model.

MultiR (Hoffmann et al., 2011) proposes a
probabilistic, graphical model of multi-instance
learning which handles overlapping relations.

MIML (Surdeanu et al., 2012) jointly models

both multiple instances and multiple relations.

We implement them with the source codes re-

leased by the authors.

Table 2: P@N for relation extraction in the entity pairs with different number of sentences

Test Settings
100
P@N(%)
68.3
CNN+ONE
75.2
+AVE
76.2
+ATT
PCNN+ONE 73.3
71.3
+AVE
+ATT
73.3

One

200
60.7
67.2
65.2
64.8
63.7
69.2

300 Mean
60.9
53.8
67.1
58.8
60.8
67.4
65.0
56.8
64.3
57.8
67.8
60.8

Two

All

100
70.3
68.3
76.2
70.3
73.3
77.2

200
62.7
63.2
65.7
67.2
65.2
71.6

300 Mean
62.9
55.8
64.0
60.5
68.0
62.1
66.9
63.1
66.9
62.1
71.6
66.1

100
67.3
64.4
76.2
72.3
73.3
76.2

200
64.7
60.2
68.6
69.7
66.7
73.1

300 Mean
63.4
58.1
60.4
60.1
68.2
59.8
68.7
64.1
67.6
62.8
72.2
67.4

Fig.

4 shows the precision/recall curves
for each method. We can observe that:
(1)
CNN/PCNN+ATT signiﬁcantly outperforms all
feature-based methods over the entire range of re-
call. When the recall is greater than 0.1, the perfor-
mance of feature-based method drop out quickly.
In contrast, our model has a reasonable preci-
sion until the recall approximately reaches 0.3.
It demonstrates that the human-designed feature
cannot concisely express the semantic meaning of
the sentences, and the inevitable error brought by
NLP tools will hurt the performance of relation
extraction. In contrast, CNN/PCNN+ATT which
learns the representation of each sentences auto-
matically can express each sentence well.
(2)
PCNN+ATT performs much better as compared
to CNN+ATT over the entire range of recall.
It
means that the selective attention considers the
global information of all sentences except the in-
formation inside each sentence. Hence, the perfor-
mance of our model can be further improved if we
have a better sentence encoder.

4.6 Case Study

Table 3 shows two examples of selective at-
tention from the testing data. For each relation,
we show the corresponding sentences with high-
est and lowest attention weight respectively. And
we highlight the entity pairs with bold formatting.

From the table we ﬁnd that: The former exam-
ple is related to the relation employer of. The
sentence with low attention weight does not ex-
press the relation between two entities, while the
high one shows that Mel Karmazin is the chief ex-
ecutive of Sirius Satellite Radio. The later exam-
ple is related to the relation place of birth.
The sentence with low attention weight expresses
where Ernst Haeﬂiger is died in, while the high
one expresses where he is born in.

Table 3: Some examples of selective attention in
NYT corpus

Relation

employer of

Low When Howard Stern was prepar-
ing to take his talk show to Sirius
Satellite Radio, following his for-
mer boss, Mel Karmazin, Mr. Hol-
lander argued that ...

Relation
Low

High Mel Karmazin, the chief executive
of Sirius Satellite Radio, made a
lot of phone calls ...
place of birth
Ernst Haeﬂiger, a Swiss tenor
who ...
roles , died on Saturday
in Davos, Switzerland, where he
maintained a second home.
Ernst Haeﬂiger was born in Davos
on July 6, 1919, and studied at the
Wettinger Seminary ...

High

5 Conclusion and Future Works

In this paper, we develop CNN with sentence-
level selective attention. Our model can make full
use of all informative sentences and alleviate the
wrong labelling problem for distant supervised re-
lation extraction. In experiments, we evaluate our
model on relation extraction task. The experimen-
tal results show that our model signiﬁcantly and
consistently outperforms state-of-the-art feature-
based methods and neural network methods.

In the future, we will explore the following di-

rections:

• Our model incorporates multi-instance learn-
ing with neural network via instance-level se-
lective attention. It can be used in not only
distant supervised relation extraction but also
other multi-instance learning tasks. We will
explore our model in other area such as text

categorization.

• CNN is one of the effective neural net-
works for neural relation extraction. Re-
searchers also propose many other neural net-
In the
work models for relation extraction.
future, we will incorporate our instance-level
selective attention technique with those mod-
els for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61303075) and the Tsinghua University Initiative
Scientiﬁc Research Program (20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
In Proceedings of ACL, volume 45,
supervision.
page 576.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In Pro-
ceedings of NIPS, pages 2204–2212.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with compo-
In Proceedings of ACL.
sitional vector grammars.
Citeseer.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
nn: ﬁrst results. arXiv preprint arXiv:1412.1602.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artiﬁcial in-
telligence, 89(1):31–71.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image
caption generation with visual attention. Proceed-
ings of ICML.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

Neural Relation Extraction with Selective Attention over Instances

Yankai Lin1, Shiqi Shen1, Zhiyuan Liu1,2∗, Huanbo Luan1, Maosong Sun1,2
1 Department of Computer Science and Technology,
State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China

Abstract

Distant supervised relation extraction has
been widely used to ﬁnd novel relational
facts from text. However, distant su-
pervision inevitably accompanies with the
wrong labelling problem, and these noisy
data will substantially hurt
the perfor-
mance of relation extraction. To allevi-
ate this issue, we propose a sentence-level
attention-based model for relation extrac-
tion. In this model, we employ convolu-
tional neural networks to embed the se-
mantics of sentences. Afterwards, we
build sentence-level attention over multi-
ple instances, which is expected to dy-
namically reduce the weights of those
noisy instances. Experimental results on
real-world datasets show that, our model
can make full use of all informative sen-
tences and effectively reduce the inﬂuence
of wrong labelled instances. Our model
im-
achieves signiﬁcant and consistent
provements on relation extraction as com-
pared with baselines. The source code of
this paper can be obtained from https:
//github.com/thunlp/NRE.

1

Introduction

In recent years, various large-scale knowledge
bases (KBs) such as Freebase (Bollacker et al.,
2008), DBpedia (Auer et al., 2007) and YAGO
(Suchanek et al., 2007) have been built and widely
used in many natural language processing (NLP)
tasks, including web search and question answer-
ing. These KBs mostly compose of relational facts
with triple format, e.g., (Microsoft, founder,
Bill Gates). Although existing KBs contain a

∗ Corresponding
uzy@tsinghua.edu.cn).

author:

Zhiyuan

Liu

(li-

massive amount of facts, they are still far from
complete compared to the inﬁnite real-world facts.
To enrich KBs, many efforts have been invested
in automatically ﬁnding unknown relational facts.
Therefore, relation extraction (RE), the process of
generating relational data from plain text, is a cru-
cial task in NLP.

Most existing supervised RE systems require a
large amount of labelled relation-speciﬁc training
data, which is very time consuming and labor in-
tensive. (Mintz et al., 2009) proposes distant su-
pervision to automatically generate training data
via aligning KBs and texts. They assume that if
two entities have a relation in KBs, then all sen-
tences that contain these two entities will express
this relation. For example, (Microsoft, founder,
Bill Gates) is a relational fact in KB. Distant su-
pervision will regard all sentences that contain
these two entities as active instances for relation
founder. Although distant supervision is an
effective strategy to automatically label training
data, it always suffers from wrong labelling prob-
lem. For example, the sentence “Bill Gates ’s turn
to philanthropy was linked to the antitrust prob-
lems Microsoft had in the U.S. and the European
union.” does not express the relation founder
but will still be regarded as an active instance.
Hence, (Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012) adopt multi-instance learn-
ing to alleviate the wrong labelling problem. The
main weakness of these conventional methods is
that most features are explicitly derived from NLP
tools such as POS tagging and the errors generated
by NLP tools will propagate in these methods.

Some recent works (Socher et al., 2012; Zeng
et al., 2014; dos Santos et al., 2015) attempt to
use deep neural networks in relation classiﬁca-
tion without handcrafted features. These meth-
ods build classiﬁer based on sentence-level anno-
tated data, which cannot be applied in large-scale

rized as follows:

• As compared to existing neural relation ex-
traction model, our model can make full use
of all informative sentences of each entity
pair.

• To address the wrong labelling problem in
distant supervision, we propose selective
attention to de-emphasize those noisy in-
stances.

• In the experiments, we show that selective
attention is beneﬁcial to two kinds of CNN
models in the task of relation extraction.

2 Related Work

Relation extraction is one of the most impor-
tant tasks in NLP. Many efforts have been invested
in relation extraction, especially in supervised re-
lation extraction. Most of these methods need a
great deal of annotated data, which is time con-
suming and labor intensive. To address this issue,
(Mintz et al., 2009) aligns plain text with Free-
base by distant supervision. However, distant su-
pervision inevitably accompanies with the wrong
labelling problem. To alleviate the wrong la-
belling problem, (Riedel et al., 2010) models dis-
tant supervision for relation extraction as a multi-
instance single-label problem, and (Hoffmann et
al., 2011; Surdeanu et al., 2012) adopt multi-
instance multi-label learning in relation extraction.
Multi-instance learning was originally proposed to
address the issue of ambiguously-labelled training
data when predicting the activity of drugs (Diet-
terich et al., 1997). Multi-instance learning con-
siders the reliability of the labels for each instance.
(Bunescu and Mooney, 2007) connects weak su-
pervision with multi-instance learning and extends
it to relation extraction. But all the feature-based
methods depend strongly on the quality of the fea-
tures generated by NLP tools, which will suffer
from error propagation problem.

Recently, deep learning (Bengio, 2009) has
been widely used for various areas, including com-
puter vision, speech recognition and so on. It has
also been successfully applied to different NLP
tasks such as part-of-speech tagging (Collobert
et al., 2011), sentiment analysis (dos Santos and
Gatti, 2014), parsing (Socher et al., 2013), and
machine translation (Sutskever et al., 2014). Due
to the recent success in deep learning, many re-

Figure 1: The architecture of sentence-level
attention-based CNN, where xi and xi indicate the
original sentence for an entity pair and its corre-
sponding sentence representation, αi is the weight
given by sentence-level attention, and s indicates
the representation of the sentence set.

KBs due to the lack of human-annotated train-
ing data. Therefore, (Zeng et al., 2015) incor-
porates multi-instance learning with neural net-
work model, which can build relation extractor
based on distant supervision data. Although the
method achieves signiﬁcant improvement in re-
lation extraction, it is still far from satisfactory.
The method assumes that at least one sentence that
mentions these two entities will express their rela-
tion, and only selects the most likely sentence for
each entity pair in training and prediction. It’s ap-
parent that the method will lose a large amount
of rich information containing in neglected sen-
tences.

In this paper, we propose a sentence-level
attention-based convolutional neural network
(CNN) for distant supervised relation extraction.
As illustrated in Fig. 1, we employ a CNN to
embed the semantics of sentences. Afterwards, to
utilize all informative sentences, we represent the
relation as semantic composition of sentence em-
beddings. To address the wrong labelling prob-
lem, we build sentence-level attention over mul-
tiple instances, which is expected to dynamically
reduce the weights of those noisy instances. Fi-
nally, we extract relation with the relation vector
weighted by sentence-level attention. We evaluate
our model on a real-world dataset in the task of
relation extraction. The experimental results show
that our model achieves signiﬁcant and consistent
improvements in relation extraction as compared
with the state-of-the-art methods.

The contributions of this paper can be summa-

searchers have investigated the possibility of us-
ing neural networks to automatically learn features
for relation extraction. (Socher et al., 2012) uses
a recursive neural network in relation extraction.
They parse the sentences ﬁrst and then represent
each node in the parsing tree as a vector. More-
over, (Zeng et al., 2014; dos Santos et al., 2015)
adopt an end-to-end convolutional neural network
for relation extraction. Besides, (Xie et al., 2016)
attempts to incorporate the text information of en-
tities for relation extraction.

Although these methods achieve great success,
they still extract relations on sentence-level and
suffer from a lack of sufﬁcient training data.
In
addition, the multi-instance learning strategy of
conventional methods cannot be easily applied in
neural network models. Therefore, (Zeng et al.,
2015) combines at-least-one multi-instance learn-
ing with neural network model to extract relations
on distant supervision data. However, they assume
that only one sentence is active for each entity pair.
Hence, it will lose a large amount of rich informa-
tion containing in those neglected sentences. Dif-
ferent from their methods, we propose sentence-
level attention over multiple instances, which can
utilize all informative sentences.

The attention-based models have attracted a lot
of interests of researchers recently. The selectiv-
ity of attention-based models allows them to learn
alignments between different modalities.
It has
been applied to various areas such as image clas-
siﬁcation (Mnih et al., 2014), speech recognition
(Chorowski et al., 2014), image caption generation
(Xu et al., 2015) and machine translation (Bah-
danau et al., 2014). To the best of our knowl-
edge, this is the ﬁrst effort to adopt attention-based
model in distant supervised relation extraction.

3 Methodology

Given a set of sentences {x1, x2, · · · , xn} and
two corresponding entities, our model measures
the probability of each relation r. In this section,
we will introduce our model in two main parts:

• Sentence Encoder. Given a sentence x and
two target entities, a convolutional neutral
network (CNN) is used to construct a dis-
tributed representation x of the sentence.

• Selective Attention over Instances. When
the distributed vector representations of all
sentences are learnt, we use sentence-level at-

tention to select the sentences which really
express the corresponding relation.

3.1 Sentence Encoder

Figure 2: The architecture of CNN/PCNN used for
sentence encoder.

As shown in Fig. 2, we transform the sentence
x into its distributed representation x by a CNN.
First, words in the sentence are transformed into
dense real-valued feature vectors. Next, convo-
lutional layer, max-pooling layer and non-linear
transformation layer are used to construct a dis-
tributed representation of the sentence, i.e., x.

3.1.1

Input Representation

The inputs of the CNN are raw words of the
sentence x. We ﬁrst transform words into low-
dimensional vectors. Here, each input word is
transformed into a vector via word embedding ma-
trix. In addition, to specify the position of each en-
tity pair, we also use position embeddings for all
words in the sentence.

Word Embeddings. Word embeddings aim to
transform words into distributed representations
which capture syntactic and semantic meanings
of the words. Given a sentence x consisting of
m words x = {w1, w2, · · · , wm}, every word
wi is represented by a real-valued vector. Word
representations are encoded by column vectors in
an embedding matrix V ∈ Rda×|V |where V is a
ﬁxed-sized vocabulary.

Position Embeddings. In the task of relation
extraction, the words close to the target entities are
usually informative to determine the relation be-
tween entities. Similar to (Zeng et al., 2014), we

use position embeddings speciﬁed by entity pairs.
It can help the CNN to keep track of how close
each word is to head or tail entities. It is deﬁned
as the combination of the relative distances from
the current word to head or tail entities. For ex-
ample, in the sentence “Bill Gates is the founder
of Microsoft.”, the relative distance from the word
“founder” to head entity Bill Gates is 3 and tail
entity Microsoft is 2.

In the example shown in Fig. 2, it is assumed
that the dimension da of the word embedding is 3
and the dimension db of the position embedding is
1. Finally, we concatenate the word embeddings
and position embeddings of all words and denote
it as a vector sequence w = {w1, w2, · · · , wm},
where wi ∈ Rd(d = da + db × 2).

3.1.2 Convolution, Max-pooling and

Non-linear Layers

In relation extraction, the main challenges are
that the length of the sentences is variable and the
important information can appear in any area of
the sentences. Hence, we should utilize all lo-
cal features and perform relation prediction glob-
ally. Here, we use a convolutional layer to merge
all these features. The convolutional layer ﬁrst
extracts local features with a sliding window of
length l over the sentence. In the example shown
in Fig. 2, we assume that the length of the sliding
window l is 3. Then, it combines all local features
via a max-pooling operation to obtain a ﬁxed-sized
vector for the input sentence.

Here, convolution is deﬁned as an operation be-
tween a vector sequence w and a convolution ma-
trix W ∈ Rdc×(l×d), where dc is the sentence em-
bedding size. Let us deﬁne the vector qi ∈ Rl×d
as the concatenation of a sequence of w word em-
beddings within the i-th window:

qi = wi−l+1:i

(1 ≤ i ≤ m + l − 1).

(1)

Since the window may be outside of the sen-
tence boundaries when it slides near the boundary,
we set special padding tokens for the sentence. It
means that we regard all out-of-range input vec-
tors wi(i < 1 or i > m) as zero vector.

Hence, the i-th ﬁlter of convolutional layer is

computed as:

where b is bias vector. And the i-th element of the
vector x ∈ Rdc

as follows:

pi = [Wq + b]i

[x]i = max(pi),

(2)

(3)

Further, PCNN (Zeng et al., 2015), which is a
variation of CNN, adopts piecewise max pooling
in relation extraction. Each convolutional ﬁlter pi
is divided into three segments (pi1, pi2, pi3) by
head and tail entities. And the max pooling pro-
cedure is performed in three segments separately,
which is deﬁned as:

[x]ij = max(pij),

(4)

And [x]i is set as the concatenation of [x]ij.

Finally, we apply a non-linear function at the

output, such as the hyperbolic tangent.

3.2 Selective Attention over Instances

Suppose there is a set S contains n sen-
i.e., S =

tences for entity pair (head, tail),
{x1, x2, · · · , xn}.

To exploit the information of all sentences, our
model represents the set S with a real-valued vec-
tor s when predicting relation r. It is straightfor-
ward that the representation of the set S depends
on all sentences’ representations x1, x2, · · · , xn.
Each sentence representation xi contains informa-
tion about whether entity pair (head, tail) con-
tains relation r for input sentence xi.

The set vector s is,

then, computed as a

weighted sum of these sentence vector xi:

where αi is the weight of each sentence vector xi.
In this paper, we deﬁne αi in two ways:

Average: We assume that all sentences in the
set X have the same contribution to the represen-
tation of the set. It means the embedding of the set
S is the average of all the sentence vectors:

(cid:88)

s =

αixi,

i

(cid:88)

s =

1
n

xi,

i

(5)

(6)

It’s a naive baseline of our selective attention.

Selective Attention: However, the wrong la-
belling problem inevitably occurs. Thus, if we
regard each sentence equally, the wrong labelling
sentences will bring in massive of noise during
training and testing. Hence, we use a selec-
tive attention to de-emphasize the noisy sentence.
Hence, αi is further deﬁned as:

αi =

exp(ei)
k exp(ek)

,

(cid:80)

(7)

where ei is referred as a query-based function
which scores how well the input sentence xi and
the predict relation r matches. We select the bilin-
ear form which achieves best performance in dif-
ferent alternatives:

ei = xiAr,

(8)

where A is a weighted diagonal matrix, and r is
the query vector associated with relation r which
indicates the representation of relation r.

Finally, we deﬁne the conditional probability

p(r|S, θ) through a softmax layer as follows:

p(r|S, θ) =

exp(or)
k=1 exp(ok)

,

(cid:80)nr

(9)

where nr is the total number of relations and o is
the ﬁnal output of the neural network which cor-
responds to the scores associated to all relation
types, which is deﬁned as follows:

o = Ms + d,

(10)

where d ∈ Rnr is a bias vector and M is the rep-
resentation matrix of relations.

(Zeng et al., 2015) follows the assumption that
at least one mention of the entity pair will reﬂect
their relation, and only uses the sentence with the
highest probability in each set for training. Hence,
the method which they adopted for multi-instance
learning can be regarded as a special case as our
selective attention when the weight of the sentence
with the highest probability is set to 1 and others
to 0.

3.3 Optimization and Implementation Details

Here we introduce the learning and optimiza-
tion details of our model. We deﬁne the objective
function using cross-entropy at the set level as fol-
lows:

J(θ) =

log p(ri|Si, θ),

(11)

s
(cid:88)

i=1

where s indicates the number of sentence sets and
θ indicates all parameters of our model. To solve
the optimization problem, we adopt stochastic gra-
dient descent (SGD) to minimize the objective
function. For learning, we iterate by randomly
selecting a mini-batch from the training set until
converge.

In the implementation, we employ dropout (Sri-
vastava et al., 2014) on the output layer to pre-
vent overﬁtting. The dropout layer is deﬁned as

an element-wise multiplication with a a vector h
of Bernoulli random variables with probability p.
Then equation (10) is rewritten as:

o = M(s ◦ h) + d.

(12)

In the test phase, the learnt set representations
are scaled by p, i.e., ˆsi = psi. And the scaled set
vector ˆri is ﬁnally used to predict relations.

4 Experiments

Our experiments are intended to demonstrate
that our neural models with sentence-level selec-
tive attention can alleviate the wrong labelling
problem and take full advantage of informative
sentences for distant supervised relation extrac-
tion. To this end, we ﬁrst introduce the dataset and
evaluation metrics used in the experiments. Next,
we use cross-validation to determine the parame-
ters of our model. And then we evaluate the ef-
fects of our selective attention and show its per-
formance on the data with different set size. Fi-
nally, we compare the performance of our method
to several state-of-the-art feature-based methods.

4.1 Dataset and Evaluation Metrics

We evaluate our model on a widely used
dataset1 which is developed by (Riedel et al.,
2010) and has also been used by (Hoffmann et
al., 2011; Surdeanu et al., 2012). This dataset was
generated by aligning Freebase relations with the
New York Times corpus (NYT). Entity mentions
are found using the Stanford named entity tagger
(Finkel et al., 2005), and are further matched to the
names of Freebase entities. The Freebase relations
are divided into two parts, one for training and one
for testing.
It aligns the the sentences from the
corpus of the years 2005-2006 and regards them
as training instances. And the testing instances
are the aligned sentences from 2007. There are
53 possible relationships including a special rela-
tion NA which indicates there is no relation be-
tween head and tail entities. The training data con-
tains 522,611 sentences, 281,270 entity pairs and
18,252 relational facts. The testing set contains
172,448 sentences, 96,678 entity pairs and 1,950
relational facts.

Similar to previous work (Mintz et al., 2009),
we evaluate our model in the held-out evaluation.
It evaluates our model by comparing the relation

1http://iesl.cs.umass.edu/riedel/ecml/

facts discovered from the test articles with those
It assumes that the testing systems
in Freebase.
have similar performances in relation facts inside
and outside Freebase. Hence, the held-out evalua-
tion provides an approximate measure of precision
without time consumed human evaluation. We
report both the aggregate curves precision/recall
curves and Precision@N (P@N) in our experi-
ments.

et al., 2015) as our sentence encoders and imple-
ment them by ourselves which achieve compara-
ble results as the authors reported. And we com-
pare the performance of the two different kinds
of CNN with sentence-level attention (ATT) , its
naive version (AVE) which represents each sen-
tence set as the average vector of sentences inside
the set and the at-least-one multi-instance learning
(ONE) used in (Zeng et al., 2015).

4.2 Experimental Settings

4.2.1 Word Embeddings

In this paper, we use the word2vec tool 2 to train
the word embeddings on NYT corpus. We keep
the words which appear more than 100 times in
the corpus as vocabulary. Besides, we concatenate
the words of an entity when it has multiple words.

4.2.2 Parameter Settings

Following previous work, we tune our mod-
els using three-fold validation on the training set.
We use a grid search to determine the optimal
parameters and select learning rate λ for SGD
among {0.1, 0.01, 0.001, 0.0001}, the sliding win-
dow size l ∈ {1, 2, 3, · · · , 8}, the sentence embed-
ding size n ∈ {50, 60, · · · , 300}, and the batch
size B among {40, 160, 640, 1280}. For other pa-
rameters, since they have little effect on the results,
we follow the settings used in (Zeng et al., 2014).
For training, we set the iteration number over all
the training data as 25.
In Table 1 we show all
parameters used in the experiments.

Table 1: Parameter settings

Window size l
Sentence embedding size dc
Word dimension da
Position dimension db
Batch size B
Learning rate λ
Dropout probability p

3
230
50
5
160
0.01
0.5

4.3 Effect of Sentence-level Selective

Attention

To demonstrate the effects of the sentence-level
selective attention, we empirically compare dif-
ferent methods through held-out evaluation. We
select the CNN model proposed in (Zeng et al.,
2014) and the PCNN model proposed in (Zeng

2https://code.google.com/p/word2vec/

Figure 3: Top: Aggregate precion/recall curves of
CNN, CNN+ONE, CNN+AVE, CNN+ATT. Bot-
tom: Aggregate precion/recall curves of PCNN,
PCNN+ONE, PCNN+AVE, PCNN+ATT

From Fig. 3, we have the following observa-
(1) For both CNN and PCNN, the ONE
tion:
method brings better performance as compared to
CNN/PCNN. The reason is that the original distant
supervision training data contains a lot of noise
and the noisy data will damage the performance of
relation extraction. (2) For both CNN and PCNN,
the AVE method is useful for relation extraction
as compared to CNN/PCNN. It indicates that con-
sidering more sentences is beneﬁcial to relation
extraction since the noise can be reduced by mu-
tual complementation of information. (3) For both

CNN and PCNN, the AVE method has a similar
performance compared to the ONE method. It in-
dicates that, although the AVE method brings in
information of more sentences, since it regards
each sentence equally, it also brings in the noise
from the wrong labelling sentences which may
hurt the performance of relation extraction. (4) For
both CNN and PCNN, the ATT method achieves
the highest precision over the entire range of re-
call compared to other methods including the AVE
method. It indicates that the proposed selective at-
tention is beneﬁcial.
It can effectively ﬁlter out
meaningless sentences and alleviate the wrong la-
belling problem in distant supervised relation ex-
traction.

4.4 Effect of Sentence Number

In the original testing data set, there are 74,857
entity pairs that correspond to only one sen-
Since
tence, nearly 3/4 over all entity pairs.
the superiority of our selective attention lies in
the entity pairs containing multiple sentences, we
compare the performance of CNN/PCNN+ONE,
CNN/PCNN+AVE and CNN/PCNN+ATT on the
entity pairs which have more than one sentence.
And then we examine these three methods in three
test settings:

• One: For each testing entity pair, we ran-
domly select one sentence and use this sen-
tence to predict relation.

• Two: For each testing entity pair, we ran-
domly select two sentences and proceed re-
lation extraction.

• All: We use all sentences of each entity pair

for relation extraction.

Note that, we use all the sentences in training. We
will report the P@100, P@200, P@300 and the
mean of them for each model in held-out evalua-
tion.

Table 2 shows the P@N for compared models in
three test settings. From the table, we can see that:
(1) For both CNN and PCNN, the ATT method
achieves the best performance in all test settings.
It demonstrates the effectiveness of sentence-level
selective attention for multi-instance learning. (2)
For both CNN and PCNN, the AVE method is
comparable to the ATT method in the One test set-
ting. However, when the number of testing sen-
tences per entity pair grows, the performance of

the AVE methods has almost no improvement. It
even drops gradually in P@100, P@200 as the
sentence number increases. The reason is that,
since we regard each sentence equally, the noise
contained in the sentences that do not express any
relation will have negative inﬂuence in the perfor-
mance of relation extraction. (3) CNN+AVE and
CNN+ATT have 5% to 8% improvements com-
pared to CNN+ONE in the ONE test setting. Since
each entity pair has only one sentence in this test
setting, the only difference of these methods is
from training. Hence, it shows that utilizing all
sentences will bring in more information although
it may also bring in some extra noises.
(4) For
both CNN and PCNN, the ATT method outper-
forms other two baselines over 5% and 9% in the
Two and All test settings. It indicates that by tak-
ing more useful information into account, the re-
lational facts which CNN+ATT ranks higher are
more reliable and beneﬁcial to relation extraction.

4.5 Comparison with Feature-based

Approaches

Figure 4: Performance comparison of proposed
model and traditional methods

To evaluate the proposed method, we select the
following three feature-based methods for com-
parison through held-out evaluation:

Mintz (Mintz et al., 2009) is a traditional distant

supervised model.

MultiR (Hoffmann et al., 2011) proposes a
probabilistic, graphical model of multi-instance
learning which handles overlapping relations.

MIML (Surdeanu et al., 2012) jointly models

both multiple instances and multiple relations.

We implement them with the source codes re-

leased by the authors.

Table 2: P@N for relation extraction in the entity pairs with different number of sentences

Test Settings
100
P@N(%)
68.3
CNN+ONE
75.2
+AVE
76.2
+ATT
PCNN+ONE 73.3
71.3
+AVE
+ATT
73.3

One

200
60.7
67.2
65.2
64.8
63.7
69.2

300 Mean
60.9
53.8
67.1
58.8
60.8
67.4
65.0
56.8
64.3
57.8
67.8
60.8

Two

All

100
70.3
68.3
76.2
70.3
73.3
77.2

200
62.7
63.2
65.7
67.2
65.2
71.6

300 Mean
62.9
55.8
64.0
60.5
68.0
62.1
66.9
63.1
66.9
62.1
71.6
66.1

100
67.3
64.4
76.2
72.3
73.3
76.2

200
64.7
60.2
68.6
69.7
66.7
73.1

300 Mean
63.4
58.1
60.4
60.1
68.2
59.8
68.7
64.1
67.6
62.8
72.2
67.4

Fig.

4 shows the precision/recall curves
for each method. We can observe that:
(1)
CNN/PCNN+ATT signiﬁcantly outperforms all
feature-based methods over the entire range of re-
call. When the recall is greater than 0.1, the perfor-
mance of feature-based method drop out quickly.
In contrast, our model has a reasonable preci-
sion until the recall approximately reaches 0.3.
It demonstrates that the human-designed feature
cannot concisely express the semantic meaning of
the sentences, and the inevitable error brought by
NLP tools will hurt the performance of relation
extraction. In contrast, CNN/PCNN+ATT which
learns the representation of each sentences auto-
matically can express each sentence well.
(2)
PCNN+ATT performs much better as compared
to CNN+ATT over the entire range of recall.
It
means that the selective attention considers the
global information of all sentences except the in-
formation inside each sentence. Hence, the perfor-
mance of our model can be further improved if we
have a better sentence encoder.

4.6 Case Study

Table 3 shows two examples of selective at-
tention from the testing data. For each relation,
we show the corresponding sentences with high-
est and lowest attention weight respectively. And
we highlight the entity pairs with bold formatting.

From the table we ﬁnd that: The former exam-
ple is related to the relation employer of. The
sentence with low attention weight does not ex-
press the relation between two entities, while the
high one shows that Mel Karmazin is the chief ex-
ecutive of Sirius Satellite Radio. The later exam-
ple is related to the relation place of birth.
The sentence with low attention weight expresses
where Ernst Haeﬂiger is died in, while the high
one expresses where he is born in.

Table 3: Some examples of selective attention in
NYT corpus

Relation

employer of

Low When Howard Stern was prepar-
ing to take his talk show to Sirius
Satellite Radio, following his for-
mer boss, Mel Karmazin, Mr. Hol-
lander argued that ...

Relation
Low

High Mel Karmazin, the chief executive
of Sirius Satellite Radio, made a
lot of phone calls ...
place of birth
Ernst Haeﬂiger, a Swiss tenor
who ...
roles , died on Saturday
in Davos, Switzerland, where he
maintained a second home.
Ernst Haeﬂiger was born in Davos
on July 6, 1919, and studied at the
Wettinger Seminary ...

High

5 Conclusion and Future Works

In this paper, we develop CNN with sentence-
level selective attention. Our model can make full
use of all informative sentences and alleviate the
wrong labelling problem for distant supervised re-
lation extraction. In experiments, we evaluate our
model on relation extraction task. The experimen-
tal results show that our model signiﬁcantly and
consistently outperforms state-of-the-art feature-
based methods and neural network methods.

In the future, we will explore the following di-

rections:

• Our model incorporates multi-instance learn-
ing with neural network via instance-level se-
lective attention. It can be used in not only
distant supervised relation extraction but also
other multi-instance learning tasks. We will
explore our model in other area such as text

categorization.

• CNN is one of the effective neural net-
works for neural relation extraction. Re-
searchers also propose many other neural net-
In the
work models for relation extraction.
future, we will incorporate our instance-level
selective attention technique with those mod-
els for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61303075) and the Tsinghua University Initiative
Scientiﬁc Research Program (20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
In Proceedings of ACL, volume 45,
supervision.
page 576.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In Pro-
ceedings of NIPS, pages 2204–2212.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with compo-
In Proceedings of ACL.
sitional vector grammars.
Citeseer.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
nn: ﬁrst results. arXiv preprint arXiv:1412.1602.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artiﬁcial in-
telligence, 89(1):31–71.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image
caption generation with visual attention. Proceed-
ings of ICML.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

