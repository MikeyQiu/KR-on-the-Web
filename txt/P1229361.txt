7
1
0
2
 
r
p
A
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
2
v
6
2
8
8
0
.
6
0
5
1
:
v
i
X
r
a

Electronic Journal of Statistics
ISSN: 1935-7524
arXiv: http://arxiv.org/abs/1506.08826

Statistical Inference Using the
Morse-Smale Complex

Yen-Chi Chen, and Christopher R. Genovese, and Larry Wasserman

University of Washington,
Department of Statistics
Box 354322,
Seattle, WA 98195
e-mail: yenchic@uw.edu

Carnegie Mellon University,
Department of Statistics
5000 Forbes Avenue,
Pittsburgh, PA 15213
e-mail: genovese@stat.cmu.edu; larry@stat.cmu.edu

Abstract: The Morse-Smale complex of a function f decomposes the sam-
ple space into cells where f is increasing or decreasing. When applied to
nonparametric density estimation and regression, it provides a way to rep-
resent, visualize, and compare multivariate functions. In this paper, we
present some statistical results on estimating Morse-Smale complexes. This
allows us to derive new results for two existing methods: mode clustering
and Morse-Smale regression. We also develop two new methods based on
the Morse-Smale complex: a visualization technique for multivariate func-
tions and a two-sample, multivariate hypothesis test.

MSC 2010 subject classiﬁcations: Primary 62G20; secondary 62G86,
62H30.
Keywords and phrases: nonparametric estimation, mode clustering, non-
parametric regression, two sample test, visualization.

1. Introduction

Let f be a smooth, real-valued function deﬁned on a compact set K ∈ Rd. In
this paper, f will be a regression function or a density function. The Morse-
Smale complex of f is a partition of K based on the gradient ﬂow induced by f .
Roughly speaking, the complex consists of sets, called crystals or cells, comprised
of regions where f is increasing or decreasing. Figure 1 shows the Morse-Smale
complex for a two-dimensional function. The cells are the intersections of the
basins of attractions (under the gradient ﬂow) of the function’s maxima and
minima. The function f is piecewise monotonic over cells with respect to some
directions. In a sense, the Morse-Smale complex provides a generalization of
isotonic regression.

Because the Morse-Smale complex represents a multivariate function in terms
of regions on which the function has simple behavior, the Morse-Smale complex
has useful applications in statistics, including in clustering, regression, testing,
and visualization. For instance, when f is a density function, the basins of at-
traction of f ’s modes are the (population) clusters for density-mode clustering

1

Chen et al./Inference using the Morse-Smale

2

(a) Descending manifold

(b) Ascending manifold

(c) d-cell

(d) Morse-Smale complex

Fig 1. An example of a Morse-Smale complex. The green dots are local minima; the blue
dots are local modes; the violet dots are saddle points. Panels (a) and (b) give examples of
descending d-manifolds (blue region) and an ascending 0-manifold (green region). Panel (c)
shows the corresponding d-cell (yellow region). Panel (d) is shows all d-cells.

(also known as mean shift clustering (Fukunaga and Hostetler, 1975; Chac´on
et al., 2015)), each of which is a union of cells from the Morse-Smale complex.
Similarly, when f is a regression function, the cells of the Morse-Smale complex
give regions on which f has simple behavior. Fitting f over the Morse-Smale
cells provides a generalization of nonparametric, isotone regression; Gerber et al.
(2013) proposes such a method. The Morse-Smale representation of a multivari-
ate function f is a useful tool for visualizing f ’s structure, as shown by Gerber
et al. (2010). In addition, suppose we want to compare two multi-dimensional
datasets X = (X1, . . . , Xn) and Y = (Y1, . . . , Ym). We start by forming the
Morse-Smale complex of (cid:98)p − (cid:98)q where (cid:98)p is density estimate from X and (cid:98)q is den-
sity estimate from Y . Figure 2 shows a visualization built from this complex.
The circles represent cells of the Morse-Smale complex. Attached to each cell is
a pie-chart showing what fraction of the cell has (cid:98)p signiﬁcantly larger than (cid:98)q.
This visualization is a multi-dimensional extension of the method proposed for
two or three dimensions in Duong (2013).

For all these applications, the Morse-Smale complex needs to be estimated.
To the best of our knowledge, no theory has been developed for this estimation
problem, prior to this paper. We have three goals in this paper: to show that
many existing problems can be cast in terms of the Morse-Smale complex, to

Chen et al./Inference using the Morse-Smale

3

Fig 2. Graft-versus-Host Disease (GvHD) dataset (Brinkman et al., 2007). This is a d = 4
dimensional dataset. We estimate the density diﬀerence based on the kernel density estimator
and ﬁnd regions where the two densities are signiﬁcantly diﬀerent. Then we visualize the
density diﬀerence using the Morse-Smale complex. Each green circle denotes a d-cell, which
is a partition for the support K. The size of circle is proportional to the size of cell. If two
cells are neighborhors, we add a line connecting them; the thickness of the line denotes the
amount of boundary they share. The pie charts show the ratio of the regions within each cell
where the two densities are signiﬁcantly diﬀerent from each other. See Section 3.4 for more
details.

develop some new statistical methods based on the Morse-Smale complex, and
to develop the statistical theory for estimating the complex.

Main results. The main results of this paper are:

1. Consistency of the Morse-Smale Complex. We prove the stability of the
Morse-Smale complex (Theorem 1) in the following sense: if B and (cid:101)B are
boundaries of the descending d-manifolds (or ascending 0-manifolds) of p
and (cid:101)p (deﬁned in Section 2), then

Haus(B, (cid:101)B) = O ((cid:107)∇p − ∇(cid:101)p(cid:107)∞) .
2. Risk Bound for Mode clustering (mean-shift clustering; section 3.1): We

bound the risk of mode clustering in Theorem 2.

3. Morse-Smale regression (section 3.2): In Theorems 4 and 5, we bound the
risk of Morse-Smale regression, a multivariate regression method proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013)
that synthesizes nonparametric regression and linear regression.

4. Morse-Smale signatures (section 3.3): We introduce a new visualization

method for densities and regression functions.

5. Morse-Smale two-sample testing (section 3.4): We develop a new method

for multivariate two-sample testing that can have good power.

Related work. The mathematical foundations for the Morse-Smale complex

Chen et al./Inference using the Morse-Smale

4

Fig 3. A one dimensional example. The blue dots are local modes and the green dots are
local minima. Left panel: the basins of attraction for two local modes are colored by brown
and orange. Middle panel: the basin of attraction (negative gradient) for the local minima are
colored by red, purple and violet. Right panel: The intersection of the basins, which are called
d-cells.

are from Morse theory (Morse, 1925, 1930; Milnor, 1963). Morse theory has
many applications including computer vision (Paris and Durand, 2007), com-
putational geometry (Cohen-Steiner et al., 2007) and topological data analysis
(Chazal et al., 2014).

Previous work on the stability of the Morse-Smale complex can be found in
Chen et al. (2016) and Chazal et al. (2014) but they only consider critical points
rather than the whole Morse-Smale complex. Arias-Castro et al. (2016) prove
pointwise convergence for the gradient ascent curves but this is not suﬃcient
for proving the stability of the complex because the convergence of complexes
requires convergence of multiple curves and the constants in the convergence
rate derived from Arias-Castro et al. (2016) vary from points to points and some
constants diverge when we are getting closer to the boundaries of complexes.
Thus, we cannot obtain a uniform convergence of gradient ascent curves directly
based on their results. Morse-Smale regression and visualization were proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013).

The R code (Algorithm 1, 2, and 3) used in this paper can be found at

https://github.com/yenchic/Morse_Smale.

2. Morse Theory

To motivate formal deﬁnitions, we start with the simple, one-dimensional ex-
ample depicted in Figure 3. The left panel shows the sets associated with each
local maximum (i.e. the basins of attraction of the maxima). The middle panel
shows the sets associated with each local minimum. The right panel show the
intersections of these basins, which gives the Morse-Smale complex deﬁned by
the function. Each interval in the complex, called a cell, is a region where the
function is increasing or decreasing.

Now we give a formal deﬁnition. Let f : K ⊂ Rd (cid:55)→ R be a function with
bounded third derivatives that is deﬁned on a compact set K. Let g(x) = ∇f (x)
and H(x) = ∇∇f (x) be the gradient and Hessian matrix of f , respectively, and
let λj(x) be the jth largest eigenvalue of H(x). Deﬁne C = {x ∈ K : g(x) = 0}
to be the set of all f ’s critical points, which we call the critical set. Using the

Chen et al./Inference using the Morse-Smale

5

signs of the eigenvalues of the Hessian, the critical set C can be partitioned into
d + 1 distinct subsets C0, · · · , Cd, where

Ck = {x ∈ K : g(x) = 0, λk(x) > 0, λk+1(x) < 0},

k = 1, · · · , d − 1.

(1)

We deﬁne C0, Cd to be the sets of all local maxima and minima (corresponding
to all eigenvalues being negative and positive respectively). The set Ck is called
k−th order critical set.

A smooth function f is called a Morse function (Morse, 1925; Milnor, 1963)
if its Hessian matrix is non-degenerate at each critical point. That is, |λj(x)| >
0, ∀x ∈ C for all j. In what follows we assume f is a Morse function (actually,
later we will assume further that f is a Morse-Smale function).

Given any point x ∈ K, we deﬁne the gradient ascent ﬂow starting at x,

πx : R+ (cid:55)→ K, by

πx(0) = x
π(cid:48)
x(t) = g(π(t)).

(2)

A particle on this ﬂow moves along the gradient from x towards a “destination”
given by

dest(x) ≡ lim
t→∞
It can be shown that dest(x) ∈ C for x ∈ K.

πx(t).

We can thus partition K based on the value of dest(x). These partitions are
called descending manifolds in Morse theory (Morse, 1925; Milnor, 1963). Recall
Ck is the k-th order critical points, we assume Ck = {ck,1, · · · , ck,mk } contains
mk distinct elements. For each k, deﬁne

Dk = {x : dest(x) ∈ Cd−k}
Dk,j = {x : dest(x) = cd−k,j} ,

j = 1, · · · md−k.

(3)

That is, Dk is the collection of all points whose gradient ascent ﬂow converges to
a (d−k)-th order critical point and Dk,j is the collection of points whose gradient
ascent ﬂow converges to the j-th element of Cd−k. Thus, Dk = (cid:83)md−k
j=1 Dk,j. From
Theorem 4.2 in Banyaga and Hurtubise (2004), each Dk is a disjoint union
of k-dimensional manifolds (Dk,j is a k-dimensional manifold). We call Dk,j
a descending k-manifold of f . Each descending k-manifold is a k-dimensional
manifold such that the gradient ﬂow from every point converges to the same
(d − k)-th order critical point. Note that {D0, · · · , Dk} forms a partition of K.
The top panels of Figure 4 give an example of the descending manifolds for a
two dimensional case.

The ascending manifolds are similar to descending manifolds but are deﬁned
through the gradient descent ﬂow. More precisely, given any x ∈ K, a gradient
descent ﬂow γ : R+ (cid:55)→ K starting from x is given by

γx(0) = x
γ(cid:48)
x(t) = −g(π(t)).

(4)

Chen et al./Inference using the Morse-Smale

6

(a)

(b)

(c)

(d)

Fig 4. Two-dimensional examples of critical points, descending manifolds, ascending mani-
folds, and 2-cells. This is the same function as Figure 1. (a): The set Ck for k = 0, 1, 2. The
four blue dots are C0, the collection of local modes (each of them is c0,j some j = 1, · · · , 4).
The four orange dots are C1, the collection of saddle points (each of them is c1,j for some
j = 1, · · · , 4). The green dots are C2, the collection of local minima (each green dot is c2,j
for some j = 1, · · · , 9). (b): The set Dk for k = 0, 1, 2. The yellow area is D2 (each subregion
separated by blue curves are D2,j , j = 1, · · · , 4). The two blue curves are D1 (each of the 4
blue segments are D1,j , j = 1, · · · , 4). The green dots are D0 (also C2), the collection of local
minima (each green dot is D0,j for some j = 1, · · · , 9). (b): The set Ak for k = 0, 1, 2. The
yellow area is A0 (each subregion separated by red curves are A0,j , j = 1, · · · , 9). The two
red curves are A1 (each of the 4 red segments are A1,j , j = 1, · · · , 4). The blue dots are A2
(also C0), the collection of local modes (each green dot is A0,j for some j = 1, · · · , 4). (d):
Example for 2-cells. The thick blue curves are D1 and thick red curves are A1.

Chen et al./Inference using the Morse-Smale

7

Unlike the ascending ﬂow deﬁned in (2), γx is a ﬂow that moves along the
gradient descent direction. The descent ﬂow γx shares similar properties to the
ascent ﬂow πx; the limiting point limt→∞ γx(t) ∈ C is also in critical set when
f is a Morse function. Thus, similarly to Dk and Dk,j, we deﬁne

Ak =

x : lim
t→∞

γx(t) ∈ Cd−k

(cid:110)

(cid:110)

(cid:111)

(cid:111)

Ak,j =

x : lim
t→∞

γx(t) = cd−k,j

,

j = 1, · · · , mj−k.

(5)

Ak and Ak,j have dimension d − k and each Ak,j is a partition for Ak and
{A0, · · · , Ad} consist of a partition for K. We call each Ak,j an ascending k-
manifold to f .

A smooth function f is called a Morse-Smale function if it is a Morse function
and any pair of the ascending and descending manifolds of f intersect each
other transversely (which means that pairs of manifolds are not parallel at their
intersections); see e.g. Banyaga and Hurtubise (2004) for more details. In this
paper, we also assume that f is a Morse-Smale function. Note that by the
Kupka-Smale Theorem (see e.g. Theorem 6.6 in Banyaga and Hurtubise (2004)),
Morse-Smale functions are generic (dense) in the collection of smooth functions.
For more details, we refer to Section 6.1 in Banyaga and Hurtubise (2004).

A k-cell (also called Morse-Smale cell or crystal) is the non-empty intersection
between any descending k1-manifold and an ascending (d − k2)-manifold such
that k = min{k1, k2} (the ascending (d − k2)-manifold has dimension k2). When
we simply say a cell, we are referring to the d-cell since d-cells consists of the
majority of K (the totality of k-cells with k < d has Lebesgue measure 0). The
Morse-Smale complex for f is the collection of all k-cells for k = 0, · · · , d. The
bottom panels of Figure 4 give examples for the ascending manifolds and the
d-cells for d = 2. Another example is given in Figure 1.

The cells of a smooth function can be used to construct an additive de-
composition that is useful in data analysis. For a Morse-Smale function f , let
E1, · · · , EL be its associated cells. Then we can decompose f into

f (x) =

f(cid:96)(x)1(x ∈ E(cid:96)),

(6)

L
(cid:88)

(cid:96)=1

where each f(cid:96)(x) behaves like a multivariate isotonic function (Barlow et al.,
1972; Bacchetti, 1989). Namely, f (x) = f(cid:96)(x) when x ∈ E(cid:96). This decomposition
is because within each E(cid:96), f has exact a local mode and a local minimum on
the boundary of E(cid:96). The fact that f admits such a decomposition will be used
frequently in Section 3.2 and 3.3.

Among all descending/ascending manifolds, the descending d-manifolds and
the ascending 0-manifolds are often of great interest. For instance, mode cluster-
ing (Li et al., 2007; Azzalini and Torelli, 2007) uses the descending d-manifolds
to partition the domain K into clusters. Morse-Smale regression (Gerber and
Potter, 2011; Gerber et al., 2013) ﬁts a linear regression individually over each
d-cell (non-empty intersection of pairs of descending d-manifolds and ascending

Chen et al./Inference using the Morse-Smale

8

(a) Basins of attraction

(b) Gradient ascent

(c) Mode clustering

Fig 5. An example of mode clustering. (a): Basin of attraction for each local mode (red +).
Black dots are data points. (b): Gradient ﬂow (blue lines) for each data point. The gradient
ﬂow starts at one data point and ends at one local modes. (c): Mode clustering; we use the
destination for gradient ﬂow to cluster data points.

0-manifolds). Regions outside descending d-manifolds or ascending 0-manifolds
have Lebesgue measure 0. Thus, later in our theoretical analysis, we will focus
on the stability of the set Dd and A0 (see Section 4.1). We deﬁne boundaries of
Dd as

B ≡ ∂Dd = Dd−1 ∪ · · · ∪ D0.

(7)

The set B will be used frequently in Section 4.

3. Applications in Statistics

3.1. Mode Clustering

Mode clustering (Li et al., 2007; Azzalini and Torelli, 2007; Chac´on and Duong,
2013; Arias-Castro et al., 2016; Chac´on et al., 2015; Chen et al., 2016) is a
clustering technique based on the Morse-Smale complex and is also known as
mean-shift clustering (Fukunaga and Hostetler, 1975; Cheng, 1995; Comaniciu
and Meer, 2002). Mode clustering uses the descending d-manifolds of the density
function p to partition the whole space K. (Although the d-manifolds do not
contain all points in K, the regions outside d-manifolds have Lebesgue measure
0). See Figure 5 for an example.

Now, we brieﬂy describe the procedure of mode clustering. Let X = {X1, · · · , Xn}

be a random sample from density p deﬁned on a compact set K and assumed to
be a Morse function. Recall that dest(x) is the destination of the gradient ascent
ﬂow starting from x. Mode clustering partitions the sample based on dest(x)
for each point; speciﬁcally, it partitions X = X1

(cid:83) · · · (cid:83) XK such that

X(cid:96) = {Xi ∈ X : dest(Xi) = m(cid:96)},

where each m(cid:96) is a local mode of p. We can also view mode clustering as a cluster-
(cid:83) · · · (cid:83) Dd,L
ing technique based on the d-descending manifolds. Let Dd = Dd,1

Chen et al./Inference using the Morse-Smale

9

be the d-descending manifolds of p, assuming that L is the number of local
modes. Then each cluster X(cid:96) = X (cid:84) Dd,(cid:96).

In practice, however, we do not know p so we have to use a density estimator

(cid:98)pn. A common density estimator is the kernel density estimator (KDE):

(cid:98)pn(x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(8)

where K is a smooth kernel function and h > 0 is the smoothing parameter.
Note that mode clustering is not limited to the KDE; other density estimators
also give us a sample-based mode clustering. Based on the KDE, we are able to
estimate gradient (cid:98)gn(x), the gradient ﬂows (cid:98)πx(t), and the destination (cid:100)destn(x)
(note that the mean shift algorithm is an algorithm to perform these tasks).
Thus, we can estimate the d-descending manifolds by the plug-in from (cid:98)pn. Let
(cid:83) · · · (cid:83) (cid:98)Dd,(cid:98)L be the d-descending manifolds of (cid:98)pn, where (cid:98)L is the
(cid:98)Dd = (cid:98)Dd,1
number of local modes of (cid:98)pn. The estimated clusters will be (cid:98)X1, · · · , (cid:98)X
(cid:98)L, where
each (cid:98)X(cid:96) = X (cid:84) (cid:98)Dd,(cid:96). Figure 5 displays an example of mode clustering using the
KDE.

A nice property of mode clustering is that there is a clear population quan-
tity that our estimator (clusters based on the given sample) is estimating: the
population partition of the data points. Thus we can consider properties of the
procedure such as consistency, which we discuss in detail in Section 4.2.

3.2. Morse-Smale Regression

Let (X, Y ) be a random pair where Y ∈ R and Xi ∈ K ⊂ Rd. Estimating the
regression function m(x) = E[Y |X = x] is challenging for d of even moderate
size. A common way to address this problem is to use a simple regression function
that can be estimated with low variance. For example, one might use an additive
regression of the form m(x) = (cid:80)
j mj(xj) which is a sum of one-dimensional
smooth functions. Although the true regression function is unlikely to be of this
form, it is often the case that the resulting estimator is useful.

A diﬀerent approach, Morse-Smale regression (MSR), is suggested in Gerber
et al. (2013). This takes advantage of the (relatively) simple structure of the
Morse-Smale complex and the isotone behavior of the function on each cell.
Speciﬁcally, MSR constructs a piecewise linear approximation to m(x) over the
cells of the Morse-Smale complex.

We ﬁrst deﬁne the population version of the MSR. Let m(x) = E(Y |X = x)
be the regression function and is assumed to be a Morse-Smale function. Let
E1, · · · EL be the d-cells for m. The Morse-Smale Regression for m is a piecewise
linear function within each cell E(cid:96) such that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

(9)

Chen et al./Inference using the Morse-Smale

10

where (µ(cid:96), β(cid:96)) are obtained by minimizing mean square error:
E (cid:0)(Y − mMSR(X))2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1)

= argmin

E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(cid:1)

(10)

µ,β

µ,β

That is, mMSR is the best linear piecewise predictor using the d-cells. One can
also view MSR as using a linear function to approximate f(cid:96) in the additive
model (6). Note that mMSR is well deﬁned except on the boundaries of E(cid:96) that
have Lebesgue measure 0.

Now we deﬁne the sample version of the MSR. Let (X1, Y1), · · · , (Xn, Yn) be
the random sample from the probability measure PX × PY such that Xi ∈ K ⊂
Rd and Yi ∈ R. Throughout section 3.2, we assume the density of covariates X
is bounded, positive and has a compact support K and the response Y has ﬁnite
second moment.

Let (cid:98)mn be a smooth nonparametric regression estimator for m. We call (cid:98)mn
the pilot estimator. For instance, one may use the kernel regression Nadaraya
(1964) (cid:98)mn(x) =
(cid:98)mn as (cid:98)E1, · · · , (cid:98)E(cid:98)L. Using the data (Xi, Yi) within each estimated d-cell, (cid:98)E(cid:96), the
MSR for (cid:98)mn is given by

as the pilot estimator. We deﬁne d-cells for

i=1 YiK( x−Xi
h )
(cid:80)n
i=1 K( x−Xi
h )

(cid:80)n

(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where ((cid:98)µ(cid:96), (cid:98)β(cid:96)) are obtained by minimizing the empirical squared error:

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

(cid:88)

i:Xi∈ (cid:98)E(cid:96)

(Yi − µ − βT Xi)2

(11)

(12)

This MSR is slightly diﬀerent from the original version in Gerber et al. (2013).
We will discuss the diﬀerence in Remark 1. Computing the parameters of MSR
is not very diﬃcult–we only need to compute the cell labels of each observation
(this can be done by the mean shift algorithm or some fast variants such as the
quick-shift algorithm Vedaldi and Soatto 2008) and then ﬁt a linear regression
within each cell.

MSR may give low prediction error in some cases; see Gerber et al. (2013) for
some concrete examples. In Theorem 5, we prove that we may estimate mMSR at
a fast rate. Moreover, the regression function may be visualized by the methods
discussed later.

Remark 1 The original version of Morse-Smale regression proposed in Gerber
et al. (2013) does not use d-cells of a pilot nonparametric estimate (cid:98)mn. Instead,
they directly ﬁnd local modes and minima using the original data points (Xi, Yi).
This saves computational eﬀort but comes with a price: there is no clear popu-
lation quantity being estimated by their approach. That is, when the sample size
increases to inﬁnity, there is no guarantee that their method will converge. In
our case, we apply a consistent pilot estimate for m and construct d-cells on
this pilot estimate. As is shown in Theorem 4, our method is consistent for this
population quantity.

Chen et al./Inference using the Morse-Smale

11

3.3. Morse-Smale Signatures and Visualization

In this section we deﬁne a new method for visualizing multivariate functions
based on the Morse-Smale complex, called Morse-Smale signatures. The idea is
very similar to the Morse-Smale regression but the signatures can be applied to
any Morse-Smale function.

Let E1, · · · , EK be the d-cells (nonempty intersection of a descending d-
manifold and an ascending 0-manifold) for a Morse-Smale function f that has
a compact support K. The function f depends on the context of the problem.
For density estimation, f is the density p or its estimator (cid:98)pn. For regression
problem, f is the regression function m or a nonparametric estimator (cid:98)mn .
For two sample test, f is the density diﬀerence p1 − p2 or the estimated density
diﬀerence (cid:98)p1−(cid:98)p2. Note that E1, · · · , EK form a partition for K except a Lebesgue
measure 0 set. Each cell corresponds to a unique pair of a local mode and a local
minimum. Thus, the local modes and minima along with d-cells form a bipartite
graph which we call it signature graph. The signature graph contains geometric
information about f . See Figure 6 and 7 for examples.

The signature is deﬁned as follows. We project the maxima and minima of
the function into R2 using multidimensional scaling. We connect a maximum
and minimum by an edge if there exists a cell that connects them. The width
of the edge is proportional to the norm of the linear coeﬃcients of the linear
approximation to the function within the cell. The linear approximation is

fMS(x) = η†

(cid:96) + γ†T

(cid:96) x,

for x ∈ E(cid:96),

where η†

(cid:96) ∈ R and γ†

(cid:96) ∈ Rd are parameters from

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

(13)

(14)

This is again a linear approximation for f(cid:96) in the additive model (6). Note that
fMS may not be continuos when we move from one cell to another. The summary
statistics for the edge associated with cell E(cid:96) are the parameters (η†
(cid:96) ). We
call the function fMS the (Morse-Smale) approximation function; it is the best
piecewise-linear representation for f (piecewise linear within each cell) under L2
error given the d-cells. This function is well-deﬁned except on a set of Lebesgue
measure 0 (the boundaries of each cell). See Figure 6 for a example on the
approximation function. The details are in Algorithm 1.

(cid:96) , γ†

Example. Figure 7 is an example using the GvHD dataset. We ﬁrst conduct
multidimensional scaling (Kruskal, 1964) on the local modes and minima for f
and plot them on the 2-D plane. In Figure 7, the blue dots are local modes and
the green dots are local minima. These dots act as the nodes for the signature
graph. Then we add edges, representing the cells for f that connect pairs of local
modes and minima, to form the signature graph. Lastly, we adjust the width
for the edges according to the strength (L2 norm) of regression function within
each cell (i.e. (cid:107)γ†
(cid:96) (cid:107)). Algorithm 1 provides a summary for visualizing a general
multivariate function using what we described in this paragraph.

Chen et al./Inference using the Morse-Smale

12

(a) Original function

(b) Approximation function

(c) Signature graph

Fig 6. Morse-Smale signatures for a smooth function. (a): The original function. The blue
dots are local modes, the green dots are local minima and the pink dot is a saddle point. (b):
The Morse-Smale approximation to (a). This is the best piecewise linear approximation to the
original function given d-cells. (c): This bipartite graph has nodes that are local modes and
minima and edges that represent the d-cells. Note that we can summarize the smooth function
(a) by the signature graph (c) and the parameters for constructing approximation function
(b). The signature graph and parameters for approximation function deﬁne the Morse-Smale
signatures.

Algorithm 1 Visualization using Morse-Smale Signatures

Input: Grid points x1, · · · , xN and the functional evaluations f (x1), · · · , f (xN ).
1. Find local modes and minima of f on the discretized points x1, · · · , xN . Let M1, · · · MK
and m1, · · · , mS denote the grid points for modes and minima.
2. Partition {x1, · · · , xN } into X1, · · · XL according to the d-cells of f (1. and 2. can be done
by using a k-nearest neighbor gradient ascent/descent method; see Algorithm 1 in Gerber
et al. (2013)).
3. For each cell X(cid:96), ﬁt a linear regression with (Xi, Yi) = (xi, f (xi)), where xi ∈ X(cid:96). Let
the regression coeﬃcients (without intercept) be β(cid:96).
4. Apply multidimensional scaling to modes and minima jointly. Denote their 2 dimensional
representation points as

1 , · · · M ∗

5. Plot {M ∗
6. Add edge to a pair of mode and minimum if there exist a cell that connects them. The
width of the edge is in proportional to (cid:107)β(cid:96)(cid:107) (for cell X(cid:96)).

1, · · · , m∗

K , m∗

1 , · · · M ∗

K , m∗

1, · · · , m∗

S }.

{M ∗
S }.

Chen et al./Inference using the Morse-Smale

13

Fig 7. Morse-Smale Signature visualization (Algorithm 1) of the density diﬀerence for GvHD
dataset (see Figure 2). The blue dots are local modes; the green dots are local minima; the
brown lines are d-cells. These dots and lines form the signature graph. The width indicates
the L2 norm for the slope of regression coeﬃcients. i.e. (cid:107)γ†
(cid:96) (cid:107). The location for modes and
minima are obtained by multidimensional scaling so that the relative distance is preserved.

3.4. Two Sample Comparison

The Morse-Smale complex can be used to compare two samples. There are two
ways to do this. The ﬁrst one is to test the diﬀerence in two density functions
locally and then use the Morse-Smale signatures to visualize regions where the
two samples are diﬀerent. The second approach is to conduct a nonparametric
two sample test within each Morse-Smale cell. The advantage of the ﬁrst ap-
proach is that we obtain a visual display on where the two densities are diﬀerent.
The merit of the second method is that we gain additional power in testing the
density diﬀerence by using the shape information.

3.4.1. Visualizing the Density Diﬀerence

Let X1, . . . Xn and Y1, . . . , Ym be two random sample with densities pX and pY .
In a two sample comparison, we not only want to know if pX = pY but we also
want to ﬁnd the regions that they signiﬁcantly disagree. That is, we are doing
the local tests

H0(x) : pX (x) = pY (x)
(15)
simultaneously for all x ∈ K and we are interested in the regions where we reject
H0(x). A common approach is to estimate the density for both sample by the
KDE and set a threshold to pickup those regions that the density diﬀerence is

Chen et al./Inference using the Morse-Smale

14

large. Namely, we ﬁrst construct density estimates

(cid:98)pX (x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(cid:98)pY (x) =

1
mhd

m
(cid:88)

i=1

(cid:19)

(cid:18) x − Yi
h

K

and then compute (cid:98)f (x) = (cid:98)pX (x) − (cid:98)pY (x). The regions
(cid:111)
(cid:110)
x ∈ K : | (cid:98)f (x)| > λ

Γ(λ) =

(16)

(17)

are where we have strong evidence to reject H0(x). The threshold λ can be
picked by quantile values of the bootstrapped L∞ density deviation to control
type 1 error or can be chosen by controlling the false discovery rate (Duong,
2013).

Unfortunately, Γ(λ) is hard to visualize when d > 3. So we use the Morse-
Smale complex for (cid:98)f and visualize Γ(λ) by its behavior on the d-cells of the
complex. Algorithm 2 gives a method for visualizing density diﬀerences like
Γ(λ) in the context of comparing two independent samples.

Algorithm 2 Visualization For Two Sample Test

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, threshold λ and radius constant r0
1. Compute the density estimates (cid:98)pX and (cid:98)pY .
2. Compute the diﬀerence function (cid:98)f = (cid:98)pX − (cid:98)pY and the signiﬁcant regions
(cid:111)
x ∈ K : (cid:98)f (x) < −λ

x ∈ K : (cid:98)f (x) > λ

, Γ−(λ) =

Γ+(λ) =

(18)

(cid:111)

(cid:110)

(cid:110)

3. Find the d-cells for (cid:98)f , denoted as E1, · · · , EL.
4. For cell E(cid:96), do (4-1) and (4-2):
4-1. compute the cell center e(cid:96), cell size V(cid:96) = Vol(E(cid:96)),
4-2. compute the positive signiﬁcant ratio and negative signiﬁcant ratio

r+
(cid:96) =

Vol(E(cid:96) ∩ Γ+(λ))
Vol(E(cid:96))

,

r−
(cid:96) =

Vol(E(cid:96) ∩ Γ−(λ))
Vol(E(cid:96))

.

5. For every pair of cell Ej and E(cid:96) (j (cid:54)= (cid:96)), compute the shared boundary size:

Bj(cid:96) = Vold−1( ¯Ej ∩ ¯E(cid:96)),

(19)

(20)

where Vold−1 is the d − 1 dimensional Lebesgue measure.
6. Do multidimensional scaling (Kruskal, 1964) to e1, · · · , eL to obtain low dimensional
representation (cid:101)e1, · · · , (cid:101)eL.
7. Place a ball center at each (cid:101)e(cid:96) with radius r0 ×
8. If r+

(cid:96) > 0, add a pie chart center at (cid:101)e(cid:96) with radius r0 ×
.

(cid:18) r+
(cid:96) +r−
r+
9. Add a line to connect two nodes (cid:101)ej and (cid:101)e(cid:96) if Bj(cid:96) > 0. We may adjust the thickness of
the line according to Bj(cid:96).

chart contains two groups, each with ratio

r−
(cid:96)
(cid:96) +r−
r+

(cid:96) ). The pie

V(cid:96) × (r+

(cid:96) + r−

(cid:96) + r−

V(cid:96).

√

√

(cid:19)

,

(cid:96)

(cid:96)

(cid:96)

An example for Algorithm 2 is in Figure 2, in which we apply the visualization
algorithm for the the GvHD dataset by using kernel density estimator. We
choose the threshold λ by bootstrapping the L∞ diﬀerence for (cid:98)f i.e. supx | (cid:98)f ∗(x)−
(cid:98)f (x)|, where (cid:98)f ∗ is the density diﬀerence for the bootstrap sample. We pick
α = 95% upper quantile value for the bootstrap deviation as the threshold.

Chen et al./Inference using the Morse-Smale

15

The radius constant r0 is deﬁned by the user. It is a constant for visualiza-
tion and does not aﬀect the analysis. Algorithm 2 preserves the relative position
for each cell and visualizes the cell according to its size. The pie-chart provides
the ratio of regions where the two densities are signiﬁcantly diﬀerent. The lines
connecting two cells provide the geometric information about how cells are con-
nected to each other.

By applying Algorithm 2 to the GvHD dataset (Figure 2), we ﬁnd that there
are 6 cells and one cell much larger than the others. Moreover, in most regions,
the blue regions are larger than the red areas. This indicates that compared
to the density of the control group, the density of the GvHD group seem to
concentrates more so that the regions above the threshold are larger.

3.4.2. Morse-Smale Two-Sample Test

Here we introduce a technique combining the energy test (Baringhaus and Franz,
2004; Sz´ekely and Rizzo, 2004, 2013) and the Morse-Smale complex to conduct
a two sample test. We call our method the Morse-Smale Energy test (MSE test).
The advantage of the MSE test is that it is a nonparametric test and its power
can be higher than the energy test; see Figure 8. Moreover, we can combine
our test with the visualization tool proposed in the previous section (Algorithm
2); see Figure 9 for an example for displaying p-values from MSE test when
visualizing the density diﬀerence.

Before we introduce our method, we ﬁrst review the ordinary energy test.
Given two random variables X ∈ Rd and Y ∈ Rd, the energy distance is deﬁned
as

E(X, Y ) = 2E(cid:107)X − Y (cid:107) − E(cid:107)X − X (cid:48)(cid:107) − E(cid:107)Y − Y (cid:48)(cid:107),
where X (cid:48) and Y (cid:48) are iid copies of X and Y . The energy distance has several
useful applications such as the goodness-of-ﬁt testing (Sz´ekely and Rizzo, 2005),
two sample testing (Baringhaus and Franz, 2004; Sz´ekely and Rizzo, 2004, 2013),
clustering (Szekely and Rizzo, 2005), and distance components (Rizzo et al.,
2010) to name but few. We recommend an excellent review paper in (Sz´ekely
and Rizzo, 2013).

(21)

For the two sample test, let X1, · · · , Xn and Y1, · · · , Ym be the two samples

we want to test. The sample version of energy distance is

(cid:98)E(X, Y ) =

(cid:107)Xi − Yj(cid:107) −

(cid:107)Xi − Xj(cid:107) −

(cid:107)Yi − Yj(cid:107).

2
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

(22)
P→ 0.
If X and Y are from the sample population (the same density), (cid:98)E(X, Y )
Numerically, we use the permutation test for computing the p-value for (cid:98)E(X, Y ).
This can be done quickly in the R-package ‘energy’ (Rizzo and Szekely, 2008).
Now we formally introduce our testing procedure: the MSE test (see Algo-
rithm 3 for a summary). Our test consists of three steps. First, we split the data
into two halves. Second, we use one half of the data (contains both samples)

Chen et al./Inference using the Morse-Smale

16

to do a nonparametric density estimation (e.g. the KDE) and then compute
the Morse-Smale complex (d-cells). Last, we use the other half of the data to
conduct the energy distance two sample test ‘within each d-cell’. That is, we
partition the second half of the data by the d-cells. Within each cell, we do the
energy distance test. If we have L cells, we will have L p-values from the energy
distance test. We reject H0 if any one of the L p-values is smaller than α/L
(this is from Bonferroni correction). Figure 9 provides an example for using the
above procedure (Algorithm 3) along with the visualization method proposed
in Algorithm 2. Data splitting is used to avoid using the same data twice, which
ensures we have a valid test.

Algorithm 3 Morse-Smale Energy Test (MSE test)

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, smoothing parameter h, signiﬁcance
level α
1. Randomly split the data into halves D1 and D2; both contain equal number of X and Y
(assuming n and m are even).
2. Compute the KDE (cid:98)pX and (cid:98)pY by the ﬁrst sample D1.
3. Find the d-cells for (cid:98)f = (cid:98)pX − (cid:98)pY , denoted as E1, · · · , EL.
4. For cell E(cid:96), do 4-1 and 4-2:
4-1. Find X and Y in the second sample D2,
4-2. Do the energy test for two sample comparison. Let the p-value be p((cid:96))
5. Reject H0 if p((cid:96)) < α/L for some (cid:96).

Example. Figure 8 shows a simple comparison for the proposed MSE test to
the usual Energy test. We consider a K = 4 Gaussian mixture model in d = 2
with standard deviation of each component being the same σ = 0.2 and the
proportion for each component is (0.2, 0.5, 0.2, 0.1). The left panel displays a
sample with N = 500 from this mixture distribution. We draw the ﬁrst sample
from this Gaussian mixture model. For the second sample, we draw a similar
Gaussian mixture model except that we change the deviation of one component.
In the middle panel, we change the deviation to the third component (C3 in
left panel, which contains 20% data points). In the right panel, we change the
deviation to the fourth component (C4 in left panel, which contains 10% data
points). We use signiﬁcance level α = 0.05 and for MSE test, we consider the
Bonferroni correction and the smoothing bandwidth is chosen using Silverman’s
rule of thumb (Silverman, 1986).

Note that in both the middle and the right panels, the left most case (added
deviation equals 0) is where H0 should not be rejected. As can be seen from
Figure 8, the MSE test has much stronger power compared to the usual Energy
test.

The original energy test has low power while the MSE test has higher power.
This is because the two distributions only diﬀer at a small portion of the regions
so that a global test like energy test requires large sample sizes to detect the
diﬀerence. On the other hand, the MSE test partitions the space according to
the density diﬀerence so that it is capable of detecting the local diﬀerence.

Example. In addition to the higher power, we may combine the MSE test
with the visualization tool in Algorithm 2. Figure 9 displays an example where

Chen et al./Inference using the Morse-Smale

17

Fig 8. An example comparing the Morse-Smale Energy test to the original Energy test. We
consider a d = 2, K = 4 Gaussian mixture model. Left panel: an instance for the Gaussian
mixture. We have four mixture components, denoting as C1, C2, C3 and C4. They have equal
standard deviation (σ = 0.2) and the proportions for each components are (0.2, 0.5, 0.2, 0.1).
Middle panel: We changed the standard deviations of component C3 to 0.3, 0.4 and 0.5 and
compute the power for the MSE test and the usual Energy test at sample size N = 500 and
1000. (Standard deviation equals 0.2 is where H0 should not be rejected.) Right panel: We
add the variance of component C4 (the smallest component) and do the same comparison as
in the middle panel. We pick the signiﬁcance level α = 0.05 (gray horizontal line) and in the
MSE test, we reject H0 if the minimal p-value is less than α/L, where L is the number of
cells (i.e. we are using the Bonferroni correction).

we visualize the density diﬀerence and simultaneously indicate the p-values from
the Energy test within each cell using the GvHD dataset. This provides us more
information about how two distributions diﬀer from each other.

4. Theoretical Analysis

We ﬁrst deﬁne some notation for the theoretical analysis. Let f be a smooth
function. We deﬁne (cid:107)f (cid:107)∞ = supx |f (x)| to be the L∞-norm of f . In addition, let
(cid:107)f (cid:107)j,max denote the elementwise L∞-norm for j-th derivatives of f . For instance,

(cid:107)f (cid:107)1,max = max

(cid:107)gi(x)(cid:107)∞,

i

(cid:107)f (cid:107)2,max = max
i,j

(cid:107)Hij(x)(cid:107)∞.

We also deﬁne (cid:107)f (cid:107)0,max = (cid:107)f (cid:107)∞. We further deﬁne

(cid:107)f (cid:107)∗
The quantity (cid:107)f − h(cid:107)∗
h up to (cid:96)-th order derivative.

(cid:96),max = max {(cid:107)f (cid:107)j,max : j = 0, · · · , (cid:96)} .
(cid:96),max measures the diﬀerence between two functions f and

(23)

For two sets A, B, the Hausdorﬀ distance is

Haus(A, B) = inf{r : A ⊂ B ⊕ r, B ⊂ A ⊕ r},

(24)

where A ⊕ r = {y : minx∈A (cid:107)x − y(cid:107) ≤ r}. The Hausdorﬀ distance is like the L∞
distance for sets.

Let (cid:101)f : K ⊂ Rd (cid:55)→ R be a smooth function with bounded third derivatives.
Note that as long as (cid:107) (cid:101)f −f (cid:107)∗
3,max is small, (cid:101)f is also a Morse function by Lemma 9.
Let (cid:101)D denote the boundaries of the descending d-manifolds of (cid:101)f . We will show
if (cid:107)f − (cid:101)f (cid:107)∗

3,maxis suﬃciently small, then Haus( (cid:101)D, D) = O((cid:107) (cid:101)f − f (cid:107)1,max).

Chen et al./Inference using the Morse-Smale

18

Fig 9. An example using both Algorithm 2 and 3 to the GvHD dataset introduced in Figure 2.
We use data splitting as described in Algorithm 3. For the ﬁrst part of the data, we compute
the cells and visualize the cells using Algorithm 2. Then we apply the energy distance two
sample test for each cell as described in Algorithm 3 and we annotate each cell with a p-
value. Note that the visualization is slightly diﬀerent to Figure 2 since we use only half of the
original dataset in this case.

4.1. Stability of the Morse-Smale Complex

Before we state our theorem, we ﬁrst derive some properties of descending mani-
folds. Recall that we are interested in B = ∂Dd, the boundary of the descending
d-manifolds (and B is also the union of all j-descending manifolds for j < d).
Since each Dj is a collection of smooth j-dimensional manifolds embedded in
Rd, for every x ∈ Dj, there exists a basis v1(x), · · · , vd−j(x) such that each vk(x)
is perpendicular to Dj at x for k = 1, · · · d − j (Bredon, 1993; Helgason, 1979).
That is, v1(x), · · · , vd−j(x) span the normal space to Dj at x. For simplicity, we
write

V (x) = (v1(x), · · · , vd−j(x)) ∈ Rd×(d−j)

(25)

for x ∈ Dj.

Note the number of columns d − j ≡ d − j(x) in V (x) depends on which Dj
the point x belongs to. We use j rather than j(x) to simplify the notation. For
instance, if x ∈ D1, V (x) ∈ Rd×(d−1) and if x ∈ Dd−1, V (x) ∈ Rd×1. We also
let

V(x) = span{v1(x), · · · , vd−j(x)}
(26)
denote the normal space to B at x. One can view V(x) as the normal map of
the manifold Dj at x ∈ Dj.

For each x ∈ B, deﬁne the projected Hessian

HV (x) = V (x)T H(x)V (x),

(27)

which is the Hessian matrix of p by taking gradients along column space of
V (x). If x ∈ Dj, HV (x) is a (d − j) × (d − j) matrix. The eigenvalues of HV (x)

Chen et al./Inference using the Morse-Smale

19

determine how the gradient ﬂows are moving away from B. We let λmin(M ) be
the smallest eigenvalue for a symmetric matrix M . If M is a scalar (just one
point), then λmin(M ) = M .

Assumption (D): We assume that Hmin = minx∈B λmin(HV (x)) > 0.

This assumption is very mild; it requires that the gradient ﬂow moves away
from the boundary of ascending manifolds. In terms of mode clustering, this
requires the gradient ﬂow to move away from the boundaries of clusters. For a
point x ∈ Dd−1, let v1(x) be the corresponding normal direction. Then the gradi-
ent g(x) is normal to v1(x) by deﬁnition. That is, v1(x)T g(x) = v1(x)T ∇p(x) =
0, which means that the gradient along v1(x) is 0. Assumption (D) means that
the the second derivative along v1(x) is positive, which implies that the density
along direction v1(x) behaves like a local minimum at point x. Intuitively, this
is how we expect the density to behave around the boundaries: gradient ﬂows
are moving away from the boundaries (except for those ﬂows that are already
on the boundaries).
Theorem 1 (Stability of descending d-manifolds) Let f, (cid:101)f : K ⊂ Rd (cid:55)→ R
be two smooth functions with bounded third derivatives deﬁned as above and
let B, (cid:101)B be the boundaries of the associated ascending manifolds. Assume f is
a Morse function satisfying condition (D). When (cid:107)f − (cid:101)f (cid:107)∗
3,max is suﬃciently
small,

Haus( (cid:101)B, B) = O((cid:107) (cid:101)f − f (cid:107)1,max).

(28)

This theorem shows that the boundaries of descending d-manifolds for two Morse
functions are close to each other and the diﬀerence between the boundaries is
controlled by the rate of the ﬁrst derivative diﬀerence.

Similarly to descending manifolds, we can deﬁne all the analogous quantities

for ascending manifolds. We introduce the following assumption:

Assumption (A): We assume Hmax = maxx∈∂A0 λmax(HV (x)) < 0.

Note that λmax(M ) denotes the largest eigenvalue of a matrix M . If M is a
scalar, λmax(M ) = M . Under assumption (A), we have a similar stability result
(Theorem 1) for ascending manifolds. Assumptions (A) and (D) together imply
the stability of d-cells.

Theorem 1 can be applied to nonparametric density estimation. Our goal is to
estimate the boundary of the descending d-manifolds, B, of the unknown popu-
lation density function p. Our estimator is (cid:98)Bn, the boundary of the descending

Chen et al./Inference using the Morse-Smale

20

d-manifolds to a nonparametric density estimator e.g. the kernel density esti-
mate (cid:98)pn. Then under certain regularity condition, their diﬀerence is given by

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107)(cid:98)pn − p(cid:107)1,max) .

We will see this result in the next section when we discuss mode clustering.

Similar reasoning works for the nonparametric regression case. Assume that
we are interested in B, the boundary of descending d-manifolds, for the regres-
sion function m(x) = E(Y |X = x). And our estimator (cid:98)B is again a plug-in
estimate based on (cid:98)mn(x), a nonparametric regression estimator (e.g., kernel
estimator). Then under mild regularity conditions,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107) (cid:98)mn − m(cid:107)1,max) .

4.2. Consistency of Mode Clustering

A direct application of Theorem 1 is the consistency of mode clustering. Let
K (α) be the α-th derivative of K and let BCr denote the collection of functions
with bounded continuously derivatives up to the r-th order. We consider the
following two assumptions on the kernel function:
(K1) The kernel function K ∈ BC3 and is symmetric, non-negative and

(cid:90)

x2K (α)(x)dx < ∞,

(cid:90) (cid:16)

(cid:17)2

K (α)(x)

dx < ∞

for all α = 0, 1, 2, 3.

(K2) The kernel function satisﬁes condition K1 of Gine and Guillou (2002). That

is, there exists some A, v > 0 such that for all 0 < (cid:15) < 1, supQ N (K, L2(Q), CK(cid:15)) ≤
(cid:0) A
, where N (T, d, (cid:15)) is the (cid:15)−covering number for a semi-metric space
(cid:15)

(cid:1)v

(T, d) and

(cid:40)

K =

u (cid:55)→ K (α)

: x ∈ Rd, h > 0, |α| = 0, 1, 2

(cid:19)

(cid:18) x − u
h

(cid:41)
.

(K1) is a common assumption; see Wasserman (2006). (K2) is a weak assump-
tion guarantee the consistency for KDE under L∞ norm; this assumption ﬁrst
appeared in Gine and Guillou (2002) and has been widely assumed (Einmahl
and Mason, 2005; Rinaldo et al., 2010; Genovese et al., 2012; Rinaldo et al.,
2012; Genovese et al., 2014; Chen et al., 2015).
Theorem 2 (Consistency for mode clustering) Let p, (cid:98)pn be the density func-
tion and the KDE. Let B and (cid:98)Bn be the boundaries of clusters by mode clus-
tering over p and (cid:98)pn respectively. Assume (D) for p and (K1–2), then when
log n
nhd+6 → 0, h → 0,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O((cid:107)(cid:98)pn − p(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Chen et al./Inference using the Morse-Smale

21

The proof is simply to combine Theorem 1 and the rate of convergence for
estimating the gradient of density using KDE (Theorem 8). Thus, we omit the
proof. Theorem 2 gives a bound for the rate of convergence for the boundaries
for mode clustering. The rate can be decomposed into two parts, the bias O(h2)

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

and the (square root of) variance OP

. This rate is the same for the

L∞-loss of estimating the gradient of a density function, which makes sense
since the mode clustering is completely determined by the gradient of density.
Another way to describe the consistency for mode clustering is to show that
the proportion of data points that are incorrectly clustered (mis-clustered) con-
verges to 0. This can be quantiﬁed by the use of Rand index (Rand, 1971; Hubert
and Arabie, 1985; Vinh et al., 2009), which measures the similarity between two
partitions of the data points. Let dest(x) and (cid:100)destn(x) be the destination of
gradient of the true density function p(x) and the KDE (cid:98)pn(x). For a pair of
points x, y, we deﬁne

Ψ(x, y) =

(cid:26) 1
0

if dest(x) = dest(y)
if dest(x) (cid:54)= dest(y)

,

(cid:98)Ψn(x, y) =

(cid:40)

1
0

if (cid:100)destn(x) = (cid:100)destn(y)
if (cid:100)destn(x) (cid:54)= (cid:100)destn(y)

(29)
Thus, Ψ(x, y) = 1 if x, y are in the same cluster and 0 if they are not. The Rand
index for mode clustering using p versus using (cid:98)pn is

rand ((cid:98)pn, p) = 1 −

(cid:19)−1

(cid:18)n
2

(cid:88)

i(cid:54)=j

(cid:12)
(cid:12)
(cid:12)Ψ(Xi, Xj) − (cid:98)Ψn(Xi, Xj)

(cid:12)
(cid:12)
(cid:12) ,

(30)

which is the proportion of pairs of data points that the two clustering results
disagree on. If two clusterings output the same partition, the Rand index will
be 1.

Theorem 3 (Bound on Rand Index) Assume (D) for p and (K1–2). Then
when log n

nhd+6 → 0, h → 0, the adjusted Rand index

rand ((cid:98)pn, p) = 1 − O(h2) − OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Theorem 3 shows that the Rand index converges to 1 in probability, which
establishes the consistency of mode clustering in an alternative way. Theo-
rem 3 shows that the proportion of data points that are incorrectly assigned
(compared with mode clustering using population p) is bounded by the rate

O(h2) + OP

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

asymptotically.

Azizyan et al. (2015) also derived the convergence rate of the mode clustering
for the rand index. Here we brieﬂy compare our results to theirs. Azizyan et al.
(2015) consider a low-noise condition that leads to a fast convergence rate when
clusters are well-separated. Their approach can even be applied to the case of

(31)

(32)

(33)

(34)

Chen et al./Inference using the Morse-Smale

22

increasing dimensions. In our case (Theorem 3), we consider a ﬁxed dimension
scenario but we do not assume the low-noise condition. Thus, the main diﬀerence
between Theorem 3 and the result in Azizyan et al. (2015) is the assumptions
being made so our result complements the ﬁndings in Azizyan et al. (2015).

4.3. Consistency of Morse-Smale Regression

In what follows, we will show that (cid:98)mn,MSR(x) is a consistent estimator of mMSR(x).
Recall that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

where E(cid:96) is the d-cell deﬁned on m and the parameters are
E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1) .

µ,β

And (cid:98)mn,MSR is the two-stage estimator to mMSR(x) deﬁned by
(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where { (cid:98)E(cid:96) : (cid:96) = 1, · · · , (cid:98)L} are the collection of cells of the pilot nonparametric
regression estimator (cid:98)mn and (cid:98)µ(cid:96), (cid:98)β(cid:96) are the regression parameters from equation
(12):
(cid:88)

(Yi − µ − βT Xi)2.

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

i:Xi∈ (cid:98)E(cid:96)
Theorem 4 (Consistency of Morse-Smale Regression) Assume (A) and
(D) for m and assume m is a Morse-Smale function. Then when log n
nhd+6 →
0, h → 0, we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

+ O ((cid:107) (cid:98)mn − m(cid:107)1,max)

(35)

(cid:19)

(cid:18) 1
√
n

uniformly for all x except for a set Nn with Lebesgue measure OP((cid:107) (cid:98)mn−m(cid:107)1,max),
Theorem 4 states that when we have a consistent pilot nonparametric regression
estimator (such as the kernel regression), the proposed MSR estimator converges
to the population MSR. Similarly as in Theorem 6, the set Nn are regions around
the boundaries of cells where we cannot distinguish their host cell. Note that
when we use the kernel regression as the pilot estimator (cid:98)mn, Theorem 4 becomes

|mMSR(x) − (cid:98)mn,MSR(x)| = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

under regular smoothness conditions.

Now we consider a special case where we may obtain parametric rate of
(cid:83) · · · (cid:83) EL) be the boundaries

convergence for estimating mMSR. Let E = ∂ (E1
of all cells. We consider the following low-noise condition:

P (X ∈ E ⊕ (cid:15)) ≤ A(cid:15)β,

(36)

Chen et al./Inference using the Morse-Smale

23

for some A, β > 0. Equation (36) is Tsybakov’s low noise condition (Audibert
et al., 2007) applied to the boundaries of cells. Namely, (36) states that it is
unlikely to many observations near the boundaries of cells of m. Under this
low-noise condition, we obtain the following result using kernel regression.

Theorem 5 (Fast Rate of Convergence for Morse-Smale Regression)
Let the pilot estimator (cid:98)mn be the kernel regression estimator. Assume (A)
and (D) for m and assume m is a Morse-Smale function. Assume also (36)
holds for the covariate X and (K1-2) for the kernel function. Also assume that
. Then uniformly for all x except for a set Nn with

(cid:17)1/(d+6)(cid:19)

(cid:18)(cid:16) log n

h = O

n

Lebesgue measure OP

(cid:18)(cid:16) log n

(cid:17)2/(d+6)(cid:19)

,

n

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(cid:19)

(cid:18) 1
√
n

+ OP

(cid:32)(cid:18) log n
n

(cid:19)2β/(d+6)(cid:33)

.

(37)

Therefore, when β > 6+d

4 , we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(38)

(cid:18) 1
√
n

(cid:19)

.

Theorem 5 shows that when the low noise condition holds, we obtain a fast
rate of convergence for estimating mMSR. Note that the pilot estimator (cid:98)mn does
not ahve to be a kernel estimator; other approaches such as the local polynomial
regression will also work.

4.4. Consistency of the Morse-Smale Signature

Another application of Theorem 1 is to bound the diﬀerence of two Morse-
Smale signatures. Let f be a Morse-Smale function with cells E1, . . . , EL. Recall
that the Morse-Smale signatures are the bipartite graph and summary statistics
(locations, density values) for local modes, local minima, and cells. It is known in
the literature (see, e.g., Lemma 9) that when two functions (cid:101)f , f are suﬃciently
close, then

(cid:16)

(cid:17)

max
j

(cid:107)(cid:101)cj − cj(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)1,max

, max

(cid:107) (cid:101)f ((cid:101)cj) − f (cj)(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)∞

j

(39)
where (cid:101)cj, cj are critical points (cid:101)f and f respectively. This implies the stability of
local modes and minima.

So what we need is the stability of the summary statistics (η†

(cid:96) , γ†

(cid:96) ) associated

with the edges (cells). Recall that these summaries are deﬁned through (14)

(cid:16)

(cid:17)

,

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Chen et al./Inference using the Morse-Smale

24

For another function (cid:101)f , let ((cid:101)η†
(cid:96) ) be its signatures for cell (cid:101)E(cid:96). The following
theorem shows that if two functions are close, their corresponding Morse-Smale
signatures are also close.

(cid:96) , (cid:101)γ†

Theorem 6 Let f be a Morse-Smale function satisfying assumptions A and D,
and let (cid:101)f be a smooth function. Then when log n
nhd+6 → 0, h → 0, after relabeling
the indices of cells of (cid:101)f ,

(cid:110)
(cid:107)(cid:101)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:101)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

1,max

(cid:17)

.

Theorem 6 shows stability of the signatures (η†

(cid:96) , γ†

(cid:96) ). Note that Theorem 6

also implies that the stability of piecewise approximation

|fMS(x) − (cid:101)fMS(x)| = O

(cid:16)

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

Together with the stability of critical points (39), Theorem 6 proves the stability
of Morse-Smale signatures.

4.4.1. Example: Morse-Smale Density Estimation

As an example for Theorem 6, we consider density estimation. Let p be the
density of random sample X1, · · · , Xn and recall that (cid:98)pn is the kernel density
estimator. Let (η†
(cid:96) ) be the
signature for (cid:98)pn under cell (cid:98)E(cid:96). The following corollary guarantees the consistency
of Morse-Smale signatures for the KDE.

(cid:96) ) be the signature for p under cell E(cid:96) and ((cid:98)η†

(cid:96) , (cid:98)γ†

(cid:96) , γ†

Corollary 7 Assume (A,D) holds for p and the kernel function satisﬁes (K1–
2). Then when log n

nhd+6 → 0, h → 0, after relabeling we have
(cid:32)(cid:114)

(cid:110)
(cid:107)(cid:98)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:98)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

= O(h2) + OP

(cid:33)

.

log n
nhd+2

The proof to Corollary 7 is a simple application of Theorem 6 with the rate of
convergence for the ﬁrst derivative of the KDE (Theorem 8). So we omit the

proof. The optimal rate in Corollary 7 is OP

when we choose h to

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 2

n

be of order O

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 1

.

n

Remark 2 When we compute the Morse-Smale approximation function, we
may have some numerical problem in low-density regions because the density
estimate (cid:98)pn may have unbounded support. In this case, some cells may be un-
bounded, and the majority of these cells may have extremely low density value,
which makes the approximation function 0. Thus, in practice, we will restrict

Chen et al./Inference using the Morse-Smale

25

ourselves only to the regions whose density is above a pre-deﬁned threshold λ so
that every cell is bounded. A simple data-driven threshold is λ = 0.05 supx (cid:98)pn(x).
Note that Theorem 7 still works in this case but with a slight modiﬁcation: the
cells are deﬁne on the regions {x : ph(x) ≥ 0.05 × supx ph(x)}.

Remark 3 Note that for a density function, local minima may not exist or the
gradient ﬂow may not lead us to a local minimum in some regions. For instance,
for a Gaussian distribution, there is no local minimum and except for the center
of the Gaussian, if we follow the gradient descent path, we will move to inﬁnity.
Thus, in this case we only consider the boundaries of ascending 0-manifolds
corresponding to well-deﬁned local minima and assumptions (A) is only for the
boundaries corresponding to these ascending manifolds.

Remark 4 When we apply the Morse-Smale complex to nonparametric density
estimation or regression, we need to choose the tuning parameter. For instance,
in the MSR, we may use kernel regression or local polynomial regression so we
need to choose the smoothing bandwidth. For the density estimation problem
or mode clustering, we need to choose the smoothing bandwidth for the kernel
smoother. In the case of regression, because we have the response variable, we
would recommend to choose the tuning parameter by cross-validation. For the
kernel density estimator (and mode clustering), because the optimal rate depends
on the gradient estimation, we recommend choosing the smoothing bandwidth
using the normal reference rule for gradient estimation or the cross-validation
method for gradient estimation (Duong et al., 2007; Chac´on et al., 2011).

5. Discussion

In this paper, we introduced the Morse-Smale complex and the summary sig-
natures for nonparametric inference. We demonstrated that the Morse-Smale
complex can be applied to various statistical problems such as clustering, re-
gression and two sample comparisons. We showed that a smooth multivariate
function can be summarized by a few parameters associated with a bipartite
graph, representing the local modes, minima and the complex for the underly-
ing function. Moreover, we proved a fundamental theorem about the stability
of the Morse-Smale complex. Based on the stability theorem, we derived con-
sistency for mode clustering and regression.

The Morse-Smale complex provides a method to synthesize both paramet-
ric and nonparametric inference. Compared to parametric inference, we have a
more ﬂexible model to study the structure of the underlying distribution. Com-
pared to nonparametric inference, the use of the Morse-Smale complex yields a
visualizable representation for the underlying multivariate structures. This re-
veals that we may gain additional insights in data analysis by using geometric
features.

Although the Morse-Smale complex has many potential statistical applica-
tions, we need to be careful when applying it to a data set whose dimension
is large (say d > 10). When the dimension is large, the curse of dimensionality

Chen et al./Inference using the Morse-Smale

26

kicks in and the nonparametric estimators (in both density estimation problems
or regression analysis) are not accurate so the errors of the estimated Morse-
Smale complex can be huge.

Here we list some possible extensions for future research:

• Asymptotic distribution. We have proved the consistency (and the rate of
convergence) for estimating the complex but the limiting distribution is
still unknown. If we can derive the limiting distribution and show that
some resampling method (e.g. the bootstrap Efron (1979)) converges to
the same distribution, we can construct conﬁdence sets for the complex.
• Minimax theory. Despite the fact that we have derived the rate of con-
vergence for a plug-in estimator for the complex, we did not prove its
optimality. We conjecture the minimax rate for estimating the complex
should be related to the rate for estimating the gradient and the smooth-
ness around complex (Audibert et al., 2007; Singh et al., 2009).

Acknowledgement

We thank the referees and the Associate Editor for their very constructive com-
ments and suggestions.

Appendix A: Appendix: Proofs

First, we include a Theorem about the rate of convergence for the kernel density
estimator. This Lemma will be used in deriving the convergence rates.

Theorem 8 (Lemma 10 in Chen et al. (2015); see also Genovese et al. (2014))
Assume (K1–2) and that log n/n ≤ hd ≤ b for some 0 < b < 1. Then we have

(cid:107)(cid:98)pn − p(cid:107)∗

(cid:96),max = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

log n
nhd+2(cid:96)

for (cid:96) = 0, 1, 2.

of critical points.

To prove Theorem 1, we introduce the following useful Lemma for stability

Lemma 9 (Lemma 16 of Chazal et al. (2014)) Let p be a density with com-
pact support K of Rd. Assume p is a Morse function with ﬁnitely many, distinct,
critical values with corresponding critical points C = {c1, · · · , ck}. Also assume
that p is at least twice diﬀerentiable on the interior of K, continuous and dif-
ferentiable with non vanishing gradient on the boundary of K. Then there exists
(cid:15)0 > 0 such that for all 0 < (cid:15) < (cid:15)0 the following is true: for some positive
constant c, there exists η ≥ c(cid:15)0 such that, for any density q with support K
satisfying (cid:107)p − q(cid:107)∗

2,max ≤ η, we have

1. q is a Morse function with exact k critical points c(cid:48)

1, · · · , c(cid:48)

k and

Chen et al./Inference using the Morse-Smale

27

Fig 10. Diagram for lemmas and Theorem 1.

2. after suitable relabeling the indices, maxi=1,··· ,k (cid:107)ci − c(cid:48)

i(cid:107) ≤ (cid:15).

Note that similar result appears in Theorem 1 of Chen et al. (2016). This lemma
shows that two close Morse functions p, q will have similar critical points.

The proof of Theorem 1 requires several working lemmas. We provide a chart

for how we are going to prove Theorem 1.

First, we deﬁne some notations about gradient ﬂows. Recall that πx(t) ∈ K

is the gradient (ascent) ﬂow starting at x:

πx(0) = x,

π(cid:48)
x(t) = g(πx(t)).

For x that is not on the boundary set D, we deﬁne the time:

t(cid:15)(x) = inf{t : πx(s) ∈ B(m,

(cid:15)), for alls ≥ t},

√

where m is the destination of πx. That is, t(cid:15)(x) is the time to arrive the regions
around a local mode.

First, we prove a property for the direction of the gradient ﬁeld around bound-

aries.

Lemma 10 (Gradient ﬁeld and boundaries) Assume the notations in The-
orem 1 and assume f is a Morse function with bounded third derivatives and
satisﬁes assumption (D). Let s(x) = x − Πx, where Πx ∈ B is the projected
point from x onto B (when Πx is not unique, just pick any projected point). For
any q ∈ B, let x be a point near q such that x − q ∈ V(q), the normal space of
B at q. Let δ(x) = (cid:107)x − q(cid:107) and e(x) = x−q

(cid:107)x−q(cid:107) denote the unit vector. Then

Chen et al./Inference using the Morse-Smale

28

(a) Lemma 10

(b) Lemma 11

Fig 11. Illustration for Lemma 10 and 11. (a): We show that the angle between projection
vector s(x) and the gradient g(x) is always right whenever x is closed to the boundaries B. (b):
According to (a), any gradient ﬂow line start from a point x that is close to the boundaries
(distance < δ1), this ﬂow line is always moving away from the boundaries when the current
location is close to the boundaries. The ﬂow line can temporally get closer to the boundaries
when it is away from boundaries (distance > δ1)

1. For every point x such that

d(x, B) ≤ δ1 =

2Hmin
d2 · (cid:107)f (cid:107)3,max

,

we have

g(x)T s(x) ≥ 0.

That is, the gradient is pushing x away from the boundaries.

2. When δ(x) ≤

Hmin
d2·(cid:107)f (cid:107)3,max

,

(cid:96)(x) = e(x)T g(x) ≥

Hminδ(x).

1
2

Proof.
Claim 1. Because the projection of x onto B is Πx, s(x) ∈ V(Πx) and
s(x)T g(Πx) = 0 (recall that for p ∈ B, V(p) is the collection of normal vectors
of B at p).

Recall that d(x, B) = (cid:107)s(x)(cid:107) is the projected distance. By the fact that

Chen et al./Inference using the Morse-Smale

29

s(x)T g(Πx) = 0,

s(x)T g(x) = s(x)T (g(x) − g(Πx))
d2
2

≥ s(x)T H(Πx)s(x) −
= d(x, B)2 s(x)T
d(x, B)
(cid:18)

H(Πx)

(cid:107)f (cid:107)3,maxd(x, B)3

(Taylor’s theorem)

s(x)
d(x, B)

−

d2
2

(cid:107)f (cid:107)3,maxd(x, B)3

(cid:19)

≥ d(x, B)2

Hmin −

(cid:107)f (cid:107)3,maxd(x, B)

.

d2
2

(40)
Note that we use the vector-value Taylor’s theorem in the ﬁrst inequality and
the fact that for two close points x, y, the diﬀerence in the j-the element of
gradient gj(x) − gj(y) has the following expansion

gj(x) − gj(y) = Hj(y)T (x − y) +

(u − y)Tj(u)du

(cid:90) x

sup
u

u=y
1
2
d2
2

≥ Hj(y)T (x − y) −

(cid:107)Tj(u)(cid:107)2(cid:107)x − y(cid:107)2

≥ Hj(y)T (x − y) −

(cid:107)f (cid:107)3,max(cid:107)x − y(cid:107)2,

where Hj(y) = ∇gj(y) and Tj(y) = ∇∇gj(y) is the Hessian matrix of gj(y),
whose elements are the third derivatives of f (y).

Thus, when d(x, B) ≤ 2Hmin
, s(x)T g(x) ≥ 0, which proves the ﬁrst claim.
Claim 2. By deﬁnition, e(x)T g(q) = 0 because g(q) is in tangent space of B

d2·(cid:107)f (cid:107)3,max

at q and e(x) is in the normal space of B at q. Thus,

(cid:96)(x) = e(x)T g(x)

= e(x)T (g(x) − g(q))

≥ e(x)T H(q)(x − q) −

(cid:107)f (cid:107)3,max(cid:107)x − q(cid:107)2

(41)

= e(x)T H(π(x))e(x)δ(x) −

(cid:107)f (cid:107)3,maxδ(x)2

d2
2

d2
2

≥

Hminδ(x)

1
2

whenever δ(x) = (cid:107)x − q(cid:107) ≤
. Note that in the ﬁrst inequality we use
the same lower bound as the one in claim 1. Also note that x − q = e(x)δ(x)
and e(x) is in the normal space of B at π(x) so the third inequality follows from
assumption (D).

Hmin
d2·(cid:107)f (cid:107)3,max

(cid:3)

Lemma 10 can be used to prove the following result.

Chen et al./Inference using the Morse-Smale

30

Lemma 11 (Distance between ﬂows and boundaries) Assume the nota-
tions as the above and assumption (D). Then for all x such that 0 < d(x, B) =
δ ≤ δ1 = 2Hmin

,

d2(cid:107)f (cid:107)3,max

d(πx(t), B) ≥ δ,

for all t ≥ 0.

The main idea is that the projected gradient (gradient projected to the normal
space of nearby boundaries) is always positive. This means that the ﬂow cannot
move “closer” to the boundaries.

Proof. By Lemma 10, for every point x near to the boundaries (d(x, B) <
δ1), the gradient is moving this point away from the boundaries. Thus, for any
ﬂow πx(t), once it touches the region B ⊕ δ1, it will move away from this region.
So when a ﬂow leaves B ⊕ δ1, it can never come back.

Therefore, the only case that a ﬂow can be within the region B ⊕ δ1 is that

it starts at some x ∈ B ⊕ δ1. i.e. d(x, B) < δ1.

Now consider a ﬂow start at x such that 0 < d(x, B) ≤ δ1. By Lemma 10,
the gradient g(x) leads x to move away from the boundaries B. Thus, whenever
πx(t) ∈ B ⊕ δ1, the gradient is pushing πx(t) away from B. As a result, the time
that πx(t) is closest to B is at the beginning of the ﬂow .i.e. t = 0. This implies
that d(πx(t), B) ≥ d(πx(0), B) = d(x, B) = δ.

(cid:3)

With Lemma 11, we are able to bound the low gradient regions since the
ﬂow cannot move inﬁnitely close to critical points except its destination. Let
λmin > 0 be the minimal ‘absolute’ value of eigenvalues of all critical points.

Lemma 12 (Bounds on low gradient regions) Assume the density func-
tion f is a Morse function and has bounded third derivatives. Let C denote
the collection of all critical points and let λmin is the minimal ‘absolute’ eigen-
value for Hessian matrix H(x) evaluated at x ∈ C. Then there exists a constant
δ2 > 0 such that

G(δ) ≡

x : (cid:107)g(x)(cid:107) ≤

⊂ C ⊕ δ

(42)

(cid:26)

(cid:27)

λmin
2

δ

for every δ ≤ δ2.

Proof. Because the support K is compact and x ∈ K (cid:55)→ (cid:107)g(x)(cid:107) is contin-
uous, for any g0 > 0 suﬃciently small, there exists a constant R(g0) > 0 such
that

G1(g0) ≡ {x : (cid:107)g(x)(cid:107) ≤ g0} ⊂ C ⊕ R(g0)

and when g0 → 0, R(g0) → 0. Thus, there is a constant g1 > 0 such that
R(g1) =

.

λmin
2d3(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

31

The set C ⊕ λmin

2(cid:107)f (cid:107)3,max

has a useful feature: for any x ∈ C ⊕ λmin

,

2(cid:107)f (cid:107)3,max

(cid:107)H(x) − H(c)(cid:107)F = (cid:107)(x − c)f (3)(c + t(x − c))dt(cid:107)F

≤ d3(cid:107)x − c(cid:107)(cid:107)f (cid:107)3,max
λmin
2d3(cid:107)f (cid:107)3,max

≤ d3

· (cid:107)f (cid:107)3,max

=

λmin
2

,

where f (3) is a d × d × d array of the third derivative of f and (cid:107)A(cid:107)F is the
Frobenius norm of the matrix A. By Hoﬀman–Wielandt theorem (see, e.g., page
165 of Bhatia 1997), the eigenvalues between H(x) and H(c) is bounded by
(cid:107)H(x) − H(c)(cid:107)F . Therefore, the smallest eigenvalue of H(x) must be greater
than or equal to the smallest eigenvalue of H(c) minus λmin
2 . Because λmin is
the smallest absolute eigenvalues of H(c) for all c ∈ C, the smallest eigenvalue
of H(x) is greater than or equal to λmin
.

2 , for all x ∈ C ⊕ R(g1) = C ⊕
λmin
2d3(cid:107)f (cid:107)3,max

λmin
2d3(cid:107)f (cid:107)3,max
, for any

Using the above feature and the fact that G1(g1) ⊂ C ⊕

x ∈ G1(g1), we have the following inequalities:

g1 ≥ (cid:107)g(x)(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

=

0

≥ (cid:107)x − c(cid:107)

λmin.

1
2

(x − c)H(c + t(x − c))dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Thus, (cid:107)x − c(cid:107) ≤ 2g1
λmin

, which implies

Moreover, because G1(g2) ⊂ G1(g3) for any g2 ≤ g3, any g2 ≤ g1 satisﬁes

Now pick δ = 2g2
λmin

, we conclude

G1(g1) ⊂ C ⊕

G1(g2) ⊂ C ⊕

2g1
λmin

.

2g2
λmin

.

G1

(cid:19)

(cid:18) λmin
2δ

= G(δ) ⊂ C ⊕ δ

δ =

2g2
λmin

≤

2g1
λmin

= δ2,

for all

(cid:3)

where g1 is the constant such that R(g1) =

λmin
2d3(cid:107)f (cid:107)3,max

.

(43)

Chen et al./Inference using the Morse-Smale

32

Fig 12. Illustration for H((cid:15), δ). The thick black lines are boundaries B; solid dots are local
modes; box is local minimum; empty dots are saddle points. The three purple lines denote
possible gradient ﬂows starting from some points x with d(x, B) = δ. The gray disks denote
all possible regions such that (cid:107)g(cid:107) ≤ λmin
2 δ. Thus, the amount of gradient within the set H((cid:15), δ)
is greater or equal to λmin

2 δ.

Lemma 13 (Bounds on gradient ﬂow) Using the notations above and as-
sumption (D), let δ1 be deﬁned in Lemma 11 and δ2 be deﬁned in Lemma 12,
equation (43). Then for all x such that

d(x, B) = δ < δ0 = min

δ1, δ2,

(cid:26)

Hmin
d2 · (cid:107)f (cid:107)3,max

(cid:27)

,

and picking (cid:15) such that δ2 > (cid:15)2 > δ, we have

η(cid:15)(x) ≡ inf

(cid:107)g(πx(t))(cid:107) ≥ δ

0≤t≤t(cid:15)(x)

λmin
2

.

γ(cid:15)(δ) ≡ inf
x∈Bδ

η(cid:15)(x) ≥ δ

λmin
2

,

Moreover,

where Bδ = {x : d(x, B) = δ}.

Proof.
We consider the ﬂow πx starting at x (not on the boundaries) such that

d(x, B) = δ < min {δ1, δ2} .

For 0 ≤ t ≤ t(cid:15)(x), the entire ﬂow is within the set

H((cid:15), δ) = {x : d(x, B) ≥ δ, d(x, M ) ≥

(cid:15)}.

(44)

√

Chen et al./Inference using the Morse-Smale

That is,

{πx(t) : 0 ≤ t ≤ t(cid:15)(x)} ⊂ H((cid:15), δ).

This is because by Lemma 11, the ﬂow line cannot get closer to the boundaries
B within distance δ, and the ﬂow stops when its distance to its destination is
at (cid:15). Thus, if we can prove that every point within H((cid:15), δ) has gradient lowered
bounded by δ λmin
2 , we have completed the proof. That is, we want to show that

33

(45)

(46)

To show the lower bound, we focus on those points whose gradient is small.

Let

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

(cid:26)

S(δ) =

x : (cid:107)g(x)(cid:107) ≤ δ

(cid:27)

.

λmin
2

S(δ) ⊂ C ⊕ δ.

By Lemma 12, the S(δ) are regions around critical points such that

Since we have chosen (cid:15) such that (cid:15) ≥ δ2 and by the fact that critical points
are either in M , the collection of all local modes, or in B the boundaries so that,
the minimal distance between H((cid:15), δ) and critical points C is greater that δ (see
equation (44) for the deﬁnition of H((cid:15), δ)). Thus,

which implies equation (46):

(C ⊕ δ) ∩ H((cid:15), δ) = ∅,

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

Now by the fact that all πx(t) with d(x, B) < δ are within the set H((cid:15), δ)
(equation (45)), we conclude the result.

(cid:3)

Lemma 13 links the constant γ(cid:15)(δ) and the minimal gradient, which can be
used to bound the time t(cid:15)(x) uniformly and further leads to the following result.
Lemma 14 Let K(δ) = {x ∈ K : d(x, B) ≥ δ} = K\(B ⊕δ) and δ0 be deﬁned as
Lemma 13 and M is the collection of all local modes. Assume that f has bounded
third derivative and is a Morse function and that assumption (D) holds. Let (cid:101)f
be another smooth function. There exists constants c∗, c0, c1, (cid:15)0 that all depend
only on f such that when ((cid:15), δ) satisfy the following condition

δ < (cid:15) < (cid:15)0,

δ < min{δ0, Haus(K(δ), B(M,

(cid:15)))}

√

and if

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤ c0

(cid:107)f − (cid:101)f (cid:107)1,max ≤ c1 exp

−

(cid:32)

√
4

d(cid:107)f (cid:107)2,max(cid:107)f (cid:107)∞

(cid:33)

,

δ2λ2

min

(47)

(48)

Chen et al./Inference using the Morse-Smale

34

Fig 13. Result from Lemma 13: lower bound on minimal gradient. This plot shows possible
values for minimal gradient η(cid:15)(x) (pink regions) when d(x, B) is known. Note that we have
chosen (cid:15)2 < δ2.

then for all x ∈ K(δ)

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞.

(49)

(cid:113)

Note that condition (47) holds when ((cid:15), δ) are suﬃciently small.

Proof. The proof of this lemma is closely related to the proof of Theorem
2 of Arias-Castro et al. (2016). The results in Arias-Castro et al. (2016) is a
pointwise convergence of gradient ﬂows; now we will generalize their ﬁndings to
the uniform convergence.

Note that K(δ) = H((cid:15), δ) ∪ B(x,

(cid:15)). For x ∈ B(x,

(cid:15)), the result is trivial

√

√

when (cid:15) is suﬃciently small. Thus, we assume x ∈ H((cid:15), δ).

From equation (40–44) in Arias-Castro et al. (2016) (proof to their Theorem

2),

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107)
(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

2
λmin

2λmin(cid:15) +

√

(cid:107)f − (cid:101)f (cid:107)1,maxe

d(cid:107)f (cid:107)2,maxt(cid:15)(x) + 2(cid:107)f − (cid:101)f (cid:107)∞

√

(cid:107)f (cid:107)1,max
d(cid:107)f (cid:107)2,max

(cid:33)

(50)

under condition (48) and (cid:15) < (cid:15)0 for some constant (cid:15)0.

Thus, the key is to bound t(cid:15)(x). Recall that x ∈ H((cid:15), δ). Now consider the

Chen et al./Inference using the Morse-Smale

35

gradient ﬂow πx and deﬁne z = πx(t(cid:15)(x)).

f (z) − f (x) =

∂f (πx(s))
∂s

ds =

0

(cid:90) t(cid:15)(x)

g(πx(s))T π(cid:48)

x(s)ds

(cid:107)g(πx(s))(cid:107)2ds ≥ γ(cid:15)(δ)2t(cid:15)(x).

(cid:90) t(cid:15)(x)

(cid:90) t(cid:15)(x)

=

0

0

Since f (z) − f (x) ≤ 2(cid:107)f (cid:107)∞, we have

(cid:107)f (cid:107)∞ ≥

γ(cid:15)(δ)2t(cid:15)(x)

1
2

t(cid:15)(x) ≤

2(cid:107)f (cid:107)∞
γ(cid:15)(δ)2

≤

8(cid:107)f (cid:107)∞
δ2λ2

min

and by Lemma 13,

for all x ∈ H((cid:15), δ).

Now plug-in (52) into (50), we have

(51)

(52)

(cid:114)

(cid:107) lim
t→∞

πx(t)− lim

t→∞ (cid:101)πx(t)(cid:107) ≤

a0(cid:15) + a1(cid:107)f − (cid:101)f (cid:107)1,maxe

(53)
for some constants a0, a1, a2. Now using condition (48) to replace the second
term of right hand side, we conclude

√

d(cid:107)f (cid:107)2,max

8(cid:107)f (cid:107)∞
δ2λ2

min + a2(cid:107)f − (cid:101)f (cid:107)∞

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max

By Lemma 7 in Arias-Castro et al. (2016), there exists some constant c3 such

for some constant a3.

that when a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max < 1/c3,

(cid:113)

(cid:113)

√

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤

2c3(cid:107)f − (cid:101)f (cid:107).

Thus, when both (cid:15) and (cid:107)f − (cid:101)f ∗
constant c∗ such that

3,max(cid:107) are suﬃciently small, there exists some

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗(cid:107)f − (cid:101)f (cid:107)

for all x ∈ H((cid:15), δ).

(cid:3)

Now we turn to the proof of Theorem 1.

we show that when (cid:107)f − (cid:101)f (cid:107)∗

Proof of Theorem 1. The proof contains two parts. In the ﬁrst part,
3,max is suﬃciently small, we have Haus(B, (cid:101)B) <
, where B and (cid:101)B are the boundary of descending d-manifolds for f

Hmin
d2(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

36

Hmin
d2(cid:107)f (cid:107)3,max

and (cid:101)f . The second part of the proof is to derive the convergence rate. Because
Haus(B, (cid:101)B) <
, we can apply the second assertion of Lemma 10 to
derive the rate of convergence. Note that C and (cid:101)C are the critical points for f
and (cid:101)f and M ≡ C0, (cid:102)M ≡ (cid:101)C0 are the local modes for f and (cid:101)f .
Hmin
d2·(cid:107)f (cid:107)3,max

, the upper bound for Hausdorﬀ dis-
tance. Let σ = min{(cid:107)x − y(cid:107) : x, y ∈ M, x (cid:54)= y}. That is, σ is the smallest dis-
tance between any pair of distinct local modes. By Lemma 9, when (cid:107)f − (cid:101)f (cid:107)∗
is small, f and (cid:101)f have the same number of critical points and

Part 1: Haus(B, (cid:101)B) <

3,max

Haus(C, (cid:101)C) ≤ A(cid:107)f − (cid:101)f (cid:107)∗

2,max ≤ A(cid:107)f − (cid:101)f (cid:107)∗

3,max,

where A is a constant that depends only on f (actually, we only need (cid:107)f − (cid:101)f (cid:107)∗
to be small here).

2,max

Thus, whenever (cid:107)f − (cid:101)f (cid:107)∗

3,max satisﬁes

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

σ
3A

,

every M has an unique corresponding point in (cid:102)M and vice versa. In addition,
for a pair of local modes (mj, (cid:101)mj) : mj ∈ M, (cid:101)mj ∈ (cid:102)M , their distance is bounded
by (cid:107)mj − (cid:101)mj(cid:107) ≤ σ
3 .

Now we pick ((cid:15), δ) such that they satisfy equation (47). Then when (cid:107)f −
3,max is suﬃciently small, by Lemma 14, for every x ∈ H((cid:15), δ) we have

(cid:101)f (cid:107)∗

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞ ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∗

3,max.

(cid:113)

(cid:113)

Thus, whenever

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

1
c2
∗

(cid:16) σ
3

(cid:17)2

,

πx(t) and (cid:101)πx(t) leads to the same pair of modes. Namely, the boundaries (cid:101)B
will not intersect the region H((cid:15), δ). And it is obvious that (cid:101)B cannot intersect
B(M,

(cid:15)). To conclude,

√

(54)

(55)

(56)

(cid:101)B ∩ H((cid:15), δ) = ∅
√

(cid:101)B ∩ B(M,

(cid:15)) = ∅
⇒ (cid:101)B ∩ K(δ) = ∅,
(cid:15)).

√

because by deﬁnition, K(δ) = H((cid:15), δ) ∩ B(M,

Thus, (cid:101)B ⊂ K(δ)C = B ⊕ δ, which implies Haus(B, (cid:101)B) ≤ δ < Hmin

(note

d2(cid:107)f (cid:107)3,max

that δ < δ0 ≤ Hmin

d2(cid:107)f (cid:107)3,max

appears in equation (47) and Lemma 13).

Part 2: Rate of convergence. To derive the convergence rate, we use proof
by contradiction. Let q ∈ B, (cid:101)q ∈ (cid:101)B a pair of points such that their distance
attains the Hausdorﬀ distance Haus
(cid:101)B, B

(cid:17)

(cid:16)

. Namely, q and (cid:101)q satisfy
(cid:16)

(cid:17)

(cid:107)q − (cid:101)q(cid:107) = Haus

(cid:101)B, B

Chen et al./Inference using the Morse-Smale

37

and either q is the projected point from (cid:101)q onto B or (cid:101)q is the projected point
from q onto (cid:101)B.

Recall that V(x) is the normal space to B at x ∈ B and we deﬁne (cid:101)V(x)
similarly for x ∈ (cid:101)B. An important property of the pair q, (cid:101)q is that q − (cid:101)q ∈
V(q), (cid:101)V((cid:101)q). If this is not true, we can slightly perturb q (or (cid:101)q) on B (or (cid:101)B) to
get a projection distance larger than the Hausdorﬀ distance, which leads to a
contradiction.

Now we choose x to be a point between q, (cid:101)q such that x = 1

3 (cid:101)q. We
(cid:107)(cid:101)q−x(cid:107) . Then e(x) ∈ V(q) and (cid:101)e(x) ∈ (cid:101)V((cid:101)q) and

3 q + 2

(cid:107)q−x(cid:107) and (cid:101)e(x) = (cid:101)q−x

deﬁne e(x) = q−x
e(x) = −(cid:101)e(x).

By Lemma 10 (second assertion),

(57)

(58)

(cid:96)(x) = e(x)T g(x) ≥

Hmin(cid:107)q − x(cid:107) > 0

(cid:101)(cid:96)(x) = (cid:101)e(x)T

(cid:101)g(x) ≥

(cid:101)Hmin(cid:107)(cid:101)q − x(cid:107) > 0.

1
2
1
2

Thus, for every x between q, (cid:101)q,
e(x)T g(x) > 0,

, e(x)T

(cid:101)g(x) = −(cid:101)e(x)T

(cid:101)g(x) < 0.

Note that we can apply Lemma 10 to (cid:101)f and its gradient because when (cid:107)f − (cid:101)f (cid:107)∗
2
is suﬃciently small, the assumption (D) holds for (cid:101)f as well.

To get the upper bound of (cid:107)q−(cid:101)q(cid:107) = Haus( (cid:101)B, B), note that (cid:107)q−x(cid:107) = 2

3 (cid:107)q−(cid:101)q(cid:107),

so

e(x)T

(cid:101)g(x) = e(x)T ((cid:101)g(x) − g(x)) + e(x)T g(x)
≥ e(x)T g(x) − (cid:107) (cid:101)f − f (cid:107)1,max

Hmin(cid:107)q − x(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max

(By Lemma 10)

(59)

≥

=

1
2
1
3

Hmin(cid:107)q − (cid:101)q(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max.

Thus, as long as

we have e(x)T
that

(cid:3)

Haus( (cid:101)B, B) = (cid:107)q − (cid:101)q(cid:107) > 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

,

(cid:101)g(x) > 0, a contradiction to equation (58). Hence, we conclude

Haus( (cid:101)B, B) ≤ 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

(cid:16)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

.

Proof of Theorem 3.
To prove the asymptotic rate of the rand index, we assume that for every local
mode of p, there exists one and only one local mode of (cid:98)pn that is close to the

Chen et al./Inference using the Morse-Smale

38

speciﬁc mode of p. By Lemma 9, this is true when (cid:107)(cid:98)pn − p(cid:107)∗
3,max is suﬃciently
small. Thus, after relabeling, the local mode (cid:98)m(cid:96) of (cid:98)pn is an estimator to the
local mode m(cid:96) of p. Let (cid:99)W(cid:96) be the basin of attraction to (cid:98)m(cid:96) using ∇(cid:98)pn and W(cid:96)
be the basin of attraction to m(cid:96) using ∇p. Let A(cid:52)B = {x : x ∈ A, x /∈ B} ∪ {x :
x ∈ B, x /∈ A} be the symmetric diﬀerence between sets A and B. The regions

En =

(cid:99)W(cid:96)(cid:52)W(cid:96)

(cid:17)

⊂ K

(cid:91)

(cid:16)

(cid:96)

(60)

are where the two mode clustering disagree with each other. Note that En are
regions between the two boundaries (cid:98)Bn and B

Given a pair of points Xi and Xj,

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj) =⇒ Xi or Xj ∈ En.

(61)

By the deﬁnition of rand index (30),

1 − rand ((cid:98)pn, p) =

(cid:19)−1

(cid:88)

(cid:16)

(cid:18)n
2

i,j

1

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj)

(62)

(cid:17)

Thus, if we can bound the ratio of data points within En, we can bound the
rate of rand index.

Since K is compact and p has bounded second derivatives, the volume of En

is bounded by

Vol(En) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Note Vol(A) denotes the volume (Lebesgue measure) of a set A. We now con-
struct a region surrounding B such that

and

En ⊂ B ⊕ Haus( (cid:98)Bn, B) = Vn

Vol(Vn) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Now we consider a collection of subsets of K:

V = {B ⊕ r : R > r > 0},

(cid:80)n

where R < ∞ is the diameter for K. For any set A ⊂ K, let P (Xi ∈ A) and
(cid:98)Pn(A) = 1
i=1 1(Xi ∈ A) denote the probability of an observation within A
n
and the empirical estimate for that probability, respectively. It is easy to see
that Vn ∈ V for all n and the class V has a ﬁnite VC dimension (actually, the
VC dimension is 1). By the empirical process theory (or so-called VC theory,
see e.g. Vapnik and Chervonenkis (1971)),

(cid:12)
(cid:12)
(cid:12)P (Xi ∈ A) − (cid:98)Pn(A)

(cid:12)
(cid:12)
(cid:12) = OP

sup
A∈V

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(63)

(64)

(65)

(66)

(67)

Chen et al./Inference using the Morse-Smale

39

Thus,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)P (Xi ∈ Vn) − (cid:98)Pn(Vn)
(cid:12) = OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(68)

Now by equations (61) and (62),

1 − rand ((cid:98)pn, p) ≤ 8 (cid:98)Pn(En) ≤ 8 (cid:98)Pn(Vn) ≤ 8P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n

. (69)

Therefore,

1 − rand ((cid:98)pn, p) ≤ P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n
(cid:32)(cid:114)

(cid:33)

log(n)
n

(cid:32)(cid:114)

(cid:33)

log(n)
n

p(x) × Vol(Vn) + OP

≤ sup
x∈K

(cid:16)

≤ O

Haus( (cid:98)Bn, B)

+ OP

(cid:17)

= O (cid:0)h2(cid:1) + OP

(cid:32)(cid:114)

(cid:33)

,

log(n)
nhd+2

which completes the proof. Note that we apply Theorem 2 in the last equality.

(cid:3)

Proof of Theorem 4. Let (X1, Y1), · · · , (Xn, Yn) be the observed data.
Let (cid:98)E(cid:96) denote the d-cell for the nonparametric pilot regression estimator (cid:98)mn.
With I(cid:96) = {i : Xi ∈ (cid:98)E(cid:96)}, we deﬁne X(cid:96) as the matrix with rows Xi, i ∈ I(cid:96) and
similarly we deﬁne Y(cid:96).

We deﬁne X0,(cid:96) to be the matrix similar to X(cid:96) except that the row elements
are those Xi within E(cid:96), the d-cell deﬁned on true regression function m. We
also deﬁne Y0,(cid:96) to be the corresponding Yi.

By the theory of linear regression, the estimated parameters (cid:98)µ(cid:96), (cid:98)β(cid:96) have a

closed form solution:

Similarly, we deﬁne

((cid:98)µ(cid:96), (cid:98)β(cid:96))T = (XT

(cid:96)

X(cid:96))−1XT
(cid:96)

Y(cid:96).

((cid:98)µ0,(cid:96), (cid:98)β0,(cid:96))T = (XT

0,(cid:96)

X0,(cid:96))−1XT
0,(cid:96)

Y0,(cid:96)

as the estimated coeﬃcients using X0,(cid:96) and Y0,(cid:96).

As (cid:107) (cid:101)m − m(cid:107)∗

3,max is small, by Theorem 3, the number of rows at which
X(cid:96) and X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max). This is because an

(70)

(71)

(72)

Chen et al./Inference using the Morse-Smale

40

observation (a row vector) that appears only in one of X(cid:96) and X0,(cid:96) is those
fallen within either (cid:98)E(cid:96) or E(cid:96) but not both. Despite the fact that Theorem 3
is for basins of attraction (d-descending manifolds) of local modes, it can be
easily generalized to 0-ascending manifolds of local minima under assumption
(A). Thus, the similar bound holds for d-cells as well. Thus, we conclude that
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
since (X(cid:96), Y(cid:96)) and (X0,(cid:96), Y0,(cid:96)) only diﬀer by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max) elements.
Thus,

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

1
n
1
n

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(73)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)µ0,(cid:96) − (cid:98)µ(cid:96), (cid:98)β0,(cid:96) − (cid:98)β(cid:96))
(cid:13)∞

=

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
(cid:13)
n
(cid:13)

XT
0,(cid:96)

X0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96) −

XT
(cid:96)

X(cid:96)

(cid:19)−1 1
n

(cid:18) 1
n

(cid:19)−1 1
n

XT
(cid:96)

Y(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= O((cid:107) (cid:98)mn − m(cid:107)1,max),

which implies.

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − (cid:98)µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − (cid:98)β(cid:96)(cid:107)

= O((cid:107) (cid:98)mn − m(cid:107)1,max).

Now by the theory of linear regression,

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − β(cid:96)(cid:107)

= OP

(cid:18) 1
√
n

(cid:19)

.

(74)

(75)

(76)

Thus, combining (75) and (76) and use the fact that all the above bounds are
uniform over each cell, we have proved that the parameters converge at rate
O((cid:107) (cid:98)mn − m(cid:107)1,max) + OP

(cid:16) 1√

(cid:17)

n

.

For points within the regions where E(cid:96) and (cid:98)E(cid:96) agree with each other, the rate
of convergence for parameter estimation translates into the rate of (cid:98)mn,MSR −
mMSR. The regions that E(cid:96) and (cid:98)E(cid:96) disagree to each other, denoted as Nn, have
Lebesgue O((cid:107) (cid:98)mn − m(cid:107)1,max) by Theorem 1. Thus, we have completed the proof.

(cid:3)

Proof of Theorem 5. The proof of Theorem 5 is nearly identical to the
proof of Theorem 4. The only diﬀerence is that the number of rows that X(cid:96) and
X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)β
1,max) due to the low noise condition
(36). Thus, equation (73) becomes
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O((cid:107) (cid:98)mn − m(cid:107)β

= O((cid:107) (cid:98)mn − m(cid:107)β

1
n
1
n

1
n
1
n

1,max)

1,max)

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(77)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Chen et al./Inference using the Morse-Smale

41

so the parameter estimation error (76) is O((cid:107) (cid:98)mn − m(cid:107)β

1,max) + OP

(cid:16) 1√

(cid:17)

.

n

Under assumption (K1–2) and using Theorem 8 (the same result works for

kernel regression),

O((cid:107) (cid:98)mn − m(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

Thus, with the choice that h = O
(cid:17)2/(d+6)(cid:19)

(cid:18)(cid:16) log n

OP

n

(cid:3)

, which proves equation (37).

(cid:18)(cid:16) log n

(cid:17)1/(d+6)(cid:19)

n

, we have O((cid:107) (cid:98)mn−m(cid:107)1,max) =

Proof of Theorem 6.
We ﬁrst derive the explicit form of the parameters (η†

Note that the parameters are obtained by (14):

(cid:96) , γ†

(cid:96) ) within cell E(cid:96).

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:90)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Now we deﬁne a random variable U(cid:96) ∈ Rd that is uniformly distributed over E(cid:96).
Then equation (14) is equivalent to

(η†

(cid:96) , γ†

(cid:96) ) = argmin

E

η,γ

(cid:16)(cid:0)f (U(cid:96)) − η − γT U(cid:96)

(cid:1)2(cid:17)

.

The analytical solution to the above problem is

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:19)

(cid:18)

=

1

E(U(cid:96))T
E(U(cid:96)) E(U(cid:96)U T
(cid:96) )

(cid:19)−1 (cid:18) E(f (U(cid:96)))
E(U(cid:96)f (U(cid:96)))

(cid:19)

Now we consider another smooth function (cid:101)f that is close to f such that
(cid:107) (cid:101)f − f (cid:107)∗
3,max is small so we can apply Theorem 1 to obtain consistency for both
descending d-manifolds and ascending 0-manifolds. Note that by Lemma 9, all
the critical points are close to each other and after relabeling, each d-cell E(cid:96) of
f is estimated by another d-cell (cid:101)E(cid:96) of (cid:101)f . Theorem 1 further implies that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Leb( (cid:101)E(cid:96)) − Leb(E(cid:96))
(cid:12) = O
(cid:17)
(cid:16)

(cid:16)

(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

(cid:17)

Leb

(cid:101)E(cid:96)(cid:52)E(cid:96)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

,

(78)

(79)

(80)

where Leb(A) is the Lebesgue measure for set A and A(cid:52)B = (A\B) ∪ (B\A) is

Chen et al./Inference using the Morse-Smale

42

the symmetric diﬀerence. By simple algebra, equation (80) implies that

(cid:107)E( (cid:101)U(cid:96)) − E(U(cid:96))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)U T

(cid:96) ) − E(U(cid:96)U T

|E( (cid:101)f ( (cid:101)U(cid:96))) − E(f (U(cid:96)))| = O

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:96) )(cid:107)∞ = O
(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max
(cid:17)

(cid:107) (cid:101)f − f (cid:107)∗
(cid:16)

1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)f ( (cid:101)U(cid:96))) − E(U(cid:96)f (U(cid:96)))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

(81)

By (81) and the analytic solution to ((cid:101)η†
(cid:101)η†
(cid:101)γ†

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:19)

(cid:18)

(cid:96)

(cid:96)

(cid:96) , (cid:101)γ†
(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

(cid:17)

.

1,max

(cid:96) ) from (79), we have proved

(82)

Since the bound does not depend on the cell indices (cid:96), (82) holds uniformly for
all (cid:96) = 1, · · · , K.

(cid:3)

References

E. Arias-Castro, D. Mason, and B. Pelletier. On the estimation of the gradient
lines of a density and the consistency of the mean-shift algorithm. Journal of
Machine Learning Research, 17(43):1–28, 2016.

J.-Y. Audibert, A. B. Tsybakov, et al. Fast learning rates for plug-in classiﬁers.

The Annals of statistics, 35(2):608–633, 2007.

M. Azizyan, Y.-C. Chen, A. Singh, and L. Wasserman. Risk bounds for mode

clustering. arXiv preprint arXiv:1505.00482, 2015.

A. Azzalini and N. Torelli. Clustering via nonparametric density estimation.

Statistics and Computing, 17(1):71–80, 2007.

P. Bacchetti. Additive isotonic models. Journal of the American Statistical

A. Banyaga and D. Hurtubise. Lectures on Morse homology, volume 29. Springer

Association, 84(405):289–294, 1989.

Science & Business Media, 2004.

L. Baringhaus and C. Franz. On a new multivariate two-sample test. Journal

of multivariate analysis, 88(1):190–206, 2004.

R. E. Barlow, D. J. Bartholomew, J. Bremner, and H. D. Brunk. Statistical
inference under order restrictions: the theory and application of isotonic re-
gression. Wiley New York, 1972.

R. Bhatia. Matrix Analysis. Springer, 1997.
G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business

Media, 1993.

R. R. Brinkman, M. Gasparetto, S.-J. J. Lee, A. J. Ribickas, J. Perkins,
W. Janssen, R. Smiley, and C. Smith. High-content ﬂow cytometry and
temporal data analysis for deﬁning a cellular signature of graft-versus-host
disease. Biology of Blood and Marrow Transplantation, 13(6):691–700, 2007.

Chen et al./Inference using the Morse-Smale

43

J. Chac´on and T. Duong. Data-driven density derivative estimation, with ap-
plications to nonparametric clustering and bump hunting. Electronic Journal
of Statistics, 7:499–532, 2013.

J. Chac´on, T. Duong, and M. Wand. Asymptotics for general multivariate kernel

density derivative estimators. Statistica Sinica, 2011.

J. E. Chac´on et al. A population background for nonparametric density-based

clustering. Statistical Science, 30(4):518–532, 2015.

F. Chazal, B. T. Fasy, F. Lecci, B. Michel, A. Rinaldo, and L. Wasserman. Ro-
bust topological inference: Distance to a measure and kernel distance. arXiv
preprint arXiv:1412.7197, 2014.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. Asymptotic theory for density

ridges. The Annals of Statistics, 43(5):1896–1928, 2015.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. A comprehensive approach
to mode clustering. Electronic Journal of Statistics, 10(1):210–241, 2016.
Y. Cheng. Mean shift, mode seeking, and clustering. Pattern Analysis and

Machine Intelligence, IEEE Transactions on, 17(8):790–799, 1995.

D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence dia-

grams. Discrete & Computational Geometry, 37(1):103–120, 2007.

D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space
analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
24(5):603–619, 2002.

T. Duong. Local signiﬁcant diﬀerences from nonparametric two-sample tests.

Journal of Nonparametric Statistics, 25(3):635–645, 2013.

T. Duong et al. ks: Kernel density estimation and kernel discriminant analysis
for multivariate data in r. Journal of Statistical Software, 21(7):1–16, 2007.
B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statis-

tics, 7(1):1–26, 1979.

U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type

function estimators. The Annals of Statistics, 2005.

K. Fukunaga and L. Hostetler. The estimation of the gradient of a density
function, with applications in pattern recognition. Information Theory, IEEE
Transactions on, 21(1):32–40, 1975.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, and L. Wasserman. The
geometry of nonparametric ﬁlament estimation. Journal of the American
Statistical Association, 107(498):788–799, 2012.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, L. Wasserman, et al. Non-
parametric ridge estimation. The Annals of Statistics, 42(4):1511–1545, 2014.
S. Gerber and K. Potter. Data analysis with the morse-smale complex: The msr

package for r. Journal of Statistical Software, 2011.

S. Gerber, P.-T. Bremer, V. Pascucci, and R. Whitaker. Visual exploration
of high dimensional scalar functions. Visualization and Computer Graphics,
IEEE Transactions on, 16(6):1271–1280, 2010.

S. Gerber, O. R¨ubel, P.-T. Bremer, V. Pascucci, and R. T. Whitaker. Morse–
smale regression. Journal of Computational and Graphical Statistics, 22(1):
193–214, 2013.

E. Gine and A. Guillou. Rates of strong uniform consistency for multivari-

Chen et al./Inference using the Morse-Smale

44

ate kernel density estimators. In Annales de l’Institut Henri Poincare (B)
Probability and Statistics, 2002.

S. Helgason. Diﬀerential geometry, Lie groups, and symmetric spaces, vol-

ume 80. Academic press, 1979.

L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation, 2(1):

193–218, 1985.

J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a

nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.

J. Li, S. Ray, and B. G. Lindsay. A nonparametric statistical approach to
clustering via mode identiﬁcation. Journal of Machine Learning Research,
2007.

J. W. Milnor. Morse theory. Number 51. Princeton university press, 1963.
M. Morse. Relations between the critical points of a real function of n indepen-
dent variables. Transactions of the American Mathematical Society, 27(3):
345–396, 1925.

M. Morse. The foundations of a theory of the calculus of variations in the
large in m-space (second paper). Transactions of the American Mathematical
Society, 32(4):599–631, 1930.

E. A. Nadaraya. On estimating regression. Theory of Probability & Its Appli-

cations, 9(1):141–142, 1964.

S. Paris and F. Durand. A topological approach to hierarchical segmentation us-
ing mean shift. In Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007.

W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal

of the American Statistical association, 66(336):846–850, 1971.

A. Rinaldo, L. Wasserman, et al. Generalized density clustering. The Annals of

Statistics, 38(5):2678–2722, 2010.

A. Rinaldo, A. Singh, R. Nugent, and L. Wasserman. Stability of density-based
clustering. The Journal of Machine Learning Research, 13(1):905–948, 2012.
M. Rizzo and G. Szekely. energy: E-statistics (energy statistics). R package

version, 1:1, 2008.

M. L. Rizzo, G. J. Sz´ekely, et al. Disco analysis: A nonparametric extension of
analysis of variance. The Annals of Applied Statistics, 4(2):1034–1055, 2010.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chap-

man and Hall, 1986.

A. Singh, C. Scott, R. Nowak, et al. Adaptive hausdorﬀ estimation of density

level sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

G. J. Sz´ekely and M. L. Rizzo. Testing for equal distributions in high dimension.

InterStat, 5, 2004.

G. J. Szekely and M. L. Rizzo. Hierarchical clustering via joint between-within
distances: Extending ward’s minimum variance method. Journal of classiﬁ-
cation, 22(2):151–183, 2005.

G. J. Sz´ekely and M. L. Rizzo. A new test for multivariate normality. Journal

of Multivariate Analysis, 93(1):58–80, 2005.

G. J. Sz´ekely and M. L. Rizzo. Energy statistics: A class of statistics based
on distances. Journal of statistical planning and inference, 143(8):1249–1272,

Chen et al./Inference using the Morse-Smale

45

2013.

V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of rela-
tive frequencies of events to their probabilities. Theory of Probability & Its
Applications, 16(2):264–280, 1971.

A. Vedaldi and S. Soatto. Quick shift and kernel methods for mode seeking. In
European Conference on Computer Vision, pages 705–718. Springer, 2008.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clus-
terings comparison: is a correction for chance necessary? In Proceedings of
the 26th Annual International Conference on Machine Learning, pages 1073–
1080. ACM, 2009.

L. Wasserman. All of nonparametric statistics. Springer, 2006.

7
1
0
2
 
r
p
A
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
2
v
6
2
8
8
0
.
6
0
5
1
:
v
i
X
r
a

Electronic Journal of Statistics
ISSN: 1935-7524
arXiv: http://arxiv.org/abs/1506.08826

Statistical Inference Using the
Morse-Smale Complex

Yen-Chi Chen, and Christopher R. Genovese, and Larry Wasserman

University of Washington,
Department of Statistics
Box 354322,
Seattle, WA 98195
e-mail: yenchic@uw.edu

Carnegie Mellon University,
Department of Statistics
5000 Forbes Avenue,
Pittsburgh, PA 15213
e-mail: genovese@stat.cmu.edu; larry@stat.cmu.edu

Abstract: The Morse-Smale complex of a function f decomposes the sam-
ple space into cells where f is increasing or decreasing. When applied to
nonparametric density estimation and regression, it provides a way to rep-
resent, visualize, and compare multivariate functions. In this paper, we
present some statistical results on estimating Morse-Smale complexes. This
allows us to derive new results for two existing methods: mode clustering
and Morse-Smale regression. We also develop two new methods based on
the Morse-Smale complex: a visualization technique for multivariate func-
tions and a two-sample, multivariate hypothesis test.

MSC 2010 subject classiﬁcations: Primary 62G20; secondary 62G86,
62H30.
Keywords and phrases: nonparametric estimation, mode clustering, non-
parametric regression, two sample test, visualization.

1. Introduction

Let f be a smooth, real-valued function deﬁned on a compact set K ∈ Rd. In
this paper, f will be a regression function or a density function. The Morse-
Smale complex of f is a partition of K based on the gradient ﬂow induced by f .
Roughly speaking, the complex consists of sets, called crystals or cells, comprised
of regions where f is increasing or decreasing. Figure 1 shows the Morse-Smale
complex for a two-dimensional function. The cells are the intersections of the
basins of attractions (under the gradient ﬂow) of the function’s maxima and
minima. The function f is piecewise monotonic over cells with respect to some
directions. In a sense, the Morse-Smale complex provides a generalization of
isotonic regression.

Because the Morse-Smale complex represents a multivariate function in terms
of regions on which the function has simple behavior, the Morse-Smale complex
has useful applications in statistics, including in clustering, regression, testing,
and visualization. For instance, when f is a density function, the basins of at-
traction of f ’s modes are the (population) clusters for density-mode clustering

1

Chen et al./Inference using the Morse-Smale

2

(a) Descending manifold

(b) Ascending manifold

(c) d-cell

(d) Morse-Smale complex

Fig 1. An example of a Morse-Smale complex. The green dots are local minima; the blue
dots are local modes; the violet dots are saddle points. Panels (a) and (b) give examples of
descending d-manifolds (blue region) and an ascending 0-manifold (green region). Panel (c)
shows the corresponding d-cell (yellow region). Panel (d) is shows all d-cells.

(also known as mean shift clustering (Fukunaga and Hostetler, 1975; Chac´on
et al., 2015)), each of which is a union of cells from the Morse-Smale complex.
Similarly, when f is a regression function, the cells of the Morse-Smale complex
give regions on which f has simple behavior. Fitting f over the Morse-Smale
cells provides a generalization of nonparametric, isotone regression; Gerber et al.
(2013) proposes such a method. The Morse-Smale representation of a multivari-
ate function f is a useful tool for visualizing f ’s structure, as shown by Gerber
et al. (2010). In addition, suppose we want to compare two multi-dimensional
datasets X = (X1, . . . , Xn) and Y = (Y1, . . . , Ym). We start by forming the
Morse-Smale complex of (cid:98)p − (cid:98)q where (cid:98)p is density estimate from X and (cid:98)q is den-
sity estimate from Y . Figure 2 shows a visualization built from this complex.
The circles represent cells of the Morse-Smale complex. Attached to each cell is
a pie-chart showing what fraction of the cell has (cid:98)p signiﬁcantly larger than (cid:98)q.
This visualization is a multi-dimensional extension of the method proposed for
two or three dimensions in Duong (2013).

For all these applications, the Morse-Smale complex needs to be estimated.
To the best of our knowledge, no theory has been developed for this estimation
problem, prior to this paper. We have three goals in this paper: to show that
many existing problems can be cast in terms of the Morse-Smale complex, to

Chen et al./Inference using the Morse-Smale

3

Fig 2. Graft-versus-Host Disease (GvHD) dataset (Brinkman et al., 2007). This is a d = 4
dimensional dataset. We estimate the density diﬀerence based on the kernel density estimator
and ﬁnd regions where the two densities are signiﬁcantly diﬀerent. Then we visualize the
density diﬀerence using the Morse-Smale complex. Each green circle denotes a d-cell, which
is a partition for the support K. The size of circle is proportional to the size of cell. If two
cells are neighborhors, we add a line connecting them; the thickness of the line denotes the
amount of boundary they share. The pie charts show the ratio of the regions within each cell
where the two densities are signiﬁcantly diﬀerent from each other. See Section 3.4 for more
details.

develop some new statistical methods based on the Morse-Smale complex, and
to develop the statistical theory for estimating the complex.

Main results. The main results of this paper are:

1. Consistency of the Morse-Smale Complex. We prove the stability of the
Morse-Smale complex (Theorem 1) in the following sense: if B and (cid:101)B are
boundaries of the descending d-manifolds (or ascending 0-manifolds) of p
and (cid:101)p (deﬁned in Section 2), then

Haus(B, (cid:101)B) = O ((cid:107)∇p − ∇(cid:101)p(cid:107)∞) .
2. Risk Bound for Mode clustering (mean-shift clustering; section 3.1): We

bound the risk of mode clustering in Theorem 2.

3. Morse-Smale regression (section 3.2): In Theorems 4 and 5, we bound the
risk of Morse-Smale regression, a multivariate regression method proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013)
that synthesizes nonparametric regression and linear regression.

4. Morse-Smale signatures (section 3.3): We introduce a new visualization

method for densities and regression functions.

5. Morse-Smale two-sample testing (section 3.4): We develop a new method

for multivariate two-sample testing that can have good power.

Related work. The mathematical foundations for the Morse-Smale complex

Chen et al./Inference using the Morse-Smale

4

Fig 3. A one dimensional example. The blue dots are local modes and the green dots are
local minima. Left panel: the basins of attraction for two local modes are colored by brown
and orange. Middle panel: the basin of attraction (negative gradient) for the local minima are
colored by red, purple and violet. Right panel: The intersection of the basins, which are called
d-cells.

are from Morse theory (Morse, 1925, 1930; Milnor, 1963). Morse theory has
many applications including computer vision (Paris and Durand, 2007), com-
putational geometry (Cohen-Steiner et al., 2007) and topological data analysis
(Chazal et al., 2014).

Previous work on the stability of the Morse-Smale complex can be found in
Chen et al. (2016) and Chazal et al. (2014) but they only consider critical points
rather than the whole Morse-Smale complex. Arias-Castro et al. (2016) prove
pointwise convergence for the gradient ascent curves but this is not suﬃcient
for proving the stability of the complex because the convergence of complexes
requires convergence of multiple curves and the constants in the convergence
rate derived from Arias-Castro et al. (2016) vary from points to points and some
constants diverge when we are getting closer to the boundaries of complexes.
Thus, we cannot obtain a uniform convergence of gradient ascent curves directly
based on their results. Morse-Smale regression and visualization were proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013).

The R code (Algorithm 1, 2, and 3) used in this paper can be found at

https://github.com/yenchic/Morse_Smale.

2. Morse Theory

To motivate formal deﬁnitions, we start with the simple, one-dimensional ex-
ample depicted in Figure 3. The left panel shows the sets associated with each
local maximum (i.e. the basins of attraction of the maxima). The middle panel
shows the sets associated with each local minimum. The right panel show the
intersections of these basins, which gives the Morse-Smale complex deﬁned by
the function. Each interval in the complex, called a cell, is a region where the
function is increasing or decreasing.

Now we give a formal deﬁnition. Let f : K ⊂ Rd (cid:55)→ R be a function with
bounded third derivatives that is deﬁned on a compact set K. Let g(x) = ∇f (x)
and H(x) = ∇∇f (x) be the gradient and Hessian matrix of f , respectively, and
let λj(x) be the jth largest eigenvalue of H(x). Deﬁne C = {x ∈ K : g(x) = 0}
to be the set of all f ’s critical points, which we call the critical set. Using the

Chen et al./Inference using the Morse-Smale

5

signs of the eigenvalues of the Hessian, the critical set C can be partitioned into
d + 1 distinct subsets C0, · · · , Cd, where

Ck = {x ∈ K : g(x) = 0, λk(x) > 0, λk+1(x) < 0},

k = 1, · · · , d − 1.

(1)

We deﬁne C0, Cd to be the sets of all local maxima and minima (corresponding
to all eigenvalues being negative and positive respectively). The set Ck is called
k−th order critical set.

A smooth function f is called a Morse function (Morse, 1925; Milnor, 1963)
if its Hessian matrix is non-degenerate at each critical point. That is, |λj(x)| >
0, ∀x ∈ C for all j. In what follows we assume f is a Morse function (actually,
later we will assume further that f is a Morse-Smale function).

Given any point x ∈ K, we deﬁne the gradient ascent ﬂow starting at x,

πx : R+ (cid:55)→ K, by

πx(0) = x
π(cid:48)
x(t) = g(π(t)).

(2)

A particle on this ﬂow moves along the gradient from x towards a “destination”
given by

dest(x) ≡ lim
t→∞
It can be shown that dest(x) ∈ C for x ∈ K.

πx(t).

We can thus partition K based on the value of dest(x). These partitions are
called descending manifolds in Morse theory (Morse, 1925; Milnor, 1963). Recall
Ck is the k-th order critical points, we assume Ck = {ck,1, · · · , ck,mk } contains
mk distinct elements. For each k, deﬁne

Dk = {x : dest(x) ∈ Cd−k}
Dk,j = {x : dest(x) = cd−k,j} ,

j = 1, · · · md−k.

(3)

That is, Dk is the collection of all points whose gradient ascent ﬂow converges to
a (d−k)-th order critical point and Dk,j is the collection of points whose gradient
ascent ﬂow converges to the j-th element of Cd−k. Thus, Dk = (cid:83)md−k
j=1 Dk,j. From
Theorem 4.2 in Banyaga and Hurtubise (2004), each Dk is a disjoint union
of k-dimensional manifolds (Dk,j is a k-dimensional manifold). We call Dk,j
a descending k-manifold of f . Each descending k-manifold is a k-dimensional
manifold such that the gradient ﬂow from every point converges to the same
(d − k)-th order critical point. Note that {D0, · · · , Dk} forms a partition of K.
The top panels of Figure 4 give an example of the descending manifolds for a
two dimensional case.

The ascending manifolds are similar to descending manifolds but are deﬁned
through the gradient descent ﬂow. More precisely, given any x ∈ K, a gradient
descent ﬂow γ : R+ (cid:55)→ K starting from x is given by

γx(0) = x
γ(cid:48)
x(t) = −g(π(t)).

(4)

Chen et al./Inference using the Morse-Smale

6

(a)

(b)

(c)

(d)

Fig 4. Two-dimensional examples of critical points, descending manifolds, ascending mani-
folds, and 2-cells. This is the same function as Figure 1. (a): The set Ck for k = 0, 1, 2. The
four blue dots are C0, the collection of local modes (each of them is c0,j some j = 1, · · · , 4).
The four orange dots are C1, the collection of saddle points (each of them is c1,j for some
j = 1, · · · , 4). The green dots are C2, the collection of local minima (each green dot is c2,j
for some j = 1, · · · , 9). (b): The set Dk for k = 0, 1, 2. The yellow area is D2 (each subregion
separated by blue curves are D2,j , j = 1, · · · , 4). The two blue curves are D1 (each of the 4
blue segments are D1,j , j = 1, · · · , 4). The green dots are D0 (also C2), the collection of local
minima (each green dot is D0,j for some j = 1, · · · , 9). (b): The set Ak for k = 0, 1, 2. The
yellow area is A0 (each subregion separated by red curves are A0,j , j = 1, · · · , 9). The two
red curves are A1 (each of the 4 red segments are A1,j , j = 1, · · · , 4). The blue dots are A2
(also C0), the collection of local modes (each green dot is A0,j for some j = 1, · · · , 4). (d):
Example for 2-cells. The thick blue curves are D1 and thick red curves are A1.

Chen et al./Inference using the Morse-Smale

7

Unlike the ascending ﬂow deﬁned in (2), γx is a ﬂow that moves along the
gradient descent direction. The descent ﬂow γx shares similar properties to the
ascent ﬂow πx; the limiting point limt→∞ γx(t) ∈ C is also in critical set when
f is a Morse function. Thus, similarly to Dk and Dk,j, we deﬁne

Ak =

x : lim
t→∞

γx(t) ∈ Cd−k

(cid:110)

(cid:110)

(cid:111)

(cid:111)

Ak,j =

x : lim
t→∞

γx(t) = cd−k,j

,

j = 1, · · · , mj−k.

(5)

Ak and Ak,j have dimension d − k and each Ak,j is a partition for Ak and
{A0, · · · , Ad} consist of a partition for K. We call each Ak,j an ascending k-
manifold to f .

A smooth function f is called a Morse-Smale function if it is a Morse function
and any pair of the ascending and descending manifolds of f intersect each
other transversely (which means that pairs of manifolds are not parallel at their
intersections); see e.g. Banyaga and Hurtubise (2004) for more details. In this
paper, we also assume that f is a Morse-Smale function. Note that by the
Kupka-Smale Theorem (see e.g. Theorem 6.6 in Banyaga and Hurtubise (2004)),
Morse-Smale functions are generic (dense) in the collection of smooth functions.
For more details, we refer to Section 6.1 in Banyaga and Hurtubise (2004).

A k-cell (also called Morse-Smale cell or crystal) is the non-empty intersection
between any descending k1-manifold and an ascending (d − k2)-manifold such
that k = min{k1, k2} (the ascending (d − k2)-manifold has dimension k2). When
we simply say a cell, we are referring to the d-cell since d-cells consists of the
majority of K (the totality of k-cells with k < d has Lebesgue measure 0). The
Morse-Smale complex for f is the collection of all k-cells for k = 0, · · · , d. The
bottom panels of Figure 4 give examples for the ascending manifolds and the
d-cells for d = 2. Another example is given in Figure 1.

The cells of a smooth function can be used to construct an additive de-
composition that is useful in data analysis. For a Morse-Smale function f , let
E1, · · · , EL be its associated cells. Then we can decompose f into

f (x) =

f(cid:96)(x)1(x ∈ E(cid:96)),

(6)

L
(cid:88)

(cid:96)=1

where each f(cid:96)(x) behaves like a multivariate isotonic function (Barlow et al.,
1972; Bacchetti, 1989). Namely, f (x) = f(cid:96)(x) when x ∈ E(cid:96). This decomposition
is because within each E(cid:96), f has exact a local mode and a local minimum on
the boundary of E(cid:96). The fact that f admits such a decomposition will be used
frequently in Section 3.2 and 3.3.

Among all descending/ascending manifolds, the descending d-manifolds and
the ascending 0-manifolds are often of great interest. For instance, mode cluster-
ing (Li et al., 2007; Azzalini and Torelli, 2007) uses the descending d-manifolds
to partition the domain K into clusters. Morse-Smale regression (Gerber and
Potter, 2011; Gerber et al., 2013) ﬁts a linear regression individually over each
d-cell (non-empty intersection of pairs of descending d-manifolds and ascending

Chen et al./Inference using the Morse-Smale

8

(a) Basins of attraction

(b) Gradient ascent

(c) Mode clustering

Fig 5. An example of mode clustering. (a): Basin of attraction for each local mode (red +).
Black dots are data points. (b): Gradient ﬂow (blue lines) for each data point. The gradient
ﬂow starts at one data point and ends at one local modes. (c): Mode clustering; we use the
destination for gradient ﬂow to cluster data points.

0-manifolds). Regions outside descending d-manifolds or ascending 0-manifolds
have Lebesgue measure 0. Thus, later in our theoretical analysis, we will focus
on the stability of the set Dd and A0 (see Section 4.1). We deﬁne boundaries of
Dd as

B ≡ ∂Dd = Dd−1 ∪ · · · ∪ D0.

(7)

The set B will be used frequently in Section 4.

3. Applications in Statistics

3.1. Mode Clustering

Mode clustering (Li et al., 2007; Azzalini and Torelli, 2007; Chac´on and Duong,
2013; Arias-Castro et al., 2016; Chac´on et al., 2015; Chen et al., 2016) is a
clustering technique based on the Morse-Smale complex and is also known as
mean-shift clustering (Fukunaga and Hostetler, 1975; Cheng, 1995; Comaniciu
and Meer, 2002). Mode clustering uses the descending d-manifolds of the density
function p to partition the whole space K. (Although the d-manifolds do not
contain all points in K, the regions outside d-manifolds have Lebesgue measure
0). See Figure 5 for an example.

Now, we brieﬂy describe the procedure of mode clustering. Let X = {X1, · · · , Xn}

be a random sample from density p deﬁned on a compact set K and assumed to
be a Morse function. Recall that dest(x) is the destination of the gradient ascent
ﬂow starting from x. Mode clustering partitions the sample based on dest(x)
for each point; speciﬁcally, it partitions X = X1

(cid:83) · · · (cid:83) XK such that

X(cid:96) = {Xi ∈ X : dest(Xi) = m(cid:96)},

where each m(cid:96) is a local mode of p. We can also view mode clustering as a cluster-
(cid:83) · · · (cid:83) Dd,L
ing technique based on the d-descending manifolds. Let Dd = Dd,1

Chen et al./Inference using the Morse-Smale

9

be the d-descending manifolds of p, assuming that L is the number of local
modes. Then each cluster X(cid:96) = X (cid:84) Dd,(cid:96).

In practice, however, we do not know p so we have to use a density estimator

(cid:98)pn. A common density estimator is the kernel density estimator (KDE):

(cid:98)pn(x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(8)

where K is a smooth kernel function and h > 0 is the smoothing parameter.
Note that mode clustering is not limited to the KDE; other density estimators
also give us a sample-based mode clustering. Based on the KDE, we are able to
estimate gradient (cid:98)gn(x), the gradient ﬂows (cid:98)πx(t), and the destination (cid:100)destn(x)
(note that the mean shift algorithm is an algorithm to perform these tasks).
Thus, we can estimate the d-descending manifolds by the plug-in from (cid:98)pn. Let
(cid:83) · · · (cid:83) (cid:98)Dd,(cid:98)L be the d-descending manifolds of (cid:98)pn, where (cid:98)L is the
(cid:98)Dd = (cid:98)Dd,1
number of local modes of (cid:98)pn. The estimated clusters will be (cid:98)X1, · · · , (cid:98)X
(cid:98)L, where
each (cid:98)X(cid:96) = X (cid:84) (cid:98)Dd,(cid:96). Figure 5 displays an example of mode clustering using the
KDE.

A nice property of mode clustering is that there is a clear population quan-
tity that our estimator (clusters based on the given sample) is estimating: the
population partition of the data points. Thus we can consider properties of the
procedure such as consistency, which we discuss in detail in Section 4.2.

3.2. Morse-Smale Regression

Let (X, Y ) be a random pair where Y ∈ R and Xi ∈ K ⊂ Rd. Estimating the
regression function m(x) = E[Y |X = x] is challenging for d of even moderate
size. A common way to address this problem is to use a simple regression function
that can be estimated with low variance. For example, one might use an additive
regression of the form m(x) = (cid:80)
j mj(xj) which is a sum of one-dimensional
smooth functions. Although the true regression function is unlikely to be of this
form, it is often the case that the resulting estimator is useful.

A diﬀerent approach, Morse-Smale regression (MSR), is suggested in Gerber
et al. (2013). This takes advantage of the (relatively) simple structure of the
Morse-Smale complex and the isotone behavior of the function on each cell.
Speciﬁcally, MSR constructs a piecewise linear approximation to m(x) over the
cells of the Morse-Smale complex.

We ﬁrst deﬁne the population version of the MSR. Let m(x) = E(Y |X = x)
be the regression function and is assumed to be a Morse-Smale function. Let
E1, · · · EL be the d-cells for m. The Morse-Smale Regression for m is a piecewise
linear function within each cell E(cid:96) such that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

(9)

Chen et al./Inference using the Morse-Smale

10

where (µ(cid:96), β(cid:96)) are obtained by minimizing mean square error:
E (cid:0)(Y − mMSR(X))2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1)

= argmin

E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(cid:1)

(10)

µ,β

µ,β

That is, mMSR is the best linear piecewise predictor using the d-cells. One can
also view MSR as using a linear function to approximate f(cid:96) in the additive
model (6). Note that mMSR is well deﬁned except on the boundaries of E(cid:96) that
have Lebesgue measure 0.

Now we deﬁne the sample version of the MSR. Let (X1, Y1), · · · , (Xn, Yn) be
the random sample from the probability measure PX × PY such that Xi ∈ K ⊂
Rd and Yi ∈ R. Throughout section 3.2, we assume the density of covariates X
is bounded, positive and has a compact support K and the response Y has ﬁnite
second moment.

Let (cid:98)mn be a smooth nonparametric regression estimator for m. We call (cid:98)mn
the pilot estimator. For instance, one may use the kernel regression Nadaraya
(1964) (cid:98)mn(x) =
(cid:98)mn as (cid:98)E1, · · · , (cid:98)E(cid:98)L. Using the data (Xi, Yi) within each estimated d-cell, (cid:98)E(cid:96), the
MSR for (cid:98)mn is given by

as the pilot estimator. We deﬁne d-cells for

i=1 YiK( x−Xi
h )
(cid:80)n
i=1 K( x−Xi
h )

(cid:80)n

(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where ((cid:98)µ(cid:96), (cid:98)β(cid:96)) are obtained by minimizing the empirical squared error:

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

(cid:88)

i:Xi∈ (cid:98)E(cid:96)

(Yi − µ − βT Xi)2

(11)

(12)

This MSR is slightly diﬀerent from the original version in Gerber et al. (2013).
We will discuss the diﬀerence in Remark 1. Computing the parameters of MSR
is not very diﬃcult–we only need to compute the cell labels of each observation
(this can be done by the mean shift algorithm or some fast variants such as the
quick-shift algorithm Vedaldi and Soatto 2008) and then ﬁt a linear regression
within each cell.

MSR may give low prediction error in some cases; see Gerber et al. (2013) for
some concrete examples. In Theorem 5, we prove that we may estimate mMSR at
a fast rate. Moreover, the regression function may be visualized by the methods
discussed later.

Remark 1 The original version of Morse-Smale regression proposed in Gerber
et al. (2013) does not use d-cells of a pilot nonparametric estimate (cid:98)mn. Instead,
they directly ﬁnd local modes and minima using the original data points (Xi, Yi).
This saves computational eﬀort but comes with a price: there is no clear popu-
lation quantity being estimated by their approach. That is, when the sample size
increases to inﬁnity, there is no guarantee that their method will converge. In
our case, we apply a consistent pilot estimate for m and construct d-cells on
this pilot estimate. As is shown in Theorem 4, our method is consistent for this
population quantity.

Chen et al./Inference using the Morse-Smale

11

3.3. Morse-Smale Signatures and Visualization

In this section we deﬁne a new method for visualizing multivariate functions
based on the Morse-Smale complex, called Morse-Smale signatures. The idea is
very similar to the Morse-Smale regression but the signatures can be applied to
any Morse-Smale function.

Let E1, · · · , EK be the d-cells (nonempty intersection of a descending d-
manifold and an ascending 0-manifold) for a Morse-Smale function f that has
a compact support K. The function f depends on the context of the problem.
For density estimation, f is the density p or its estimator (cid:98)pn. For regression
problem, f is the regression function m or a nonparametric estimator (cid:98)mn .
For two sample test, f is the density diﬀerence p1 − p2 or the estimated density
diﬀerence (cid:98)p1−(cid:98)p2. Note that E1, · · · , EK form a partition for K except a Lebesgue
measure 0 set. Each cell corresponds to a unique pair of a local mode and a local
minimum. Thus, the local modes and minima along with d-cells form a bipartite
graph which we call it signature graph. The signature graph contains geometric
information about f . See Figure 6 and 7 for examples.

The signature is deﬁned as follows. We project the maxima and minima of
the function into R2 using multidimensional scaling. We connect a maximum
and minimum by an edge if there exists a cell that connects them. The width
of the edge is proportional to the norm of the linear coeﬃcients of the linear
approximation to the function within the cell. The linear approximation is

fMS(x) = η†

(cid:96) + γ†T

(cid:96) x,

for x ∈ E(cid:96),

where η†

(cid:96) ∈ R and γ†

(cid:96) ∈ Rd are parameters from

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

(13)

(14)

This is again a linear approximation for f(cid:96) in the additive model (6). Note that
fMS may not be continuos when we move from one cell to another. The summary
statistics for the edge associated with cell E(cid:96) are the parameters (η†
(cid:96) ). We
call the function fMS the (Morse-Smale) approximation function; it is the best
piecewise-linear representation for f (piecewise linear within each cell) under L2
error given the d-cells. This function is well-deﬁned except on a set of Lebesgue
measure 0 (the boundaries of each cell). See Figure 6 for a example on the
approximation function. The details are in Algorithm 1.

(cid:96) , γ†

Example. Figure 7 is an example using the GvHD dataset. We ﬁrst conduct
multidimensional scaling (Kruskal, 1964) on the local modes and minima for f
and plot them on the 2-D plane. In Figure 7, the blue dots are local modes and
the green dots are local minima. These dots act as the nodes for the signature
graph. Then we add edges, representing the cells for f that connect pairs of local
modes and minima, to form the signature graph. Lastly, we adjust the width
for the edges according to the strength (L2 norm) of regression function within
each cell (i.e. (cid:107)γ†
(cid:96) (cid:107)). Algorithm 1 provides a summary for visualizing a general
multivariate function using what we described in this paragraph.

Chen et al./Inference using the Morse-Smale

12

(a) Original function

(b) Approximation function

(c) Signature graph

Fig 6. Morse-Smale signatures for a smooth function. (a): The original function. The blue
dots are local modes, the green dots are local minima and the pink dot is a saddle point. (b):
The Morse-Smale approximation to (a). This is the best piecewise linear approximation to the
original function given d-cells. (c): This bipartite graph has nodes that are local modes and
minima and edges that represent the d-cells. Note that we can summarize the smooth function
(a) by the signature graph (c) and the parameters for constructing approximation function
(b). The signature graph and parameters for approximation function deﬁne the Morse-Smale
signatures.

Algorithm 1 Visualization using Morse-Smale Signatures

Input: Grid points x1, · · · , xN and the functional evaluations f (x1), · · · , f (xN ).
1. Find local modes and minima of f on the discretized points x1, · · · , xN . Let M1, · · · MK
and m1, · · · , mS denote the grid points for modes and minima.
2. Partition {x1, · · · , xN } into X1, · · · XL according to the d-cells of f (1. and 2. can be done
by using a k-nearest neighbor gradient ascent/descent method; see Algorithm 1 in Gerber
et al. (2013)).
3. For each cell X(cid:96), ﬁt a linear regression with (Xi, Yi) = (xi, f (xi)), where xi ∈ X(cid:96). Let
the regression coeﬃcients (without intercept) be β(cid:96).
4. Apply multidimensional scaling to modes and minima jointly. Denote their 2 dimensional
representation points as

1 , · · · M ∗

5. Plot {M ∗
6. Add edge to a pair of mode and minimum if there exist a cell that connects them. The
width of the edge is in proportional to (cid:107)β(cid:96)(cid:107) (for cell X(cid:96)).

1, · · · , m∗

K , m∗

1 , · · · M ∗

K , m∗

1, · · · , m∗

S }.

{M ∗
S }.

Chen et al./Inference using the Morse-Smale

13

Fig 7. Morse-Smale Signature visualization (Algorithm 1) of the density diﬀerence for GvHD
dataset (see Figure 2). The blue dots are local modes; the green dots are local minima; the
brown lines are d-cells. These dots and lines form the signature graph. The width indicates
the L2 norm for the slope of regression coeﬃcients. i.e. (cid:107)γ†
(cid:96) (cid:107). The location for modes and
minima are obtained by multidimensional scaling so that the relative distance is preserved.

3.4. Two Sample Comparison

The Morse-Smale complex can be used to compare two samples. There are two
ways to do this. The ﬁrst one is to test the diﬀerence in two density functions
locally and then use the Morse-Smale signatures to visualize regions where the
two samples are diﬀerent. The second approach is to conduct a nonparametric
two sample test within each Morse-Smale cell. The advantage of the ﬁrst ap-
proach is that we obtain a visual display on where the two densities are diﬀerent.
The merit of the second method is that we gain additional power in testing the
density diﬀerence by using the shape information.

3.4.1. Visualizing the Density Diﬀerence

Let X1, . . . Xn and Y1, . . . , Ym be two random sample with densities pX and pY .
In a two sample comparison, we not only want to know if pX = pY but we also
want to ﬁnd the regions that they signiﬁcantly disagree. That is, we are doing
the local tests

H0(x) : pX (x) = pY (x)
(15)
simultaneously for all x ∈ K and we are interested in the regions where we reject
H0(x). A common approach is to estimate the density for both sample by the
KDE and set a threshold to pickup those regions that the density diﬀerence is

Chen et al./Inference using the Morse-Smale

14

large. Namely, we ﬁrst construct density estimates

(cid:98)pX (x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(cid:98)pY (x) =

1
mhd

m
(cid:88)

i=1

(cid:19)

(cid:18) x − Yi
h

K

and then compute (cid:98)f (x) = (cid:98)pX (x) − (cid:98)pY (x). The regions
(cid:111)
(cid:110)
x ∈ K : | (cid:98)f (x)| > λ

Γ(λ) =

(16)

(17)

are where we have strong evidence to reject H0(x). The threshold λ can be
picked by quantile values of the bootstrapped L∞ density deviation to control
type 1 error or can be chosen by controlling the false discovery rate (Duong,
2013).

Unfortunately, Γ(λ) is hard to visualize when d > 3. So we use the Morse-
Smale complex for (cid:98)f and visualize Γ(λ) by its behavior on the d-cells of the
complex. Algorithm 2 gives a method for visualizing density diﬀerences like
Γ(λ) in the context of comparing two independent samples.

Algorithm 2 Visualization For Two Sample Test

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, threshold λ and radius constant r0
1. Compute the density estimates (cid:98)pX and (cid:98)pY .
2. Compute the diﬀerence function (cid:98)f = (cid:98)pX − (cid:98)pY and the signiﬁcant regions
(cid:111)
x ∈ K : (cid:98)f (x) < −λ

x ∈ K : (cid:98)f (x) > λ

, Γ−(λ) =

Γ+(λ) =

(18)

(cid:110)

(cid:111)

(cid:110)

3. Find the d-cells for (cid:98)f , denoted as E1, · · · , EL.
4. For cell E(cid:96), do (4-1) and (4-2):
4-1. compute the cell center e(cid:96), cell size V(cid:96) = Vol(E(cid:96)),
4-2. compute the positive signiﬁcant ratio and negative signiﬁcant ratio

r+
(cid:96) =

Vol(E(cid:96) ∩ Γ+(λ))
Vol(E(cid:96))

,

r−
(cid:96) =

Vol(E(cid:96) ∩ Γ−(λ))
Vol(E(cid:96))

.

5. For every pair of cell Ej and E(cid:96) (j (cid:54)= (cid:96)), compute the shared boundary size:

Bj(cid:96) = Vold−1( ¯Ej ∩ ¯E(cid:96)),

(19)

(20)

where Vold−1 is the d − 1 dimensional Lebesgue measure.
6. Do multidimensional scaling (Kruskal, 1964) to e1, · · · , eL to obtain low dimensional
representation (cid:101)e1, · · · , (cid:101)eL.
7. Place a ball center at each (cid:101)e(cid:96) with radius r0 ×
8. If r+

(cid:96) > 0, add a pie chart center at (cid:101)e(cid:96) with radius r0 ×
.

(cid:18) r+
(cid:96) +r−
r+
9. Add a line to connect two nodes (cid:101)ej and (cid:101)e(cid:96) if Bj(cid:96) > 0. We may adjust the thickness of
the line according to Bj(cid:96).

chart contains two groups, each with ratio

r−
(cid:96)
(cid:96) +r−
r+

(cid:96) ). The pie

V(cid:96) × (r+

(cid:96) + r−

(cid:96) + r−

V(cid:96).

√

√

(cid:19)

,

(cid:96)

(cid:96)

(cid:96)

An example for Algorithm 2 is in Figure 2, in which we apply the visualization
algorithm for the the GvHD dataset by using kernel density estimator. We
choose the threshold λ by bootstrapping the L∞ diﬀerence for (cid:98)f i.e. supx | (cid:98)f ∗(x)−
(cid:98)f (x)|, where (cid:98)f ∗ is the density diﬀerence for the bootstrap sample. We pick
α = 95% upper quantile value for the bootstrap deviation as the threshold.

Chen et al./Inference using the Morse-Smale

15

The radius constant r0 is deﬁned by the user. It is a constant for visualiza-
tion and does not aﬀect the analysis. Algorithm 2 preserves the relative position
for each cell and visualizes the cell according to its size. The pie-chart provides
the ratio of regions where the two densities are signiﬁcantly diﬀerent. The lines
connecting two cells provide the geometric information about how cells are con-
nected to each other.

By applying Algorithm 2 to the GvHD dataset (Figure 2), we ﬁnd that there
are 6 cells and one cell much larger than the others. Moreover, in most regions,
the blue regions are larger than the red areas. This indicates that compared
to the density of the control group, the density of the GvHD group seem to
concentrates more so that the regions above the threshold are larger.

3.4.2. Morse-Smale Two-Sample Test

Here we introduce a technique combining the energy test (Baringhaus and Franz,
2004; Sz´ekely and Rizzo, 2004, 2013) and the Morse-Smale complex to conduct
a two sample test. We call our method the Morse-Smale Energy test (MSE test).
The advantage of the MSE test is that it is a nonparametric test and its power
can be higher than the energy test; see Figure 8. Moreover, we can combine
our test with the visualization tool proposed in the previous section (Algorithm
2); see Figure 9 for an example for displaying p-values from MSE test when
visualizing the density diﬀerence.

Before we introduce our method, we ﬁrst review the ordinary energy test.
Given two random variables X ∈ Rd and Y ∈ Rd, the energy distance is deﬁned
as

E(X, Y ) = 2E(cid:107)X − Y (cid:107) − E(cid:107)X − X (cid:48)(cid:107) − E(cid:107)Y − Y (cid:48)(cid:107),
where X (cid:48) and Y (cid:48) are iid copies of X and Y . The energy distance has several
useful applications such as the goodness-of-ﬁt testing (Sz´ekely and Rizzo, 2005),
two sample testing (Baringhaus and Franz, 2004; Sz´ekely and Rizzo, 2004, 2013),
clustering (Szekely and Rizzo, 2005), and distance components (Rizzo et al.,
2010) to name but few. We recommend an excellent review paper in (Sz´ekely
and Rizzo, 2013).

(21)

For the two sample test, let X1, · · · , Xn and Y1, · · · , Ym be the two samples

we want to test. The sample version of energy distance is

(cid:98)E(X, Y ) =

(cid:107)Xi − Yj(cid:107) −

(cid:107)Xi − Xj(cid:107) −

(cid:107)Yi − Yj(cid:107).

2
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

(22)
P→ 0.
If X and Y are from the sample population (the same density), (cid:98)E(X, Y )
Numerically, we use the permutation test for computing the p-value for (cid:98)E(X, Y ).
This can be done quickly in the R-package ‘energy’ (Rizzo and Szekely, 2008).
Now we formally introduce our testing procedure: the MSE test (see Algo-
rithm 3 for a summary). Our test consists of three steps. First, we split the data
into two halves. Second, we use one half of the data (contains both samples)

Chen et al./Inference using the Morse-Smale

16

to do a nonparametric density estimation (e.g. the KDE) and then compute
the Morse-Smale complex (d-cells). Last, we use the other half of the data to
conduct the energy distance two sample test ‘within each d-cell’. That is, we
partition the second half of the data by the d-cells. Within each cell, we do the
energy distance test. If we have L cells, we will have L p-values from the energy
distance test. We reject H0 if any one of the L p-values is smaller than α/L
(this is from Bonferroni correction). Figure 9 provides an example for using the
above procedure (Algorithm 3) along with the visualization method proposed
in Algorithm 2. Data splitting is used to avoid using the same data twice, which
ensures we have a valid test.

Algorithm 3 Morse-Smale Energy Test (MSE test)

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, smoothing parameter h, signiﬁcance
level α
1. Randomly split the data into halves D1 and D2; both contain equal number of X and Y
(assuming n and m are even).
2. Compute the KDE (cid:98)pX and (cid:98)pY by the ﬁrst sample D1.
3. Find the d-cells for (cid:98)f = (cid:98)pX − (cid:98)pY , denoted as E1, · · · , EL.
4. For cell E(cid:96), do 4-1 and 4-2:
4-1. Find X and Y in the second sample D2,
4-2. Do the energy test for two sample comparison. Let the p-value be p((cid:96))
5. Reject H0 if p((cid:96)) < α/L for some (cid:96).

Example. Figure 8 shows a simple comparison for the proposed MSE test to
the usual Energy test. We consider a K = 4 Gaussian mixture model in d = 2
with standard deviation of each component being the same σ = 0.2 and the
proportion for each component is (0.2, 0.5, 0.2, 0.1). The left panel displays a
sample with N = 500 from this mixture distribution. We draw the ﬁrst sample
from this Gaussian mixture model. For the second sample, we draw a similar
Gaussian mixture model except that we change the deviation of one component.
In the middle panel, we change the deviation to the third component (C3 in
left panel, which contains 20% data points). In the right panel, we change the
deviation to the fourth component (C4 in left panel, which contains 10% data
points). We use signiﬁcance level α = 0.05 and for MSE test, we consider the
Bonferroni correction and the smoothing bandwidth is chosen using Silverman’s
rule of thumb (Silverman, 1986).

Note that in both the middle and the right panels, the left most case (added
deviation equals 0) is where H0 should not be rejected. As can be seen from
Figure 8, the MSE test has much stronger power compared to the usual Energy
test.

The original energy test has low power while the MSE test has higher power.
This is because the two distributions only diﬀer at a small portion of the regions
so that a global test like energy test requires large sample sizes to detect the
diﬀerence. On the other hand, the MSE test partitions the space according to
the density diﬀerence so that it is capable of detecting the local diﬀerence.

Example. In addition to the higher power, we may combine the MSE test
with the visualization tool in Algorithm 2. Figure 9 displays an example where

Chen et al./Inference using the Morse-Smale

17

Fig 8. An example comparing the Morse-Smale Energy test to the original Energy test. We
consider a d = 2, K = 4 Gaussian mixture model. Left panel: an instance for the Gaussian
mixture. We have four mixture components, denoting as C1, C2, C3 and C4. They have equal
standard deviation (σ = 0.2) and the proportions for each components are (0.2, 0.5, 0.2, 0.1).
Middle panel: We changed the standard deviations of component C3 to 0.3, 0.4 and 0.5 and
compute the power for the MSE test and the usual Energy test at sample size N = 500 and
1000. (Standard deviation equals 0.2 is where H0 should not be rejected.) Right panel: We
add the variance of component C4 (the smallest component) and do the same comparison as
in the middle panel. We pick the signiﬁcance level α = 0.05 (gray horizontal line) and in the
MSE test, we reject H0 if the minimal p-value is less than α/L, where L is the number of
cells (i.e. we are using the Bonferroni correction).

we visualize the density diﬀerence and simultaneously indicate the p-values from
the Energy test within each cell using the GvHD dataset. This provides us more
information about how two distributions diﬀer from each other.

4. Theoretical Analysis

We ﬁrst deﬁne some notation for the theoretical analysis. Let f be a smooth
function. We deﬁne (cid:107)f (cid:107)∞ = supx |f (x)| to be the L∞-norm of f . In addition, let
(cid:107)f (cid:107)j,max denote the elementwise L∞-norm for j-th derivatives of f . For instance,

(cid:107)f (cid:107)1,max = max

(cid:107)gi(x)(cid:107)∞,

i

(cid:107)f (cid:107)2,max = max
i,j

(cid:107)Hij(x)(cid:107)∞.

We also deﬁne (cid:107)f (cid:107)0,max = (cid:107)f (cid:107)∞. We further deﬁne

(cid:107)f (cid:107)∗
The quantity (cid:107)f − h(cid:107)∗
h up to (cid:96)-th order derivative.

(cid:96),max = max {(cid:107)f (cid:107)j,max : j = 0, · · · , (cid:96)} .
(cid:96),max measures the diﬀerence between two functions f and

(23)

For two sets A, B, the Hausdorﬀ distance is

Haus(A, B) = inf{r : A ⊂ B ⊕ r, B ⊂ A ⊕ r},

(24)

where A ⊕ r = {y : minx∈A (cid:107)x − y(cid:107) ≤ r}. The Hausdorﬀ distance is like the L∞
distance for sets.

Let (cid:101)f : K ⊂ Rd (cid:55)→ R be a smooth function with bounded third derivatives.
Note that as long as (cid:107) (cid:101)f −f (cid:107)∗
3,max is small, (cid:101)f is also a Morse function by Lemma 9.
Let (cid:101)D denote the boundaries of the descending d-manifolds of (cid:101)f . We will show
if (cid:107)f − (cid:101)f (cid:107)∗

3,maxis suﬃciently small, then Haus( (cid:101)D, D) = O((cid:107) (cid:101)f − f (cid:107)1,max).

Chen et al./Inference using the Morse-Smale

18

Fig 9. An example using both Algorithm 2 and 3 to the GvHD dataset introduced in Figure 2.
We use data splitting as described in Algorithm 3. For the ﬁrst part of the data, we compute
the cells and visualize the cells using Algorithm 2. Then we apply the energy distance two
sample test for each cell as described in Algorithm 3 and we annotate each cell with a p-
value. Note that the visualization is slightly diﬀerent to Figure 2 since we use only half of the
original dataset in this case.

4.1. Stability of the Morse-Smale Complex

Before we state our theorem, we ﬁrst derive some properties of descending mani-
folds. Recall that we are interested in B = ∂Dd, the boundary of the descending
d-manifolds (and B is also the union of all j-descending manifolds for j < d).
Since each Dj is a collection of smooth j-dimensional manifolds embedded in
Rd, for every x ∈ Dj, there exists a basis v1(x), · · · , vd−j(x) such that each vk(x)
is perpendicular to Dj at x for k = 1, · · · d − j (Bredon, 1993; Helgason, 1979).
That is, v1(x), · · · , vd−j(x) span the normal space to Dj at x. For simplicity, we
write

V (x) = (v1(x), · · · , vd−j(x)) ∈ Rd×(d−j)

(25)

for x ∈ Dj.

Note the number of columns d − j ≡ d − j(x) in V (x) depends on which Dj
the point x belongs to. We use j rather than j(x) to simplify the notation. For
instance, if x ∈ D1, V (x) ∈ Rd×(d−1) and if x ∈ Dd−1, V (x) ∈ Rd×1. We also
let

V(x) = span{v1(x), · · · , vd−j(x)}
(26)
denote the normal space to B at x. One can view V(x) as the normal map of
the manifold Dj at x ∈ Dj.

For each x ∈ B, deﬁne the projected Hessian

HV (x) = V (x)T H(x)V (x),

(27)

which is the Hessian matrix of p by taking gradients along column space of
V (x). If x ∈ Dj, HV (x) is a (d − j) × (d − j) matrix. The eigenvalues of HV (x)

Chen et al./Inference using the Morse-Smale

19

determine how the gradient ﬂows are moving away from B. We let λmin(M ) be
the smallest eigenvalue for a symmetric matrix M . If M is a scalar (just one
point), then λmin(M ) = M .

Assumption (D): We assume that Hmin = minx∈B λmin(HV (x)) > 0.

This assumption is very mild; it requires that the gradient ﬂow moves away
from the boundary of ascending manifolds. In terms of mode clustering, this
requires the gradient ﬂow to move away from the boundaries of clusters. For a
point x ∈ Dd−1, let v1(x) be the corresponding normal direction. Then the gradi-
ent g(x) is normal to v1(x) by deﬁnition. That is, v1(x)T g(x) = v1(x)T ∇p(x) =
0, which means that the gradient along v1(x) is 0. Assumption (D) means that
the the second derivative along v1(x) is positive, which implies that the density
along direction v1(x) behaves like a local minimum at point x. Intuitively, this
is how we expect the density to behave around the boundaries: gradient ﬂows
are moving away from the boundaries (except for those ﬂows that are already
on the boundaries).
Theorem 1 (Stability of descending d-manifolds) Let f, (cid:101)f : K ⊂ Rd (cid:55)→ R
be two smooth functions with bounded third derivatives deﬁned as above and
let B, (cid:101)B be the boundaries of the associated ascending manifolds. Assume f is
a Morse function satisfying condition (D). When (cid:107)f − (cid:101)f (cid:107)∗
3,max is suﬃciently
small,

Haus( (cid:101)B, B) = O((cid:107) (cid:101)f − f (cid:107)1,max).

(28)

This theorem shows that the boundaries of descending d-manifolds for two Morse
functions are close to each other and the diﬀerence between the boundaries is
controlled by the rate of the ﬁrst derivative diﬀerence.

Similarly to descending manifolds, we can deﬁne all the analogous quantities

for ascending manifolds. We introduce the following assumption:

Assumption (A): We assume Hmax = maxx∈∂A0 λmax(HV (x)) < 0.

Note that λmax(M ) denotes the largest eigenvalue of a matrix M . If M is a
scalar, λmax(M ) = M . Under assumption (A), we have a similar stability result
(Theorem 1) for ascending manifolds. Assumptions (A) and (D) together imply
the stability of d-cells.

Theorem 1 can be applied to nonparametric density estimation. Our goal is to
estimate the boundary of the descending d-manifolds, B, of the unknown popu-
lation density function p. Our estimator is (cid:98)Bn, the boundary of the descending

Chen et al./Inference using the Morse-Smale

20

d-manifolds to a nonparametric density estimator e.g. the kernel density esti-
mate (cid:98)pn. Then under certain regularity condition, their diﬀerence is given by

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107)(cid:98)pn − p(cid:107)1,max) .

We will see this result in the next section when we discuss mode clustering.

Similar reasoning works for the nonparametric regression case. Assume that
we are interested in B, the boundary of descending d-manifolds, for the regres-
sion function m(x) = E(Y |X = x). And our estimator (cid:98)B is again a plug-in
estimate based on (cid:98)mn(x), a nonparametric regression estimator (e.g., kernel
estimator). Then under mild regularity conditions,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107) (cid:98)mn − m(cid:107)1,max) .

4.2. Consistency of Mode Clustering

A direct application of Theorem 1 is the consistency of mode clustering. Let
K (α) be the α-th derivative of K and let BCr denote the collection of functions
with bounded continuously derivatives up to the r-th order. We consider the
following two assumptions on the kernel function:
(K1) The kernel function K ∈ BC3 and is symmetric, non-negative and

(cid:90)

x2K (α)(x)dx < ∞,

(cid:90) (cid:16)

(cid:17)2

K (α)(x)

dx < ∞

for all α = 0, 1, 2, 3.

(K2) The kernel function satisﬁes condition K1 of Gine and Guillou (2002). That

is, there exists some A, v > 0 such that for all 0 < (cid:15) < 1, supQ N (K, L2(Q), CK(cid:15)) ≤
(cid:0) A
, where N (T, d, (cid:15)) is the (cid:15)−covering number for a semi-metric space
(cid:15)

(cid:1)v

(T, d) and

(cid:40)

K =

u (cid:55)→ K (α)

: x ∈ Rd, h > 0, |α| = 0, 1, 2

(cid:19)

(cid:18) x − u
h

(cid:41)
.

(K1) is a common assumption; see Wasserman (2006). (K2) is a weak assump-
tion guarantee the consistency for KDE under L∞ norm; this assumption ﬁrst
appeared in Gine and Guillou (2002) and has been widely assumed (Einmahl
and Mason, 2005; Rinaldo et al., 2010; Genovese et al., 2012; Rinaldo et al.,
2012; Genovese et al., 2014; Chen et al., 2015).
Theorem 2 (Consistency for mode clustering) Let p, (cid:98)pn be the density func-
tion and the KDE. Let B and (cid:98)Bn be the boundaries of clusters by mode clus-
tering over p and (cid:98)pn respectively. Assume (D) for p and (K1–2), then when
log n
nhd+6 → 0, h → 0,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O((cid:107)(cid:98)pn − p(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Chen et al./Inference using the Morse-Smale

21

The proof is simply to combine Theorem 1 and the rate of convergence for
estimating the gradient of density using KDE (Theorem 8). Thus, we omit the
proof. Theorem 2 gives a bound for the rate of convergence for the boundaries
for mode clustering. The rate can be decomposed into two parts, the bias O(h2)

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

and the (square root of) variance OP

. This rate is the same for the

L∞-loss of estimating the gradient of a density function, which makes sense
since the mode clustering is completely determined by the gradient of density.
Another way to describe the consistency for mode clustering is to show that
the proportion of data points that are incorrectly clustered (mis-clustered) con-
verges to 0. This can be quantiﬁed by the use of Rand index (Rand, 1971; Hubert
and Arabie, 1985; Vinh et al., 2009), which measures the similarity between two
partitions of the data points. Let dest(x) and (cid:100)destn(x) be the destination of
gradient of the true density function p(x) and the KDE (cid:98)pn(x). For a pair of
points x, y, we deﬁne

Ψ(x, y) =

(cid:26) 1
0

if dest(x) = dest(y)
if dest(x) (cid:54)= dest(y)

,

(cid:98)Ψn(x, y) =

(cid:40)

1
0

if (cid:100)destn(x) = (cid:100)destn(y)
if (cid:100)destn(x) (cid:54)= (cid:100)destn(y)

(29)
Thus, Ψ(x, y) = 1 if x, y are in the same cluster and 0 if they are not. The Rand
index for mode clustering using p versus using (cid:98)pn is

rand ((cid:98)pn, p) = 1 −

(cid:19)−1

(cid:18)n
2

(cid:88)

i(cid:54)=j

(cid:12)
(cid:12)
(cid:12)Ψ(Xi, Xj) − (cid:98)Ψn(Xi, Xj)

(cid:12)
(cid:12)
(cid:12) ,

(30)

which is the proportion of pairs of data points that the two clustering results
disagree on. If two clusterings output the same partition, the Rand index will
be 1.

Theorem 3 (Bound on Rand Index) Assume (D) for p and (K1–2). Then
when log n

nhd+6 → 0, h → 0, the adjusted Rand index

rand ((cid:98)pn, p) = 1 − O(h2) − OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Theorem 3 shows that the Rand index converges to 1 in probability, which
establishes the consistency of mode clustering in an alternative way. Theo-
rem 3 shows that the proportion of data points that are incorrectly assigned
(compared with mode clustering using population p) is bounded by the rate

O(h2) + OP

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

asymptotically.

Azizyan et al. (2015) also derived the convergence rate of the mode clustering
for the rand index. Here we brieﬂy compare our results to theirs. Azizyan et al.
(2015) consider a low-noise condition that leads to a fast convergence rate when
clusters are well-separated. Their approach can even be applied to the case of

(31)

(32)

(33)

(34)

Chen et al./Inference using the Morse-Smale

22

increasing dimensions. In our case (Theorem 3), we consider a ﬁxed dimension
scenario but we do not assume the low-noise condition. Thus, the main diﬀerence
between Theorem 3 and the result in Azizyan et al. (2015) is the assumptions
being made so our result complements the ﬁndings in Azizyan et al. (2015).

4.3. Consistency of Morse-Smale Regression

In what follows, we will show that (cid:98)mn,MSR(x) is a consistent estimator of mMSR(x).
Recall that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

where E(cid:96) is the d-cell deﬁned on m and the parameters are
E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1) .

µ,β

And (cid:98)mn,MSR is the two-stage estimator to mMSR(x) deﬁned by
(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where { (cid:98)E(cid:96) : (cid:96) = 1, · · · , (cid:98)L} are the collection of cells of the pilot nonparametric
regression estimator (cid:98)mn and (cid:98)µ(cid:96), (cid:98)β(cid:96) are the regression parameters from equation
(12):
(cid:88)

(Yi − µ − βT Xi)2.

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

i:Xi∈ (cid:98)E(cid:96)
Theorem 4 (Consistency of Morse-Smale Regression) Assume (A) and
(D) for m and assume m is a Morse-Smale function. Then when log n
nhd+6 →
0, h → 0, we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

+ O ((cid:107) (cid:98)mn − m(cid:107)1,max)

(35)

(cid:19)

(cid:18) 1
√
n

uniformly for all x except for a set Nn with Lebesgue measure OP((cid:107) (cid:98)mn−m(cid:107)1,max),
Theorem 4 states that when we have a consistent pilot nonparametric regression
estimator (such as the kernel regression), the proposed MSR estimator converges
to the population MSR. Similarly as in Theorem 6, the set Nn are regions around
the boundaries of cells where we cannot distinguish their host cell. Note that
when we use the kernel regression as the pilot estimator (cid:98)mn, Theorem 4 becomes

|mMSR(x) − (cid:98)mn,MSR(x)| = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

under regular smoothness conditions.

Now we consider a special case where we may obtain parametric rate of
(cid:83) · · · (cid:83) EL) be the boundaries

convergence for estimating mMSR. Let E = ∂ (E1
of all cells. We consider the following low-noise condition:

P (X ∈ E ⊕ (cid:15)) ≤ A(cid:15)β,

(36)

Chen et al./Inference using the Morse-Smale

23

for some A, β > 0. Equation (36) is Tsybakov’s low noise condition (Audibert
et al., 2007) applied to the boundaries of cells. Namely, (36) states that it is
unlikely to many observations near the boundaries of cells of m. Under this
low-noise condition, we obtain the following result using kernel regression.

Theorem 5 (Fast Rate of Convergence for Morse-Smale Regression)
Let the pilot estimator (cid:98)mn be the kernel regression estimator. Assume (A)
and (D) for m and assume m is a Morse-Smale function. Assume also (36)
holds for the covariate X and (K1-2) for the kernel function. Also assume that
. Then uniformly for all x except for a set Nn with

(cid:17)1/(d+6)(cid:19)

(cid:18)(cid:16) log n

h = O

n

Lebesgue measure OP

(cid:18)(cid:16) log n

(cid:17)2/(d+6)(cid:19)

,

n

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(cid:19)

(cid:18) 1
√
n

+ OP

(cid:32)(cid:18) log n
n

(cid:19)2β/(d+6)(cid:33)

.

(37)

Therefore, when β > 6+d

4 , we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(38)

(cid:18) 1
√
n

(cid:19)

.

Theorem 5 shows that when the low noise condition holds, we obtain a fast
rate of convergence for estimating mMSR. Note that the pilot estimator (cid:98)mn does
not ahve to be a kernel estimator; other approaches such as the local polynomial
regression will also work.

4.4. Consistency of the Morse-Smale Signature

Another application of Theorem 1 is to bound the diﬀerence of two Morse-
Smale signatures. Let f be a Morse-Smale function with cells E1, . . . , EL. Recall
that the Morse-Smale signatures are the bipartite graph and summary statistics
(locations, density values) for local modes, local minima, and cells. It is known in
the literature (see, e.g., Lemma 9) that when two functions (cid:101)f , f are suﬃciently
close, then

(cid:16)

(cid:17)

max
j

(cid:107)(cid:101)cj − cj(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)1,max

, max

(cid:107) (cid:101)f ((cid:101)cj) − f (cj)(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)∞

j

(39)
where (cid:101)cj, cj are critical points (cid:101)f and f respectively. This implies the stability of
local modes and minima.

So what we need is the stability of the summary statistics (η†

(cid:96) , γ†

(cid:96) ) associated

with the edges (cells). Recall that these summaries are deﬁned through (14)

(cid:16)

(cid:17)

,

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Chen et al./Inference using the Morse-Smale

24

For another function (cid:101)f , let ((cid:101)η†
(cid:96) ) be its signatures for cell (cid:101)E(cid:96). The following
theorem shows that if two functions are close, their corresponding Morse-Smale
signatures are also close.

(cid:96) , (cid:101)γ†

Theorem 6 Let f be a Morse-Smale function satisfying assumptions A and D,
and let (cid:101)f be a smooth function. Then when log n
nhd+6 → 0, h → 0, after relabeling
the indices of cells of (cid:101)f ,

(cid:110)
(cid:107)(cid:101)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:101)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

1,max

(cid:17)

.

Theorem 6 shows stability of the signatures (η†

(cid:96) , γ†

(cid:96) ). Note that Theorem 6

also implies that the stability of piecewise approximation

|fMS(x) − (cid:101)fMS(x)| = O

(cid:16)

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

Together with the stability of critical points (39), Theorem 6 proves the stability
of Morse-Smale signatures.

4.4.1. Example: Morse-Smale Density Estimation

As an example for Theorem 6, we consider density estimation. Let p be the
density of random sample X1, · · · , Xn and recall that (cid:98)pn is the kernel density
estimator. Let (η†
(cid:96) ) be the
signature for (cid:98)pn under cell (cid:98)E(cid:96). The following corollary guarantees the consistency
of Morse-Smale signatures for the KDE.

(cid:96) ) be the signature for p under cell E(cid:96) and ((cid:98)η†

(cid:96) , (cid:98)γ†

(cid:96) , γ†

Corollary 7 Assume (A,D) holds for p and the kernel function satisﬁes (K1–
2). Then when log n

nhd+6 → 0, h → 0, after relabeling we have
(cid:32)(cid:114)

(cid:110)
(cid:107)(cid:98)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:98)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

= O(h2) + OP

(cid:33)

.

log n
nhd+2

The proof to Corollary 7 is a simple application of Theorem 6 with the rate of
convergence for the ﬁrst derivative of the KDE (Theorem 8). So we omit the

proof. The optimal rate in Corollary 7 is OP

when we choose h to

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 2

n

be of order O

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 1

.

n

Remark 2 When we compute the Morse-Smale approximation function, we
may have some numerical problem in low-density regions because the density
estimate (cid:98)pn may have unbounded support. In this case, some cells may be un-
bounded, and the majority of these cells may have extremely low density value,
which makes the approximation function 0. Thus, in practice, we will restrict

Chen et al./Inference using the Morse-Smale

25

ourselves only to the regions whose density is above a pre-deﬁned threshold λ so
that every cell is bounded. A simple data-driven threshold is λ = 0.05 supx (cid:98)pn(x).
Note that Theorem 7 still works in this case but with a slight modiﬁcation: the
cells are deﬁne on the regions {x : ph(x) ≥ 0.05 × supx ph(x)}.

Remark 3 Note that for a density function, local minima may not exist or the
gradient ﬂow may not lead us to a local minimum in some regions. For instance,
for a Gaussian distribution, there is no local minimum and except for the center
of the Gaussian, if we follow the gradient descent path, we will move to inﬁnity.
Thus, in this case we only consider the boundaries of ascending 0-manifolds
corresponding to well-deﬁned local minima and assumptions (A) is only for the
boundaries corresponding to these ascending manifolds.

Remark 4 When we apply the Morse-Smale complex to nonparametric density
estimation or regression, we need to choose the tuning parameter. For instance,
in the MSR, we may use kernel regression or local polynomial regression so we
need to choose the smoothing bandwidth. For the density estimation problem
or mode clustering, we need to choose the smoothing bandwidth for the kernel
smoother. In the case of regression, because we have the response variable, we
would recommend to choose the tuning parameter by cross-validation. For the
kernel density estimator (and mode clustering), because the optimal rate depends
on the gradient estimation, we recommend choosing the smoothing bandwidth
using the normal reference rule for gradient estimation or the cross-validation
method for gradient estimation (Duong et al., 2007; Chac´on et al., 2011).

5. Discussion

In this paper, we introduced the Morse-Smale complex and the summary sig-
natures for nonparametric inference. We demonstrated that the Morse-Smale
complex can be applied to various statistical problems such as clustering, re-
gression and two sample comparisons. We showed that a smooth multivariate
function can be summarized by a few parameters associated with a bipartite
graph, representing the local modes, minima and the complex for the underly-
ing function. Moreover, we proved a fundamental theorem about the stability
of the Morse-Smale complex. Based on the stability theorem, we derived con-
sistency for mode clustering and regression.

The Morse-Smale complex provides a method to synthesize both paramet-
ric and nonparametric inference. Compared to parametric inference, we have a
more ﬂexible model to study the structure of the underlying distribution. Com-
pared to nonparametric inference, the use of the Morse-Smale complex yields a
visualizable representation for the underlying multivariate structures. This re-
veals that we may gain additional insights in data analysis by using geometric
features.

Although the Morse-Smale complex has many potential statistical applica-
tions, we need to be careful when applying it to a data set whose dimension
is large (say d > 10). When the dimension is large, the curse of dimensionality

Chen et al./Inference using the Morse-Smale

26

kicks in and the nonparametric estimators (in both density estimation problems
or regression analysis) are not accurate so the errors of the estimated Morse-
Smale complex can be huge.

Here we list some possible extensions for future research:

• Asymptotic distribution. We have proved the consistency (and the rate of
convergence) for estimating the complex but the limiting distribution is
still unknown. If we can derive the limiting distribution and show that
some resampling method (e.g. the bootstrap Efron (1979)) converges to
the same distribution, we can construct conﬁdence sets for the complex.
• Minimax theory. Despite the fact that we have derived the rate of con-
vergence for a plug-in estimator for the complex, we did not prove its
optimality. We conjecture the minimax rate for estimating the complex
should be related to the rate for estimating the gradient and the smooth-
ness around complex (Audibert et al., 2007; Singh et al., 2009).

Acknowledgement

We thank the referees and the Associate Editor for their very constructive com-
ments and suggestions.

Appendix A: Appendix: Proofs

First, we include a Theorem about the rate of convergence for the kernel density
estimator. This Lemma will be used in deriving the convergence rates.

Theorem 8 (Lemma 10 in Chen et al. (2015); see also Genovese et al. (2014))
Assume (K1–2) and that log n/n ≤ hd ≤ b for some 0 < b < 1. Then we have

(cid:107)(cid:98)pn − p(cid:107)∗

(cid:96),max = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

log n
nhd+2(cid:96)

for (cid:96) = 0, 1, 2.

of critical points.

To prove Theorem 1, we introduce the following useful Lemma for stability

Lemma 9 (Lemma 16 of Chazal et al. (2014)) Let p be a density with com-
pact support K of Rd. Assume p is a Morse function with ﬁnitely many, distinct,
critical values with corresponding critical points C = {c1, · · · , ck}. Also assume
that p is at least twice diﬀerentiable on the interior of K, continuous and dif-
ferentiable with non vanishing gradient on the boundary of K. Then there exists
(cid:15)0 > 0 such that for all 0 < (cid:15) < (cid:15)0 the following is true: for some positive
constant c, there exists η ≥ c(cid:15)0 such that, for any density q with support K
satisfying (cid:107)p − q(cid:107)∗

2,max ≤ η, we have

1. q is a Morse function with exact k critical points c(cid:48)

1, · · · , c(cid:48)

k and

Chen et al./Inference using the Morse-Smale

27

Fig 10. Diagram for lemmas and Theorem 1.

2. after suitable relabeling the indices, maxi=1,··· ,k (cid:107)ci − c(cid:48)

i(cid:107) ≤ (cid:15).

Note that similar result appears in Theorem 1 of Chen et al. (2016). This lemma
shows that two close Morse functions p, q will have similar critical points.

The proof of Theorem 1 requires several working lemmas. We provide a chart

for how we are going to prove Theorem 1.

First, we deﬁne some notations about gradient ﬂows. Recall that πx(t) ∈ K

is the gradient (ascent) ﬂow starting at x:

πx(0) = x,

π(cid:48)
x(t) = g(πx(t)).

For x that is not on the boundary set D, we deﬁne the time:

t(cid:15)(x) = inf{t : πx(s) ∈ B(m,

(cid:15)), for alls ≥ t},

√

where m is the destination of πx. That is, t(cid:15)(x) is the time to arrive the regions
around a local mode.

First, we prove a property for the direction of the gradient ﬁeld around bound-

aries.

Lemma 10 (Gradient ﬁeld and boundaries) Assume the notations in The-
orem 1 and assume f is a Morse function with bounded third derivatives and
satisﬁes assumption (D). Let s(x) = x − Πx, where Πx ∈ B is the projected
point from x onto B (when Πx is not unique, just pick any projected point). For
any q ∈ B, let x be a point near q such that x − q ∈ V(q), the normal space of
B at q. Let δ(x) = (cid:107)x − q(cid:107) and e(x) = x−q

(cid:107)x−q(cid:107) denote the unit vector. Then

Chen et al./Inference using the Morse-Smale

28

(a) Lemma 10

(b) Lemma 11

Fig 11. Illustration for Lemma 10 and 11. (a): We show that the angle between projection
vector s(x) and the gradient g(x) is always right whenever x is closed to the boundaries B. (b):
According to (a), any gradient ﬂow line start from a point x that is close to the boundaries
(distance < δ1), this ﬂow line is always moving away from the boundaries when the current
location is close to the boundaries. The ﬂow line can temporally get closer to the boundaries
when it is away from boundaries (distance > δ1)

1. For every point x such that

d(x, B) ≤ δ1 =

2Hmin
d2 · (cid:107)f (cid:107)3,max

,

we have

g(x)T s(x) ≥ 0.

That is, the gradient is pushing x away from the boundaries.

2. When δ(x) ≤

Hmin
d2·(cid:107)f (cid:107)3,max

,

(cid:96)(x) = e(x)T g(x) ≥

Hminδ(x).

1
2

Proof.
Claim 1. Because the projection of x onto B is Πx, s(x) ∈ V(Πx) and
s(x)T g(Πx) = 0 (recall that for p ∈ B, V(p) is the collection of normal vectors
of B at p).

Recall that d(x, B) = (cid:107)s(x)(cid:107) is the projected distance. By the fact that

Chen et al./Inference using the Morse-Smale

29

s(x)T g(Πx) = 0,

s(x)T g(x) = s(x)T (g(x) − g(Πx))
d2
2

≥ s(x)T H(Πx)s(x) −
= d(x, B)2 s(x)T
d(x, B)
(cid:18)

H(Πx)

(cid:107)f (cid:107)3,maxd(x, B)3

(Taylor’s theorem)

s(x)
d(x, B)

−

d2
2

(cid:107)f (cid:107)3,maxd(x, B)3

(cid:19)

≥ d(x, B)2

Hmin −

(cid:107)f (cid:107)3,maxd(x, B)

.

d2
2

(40)
Note that we use the vector-value Taylor’s theorem in the ﬁrst inequality and
the fact that for two close points x, y, the diﬀerence in the j-the element of
gradient gj(x) − gj(y) has the following expansion

gj(x) − gj(y) = Hj(y)T (x − y) +

(u − y)Tj(u)du

(cid:90) x

sup
u

u=y
1
2
d2
2

≥ Hj(y)T (x − y) −

(cid:107)Tj(u)(cid:107)2(cid:107)x − y(cid:107)2

≥ Hj(y)T (x − y) −

(cid:107)f (cid:107)3,max(cid:107)x − y(cid:107)2,

where Hj(y) = ∇gj(y) and Tj(y) = ∇∇gj(y) is the Hessian matrix of gj(y),
whose elements are the third derivatives of f (y).

Thus, when d(x, B) ≤ 2Hmin
, s(x)T g(x) ≥ 0, which proves the ﬁrst claim.
Claim 2. By deﬁnition, e(x)T g(q) = 0 because g(q) is in tangent space of B

d2·(cid:107)f (cid:107)3,max

at q and e(x) is in the normal space of B at q. Thus,

(cid:96)(x) = e(x)T g(x)

= e(x)T (g(x) − g(q))

≥ e(x)T H(q)(x − q) −

(cid:107)f (cid:107)3,max(cid:107)x − q(cid:107)2

(41)

= e(x)T H(π(x))e(x)δ(x) −

(cid:107)f (cid:107)3,maxδ(x)2

d2
2

d2
2

≥

Hminδ(x)

1
2

whenever δ(x) = (cid:107)x − q(cid:107) ≤
. Note that in the ﬁrst inequality we use
the same lower bound as the one in claim 1. Also note that x − q = e(x)δ(x)
and e(x) is in the normal space of B at π(x) so the third inequality follows from
assumption (D).

Hmin
d2·(cid:107)f (cid:107)3,max

(cid:3)

Lemma 10 can be used to prove the following result.

Chen et al./Inference using the Morse-Smale

30

Lemma 11 (Distance between ﬂows and boundaries) Assume the nota-
tions as the above and assumption (D). Then for all x such that 0 < d(x, B) =
δ ≤ δ1 = 2Hmin

,

d2(cid:107)f (cid:107)3,max

d(πx(t), B) ≥ δ,

for all t ≥ 0.

The main idea is that the projected gradient (gradient projected to the normal
space of nearby boundaries) is always positive. This means that the ﬂow cannot
move “closer” to the boundaries.

Proof. By Lemma 10, for every point x near to the boundaries (d(x, B) <
δ1), the gradient is moving this point away from the boundaries. Thus, for any
ﬂow πx(t), once it touches the region B ⊕ δ1, it will move away from this region.
So when a ﬂow leaves B ⊕ δ1, it can never come back.

Therefore, the only case that a ﬂow can be within the region B ⊕ δ1 is that

it starts at some x ∈ B ⊕ δ1. i.e. d(x, B) < δ1.

Now consider a ﬂow start at x such that 0 < d(x, B) ≤ δ1. By Lemma 10,
the gradient g(x) leads x to move away from the boundaries B. Thus, whenever
πx(t) ∈ B ⊕ δ1, the gradient is pushing πx(t) away from B. As a result, the time
that πx(t) is closest to B is at the beginning of the ﬂow .i.e. t = 0. This implies
that d(πx(t), B) ≥ d(πx(0), B) = d(x, B) = δ.

(cid:3)

With Lemma 11, we are able to bound the low gradient regions since the
ﬂow cannot move inﬁnitely close to critical points except its destination. Let
λmin > 0 be the minimal ‘absolute’ value of eigenvalues of all critical points.

Lemma 12 (Bounds on low gradient regions) Assume the density func-
tion f is a Morse function and has bounded third derivatives. Let C denote
the collection of all critical points and let λmin is the minimal ‘absolute’ eigen-
value for Hessian matrix H(x) evaluated at x ∈ C. Then there exists a constant
δ2 > 0 such that

G(δ) ≡

x : (cid:107)g(x)(cid:107) ≤

⊂ C ⊕ δ

(42)

(cid:26)

(cid:27)

λmin
2

δ

for every δ ≤ δ2.

Proof. Because the support K is compact and x ∈ K (cid:55)→ (cid:107)g(x)(cid:107) is contin-
uous, for any g0 > 0 suﬃciently small, there exists a constant R(g0) > 0 such
that

G1(g0) ≡ {x : (cid:107)g(x)(cid:107) ≤ g0} ⊂ C ⊕ R(g0)

and when g0 → 0, R(g0) → 0. Thus, there is a constant g1 > 0 such that
R(g1) =

.

λmin
2d3(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

31

The set C ⊕ λmin

2(cid:107)f (cid:107)3,max

has a useful feature: for any x ∈ C ⊕ λmin

,

2(cid:107)f (cid:107)3,max

(cid:107)H(x) − H(c)(cid:107)F = (cid:107)(x − c)f (3)(c + t(x − c))dt(cid:107)F

≤ d3(cid:107)x − c(cid:107)(cid:107)f (cid:107)3,max
λmin
2d3(cid:107)f (cid:107)3,max

≤ d3

· (cid:107)f (cid:107)3,max

=

λmin
2

,

where f (3) is a d × d × d array of the third derivative of f and (cid:107)A(cid:107)F is the
Frobenius norm of the matrix A. By Hoﬀman–Wielandt theorem (see, e.g., page
165 of Bhatia 1997), the eigenvalues between H(x) and H(c) is bounded by
(cid:107)H(x) − H(c)(cid:107)F . Therefore, the smallest eigenvalue of H(x) must be greater
than or equal to the smallest eigenvalue of H(c) minus λmin
2 . Because λmin is
the smallest absolute eigenvalues of H(c) for all c ∈ C, the smallest eigenvalue
of H(x) is greater than or equal to λmin
.

2 , for all x ∈ C ⊕ R(g1) = C ⊕
λmin
2d3(cid:107)f (cid:107)3,max

λmin
2d3(cid:107)f (cid:107)3,max
, for any

Using the above feature and the fact that G1(g1) ⊂ C ⊕

x ∈ G1(g1), we have the following inequalities:

g1 ≥ (cid:107)g(x)(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

=

0

≥ (cid:107)x − c(cid:107)

λmin.

1
2

(x − c)H(c + t(x − c))dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Thus, (cid:107)x − c(cid:107) ≤ 2g1
λmin

, which implies

Moreover, because G1(g2) ⊂ G1(g3) for any g2 ≤ g3, any g2 ≤ g1 satisﬁes

Now pick δ = 2g2
λmin

, we conclude

G1(g1) ⊂ C ⊕

G1(g2) ⊂ C ⊕

2g1
λmin

.

2g2
λmin

.

G1

(cid:19)

(cid:18) λmin
2δ

= G(δ) ⊂ C ⊕ δ

δ =

2g2
λmin

≤

2g1
λmin

= δ2,

for all

(cid:3)

where g1 is the constant such that R(g1) =

λmin
2d3(cid:107)f (cid:107)3,max

.

(43)

Chen et al./Inference using the Morse-Smale

32

Fig 12. Illustration for H((cid:15), δ). The thick black lines are boundaries B; solid dots are local
modes; box is local minimum; empty dots are saddle points. The three purple lines denote
possible gradient ﬂows starting from some points x with d(x, B) = δ. The gray disks denote
all possible regions such that (cid:107)g(cid:107) ≤ λmin
2 δ. Thus, the amount of gradient within the set H((cid:15), δ)
is greater or equal to λmin

2 δ.

Lemma 13 (Bounds on gradient ﬂow) Using the notations above and as-
sumption (D), let δ1 be deﬁned in Lemma 11 and δ2 be deﬁned in Lemma 12,
equation (43). Then for all x such that

d(x, B) = δ < δ0 = min

δ1, δ2,

(cid:26)

Hmin
d2 · (cid:107)f (cid:107)3,max

(cid:27)

,

and picking (cid:15) such that δ2 > (cid:15)2 > δ, we have

η(cid:15)(x) ≡ inf

(cid:107)g(πx(t))(cid:107) ≥ δ

0≤t≤t(cid:15)(x)

λmin
2

.

γ(cid:15)(δ) ≡ inf
x∈Bδ

η(cid:15)(x) ≥ δ

λmin
2

,

Moreover,

where Bδ = {x : d(x, B) = δ}.

Proof.
We consider the ﬂow πx starting at x (not on the boundaries) such that

d(x, B) = δ < min {δ1, δ2} .

For 0 ≤ t ≤ t(cid:15)(x), the entire ﬂow is within the set

H((cid:15), δ) = {x : d(x, B) ≥ δ, d(x, M ) ≥

(cid:15)}.

(44)

√

Chen et al./Inference using the Morse-Smale

That is,

{πx(t) : 0 ≤ t ≤ t(cid:15)(x)} ⊂ H((cid:15), δ).

This is because by Lemma 11, the ﬂow line cannot get closer to the boundaries
B within distance δ, and the ﬂow stops when its distance to its destination is
at (cid:15). Thus, if we can prove that every point within H((cid:15), δ) has gradient lowered
bounded by δ λmin
2 , we have completed the proof. That is, we want to show that

33

(45)

(46)

To show the lower bound, we focus on those points whose gradient is small.

Let

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

(cid:26)

S(δ) =

x : (cid:107)g(x)(cid:107) ≤ δ

(cid:27)

.

λmin
2

S(δ) ⊂ C ⊕ δ.

By Lemma 12, the S(δ) are regions around critical points such that

Since we have chosen (cid:15) such that (cid:15) ≥ δ2 and by the fact that critical points
are either in M , the collection of all local modes, or in B the boundaries so that,
the minimal distance between H((cid:15), δ) and critical points C is greater that δ (see
equation (44) for the deﬁnition of H((cid:15), δ)). Thus,

which implies equation (46):

(C ⊕ δ) ∩ H((cid:15), δ) = ∅,

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

Now by the fact that all πx(t) with d(x, B) < δ are within the set H((cid:15), δ)
(equation (45)), we conclude the result.

(cid:3)

Lemma 13 links the constant γ(cid:15)(δ) and the minimal gradient, which can be
used to bound the time t(cid:15)(x) uniformly and further leads to the following result.
Lemma 14 Let K(δ) = {x ∈ K : d(x, B) ≥ δ} = K\(B ⊕δ) and δ0 be deﬁned as
Lemma 13 and M is the collection of all local modes. Assume that f has bounded
third derivative and is a Morse function and that assumption (D) holds. Let (cid:101)f
be another smooth function. There exists constants c∗, c0, c1, (cid:15)0 that all depend
only on f such that when ((cid:15), δ) satisfy the following condition

δ < (cid:15) < (cid:15)0,

δ < min{δ0, Haus(K(δ), B(M,

(cid:15)))}

√

and if

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤ c0

(cid:107)f − (cid:101)f (cid:107)1,max ≤ c1 exp

−

(cid:32)

√
4

d(cid:107)f (cid:107)2,max(cid:107)f (cid:107)∞

(cid:33)

,

δ2λ2

min

(47)

(48)

Chen et al./Inference using the Morse-Smale

34

Fig 13. Result from Lemma 13: lower bound on minimal gradient. This plot shows possible
values for minimal gradient η(cid:15)(x) (pink regions) when d(x, B) is known. Note that we have
chosen (cid:15)2 < δ2.

then for all x ∈ K(δ)

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞.

(49)

(cid:113)

Note that condition (47) holds when ((cid:15), δ) are suﬃciently small.

Proof. The proof of this lemma is closely related to the proof of Theorem
2 of Arias-Castro et al. (2016). The results in Arias-Castro et al. (2016) is a
pointwise convergence of gradient ﬂows; now we will generalize their ﬁndings to
the uniform convergence.

Note that K(δ) = H((cid:15), δ) ∪ B(x,

(cid:15)). For x ∈ B(x,

(cid:15)), the result is trivial

√

√

when (cid:15) is suﬃciently small. Thus, we assume x ∈ H((cid:15), δ).

From equation (40–44) in Arias-Castro et al. (2016) (proof to their Theorem

2),

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107)
(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

2
λmin

2λmin(cid:15) +

√

(cid:107)f − (cid:101)f (cid:107)1,maxe

d(cid:107)f (cid:107)2,maxt(cid:15)(x) + 2(cid:107)f − (cid:101)f (cid:107)∞

√

(cid:107)f (cid:107)1,max
d(cid:107)f (cid:107)2,max

(cid:33)

(50)

under condition (48) and (cid:15) < (cid:15)0 for some constant (cid:15)0.

Thus, the key is to bound t(cid:15)(x). Recall that x ∈ H((cid:15), δ). Now consider the

Chen et al./Inference using the Morse-Smale

35

gradient ﬂow πx and deﬁne z = πx(t(cid:15)(x)).

f (z) − f (x) =

∂f (πx(s))
∂s

ds =

0

(cid:90) t(cid:15)(x)

g(πx(s))T π(cid:48)

x(s)ds

(cid:107)g(πx(s))(cid:107)2ds ≥ γ(cid:15)(δ)2t(cid:15)(x).

(cid:90) t(cid:15)(x)

(cid:90) t(cid:15)(x)

=

0

0

Since f (z) − f (x) ≤ 2(cid:107)f (cid:107)∞, we have

(cid:107)f (cid:107)∞ ≥

γ(cid:15)(δ)2t(cid:15)(x)

1
2

t(cid:15)(x) ≤

2(cid:107)f (cid:107)∞
γ(cid:15)(δ)2

≤

8(cid:107)f (cid:107)∞
δ2λ2

min

and by Lemma 13,

for all x ∈ H((cid:15), δ).

Now plug-in (52) into (50), we have

(51)

(52)

(cid:114)

(cid:107) lim
t→∞

πx(t)− lim

t→∞ (cid:101)πx(t)(cid:107) ≤

a0(cid:15) + a1(cid:107)f − (cid:101)f (cid:107)1,maxe

(53)
for some constants a0, a1, a2. Now using condition (48) to replace the second
term of right hand side, we conclude

√

d(cid:107)f (cid:107)2,max

8(cid:107)f (cid:107)∞
δ2λ2

min + a2(cid:107)f − (cid:101)f (cid:107)∞

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max

By Lemma 7 in Arias-Castro et al. (2016), there exists some constant c3 such

for some constant a3.

that when a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max < 1/c3,

(cid:113)

(cid:113)

√

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤

2c3(cid:107)f − (cid:101)f (cid:107).

Thus, when both (cid:15) and (cid:107)f − (cid:101)f ∗
constant c∗ such that

3,max(cid:107) are suﬃciently small, there exists some

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗(cid:107)f − (cid:101)f (cid:107)

for all x ∈ H((cid:15), δ).

(cid:3)

Now we turn to the proof of Theorem 1.

we show that when (cid:107)f − (cid:101)f (cid:107)∗

Proof of Theorem 1. The proof contains two parts. In the ﬁrst part,
3,max is suﬃciently small, we have Haus(B, (cid:101)B) <
, where B and (cid:101)B are the boundary of descending d-manifolds for f

Hmin
d2(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

36

Hmin
d2(cid:107)f (cid:107)3,max

and (cid:101)f . The second part of the proof is to derive the convergence rate. Because
Haus(B, (cid:101)B) <
, we can apply the second assertion of Lemma 10 to
derive the rate of convergence. Note that C and (cid:101)C are the critical points for f
and (cid:101)f and M ≡ C0, (cid:102)M ≡ (cid:101)C0 are the local modes for f and (cid:101)f .
Hmin
d2·(cid:107)f (cid:107)3,max

, the upper bound for Hausdorﬀ dis-
tance. Let σ = min{(cid:107)x − y(cid:107) : x, y ∈ M, x (cid:54)= y}. That is, σ is the smallest dis-
tance between any pair of distinct local modes. By Lemma 9, when (cid:107)f − (cid:101)f (cid:107)∗
is small, f and (cid:101)f have the same number of critical points and

Part 1: Haus(B, (cid:101)B) <

3,max

Haus(C, (cid:101)C) ≤ A(cid:107)f − (cid:101)f (cid:107)∗

2,max ≤ A(cid:107)f − (cid:101)f (cid:107)∗

3,max,

where A is a constant that depends only on f (actually, we only need (cid:107)f − (cid:101)f (cid:107)∗
to be small here).

2,max

Thus, whenever (cid:107)f − (cid:101)f (cid:107)∗

3,max satisﬁes

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

σ
3A

,

every M has an unique corresponding point in (cid:102)M and vice versa. In addition,
for a pair of local modes (mj, (cid:101)mj) : mj ∈ M, (cid:101)mj ∈ (cid:102)M , their distance is bounded
by (cid:107)mj − (cid:101)mj(cid:107) ≤ σ
3 .

Now we pick ((cid:15), δ) such that they satisfy equation (47). Then when (cid:107)f −
3,max is suﬃciently small, by Lemma 14, for every x ∈ H((cid:15), δ) we have

(cid:101)f (cid:107)∗

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞ ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∗

3,max.

(cid:113)

(cid:113)

Thus, whenever

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

1
c2
∗

(cid:16) σ
3

(cid:17)2

,

πx(t) and (cid:101)πx(t) leads to the same pair of modes. Namely, the boundaries (cid:101)B
will not intersect the region H((cid:15), δ). And it is obvious that (cid:101)B cannot intersect
B(M,

(cid:15)). To conclude,

√

(54)

(55)

(56)

(cid:101)B ∩ H((cid:15), δ) = ∅
√

(cid:101)B ∩ B(M,

(cid:15)) = ∅
⇒ (cid:101)B ∩ K(δ) = ∅,
(cid:15)).

√

because by deﬁnition, K(δ) = H((cid:15), δ) ∩ B(M,

Thus, (cid:101)B ⊂ K(δ)C = B ⊕ δ, which implies Haus(B, (cid:101)B) ≤ δ < Hmin

(note

d2(cid:107)f (cid:107)3,max

that δ < δ0 ≤ Hmin

d2(cid:107)f (cid:107)3,max

appears in equation (47) and Lemma 13).

Part 2: Rate of convergence. To derive the convergence rate, we use proof
by contradiction. Let q ∈ B, (cid:101)q ∈ (cid:101)B a pair of points such that their distance
attains the Hausdorﬀ distance Haus
(cid:101)B, B

(cid:17)

(cid:16)

. Namely, q and (cid:101)q satisfy
(cid:16)

(cid:17)

(cid:107)q − (cid:101)q(cid:107) = Haus

(cid:101)B, B

Chen et al./Inference using the Morse-Smale

37

and either q is the projected point from (cid:101)q onto B or (cid:101)q is the projected point
from q onto (cid:101)B.

Recall that V(x) is the normal space to B at x ∈ B and we deﬁne (cid:101)V(x)
similarly for x ∈ (cid:101)B. An important property of the pair q, (cid:101)q is that q − (cid:101)q ∈
V(q), (cid:101)V((cid:101)q). If this is not true, we can slightly perturb q (or (cid:101)q) on B (or (cid:101)B) to
get a projection distance larger than the Hausdorﬀ distance, which leads to a
contradiction.

Now we choose x to be a point between q, (cid:101)q such that x = 1

3 (cid:101)q. We
(cid:107)(cid:101)q−x(cid:107) . Then e(x) ∈ V(q) and (cid:101)e(x) ∈ (cid:101)V((cid:101)q) and

3 q + 2

(cid:107)q−x(cid:107) and (cid:101)e(x) = (cid:101)q−x

deﬁne e(x) = q−x
e(x) = −(cid:101)e(x).

By Lemma 10 (second assertion),

(57)

(58)

(cid:96)(x) = e(x)T g(x) ≥

Hmin(cid:107)q − x(cid:107) > 0

(cid:101)(cid:96)(x) = (cid:101)e(x)T

(cid:101)g(x) ≥

(cid:101)Hmin(cid:107)(cid:101)q − x(cid:107) > 0.

1
2
1
2

Thus, for every x between q, (cid:101)q,
e(x)T g(x) > 0,

, e(x)T

(cid:101)g(x) = −(cid:101)e(x)T

(cid:101)g(x) < 0.

Note that we can apply Lemma 10 to (cid:101)f and its gradient because when (cid:107)f − (cid:101)f (cid:107)∗
2
is suﬃciently small, the assumption (D) holds for (cid:101)f as well.

To get the upper bound of (cid:107)q−(cid:101)q(cid:107) = Haus( (cid:101)B, B), note that (cid:107)q−x(cid:107) = 2

3 (cid:107)q−(cid:101)q(cid:107),

so

e(x)T

(cid:101)g(x) = e(x)T ((cid:101)g(x) − g(x)) + e(x)T g(x)
≥ e(x)T g(x) − (cid:107) (cid:101)f − f (cid:107)1,max

Hmin(cid:107)q − x(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max

(By Lemma 10)

(59)

≥

=

1
2
1
3

Hmin(cid:107)q − (cid:101)q(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max.

Thus, as long as

we have e(x)T
that

(cid:3)

Haus( (cid:101)B, B) = (cid:107)q − (cid:101)q(cid:107) > 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

,

(cid:101)g(x) > 0, a contradiction to equation (58). Hence, we conclude

Haus( (cid:101)B, B) ≤ 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

(cid:16)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

.

Proof of Theorem 3.
To prove the asymptotic rate of the rand index, we assume that for every local
mode of p, there exists one and only one local mode of (cid:98)pn that is close to the

Chen et al./Inference using the Morse-Smale

38

speciﬁc mode of p. By Lemma 9, this is true when (cid:107)(cid:98)pn − p(cid:107)∗
3,max is suﬃciently
small. Thus, after relabeling, the local mode (cid:98)m(cid:96) of (cid:98)pn is an estimator to the
local mode m(cid:96) of p. Let (cid:99)W(cid:96) be the basin of attraction to (cid:98)m(cid:96) using ∇(cid:98)pn and W(cid:96)
be the basin of attraction to m(cid:96) using ∇p. Let A(cid:52)B = {x : x ∈ A, x /∈ B} ∪ {x :
x ∈ B, x /∈ A} be the symmetric diﬀerence between sets A and B. The regions

En =

(cid:99)W(cid:96)(cid:52)W(cid:96)

(cid:17)

⊂ K

(cid:91)

(cid:16)

(cid:96)

(60)

are where the two mode clustering disagree with each other. Note that En are
regions between the two boundaries (cid:98)Bn and B

Given a pair of points Xi and Xj,

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj) =⇒ Xi or Xj ∈ En.

(61)

By the deﬁnition of rand index (30),

1 − rand ((cid:98)pn, p) =

(cid:19)−1

(cid:88)

(cid:16)

(cid:18)n
2

i,j

1

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj)

(62)

(cid:17)

Thus, if we can bound the ratio of data points within En, we can bound the
rate of rand index.

Since K is compact and p has bounded second derivatives, the volume of En

is bounded by

Vol(En) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Note Vol(A) denotes the volume (Lebesgue measure) of a set A. We now con-
struct a region surrounding B such that

and

En ⊂ B ⊕ Haus( (cid:98)Bn, B) = Vn

Vol(Vn) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Now we consider a collection of subsets of K:

V = {B ⊕ r : R > r > 0},

(cid:80)n

where R < ∞ is the diameter for K. For any set A ⊂ K, let P (Xi ∈ A) and
(cid:98)Pn(A) = 1
i=1 1(Xi ∈ A) denote the probability of an observation within A
n
and the empirical estimate for that probability, respectively. It is easy to see
that Vn ∈ V for all n and the class V has a ﬁnite VC dimension (actually, the
VC dimension is 1). By the empirical process theory (or so-called VC theory,
see e.g. Vapnik and Chervonenkis (1971)),

(cid:12)
(cid:12)
(cid:12)P (Xi ∈ A) − (cid:98)Pn(A)

(cid:12)
(cid:12)
(cid:12) = OP

sup
A∈V

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(63)

(64)

(65)

(66)

(67)

Chen et al./Inference using the Morse-Smale

39

Thus,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)P (Xi ∈ Vn) − (cid:98)Pn(Vn)
(cid:12) = OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(68)

Now by equations (61) and (62),

1 − rand ((cid:98)pn, p) ≤ 8 (cid:98)Pn(En) ≤ 8 (cid:98)Pn(Vn) ≤ 8P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n

. (69)

Therefore,

1 − rand ((cid:98)pn, p) ≤ P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n
(cid:32)(cid:114)

(cid:33)

log(n)
n

(cid:32)(cid:114)

(cid:33)

log(n)
n

p(x) × Vol(Vn) + OP

≤ sup
x∈K

(cid:16)

≤ O

Haus( (cid:98)Bn, B)

+ OP

(cid:17)

= O (cid:0)h2(cid:1) + OP

(cid:32)(cid:114)

(cid:33)

,

log(n)
nhd+2

which completes the proof. Note that we apply Theorem 2 in the last equality.

(cid:3)

Proof of Theorem 4. Let (X1, Y1), · · · , (Xn, Yn) be the observed data.
Let (cid:98)E(cid:96) denote the d-cell for the nonparametric pilot regression estimator (cid:98)mn.
With I(cid:96) = {i : Xi ∈ (cid:98)E(cid:96)}, we deﬁne X(cid:96) as the matrix with rows Xi, i ∈ I(cid:96) and
similarly we deﬁne Y(cid:96).

We deﬁne X0,(cid:96) to be the matrix similar to X(cid:96) except that the row elements
are those Xi within E(cid:96), the d-cell deﬁned on true regression function m. We
also deﬁne Y0,(cid:96) to be the corresponding Yi.

By the theory of linear regression, the estimated parameters (cid:98)µ(cid:96), (cid:98)β(cid:96) have a

closed form solution:

Similarly, we deﬁne

((cid:98)µ(cid:96), (cid:98)β(cid:96))T = (XT

(cid:96)

X(cid:96))−1XT
(cid:96)

Y(cid:96).

((cid:98)µ0,(cid:96), (cid:98)β0,(cid:96))T = (XT

0,(cid:96)

X0,(cid:96))−1XT
0,(cid:96)

Y0,(cid:96)

as the estimated coeﬃcients using X0,(cid:96) and Y0,(cid:96).

As (cid:107) (cid:101)m − m(cid:107)∗

3,max is small, by Theorem 3, the number of rows at which
X(cid:96) and X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max). This is because an

(70)

(71)

(72)

Chen et al./Inference using the Morse-Smale

40

observation (a row vector) that appears only in one of X(cid:96) and X0,(cid:96) is those
fallen within either (cid:98)E(cid:96) or E(cid:96) but not both. Despite the fact that Theorem 3
is for basins of attraction (d-descending manifolds) of local modes, it can be
easily generalized to 0-ascending manifolds of local minima under assumption
(A). Thus, the similar bound holds for d-cells as well. Thus, we conclude that
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
since (X(cid:96), Y(cid:96)) and (X0,(cid:96), Y0,(cid:96)) only diﬀer by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max) elements.
Thus,

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

1
n
1
n

Y(cid:96) −

X(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96)

X0,(cid:96)

(73)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)µ0,(cid:96) − (cid:98)µ(cid:96), (cid:98)β0,(cid:96) − (cid:98)β(cid:96))
(cid:13)∞

=

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
(cid:13)
n
(cid:13)

XT
0,(cid:96)

X0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96) −

XT
(cid:96)

X(cid:96)

(cid:19)−1 1
n

(cid:18) 1
n

(cid:19)−1 1
n

XT
(cid:96)

Y(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= O((cid:107) (cid:98)mn − m(cid:107)1,max),

which implies.

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − (cid:98)µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − (cid:98)β(cid:96)(cid:107)

= O((cid:107) (cid:98)mn − m(cid:107)1,max).

Now by the theory of linear regression,

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − β(cid:96)(cid:107)

= OP

(cid:18) 1
√
n

(cid:19)

.

(74)

(75)

(76)

Thus, combining (75) and (76) and use the fact that all the above bounds are
uniform over each cell, we have proved that the parameters converge at rate
O((cid:107) (cid:98)mn − m(cid:107)1,max) + OP

(cid:16) 1√

(cid:17)

n

.

For points within the regions where E(cid:96) and (cid:98)E(cid:96) agree with each other, the rate
of convergence for parameter estimation translates into the rate of (cid:98)mn,MSR −
mMSR. The regions that E(cid:96) and (cid:98)E(cid:96) disagree to each other, denoted as Nn, have
Lebesgue O((cid:107) (cid:98)mn − m(cid:107)1,max) by Theorem 1. Thus, we have completed the proof.

(cid:3)

Proof of Theorem 5. The proof of Theorem 5 is nearly identical to the
proof of Theorem 4. The only diﬀerence is that the number of rows that X(cid:96) and
X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)β
1,max) due to the low noise condition
(36). Thus, equation (73) becomes
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O((cid:107) (cid:98)mn − m(cid:107)β

= O((cid:107) (cid:98)mn − m(cid:107)β

1
n
1
n

1
n
1
n

1,max)

1,max)

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(77)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Chen et al./Inference using the Morse-Smale

41

so the parameter estimation error (76) is O((cid:107) (cid:98)mn − m(cid:107)β

1,max) + OP

(cid:16) 1√

(cid:17)

.

n

Under assumption (K1–2) and using Theorem 8 (the same result works for

kernel regression),

O((cid:107) (cid:98)mn − m(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

Thus, with the choice that h = O
(cid:17)2/(d+6)(cid:19)

(cid:18)(cid:16) log n

OP

n

(cid:3)

, which proves equation (37).

(cid:18)(cid:16) log n

(cid:17)1/(d+6)(cid:19)

n

, we have O((cid:107) (cid:98)mn−m(cid:107)1,max) =

Proof of Theorem 6.
We ﬁrst derive the explicit form of the parameters (η†

Note that the parameters are obtained by (14):

(cid:96) , γ†

(cid:96) ) within cell E(cid:96).

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:90)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Now we deﬁne a random variable U(cid:96) ∈ Rd that is uniformly distributed over E(cid:96).
Then equation (14) is equivalent to

(η†

(cid:96) , γ†

(cid:96) ) = argmin

E

η,γ

(cid:16)(cid:0)f (U(cid:96)) − η − γT U(cid:96)

(cid:1)2(cid:17)

.

The analytical solution to the above problem is

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:19)

(cid:18)

=

1

E(U(cid:96))T
E(U(cid:96)) E(U(cid:96)U T
(cid:96) )

(cid:19)−1 (cid:18) E(f (U(cid:96)))
E(U(cid:96)f (U(cid:96)))

(cid:19)

Now we consider another smooth function (cid:101)f that is close to f such that
(cid:107) (cid:101)f − f (cid:107)∗
3,max is small so we can apply Theorem 1 to obtain consistency for both
descending d-manifolds and ascending 0-manifolds. Note that by Lemma 9, all
the critical points are close to each other and after relabeling, each d-cell E(cid:96) of
f is estimated by another d-cell (cid:101)E(cid:96) of (cid:101)f . Theorem 1 further implies that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Leb( (cid:101)E(cid:96)) − Leb(E(cid:96))
(cid:12) = O
(cid:17)
(cid:16)

(cid:16)

(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

(cid:17)

Leb

(cid:101)E(cid:96)(cid:52)E(cid:96)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

,

(78)

(79)

(80)

where Leb(A) is the Lebesgue measure for set A and A(cid:52)B = (A\B) ∪ (B\A) is

Chen et al./Inference using the Morse-Smale

42

the symmetric diﬀerence. By simple algebra, equation (80) implies that

(cid:107)E( (cid:101)U(cid:96)) − E(U(cid:96))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)U T

(cid:96) ) − E(U(cid:96)U T

|E( (cid:101)f ( (cid:101)U(cid:96))) − E(f (U(cid:96)))| = O

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:96) )(cid:107)∞ = O
(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max
(cid:17)

(cid:107) (cid:101)f − f (cid:107)∗
(cid:16)

1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)f ( (cid:101)U(cid:96))) − E(U(cid:96)f (U(cid:96)))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

(81)

By (81) and the analytic solution to ((cid:101)η†
(cid:101)η†
(cid:101)γ†

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:19)

(cid:18)

(cid:96)

(cid:96)

(cid:96) , (cid:101)γ†
(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

(cid:17)

.

1,max

(cid:96) ) from (79), we have proved

(82)

Since the bound does not depend on the cell indices (cid:96), (82) holds uniformly for
all (cid:96) = 1, · · · , K.

(cid:3)

References

E. Arias-Castro, D. Mason, and B. Pelletier. On the estimation of the gradient
lines of a density and the consistency of the mean-shift algorithm. Journal of
Machine Learning Research, 17(43):1–28, 2016.

J.-Y. Audibert, A. B. Tsybakov, et al. Fast learning rates for plug-in classiﬁers.

The Annals of statistics, 35(2):608–633, 2007.

M. Azizyan, Y.-C. Chen, A. Singh, and L. Wasserman. Risk bounds for mode

clustering. arXiv preprint arXiv:1505.00482, 2015.

A. Azzalini and N. Torelli. Clustering via nonparametric density estimation.

Statistics and Computing, 17(1):71–80, 2007.

P. Bacchetti. Additive isotonic models. Journal of the American Statistical

A. Banyaga and D. Hurtubise. Lectures on Morse homology, volume 29. Springer

Association, 84(405):289–294, 1989.

Science & Business Media, 2004.

L. Baringhaus and C. Franz. On a new multivariate two-sample test. Journal

of multivariate analysis, 88(1):190–206, 2004.

R. E. Barlow, D. J. Bartholomew, J. Bremner, and H. D. Brunk. Statistical
inference under order restrictions: the theory and application of isotonic re-
gression. Wiley New York, 1972.

R. Bhatia. Matrix Analysis. Springer, 1997.
G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business

Media, 1993.

R. R. Brinkman, M. Gasparetto, S.-J. J. Lee, A. J. Ribickas, J. Perkins,
W. Janssen, R. Smiley, and C. Smith. High-content ﬂow cytometry and
temporal data analysis for deﬁning a cellular signature of graft-versus-host
disease. Biology of Blood and Marrow Transplantation, 13(6):691–700, 2007.

Chen et al./Inference using the Morse-Smale

43

J. Chac´on and T. Duong. Data-driven density derivative estimation, with ap-
plications to nonparametric clustering and bump hunting. Electronic Journal
of Statistics, 7:499–532, 2013.

J. Chac´on, T. Duong, and M. Wand. Asymptotics for general multivariate kernel

density derivative estimators. Statistica Sinica, 2011.

J. E. Chac´on et al. A population background for nonparametric density-based

clustering. Statistical Science, 30(4):518–532, 2015.

F. Chazal, B. T. Fasy, F. Lecci, B. Michel, A. Rinaldo, and L. Wasserman. Ro-
bust topological inference: Distance to a measure and kernel distance. arXiv
preprint arXiv:1412.7197, 2014.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. Asymptotic theory for density

ridges. The Annals of Statistics, 43(5):1896–1928, 2015.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. A comprehensive approach
to mode clustering. Electronic Journal of Statistics, 10(1):210–241, 2016.
Y. Cheng. Mean shift, mode seeking, and clustering. Pattern Analysis and

Machine Intelligence, IEEE Transactions on, 17(8):790–799, 1995.

D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence dia-

grams. Discrete & Computational Geometry, 37(1):103–120, 2007.

D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space
analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
24(5):603–619, 2002.

T. Duong. Local signiﬁcant diﬀerences from nonparametric two-sample tests.

Journal of Nonparametric Statistics, 25(3):635–645, 2013.

T. Duong et al. ks: Kernel density estimation and kernel discriminant analysis
for multivariate data in r. Journal of Statistical Software, 21(7):1–16, 2007.
B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statis-

tics, 7(1):1–26, 1979.

U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type

function estimators. The Annals of Statistics, 2005.

K. Fukunaga and L. Hostetler. The estimation of the gradient of a density
function, with applications in pattern recognition. Information Theory, IEEE
Transactions on, 21(1):32–40, 1975.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, and L. Wasserman. The
geometry of nonparametric ﬁlament estimation. Journal of the American
Statistical Association, 107(498):788–799, 2012.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, L. Wasserman, et al. Non-
parametric ridge estimation. The Annals of Statistics, 42(4):1511–1545, 2014.
S. Gerber and K. Potter. Data analysis with the morse-smale complex: The msr

package for r. Journal of Statistical Software, 2011.

S. Gerber, P.-T. Bremer, V. Pascucci, and R. Whitaker. Visual exploration
of high dimensional scalar functions. Visualization and Computer Graphics,
IEEE Transactions on, 16(6):1271–1280, 2010.

S. Gerber, O. R¨ubel, P.-T. Bremer, V. Pascucci, and R. T. Whitaker. Morse–
smale regression. Journal of Computational and Graphical Statistics, 22(1):
193–214, 2013.

E. Gine and A. Guillou. Rates of strong uniform consistency for multivari-

Chen et al./Inference using the Morse-Smale

44

ate kernel density estimators. In Annales de l’Institut Henri Poincare (B)
Probability and Statistics, 2002.

S. Helgason. Diﬀerential geometry, Lie groups, and symmetric spaces, vol-

ume 80. Academic press, 1979.

L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation, 2(1):

193–218, 1985.

J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a

nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.

J. Li, S. Ray, and B. G. Lindsay. A nonparametric statistical approach to
clustering via mode identiﬁcation. Journal of Machine Learning Research,
2007.

J. W. Milnor. Morse theory. Number 51. Princeton university press, 1963.
M. Morse. Relations between the critical points of a real function of n indepen-
dent variables. Transactions of the American Mathematical Society, 27(3):
345–396, 1925.

M. Morse. The foundations of a theory of the calculus of variations in the
large in m-space (second paper). Transactions of the American Mathematical
Society, 32(4):599–631, 1930.

E. A. Nadaraya. On estimating regression. Theory of Probability & Its Appli-

cations, 9(1):141–142, 1964.

S. Paris and F. Durand. A topological approach to hierarchical segmentation us-
ing mean shift. In Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007.

W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal

of the American Statistical association, 66(336):846–850, 1971.

A. Rinaldo, L. Wasserman, et al. Generalized density clustering. The Annals of

Statistics, 38(5):2678–2722, 2010.

A. Rinaldo, A. Singh, R. Nugent, and L. Wasserman. Stability of density-based
clustering. The Journal of Machine Learning Research, 13(1):905–948, 2012.
M. Rizzo and G. Szekely. energy: E-statistics (energy statistics). R package

version, 1:1, 2008.

M. L. Rizzo, G. J. Sz´ekely, et al. Disco analysis: A nonparametric extension of
analysis of variance. The Annals of Applied Statistics, 4(2):1034–1055, 2010.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chap-

man and Hall, 1986.

A. Singh, C. Scott, R. Nowak, et al. Adaptive hausdorﬀ estimation of density

level sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

G. J. Sz´ekely and M. L. Rizzo. Testing for equal distributions in high dimension.

InterStat, 5, 2004.

G. J. Szekely and M. L. Rizzo. Hierarchical clustering via joint between-within
distances: Extending ward’s minimum variance method. Journal of classiﬁ-
cation, 22(2):151–183, 2005.

G. J. Sz´ekely and M. L. Rizzo. A new test for multivariate normality. Journal

of Multivariate Analysis, 93(1):58–80, 2005.

G. J. Sz´ekely and M. L. Rizzo. Energy statistics: A class of statistics based
on distances. Journal of statistical planning and inference, 143(8):1249–1272,

Chen et al./Inference using the Morse-Smale

45

2013.

V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of rela-
tive frequencies of events to their probabilities. Theory of Probability & Its
Applications, 16(2):264–280, 1971.

A. Vedaldi and S. Soatto. Quick shift and kernel methods for mode seeking. In
European Conference on Computer Vision, pages 705–718. Springer, 2008.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clus-
terings comparison: is a correction for chance necessary? In Proceedings of
the 26th Annual International Conference on Machine Learning, pages 1073–
1080. ACM, 2009.

L. Wasserman. All of nonparametric statistics. Springer, 2006.

7
1
0
2
 
r
p
A
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
2
v
6
2
8
8
0
.
6
0
5
1
:
v
i
X
r
a

Electronic Journal of Statistics
ISSN: 1935-7524
arXiv: http://arxiv.org/abs/1506.08826

Statistical Inference Using the
Morse-Smale Complex

Yen-Chi Chen, and Christopher R. Genovese, and Larry Wasserman

University of Washington,
Department of Statistics
Box 354322,
Seattle, WA 98195
e-mail: yenchic@uw.edu

Carnegie Mellon University,
Department of Statistics
5000 Forbes Avenue,
Pittsburgh, PA 15213
e-mail: genovese@stat.cmu.edu; larry@stat.cmu.edu

Abstract: The Morse-Smale complex of a function f decomposes the sam-
ple space into cells where f is increasing or decreasing. When applied to
nonparametric density estimation and regression, it provides a way to rep-
resent, visualize, and compare multivariate functions. In this paper, we
present some statistical results on estimating Morse-Smale complexes. This
allows us to derive new results for two existing methods: mode clustering
and Morse-Smale regression. We also develop two new methods based on
the Morse-Smale complex: a visualization technique for multivariate func-
tions and a two-sample, multivariate hypothesis test.

MSC 2010 subject classiﬁcations: Primary 62G20; secondary 62G86,
62H30.
Keywords and phrases: nonparametric estimation, mode clustering, non-
parametric regression, two sample test, visualization.

1. Introduction

Let f be a smooth, real-valued function deﬁned on a compact set K ∈ Rd. In
this paper, f will be a regression function or a density function. The Morse-
Smale complex of f is a partition of K based on the gradient ﬂow induced by f .
Roughly speaking, the complex consists of sets, called crystals or cells, comprised
of regions where f is increasing or decreasing. Figure 1 shows the Morse-Smale
complex for a two-dimensional function. The cells are the intersections of the
basins of attractions (under the gradient ﬂow) of the function’s maxima and
minima. The function f is piecewise monotonic over cells with respect to some
directions. In a sense, the Morse-Smale complex provides a generalization of
isotonic regression.

Because the Morse-Smale complex represents a multivariate function in terms
of regions on which the function has simple behavior, the Morse-Smale complex
has useful applications in statistics, including in clustering, regression, testing,
and visualization. For instance, when f is a density function, the basins of at-
traction of f ’s modes are the (population) clusters for density-mode clustering

1

Chen et al./Inference using the Morse-Smale

2

(a) Descending manifold

(b) Ascending manifold

(c) d-cell

(d) Morse-Smale complex

Fig 1. An example of a Morse-Smale complex. The green dots are local minima; the blue
dots are local modes; the violet dots are saddle points. Panels (a) and (b) give examples of
descending d-manifolds (blue region) and an ascending 0-manifold (green region). Panel (c)
shows the corresponding d-cell (yellow region). Panel (d) is shows all d-cells.

(also known as mean shift clustering (Fukunaga and Hostetler, 1975; Chac´on
et al., 2015)), each of which is a union of cells from the Morse-Smale complex.
Similarly, when f is a regression function, the cells of the Morse-Smale complex
give regions on which f has simple behavior. Fitting f over the Morse-Smale
cells provides a generalization of nonparametric, isotone regression; Gerber et al.
(2013) proposes such a method. The Morse-Smale representation of a multivari-
ate function f is a useful tool for visualizing f ’s structure, as shown by Gerber
et al. (2010). In addition, suppose we want to compare two multi-dimensional
datasets X = (X1, . . . , Xn) and Y = (Y1, . . . , Ym). We start by forming the
Morse-Smale complex of (cid:98)p − (cid:98)q where (cid:98)p is density estimate from X and (cid:98)q is den-
sity estimate from Y . Figure 2 shows a visualization built from this complex.
The circles represent cells of the Morse-Smale complex. Attached to each cell is
a pie-chart showing what fraction of the cell has (cid:98)p signiﬁcantly larger than (cid:98)q.
This visualization is a multi-dimensional extension of the method proposed for
two or three dimensions in Duong (2013).

For all these applications, the Morse-Smale complex needs to be estimated.
To the best of our knowledge, no theory has been developed for this estimation
problem, prior to this paper. We have three goals in this paper: to show that
many existing problems can be cast in terms of the Morse-Smale complex, to

Chen et al./Inference using the Morse-Smale

3

Fig 2. Graft-versus-Host Disease (GvHD) dataset (Brinkman et al., 2007). This is a d = 4
dimensional dataset. We estimate the density diﬀerence based on the kernel density estimator
and ﬁnd regions where the two densities are signiﬁcantly diﬀerent. Then we visualize the
density diﬀerence using the Morse-Smale complex. Each green circle denotes a d-cell, which
is a partition for the support K. The size of circle is proportional to the size of cell. If two
cells are neighborhors, we add a line connecting them; the thickness of the line denotes the
amount of boundary they share. The pie charts show the ratio of the regions within each cell
where the two densities are signiﬁcantly diﬀerent from each other. See Section 3.4 for more
details.

develop some new statistical methods based on the Morse-Smale complex, and
to develop the statistical theory for estimating the complex.

Main results. The main results of this paper are:

1. Consistency of the Morse-Smale Complex. We prove the stability of the
Morse-Smale complex (Theorem 1) in the following sense: if B and (cid:101)B are
boundaries of the descending d-manifolds (or ascending 0-manifolds) of p
and (cid:101)p (deﬁned in Section 2), then

Haus(B, (cid:101)B) = O ((cid:107)∇p − ∇(cid:101)p(cid:107)∞) .
2. Risk Bound for Mode clustering (mean-shift clustering; section 3.1): We

bound the risk of mode clustering in Theorem 2.

3. Morse-Smale regression (section 3.2): In Theorems 4 and 5, we bound the
risk of Morse-Smale regression, a multivariate regression method proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013)
that synthesizes nonparametric regression and linear regression.

4. Morse-Smale signatures (section 3.3): We introduce a new visualization

method for densities and regression functions.

5. Morse-Smale two-sample testing (section 3.4): We develop a new method

for multivariate two-sample testing that can have good power.

Related work. The mathematical foundations for the Morse-Smale complex

Chen et al./Inference using the Morse-Smale

4

Fig 3. A one dimensional example. The blue dots are local modes and the green dots are
local minima. Left panel: the basins of attraction for two local modes are colored by brown
and orange. Middle panel: the basin of attraction (negative gradient) for the local minima are
colored by red, purple and violet. Right panel: The intersection of the basins, which are called
d-cells.

are from Morse theory (Morse, 1925, 1930; Milnor, 1963). Morse theory has
many applications including computer vision (Paris and Durand, 2007), com-
putational geometry (Cohen-Steiner et al., 2007) and topological data analysis
(Chazal et al., 2014).

Previous work on the stability of the Morse-Smale complex can be found in
Chen et al. (2016) and Chazal et al. (2014) but they only consider critical points
rather than the whole Morse-Smale complex. Arias-Castro et al. (2016) prove
pointwise convergence for the gradient ascent curves but this is not suﬃcient
for proving the stability of the complex because the convergence of complexes
requires convergence of multiple curves and the constants in the convergence
rate derived from Arias-Castro et al. (2016) vary from points to points and some
constants diverge when we are getting closer to the boundaries of complexes.
Thus, we cannot obtain a uniform convergence of gradient ascent curves directly
based on their results. Morse-Smale regression and visualization were proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013).

The R code (Algorithm 1, 2, and 3) used in this paper can be found at

https://github.com/yenchic/Morse_Smale.

2. Morse Theory

To motivate formal deﬁnitions, we start with the simple, one-dimensional ex-
ample depicted in Figure 3. The left panel shows the sets associated with each
local maximum (i.e. the basins of attraction of the maxima). The middle panel
shows the sets associated with each local minimum. The right panel show the
intersections of these basins, which gives the Morse-Smale complex deﬁned by
the function. Each interval in the complex, called a cell, is a region where the
function is increasing or decreasing.

Now we give a formal deﬁnition. Let f : K ⊂ Rd (cid:55)→ R be a function with
bounded third derivatives that is deﬁned on a compact set K. Let g(x) = ∇f (x)
and H(x) = ∇∇f (x) be the gradient and Hessian matrix of f , respectively, and
let λj(x) be the jth largest eigenvalue of H(x). Deﬁne C = {x ∈ K : g(x) = 0}
to be the set of all f ’s critical points, which we call the critical set. Using the

Chen et al./Inference using the Morse-Smale

5

signs of the eigenvalues of the Hessian, the critical set C can be partitioned into
d + 1 distinct subsets C0, · · · , Cd, where

Ck = {x ∈ K : g(x) = 0, λk(x) > 0, λk+1(x) < 0},

k = 1, · · · , d − 1.

(1)

We deﬁne C0, Cd to be the sets of all local maxima and minima (corresponding
to all eigenvalues being negative and positive respectively). The set Ck is called
k−th order critical set.

A smooth function f is called a Morse function (Morse, 1925; Milnor, 1963)
if its Hessian matrix is non-degenerate at each critical point. That is, |λj(x)| >
0, ∀x ∈ C for all j. In what follows we assume f is a Morse function (actually,
later we will assume further that f is a Morse-Smale function).

Given any point x ∈ K, we deﬁne the gradient ascent ﬂow starting at x,

πx : R+ (cid:55)→ K, by

πx(0) = x
π(cid:48)
x(t) = g(π(t)).

(2)

A particle on this ﬂow moves along the gradient from x towards a “destination”
given by

dest(x) ≡ lim
t→∞
It can be shown that dest(x) ∈ C for x ∈ K.

πx(t).

We can thus partition K based on the value of dest(x). These partitions are
called descending manifolds in Morse theory (Morse, 1925; Milnor, 1963). Recall
Ck is the k-th order critical points, we assume Ck = {ck,1, · · · , ck,mk } contains
mk distinct elements. For each k, deﬁne

Dk = {x : dest(x) ∈ Cd−k}
Dk,j = {x : dest(x) = cd−k,j} ,

j = 1, · · · md−k.

(3)

That is, Dk is the collection of all points whose gradient ascent ﬂow converges to
a (d−k)-th order critical point and Dk,j is the collection of points whose gradient
ascent ﬂow converges to the j-th element of Cd−k. Thus, Dk = (cid:83)md−k
j=1 Dk,j. From
Theorem 4.2 in Banyaga and Hurtubise (2004), each Dk is a disjoint union
of k-dimensional manifolds (Dk,j is a k-dimensional manifold). We call Dk,j
a descending k-manifold of f . Each descending k-manifold is a k-dimensional
manifold such that the gradient ﬂow from every point converges to the same
(d − k)-th order critical point. Note that {D0, · · · , Dk} forms a partition of K.
The top panels of Figure 4 give an example of the descending manifolds for a
two dimensional case.

The ascending manifolds are similar to descending manifolds but are deﬁned
through the gradient descent ﬂow. More precisely, given any x ∈ K, a gradient
descent ﬂow γ : R+ (cid:55)→ K starting from x is given by

γx(0) = x
γ(cid:48)
x(t) = −g(π(t)).

(4)

Chen et al./Inference using the Morse-Smale

6

(a)

(b)

(c)

(d)

Fig 4. Two-dimensional examples of critical points, descending manifolds, ascending mani-
folds, and 2-cells. This is the same function as Figure 1. (a): The set Ck for k = 0, 1, 2. The
four blue dots are C0, the collection of local modes (each of them is c0,j some j = 1, · · · , 4).
The four orange dots are C1, the collection of saddle points (each of them is c1,j for some
j = 1, · · · , 4). The green dots are C2, the collection of local minima (each green dot is c2,j
for some j = 1, · · · , 9). (b): The set Dk for k = 0, 1, 2. The yellow area is D2 (each subregion
separated by blue curves are D2,j , j = 1, · · · , 4). The two blue curves are D1 (each of the 4
blue segments are D1,j , j = 1, · · · , 4). The green dots are D0 (also C2), the collection of local
minima (each green dot is D0,j for some j = 1, · · · , 9). (b): The set Ak for k = 0, 1, 2. The
yellow area is A0 (each subregion separated by red curves are A0,j , j = 1, · · · , 9). The two
red curves are A1 (each of the 4 red segments are A1,j , j = 1, · · · , 4). The blue dots are A2
(also C0), the collection of local modes (each green dot is A0,j for some j = 1, · · · , 4). (d):
Example for 2-cells. The thick blue curves are D1 and thick red curves are A1.

Chen et al./Inference using the Morse-Smale

7

Unlike the ascending ﬂow deﬁned in (2), γx is a ﬂow that moves along the
gradient descent direction. The descent ﬂow γx shares similar properties to the
ascent ﬂow πx; the limiting point limt→∞ γx(t) ∈ C is also in critical set when
f is a Morse function. Thus, similarly to Dk and Dk,j, we deﬁne

Ak =

x : lim
t→∞

γx(t) ∈ Cd−k

(cid:110)

(cid:110)

(cid:111)

(cid:111)

Ak,j =

x : lim
t→∞

γx(t) = cd−k,j

,

j = 1, · · · , mj−k.

(5)

Ak and Ak,j have dimension d − k and each Ak,j is a partition for Ak and
{A0, · · · , Ad} consist of a partition for K. We call each Ak,j an ascending k-
manifold to f .

A smooth function f is called a Morse-Smale function if it is a Morse function
and any pair of the ascending and descending manifolds of f intersect each
other transversely (which means that pairs of manifolds are not parallel at their
intersections); see e.g. Banyaga and Hurtubise (2004) for more details. In this
paper, we also assume that f is a Morse-Smale function. Note that by the
Kupka-Smale Theorem (see e.g. Theorem 6.6 in Banyaga and Hurtubise (2004)),
Morse-Smale functions are generic (dense) in the collection of smooth functions.
For more details, we refer to Section 6.1 in Banyaga and Hurtubise (2004).

A k-cell (also called Morse-Smale cell or crystal) is the non-empty intersection
between any descending k1-manifold and an ascending (d − k2)-manifold such
that k = min{k1, k2} (the ascending (d − k2)-manifold has dimension k2). When
we simply say a cell, we are referring to the d-cell since d-cells consists of the
majority of K (the totality of k-cells with k < d has Lebesgue measure 0). The
Morse-Smale complex for f is the collection of all k-cells for k = 0, · · · , d. The
bottom panels of Figure 4 give examples for the ascending manifolds and the
d-cells for d = 2. Another example is given in Figure 1.

The cells of a smooth function can be used to construct an additive de-
composition that is useful in data analysis. For a Morse-Smale function f , let
E1, · · · , EL be its associated cells. Then we can decompose f into

f (x) =

f(cid:96)(x)1(x ∈ E(cid:96)),

(6)

L
(cid:88)

(cid:96)=1

where each f(cid:96)(x) behaves like a multivariate isotonic function (Barlow et al.,
1972; Bacchetti, 1989). Namely, f (x) = f(cid:96)(x) when x ∈ E(cid:96). This decomposition
is because within each E(cid:96), f has exact a local mode and a local minimum on
the boundary of E(cid:96). The fact that f admits such a decomposition will be used
frequently in Section 3.2 and 3.3.

Among all descending/ascending manifolds, the descending d-manifolds and
the ascending 0-manifolds are often of great interest. For instance, mode cluster-
ing (Li et al., 2007; Azzalini and Torelli, 2007) uses the descending d-manifolds
to partition the domain K into clusters. Morse-Smale regression (Gerber and
Potter, 2011; Gerber et al., 2013) ﬁts a linear regression individually over each
d-cell (non-empty intersection of pairs of descending d-manifolds and ascending

Chen et al./Inference using the Morse-Smale

8

(a) Basins of attraction

(b) Gradient ascent

(c) Mode clustering

Fig 5. An example of mode clustering. (a): Basin of attraction for each local mode (red +).
Black dots are data points. (b): Gradient ﬂow (blue lines) for each data point. The gradient
ﬂow starts at one data point and ends at one local modes. (c): Mode clustering; we use the
destination for gradient ﬂow to cluster data points.

0-manifolds). Regions outside descending d-manifolds or ascending 0-manifolds
have Lebesgue measure 0. Thus, later in our theoretical analysis, we will focus
on the stability of the set Dd and A0 (see Section 4.1). We deﬁne boundaries of
Dd as

B ≡ ∂Dd = Dd−1 ∪ · · · ∪ D0.

(7)

The set B will be used frequently in Section 4.

3. Applications in Statistics

3.1. Mode Clustering

Mode clustering (Li et al., 2007; Azzalini and Torelli, 2007; Chac´on and Duong,
2013; Arias-Castro et al., 2016; Chac´on et al., 2015; Chen et al., 2016) is a
clustering technique based on the Morse-Smale complex and is also known as
mean-shift clustering (Fukunaga and Hostetler, 1975; Cheng, 1995; Comaniciu
and Meer, 2002). Mode clustering uses the descending d-manifolds of the density
function p to partition the whole space K. (Although the d-manifolds do not
contain all points in K, the regions outside d-manifolds have Lebesgue measure
0). See Figure 5 for an example.

Now, we brieﬂy describe the procedure of mode clustering. Let X = {X1, · · · , Xn}

be a random sample from density p deﬁned on a compact set K and assumed to
be a Morse function. Recall that dest(x) is the destination of the gradient ascent
ﬂow starting from x. Mode clustering partitions the sample based on dest(x)
for each point; speciﬁcally, it partitions X = X1

(cid:83) · · · (cid:83) XK such that

X(cid:96) = {Xi ∈ X : dest(Xi) = m(cid:96)},

where each m(cid:96) is a local mode of p. We can also view mode clustering as a cluster-
(cid:83) · · · (cid:83) Dd,L
ing technique based on the d-descending manifolds. Let Dd = Dd,1

Chen et al./Inference using the Morse-Smale

9

be the d-descending manifolds of p, assuming that L is the number of local
modes. Then each cluster X(cid:96) = X (cid:84) Dd,(cid:96).

In practice, however, we do not know p so we have to use a density estimator

(cid:98)pn. A common density estimator is the kernel density estimator (KDE):

(cid:98)pn(x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(8)

where K is a smooth kernel function and h > 0 is the smoothing parameter.
Note that mode clustering is not limited to the KDE; other density estimators
also give us a sample-based mode clustering. Based on the KDE, we are able to
estimate gradient (cid:98)gn(x), the gradient ﬂows (cid:98)πx(t), and the destination (cid:100)destn(x)
(note that the mean shift algorithm is an algorithm to perform these tasks).
Thus, we can estimate the d-descending manifolds by the plug-in from (cid:98)pn. Let
(cid:83) · · · (cid:83) (cid:98)Dd,(cid:98)L be the d-descending manifolds of (cid:98)pn, where (cid:98)L is the
(cid:98)Dd = (cid:98)Dd,1
number of local modes of (cid:98)pn. The estimated clusters will be (cid:98)X1, · · · , (cid:98)X
(cid:98)L, where
each (cid:98)X(cid:96) = X (cid:84) (cid:98)Dd,(cid:96). Figure 5 displays an example of mode clustering using the
KDE.

A nice property of mode clustering is that there is a clear population quan-
tity that our estimator (clusters based on the given sample) is estimating: the
population partition of the data points. Thus we can consider properties of the
procedure such as consistency, which we discuss in detail in Section 4.2.

3.2. Morse-Smale Regression

Let (X, Y ) be a random pair where Y ∈ R and Xi ∈ K ⊂ Rd. Estimating the
regression function m(x) = E[Y |X = x] is challenging for d of even moderate
size. A common way to address this problem is to use a simple regression function
that can be estimated with low variance. For example, one might use an additive
regression of the form m(x) = (cid:80)
j mj(xj) which is a sum of one-dimensional
smooth functions. Although the true regression function is unlikely to be of this
form, it is often the case that the resulting estimator is useful.

A diﬀerent approach, Morse-Smale regression (MSR), is suggested in Gerber
et al. (2013). This takes advantage of the (relatively) simple structure of the
Morse-Smale complex and the isotone behavior of the function on each cell.
Speciﬁcally, MSR constructs a piecewise linear approximation to m(x) over the
cells of the Morse-Smale complex.

We ﬁrst deﬁne the population version of the MSR. Let m(x) = E(Y |X = x)
be the regression function and is assumed to be a Morse-Smale function. Let
E1, · · · EL be the d-cells for m. The Morse-Smale Regression for m is a piecewise
linear function within each cell E(cid:96) such that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

(9)

Chen et al./Inference using the Morse-Smale

10

where (µ(cid:96), β(cid:96)) are obtained by minimizing mean square error:
E (cid:0)(Y − mMSR(X))2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1)

= argmin

E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(cid:1)

(10)

µ,β

µ,β

That is, mMSR is the best linear piecewise predictor using the d-cells. One can
also view MSR as using a linear function to approximate f(cid:96) in the additive
model (6). Note that mMSR is well deﬁned except on the boundaries of E(cid:96) that
have Lebesgue measure 0.

Now we deﬁne the sample version of the MSR. Let (X1, Y1), · · · , (Xn, Yn) be
the random sample from the probability measure PX × PY such that Xi ∈ K ⊂
Rd and Yi ∈ R. Throughout section 3.2, we assume the density of covariates X
is bounded, positive and has a compact support K and the response Y has ﬁnite
second moment.

Let (cid:98)mn be a smooth nonparametric regression estimator for m. We call (cid:98)mn
the pilot estimator. For instance, one may use the kernel regression Nadaraya
(1964) (cid:98)mn(x) =
(cid:98)mn as (cid:98)E1, · · · , (cid:98)E(cid:98)L. Using the data (Xi, Yi) within each estimated d-cell, (cid:98)E(cid:96), the
MSR for (cid:98)mn is given by

as the pilot estimator. We deﬁne d-cells for

i=1 YiK( x−Xi
h )
(cid:80)n
i=1 K( x−Xi
h )

(cid:80)n

(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where ((cid:98)µ(cid:96), (cid:98)β(cid:96)) are obtained by minimizing the empirical squared error:

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

(cid:88)

i:Xi∈ (cid:98)E(cid:96)

(Yi − µ − βT Xi)2

(11)

(12)

This MSR is slightly diﬀerent from the original version in Gerber et al. (2013).
We will discuss the diﬀerence in Remark 1. Computing the parameters of MSR
is not very diﬃcult–we only need to compute the cell labels of each observation
(this can be done by the mean shift algorithm or some fast variants such as the
quick-shift algorithm Vedaldi and Soatto 2008) and then ﬁt a linear regression
within each cell.

MSR may give low prediction error in some cases; see Gerber et al. (2013) for
some concrete examples. In Theorem 5, we prove that we may estimate mMSR at
a fast rate. Moreover, the regression function may be visualized by the methods
discussed later.

Remark 1 The original version of Morse-Smale regression proposed in Gerber
et al. (2013) does not use d-cells of a pilot nonparametric estimate (cid:98)mn. Instead,
they directly ﬁnd local modes and minima using the original data points (Xi, Yi).
This saves computational eﬀort but comes with a price: there is no clear popu-
lation quantity being estimated by their approach. That is, when the sample size
increases to inﬁnity, there is no guarantee that their method will converge. In
our case, we apply a consistent pilot estimate for m and construct d-cells on
this pilot estimate. As is shown in Theorem 4, our method is consistent for this
population quantity.

Chen et al./Inference using the Morse-Smale

11

3.3. Morse-Smale Signatures and Visualization

In this section we deﬁne a new method for visualizing multivariate functions
based on the Morse-Smale complex, called Morse-Smale signatures. The idea is
very similar to the Morse-Smale regression but the signatures can be applied to
any Morse-Smale function.

Let E1, · · · , EK be the d-cells (nonempty intersection of a descending d-
manifold and an ascending 0-manifold) for a Morse-Smale function f that has
a compact support K. The function f depends on the context of the problem.
For density estimation, f is the density p or its estimator (cid:98)pn. For regression
problem, f is the regression function m or a nonparametric estimator (cid:98)mn .
For two sample test, f is the density diﬀerence p1 − p2 or the estimated density
diﬀerence (cid:98)p1−(cid:98)p2. Note that E1, · · · , EK form a partition for K except a Lebesgue
measure 0 set. Each cell corresponds to a unique pair of a local mode and a local
minimum. Thus, the local modes and minima along with d-cells form a bipartite
graph which we call it signature graph. The signature graph contains geometric
information about f . See Figure 6 and 7 for examples.

The signature is deﬁned as follows. We project the maxima and minima of
the function into R2 using multidimensional scaling. We connect a maximum
and minimum by an edge if there exists a cell that connects them. The width
of the edge is proportional to the norm of the linear coeﬃcients of the linear
approximation to the function within the cell. The linear approximation is

fMS(x) = η†

(cid:96) + γ†T

(cid:96) x,

for x ∈ E(cid:96),

where η†

(cid:96) ∈ R and γ†

(cid:96) ∈ Rd are parameters from

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

(13)

(14)

This is again a linear approximation for f(cid:96) in the additive model (6). Note that
fMS may not be continuos when we move from one cell to another. The summary
statistics for the edge associated with cell E(cid:96) are the parameters (η†
(cid:96) ). We
call the function fMS the (Morse-Smale) approximation function; it is the best
piecewise-linear representation for f (piecewise linear within each cell) under L2
error given the d-cells. This function is well-deﬁned except on a set of Lebesgue
measure 0 (the boundaries of each cell). See Figure 6 for a example on the
approximation function. The details are in Algorithm 1.

(cid:96) , γ†

Example. Figure 7 is an example using the GvHD dataset. We ﬁrst conduct
multidimensional scaling (Kruskal, 1964) on the local modes and minima for f
and plot them on the 2-D plane. In Figure 7, the blue dots are local modes and
the green dots are local minima. These dots act as the nodes for the signature
graph. Then we add edges, representing the cells for f that connect pairs of local
modes and minima, to form the signature graph. Lastly, we adjust the width
for the edges according to the strength (L2 norm) of regression function within
each cell (i.e. (cid:107)γ†
(cid:96) (cid:107)). Algorithm 1 provides a summary for visualizing a general
multivariate function using what we described in this paragraph.

Chen et al./Inference using the Morse-Smale

12

(a) Original function

(b) Approximation function

(c) Signature graph

Fig 6. Morse-Smale signatures for a smooth function. (a): The original function. The blue
dots are local modes, the green dots are local minima and the pink dot is a saddle point. (b):
The Morse-Smale approximation to (a). This is the best piecewise linear approximation to the
original function given d-cells. (c): This bipartite graph has nodes that are local modes and
minima and edges that represent the d-cells. Note that we can summarize the smooth function
(a) by the signature graph (c) and the parameters for constructing approximation function
(b). The signature graph and parameters for approximation function deﬁne the Morse-Smale
signatures.

Algorithm 1 Visualization using Morse-Smale Signatures

Input: Grid points x1, · · · , xN and the functional evaluations f (x1), · · · , f (xN ).
1. Find local modes and minima of f on the discretized points x1, · · · , xN . Let M1, · · · MK
and m1, · · · , mS denote the grid points for modes and minima.
2. Partition {x1, · · · , xN } into X1, · · · XL according to the d-cells of f (1. and 2. can be done
by using a k-nearest neighbor gradient ascent/descent method; see Algorithm 1 in Gerber
et al. (2013)).
3. For each cell X(cid:96), ﬁt a linear regression with (Xi, Yi) = (xi, f (xi)), where xi ∈ X(cid:96). Let
the regression coeﬃcients (without intercept) be β(cid:96).
4. Apply multidimensional scaling to modes and minima jointly. Denote their 2 dimensional
representation points as

1 , · · · M ∗

5. Plot {M ∗
6. Add edge to a pair of mode and minimum if there exist a cell that connects them. The
width of the edge is in proportional to (cid:107)β(cid:96)(cid:107) (for cell X(cid:96)).

1, · · · , m∗

K , m∗

1 , · · · M ∗

K , m∗

1, · · · , m∗

S }.

{M ∗
S }.

Chen et al./Inference using the Morse-Smale

13

Fig 7. Morse-Smale Signature visualization (Algorithm 1) of the density diﬀerence for GvHD
dataset (see Figure 2). The blue dots are local modes; the green dots are local minima; the
brown lines are d-cells. These dots and lines form the signature graph. The width indicates
the L2 norm for the slope of regression coeﬃcients. i.e. (cid:107)γ†
(cid:96) (cid:107). The location for modes and
minima are obtained by multidimensional scaling so that the relative distance is preserved.

3.4. Two Sample Comparison

The Morse-Smale complex can be used to compare two samples. There are two
ways to do this. The ﬁrst one is to test the diﬀerence in two density functions
locally and then use the Morse-Smale signatures to visualize regions where the
two samples are diﬀerent. The second approach is to conduct a nonparametric
two sample test within each Morse-Smale cell. The advantage of the ﬁrst ap-
proach is that we obtain a visual display on where the two densities are diﬀerent.
The merit of the second method is that we gain additional power in testing the
density diﬀerence by using the shape information.

3.4.1. Visualizing the Density Diﬀerence

Let X1, . . . Xn and Y1, . . . , Ym be two random sample with densities pX and pY .
In a two sample comparison, we not only want to know if pX = pY but we also
want to ﬁnd the regions that they signiﬁcantly disagree. That is, we are doing
the local tests

H0(x) : pX (x) = pY (x)
(15)
simultaneously for all x ∈ K and we are interested in the regions where we reject
H0(x). A common approach is to estimate the density for both sample by the
KDE and set a threshold to pickup those regions that the density diﬀerence is

Chen et al./Inference using the Morse-Smale

14

large. Namely, we ﬁrst construct density estimates

(cid:98)pX (x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(cid:98)pY (x) =

1
mhd

m
(cid:88)

i=1

(cid:19)

(cid:18) x − Yi
h

K

and then compute (cid:98)f (x) = (cid:98)pX (x) − (cid:98)pY (x). The regions
(cid:111)
(cid:110)
x ∈ K : | (cid:98)f (x)| > λ

Γ(λ) =

(16)

(17)

are where we have strong evidence to reject H0(x). The threshold λ can be
picked by quantile values of the bootstrapped L∞ density deviation to control
type 1 error or can be chosen by controlling the false discovery rate (Duong,
2013).

Unfortunately, Γ(λ) is hard to visualize when d > 3. So we use the Morse-
Smale complex for (cid:98)f and visualize Γ(λ) by its behavior on the d-cells of the
complex. Algorithm 2 gives a method for visualizing density diﬀerences like
Γ(λ) in the context of comparing two independent samples.

Algorithm 2 Visualization For Two Sample Test

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, threshold λ and radius constant r0
1. Compute the density estimates (cid:98)pX and (cid:98)pY .
2. Compute the diﬀerence function (cid:98)f = (cid:98)pX − (cid:98)pY and the signiﬁcant regions
(cid:111)
x ∈ K : (cid:98)f (x) < −λ

x ∈ K : (cid:98)f (x) > λ

, Γ−(λ) =

Γ+(λ) =

(18)

(cid:111)

(cid:110)

(cid:110)

3. Find the d-cells for (cid:98)f , denoted as E1, · · · , EL.
4. For cell E(cid:96), do (4-1) and (4-2):
4-1. compute the cell center e(cid:96), cell size V(cid:96) = Vol(E(cid:96)),
4-2. compute the positive signiﬁcant ratio and negative signiﬁcant ratio

r+
(cid:96) =

Vol(E(cid:96) ∩ Γ+(λ))
Vol(E(cid:96))

,

r−
(cid:96) =

Vol(E(cid:96) ∩ Γ−(λ))
Vol(E(cid:96))

.

5. For every pair of cell Ej and E(cid:96) (j (cid:54)= (cid:96)), compute the shared boundary size:

Bj(cid:96) = Vold−1( ¯Ej ∩ ¯E(cid:96)),

(19)

(20)

where Vold−1 is the d − 1 dimensional Lebesgue measure.
6. Do multidimensional scaling (Kruskal, 1964) to e1, · · · , eL to obtain low dimensional
representation (cid:101)e1, · · · , (cid:101)eL.
7. Place a ball center at each (cid:101)e(cid:96) with radius r0 ×
8. If r+

(cid:96) > 0, add a pie chart center at (cid:101)e(cid:96) with radius r0 ×
.

(cid:18) r+
(cid:96) +r−
r+
9. Add a line to connect two nodes (cid:101)ej and (cid:101)e(cid:96) if Bj(cid:96) > 0. We may adjust the thickness of
the line according to Bj(cid:96).

chart contains two groups, each with ratio

r−
(cid:96)
(cid:96) +r−
r+

(cid:96) ). The pie

V(cid:96) × (r+

(cid:96) + r−

(cid:96) + r−

V(cid:96).

√

√

(cid:19)

,

(cid:96)

(cid:96)

(cid:96)

An example for Algorithm 2 is in Figure 2, in which we apply the visualization
algorithm for the the GvHD dataset by using kernel density estimator. We
choose the threshold λ by bootstrapping the L∞ diﬀerence for (cid:98)f i.e. supx | (cid:98)f ∗(x)−
(cid:98)f (x)|, where (cid:98)f ∗ is the density diﬀerence for the bootstrap sample. We pick
α = 95% upper quantile value for the bootstrap deviation as the threshold.

Chen et al./Inference using the Morse-Smale

15

The radius constant r0 is deﬁned by the user. It is a constant for visualiza-
tion and does not aﬀect the analysis. Algorithm 2 preserves the relative position
for each cell and visualizes the cell according to its size. The pie-chart provides
the ratio of regions where the two densities are signiﬁcantly diﬀerent. The lines
connecting two cells provide the geometric information about how cells are con-
nected to each other.

By applying Algorithm 2 to the GvHD dataset (Figure 2), we ﬁnd that there
are 6 cells and one cell much larger than the others. Moreover, in most regions,
the blue regions are larger than the red areas. This indicates that compared
to the density of the control group, the density of the GvHD group seem to
concentrates more so that the regions above the threshold are larger.

3.4.2. Morse-Smale Two-Sample Test

Here we introduce a technique combining the energy test (Baringhaus and Franz,
2004; Sz´ekely and Rizzo, 2004, 2013) and the Morse-Smale complex to conduct
a two sample test. We call our method the Morse-Smale Energy test (MSE test).
The advantage of the MSE test is that it is a nonparametric test and its power
can be higher than the energy test; see Figure 8. Moreover, we can combine
our test with the visualization tool proposed in the previous section (Algorithm
2); see Figure 9 for an example for displaying p-values from MSE test when
visualizing the density diﬀerence.

Before we introduce our method, we ﬁrst review the ordinary energy test.
Given two random variables X ∈ Rd and Y ∈ Rd, the energy distance is deﬁned
as

E(X, Y ) = 2E(cid:107)X − Y (cid:107) − E(cid:107)X − X (cid:48)(cid:107) − E(cid:107)Y − Y (cid:48)(cid:107),
where X (cid:48) and Y (cid:48) are iid copies of X and Y . The energy distance has several
useful applications such as the goodness-of-ﬁt testing (Sz´ekely and Rizzo, 2005),
two sample testing (Baringhaus and Franz, 2004; Sz´ekely and Rizzo, 2004, 2013),
clustering (Szekely and Rizzo, 2005), and distance components (Rizzo et al.,
2010) to name but few. We recommend an excellent review paper in (Sz´ekely
and Rizzo, 2013).

(21)

For the two sample test, let X1, · · · , Xn and Y1, · · · , Ym be the two samples

we want to test. The sample version of energy distance is

(cid:98)E(X, Y ) =

(cid:107)Xi − Yj(cid:107) −

(cid:107)Xi − Xj(cid:107) −

(cid:107)Yi − Yj(cid:107).

2
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

(22)
P→ 0.
If X and Y are from the sample population (the same density), (cid:98)E(X, Y )
Numerically, we use the permutation test for computing the p-value for (cid:98)E(X, Y ).
This can be done quickly in the R-package ‘energy’ (Rizzo and Szekely, 2008).
Now we formally introduce our testing procedure: the MSE test (see Algo-
rithm 3 for a summary). Our test consists of three steps. First, we split the data
into two halves. Second, we use one half of the data (contains both samples)

Chen et al./Inference using the Morse-Smale

16

to do a nonparametric density estimation (e.g. the KDE) and then compute
the Morse-Smale complex (d-cells). Last, we use the other half of the data to
conduct the energy distance two sample test ‘within each d-cell’. That is, we
partition the second half of the data by the d-cells. Within each cell, we do the
energy distance test. If we have L cells, we will have L p-values from the energy
distance test. We reject H0 if any one of the L p-values is smaller than α/L
(this is from Bonferroni correction). Figure 9 provides an example for using the
above procedure (Algorithm 3) along with the visualization method proposed
in Algorithm 2. Data splitting is used to avoid using the same data twice, which
ensures we have a valid test.

Algorithm 3 Morse-Smale Energy Test (MSE test)

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, smoothing parameter h, signiﬁcance
level α
1. Randomly split the data into halves D1 and D2; both contain equal number of X and Y
(assuming n and m are even).
2. Compute the KDE (cid:98)pX and (cid:98)pY by the ﬁrst sample D1.
3. Find the d-cells for (cid:98)f = (cid:98)pX − (cid:98)pY , denoted as E1, · · · , EL.
4. For cell E(cid:96), do 4-1 and 4-2:
4-1. Find X and Y in the second sample D2,
4-2. Do the energy test for two sample comparison. Let the p-value be p((cid:96))
5. Reject H0 if p((cid:96)) < α/L for some (cid:96).

Example. Figure 8 shows a simple comparison for the proposed MSE test to
the usual Energy test. We consider a K = 4 Gaussian mixture model in d = 2
with standard deviation of each component being the same σ = 0.2 and the
proportion for each component is (0.2, 0.5, 0.2, 0.1). The left panel displays a
sample with N = 500 from this mixture distribution. We draw the ﬁrst sample
from this Gaussian mixture model. For the second sample, we draw a similar
Gaussian mixture model except that we change the deviation of one component.
In the middle panel, we change the deviation to the third component (C3 in
left panel, which contains 20% data points). In the right panel, we change the
deviation to the fourth component (C4 in left panel, which contains 10% data
points). We use signiﬁcance level α = 0.05 and for MSE test, we consider the
Bonferroni correction and the smoothing bandwidth is chosen using Silverman’s
rule of thumb (Silverman, 1986).

Note that in both the middle and the right panels, the left most case (added
deviation equals 0) is where H0 should not be rejected. As can be seen from
Figure 8, the MSE test has much stronger power compared to the usual Energy
test.

The original energy test has low power while the MSE test has higher power.
This is because the two distributions only diﬀer at a small portion of the regions
so that a global test like energy test requires large sample sizes to detect the
diﬀerence. On the other hand, the MSE test partitions the space according to
the density diﬀerence so that it is capable of detecting the local diﬀerence.

Example. In addition to the higher power, we may combine the MSE test
with the visualization tool in Algorithm 2. Figure 9 displays an example where

Chen et al./Inference using the Morse-Smale

17

Fig 8. An example comparing the Morse-Smale Energy test to the original Energy test. We
consider a d = 2, K = 4 Gaussian mixture model. Left panel: an instance for the Gaussian
mixture. We have four mixture components, denoting as C1, C2, C3 and C4. They have equal
standard deviation (σ = 0.2) and the proportions for each components are (0.2, 0.5, 0.2, 0.1).
Middle panel: We changed the standard deviations of component C3 to 0.3, 0.4 and 0.5 and
compute the power for the MSE test and the usual Energy test at sample size N = 500 and
1000. (Standard deviation equals 0.2 is where H0 should not be rejected.) Right panel: We
add the variance of component C4 (the smallest component) and do the same comparison as
in the middle panel. We pick the signiﬁcance level α = 0.05 (gray horizontal line) and in the
MSE test, we reject H0 if the minimal p-value is less than α/L, where L is the number of
cells (i.e. we are using the Bonferroni correction).

we visualize the density diﬀerence and simultaneously indicate the p-values from
the Energy test within each cell using the GvHD dataset. This provides us more
information about how two distributions diﬀer from each other.

4. Theoretical Analysis

We ﬁrst deﬁne some notation for the theoretical analysis. Let f be a smooth
function. We deﬁne (cid:107)f (cid:107)∞ = supx |f (x)| to be the L∞-norm of f . In addition, let
(cid:107)f (cid:107)j,max denote the elementwise L∞-norm for j-th derivatives of f . For instance,

(cid:107)f (cid:107)1,max = max

(cid:107)gi(x)(cid:107)∞,

i

(cid:107)f (cid:107)2,max = max
i,j

(cid:107)Hij(x)(cid:107)∞.

We also deﬁne (cid:107)f (cid:107)0,max = (cid:107)f (cid:107)∞. We further deﬁne

(cid:107)f (cid:107)∗
The quantity (cid:107)f − h(cid:107)∗
h up to (cid:96)-th order derivative.

(cid:96),max = max {(cid:107)f (cid:107)j,max : j = 0, · · · , (cid:96)} .
(cid:96),max measures the diﬀerence between two functions f and

(23)

For two sets A, B, the Hausdorﬀ distance is

Haus(A, B) = inf{r : A ⊂ B ⊕ r, B ⊂ A ⊕ r},

(24)

where A ⊕ r = {y : minx∈A (cid:107)x − y(cid:107) ≤ r}. The Hausdorﬀ distance is like the L∞
distance for sets.

Let (cid:101)f : K ⊂ Rd (cid:55)→ R be a smooth function with bounded third derivatives.
Note that as long as (cid:107) (cid:101)f −f (cid:107)∗
3,max is small, (cid:101)f is also a Morse function by Lemma 9.
Let (cid:101)D denote the boundaries of the descending d-manifolds of (cid:101)f . We will show
if (cid:107)f − (cid:101)f (cid:107)∗

3,maxis suﬃciently small, then Haus( (cid:101)D, D) = O((cid:107) (cid:101)f − f (cid:107)1,max).

Chen et al./Inference using the Morse-Smale

18

Fig 9. An example using both Algorithm 2 and 3 to the GvHD dataset introduced in Figure 2.
We use data splitting as described in Algorithm 3. For the ﬁrst part of the data, we compute
the cells and visualize the cells using Algorithm 2. Then we apply the energy distance two
sample test for each cell as described in Algorithm 3 and we annotate each cell with a p-
value. Note that the visualization is slightly diﬀerent to Figure 2 since we use only half of the
original dataset in this case.

4.1. Stability of the Morse-Smale Complex

Before we state our theorem, we ﬁrst derive some properties of descending mani-
folds. Recall that we are interested in B = ∂Dd, the boundary of the descending
d-manifolds (and B is also the union of all j-descending manifolds for j < d).
Since each Dj is a collection of smooth j-dimensional manifolds embedded in
Rd, for every x ∈ Dj, there exists a basis v1(x), · · · , vd−j(x) such that each vk(x)
is perpendicular to Dj at x for k = 1, · · · d − j (Bredon, 1993; Helgason, 1979).
That is, v1(x), · · · , vd−j(x) span the normal space to Dj at x. For simplicity, we
write

V (x) = (v1(x), · · · , vd−j(x)) ∈ Rd×(d−j)

(25)

for x ∈ Dj.

Note the number of columns d − j ≡ d − j(x) in V (x) depends on which Dj
the point x belongs to. We use j rather than j(x) to simplify the notation. For
instance, if x ∈ D1, V (x) ∈ Rd×(d−1) and if x ∈ Dd−1, V (x) ∈ Rd×1. We also
let

V(x) = span{v1(x), · · · , vd−j(x)}
(26)
denote the normal space to B at x. One can view V(x) as the normal map of
the manifold Dj at x ∈ Dj.

For each x ∈ B, deﬁne the projected Hessian

HV (x) = V (x)T H(x)V (x),

(27)

which is the Hessian matrix of p by taking gradients along column space of
V (x). If x ∈ Dj, HV (x) is a (d − j) × (d − j) matrix. The eigenvalues of HV (x)

Chen et al./Inference using the Morse-Smale

19

determine how the gradient ﬂows are moving away from B. We let λmin(M ) be
the smallest eigenvalue for a symmetric matrix M . If M is a scalar (just one
point), then λmin(M ) = M .

Assumption (D): We assume that Hmin = minx∈B λmin(HV (x)) > 0.

This assumption is very mild; it requires that the gradient ﬂow moves away
from the boundary of ascending manifolds. In terms of mode clustering, this
requires the gradient ﬂow to move away from the boundaries of clusters. For a
point x ∈ Dd−1, let v1(x) be the corresponding normal direction. Then the gradi-
ent g(x) is normal to v1(x) by deﬁnition. That is, v1(x)T g(x) = v1(x)T ∇p(x) =
0, which means that the gradient along v1(x) is 0. Assumption (D) means that
the the second derivative along v1(x) is positive, which implies that the density
along direction v1(x) behaves like a local minimum at point x. Intuitively, this
is how we expect the density to behave around the boundaries: gradient ﬂows
are moving away from the boundaries (except for those ﬂows that are already
on the boundaries).
Theorem 1 (Stability of descending d-manifolds) Let f, (cid:101)f : K ⊂ Rd (cid:55)→ R
be two smooth functions with bounded third derivatives deﬁned as above and
let B, (cid:101)B be the boundaries of the associated ascending manifolds. Assume f is
a Morse function satisfying condition (D). When (cid:107)f − (cid:101)f (cid:107)∗
3,max is suﬃciently
small,

Haus( (cid:101)B, B) = O((cid:107) (cid:101)f − f (cid:107)1,max).

(28)

This theorem shows that the boundaries of descending d-manifolds for two Morse
functions are close to each other and the diﬀerence between the boundaries is
controlled by the rate of the ﬁrst derivative diﬀerence.

Similarly to descending manifolds, we can deﬁne all the analogous quantities

for ascending manifolds. We introduce the following assumption:

Assumption (A): We assume Hmax = maxx∈∂A0 λmax(HV (x)) < 0.

Note that λmax(M ) denotes the largest eigenvalue of a matrix M . If M is a
scalar, λmax(M ) = M . Under assumption (A), we have a similar stability result
(Theorem 1) for ascending manifolds. Assumptions (A) and (D) together imply
the stability of d-cells.

Theorem 1 can be applied to nonparametric density estimation. Our goal is to
estimate the boundary of the descending d-manifolds, B, of the unknown popu-
lation density function p. Our estimator is (cid:98)Bn, the boundary of the descending

Chen et al./Inference using the Morse-Smale

20

d-manifolds to a nonparametric density estimator e.g. the kernel density esti-
mate (cid:98)pn. Then under certain regularity condition, their diﬀerence is given by

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107)(cid:98)pn − p(cid:107)1,max) .

We will see this result in the next section when we discuss mode clustering.

Similar reasoning works for the nonparametric regression case. Assume that
we are interested in B, the boundary of descending d-manifolds, for the regres-
sion function m(x) = E(Y |X = x). And our estimator (cid:98)B is again a plug-in
estimate based on (cid:98)mn(x), a nonparametric regression estimator (e.g., kernel
estimator). Then under mild regularity conditions,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107) (cid:98)mn − m(cid:107)1,max) .

4.2. Consistency of Mode Clustering

A direct application of Theorem 1 is the consistency of mode clustering. Let
K (α) be the α-th derivative of K and let BCr denote the collection of functions
with bounded continuously derivatives up to the r-th order. We consider the
following two assumptions on the kernel function:
(K1) The kernel function K ∈ BC3 and is symmetric, non-negative and

(cid:90)

x2K (α)(x)dx < ∞,

(cid:90) (cid:16)

(cid:17)2

K (α)(x)

dx < ∞

for all α = 0, 1, 2, 3.

(K2) The kernel function satisﬁes condition K1 of Gine and Guillou (2002). That

is, there exists some A, v > 0 such that for all 0 < (cid:15) < 1, supQ N (K, L2(Q), CK(cid:15)) ≤
(cid:0) A
, where N (T, d, (cid:15)) is the (cid:15)−covering number for a semi-metric space
(cid:15)

(cid:1)v

(T, d) and

(cid:40)

K =

u (cid:55)→ K (α)

: x ∈ Rd, h > 0, |α| = 0, 1, 2

(cid:19)

(cid:18) x − u
h

(cid:41)
.

(K1) is a common assumption; see Wasserman (2006). (K2) is a weak assump-
tion guarantee the consistency for KDE under L∞ norm; this assumption ﬁrst
appeared in Gine and Guillou (2002) and has been widely assumed (Einmahl
and Mason, 2005; Rinaldo et al., 2010; Genovese et al., 2012; Rinaldo et al.,
2012; Genovese et al., 2014; Chen et al., 2015).
Theorem 2 (Consistency for mode clustering) Let p, (cid:98)pn be the density func-
tion and the KDE. Let B and (cid:98)Bn be the boundaries of clusters by mode clus-
tering over p and (cid:98)pn respectively. Assume (D) for p and (K1–2), then when
log n
nhd+6 → 0, h → 0,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O((cid:107)(cid:98)pn − p(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Chen et al./Inference using the Morse-Smale

21

The proof is simply to combine Theorem 1 and the rate of convergence for
estimating the gradient of density using KDE (Theorem 8). Thus, we omit the
proof. Theorem 2 gives a bound for the rate of convergence for the boundaries
for mode clustering. The rate can be decomposed into two parts, the bias O(h2)

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

and the (square root of) variance OP

. This rate is the same for the

L∞-loss of estimating the gradient of a density function, which makes sense
since the mode clustering is completely determined by the gradient of density.
Another way to describe the consistency for mode clustering is to show that
the proportion of data points that are incorrectly clustered (mis-clustered) con-
verges to 0. This can be quantiﬁed by the use of Rand index (Rand, 1971; Hubert
and Arabie, 1985; Vinh et al., 2009), which measures the similarity between two
partitions of the data points. Let dest(x) and (cid:100)destn(x) be the destination of
gradient of the true density function p(x) and the KDE (cid:98)pn(x). For a pair of
points x, y, we deﬁne

Ψ(x, y) =

(cid:26) 1
0

if dest(x) = dest(y)
if dest(x) (cid:54)= dest(y)

,

(cid:98)Ψn(x, y) =

(cid:40)

1
0

if (cid:100)destn(x) = (cid:100)destn(y)
if (cid:100)destn(x) (cid:54)= (cid:100)destn(y)

(29)
Thus, Ψ(x, y) = 1 if x, y are in the same cluster and 0 if they are not. The Rand
index for mode clustering using p versus using (cid:98)pn is

rand ((cid:98)pn, p) = 1 −

(cid:19)−1

(cid:18)n
2

(cid:88)

i(cid:54)=j

(cid:12)
(cid:12)
(cid:12)Ψ(Xi, Xj) − (cid:98)Ψn(Xi, Xj)

(cid:12)
(cid:12)
(cid:12) ,

(30)

which is the proportion of pairs of data points that the two clustering results
disagree on. If two clusterings output the same partition, the Rand index will
be 1.

Theorem 3 (Bound on Rand Index) Assume (D) for p and (K1–2). Then
when log n

nhd+6 → 0, h → 0, the adjusted Rand index

rand ((cid:98)pn, p) = 1 − O(h2) − OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Theorem 3 shows that the Rand index converges to 1 in probability, which
establishes the consistency of mode clustering in an alternative way. Theo-
rem 3 shows that the proportion of data points that are incorrectly assigned
(compared with mode clustering using population p) is bounded by the rate

O(h2) + OP

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

asymptotically.

Azizyan et al. (2015) also derived the convergence rate of the mode clustering
for the rand index. Here we brieﬂy compare our results to theirs. Azizyan et al.
(2015) consider a low-noise condition that leads to a fast convergence rate when
clusters are well-separated. Their approach can even be applied to the case of

(31)

(32)

(33)

(34)

Chen et al./Inference using the Morse-Smale

22

increasing dimensions. In our case (Theorem 3), we consider a ﬁxed dimension
scenario but we do not assume the low-noise condition. Thus, the main diﬀerence
between Theorem 3 and the result in Azizyan et al. (2015) is the assumptions
being made so our result complements the ﬁndings in Azizyan et al. (2015).

4.3. Consistency of Morse-Smale Regression

In what follows, we will show that (cid:98)mn,MSR(x) is a consistent estimator of mMSR(x).
Recall that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

where E(cid:96) is the d-cell deﬁned on m and the parameters are
E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1) .

µ,β

And (cid:98)mn,MSR is the two-stage estimator to mMSR(x) deﬁned by
(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where { (cid:98)E(cid:96) : (cid:96) = 1, · · · , (cid:98)L} are the collection of cells of the pilot nonparametric
regression estimator (cid:98)mn and (cid:98)µ(cid:96), (cid:98)β(cid:96) are the regression parameters from equation
(12):
(cid:88)

(Yi − µ − βT Xi)2.

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

i:Xi∈ (cid:98)E(cid:96)
Theorem 4 (Consistency of Morse-Smale Regression) Assume (A) and
(D) for m and assume m is a Morse-Smale function. Then when log n
nhd+6 →
0, h → 0, we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

+ O ((cid:107) (cid:98)mn − m(cid:107)1,max)

(35)

(cid:19)

(cid:18) 1
√
n

uniformly for all x except for a set Nn with Lebesgue measure OP((cid:107) (cid:98)mn−m(cid:107)1,max),
Theorem 4 states that when we have a consistent pilot nonparametric regression
estimator (such as the kernel regression), the proposed MSR estimator converges
to the population MSR. Similarly as in Theorem 6, the set Nn are regions around
the boundaries of cells where we cannot distinguish their host cell. Note that
when we use the kernel regression as the pilot estimator (cid:98)mn, Theorem 4 becomes

|mMSR(x) − (cid:98)mn,MSR(x)| = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

under regular smoothness conditions.

Now we consider a special case where we may obtain parametric rate of
(cid:83) · · · (cid:83) EL) be the boundaries

convergence for estimating mMSR. Let E = ∂ (E1
of all cells. We consider the following low-noise condition:

P (X ∈ E ⊕ (cid:15)) ≤ A(cid:15)β,

(36)

Chen et al./Inference using the Morse-Smale

23

for some A, β > 0. Equation (36) is Tsybakov’s low noise condition (Audibert
et al., 2007) applied to the boundaries of cells. Namely, (36) states that it is
unlikely to many observations near the boundaries of cells of m. Under this
low-noise condition, we obtain the following result using kernel regression.

Theorem 5 (Fast Rate of Convergence for Morse-Smale Regression)
Let the pilot estimator (cid:98)mn be the kernel regression estimator. Assume (A)
and (D) for m and assume m is a Morse-Smale function. Assume also (36)
holds for the covariate X and (K1-2) for the kernel function. Also assume that
. Then uniformly for all x except for a set Nn with

(cid:17)1/(d+6)(cid:19)

(cid:18)(cid:16) log n

h = O

n

Lebesgue measure OP

(cid:18)(cid:16) log n

(cid:17)2/(d+6)(cid:19)

,

n

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(cid:19)

(cid:18) 1
√
n

+ OP

(cid:32)(cid:18) log n
n

(cid:19)2β/(d+6)(cid:33)

.

(37)

Therefore, when β > 6+d

4 , we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(38)

(cid:18) 1
√
n

(cid:19)

.

Theorem 5 shows that when the low noise condition holds, we obtain a fast
rate of convergence for estimating mMSR. Note that the pilot estimator (cid:98)mn does
not ahve to be a kernel estimator; other approaches such as the local polynomial
regression will also work.

4.4. Consistency of the Morse-Smale Signature

Another application of Theorem 1 is to bound the diﬀerence of two Morse-
Smale signatures. Let f be a Morse-Smale function with cells E1, . . . , EL. Recall
that the Morse-Smale signatures are the bipartite graph and summary statistics
(locations, density values) for local modes, local minima, and cells. It is known in
the literature (see, e.g., Lemma 9) that when two functions (cid:101)f , f are suﬃciently
close, then

(cid:16)

(cid:17)

max
j

(cid:107)(cid:101)cj − cj(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)1,max

, max

(cid:107) (cid:101)f ((cid:101)cj) − f (cj)(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)∞

j

(39)
where (cid:101)cj, cj are critical points (cid:101)f and f respectively. This implies the stability of
local modes and minima.

So what we need is the stability of the summary statistics (η†

(cid:96) , γ†

(cid:96) ) associated

with the edges (cells). Recall that these summaries are deﬁned through (14)

(cid:16)

(cid:17)

,

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Chen et al./Inference using the Morse-Smale

24

For another function (cid:101)f , let ((cid:101)η†
(cid:96) ) be its signatures for cell (cid:101)E(cid:96). The following
theorem shows that if two functions are close, their corresponding Morse-Smale
signatures are also close.

(cid:96) , (cid:101)γ†

Theorem 6 Let f be a Morse-Smale function satisfying assumptions A and D,
and let (cid:101)f be a smooth function. Then when log n
nhd+6 → 0, h → 0, after relabeling
the indices of cells of (cid:101)f ,

(cid:110)
(cid:107)(cid:101)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:101)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

1,max

(cid:17)

.

Theorem 6 shows stability of the signatures (η†

(cid:96) , γ†

(cid:96) ). Note that Theorem 6

also implies that the stability of piecewise approximation

|fMS(x) − (cid:101)fMS(x)| = O

(cid:16)

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

Together with the stability of critical points (39), Theorem 6 proves the stability
of Morse-Smale signatures.

4.4.1. Example: Morse-Smale Density Estimation

As an example for Theorem 6, we consider density estimation. Let p be the
density of random sample X1, · · · , Xn and recall that (cid:98)pn is the kernel density
estimator. Let (η†
(cid:96) ) be the
signature for (cid:98)pn under cell (cid:98)E(cid:96). The following corollary guarantees the consistency
of Morse-Smale signatures for the KDE.

(cid:96) ) be the signature for p under cell E(cid:96) and ((cid:98)η†

(cid:96) , (cid:98)γ†

(cid:96) , γ†

Corollary 7 Assume (A,D) holds for p and the kernel function satisﬁes (K1–
2). Then when log n

nhd+6 → 0, h → 0, after relabeling we have
(cid:32)(cid:114)

(cid:110)
(cid:107)(cid:98)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:98)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

= O(h2) + OP

(cid:33)

.

log n
nhd+2

The proof to Corollary 7 is a simple application of Theorem 6 with the rate of
convergence for the ﬁrst derivative of the KDE (Theorem 8). So we omit the

proof. The optimal rate in Corollary 7 is OP

when we choose h to

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 2

n

be of order O

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 1

.

n

Remark 2 When we compute the Morse-Smale approximation function, we
may have some numerical problem in low-density regions because the density
estimate (cid:98)pn may have unbounded support. In this case, some cells may be un-
bounded, and the majority of these cells may have extremely low density value,
which makes the approximation function 0. Thus, in practice, we will restrict

Chen et al./Inference using the Morse-Smale

25

ourselves only to the regions whose density is above a pre-deﬁned threshold λ so
that every cell is bounded. A simple data-driven threshold is λ = 0.05 supx (cid:98)pn(x).
Note that Theorem 7 still works in this case but with a slight modiﬁcation: the
cells are deﬁne on the regions {x : ph(x) ≥ 0.05 × supx ph(x)}.

Remark 3 Note that for a density function, local minima may not exist or the
gradient ﬂow may not lead us to a local minimum in some regions. For instance,
for a Gaussian distribution, there is no local minimum and except for the center
of the Gaussian, if we follow the gradient descent path, we will move to inﬁnity.
Thus, in this case we only consider the boundaries of ascending 0-manifolds
corresponding to well-deﬁned local minima and assumptions (A) is only for the
boundaries corresponding to these ascending manifolds.

Remark 4 When we apply the Morse-Smale complex to nonparametric density
estimation or regression, we need to choose the tuning parameter. For instance,
in the MSR, we may use kernel regression or local polynomial regression so we
need to choose the smoothing bandwidth. For the density estimation problem
or mode clustering, we need to choose the smoothing bandwidth for the kernel
smoother. In the case of regression, because we have the response variable, we
would recommend to choose the tuning parameter by cross-validation. For the
kernel density estimator (and mode clustering), because the optimal rate depends
on the gradient estimation, we recommend choosing the smoothing bandwidth
using the normal reference rule for gradient estimation or the cross-validation
method for gradient estimation (Duong et al., 2007; Chac´on et al., 2011).

5. Discussion

In this paper, we introduced the Morse-Smale complex and the summary sig-
natures for nonparametric inference. We demonstrated that the Morse-Smale
complex can be applied to various statistical problems such as clustering, re-
gression and two sample comparisons. We showed that a smooth multivariate
function can be summarized by a few parameters associated with a bipartite
graph, representing the local modes, minima and the complex for the underly-
ing function. Moreover, we proved a fundamental theorem about the stability
of the Morse-Smale complex. Based on the stability theorem, we derived con-
sistency for mode clustering and regression.

The Morse-Smale complex provides a method to synthesize both paramet-
ric and nonparametric inference. Compared to parametric inference, we have a
more ﬂexible model to study the structure of the underlying distribution. Com-
pared to nonparametric inference, the use of the Morse-Smale complex yields a
visualizable representation for the underlying multivariate structures. This re-
veals that we may gain additional insights in data analysis by using geometric
features.

Although the Morse-Smale complex has many potential statistical applica-
tions, we need to be careful when applying it to a data set whose dimension
is large (say d > 10). When the dimension is large, the curse of dimensionality

Chen et al./Inference using the Morse-Smale

26

kicks in and the nonparametric estimators (in both density estimation problems
or regression analysis) are not accurate so the errors of the estimated Morse-
Smale complex can be huge.

Here we list some possible extensions for future research:

• Asymptotic distribution. We have proved the consistency (and the rate of
convergence) for estimating the complex but the limiting distribution is
still unknown. If we can derive the limiting distribution and show that
some resampling method (e.g. the bootstrap Efron (1979)) converges to
the same distribution, we can construct conﬁdence sets for the complex.
• Minimax theory. Despite the fact that we have derived the rate of con-
vergence for a plug-in estimator for the complex, we did not prove its
optimality. We conjecture the minimax rate for estimating the complex
should be related to the rate for estimating the gradient and the smooth-
ness around complex (Audibert et al., 2007; Singh et al., 2009).

Acknowledgement

We thank the referees and the Associate Editor for their very constructive com-
ments and suggestions.

Appendix A: Appendix: Proofs

First, we include a Theorem about the rate of convergence for the kernel density
estimator. This Lemma will be used in deriving the convergence rates.

Theorem 8 (Lemma 10 in Chen et al. (2015); see also Genovese et al. (2014))
Assume (K1–2) and that log n/n ≤ hd ≤ b for some 0 < b < 1. Then we have

(cid:107)(cid:98)pn − p(cid:107)∗

(cid:96),max = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

log n
nhd+2(cid:96)

for (cid:96) = 0, 1, 2.

of critical points.

To prove Theorem 1, we introduce the following useful Lemma for stability

Lemma 9 (Lemma 16 of Chazal et al. (2014)) Let p be a density with com-
pact support K of Rd. Assume p is a Morse function with ﬁnitely many, distinct,
critical values with corresponding critical points C = {c1, · · · , ck}. Also assume
that p is at least twice diﬀerentiable on the interior of K, continuous and dif-
ferentiable with non vanishing gradient on the boundary of K. Then there exists
(cid:15)0 > 0 such that for all 0 < (cid:15) < (cid:15)0 the following is true: for some positive
constant c, there exists η ≥ c(cid:15)0 such that, for any density q with support K
satisfying (cid:107)p − q(cid:107)∗

2,max ≤ η, we have

1. q is a Morse function with exact k critical points c(cid:48)

1, · · · , c(cid:48)

k and

Chen et al./Inference using the Morse-Smale

27

Fig 10. Diagram for lemmas and Theorem 1.

2. after suitable relabeling the indices, maxi=1,··· ,k (cid:107)ci − c(cid:48)

i(cid:107) ≤ (cid:15).

Note that similar result appears in Theorem 1 of Chen et al. (2016). This lemma
shows that two close Morse functions p, q will have similar critical points.

The proof of Theorem 1 requires several working lemmas. We provide a chart

for how we are going to prove Theorem 1.

First, we deﬁne some notations about gradient ﬂows. Recall that πx(t) ∈ K

is the gradient (ascent) ﬂow starting at x:

πx(0) = x,

π(cid:48)
x(t) = g(πx(t)).

For x that is not on the boundary set D, we deﬁne the time:

t(cid:15)(x) = inf{t : πx(s) ∈ B(m,

(cid:15)), for alls ≥ t},

√

where m is the destination of πx. That is, t(cid:15)(x) is the time to arrive the regions
around a local mode.

First, we prove a property for the direction of the gradient ﬁeld around bound-

aries.

Lemma 10 (Gradient ﬁeld and boundaries) Assume the notations in The-
orem 1 and assume f is a Morse function with bounded third derivatives and
satisﬁes assumption (D). Let s(x) = x − Πx, where Πx ∈ B is the projected
point from x onto B (when Πx is not unique, just pick any projected point). For
any q ∈ B, let x be a point near q such that x − q ∈ V(q), the normal space of
B at q. Let δ(x) = (cid:107)x − q(cid:107) and e(x) = x−q

(cid:107)x−q(cid:107) denote the unit vector. Then

Chen et al./Inference using the Morse-Smale

28

(a) Lemma 10

(b) Lemma 11

Fig 11. Illustration for Lemma 10 and 11. (a): We show that the angle between projection
vector s(x) and the gradient g(x) is always right whenever x is closed to the boundaries B. (b):
According to (a), any gradient ﬂow line start from a point x that is close to the boundaries
(distance < δ1), this ﬂow line is always moving away from the boundaries when the current
location is close to the boundaries. The ﬂow line can temporally get closer to the boundaries
when it is away from boundaries (distance > δ1)

1. For every point x such that

d(x, B) ≤ δ1 =

2Hmin
d2 · (cid:107)f (cid:107)3,max

,

we have

g(x)T s(x) ≥ 0.

That is, the gradient is pushing x away from the boundaries.

2. When δ(x) ≤

Hmin
d2·(cid:107)f (cid:107)3,max

,

(cid:96)(x) = e(x)T g(x) ≥

Hminδ(x).

1
2

Proof.
Claim 1. Because the projection of x onto B is Πx, s(x) ∈ V(Πx) and
s(x)T g(Πx) = 0 (recall that for p ∈ B, V(p) is the collection of normal vectors
of B at p).

Recall that d(x, B) = (cid:107)s(x)(cid:107) is the projected distance. By the fact that

Chen et al./Inference using the Morse-Smale

29

s(x)T g(Πx) = 0,

s(x)T g(x) = s(x)T (g(x) − g(Πx))
d2
2

≥ s(x)T H(Πx)s(x) −
= d(x, B)2 s(x)T
d(x, B)
(cid:18)

H(Πx)

(cid:107)f (cid:107)3,maxd(x, B)3

(Taylor’s theorem)

s(x)
d(x, B)

−

d2
2

(cid:107)f (cid:107)3,maxd(x, B)3

(cid:19)

≥ d(x, B)2

Hmin −

(cid:107)f (cid:107)3,maxd(x, B)

.

d2
2

(40)
Note that we use the vector-value Taylor’s theorem in the ﬁrst inequality and
the fact that for two close points x, y, the diﬀerence in the j-the element of
gradient gj(x) − gj(y) has the following expansion

gj(x) − gj(y) = Hj(y)T (x − y) +

(u − y)Tj(u)du

(cid:90) x

sup
u

u=y
1
2
d2
2

≥ Hj(y)T (x − y) −

(cid:107)Tj(u)(cid:107)2(cid:107)x − y(cid:107)2

≥ Hj(y)T (x − y) −

(cid:107)f (cid:107)3,max(cid:107)x − y(cid:107)2,

where Hj(y) = ∇gj(y) and Tj(y) = ∇∇gj(y) is the Hessian matrix of gj(y),
whose elements are the third derivatives of f (y).

Thus, when d(x, B) ≤ 2Hmin
, s(x)T g(x) ≥ 0, which proves the ﬁrst claim.
Claim 2. By deﬁnition, e(x)T g(q) = 0 because g(q) is in tangent space of B

d2·(cid:107)f (cid:107)3,max

at q and e(x) is in the normal space of B at q. Thus,

(cid:96)(x) = e(x)T g(x)

= e(x)T (g(x) − g(q))

≥ e(x)T H(q)(x − q) −

(cid:107)f (cid:107)3,max(cid:107)x − q(cid:107)2

(41)

= e(x)T H(π(x))e(x)δ(x) −

(cid:107)f (cid:107)3,maxδ(x)2

d2
2

d2
2

≥

Hminδ(x)

1
2

whenever δ(x) = (cid:107)x − q(cid:107) ≤
. Note that in the ﬁrst inequality we use
the same lower bound as the one in claim 1. Also note that x − q = e(x)δ(x)
and e(x) is in the normal space of B at π(x) so the third inequality follows from
assumption (D).

Hmin
d2·(cid:107)f (cid:107)3,max

(cid:3)

Lemma 10 can be used to prove the following result.

Chen et al./Inference using the Morse-Smale

30

Lemma 11 (Distance between ﬂows and boundaries) Assume the nota-
tions as the above and assumption (D). Then for all x such that 0 < d(x, B) =
δ ≤ δ1 = 2Hmin

,

d2(cid:107)f (cid:107)3,max

d(πx(t), B) ≥ δ,

for all t ≥ 0.

The main idea is that the projected gradient (gradient projected to the normal
space of nearby boundaries) is always positive. This means that the ﬂow cannot
move “closer” to the boundaries.

Proof. By Lemma 10, for every point x near to the boundaries (d(x, B) <
δ1), the gradient is moving this point away from the boundaries. Thus, for any
ﬂow πx(t), once it touches the region B ⊕ δ1, it will move away from this region.
So when a ﬂow leaves B ⊕ δ1, it can never come back.

Therefore, the only case that a ﬂow can be within the region B ⊕ δ1 is that

it starts at some x ∈ B ⊕ δ1. i.e. d(x, B) < δ1.

Now consider a ﬂow start at x such that 0 < d(x, B) ≤ δ1. By Lemma 10,
the gradient g(x) leads x to move away from the boundaries B. Thus, whenever
πx(t) ∈ B ⊕ δ1, the gradient is pushing πx(t) away from B. As a result, the time
that πx(t) is closest to B is at the beginning of the ﬂow .i.e. t = 0. This implies
that d(πx(t), B) ≥ d(πx(0), B) = d(x, B) = δ.

(cid:3)

With Lemma 11, we are able to bound the low gradient regions since the
ﬂow cannot move inﬁnitely close to critical points except its destination. Let
λmin > 0 be the minimal ‘absolute’ value of eigenvalues of all critical points.

Lemma 12 (Bounds on low gradient regions) Assume the density func-
tion f is a Morse function and has bounded third derivatives. Let C denote
the collection of all critical points and let λmin is the minimal ‘absolute’ eigen-
value for Hessian matrix H(x) evaluated at x ∈ C. Then there exists a constant
δ2 > 0 such that

G(δ) ≡

x : (cid:107)g(x)(cid:107) ≤

⊂ C ⊕ δ

(42)

(cid:26)

(cid:27)

λmin
2

δ

for every δ ≤ δ2.

Proof. Because the support K is compact and x ∈ K (cid:55)→ (cid:107)g(x)(cid:107) is contin-
uous, for any g0 > 0 suﬃciently small, there exists a constant R(g0) > 0 such
that

G1(g0) ≡ {x : (cid:107)g(x)(cid:107) ≤ g0} ⊂ C ⊕ R(g0)

and when g0 → 0, R(g0) → 0. Thus, there is a constant g1 > 0 such that
R(g1) =

.

λmin
2d3(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

31

The set C ⊕ λmin

2(cid:107)f (cid:107)3,max

has a useful feature: for any x ∈ C ⊕ λmin

,

2(cid:107)f (cid:107)3,max

(cid:107)H(x) − H(c)(cid:107)F = (cid:107)(x − c)f (3)(c + t(x − c))dt(cid:107)F

≤ d3(cid:107)x − c(cid:107)(cid:107)f (cid:107)3,max
λmin
2d3(cid:107)f (cid:107)3,max

≤ d3

· (cid:107)f (cid:107)3,max

=

λmin
2

,

where f (3) is a d × d × d array of the third derivative of f and (cid:107)A(cid:107)F is the
Frobenius norm of the matrix A. By Hoﬀman–Wielandt theorem (see, e.g., page
165 of Bhatia 1997), the eigenvalues between H(x) and H(c) is bounded by
(cid:107)H(x) − H(c)(cid:107)F . Therefore, the smallest eigenvalue of H(x) must be greater
than or equal to the smallest eigenvalue of H(c) minus λmin
2 . Because λmin is
the smallest absolute eigenvalues of H(c) for all c ∈ C, the smallest eigenvalue
of H(x) is greater than or equal to λmin
.

2 , for all x ∈ C ⊕ R(g1) = C ⊕
λmin
2d3(cid:107)f (cid:107)3,max

λmin
2d3(cid:107)f (cid:107)3,max
, for any

Using the above feature and the fact that G1(g1) ⊂ C ⊕

x ∈ G1(g1), we have the following inequalities:

g1 ≥ (cid:107)g(x)(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

=

0

≥ (cid:107)x − c(cid:107)

λmin.

1
2

(x − c)H(c + t(x − c))dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Thus, (cid:107)x − c(cid:107) ≤ 2g1
λmin

, which implies

Moreover, because G1(g2) ⊂ G1(g3) for any g2 ≤ g3, any g2 ≤ g1 satisﬁes

Now pick δ = 2g2
λmin

, we conclude

G1(g1) ⊂ C ⊕

G1(g2) ⊂ C ⊕

2g1
λmin

.

2g2
λmin

.

G1

(cid:19)

(cid:18) λmin
2δ

= G(δ) ⊂ C ⊕ δ

δ =

2g2
λmin

≤

2g1
λmin

= δ2,

for all

(cid:3)

where g1 is the constant such that R(g1) =

λmin
2d3(cid:107)f (cid:107)3,max

.

(43)

Chen et al./Inference using the Morse-Smale

32

Fig 12. Illustration for H((cid:15), δ). The thick black lines are boundaries B; solid dots are local
modes; box is local minimum; empty dots are saddle points. The three purple lines denote
possible gradient ﬂows starting from some points x with d(x, B) = δ. The gray disks denote
all possible regions such that (cid:107)g(cid:107) ≤ λmin
2 δ. Thus, the amount of gradient within the set H((cid:15), δ)
is greater or equal to λmin

2 δ.

Lemma 13 (Bounds on gradient ﬂow) Using the notations above and as-
sumption (D), let δ1 be deﬁned in Lemma 11 and δ2 be deﬁned in Lemma 12,
equation (43). Then for all x such that

d(x, B) = δ < δ0 = min

δ1, δ2,

(cid:26)

Hmin
d2 · (cid:107)f (cid:107)3,max

(cid:27)

,

and picking (cid:15) such that δ2 > (cid:15)2 > δ, we have

η(cid:15)(x) ≡ inf

(cid:107)g(πx(t))(cid:107) ≥ δ

0≤t≤t(cid:15)(x)

λmin
2

.

γ(cid:15)(δ) ≡ inf
x∈Bδ

η(cid:15)(x) ≥ δ

λmin
2

,

Moreover,

where Bδ = {x : d(x, B) = δ}.

Proof.
We consider the ﬂow πx starting at x (not on the boundaries) such that

d(x, B) = δ < min {δ1, δ2} .

For 0 ≤ t ≤ t(cid:15)(x), the entire ﬂow is within the set

H((cid:15), δ) = {x : d(x, B) ≥ δ, d(x, M ) ≥

(cid:15)}.

(44)

√

Chen et al./Inference using the Morse-Smale

That is,

{πx(t) : 0 ≤ t ≤ t(cid:15)(x)} ⊂ H((cid:15), δ).

This is because by Lemma 11, the ﬂow line cannot get closer to the boundaries
B within distance δ, and the ﬂow stops when its distance to its destination is
at (cid:15). Thus, if we can prove that every point within H((cid:15), δ) has gradient lowered
bounded by δ λmin
2 , we have completed the proof. That is, we want to show that

33

(45)

(46)

To show the lower bound, we focus on those points whose gradient is small.

Let

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

(cid:26)

S(δ) =

x : (cid:107)g(x)(cid:107) ≤ δ

(cid:27)

.

λmin
2

S(δ) ⊂ C ⊕ δ.

By Lemma 12, the S(δ) are regions around critical points such that

Since we have chosen (cid:15) such that (cid:15) ≥ δ2 and by the fact that critical points
are either in M , the collection of all local modes, or in B the boundaries so that,
the minimal distance between H((cid:15), δ) and critical points C is greater that δ (see
equation (44) for the deﬁnition of H((cid:15), δ)). Thus,

which implies equation (46):

(C ⊕ δ) ∩ H((cid:15), δ) = ∅,

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

Now by the fact that all πx(t) with d(x, B) < δ are within the set H((cid:15), δ)
(equation (45)), we conclude the result.

(cid:3)

Lemma 13 links the constant γ(cid:15)(δ) and the minimal gradient, which can be
used to bound the time t(cid:15)(x) uniformly and further leads to the following result.
Lemma 14 Let K(δ) = {x ∈ K : d(x, B) ≥ δ} = K\(B ⊕δ) and δ0 be deﬁned as
Lemma 13 and M is the collection of all local modes. Assume that f has bounded
third derivative and is a Morse function and that assumption (D) holds. Let (cid:101)f
be another smooth function. There exists constants c∗, c0, c1, (cid:15)0 that all depend
only on f such that when ((cid:15), δ) satisfy the following condition

δ < (cid:15) < (cid:15)0,

δ < min{δ0, Haus(K(δ), B(M,

(cid:15)))}

√

and if

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤ c0

(cid:107)f − (cid:101)f (cid:107)1,max ≤ c1 exp

−

(cid:32)

√
4

d(cid:107)f (cid:107)2,max(cid:107)f (cid:107)∞

(cid:33)

,

δ2λ2

min

(47)

(48)

Chen et al./Inference using the Morse-Smale

34

Fig 13. Result from Lemma 13: lower bound on minimal gradient. This plot shows possible
values for minimal gradient η(cid:15)(x) (pink regions) when d(x, B) is known. Note that we have
chosen (cid:15)2 < δ2.

then for all x ∈ K(δ)

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞.

(49)

(cid:113)

Note that condition (47) holds when ((cid:15), δ) are suﬃciently small.

Proof. The proof of this lemma is closely related to the proof of Theorem
2 of Arias-Castro et al. (2016). The results in Arias-Castro et al. (2016) is a
pointwise convergence of gradient ﬂows; now we will generalize their ﬁndings to
the uniform convergence.

Note that K(δ) = H((cid:15), δ) ∪ B(x,

(cid:15)). For x ∈ B(x,

(cid:15)), the result is trivial

√

√

when (cid:15) is suﬃciently small. Thus, we assume x ∈ H((cid:15), δ).

From equation (40–44) in Arias-Castro et al. (2016) (proof to their Theorem

2),

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107)
(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

2
λmin

2λmin(cid:15) +

√

(cid:107)f − (cid:101)f (cid:107)1,maxe

d(cid:107)f (cid:107)2,maxt(cid:15)(x) + 2(cid:107)f − (cid:101)f (cid:107)∞

√

(cid:107)f (cid:107)1,max
d(cid:107)f (cid:107)2,max

(cid:33)

(50)

under condition (48) and (cid:15) < (cid:15)0 for some constant (cid:15)0.

Thus, the key is to bound t(cid:15)(x). Recall that x ∈ H((cid:15), δ). Now consider the

Chen et al./Inference using the Morse-Smale

35

gradient ﬂow πx and deﬁne z = πx(t(cid:15)(x)).

f (z) − f (x) =

∂f (πx(s))
∂s

ds =

0

(cid:90) t(cid:15)(x)

g(πx(s))T π(cid:48)

x(s)ds

(cid:107)g(πx(s))(cid:107)2ds ≥ γ(cid:15)(δ)2t(cid:15)(x).

(cid:90) t(cid:15)(x)

(cid:90) t(cid:15)(x)

=

0

0

Since f (z) − f (x) ≤ 2(cid:107)f (cid:107)∞, we have

(cid:107)f (cid:107)∞ ≥

γ(cid:15)(δ)2t(cid:15)(x)

1
2

t(cid:15)(x) ≤

2(cid:107)f (cid:107)∞
γ(cid:15)(δ)2

≤

8(cid:107)f (cid:107)∞
δ2λ2

min

and by Lemma 13,

for all x ∈ H((cid:15), δ).

Now plug-in (52) into (50), we have

(51)

(52)

(cid:114)

(cid:107) lim
t→∞

πx(t)− lim

t→∞ (cid:101)πx(t)(cid:107) ≤

a0(cid:15) + a1(cid:107)f − (cid:101)f (cid:107)1,maxe

(53)
for some constants a0, a1, a2. Now using condition (48) to replace the second
term of right hand side, we conclude

√

d(cid:107)f (cid:107)2,max

8(cid:107)f (cid:107)∞
δ2λ2

min + a2(cid:107)f − (cid:101)f (cid:107)∞

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max

By Lemma 7 in Arias-Castro et al. (2016), there exists some constant c3 such

for some constant a3.

that when a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max < 1/c3,

(cid:113)

(cid:113)

√

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤

2c3(cid:107)f − (cid:101)f (cid:107).

Thus, when both (cid:15) and (cid:107)f − (cid:101)f ∗
constant c∗ such that

3,max(cid:107) are suﬃciently small, there exists some

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗(cid:107)f − (cid:101)f (cid:107)

for all x ∈ H((cid:15), δ).

(cid:3)

Now we turn to the proof of Theorem 1.

we show that when (cid:107)f − (cid:101)f (cid:107)∗

Proof of Theorem 1. The proof contains two parts. In the ﬁrst part,
3,max is suﬃciently small, we have Haus(B, (cid:101)B) <
, where B and (cid:101)B are the boundary of descending d-manifolds for f

Hmin
d2(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

36

Hmin
d2(cid:107)f (cid:107)3,max

and (cid:101)f . The second part of the proof is to derive the convergence rate. Because
Haus(B, (cid:101)B) <
, we can apply the second assertion of Lemma 10 to
derive the rate of convergence. Note that C and (cid:101)C are the critical points for f
and (cid:101)f and M ≡ C0, (cid:102)M ≡ (cid:101)C0 are the local modes for f and (cid:101)f .
Hmin
d2·(cid:107)f (cid:107)3,max

, the upper bound for Hausdorﬀ dis-
tance. Let σ = min{(cid:107)x − y(cid:107) : x, y ∈ M, x (cid:54)= y}. That is, σ is the smallest dis-
tance between any pair of distinct local modes. By Lemma 9, when (cid:107)f − (cid:101)f (cid:107)∗
is small, f and (cid:101)f have the same number of critical points and

Part 1: Haus(B, (cid:101)B) <

3,max

Haus(C, (cid:101)C) ≤ A(cid:107)f − (cid:101)f (cid:107)∗

2,max ≤ A(cid:107)f − (cid:101)f (cid:107)∗

3,max,

where A is a constant that depends only on f (actually, we only need (cid:107)f − (cid:101)f (cid:107)∗
to be small here).

2,max

Thus, whenever (cid:107)f − (cid:101)f (cid:107)∗

3,max satisﬁes

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

σ
3A

,

every M has an unique corresponding point in (cid:102)M and vice versa. In addition,
for a pair of local modes (mj, (cid:101)mj) : mj ∈ M, (cid:101)mj ∈ (cid:102)M , their distance is bounded
by (cid:107)mj − (cid:101)mj(cid:107) ≤ σ
3 .

Now we pick ((cid:15), δ) such that they satisfy equation (47). Then when (cid:107)f −
3,max is suﬃciently small, by Lemma 14, for every x ∈ H((cid:15), δ) we have

(cid:101)f (cid:107)∗

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞ ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∗

3,max.

(cid:113)

(cid:113)

Thus, whenever

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

1
c2
∗

(cid:16) σ
3

(cid:17)2

,

πx(t) and (cid:101)πx(t) leads to the same pair of modes. Namely, the boundaries (cid:101)B
will not intersect the region H((cid:15), δ). And it is obvious that (cid:101)B cannot intersect
B(M,

(cid:15)). To conclude,

√

(54)

(55)

(56)

(cid:101)B ∩ H((cid:15), δ) = ∅
√

(cid:101)B ∩ B(M,

(cid:15)) = ∅
⇒ (cid:101)B ∩ K(δ) = ∅,
(cid:15)).

√

because by deﬁnition, K(δ) = H((cid:15), δ) ∩ B(M,

Thus, (cid:101)B ⊂ K(δ)C = B ⊕ δ, which implies Haus(B, (cid:101)B) ≤ δ < Hmin

(note

d2(cid:107)f (cid:107)3,max

that δ < δ0 ≤ Hmin

d2(cid:107)f (cid:107)3,max

appears in equation (47) and Lemma 13).

Part 2: Rate of convergence. To derive the convergence rate, we use proof
by contradiction. Let q ∈ B, (cid:101)q ∈ (cid:101)B a pair of points such that their distance
attains the Hausdorﬀ distance Haus
(cid:101)B, B

(cid:17)

(cid:16)

. Namely, q and (cid:101)q satisfy
(cid:16)

(cid:17)

(cid:107)q − (cid:101)q(cid:107) = Haus

(cid:101)B, B

Chen et al./Inference using the Morse-Smale

37

and either q is the projected point from (cid:101)q onto B or (cid:101)q is the projected point
from q onto (cid:101)B.

Recall that V(x) is the normal space to B at x ∈ B and we deﬁne (cid:101)V(x)
similarly for x ∈ (cid:101)B. An important property of the pair q, (cid:101)q is that q − (cid:101)q ∈
V(q), (cid:101)V((cid:101)q). If this is not true, we can slightly perturb q (or (cid:101)q) on B (or (cid:101)B) to
get a projection distance larger than the Hausdorﬀ distance, which leads to a
contradiction.

Now we choose x to be a point between q, (cid:101)q such that x = 1

3 (cid:101)q. We
(cid:107)(cid:101)q−x(cid:107) . Then e(x) ∈ V(q) and (cid:101)e(x) ∈ (cid:101)V((cid:101)q) and

3 q + 2

(cid:107)q−x(cid:107) and (cid:101)e(x) = (cid:101)q−x

deﬁne e(x) = q−x
e(x) = −(cid:101)e(x).

By Lemma 10 (second assertion),

(57)

(58)

(cid:96)(x) = e(x)T g(x) ≥

Hmin(cid:107)q − x(cid:107) > 0

(cid:101)(cid:96)(x) = (cid:101)e(x)T

(cid:101)g(x) ≥

(cid:101)Hmin(cid:107)(cid:101)q − x(cid:107) > 0.

1
2
1
2

Thus, for every x between q, (cid:101)q,
e(x)T g(x) > 0,

, e(x)T

(cid:101)g(x) = −(cid:101)e(x)T

(cid:101)g(x) < 0.

Note that we can apply Lemma 10 to (cid:101)f and its gradient because when (cid:107)f − (cid:101)f (cid:107)∗
2
is suﬃciently small, the assumption (D) holds for (cid:101)f as well.

To get the upper bound of (cid:107)q−(cid:101)q(cid:107) = Haus( (cid:101)B, B), note that (cid:107)q−x(cid:107) = 2

3 (cid:107)q−(cid:101)q(cid:107),

so

e(x)T

(cid:101)g(x) = e(x)T ((cid:101)g(x) − g(x)) + e(x)T g(x)
≥ e(x)T g(x) − (cid:107) (cid:101)f − f (cid:107)1,max

Hmin(cid:107)q − x(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max

(By Lemma 10)

(59)

≥

=

1
2
1
3

Hmin(cid:107)q − (cid:101)q(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max.

Thus, as long as

we have e(x)T
that

(cid:3)

Haus( (cid:101)B, B) = (cid:107)q − (cid:101)q(cid:107) > 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

,

(cid:101)g(x) > 0, a contradiction to equation (58). Hence, we conclude

Haus( (cid:101)B, B) ≤ 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

(cid:16)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

.

Proof of Theorem 3.
To prove the asymptotic rate of the rand index, we assume that for every local
mode of p, there exists one and only one local mode of (cid:98)pn that is close to the

Chen et al./Inference using the Morse-Smale

38

speciﬁc mode of p. By Lemma 9, this is true when (cid:107)(cid:98)pn − p(cid:107)∗
3,max is suﬃciently
small. Thus, after relabeling, the local mode (cid:98)m(cid:96) of (cid:98)pn is an estimator to the
local mode m(cid:96) of p. Let (cid:99)W(cid:96) be the basin of attraction to (cid:98)m(cid:96) using ∇(cid:98)pn and W(cid:96)
be the basin of attraction to m(cid:96) using ∇p. Let A(cid:52)B = {x : x ∈ A, x /∈ B} ∪ {x :
x ∈ B, x /∈ A} be the symmetric diﬀerence between sets A and B. The regions

En =

(cid:99)W(cid:96)(cid:52)W(cid:96)

(cid:17)

⊂ K

(cid:91)

(cid:16)

(cid:96)

(60)

are where the two mode clustering disagree with each other. Note that En are
regions between the two boundaries (cid:98)Bn and B

Given a pair of points Xi and Xj,

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj) =⇒ Xi or Xj ∈ En.

(61)

By the deﬁnition of rand index (30),

1 − rand ((cid:98)pn, p) =

(cid:19)−1

(cid:88)

(cid:16)

(cid:18)n
2

i,j

1

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj)

(62)

(cid:17)

Thus, if we can bound the ratio of data points within En, we can bound the
rate of rand index.

Since K is compact and p has bounded second derivatives, the volume of En

is bounded by

Vol(En) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Note Vol(A) denotes the volume (Lebesgue measure) of a set A. We now con-
struct a region surrounding B such that

and

En ⊂ B ⊕ Haus( (cid:98)Bn, B) = Vn

Vol(Vn) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Now we consider a collection of subsets of K:

V = {B ⊕ r : R > r > 0},

(cid:80)n

where R < ∞ is the diameter for K. For any set A ⊂ K, let P (Xi ∈ A) and
(cid:98)Pn(A) = 1
i=1 1(Xi ∈ A) denote the probability of an observation within A
n
and the empirical estimate for that probability, respectively. It is easy to see
that Vn ∈ V for all n and the class V has a ﬁnite VC dimension (actually, the
VC dimension is 1). By the empirical process theory (or so-called VC theory,
see e.g. Vapnik and Chervonenkis (1971)),

(cid:12)
(cid:12)
(cid:12)P (Xi ∈ A) − (cid:98)Pn(A)

(cid:12)
(cid:12)
(cid:12) = OP

sup
A∈V

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(63)

(64)

(65)

(66)

(67)

Chen et al./Inference using the Morse-Smale

39

Thus,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)P (Xi ∈ Vn) − (cid:98)Pn(Vn)
(cid:12) = OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(68)

Now by equations (61) and (62),

1 − rand ((cid:98)pn, p) ≤ 8 (cid:98)Pn(En) ≤ 8 (cid:98)Pn(Vn) ≤ 8P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n

. (69)

Therefore,

1 − rand ((cid:98)pn, p) ≤ P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n
(cid:32)(cid:114)

(cid:33)

log(n)
n

(cid:32)(cid:114)

(cid:33)

log(n)
n

p(x) × Vol(Vn) + OP

≤ sup
x∈K

(cid:16)

≤ O

Haus( (cid:98)Bn, B)

+ OP

(cid:17)

= O (cid:0)h2(cid:1) + OP

(cid:32)(cid:114)

(cid:33)

,

log(n)
nhd+2

which completes the proof. Note that we apply Theorem 2 in the last equality.

(cid:3)

Proof of Theorem 4. Let (X1, Y1), · · · , (Xn, Yn) be the observed data.
Let (cid:98)E(cid:96) denote the d-cell for the nonparametric pilot regression estimator (cid:98)mn.
With I(cid:96) = {i : Xi ∈ (cid:98)E(cid:96)}, we deﬁne X(cid:96) as the matrix with rows Xi, i ∈ I(cid:96) and
similarly we deﬁne Y(cid:96).

We deﬁne X0,(cid:96) to be the matrix similar to X(cid:96) except that the row elements
are those Xi within E(cid:96), the d-cell deﬁned on true regression function m. We
also deﬁne Y0,(cid:96) to be the corresponding Yi.

By the theory of linear regression, the estimated parameters (cid:98)µ(cid:96), (cid:98)β(cid:96) have a

closed form solution:

Similarly, we deﬁne

((cid:98)µ(cid:96), (cid:98)β(cid:96))T = (XT

(cid:96)

X(cid:96))−1XT
(cid:96)

Y(cid:96).

((cid:98)µ0,(cid:96), (cid:98)β0,(cid:96))T = (XT

0,(cid:96)

X0,(cid:96))−1XT
0,(cid:96)

Y0,(cid:96)

as the estimated coeﬃcients using X0,(cid:96) and Y0,(cid:96).

As (cid:107) (cid:101)m − m(cid:107)∗

3,max is small, by Theorem 3, the number of rows at which
X(cid:96) and X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max). This is because an

(70)

(71)

(72)

Chen et al./Inference using the Morse-Smale

40

observation (a row vector) that appears only in one of X(cid:96) and X0,(cid:96) is those
fallen within either (cid:98)E(cid:96) or E(cid:96) but not both. Despite the fact that Theorem 3
is for basins of attraction (d-descending manifolds) of local modes, it can be
easily generalized to 0-ascending manifolds of local minima under assumption
(A). Thus, the similar bound holds for d-cells as well. Thus, we conclude that
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
since (X(cid:96), Y(cid:96)) and (X0,(cid:96), Y0,(cid:96)) only diﬀer by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max) elements.
Thus,

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

1
n
1
n

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96)

X0,(cid:96)

(73)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)µ0,(cid:96) − (cid:98)µ(cid:96), (cid:98)β0,(cid:96) − (cid:98)β(cid:96))
(cid:13)∞

=

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
(cid:13)
n
(cid:13)

XT
0,(cid:96)

X0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96) −

XT
(cid:96)

X(cid:96)

(cid:19)−1 1
n

(cid:18) 1
n

(cid:19)−1 1
n

XT
(cid:96)

Y(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= O((cid:107) (cid:98)mn − m(cid:107)1,max),

which implies.

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − (cid:98)µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − (cid:98)β(cid:96)(cid:107)

= O((cid:107) (cid:98)mn − m(cid:107)1,max).

Now by the theory of linear regression,

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − β(cid:96)(cid:107)

= OP

(cid:18) 1
√
n

(cid:19)

.

(74)

(75)

(76)

Thus, combining (75) and (76) and use the fact that all the above bounds are
uniform over each cell, we have proved that the parameters converge at rate
O((cid:107) (cid:98)mn − m(cid:107)1,max) + OP

(cid:16) 1√

(cid:17)

n

.

For points within the regions where E(cid:96) and (cid:98)E(cid:96) agree with each other, the rate
of convergence for parameter estimation translates into the rate of (cid:98)mn,MSR −
mMSR. The regions that E(cid:96) and (cid:98)E(cid:96) disagree to each other, denoted as Nn, have
Lebesgue O((cid:107) (cid:98)mn − m(cid:107)1,max) by Theorem 1. Thus, we have completed the proof.

(cid:3)

Proof of Theorem 5. The proof of Theorem 5 is nearly identical to the
proof of Theorem 4. The only diﬀerence is that the number of rows that X(cid:96) and
X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)β
1,max) due to the low noise condition
(36). Thus, equation (73) becomes
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O((cid:107) (cid:98)mn − m(cid:107)β

= O((cid:107) (cid:98)mn − m(cid:107)β

1
n
1
n

1
n
1
n

1,max)

1,max)

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(77)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Chen et al./Inference using the Morse-Smale

41

so the parameter estimation error (76) is O((cid:107) (cid:98)mn − m(cid:107)β

1,max) + OP

(cid:16) 1√

(cid:17)

.

n

Under assumption (K1–2) and using Theorem 8 (the same result works for

kernel regression),

O((cid:107) (cid:98)mn − m(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

Thus, with the choice that h = O
(cid:17)2/(d+6)(cid:19)

(cid:18)(cid:16) log n

OP

n

(cid:3)

, which proves equation (37).

(cid:18)(cid:16) log n

(cid:17)1/(d+6)(cid:19)

n

, we have O((cid:107) (cid:98)mn−m(cid:107)1,max) =

Proof of Theorem 6.
We ﬁrst derive the explicit form of the parameters (η†

Note that the parameters are obtained by (14):

(cid:96) , γ†

(cid:96) ) within cell E(cid:96).

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:90)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Now we deﬁne a random variable U(cid:96) ∈ Rd that is uniformly distributed over E(cid:96).
Then equation (14) is equivalent to

(η†

(cid:96) , γ†

(cid:96) ) = argmin

E

η,γ

(cid:16)(cid:0)f (U(cid:96)) − η − γT U(cid:96)

(cid:1)2(cid:17)

.

The analytical solution to the above problem is

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:19)

(cid:18)

=

1

E(U(cid:96))T
E(U(cid:96)) E(U(cid:96)U T
(cid:96) )

(cid:19)−1 (cid:18) E(f (U(cid:96)))
E(U(cid:96)f (U(cid:96)))

(cid:19)

Now we consider another smooth function (cid:101)f that is close to f such that
(cid:107) (cid:101)f − f (cid:107)∗
3,max is small so we can apply Theorem 1 to obtain consistency for both
descending d-manifolds and ascending 0-manifolds. Note that by Lemma 9, all
the critical points are close to each other and after relabeling, each d-cell E(cid:96) of
f is estimated by another d-cell (cid:101)E(cid:96) of (cid:101)f . Theorem 1 further implies that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Leb( (cid:101)E(cid:96)) − Leb(E(cid:96))
(cid:12) = O
(cid:17)
(cid:16)

(cid:16)

(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

(cid:17)

Leb

(cid:101)E(cid:96)(cid:52)E(cid:96)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

,

(78)

(79)

(80)

where Leb(A) is the Lebesgue measure for set A and A(cid:52)B = (A\B) ∪ (B\A) is

Chen et al./Inference using the Morse-Smale

42

the symmetric diﬀerence. By simple algebra, equation (80) implies that

(cid:107)E( (cid:101)U(cid:96)) − E(U(cid:96))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)U T

(cid:96) ) − E(U(cid:96)U T

|E( (cid:101)f ( (cid:101)U(cid:96))) − E(f (U(cid:96)))| = O

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:96) )(cid:107)∞ = O
(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max
(cid:17)

(cid:107) (cid:101)f − f (cid:107)∗
(cid:16)

1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)f ( (cid:101)U(cid:96))) − E(U(cid:96)f (U(cid:96)))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

(81)

By (81) and the analytic solution to ((cid:101)η†
(cid:101)η†
(cid:101)γ†

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:19)

(cid:18)

(cid:96)

(cid:96)

(cid:96) , (cid:101)γ†
(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

(cid:17)

.

1,max

(cid:96) ) from (79), we have proved

(82)

Since the bound does not depend on the cell indices (cid:96), (82) holds uniformly for
all (cid:96) = 1, · · · , K.

(cid:3)

References

E. Arias-Castro, D. Mason, and B. Pelletier. On the estimation of the gradient
lines of a density and the consistency of the mean-shift algorithm. Journal of
Machine Learning Research, 17(43):1–28, 2016.

J.-Y. Audibert, A. B. Tsybakov, et al. Fast learning rates for plug-in classiﬁers.

The Annals of statistics, 35(2):608–633, 2007.

M. Azizyan, Y.-C. Chen, A. Singh, and L. Wasserman. Risk bounds for mode

clustering. arXiv preprint arXiv:1505.00482, 2015.

A. Azzalini and N. Torelli. Clustering via nonparametric density estimation.

Statistics and Computing, 17(1):71–80, 2007.

P. Bacchetti. Additive isotonic models. Journal of the American Statistical

A. Banyaga and D. Hurtubise. Lectures on Morse homology, volume 29. Springer

Association, 84(405):289–294, 1989.

Science & Business Media, 2004.

L. Baringhaus and C. Franz. On a new multivariate two-sample test. Journal

of multivariate analysis, 88(1):190–206, 2004.

R. E. Barlow, D. J. Bartholomew, J. Bremner, and H. D. Brunk. Statistical
inference under order restrictions: the theory and application of isotonic re-
gression. Wiley New York, 1972.

R. Bhatia. Matrix Analysis. Springer, 1997.
G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business

Media, 1993.

R. R. Brinkman, M. Gasparetto, S.-J. J. Lee, A. J. Ribickas, J. Perkins,
W. Janssen, R. Smiley, and C. Smith. High-content ﬂow cytometry and
temporal data analysis for deﬁning a cellular signature of graft-versus-host
disease. Biology of Blood and Marrow Transplantation, 13(6):691–700, 2007.

Chen et al./Inference using the Morse-Smale

43

J. Chac´on and T. Duong. Data-driven density derivative estimation, with ap-
plications to nonparametric clustering and bump hunting. Electronic Journal
of Statistics, 7:499–532, 2013.

J. Chac´on, T. Duong, and M. Wand. Asymptotics for general multivariate kernel

density derivative estimators. Statistica Sinica, 2011.

J. E. Chac´on et al. A population background for nonparametric density-based

clustering. Statistical Science, 30(4):518–532, 2015.

F. Chazal, B. T. Fasy, F. Lecci, B. Michel, A. Rinaldo, and L. Wasserman. Ro-
bust topological inference: Distance to a measure and kernel distance. arXiv
preprint arXiv:1412.7197, 2014.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. Asymptotic theory for density

ridges. The Annals of Statistics, 43(5):1896–1928, 2015.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. A comprehensive approach
to mode clustering. Electronic Journal of Statistics, 10(1):210–241, 2016.
Y. Cheng. Mean shift, mode seeking, and clustering. Pattern Analysis and

Machine Intelligence, IEEE Transactions on, 17(8):790–799, 1995.

D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence dia-

grams. Discrete & Computational Geometry, 37(1):103–120, 2007.

D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space
analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
24(5):603–619, 2002.

T. Duong. Local signiﬁcant diﬀerences from nonparametric two-sample tests.

Journal of Nonparametric Statistics, 25(3):635–645, 2013.

T. Duong et al. ks: Kernel density estimation and kernel discriminant analysis
for multivariate data in r. Journal of Statistical Software, 21(7):1–16, 2007.
B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statis-

tics, 7(1):1–26, 1979.

U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type

function estimators. The Annals of Statistics, 2005.

K. Fukunaga and L. Hostetler. The estimation of the gradient of a density
function, with applications in pattern recognition. Information Theory, IEEE
Transactions on, 21(1):32–40, 1975.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, and L. Wasserman. The
geometry of nonparametric ﬁlament estimation. Journal of the American
Statistical Association, 107(498):788–799, 2012.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, L. Wasserman, et al. Non-
parametric ridge estimation. The Annals of Statistics, 42(4):1511–1545, 2014.
S. Gerber and K. Potter. Data analysis with the morse-smale complex: The msr

package for r. Journal of Statistical Software, 2011.

S. Gerber, P.-T. Bremer, V. Pascucci, and R. Whitaker. Visual exploration
of high dimensional scalar functions. Visualization and Computer Graphics,
IEEE Transactions on, 16(6):1271–1280, 2010.

S. Gerber, O. R¨ubel, P.-T. Bremer, V. Pascucci, and R. T. Whitaker. Morse–
smale regression. Journal of Computational and Graphical Statistics, 22(1):
193–214, 2013.

E. Gine and A. Guillou. Rates of strong uniform consistency for multivari-

Chen et al./Inference using the Morse-Smale

44

ate kernel density estimators. In Annales de l’Institut Henri Poincare (B)
Probability and Statistics, 2002.

S. Helgason. Diﬀerential geometry, Lie groups, and symmetric spaces, vol-

ume 80. Academic press, 1979.

L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation, 2(1):

193–218, 1985.

J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a

nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.

J. Li, S. Ray, and B. G. Lindsay. A nonparametric statistical approach to
clustering via mode identiﬁcation. Journal of Machine Learning Research,
2007.

J. W. Milnor. Morse theory. Number 51. Princeton university press, 1963.
M. Morse. Relations between the critical points of a real function of n indepen-
dent variables. Transactions of the American Mathematical Society, 27(3):
345–396, 1925.

M. Morse. The foundations of a theory of the calculus of variations in the
large in m-space (second paper). Transactions of the American Mathematical
Society, 32(4):599–631, 1930.

E. A. Nadaraya. On estimating regression. Theory of Probability & Its Appli-

cations, 9(1):141–142, 1964.

S. Paris and F. Durand. A topological approach to hierarchical segmentation us-
ing mean shift. In Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007.

W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal

of the American Statistical association, 66(336):846–850, 1971.

A. Rinaldo, L. Wasserman, et al. Generalized density clustering. The Annals of

Statistics, 38(5):2678–2722, 2010.

A. Rinaldo, A. Singh, R. Nugent, and L. Wasserman. Stability of density-based
clustering. The Journal of Machine Learning Research, 13(1):905–948, 2012.
M. Rizzo and G. Szekely. energy: E-statistics (energy statistics). R package

version, 1:1, 2008.

M. L. Rizzo, G. J. Sz´ekely, et al. Disco analysis: A nonparametric extension of
analysis of variance. The Annals of Applied Statistics, 4(2):1034–1055, 2010.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chap-

man and Hall, 1986.

A. Singh, C. Scott, R. Nowak, et al. Adaptive hausdorﬀ estimation of density

level sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

G. J. Sz´ekely and M. L. Rizzo. Testing for equal distributions in high dimension.

InterStat, 5, 2004.

G. J. Szekely and M. L. Rizzo. Hierarchical clustering via joint between-within
distances: Extending ward’s minimum variance method. Journal of classiﬁ-
cation, 22(2):151–183, 2005.

G. J. Sz´ekely and M. L. Rizzo. A new test for multivariate normality. Journal

of Multivariate Analysis, 93(1):58–80, 2005.

G. J. Sz´ekely and M. L. Rizzo. Energy statistics: A class of statistics based
on distances. Journal of statistical planning and inference, 143(8):1249–1272,

Chen et al./Inference using the Morse-Smale

45

2013.

V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of rela-
tive frequencies of events to their probabilities. Theory of Probability & Its
Applications, 16(2):264–280, 1971.

A. Vedaldi and S. Soatto. Quick shift and kernel methods for mode seeking. In
European Conference on Computer Vision, pages 705–718. Springer, 2008.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clus-
terings comparison: is a correction for chance necessary? In Proceedings of
the 26th Annual International Conference on Machine Learning, pages 1073–
1080. ACM, 2009.

L. Wasserman. All of nonparametric statistics. Springer, 2006.

7
1
0
2
 
r
p
A
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
2
v
6
2
8
8
0
.
6
0
5
1
:
v
i
X
r
a

Electronic Journal of Statistics
ISSN: 1935-7524
arXiv: http://arxiv.org/abs/1506.08826

Statistical Inference Using the
Morse-Smale Complex

Yen-Chi Chen, and Christopher R. Genovese, and Larry Wasserman

University of Washington,
Department of Statistics
Box 354322,
Seattle, WA 98195
e-mail: yenchic@uw.edu

Carnegie Mellon University,
Department of Statistics
5000 Forbes Avenue,
Pittsburgh, PA 15213
e-mail: genovese@stat.cmu.edu; larry@stat.cmu.edu

Abstract: The Morse-Smale complex of a function f decomposes the sam-
ple space into cells where f is increasing or decreasing. When applied to
nonparametric density estimation and regression, it provides a way to rep-
resent, visualize, and compare multivariate functions. In this paper, we
present some statistical results on estimating Morse-Smale complexes. This
allows us to derive new results for two existing methods: mode clustering
and Morse-Smale regression. We also develop two new methods based on
the Morse-Smale complex: a visualization technique for multivariate func-
tions and a two-sample, multivariate hypothesis test.

MSC 2010 subject classiﬁcations: Primary 62G20; secondary 62G86,
62H30.
Keywords and phrases: nonparametric estimation, mode clustering, non-
parametric regression, two sample test, visualization.

1. Introduction

Let f be a smooth, real-valued function deﬁned on a compact set K ∈ Rd. In
this paper, f will be a regression function or a density function. The Morse-
Smale complex of f is a partition of K based on the gradient ﬂow induced by f .
Roughly speaking, the complex consists of sets, called crystals or cells, comprised
of regions where f is increasing or decreasing. Figure 1 shows the Morse-Smale
complex for a two-dimensional function. The cells are the intersections of the
basins of attractions (under the gradient ﬂow) of the function’s maxima and
minima. The function f is piecewise monotonic over cells with respect to some
directions. In a sense, the Morse-Smale complex provides a generalization of
isotonic regression.

Because the Morse-Smale complex represents a multivariate function in terms
of regions on which the function has simple behavior, the Morse-Smale complex
has useful applications in statistics, including in clustering, regression, testing,
and visualization. For instance, when f is a density function, the basins of at-
traction of f ’s modes are the (population) clusters for density-mode clustering

1

Chen et al./Inference using the Morse-Smale

2

(a) Descending manifold

(b) Ascending manifold

(c) d-cell

(d) Morse-Smale complex

Fig 1. An example of a Morse-Smale complex. The green dots are local minima; the blue
dots are local modes; the violet dots are saddle points. Panels (a) and (b) give examples of
descending d-manifolds (blue region) and an ascending 0-manifold (green region). Panel (c)
shows the corresponding d-cell (yellow region). Panel (d) is shows all d-cells.

(also known as mean shift clustering (Fukunaga and Hostetler, 1975; Chac´on
et al., 2015)), each of which is a union of cells from the Morse-Smale complex.
Similarly, when f is a regression function, the cells of the Morse-Smale complex
give regions on which f has simple behavior. Fitting f over the Morse-Smale
cells provides a generalization of nonparametric, isotone regression; Gerber et al.
(2013) proposes such a method. The Morse-Smale representation of a multivari-
ate function f is a useful tool for visualizing f ’s structure, as shown by Gerber
et al. (2010). In addition, suppose we want to compare two multi-dimensional
datasets X = (X1, . . . , Xn) and Y = (Y1, . . . , Ym). We start by forming the
Morse-Smale complex of (cid:98)p − (cid:98)q where (cid:98)p is density estimate from X and (cid:98)q is den-
sity estimate from Y . Figure 2 shows a visualization built from this complex.
The circles represent cells of the Morse-Smale complex. Attached to each cell is
a pie-chart showing what fraction of the cell has (cid:98)p signiﬁcantly larger than (cid:98)q.
This visualization is a multi-dimensional extension of the method proposed for
two or three dimensions in Duong (2013).

For all these applications, the Morse-Smale complex needs to be estimated.
To the best of our knowledge, no theory has been developed for this estimation
problem, prior to this paper. We have three goals in this paper: to show that
many existing problems can be cast in terms of the Morse-Smale complex, to

Chen et al./Inference using the Morse-Smale

3

Fig 2. Graft-versus-Host Disease (GvHD) dataset (Brinkman et al., 2007). This is a d = 4
dimensional dataset. We estimate the density diﬀerence based on the kernel density estimator
and ﬁnd regions where the two densities are signiﬁcantly diﬀerent. Then we visualize the
density diﬀerence using the Morse-Smale complex. Each green circle denotes a d-cell, which
is a partition for the support K. The size of circle is proportional to the size of cell. If two
cells are neighborhors, we add a line connecting them; the thickness of the line denotes the
amount of boundary they share. The pie charts show the ratio of the regions within each cell
where the two densities are signiﬁcantly diﬀerent from each other. See Section 3.4 for more
details.

develop some new statistical methods based on the Morse-Smale complex, and
to develop the statistical theory for estimating the complex.

Main results. The main results of this paper are:

1. Consistency of the Morse-Smale Complex. We prove the stability of the
Morse-Smale complex (Theorem 1) in the following sense: if B and (cid:101)B are
boundaries of the descending d-manifolds (or ascending 0-manifolds) of p
and (cid:101)p (deﬁned in Section 2), then

Haus(B, (cid:101)B) = O ((cid:107)∇p − ∇(cid:101)p(cid:107)∞) .
2. Risk Bound for Mode clustering (mean-shift clustering; section 3.1): We

bound the risk of mode clustering in Theorem 2.

3. Morse-Smale regression (section 3.2): In Theorems 4 and 5, we bound the
risk of Morse-Smale regression, a multivariate regression method proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013)
that synthesizes nonparametric regression and linear regression.

4. Morse-Smale signatures (section 3.3): We introduce a new visualization

method for densities and regression functions.

5. Morse-Smale two-sample testing (section 3.4): We develop a new method

for multivariate two-sample testing that can have good power.

Related work. The mathematical foundations for the Morse-Smale complex

Chen et al./Inference using the Morse-Smale

4

Fig 3. A one dimensional example. The blue dots are local modes and the green dots are
local minima. Left panel: the basins of attraction for two local modes are colored by brown
and orange. Middle panel: the basin of attraction (negative gradient) for the local minima are
colored by red, purple and violet. Right panel: The intersection of the basins, which are called
d-cells.

are from Morse theory (Morse, 1925, 1930; Milnor, 1963). Morse theory has
many applications including computer vision (Paris and Durand, 2007), com-
putational geometry (Cohen-Steiner et al., 2007) and topological data analysis
(Chazal et al., 2014).

Previous work on the stability of the Morse-Smale complex can be found in
Chen et al. (2016) and Chazal et al. (2014) but they only consider critical points
rather than the whole Morse-Smale complex. Arias-Castro et al. (2016) prove
pointwise convergence for the gradient ascent curves but this is not suﬃcient
for proving the stability of the complex because the convergence of complexes
requires convergence of multiple curves and the constants in the convergence
rate derived from Arias-Castro et al. (2016) vary from points to points and some
constants diverge when we are getting closer to the boundaries of complexes.
Thus, we cannot obtain a uniform convergence of gradient ascent curves directly
based on their results. Morse-Smale regression and visualization were proposed
in Gerber et al. (2010); Gerber and Potter (2011); Gerber et al. (2013).

The R code (Algorithm 1, 2, and 3) used in this paper can be found at

https://github.com/yenchic/Morse_Smale.

2. Morse Theory

To motivate formal deﬁnitions, we start with the simple, one-dimensional ex-
ample depicted in Figure 3. The left panel shows the sets associated with each
local maximum (i.e. the basins of attraction of the maxima). The middle panel
shows the sets associated with each local minimum. The right panel show the
intersections of these basins, which gives the Morse-Smale complex deﬁned by
the function. Each interval in the complex, called a cell, is a region where the
function is increasing or decreasing.

Now we give a formal deﬁnition. Let f : K ⊂ Rd (cid:55)→ R be a function with
bounded third derivatives that is deﬁned on a compact set K. Let g(x) = ∇f (x)
and H(x) = ∇∇f (x) be the gradient and Hessian matrix of f , respectively, and
let λj(x) be the jth largest eigenvalue of H(x). Deﬁne C = {x ∈ K : g(x) = 0}
to be the set of all f ’s critical points, which we call the critical set. Using the

Chen et al./Inference using the Morse-Smale

5

signs of the eigenvalues of the Hessian, the critical set C can be partitioned into
d + 1 distinct subsets C0, · · · , Cd, where

Ck = {x ∈ K : g(x) = 0, λk(x) > 0, λk+1(x) < 0},

k = 1, · · · , d − 1.

(1)

We deﬁne C0, Cd to be the sets of all local maxima and minima (corresponding
to all eigenvalues being negative and positive respectively). The set Ck is called
k−th order critical set.

A smooth function f is called a Morse function (Morse, 1925; Milnor, 1963)
if its Hessian matrix is non-degenerate at each critical point. That is, |λj(x)| >
0, ∀x ∈ C for all j. In what follows we assume f is a Morse function (actually,
later we will assume further that f is a Morse-Smale function).

Given any point x ∈ K, we deﬁne the gradient ascent ﬂow starting at x,

πx : R+ (cid:55)→ K, by

πx(0) = x
π(cid:48)
x(t) = g(π(t)).

(2)

A particle on this ﬂow moves along the gradient from x towards a “destination”
given by

dest(x) ≡ lim
t→∞
It can be shown that dest(x) ∈ C for x ∈ K.

πx(t).

We can thus partition K based on the value of dest(x). These partitions are
called descending manifolds in Morse theory (Morse, 1925; Milnor, 1963). Recall
Ck is the k-th order critical points, we assume Ck = {ck,1, · · · , ck,mk } contains
mk distinct elements. For each k, deﬁne

Dk = {x : dest(x) ∈ Cd−k}
Dk,j = {x : dest(x) = cd−k,j} ,

j = 1, · · · md−k.

(3)

That is, Dk is the collection of all points whose gradient ascent ﬂow converges to
a (d−k)-th order critical point and Dk,j is the collection of points whose gradient
ascent ﬂow converges to the j-th element of Cd−k. Thus, Dk = (cid:83)md−k
j=1 Dk,j. From
Theorem 4.2 in Banyaga and Hurtubise (2004), each Dk is a disjoint union
of k-dimensional manifolds (Dk,j is a k-dimensional manifold). We call Dk,j
a descending k-manifold of f . Each descending k-manifold is a k-dimensional
manifold such that the gradient ﬂow from every point converges to the same
(d − k)-th order critical point. Note that {D0, · · · , Dk} forms a partition of K.
The top panels of Figure 4 give an example of the descending manifolds for a
two dimensional case.

The ascending manifolds are similar to descending manifolds but are deﬁned
through the gradient descent ﬂow. More precisely, given any x ∈ K, a gradient
descent ﬂow γ : R+ (cid:55)→ K starting from x is given by

γx(0) = x
γ(cid:48)
x(t) = −g(π(t)).

(4)

Chen et al./Inference using the Morse-Smale

6

(a)

(b)

(c)

(d)

Fig 4. Two-dimensional examples of critical points, descending manifolds, ascending mani-
folds, and 2-cells. This is the same function as Figure 1. (a): The set Ck for k = 0, 1, 2. The
four blue dots are C0, the collection of local modes (each of them is c0,j some j = 1, · · · , 4).
The four orange dots are C1, the collection of saddle points (each of them is c1,j for some
j = 1, · · · , 4). The green dots are C2, the collection of local minima (each green dot is c2,j
for some j = 1, · · · , 9). (b): The set Dk for k = 0, 1, 2. The yellow area is D2 (each subregion
separated by blue curves are D2,j , j = 1, · · · , 4). The two blue curves are D1 (each of the 4
blue segments are D1,j , j = 1, · · · , 4). The green dots are D0 (also C2), the collection of local
minima (each green dot is D0,j for some j = 1, · · · , 9). (b): The set Ak for k = 0, 1, 2. The
yellow area is A0 (each subregion separated by red curves are A0,j , j = 1, · · · , 9). The two
red curves are A1 (each of the 4 red segments are A1,j , j = 1, · · · , 4). The blue dots are A2
(also C0), the collection of local modes (each green dot is A0,j for some j = 1, · · · , 4). (d):
Example for 2-cells. The thick blue curves are D1 and thick red curves are A1.

Chen et al./Inference using the Morse-Smale

7

Unlike the ascending ﬂow deﬁned in (2), γx is a ﬂow that moves along the
gradient descent direction. The descent ﬂow γx shares similar properties to the
ascent ﬂow πx; the limiting point limt→∞ γx(t) ∈ C is also in critical set when
f is a Morse function. Thus, similarly to Dk and Dk,j, we deﬁne

Ak =

x : lim
t→∞

γx(t) ∈ Cd−k

(cid:110)

(cid:110)

(cid:111)

(cid:111)

Ak,j =

x : lim
t→∞

γx(t) = cd−k,j

,

j = 1, · · · , mj−k.

(5)

Ak and Ak,j have dimension d − k and each Ak,j is a partition for Ak and
{A0, · · · , Ad} consist of a partition for K. We call each Ak,j an ascending k-
manifold to f .

A smooth function f is called a Morse-Smale function if it is a Morse function
and any pair of the ascending and descending manifolds of f intersect each
other transversely (which means that pairs of manifolds are not parallel at their
intersections); see e.g. Banyaga and Hurtubise (2004) for more details. In this
paper, we also assume that f is a Morse-Smale function. Note that by the
Kupka-Smale Theorem (see e.g. Theorem 6.6 in Banyaga and Hurtubise (2004)),
Morse-Smale functions are generic (dense) in the collection of smooth functions.
For more details, we refer to Section 6.1 in Banyaga and Hurtubise (2004).

A k-cell (also called Morse-Smale cell or crystal) is the non-empty intersection
between any descending k1-manifold and an ascending (d − k2)-manifold such
that k = min{k1, k2} (the ascending (d − k2)-manifold has dimension k2). When
we simply say a cell, we are referring to the d-cell since d-cells consists of the
majority of K (the totality of k-cells with k < d has Lebesgue measure 0). The
Morse-Smale complex for f is the collection of all k-cells for k = 0, · · · , d. The
bottom panels of Figure 4 give examples for the ascending manifolds and the
d-cells for d = 2. Another example is given in Figure 1.

The cells of a smooth function can be used to construct an additive de-
composition that is useful in data analysis. For a Morse-Smale function f , let
E1, · · · , EL be its associated cells. Then we can decompose f into

f (x) =

f(cid:96)(x)1(x ∈ E(cid:96)),

(6)

L
(cid:88)

(cid:96)=1

where each f(cid:96)(x) behaves like a multivariate isotonic function (Barlow et al.,
1972; Bacchetti, 1989). Namely, f (x) = f(cid:96)(x) when x ∈ E(cid:96). This decomposition
is because within each E(cid:96), f has exact a local mode and a local minimum on
the boundary of E(cid:96). The fact that f admits such a decomposition will be used
frequently in Section 3.2 and 3.3.

Among all descending/ascending manifolds, the descending d-manifolds and
the ascending 0-manifolds are often of great interest. For instance, mode cluster-
ing (Li et al., 2007; Azzalini and Torelli, 2007) uses the descending d-manifolds
to partition the domain K into clusters. Morse-Smale regression (Gerber and
Potter, 2011; Gerber et al., 2013) ﬁts a linear regression individually over each
d-cell (non-empty intersection of pairs of descending d-manifolds and ascending

Chen et al./Inference using the Morse-Smale

8

(a) Basins of attraction

(b) Gradient ascent

(c) Mode clustering

Fig 5. An example of mode clustering. (a): Basin of attraction for each local mode (red +).
Black dots are data points. (b): Gradient ﬂow (blue lines) for each data point. The gradient
ﬂow starts at one data point and ends at one local modes. (c): Mode clustering; we use the
destination for gradient ﬂow to cluster data points.

0-manifolds). Regions outside descending d-manifolds or ascending 0-manifolds
have Lebesgue measure 0. Thus, later in our theoretical analysis, we will focus
on the stability of the set Dd and A0 (see Section 4.1). We deﬁne boundaries of
Dd as

B ≡ ∂Dd = Dd−1 ∪ · · · ∪ D0.

(7)

The set B will be used frequently in Section 4.

3. Applications in Statistics

3.1. Mode Clustering

Mode clustering (Li et al., 2007; Azzalini and Torelli, 2007; Chac´on and Duong,
2013; Arias-Castro et al., 2016; Chac´on et al., 2015; Chen et al., 2016) is a
clustering technique based on the Morse-Smale complex and is also known as
mean-shift clustering (Fukunaga and Hostetler, 1975; Cheng, 1995; Comaniciu
and Meer, 2002). Mode clustering uses the descending d-manifolds of the density
function p to partition the whole space K. (Although the d-manifolds do not
contain all points in K, the regions outside d-manifolds have Lebesgue measure
0). See Figure 5 for an example.

Now, we brieﬂy describe the procedure of mode clustering. Let X = {X1, · · · , Xn}

be a random sample from density p deﬁned on a compact set K and assumed to
be a Morse function. Recall that dest(x) is the destination of the gradient ascent
ﬂow starting from x. Mode clustering partitions the sample based on dest(x)
for each point; speciﬁcally, it partitions X = X1

(cid:83) · · · (cid:83) XK such that

X(cid:96) = {Xi ∈ X : dest(Xi) = m(cid:96)},

where each m(cid:96) is a local mode of p. We can also view mode clustering as a cluster-
(cid:83) · · · (cid:83) Dd,L
ing technique based on the d-descending manifolds. Let Dd = Dd,1

Chen et al./Inference using the Morse-Smale

9

be the d-descending manifolds of p, assuming that L is the number of local
modes. Then each cluster X(cid:96) = X (cid:84) Dd,(cid:96).

In practice, however, we do not know p so we have to use a density estimator

(cid:98)pn. A common density estimator is the kernel density estimator (KDE):

(cid:98)pn(x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(8)

where K is a smooth kernel function and h > 0 is the smoothing parameter.
Note that mode clustering is not limited to the KDE; other density estimators
also give us a sample-based mode clustering. Based on the KDE, we are able to
estimate gradient (cid:98)gn(x), the gradient ﬂows (cid:98)πx(t), and the destination (cid:100)destn(x)
(note that the mean shift algorithm is an algorithm to perform these tasks).
Thus, we can estimate the d-descending manifolds by the plug-in from (cid:98)pn. Let
(cid:83) · · · (cid:83) (cid:98)Dd,(cid:98)L be the d-descending manifolds of (cid:98)pn, where (cid:98)L is the
(cid:98)Dd = (cid:98)Dd,1
number of local modes of (cid:98)pn. The estimated clusters will be (cid:98)X1, · · · , (cid:98)X
(cid:98)L, where
each (cid:98)X(cid:96) = X (cid:84) (cid:98)Dd,(cid:96). Figure 5 displays an example of mode clustering using the
KDE.

A nice property of mode clustering is that there is a clear population quan-
tity that our estimator (clusters based on the given sample) is estimating: the
population partition of the data points. Thus we can consider properties of the
procedure such as consistency, which we discuss in detail in Section 4.2.

3.2. Morse-Smale Regression

Let (X, Y ) be a random pair where Y ∈ R and Xi ∈ K ⊂ Rd. Estimating the
regression function m(x) = E[Y |X = x] is challenging for d of even moderate
size. A common way to address this problem is to use a simple regression function
that can be estimated with low variance. For example, one might use an additive
regression of the form m(x) = (cid:80)
j mj(xj) which is a sum of one-dimensional
smooth functions. Although the true regression function is unlikely to be of this
form, it is often the case that the resulting estimator is useful.

A diﬀerent approach, Morse-Smale regression (MSR), is suggested in Gerber
et al. (2013). This takes advantage of the (relatively) simple structure of the
Morse-Smale complex and the isotone behavior of the function on each cell.
Speciﬁcally, MSR constructs a piecewise linear approximation to m(x) over the
cells of the Morse-Smale complex.

We ﬁrst deﬁne the population version of the MSR. Let m(x) = E(Y |X = x)
be the regression function and is assumed to be a Morse-Smale function. Let
E1, · · · EL be the d-cells for m. The Morse-Smale Regression for m is a piecewise
linear function within each cell E(cid:96) such that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

(9)

Chen et al./Inference using the Morse-Smale

10

where (µ(cid:96), β(cid:96)) are obtained by minimizing mean square error:
E (cid:0)(Y − mMSR(X))2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1)

= argmin

E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(cid:1)

(10)

µ,β

µ,β

That is, mMSR is the best linear piecewise predictor using the d-cells. One can
also view MSR as using a linear function to approximate f(cid:96) in the additive
model (6). Note that mMSR is well deﬁned except on the boundaries of E(cid:96) that
have Lebesgue measure 0.

Now we deﬁne the sample version of the MSR. Let (X1, Y1), · · · , (Xn, Yn) be
the random sample from the probability measure PX × PY such that Xi ∈ K ⊂
Rd and Yi ∈ R. Throughout section 3.2, we assume the density of covariates X
is bounded, positive and has a compact support K and the response Y has ﬁnite
second moment.

Let (cid:98)mn be a smooth nonparametric regression estimator for m. We call (cid:98)mn
the pilot estimator. For instance, one may use the kernel regression Nadaraya
(1964) (cid:98)mn(x) =
(cid:98)mn as (cid:98)E1, · · · , (cid:98)E(cid:98)L. Using the data (Xi, Yi) within each estimated d-cell, (cid:98)E(cid:96), the
MSR for (cid:98)mn is given by

as the pilot estimator. We deﬁne d-cells for

i=1 YiK( x−Xi
h )
(cid:80)n
i=1 K( x−Xi
h )

(cid:80)n

(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where ((cid:98)µ(cid:96), (cid:98)β(cid:96)) are obtained by minimizing the empirical squared error:

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

(cid:88)

i:Xi∈ (cid:98)E(cid:96)

(Yi − µ − βT Xi)2

(11)

(12)

This MSR is slightly diﬀerent from the original version in Gerber et al. (2013).
We will discuss the diﬀerence in Remark 1. Computing the parameters of MSR
is not very diﬃcult–we only need to compute the cell labels of each observation
(this can be done by the mean shift algorithm or some fast variants such as the
quick-shift algorithm Vedaldi and Soatto 2008) and then ﬁt a linear regression
within each cell.

MSR may give low prediction error in some cases; see Gerber et al. (2013) for
some concrete examples. In Theorem 5, we prove that we may estimate mMSR at
a fast rate. Moreover, the regression function may be visualized by the methods
discussed later.

Remark 1 The original version of Morse-Smale regression proposed in Gerber
et al. (2013) does not use d-cells of a pilot nonparametric estimate (cid:98)mn. Instead,
they directly ﬁnd local modes and minima using the original data points (Xi, Yi).
This saves computational eﬀort but comes with a price: there is no clear popu-
lation quantity being estimated by their approach. That is, when the sample size
increases to inﬁnity, there is no guarantee that their method will converge. In
our case, we apply a consistent pilot estimate for m and construct d-cells on
this pilot estimate. As is shown in Theorem 4, our method is consistent for this
population quantity.

Chen et al./Inference using the Morse-Smale

11

3.3. Morse-Smale Signatures and Visualization

In this section we deﬁne a new method for visualizing multivariate functions
based on the Morse-Smale complex, called Morse-Smale signatures. The idea is
very similar to the Morse-Smale regression but the signatures can be applied to
any Morse-Smale function.

Let E1, · · · , EK be the d-cells (nonempty intersection of a descending d-
manifold and an ascending 0-manifold) for a Morse-Smale function f that has
a compact support K. The function f depends on the context of the problem.
For density estimation, f is the density p or its estimator (cid:98)pn. For regression
problem, f is the regression function m or a nonparametric estimator (cid:98)mn .
For two sample test, f is the density diﬀerence p1 − p2 or the estimated density
diﬀerence (cid:98)p1−(cid:98)p2. Note that E1, · · · , EK form a partition for K except a Lebesgue
measure 0 set. Each cell corresponds to a unique pair of a local mode and a local
minimum. Thus, the local modes and minima along with d-cells form a bipartite
graph which we call it signature graph. The signature graph contains geometric
information about f . See Figure 6 and 7 for examples.

The signature is deﬁned as follows. We project the maxima and minima of
the function into R2 using multidimensional scaling. We connect a maximum
and minimum by an edge if there exists a cell that connects them. The width
of the edge is proportional to the norm of the linear coeﬃcients of the linear
approximation to the function within the cell. The linear approximation is

fMS(x) = η†

(cid:96) + γ†T

(cid:96) x,

for x ∈ E(cid:96),

where η†

(cid:96) ∈ R and γ†

(cid:96) ∈ Rd are parameters from

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

(13)

(14)

This is again a linear approximation for f(cid:96) in the additive model (6). Note that
fMS may not be continuos when we move from one cell to another. The summary
statistics for the edge associated with cell E(cid:96) are the parameters (η†
(cid:96) ). We
call the function fMS the (Morse-Smale) approximation function; it is the best
piecewise-linear representation for f (piecewise linear within each cell) under L2
error given the d-cells. This function is well-deﬁned except on a set of Lebesgue
measure 0 (the boundaries of each cell). See Figure 6 for a example on the
approximation function. The details are in Algorithm 1.

(cid:96) , γ†

Example. Figure 7 is an example using the GvHD dataset. We ﬁrst conduct
multidimensional scaling (Kruskal, 1964) on the local modes and minima for f
and plot them on the 2-D plane. In Figure 7, the blue dots are local modes and
the green dots are local minima. These dots act as the nodes for the signature
graph. Then we add edges, representing the cells for f that connect pairs of local
modes and minima, to form the signature graph. Lastly, we adjust the width
for the edges according to the strength (L2 norm) of regression function within
each cell (i.e. (cid:107)γ†
(cid:96) (cid:107)). Algorithm 1 provides a summary for visualizing a general
multivariate function using what we described in this paragraph.

Chen et al./Inference using the Morse-Smale

12

(a) Original function

(b) Approximation function

(c) Signature graph

Fig 6. Morse-Smale signatures for a smooth function. (a): The original function. The blue
dots are local modes, the green dots are local minima and the pink dot is a saddle point. (b):
The Morse-Smale approximation to (a). This is the best piecewise linear approximation to the
original function given d-cells. (c): This bipartite graph has nodes that are local modes and
minima and edges that represent the d-cells. Note that we can summarize the smooth function
(a) by the signature graph (c) and the parameters for constructing approximation function
(b). The signature graph and parameters for approximation function deﬁne the Morse-Smale
signatures.

Algorithm 1 Visualization using Morse-Smale Signatures

Input: Grid points x1, · · · , xN and the functional evaluations f (x1), · · · , f (xN ).
1. Find local modes and minima of f on the discretized points x1, · · · , xN . Let M1, · · · MK
and m1, · · · , mS denote the grid points for modes and minima.
2. Partition {x1, · · · , xN } into X1, · · · XL according to the d-cells of f (1. and 2. can be done
by using a k-nearest neighbor gradient ascent/descent method; see Algorithm 1 in Gerber
et al. (2013)).
3. For each cell X(cid:96), ﬁt a linear regression with (Xi, Yi) = (xi, f (xi)), where xi ∈ X(cid:96). Let
the regression coeﬃcients (without intercept) be β(cid:96).
4. Apply multidimensional scaling to modes and minima jointly. Denote their 2 dimensional
representation points as

1 , · · · M ∗

5. Plot {M ∗
6. Add edge to a pair of mode and minimum if there exist a cell that connects them. The
width of the edge is in proportional to (cid:107)β(cid:96)(cid:107) (for cell X(cid:96)).

1, · · · , m∗

K , m∗

1 , · · · M ∗

K , m∗

1, · · · , m∗

S }.

{M ∗
S }.

Chen et al./Inference using the Morse-Smale

13

Fig 7. Morse-Smale Signature visualization (Algorithm 1) of the density diﬀerence for GvHD
dataset (see Figure 2). The blue dots are local modes; the green dots are local minima; the
brown lines are d-cells. These dots and lines form the signature graph. The width indicates
the L2 norm for the slope of regression coeﬃcients. i.e. (cid:107)γ†
(cid:96) (cid:107). The location for modes and
minima are obtained by multidimensional scaling so that the relative distance is preserved.

3.4. Two Sample Comparison

The Morse-Smale complex can be used to compare two samples. There are two
ways to do this. The ﬁrst one is to test the diﬀerence in two density functions
locally and then use the Morse-Smale signatures to visualize regions where the
two samples are diﬀerent. The second approach is to conduct a nonparametric
two sample test within each Morse-Smale cell. The advantage of the ﬁrst ap-
proach is that we obtain a visual display on where the two densities are diﬀerent.
The merit of the second method is that we gain additional power in testing the
density diﬀerence by using the shape information.

3.4.1. Visualizing the Density Diﬀerence

Let X1, . . . Xn and Y1, . . . , Ym be two random sample with densities pX and pY .
In a two sample comparison, we not only want to know if pX = pY but we also
want to ﬁnd the regions that they signiﬁcantly disagree. That is, we are doing
the local tests

H0(x) : pX (x) = pY (x)
(15)
simultaneously for all x ∈ K and we are interested in the regions where we reject
H0(x). A common approach is to estimate the density for both sample by the
KDE and set a threshold to pickup those regions that the density diﬀerence is

Chen et al./Inference using the Morse-Smale

14

large. Namely, we ﬁrst construct density estimates

(cid:98)pX (x) =

1
nhd

n
(cid:88)

i=1

(cid:18) x − Xi
h

(cid:19)

,

K

(cid:98)pY (x) =

1
mhd

m
(cid:88)

i=1

(cid:19)

(cid:18) x − Yi
h

K

and then compute (cid:98)f (x) = (cid:98)pX (x) − (cid:98)pY (x). The regions
(cid:111)
(cid:110)
x ∈ K : | (cid:98)f (x)| > λ

Γ(λ) =

(16)

(17)

are where we have strong evidence to reject H0(x). The threshold λ can be
picked by quantile values of the bootstrapped L∞ density deviation to control
type 1 error or can be chosen by controlling the false discovery rate (Duong,
2013).

Unfortunately, Γ(λ) is hard to visualize when d > 3. So we use the Morse-
Smale complex for (cid:98)f and visualize Γ(λ) by its behavior on the d-cells of the
complex. Algorithm 2 gives a method for visualizing density diﬀerences like
Γ(λ) in the context of comparing two independent samples.

Algorithm 2 Visualization For Two Sample Test

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, threshold λ and radius constant r0
1. Compute the density estimates (cid:98)pX and (cid:98)pY .
2. Compute the diﬀerence function (cid:98)f = (cid:98)pX − (cid:98)pY and the signiﬁcant regions
(cid:111)
x ∈ K : (cid:98)f (x) < −λ

x ∈ K : (cid:98)f (x) > λ

, Γ−(λ) =

Γ+(λ) =

(18)

(cid:111)

(cid:110)

(cid:110)

3. Find the d-cells for (cid:98)f , denoted as E1, · · · , EL.
4. For cell E(cid:96), do (4-1) and (4-2):
4-1. compute the cell center e(cid:96), cell size V(cid:96) = Vol(E(cid:96)),
4-2. compute the positive signiﬁcant ratio and negative signiﬁcant ratio

r+
(cid:96) =

Vol(E(cid:96) ∩ Γ+(λ))
Vol(E(cid:96))

,

r−
(cid:96) =

Vol(E(cid:96) ∩ Γ−(λ))
Vol(E(cid:96))

.

5. For every pair of cell Ej and E(cid:96) (j (cid:54)= (cid:96)), compute the shared boundary size:

Bj(cid:96) = Vold−1( ¯Ej ∩ ¯E(cid:96)),

(19)

(20)

where Vold−1 is the d − 1 dimensional Lebesgue measure.
6. Do multidimensional scaling (Kruskal, 1964) to e1, · · · , eL to obtain low dimensional
representation (cid:101)e1, · · · , (cid:101)eL.
7. Place a ball center at each (cid:101)e(cid:96) with radius r0 ×
8. If r+

(cid:96) > 0, add a pie chart center at (cid:101)e(cid:96) with radius r0 ×
.

(cid:18) r+
(cid:96) +r−
r+
9. Add a line to connect two nodes (cid:101)ej and (cid:101)e(cid:96) if Bj(cid:96) > 0. We may adjust the thickness of
the line according to Bj(cid:96).

chart contains two groups, each with ratio

r−
(cid:96)
(cid:96) +r−
r+

(cid:96) ). The pie

V(cid:96) × (r+

(cid:96) + r−

(cid:96) + r−

V(cid:96).

√

√

(cid:19)

,

(cid:96)

(cid:96)

(cid:96)

An example for Algorithm 2 is in Figure 2, in which we apply the visualization
algorithm for the the GvHD dataset by using kernel density estimator. We
choose the threshold λ by bootstrapping the L∞ diﬀerence for (cid:98)f i.e. supx | (cid:98)f ∗(x)−
(cid:98)f (x)|, where (cid:98)f ∗ is the density diﬀerence for the bootstrap sample. We pick
α = 95% upper quantile value for the bootstrap deviation as the threshold.

Chen et al./Inference using the Morse-Smale

15

The radius constant r0 is deﬁned by the user. It is a constant for visualiza-
tion and does not aﬀect the analysis. Algorithm 2 preserves the relative position
for each cell and visualizes the cell according to its size. The pie-chart provides
the ratio of regions where the two densities are signiﬁcantly diﬀerent. The lines
connecting two cells provide the geometric information about how cells are con-
nected to each other.

By applying Algorithm 2 to the GvHD dataset (Figure 2), we ﬁnd that there
are 6 cells and one cell much larger than the others. Moreover, in most regions,
the blue regions are larger than the red areas. This indicates that compared
to the density of the control group, the density of the GvHD group seem to
concentrates more so that the regions above the threshold are larger.

3.4.2. Morse-Smale Two-Sample Test

Here we introduce a technique combining the energy test (Baringhaus and Franz,
2004; Sz´ekely and Rizzo, 2004, 2013) and the Morse-Smale complex to conduct
a two sample test. We call our method the Morse-Smale Energy test (MSE test).
The advantage of the MSE test is that it is a nonparametric test and its power
can be higher than the energy test; see Figure 8. Moreover, we can combine
our test with the visualization tool proposed in the previous section (Algorithm
2); see Figure 9 for an example for displaying p-values from MSE test when
visualizing the density diﬀerence.

Before we introduce our method, we ﬁrst review the ordinary energy test.
Given two random variables X ∈ Rd and Y ∈ Rd, the energy distance is deﬁned
as

E(X, Y ) = 2E(cid:107)X − Y (cid:107) − E(cid:107)X − X (cid:48)(cid:107) − E(cid:107)Y − Y (cid:48)(cid:107),
where X (cid:48) and Y (cid:48) are iid copies of X and Y . The energy distance has several
useful applications such as the goodness-of-ﬁt testing (Sz´ekely and Rizzo, 2005),
two sample testing (Baringhaus and Franz, 2004; Sz´ekely and Rizzo, 2004, 2013),
clustering (Szekely and Rizzo, 2005), and distance components (Rizzo et al.,
2010) to name but few. We recommend an excellent review paper in (Sz´ekely
and Rizzo, 2013).

(21)

For the two sample test, let X1, · · · , Xn and Y1, · · · , Ym be the two samples

we want to test. The sample version of energy distance is

(cid:98)E(X, Y ) =

(cid:107)Xi − Yj(cid:107) −

(cid:107)Xi − Xj(cid:107) −

(cid:107)Yi − Yj(cid:107).

2
nm

n
(cid:88)

m
(cid:88)

i=1

j=1

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
m2

m
(cid:88)

m
(cid:88)

i=1

j=1

(22)
P→ 0.
If X and Y are from the sample population (the same density), (cid:98)E(X, Y )
Numerically, we use the permutation test for computing the p-value for (cid:98)E(X, Y ).
This can be done quickly in the R-package ‘energy’ (Rizzo and Szekely, 2008).
Now we formally introduce our testing procedure: the MSE test (see Algo-
rithm 3 for a summary). Our test consists of three steps. First, we split the data
into two halves. Second, we use one half of the data (contains both samples)

Chen et al./Inference using the Morse-Smale

16

to do a nonparametric density estimation (e.g. the KDE) and then compute
the Morse-Smale complex (d-cells). Last, we use the other half of the data to
conduct the energy distance two sample test ‘within each d-cell’. That is, we
partition the second half of the data by the d-cells. Within each cell, we do the
energy distance test. If we have L cells, we will have L p-values from the energy
distance test. We reject H0 if any one of the L p-values is smaller than α/L
(this is from Bonferroni correction). Figure 9 provides an example for using the
above procedure (Algorithm 3) along with the visualization method proposed
in Algorithm 2. Data splitting is used to avoid using the same data twice, which
ensures we have a valid test.

Algorithm 3 Morse-Smale Energy Test (MSE test)

Input: Sample 1: {X1, ...Xn}, Sample 2: {Y1, · · · , Ym}, smoothing parameter h, signiﬁcance
level α
1. Randomly split the data into halves D1 and D2; both contain equal number of X and Y
(assuming n and m are even).
2. Compute the KDE (cid:98)pX and (cid:98)pY by the ﬁrst sample D1.
3. Find the d-cells for (cid:98)f = (cid:98)pX − (cid:98)pY , denoted as E1, · · · , EL.
4. For cell E(cid:96), do 4-1 and 4-2:
4-1. Find X and Y in the second sample D2,
4-2. Do the energy test for two sample comparison. Let the p-value be p((cid:96))
5. Reject H0 if p((cid:96)) < α/L for some (cid:96).

Example. Figure 8 shows a simple comparison for the proposed MSE test to
the usual Energy test. We consider a K = 4 Gaussian mixture model in d = 2
with standard deviation of each component being the same σ = 0.2 and the
proportion for each component is (0.2, 0.5, 0.2, 0.1). The left panel displays a
sample with N = 500 from this mixture distribution. We draw the ﬁrst sample
from this Gaussian mixture model. For the second sample, we draw a similar
Gaussian mixture model except that we change the deviation of one component.
In the middle panel, we change the deviation to the third component (C3 in
left panel, which contains 20% data points). In the right panel, we change the
deviation to the fourth component (C4 in left panel, which contains 10% data
points). We use signiﬁcance level α = 0.05 and for MSE test, we consider the
Bonferroni correction and the smoothing bandwidth is chosen using Silverman’s
rule of thumb (Silverman, 1986).

Note that in both the middle and the right panels, the left most case (added
deviation equals 0) is where H0 should not be rejected. As can be seen from
Figure 8, the MSE test has much stronger power compared to the usual Energy
test.

The original energy test has low power while the MSE test has higher power.
This is because the two distributions only diﬀer at a small portion of the regions
so that a global test like energy test requires large sample sizes to detect the
diﬀerence. On the other hand, the MSE test partitions the space according to
the density diﬀerence so that it is capable of detecting the local diﬀerence.

Example. In addition to the higher power, we may combine the MSE test
with the visualization tool in Algorithm 2. Figure 9 displays an example where

Chen et al./Inference using the Morse-Smale

17

Fig 8. An example comparing the Morse-Smale Energy test to the original Energy test. We
consider a d = 2, K = 4 Gaussian mixture model. Left panel: an instance for the Gaussian
mixture. We have four mixture components, denoting as C1, C2, C3 and C4. They have equal
standard deviation (σ = 0.2) and the proportions for each components are (0.2, 0.5, 0.2, 0.1).
Middle panel: We changed the standard deviations of component C3 to 0.3, 0.4 and 0.5 and
compute the power for the MSE test and the usual Energy test at sample size N = 500 and
1000. (Standard deviation equals 0.2 is where H0 should not be rejected.) Right panel: We
add the variance of component C4 (the smallest component) and do the same comparison as
in the middle panel. We pick the signiﬁcance level α = 0.05 (gray horizontal line) and in the
MSE test, we reject H0 if the minimal p-value is less than α/L, where L is the number of
cells (i.e. we are using the Bonferroni correction).

we visualize the density diﬀerence and simultaneously indicate the p-values from
the Energy test within each cell using the GvHD dataset. This provides us more
information about how two distributions diﬀer from each other.

4. Theoretical Analysis

We ﬁrst deﬁne some notation for the theoretical analysis. Let f be a smooth
function. We deﬁne (cid:107)f (cid:107)∞ = supx |f (x)| to be the L∞-norm of f . In addition, let
(cid:107)f (cid:107)j,max denote the elementwise L∞-norm for j-th derivatives of f . For instance,

(cid:107)f (cid:107)1,max = max

(cid:107)gi(x)(cid:107)∞,

i

(cid:107)f (cid:107)2,max = max
i,j

(cid:107)Hij(x)(cid:107)∞.

We also deﬁne (cid:107)f (cid:107)0,max = (cid:107)f (cid:107)∞. We further deﬁne

(cid:107)f (cid:107)∗
The quantity (cid:107)f − h(cid:107)∗
h up to (cid:96)-th order derivative.

(cid:96),max = max {(cid:107)f (cid:107)j,max : j = 0, · · · , (cid:96)} .
(cid:96),max measures the diﬀerence between two functions f and

(23)

For two sets A, B, the Hausdorﬀ distance is

Haus(A, B) = inf{r : A ⊂ B ⊕ r, B ⊂ A ⊕ r},

(24)

where A ⊕ r = {y : minx∈A (cid:107)x − y(cid:107) ≤ r}. The Hausdorﬀ distance is like the L∞
distance for sets.

Let (cid:101)f : K ⊂ Rd (cid:55)→ R be a smooth function with bounded third derivatives.
Note that as long as (cid:107) (cid:101)f −f (cid:107)∗
3,max is small, (cid:101)f is also a Morse function by Lemma 9.
Let (cid:101)D denote the boundaries of the descending d-manifolds of (cid:101)f . We will show
if (cid:107)f − (cid:101)f (cid:107)∗

3,maxis suﬃciently small, then Haus( (cid:101)D, D) = O((cid:107) (cid:101)f − f (cid:107)1,max).

Chen et al./Inference using the Morse-Smale

18

Fig 9. An example using both Algorithm 2 and 3 to the GvHD dataset introduced in Figure 2.
We use data splitting as described in Algorithm 3. For the ﬁrst part of the data, we compute
the cells and visualize the cells using Algorithm 2. Then we apply the energy distance two
sample test for each cell as described in Algorithm 3 and we annotate each cell with a p-
value. Note that the visualization is slightly diﬀerent to Figure 2 since we use only half of the
original dataset in this case.

4.1. Stability of the Morse-Smale Complex

Before we state our theorem, we ﬁrst derive some properties of descending mani-
folds. Recall that we are interested in B = ∂Dd, the boundary of the descending
d-manifolds (and B is also the union of all j-descending manifolds for j < d).
Since each Dj is a collection of smooth j-dimensional manifolds embedded in
Rd, for every x ∈ Dj, there exists a basis v1(x), · · · , vd−j(x) such that each vk(x)
is perpendicular to Dj at x for k = 1, · · · d − j (Bredon, 1993; Helgason, 1979).
That is, v1(x), · · · , vd−j(x) span the normal space to Dj at x. For simplicity, we
write

V (x) = (v1(x), · · · , vd−j(x)) ∈ Rd×(d−j)

(25)

for x ∈ Dj.

Note the number of columns d − j ≡ d − j(x) in V (x) depends on which Dj
the point x belongs to. We use j rather than j(x) to simplify the notation. For
instance, if x ∈ D1, V (x) ∈ Rd×(d−1) and if x ∈ Dd−1, V (x) ∈ Rd×1. We also
let

V(x) = span{v1(x), · · · , vd−j(x)}
(26)
denote the normal space to B at x. One can view V(x) as the normal map of
the manifold Dj at x ∈ Dj.

For each x ∈ B, deﬁne the projected Hessian

HV (x) = V (x)T H(x)V (x),

(27)

which is the Hessian matrix of p by taking gradients along column space of
V (x). If x ∈ Dj, HV (x) is a (d − j) × (d − j) matrix. The eigenvalues of HV (x)

Chen et al./Inference using the Morse-Smale

19

determine how the gradient ﬂows are moving away from B. We let λmin(M ) be
the smallest eigenvalue for a symmetric matrix M . If M is a scalar (just one
point), then λmin(M ) = M .

Assumption (D): We assume that Hmin = minx∈B λmin(HV (x)) > 0.

This assumption is very mild; it requires that the gradient ﬂow moves away
from the boundary of ascending manifolds. In terms of mode clustering, this
requires the gradient ﬂow to move away from the boundaries of clusters. For a
point x ∈ Dd−1, let v1(x) be the corresponding normal direction. Then the gradi-
ent g(x) is normal to v1(x) by deﬁnition. That is, v1(x)T g(x) = v1(x)T ∇p(x) =
0, which means that the gradient along v1(x) is 0. Assumption (D) means that
the the second derivative along v1(x) is positive, which implies that the density
along direction v1(x) behaves like a local minimum at point x. Intuitively, this
is how we expect the density to behave around the boundaries: gradient ﬂows
are moving away from the boundaries (except for those ﬂows that are already
on the boundaries).
Theorem 1 (Stability of descending d-manifolds) Let f, (cid:101)f : K ⊂ Rd (cid:55)→ R
be two smooth functions with bounded third derivatives deﬁned as above and
let B, (cid:101)B be the boundaries of the associated ascending manifolds. Assume f is
a Morse function satisfying condition (D). When (cid:107)f − (cid:101)f (cid:107)∗
3,max is suﬃciently
small,

Haus( (cid:101)B, B) = O((cid:107) (cid:101)f − f (cid:107)1,max).

(28)

This theorem shows that the boundaries of descending d-manifolds for two Morse
functions are close to each other and the diﬀerence between the boundaries is
controlled by the rate of the ﬁrst derivative diﬀerence.

Similarly to descending manifolds, we can deﬁne all the analogous quantities

for ascending manifolds. We introduce the following assumption:

Assumption (A): We assume Hmax = maxx∈∂A0 λmax(HV (x)) < 0.

Note that λmax(M ) denotes the largest eigenvalue of a matrix M . If M is a
scalar, λmax(M ) = M . Under assumption (A), we have a similar stability result
(Theorem 1) for ascending manifolds. Assumptions (A) and (D) together imply
the stability of d-cells.

Theorem 1 can be applied to nonparametric density estimation. Our goal is to
estimate the boundary of the descending d-manifolds, B, of the unknown popu-
lation density function p. Our estimator is (cid:98)Bn, the boundary of the descending

Chen et al./Inference using the Morse-Smale

20

d-manifolds to a nonparametric density estimator e.g. the kernel density esti-
mate (cid:98)pn. Then under certain regularity condition, their diﬀerence is given by

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107)(cid:98)pn − p(cid:107)1,max) .

We will see this result in the next section when we discuss mode clustering.

Similar reasoning works for the nonparametric regression case. Assume that
we are interested in B, the boundary of descending d-manifolds, for the regres-
sion function m(x) = E(Y |X = x). And our estimator (cid:98)B is again a plug-in
estimate based on (cid:98)mn(x), a nonparametric regression estimator (e.g., kernel
estimator). Then under mild regularity conditions,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O ((cid:107) (cid:98)mn − m(cid:107)1,max) .

4.2. Consistency of Mode Clustering

A direct application of Theorem 1 is the consistency of mode clustering. Let
K (α) be the α-th derivative of K and let BCr denote the collection of functions
with bounded continuously derivatives up to the r-th order. We consider the
following two assumptions on the kernel function:
(K1) The kernel function K ∈ BC3 and is symmetric, non-negative and

(cid:90)

x2K (α)(x)dx < ∞,

(cid:90) (cid:16)

(cid:17)2

K (α)(x)

dx < ∞

for all α = 0, 1, 2, 3.

(K2) The kernel function satisﬁes condition K1 of Gine and Guillou (2002). That

is, there exists some A, v > 0 such that for all 0 < (cid:15) < 1, supQ N (K, L2(Q), CK(cid:15)) ≤
(cid:0) A
, where N (T, d, (cid:15)) is the (cid:15)−covering number for a semi-metric space
(cid:15)

(cid:1)v

(T, d) and

(cid:40)

K =

u (cid:55)→ K (α)

: x ∈ Rd, h > 0, |α| = 0, 1, 2

(cid:19)

(cid:18) x − u
h

(cid:41)
.

(K1) is a common assumption; see Wasserman (2006). (K2) is a weak assump-
tion guarantee the consistency for KDE under L∞ norm; this assumption ﬁrst
appeared in Gine and Guillou (2002) and has been widely assumed (Einmahl
and Mason, 2005; Rinaldo et al., 2010; Genovese et al., 2012; Rinaldo et al.,
2012; Genovese et al., 2014; Chen et al., 2015).
Theorem 2 (Consistency for mode clustering) Let p, (cid:98)pn be the density func-
tion and the KDE. Let B and (cid:98)Bn be the boundaries of clusters by mode clus-
tering over p and (cid:98)pn respectively. Assume (D) for p and (K1–2), then when
log n
nhd+6 → 0, h → 0,

(cid:16)

(cid:17)

Haus

(cid:98)Bn, B

= O((cid:107)(cid:98)pn − p(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Chen et al./Inference using the Morse-Smale

21

The proof is simply to combine Theorem 1 and the rate of convergence for
estimating the gradient of density using KDE (Theorem 8). Thus, we omit the
proof. Theorem 2 gives a bound for the rate of convergence for the boundaries
for mode clustering. The rate can be decomposed into two parts, the bias O(h2)

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

and the (square root of) variance OP

. This rate is the same for the

L∞-loss of estimating the gradient of a density function, which makes sense
since the mode clustering is completely determined by the gradient of density.
Another way to describe the consistency for mode clustering is to show that
the proportion of data points that are incorrectly clustered (mis-clustered) con-
verges to 0. This can be quantiﬁed by the use of Rand index (Rand, 1971; Hubert
and Arabie, 1985; Vinh et al., 2009), which measures the similarity between two
partitions of the data points. Let dest(x) and (cid:100)destn(x) be the destination of
gradient of the true density function p(x) and the KDE (cid:98)pn(x). For a pair of
points x, y, we deﬁne

Ψ(x, y) =

(cid:26) 1
0

if dest(x) = dest(y)
if dest(x) (cid:54)= dest(y)

,

(cid:98)Ψn(x, y) =

(cid:40)

1
0

if (cid:100)destn(x) = (cid:100)destn(y)
if (cid:100)destn(x) (cid:54)= (cid:100)destn(y)

(29)
Thus, Ψ(x, y) = 1 if x, y are in the same cluster and 0 if they are not. The Rand
index for mode clustering using p versus using (cid:98)pn is

rand ((cid:98)pn, p) = 1 −

(cid:19)−1

(cid:18)n
2

(cid:88)

i(cid:54)=j

(cid:12)
(cid:12)
(cid:12)Ψ(Xi, Xj) − (cid:98)Ψn(Xi, Xj)

(cid:12)
(cid:12)
(cid:12) ,

(30)

which is the proportion of pairs of data points that the two clustering results
disagree on. If two clusterings output the same partition, the Rand index will
be 1.

Theorem 3 (Bound on Rand Index) Assume (D) for p and (K1–2). Then
when log n

nhd+6 → 0, h → 0, the adjusted Rand index

rand ((cid:98)pn, p) = 1 − O(h2) − OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
nhd+2

Theorem 3 shows that the Rand index converges to 1 in probability, which
establishes the consistency of mode clustering in an alternative way. Theo-
rem 3 shows that the proportion of data points that are incorrectly assigned
(compared with mode clustering using population p) is bounded by the rate

O(h2) + OP

(cid:18)(cid:113) log(n)

(cid:19)

nhd+2

asymptotically.

Azizyan et al. (2015) also derived the convergence rate of the mode clustering
for the rand index. Here we brieﬂy compare our results to theirs. Azizyan et al.
(2015) consider a low-noise condition that leads to a fast convergence rate when
clusters are well-separated. Their approach can even be applied to the case of

(31)

(32)

(33)

(34)

Chen et al./Inference using the Morse-Smale

22

increasing dimensions. In our case (Theorem 3), we consider a ﬁxed dimension
scenario but we do not assume the low-noise condition. Thus, the main diﬀerence
between Theorem 3 and the result in Azizyan et al. (2015) is the assumptions
being made so our result complements the ﬁndings in Azizyan et al. (2015).

4.3. Consistency of Morse-Smale Regression

In what follows, we will show that (cid:98)mn,MSR(x) is a consistent estimator of mMSR(x).
Recall that

mMSR(x) = µ(cid:96) + βT

(cid:96) x, for x ∈ E(cid:96),

where E(cid:96) is the d-cell deﬁned on m and the parameters are
E (cid:0)(Y − µ − βT X)2|X ∈ E(cid:96)

(µ(cid:96), β(cid:96)) = argmin

(cid:1) .

µ,β

And (cid:98)mn,MSR is the two-stage estimator to mMSR(x) deﬁned by
(cid:98)mn,MSR(x) = (cid:98)µ(cid:96) + (cid:98)βT

(cid:96) x, for x ∈ (cid:98)E(cid:96),

where { (cid:98)E(cid:96) : (cid:96) = 1, · · · , (cid:98)L} are the collection of cells of the pilot nonparametric
regression estimator (cid:98)mn and (cid:98)µ(cid:96), (cid:98)β(cid:96) are the regression parameters from equation
(12):
(cid:88)

(Yi − µ − βT Xi)2.

((cid:98)µ(cid:96), (cid:98)β(cid:96)) = argmin

µ,β

i:Xi∈ (cid:98)E(cid:96)
Theorem 4 (Consistency of Morse-Smale Regression) Assume (A) and
(D) for m and assume m is a Morse-Smale function. Then when log n
nhd+6 →
0, h → 0, we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

+ O ((cid:107) (cid:98)mn − m(cid:107)1,max)

(35)

(cid:19)

(cid:18) 1
√
n

uniformly for all x except for a set Nn with Lebesgue measure OP((cid:107) (cid:98)mn−m(cid:107)1,max),
Theorem 4 states that when we have a consistent pilot nonparametric regression
estimator (such as the kernel regression), the proposed MSR estimator converges
to the population MSR. Similarly as in Theorem 6, the set Nn are regions around
the boundaries of cells where we cannot distinguish their host cell. Note that
when we use the kernel regression as the pilot estimator (cid:98)mn, Theorem 4 becomes

|mMSR(x) − (cid:98)mn,MSR(x)| = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

under regular smoothness conditions.

Now we consider a special case where we may obtain parametric rate of
(cid:83) · · · (cid:83) EL) be the boundaries

convergence for estimating mMSR. Let E = ∂ (E1
of all cells. We consider the following low-noise condition:

P (X ∈ E ⊕ (cid:15)) ≤ A(cid:15)β,

(36)

Chen et al./Inference using the Morse-Smale

23

for some A, β > 0. Equation (36) is Tsybakov’s low noise condition (Audibert
et al., 2007) applied to the boundaries of cells. Namely, (36) states that it is
unlikely to many observations near the boundaries of cells of m. Under this
low-noise condition, we obtain the following result using kernel regression.

Theorem 5 (Fast Rate of Convergence for Morse-Smale Regression)
Let the pilot estimator (cid:98)mn be the kernel regression estimator. Assume (A)
and (D) for m and assume m is a Morse-Smale function. Assume also (36)
holds for the covariate X and (K1-2) for the kernel function. Also assume that
. Then uniformly for all x except for a set Nn with

(cid:17)1/(d+6)(cid:19)

(cid:18)(cid:16) log n

h = O

n

Lebesgue measure OP

(cid:18)(cid:16) log n

(cid:17)2/(d+6)(cid:19)

,

n

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(cid:19)

(cid:18) 1
√
n

+ OP

(cid:32)(cid:18) log n
n

(cid:19)2β/(d+6)(cid:33)

.

(37)

Therefore, when β > 6+d

4 , we have

|mMSR(x) − (cid:98)mn,MSR(x)| = OP

(38)

(cid:18) 1
√
n

(cid:19)

.

Theorem 5 shows that when the low noise condition holds, we obtain a fast
rate of convergence for estimating mMSR. Note that the pilot estimator (cid:98)mn does
not ahve to be a kernel estimator; other approaches such as the local polynomial
regression will also work.

4.4. Consistency of the Morse-Smale Signature

Another application of Theorem 1 is to bound the diﬀerence of two Morse-
Smale signatures. Let f be a Morse-Smale function with cells E1, . . . , EL. Recall
that the Morse-Smale signatures are the bipartite graph and summary statistics
(locations, density values) for local modes, local minima, and cells. It is known in
the literature (see, e.g., Lemma 9) that when two functions (cid:101)f , f are suﬃciently
close, then

(cid:16)

(cid:17)

max
j

(cid:107)(cid:101)cj − cj(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)1,max

, max

(cid:107) (cid:101)f ((cid:101)cj) − f (cj)(cid:107) = O

(cid:107) (cid:101)f − f (cid:107)∞

j

(39)
where (cid:101)cj, cj are critical points (cid:101)f and f respectively. This implies the stability of
local modes and minima.

So what we need is the stability of the summary statistics (η†

(cid:96) , γ†

(cid:96) ) associated

with the edges (cells). Recall that these summaries are deﬁned through (14)

(cid:16)

(cid:17)

,

(cid:90)

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Chen et al./Inference using the Morse-Smale

24

For another function (cid:101)f , let ((cid:101)η†
(cid:96) ) be its signatures for cell (cid:101)E(cid:96). The following
theorem shows that if two functions are close, their corresponding Morse-Smale
signatures are also close.

(cid:96) , (cid:101)γ†

Theorem 6 Let f be a Morse-Smale function satisfying assumptions A and D,
and let (cid:101)f be a smooth function. Then when log n
nhd+6 → 0, h → 0, after relabeling
the indices of cells of (cid:101)f ,

(cid:110)
(cid:107)(cid:101)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:101)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

1,max

(cid:17)

.

Theorem 6 shows stability of the signatures (η†

(cid:96) , γ†

(cid:96) ). Note that Theorem 6

also implies that the stability of piecewise approximation

|fMS(x) − (cid:101)fMS(x)| = O

(cid:16)

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

Together with the stability of critical points (39), Theorem 6 proves the stability
of Morse-Smale signatures.

4.4.1. Example: Morse-Smale Density Estimation

As an example for Theorem 6, we consider density estimation. Let p be the
density of random sample X1, · · · , Xn and recall that (cid:98)pn is the kernel density
estimator. Let (η†
(cid:96) ) be the
signature for (cid:98)pn under cell (cid:98)E(cid:96). The following corollary guarantees the consistency
of Morse-Smale signatures for the KDE.

(cid:96) ) be the signature for p under cell E(cid:96) and ((cid:98)η†

(cid:96) , (cid:98)γ†

(cid:96) , γ†

Corollary 7 Assume (A,D) holds for p and the kernel function satisﬁes (K1–
2). Then when log n

nhd+6 → 0, h → 0, after relabeling we have
(cid:32)(cid:114)

(cid:110)
(cid:107)(cid:98)η†

max
(cid:96)

(cid:96) − η†

(cid:96) (cid:107), (cid:107)(cid:98)γ†

(cid:96) − γ†
(cid:96) (cid:107)

(cid:111)

= O(h2) + OP

(cid:33)

.

log n
nhd+2

The proof to Corollary 7 is a simple application of Theorem 6 with the rate of
convergence for the ﬁrst derivative of the KDE (Theorem 8). So we omit the

proof. The optimal rate in Corollary 7 is OP

when we choose h to

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 2

n

be of order O

(cid:18)(cid:16) log n

d+6 (cid:19)

(cid:17) 1

.

n

Remark 2 When we compute the Morse-Smale approximation function, we
may have some numerical problem in low-density regions because the density
estimate (cid:98)pn may have unbounded support. In this case, some cells may be un-
bounded, and the majority of these cells may have extremely low density value,
which makes the approximation function 0. Thus, in practice, we will restrict

Chen et al./Inference using the Morse-Smale

25

ourselves only to the regions whose density is above a pre-deﬁned threshold λ so
that every cell is bounded. A simple data-driven threshold is λ = 0.05 supx (cid:98)pn(x).
Note that Theorem 7 still works in this case but with a slight modiﬁcation: the
cells are deﬁne on the regions {x : ph(x) ≥ 0.05 × supx ph(x)}.

Remark 3 Note that for a density function, local minima may not exist or the
gradient ﬂow may not lead us to a local minimum in some regions. For instance,
for a Gaussian distribution, there is no local minimum and except for the center
of the Gaussian, if we follow the gradient descent path, we will move to inﬁnity.
Thus, in this case we only consider the boundaries of ascending 0-manifolds
corresponding to well-deﬁned local minima and assumptions (A) is only for the
boundaries corresponding to these ascending manifolds.

Remark 4 When we apply the Morse-Smale complex to nonparametric density
estimation or regression, we need to choose the tuning parameter. For instance,
in the MSR, we may use kernel regression or local polynomial regression so we
need to choose the smoothing bandwidth. For the density estimation problem
or mode clustering, we need to choose the smoothing bandwidth for the kernel
smoother. In the case of regression, because we have the response variable, we
would recommend to choose the tuning parameter by cross-validation. For the
kernel density estimator (and mode clustering), because the optimal rate depends
on the gradient estimation, we recommend choosing the smoothing bandwidth
using the normal reference rule for gradient estimation or the cross-validation
method for gradient estimation (Duong et al., 2007; Chac´on et al., 2011).

5. Discussion

In this paper, we introduced the Morse-Smale complex and the summary sig-
natures for nonparametric inference. We demonstrated that the Morse-Smale
complex can be applied to various statistical problems such as clustering, re-
gression and two sample comparisons. We showed that a smooth multivariate
function can be summarized by a few parameters associated with a bipartite
graph, representing the local modes, minima and the complex for the underly-
ing function. Moreover, we proved a fundamental theorem about the stability
of the Morse-Smale complex. Based on the stability theorem, we derived con-
sistency for mode clustering and regression.

The Morse-Smale complex provides a method to synthesize both paramet-
ric and nonparametric inference. Compared to parametric inference, we have a
more ﬂexible model to study the structure of the underlying distribution. Com-
pared to nonparametric inference, the use of the Morse-Smale complex yields a
visualizable representation for the underlying multivariate structures. This re-
veals that we may gain additional insights in data analysis by using geometric
features.

Although the Morse-Smale complex has many potential statistical applica-
tions, we need to be careful when applying it to a data set whose dimension
is large (say d > 10). When the dimension is large, the curse of dimensionality

Chen et al./Inference using the Morse-Smale

26

kicks in and the nonparametric estimators (in both density estimation problems
or regression analysis) are not accurate so the errors of the estimated Morse-
Smale complex can be huge.

Here we list some possible extensions for future research:

• Asymptotic distribution. We have proved the consistency (and the rate of
convergence) for estimating the complex but the limiting distribution is
still unknown. If we can derive the limiting distribution and show that
some resampling method (e.g. the bootstrap Efron (1979)) converges to
the same distribution, we can construct conﬁdence sets for the complex.
• Minimax theory. Despite the fact that we have derived the rate of con-
vergence for a plug-in estimator for the complex, we did not prove its
optimality. We conjecture the minimax rate for estimating the complex
should be related to the rate for estimating the gradient and the smooth-
ness around complex (Audibert et al., 2007; Singh et al., 2009).

Acknowledgement

We thank the referees and the Associate Editor for their very constructive com-
ments and suggestions.

Appendix A: Appendix: Proofs

First, we include a Theorem about the rate of convergence for the kernel density
estimator. This Lemma will be used in deriving the convergence rates.

Theorem 8 (Lemma 10 in Chen et al. (2015); see also Genovese et al. (2014))
Assume (K1–2) and that log n/n ≤ hd ≤ b for some 0 < b < 1. Then we have

(cid:107)(cid:98)pn − p(cid:107)∗

(cid:96),max = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

log n
nhd+2(cid:96)

for (cid:96) = 0, 1, 2.

of critical points.

To prove Theorem 1, we introduce the following useful Lemma for stability

Lemma 9 (Lemma 16 of Chazal et al. (2014)) Let p be a density with com-
pact support K of Rd. Assume p is a Morse function with ﬁnitely many, distinct,
critical values with corresponding critical points C = {c1, · · · , ck}. Also assume
that p is at least twice diﬀerentiable on the interior of K, continuous and dif-
ferentiable with non vanishing gradient on the boundary of K. Then there exists
(cid:15)0 > 0 such that for all 0 < (cid:15) < (cid:15)0 the following is true: for some positive
constant c, there exists η ≥ c(cid:15)0 such that, for any density q with support K
satisfying (cid:107)p − q(cid:107)∗

2,max ≤ η, we have

1. q is a Morse function with exact k critical points c(cid:48)

1, · · · , c(cid:48)

k and

Chen et al./Inference using the Morse-Smale

27

Fig 10. Diagram for lemmas and Theorem 1.

2. after suitable relabeling the indices, maxi=1,··· ,k (cid:107)ci − c(cid:48)

i(cid:107) ≤ (cid:15).

Note that similar result appears in Theorem 1 of Chen et al. (2016). This lemma
shows that two close Morse functions p, q will have similar critical points.

The proof of Theorem 1 requires several working lemmas. We provide a chart

for how we are going to prove Theorem 1.

First, we deﬁne some notations about gradient ﬂows. Recall that πx(t) ∈ K

is the gradient (ascent) ﬂow starting at x:

πx(0) = x,

π(cid:48)
x(t) = g(πx(t)).

For x that is not on the boundary set D, we deﬁne the time:

t(cid:15)(x) = inf{t : πx(s) ∈ B(m,

(cid:15)), for alls ≥ t},

√

where m is the destination of πx. That is, t(cid:15)(x) is the time to arrive the regions
around a local mode.

First, we prove a property for the direction of the gradient ﬁeld around bound-

aries.

Lemma 10 (Gradient ﬁeld and boundaries) Assume the notations in The-
orem 1 and assume f is a Morse function with bounded third derivatives and
satisﬁes assumption (D). Let s(x) = x − Πx, where Πx ∈ B is the projected
point from x onto B (when Πx is not unique, just pick any projected point). For
any q ∈ B, let x be a point near q such that x − q ∈ V(q), the normal space of
B at q. Let δ(x) = (cid:107)x − q(cid:107) and e(x) = x−q

(cid:107)x−q(cid:107) denote the unit vector. Then

Chen et al./Inference using the Morse-Smale

28

(a) Lemma 10

(b) Lemma 11

Fig 11. Illustration for Lemma 10 and 11. (a): We show that the angle between projection
vector s(x) and the gradient g(x) is always right whenever x is closed to the boundaries B. (b):
According to (a), any gradient ﬂow line start from a point x that is close to the boundaries
(distance < δ1), this ﬂow line is always moving away from the boundaries when the current
location is close to the boundaries. The ﬂow line can temporally get closer to the boundaries
when it is away from boundaries (distance > δ1)

1. For every point x such that

d(x, B) ≤ δ1 =

2Hmin
d2 · (cid:107)f (cid:107)3,max

,

we have

g(x)T s(x) ≥ 0.

That is, the gradient is pushing x away from the boundaries.

2. When δ(x) ≤

Hmin
d2·(cid:107)f (cid:107)3,max

,

(cid:96)(x) = e(x)T g(x) ≥

Hminδ(x).

1
2

Proof.
Claim 1. Because the projection of x onto B is Πx, s(x) ∈ V(Πx) and
s(x)T g(Πx) = 0 (recall that for p ∈ B, V(p) is the collection of normal vectors
of B at p).

Recall that d(x, B) = (cid:107)s(x)(cid:107) is the projected distance. By the fact that

Chen et al./Inference using the Morse-Smale

29

s(x)T g(Πx) = 0,

s(x)T g(x) = s(x)T (g(x) − g(Πx))
d2
2

≥ s(x)T H(Πx)s(x) −
= d(x, B)2 s(x)T
d(x, B)
(cid:18)

H(Πx)

(cid:107)f (cid:107)3,maxd(x, B)3

(Taylor’s theorem)

s(x)
d(x, B)

−

d2
2

(cid:107)f (cid:107)3,maxd(x, B)3

(cid:19)

≥ d(x, B)2

Hmin −

(cid:107)f (cid:107)3,maxd(x, B)

.

d2
2

(40)
Note that we use the vector-value Taylor’s theorem in the ﬁrst inequality and
the fact that for two close points x, y, the diﬀerence in the j-the element of
gradient gj(x) − gj(y) has the following expansion

gj(x) − gj(y) = Hj(y)T (x − y) +

(u − y)Tj(u)du

(cid:90) x

sup
u

u=y
1
2
d2
2

≥ Hj(y)T (x − y) −

(cid:107)Tj(u)(cid:107)2(cid:107)x − y(cid:107)2

≥ Hj(y)T (x − y) −

(cid:107)f (cid:107)3,max(cid:107)x − y(cid:107)2,

where Hj(y) = ∇gj(y) and Tj(y) = ∇∇gj(y) is the Hessian matrix of gj(y),
whose elements are the third derivatives of f (y).

Thus, when d(x, B) ≤ 2Hmin
, s(x)T g(x) ≥ 0, which proves the ﬁrst claim.
Claim 2. By deﬁnition, e(x)T g(q) = 0 because g(q) is in tangent space of B

d2·(cid:107)f (cid:107)3,max

at q and e(x) is in the normal space of B at q. Thus,

(cid:96)(x) = e(x)T g(x)

= e(x)T (g(x) − g(q))

≥ e(x)T H(q)(x − q) −

(cid:107)f (cid:107)3,max(cid:107)x − q(cid:107)2

(41)

= e(x)T H(π(x))e(x)δ(x) −

(cid:107)f (cid:107)3,maxδ(x)2

d2
2

d2
2

≥

Hminδ(x)

1
2

whenever δ(x) = (cid:107)x − q(cid:107) ≤
. Note that in the ﬁrst inequality we use
the same lower bound as the one in claim 1. Also note that x − q = e(x)δ(x)
and e(x) is in the normal space of B at π(x) so the third inequality follows from
assumption (D).

Hmin
d2·(cid:107)f (cid:107)3,max

(cid:3)

Lemma 10 can be used to prove the following result.

Chen et al./Inference using the Morse-Smale

30

Lemma 11 (Distance between ﬂows and boundaries) Assume the nota-
tions as the above and assumption (D). Then for all x such that 0 < d(x, B) =
δ ≤ δ1 = 2Hmin

,

d2(cid:107)f (cid:107)3,max

d(πx(t), B) ≥ δ,

for all t ≥ 0.

The main idea is that the projected gradient (gradient projected to the normal
space of nearby boundaries) is always positive. This means that the ﬂow cannot
move “closer” to the boundaries.

Proof. By Lemma 10, for every point x near to the boundaries (d(x, B) <
δ1), the gradient is moving this point away from the boundaries. Thus, for any
ﬂow πx(t), once it touches the region B ⊕ δ1, it will move away from this region.
So when a ﬂow leaves B ⊕ δ1, it can never come back.

Therefore, the only case that a ﬂow can be within the region B ⊕ δ1 is that

it starts at some x ∈ B ⊕ δ1. i.e. d(x, B) < δ1.

Now consider a ﬂow start at x such that 0 < d(x, B) ≤ δ1. By Lemma 10,
the gradient g(x) leads x to move away from the boundaries B. Thus, whenever
πx(t) ∈ B ⊕ δ1, the gradient is pushing πx(t) away from B. As a result, the time
that πx(t) is closest to B is at the beginning of the ﬂow .i.e. t = 0. This implies
that d(πx(t), B) ≥ d(πx(0), B) = d(x, B) = δ.

(cid:3)

With Lemma 11, we are able to bound the low gradient regions since the
ﬂow cannot move inﬁnitely close to critical points except its destination. Let
λmin > 0 be the minimal ‘absolute’ value of eigenvalues of all critical points.

Lemma 12 (Bounds on low gradient regions) Assume the density func-
tion f is a Morse function and has bounded third derivatives. Let C denote
the collection of all critical points and let λmin is the minimal ‘absolute’ eigen-
value for Hessian matrix H(x) evaluated at x ∈ C. Then there exists a constant
δ2 > 0 such that

G(δ) ≡

x : (cid:107)g(x)(cid:107) ≤

⊂ C ⊕ δ

(42)

(cid:26)

(cid:27)

λmin
2

δ

for every δ ≤ δ2.

Proof. Because the support K is compact and x ∈ K (cid:55)→ (cid:107)g(x)(cid:107) is contin-
uous, for any g0 > 0 suﬃciently small, there exists a constant R(g0) > 0 such
that

G1(g0) ≡ {x : (cid:107)g(x)(cid:107) ≤ g0} ⊂ C ⊕ R(g0)

and when g0 → 0, R(g0) → 0. Thus, there is a constant g1 > 0 such that
R(g1) =

.

λmin
2d3(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

31

The set C ⊕ λmin

2(cid:107)f (cid:107)3,max

has a useful feature: for any x ∈ C ⊕ λmin

,

2(cid:107)f (cid:107)3,max

(cid:107)H(x) − H(c)(cid:107)F = (cid:107)(x − c)f (3)(c + t(x − c))dt(cid:107)F

≤ d3(cid:107)x − c(cid:107)(cid:107)f (cid:107)3,max
λmin
2d3(cid:107)f (cid:107)3,max

≤ d3

· (cid:107)f (cid:107)3,max

=

λmin
2

,

where f (3) is a d × d × d array of the third derivative of f and (cid:107)A(cid:107)F is the
Frobenius norm of the matrix A. By Hoﬀman–Wielandt theorem (see, e.g., page
165 of Bhatia 1997), the eigenvalues between H(x) and H(c) is bounded by
(cid:107)H(x) − H(c)(cid:107)F . Therefore, the smallest eigenvalue of H(x) must be greater
than or equal to the smallest eigenvalue of H(c) minus λmin
2 . Because λmin is
the smallest absolute eigenvalues of H(c) for all c ∈ C, the smallest eigenvalue
of H(x) is greater than or equal to λmin
.

2 , for all x ∈ C ⊕ R(g1) = C ⊕
λmin
2d3(cid:107)f (cid:107)3,max

λmin
2d3(cid:107)f (cid:107)3,max
, for any

Using the above feature and the fact that G1(g1) ⊂ C ⊕

x ∈ G1(g1), we have the following inequalities:

g1 ≥ (cid:107)g(x)(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

=

0

≥ (cid:107)x − c(cid:107)

λmin.

1
2

(x − c)H(c + t(x − c))dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Thus, (cid:107)x − c(cid:107) ≤ 2g1
λmin

, which implies

Moreover, because G1(g2) ⊂ G1(g3) for any g2 ≤ g3, any g2 ≤ g1 satisﬁes

Now pick δ = 2g2
λmin

, we conclude

G1(g1) ⊂ C ⊕

G1(g2) ⊂ C ⊕

2g1
λmin

.

2g2
λmin

.

G1

(cid:19)

(cid:18) λmin
2δ

= G(δ) ⊂ C ⊕ δ

δ =

2g2
λmin

≤

2g1
λmin

= δ2,

for all

(cid:3)

where g1 is the constant such that R(g1) =

λmin
2d3(cid:107)f (cid:107)3,max

.

(43)

Chen et al./Inference using the Morse-Smale

32

Fig 12. Illustration for H((cid:15), δ). The thick black lines are boundaries B; solid dots are local
modes; box is local minimum; empty dots are saddle points. The three purple lines denote
possible gradient ﬂows starting from some points x with d(x, B) = δ. The gray disks denote
all possible regions such that (cid:107)g(cid:107) ≤ λmin
2 δ. Thus, the amount of gradient within the set H((cid:15), δ)
is greater or equal to λmin

2 δ.

Lemma 13 (Bounds on gradient ﬂow) Using the notations above and as-
sumption (D), let δ1 be deﬁned in Lemma 11 and δ2 be deﬁned in Lemma 12,
equation (43). Then for all x such that

d(x, B) = δ < δ0 = min

δ1, δ2,

(cid:26)

Hmin
d2 · (cid:107)f (cid:107)3,max

(cid:27)

,

and picking (cid:15) such that δ2 > (cid:15)2 > δ, we have

η(cid:15)(x) ≡ inf

(cid:107)g(πx(t))(cid:107) ≥ δ

0≤t≤t(cid:15)(x)

λmin
2

.

γ(cid:15)(δ) ≡ inf
x∈Bδ

η(cid:15)(x) ≥ δ

λmin
2

,

Moreover,

where Bδ = {x : d(x, B) = δ}.

Proof.
We consider the ﬂow πx starting at x (not on the boundaries) such that

d(x, B) = δ < min {δ1, δ2} .

For 0 ≤ t ≤ t(cid:15)(x), the entire ﬂow is within the set

H((cid:15), δ) = {x : d(x, B) ≥ δ, d(x, M ) ≥

(cid:15)}.

(44)

√

Chen et al./Inference using the Morse-Smale

That is,

{πx(t) : 0 ≤ t ≤ t(cid:15)(x)} ⊂ H((cid:15), δ).

This is because by Lemma 11, the ﬂow line cannot get closer to the boundaries
B within distance δ, and the ﬂow stops when its distance to its destination is
at (cid:15). Thus, if we can prove that every point within H((cid:15), δ) has gradient lowered
bounded by δ λmin
2 , we have completed the proof. That is, we want to show that

33

(45)

(46)

To show the lower bound, we focus on those points whose gradient is small.

Let

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

(cid:26)

S(δ) =

x : (cid:107)g(x)(cid:107) ≤ δ

(cid:27)

.

λmin
2

S(δ) ⊂ C ⊕ δ.

By Lemma 12, the S(δ) are regions around critical points such that

Since we have chosen (cid:15) such that (cid:15) ≥ δ2 and by the fact that critical points
are either in M , the collection of all local modes, or in B the boundaries so that,
the minimal distance between H((cid:15), δ) and critical points C is greater that δ (see
equation (44) for the deﬁnition of H((cid:15), δ)). Thus,

which implies equation (46):

(C ⊕ δ) ∩ H((cid:15), δ) = ∅,

inf
x∈H((cid:15),δ)

(cid:107)g(x)(cid:107) ≥ δ

λmin
2

.

Now by the fact that all πx(t) with d(x, B) < δ are within the set H((cid:15), δ)
(equation (45)), we conclude the result.

(cid:3)

Lemma 13 links the constant γ(cid:15)(δ) and the minimal gradient, which can be
used to bound the time t(cid:15)(x) uniformly and further leads to the following result.
Lemma 14 Let K(δ) = {x ∈ K : d(x, B) ≥ δ} = K\(B ⊕δ) and δ0 be deﬁned as
Lemma 13 and M is the collection of all local modes. Assume that f has bounded
third derivative and is a Morse function and that assumption (D) holds. Let (cid:101)f
be another smooth function. There exists constants c∗, c0, c1, (cid:15)0 that all depend
only on f such that when ((cid:15), δ) satisfy the following condition

δ < (cid:15) < (cid:15)0,

δ < min{δ0, Haus(K(δ), B(M,

(cid:15)))}

√

and if

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤ c0

(cid:107)f − (cid:101)f (cid:107)1,max ≤ c1 exp

−

(cid:32)

√
4

d(cid:107)f (cid:107)2,max(cid:107)f (cid:107)∞

(cid:33)

,

δ2λ2

min

(47)

(48)

Chen et al./Inference using the Morse-Smale

34

Fig 13. Result from Lemma 13: lower bound on minimal gradient. This plot shows possible
values for minimal gradient η(cid:15)(x) (pink regions) when d(x, B) is known. Note that we have
chosen (cid:15)2 < δ2.

then for all x ∈ K(δ)

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞.

(49)

(cid:113)

Note that condition (47) holds when ((cid:15), δ) are suﬃciently small.

Proof. The proof of this lemma is closely related to the proof of Theorem
2 of Arias-Castro et al. (2016). The results in Arias-Castro et al. (2016) is a
pointwise convergence of gradient ﬂows; now we will generalize their ﬁndings to
the uniform convergence.

Note that K(δ) = H((cid:15), δ) ∪ B(x,

(cid:15)). For x ∈ B(x,

(cid:15)), the result is trivial

√

√

when (cid:15) is suﬃciently small. Thus, we assume x ∈ H((cid:15), δ).

From equation (40–44) in Arias-Castro et al. (2016) (proof to their Theorem

2),

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107)
(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

2
λmin

2λmin(cid:15) +

√

(cid:107)f − (cid:101)f (cid:107)1,maxe

d(cid:107)f (cid:107)2,maxt(cid:15)(x) + 2(cid:107)f − (cid:101)f (cid:107)∞

√

(cid:107)f (cid:107)1,max
d(cid:107)f (cid:107)2,max

(cid:33)

(50)

under condition (48) and (cid:15) < (cid:15)0 for some constant (cid:15)0.

Thus, the key is to bound t(cid:15)(x). Recall that x ∈ H((cid:15), δ). Now consider the

Chen et al./Inference using the Morse-Smale

35

gradient ﬂow πx and deﬁne z = πx(t(cid:15)(x)).

f (z) − f (x) =

∂f (πx(s))
∂s

ds =

0

(cid:90) t(cid:15)(x)

g(πx(s))T π(cid:48)

x(s)ds

(cid:107)g(πx(s))(cid:107)2ds ≥ γ(cid:15)(δ)2t(cid:15)(x).

(cid:90) t(cid:15)(x)

(cid:90) t(cid:15)(x)

=

0

0

Since f (z) − f (x) ≤ 2(cid:107)f (cid:107)∞, we have

(cid:107)f (cid:107)∞ ≥

γ(cid:15)(δ)2t(cid:15)(x)

1
2

t(cid:15)(x) ≤

2(cid:107)f (cid:107)∞
γ(cid:15)(δ)2

≤

8(cid:107)f (cid:107)∞
δ2λ2

min

and by Lemma 13,

for all x ∈ H((cid:15), δ).

Now plug-in (52) into (50), we have

(51)

(52)

(cid:114)

(cid:107) lim
t→∞

πx(t)− lim

t→∞ (cid:101)πx(t)(cid:107) ≤

a0(cid:15) + a1(cid:107)f − (cid:101)f (cid:107)1,maxe

(53)
for some constants a0, a1, a2. Now using condition (48) to replace the second
term of right hand side, we conclude

√

d(cid:107)f (cid:107)2,max

8(cid:107)f (cid:107)∞
δ2λ2

min + a2(cid:107)f − (cid:101)f (cid:107)∞

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max

By Lemma 7 in Arias-Castro et al. (2016), there exists some constant c3 such

for some constant a3.

that when a3

(cid:15) + (cid:107)f − (cid:101)f (cid:107)∗

1,max < 1/c3,

(cid:113)

(cid:113)

√

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤

2c3(cid:107)f − (cid:101)f (cid:107).

Thus, when both (cid:15) and (cid:107)f − (cid:101)f ∗
constant c∗ such that

3,max(cid:107) are suﬃciently small, there exists some

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗(cid:107)f − (cid:101)f (cid:107)

for all x ∈ H((cid:15), δ).

(cid:3)

Now we turn to the proof of Theorem 1.

we show that when (cid:107)f − (cid:101)f (cid:107)∗

Proof of Theorem 1. The proof contains two parts. In the ﬁrst part,
3,max is suﬃciently small, we have Haus(B, (cid:101)B) <
, where B and (cid:101)B are the boundary of descending d-manifolds for f

Hmin
d2(cid:107)f (cid:107)3,max

Chen et al./Inference using the Morse-Smale

36

Hmin
d2(cid:107)f (cid:107)3,max

and (cid:101)f . The second part of the proof is to derive the convergence rate. Because
Haus(B, (cid:101)B) <
, we can apply the second assertion of Lemma 10 to
derive the rate of convergence. Note that C and (cid:101)C are the critical points for f
and (cid:101)f and M ≡ C0, (cid:102)M ≡ (cid:101)C0 are the local modes for f and (cid:101)f .
Hmin
d2·(cid:107)f (cid:107)3,max

, the upper bound for Hausdorﬀ dis-
tance. Let σ = min{(cid:107)x − y(cid:107) : x, y ∈ M, x (cid:54)= y}. That is, σ is the smallest dis-
tance between any pair of distinct local modes. By Lemma 9, when (cid:107)f − (cid:101)f (cid:107)∗
is small, f and (cid:101)f have the same number of critical points and

Part 1: Haus(B, (cid:101)B) <

3,max

Haus(C, (cid:101)C) ≤ A(cid:107)f − (cid:101)f (cid:107)∗

2,max ≤ A(cid:107)f − (cid:101)f (cid:107)∗

3,max,

where A is a constant that depends only on f (actually, we only need (cid:107)f − (cid:101)f (cid:107)∗
to be small here).

2,max

Thus, whenever (cid:107)f − (cid:101)f (cid:107)∗

3,max satisﬁes

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

σ
3A

,

every M has an unique corresponding point in (cid:102)M and vice versa. In addition,
for a pair of local modes (mj, (cid:101)mj) : mj ∈ M, (cid:101)mj ∈ (cid:102)M , their distance is bounded
by (cid:107)mj − (cid:101)mj(cid:107) ≤ σ
3 .

Now we pick ((cid:15), δ) such that they satisfy equation (47). Then when (cid:107)f −
3,max is suﬃciently small, by Lemma 14, for every x ∈ H((cid:15), δ) we have

(cid:101)f (cid:107)∗

(cid:107) lim
t→∞

πx(t) − lim

t→∞ (cid:101)πx(t)(cid:107) ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∞ ≤ c∗

(cid:107)f − (cid:101)f (cid:107)∗

3,max.

(cid:113)

(cid:113)

Thus, whenever

(cid:107)f − (cid:101)f (cid:107)∗

3,max ≤

1
c2
∗

(cid:16) σ
3

(cid:17)2

,

πx(t) and (cid:101)πx(t) leads to the same pair of modes. Namely, the boundaries (cid:101)B
will not intersect the region H((cid:15), δ). And it is obvious that (cid:101)B cannot intersect
B(M,

(cid:15)). To conclude,

√

(54)

(55)

(56)

(cid:101)B ∩ H((cid:15), δ) = ∅
√

(cid:101)B ∩ B(M,

(cid:15)) = ∅
⇒ (cid:101)B ∩ K(δ) = ∅,
(cid:15)).

√

because by deﬁnition, K(δ) = H((cid:15), δ) ∩ B(M,

Thus, (cid:101)B ⊂ K(δ)C = B ⊕ δ, which implies Haus(B, (cid:101)B) ≤ δ < Hmin

(note

d2(cid:107)f (cid:107)3,max

that δ < δ0 ≤ Hmin

d2(cid:107)f (cid:107)3,max

appears in equation (47) and Lemma 13).

Part 2: Rate of convergence. To derive the convergence rate, we use proof
by contradiction. Let q ∈ B, (cid:101)q ∈ (cid:101)B a pair of points such that their distance
attains the Hausdorﬀ distance Haus
(cid:101)B, B

(cid:17)

(cid:16)

. Namely, q and (cid:101)q satisfy
(cid:16)

(cid:17)

(cid:107)q − (cid:101)q(cid:107) = Haus

(cid:101)B, B

Chen et al./Inference using the Morse-Smale

37

and either q is the projected point from (cid:101)q onto B or (cid:101)q is the projected point
from q onto (cid:101)B.

Recall that V(x) is the normal space to B at x ∈ B and we deﬁne (cid:101)V(x)
similarly for x ∈ (cid:101)B. An important property of the pair q, (cid:101)q is that q − (cid:101)q ∈
V(q), (cid:101)V((cid:101)q). If this is not true, we can slightly perturb q (or (cid:101)q) on B (or (cid:101)B) to
get a projection distance larger than the Hausdorﬀ distance, which leads to a
contradiction.

Now we choose x to be a point between q, (cid:101)q such that x = 1

3 (cid:101)q. We
(cid:107)(cid:101)q−x(cid:107) . Then e(x) ∈ V(q) and (cid:101)e(x) ∈ (cid:101)V((cid:101)q) and

3 q + 2

(cid:107)q−x(cid:107) and (cid:101)e(x) = (cid:101)q−x

deﬁne e(x) = q−x
e(x) = −(cid:101)e(x).

By Lemma 10 (second assertion),

(57)

(58)

(cid:96)(x) = e(x)T g(x) ≥

Hmin(cid:107)q − x(cid:107) > 0

(cid:101)(cid:96)(x) = (cid:101)e(x)T

(cid:101)g(x) ≥

(cid:101)Hmin(cid:107)(cid:101)q − x(cid:107) > 0.

1
2
1
2

Thus, for every x between q, (cid:101)q,
e(x)T g(x) > 0,

, e(x)T

(cid:101)g(x) = −(cid:101)e(x)T

(cid:101)g(x) < 0.

Note that we can apply Lemma 10 to (cid:101)f and its gradient because when (cid:107)f − (cid:101)f (cid:107)∗
2
is suﬃciently small, the assumption (D) holds for (cid:101)f as well.

To get the upper bound of (cid:107)q−(cid:101)q(cid:107) = Haus( (cid:101)B, B), note that (cid:107)q−x(cid:107) = 2

3 (cid:107)q−(cid:101)q(cid:107),

so

e(x)T

(cid:101)g(x) = e(x)T ((cid:101)g(x) − g(x)) + e(x)T g(x)
≥ e(x)T g(x) − (cid:107) (cid:101)f − f (cid:107)1,max

Hmin(cid:107)q − x(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max

(By Lemma 10)

(59)

≥

=

1
2
1
3

Hmin(cid:107)q − (cid:101)q(cid:107) − (cid:107) (cid:101)f − f (cid:107)1,max.

Thus, as long as

we have e(x)T
that

(cid:3)

Haus( (cid:101)B, B) = (cid:107)q − (cid:101)q(cid:107) > 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

,

(cid:101)g(x) > 0, a contradiction to equation (58). Hence, we conclude

Haus( (cid:101)B, B) ≤ 3

(cid:107) (cid:101)f − f (cid:107)1,max
Hmin

(cid:16)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

.

Proof of Theorem 3.
To prove the asymptotic rate of the rand index, we assume that for every local
mode of p, there exists one and only one local mode of (cid:98)pn that is close to the

Chen et al./Inference using the Morse-Smale

38

speciﬁc mode of p. By Lemma 9, this is true when (cid:107)(cid:98)pn − p(cid:107)∗
3,max is suﬃciently
small. Thus, after relabeling, the local mode (cid:98)m(cid:96) of (cid:98)pn is an estimator to the
local mode m(cid:96) of p. Let (cid:99)W(cid:96) be the basin of attraction to (cid:98)m(cid:96) using ∇(cid:98)pn and W(cid:96)
be the basin of attraction to m(cid:96) using ∇p. Let A(cid:52)B = {x : x ∈ A, x /∈ B} ∪ {x :
x ∈ B, x /∈ A} be the symmetric diﬀerence between sets A and B. The regions

En =

(cid:99)W(cid:96)(cid:52)W(cid:96)

(cid:17)

⊂ K

(cid:91)

(cid:16)

(cid:96)

(60)

are where the two mode clustering disagree with each other. Note that En are
regions between the two boundaries (cid:98)Bn and B

Given a pair of points Xi and Xj,

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj) =⇒ Xi or Xj ∈ En.

(61)

By the deﬁnition of rand index (30),

1 − rand ((cid:98)pn, p) =

(cid:19)−1

(cid:88)

(cid:16)

(cid:18)n
2

i,j

1

Ψ(Xi, Xj) (cid:54)= (cid:98)Ψn(Xi, Xj)

(62)

(cid:17)

Thus, if we can bound the ratio of data points within En, we can bound the
rate of rand index.

Since K is compact and p has bounded second derivatives, the volume of En

is bounded by

Vol(En) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Note Vol(A) denotes the volume (Lebesgue measure) of a set A. We now con-
struct a region surrounding B such that

and

En ⊂ B ⊕ Haus( (cid:98)Bn, B) = Vn

Vol(Vn) = O

Haus( (cid:98)Bn, B)

.

(cid:16)

(cid:17)

Now we consider a collection of subsets of K:

V = {B ⊕ r : R > r > 0},

(cid:80)n

where R < ∞ is the diameter for K. For any set A ⊂ K, let P (Xi ∈ A) and
(cid:98)Pn(A) = 1
i=1 1(Xi ∈ A) denote the probability of an observation within A
n
and the empirical estimate for that probability, respectively. It is easy to see
that Vn ∈ V for all n and the class V has a ﬁnite VC dimension (actually, the
VC dimension is 1). By the empirical process theory (or so-called VC theory,
see e.g. Vapnik and Chervonenkis (1971)),

(cid:12)
(cid:12)
(cid:12)P (Xi ∈ A) − (cid:98)Pn(A)

(cid:12)
(cid:12)
(cid:12) = OP

sup
A∈V

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(63)

(64)

(65)

(66)

(67)

Chen et al./Inference using the Morse-Smale

39

Thus,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)P (Xi ∈ Vn) − (cid:98)Pn(Vn)
(cid:12) = OP

(cid:32)(cid:114)

(cid:33)

.

log(n)
n

(68)

Now by equations (61) and (62),

1 − rand ((cid:98)pn, p) ≤ 8 (cid:98)Pn(En) ≤ 8 (cid:98)Pn(Vn) ≤ 8P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n

. (69)

Therefore,

1 − rand ((cid:98)pn, p) ≤ P (Xi ∈ Vn) + OP

(cid:32)(cid:114)

(cid:33)

log(n)
n
(cid:32)(cid:114)

(cid:33)

log(n)
n

(cid:32)(cid:114)

(cid:33)

log(n)
n

p(x) × Vol(Vn) + OP

≤ sup
x∈K

(cid:16)

≤ O

Haus( (cid:98)Bn, B)

+ OP

(cid:17)

= O (cid:0)h2(cid:1) + OP

(cid:32)(cid:114)

(cid:33)

,

log(n)
nhd+2

which completes the proof. Note that we apply Theorem 2 in the last equality.

(cid:3)

Proof of Theorem 4. Let (X1, Y1), · · · , (Xn, Yn) be the observed data.
Let (cid:98)E(cid:96) denote the d-cell for the nonparametric pilot regression estimator (cid:98)mn.
With I(cid:96) = {i : Xi ∈ (cid:98)E(cid:96)}, we deﬁne X(cid:96) as the matrix with rows Xi, i ∈ I(cid:96) and
similarly we deﬁne Y(cid:96).

We deﬁne X0,(cid:96) to be the matrix similar to X(cid:96) except that the row elements
are those Xi within E(cid:96), the d-cell deﬁned on true regression function m. We
also deﬁne Y0,(cid:96) to be the corresponding Yi.

By the theory of linear regression, the estimated parameters (cid:98)µ(cid:96), (cid:98)β(cid:96) have a

closed form solution:

Similarly, we deﬁne

((cid:98)µ(cid:96), (cid:98)β(cid:96))T = (XT

(cid:96)

X(cid:96))−1XT
(cid:96)

Y(cid:96).

((cid:98)µ0,(cid:96), (cid:98)β0,(cid:96))T = (XT

0,(cid:96)

X0,(cid:96))−1XT
0,(cid:96)

Y0,(cid:96)

as the estimated coeﬃcients using X0,(cid:96) and Y0,(cid:96).

As (cid:107) (cid:101)m − m(cid:107)∗

3,max is small, by Theorem 3, the number of rows at which
X(cid:96) and X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max). This is because an

(70)

(71)

(72)

Chen et al./Inference using the Morse-Smale

40

observation (a row vector) that appears only in one of X(cid:96) and X0,(cid:96) is those
fallen within either (cid:98)E(cid:96) or E(cid:96) but not both. Despite the fact that Theorem 3
is for basins of attraction (d-descending manifolds) of local modes, it can be
easily generalized to 0-ascending manifolds of local minima under assumption
(A). Thus, the similar bound holds for d-cells as well. Thus, we conclude that
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
since (X(cid:96), Y(cid:96)) and (X0,(cid:96), Y0,(cid:96)) only diﬀer by O(n × (cid:107) (cid:98)mn − m(cid:107)1,max) elements.
Thus,

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

= O((cid:107) (cid:98)mn − m(cid:107)1,max)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

1
n
1
n

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(73)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)µ0,(cid:96) − (cid:98)µ(cid:96), (cid:98)β0,(cid:96) − (cid:98)β(cid:96))
(cid:13)∞

=

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
(cid:13)
n
(cid:13)

XT
0,(cid:96)

X0,(cid:96)

XT
0,(cid:96)

Y0,(cid:96) −

XT
(cid:96)

X(cid:96)

(cid:19)−1 1
n

(cid:18) 1
n

(cid:19)−1 1
n

XT
(cid:96)

Y(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= O((cid:107) (cid:98)mn − m(cid:107)1,max),

which implies.

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − (cid:98)µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − (cid:98)β(cid:96)(cid:107)

= O((cid:107) (cid:98)mn − m(cid:107)1,max).

Now by the theory of linear regression,

max

(cid:111)
(cid:110)
(cid:107)(cid:98)µ0,(cid:96) − µ(cid:96)(cid:107), (cid:107) (cid:98)β0,(cid:96) − β(cid:96)(cid:107)

= OP

(cid:18) 1
√
n

(cid:19)

.

(74)

(75)

(76)

Thus, combining (75) and (76) and use the fact that all the above bounds are
uniform over each cell, we have proved that the parameters converge at rate
O((cid:107) (cid:98)mn − m(cid:107)1,max) + OP

(cid:16) 1√

(cid:17)

n

.

For points within the regions where E(cid:96) and (cid:98)E(cid:96) agree with each other, the rate
of convergence for parameter estimation translates into the rate of (cid:98)mn,MSR −
mMSR. The regions that E(cid:96) and (cid:98)E(cid:96) disagree to each other, denoted as Nn, have
Lebesgue O((cid:107) (cid:98)mn − m(cid:107)1,max) by Theorem 1. Thus, we have completed the proof.

(cid:3)

Proof of Theorem 5. The proof of Theorem 5 is nearly identical to the
proof of Theorem 4. The only diﬀerence is that the number of rows that X(cid:96) and
X0,(cid:96) diﬀer is bounded by O(n × (cid:107) (cid:98)mn − m(cid:107)β
1,max) due to the low noise condition
(36). Thus, equation (73) becomes
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O((cid:107) (cid:98)mn − m(cid:107)β

= O((cid:107) (cid:98)mn − m(cid:107)β

1
n
1
n

1
n
1
n

1,max)

1,max)

X(cid:96) −

Y(cid:96) −

XT
0,(cid:96)

XT
0,(cid:96)

X0,(cid:96)

Y0,(cid:96)

(77)

XT
(cid:96)

XT
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Chen et al./Inference using the Morse-Smale

41

so the parameter estimation error (76) is O((cid:107) (cid:98)mn − m(cid:107)β

1,max) + OP

(cid:16) 1√

(cid:17)

.

n

Under assumption (K1–2) and using Theorem 8 (the same result works for

kernel regression),

O((cid:107) (cid:98)mn − m(cid:107)1,max) = O(h2) + OP

(cid:32)(cid:114)

(cid:33)

.

log n
nhd+2

Thus, with the choice that h = O
(cid:17)2/(d+6)(cid:19)

(cid:18)(cid:16) log n

OP

n

(cid:3)

, which proves equation (37).

(cid:18)(cid:16) log n

(cid:17)1/(d+6)(cid:19)

n

, we have O((cid:107) (cid:98)mn−m(cid:107)1,max) =

Proof of Theorem 6.
We ﬁrst derive the explicit form of the parameters (η†

Note that the parameters are obtained by (14):

(cid:96) , γ†

(cid:96) ) within cell E(cid:96).

(η†

(cid:96) , γ†

(cid:96) ) = argmin

η,γ

E(cid:96)

(cid:90)

(cid:0)f (x) − η − γT x(cid:1)2

dx.

Now we deﬁne a random variable U(cid:96) ∈ Rd that is uniformly distributed over E(cid:96).
Then equation (14) is equivalent to

(η†

(cid:96) , γ†

(cid:96) ) = argmin

E

η,γ

(cid:16)(cid:0)f (U(cid:96)) − η − γT U(cid:96)

(cid:1)2(cid:17)

.

The analytical solution to the above problem is

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:19)

(cid:18)

=

1

E(U(cid:96))T
E(U(cid:96)) E(U(cid:96)U T
(cid:96) )

(cid:19)−1 (cid:18) E(f (U(cid:96)))
E(U(cid:96)f (U(cid:96)))

(cid:19)

Now we consider another smooth function (cid:101)f that is close to f such that
(cid:107) (cid:101)f − f (cid:107)∗
3,max is small so we can apply Theorem 1 to obtain consistency for both
descending d-manifolds and ascending 0-manifolds. Note that by Lemma 9, all
the critical points are close to each other and after relabeling, each d-cell E(cid:96) of
f is estimated by another d-cell (cid:101)E(cid:96) of (cid:101)f . Theorem 1 further implies that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)Leb( (cid:101)E(cid:96)) − Leb(E(cid:96))
(cid:12) = O
(cid:17)
(cid:16)

(cid:16)

(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:17)

(cid:17)

Leb

(cid:101)E(cid:96)(cid:52)E(cid:96)

= O

(cid:107) (cid:101)f − f (cid:107)1,max

,

(78)

(79)

(80)

where Leb(A) is the Lebesgue measure for set A and A(cid:52)B = (A\B) ∪ (B\A) is

Chen et al./Inference using the Morse-Smale

42

the symmetric diﬀerence. By simple algebra, equation (80) implies that

(cid:107)E( (cid:101)U(cid:96)) − E(U(cid:96))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)U T

(cid:96) ) − E(U(cid:96)U T

|E( (cid:101)f ( (cid:101)U(cid:96))) − E(f (U(cid:96)))| = O

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:96) )(cid:107)∞ = O
(cid:16)

(cid:107) (cid:101)f − f (cid:107)1,max
(cid:17)

(cid:107) (cid:101)f − f (cid:107)∗
(cid:16)

1,max

(cid:107)E( (cid:101)U(cid:96) (cid:101)f ( (cid:101)U(cid:96))) − E(U(cid:96)f (U(cid:96)))(cid:107)∞ = O

(cid:107) (cid:101)f − f (cid:107)∗

1,max

(cid:17)

.

(81)

By (81) and the analytic solution to ((cid:101)η†
(cid:101)η†
(cid:101)γ†

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:18) η†
(cid:96)
γ†
(cid:96)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:19)

(cid:18)

(cid:96)

(cid:96)

(cid:96) , (cid:101)γ†
(cid:16)
(cid:107) (cid:101)f − f (cid:107)∗

= O

(cid:17)

.

1,max

(cid:96) ) from (79), we have proved

(82)

Since the bound does not depend on the cell indices (cid:96), (82) holds uniformly for
all (cid:96) = 1, · · · , K.

(cid:3)

References

E. Arias-Castro, D. Mason, and B. Pelletier. On the estimation of the gradient
lines of a density and the consistency of the mean-shift algorithm. Journal of
Machine Learning Research, 17(43):1–28, 2016.

J.-Y. Audibert, A. B. Tsybakov, et al. Fast learning rates for plug-in classiﬁers.

The Annals of statistics, 35(2):608–633, 2007.

M. Azizyan, Y.-C. Chen, A. Singh, and L. Wasserman. Risk bounds for mode

clustering. arXiv preprint arXiv:1505.00482, 2015.

A. Azzalini and N. Torelli. Clustering via nonparametric density estimation.

Statistics and Computing, 17(1):71–80, 2007.

P. Bacchetti. Additive isotonic models. Journal of the American Statistical

A. Banyaga and D. Hurtubise. Lectures on Morse homology, volume 29. Springer

Association, 84(405):289–294, 1989.

Science & Business Media, 2004.

L. Baringhaus and C. Franz. On a new multivariate two-sample test. Journal

of multivariate analysis, 88(1):190–206, 2004.

R. E. Barlow, D. J. Bartholomew, J. Bremner, and H. D. Brunk. Statistical
inference under order restrictions: the theory and application of isotonic re-
gression. Wiley New York, 1972.

R. Bhatia. Matrix Analysis. Springer, 1997.
G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business

Media, 1993.

R. R. Brinkman, M. Gasparetto, S.-J. J. Lee, A. J. Ribickas, J. Perkins,
W. Janssen, R. Smiley, and C. Smith. High-content ﬂow cytometry and
temporal data analysis for deﬁning a cellular signature of graft-versus-host
disease. Biology of Blood and Marrow Transplantation, 13(6):691–700, 2007.

Chen et al./Inference using the Morse-Smale

43

J. Chac´on and T. Duong. Data-driven density derivative estimation, with ap-
plications to nonparametric clustering and bump hunting. Electronic Journal
of Statistics, 7:499–532, 2013.

J. Chac´on, T. Duong, and M. Wand. Asymptotics for general multivariate kernel

density derivative estimators. Statistica Sinica, 2011.

J. E. Chac´on et al. A population background for nonparametric density-based

clustering. Statistical Science, 30(4):518–532, 2015.

F. Chazal, B. T. Fasy, F. Lecci, B. Michel, A. Rinaldo, and L. Wasserman. Ro-
bust topological inference: Distance to a measure and kernel distance. arXiv
preprint arXiv:1412.7197, 2014.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. Asymptotic theory for density

ridges. The Annals of Statistics, 43(5):1896–1928, 2015.

Y.-C. Chen, C. R. Genovese, L. Wasserman, et al. A comprehensive approach
to mode clustering. Electronic Journal of Statistics, 10(1):210–241, 2016.
Y. Cheng. Mean shift, mode seeking, and clustering. Pattern Analysis and

Machine Intelligence, IEEE Transactions on, 17(8):790–799, 1995.

D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence dia-

grams. Discrete & Computational Geometry, 37(1):103–120, 2007.

D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space
analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
24(5):603–619, 2002.

T. Duong. Local signiﬁcant diﬀerences from nonparametric two-sample tests.

Journal of Nonparametric Statistics, 25(3):635–645, 2013.

T. Duong et al. ks: Kernel density estimation and kernel discriminant analysis
for multivariate data in r. Journal of Statistical Software, 21(7):1–16, 2007.
B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statis-

tics, 7(1):1–26, 1979.

U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type

function estimators. The Annals of Statistics, 2005.

K. Fukunaga and L. Hostetler. The estimation of the gradient of a density
function, with applications in pattern recognition. Information Theory, IEEE
Transactions on, 21(1):32–40, 1975.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, and L. Wasserman. The
geometry of nonparametric ﬁlament estimation. Journal of the American
Statistical Association, 107(498):788–799, 2012.

C. R. Genovese, M. Perone-Paciﬁco, I. Verdinelli, L. Wasserman, et al. Non-
parametric ridge estimation. The Annals of Statistics, 42(4):1511–1545, 2014.
S. Gerber and K. Potter. Data analysis with the morse-smale complex: The msr

package for r. Journal of Statistical Software, 2011.

S. Gerber, P.-T. Bremer, V. Pascucci, and R. Whitaker. Visual exploration
of high dimensional scalar functions. Visualization and Computer Graphics,
IEEE Transactions on, 16(6):1271–1280, 2010.

S. Gerber, O. R¨ubel, P.-T. Bremer, V. Pascucci, and R. T. Whitaker. Morse–
smale regression. Journal of Computational and Graphical Statistics, 22(1):
193–214, 2013.

E. Gine and A. Guillou. Rates of strong uniform consistency for multivari-

Chen et al./Inference using the Morse-Smale

44

ate kernel density estimators. In Annales de l’Institut Henri Poincare (B)
Probability and Statistics, 2002.

S. Helgason. Diﬀerential geometry, Lie groups, and symmetric spaces, vol-

ume 80. Academic press, 1979.

L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation, 2(1):

193–218, 1985.

J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a

nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.

J. Li, S. Ray, and B. G. Lindsay. A nonparametric statistical approach to
clustering via mode identiﬁcation. Journal of Machine Learning Research,
2007.

J. W. Milnor. Morse theory. Number 51. Princeton university press, 1963.
M. Morse. Relations between the critical points of a real function of n indepen-
dent variables. Transactions of the American Mathematical Society, 27(3):
345–396, 1925.

M. Morse. The foundations of a theory of the calculus of variations in the
large in m-space (second paper). Transactions of the American Mathematical
Society, 32(4):599–631, 1930.

E. A. Nadaraya. On estimating regression. Theory of Probability & Its Appli-

cations, 9(1):141–142, 1964.

S. Paris and F. Durand. A topological approach to hierarchical segmentation us-
ing mean shift. In Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007.

W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal

of the American Statistical association, 66(336):846–850, 1971.

A. Rinaldo, L. Wasserman, et al. Generalized density clustering. The Annals of

Statistics, 38(5):2678–2722, 2010.

A. Rinaldo, A. Singh, R. Nugent, and L. Wasserman. Stability of density-based
clustering. The Journal of Machine Learning Research, 13(1):905–948, 2012.
M. Rizzo and G. Szekely. energy: E-statistics (energy statistics). R package

version, 1:1, 2008.

M. L. Rizzo, G. J. Sz´ekely, et al. Disco analysis: A nonparametric extension of
analysis of variance. The Annals of Applied Statistics, 4(2):1034–1055, 2010.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chap-

man and Hall, 1986.

A. Singh, C. Scott, R. Nowak, et al. Adaptive hausdorﬀ estimation of density

level sets. The Annals of Statistics, 37(5B):2760–2782, 2009.

G. J. Sz´ekely and M. L. Rizzo. Testing for equal distributions in high dimension.

InterStat, 5, 2004.

G. J. Szekely and M. L. Rizzo. Hierarchical clustering via joint between-within
distances: Extending ward’s minimum variance method. Journal of classiﬁ-
cation, 22(2):151–183, 2005.

G. J. Sz´ekely and M. L. Rizzo. A new test for multivariate normality. Journal

of Multivariate Analysis, 93(1):58–80, 2005.

G. J. Sz´ekely and M. L. Rizzo. Energy statistics: A class of statistics based
on distances. Journal of statistical planning and inference, 143(8):1249–1272,

Chen et al./Inference using the Morse-Smale

45

2013.

V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of rela-
tive frequencies of events to their probabilities. Theory of Probability & Its
Applications, 16(2):264–280, 1971.

A. Vedaldi and S. Soatto. Quick shift and kernel methods for mode seeking. In
European Conference on Computer Vision, pages 705–718. Springer, 2008.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clus-
terings comparison: is a correction for chance necessary? In Proceedings of
the 26th Annual International Conference on Machine Learning, pages 1073–
1080. ACM, 2009.

L. Wasserman. All of nonparametric statistics. Springer, 2006.

