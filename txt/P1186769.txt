A Simple yet Effective Joint Training Method for
Cross-Lingual Universal Dependency Parsing

Danlu Chen*1, Mengxiao Lin*12, Zhifeng Hu*1, Xipeng Qiu1
1Fudan University
2Megvii Inc
danluchen@fb.com, linmengxiao@megvii.com, {zfhu16,xipeng}@fudan.edu.cn

Abstract

This paper describes Fudan’s submission
to CoNLL 2018’s shared task Universal
Dependency Parsing. We jointly train
models when two languages are simi-
lar according to linguistic typology and
then do an ensemble of the models us-
ing a simple re-parse algorithm. Our
system outperforms the baseline method
by 4.4% and 2.1% on the develop-
ment and test set of CoNLL 2018 UD
Shared Task, separately.1. Our code is
available on https://github.com/
taineleau/FudanParser.

1

Introduction

Dependency Parsing has been a fundamental task
in Natural Language Processing (NLP). Recently,
universal dependency parsing (Zeman et al.,
2018a,b; Nivre et al., 2018) has uniﬁed the an-
notations of different languages and thus made
transfer learning among languages possible. Sev-
eral works using cross-lingual embedding (Duong
et al., 2015; Guo et al., 2015) have successfully in-
creased the accuracy of cross-lingual parsing. Be-
yond embedding-based methods, a natural ques-
tion is whether we can use a simple way to utilize
the universal information. Some previous research
either regarded the universal information as ex-
tra training signals (e.g., delexicalized embedding
(Dehouck and Denis, 2017)), or implicitly trained
a network with all features (e.g., adversarial train-
ing for parsing in Sato et al. (2017)). In our sys-
tem, we manually and explicitly share the univer-
sal annotations via a shared LSTM component.

* Authors contributed equally.
1Unfortunately, we did not ﬁnish the run before the dead-
line. As a result, the ofﬁcial accuracy gain for test set is only
0.54% and we ranks 17th out of 27 teams.

Similar to Vania et al. (2017), different lan-
guages are ﬁrst grouped based on typology, as
shown in table 1. Then, we train a shared model
for each pair of languages within the same group,
and apply a simple ensemble method over all
trained models. Note that our method is orthogo-
nal to other cross-lingual approaches for universal
parsing such as cross-lingual embedding.

In the following parts, we ﬁrst describe the
baseline method (Section 2) and our system (Sec-
tion 3). We show the result on both development
set and test set in Section 4 and provide some anal-
ysis of the model in Section 5.

2 Baseline

In this section, we brieﬂy introduce the baseline
system, UDPipe 1.2 (Straka and Strakov´a, 2017),
which is an improved version of original UDPipe
(Straka et al., 2016). The tokenizing, POS tagging
and lemma outputs of UDPipe are utilized by Fu-
danParser.

UDPipe employs a GRU network during the in-
ference of segmentation and tokenization. The
tagger uses characters features to predict the POS
and lemma tags. Finally, a transition-based neu-
ral dependency parser with one hidden layer pre-
dicts the transition actions. The parser also makes
use of the information from lemmas, POS taggings
and dependency relationships through a group of
embeddings precomputed by word2vec.

In the later discussion, we take the baseline per-
formance result from the web page of the shared
task 2 for comparison.

3 System Description

In this submission, we only consider parsing in an
end-to-end manner and handle each treebank sep-

2http://universaldependencies.org/

conll18/baseline.html

Group
germanic

Datasets
Afrikaans-AfriBooms Danish-
DDT Dutch-Alpino Dutch-
LassySmall
English-EWT
English-
English-GUM
LinES German-GSD Gothic-
PROIEL Norwegian-Bokmaal
Norwegian-Nynorsk Swedish-
LinES Swedish-Talbanken

latin

romance

Persian-Seraji

Latin-PROIEL

indo-iranian Hindi-HDTB
Urdu-UDTB
Latin-ITTB
Latvian-LVTB
Catalan-AnCora
French-GSD
French-Sequoia French-Spoken
Italian-ISDT
Galician-CTG
Italian-PoSTWITA Old French-
Portuguese-Bosque
SRCMF
Spanish-
Romanian-RRT
AnCora
Arabic-PADT Hebrew-HTB
Bulgarian-BTB
SET
FicTree
Old Church Slavonic-PROIEL
Polish-LFG Polish-SZ Russian-
Serbian-SET
SynTagRus
Slovak-SNK Slovenian-SSJ
Turkish-IMST
Uyghur-UDT
Estonian-EDT
Finnish-TDT

Croatian-
Czech-
Czech-PDT

semitic
slavic

Ukrainian-IU

Finnish-FTB

Czech-CAC

turkish

uralic

Table 1: Grouping languages according to typol-
ogy.

arately. We ﬁrst train a monotonic model for all
“big” treebanks. Besides, for each language, there
are N −1 models ﬁne-tuned from joint-trained (see
Figure 2), where N is the number of languages in
the same language group.

For small treebanks where training set is less
than 50 sentences, we use the delexicalized
method the same as Shi et al. (2017)’s approach
for the surprise languages. Shi et al. (2017) took
delexicalized features (morphology and POS tag)
as input and apply 50% dropout rate to the input.
In practice, we found that the baseline method per-
forms much better than ours on“ﬁ pud”, “br keb”
“ja modern” and “th pud”, so we use the baseline
method instead for these languages.

Our whole system needs about 90 hours to do
the inference of all models on TIRA and requires
no more than 560M main memory.

3.1 Architecture

Features We use words, characters as the lex-
ical information, and use morphological features3
and POS tags as the delexicalized information. We
also tried subword embeddings, but it mostly did
not help. More precisely, the character-level fea-
tures are treated as bag-of-characters. Similarly,
we use bag-of-morphology for morphological fea-
tures (one can see number=single as a charac-
ter). We ﬁrst assign the embedding vectors for
characters and morphological features, and then
for each word, we apply a Convolutional Network
(CNN) to encode variable length embeddings into
one ﬁxed length feature.

Biafﬁne BiLSTM. Similar to Shi et al. (2017);
Sato et al. (2017); Vania et al. (2017), we use last
year’s ﬁrst-place model (Dozat et al., 2017), the
graph-based biafﬁne bizLSTM model as our back-
bone. Given a sentence of N words, the input is
ﬁrst fed to a bi-directional LSTM and obtain the
feature of each word wi. A head MLP and a de-
pendent MLP are used to translate the features,
which is then fed into a hidden layer to calculate
the biafﬁne attention. Finally, we are able to com-
pute the score of arcs and labels in following way:

the

take

features

3we
the morphological

as
number,
tense, mood and so on.
//universaldependencies.org/u/feat/
index.html for detailed information.

column of

the UD data
features, which includes case,
See http:

Figure 1: An illustration of the joint training framework for two languages.

hh
i = MLPhead(wi)
hd
i = MLPdep(wi)
si = H hU1hd

i + H hu2

where U1 ∈ Rd×d and u2 ∈ Rd are trainable pa-
rameters.

3.2

Joint Training

For a joint training model of N languages, we have
N +1 Biaffne Bi-LSTMs (called LSTMs), see Fig-
ure 1. For each language, we have a language-
speciﬁc LSTM to process the lexical information
such as word- or character- level embedding, and
the output is wl
i,j. For all languages we have a
shared LSTM which takes delexicalized informa-
tion such as morphology and POS tags as input
and the output is wd
Inspired by Sato et al.
(2017), we use a gating mechanism to combine
these two set of features. Formally,

i,j.

x = [wl

i,j; wd

i,j],

g = G(x), y = x (cid:12) g,

where wl indicates lexical feature, wd indicates
delexicalized feature, and (cid:12) is element-wise mul-
tiplication.

The difference between Sato et al. (2017) and
ours is that we remove the adversarial training
loss, which is because we have already use the uni-
versal information in the shared network.

3.3 Fine-tuning

We ﬁne-tunning each joint-training model for 100
steps (see Figure 2).

3.4 Tree Ensemble

We follow the re-parsing method proposed in
Sagae and Lavie (2006) to perform model en-
semble. Suppose k parsing trees have been ob-
tained, denoted by T1, T2, ...Tk, a new graph is
constructed by setting the score of each edge to

S[u → v] =

[u → v] ∈ Tk

k
(cid:88)

i=1

This graph is feed to a MST algorithm to get the
ensemble parsing tree Te. Then the relation label
of edge [u → v] in Te is voted by all inputs Ti that
contains edge [u → v].

3.5 Hyper-parameters

We followed the hyper-parameter settings in
(Dozat et al., 2017). We train 30, 000 steps for
each model and then ﬁne-tune (onot necessary) for
100 steps for the given language. For all the in-
put features, the dimension is 100. For LSTM, we
use hidden size equals to 400 and the number of
layers is 3. 0.33% dropout rate is applied to the
input and LSTM hidden layer. We use Bayesian
dropout (Gal and Ghahramani, 2016) in the LSTM
layers. We also use word dropout (dropping the
whole word with a probability) in the input layer.

4 Results

The results of the test and development set are
shown in Table 5 and Table 6, respectively. The
ﬁrst three columns are the baseline results and the
second three columns are the results of our sub-
mission. Also, we list the performance improve-
ment of Fudan Parser compared to the baseline
system in the last three columns.

Figure 2: Take four languages as an example. We aim at testing sentence in language 1. We ﬁrst jointly
train languages 1 and other three languages in three separate network. And then we only keep LSTM 1
and the shared LSTM part to ﬁne tune the models for language 1. Finally we re-parse it as an ensemble
to obtain the ﬁnal parsing tree for a given sentence in language 1.

As shown in both Table 6 and 6, we ﬁnd that
our system achieves higher improvements on the
datasets with large size of training data. It is rea-
sonable since our model contains enormous pa-
rameters, which is easy to get overﬁtting if the
training set is too small. More analysis are in-
cluded in Section 5.

5 Analysis

5.1 Language similarity

The accuracy of the joint training model actually
reveals the syntactic similarity between two lan-
guages. The accuracy of three language groups,
Slavic (Table 2), Romance (Table 3) and Germanic
(Table 4). A number in row i, column j means the
accuracy of language i testing on the model jointly
training on language i and language j. The bold
font indicates it is the best model for language i.
We can see that for every language, jointly trained
models consistently beat single models (the num-
ber on the diagonal) which shows the efﬁcacy of
the proposed approach.

5.2 Morphology

Morphology is extremely helpful when predict-
ing the dependency between words, especially for
those morphology rich languages. However, the
UD Parsing task is not done in an end-to-end fash-
ion (i.e. the input morphological features are not
the ground-true labels) and thus the morphology
information is noisy. The performance is hurt
greatly because of the noisy predicted morphology
features. A signiﬁcant accuracy gain should be ob-
tained if a better morphology prediction model is
used.

6 Conclusion

Our system provided a simple yet effective method
–sharing the universal features to the same part of
neural network– to boost the accuracy of syntactic
parsing. We also demonstrated that morphological
feature plays an important role in syntactic pars-
ing, which is a promising direction to work on.

In the future, we can investigate a better way to
do the ensemble or apply a multi-model compres-
sion method (e.g. knowledge distillation) to re-
duce the computational cost. Also, we can explore

Acc.(%)
bg
hr
cs
pl
ru
sk
sl
Avg.
# samples

Table 2: Slavic languages joint training result.
hr
92.5
86
91.2
89.8
84.7
86.2
91.8

sl
92.4
85.5
91.2
90.8
84.6
86.1
91.2

sk
92.3
85.8
91.3
90.4
84.1
86.7
91

ru
92.7
85.5
91.3
90.2
83.8
86.4
91.4

pl
92.7
85.2
91.1
90.1
84.4
85.9
91.4

cs
92.8
86.1
91.2
90.2
85.2
87.8
91.7

bg
92.6
85.7
91.2
90.4
84.4
86.4
91.4

8908

7690

68496

6101

3851

8484

6479

max improvement
0.2
0.1
0.1
0.3
1.4
1.1
0.6
0.54

ca
92.6
93.1
86.9
93
93
88.8
90.9

Acc. (%)
ca
fr
gl
it
pt
ro
es
Avg.
# samples

Table 3: Romance languages joint training result.

fr
92.4
92.9
86.3
92.6
92.8
88.9
90.8

gl
92.6
93
86.1
92.7
92.7
88.9
90.5

it
92.7
93.4
86.7
92.3
92.8
89
91.1

pt
92.6
93.2
86.4
93
92.6
88.7
91

ro
92.4
93.1
86.3
92.8
92.9
88.4
90.6

es
92.5
93.2
86.4
93.1
92.8
88.7
90.7

max improvement
0.1
0.5
0.8
0.8
0.4
0.6
0.4
0.56

13124

14554

2277

12839

8332

8044

14188

Acc. (%)
da
nl
en
de
sv
Avg.
# samples

Table 4: Germanic languages joint training result.
da
85
89
89.4
88.5
85.7

de
85.2
89.3
88.9
88.6
85.9

sv
85.6
88.6
89.1
88.5
85

nl
85.1
88.8
89.1
88.9
85

en
85
88.7
88.9
88.7
86.5

max improvement
0.6
0.5
0.5
0.3
1.5
0.68

4384

12331

12544

14119

4303

a more sophisticated model (e.g., Neural Architec-
ture Search (Zoph and Le, 2016)) for joint training
on multiple languages.

Language code
af afribooms
grc perseus
grc proiel
ar padt
hy armtdp
eu bdt
br keb
bg btb
bxr bdt
ca ancora
hr set
cs cac
cs ﬁctree
cs pdt
cs pud
da ddt
nl alpino
nl lassysmall
en ewt
en gum
en lines
en pud
et edt
fo oft
ﬁ ftb
ﬁ pud
ﬁ tdt
fr gsd
fr sequoia
fr spoken
gl ctg
gl treegal
de gsd
got proiel
el gdt
he htb
hi hdtb
hu szeged
zh gsd
id gsd
ga idt
it isdt
it postwita
ja gsd
ja modern
kk ktb
ko gsd
ko kaist
kmr mg
la ittb
la perseus
la proiel
lv lvtb
pcm nsc
sme giella
no bokmaal
no nynorsk
no nynorsklia
fro srcmf
cu proiel
fa seraji
pl lfg
pl sz
pt bosque
ro rrt
ru syntagrus
ru taiga
sr set
sk snk
sl ssj
sl sst
es ancora
sv lines
sv pud
sv talbanken
th pud
tr imst
uk iu
hsb ufal
ur udtb
ug udt
vi vtb

LAS
77.88%
57.75%
67.57%
66.41%
21.79%
70.13%
10.25%
84.91%
12.61%
85.61%
78.61%
83.72%
82.49%
83.94%
80.08%
75.43%
77.60%
74.56%
77.56%
74.20%
73.10%
79.56%
75.02%
25.19%
75.64%
80.15%
76.45%
81.05%
81.12%
65.56%
76.10%
66.16%
70.85%
62.16%
82.11%
57.86%
87.15%
66.76%
57.91%
74.37%
62.93%
86.26%
66.81%
72.32%
22.71%
24.21%
61.40%
70.25%
23.92%
75.95%
47.61%
59.66%
69.43%
12.18%
56.98%
83.47%
82.13%
48.95%
79.27%
65.46%
79.10%
87.53%
81.90%
82.07%
80.27%
84.59%
55.51%
82.07%
75.41%
77.33%
46.95%
84.43%
74.06%
70.63%
77.91%
0.70%
54.04%
74.91%
23.64%
77.29%
56.26%
39.63%

Baseline
MLAS
64.48%
31.05%
49.51%
55.01%
5.00%
57.65%
0.00%
75.30%
0.00%
76.74%
58.72%
70.89%
69.26%
74.32%
66.53%
65.41%
61.55%
61.85%
68.70%
62.66%
64.03%
67.59%
67.12%
0.00%
65.22%
73.16%
68.58%
72.16%
71.34%
53.46%
62.11%
49.13%
34.09%
48.57%
65.33%
44.09%
69.09%
52.82%
48.49%
63.42%
37.66%
77.06%
53.64%
58.35%
10.00%
10.00%
54.10%
61.49%
5.00%
66.08%
30.16%
47.05%
54.96%
5.00%
46.05%
74.65%
72.40%
37.60%
70.70%
53.96%
72.20%
74.54%
63.84%
67.40%
71.48%
76.87%
36.79%
70.04%
54.38%
63.47%
34.19%
76.01%
58.62%
43.38%
69.22%
0.03%
44.50%
56.78%
5.00%
50.31%
36.82%
33.49%

BLEX
66.60%
38.74%
55.85%
57.60%
11.94%
63.50%
0.00%
73.78%
5.00%
77.27%
70.26%
77.65%
74.96%
79.39%
73.79%
66.04%
64.76%
63.14%
71.02%
62.14%
65.42%
71.14%
63.85%
5.00%
61.76%
65.46%
62.19%
74.22%
74.41%
54.67%
65.29%
51.60%
60.56%
55.02%
68.67%
46.51%
79.93%
56.92%
52.92%
62.50%
42.06%
77.12%
53.99%
60.17%
10.00%
10.00%
50.50%
57.68%
11.86%
71.87%
32.19%
53.65%
58.25%
10.87%
42.35%
76.32%
74.22%
40.69%
74.45%
58.39%
69.43%
78.58%
71.98%
72.04%
71.87%
78.01%
39.79%
74.12%
60.35%
68.93%
38.73%
76.43%
66.39%
54.47%
70.01%
0.42%
45.91%
63.72%
11.72%
63.74%
43.53%
35.72%

LAS
80.02%
63.31%
69.54%
67.33%
26.24%
72.74%
10.25%
86.47%
12.61%
88.37%
81.01%
86.28%
85.22%
85.35%
81.05%
78.38%
79.02%
77.41%
78.44%
75.29%
74.83%
78.80%
77.80%
26.95%
78.27%
80.15%
79.18%
83.19%
83.39%
65.63%
80.38%
68.08%
71.88%
65.49%
82.56%
58.87%
88.43%
68.74%
60.13%
75.51%
64.87%
88.28%
67.58%
73.16%
22.71%
24.21%
74.94%
82.74%
23.92%
80.07%
49.99%
63.93%
70.89%
10.00%
61.58%
85.29%
84.09%
52.84%
82.70%
70.03%
79.57%
88.78%
83.54%
84.59%
82.67%
87.70%
57.94%
83.54%
78.45%
79.15%
46.19%
87.66%
75.87%
72.26%
80.00%
0.70%
57.57%
76.27%
29.92%
77.91%
55.88%
39.53%

Fudan
MLAS
67.34%
34.58%
51.35%
55.88%
10.00%
58.98%
0.00%
77.04%
0.00%
80.17%
60.92%
73.90%
72.01%
75.89%
67.97%
68.20%
63.89%
64.64%
68.99%
63.36%
65.78%
67.20%
69.82%
0.00%
68.03%
73.16%
70.74%
74.01%
73.59%
52.96%
67.42%
50.06%
35.12%
51.72%
65.58%
44.89%
70.48%
54.66%
49.17%
63.54%
39.22%
79.48%
53.93%
59.39%
10.00%
10.00%
68.34%
75.55%
5.00%
71.95%
31.35%
51.19%
56.14%
5.00%
49.88%
76.97%
74.71%
40.67%
75.06%
58.51%
71.96%
75.92%
65.25%
70.21%
74.11%
79.58%
38.59%
70.86%
56.57%
65.05%
33.61%
80.08%
59.96%
44.27%
70.66%
0.03%
46.59%
57.66%
10.00%
50.78%
35.84%
32.33%

BLEX
66.04%
38.22%
53.03%
58.63%
13.85%
57.41%
0.00%
77.70%
5.00%
66.01%
66.61%
79.54%
77.18%
74.68%
68.99%
62.51%
60.53%
49.81%
62.92%
57.45%
62.80%
64.26%
61.53%
5.00%
66.99%
65.46%
59.79%
68.58%
69.28%
52.82%
71.64%
52.80%
34.30%
54.63%
64.68%
47.37%
81.52%
53.52%
54.29%
71.50%
42.44%
72.47%
44.53%
60.92%
10.00%
10.00%
62.21%
69.47%
11.86%
76.29%
33.75%
54.64%
57.30%
5.00%
44.19%
70.82%
69.97%
43.70%
78.96%
63.28%
69.42%
77.55%
72.25%
62.91%
71.11%
82.35%
42.12%
66.69%
67.75%
69.10%
38.00%
68.03%
64.81%
52.52%
71.49%
0.42%
46.27%
63.11%
15.16%
64.30%
43.16%
32.07%

LAS
2.14%
5.56%
1.97%
0.92%
4.45%
2.61%
0.00%
1.56%
0.00%
2.76%
2.40%
2.56%
2.73%
1.41%
0.97%
2.95%
1.42%
2.85%
0.88%
1.09%
1.73%
-0.76%
2.78%
1.76%
2.63%
0.00%
2.73%
2.14%
2.27%
0.07%
4.28%
1.92%
1.03%
3.33%
0.45%
1.01%
1.28%
1.98%
2.22%
1.14%
1.94%
2.02%
0.77%
0.84%
0.00%
0.00%
13.54%
12.49%
0.00%
4.12%
2.38%
4.27%
1.46%
-2.18%
4.60%
1.82%
1.96%
3.89%
3.43%
4.57%
0.47%
1.25%
1.64%
2.52%
2.40%
3.11%
2.43%
1.47%
3.04%
1.82%
-0.76%
3.23%
1.81%
1.63%
2.09%
0.00%
3.53%
1.36%
6.28%
0.62%
-0.38%
-0.10%

Improvement
MLAS
2.86%
3.53%
1.84%
0.87%
5.00%
1.33%
0.00%
1.74%
0.00%
3.43%
2.20%
3.01%
2.75%
1.57%
1.44%
2.79%
2.34%
2.79%
0.29%
0.70%
1.75%
-0.39%
2.70%
0.00%
2.81%
0.00%
2.16%
1.85%
2.25%
-0.50%
5.31%
0.93%
1.03%
3.15%
0.25%
0.80%
1.39%
1.84%
0.68%
0.12%
1.56%
2.42%
0.29%
1.04%
0.00%
0.00%
14.24%
14.06%
0.00%
5.87%
1.19%
4.14%
1.18%
0.00%
3.83%
2.32%
2.31%
3.07%
4.36%
4.55%
-0.24%
1.38%
1.41%
2.81%
2.63%
2.71%
1.80%
0.82%
2.19%
1.58%
-0.58%
4.07%
1.34%
0.89%
1.44%
0.00%
2.09%
0.88%
5.00%
0.47%
-0.98%
-1.16%

BLEX
-0.56%
-0.52%
-2.82%
1.03%
1.91%
-6.09%
0.00%
3.92%
0.00%
-11.26%
-3.65%
1.89%
2.22%
-4.71%
-4.80%
-3.53%
-4.23%
-13.33%
-8.10%
-4.69%
-2.62%
-6.88%
-2.32%
0.00%
5.23%
0.00%
-2.40%
-5.64%
-5.13%
-1.85%
6.35%
1.20%
-26.26%
-0.39%
-3.99%
0.86%
1.59%
-3.40%
1.37%
9.00%
0.38%
-4.65%
-9.46%
0.75%
0.00%
0.00%
11.71%
11.79%
0.00%
4.42%
1.56%
0.99%
-0.95%
-5.87%
1.84%
-5.50%
-4.25%
3.01%
4.51%
4.89%
-0.01%
-1.03%
0.27%
-9.13%
-0.76%
4.34%
2.33%
-7.43%
7.40%
0.17%
-0.73%
-8.40%
-1.58%
-1.95%
1.48%
0.00%
0.36%
-0.61%
3.44%
0.56%
-0.37%
-3.65%

Table 5: All results on test set.

Language code
el gdt
tr imst
id gsd
da ddt
et edt
got proiel
sl ssj
en gum
cu proiel
ur udtb
fro srcmf
hi hdtb
ko gsd
cs ﬁctree
gl ctg
lv lvtb
fr gsd
ru syntagrus
hu szeged
sv lines
no bokmaal
sv talbanken
es ancora
he htb
uk iu
grc proiel
eu bdt
ﬁ ftb
cs pdt
sk snk
hr set
no nynorsk
grc perseus
fr spoken
pl sz
ﬁ tdt
ca ancora
ar padt
sr set
bg btb
vi vtb
de gsd
fr seguoia
cs cac
pl lfg
en lines
zh gsd
it postwita
la proiel
fa seraji
af afribooms
ko kaist
la ittb
en ewt
ug udt
pt bosque
ro rrt
nl lassysmall
it isdt
nl alpino
ja gsd

LAS
81.37%
54.83%
74.40%
75.16%
76.50%
62.03%
77.72%
76.63%
66.12%
77.44%
79.15%
87.26%
57.25%
83.16%
76.32%
70.67%
85.81%
83.87%
68.41%
76.23%
84.56%
75.39%
85.08%
61.95%
77.94%
69.13%
70.06%
75.76%
84.85%
75.73%
77.84%
82.75%
57.89%
65.09%
82.65%
76.39%
85.63%
66.81%
82.12%
84.67%
43.65%
75.55%
82.72%
84.42%
88.79%
75.78%
57.39%
65.85%
61.33%
79.78%
80.19%
71.00%
73.23%
77.62%
56.88%
84.93%
80.32%
73.61%
85.95%
80.21%
75.48%

Baseline
MLAS
63.92%
44.25%
63.51%
65.29%
68.27%
48.16%
63.96%
65.57%
54.48%
49.91%
70.43%
69.78%
49.06%
70.72%
62.58%
57.79%
77.80%
75.78%
56.47%
62.16%
75.95%
66.87%
76.81%
49.28%
59.66%
52.42%
57.46%
65.72%
75.35%
54.34%
59.60%
73.88%
30.80%
54.00%
63.92%
68.60%
77.04%
55.67%
69.12%
74.54%
37.39%
38.52%
74.13%
72.17%
75.15%
66.29%
48.19%
52.14%
48.40%
73.03%
65.98%
63.32%
59.94%
68.58%
37.43%
73.22%
71.21%
59.99%
77.20%
67.14%
62.39%

BLEX
65.21%
45.81%
63.29%
66.07%
64.17%
54.39%
68.97%
67.20%
59.16%
63.55%
74.27%
80.59%
44.24%
75.80%
65.57%
60.96%
79.16%
77.27%
60.17%
67.63%
78.04%
68.25%
77.48%
51.45%
68.07%
57.92%
63.39%
62.68%
80.55%
59.71%
69.99%
75.76%
40.49%
55.42%
72.59%
62.33%
77.56%
57.90%
73.06%
73.78%
39.18%
65.39%
76.34%
78.29%
79.18%
68.57%
52.84%
52.90%
55.10%
73.35%
70.40%
59.19%
67.43%
70.98%
43.34%
76.02%
71.82%
61.71%
77.37%
69.77%
64.58%

LAS
82.31%
58.65%
74.82%
78.20%
79.86%
75.09%
83.77%
78.81%
79.39%
77.92%
81.90%
88.55%
72.41%
85.99%
81.75%
0.00%
88.83%
0.00%
70.67%
77.91%
86.54%
0.00%
88.21%
79.89%
78.75%
77.67%
72.94%
79.90%
86.83%
80.35%
80.63%
85.07%
63.21%
73.46%
84.02%
0.00%
88.30%
76.11%
84.49%
86.73%
57.34%
77.00%
85.71%
86.46%
89.98%
78.17%
70.09%
77.23%
74.41%
80.41%
80.95%
83.17%
78.06%
0.00%
57.78%
88.19%
83.42%
77.63%
87.81%
81.66%
92.51%

Fudan
MLAS
64.62%
46.16%
63.16%
68.34%
71.38%
61.20%
69.66%
67.61%
67.52%
50.79%
74.01%
71.17%
65.18%
73.49%
68.93%
0.00%
81.24%
0.00%
58.33%
63.39%
78.64%
0.00%
80.75%
65.68%
60.11%
61.50%
59.11%
70.61%
77.45%
57.64%
61.89%
76.64%
34.03%
64.24%
65.11%
0.00%
80.31%
63.98%
70.73%
76.96%
49.57%
39.91%
76.53%
74.65%
76.88%
67.59%
58.37%
66.00%
61.54%
72.61%
67.26%
77.37%
66.21%
0.00%
37.54%
76.65%
74.27%
64.44%
79.30%
69.04%
83.13%

BLEX
61.66%
45.81%
71.21%
63.80%
62.30%
63.54%
72.37%
62.09%
70.80%
64.01%
77.87%
82.16%
57.04%
77.82%
73.42%
0.00%
70.55%
0.00%
57.56%
65.60%
72.31%
0.00%
68.74%
67.50%
66.23%
59.95%
56.95%
69.82%
75.86%
69.80%
66.75%
72.06%
39.99%
63.19%
73.04%
0.00%
67.64%
65.94%
67.66%
78.24%
47.08%
40.77%
71.16%
77.70%
79.01%
67.22%
64.74%
53.42%
64.39%
73.63%
68.10%
71.51%
72.02%
0.00%
44.08%
66.58%
71.73%
52.55%
71.82%
58.68%
85.11%

LAS
0.94%
3.82%
0.42%
3.04%
3.36%
13.06%
6.05%
2.18%
13.27%
0.48%
2.75%
1.29%
15.16%
2.83%
5.43%
0.00%
3.02%
0.00%
2.26%
1.68%
1.98%
0.00%
3.13%
17.94%
0.81%
8.54%
2.88%
4.14%
1.98%
4.62%
2.79%
2.32%
5.32%
8.37%
1.37%
0.00%
2.67%
9.30%
2.37%
2.06%
13.69%
1.45%
2.99%
2.04%
1.19%
2.39%
12.70%
11.38%
13.08%
0.63%
0.76%
12.17%
4.83%
0.00%
0.90%
3.26%
3.10%
4.02%
1.86%
1.45%
17.03%

Improvement
MLAS
0.70%
1.91%
-0.35%
3.05%
3.11%
13.04%
5.70%
2.04%
13.04%
0.88%
3.58%
1.39%
16.12%
2.77%
6.35%
0.00%
3.44%
0.00%
1.86%
1.23%
2.69%
0.00%
3.94%
16.40%
0.45%
9.08%
1.65%
4.89%
2.10%
3.30%
2.29%
2.76%
3.23%
10.24%
1.19%
0.00%
3.27%
8.31%
1.61%
2.42%
12.18%
1.39%
2.40%
2.48%
1.73%
1.30%
10.18%
13.86%
13.14%
-0.42%
1.28%
14.05%
6.27%
0.00%
0.11%
3.43%
3.06%
4.45%
2.10%
1.90%
20.74%

BLEX
-3.55%
0.00%
7.92%
-2.27%
-1.87%
9.15%
3.40%
-5.11%
11.64%
0.46%
3.60%
1.57%
12.80%
2.02%
7.85%
0.00%
-8.61%
0.00%
-2.61%
-2.03%
-5.73%
0.00%
-8.74%
16.05%
-1.84%
2.03%
-6.44%
7.14%
-4.69%
10.09%
-3.24%
-3.70%
-0.50%
7.77%
0.45%
0.00%
-9.92%
8.04%
-5.40%
4.46%
7.90%
-24.62%
-5.18%
-0.59%
-0.17%
-1.35%
11.90%
0.52%
9.29%
0.28%
-2.30%
12.32%
4.59%
0.00%
0.74%
-9.44%
-0.09%
-9.16%
-5.55%
-11.09%
20.53%

Table 6: All results on development set.

Milan Straka, Jan Hajiˇc, and Jana Strakov´a. 2016. UD-
trainable pipeline for processing CoNLL-U
Pipe:
ﬁles performing tokenization, morphological anal-
In Proceedings
ysis, POS tagging and parsing.
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association, Portoro, Slovenia.

Milan Straka and Jana Strakov´a. 2017.

Tokeniz-
ing, pos tagging, lemmatizing and parsing ud 2.0
In Proceedings of the CoNLL 2017
with udpipe.
Shared Task: Multilingual Parsing from Raw Text
to Universal Dependencies. Association for Compu-
tational Linguistics, Vancouver, Canada, pages 88–
99. http://www.aclweb.org/anthology/K/K17/K17-
3009.pdf.

Clara Vania, Xingxing Zhang, and Adam Lopez.
the edinburgh system for the
2017.
Uparse:
In Proceedings of the
conll 2017 ud shared task.
CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies. Associa-
tion for Computational Linguistics, pages 100–110.
https://doi.org/10.18653/v1/K17-3010.

Dan Zeman et al. 2018a. Universal Dependencies
2.2 CoNLL 2018 shared task development and test
data. LINDAT/CLARIN digital library at the Insti-
tute of Formal and Applied Linguistics, Charles Uni-
versity, Prague, http://hdl.handle.net/
11234/1-2184.
http://hdl.handle.net/11234/1-
2184.

Daniel Zeman, Filip Ginter, Jan Hajiˇc, Joakim Nivre,
Martin Popel, Milan Straka, and et al. 2018b.
CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies. In Pro-
ceedings of the CoNLL 2018 Shared Task: Multi-
lingual Parsing from Raw Text to Universal Depen-
dencies. Association for Computational Linguistics,
pages 1–20.

Barret Zoph and Quoc V Le. 2016. Neural architecture
search with reinforcement learning. arXiv preprint
arXiv:1611.01578 .

References

Mathieu Dehouck and Pascal Denis. 2017. Delexical-
ized word embeddings for cross-lingual dependency
parsing. In EACL. hal.inria.fr, volume 1, pages 241–
250.

Timothy Dozat, Peng Qi, and Christopher D Manning.
2017. Stanford’s graph-based neural dependency
parser at the conll 2017 shared task. Proceedings
of the CoNLL 2017 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies pages
20–30.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low resource dependency parsing:
Cross-lingual parameter sharing in a neural network
In Proceedings of the 53rd Annual Meet-
parser.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers).
volume 2, pages 845–850.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems. pages 1019–1027.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). vol-
ume 1, pages 1234–1244.

Joakim Nivre et al. 2018.

LINDAT/CLARIN digital

Universal Dependen-
library
cies 2.2.
at
the Institute of Formal and Applied Lin-
guistics, Charles University, Prague, http:
//hdl.handle.net/11234/1-1983xxx.
http://hdl.handle.net/11234/1-1983xxx.

Kenji Sagae and Alon Lavie. 2006. Parser combination
In Proceedings of the Human Lan-
by reparsing.
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers. Association for Com-
putational Linguistics, pages 129–132.

Motoki Sato, Hitoshi Manabe, Hiroshi Noji, and Yuji
Matsumoto. 2017. Adversarial training for cross-
domain universal dependency parsing. In Proceed-
ings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies.
Association for Computational Linguistics, pages
71–79. https://doi.org/10.18653/v1/K17-3007.

Tianze Shi, Felix G. Wu, Xilun Chen, and Yao
Cheng. 2017. Combining global models for pars-
In Proceedings of the
ing universal dependencies.
CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies. Associ-
ation for Computational Linguistics, pages 31–39.
https://doi.org/10.18653/v1/K17-3003.

