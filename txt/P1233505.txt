Gaussian Mixture Embeddings for Multiple Word Prototypes

Xinchi Chen, Xipeng Qiu∗, Jingxiang Jiang, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{xinchichen13,xpqiu,jxjiang14,xjhuang}@fudan.edu.cn

5
1
0
2
 
v
o
N
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
4
2
6
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Recently, word representation has been increasingly fo-
cused on for its excellent properties in representing
the word semantics. Previous works mainly suffer from
the problem of polysemy phenomenon. To address this
problem, most of previous models represent words as
multiple distributed vectors. However, it cannot reﬂect
the rich relations between words by representing words
as points in the embedded space. In this paper, we pro-
pose the Gaussian mixture skip-gram (GMSG) model to
learn the Gaussian mixture embeddings for words based
on skip-gram framework. Each word can be regarded as
a gaussian mixture distribution in the embedded space,
and each gaussian component represents a word sense.
Since the number of senses varies from word to word,
we further propose the Dynamic GMSG (D-GMSG)
model by adaptively increasing the sense number of
words during training. Experiments on four benchmarks
show the effectiveness of our proposed model.

Introduction
Distributed word representation has been studied for a con-
siderable efforts (Bengio et al. 2003; Morin and Bengio
2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov
et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger
and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a;
2013b). By representing a word in the embedded space, it
could address the problem of curse of dimensionality and
capture syntactic and semantic properties. In the embed-
ded space, words that have similar syntactic and seman-
tic roles are also close with each other. Thus, distributed
word representation is applied to a abundant natural lan-
guage processing (NLP) tasks (Collobert and Weston 2008;
Collobert et al. 2011; Socher et al. 2011; 2013).

Most of previous models map a word as a single point
vector in the embedded space, which surfer from polysemy
problems. Concretely, many words have different senses in
different contextual surroundings. For instance, word “ap-
ple” could mean a kind of fruit when the contextual infor-
mation implies that the topic of the text is about food. At
the meanwhile, the word “apple” could represent the Apple
Inc. when the context is about information technology. To

∗ Corresponding author.

Figure 1: An example of Gaussian mixture embeddings.

address this problem, previous works have gained great suc-
cess in representing multiple word prototypes. Reisinger and
Mooney (2010) constructs multiple high-dimensional vec-
tors for each word. Huang et al. (2012) learns multiple dense
embeddings for each word using global document context.
Neelakantan et al. (2014) proposed multi-sense skip-gram
(MSSG) to learn word sense embeddings with online word
sense discrimination. Most of previous works are trying to
use multiple points to represent the multiple senses of words,
which lead to a drawback. It cannot reﬂect the rich relations
between words by simply representing words as points in the
embedded space. For instance, we cannot infer that the word
“fruit” is the hypernym of the word “apple” by representing
these two words as two point vectors in the embedded space.
In this paper, we propose the Gaussian mixture skip-gram
(GMSG) model inspired by Vilnis and McCallum (2014),
who maps a word as a gaussian distribution instead of a
point vector in the embedded space. GMSG model repre-
sents each word as a gaussian mixture distribution in the
embedded space. Each gaussian component can be regarded
as a sense of a word. Figure 1 gives an illustration of Gaus-
sian mixture embeddings. In this way, much richer relations
can be reﬂected via the relation of two gaussian distribu-
tions. For instance, if the word “fruit” has larger variance
than the word “apple”, it could show that the word “fruit” is
the hypernym of the word “apple”. In addition, using differ-
ent distance metrics, the relations between words vary. The
pair (apple, pear) might be much closer when we use Eu-
clidean distance, while the pair (apple, fruit) might be much

closer when we measure them using KL divergence. Fur-
ther, since the number of senses varies from word to word,
we propose the Dynamic GMSG (D-GMSG) model to han-
dle the varying sense number. D-GMSG automatically in-
creases the sense numbers of words during training. Exper-
iments on four benchmarks show the effectiveness of our
proposed models.

Skip-Gram
In the skip-gram model (Figure 2a) (Mikolov et al. 2013c),
each word w is represented as a distributed vector ew ∈ W,
where W ∈ R|V |∗d is the word embedding matrix for all
words in word vocabulary V . d is the dimensionality of the
word embeddings. Correspondingly, each context word cw
also has a distributed representation vector ˆecw ∈ C, where
C ∈ R|V |∗d is another distinguished space.

The skip-gram aims to maximize the probability of the
co-occurrence of a word w and its context word cw, which
can be formalized as:

v(w|cw) =

p(u|cw),

(1)

(cid:89)

u∈{w} (cid:83) N EG(w)

where function N EG(·) returns a set of negative sampling
context words and p(u|cw) can be formalized as:

w ˆeu)]1{u(cid:54)=cw},

p(u|cw) = [σ(e(cid:62)

w ˆeu)]1{u=cw}[1 − σ(e(cid:62)
where 1{·} is a indicator function and σ(·) is the sigmoid
function.
Given

of
(word, context) pairs (w, cw) ∈ D, the goal of skip-
gram model is to maximize the objective function:

corpus D which

training

insists

(2)

a

J(θ) =

(cid:88)

(w,cw)∈D

log v(w|cw),

(3)

where θ is the parameter set of the model.

Concretely, the N EG(w) function samples negative con-
textual words for current word w drawn on the distribution:

P (w) ∝ Punigram(w)sr,

(4)

where Punigram(w) is a unigram distribution of words and
sr is a hyper-parameter.

Gaussian Skip-Gram Model
Although the skip-gram model is extremely efﬁcient and the
learned word embeddings have greet properties on the syn-
tactic and semantic roles, it cannot give the asymmetric dis-
tance between words.

Skip-gram model deﬁnes the representation of a word as
a vector, and deﬁnes the similarity of two words w1 and w2
using cosine distance of vectors ew1 and ew2. Unlike the
deﬁnition of skip-gram model, in this paper, we argue that a
word can be regarded as a function. The similarity between
two function f and g can be formalized as:

sim(f, g) =

f (x)g(x)dx

(5)

(cid:90)

x∈Rn

Speciﬁcally, when we choose the gaussian distribution as

the function,

(6)
(7)

(8)

(9)

f (x) = N (x; µα, Σα),
g(x) = N (x; µβ, Σβ),

the similarity between two gaussian distributions f and g
can be formalized as:

sim(f, g) =

N (x; µα, Σα)N (x; µβ, Σβ)dx

(cid:90)

x∈Rn

= N (0; µα − µα, Σβ + Σβ),

where µα, µβ and Σα, Σβ are the mean and covariance ma-
trix of word f and word g respectively.

In this way, we can use KL divergence to measure the
distance of two words which are represented as distribu-
tions. Since KL divergence is not symmetric metric, Gaus-
sian skip-gram (GSG) model (Figure 2b) (Vilnis and McCal-
lum 2014) could measure the distance between words asym-
metrically.

Gaussian Mixture Skip-Gram Model
Although GSG model seems work well, it cannot handle the
problem of polysemy phenomenon of words. In this paper,
we propose the Gaussian mixture skip-gram (GMSG) model
(Figure 2c). GMSG regard each word as a gaussian mixture
distribution. Each component of the gaussian mixture dis-
tribution of a word can be regarded as a sense of the word.
The senses are automatically learned during training using
the information of occurrences of words and their context.
Besides handling the polysemy problem, GMSG also cap-
tures the richer relations between words. As shown in Fig-
ure 1, it is tricky to tell whose distance is smaller between
word pairs (apple, fruit) and (apple, pear). On the one hand,
word “apple” seems much closer with word “fruit”, in a
manner, since apple is a kind of fruit while “pear” and “ap-
ple” are just syntagmatic relation. On the other hand, word
“apple” seems much closer with word “pear”, since they are
in the same level of semantic granularity. Actually, in dif-
ferent distance metrics, the relations between words varies.
The pair (apple, pear) might be much closer when we use
Euclidean distance, while the pair (apple, fruit) might be
much closer when we measure them using KL divergence.
However, whatever the relationship between them are, their
representations are learned and ﬁxed.

Formally, we deﬁne the distributions of two words as f

and g:

f (x) = N (x; φα, µα, Σα),
g(x) = N (x; φβ, µβ, Σβ),

(10)
(11)

where φα and φβ are the parameters of multinomial distri-
butions.

The similarity of two distributions can be formalized as:

sim(f, g) =

N (x; φα, µα, Σα)N (x; φβ, µβ, Σβ)dx

(cid:90)

x∈Rn

(12)

(a) Skip-Gram

(b) Gaussian Skip-Gram

(c) Gaussian Mixture Skip-Gram

(d) Dynamic Gaussian Mixture Skip-Gram

Figure 2: Four architectures for learning word representations.

N (x|z = i; µα, Σα)M(z = i; φα)

×

(cid:35)

(13)

N (x|z = j; µβ, Σβ)M(z = i; φβ)

 dx

(14)



(cid:35)



N (x; µαi, Σαi)φαi)

×

(15)

N (x; µβj, Σβj)φβj)

 dx

(16)

(cid:90)

=

(cid:34)

(cid:88)

x∈Rn

i





(cid:88)

j

(cid:90)

=

(cid:34)

(cid:88)

i

x∈Rn


(cid:88)



j

(cid:90)

(cid:88)

(cid:88)

=

φαiφβj

i

j

x∈Rn

(17)

(18)

(cid:88)

(cid:88)

=

i

j

φαiφβjN (0; µαi − µβj, Σαi + Σβj),

where M represents multinomial distribution.

Algorithm 1 shows the details, where φ, µ and Σ are pa-
rameters of word representations in word space. ˆφ, ˆµ and ˆΣ
are parameters of word representations in context space.

Dynamic Gaussian Mixture Skip-Gram Model
Although we could represent polysemy of words by us-
ing gaussian mixture distribution, there is still a short that

should be pointed out. Actually, the number of word senses
varies from word to word. To dynamically increasing the
numbers of gaussian components of words, we propose the
Dynamic Gaussian Mixture Skip-Gram (D-GMSG) model
(Figure 2d). The number of senses for a word is unknown
and is automatically learned during training.

At the beginning, each word is assigned a random gaus-
sian distribution. During training, a new gaussian compo-
nent will be generated when the similarity of the word and
its context is less than γ, where γ is a hyper-parameter of
D-GMSG model.

Concretely, consider the word w and its context c(w). The

similarity of them is deﬁned as:

For word w, assuming that w already has k Gaussian com-

ponents, it is represented as

fw = N (·; φw, µw, Σw),
φw = {φwi }k
µw = {µwi }k
Σw = {Σwi}k

i=1,
i=1,
i=1.

(20)

When s(w, c(w)) < γ, we generate a new random gaus-
sian component N (·; µwk+1, Σwk+1) for word w, and fw is

N (x; µαi, Σαi)N (x; µβj, Σβj)dx

s(w, c(w)) =

sim(fw, fc).

(19)

1
|c(w)|

(cid:88)

c∈c(w)

Input

: Training corpus: w1, w2, . . . , wT ;
Sense number: K;
Dimensionality of mean: d;
Context window size: N ;
Max-margin: κ.

Initialize: Multinomial parameter: φ, ˆφ ∈ R|V |×K;
Means of Gaussian mixture distribution: µ, ˆµ ∈ R|V |×d;
Covariances of Gaussian mixture distribution:
Σ, ˆΣ ∈ R|V |×d×d;
for w = w1 · · · wT do
nw ∼ {1, . . . , N }
c(w) = {wt−nw , . . . , wt−1, wt+1, . . . , wt+nw }
for c in c(w) do

for ˆc in N EG(ˆc) do

l = κ − sim(fw, fc) + sim(fw, fˆc);
if l > 0 then

Accumulate gradient for φw, µw, Σw;
Gradient update on ˆφc, ˆφˆc, ˆµc, ˆµˆc, ˆΣc,
ˆΣˆc;

end
Gradient update for L2 normalization term
of ˆφc, ˆφˆc, ˆµc, ˆµˆc, ˆΣc, ˆΣˆc;

end
Gradient update on φw, µw, Σw;
Gradient update for L2 normalization term of
φw, µw, Σw;

Output: φ, µ and Σ
Algorithm 1: Training algorithm of GMSG model using
max-margin criterion.

end

end

then updated as

w, µ∗

w),
i=1 ⊕ ξ,

f ∗
w, Σ∗
w = N (·; φ∗
φ∗
w = {(1 − ξ)φwi}k
µ∗
w = {µwi}k
w = {Σwi}k
Σ∗

i=1 ⊕ µwk+1,
i=1 ⊕ Σwk+1,

(21)

where mixture coefﬁcient ξ is a hyper-parameter and opera-
tor ⊕ is an set union operation.

Relation with Skip-Gram
In this section, we would like to introduce the relation be-
tween GMSG model and prevalent skip-gram model.

Skip-gram model (Mikolov et al. 2013c) is a well known
model for learning word embeddings for its efﬁciency and
effectiveness. Skip-gram deﬁnes the similarity of word w
and its context c as:

sim(w, c) =

1

1 + exp(−e(cid:62)

w ˆec)

(22)

When the covariances of all words and context words
2 I are ﬁxed, µfw = ew, µfc = ˆec and sense

Σw = ˆΣc = 1

number K = 1, the similarity of GMSG model can be for-
malized as:

sim(fw, fc) = N (0; µfw − µfc, Σfw + Σfc)

= N (0; ew − ˆec, Σw + ˆΣc)
= N (0; ew − ˆec, I)
∝ exp((ew − ˆec)(cid:62)(ew − ˆec)).

(23)

The deﬁnition of similarity of skip-gram is a function of
dot product of ew and ˆec, while it is a function of Euclidian
distance of ew and ˆec. In a manner, skip-gram model is a
related model of GMSG model conditioned on ﬁxed Σw =
ˆΣc = 1

2 I, µfw = ew, µfc = ˆec and K = 1.

Training
The similarity function of the skip-gram model is dot prod-
uct of vectors, which could perform a binary classiﬁer to
tell the positive and negative (word, context) pairs. In this
paper, we use a more complex similarity function, which
deﬁnes a absolute value for each positive and negative pair
rather than a relative relation. Thus, we minimise a different
max-margin based loss function L(θ) following (Joachims
2002) and (Weston, Bengio, and Usunier 2011):

L(θ) =

1
Z

(cid:88)

(cid:88)

l(θ) +

λ(cid:107)θ(cid:107)2
2,

1
2

(24)

(w,cw )∈D

c−∈N EG(w)
l(θ) = max{0, κ − sim(fw, fcw ) + sim(fw, fc− )},
where θ is the parameter set of our model and the margin κ
is a hyper-parameter. Z = (cid:80)
(w,cw)∈D |N EG(w)| is a nor-
malization term. Conventionally, we add a L2 regularization
term for all parameters, weighted by a hyper-parameter λ.

Since the objective function is not differentiable due to the
hinge loss, we use the sub-gradient method (Duchi, Hazan,
and Singer 2011). Thus, the subgradient of Eq. 24 is:

∂L(θ)
∂θ

=

1
Z

∂l(θ)
∂θ

= −

(cid:88)

(cid:88)

∂l(θ)
∂θ

+ λθ,

c−∈N EG(w)

(w,cw)∈D
∂sim(fw, fcw )
∂θ

+

∂sim(fw, fc−)
∂θ

.

(25)

In addition, the covariance matrices need to be kept pos-
itive deﬁnite. Following Vilnis and McCallum (2014), we
use diagonal covariance matrices with a hard constraint that
the eigenvalues (cid:37) of the covariance matrices lie within the
interval [m, M ].

Experiments
To evaluate our proposed methods, we learn the word rep-
resentation using the Wikipedia corpus1. We experiment on
four different benchmarks: WordSim-353, Rel-122, MC and
SCWS. Only SCWS provides the contextual information.

WordSim-353 WordSim-3532 (Finkelstein et al. 2001)
consists of 353 pairs of words and their similarity scores.

1http://mattmahoney.net/dc/enwik9.zip
2http://www.cs.technion.ac.il/˜gabr/resour

ces/data/wordsim353/wordsim353.html

Sampling rate
Context window size
Dimensionality of mean
Initial learning rate
Margin
Regularization
Min count
Number of learning iteration
Sense Number for GMSG
Weight of new Gaussian component
Threshold for generating a new Gaussian component

sr = 3/4
N = 5
d = 50
α = 0.025
κ = 0.5
λ = 10−8
mc = 5
iter = 5
K = 3
ξ = 0.2
γ = 0.02

Table 1: Hyper-parameter settings.

Rel-122 Rel-1223 (Szumlanski, Gomez, and Sims 2013)
contains 122 pairs of nouns and compiled them into a new
set of relatedness norms.

MC MC4 (Miller and Charles 1991) contains 30 pairs of
nouns that vary from high to low semantic similarity.

SCWS SCWS5 (Huang et al. 2012) consists of 2003
word pairs and their contextual information. Concretely, the
dataset consists of 1328 noun-noun, 97 adjective-adjective,
30 noun-adjective, 9 verb-adjective, 399 verb-verb, 140
verb-noun and 241 same-word pairs.

Hyper-parameters
The hyper-parameter settings are listed in the Table 1. In this
paper, we evaluate our models conditioned on the dimen-
sionality of means d = 50, and we use diagonal covariance
matrices for experiments. We remove all the word with oc-
currences less than 5 (Min count).

Model Selection
Figure 3 shows the performances using different sense num-
ber for words. According to the results, sense number sn =
3 for GMSG model is a good trade off between efﬁciency
and model performance.

Word Similarity and Polysemy Phenomenon
To evaluate our proposed models, we experiment on four
benchmarks, which can be divided into two kinds. Datasets
(Sim-353, Rel-122 and MC) only provide the word pairs and
their similarity scores, while SCWS dataset additionally pro-
vides the contexts of word pairs. It is natural way to tackle
the polysemy problem using contextual information, which
means a word in different contextual surroundings might
have different senses.

Table 2 shows the results on Sim-353, Rel-122 and MC
datasets, which shows that our models have excellent perfor-
mance on word similarity tasks. Table 3 shows the results on

3http://www.cs.ucf.edu/˜seansz/rel-122/
4http://www.cs.cmu.edu/˜mfaruqui/word-sim

/EN-MC-30.txt

5http://www.socher.org/index.php/Main/Impr
ovingWordRepresentationsViaGlobalContextAndM
ultipleWordPrototypes

0
0
1
×
ρ

70

60

50

WordSim-353

Rel-122

MC

2

3
5
4
Number of senses: K

6

Figure 3: Performance of GMSG using different sense num-
bers.

Methods WordSim-353 Rel-122 MC
63.96
SkipGram
-
MSSG
-
NP-MSSG
69.17
GSG-S
68.50
GSG-D
77.3
GMSG
47.3
D-GMSG

59.89
63.2
62.4
62.03
61.00
67.8
56.3

49.14
-
-
51.09
53.54
57.3
47.1

Table 2: Performances on WordSim-353, Rel-122 and MC
datasets. MSSG (Neelakantan et al. 2014) model sets the
same sense number for each word. NP-MSSG model au-
tomatically learns the sense number for each word during
training. GSG (Vilnis and McCallum 2014) has several vari-
ations. “S” indicates the GSG uses spherical covariance ma-
trices. “D” indicates the GSG uses diagonal covariance ma-
trices.

SCWS dataset, which shows that our models perform well
on polysemy phenomenon.

In this paper, we deﬁne the similarity of two words w and

w(cid:48) as:

AvgSim(fw, fw(cid:48) ) = sim(fw, fw(cid:48) )

(cid:88)

(cid:88)

=

i

j

φwiφw(cid:48)jsim(fwi, fw(cid:48)j),

(26)

where fw and fw(cid:48) are the distribution representations of the
corresponding words. Table 2 gives the results on the three
benchmarks. The size of word representation of all the pre-
vious models are chosen to be 50 in this paper.

To tackle the polysemy problem, we incorporate the con-
textual information. In this paper, we deﬁne the similarity of
two words (w, w(cid:48)) with their contexts (c(w), c(w(cid:48))) as:

M axSimC(fw, fw(cid:48) ) = sim(fwk, fw(cid:48)k(cid:48) )

(27)

=

=
where k
arg maxj P (j|w(cid:48), c(w(cid:48))). Here, P (i|w, c(w)) gives the
probability of the i-th sense of the current word w will take

arg maxi P (i|w, c(w))

and k(cid:48)

Model
Pruned TFIDF
Skip-Gram
C&W

ρ × 100
62.5
63.4
57.0
AvgSim MaxSimC

Huang
MSSG
NP-MSSG
our models
GMSG
D-GMSG

62.8
64.2
64.0

64.6
56.0

26.1
49.17
50.27

53.6
41.4

Table 3: Performance on SCWS dataset.Pruned TFIDF
(Reisinger and Mooney 2010) uses spare, high-dimensional
word representations. C&W (Collobert and Weston 2008) is
a language model. Huang (Huang et al. 2012) is a neural
network model for learning multi-representations per word.

Figure 4: Distribution of sense numbers of words learned by
D-GMSG.

conditioned on the speciﬁc contextual surrounding c(w),
where P (i|w, c(w)) is deﬁned as:

P (i|w, c(w)) =

(cid:80)

1
|c(w)|

(cid:80)
j

1
|c(w)|

c∈c(w) φwisim(fwi, fc)
c(cid:48)∈c(w) φwjsim(fwj, fc(cid:48) )

(cid:80)

.

(28)

Table 3 gives the results on the SCWS benchmark.
Figure 4 gives the distribution of sense numbers of words
using logarithmic scale, which is trained by D-GMSG
model. The vocabulary size is 71084 here. As shown in Fig-
ure 4, majority of words have only one sense, and the num-
ber of words decreases progressively with the increase of
word sense number.

Related Work
Recently, it has attracted lots of interests to learn word rep-
resentation. Much previous works focus on learning word as
a point vector in the embedded space. Bengio et al. (2003)
applies the neural network to sentence modelling instead
of using n-gram language models. Since the Bengio et al.
(2003) model need expensive computational resource. Lots
of works are trying to optimize it. Mikolov et al. (2013a)
proposed the skip-gram model which is extremely efﬁcient
by removing the hidden layers of the neural networks, so that
larger corpus could be used to train the word representation.
By representing a word as a distributed vector, we gain a

lot of interesting properties, such as similar words in syn-
tactic or semantic roles are also close with each other in the
embedded space in cosine distance or Euclidian distance. In
addition, word embeddings also perform excellently in anal-
ogy tasks. For instance, eking − equeen ≈ eman − ewoman.
However, previous models mainly suffer from the pol-
ysemy problem. To address this problem, Reisinger and
Mooney (2010) represents words by constructing multiple
sparse, high-dimensional vectors. Huang et al. (2012) is
an neural network based approach, which learns multiple
dense, low-dimensional embeddings using global document
context. Tian et al. (2014) modeled the word polysemy from
a probabilistic perspective and integrate it with the highly
efﬁcient continuous Skip-Gram model. Neelakantan et al.
(2014) proposed Multi-Sense Skip-Gram (MSSG) to learn
word sense embeddings with online word sense discrimina-
tion. These models perform word sense discrimination by
clustering context of words. Liu et al. (2015) discriminates
word sense by introducing latent topic model to globally
cluster the words into different topics. Liu, Qiu, and Huang
(2015) further extended this work to model the complicated
interactions of word embedding and its corresponding topic
embedding by incorporating the tensor method.

Almost previous works are trying to use multiple points
to represent the multiple senses of words. However, it can-
not reﬂect the rich relations between words by simply rep-
resenting words as points in the embedded space. Vilnis and
McCallum (2014) represented a word as a gaussian distribu-
tion. Gaussian mixture skip-gram (GMSG) model represents
a word as a gaussian mixture distribution. Each sense of a
word can be regarded as a gaussian component of the word.
GMSG model gives different relations of words under dif-
ferent distance metrics, such as cosine distance, dot product,
Euclidian distance KL divergence, etc.

Conclusions and Further Works
In this paper, we propose the Gaussian mixture skip-gram
(GMSG) model to map a word as a density in the embedded
space. A word is represented as a gaussian mixture distribu-
tion whose components can be regarded as the senses of the
word. GMSG could reﬂect the rich relations of words when
using different distance metrics. Since the number of word
senses varies from word to word, we further propose the Dy-
namic GMSG (D-GMSG) model to adaptively increase the
sense numbers of words during training.

Actually, a word can be regarded as any function includ-
ing gaussian mixture distribution. In the further, we would
like to investigate the properties of other functions for word
representations and try to ﬁgure out the nature of the word
semantic.

References
Bengio, Y.; Ducharme, R.; Vincent, P.; and Janvin, C. 2003.
A neural probabilistic language model. The Journal of Ma-
chine Learning Research 3:1137–1155.
Bengio, Y.; Schwenk, H.; Sen´ecal, J.-S.; Morin, F.; and Gau-
vain, J.-L. 2006. Neural probabilistic language models. In
Innovations in Machine Learning. Springer. 137–186.

Reisinger, J., and Mooney, R. J. 2010. Multi-prototype
In Human Lan-
vector-space models of word meaning.
guage Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computa-
tional Linguistics, 109–117. Association for Computational
Linguistics.
Socher, R.; Lin, C. C.; Manning, C.; and Ng, A. Y. 2011.
Parsing natural scenes and natural language with recursive
neural networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), 129–136.
Socher, R.; Bauer, J.; Manning, C. D.; and Ng, A. Y. 2013.
Parsing with compositional vector grammars. In In Proceed-
ings of the ACL conference. Citeseer.
Szumlanski, S. R.; Gomez, F.; and Sims, V. K. 2013. A new
set of norms for semantic relatedness measures. In ACL (2),
890–895.
Tian, F.; Dai, H.; Bian, J.; Gao, B.; Zhang, R.; Chen, E.; and
Liu, T.-Y. 2014. A probabilistic model for learning multi-
In Proceedings of COLING,
prototype word embeddings.
151–160.
Turian, J.; Ratinov, L.; and Bengio, Y. 2010. Word repre-
sentations: a simple and general method for semi-supervised
learning. In Proceedings of the 48th annual meeting of the
association for computational linguistics, 384–394. Associ-
ation for Computational Linguistics.
Vilnis, L., and McCallum, A. 2014. Word representations
via gaussian embedding. arXiv preprint arXiv:1412.6623.
Weston, J.; Bengio, S.; and Usunier, N. 2011. Wsabie: Scal-
ing up to large vocabulary image annotation. In IJCAI, vol-
ume 11, 2764–2770.

Collobert, R., and Weston, J. 2008. A uniﬁed architecture
for natural language processing: Deep neural networks with
multitask learning. In Proceedings of ICML.
Collobert, R.; Weston,
J.; Bottou, L.; Karlen, M.;
Kavukcuoglu, K.; and Kuksa, P. 2011. Natural language
processing (almost) from scratch. The Journal of Machine
Learning Research 12:2493–2537.
Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. The Journal of Machine Learning Research 12:2121–
2159.
Finkelstein, L.; Gabrilovich, E.; Matias, Y.; Rivlin, E.;
Solan, Z.; Wolfman, G.; and Ruppin, E. 2001. Placing
search in context: The concept revisited. In Proceedings of
the 10th international conference on World Wide Web, 406–
414. ACM.
Huang, E.; Socher, R.; Manning, C.; and Ng, A. 2012. Im-
proving word representations via global context and mul-
In Proceedings of the 50th Annual
tiple word prototypes.
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 873–882. Jeju Island, Korea: As-
sociation for Computational Linguistics.
Joachims, T. 2002. Optimizing search engines using click-
through data. Proc. of SIGKDD.
Liu, Y.; Liu, Z.; Chua, T.-S.; and Sun, M. 2015. Topical
word embeddings. In AAAI.
Liu, P.; Qiu, X.; and Huang, X. 2015. Learning context-
sensitive word embeddings with neural tensor skip-gram
model. In Proceedings of IJCAI.
Mikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock`y, J.; and Khu-
danpur, S. 2010. Recurrent neural network based language
model. In INTERSPEECH.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a.
Efﬁcient estimation of word representations in vector space.
arXiv preprint arXiv:1301.3781.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013b. Distributed representations of words and
phrases and their compositionality. In NIPS, 3111–3119.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013c. Distributed representations of words and
phrases and their compositionality. In Advances in Neural
Information Processing Systems.
Miller, G. A., and Charles, W. G. 1991. Contextual corre-
lates of semantic similarity. Language and cognitive pro-
cesses 6(1):1–28.
Mnih, A., and Hinton, G. 2007. Three new graphical models
for statistical language modelling. In Proceedings of ICML.
Morin, F., and Bengio, Y. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings of the
international workshop on artiﬁcial intelligence and statis-
tics, 246–252. Citeseer.
Neelakantan, A.; Shankar, J.; Passos, A.; and McCallum,
A. 2014. Efﬁcient non-parametric estimation of multiple
In Proceedings of
embeddings per word in vector space.
the Conference on Empirical Methods in Natural Language
Processing (EMNLP).

