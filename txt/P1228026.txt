SOURCE: SOURce-Conditional Elmo-style Model
for Machine Translation Quality Estimation

Junpei Zhou∗ Zhisong Zhang∗ Zecong Hu∗
Language Technologies Institute
Carnegie Mellon University
{junpeiz, zhisongz, zeconghu}@andrew.cmu.edu

Abstract

on translations that are estimated of low-quality,
further reducing post-editing cost.

Quality estimation (QE) of machine transla-
tion (MT) systems is a task of growing im-
portance. It reduces the cost of post-editing,
allowing machine-translated text to be used
in formal occasions.
In this work, we de-
scribe our submission system in WMT 2019
sentence-level QE task. We mainly explore the
utilization of pre-trained translation models in
QE and adopt a bi-directional translation-like
strategy. The strategy is similar to ELMo, but
additionally conditions on source sentences.
Experiments on WMT QE dataset show that
our strategy, which makes the pre-training
slightly harder, can bring improvements for
QE. In WMT-2019 QE task, our system ranked
in the second place on En-De NMT dataset and
the third place on En-Ru NMT dataset.

1

Introduction

The quality of machine translation systems have
been signiﬁcantly improved over the past few
years (Chatterjee et al., 2018), especially with the
development of neural machine translation (NMT)
models (Sutskever et al., 2014; Bahdanau et al.,
2014). Despite such inspiring improvements,
some machine translated texts are still error-prone
and unreliable compared to those by professional
humans. It is often desirable to have human ex-
perts perform post-editing on machine-translated
text to achieve a balance between cost and cor-
rectness. Correspondingly, we may also want to
develop automatic quality estimation systems to
judge the quality of machine translation outputs,
leading to the development of the Machine Trans-
lation Quality Estimation task. The task of QE
aims to evaluate the output of a machine trans-
lation system without access to reference transla-
tions. It would allow human experts to concentrate

∗equal contribution

In this work, we focus on sentence-level QE
and describe our submission to the WMT 2019
QE task. Sentence-level QE aims to predict a
score for the entire source–translation pair that in-
dicates the effort required for further post-editing.
The goals of the task are two-fold: 1) to pre-
dict the required post-editing cost, measured in
HTER (Snover et al., 2006); 2) to rank all sentence
pairs in descending translation quality.

In previous works, including the participating
systems in previous WMT shared tasks,
there
have been various methods to tackle this prob-
lem. Traditional linear models are based on hand-
crafted features, while recent state-of-the-art sys-
tems adopt end-to-end neural models (Kim and
Lee, 2016; Wang et al., 2018). The neural sys-
tems are usually composed of two modules: the
bottom part is an MT-like source–target encod-
ing model pre-trained with large parallel corpora,
stacked with a top-level QE scorer based on the
neural features extracted by the bottom model. Es-
pecially, Wang et al. (2018) adopted the “Bilingual
Expert” model (Fan et al., 2018) for pre-training
the bottom model and obtained several best re-
sults in WMT 2018.
In this work, we improve
the “Bilingual Expert” model with a SOURce-
Conditional ELMo-style (SOURCE) strategy: in-
stead of predicting target words based on contexts
from both sides, we train two conditioned lan-
guage (translation) models, each restricted to con-
text from one side only. This harder setting may
force the model to condition more on the source.
Experiments show that this strategy can bring im-
provements for QE.

Figure 1: The architecture of our QE system, which consists of two modules: 1) the MT Module encodes the
bilingual information and can be pre-trained with large parallel data, 2) the QE Module adopts the source and
target representations from the MT Module and further encodes those information followed by a ﬁnal linear layer
for QE scoring.

2 System

2.1 Basic Framework

We follow previous works and adopt the end-to-
end styled model for the QE scoring task. The
overall system architecture is shown in Figure 1.
The system consists of two components: 1) a pre-
trained MT module which learns the representa-
tions of the source and target sentences, 2) a QE
scorer which takes the representations from the
MT module as inputs and predicts the translation
quality score.

The MT module is pre-trained on large paral-
lel corpus.
It is trained to predict each token in
the translated sentence by using the information in
source sentence and tokens in the translated sen-
tence. Details of the model will be described in
Section 2.2.

In the QE scorer module, the problem can be
cast as a regression task, where the QE score is
predicted given the source and target sentences.
The original inputs are encoded by the pre-trained
MT module, whose outputs are taken as input fea-
tures for this module. We basically follow the
model architecture of Wang et al. (2018). For each
token, a quality vector is formed as:

qj = Concat(←−zj , −→zj , et

j−1, et

j+1, f mm
j

),

(1)

where ←−zj , −→zj are state vectors produced by the bi-
directional Transformer, and et
j+1 are em-
bedding vectors. The “mismatching feature” f mm
is formed by extracting the score corresponding

j−1, et

j

to yj, the highest score in the distribution, their
difference, and an indicator of whether yj has the
highest score. After this, the quality vectors are
viewed as another sequence and encoded by the
Bi-LSTM/Transformer Quality Estimator to pre-
dict the QE score. The loss function for training is
mean squared error which is typical for regression
tasks.

2.2 Pre-trained Translation Models

Bilingual Expert We start with a short descrip-
tion for the model of Wang et al. (2018). The
model can be seen as a token-level reconstruction-
styled translation model: each target word yj is
predicted given a source sentence and all other
target words {. . . , yj−1, yj+1, . . . }. This setting
is different to the traditional MT scenario where
only previous target words can be seen. The model
uses the encoder–decoder architecture. An en-
coder is applied over the source tokens to obtain
the contextual representations of the source sen-
tence. A bidirectional pair of decoders (one for-
ward and one backward) are adopted to encode the
target translation sentence, while conditioning on
the source sentence via attention mechanism. For-
mally, for source tokens {x1, . . . , xms} and trans-
lation tokens {y1, . . . , ymt}, the forward and back-
ward target representations {−→z1, . . . , −→zmt} and

therefore, we use the forward representation of the
previous token −→zj −1 and the backward representa-
tion of the next token ←−zj +1.

SOURCE In the Bilingual Expert model, each
target token is predicted given all target tokens on
both sides. However, this training scheme makes
too much information visible to the model, such
that the model could predict the target word even
without seeing the source sentence.

For example, we can easily infer that the miss-
ing word in “He loves playing
and his
favorite basketball player is Michael Jordan” is
“basketball”.
In another words, too much visi-
ble information on the target side provides an in-
ductive bias that pushes the model towards learn-
ing a bi-directional language model instead of a
translation-like model, by omitting the informa-
tion on the source sentence.

We want to force our model to exploit the re-
lationship between the source tokens and target
tokens. Thus, we no longer make the words
on both sides visible to our model at the same
time. Instead, we separate the two directions, so
that the model must predict each target word de-
pending only on the source sentence and target
words on one side. More speciﬁcally, we com-
pute two losses, (cid:96)1 and (cid:96)2. The cross-entropy
loss (cid:96)1 is derived by predicting the target word
yj based on the source sentence {x0, . . . , xms}
and left-side target words {y0, . . . , yj−1}. An-
other cross-entropy loss (cid:96)2 is derived by predict-
ing yj based on the source sentence and right-
side target words {yj+1, . . . , ymt}. This train-
ing scheme corresponds to the strategy used in
ELMo (Peters et al., 2018), but the difference is
that here we condition on additional source in-
formation, hence the name SOURce-Conditional
Elmo-style (SOURCE) model.

Another method to force the model to attend
more to source is using BERT (Devlin et al.,
2018), which masks several words and try to pre-
dict those words at once.
Inspired by the work
of Cross-lingual BERT (Lample and Conneau,
2019), we choose to use the structure as shown
in Figure 2. It can reduce the information seen by
the decoder and force it to condition more on the
source sentence. Due to limitations on time and
computing resources, we did not manage to pro-
duce successful results using BERT. This would be
an interesting and promising direction to explore
in future work.

Figure 2:
Illustration of reconstruction loss for the
token “Wi” in different pre-training strategies. a) In
Bilingual Expert, one reconstruction loss is computed
for each token, conditioned on the entire target con-
text provided by the Forward and Backward decoders.
b) With Elmo-Style, it is equivalent to training bidi-
rectional translation models. Two reconstruction losses
are computed for each token, each only depending on
one side of the context. c) With BERT-Style, certain
inputs are masked out (colored in grey) and a masked-
LM is learned. One reconstruction loss is computed for
each masked token.

{←−z1, . . . , ←−zmt} are computed as:
c1, . . . , cms = Encoder(x1, . . . , xms),
−−−−−→
−→z1, . . . , −→zmt =
Decoder(y1, . . . , ymt, c1, . . . , cms),
←−−−−−
←−z1, . . . , ←−zmt =
Decoder(y1, . . . , ymt, c1, . . . , cms).
Both
use Trans-
encoder
former (Vaswani et al., 2017) as their backbone
for its better performances in machine translation
tasks.

decoders

and

After obtaining these representations, the model
is trained with the token reconstruction cross-
entropy loss for each target token with contextual
information from both sides:
log p(yj | yi(cid:54)=j, x) = softmax(ﬀ([−→zj −1; ←−zj +1])).
(2)
Here “ﬀ” denotes a feed-forward layer. Note
that we cannot use representations that capture yj,

Dataset

Parallel

QE

train

dev

test

En-De-SMT
En-De-NMT

32.8M

26299
13442

1000
1000

1926
1023

En-Ru

8.0M

15089

1000

1023

Table 1: Statistics of the parallel data and QE data.

From empirical results of SOURCE, we ﬁnd
that although the prediction accuracy on MT par-
allel data decreases, the ﬁnal performance on QE
increases signiﬁcantly. This shows that decreas-
ing the visible information makes token-prediction
more difﬁcult, and forces the model to learn more
useful structures from the data, which in turn be-
comes features of higher quality for the QE task.

2.3 Model Ensemble

We perform model ensembling by stacking, which
means we use the prediction results of different
models on the development set as new features,
and train a simple regression model to predict the
actual development set labels. Finally, the regres-
sion model is applied on the predictions of differ-
ent models on test set. We use ridge regression
here as the regression model. We also use grid
search and cross-validation to select the regular-
ization weight for ridge regression.

We train both the pre-trained MT module
and the QE scorer module with different hyper-
parameters to produce different models for en-
sembling. For the pre-trained MT module, tog-
gled hyper-parameters include number of lay-
ers, number of self-attention heads, learning rate,
label-smoothing rate, warm-up steps, and dropout
rates. For the QE scorer module, toggled hyper-
parameters include number of layers, hidden
size, percentage of augmented data, encoder type
(LSTM or Transformer), and dropout rate.

3 Experiments

3.1 Settings

Our system is evaluated on the WMT18/19 QE
sentence-level task. The main metric is the Pear-
son’s r correlation score between predicted scores
and ground-truth HTER scores. There are other
metrics including Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE) for scoring
and the Spearman’s ρ rank correlation for rank-
ing. We evaluate our models on datasets in the

WMT 18/19 shared task with different translation
systems: WMT-18 En-De-SMT, WMT-18/19 En-
De-NMT, WMT-19 En-Ru-NMT. For experiments
on WMT-19 data, we report results based on ofﬁ-
cial evaluation results.

Data For the parallel data used in pre-training
of the MT module, we collect large-scale parallel
corpora for En-De and En-Ru from the WMT-18
translation task. Ofﬁcially pre-processed data1 are
utilized. To make it compatible with QE data, we
re-escaped certain punctuation tokens. To reduce
the corpus size, we further apply a more strict ﬁl-
tering step by discarding sentence pairs with too
many overlapping words in their source and target
sentences (> 0.9 for En-De, > 0.5 for En-Ru).
Finally, we obtain 32.1M EN-De and 7.8M En-Ru
sentence pairs and mix it with the training set of
the QE data (using post-edited sentences as target).
Our mixing strategy is to mix one copy of QE data
for every 1M of parallel data. The statistics of the
mixed parallel data and QE data are summarized
in Table 1.

Following Wang et al. (2018), we also prepare
artiﬁcial data via round-trip translation (Junczys-
Dowmunt and Grundkiewicz, 2016, 2017). Since
the QE data are obtained with two kinds of transla-
tion systems: SMT and NMT, we also prepare two
kinds of artiﬁcial data. For simplicity, we take the
back-translated corpus by the Edinburgh’s transla-
tion system,2 which contains 3.6M back-translated
sentences for En-De and 1.9M for En-Ru. We fur-
ther train a SMT system with Moses and decode
the English sentence back to German. For NMT,
we simply take a pre-trained NMT system (also
Edinburgh’s system3) for decoding.

Implementation We implement our
system
from scratch in Python with TensorFlow (Abadi
et al., 2015) and OpenNMT (Klein et al., 2017).
Because of limited resources, we manually search
for good hyper-parameters by heuristics evaluated
on the development set. The training of the MT
module takes around 4 to 5 days and the training
of the QE module takes a couple of hours with one
GPU.

1http://data.statmt.org/wmt18/

translation-task/preprocessed/. According to
the ofﬁcial script, the data is processed with tokenization,
cleaning and truecase with standard Moses scripts.

2http://data.statmt.org/rsennrich/

wmt16_backtranslations/.

3http://data.statmt.org/wmt17_systems/.

System

test 2018 En-De-SMT

test 2018 En-De-NMT

Pearson r ↑ MAE↓

RMSE↓

Pearson r ↑ MAE↓

RMSE↓

Spearman ρ ↑

Alibaba (ensemble)
JXNU (ensemble)
SOURCE (ours, ensemble)

SOURCE (ours)
Bilingual Expert (our impl.)
melaniad
cscarton

0.7397
0.7000
—

0.6970
0.6645
0.4877
0.4872

0.0937
0.0962
—

0.1009
0.1089
0.1316
0.1321

0.1362
0.1382
—

0.1409
0.1488
0.1675
0.1714

0.5012
0.5129
0.5474

0.4956
0.4447
0.4198
0.3808

0.1131
0.1114
0.1123

0.1197
0.1240
0.1359
0.1297

0.1742
0.1749
0.1623

0.1797
0.1791
0.1770
0.1785

0.6049
0.6052
0.6155

—
—
—
—

Table 2: Evaluation results on the test sets of WMT-18 En-De-SMT and WMT-18 NMT. The two leading teams
only provide ensemble results on 2018 test data. We re-implement Alibaba’s single model (Bilingual Expert) and
achieved similar results on 2017 data as reported in their paper. We test that Bilingual Expert on 2018 data to
make a fair comparison for single model. With limited computational resource, we only run the ensemble for
En-De-NMT, because in WMT2019 they only requires NMT submission.

System

En-De-NMT

En-Ru-NMT

Pearson r ↑

Spearman ρ ↑

Pearson r ↑

Spearman ρ ↑

UNBABEL
SOURCE (ours)
NJU
ETRI

0.5718
0.5474
0.5433
0.5260

0.6221
0.5947
0.5694
0.5745

0.5923
0.4575
–
0.5327

0.5388
0.4039
–
0.5222

Table 3: Evaluation results on the WMT-19 QE sentence-level shared task. Here we only show the top four teams.

3.2 Results

WMT-18 En-De-SMT Results are shown on the
left side of Table 2. We can see that our SOURCE
model signiﬁcantly outperforms the state-of-the-
art single model from the previous year (Bilingual
Expert) and is comparable to the ensemble model
from JXNU.

WMT-18 En-De-NMT We evaluate our model
through CodaLab, which is recommended by the
host. Results are shown on the right side of Table
2. The results are similar to the SMT ones, our sin-
gle SOURCE model can obtain results comparable
to the best ensemble systems.
It is worth men-
tioning that our ensemble model signiﬁcantly out-
performs the best system from the previous year
on both scoring (Pearson r) and ranking (Spear-
man ρ) subtasks.

WMT-19 En-De-NMT and En-Ru-NMT The
ofﬁcial result from WMT-19 is shown in Table 3.
Our system achieves the second place on En-De
and the third place on En-Ru.
It is worth men-
tioning that due to the limitation of computational
resource, we train far fewer models for En-Ru than
En-De, so it is reasonable that our system performs

much better on the En-De dataset.

4 Conclusion and Discussion

Empirical results indicate that decreasing the visi-
ble information makes token-prediction more dif-
ﬁcult, and forces the model to learn more useful
structures from the data, which in turn becomes
features of higher quality for the QE task. The
experimental results on WMT-18 shows the effec-
tiveness of our SOURCE model as well as our
stacking ensemble strategy. According to the ofﬁ-
cial evaluation results on WMT-19 dataset, our en-
semble SOURCE modela chieves the second place
on En-De dataset and the third place on En-Ru
dataset.

We will explore the BERT-style structure to bet-

ter condition on source sentences in the future.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Man´e, Rajat Monga, Sherry

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, volume 200.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Jiayi Wang, Kai Fan, Bo Li, Fengming Zhou, Boxing
Chen, Yangbin Shi, and Luo Si. 2018. Alibaba sub-
mission for WMT18 quality estimation task. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Shared Task Papers, pages 809–815, Bel-
gium, Brussels.

Moore, Derek Murray, Chris Olah, Mike Schus-
ter, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorﬂow.org.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Rajen Chatterjee, Matteo Negri, Raphael Rubino, and
Marco Turchi. 2018. Findings of the WMT 2018
shared task on automatic post-editing. In Proceed-
ings of the Third Conference on Machine Transla-
tion: Shared Task Papers, pages 710–725.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Kai Fan, Bo Li, Fengming Zhou, and Jiayi Wang. 2018.
“bilingual expert” can ﬁnd translation errors. CoRR,
abs/1807.09433.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Log-linear combinations of monolingual and
bilingual neural machine translation models for au-
In Proceedings of the First
tomatic post-editing.
Conference on Machine Translation, pages 751–
758, Berlin, Germany.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2017.
An exploration of neural sequence-to-
sequence architectures for automatic post-editing.
In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 120–129, Taipei, Tai-
wan.

Hyun Kim and Jong-Hyeok Lee. 2016. A recurrent
neural networks approach for estimating the quality
of machine translation output. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 494–498, San
Diego, California.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

