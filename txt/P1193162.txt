Intuitive, Interactive Beard and Hair Synthesis with Generative Models

Kyle Olszewski13, Duygu Ceylan2, Jun Xing35, Jose Echevarria2, Zhili Chen27, Weikai Chen36, and Hao Li134
1University of Southern California, 2Adobe Inc., 3USC ICT, 4Pinscreen Inc.,
5miHoYo, 6Tencent America, 7ByteDance Research
{olszewski.kyle,duygu.ceylan,junxnui}@gmail.com echevarr@adobe.com

{iamchenzhili,chenwk891}@gmail.com hao@hao-li.com

0
2
0
2
 
r
p
A
 
5
1
 
 
]

V
C
.
s
c
[
 
 
1
v
8
4
8
6
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

We present an interactive approach to synthesizing real-
istic variations in facial hair in images, ranging from subtle
edits to existing hair to the addition of complex and chal-
lenging hair in images of clean-shaven subjects. To cir-
cumvent the tedious and computationally expensive tasks of
modeling, rendering and compositing the 3D geometry of
the target hairstyle using the traditional graphics pipeline,
we employ a neural network pipeline that synthesizes real-
istic and detailed images of facial hair directly in the tar-
get image in under one second. The synthesis is controlled
by simple and sparse guide strokes from the user deﬁning
the general structural and color properties of the target
hairstyle. We qualitatively and quantitatively evaluate our
chosen method compared to several alternative approaches.
We show compelling interactive editing results with a proto-
type user interface that allows novice users to progressively
reﬁne the generated image to match their desired hairstyle,
and demonstrate that our approach also allows for ﬂexible
and high-ﬁdelity scalp hair synthesis.

1. Introduction

The ability to create and edit realistic facial hair in im-
ages has several important, wide-ranging applications. For
example, law enforcement agencies could provide multi-
ple images portraying how missing or wanted individuals
would look if they tried to disguise their identity by grow-
ing a beard or mustache, or how such features would change
over time as the subject aged. Someone considering grow-
ing or changing their current facial hair may want to pre-
visualize their appearance with a variety of potential styles
without making long-lasting changes to their physical ap-
pearance. Editing facial hair in pre-existing images would
also allow users to enhance their appearance, for example
in images used for their social media proﬁle pictures. In-
sights into how to perform high-quality and controllable fa-

Figure 1: Given a target subject image, a masked region in
which to perform synthesis, and a set of strokes of varying
colors provided by the user, our approach interactively syn-
thesizes hair with the appropriate structure and appearance.

cial hair synthesis would also prove useful in improving
face-swapping technology such as Deepfakes for subjects
with complex facial hair.

One approach would be to infer the 3D geometry and ap-
pearance of any facial hair present in the input image, then
manipulate or replace it as as desired before rendering and
compositing into the original image. However, single view
3D facial reconstruction is in itself an ill-posed and under
constrained problem, and most state-of-the-art approaches
struggle in the presence of large facial hair, and rely on para-
metric facial models which cannot accurately represent such
structures. Furthermore, even state-of-the-art 3D hair ren-
dering methods would struggle to provide sufﬁciently real-
istic results quickly enough to allow for interactive feedback
for users exploring numerous subtle stylistic variations.

One could instead adopt a more direct and naive ap-
proach, such as copying regions of facial hair from exem-
plar images of a desired style into the target image. How-
ever, it would be extremely time-consuming and tedious
to either ﬁnd appropriate exemplars matching the position,
perspective, color, and lighting conditions in the target im-
age, or to modify these properties in the selected exemplar
regions so as to assemble them into a coherent style match-
ing both the target image and the desired hairstyle.

In this paper we propose a learning-based interactive ap-
proach to image-based hair editing and synthesis. We ex-

1

ploit the power of generative adversarial networks (GANs),
which have shown impressive results for various image edit-
ing tasks. However, a crucial choice in our task is the input
to the network guiding the synthesis process used during
training and inference. This input must be sufﬁciently de-
tailed to allow for synthesizing an image that corresponds to
the user’s desires. Furthermore, it must also be tractable to
obtain training data and extract input closely corresponding
to that provided by users, so as to allow for training a gen-
erative model to perform this task. Finally, to allow novice
artists to use such a system, authoring this input should be
intuitive, while retaining interactive performance to allow
for iterative reﬁnement based on realtime feedback.

A set of sketch-like “guide strokes” describing the local
shape and color of the hair to be synthesized is a natural
way to represent such input that corresponds to how hu-
mans draw images. Using straightforward techniques such
as edge detection or image gradients would be an intuitive
approach to automatically extract such input from training
images. However, while these could roughly approximate
the types of strokes that a user might provide when draw-
ing hair, we seek to ﬁnd a representation that lends itself
to intuitively editing the synthesis results without explicitly
erasing and replacing each individual stroke.

Consider a vector ﬁeld deﬁning the dominant local ori-
entation across the region in which hair editing and synthe-
sis is to be performed. This is a natural representation for
complex structures such as hair, which generally consists of
strands or wisps of hair with local coherence, which could
easily be converted to a set of guide strokes by integrating
the vector ﬁeld starting from randomly sampled positions in
the input image. However, this representation provides ad-
ditional beneﬁts that enable more intuitive user interaction.
By extracting this vector ﬁeld from the original facial hair
in the region to be edited, or by creating one using a small
number of coarse brush strokes, we could generate a dense
set of guide strokes from this vector ﬁeld that could serve as
input to the network for image synthesis. Editing this vec-
tor ﬁeld would allow for adjusting the overall structure of
the selected hairstyle (e.g., making a straight hairstyle more
curly or tangled, or vice versa) with relatively little user in-
put, while still synthesizing a large number of guide strokes
corresponding to the user’s input. As these strokes are used
as the ﬁnal input to the image synthesis networks, subtle lo-
cal changes to the shape and color of the ﬁnal image can be
accomplished by simply editing, adding or removing indi-
vidual strokes.

We carefully chose our network architectures and train-
ing techniques to allow for high-ﬁdelity image synthesis,
tractable training with appropriate input data, and inter-
active performance. Speciﬁcally, we propose a two-stage
pipeline. While the ﬁrst stage focuses on synthesizing real-
istic facial hair, the second stage aims to reﬁne this initial

result and generate plausible compositions of the generated
hair within the input image.

The success of such a learning-based method depends
on the availability of a large-scale training set that covers
a wide range of facial hairstyles. To our knowledge, no
such dataset exists, so we ﬁll this void by creating a new
synthetic dataset that provides variation along many axes
such as the style, color, and viewpoint in a controlled man-
ner. We also collect a smaller dataset of real facial hair im-
ages we use to allow our method to better generalize to real
images. We demonstrate how our networks can be trained
using these datasets to achieve realistic results despite the
relatively small amount of real images used during training.
We introduce a user interface with tools that allow for in-
tuitive creation and manipulation of the vector ﬁelds used to
generate the input to our synthesis framework. We conduct
comparisons to alternative approaches, as well as extensive
ablations demonstrating the utility of each component of
our approach. Finally, we perform a perceptual study to
evaluate the realism of images authored using our approach,
and a user study to evaluate the utility of our proposed user
interface. These results demonstrate that our approach is
indeed a powerful and intuitive approach to quickly author
realistic illustrations of complex hairstyles.

2. Related Work

Texture Synthesis As a complete review of example-
based texture synthesis methods is out of the scope of
this paper, we refer the reader to the surveys of [67, 2]
for comprehensive overviews of modern texture synthe-
sis techniques.
In terms of methodology, example-based
texture synthesis approaches can be mainly categorized
into pixel-based methods [68, 20], stitching-based methods
[19, 42, 44], optimization-based approaches [41, 27, 70, 36]
and appearance-space texture synthesis [46]. Close to our
work, Lukáˇc et al. [50] present a method that allows users
to paint using the visual style of an arbitrary example tex-
ture. In [49], an intuitive editing tool is developed to support
example-based painting that globally follows user-speciﬁed
shapes while generating interior content that preserves the
textural details of the source image. This tool is not specif-
ically designed for hair synthesis, however, and thus lacks
local controls that users desire, as shown by our user study.
Recently, many researchers have attempted to leverage
neural networks for texture synthesis [24, 47, 54]. How-
ever, it remains nontrivial for such techniques to accomplish
simple editing operations, e.g. changing the local color or
structure of the output, which are necessary in our scenario.

Style Transfer The recent surge of style transfer research
suggests an alternate approach to replicating stylized fea-
tures from an example image to a target domain [23, 34,

2

48, 60]. However, such techniques make it possible to han-
dle varying styles from only one exemplar image. When
considering multiple frames of images, a number of works
have been proposed to extend the original technique to han-
dle video [59, 62] and facial animations [22]. Despite the
great success of such neural-based style transfer techniques,
one key limitation lies in their inability to capture ﬁne-scale
texture details. Fišer et al. [21] present a non-parametric
model that is able to reproduce such details. However, the
guidance channels employed in their approach is specially
tailored for stylized 3D rendering, limiting its application.

Hair Modeling Hair is a crucial component for photore-
alistic avatars and CG characters. In professional produc-
tion, human hair is modeled and rendered with sophisticated
devices and tools [11, 39, 69, 73]. We refer to [66] for an ex-
tensive survey of hair modeling techniques. In recent years,
several multi-view [51, 29] and single-view [9, 8, 30, 7]
hair modeling methods have been proposed. An automatic
pipeline for creating a full head avatar from a single portrait
image has also been proposed [31]. Despite the large body
of work in hair modeling, however, techniques applicable to
facial hair reconstruction remain largely unexplored. In [3],
a coupled 3D reconstruction method is proposed to recover
both the geometry of sparse facial hair and its underlying
skin surface. More recently, Hairbrush [71] demonstrates an
immersive data-driven modeling system for 3D strip-based
hair and beard models.

Image Editing
Interactive image editing has been exten-
sively explored in computer graphics community over the
past decades. Here, we only discuss prior works that are
highly related to ours. In the seminal work of Bertalmio et
al. [4], a novel technique is introduced to digitally inpaint
missing regions using isophote lines. Pérez et al. [56] later
propose a landmark algorithm that supports general interpo-
lation machinery by solving Poisson equations. Patch-based
approaches [19, 5, 13, 1, 14] provide a popular alternative
solution by using image patches adjacent to missing con-
text or in a dedicated source image to replace the missing
regions. Recently, several techniques [32, 61, 76, 55] based
on deep learning have been proposed to translate the content
of a given input image to a target domain.

Closer to our work, a number of works investigate edit-
ing techniques that directly operate on semantic image at-
tributes. Nguyen et al. [53] propose to edit and synthesize
beards by modeling faces as a composition of multiple lay-
ers. Mohammed et al. [52] perform facial image editing by
leveraging a parametric model learned from a large facial
image database. Kemelmacher-Shlizerman [38] presents a
system that enables editing the visual appearance of a target
portrait photo by replicating the visual appearance from a
reference image. Inspired by recent advances in deep neu-

ral networks, Brock et al. [6] propose a neural algorithm to
make large semantic changes to natural images. This tech-
nique has inspired follow-up works which leverage deep
generative networks for eye inpainting [18], semantic fea-
ture interpolation [65] and face completion [72]. The ad-
vent of generative adversarial networks (GANs) [26] has
inspired a large body of high-quality image synthesis and
editing approaches [10, 74, 75, 17, 63] using the power of
GANs to synthesize complex and realistic images. The lat-
est advances in sketch [57, 33] or contour [16] based fa-
cial image editing enables users to manipulate facial fea-
tures via intuitive sketching interfaces or copy-pasting from
exemplar images while synthesizing results plausibly corre-
sponding to the provided input. While our system also uses
guide strokes for hair editing and synthesis, we ﬁnd that in-
tuitively synthesizing realistic and varied facial hair details
requires more precise control and a training dataset with
sufﬁcient examples of such facial hairstyles. Our interac-
tive system allows for editing both the color and orientation
of the hair, as well as providing additional tools to author
varying styles such as sparse or dense hair. Despite the sig-
niﬁcant research in the domain of image editing, few prior
works investigate high quality and intuitive synthesis of fa-
cial hair. Though Brock et al. [6] allows for adding or edit-
ing the overall appearance of the subject’s facial hair, their
results lack details and can only operate on low-resolution
images. To the best of our knowledge, we present the ﬁrst
interactive framework that is specially tailored for synthe-
sizing high-ﬁdelity facial hair with large variations.

3. Overview

In Sec. 4 we describe our network pipeline (Fig. 2),
the architectures of our networks, and the training process.
Sec. 5 describes the datasets we use, including the large syn-
thetic dataset we generate for the initial stage of our training
process, our dataset of real facial hair images we use for the
ﬁnal stage of training for reﬁnement, and our method for an-
notating these images with the guide strokes used as input
during training (Fig. 3). Sec. 6 describes the user interface
tools we provide to allow for intuitive and efﬁcient author-
ing of input data describing the desired hairstyle (Fig. 4).
Finally, Sec. 7 provides sample results (Fig. 5), comparisons
with alternative approaches (Figs. 6 and 7), an ablation anal-
ysis of our architecture and training process (Table 1), and
descriptions of the perceptual and user study we use to eval-
uate the quality of our results and the utility of our interface.

4. Network Architecture and Training

Given an image with a segmented region deﬁning the
area in which synthesis is to be performed, and a set of guide
strokes, we use a two-stage inference process that populates
the selected region of the input image with desired hairstyle

3

Figure 2: We propose a two-stage network architecture to synthesize realistic facial hair. Given an input image with a user-
provided region of interest and sparse guide strokes deﬁning the local color and structure of the desired hairstyle, the ﬁrst
stage synthesizes the hair in this region. The second stage reﬁnes and composites the synthesized hair into the input image.

as shown in Fig. 2. The ﬁrst stage synthesizes an initial
approximation of the content of the segmented region, while
the second stage reﬁnes this initial result and adjusts it to
allow for appropriate compositing into the ﬁnal image.

Initial Facial Hair Synthesis. The input to the ﬁrst net-
work consists of a 1-channel segmentation map of the tar-
get region, and a 4-channel (RGBA) image of the provided
guide strokes within this region. The output is a synthesized
approximation of the hair in the segmented region.

The generator network is an encoder-decoder architec-
ture extending upon the image-to-image translation network
of [32]. We extend the decoder architecture with a ﬁnal
3x3 convolution layer, with a step size of 1 and 1-pixel
padding, to reﬁne the ﬁnal output and reduce noise. To ex-
ploit the rough spatial correspondence between the guide
strokes drawn on the segmented region of the target image
and the expected output, we utilize skip connections [58] to
capture low-level details in the synthesized image.

We train this network using the L1 loss between the
ground-truth hair region and the synthesized output. We
compute this loss only in the segmented region encourag-
ing the network to focus its capacity on synthesizing the
facial hair with no additional compositing constraints. We
also employ an adversarial loss [26] by using a discrimi-
nator based on the architecture of [32]. We use a condi-
tional discriminator, which accepts both the input image
channels and the corresponding synthesized or real image.
This discriminator is trained in conjunction with the gen-
erator to determine whether a given hair image is real or
synthesized, and whether it plausibly corresponds to the
speciﬁed input. Finally, we use a perceptual loss met-
ric [35, 25], represented using a set of higher-level feature
maps extracted from a pre-existing image classiﬁcation net-
work (i.e., VGG-19 [64]). This is effective in encouraging
the network to synthesize results with content that corre-
sponds well with plausible images of real hair. The ﬁnal
loss L(Is, Igt ) between the synthesized (Is) and ground truth

facial hair images (Igt ) is thus:

L f (Is, Igt ) = ω1L1(Is, Igt )+ωadvLadv(Is, Igt )+ωperLper(Is, Igt ),

(1)
where L1, Ladv, and Lper denote the L1, adversarial, and per-
ceptual losses respectively. The relative weighting of these
losses is determined by ω1, ωadv, and ωper. We set these
weights (ω1 = 50, ωadv = 1, ωper = 0.1), such that the av-
erage gradient of each loss is at the same scale. We ﬁrst
train this network until convergence on the test set using
our large synthetic dataset (see Sec. 5). It is then trained
in conjunction with the reﬁnement/compositing network on
the smaller real image dataset to allow for better generaliza-
tion to unconstrained real-world images.

Reﬁnement and Compositing. Once the initial facial
hair region is synthesized, we perform reﬁnement and com-
positing into the input image. This is achieved by a second
encoder-decoder network. The input to this network is the
output of the initial synthesis stage, the corresponding seg-
mentation map, and the segmented target image (the target
image with the region to be synthesized covered by the seg-
mentation mask). The output is the image with the synthe-
sized facial hair reﬁned and composited into it.

The architecture of the second generator and discrimina-
tor networks are identical to the ﬁrst network, with only the
input channel sizes adjusted accordingly. While we use the
adversarial and perceptual losses in the same manner as the
previous stage, we deﬁne the L1 loss on the entire synthe-
sized image. However, we increase the weight of this loss
by a factor of 0.5 in the segmented region containing the fa-
cial hair. The boundary between the synthesized facial hair
region and the rest of the image is particularly important for
plausible compositions. Using erosion/dilation operations
on the segmented region (with a kernel size of 10 for each
operation), we compute a mask covering this boundary. We
further increase the weight of the loss for these boundary
region pixels by a factor of 0.5. More details on the training
process can be found in the appendix.

4

5. Dataset

To train a network to synthesize realistic facial hair, we
need a sufﬁcient number of training images to represent
the wide variety of existing facial hairstyles (e.g., vary-
ing shape, length, density, and material and color prop-
erties), captured under varying conditions (e.g., different
viewpoints). We also need a method to represent the distin-
guishing features of these hairstyles in a simple and abstract
manner that can be easily replicated by a novice user.

Figure 3: We train our network with both real (row 1,
columns 1-2) and synthetic (row 1, columns 3-4) data. For
each input image we have a segmentation mask denoting
the facial hair region, and a set of guide strokes (row 2) that
deﬁne the hair’s local structure and appearance.

Data collection To capture variations across different fa-
cial hairstyles in a controlled manner, we generate a large-
scale synthetic dataset using the Whiskers plugin [45] pro-
vided for the Daz 3D modeling framework [15]. This plu-
gin provides 50 different facial hairstyles (e.g. full beards,
moustaches, goatees), with parameters controlling the color
and length of the selected hairstyle. The scripting interface
provided by this modeling framework allows for program-
matically adjusting the aforementioned parameters and ren-
dering the corresponding images. By rendering the al-
pha mask for the depicted facial hair, we automatically ex-
tract the corresponding segmentation map. For each facial
hairstyle, we synthesize it at 4 different lengths and 8 dif-
ferent colors. We render each hairstyle from 19 viewpoints
sampled by rotating the 3D facial hair model around its
central vertical axis in the range [−90°, 90°] at 10° inter-
vals, where 0° corresponds to a completely frontal view and
90° corresponds to a proﬁle view (see Fig. 3, columns 3-
4 for examples of these styles and viewpoints). We use the
Iray [37] physically-based renderer to generate 30400 facial
hair images with corresponding segmentation maps.

To ensure our trained model generalizes to real images,
we collect and manually segment the facial hair region in
a small dataset of such images (approximately 1300 im-
ages) from online image repositories containing a variety
of styles, e.g. short, stubble, long, dense, curly, and straight,
and large variations in illumination, pose and skin color.

5

Dataset Annotation Given input images with masks de-
noting the target region to be synthesized, we require guide
strokes providing an abstract representation of the desired
facial hair properties (e.g., the local color and shape of
the hair). We simulate guide strokes by integrating a vec-
tor ﬁeld computed based on the approach of
[43], which
computes the dominant local orientation from the per-pixel
structure tensor, then produces abstract representations of
images by smoothing them using line integral convolution
in the direction of minimum change. Integrating at points
randomly sampled in the vector ﬁeld extracted from the seg-
mented hair region in the image produces guide lines that
resemble the types of strokes speciﬁed by the users. These
lines generally follow prominent wisps or strands of facial
hair in the image (see Fig. 3).

6. Interactive Editing

We provide an interactive user interface with tools to per-
form intuitive facial hair editing and synthesis in an arbi-
trary input image. The user ﬁrst speciﬁes the hair region
via the mask brush in the input image, then draws guide
strokes within the mask abstractly describing the overall de-
sired hairstyle. Our system provides real-time synthesized
resulted after each edit to allow for iterative reﬁnement with
instant feedback. Please refer to the supplementary video
for example sessions and the appendix for more details on
our user interface. Our use of guide strokes extracted from
vector ﬁelds during training enables the use of various in-
tuitive and lightweight tools to facilitate the authoring pro-
cess. In addition, the generative power of our network al-
lows for synthesizing a rough initial approximation of the
desired hairstyle with minimal user input.

Guide stroke initialization. We provide an optional ini-
tialization stage where an approximation of the desired
hairstyle is generated given only the input image, segmenta-
tion mask, and a corresponding color for the masked region.
This is done by adapting our training procedure to train
a separate set of networks with the same architectures de-
scribed in Sec. 4 using this data without the aforementioned
guide strokes. Given a segmented region and the mean RGB
color in this region, the network learns a form of conditional
inpainting, synthesizing appropriate facial hair based on the
region’s size, shape, color, and the context provided by the
unmasked region of the image. For example, using small
masked regions with colors close to the surrounding skin
tone produces sparse, short facial hair, while large regions
with a color radically different from the skin tone produces
longer, denser hairstyles. The resulting facial hair is realis-
tic enough to extract an initial set of guide strokes from the
generated image as is done with real images (see Sec. 5).
Fig. 4 (top row) demonstrates this process.

region, we can generate a relatively long, mostly opaque
style (column 4) or a shorter, stubbly and more translucent
style (column 7). Please consult the supplementary video
for live recordings of several editing sessions and timing
statistics for the creation of these example results.

Perceptual study To evaluate the perceived realism of the
editing results generated by our system, we conducted a per-
ceptual study in which 11 subjects viewed 10 images of
faces with only real facial hair and 10 images with facial
hair manually created using our method, seen in a random
order. Users observed each image for up to 10 seconds and
decided whether the facial hair was real or fake/synthesized.
Real images were deemed real 80% of the time, while edited
images were deemed real 56% of the time. In general, facial
hair synthesized with more variation in color, texture, and
density were perceived as real, demonstrating the beneﬁts
of the local control tools in our interface. Overall, our sys-
tem’s results were perceived as generally plausible by all of
the subjects, demonstrating the effectiveness of our method.

Comparison with naive copy-paste A straightforward
solution to facial hair editing is to simply copy similar fa-
cial hairstyles from a reference image. While this may work
for reference images depicting simple styles captured under
nearly identical poses and lighting conditions to those in the
the target photograph, slight disparities in these conditions
result in jarring incoherence between the copied region and
the underlying image. In contrast, our method allows for
ﬂexible and plausible synthesis of various styles, and en-
ables the easy alteration of details, such as the shape and
color of the style depicted in the reference photograph to al-
low for more variety in the ﬁnal result. See Fig. 6 for some
examples of copy-pasting vs. our method. Note that when
copy-pasting, the total time to cut, paste, and transform (ro-
tate and scale) the copied region to match the underlying
image was in the range of 2-3 minutes, which is compara-
ble to the amount of time spent when using our method.

Comparison with texture synthesis We compare our
method to Brushables [49], which has an intuitive interface
for orientation maps to synthesize images that match the
target shape and orientation while maintaining the textural
details of a reference image. We can use Brushables to syn-
thesize facial hair by providing it with samples of a real fa-
cial hair image and orientation map, as shown in Fig. 7. For
comparison, with our system we draw strokes in the same
masked region on the face image. While Brushables synthe-
sizes hair regions matching the provided orientation map,
our results produce a more realistic hair distribution and ap-
pear appropriately volumetric in nature. Our method also
handles skin tones noticeably better near hair boundaries
and sparse, stubbly regions. Our method takes 1-2 seconds
to process each input operation, while the optimization in
Brushables takes 30-50 seconds for the same image size.

Figure 4: Editing examples. Row 1: Synthesizing an initial
estimate given a user-speciﬁed mask and color. Extracting
the vector ﬁeld from the result allows for creating an initial
set of strokes that can be then used to perform local edits.
Row 2: Editing the extracted vector ﬁeld to change the fa-
cial hair structure while retaining the overall color. Row 3:
Changing the color ﬁeld while using the vector ﬁeld from
the initial synthesis result allows for the creation of strokes
with different colors but similar shapes to the initially gen-
erated results. Row 4: Editing the strokes extracted from the
initial synthesis result allows for subtle updates, e.g. making
the beard sparser around the upper cheeks.

Guide stroke editing These initial strokes provide a rea-
sonable initialization the user’s editing. The vector ﬁeld
used to compute these strokes and the initial synthesis re-
sult, which acts as the underlying color ﬁeld used to com-
pute the guide stroke color, can also be edited. As they
are changed, we automatically repopulate the edited region
with strokes corresponding to the speciﬁed changes. We
provide brush tools to make such modiﬁcations to the color
or vector ﬁelds, as seen in Fig. 4. The user can adjust the
brush radius to alter the size of the region affected region,
as well as the intensity used when blending with previous
brush strokes. The users can also add, delete, or edit the
color of guide strokes to achieve the desired alterations.

7. Results

We show various examples generated by our system in
Figs. 1 and 5. It can be used to synthesize hair from scratch
(Fig. 5, rows 1-2) or to edit existing facial hair (Fig. 5, rows
3-4). As shown, our system can generate facial hair of vary-
ing overall color (red vs. brown), length (trimmed vs. long),
density (sparse vs. dense), and style (curly vs. straight).
Row 2 depicts a complex example of a white, sparse beard
on an elderly subject, created using light strokes with vary-
ing transparency. By varying these strokes and the masked

6

Figure 5: Example results. We show several example applications, including creating and editing facial hair on subjects with
no facial hair, as well as making modiﬁcations to the overall style and color of facial hair on bearded individuals.

L1↓

Isola et al. [32]
23.78
0.0304 168.5030 332.69
274.88
24.63
Single Network
0.0298 181.75
24.78
334.51
Ours, w/o GAN 0.0295 225.75
23.20
Ours, w/o VGG 0.0323 168.3303 370.19
Ours, w/o synth. 0.0327
23.55
413.09
Ours, only synth. 0.0547 235.6273 1747.00 16.11
0.0275 119.00
24.31
Ours

VGG↓ MSE↓ PSNR↑ SSIM↑ FID↓
0.66 121.18
0.67
75.32
0.70 116.42
67.82
0.63
0.62
91.99
0.60 278.17
53.15
0.68

291.83

234.5

Table 1: Quantitative ablation analysis.

Ablation analysis As described in Sec. 4, we use a two-
stage network pipeline trained with perceptual and adver-
sarial loss, trained with both synthetic and real images. We
show the importance of each component with an ablation
study. With each component, our networks produce much
higher quality results than the baseline network of [32].

A quantitative comparison is shown in Table 1, in which
we summarize the loss values computed over our test data
set using several metrics, using 100 challenging ground
truth validation images not used when computing the train-
ing or testing loss. While using some naive metrics varia-
tions on our approach perform comparably well to our ﬁnal
approach, we note that ours outperforms all of the others

Figure 6: Comparison to naive copy-pasting images from
reference photographs. Aside from producing more plau-
sible results, our approach enables editing the hair’s color
(row 1, column 5) and shape (row 2, column 5).

Figure 7: Results of our comparison with Brushables.

7

(a) Input

(b) Isola et al. (c) Single Network (d) w/o GAN

(e) w/o VGG (f) w/o synth. data (g) Ours, ﬁnal (h) Ground truth

Figure 8: Qualitative comparisons for ablation study.

Figure 9: Scalp hair synthesis and compositing examples.

in terms of the Fréchet Inception Distance (FID) [28], as
well as the MSE loss on the VGG features computed for
the synthesized and ground truth images. This indicates that
our images are perceptually closer to the actual ground truth
images. Selected qualitative examples of the results of this
ablation analysis can be seen in Fig. 8. More can be found
in the appendix.

User study. We conducted a preliminary user study to
evaluate the usability of our system. The study included 8
users, of which one user was a professional technical artist.
The participants were given a reference portrait image and
asked to create similar hair on a different clean-shaven sub-
ject via our interface. Overall, participants were able to
achieve reasonable results. From the feedback, the partici-
pants found our system novel and useful. When asked what
features they found most useful, some users commented that
they liked the ability to create a rough approximation of the
target hairstyle given only a mask and average color. Oth-
ers strongly appreciated the color and vector ﬁeld brushes,
as these allowed them to separately change the color and
structure of the initial estimate, and to change large regions
of the image without drawing each individual stroke with
the appropriate shape and color. Please refer to the appendix
for the detailed results of the user study and example results
created by the participants.

Application to non-facial hair. While we primarily focus
on the unique challenges of synthesizing and editing facial
hair in this work, our method can easily be extended to scalp
hair with suitable training data. To this end, we reﬁne our
networks trained on facial hair with an additional training

stage using 5320 real images with corresponding scalp hair
segmentations, much in the same manner as we reﬁne our
initial network trained on synthetic data. This dataset was
sufﬁcient to obtain reasonable scalp synthesis and editing
results. See Fig. 9 for scalp hair generation results. Inter-
estingly, this still allows for the synthesis of plausible facial
hair along with scalp hair within the same target image us-
ing the same trained model, given appropriately masks and
guide strokes. Please consult the appendix for examples and
further details.

Figure 10: Limitations: our method does not produce satis-
factory results in some extremely challenging cases.

8. Limitations and Future Work

While we demonstrate impressive results, our approach
has several limitations. As with other data-driven algo-
rithms, our approach is limited by the amount of variation
found in the training dataset. Close-up images of high-
resolution complex structures fail to capture all the com-
plexity of the hair structure, limiting the plausibility of the
synthesized images. As our training datasets mostly con-
sist of images of natural hair colors, using input with very
unusual hair colors also causes noticeable artifacts. See
Fig. 10 for examples of these limitations.

We demonstrate that our approach, though designed to
address challenges speciﬁc to facial hair, synthesizes com-
pelling results when applied to scalp hair given appropriate
training data. It would be interesting to explore how well
this approach extends to other related domains such as ani-
mal fur, or even radically different domains such as editing
and synthesizing images or videos containing ﬂuids or other
materials for which vector ﬁelds might serve as an appropri-
ate abstract representation of the desired image content.

8

References

[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans-
actions on Graphics-TOG, 28(3):24, 2009. 3

[2] Connelly Barnes and Fang-Lue Zhang. A survey of the state-
of-the-art in patch-based synthesis. Computational Visual
Media, 3(1):3–20, 2017. 2

[3] Thabo Beeler, Bernd Bickel, Gioacchino Noris, Paul Beards-
ley, Steve Marschner, Robert W Sumner, and Markus Gross.
Coupled 3d reconstruction of sparse facial hair and skin.
ACM Transactions on Graphics (ToG), 31(4):117, 2012. 3
[4] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
In Proceedings of
Image inpainting.
Coloma Ballester.
the 27th annual conference on Computer graphics and in-
teractive techniques, pages 417–424. ACM Press/Addison-
Wesley Publishing Co., 2000. 3

[5] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and
Simultaneous structure and texture im-
IEEE transactions on image processing,

Stanley Osher.
age inpainting.
12(8):882–889, 2003. 3

[6] Andrew Brock, Theodore Lim, James M Ritchie, and Nick
Weston. Neural photo editing with introspective adversarial
networks. arXiv preprint arXiv:1609.07093, 2016. 3

[7] Menglei Chai, Tianjia Shao, Hongzhi Wu, Yanlin Weng, and
Kun Zhou. Autohair: Fully automatic hair modeling from
a single image. ACM Transactions on Graphics (TOG),
35(4):116, 2016. 3

[8] Menglei Chai, Lvdi Wang, Yanlin Weng, Xiaogang Jin, and
Kun Zhou. Dynamic hair manipulation in images and videos.
ACM Trans. Graph., 32(4):75:1–75:8, July 2013. 3

[9] Menglei Chai, Lvdi Wang, Yanlin Weng, Yizhou Yu, Bain-
ing Guo, and Kun Zhou. Single-view hair modeling for por-
trait manipulation. ACM Transactions on Graphics (TOG),
31(4):116, 2012. 3

[10] Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel-
stein. Pairedcyclegan: Asymmetric style transfer for apply-
In 2018 IEEE Conference on
ing and removing makeup.
Computer Vision and Pattern Recognition (CVPR), 2018. 3
[11] Byoungwon Choe and Hyeong-Seok Ko. A statistical
wisp model and pseudophysical approaches for interactive
hairstyle generation. IEEE Transactions on Visualization and
Computer Graphics, 11(2):160–170, 2005. 3

[12] Ronan Collobert, Samy Bengio, and Johnny Marithoz.
Torch: A modular machine learning software library, 2002.
16

[13] Antonio Criminisi, Patrick Pérez, and Kentaro Toyama.
Region ﬁlling and object removal by exemplar-based im-
IEEE Transactions on image processing,
age inpainting.
13(9):1200–1212, 2004. 3

[14] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B
Goldman, and Pradeep Sen. Image melding: Combining in-
consistent images using patch-based synthesis. ACM Trans.
Graph., 31(4):82–1, 2012. 3

[15] Daz Productions, 2017. https://www.daz3d.com/. 5
[16] Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, and
William T Freeman. Sparse, smart contours to represent

In Proceedings of the IEEE Conference
and edit images.
on Computer Vision and Pattern Recognition, pages 3511–
3520, 2018. 3

[17] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In Advances in neural information process-
ing systems, pages 1486–1494, 2015. 3

[18] Brian Dolhansky and Cristian Canton Ferrer. Eye in-painting
arXiv

with exemplar generative adversarial networks.
preprint arXiv:1712.03999, 2017. 3

[19] Alexei A. Efros and William T. Freeman. Image quilting for
In Proceedings of the 28th
texture synthesis and transfer.
Annual Conference on Computer Graphics and Interactive
Techniques, SIGGRAPH ’01, pages 341–346. ACM, 2001.
2, 3

[20] Alexei A. Efros and Thomas K. Leung. Texture synthesis
by non-parametric sampling. In IEEE ICCV, pages 1033–,
1999. 2

[21] Jakub Fišer, Ondˇrej Jamriška, Michal Lukáˇc, Eli Shecht-
man, Paul Asente, Jingwan Lu, and Daniel S`ykora. Stylit:
illumination-guided example-based stylization of 3d render-
ings. ACM Transactions on Graphics (TOG), 35(4):92, 2016.
3

[22] Jakub Fišer, Ondˇrej Jamriška, David Simons, Eli Shechtman,
Jingwan Lu, Paul Asente, Michal Lukáˇc, and Daniel Sýkora.
Example-based synthesis of stylized facial animations. ACM
Trans. Graph., 36(4):155:1–155:11, July 2017. 3

[23] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
A neural algorithm of artistic style. CoRR, abs/1508.06576,
2015. 2

[24] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
In
Texture synthesis using convolutional neural networks.
Proceedings of the 28th International Conference on Neural
Information Processing Systems, NIPS’15, pages 262–270,
Cambridge, MA, USA, 2015. MIT Press. 2

[25] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In
age style transfer using convolutional neural networks.
Proc. CVPR, pages 2414–2423, 2016. 4

[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
In Advances
Yoshua Bengio. Generative adversarial nets.
in neural information processing systems, pages 2672–2680,
2014. 3, 4

[27] Jianwei Han, Kun Zhou, Li-Yi Wei, Minmin Gong, Hujun
Bao, Xinming Zhang, and Baining Guo. Fast example-based
surface texture synthesis via discrete optimization. The Vi-
sual Computer, 22(9-11):918–925, 2006. 2

[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, Günter Klambauer, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a
nash equilibrium. CoRR, abs/1706.08500, 2017. 8

[29] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Robust
hair capture using simulated examples. ACM Transactions
on Graphics (TOG), 33(4):126, 2014. 3

[30] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Single-
view hair modeling using a hairstyle database. ACM Trans.
Graph., 34(4):125:1–125:9, July 2015. 3

9

[31] Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jae-
woo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-
Chun Chen, and Hao Li. Avatar digitization from a single
image for real-time rendering. ACM Transactions on Graph-
ics (TOG), 36(6):195, 2017. 3

[32] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. arXiv preprint arXiv:1611.07004, 2016. 3,
4, 7, 13

[33] Youngjoo Jo and Jongyoul Park. SC-FEGAN: face editing
generative adversarial network with user’s sketch and color.
CoRR, abs/1902.06838, 2019. 3

[34] Justin Johnson, Alexandre Alahi, and Fei-Fei Li. Percep-
tual losses for real-time style transfer and super-resolution.
CoRR, abs/1603.08155, 2016. 2

[35] Justin Johnson, Alexandre Alahi, and Fei-Fei Li. Percep-
tual losses for real-time style transfer and super-resolution.
CoRR, abs/1603.08155, 2016. 4

[36] Alexandre Kaspar, Boris Neubert, Dani Lischinski, Mark
Pauly, and Johannes Kopf. Self tuning texture optimization.
Computer Graphics Forum, 34(2):349–359, 2015. 2

[37] Alexander Keller, Carsten Wächter, Matthias Raab, Daniel
Seibert, Dietger van Antwerpen, Johann Korndörfer, and
Lutz Kettner. The iray light transport simulation and ren-
dering system. CoRR, abs/1705.01263, 2017. 5

[38] Ira Kemelmacher-Shlizerman. Transﬁguring portraits. ACM

Transactions on Graphics (TOG), 35(4):94, 2016. 3

[39] Tae-Yong Kim and Ulrich Neumann.

Interactive multires-
olution hair modeling and editing. ACM Transactions on
Graphics (TOG), 21(3):620–629, 2002. 3

[40] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014. 16
[41] Vivek Kwatra, Irfan Essa, Aaron Bobick, and Nipun Kwa-
tra. Texture optimization for example-based synthesis. ACM
Trans. Graph., 24(3):795–802, July 2005. 2

[42] Vivek Kwatra, Arno Schödl, Irfan Essa, Greg Turk, and
Aaron Bobick. Graphcut textures: Image and video synthe-
sis using graph cuts. In Proc. SIGGRAPH, SIGGRAPH ’03,
pages 277–286. ACM, 2003. 2

[43] Jan Eric Kyprianidis and Henry Kang.

Image and video
abstraction by coherence-enhancing ﬁltering. Computer
Graphics Forum, 30(2):593–â ˘A ¸S602, 2011. Proceedings Eu-
rographics 2011. 5

[44] Anass Lasram and Sylvain Lefebvre. Parallel patch-based
the Fourth ACM
In Proceedings of
texture synthesis.
SIGGRAPH/Eurographics conference on High-Performance
Graphics, pages 115–124. Eurographics Association, 2012.
2
[45] Laticis

https://www.daz3d.com/

Imagery, 2017.

whiskers-for-genesis-3-male-s. 5

[46] Sylvain Lefebvre and Hugues Hoppe. Appearance-space tex-
ture synthesis. ACM Trans. Graph., 25(3):541–548, 2006. 2
[47] Chuan Li and Michael Wand. Precomputed real-time texture
synthesis with markovian generative adversarial networks.
CoRR, abs/1604.04382, 2016. 2

[48] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing
Kang. Visual attribute transfer through deep image analogy.
ACM Trans. Graph., 36(4):120:1–120:15, July 2017. 2

[49] Michal Lukáˇc, Jakub Fiser, Paul Asente, Jingwan Lu, Eli
Shechtman, and Daniel Sýkora. Brushables: Example-based
edge-aware directional texture painting. Comput. Graph. Fo-
rum, 34(7):257–267, 2015. 2, 6, 13

[50] Michal Lukáˇc, Jakub Fišer, Jean-Charles Bazin, Ondˇrej
Jamriška, Alexander Sorkine-Hornung, and Daniel Sýkora.
Painting by feature: Texture boundaries for example-based
image creation. ACM Trans. Graph., 32(4):116:1–116:8,
July 2013. 2

[51] Linjie Luo, Hao Li, Sylvain Paris, Thibaut Weise, Mark
Pauly, and Szymon Rusinkiewicz. Multi-view hair cap-
In Computer Vision and Pat-
ture using orientation ﬁelds.
tern Recognition (CVPR), 2012 IEEE Conference on, pages
1490–1497. IEEE, 2012. 3

[52] Umar Mohammed, Simon JD Prince, and Jan Kautz. Visio-
lization: generating novel facial images. ACM Transactions
on Graphics (TOG), 28(3):57, 2009. 3

[53] Minh Hoai Nguyen, Jean-Francois Lalonde, Alexei A Efros,
and Fernando De la Torre. Image-based shaving. Computer
Graphics Forum, 27(2):627–635, 2008. 3

[54] Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu,
Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli,
and Hao Li. Realistic dynamic facial textures from a sin-
gle image using gans. In IEEE International Conference on
Computer Vision (ICCV), pages 5429–5438, 2017. 2
[55] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2019. 3

[56] Patrick Pérez, Michel Gangnet, and Andrew Blake. Pois-
son image editing. ACM Transactions on graphics (TOG),
22(3):313–318, 2003. 3

[57] Tiziano Portenier, Qiyang Hu, Attila Szabó, Siavash Ar-
jomand Bigdeli, Paolo Favaro, and Matthias Zwicker.
Faceshop: Deep sketch-based face image editing. ACM
Trans. Graph., 37(4):99:1–99:13, July 2018. 3

[58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
CoRR, abs/1505.04597, 2015. 4

[59] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
report,

for videos.

Technical

Artistic style transfer
arXiv:1604.08610, 2016. 3

[60] Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and
Hao Li. Photorealistic facial texture inference using deep
neural networks. arXiv preprint arXiv:1612.00523, 2016. 2
[61] Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and
James Hays. Scribbler: Controlling deep image synthesis
In IEEE Conference on Computer
with sketch and color.
Vision and Pattern Recognition (CVPR), volume 2, 2017. 3
[62] Ahmed Selim, Mohamed Elgharib, and Linda Doyle. Paint-
ing style transfer for head portraits using convolutional
neural networks. ACM Transactions on Graphics (ToG),
35(4):129, 2016. 3

[63] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb.
Learning
from simulated and unsupervised images through adversarial

10

training. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2107–2116,
2017. 3

[64] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 4

[65] Paul Upchurch, Jacob Gardner, Kavita Bala, Robert Pless,
Noah Snavely, and Kilian Weinberger. Deep feature interpo-
lation for image content changes. In Proc. IEEE Conf. Com-
puter Vision and Pattern Recognition, pages 6090–6099,
2016. 3

[66] Kelly Ward, Florence Bertails, Tae-Yong Kim, Stephen R
Marschner, Marie-Paule Cani, and Ming C Lin. A sur-
vey on hair modeling: Styling, simulation, and rendering.
IEEE transactions on visualization and computer graphics,
13(2):213–234, 2007. 3

[67] Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk.
State of the art in example-based texture synthesis. In Eu-
rographics 2009, State of the Art Report, EG-STAR, pages
93–117. Eurographics Association, 2009. 2

[68] Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-
In Proceedings of the 27th
structured vector quantization.
Annual Conference on Computer Graphics and Interactive
Techniques, SIGGRAPH ’00, pages 479–488, 2000. 2
[69] Yanlin Weng, Lvdi Wang, Xiao Li, Menglei Chai, and Kun
Zhou. Hair interpolation for portrait morphing. Computer
Graphics Forum, 32(7):79–84, 2013. 3

[70] Y. Wexler, E. Shechtman, and M. Irani. Space-time comple-
IEEE Transactions on Pattern Analysis and

tion of video.
Machine Intelligence, 29(3):463–476, March 2007. 2
[71] Jun Xing, Koki Nagano, Weikai Chen, Haotian Xu, Li-yi
Wei, Yajie Zhao, Jingwan Lu, Byungmoon Kim, and Hao
Li. Hairbrush for immersive data-driven hair modeling. In
Proceedings of the 32Nd Annual ACM Symposium on User
Interface Software and Technology, UIST ’19, 2019. 3
[72] Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G
Schwing, Mark Hasegawa-Johnson, and Minh N Do. Seman-
tic image inpainting with deep generative models. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5485–5493, 2017. 3

[73] Cem Yuksel, Scott Schaefer, and John Keyser. Hair meshes.
ACM Transactions on Graphics (TOG), 28(5):166, 2009. 3
[74] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 5907–
5915, 2017. 3

[75] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and
Alexei A Efros. Generative visual manipulation on the nat-
ural image manifold. In European Conference on Computer
Vision, pages 597–613. Springer, 2016. 3

[76] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
translation using
arXiv preprint

Efros.
Unpaired image-to-image
cycle-consistent adversarial networks.
arXiv:1703.10593, 2017. 3

11

A1. Interactive Editing

Fig. 11 shows the interface of our system. As described
in the paper (Sec. 6), we provide tools for synthesizing a
coarse initialization of the target hairstyle given only the
user-drawn mask and selected color; separately manipulat-
ing the color and vector ﬁelds used to automatically extract
guide strokes of the appropriate shape and color from this
initial estimate; and drawing, removing, and changing in-
dividual strokes to make local edits to the ﬁnal synthesized
image. See Fig. 12 for an example of iterative reﬁnement
of an image using our provided input tools for mask cre-
ation and individual stroke drawing with the correspond-
ing output, and Fig. 13 for an example of vector/color ﬁeld
editing, that easily changes the structure and color of the
guide strokes. Also see Fig. 4 in the paper and the example
sessions in the supplementary video for examples of condi-
tional inpainting and other editing operations.

Figure 11: The user interface, consisting of global (a) and
contextual (b) toolbars, and input (c) and result preview (d)
canvases.

Fig. 14 shows an example of how the overall structure of
a synthesized hairstyle can be changed by making adjust-
ments to the structure of the user-provided guide strokes.
By using strokes with the overall colors of those in row 1,
column 1, but with different shapes, such as the smoother
and more coherent strokes as in row 2, column 1, we can
generate a correspondingly smooth and coherent hairstyle,
(row 2, column 2).

Facial hair reference database We provide a library of
sample images from our facial hair database that can be
used as visual references for target hairstyles. The user
can select colors from regions of these images for the ini-
tial color mask, individual strokes and brush-based color
ﬁeld editing. This allows users to easily choose colors
that represent the overall and local appearance of the de-
sired hairstyle. Users may also copy and paste selected
strokes from these images directly into the target region,
so as to directly emulate the appearance of the reference
image. This can be done using either the color of these se-

12

Figure 12: An example of interactive facial hair creation
from scratch on a clean-shaven target image. After creating
the initial mask (a), the user draws strokes in this region that
deﬁne the local color and shape of the hair (b-e). While a
single stroke (b) has little effect on much of the masked re-
gion (c), adding more strokes (d) results in more control
over the output (e). The user can also change the mask
shape or add more strokes (f) to adjust the overall hairstyle.

Figure 13: Changes to the vector and color ﬁelds cause cor-
responding changes in the guide strokes, and also the ﬁnal
synthesized results.

lected strokes in the reference image, or merely their shape
with the selected color and transparency settings so as to
emulate the local structure of the reference image within
the global structure and appearance of the target image.

A2. Scalp and Facial Hair Synthesis Results

As described in Sec. 7 and displayed in Fig. 9, we found
that introducing segmented scalp hair with correspond-
ing guide strokes (automatically extracted as described in
Sec. 5) allows for synthesizing high-quality scalp hair and
plausible facial hair with a single pair of trained networks to
perform initial synthesis, followed by reﬁnement and com-
positing.

We use 5320 real images with segmented scalp hair re-
gions for these experiments. We do this by adding a second
end-to-end training stage as described in Sec. A5 in which
the two-stage network pipeline is reﬁned using only these
scalp hair images. Interestingly, simply using the the real
scalp and facial hair dataset simultaneously did not produce
acceptable results. This suggests that the multi-stage re-
ﬁnement process we used to adapt our synthetic facial hair
dataset to real facial hair images is also useful for further
adapting the trained model to more general hairstyles.

than when using only one network. Adversarial loss adds
more ﬁne-scale details, while VGG perceptual loss sub-
stantially reduces noisy artifacts. Compared with networks
trained with only real image data, our ﬁnal result has clearer
deﬁnition for individual hair strands and has a higher dy-
namic range, which is preferable if users are to perform
elaborate image editing. With all these components, our
networks produce results of much higher quality than the
baseline network of [32].

A4. User study

We conducted a preliminary user study to evaluate the
usability of our system. The study included 8 users. 1 user
was a professional technical artist, while the others were
non-professionals, including novices with minimal to mod-
erate prior experience with technical drawing or image edit-
ing. When asked to rate their prior experience as a techni-
cal artist on a scale of 1 − 5, with 1 indicating no prior ex-
perience and 5 indicating a professionally trained technical
artist, the average score was 3.19.

Procedure The study consisted of three sessions: a warm-
up session (approximately 10 min), a target session (15-25
min), and an open session (10 min). The users were then
asked to provide feedback by answering a set of questions
to quantitatively and qualitatively evaluate their experience.
In the warm-up session, users were introduced to the in-
put operations and workﬂow and then asked to familiarize
themselves with these tools by synthesizing facial hair on
a clean-shaven source image similar to that seen in a ref-
erence image. For the target session, the participants were
given a new reference portrait image and asked to create
similar hair on a different clean-shaven subject via (1) our
interface, and (2) Brushables [49]. For Brushables, the user
was asked to draw an vector ﬁeld corresponding to the over-
all shape and orientation of the facial hairstyle in the target
image. A patch of facial hair taken directly from the target
image was used with this input to automatically synthesize
the output facial hair. For the open session, we let the par-
ticipants explore the full functionality of our system and un-
cover potential usability issues by creating facial hair with
arbitrary structures and colors.

Outcome Fig. 17 provides qualitative results from the
target session. Row 2, column depicts the result from
the professional artist, while other results are from non-
professionals with no artistic training. The users took be-
tween 6 − 19 minutes to create the target image using our
tool. The average session time was 14 minutes. The users
required an average of 116 strokes to synthesize the target
facial hairstyle on the source subject. 39 of these strokes
were required to draw and edit the initial mask deﬁning the

Figure 14: Subtle changes to the structure and appearance
of the strokes used for ﬁnal image synthesis cause corre-
sponding changes in the ﬁnal synthesized result.

Fig. 15 portrays several additional qualitative results
from these experiments (all other results seen in the paper
and supplementary video, with the exception of Fig. 9, were
generated using a model trained using only the synthetic and
real facial hair datasets). As can be seen, we can synthe-
size a large variety of hairstyles with varying structure and
appearance for both female (rows 1-2) and male (rows 3-5)
subjects using this model, and can synthesize both scalp and
facial hair simultaneously (rows 3-5, columns 6-7). Though
this increased ﬂexibility in the types of hair that can be syn-
thesized using this model comes with a small decrease in
quality in some types of facial hair quite different from that
seen in the scalp hair database (e.g., the short, sparse facial
hair seen in Fig. 5, row 2, column 7, which was synthe-
sized using a model trained using only facial hair images),
relatively dense facial hairstyles such as those portrayed in
Fig. 15 can still be plausibly synthesized simultaneously
with a wide variety of scalp hairstyles.

A3. Ablation Study Results

We show additional selected qualitative results from the
ablation study described in Sec. 7 in Fig. 16. With the ad-
dition of the reﬁnement network, our results contain more
subtle details and have fewer artifacts at skin boundaries

13

Figure 15: Example results for synthesizing scalp hair, both with and without facial hair. Rows 1-2 shows examples for female
subjects, while rows 3-5 depict male subjects. For the male subjects, columns 6-7 depict input and output to synthesize facial
hair with scalp hair. These results are generated using the same models trained on a combination of facial and scalp hair.

region in which synthesis is performed. The remaining 77
were brush strokes used to edit the color and vector ﬁelds
used to automatically generate strokes in this region, and to
draw the individual strokes used to perform the ﬁnal reﬁne-
ment. On average a user performed 19 brush strokes to edit
the vector ﬁeld, 17 brush strokes to edit the color ﬁeld, and
drew 41 individual strokes. We note that these numbers in-
clude individual strokes deleted by the user if their impact
on the resulting image was deemed unsatisfactory. Overall,
these numbers indicate, as does the provided feedback, that
the color and brush editing tools were useful in reducing the
number of individual strokes required to synthesize the ﬁnal
image. We use the original Brushables implementation for
comparison, which does not provide statistics on the num-
ber of operations performed by the user, and as such we can
not report these statistics for the Brushables session.

Feedback We asked the users to rate our method in terms
of ease-of-use and their perceived quality of the ﬁnal image
they created during the target session. On a scale of 1 − 5
(higher is better), the users rated our system 3.6 in terms
of ease of use, and 4.06 in terms of their synthesized result
matching the target facial hairstyle. When asked to measure
how satisﬁed they were with the result given the amount of
time they spent creating it and becoming familiar with the
system, the average score was 4.0. Furthermore, 100% of
the users preferred our system over Brushables for the task
of facial hair editing.

After being introduced to its interface, users spent be-
tween 3 − 5 minutes (4 minutes on average) working with
Brushables to attempt to synthesize the target hairstyle.
While less time was required to synthesize the results with
Brushables, the users generally found the results achieved
by copying regions of the source texture sample directly
into the speciﬁed target region to be very unsatisfactory. By

14

(a) Input

(b) Isola et al.

(c) Single Network

(d) w/o GAN

(e) w/o VGG (f) w/o synth. data

(g) Ours, ﬁnal

(h) Ground truth

Figure 16: Qualitative comparisons for our ablation study.

simply attempting to create an vector ﬁeld roughly match-
ing the structure of the target hairstyle and then synthesizing
the result, users had little control over the subtle local de-
tails necessary to synthesize a plausible result. Furthermore,
as Brushables required approximately 30 seconds to syn-
thesize the entire facial hairstyle given the complete user-
deﬁned vector ﬁeld, iterative experimentation was far more
difﬁcult than when using our approach, which allows for
immediately visualizing the results of minor editing opera-
tions. Thus, users chose not to experiment with Brushables
long enough to produce more satisfactory results.

Overall, the participants found our system novel and use-
ful. When asked what features they found most useful,
some users commented that they liked the ability to cre-
ate a rough approximation of the target hairstyle given only
a mask and average color. Others strongly appreciated the
color and orientation brushes, as these allowed them to sep-
arately change the color and structure of the initial estimate,
and to change large regions of the image without draw-
ing each individual stroke with the appropriate shape and
color. In contrast, drawing each individual stroke manually
was not perceived as especially useful, as it required signif-
icantly more effort to experiment with creating and remov-

ing individual strokes to produce a combination with the
appropriate shape and color to achieve the desired result.
However, overall participants were able to achieve reason-
able results such as those in Fig. 17 primarily relying on the
color and vector ﬁeld brushes to edit the initial synthesis
results produced when selecting the mask shape and color.
Relatively few individual strokes were ultimately required
to reﬁne the results.

This feedback suggests that the increasing level of gran-
ularity enabled by our system (creating a rough initial esti-
mate, modifying the local shape and color, and then reﬁning
small details with a few individual strokes) is an effective
approach. Furthermore, the participants reported that the
real-time synthesis of the generated image allowed for in-
tuitive iterative reﬁnement, which provided helpful visual
guidance crucial for producing satisfactory results.

During the open session at the end, users enjoyed ex-
perimenting with our tools to creatively generate unconven-
tional hairstyles with unusual shapes and colors. However,
as many of these styles were well outside the range of nat-
ural shapes and colors seen in the images used to train our
system, the results were less realistic than those constrained
to resemble a more conventional hairstyle.

15

A5. Implementation Details

Network Training. We train the ﬁrst network in our two-
stage pipeline ﬁrst with synthetic then with real data. Then,
we train both stages in an end-to-end manner with real im-
ages while keeping the losses for both stages.

When training the ﬁrst network individually, we use an
initial learning rate of 0.0002 and momentum of 0.5. The
learning rate is halved twice during this training process
such that in the ﬁnal epochs the learning rate is reduced
to 0.00005. During end-to-end training of both networks,
the initial learning rate is reduced to 0.0001 and a momen-
tum of 0.75 is used. As before, the learning rate is halved
twice during training, resulting in a ﬁnal learning rate of
0.000025.

Both architectures are fully convolutional and thus can
take input images of any resolution. However, we scale all
our training data to a resolution of 512 × 512. We train both
networks via the Adam optimizer [40] on an NVIDIA Titan
X GPU using the Torch framework [12]. We ﬁrst train each
stage of the networks for 50 epochs, which takes about 24
hours. Then reﬁne the ﬁrst network using real image data to
train for 25 epochs, which takes about 12 hours.

Runtime Performance. The user interface is designed to
allow for input using either a traditional mouse for novice
users, or the tablet and stylus tools used by digital artists.
For run-time interaction, passing one image through our
network takes a total of 600 milliseconds. We transmit the
input image to a server running our networks. The total
time between making an update to the input and seeing the
corresponding on average thus takes roughly 1.5 seconds.
The vector ﬁeld used for the initial stroke extraction is also
performed using CUDA for GPU acceleration, and takes
roughly 120 milliseconds for a 512 × 512 image.

A6. User Interaction Time

Each interactively generated result seen in the ﬁgures in
this paper (with the exception of some examples from the
user study described below) was made in no more than 15
minutes by a user with no artistic training. On average, the
depicted examples in this paper took between 2-6 minutes,
depending on the size and complexity of the hairstyle. Sim-
ple examples, such as the short hairstyle in Fig. 1. in the
main paper can be created in 2-3 minutes, while more com-
plex styles, such as the sparse beard in Fig. 7 (row 2, col-
umn 7) in the main paper, took 10-15 minutes. We refer
to the supplementary video for a live recording of several
editing sessions.

Figure 17: Example results generated by participants in our
user study, when asked to synthesize a style resembling the
target image (top left). No user had prior experience using
our interface. Subjects took an average of 14 minutes (and
no more than 19 minutes) to create the portrayed images.

16

