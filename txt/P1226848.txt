6
1
0
2
 
t
c
O
 
7
1
 
 
]
L
C
.
s
c
[
 
 
1
v
9
8
9
4
0
.
0
1
6
1
:
v
i
X
r
a

Cached Long Short-Term Memory Neural Networks
for Document-Level Sentiment Classiﬁcation

Jiacheng Xu† Danlu Chen‡ Xipeng Qiu∗‡ Xuanjing Huang‡
Software School, Fudan University †
School of Computer Science, Fudan University ‡
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University†‡
825 Zhangheng Road, Shanghai, China†‡
{jcxu13,dlchen13,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Recently, neural networks have achieved great
success on sentiment classiﬁcation due to their
ability to alleviate feature engineering. How-
ever, one of the remaining challenges is to
model long texts in document-level sentiment
classiﬁcation under a recurrent architecture
because of the deﬁciency of the memory unit.
To address this problem, we present a Cached
Long Short-Term Memory neural networks
(CLSTM) to capture the overall semantic in-
formation in long texts. CLSTM introduces
a cache mechanism, which divides memory
into several groups with different forgetting
rates and thus enables the network to keep
sentiment information better within a recur-
rent unit. The proposed CLSTM outperforms
the state-of-the-art models on three publicly
available document-level sentiment analysis
datasets.

1

Introduction

Sentiment classiﬁcation is one of the most widely
language processing techniques in
used natural
many areas, such as E-commerce websites, online
social networks, political orientation analyses (Wil-
son et al., 2009; O’Connor et al., 2010), etc.

Recently, deep learning approaches (Socher et al.,
2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016)
have gained encouraging results on sentiment clas-
siﬁcation, which frees researchers from handcrafted
feature engineering. Among these methods, Recur-
rent Neural Networks (RNNs) are one of the most

∗ Corresponding author.

prevalent architectures because of the ability to han-
dle variable-length texts.

Sentence- or paragraph-level sentiment analysis
expects the model to extract features from limited
source of information, while document-level senti-
ment analysis demands more on selecting and stor-
ing global sentiment message from long texts with
noises and redundant local pattern. Simple RNNs
are not powerful enough to handle the overﬂow and
to pick up key sentiment messages from relatively
far time-steps .

Efforts have been made to solve such a scalabil-
ity problem on long texts by extracting semantic in-
formation hierarchically (Tang et al., 2015a; Tai et
al., 2015), which ﬁrst obtain sentence representa-
tions and then combine them to generate high-level
document embeddings. However, some of these so-
lutions either rely on explicit a priori structural as-
sumptions or discard the order information within
a sentence, which are vulnerable to sudden change
or twists in texts especially a long-range one (Mc-
Donald et al., 2007; Mikolov et al., 2013). Re-
current models match people’s intuition of reading
word by word and are capable to model the intrinsic
relations between sentences. By keeping the word
order, RNNs could extract the sentence representa-
tion implicitly and meanwhile analyze the semantic
meaning of a whole document without any explicit
boundary.

Partially inspired by neural structure of human
brain and computer system architecture, we present
the Cached Long Short-Term Memory neural net-
works (CLSTM) to capture the long-range senti-
ment information. In the dual store memory model

proposed by Atkinson and Shiffrin (1968), memo-
ries can reside in the short-term “buffer” for a lim-
ited time while they are simultaneously strengthen-
ing their associations in long-term memory. Accord-
ingly, CLSTM equips a standard LSTM with a sim-
ilar cache mechanism, whose internal memory is di-
vided into several groups with different forgetting
rates. A group with high forgetting rate plays a role
as a cache in our model, bridging and transiting the
information to groups with relatively lower forget-
ting rates. With different forgetting rates, CLSTM
learns to capture, remember and forget semantics in-
formation through a very long distance.

Our main contributions are as follows:

• We introduce a cache mechanism to diversify
the internal memory into several distinct groups
with different memory cycles by squashing
their forgetting rates. As a result, our model can
capture the local and global emotional informa-
tion, thereby better summarizing and analyzing
sentiment on long texts in an RNN fashion.
• Beneﬁting from long-term memory unit with a
low forgetting rate, we could keep the gradi-
ent stable in the long back-propagation process.
Hence, our model could converge faster than a
standard LSTM.

• Our model outperforms state-of-the-art meth-
ods by a large margin on three document-level
datasets (Yelp 2013, Yelp 2014 and IMDB). It
worth noticing that some of the previous meth-
ods have utilized extra user and product infor-
mation.

2 Related Work

In this section, we brieﬂy introduce related work in
two areas: First, we discuss the existing document-
level sentiment classiﬁcation approaches; Second,
we discuss some variants of LSTM which address
the problem on storing the long-term information.

2.1 Document-level Sentiment Classiﬁcation

Document-level sentiment classiﬁcation is a sticky
task in sentiment analysis (Pang and Lee, 2008),
which is to infer the sentiment polarity or intensity
of a whole document. The most challenging part is
that not every part of the document is equally in-
formative for inferring the sentiment of the whole

document (Pang and Lee, 2004; Yessenalina et al.,
2010). Various methods have been investigated and
explored over years (Wilson et al., 2005; Pang and
Lee, 2008; Pak and Paroubek, 2010; Yessenalina
et al., 2010; Moraes et al., 2013). Most of these
methods depend on traditional machine learning al-
gorithms, and are in need of effective handcrafted
features.

Recently, neural network based methods are
prevalent due to their ability of learning discrimina-
tive features from data (Socher et al., 2013; Le and
Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015)
and Tai et al. (2015) integrate a tree-structured
model
into LSTM for better semantic composi-
tion; Bhatia et al. (2015) enhances document-level
sentiment analysis by using extra discourse par-
ing results. Most of these models work well on
sentence-level or paragraph-level sentiment classiﬁ-
cation. When it comes to the document-level sen-
timent classiﬁcation, a bottom-up hierarchical strat-
egy is often adopted to alleviate the model complex-
ity (Denil et al., 2014; Tang et al., 2015b).

2.2 Memory Augmented Recurrent Models

Although it is widely accepted that LSTM has more
long-lasting memory units than RNNs, it still suffers
from “forgetting” information which is too far away
from the current point (Le et al., 2015; Karpathy et
al., 2015). Such a scalability problem of LSTMs is
crucial to extend some previous sentence-level work
to document-level sentiment analysis.

Various models have been proposed to increase
the ability of LSTMs to store long-range informa-
tion (Le et al., 2015; Salehinejad, 2016) and two
kinds of approaches gain attraction. One is to aug-
ment LSTM with an external memory (Sukhbaatar
et al., 2015; Monz, 2016), but they are of poor per-
formance on time because of the huge external mem-
ory matrix. Unlike these methods, we fully exploit
the potential of internal memory of LSTM by adjust-
ing its forgetting rates.

The other one tries to use multiple time-scales
to distinguish different states (El Hihi and Bengio,
1995; Koutn´ık et al., 2014; Liu et al., 2015). They
partition the hidden states into several groups and
each group is activated and updated at different fre-
quencies (e.g. one group updates every 2 time-step,
the other updates every 4 time-step). In these meth-

ods, different memory groups are not fully inter-
connected, and the information is transmitted from
faster groups to slower ones, or vice versa.

However, the memory of slower groups are not
updated at every step, which may lead to senti-
ment information loss and semantic inconsistency.
In our proposed CLSTM, we assign different forget-
ting rates to memory groups. This novel strategy
enable each memory group to be updated at every
time-step, and every bit of the long-term and short-
term memories in previous time-step to be taken into
account when updating.

3 Long Short-Term Memory Networks

Long short-term memory network (LSTM) (Hochre-
iter and Schmidhuber, 1997) is a typical recurrent
neural network, which alleviates the problem of gra-
dient diffusion and explosion. LSTM can capture
the long dependencies in a sequence by introducing
a memory unit and a gate mechanism which aims
to decide how to utilize and update the information
kept in memory cell.

Formally, the update of each LSTM component

can be formalized as:

i(t) = σ(Wix(t) + Uih(t−1)),
f (t) = σ(Wf x(t) + Uf h(t−1)),
o(t) = σ(Wox(t) + Uoh(t−1)),
˜c(t) = tanh(Wcx(t) + Uch(t−1)),
c(t) = f (t) (cid:12) c(t−1) + i(t) (cid:12) ˜c(t),
h(t) = o(t) (cid:12) tanh(c(t)),

(1)

(2)

(3)

(4)

(5)

(6)

where σ is the logistic sigmoid function. Opera-
tor (cid:12) is the element-wise multiplication operation.
i(t), f (t), o(t) and c(t) are the input gate, forget gate,
output gate, and memory cell activation vector at
time-step t respectively, all of which have the same
size as the hidden vector h(t) ∈ RH . Wi, Wf ,
Wo ∈ RH×d and Ui, Uf , Uo ∈ RH×H are train-
able parameters. Here, H and d are the dimension-
ality of hidden layer and input respectively.

4 Cached Long Short-Term Memory

Neural Network

LSTM is supposed to capture the long-term and
short-term dependencies simultaneously, but when

Figure 1: (a) A standard LSTM unit and (b) a CIFG-
LSTM unit. There are three gates in (a), the input
gate, forget gate and output gates, while in (b), there
are only two gates, the CIFG gate and output gate.

dealing with considerably long texts, LSTM also
fails on capturing and understanding signiﬁcant sen-
timent message (Le et al., 2015). Speciﬁcally, the
error signal would nevertheless suffer from gradient
vanishing in modeling long texts with hundreds of
words and thus the network is difﬁcult to train.

Since the standard LSTM inevitably loses valu-
able features, we propose a cached long short-term
memory neural networks (CLSTM) to capture in-
formation in a longer steps by introducing a cache
mechanism. Moreover, in order to better control and
balance the historical message and the incoming in-
formation, we adopt one particular variant of LSTM
proposed by Greff et al. (2015), the Coupled Input
and Forget Gate LSTM (CIFG-LSTM).

Coupled Input and Forget Gate LSTM Previous
studies show that the merged version gives perfor-
mance comparable to a standard LSTM on language
modeling and classiﬁcation tasks because using the
input gate and forget gate simultaneously incurs re-
dundant information (Chung et al., 2014; Greff et
al., 2015).

In the CIFG-LSTM, the input gate and forget gate
are coupled as one uniform gate, that is, let i(t) =
1 − f (t). We use f (t) to denote the coupled gate.
Formally, we will replace Eq. 5 as below:

c(t) = f (t) (cid:12) c(t−1) + (1 − f (t)) (cid:12) ˜c(t)

(7)

Figure 1 gives an illustrative comparison of a stan-

dard LSTM and the CIFG-LSTM.

Cached LSTM Cached long short-term mem-
ory neural networks (CLSTM) aims at capturing
the long-range information by a cache mechanism,
which divides memory into several groups, and dif-

ferent forgetting rates, regarded as ﬁlters, are as-
signed to different groups.

Different groups capture different-scale depen-
dencies by squashing the scales of forgetting rates.
The groups with high forgetting rates are short-term
memories, while the groups with low forgetting rates
are long-term memories.

Specially, we divide the memory cells into K
groups {G1, · · · , GK}. Each group includes a in-
ternal memory ck, output gate ok and forgetting
rate rk. The forgetting rate of different groups are
squashed in distinct ranges.

We modify the update of a LSTM as follows.


σ(Wk

r x(t) +

r(t)
k = ψk



Uj→k
f

h(t−1)
j

)
 ,

K
(cid:88)

j=1

(8)

(9)

(11)

(12)

K
(cid:88)

j=1

K
(cid:88)

j=1

o(t)
k = σ(Wk

o x(t) +

Uj→k
o

h(t−1)
j

),

˜c(t)
k = tanh(Wk

c x(t) +

Uj→k
c

h(t−1)
j

),

(10)

k = (1 − r(t)
c(t)
k = o(t)
h(t)

k ) (cid:12) c(t−1)
k
k (cid:12) tanh(c(t)
k ),

+ (r(t)

k ) (cid:12) ˜c(t)
k ,

where r(t)
represents forgetting rate of the k-th
k
memory group at step t; ψk is a squash function,
which constrains the value of forgetting rate rk
within a range. To better distinguish the different
role of each group, its forgetting rate is squashed into
a distinct area. The squash function ψk(z) could be
formalized as:

rk = ψk(z) =

· z +

(13)

1
K

k − 1
K

,

where z ∈ (0, 1) is computed by logistic sigmoid
function. Therefore, rk can constrain the forgetting
K , k
rate in the range of ( k−1

K ).
Intuitively, if a forgetting rate rk approaches to 0,
the group k tends to be the long-term memory; if a
rk approaches to 1, the group k tends to be the short-
term memory. Therefore, group G1 is the slowest,
while group GK is the fastest one. The faster groups
are supposed to play a role as a cache, transiting in-
formation from faster groups to slower groups.

Figure 2: An overview of the proposed architecture.
Different styles of arrows indicate different forget-
ting rates. Groups with stars are fed to a fully con-
nected layers for softmax classiﬁcation. Here is an
instance of B-CLSTM with text length equal to 4
and the number of memory groups is 3.

Bidirectional CLSTM Graves and Schmidhuber
(2005) proposed a Bidirectional LSTM (B-LSTM)
model, which utilizes additional backward informa-
tion and thus enhances the memory capability.

We also employ the bi-directional mechanism on
CLSTM and words in a text will receive informa-
tion from both sides of the context. Formally, the
outputs of forward LSTM for the k-th group is
−→
h (1)
k ]. The outputs of backward
[
k , . . . ,
k ,
←−
←−
h (T )
h (2)
k ].
LSTM for the k-th group is [
Hence, we encode each word wt in a given text

←−
h (1)
k ,

k , . . . ,

−→
h (T )

−→
h (2)

w1:T as h(t)
k :

h(t)
k =

−→
h (t)

k ⊕

←−
h (t)
k ,

(14)

where the ⊕ indicates concatenation operation.

Task-speciﬁc Output Layer for Document-level
Sentiment Classiﬁcation With the capability of
modeling long text, we can use our proposed model
to analyze sentiment in a document. Figure 2 gives
an overview of the architecture.

Since the ﬁrst group, the slowest group, is sup-
posed to keep the long-term information and can bet-
ter represent a whole document, we only utilize the
ﬁnal state of this group to represent a document. As
for the B-CLSTM, we concatenate the state of the
ﬁrst group in the forward LSTM at T -th time-step
and the ﬁrst group in the backward LSTM at ﬁrst
time-step.

Dataset
IMDB

Type
Document
Yelp 2013 Document
Yelp 2014 Document

Train Size Dev. Size

67426
62522
183019

8381
7773
22745

Test Size Class Words/Doc
10
5
5

9112
8671
25399

394.6
189.3
196.9

Sents/Doc
16.08
10.89
11.41

Table 1: Statistics of the three datasets used in this paper. The rating scale (Class) of Yelp2013 and Yelp2014
range from 1 to 5 and that of IMDB ranges from 1 to 10. Words/Doc is the average length of a sample and
Sents/Doc is the average number of sentences in a document.

Then, a fully connected layer followed by a soft-
max function is used to predict the probability distri-
bution over classes for a given input. Formally, the
probability distribution p is:

p = softmax(Wp × z + bp),

(15)

where Wp and bp are model’s parameters. Here z
−→
h (T )
1 ] in B-
is
1
CLSTM.

in CLSTM, and z is [

−→
h (T )

←−
h (1)

1 ⊕

5 Training

The objective of our model is to minimize the cross-
entropy error of the predicted and true distributions.
Besides, the objective includes an L2 regularization
term over all parameters. Formally, suppose we have
m train sentence and label pairs (w
i=1, the
object is to minimize the objective function J(θ):

, y(i))m

(i)
1:Ti

J(θ) = −

1
m

m
(cid:88)

i=1

log p(i)

y(i) +

λ
2

||θ||2,

(16)

where θ denote all the trainable parameters of our
model.

6 Experiment

In this section, we study the empirical result of our
model on three datasets for document-level senti-
ment classiﬁcation. Results show that the proposed
model outperforms competitor models from several
aspects when modelling long texts.

6.1 Datasets

Most existing datasets for sentiment classiﬁcation
such as Stanford Sentiment Treebank (Socher et al.,
2013) are composed of short paragraphs with sev-
eral sentences, which cannot evaluate the effective-
ness of the model under the circumstance of encod-
ing long texts. We evaluate our model on three pop-
ular real-world datasets, Yelp 2013, Yelp 2014 and

Dataset
Hidden layer units
Number of groups
Weight Decay
Batch size

IMDB Yelp13 Yelp14
120
4
1e−4
64

120
4
5e−4
64

120
3
1e−4
128

Table 3: Optimal hyper-parameter conﬁguration for
three datasets.

IMDB. Table 1 shows the statistical information of
the three datasets. All these datasets can be publicly
accessed1. We pre-process and split the datasets in
the same way as Tang et al. (2015b) did.

• Yelp 2013 and Yelp 2014 are review datasets
derived from Yelp Dataset Challenge2 of year
2013 and 2014 respectively. The sentiment po-
larity of each review is 1 star to 5 stars, which
reveals the consumers’ attitude and opinion to-
wards the restaurants.

• IMDB is a popular movie review dataset con-
sists of 84919 movie reviews ranging from 1
to 10. Average length of each review is 394.6
words, which is much larger than the length of
two Yelp review datasets.

6.2 Evaluation Metrics

We use Accuracy (Acc.) and MSE as evaluation
metrics for sentiment classiﬁcation. Accuracy is a
standard metric to measure the overall classiﬁcation
result and Mean Squared Error (MSE) is used to ﬁg-
ure out the divergences between predicted sentiment
labels and the ground truth ones.

6.3 Baseline Models

We compare our model, CLSTM and B-CLSTM
with the following baseline methods.

1http://ir.hit.edu.cn/˜dytang/paper/

acl2015/dataset.7z

2http://www.yelp.com/dataset_challenge

Model

CBOW
PV (Tang et al., 2015b)
RNTN+Recurrent (Tang et al., 2015b)
UPNN (CNN) (Tang et al., 2015b)
JMARS* (Diao et al., 2014)
UPNN (CNN)* (Tang et al., 2015b)
RNN
LSTM
CIFG-LSTM
CLSTM
BLSTM
CIFG-BLSTM
B-CLSTM

IMDB

Yelp 2013

Yelp 2014
Acc. (%) MSE Acc. (%) MSE Acc. (%) MSE
0.706
0.692
0.646
0.659
0.970
0.615
1.144
0.656
0.558
0.587
0.583
0.554
0.549

2.867
3.291
3.112
2.654
3.143
2.566
6.163
2.597
2.467
2.399
2.231
2.283
2.112

0.620
0.643
0.674
0.653
0.998
0.584
1.203
0.592
0.598
0.539
0.538
0.527
0.496

54.5
55.4
57.4
57.7
-
59.6
42.8
53.9
57.3
59.4
58.4
59.2
59.8

34.8
34.1
40.0
40.5
-
43.5
20.5
37.8
39.1
42.1
43.3
44.5
46.2

56.8
56.4
58.2
58.5
-
60.8
41.0
56.3
55.2
59.2
59.2
60.1
61.9

Table 2: Sentiment classiﬁcation results of our model against competitor models on IMDB, Yelp 2014 and
Yelp 2013. Evaluation metrics are classiﬁcation accuracy (Acc.) and MSE. Models with * use user and
product information as additional features. Best results in each group are in bold.

• CBOW sums the word vectors and applies a
non-linearity followed by a softmax classiﬁca-
tion layer.

• JMARS is one of the state-of-the-art recom-
mendation algorithm, which leverages user and
aspects of a review with collaborative ﬁltering
and topic modeling.

• CNN UPNN (CNN) (Tang et al., 2015b) can be
regarded as a CNN (Kim, 2014). Multiple ﬁl-
ters are sensitive to capture different semantic
features during generating a representation in a
bottom-up fashion.

• RNN is a basic sequential model to model texts

(Elman, 1991).

• LSTM is a recurrent neural network with mem-
ory cells and gating mechanism (Hochreiter
and Schmidhuber, 1997).

• BLSTM is the bidirectional version of LSTM,
and can capture more structural information
and longer distance during looking forward and
back (Graves et al., 2013).

• CIFG-LSTM & CIFG-BLSTM are Coupled
Input Forget Gate LSTM and BLSTM, de-
noted as CIFG-LSTM and CIFG-BLSTM re-
spectively (Greff et al., 2015). They combine
the input and forget gate of LSTM and require
smaller number of parameters in comparison
with the standard LSTM.

6.4 Hyper-parameters and Initialization

For parameter conﬁguration, we choose parameters
on validation set mainly according to classiﬁcation
accuracy for convenience because MSE always has
strong correlation with accuracy. The dimension of
pre-trained word vectors is 50. We use 120 as the
dimension of hidden units, and choose weight de-
cay among { 5e−4, 1e−4, 1e−5 }. We use Adagrad
(Duchi et al., 2011) as optimizer and its initial learn-
ing rate is 0.01. Batch size is chosen among { 32,
64, 128 } for efﬁciency. For CLSTM, the number of
memory groups is chosen upon each dataset, which
will be discussed later. We remain the total number
of the hidden units unchanged. Given 120 neurons
in all for instance, there are four memory groups and
each of them has 30 neurons. This makes model
comparable to (B)LSTM. Table 3 shows the optimal
hyper-parameter conﬁgurations for each dataset.

For model initialization, we initialize all recur-
rent matrices with randomly sampling from uni-
form distribution in [-0.1, 0.1]. Besides, we use
GloVe(Pennington et al., 2014) as pre-trained word
vectors. The word embeddings are ﬁne-tuned during
training. Hyper-parameters achieving best results on
the validation set are chosen for ﬁnal evaluation on
test set.

)

%

(

c
c
A

60

50

40

30

E
S
M

2.0

1.5

LSTM

CIFG-LSTM

BLSTM

CIFG-BLSTM

B-CLSTM

8

10

LSTM

CIFG-LSTM

BLSTM

CIFG-BLSTM

B-CLSTM

0

2

6
4
Epoches

(a) Accuracy on Yelp 2013

0

2

8

10

4
6
Epoches

(b) MSE on Yelp 2013

Figure 3: Convergence speed experiment on Yelp 2013. X-axis is the iteration epoches and Y-axis is the
classifcication accuracy(%) achieved.

6.5 Results

The classiﬁcation accuracy and mean square error
(MSE) of our models compared with other competi-
tive models are shown in Table 2. When comparing
our models to other neural network models, we have
several meaningful ﬁndings.

1. Among all unidirectional sequential models,
RNN fails to capture and store semantic fea-
tures while vanilla LSTM preserves sentimen-
tal messages much longer than RNN. It shows
that internal memory plays a key role in text
modeling. CIFG-LSTM gives performance
comparable to vanilla LSTM.

2. With the help of bidirectional architecture,
models could look backward and forward to
capture features in long-range from global per-
spective. In sentiment analysis, if users show
their opinion at the beginning of their review,
single directional models will possibly forget
these hints.

3. The proposed CLSTM beats the CIFG-LSTM
and vanilla LSTM and even surpasses the bidi-
In Yelp 2013, CLSTM
rectional models.
achieves 59.4% in accuracy, which is only 0.4
percent worse than B-CLSTM, which reveals
that the cache mechanism has successfully and
effectively stored valuable information without
the support from bidirectional structure.

4. Compared with existing best methods, our

model has achieved new state-of-the-art re-
sults by a large margin on all document-
level datasets in terms of classiﬁcation accu-
racy. Moreover, B-CLSTM even has surpassed
JMARS and CNN (UPNN) methods which uti-
lized extra user and product information.

5. In terms of time complexity and numbers of pa-
rameters, our model keeps almost the same as
its counterpart models while models of hierar-
chically composition may require more compu-
tational resources and time.

6.6 Rate of Convergence

We compare the convergence rates of our mod-
els, including CIFG-LSTM, CIFG-BLSTM and B-
CLSTM, and the baseline models (LSTM and
BLSTM). We conﬁgure the hyper-parameter to
make sure every competing model has approxi-
mately the same numbers of parameters, and vari-
ous models have shown different convergence rates
in Figure 3. In terms of convergence rate, B-CLSTM
beats other competing models. The reason why B-
CLSTM converges faster is that the splitting mem-
ory groups can be seen as a better initialization and
constraints during the training process.

6.7 Effectiveness on Grouping Memory

For the proposed model, the number of memory
groups is a highlight. In Figure 4, we plot the best
prediction accuracy (Y-axis) achieved in validation

61

60.5

60

59.5

)

%

(

c
c
A

62

61

60

46

45

44

1

2

3

4

5

6

1

2

3

4

5

6

1

2

3

4

5

6

Group Number

Group Number

Group Number

(a) Acc on Yelp 2013

(b) Acc on Yelp 2014

(c) Acc on IMDB

Figure 4: Classiﬁcation accuracy on different number of memory group on three datasets. X-axis is the
number of memory group(s).

)

%

(

c
c
A

50

45

40

35

30

10 20 30 40 50 60 70 80 90 100
Length Ranking (%)

CBOW

CIFG-BLSTM

CIFG-LSTM

B-CLSTM

CLSTM

Figure 5: Study of model sensitivity on document
length on IMDB. All test samples are sorted by their
length and divided into 10 parts. Left most dot
means classiﬁcation accuracy on the shortest 10%
samples. X-axis is length ranking from 0% to 100%.

capacity than those who have sufﬁcient neurons for
each group.

6.8 Sensitivity on Document Length

We also investigate the performance of our model
on IMDB when it encodes documents of different
lengths. Test samples are divided into 10 groups
with regard to the length. From Figure 5, we can
draw several thoughtful conclusions.

1. Bidirectional models have much better perfor-

mance than the counterpart models.

2. The overall performance of B-CLSTM is bet-
ter than CIFG-BLSTM. This means that our
model is adaptive to both short texts and long
documents. Besides, our model shows power
in dealing with very long texts in comparison
with CIFG-BLSTM.

3. CBOW is slightly better than CIFG-LSTM due
to LSTM forgets a large amount of information
during the unidirectional propagation.

set with different number of memory groups on all
datasets. From the diagram, we can ﬁnd that our
model outperforms the baseline method.
In Yelp
2013, when we split the memory into 4 groups, it
achieves the best result among all tested memory
group numbers. We can observe the dropping trends
when we choose more than 5 groups.

For fair comparisons, we set the total amount of
neurons in our model to be same with vanilla LSTM.
Therefore, the more groups we split, the less the neu-
rons belongs to each group, which leads to a worse

7 Conclusion

In this paper, we address the problem of effectively
analyzing the sentiment of document-level texts in
an RNN architecture. Similar to the memory struc-
ture of human, memory with low forgetting rate cap-
tures the global semantic features while memory
with high forgetting rate captures the local seman-
tic features. Empirical results on three real-world
document-level review datasets show that our model
outperforms state-of-the-art models by a large mar-
gin.

For future work, we are going to design a strategy
to dynamically adjust the forgetting rates for ﬁne-
grained document-level sentiment analysis.

Acknowledgments

We appreciate the constructive work from Xinchi
Chen. Besides, we would like to thank the anony-
mous reviewers for their valuable comments. This
work was partially funded by National Natural Sci-
ence Foundation of China (No.
61532011 and
61672162),
the National High Technology Re-
search and Development Program of China (No.
2015AA015408).

References

[Atkinson and Shiffrin1968] Richard C Atkinson and
Richard M Shiffrin. 1968. Human memory: A pro-
posed system and its control processes. The psychol-
ogy of learning and motivation, 2:89–195.

[Bhatia et al.2015] Parminder Bhatia, Yangfeng Ji, and
Jacob Eisenstein. 2015. Better document-level sen-
In Pro-
timent analysis from rst discourse parsing.
ceedings of the Conference on Empirical Methods in
Natural Language Processing,(EMNLP).

[Chen et al.2015] Xinchi Chen, Xipeng Qiu, Chenxi Zhu,
Shiyu Wu, and Xuanjing Huang. 2015. Sentence
modeling with gated recursive neural network. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.

[Chung et al.2014] Junyoung Chung, Caglar Gulcehre,
KyungHyun Cho, and Yoshua Bengio. 2014. Empir-
ical evaluation of gated recurrent neural networks on
sequence modeling. NIPS Deep Learning Workshop.

[Denil et al.2014] Misha Denil, Alban Demiraj, Nal
Kalchbrenner, Phil Blunsom, and Nando de Freitas.
2014. Modelling, visualising and summarising doc-
uments with a single convolutional neural network.
arXiv preprint arXiv:1406.3830.

[Diao et al.2014] Qiming Diao, Minghui Qiu, Chao-Yuan
Wu, Alexander J Smola, Jing Jiang, and Chong Wang.
2014. Jointly modeling aspects, ratings and sentiments
for movie recommendation (JMARS). KDD, pages
193–202.

[Duchi et al.2011] John C Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive Subgradient Methods for On-
line Learning and Stochastic Optimization. Journal of
Machine Learning Research, 12:2121–2159.

[El Hihi and Bengio1995] Salah El Hihi and Yoshua Ben-
gio. 1995. Hierarchical recurrent neural networks for
long-term dependencies. In NIPS, pages 493–499.

[Elman1991] Jeffrey L Elman. 1991. Distributed repre-
sentations, simple recurrent networks, and grammati-
cal structure. Machine Learning, 7(2-3):195–225.
[Graves and Schmidhuber2005] Alex Graves and J¨urgen
Schmidhuber. 2005. Framewise phoneme classiﬁca-
tion with bidirectional lstm and other neural network
architectures. Neural Networks, 18(5):602–610.
[Graves et al.2013] Alan Graves, Navdeep Jaitly, and
2013. Hybrid speech
Abdel-rahman Mohamed.
recognition with deep bidirectional lstm. In Automatic
Speech Recognition and Understanding (ASRU), 2013
IEEE Workshop on, pages 273–278. IEEE.

[Greff et al.2015] Klaus Greff, Rupesh Kumar Srivastava,
Jan Koutn´ık, Bas R Steunebrink, and J¨urgen Schmid-
huber.
2015. LSTM: A Search Space Odyssey.
arXiv.org, March.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Karpathy et al.2015] Andrej Karpathy, Justin Johnson,
and Fei-Fei Li. 2015. Visualizing and understand-
ing recurrent networks. International Conference on
Learning Representations (ICLR), Workshop Track.
[Kim2014] Yoon Kim. 2014. Convolutional neural net-
works for sentence classiﬁcation. Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746-1751.
[Koutn´ık et al.2014] Jan Koutn´ık, Klaus Greff, Faustino
Gomez, and J¨urgen Schmidhuber. 2014. A Clock-
work RNN. arXiv.org, February.

[Le and Mikolov2014] Quoc V Le and Tomas Mikolov.
2014. Distributed representations of sentences and
documents. In ICML, volume 14, pages 1188–1196.
[Le et al.2015] Quoc V Le, Navdeep Jaitly, and Geof-
frey E Hinton. 2015. A simple way to initialize recur-
rent networks of rectiﬁed linear units. arXiv preprint
arXiv:1504.00941.

[Liu et al.2015] PengFei Liu, Xipeng Qiu, Xinchi Chen,
Shiyu Wu, and Xuanjing Huang.
2015. Multi-
timescale long short-term memory neural network for
modelling sentences and documents. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.

[Liu et al.2016] Pengfei Liu, Xipeng Qiu, and Xuanjing
Huang. 2016. Recurrent neural network for text clas-
In Proceedings
siﬁcation with multi-task learning.
of International Joint Conference on Artiﬁcial Intel-
ligence.

[McDonald et al.2007] Ryan T McDonald, Kerry Han-
nan, Tyler Neylon, Mike Wells, and Jeffrey C Rey-
nar. 2007. Structured Models for Fine-to-Coarse Sen-
timent Analysis. ACL.

[Tang et al.2015b] Duyu Tang, Bing Qin, and Ting Liu.
2015b. Learning Semantic Representations of Users
and Products for Document Level Sentiment Classiﬁ-
cation. ACL, pages 1014–1023.

[Wilson et al.2005] Theresa Wilson, Janyce Wiebe, and
Paul Hoffmann. 2005. Recognizing contextual po-
larity in phrase-level sentiment analysis. In Proceed-
ings of the conference on human language technology
and empirical methods in natural language process-
ing, pages 347–354. Association for Computational
Linguistics.

[Wilson et al.2009] Theresa Wilson, Janyce Wiebe, and
Paul Hoffmann. 2009. Recognizing contextual polar-
ity: An exploration of features for phrase-level senti-
ment analysis. Computational linguistics, 35(3):399–
433.

[Yessenalina et al.2010] Ainur Yessenalina, Yisong Yue,
2010. Multi-level structured
and Claire Cardie.
models for document-level sentiment classiﬁcation.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1046–1056. Association for Computational Linguis-
tics.

[Zhu et al.2015] Xiaodan Zhu, Parinaz Sobhani, and
Hongyu Guo. 2015. Long short-term memory over
In Proceedings of the 32nd In-
recursive structures.
ternational Conference on Machine Learning, pages
1604–1612.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient Estimation
of Word Representations in Vector Space. arXiv.org.
[Monz2016] Ke Tran Arianna Bisazza Christof Monz.
2016. Recurrent memory networks for language mod-
eling. In Proceedings of NAACL-HLT, pages 321–331.
Joao Francisco
Valiati, and Wilson P Gavi˜aO Neto. 2013. Document-
level sentiment classiﬁcation: An empirical compari-
son between svm and ann. Expert Systems with Appli-
cations, 40(2):621–633.

[Moraes et al.2013] Rodrigo Moraes,

[O’Connor et al.2010] Brendan O’Connor, Ramnath Bal-
asubramanyan, Bryan R Routledge, and Noah A
Smith. 2010. From Tweets to Polls: Linking Text Sen-
timent to Public Opinion Time Series. ICWSM 2010.
and Patrick
Paroubek. 2010. Twitter as a corpus for sentiment
In LREc, volume 10,
analysis and opinion mining.
pages 1320–1326.

[Pak and Paroubek2010] Alexander Pak

[Pang and Lee2004] Bo Pang and Lillian Lee. 2004. A
sentimental education: Sentiment analysis using sub-
In
jectivity summarization based on minimum cuts.
Proceedings of the 42Nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Pang and Lee2008] Bo Pang and Lillian Lee.

2008.
Opinion mining and sentiment analysis. Foundations
and trends in information retrieval, 2(1-2):1–135.

[Pennington et al.2014] Jeffrey

Richard
Socher, and Christopher D Manning. 2014. Glove:
Global Vectors for Word Representation. EMNLP,
pages 1532–1543.

Pennington,

[Salehinejad2016] Hojjat Salehinejad. 2016. Learning
over long time lags. arXiv preprint arXiv:1602.04335.
[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over a
sentiment treebank. In Proceedings of the conference
on empirical methods in natural language processing
(EMNLP), volume 1631, page 1642. Citeseer.

[Sukhbaatar et al.2015] Sainbayar Sukhbaatar, Jason We-
ston, Rob Fergus, et al. 2015. End-to-end memory
networks. In Advances in Neural Information Process-
ing Systems, pages 2431–2439.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D Manning. 2015.
Improved Seman-
tic Representations From Tree-Structured Long Short-
Term Memory Networks. ACL, pages 1556–1566.
[Tang et al.2015a] Duyu Tang, Bing Qin, and Ting Liu.
2015a.
Document Modeling with Gated Recur-
rent Neural Network for Sentiment Classiﬁcation.
EMNLP, pages 1422–1432.

