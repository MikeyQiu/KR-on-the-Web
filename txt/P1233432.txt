8
1
0
2
 
v
o
N
 
3
2
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
5
9
0
.
1
1
8
1
:
v
i
X
r
a

Regret bounds for meta Bayesian optimization
with an unknown Gaussian process prior

Zi Wang∗
MIT CSAIL
ziw@csail.mit.edu

Beomjoon Kim∗
MIT CSAIL
beomjoon@mit.edu

Leslie Pack Kaelbling
MIT CSAIL
lpk@csail.mit.edu

Abstract

Bayesian optimization usually assumes that a Bayesian prior is given. However,
the strong theoretical guarantees in Bayesian optimization are often regrettably
compromised in practice because of unknown parameters in the prior. In this paper,
we adopt a variant of empirical Bayes and show that, by estimating the Gaussian
process prior from ofﬂine data sampled from the same prior and constructing
unbiased estimators of the posterior, variants of both GP-UCB and probability
of improvement achieve a near-zero regret bound, which decreases to a constant
proportional to the observational noise as the number of ofﬂine data and the
number of online evaluations increase. Empirically, we have veriﬁed our approach
on challenging simulated robotic problems featuring task and motion planning.

1

Introduction

Bayesian optimization (BO) is a popular approach to optimizing black-box functions that are expen-
sive to evaluate. Because of expensive evaluations, BO aims to approximately locate the function
maximizer without evaluating the function too many times. This requires a good strategy to adaptively
choose where to evaluate based on the current observations.

BO adopts a Bayesian perspective and assumes that there is a prior on the function; typically, we use
a Gaussian process (GP) prior. Then, the information collection strategy can rely on the prior to focus
on good inputs, where the goodness is determined by an acquisition function derived from the GP
prior and current observations. In past literature, it has been shown both theoretically and empirically
that if the function is indeed drawn from the given prior, there are many acquisition functions that
BO can use to locate the function maximizer quickly [51, 5, 53].

However, in reality, the prior we choose to use in BO often does not reﬂect the distribution from
which the function is drawn. Hence, we sometimes have to estimate the hyper-parameters of a chosen
form of the prior on the ﬂy as we collect more data [50]. One popular choice is to estimate the prior
parameters using empirical Bayes with, e.g., the maximum likelihood estimator [44] .

Despite the vast literature that shows many empirical Bayes approaches have well-founded theoretical
guarantees such as consistency [40] and admissibility [26], it is difﬁcult to analyze a version of BO
that uses empirical Bayes because of the circular dependencies between the estimated parameters and
the data acquisition strategies. The requirement to select the prior model and estimate its parameters
leads to a BO version of the chicken-and-egg dilemma: the prior model selection depends on the data
collected and the data collection strategy depends on having a “correct” prior. Theoretically, there is
little evidence that BO with unknown parameters in the prior can work well. Empirically, there is
evidence showing it works well in some situations, but not others [33, 23], which is not surprising in
light of no free lunch results [56, 22].

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

In this paper, we propose a simple yet effective strategy for learning a prior in a meta-learning setting
where training data on functions from the same Gaussian process prior are available. We use a variant
of empirical Bayes that gives unbiased estimates for both the parameters in the prior and the posterior
given observations of the function we wish to optimize. We analyze the regret bounds in two settings:
(1) ﬁnite input space, and (2) compact input space in Rd. We clarify additional assumptions on the
training data and form of Gaussian processes of both settings in Sec. 4.1 and Sec. 4.2. We prove
theorems that show a near-zero regret bound for variants of GP-UCB [2, 51] and probability of
improvement (PI) [29, 53]. The regret bound decreases to a constant proportional to the observational
noise as online evaluations and ofﬂine data size increase.

From a more pragmatic perspective on Bayesian optimization for important areas such as robotics,
we further explore how our approach works for problems in task and motion planning domains [27],
and we explain why the assumptions in our theorems make sense for these problems in Sec. 5. Indeed,
assuming a common kernel, such as squared exponential or Matérn, is very limiting for robotic
problems that involve discontinuity and non-stationarity. However, with our approach of setting the
prior and posterior parameters, BO outperforms all other methods in the task and motion planning
benchmark problems.

The contributions of this paper are (1) a stand-alone BO module that takes in only a multi-task training
data set as input and then actively selects inputs to efﬁciently optimize a new function and (2) analysis
of the regret of this module. The analysis is constructive, and determines appropriate hyperparameter
settings for the GP-UCB acquisition function. Thus, we make a step forward to resolving the problem
that, despite being used for hyperparameter tuning, BO algorithms themselves have hyperparameters.

2 Background and related work

BO optimizes a black-box objective function through sequential queries. We usually assume knowl-
edge of a Gaussian process [44] prior on the function, though other priors such as Bayesian neural
networks and their variants [17, 30] are applicable too. Then, given possibly noisy observations
and the prior distribution, we can do Bayesian posterior inference and construct acquisition func-
tions [29, 38, 2] to search for the function optimizer.

However, in practice, we do not know the prior and it must be estimated. One of the most popular
methods of prior estimation in BO is to optimize mean/kernel hyper-parameters by maximizing
data-likelihood of the current observations [44, 19]. Another popular approach is to put a prior on
the mean/kernel hyper-parameters and obtain a distribution of such hyper-parameters to adapt the
model given observations [20, 50]. These methods require a predetermined form of the mean function
and the kernel function. In the existing literature, mean functions are usually set to be 0 or linear
and the popular kernel functions include Matérn kernels, Gaussian kernels, linear kernels [44] or
additive/product combinations of the above [11, 24].

Meta BO aims to improve the optimization of a given objective function by learning from past
experiences with other similar functions. Meta BO can be viewed as a special case of transfer
learning or multi-task learning. One well-studied instance of meta BO is the machine learning (ML)
hyper-parameter tuning problem on a dataset, where, typically, the validation errors are the functions
to optimize [14]. The key question is how to transfer the knowledge from previous experiments on
other datasets to the selection of ML hyper-parameters for the current dataset.

To determine the similarity between validation error functions on different datasets, meta-features of
datasets are often used [6]. With those meta-features of datasets, one can use contextual Bayesian
optimization approaches [28] that operate with a probabilistic functional model on both the dataset
meta-features and ML hyper-parameters [3]. Feurer et al. [16], on the other hand, used meta-features
of datasets to construct a distance metric, and to sort hyper-parameters that are known to work for
similar datasets according to their distances to the current dataset. The best k hyper-parameters are
then used to initialize a vanilla BO algorithm. If the function meta-features are not given, one can
estimate the meta-features, such as the mean and variance of all observations, using Monte Carlo
methods [52], maximum likelihood estimates [57] or maximum a posteriori estimates [43, 42].

As an alternative to using meta-features of functions, one can construct a kernel between functions.
For functions that are represented by GPs, Malkomes et al. [36] studied a “kernel kernel”, a kernel
for kernels, such that one can use BO with a “kernel kernel” to select which kernel to use to model or

2

optimize an objective function [35] in a Bayesian way. However, [36] requires an initial set of kernels
to select from. Instead, Golovin et al. [18] introduced a setting where the functions come in sequence
and the posterior of the former function becomes the prior of the current function. Removing the
assumption that functions come sequentially, Feurer et al. [15] proposed a method to learn an additive
ensemble of GPs that are known to ﬁt all of those past “training functions”.

Theoretically, it has been shown that meta BO methods that use information from similar functions
may result in an improvement for the cumulative regret bound [28, 47] or the simple regret bound [42]
with the assumptions that the GP priors are given. If the form of the GP kernel is given and the prior
mean function is 0 but the kernel hyper-parameters are unknown, it is possible to obtain a regret
bound given a range of these hyper-parameters [54]. In this paper, we prove a regret bound for meta
BO where the GP prior is unknown; this means, neither the range of GP hyper-parameters nor the
form of the kernel or mean function is given.

A more ambitious approach to solving meta BO is to train an end-to-end system, such as a recurrent
neural network [21], that takes the history of observations as an input and outputs the next point to
evaluate [8]. Though it has been demonstrated that the method in [8] can learn to trade-off exploration
and exploitation for a short horizon, it is unclear how many “training instances”, in the form of
observations of BO performed on similar functions, are necessary to learn the optimization strategies
for any given horizon of optimization. In this paper, we show both theoretically and empirically how
the number of “training instances” in our method affects the performance of BO.

Our methods are most similar to the BOX algorithm [27], which uses evaluations of previous
functions to make point estimates of a mean and covariance matrix on the values over a discrete
domain. Our methods for the discrete setting (described in Sec. 4.1) directly improve on BOX by
choosing the exploration parameters in GP-UCB more effectively. This general strategy is extended
to the continuous-domain setting in Sec. 4.2, in which we extend a method for learning the GP
prior [41] and the use the learned prior in GP-UCB and PI.

Learning how to learn, or “meta learning”, has a long history in machine learning [46]. It was
argued that learning how to learn is “learning the prior” [4] with “point sets” [37], a set of iid sets
of potentially non-iid points. We follow this simple intuition and present a meta BO approach that
learns its GP prior from the data collected on functions that are assumed to have been drawn from the
same prior distribution.

Empirical Bayes [45, 26] is a standard methodology for estimating unknown parameters of a
Bayesian model. Our approach is a variant of empirical Bayes. We can view our computations
as the construction of a sequence of estimators for a Bayesian model. The key difference from
traditional empirical Bayes methods is that we are able to prove a regret bound for a BO method
that uses estimated parameters to construct priors and posteriors. In particular, we use frequentist
concentration bounds to analyze Bayesian procedures, which is one way to certify empirical Bayes in
statistics [49, 13].

3 Problem formulation and notations

Unlike the standard BO setting, we do not assume knowledge of the mean or covariance in the GP
prior, but we do assume the availability of a dataset of iid sets of potentially non-iid observations on
functions sampled from the same GP prior. Then, given a new, unknown function sampled from that
same distribution, we would like to ﬁnd its maximizer.
More formally, we assume there exists a distribution GP (µ, k), and both the mean µ : X → R and the
kernel k : X×X → R are unknown. Nevertheless, we are given a dataset ¯DN = {[(¯xij, ¯yij)]Mi
i=1,
where ¯yij is drawn independently from N (fi(¯xij), σ2) and fi : X → R is drawn independently from
GP (µ, k). The noise level σ is unknown as well. We will specify inputs ¯xij in Sec. 4.1 and Sec. 4.2.

j=1}N

Given a new function f sampled from GP (µ, k), our goal is to maximize it by sequentially querying
t=1, yt ∼ N (f (xt), σ2). We study two evaluation
the function and constructing DT = [(xt, yt)]T
criteria: (1) the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt) which indicates the
value of the best query in hindsight, and (2) the simple regret, RT = maxx∈X f (x) − f (ˆx∗
T ) which
measures how good the inferred maximizer ˆx∗

T is.

3

Notation We use N (u, V ) to denote a multivariate Gaussian distribution with mean u and variance
V and use W(V, n) to denote a Wishart distribution with n degrees of freedom and scale matrix
V . We also use [n] to denote [1, · · · , n], ∀n ∈ Z+. We overload function notation for evaluations
on vectors x = [xi]n
i=1,
and the output matrix as k(x, x(cid:48)) = [k(xi, x(cid:48)
j)]i∈[n],j∈[n(cid:48)], and we overload the kernel function
k(x) = k(x, x).

j=1 by denoting the output column vector as µ(x) = [µ(xi)]n

i=1, x(cid:48) = [xj]n(cid:48)

4 Meta BO and its theoretical guarantees

Instead of hand-crafting the mean µ and
kernel k, we estimate them using the train-
ing dataset ¯DN . Our approach is fairly
straightforward: in the ofﬂine phase, the
training dataset ¯DN is collected and we
obtain estimates of the mean function ˆµ
and kernel ˆk; in the online phase, we treat
GP (ˆµ, ˆk) as the Bayesian “prior” to do
Bayesian optimization. We illustrate the
two phases in Fig. 1.
In Alg. 1, we de-
pict our algorithm, assuming the dataset
¯DN has been collected. We use ES-
TIMATE( ¯DN ) to denote the “prior” esti-
mation and INFER(Dt; ˆµ, ˆk) the “poste-
rior” inference, both of which we will
introduce in Sec. 4.1 and Sec. 4.2. For
acquisition functions, we consider spe-
cial cases of probability of improvement
(PI) [53, 29] and upper conﬁdence bound
(GP-UCB) [51, 2]:

Algorithm 1 Meta Bayesian optimization
1: function META-BO( ¯DN , f )
2:
3:
4: end function

ˆµ(·), ˆk(·, ·) ← ESTIMATE( ¯DN )
return BO(f, ˆµ, ˆk)

5: function BO (f, ˆµ, ˆk)
D0 ← ∅
6:
for t = 1, · · · , T do
7:
8:
9:
10:
11:
12:
end for
13:
return DT
14:
15: end function

ˆµt−1(·), ˆkt−1(·) ← INFER(Dt−1; ˆµ, ˆk)
αt−1(·) ←ACQUISITION (ˆµt−1, ˆkt−1)
xt ← arg maxx∈X αt−1(x)
yt ← OBSERVE(f (xt))
Dt ← Dt−1 ∪ [(xt, yt)]

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

, αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1
2 .

Here, PI assumes additional information2 in the form of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x). For GP-UCB, we set its hyperparameter ζt to be

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

N −t log 6

δ ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

,

where N is the size of the dataset ¯DN and δ ∈ (0, 1). With probability 1 − δ, the regret bound in
Thm. 2 or Thm. 4 holds with these special cases of GP-UCB and PI. Under two different settings of
the search space X, ﬁnite X and compact X ∈ Rd, we show how our algorithm works in detail and
why it works via regret analyses on the best-sample simple regret. Finally in Sec. 4.3 we show how
the simple regret can be bounded. The proofs of the analyses can be found in the appendix.

4.1 X is a ﬁnite set

We ﬁrst study the simplest case, where the function domain X = [¯xj]M
j=1 is a ﬁnite set with cardinality
|X| = M ∈ Z+. For convenience, we treat this set as an ordered vector of items indexed by
j ∈ [M ]. We collect the training dataset ¯DN = {[(¯xj, ¯δij ¯yij)]M
i=1, where ¯yij are independently
drawn from N (fi(¯xj), σ2), fi are drawn independently from GP (µ, k) and ¯δij ∈ {0, 1}. Because
the training data can be collected ofﬂine by querying the functions {fi}N
i=1 in parallel, it is not
unreasonable to assume that such a dataset ¯DN is available. If ¯δij = 0, it means the (i, j)-th entry of
the dataset ¯DN is missing, perhaps as a result of a failed experiment.

j=1}N

2Alternatively, an upper bound ˆf ∗ can be estimated adaptively [53]. Note that here we are maximizing the PI

acquisition function and hence αPI

t−1(x) is a negative version of what was deﬁned in [53].

4

including

Estimating GP param-
If ¯δij < 1, we
eters
have missing entries in
the
observation matrix
¯Y = [¯δij ¯yij]i∈[N ],j∈[M ] ∈
RN ×M . Under additional
speciﬁed
assumptions
in
that
[7],
rank(Y ) = r and the total
number of valid observa-
tions (cid:80)N
¯δij ≥
i=1
j=1
O(rN 6
5 log N ), we can use
matrix completion [7] to
fully recover the matrix ¯Y
with high probability.
In
the following, we proceed
by considering completed
observations only.

(cid:80)M

Figure 1: Our approach estimates the mean function ˆµ and kernel ˆk
from functions sampled from GP (µ, k) in the ofﬂine phase. Those
sampled functions are illustrated by colored lines. In the online phase,
a new function f sampled from the same GP (µ, k) is given and we
can estimate its posterior mean function ˆµt and covariance function ˆkt
which will be used for Bayesian optimization.

(1)

(2)

(3)

(4)

Let the completed observation matrix be Y = [¯yij]i∈[N ],j∈[M ]. We use an unbiased sample mean and
covariance estimator for µ and k; that is, ˆµ(X) = 1
N −1 (Y − 1N ˆµ(X)T)T(Y −
1N ˆµ(X)T), where 1N is an N by 1 vector of ones. It is well known that ˆµ and ˆk are independent and
ˆµ(X) ∼ N (µ(X), 1

N Y T1N and ˆk(X) = 1

N (k(X) + σ2I)), ˆk(X) ∼ W( 1

N −1 (k(X) + σ2I), N − 1) [1].

Constructing estimators of the posterior Given noisy observations Dt = {(xτ , yτ )}t
do Bayesian posterior inference to obtain f ∼ GP (µt, kt). By the GP assumption, we get

τ =1, we can

µt(x) = µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt)), ∀x ∈ X
kt(x, x(cid:48)) = k(x, x(cid:48)) − k(x, xt)(k(xt) + σ2I)−1k(xt, x(cid:48)), ∀x, x(cid:48) ∈ X,

where yt = [yτ ]T
τ =1 [44]. The problem is that neither the posterior mean µt nor
the covariance kt are computable because the Bayesian prior mean µ, the kernel k and the noise
parameter σ are all unknown. How to estimate µt and kt without knowing those prior parameters?

τ =1, xt = [xτ ]T

We introduce the following unbiased estimators for the posterior mean and covariance,

ˆµt(x) = ˆµ(x) + ˆk(x, xt)ˆk(xt, xt)

−1

(cid:16)ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

(yt − ˆµ(xt)), ∀x ∈ X,
(cid:17)
−1ˆk(xt, x(cid:48))

, ∀x, x(cid:48) ∈ X.

ˆkt(x, x(cid:48)) =

N − 1
N − t − 1

Notice that unlike Eq. (1) and Eq. (2), our estimators ˆµt and ˆkt do not depend on any unknown values
or an additional estimate of the noise parameter σ. In Lemma 1, we show that our estimators are
indeed unbiased and we derive their concentration bounds.
Lemma 1. Pick probability δ ∈ (0, 1). For any nonnegative integer t < T , conditioned on
τ =1, the estimators in Eq. (3) and Eq. (4) satisfy E[ˆµt(X)] =
the observations Dt = {(xτ , yτ )}t
µt(X), E[ˆkt(X)] = kt(X) + σ2I. Moreover, if the size of the training dataset satisﬁes N ≥ T + 2,
then for any input x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + σ2) and 1 − 2

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

Regret bounds We show a near-zero upper bound on the best-sample simple regret of meta BO
with GP-UCB and PI that uses speciﬁc parameter settings in Thm. 2. In particular, for both GP-UCB
and PI, the regret bound converges to a residual whose scale depends on the noise level σ in the
observations.
Theorem 2. Assume there exists constant c ≥ maxx∈X k(x) and a training dataset is available
whose size is N ≥ 4 log 6
δ + T + 2. Then, with probability at least 1 − δ, the best-sample simple

(cid:112)

(cid:17)

5

regret in T iterations of meta BO with special cases of either GP-UCB or PI satisﬁes

T < ηUCB
rUCB

T

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + σ2,

where ηU CB

T

(N ) = (m+C1)(

C1, C2, C3 > 0 are constants, and ρT = max

T (N ) = (m+C2)(
1

2 log |I + σ−2k(A)|.

√
1+m√
1−m

A∈X,|A|=T

+1)+C3, m = O(

(cid:113) 1

N −T ),

(N )λT , rPI
√
1+m√
1−m

+1), ηPI

T

and ηPI

This bound reﬂects how training instances N and BO iterations T affect the best-sample simple
regret. The coefﬁcients ηUCB
T both converge to constants (more details in the appendix), with
components converging at rate O(1/(N − T ) 1
2 ). The convergence of the shared term λT depends on
ρT , the maximum information gain between function f and up to T observations yT . If, for example,
each input has dimension Rd and k(x, x(cid:48)) = xTx(cid:48), then ρT = O(d log(T )) [51], in which case λT
). Together, the bounds indicate
converges to the observational noise level σ at rate O(
that the best-sample simple regret of both our settings of GP-UCB and PI decreases to a constant
proportional to noise level σ.

(cid:113) d log(T )
T

4.2 X ⊂ Rd is compact

For compact X ⊂ Rd, we consider the primal form of GPs. We further assume that there exist basis
s=1 : X → RK, mean parameter u ∈ RK and covariance parameter Σ ∈ RK×K
functions Φ = [φs]K
such that µ(x) = Φ(x)Tu and k(x, x(cid:48)) = Φ(x)TΣΦ(x(cid:48)). Notice that Φ(x) ∈ RK is a column vector
and Φ(xt) ∈ RK×t for any xt = [xτ ]t
τ =1. This means, for any input x ∈ X, the observation satisﬁes
y ∼ N (f (x), σ2), where f = Φ(x)TW ∼ GP (µ, k) and the linear operator W ∼ N (u, Σ) [39]. In
the following analyses, we assume the basis functions Φ are given.
We assume that a training dataset ¯DN = {[(¯xj, ¯yij)]M
i=1 is given, where ¯xj ∈ X ⊂ Rd, yij are
independently drawn from N (fi(¯xj), σ2), fi are drawn independently from GP (µ, k) and M ≥ K.
Estimating GP parameters Because the basis functions Φ are given, learning the mean function
µ and the kernel k in the GP is equivalent to learning the mean parameter u and the covariance
parameter Σ that parameterize distribution of the linear operator W . Notice that ∀i ∈ [N ],

j=1}N

¯yi = Φ( ¯x)TWi + ¯(cid:15)i ∼ N (Φ( ¯x)Tu, Φ( ¯x)TΣΦ( ¯x) + σ2I),

where ¯yi = [¯yij]M
Φ( ¯x) ∈ RK×M has linearly independent rows, one unbiased estimator of Wi is

j=1 ∈ RM ×d and ¯(cid:15)i = [¯(cid:15)ij]M

j=1 ∈ RM , ¯x = [¯xj]M

j=1 ∈ RM . If the matrix

ˆWi = (Φ( ¯x)T)+ ¯yi = (Φ( ¯x)Φ( ¯x)T)−1Φ( ¯x) ¯yi ∼ N (u, Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1).

Let W = [ ˆWi]N
i=1 ∈ RN ×K. We use the estimator ˆu = 1
1N ˆu) to the estimate GP parameters. Again, ˆu and ˆΣ are independent and
ˆu ∼ N (cid:0)u, 1

N (Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1)(cid:1) , ˆΣ ∼ W

N WT1N and ˆΣ = 1

(cid:16) 1

N −1

(cid:17)
(cid:0)Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1(cid:1) , N − 1

[1].

N −1 (W − 1N ˆu)T(W −

Constructing estimators of the posterior We assume the total number of evaluations T < K.
Given noisy observations Dt = {(xτ , yτ )}t
τ =1, we have µt(x) = Φ(x)Tut and kt(x, x(cid:48)) =
Φ(x)TΣtΦ(x(cid:48)), where the posterior of W ∼ N (ut, Σt) satisﬁes

ut = u + ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1(yt − Φ(xt)Tu),
Σt = Σ − ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1Φ(xt)TΣ.

Similar to the strategy used in Sec. 4.1, we construct an estimator for the posterior of W to be

ˆut = ˆu + ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1(yt − Φ(xt)Tu),

ˆΣt =

N − 1
N − t − 1

(cid:16) ˆΣ − ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1Φ(xt)T ˆΣ

(cid:17)

.

We can compute the conditional mean and variance of the observation on x ∈ X to be
ˆµt(x) = Φ(x)T ˆut and ˆkt(x) = Φ(x)T ˆΣtΦ(x). For convenience of notation, we deﬁne ¯σ2(x) =
σ2Φ(x)T(Φ( ¯x)Φ( ¯x)T)−1Φ(x).

(5)

(6)

(7)

(8)

6

Lemma 3. Pick probability δ ∈ (0, 1). Assume Φ( ¯x) has full row rank. For any nonnegative integer
τ =1, E[ˆµt(x)] = µt(x), E[ˆkt(x)] =
t < T , T ≤ K, conditioned on the observations Dt = {(xτ , yτ )}t
kt(x) + ¯σ2(x). Moreover, if the size of the training dataset satisﬁes N ≥ T + 2, then for any input
x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + ¯σ2(x)) and 1 − 2

bt < ˆkt(x)/(kt(x) + ¯σ2(x)) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

(cid:112)

(cid:17)

Regret bounds Similar to the ﬁnite X case, we can also show a near-zero regret bound for compact
X ∈ Rd. The following theorem clariﬁes our results. The convergence rates are the same as Thm. 2.
T converges to ¯σ2(·) instead of σ2 in Thm. 2 and ¯σ2(·) is proportional to σ2 .
Note that λ2
Theorem 4. Assume all the assumptions in Thm. 2 and that Φ( ¯x) has full row rank. With probability
at least 1 − δ, the best-sample simple regret in T iterations of meta BO with either GP-UCB or PI
satisﬁes

T < ηUCB
rUCB

T

(N )λT , rPI

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + ¯σ(xτ )2,

where ηU CB
C1, C2, C3 > 0 are constants, τ = arg mint∈[T ] kt−1(xt) and ρT = max

T (N ) = (m+C2)(

(N ) = (m+C1)(

+1), ηPI

+1)+C3, m = O(
1

N −T ),
2 log |I + σ−2k(A)|.

T

A∈X,|A|=T

√
1+m√
1−m

(cid:113) 1

√
1+m√
1−m

4.3 Bounding the simple regret by the best-sample simple regret

Once we have the observations DT = {(xt, yt)}T
t=1, we can infer where the arg max of the function
is. For all the cases in which X is discrete or compact and the acquisition function is GP-UCB or PI,
we choose the inferred arg max to be ˆx∗
T = xτ where τ = arg maxτ ∈[T ] yτ . We show in Lemma 5
that with high probability, the difference between the simple regret RT and the best-sample simple
regret rT is proportional to the observation noise σ.
Lemma 5. With probability at least 1 − δ, RT ≤ rT + 2(2 log 1

2 σ.

δ ) 1

Together with the bounds on the best-sample simple regret from Thm. 2 and Thm. 4, our result shows
that, with high probability, the simple regret decreases to a constant proportional to the noise level σ
as the number of iterations and training functions increases.

5 Experiments

We evaluate our algorithm in four different
black-box function optimization problems, in-
volving discrete or continuous function domains.
One problem is optimizing a synthetic function
in R2, and the rest are optimizing decision vari-
ables in robotic task and motion planning prob-
lems that were used in [27]3.

At a high level, our task and motion planning
benchmarks involve computing kinematically
feasible collision-free motions for picking and
placing objects in a scene cluttered with obsta-
cles. This problem has a similar setup to exper-
imental design: the robot can “experiment” by
assigning values to decision variables including
grasps, base poses, and object placements until
it ﬁnds a feasible plan. Given the assigned val-
ues for these variables, the robot program makes

Figure 2: Two instances of a picking problem. A
problem instance is deﬁned by the arrangement and
number of obstacles, which vary randomly across
different instances. The objective is to select a
grasp that can pick the blue box, marked with a
circle, without violating kinematic and collision
constraints. [27].

3 Our code is available at https://github.com/beomjoonkim/MetaLearnBO.

7

Figure 3: Learning curves (top) and rewards vs number of iterations (bottom) for optimizing synthetic
functions sampled from a GP and two scoring functions from.

a call to a planner4 which then attempts to ﬁnd a sequence of motions that achieve these grasps and
placements. We score the variable assignment based on the results of planning, assigning a very low
score if the problem was infeasible and otherwise scoring based on plan length or obstacle clearance.
An example problem is given in Figure 2.

Planning problem instances are characterized by arrangements of obstacles in the scene and the
shape of the target object to be manipulated, and each problem instance deﬁnes a different score
function. Our objective is to optimize the score function for a new problem instance, given sets of
decision-variable and score pairs from a set of previous planning problem instances as training data.

In two robotics domains, we discretize the original function domain using samples from the past
planning experience, by extracting the values of the decision variables and their scores from successful
plans. This is inspired by the previous successful use of BO in a discretized domain [9] to efﬁciently
solve an adaptive locomotion problem.

We compare our approach, called point estimate meta Bayesian optimization (PEM-BO), to three
baseline methods. The ﬁrst is a plain Bayesian optimization method that uses a kernel function to
represent the covariance matrix, which we call Plain. Plain optimizes its GP hyperparameters by
maximizing the data likelihood. The second is a transfer learning sequential model-based optimiza-
tion [57] method, that, like PEM-BO, uses past function evaluations, but assumes that functions
sampled from the same GP have similar response surface values. We call this method TLSM-BO.
The third is random selection, which we call Random. We present the results on the UCB acquisition
function in the paper and results on the PI acquisition function are available in the appendix.

0 )]K

i=1 as our basis functions. In order to train the weights Wi, β(i), and β(i)

In all domains, we use the ζt value as speciﬁed in Sec. 4. For continuous domains, we use Φ(x) =
[cos(xT β(i) + β(i)
0 , we
represent the function Φ(x)T Wi with a 1-hidden-layer neural network with cosine activation function
and a linear output layer with function-speciﬁc weights Wi. We then train this network on the entire
dataset ¯DN . Then, ﬁxing Φ(x), for each set of pairs ( ¯yi, ¯xi), i = {1 · · · N }, we analytically solve
the linear regression problem yi ≈ Φ(xi)T Wi as described in Sec. 4.2.
Optimizing a continuous synthetic function In this problem, the objective is to optimize a black-
box function sampled from a GP, whose domain is R2, given a set of evaluations of different functions
from the same GP. Speciﬁcally, we consider a GP with a squared exponential kernel function. The
purpose of this problem is to show that PEM-BO, which estimates mean and covariance matrix based
on ¯DN , would perform similarly to BO methods that start with an appropriate prior. We have training
data from N = 100 functions with M = 1000 sample points each.

4We use Rapidly-exploring random tree (RRT) [32] with predeﬁned random seed, but other choices are

possible.

8

Figure 3(a) shows the learning curve, when we have different portions of data. The x-axis represents
the percentage of the dataset used to train the basis functions, u, and W from the training dataset, and
the y-axis represents the best function value found after 10 evaluations on a new function. We can see
that even with just ten percent of the training data points, PEM-BO performs just as well as Plain,
which uses the appropriate kernel for this particular problem. Compared to PEM-BO, which can
efﬁciently use all of the dataset, we had to limit the number of training data points for TLSM-BO to
1000, because even performing inference requires O(N M ) time. This leads to its noticeably worse
performance than Plain and PEM-BO.

Figure 3(d) shows the how maxt∈[T ] yt evolves, where T ∈ [1, 100]. As we can see, PEM-BO using
the UCB acquisition function performs similarly to Plain with the same acquisition function. TLSM-
BO again suffers because we had to limit the number of training data points.

Optimizing a grasp In the robot-planning problem shown in Figure 2, the robot has to choose a
grasp for picking the target object in a cluttered scene. A planning problem instance is deﬁned by the
poses of obstacles and the target objects, which changes the feasibility of a grasp across different
instances.

The reward function is the negative of the length of the picking motion if the motion is feasible, and
−k ∈ R otherwise, where −k is a suitably lower number than the lengths of possible trajectories.
We construct the discrete set of grasps by using grasps that worked in the past planning problem
instances. The original space of grasps is R58, which describes position, direction, roll, and depth of
a robot gripper with respect to the object, as used in [10]. For both Plain and TLSM-BO, we use
squared exponential kernel function on this original grasp space to represent the covariance matrix.
We note that this is a poor choice of kernel, because the grasp space includes angles, making it a
non-vector space. These methods also choose a grasp from the discrete set. We train on dataset with
N = 1800 previous problems, and let M = 162.

Figure 3(b) shows the learning curve with T = 5. The x-axis is the percentage of the dataset used
for training, ranging from one percent to ten percent. Initially, when we just use one percent of the
training data points, PEM-BO performs as poorly as TLSM-BO, which again, had only 1000 training
data points. However, PEM-BO outperforms both TLSM-BO and Plain after that. The main reason
that PEM-BO outperforms these approaches is because their prior, which is deﬁned by the squared
exponential kernel, is not suitable for this problem. PEM-BO, on the other hand, was able to avoid
this problem by estimating a distribution over values at the discrete sample points that commits only
to their joint normality, but not to any metric on the underlying space. These trends are also shown
in Figure 3(e), where we plot maxt∈[T ] yt for T ∈ [1, 100]. PEM-BO outperforms the baselines
signiﬁcantly.

Optimizing a grasp, base pose, and placement We now consider a more difﬁcult task that involves
both picking and placing objects in a cluttered scene. A planning problem instance is deﬁned by
the poses of obstacles and the poses and shapes of the target object to be pick and placed. The
reward function is again the negative of the length of the picking motion if the motion is feasible,
and −k ∈ R otherwise. For both Plain and TLSM-BO, we use three different squared exponential
kernels on the original spaces of grasp, base pose, and object placement pose respectively and then
add them together to deﬁne the kernel for the whole set. For this domain, N = 1500, and M = 1000.

Figure 3(c) shows the learning curve, when T = 5. The x-axis is the percentage of the dataset used
for training, ranging from one percent to ten percent. Initially, when we just use one percent of
the training data points, PEM-BO does not perform well. Similar to the previous domain, it then
signiﬁcantly outperforms both TLSM-BO and Plain after increasing the training data. This is also
reﬂected in Figure 3(f), where we plot maxt∈[T ] yt for T ∈ [1, 100]. PEM-BO outperforms baselines.
Notice that Plain and TLSM-BO perform worse than Random, as a result of making inappropriate
assumptions on the form of the kernel.

6 Conclusion

We proposed a new framework for meta BO that estimates its Gaussian process prior based on
past experience with functions sampled from the same prior. We established regret bounds for our
approach without the reliance on a known prior and showed its good performance on task and motion
planning benchmark problems.

9

Acknowledgments

We would like to thank Stefanie Jegelka, Tamara Broderick, Trevor Campbell, Tomás Lozano-
Pérez for discussions and comments. We would like to thank Sungkyu Jung and Brian Axelrod for
discussions on Wishart distributions. We gratefully acknowledge support from NSF grants 1420316,
1523767 and 1723381, from AFOSR grant FA9550-17-1-0165, from Honda Research and Draper
Laboratory. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of our sponsors.

References

York, 1958.

2002.

tuning. In ICML, 2013.

York, USA, 1996.

[1] Theodore Wilbur Anderson. An Introduction to Multivariate Statistical Analysis. Wiley New

[2] Peter Auer. Using conﬁdence bounds for exploitation-exploration tradeoffs. JMLR, 3:397–422,

[3] Rémi Bardenet, Mátyás Brendel, Balázs Kégl, and Michele Sebag. Collaborative hyperparameter

[4] J Baxter. A Bayesian/information theoretic model of bias learning. In COLT, New York, New

[5] Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance
reduction: A uniﬁed approach to bayesian optimization and level-set estimation. In NIPS, 2016.

[6] Pavel Brazdil, Jo¯ao Gama, and Bob Henery. Characterizing the applicability of classiﬁcation

algorithms using meta-level learning. In ECML, 1994.

[7] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics, 9(6):717, 2009.

[8] Yutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P
Lillicrap, Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by
gradient descent. In ICML, 2017.

[9] A. Cully, J. Clune, D. Tarapore, and J. Mouret. Robots that adapt like animals. Nature, 2015.

[10] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, CMU

Robotics Institute, August 2010.

[11] David K Duvenaud, Hannes Nickisch, and Carl E Rasmussen. Additive Gaussian processes. In

NIPS, 2011.

[12] M. L. Eaton. Multivariate Statistics: A Vector Space Approach. Beachwood, Ohio, USA:

Institute of Mathematical Statistics, 2007.

[13] Bradley Efron. Bayes, oracle Bayes, and empirical Bayes. 2017.

[14] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and

Frank Hutter. Efﬁcient and robust automated machine learning. In NIPS, 2015.

[15] Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for Bayesian

optimization. arXiv preprint arXiv:1802.02219, 2018.

[16] Matthias Feurer, Jost Springenberg, and Frank Hutter. Initializing Bayesian hyperparameter

optimization via meta-learning. In AAAI, 2015.

[17] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model

uncertainty in deep learning. In ICML, 2016.

[18] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Elliot Karro, and

D. Sculley. Google vizier: A service for black-box optimization. In KDD, 2017.

10

[19] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti-

mization. JMLR, 13:1809–1837, 2012.

[20] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive
entropy search for efﬁcient global optimization of black-box functions. In NIPS, 2014.

[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,

9(8):1735–1780, 1997.

[22] Christian Igel and Marc Toussaint. A no-free-lunch theorem for non-uniform distributions of
target functions. Journal of Mathematical Modelling and Algorithms, 3(4):313–322, 2005.

[23] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing.
Neural architecture search with Bayesian optimisation and optimal transport. arXiv preprint
arXiv:1802.07191, 2018.

[24] Kirthevasan Kandasamy, Jeff Schneider, and Barnabas Poczos. High dimensional Bayesian

optimisation and bandits via additive models. In ICML, 2015.

[25] Kenji Kawaguchi, Bo Xie, Vikas Verma, and Le Song. Deep semi-random features for nonlinear

function approximation. In AAAI, 2017.

[26] Robert W Keener. Theoretical Statistics: Topics for a Core Course. Springer, 2011.

[27] Beomjoon Kim, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. Learning to guide task and

motion planning using score-space representation. In ICRA, 2017.

[28] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In NIPS,

2011.

[29] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak

curve in the presence of noise. Journal of Fluids Engineering, 86(1):97–106, 1964.

[30] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable

predictive uncertainty estimation using deep ensembles. In NIPS, 2017.

[31] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. Annals of Statistics, pages 1302–1338, 2000.

[32] Steven M LaValle and James J Kuffner Jr. Rapidly-exploring random trees: Progress and

prospects. In Workshop on the Algorithmic Foundations of Robotics (WAFR), 2000.

[33] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy-
perband: A novel bandit-based approach to hyperparameter optimization. In International
Conference on Learning Representations (ICLR), 2016.

[34] Karim Lounici et al. High-dimensional covariance matrix estimation with missing observations.

Bernoulli, 20(3):1029–1058, 2014.

[35] Gustavo Malkomes and Roman Garnett. Towards automated Bayesian optimization. In ICML

[36] Gustavo Malkomes, Charles Schaff, and Roman Garnett. Bayesian optimization for automated

AutoML Workshop, 2017.

model selection. In NIPS, 2016.

MIT Media Lab, 1997.

Technical Conference, 1974.

1996.

[37] T P Minka and R W Picard. Learning how to learn is learning with point sets. Technical report,

[38] J. Mo˘ckus. On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP

[39] R.M. Neal. Bayesian Learning for Neural Networks. Lecture Notes in Statistics 118. Springer,

[40] Sonia Petrone, Judith Rousseau, and Catia Scricciolo. Bayes and empirical Bayes: do they

merge? Biometrika, 101(2):285–302, 2014.

11

The MIT Press, 2006.

Statist. Probab., 1956.

(revised), 1995.

[41] John C Platt, Christopher JC Burges, Steven Swenson, Christopher Weare, and Alice Zheng.
Learning a Gaussian process prior for automatically generating music playlists. In NIPS, 2002.

[42] Matthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. In

NIPS, 2017.

[43] Matthias Poloczek, Jialei Wang, and Peter I Frazier. Warm starting Bayesian optimization. In

Winter Simulation Conference (WSC). IEEE, 2016.

[44] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning.

[45] Herbert Robbins. An empirical Bayes approach to statistics. In Third Berkeley Symp. Math.

[46] J Schmidhuber. On learning how to learn learning strategies. Technical report, FKI-198-94

[47] Alistair Shilton, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Regret bounds for transfer

learning in Bayesian optimisation. In AISTATS, 2017.

[48] Mlnoru Slotani. Tolerance regions for a multivariate normal population. Annals of the Institute

of Statistical Mathematics, 16(1):135–153, 1964.

[49] Suzanne Sniekers, Aad van der Vaart, et al. Adaptive Bayesian credible sets in regression with

a Gaussian process prior. Electronic Journal of Statistics, 9(2):2475–2527, 2015.

[50] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine

learning algorithms. In NIPS, 2012.

[51] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: No regret and experimental design. In ICML, 2010.

[52] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In NIPS,

2013.

In ICML, 2017.

[53] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization.

[54] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown
Gaussian process hyper-parameters. In NIPS workshop on Bayesian Optimization, 2014.

[55] Eric W. Weisstein. Square root inequality. MathWorld–A Wolfram Web Resource. http:

//mathworld.wolfram.com/SquareRootInequality.html, 1999-2018.

[56] David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE

transactions on evolutionary computation, 1(1):67–82, 1997.

[57] Dani Yogatama and Gideon Mann. Efﬁcient transfer learning method for automatic hyperpa-

rameter tuning. In AISTATS, 2014.

A Discussions and conclusions

In this section, we discuss related topics to our approach. Both theoreticians and practitioners may
ﬁnd this section useful in terms of clarifying theoretical insights and precautions.

12

A.1 Connections and differences to empirical Bayes

In classic empirical Bayes [45, 26], we estimate the unknown parameters of the Bayesian model
and usually use a point estimate to proceed any Bayesian computations. One very popular approach
to estimate those unknown parameters is by maximizing the data likelihood. There also exit other
variants of empirical Bayes; for example, oracle Bayes, which “shows empirical Bayes in its most
frequentist mode” [13].

In this paper, we use a variant of empirical Bayes that constructs estimators for both the prior
distribution and the posterior distribution. For the estimators of the posterior, we do not use a plug-in
estimate like classic empirical Bayes but we construct them through Lemma. 11, which establishes
the unbiasedness and concentration bounds for those estimates.

A.2 Connections and differences to hierarchical Bayes

Hierarchical Bayes is a Bayesian hierarchical model that places priors on priors. For both of our ﬁnite
X case and continuous and compact X ∈ Rd case, we can write down a hierarchical Bayes model
that puts a normal inverse Wishart prior on µ(X), k(X) or u, Σ.

Our approach can be viewed as a special case of the hierarchical Bayes model using point estimates
to approximate the posterior. Neither our estimators nor our regret analyses depend on the prior
parameters of those hierarchical Bayes models. But one may analyze the regret of BO with a better
approximation from a full Bayesian perspective using hierarchical Bayes.

A.3 Future directions

Due to the limited space, we only give the formulation of meta BO in its simple and basic settings. Our
setting restricts the evaluated inputs in the training data to follow certain norms, such as where they
are and how many they are, but one may certainly extend our analyses to less restrictive scenarios.

Missing entries We did not consider any bounds in matrix completion [7] in our regret analyses, and
proceeded with the assumption that there is no missing entry in the training data. But if missing data
is a concern, one should deﬁnitely consider adapting bounds from [7] or use better estimators [34]
that take into account missing entries when bounding the estimates.

A.4 Broader impact

We developed a statistically sound approach for meta BO with an unknown Gaussian process prior.
We veriﬁed our approach on simulated task and motion planning problems. We showed that our
approach is able to guide task and motion planning with good action recommendations, such that
the resulting plans are better and faster to compute. We believe the theoretical guarantees may
support better explanations for more practical BO approaches. In particular, our method can serve
as a building block of artiﬁcial intelligence systems, and our analyses can be combined with the
theoretical guarantees of other parts of the system to analyze an integrated system.

A.5 Caveats

We did not expand the experiment sections to include applications other than task and motion planning
in simulation. But there are many more scenarios that this meta BO approach will be useful. For
example, our ﬁnite X formulation can be used to adaptively recommend advertisements, movies or
songs to Internet users, by learning a mean and kernel for those discrete items.

Optimization objectives Like other bandit algorithms, our approach only treats objective functions
or any metrics to be optimized as given. Practitioners need to be very careful about what exactly they
are optimizing with our approach or other optimization algorithms. For example, maximizing number
of advertisement clicks or corporation proﬁts may not be a good metric in recommendation systems;
maximizing a poorly designed reward function for robotic systems may result in unexpected hazards.

Guarantees with assumptions
In real-world applications, practitioners need to be extra cautious
with our algorithm. We provided detailed assumptions and analyses, that are only based those
assumptions, in Section 3 and Section 4. Outside those assumptions, we do not claim that our
analyses will hold in any way. For example, in robotics applications, it may not be true that the

13

underlying reward/cost functions are actually sampled from a GP, in which case using our method
may harm the physical robot; even if those objective functions are in fact from a GP, because our
regret bounds only hold with high probability, meta BO may still give dangerous actions with certain
probabilities (as in frequency).

In addition, please notice that we did not provide any theoretical guarantees for using basis functions
trained with neural networks. We assume those basis functions are given, which is usually not the
case in practice. To the best of our knowledge, proving bounds for neural networks is very hard [25].

B Proofs for Section 4.1

Recall that we assume X is a ﬁnite set. The posterior given observations Dt is GP (µt, kt) where

µt(x) = µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt)), ∀x ∈ X
kt(x, x(cid:48)) = k(x, x(cid:48)) − k(x, xt)(k(xt) + σ2I)−1k(xt, x(cid:48)), ∀x, x(cid:48) ∈ X.

We use the following estimators to approximate µt, kt:

ˆµt(x) = ˆµ(x) + ˆk(x, xt)ˆk(xt, xt)

−1

(cid:16)ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

(yt − ˆµ(xt)), ∀x ∈ X,
(cid:17)
−1ˆk(xt, x(cid:48))

, ∀x, x(cid:48) ∈ X.

ˆkt(x, x(cid:48)) =

N − 1
N − t − 1

We will prove a bound on the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt). The
evaluated inputs xt = [xτ ]t
τ are selected either by a special case of GP-UCB using the acquisition
function

αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1
2 ,

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

N −t log 6

δ ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

, δ ∈ (0, 1) (12)

or by a special case of PI using the acquisition function

(9)

(10)

(11)

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

.

This special case of PI assumes additional information of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x).
Corollary 6 ([51]). Let δ0 ∈ (0, 1). For any Gaussian variable x ∼ N (µ, σ2), x ∈ R,

Pr[x − µ ≤ ζ0σ] ≥ 1 − δ0, Pr[x − µ ≥ −ζ0σ] ≥ 1 − δ0

where ζ0 = (2 log( 1
2δ0

)) 1
2 .

Proof. Let z = µ−x

σ ∼ N (0, 1). We have

Pr[z > ζ0] =

e−z2/2 dz

e−(z−ζ0)2/2−ζ2

0 /2−zζ0 dz

1
√
2π

e−(z−ζ0)2/2 dz

(cid:90) +∞

ζ0
(cid:90) +∞

=

ζ0

≤ e−ζ2

0 /2

1
√
2π

1
√
2π
(cid:90) +∞

ζ0

=

e−ζ2

0 /2.

1
2

14

Similarly, Pr[z < −ζ0] ≤ 1

2 e−ζ2

0 /2. We reach the conclusion by rearranging the constants.

Lemma 7. Assume X1, · · · , Xn ∈ Rm are sampled i.i.d. from N (u, V ). Suppose we estimate the
n X T1n and the sample covariance to be ˆV = 1
sample mean to be ˆu = 1
n−1 (X − 1n ˆuT)T(X − 1n ˆuT)
where X = [Xi]n

i=1 ∈ Rn×m. Then, ˆu and ˆV are independent, and

ˆu ∼ N (u,

V ), ˆV ∼ W(

V, n − 1).

1
n − 1

1
n

Lemma 7 is a combination of Theorem 3.3.2 and Corollary 7.2.3 of [1]. Interested readers can ﬁnd
the proof of Lemma 7 in [1]. Corollary 8 directly follows Lemma 7.
Corollary 8. ˆµ and ˆk are independent and

ˆµ(X) ∼ N (µ(X),

(k(X) + σ2I), ˆk(X) ∼ W(

(k(X) + σ2I), N − 1).

1
N

1
N − 1

Corollary 9. For any X ∼ W(v, n), v ∈ R and b > 0, we have

≥ 1 + 2

b + 2b] ≤ e−bn, Pr[

≤ 1 − 2

b] ≤ e−bn.

Proof. Let X be a random variable such that X ∼ W(v, n). So X
chi-squared distribution with n degrees of freedom; namely, X
have

v is distributed according to a
v ∼ χ2(n). By Lemma 1 in [31], we

− n ≥ 2

na + 2a] ≤ e−a, Pr[

− n ≤ −2

na] ≤ e−a.

X
v
As a result, if a = bn,

Pr[

≥ 1 + 2

b + 2b] ≤ e−bn, Pr[

≤ 1 − 2

b] ≤ e−bn.

Pr[

X
vn

Pr[

X
vn

√

√

√

√

√

√

X
vn

X
v

X
vn

Lemma 10. Let X ∈ Rd be a sample from N (w, V ) and deﬁne Z = (X − w)TV −1(X − w). Then,
we have Z ∼ χ2(d). With probability at least 1 − δ0, Z < d + 2

(cid:113)

.

d log 1
δ0

+ 2 log 1
δ0

Proof. By [48], Z ∼ χ2(d). The bound on Z follows Lemma 1 in [31].

Lemma 11. Pick δ1 ∈ (0, 1) and δ2 ∈ (0, 1). For any ﬁxed non-negative integer t < T , conditioned
on the observations Dt = {(xτ , yτ )}t

τ =1, our estimators ˆµt and ˆkt satisfy
E[ˆµt(X)] = µt(X), E[ˆkt(X)] = kt(X) + σ2I.

Suppose N ≥ T + 2. Then, for any ﬁxed inputs x, z ∈ X,

(cid:104)

Pr

ˆµt(x) − µt(x) < ιt

(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:112)(kt(z) + σ2)

≥ 1 − δ1,

(13)

(cid:105)

ˆkt(x)

Pr[

kt(x) + σ2 < 1 + 2

(cid:112)

bt + 2bt] ≥ 1 − δ2, Pr[

ˆkt(x)

kt(x) + σ2 > 1 − 2

(cid:112)

bt] ≥ 1 − δ2.

(14)

where ιt =

(cid:114)

(cid:16)

2

N −2+t+2

(cid:113)

t log 2
δ1

+2 log 2
δ1

(cid:17)

δ1N (N −t−2)

and bt = 1

N −t−1 log 1
δ2

.

Proof. By assumption, all rows of the observation Y = [¯yij]i∈[N ],j∈[M ] are sampled i.i.d. from
N (µ(X), k(X) + σ2I). By Corollary 8,

ˆµ(X) ∼ N (µ,

(k(X) + σ2I)), ˆk(X) ∼ W(

(k(X) + σ2I), N − 1).

1
N − 1

1
N

By Proposition 8.7 in [12], we have

ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

−1ˆk(xt, x(cid:48)) ∼ W(

(kt(x, x(cid:48)) + σ21x=x(cid:48)), N − t − 1).

1
N − 1

15

Hence, the estimate ˆkt satisfy

ˆkt(x) ∼ W(

1
N − t − 1

(kt(x) + σ2), N − t − 1)

(15)

Clearly, E[ˆkt(x)] = kt(x) + σ2. Now it is easy to show Eq. (14). By Corollary 9, for any ﬁxed
t ∈ [T ] ∪ 0 and x, ∀ 1

4 ≥ bt > 0,

Pr[

kt(x) + σ2 ≥ 1 + 2

bt + 2bt] ≤ e−bt(N −t−1),

ˆkt(x)

ˆkt(x)

(cid:112)

(cid:112)

Pr[

kt(x) + σ2 ≤ 1 − 2

bt] ≤ e−bt(N −t−1).

(16)

where bt = 1

N −t−1 log 1
δ2

> 0 and δ2 ∈ (0, 1). Thus, we have shown Eq. (14).

1

We next prove the second half of the results for ˆµt in Eq. (13). We use the shorthand S =
N −1 (k(X) + σ2I). By deﬁnition of the Wishart distributions in [12] (Deﬁnition 8.1), there ex-
ist random vectors X1, · · · , XN −1 ∈ RM sampled iid from N (0, S), ∀i = 1, · · · , N − 1, and
ˆk(X) = (cid:80)n−1
i . We denote X ∈ R(N −1)×M as a matrix whose i-th row is Xi. Clearly,
i=1 XiX T
ˆk(X) = X TX and ˆk(Xa, Xb) = X T
·,aX·,b, ∀a, b ⊆ [M ]. Let the indices of xt in X be Θt ⊆ [M ] and
the index of x in X be θ ∈ [M ]. Thus we have xt = XΘt and x = Xθ.
Conditional on ˆµ(xt) and X·,Θt, the term ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) is a weighted sum of
independent Gaussian variables, because X T
·,θ consists of independent Gaussian variables and
(cid:1)−1
ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) = X T
(yt − ˆµ(xt)). Recall that
Xi ∼ N (0, S); hence, we have

·,θP where P = X·,Θt

X·,Θt

(cid:0)X T

·,Θt

X·,θ | X·,Θt ∼ N (X·,ΘtS−1
Θt

SΘt,θ, IN −1 ⊗ Sθ|Θt),
. As a result, the Gaussian variable X T

where Sθ|Θt = Sθ − Sθ,ΘtS−1
Θt
E[X T
·,θP | ˆµ(xt), X·,Θt] = Sθ,ΘtS−1
Θt

ST

θ,Θt

(yt − ˆµ(xt))

·,θP has mean

and variance

V[X T

·,θP | ˆµ(xt), X·,Θt] = (yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))Sθ|Θt.

By independence between ˆk(X) and ˆµ(X) shown in Corollary 8, we can show that ˆk(x, xt) and ˆµ(x)
are independent conditional on ˆµ(xt) and ˆk(xt), by noting that

p(ˆµ(X), ˆk(X)) = p(ˆµ(X))p(ˆk(X))

⇒p(ˆµ(xt ∪ {x}), ˆk(xt ∪ {x})) = p(ˆµ(xt ∪ {x}))p(ˆk(xt ∪ {x}))
⇒p(ˆµ(xt ∪ {x}), ˆk(xt ∪ {x})) = p(ˆµ(xt ∪ {x}) | ˆk(xt))p(ˆk(xt ∪ {x}) | ˆµ(xt))
⇒p(ˆµ(x), ˆk(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt)) = p(ˆµ(x) | ˆµ(xt), ˆk(xt))p(ˆk(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt))
⇒p(ˆµ(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt)) = p(ˆµ(x) | ˆµ(xt), ˆk(xt)))p(ˆk(x, xt) | ˆµ(xt)), ˆk(xt)).
Hence, ˆµ(x) and X T
ˆk(xt). Moreover, X T

·,θP = ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) are independent conditional on ˆµ(xt) and
·,θP is dependent on X·,Θt only through ˆk(xt) = X T
X·,Θt; hence, we have

·,Θt

ˆµt(x) | ˆµ(xt), ˆk(xt) ∼ N (¯µ, ¯S),

By linearity of expectation and the Bienaymé formula,

¯µ = E[ˆµ(x) | ˆµ(xt)] + k(x, xt)(k(xt) + σ2I)−1(yt − ˆµ(xt))

= µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt))
= µt(x),

¯S = V[ˆµ(x) | ˆµ(xt)] +

(yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))(kt(x) + σ2)
n − 1

,

=

kt(x) + σ2
N

+

(yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))(kt(x) + σ2)
N − 1

.

16

(17)

(18)

(19)

=

=

(cid:34)

(cid:34)

In Eq. (18) and Eq. (19), we use the conditional Gaussian distribution for ˆµ(x) as follows
kt(x) + σ2
N

ˆµ(x) | ˆµ(xt) ∼ N (µ(x) + k(x, xt)(k(xt) + σ2I)−1(ˆµ(xt) − µ(xt)),

).

By the law of total expectation,

E[ˆµt(x)] = E

(cid:104)

(cid:105)
E[ˆµt(x) | ˆµ(xt), ˆk(xt)]

= µt(x).

(20)

By the law of total variance,

V[ˆµt(x)] = E

(cid:104)

(cid:105)
V[ˆµt(x) | ˆµ(xt), ˆk(xt)]

+ V

(cid:105)
(cid:104)
E[ˆµt(x) | ˆµ(xt), ˆk(xt)]

= E (cid:2) ¯S(cid:3) + V [¯µ]

(cid:0)N − 2 + (yt − µ(xt))T(k(xt) + σ2I)−1(yt − µ(xt))(cid:1) (kt(x) + σ2)
N (N − t − 2)

(N − 2 + Kxt,yt) (kt(x) + σ2)
N (N − t − 2)

.

where Kxt,yt = (yt − µ(xt))T(k(xt) + σ2I)−1(yt − µ(xt)).
Notice that ˆµt(x) | ˆµ(xt), ˆk(xt) in Eq. (17) is a normal distribution centered at µt(x) regardless of
the conditional distribution. So the distribution of ˆµt(x) must be symmetric with a center at µt(x).
Hence, applying Chebyshev’s inequality, we have

Pr

ˆµt(x) − µt(x) <

(N − 2 + Kxt,yt) (kt(x) + σ2)
1N (N − t − 2)

2δ(cid:48)

≥ 1 − δ(cid:48)
1,

Pr

ˆµt(x) − µt(x) > −

(N − 2 + Kxt,yt) (kt(x) + σ2)
1N (N − t − 2)

2δ(cid:48)

≥ 1 − δ(cid:48)
1.

(cid:35)

(cid:35)

(cid:115)

(cid:115)

Notice that the randomness of Kxt,yt is from yt and yt ∼ N (µ(xt), k(xt) + σ2I). So we can
further bound Kxt,yt ≤ t + 2
1 by Corollary 9. Hence,
1 = δ1
1 = δ1
if we set δ(cid:48)

with probability at most δ(cid:48)(cid:48)

+ 2 log 1
δ(cid:48)(cid:48)
1

t log 1
δ(cid:48)(cid:48)
1

4 and δ(cid:48)(cid:48)
ˆµt(x) − µt(x) < ιt

2 , with probability at least 1 − δ1, we have
(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:112)(kt(z) + σ2),

(cid:113)

for ﬁxed inputs x, x(cid:48).

Combining this result and the results in Eq. (15), Eq. (16), Eq. (20), we proved the lemma.

Lemma 12 (Lemma 1 in the paper). Pick probability δ ∈ (0, 1). For any nonnegative integer t < T ,
conditioned on the observations Dt = {(xτ , yτ )}t
τ =1, the estimators in Eq. (9) and Eq. (10) satisfy
E[ˆµt(X)] = µt(X), E[ˆkt(X)] = kt(X) + σ2I. Moreover, if the size of the training dataset satisfy
N ≥ T + 2, then for any input x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + σ2) and 1 − 2

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

(cid:112)

(cid:17)

Proof. By a union bound on Eq. (16) of Lemma 11, we have

(cid:104)

Pr

1 − 2

(cid:112)

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

≥ 1 − 2e−bt(N −t−1)

(cid:112)

(cid:105)

N −t−1 log 1
δ2

where bt = 1
(cid:104)
ˆµt(x) − µt(x) < ιt

Pr

> 0 and δ2 ∈ (0, 1). By Lemma 11, we also have
(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:105)
(cid:112)(kt(z) + σ2)

≥ 1 − δ1,

. We get the conclusion of this lemma by setting at =

(cid:114)

(cid:16)

2

N −2+t+2

(cid:113)

t log 2
δ1

+2 log 2
δ1

(cid:17)

δ1N (N −t−2)

where ιt =
ιt, δ1 = δ2 = δ

2 , and z = x.

17

Corollary 13 (Corollary of Bernoulli’s inequality). For any 0 ≤ x ≤ c and a > 0, we have
x ≤ c log(1+ ax
c )

.

log(1+a)

Proof. By Bernoulli’s inequality, (1 + a) x
have x ≤ c log(1+ ax
c )
.

log(1+a)

c ≤ 1 + ax

c . Because log(1 + a) > 0, by rearranging, we

Lemma 14. For any 0 ≤ x ≤ c and a > 0, we have

x <

√

√

x + a − a
√

.

2

c+a

Proof. Numerically, for any n ≥ 1, 1√

n − 2

n − 1 [55]. Let n = x

a + 1. Then, we have

√

n < 2
1
(cid:112) x
a + 1
a
a + x
√

√

√

(cid:114) x
a

√

< 2

+ 1 − 2

(cid:114) x
a

√

< 2

x + a − 2

x

√

x <

x + a −

a
a + c

.

√
2

√

a
a + c

<

Lemma 15 (Lemma 5.3 of [51]). Let xT = [xt]T
function values f (xT ) and their observations yT = [yt]T
t=1 satisfy
1
1
(cid:88)k
2
2

log det(I + σ−2k(xt)) =

I(f (xT ); yT ) =

t=1

log(1 + σ−2kt−1(xt)).

t=1 ⊆ X. The mutual information between the

Theorem 16. Assume there exist constant c ≥ maxx∈X k(x) and a training dataset is available
whose size is N ≥ 4 log 6
(cid:118)
(cid:117)
(cid:117)
(cid:116)

δ + T + 2. Deﬁne
(cid:113)

δ + 2 log 6

N − 3 + t + 2

t log 6

(cid:16)

(cid:17)

6

δ

, bt−1 =

log

for any t ∈ [T ],

ιt−1 =

1
N − t

6
δ

,

δN (N − t − 1)

1

and ρT = max

2 log |I + σ−2k(A)|. Then, with probability at least 1 − δ, the best-sample
simple regret in T iterations of meta BO with GP-UCB that uses Eq. (12) as its hyperparameter
satisﬁes

A∈X,|A|=T

rGP-UCB
T

≤ ηGP-UCB

2cρT
T log(1 + cσ−2)

+ σ2 −

2 σ2

(2 log( 3
√

δ )) 1
c + σ2

,

(cid:115)

1
2

(cid:113)

where ηGP-UCB = ( ιT −1+(2 log( 3
δ ))
bT −1

1−2

√

(cid:113)

1 + 2(cid:112)bT −1 + 2bT −1 + ιT −1 + (2 log( 3

δ )) 1

2 ).

With probability at least 1 − δ, the best-sample simple regret in T iterations of meta BO with PI that
uses ˆf ∗ ≥ maxx∈X f (x) as its target value satisﬁes

(cid:115)

T < ηPI
rPI

2cρT
T log(1 + cσ−2)

+ σ2 −

(2 log( 3
√
2

2δ )) 1
c + σ2

2 σ2

,

where ηPI = (

ˆf ∗−µτ −1(x∗)
√
kτ −1(x∗)+σ2

+ ιτ −1)

arg mint∈[T ] kt−1(xt).

(cid:115)

1+2b

1
2
τ −1+2bτ −1
1
2
τ −1

1−2b

+ ιτ −1 + (2 log( 3

2δ )) 1
2 ,

τ =

Proof. We ﬁrst show the regret bound for GP-UCB with our estimators of prior and posterior. All
of the probabilities mentioned in the proofs need to be interpreted in a frequentist manner. Let
τ = arg mint∈[T ] kt−1(xt). By Corollary 6, with probability at least 1 − δ
3 ,
= f ∗ − maxt∈[T ] f (xt)
≤ f ∗ − f (xτ )
≤ f ∗ − µτ −1(xτ ) + µτ −1(xτ ) − f (xτ )
≤ µτ −1(x∗) + ζ (cid:48)(cid:112)kτ −1(x∗) − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ),

rGP-UCB
T

18

where ζ (cid:48) = (2 log( 3

δ )) 1

µτ −1(x∗) − µτ −1(xτ ) < ˆµτ −1(x∗) − ˆµτ −1(xτ ) + ιτ −1

2 . By Lemma 11, with probability at least 1 − δ
3 ,
(cid:112)kτ −1(x∗) + σ2 + ιτ −1

(cid:112)kτ −1(xτ ) + σ2,

where ιt =

(cid:114)

(cid:16)

6

N −2+t+2

t log 6

δ +2 log 6

δ

(cid:17)

√

δN (N −t−2)

≤ ιT −1.

Lemma 11 and Lemma 14 also show that with probability at least 1 − δ

6 , we have

(cid:112)kτ −1(x∗) ≤ (cid:112)kτ −1(x∗) + σ2 −

σ2
c + σ2

√
2

<

(cid:115) ˆkτ −1(x∗)
1 − 2(cid:112)bτ −1

−

σ2
c + σ2

√
2

1

where bt =
GP-UCB with ζt = ιt−1+ζ(cid:48)
√

N −t−1 log 6

(cid:113)

1−2

bt−1

δ ≤ bT −1 ∈ (0, 1

4 ). Notice that because of the input selection strategy of

, the following inequality holds with probability at least 1 − δ
6 ,

ˆµτ −1(x∗) + (ιt−1 + ζ (cid:48))(cid:112)kτ −1(x∗) + σ2 ≤ ˆµτ −1(x∗) + ζt

ˆkτ −1(x∗)

(cid:113)

(cid:113)

≤ ˆµτ −1(xτ ) + ζt

ˆkτ −1(xτ ).

Hence, with probability at least 1 − δ,

rGP-UCB
T

≤ µτ −1(x∗) + ζ (cid:48)(cid:112)kτ −1(x∗) + σ2 − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ) + σ2 −

ζ (cid:48)σ2
c + σ2
< ˆµτ −1(x∗) − ˆµτ −1(xτ ) + (ιt−1 + ζ (cid:48))((cid:112)kτ −1(x∗) + σ2 + (cid:112)kτ −1(xτ ) + σ2) −

√

√

ζ (cid:48)σ2
c + σ2

(cid:113)

(cid:113)

≤ ζt

ˆkτ −1(xτ ) + (ιt−1 + ζ (cid:48))(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

< (ζt

1 + 2(cid:112)bt−1 + 2bt−1 + ιt−1 + ζ (cid:48))(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

< ηGP-UCB(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

,

where ηGP-UCB = (

(cid:113)

ιT −1+ζ(cid:48)
√

(cid:113)

1−2

bT −1

that τ = arg mint∈[T ] kt−1(xt), we have

1 + 2(cid:112)bT −1 + 2bT −1 + ιT −1 + ζ (cid:48)). By Corollary 13 and the fact

kτ −1(xτ ) ≤

kt−1(xt)

(cid:88)T

t=1

(cid:88)T

1
T

1
T

c log(1 + cσ−2kt−1(xt)
log(1 + cσ−2)

c

)

≤

=

t=1
c
T log(1 + cσ−2)

(cid:88)T

t=1

log(1 + σ−2kt−1(xt)).

Notice that here Corollary 13 applies because 0 ≤ kτ −1(xτ ) ≤ c.

By Lemma 15, I(f (xT ); yT ) = 1
2

(cid:80)T

t=1 log(1 + σ−2kt−1(xt)) ≤ ρT , so
2cρT
T log(1 + cσ−2)

,

kτ −1(xτ ) ≤

which implies

(cid:115)

rGP-UCB
T

< η

2cρT
T log(1 + cσ−2)

+ σ2 −

√

ζ (cid:48)σ2
c + σ2

.

19

Next, we show the proof for a special case of PI with ˆf ∗, an upper bound on f , as its target value.
Again, by Corollary 6, with probability at least 1 − δ
3 ,
T = ˆf ∗ − maxt∈[T ] f (xt)
rPI

≤ ˆf ∗ − f (xτ )
≤ ˆf ∗ − µτ −1(xτ ) + µτ −1(xτ ) − f (xτ )
≤ ˆf ∗ − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ),

2δ )) 1

where ζ (cid:48) = (2 log( 3
of PI, with probability at least 1 − 2δ
3 ,
ˆf ∗ − µτ −1(xτ ) < ˆf ∗ − ˆµτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2

2 and τ = arg mint∈[T ] kt−1(xt). By Lemma 11 and the selection strategy

ˆkτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2

(cid:113)

ˆf ∗ − ˆµτ −1(x∗)
ˆkτ −1(x∗)

(cid:113)

≤

≤

(cid:113)

ˆkτ −1(x∗)

ˆf ∗ − µτ −1(x∗) + ιτ −1

(cid:112)kτ −1(x∗) + σ2

(cid:113)

ˆkτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2



<

(

ˆf ∗ − µτ −1(x∗)
(cid:112)kτ −1(x∗) + σ2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+ ιτ −1)

1
2

1 + 2b

τ −1 + 2bτ −1

1 − 2b

1
2
τ −1



+ ιτ −1



(cid:112)kτ −1(xτ ) + σ2.

Hence, with probability at least 1 − δ, the best-sample simple regret of PI satisfy

(cid:115)

T < ηPI
rPI

2cρT
T log(1 + cσ−2)

+ σ2 −

ζ (cid:48)σ2
c + σ2

√
2

,

where ηPI = (

ˆf ∗−µτ −1(x∗)
√
kτ −1(x∗)+σ2

+ ιτ −1)

(cid:115)

1+2b

1
2
τ −1+2bτ −1
1
2
τ −1

1−2b

+ ιτ −1 + ζ (cid:48).

Theorem 17 (Theorem 2 in th paper). Assume there exist constant c ≥ maxx∈X k(x) and a training
dataset is available whose size is N ≥ 4 log 6
δ + T + 2. Then, with probability at least 1 − δ, the
best-sample simple regret in T iterations of meta BO with special cases of either GP-UCB or PI
satisﬁes

T < ηUCB
rUCB

T

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + σ2,

where ηU CB

T

(N ) = (m+C1)(

C1, C2, C3 > 0 are constants, and ρT = max

T (N ) = (m+C2)(
1

2 log |I + σ−2k(A)|.

√
1+m√
1−m

A∈X,|A|=T

+1)+C3, m = O(

(cid:113) 1

N −T ),

(N )λT , rPI
√
1+m√
1−m

+1), ηPI

Proof. This theorem is a condensed version of Thm. 16 with big O notations.

C Proofs for Section 4.2

Recall that we assume X is a compact set which is a subset of Rd. We only considers a special case
of GPs that assumes f (x) = Φ(x)TW , W ∼ N (u, Σ) and the basis functions Φ(x) ∈ RK are given.
The mean function and kernel are deﬁned as

µ(x) = Φ(x)Tu and k(x) = Φ(x)TΣΦ(x).

Given noisy observations Dt = {(xτ , yτ )}t

τ =1, t ≤ K, we have

µt(x) = Φ(x)Tut and kt(x, x(cid:48)) = Φ(x)TΣtΦ(x(cid:48)),

where the posterior of W ∼ N (ut, Σt) satisﬁes

ut = u + ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1(yt − Φ(xt)Tu),
Σt = Σ − ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1Φ(xt)TΣ.

20

Our estimators for ut and Σt are

ˆut = ˆu + ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1(yt − Φ(xt)Tu),

ˆΣt =

N − 1
N − t − 1

(cid:16) ˆΣ − ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1Φ(xt)T ˆΣ

(cid:17)

.

We can compute the approximated conditional mean and variance of the observation on x ∈ X to be
ˆµt(x) = Φ(x)T ˆut and ˆkt(x) = Φ(x)T ˆΣtΦ(x).

Again, we prove a bound on the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt).
The evaluated inputs xt = [xτ ]t
τ are selected either by a special case of GP-UCB using the acquisition
function

αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1

2 , with

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ ) 1
or by a special case of PI using the acquisition function

N −t log 6

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

, δ ∈ (0, 1),

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

.

This special case of PI assumes additional information of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x).
For convenience of the notations, we deﬁne ¯σ2(x) = σ2Φ(x)T(Φ( ¯x)Φ( ¯x)T)−1Φ(x).

Corollary 18 combines Lemma 7 and basic properties of the Wishart distribution [12].
Corollary 18. Assume the matrix Φ( ¯x) ∈ RK×M has linearly independent rows. Then, ˆu and ˆΣ
are independent and

ˆu ∼ N

u,

(Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1)

, ˆΣ ∼ W

(cid:18)

1
N

(cid:18) 1

N − 1

(cid:0)Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1(cid:1) , N − 1

(cid:19)

.

For ﬁnite set of inputs x ⊂ X, ˆµ(x) and ˆk(x) are also independent; they satisfy

ˆµ(x) ∼ N

µ,

(k(x) + ¯σ2(x))

, ˆk(x) ∼ W

(cid:18)

1
N

(cid:18) 1

N − 1

(cid:0)k(x) + ¯σ2(x)(cid:1) , N − 1

(cid:19)

.

(cid:19)

(cid:19)

The proofs of Lemma 3 and Theorem 4 in the paper directly follow Corollary 18 and proofs of
Lemma 11, Theorem 16 in this appendix.

D Proofs for Section 4.3

We show that the simple regret with ˆx∗
simple regret.
Lemma 19. With probability at least 1 − δ, RT − rT ≤ 2(2 log 1

δ ) 1

2 σ.

T = xτ , τ = arg maxt∈[T ] yt is very close to the best-sample

Proof. Let τ (cid:48) = arg maxt∈[T ] f (xt) and τ = arg maxt∈[T ] yt. Note that yτ ≥ yτ (cid:48). By Corollary 6,
δ ) 1
with probability at least 1 − δ, f (xτ ) + Cσ ≥ yτ ≥ yτ (cid:48) ≥ f (xτ (cid:48)) − Cσ, where C = (2 log 1
2 .
Hence RT − rT = f (xτ (cid:48)) − f (xτ ) ≤ 2Cσ.

E Experiments

For Plain and TLSM-BO with UCB in our experiments, we used the same ζt as PEM-BO.

In the following, we include extra experiments that we performed with PI acquisition function and
matrix completion for the missing entry case in the discrete domains. The PI approach uses the
maximum function value in the training dataset ¯DN as the target value. These results show that our
approach is resilient to missing data. BO with the PI acquisition function performs similarly to UCB.

21

Figure 4: Rewards vs. Number of evals for grasp optimization, grasp, base pose, and placement
optimization, and synthetic function optimization problems (from top-left to bottom). 0.6xPEM-
BO refers to the case where we have 60 percent of the dataset missing.

22

8
1
0
2
 
v
o
N
 
3
2
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
5
9
0
.
1
1
8
1
:
v
i
X
r
a

Regret bounds for meta Bayesian optimization
with an unknown Gaussian process prior

Zi Wang∗
MIT CSAIL
ziw@csail.mit.edu

Beomjoon Kim∗
MIT CSAIL
beomjoon@mit.edu

Leslie Pack Kaelbling
MIT CSAIL
lpk@csail.mit.edu

Abstract

Bayesian optimization usually assumes that a Bayesian prior is given. However,
the strong theoretical guarantees in Bayesian optimization are often regrettably
compromised in practice because of unknown parameters in the prior. In this paper,
we adopt a variant of empirical Bayes and show that, by estimating the Gaussian
process prior from ofﬂine data sampled from the same prior and constructing
unbiased estimators of the posterior, variants of both GP-UCB and probability
of improvement achieve a near-zero regret bound, which decreases to a constant
proportional to the observational noise as the number of ofﬂine data and the
number of online evaluations increase. Empirically, we have veriﬁed our approach
on challenging simulated robotic problems featuring task and motion planning.

1

Introduction

Bayesian optimization (BO) is a popular approach to optimizing black-box functions that are expen-
sive to evaluate. Because of expensive evaluations, BO aims to approximately locate the function
maximizer without evaluating the function too many times. This requires a good strategy to adaptively
choose where to evaluate based on the current observations.

BO adopts a Bayesian perspective and assumes that there is a prior on the function; typically, we use
a Gaussian process (GP) prior. Then, the information collection strategy can rely on the prior to focus
on good inputs, where the goodness is determined by an acquisition function derived from the GP
prior and current observations. In past literature, it has been shown both theoretically and empirically
that if the function is indeed drawn from the given prior, there are many acquisition functions that
BO can use to locate the function maximizer quickly [51, 5, 53].

However, in reality, the prior we choose to use in BO often does not reﬂect the distribution from
which the function is drawn. Hence, we sometimes have to estimate the hyper-parameters of a chosen
form of the prior on the ﬂy as we collect more data [50]. One popular choice is to estimate the prior
parameters using empirical Bayes with, e.g., the maximum likelihood estimator [44] .

Despite the vast literature that shows many empirical Bayes approaches have well-founded theoretical
guarantees such as consistency [40] and admissibility [26], it is difﬁcult to analyze a version of BO
that uses empirical Bayes because of the circular dependencies between the estimated parameters and
the data acquisition strategies. The requirement to select the prior model and estimate its parameters
leads to a BO version of the chicken-and-egg dilemma: the prior model selection depends on the data
collected and the data collection strategy depends on having a “correct” prior. Theoretically, there is
little evidence that BO with unknown parameters in the prior can work well. Empirically, there is
evidence showing it works well in some situations, but not others [33, 23], which is not surprising in
light of no free lunch results [56, 22].

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

In this paper, we propose a simple yet effective strategy for learning a prior in a meta-learning setting
where training data on functions from the same Gaussian process prior are available. We use a variant
of empirical Bayes that gives unbiased estimates for both the parameters in the prior and the posterior
given observations of the function we wish to optimize. We analyze the regret bounds in two settings:
(1) ﬁnite input space, and (2) compact input space in Rd. We clarify additional assumptions on the
training data and form of Gaussian processes of both settings in Sec. 4.1 and Sec. 4.2. We prove
theorems that show a near-zero regret bound for variants of GP-UCB [2, 51] and probability of
improvement (PI) [29, 53]. The regret bound decreases to a constant proportional to the observational
noise as online evaluations and ofﬂine data size increase.

From a more pragmatic perspective on Bayesian optimization for important areas such as robotics,
we further explore how our approach works for problems in task and motion planning domains [27],
and we explain why the assumptions in our theorems make sense for these problems in Sec. 5. Indeed,
assuming a common kernel, such as squared exponential or Matérn, is very limiting for robotic
problems that involve discontinuity and non-stationarity. However, with our approach of setting the
prior and posterior parameters, BO outperforms all other methods in the task and motion planning
benchmark problems.

The contributions of this paper are (1) a stand-alone BO module that takes in only a multi-task training
data set as input and then actively selects inputs to efﬁciently optimize a new function and (2) analysis
of the regret of this module. The analysis is constructive, and determines appropriate hyperparameter
settings for the GP-UCB acquisition function. Thus, we make a step forward to resolving the problem
that, despite being used for hyperparameter tuning, BO algorithms themselves have hyperparameters.

2 Background and related work

BO optimizes a black-box objective function through sequential queries. We usually assume knowl-
edge of a Gaussian process [44] prior on the function, though other priors such as Bayesian neural
networks and their variants [17, 30] are applicable too. Then, given possibly noisy observations
and the prior distribution, we can do Bayesian posterior inference and construct acquisition func-
tions [29, 38, 2] to search for the function optimizer.

However, in practice, we do not know the prior and it must be estimated. One of the most popular
methods of prior estimation in BO is to optimize mean/kernel hyper-parameters by maximizing
data-likelihood of the current observations [44, 19]. Another popular approach is to put a prior on
the mean/kernel hyper-parameters and obtain a distribution of such hyper-parameters to adapt the
model given observations [20, 50]. These methods require a predetermined form of the mean function
and the kernel function. In the existing literature, mean functions are usually set to be 0 or linear
and the popular kernel functions include Matérn kernels, Gaussian kernels, linear kernels [44] or
additive/product combinations of the above [11, 24].

Meta BO aims to improve the optimization of a given objective function by learning from past
experiences with other similar functions. Meta BO can be viewed as a special case of transfer
learning or multi-task learning. One well-studied instance of meta BO is the machine learning (ML)
hyper-parameter tuning problem on a dataset, where, typically, the validation errors are the functions
to optimize [14]. The key question is how to transfer the knowledge from previous experiments on
other datasets to the selection of ML hyper-parameters for the current dataset.

To determine the similarity between validation error functions on different datasets, meta-features of
datasets are often used [6]. With those meta-features of datasets, one can use contextual Bayesian
optimization approaches [28] that operate with a probabilistic functional model on both the dataset
meta-features and ML hyper-parameters [3]. Feurer et al. [16], on the other hand, used meta-features
of datasets to construct a distance metric, and to sort hyper-parameters that are known to work for
similar datasets according to their distances to the current dataset. The best k hyper-parameters are
then used to initialize a vanilla BO algorithm. If the function meta-features are not given, one can
estimate the meta-features, such as the mean and variance of all observations, using Monte Carlo
methods [52], maximum likelihood estimates [57] or maximum a posteriori estimates [43, 42].

As an alternative to using meta-features of functions, one can construct a kernel between functions.
For functions that are represented by GPs, Malkomes et al. [36] studied a “kernel kernel”, a kernel
for kernels, such that one can use BO with a “kernel kernel” to select which kernel to use to model or

2

optimize an objective function [35] in a Bayesian way. However, [36] requires an initial set of kernels
to select from. Instead, Golovin et al. [18] introduced a setting where the functions come in sequence
and the posterior of the former function becomes the prior of the current function. Removing the
assumption that functions come sequentially, Feurer et al. [15] proposed a method to learn an additive
ensemble of GPs that are known to ﬁt all of those past “training functions”.

Theoretically, it has been shown that meta BO methods that use information from similar functions
may result in an improvement for the cumulative regret bound [28, 47] or the simple regret bound [42]
with the assumptions that the GP priors are given. If the form of the GP kernel is given and the prior
mean function is 0 but the kernel hyper-parameters are unknown, it is possible to obtain a regret
bound given a range of these hyper-parameters [54]. In this paper, we prove a regret bound for meta
BO where the GP prior is unknown; this means, neither the range of GP hyper-parameters nor the
form of the kernel or mean function is given.

A more ambitious approach to solving meta BO is to train an end-to-end system, such as a recurrent
neural network [21], that takes the history of observations as an input and outputs the next point to
evaluate [8]. Though it has been demonstrated that the method in [8] can learn to trade-off exploration
and exploitation for a short horizon, it is unclear how many “training instances”, in the form of
observations of BO performed on similar functions, are necessary to learn the optimization strategies
for any given horizon of optimization. In this paper, we show both theoretically and empirically how
the number of “training instances” in our method affects the performance of BO.

Our methods are most similar to the BOX algorithm [27], which uses evaluations of previous
functions to make point estimates of a mean and covariance matrix on the values over a discrete
domain. Our methods for the discrete setting (described in Sec. 4.1) directly improve on BOX by
choosing the exploration parameters in GP-UCB more effectively. This general strategy is extended
to the continuous-domain setting in Sec. 4.2, in which we extend a method for learning the GP
prior [41] and the use the learned prior in GP-UCB and PI.

Learning how to learn, or “meta learning”, has a long history in machine learning [46]. It was
argued that learning how to learn is “learning the prior” [4] with “point sets” [37], a set of iid sets
of potentially non-iid points. We follow this simple intuition and present a meta BO approach that
learns its GP prior from the data collected on functions that are assumed to have been drawn from the
same prior distribution.

Empirical Bayes [45, 26] is a standard methodology for estimating unknown parameters of a
Bayesian model. Our approach is a variant of empirical Bayes. We can view our computations
as the construction of a sequence of estimators for a Bayesian model. The key difference from
traditional empirical Bayes methods is that we are able to prove a regret bound for a BO method
that uses estimated parameters to construct priors and posteriors. In particular, we use frequentist
concentration bounds to analyze Bayesian procedures, which is one way to certify empirical Bayes in
statistics [49, 13].

3 Problem formulation and notations

Unlike the standard BO setting, we do not assume knowledge of the mean or covariance in the GP
prior, but we do assume the availability of a dataset of iid sets of potentially non-iid observations on
functions sampled from the same GP prior. Then, given a new, unknown function sampled from that
same distribution, we would like to ﬁnd its maximizer.
More formally, we assume there exists a distribution GP (µ, k), and both the mean µ : X → R and the
kernel k : X×X → R are unknown. Nevertheless, we are given a dataset ¯DN = {[(¯xij, ¯yij)]Mi
i=1,
where ¯yij is drawn independently from N (fi(¯xij), σ2) and fi : X → R is drawn independently from
GP (µ, k). The noise level σ is unknown as well. We will specify inputs ¯xij in Sec. 4.1 and Sec. 4.2.

j=1}N

Given a new function f sampled from GP (µ, k), our goal is to maximize it by sequentially querying
t=1, yt ∼ N (f (xt), σ2). We study two evaluation
the function and constructing DT = [(xt, yt)]T
criteria: (1) the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt) which indicates the
value of the best query in hindsight, and (2) the simple regret, RT = maxx∈X f (x) − f (ˆx∗
T ) which
measures how good the inferred maximizer ˆx∗

T is.

3

Notation We use N (u, V ) to denote a multivariate Gaussian distribution with mean u and variance
V and use W(V, n) to denote a Wishart distribution with n degrees of freedom and scale matrix
V . We also use [n] to denote [1, · · · , n], ∀n ∈ Z+. We overload function notation for evaluations
on vectors x = [xi]n
i=1,
and the output matrix as k(x, x(cid:48)) = [k(xi, x(cid:48)
j)]i∈[n],j∈[n(cid:48)], and we overload the kernel function
k(x) = k(x, x).

j=1 by denoting the output column vector as µ(x) = [µ(xi)]n

i=1, x(cid:48) = [xj]n(cid:48)

4 Meta BO and its theoretical guarantees

Instead of hand-crafting the mean µ and
kernel k, we estimate them using the train-
ing dataset ¯DN . Our approach is fairly
straightforward: in the ofﬂine phase, the
training dataset ¯DN is collected and we
obtain estimates of the mean function ˆµ
and kernel ˆk; in the online phase, we treat
GP (ˆµ, ˆk) as the Bayesian “prior” to do
Bayesian optimization. We illustrate the
two phases in Fig. 1.
In Alg. 1, we de-
pict our algorithm, assuming the dataset
¯DN has been collected. We use ES-
TIMATE( ¯DN ) to denote the “prior” esti-
mation and INFER(Dt; ˆµ, ˆk) the “poste-
rior” inference, both of which we will
introduce in Sec. 4.1 and Sec. 4.2. For
acquisition functions, we consider spe-
cial cases of probability of improvement
(PI) [53, 29] and upper conﬁdence bound
(GP-UCB) [51, 2]:

Algorithm 1 Meta Bayesian optimization
1: function META-BO( ¯DN , f )
2:
3:
4: end function

ˆµ(·), ˆk(·, ·) ← ESTIMATE( ¯DN )
return BO(f, ˆµ, ˆk)

5: function BO (f, ˆµ, ˆk)
D0 ← ∅
6:
for t = 1, · · · , T do
7:
8:
9:
10:
11:
12:
end for
13:
return DT
14:
15: end function

ˆµt−1(·), ˆkt−1(·) ← INFER(Dt−1; ˆµ, ˆk)
αt−1(·) ←ACQUISITION (ˆµt−1, ˆkt−1)
xt ← arg maxx∈X αt−1(x)
yt ← OBSERVE(f (xt))
Dt ← Dt−1 ∪ [(xt, yt)]

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

, αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1
2 .

Here, PI assumes additional information2 in the form of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x). For GP-UCB, we set its hyperparameter ζt to be

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

N −t log 6

δ ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

,

where N is the size of the dataset ¯DN and δ ∈ (0, 1). With probability 1 − δ, the regret bound in
Thm. 2 or Thm. 4 holds with these special cases of GP-UCB and PI. Under two different settings of
the search space X, ﬁnite X and compact X ∈ Rd, we show how our algorithm works in detail and
why it works via regret analyses on the best-sample simple regret. Finally in Sec. 4.3 we show how
the simple regret can be bounded. The proofs of the analyses can be found in the appendix.

4.1 X is a ﬁnite set

We ﬁrst study the simplest case, where the function domain X = [¯xj]M
j=1 is a ﬁnite set with cardinality
|X| = M ∈ Z+. For convenience, we treat this set as an ordered vector of items indexed by
j ∈ [M ]. We collect the training dataset ¯DN = {[(¯xj, ¯δij ¯yij)]M
i=1, where ¯yij are independently
drawn from N (fi(¯xj), σ2), fi are drawn independently from GP (µ, k) and ¯δij ∈ {0, 1}. Because
the training data can be collected ofﬂine by querying the functions {fi}N
i=1 in parallel, it is not
unreasonable to assume that such a dataset ¯DN is available. If ¯δij = 0, it means the (i, j)-th entry of
the dataset ¯DN is missing, perhaps as a result of a failed experiment.

j=1}N

2Alternatively, an upper bound ˆf ∗ can be estimated adaptively [53]. Note that here we are maximizing the PI

acquisition function and hence αPI

t−1(x) is a negative version of what was deﬁned in [53].

4

including

Estimating GP param-
If ¯δij < 1, we
eters
have missing entries in
the
observation matrix
¯Y = [¯δij ¯yij]i∈[N ],j∈[M ] ∈
RN ×M . Under additional
speciﬁed
assumptions
in
that
[7],
rank(Y ) = r and the total
number of valid observa-
tions (cid:80)N
¯δij ≥
i=1
j=1
O(rN 6
5 log N ), we can use
matrix completion [7] to
fully recover the matrix ¯Y
with high probability.
In
the following, we proceed
by considering completed
observations only.

(cid:80)M

Figure 1: Our approach estimates the mean function ˆµ and kernel ˆk
from functions sampled from GP (µ, k) in the ofﬂine phase. Those
sampled functions are illustrated by colored lines. In the online phase,
a new function f sampled from the same GP (µ, k) is given and we
can estimate its posterior mean function ˆµt and covariance function ˆkt
which will be used for Bayesian optimization.

(1)

(2)

(3)

(4)

Let the completed observation matrix be Y = [¯yij]i∈[N ],j∈[M ]. We use an unbiased sample mean and
covariance estimator for µ and k; that is, ˆµ(X) = 1
N −1 (Y − 1N ˆµ(X)T)T(Y −
1N ˆµ(X)T), where 1N is an N by 1 vector of ones. It is well known that ˆµ and ˆk are independent and
ˆµ(X) ∼ N (µ(X), 1

N Y T1N and ˆk(X) = 1

N (k(X) + σ2I)), ˆk(X) ∼ W( 1

N −1 (k(X) + σ2I), N − 1) [1].

Constructing estimators of the posterior Given noisy observations Dt = {(xτ , yτ )}t
do Bayesian posterior inference to obtain f ∼ GP (µt, kt). By the GP assumption, we get

τ =1, we can

µt(x) = µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt)), ∀x ∈ X
kt(x, x(cid:48)) = k(x, x(cid:48)) − k(x, xt)(k(xt) + σ2I)−1k(xt, x(cid:48)), ∀x, x(cid:48) ∈ X,

where yt = [yτ ]T
τ =1 [44]. The problem is that neither the posterior mean µt nor
the covariance kt are computable because the Bayesian prior mean µ, the kernel k and the noise
parameter σ are all unknown. How to estimate µt and kt without knowing those prior parameters?

τ =1, xt = [xτ ]T

We introduce the following unbiased estimators for the posterior mean and covariance,

ˆµt(x) = ˆµ(x) + ˆk(x, xt)ˆk(xt, xt)

−1

(cid:16)ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

(yt − ˆµ(xt)), ∀x ∈ X,
(cid:17)
−1ˆk(xt, x(cid:48))

, ∀x, x(cid:48) ∈ X.

ˆkt(x, x(cid:48)) =

N − 1
N − t − 1

Notice that unlike Eq. (1) and Eq. (2), our estimators ˆµt and ˆkt do not depend on any unknown values
or an additional estimate of the noise parameter σ. In Lemma 1, we show that our estimators are
indeed unbiased and we derive their concentration bounds.
Lemma 1. Pick probability δ ∈ (0, 1). For any nonnegative integer t < T , conditioned on
τ =1, the estimators in Eq. (3) and Eq. (4) satisfy E[ˆµt(X)] =
the observations Dt = {(xτ , yτ )}t
µt(X), E[ˆkt(X)] = kt(X) + σ2I. Moreover, if the size of the training dataset satisﬁes N ≥ T + 2,
then for any input x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + σ2) and 1 − 2

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

Regret bounds We show a near-zero upper bound on the best-sample simple regret of meta BO
with GP-UCB and PI that uses speciﬁc parameter settings in Thm. 2. In particular, for both GP-UCB
and PI, the regret bound converges to a residual whose scale depends on the noise level σ in the
observations.
Theorem 2. Assume there exists constant c ≥ maxx∈X k(x) and a training dataset is available
whose size is N ≥ 4 log 6
δ + T + 2. Then, with probability at least 1 − δ, the best-sample simple

(cid:112)

(cid:17)

5

regret in T iterations of meta BO with special cases of either GP-UCB or PI satisﬁes

T < ηUCB
rUCB

T

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + σ2,

where ηU CB

T

(N ) = (m+C1)(

C1, C2, C3 > 0 are constants, and ρT = max

T (N ) = (m+C2)(
1

2 log |I + σ−2k(A)|.

√
1+m√
1−m

A∈X,|A|=T

+1)+C3, m = O(

(cid:113) 1

N −T ),

(N )λT , rPI
√
1+m√
1−m

+1), ηPI

T

and ηPI

This bound reﬂects how training instances N and BO iterations T affect the best-sample simple
regret. The coefﬁcients ηUCB
T both converge to constants (more details in the appendix), with
components converging at rate O(1/(N − T ) 1
2 ). The convergence of the shared term λT depends on
ρT , the maximum information gain between function f and up to T observations yT . If, for example,
each input has dimension Rd and k(x, x(cid:48)) = xTx(cid:48), then ρT = O(d log(T )) [51], in which case λT
). Together, the bounds indicate
converges to the observational noise level σ at rate O(
that the best-sample simple regret of both our settings of GP-UCB and PI decreases to a constant
proportional to noise level σ.

(cid:113) d log(T )
T

4.2 X ⊂ Rd is compact

For compact X ⊂ Rd, we consider the primal form of GPs. We further assume that there exist basis
s=1 : X → RK, mean parameter u ∈ RK and covariance parameter Σ ∈ RK×K
functions Φ = [φs]K
such that µ(x) = Φ(x)Tu and k(x, x(cid:48)) = Φ(x)TΣΦ(x(cid:48)). Notice that Φ(x) ∈ RK is a column vector
and Φ(xt) ∈ RK×t for any xt = [xτ ]t
τ =1. This means, for any input x ∈ X, the observation satisﬁes
y ∼ N (f (x), σ2), where f = Φ(x)TW ∼ GP (µ, k) and the linear operator W ∼ N (u, Σ) [39]. In
the following analyses, we assume the basis functions Φ are given.
We assume that a training dataset ¯DN = {[(¯xj, ¯yij)]M
i=1 is given, where ¯xj ∈ X ⊂ Rd, yij are
independently drawn from N (fi(¯xj), σ2), fi are drawn independently from GP (µ, k) and M ≥ K.
Estimating GP parameters Because the basis functions Φ are given, learning the mean function
µ and the kernel k in the GP is equivalent to learning the mean parameter u and the covariance
parameter Σ that parameterize distribution of the linear operator W . Notice that ∀i ∈ [N ],

j=1}N

¯yi = Φ( ¯x)TWi + ¯(cid:15)i ∼ N (Φ( ¯x)Tu, Φ( ¯x)TΣΦ( ¯x) + σ2I),

where ¯yi = [¯yij]M
Φ( ¯x) ∈ RK×M has linearly independent rows, one unbiased estimator of Wi is

j=1 ∈ RM ×d and ¯(cid:15)i = [¯(cid:15)ij]M

j=1 ∈ RM , ¯x = [¯xj]M

j=1 ∈ RM . If the matrix

ˆWi = (Φ( ¯x)T)+ ¯yi = (Φ( ¯x)Φ( ¯x)T)−1Φ( ¯x) ¯yi ∼ N (u, Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1).

Let W = [ ˆWi]N
i=1 ∈ RN ×K. We use the estimator ˆu = 1
1N ˆu) to the estimate GP parameters. Again, ˆu and ˆΣ are independent and
ˆu ∼ N (cid:0)u, 1

N (Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1)(cid:1) , ˆΣ ∼ W

N WT1N and ˆΣ = 1

(cid:16) 1

N −1

(cid:17)
(cid:0)Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1(cid:1) , N − 1

[1].

N −1 (W − 1N ˆu)T(W −

Constructing estimators of the posterior We assume the total number of evaluations T < K.
Given noisy observations Dt = {(xτ , yτ )}t
τ =1, we have µt(x) = Φ(x)Tut and kt(x, x(cid:48)) =
Φ(x)TΣtΦ(x(cid:48)), where the posterior of W ∼ N (ut, Σt) satisﬁes

ut = u + ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1(yt − Φ(xt)Tu),
Σt = Σ − ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1Φ(xt)TΣ.

Similar to the strategy used in Sec. 4.1, we construct an estimator for the posterior of W to be

ˆut = ˆu + ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1(yt − Φ(xt)Tu),

ˆΣt =

N − 1
N − t − 1

(cid:16) ˆΣ − ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1Φ(xt)T ˆΣ

(cid:17)

.

We can compute the conditional mean and variance of the observation on x ∈ X to be
ˆµt(x) = Φ(x)T ˆut and ˆkt(x) = Φ(x)T ˆΣtΦ(x). For convenience of notation, we deﬁne ¯σ2(x) =
σ2Φ(x)T(Φ( ¯x)Φ( ¯x)T)−1Φ(x).

(5)

(6)

(7)

(8)

6

Lemma 3. Pick probability δ ∈ (0, 1). Assume Φ( ¯x) has full row rank. For any nonnegative integer
τ =1, E[ˆµt(x)] = µt(x), E[ˆkt(x)] =
t < T , T ≤ K, conditioned on the observations Dt = {(xτ , yτ )}t
kt(x) + ¯σ2(x). Moreover, if the size of the training dataset satisﬁes N ≥ T + 2, then for any input
x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + ¯σ2(x)) and 1 − 2

bt < ˆkt(x)/(kt(x) + ¯σ2(x)) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

(cid:112)

(cid:17)

Regret bounds Similar to the ﬁnite X case, we can also show a near-zero regret bound for compact
X ∈ Rd. The following theorem clariﬁes our results. The convergence rates are the same as Thm. 2.
T converges to ¯σ2(·) instead of σ2 in Thm. 2 and ¯σ2(·) is proportional to σ2 .
Note that λ2
Theorem 4. Assume all the assumptions in Thm. 2 and that Φ( ¯x) has full row rank. With probability
at least 1 − δ, the best-sample simple regret in T iterations of meta BO with either GP-UCB or PI
satisﬁes

T < ηUCB
rUCB

T

(N )λT , rPI

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + ¯σ(xτ )2,

where ηU CB
C1, C2, C3 > 0 are constants, τ = arg mint∈[T ] kt−1(xt) and ρT = max

T (N ) = (m+C2)(

(N ) = (m+C1)(

+1), ηPI

+1)+C3, m = O(
1

N −T ),
2 log |I + σ−2k(A)|.

T

A∈X,|A|=T

√
1+m√
1−m

(cid:113) 1

√
1+m√
1−m

4.3 Bounding the simple regret by the best-sample simple regret

Once we have the observations DT = {(xt, yt)}T
t=1, we can infer where the arg max of the function
is. For all the cases in which X is discrete or compact and the acquisition function is GP-UCB or PI,
we choose the inferred arg max to be ˆx∗
T = xτ where τ = arg maxτ ∈[T ] yτ . We show in Lemma 5
that with high probability, the difference between the simple regret RT and the best-sample simple
regret rT is proportional to the observation noise σ.
Lemma 5. With probability at least 1 − δ, RT ≤ rT + 2(2 log 1

2 σ.

δ ) 1

Together with the bounds on the best-sample simple regret from Thm. 2 and Thm. 4, our result shows
that, with high probability, the simple regret decreases to a constant proportional to the noise level σ
as the number of iterations and training functions increases.

5 Experiments

We evaluate our algorithm in four different
black-box function optimization problems, in-
volving discrete or continuous function domains.
One problem is optimizing a synthetic function
in R2, and the rest are optimizing decision vari-
ables in robotic task and motion planning prob-
lems that were used in [27]3.

At a high level, our task and motion planning
benchmarks involve computing kinematically
feasible collision-free motions for picking and
placing objects in a scene cluttered with obsta-
cles. This problem has a similar setup to exper-
imental design: the robot can “experiment” by
assigning values to decision variables including
grasps, base poses, and object placements until
it ﬁnds a feasible plan. Given the assigned val-
ues for these variables, the robot program makes

Figure 2: Two instances of a picking problem. A
problem instance is deﬁned by the arrangement and
number of obstacles, which vary randomly across
different instances. The objective is to select a
grasp that can pick the blue box, marked with a
circle, without violating kinematic and collision
constraints. [27].

3 Our code is available at https://github.com/beomjoonkim/MetaLearnBO.

7

Figure 3: Learning curves (top) and rewards vs number of iterations (bottom) for optimizing synthetic
functions sampled from a GP and two scoring functions from.

a call to a planner4 which then attempts to ﬁnd a sequence of motions that achieve these grasps and
placements. We score the variable assignment based on the results of planning, assigning a very low
score if the problem was infeasible and otherwise scoring based on plan length or obstacle clearance.
An example problem is given in Figure 2.

Planning problem instances are characterized by arrangements of obstacles in the scene and the
shape of the target object to be manipulated, and each problem instance deﬁnes a different score
function. Our objective is to optimize the score function for a new problem instance, given sets of
decision-variable and score pairs from a set of previous planning problem instances as training data.

In two robotics domains, we discretize the original function domain using samples from the past
planning experience, by extracting the values of the decision variables and their scores from successful
plans. This is inspired by the previous successful use of BO in a discretized domain [9] to efﬁciently
solve an adaptive locomotion problem.

We compare our approach, called point estimate meta Bayesian optimization (PEM-BO), to three
baseline methods. The ﬁrst is a plain Bayesian optimization method that uses a kernel function to
represent the covariance matrix, which we call Plain. Plain optimizes its GP hyperparameters by
maximizing the data likelihood. The second is a transfer learning sequential model-based optimiza-
tion [57] method, that, like PEM-BO, uses past function evaluations, but assumes that functions
sampled from the same GP have similar response surface values. We call this method TLSM-BO.
The third is random selection, which we call Random. We present the results on the UCB acquisition
function in the paper and results on the PI acquisition function are available in the appendix.

0 )]K

i=1 as our basis functions. In order to train the weights Wi, β(i), and β(i)

In all domains, we use the ζt value as speciﬁed in Sec. 4. For continuous domains, we use Φ(x) =
[cos(xT β(i) + β(i)
0 , we
represent the function Φ(x)T Wi with a 1-hidden-layer neural network with cosine activation function
and a linear output layer with function-speciﬁc weights Wi. We then train this network on the entire
dataset ¯DN . Then, ﬁxing Φ(x), for each set of pairs ( ¯yi, ¯xi), i = {1 · · · N }, we analytically solve
the linear regression problem yi ≈ Φ(xi)T Wi as described in Sec. 4.2.
Optimizing a continuous synthetic function In this problem, the objective is to optimize a black-
box function sampled from a GP, whose domain is R2, given a set of evaluations of different functions
from the same GP. Speciﬁcally, we consider a GP with a squared exponential kernel function. The
purpose of this problem is to show that PEM-BO, which estimates mean and covariance matrix based
on ¯DN , would perform similarly to BO methods that start with an appropriate prior. We have training
data from N = 100 functions with M = 1000 sample points each.

4We use Rapidly-exploring random tree (RRT) [32] with predeﬁned random seed, but other choices are

possible.

8

Figure 3(a) shows the learning curve, when we have different portions of data. The x-axis represents
the percentage of the dataset used to train the basis functions, u, and W from the training dataset, and
the y-axis represents the best function value found after 10 evaluations on a new function. We can see
that even with just ten percent of the training data points, PEM-BO performs just as well as Plain,
which uses the appropriate kernel for this particular problem. Compared to PEM-BO, which can
efﬁciently use all of the dataset, we had to limit the number of training data points for TLSM-BO to
1000, because even performing inference requires O(N M ) time. This leads to its noticeably worse
performance than Plain and PEM-BO.

Figure 3(d) shows the how maxt∈[T ] yt evolves, where T ∈ [1, 100]. As we can see, PEM-BO using
the UCB acquisition function performs similarly to Plain with the same acquisition function. TLSM-
BO again suffers because we had to limit the number of training data points.

Optimizing a grasp In the robot-planning problem shown in Figure 2, the robot has to choose a
grasp for picking the target object in a cluttered scene. A planning problem instance is deﬁned by the
poses of obstacles and the target objects, which changes the feasibility of a grasp across different
instances.

The reward function is the negative of the length of the picking motion if the motion is feasible, and
−k ∈ R otherwise, where −k is a suitably lower number than the lengths of possible trajectories.
We construct the discrete set of grasps by using grasps that worked in the past planning problem
instances. The original space of grasps is R58, which describes position, direction, roll, and depth of
a robot gripper with respect to the object, as used in [10]. For both Plain and TLSM-BO, we use
squared exponential kernel function on this original grasp space to represent the covariance matrix.
We note that this is a poor choice of kernel, because the grasp space includes angles, making it a
non-vector space. These methods also choose a grasp from the discrete set. We train on dataset with
N = 1800 previous problems, and let M = 162.

Figure 3(b) shows the learning curve with T = 5. The x-axis is the percentage of the dataset used
for training, ranging from one percent to ten percent. Initially, when we just use one percent of the
training data points, PEM-BO performs as poorly as TLSM-BO, which again, had only 1000 training
data points. However, PEM-BO outperforms both TLSM-BO and Plain after that. The main reason
that PEM-BO outperforms these approaches is because their prior, which is deﬁned by the squared
exponential kernel, is not suitable for this problem. PEM-BO, on the other hand, was able to avoid
this problem by estimating a distribution over values at the discrete sample points that commits only
to their joint normality, but not to any metric on the underlying space. These trends are also shown
in Figure 3(e), where we plot maxt∈[T ] yt for T ∈ [1, 100]. PEM-BO outperforms the baselines
signiﬁcantly.

Optimizing a grasp, base pose, and placement We now consider a more difﬁcult task that involves
both picking and placing objects in a cluttered scene. A planning problem instance is deﬁned by
the poses of obstacles and the poses and shapes of the target object to be pick and placed. The
reward function is again the negative of the length of the picking motion if the motion is feasible,
and −k ∈ R otherwise. For both Plain and TLSM-BO, we use three different squared exponential
kernels on the original spaces of grasp, base pose, and object placement pose respectively and then
add them together to deﬁne the kernel for the whole set. For this domain, N = 1500, and M = 1000.

Figure 3(c) shows the learning curve, when T = 5. The x-axis is the percentage of the dataset used
for training, ranging from one percent to ten percent. Initially, when we just use one percent of
the training data points, PEM-BO does not perform well. Similar to the previous domain, it then
signiﬁcantly outperforms both TLSM-BO and Plain after increasing the training data. This is also
reﬂected in Figure 3(f), where we plot maxt∈[T ] yt for T ∈ [1, 100]. PEM-BO outperforms baselines.
Notice that Plain and TLSM-BO perform worse than Random, as a result of making inappropriate
assumptions on the form of the kernel.

6 Conclusion

We proposed a new framework for meta BO that estimates its Gaussian process prior based on
past experience with functions sampled from the same prior. We established regret bounds for our
approach without the reliance on a known prior and showed its good performance on task and motion
planning benchmark problems.

9

Acknowledgments

We would like to thank Stefanie Jegelka, Tamara Broderick, Trevor Campbell, Tomás Lozano-
Pérez for discussions and comments. We would like to thank Sungkyu Jung and Brian Axelrod for
discussions on Wishart distributions. We gratefully acknowledge support from NSF grants 1420316,
1523767 and 1723381, from AFOSR grant FA9550-17-1-0165, from Honda Research and Draper
Laboratory. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of our sponsors.

References

York, 1958.

2002.

tuning. In ICML, 2013.

York, USA, 1996.

[1] Theodore Wilbur Anderson. An Introduction to Multivariate Statistical Analysis. Wiley New

[2] Peter Auer. Using conﬁdence bounds for exploitation-exploration tradeoffs. JMLR, 3:397–422,

[3] Rémi Bardenet, Mátyás Brendel, Balázs Kégl, and Michele Sebag. Collaborative hyperparameter

[4] J Baxter. A Bayesian/information theoretic model of bias learning. In COLT, New York, New

[5] Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance
reduction: A uniﬁed approach to bayesian optimization and level-set estimation. In NIPS, 2016.

[6] Pavel Brazdil, Jo¯ao Gama, and Bob Henery. Characterizing the applicability of classiﬁcation

algorithms using meta-level learning. In ECML, 1994.

[7] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics, 9(6):717, 2009.

[8] Yutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P
Lillicrap, Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by
gradient descent. In ICML, 2017.

[9] A. Cully, J. Clune, D. Tarapore, and J. Mouret. Robots that adapt like animals. Nature, 2015.

[10] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, CMU

Robotics Institute, August 2010.

[11] David K Duvenaud, Hannes Nickisch, and Carl E Rasmussen. Additive Gaussian processes. In

NIPS, 2011.

[12] M. L. Eaton. Multivariate Statistics: A Vector Space Approach. Beachwood, Ohio, USA:

Institute of Mathematical Statistics, 2007.

[13] Bradley Efron. Bayes, oracle Bayes, and empirical Bayes. 2017.

[14] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and

Frank Hutter. Efﬁcient and robust automated machine learning. In NIPS, 2015.

[15] Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for Bayesian

optimization. arXiv preprint arXiv:1802.02219, 2018.

[16] Matthias Feurer, Jost Springenberg, and Frank Hutter. Initializing Bayesian hyperparameter

optimization via meta-learning. In AAAI, 2015.

[17] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model

uncertainty in deep learning. In ICML, 2016.

[18] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Elliot Karro, and

D. Sculley. Google vizier: A service for black-box optimization. In KDD, 2017.

10

[19] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti-

mization. JMLR, 13:1809–1837, 2012.

[20] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive
entropy search for efﬁcient global optimization of black-box functions. In NIPS, 2014.

[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,

9(8):1735–1780, 1997.

[22] Christian Igel and Marc Toussaint. A no-free-lunch theorem for non-uniform distributions of
target functions. Journal of Mathematical Modelling and Algorithms, 3(4):313–322, 2005.

[23] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing.
Neural architecture search with Bayesian optimisation and optimal transport. arXiv preprint
arXiv:1802.07191, 2018.

[24] Kirthevasan Kandasamy, Jeff Schneider, and Barnabas Poczos. High dimensional Bayesian

optimisation and bandits via additive models. In ICML, 2015.

[25] Kenji Kawaguchi, Bo Xie, Vikas Verma, and Le Song. Deep semi-random features for nonlinear

function approximation. In AAAI, 2017.

[26] Robert W Keener. Theoretical Statistics: Topics for a Core Course. Springer, 2011.

[27] Beomjoon Kim, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. Learning to guide task and

motion planning using score-space representation. In ICRA, 2017.

[28] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In NIPS,

2011.

[29] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak

curve in the presence of noise. Journal of Fluids Engineering, 86(1):97–106, 1964.

[30] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable

predictive uncertainty estimation using deep ensembles. In NIPS, 2017.

[31] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. Annals of Statistics, pages 1302–1338, 2000.

[32] Steven M LaValle and James J Kuffner Jr. Rapidly-exploring random trees: Progress and

prospects. In Workshop on the Algorithmic Foundations of Robotics (WAFR), 2000.

[33] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy-
perband: A novel bandit-based approach to hyperparameter optimization. In International
Conference on Learning Representations (ICLR), 2016.

[34] Karim Lounici et al. High-dimensional covariance matrix estimation with missing observations.

Bernoulli, 20(3):1029–1058, 2014.

[35] Gustavo Malkomes and Roman Garnett. Towards automated Bayesian optimization. In ICML

[36] Gustavo Malkomes, Charles Schaff, and Roman Garnett. Bayesian optimization for automated

AutoML Workshop, 2017.

model selection. In NIPS, 2016.

MIT Media Lab, 1997.

Technical Conference, 1974.

1996.

[37] T P Minka and R W Picard. Learning how to learn is learning with point sets. Technical report,

[38] J. Mo˘ckus. On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP

[39] R.M. Neal. Bayesian Learning for Neural Networks. Lecture Notes in Statistics 118. Springer,

[40] Sonia Petrone, Judith Rousseau, and Catia Scricciolo. Bayes and empirical Bayes: do they

merge? Biometrika, 101(2):285–302, 2014.

11

The MIT Press, 2006.

Statist. Probab., 1956.

(revised), 1995.

[41] John C Platt, Christopher JC Burges, Steven Swenson, Christopher Weare, and Alice Zheng.
Learning a Gaussian process prior for automatically generating music playlists. In NIPS, 2002.

[42] Matthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. In

NIPS, 2017.

[43] Matthias Poloczek, Jialei Wang, and Peter I Frazier. Warm starting Bayesian optimization. In

Winter Simulation Conference (WSC). IEEE, 2016.

[44] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning.

[45] Herbert Robbins. An empirical Bayes approach to statistics. In Third Berkeley Symp. Math.

[46] J Schmidhuber. On learning how to learn learning strategies. Technical report, FKI-198-94

[47] Alistair Shilton, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Regret bounds for transfer

learning in Bayesian optimisation. In AISTATS, 2017.

[48] Mlnoru Slotani. Tolerance regions for a multivariate normal population. Annals of the Institute

of Statistical Mathematics, 16(1):135–153, 1964.

[49] Suzanne Sniekers, Aad van der Vaart, et al. Adaptive Bayesian credible sets in regression with

a Gaussian process prior. Electronic Journal of Statistics, 9(2):2475–2527, 2015.

[50] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine

learning algorithms. In NIPS, 2012.

[51] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: No regret and experimental design. In ICML, 2010.

[52] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In NIPS,

2013.

In ICML, 2017.

[53] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization.

[54] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown
Gaussian process hyper-parameters. In NIPS workshop on Bayesian Optimization, 2014.

[55] Eric W. Weisstein. Square root inequality. MathWorld–A Wolfram Web Resource. http:

//mathworld.wolfram.com/SquareRootInequality.html, 1999-2018.

[56] David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE

transactions on evolutionary computation, 1(1):67–82, 1997.

[57] Dani Yogatama and Gideon Mann. Efﬁcient transfer learning method for automatic hyperpa-

rameter tuning. In AISTATS, 2014.

A Discussions and conclusions

In this section, we discuss related topics to our approach. Both theoreticians and practitioners may
ﬁnd this section useful in terms of clarifying theoretical insights and precautions.

12

A.1 Connections and differences to empirical Bayes

In classic empirical Bayes [45, 26], we estimate the unknown parameters of the Bayesian model
and usually use a point estimate to proceed any Bayesian computations. One very popular approach
to estimate those unknown parameters is by maximizing the data likelihood. There also exit other
variants of empirical Bayes; for example, oracle Bayes, which “shows empirical Bayes in its most
frequentist mode” [13].

In this paper, we use a variant of empirical Bayes that constructs estimators for both the prior
distribution and the posterior distribution. For the estimators of the posterior, we do not use a plug-in
estimate like classic empirical Bayes but we construct them through Lemma. 11, which establishes
the unbiasedness and concentration bounds for those estimates.

A.2 Connections and differences to hierarchical Bayes

Hierarchical Bayes is a Bayesian hierarchical model that places priors on priors. For both of our ﬁnite
X case and continuous and compact X ∈ Rd case, we can write down a hierarchical Bayes model
that puts a normal inverse Wishart prior on µ(X), k(X) or u, Σ.

Our approach can be viewed as a special case of the hierarchical Bayes model using point estimates
to approximate the posterior. Neither our estimators nor our regret analyses depend on the prior
parameters of those hierarchical Bayes models. But one may analyze the regret of BO with a better
approximation from a full Bayesian perspective using hierarchical Bayes.

A.3 Future directions

Due to the limited space, we only give the formulation of meta BO in its simple and basic settings. Our
setting restricts the evaluated inputs in the training data to follow certain norms, such as where they
are and how many they are, but one may certainly extend our analyses to less restrictive scenarios.

Missing entries We did not consider any bounds in matrix completion [7] in our regret analyses, and
proceeded with the assumption that there is no missing entry in the training data. But if missing data
is a concern, one should deﬁnitely consider adapting bounds from [7] or use better estimators [34]
that take into account missing entries when bounding the estimates.

A.4 Broader impact

We developed a statistically sound approach for meta BO with an unknown Gaussian process prior.
We veriﬁed our approach on simulated task and motion planning problems. We showed that our
approach is able to guide task and motion planning with good action recommendations, such that
the resulting plans are better and faster to compute. We believe the theoretical guarantees may
support better explanations for more practical BO approaches. In particular, our method can serve
as a building block of artiﬁcial intelligence systems, and our analyses can be combined with the
theoretical guarantees of other parts of the system to analyze an integrated system.

A.5 Caveats

We did not expand the experiment sections to include applications other than task and motion planning
in simulation. But there are many more scenarios that this meta BO approach will be useful. For
example, our ﬁnite X formulation can be used to adaptively recommend advertisements, movies or
songs to Internet users, by learning a mean and kernel for those discrete items.

Optimization objectives Like other bandit algorithms, our approach only treats objective functions
or any metrics to be optimized as given. Practitioners need to be very careful about what exactly they
are optimizing with our approach or other optimization algorithms. For example, maximizing number
of advertisement clicks or corporation proﬁts may not be a good metric in recommendation systems;
maximizing a poorly designed reward function for robotic systems may result in unexpected hazards.

Guarantees with assumptions
In real-world applications, practitioners need to be extra cautious
with our algorithm. We provided detailed assumptions and analyses, that are only based those
assumptions, in Section 3 and Section 4. Outside those assumptions, we do not claim that our
analyses will hold in any way. For example, in robotics applications, it may not be true that the

13

underlying reward/cost functions are actually sampled from a GP, in which case using our method
may harm the physical robot; even if those objective functions are in fact from a GP, because our
regret bounds only hold with high probability, meta BO may still give dangerous actions with certain
probabilities (as in frequency).

In addition, please notice that we did not provide any theoretical guarantees for using basis functions
trained with neural networks. We assume those basis functions are given, which is usually not the
case in practice. To the best of our knowledge, proving bounds for neural networks is very hard [25].

B Proofs for Section 4.1

Recall that we assume X is a ﬁnite set. The posterior given observations Dt is GP (µt, kt) where

µt(x) = µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt)), ∀x ∈ X
kt(x, x(cid:48)) = k(x, x(cid:48)) − k(x, xt)(k(xt) + σ2I)−1k(xt, x(cid:48)), ∀x, x(cid:48) ∈ X.

We use the following estimators to approximate µt, kt:

ˆµt(x) = ˆµ(x) + ˆk(x, xt)ˆk(xt, xt)

−1

(cid:16)ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

(yt − ˆµ(xt)), ∀x ∈ X,
(cid:17)
−1ˆk(xt, x(cid:48))

, ∀x, x(cid:48) ∈ X.

ˆkt(x, x(cid:48)) =

N − 1
N − t − 1

We will prove a bound on the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt). The
evaluated inputs xt = [xτ ]t
τ are selected either by a special case of GP-UCB using the acquisition
function

αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1
2 ,

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

N −t log 6

δ ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

, δ ∈ (0, 1) (12)

or by a special case of PI using the acquisition function

(9)

(10)

(11)

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

.

This special case of PI assumes additional information of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x).
Corollary 6 ([51]). Let δ0 ∈ (0, 1). For any Gaussian variable x ∼ N (µ, σ2), x ∈ R,

Pr[x − µ ≤ ζ0σ] ≥ 1 − δ0, Pr[x − µ ≥ −ζ0σ] ≥ 1 − δ0

where ζ0 = (2 log( 1
2δ0

)) 1
2 .

Proof. Let z = µ−x

σ ∼ N (0, 1). We have

Pr[z > ζ0] =

e−z2/2 dz

e−(z−ζ0)2/2−ζ2

0 /2−zζ0 dz

1
√
2π

e−(z−ζ0)2/2 dz

(cid:90) +∞

ζ0
(cid:90) +∞

=

ζ0

≤ e−ζ2

0 /2

1
√
2π

1
√
2π
(cid:90) +∞

ζ0

=

e−ζ2

0 /2.

1
2

14

Similarly, Pr[z < −ζ0] ≤ 1

2 e−ζ2

0 /2. We reach the conclusion by rearranging the constants.

Lemma 7. Assume X1, · · · , Xn ∈ Rm are sampled i.i.d. from N (u, V ). Suppose we estimate the
n X T1n and the sample covariance to be ˆV = 1
sample mean to be ˆu = 1
n−1 (X − 1n ˆuT)T(X − 1n ˆuT)
where X = [Xi]n

i=1 ∈ Rn×m. Then, ˆu and ˆV are independent, and

ˆu ∼ N (u,

V ), ˆV ∼ W(

V, n − 1).

1
n − 1

1
n

Lemma 7 is a combination of Theorem 3.3.2 and Corollary 7.2.3 of [1]. Interested readers can ﬁnd
the proof of Lemma 7 in [1]. Corollary 8 directly follows Lemma 7.
Corollary 8. ˆµ and ˆk are independent and

ˆµ(X) ∼ N (µ(X),

(k(X) + σ2I), ˆk(X) ∼ W(

(k(X) + σ2I), N − 1).

1
N

1
N − 1

Corollary 9. For any X ∼ W(v, n), v ∈ R and b > 0, we have

≥ 1 + 2

b + 2b] ≤ e−bn, Pr[

≤ 1 − 2

b] ≤ e−bn.

Proof. Let X be a random variable such that X ∼ W(v, n). So X
chi-squared distribution with n degrees of freedom; namely, X
have

v is distributed according to a
v ∼ χ2(n). By Lemma 1 in [31], we

− n ≥ 2

na + 2a] ≤ e−a, Pr[

− n ≤ −2

na] ≤ e−a.

X
v
As a result, if a = bn,

Pr[

≥ 1 + 2

b + 2b] ≤ e−bn, Pr[

≤ 1 − 2

b] ≤ e−bn.

Pr[

X
vn

Pr[

X
vn

√

√

√

√

√

√

X
vn

X
v

X
vn

Lemma 10. Let X ∈ Rd be a sample from N (w, V ) and deﬁne Z = (X − w)TV −1(X − w). Then,
we have Z ∼ χ2(d). With probability at least 1 − δ0, Z < d + 2

(cid:113)

.

d log 1
δ0

+ 2 log 1
δ0

Proof. By [48], Z ∼ χ2(d). The bound on Z follows Lemma 1 in [31].

Lemma 11. Pick δ1 ∈ (0, 1) and δ2 ∈ (0, 1). For any ﬁxed non-negative integer t < T , conditioned
on the observations Dt = {(xτ , yτ )}t

τ =1, our estimators ˆµt and ˆkt satisfy
E[ˆµt(X)] = µt(X), E[ˆkt(X)] = kt(X) + σ2I.

Suppose N ≥ T + 2. Then, for any ﬁxed inputs x, z ∈ X,

(cid:104)

Pr

ˆµt(x) − µt(x) < ιt

(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:112)(kt(z) + σ2)

≥ 1 − δ1,

(13)

(cid:105)

ˆkt(x)

Pr[

kt(x) + σ2 < 1 + 2

(cid:112)

bt + 2bt] ≥ 1 − δ2, Pr[

ˆkt(x)

kt(x) + σ2 > 1 − 2

(cid:112)

bt] ≥ 1 − δ2.

(14)

where ιt =

(cid:114)

(cid:16)

2

N −2+t+2

(cid:113)

t log 2
δ1

+2 log 2
δ1

(cid:17)

δ1N (N −t−2)

and bt = 1

N −t−1 log 1
δ2

.

Proof. By assumption, all rows of the observation Y = [¯yij]i∈[N ],j∈[M ] are sampled i.i.d. from
N (µ(X), k(X) + σ2I). By Corollary 8,

ˆµ(X) ∼ N (µ,

(k(X) + σ2I)), ˆk(X) ∼ W(

(k(X) + σ2I), N − 1).

1
N − 1

1
N

By Proposition 8.7 in [12], we have

ˆk(x, x(cid:48)) − ˆk(x, xt)ˆk(xt, xt)

−1ˆk(xt, x(cid:48)) ∼ W(

(kt(x, x(cid:48)) + σ21x=x(cid:48)), N − t − 1).

1
N − 1

15

Hence, the estimate ˆkt satisfy

ˆkt(x) ∼ W(

1
N − t − 1

(kt(x) + σ2), N − t − 1)

(15)

Clearly, E[ˆkt(x)] = kt(x) + σ2. Now it is easy to show Eq. (14). By Corollary 9, for any ﬁxed
t ∈ [T ] ∪ 0 and x, ∀ 1

4 ≥ bt > 0,

Pr[

kt(x) + σ2 ≥ 1 + 2

bt + 2bt] ≤ e−bt(N −t−1),

ˆkt(x)

ˆkt(x)

(cid:112)

(cid:112)

Pr[

kt(x) + σ2 ≤ 1 − 2

bt] ≤ e−bt(N −t−1).

(16)

where bt = 1

N −t−1 log 1
δ2

> 0 and δ2 ∈ (0, 1). Thus, we have shown Eq. (14).

1

We next prove the second half of the results for ˆµt in Eq. (13). We use the shorthand S =
N −1 (k(X) + σ2I). By deﬁnition of the Wishart distributions in [12] (Deﬁnition 8.1), there ex-
ist random vectors X1, · · · , XN −1 ∈ RM sampled iid from N (0, S), ∀i = 1, · · · , N − 1, and
ˆk(X) = (cid:80)n−1
i . We denote X ∈ R(N −1)×M as a matrix whose i-th row is Xi. Clearly,
i=1 XiX T
ˆk(X) = X TX and ˆk(Xa, Xb) = X T
·,aX·,b, ∀a, b ⊆ [M ]. Let the indices of xt in X be Θt ⊆ [M ] and
the index of x in X be θ ∈ [M ]. Thus we have xt = XΘt and x = Xθ.
Conditional on ˆµ(xt) and X·,Θt, the term ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) is a weighted sum of
independent Gaussian variables, because X T
·,θ consists of independent Gaussian variables and
(cid:1)−1
ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) = X T
(yt − ˆµ(xt)). Recall that
Xi ∼ N (0, S); hence, we have

·,θP where P = X·,Θt

X·,Θt

(cid:0)X T

·,Θt

X·,θ | X·,Θt ∼ N (X·,ΘtS−1
Θt

SΘt,θ, IN −1 ⊗ Sθ|Θt),
. As a result, the Gaussian variable X T

where Sθ|Θt = Sθ − Sθ,ΘtS−1
Θt
E[X T
·,θP | ˆµ(xt), X·,Θt] = Sθ,ΘtS−1
Θt

ST

θ,Θt

(yt − ˆµ(xt))

·,θP has mean

and variance

V[X T

·,θP | ˆµ(xt), X·,Θt] = (yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))Sθ|Θt.

By independence between ˆk(X) and ˆµ(X) shown in Corollary 8, we can show that ˆk(x, xt) and ˆµ(x)
are independent conditional on ˆµ(xt) and ˆk(xt), by noting that

p(ˆµ(X), ˆk(X)) = p(ˆµ(X))p(ˆk(X))

⇒p(ˆµ(xt ∪ {x}), ˆk(xt ∪ {x})) = p(ˆµ(xt ∪ {x}))p(ˆk(xt ∪ {x}))
⇒p(ˆµ(xt ∪ {x}), ˆk(xt ∪ {x})) = p(ˆµ(xt ∪ {x}) | ˆk(xt))p(ˆk(xt ∪ {x}) | ˆµ(xt))
⇒p(ˆµ(x), ˆk(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt)) = p(ˆµ(x) | ˆµ(xt), ˆk(xt))p(ˆk(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt))
⇒p(ˆµ(x), ˆk(x, xt) | ˆµ(xt), ˆk(xt)) = p(ˆµ(x) | ˆµ(xt), ˆk(xt)))p(ˆk(x, xt) | ˆµ(xt)), ˆk(xt)).
Hence, ˆµ(x) and X T
ˆk(xt). Moreover, X T

·,θP = ˆk(x, xt)ˆk(xt)−1(yt − ˆµ(xt)) are independent conditional on ˆµ(xt) and
·,θP is dependent on X·,Θt only through ˆk(xt) = X T
X·,Θt; hence, we have

·,Θt

ˆµt(x) | ˆµ(xt), ˆk(xt) ∼ N (¯µ, ¯S),

By linearity of expectation and the Bienaymé formula,

¯µ = E[ˆµ(x) | ˆµ(xt)] + k(x, xt)(k(xt) + σ2I)−1(yt − ˆµ(xt))

= µ(x) + k(x, xt)(k(xt) + σ2I)−1(yt − µ(xt))
= µt(x),

¯S = V[ˆµ(x) | ˆµ(xt)] +

(yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))(kt(x) + σ2)
n − 1

,

=

kt(x) + σ2
N

+

(yt − ˆµ(xt))Tˆk(xt)−1(yt − ˆµ(xt))(kt(x) + σ2)
N − 1

.

16

(17)

(18)

(19)

=

=

(cid:34)

(cid:34)

In Eq. (18) and Eq. (19), we use the conditional Gaussian distribution for ˆµ(x) as follows
kt(x) + σ2
N

ˆµ(x) | ˆµ(xt) ∼ N (µ(x) + k(x, xt)(k(xt) + σ2I)−1(ˆµ(xt) − µ(xt)),

).

By the law of total expectation,

E[ˆµt(x)] = E

(cid:104)

(cid:105)
E[ˆµt(x) | ˆµ(xt), ˆk(xt)]

= µt(x).

(20)

By the law of total variance,

V[ˆµt(x)] = E

(cid:104)

(cid:105)
V[ˆµt(x) | ˆµ(xt), ˆk(xt)]

+ V

(cid:105)
(cid:104)
E[ˆµt(x) | ˆµ(xt), ˆk(xt)]

= E (cid:2) ¯S(cid:3) + V [¯µ]

(cid:0)N − 2 + (yt − µ(xt))T(k(xt) + σ2I)−1(yt − µ(xt))(cid:1) (kt(x) + σ2)
N (N − t − 2)

(N − 2 + Kxt,yt) (kt(x) + σ2)
N (N − t − 2)

.

where Kxt,yt = (yt − µ(xt))T(k(xt) + σ2I)−1(yt − µ(xt)).
Notice that ˆµt(x) | ˆµ(xt), ˆk(xt) in Eq. (17) is a normal distribution centered at µt(x) regardless of
the conditional distribution. So the distribution of ˆµt(x) must be symmetric with a center at µt(x).
Hence, applying Chebyshev’s inequality, we have

Pr

ˆµt(x) − µt(x) <

(N − 2 + Kxt,yt) (kt(x) + σ2)
1N (N − t − 2)

2δ(cid:48)

≥ 1 − δ(cid:48)
1,

Pr

ˆµt(x) − µt(x) > −

(N − 2 + Kxt,yt) (kt(x) + σ2)
1N (N − t − 2)

2δ(cid:48)

≥ 1 − δ(cid:48)
1.

(cid:35)

(cid:35)

(cid:115)

(cid:115)

Notice that the randomness of Kxt,yt is from yt and yt ∼ N (µ(xt), k(xt) + σ2I). So we can
further bound Kxt,yt ≤ t + 2
1 by Corollary 9. Hence,
1 = δ1
1 = δ1
if we set δ(cid:48)

with probability at most δ(cid:48)(cid:48)

+ 2 log 1
δ(cid:48)(cid:48)
1

t log 1
δ(cid:48)(cid:48)
1

4 and δ(cid:48)(cid:48)
ˆµt(x) − µt(x) < ιt

2 , with probability at least 1 − δ1, we have
(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:112)(kt(z) + σ2),

(cid:113)

for ﬁxed inputs x, x(cid:48).

Combining this result and the results in Eq. (15), Eq. (16), Eq. (20), we proved the lemma.

Lemma 12 (Lemma 1 in the paper). Pick probability δ ∈ (0, 1). For any nonnegative integer t < T ,
conditioned on the observations Dt = {(xτ , yτ )}t
τ =1, the estimators in Eq. (9) and Eq. (10) satisfy
E[ˆµt(X)] = µt(X), E[ˆkt(X)] = kt(X) + σ2I. Moreover, if the size of the training dataset satisfy
N ≥ T + 2, then for any input x ∈ X, with probability at least 1 − δ, both

|ˆµt(x) − µt(x)|2 < at(kt(x) + σ2) and 1 − 2

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

(cid:112)

hold, where at =

δN (N −t−2)

and bt = 1

N −t−1 log 4
δ .

(cid:16)

√

4

N −2+t+2

t log (4/δ)+2 log (4/δ)

(cid:112)

(cid:17)

Proof. By a union bound on Eq. (16) of Lemma 11, we have

(cid:104)

Pr

1 − 2

(cid:112)

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

bt + 2bt

≥ 1 − 2e−bt(N −t−1)

(cid:112)

(cid:105)

N −t−1 log 1
δ2

where bt = 1
(cid:104)
ˆµt(x) − µt(x) < ιt

Pr

> 0 and δ2 ∈ (0, 1). By Lemma 11, we also have
(cid:112)(kt(x) + σ2) ∧ ˆµt(z) − µt(z) > −ιt

(cid:105)
(cid:112)(kt(z) + σ2)

≥ 1 − δ1,

. We get the conclusion of this lemma by setting at =

(cid:114)

(cid:16)

2

N −2+t+2

(cid:113)

t log 2
δ1

+2 log 2
δ1

(cid:17)

δ1N (N −t−2)

where ιt =
ιt, δ1 = δ2 = δ

2 , and z = x.

17

Corollary 13 (Corollary of Bernoulli’s inequality). For any 0 ≤ x ≤ c and a > 0, we have
x ≤ c log(1+ ax
c )

.

log(1+a)

Proof. By Bernoulli’s inequality, (1 + a) x
have x ≤ c log(1+ ax
c )
.

log(1+a)

c ≤ 1 + ax

c . Because log(1 + a) > 0, by rearranging, we

Lemma 14. For any 0 ≤ x ≤ c and a > 0, we have

x <

√

√

x + a − a
√

.

2

c+a

Proof. Numerically, for any n ≥ 1, 1√

n − 2

n − 1 [55]. Let n = x

a + 1. Then, we have

√

n < 2
1
(cid:112) x
a + 1
a
a + x
√

√

√

(cid:114) x
a

√

< 2

+ 1 − 2

(cid:114) x
a

√

< 2

x + a − 2

x

√

x <

x + a −

a
a + c

.

√
2

√

a
a + c

<

Lemma 15 (Lemma 5.3 of [51]). Let xT = [xt]T
function values f (xT ) and their observations yT = [yt]T
t=1 satisfy
1
1
(cid:88)k
2
2

log det(I + σ−2k(xt)) =

I(f (xT ); yT ) =

t=1

log(1 + σ−2kt−1(xt)).

t=1 ⊆ X. The mutual information between the

Theorem 16. Assume there exist constant c ≥ maxx∈X k(x) and a training dataset is available
whose size is N ≥ 4 log 6
(cid:118)
(cid:117)
(cid:117)
(cid:116)

δ + T + 2. Deﬁne
(cid:113)

δ + 2 log 6

N − 3 + t + 2

t log 6

(cid:17)

(cid:16)

6

δ

, bt−1 =

log

for any t ∈ [T ],

ιt−1 =

1
N − t

6
δ

,

δN (N − t − 1)

1

and ρT = max

2 log |I + σ−2k(A)|. Then, with probability at least 1 − δ, the best-sample
simple regret in T iterations of meta BO with GP-UCB that uses Eq. (12) as its hyperparameter
satisﬁes

A∈X,|A|=T

rGP-UCB
T

≤ ηGP-UCB

2cρT
T log(1 + cσ−2)

+ σ2 −

2 σ2

(2 log( 3
√

δ )) 1
c + σ2

,

(cid:115)

1
2

(cid:113)

where ηGP-UCB = ( ιT −1+(2 log( 3
δ ))
bT −1

1−2

√

(cid:113)

1 + 2(cid:112)bT −1 + 2bT −1 + ιT −1 + (2 log( 3

δ )) 1

2 ).

With probability at least 1 − δ, the best-sample simple regret in T iterations of meta BO with PI that
uses ˆf ∗ ≥ maxx∈X f (x) as its target value satisﬁes

(cid:115)

T < ηPI
rPI

2cρT
T log(1 + cσ−2)

+ σ2 −

(2 log( 3
√
2

2δ )) 1
c + σ2

2 σ2

,

where ηPI = (

ˆf ∗−µτ −1(x∗)
√
kτ −1(x∗)+σ2

+ ιτ −1)

arg mint∈[T ] kt−1(xt).

(cid:115)

1+2b

1
2
τ −1+2bτ −1
1
2
τ −1

1−2b

+ ιτ −1 + (2 log( 3

2δ )) 1
2 ,

τ =

Proof. We ﬁrst show the regret bound for GP-UCB with our estimators of prior and posterior. All
of the probabilities mentioned in the proofs need to be interpreted in a frequentist manner. Let
τ = arg mint∈[T ] kt−1(xt). By Corollary 6, with probability at least 1 − δ
3 ,
= f ∗ − maxt∈[T ] f (xt)
≤ f ∗ − f (xτ )
≤ f ∗ − µτ −1(xτ ) + µτ −1(xτ ) − f (xτ )
≤ µτ −1(x∗) + ζ (cid:48)(cid:112)kτ −1(x∗) − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ),

rGP-UCB
T

18

where ζ (cid:48) = (2 log( 3

δ )) 1

µτ −1(x∗) − µτ −1(xτ ) < ˆµτ −1(x∗) − ˆµτ −1(xτ ) + ιτ −1

2 . By Lemma 11, with probability at least 1 − δ
3 ,
(cid:112)kτ −1(x∗) + σ2 + ιτ −1

(cid:112)kτ −1(xτ ) + σ2,

where ιt =

(cid:114)

(cid:16)

6

N −2+t+2

t log 6

δ +2 log 6

δ

(cid:17)

√

δN (N −t−2)

≤ ιT −1.

Lemma 11 and Lemma 14 also show that with probability at least 1 − δ

6 , we have

(cid:112)kτ −1(x∗) ≤ (cid:112)kτ −1(x∗) + σ2 −

σ2
c + σ2

√
2

<

(cid:115) ˆkτ −1(x∗)
1 − 2(cid:112)bτ −1

−

σ2
c + σ2

√
2

1

where bt =
GP-UCB with ζt = ιt−1+ζ(cid:48)
√

N −t−1 log 6

(cid:113)

1−2

bt−1

δ ≤ bT −1 ∈ (0, 1

4 ). Notice that because of the input selection strategy of

, the following inequality holds with probability at least 1 − δ
6 ,

ˆµτ −1(x∗) + (ιt−1 + ζ (cid:48))(cid:112)kτ −1(x∗) + σ2 ≤ ˆµτ −1(x∗) + ζt

ˆkτ −1(x∗)

(cid:113)

(cid:113)

≤ ˆµτ −1(xτ ) + ζt

ˆkτ −1(xτ ).

Hence, with probability at least 1 − δ,

rGP-UCB
T

≤ µτ −1(x∗) + ζ (cid:48)(cid:112)kτ −1(x∗) + σ2 − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ) + σ2 −

ζ (cid:48)σ2
c + σ2
< ˆµτ −1(x∗) − ˆµτ −1(xτ ) + (ιt−1 + ζ (cid:48))((cid:112)kτ −1(x∗) + σ2 + (cid:112)kτ −1(xτ ) + σ2) −

√

√

ζ (cid:48)σ2
c + σ2

(cid:113)

(cid:113)

≤ ζt

ˆkτ −1(xτ ) + (ιt−1 + ζ (cid:48))(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

< (ζt

1 + 2(cid:112)bt−1 + 2bt−1 + ιt−1 + ζ (cid:48))(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

< ηGP-UCB(cid:112)kτ −1(xτ ) + σ2 −

√

ζ (cid:48)σ2
c + σ2

,

where ηGP-UCB = (

(cid:113)

ιT −1+ζ(cid:48)
√

(cid:113)

1−2

bT −1

that τ = arg mint∈[T ] kt−1(xt), we have

1 + 2(cid:112)bT −1 + 2bT −1 + ιT −1 + ζ (cid:48)). By Corollary 13 and the fact

kτ −1(xτ ) ≤

kt−1(xt)

(cid:88)T

t=1

(cid:88)T

1
T

1
T

c log(1 + cσ−2kt−1(xt)
log(1 + cσ−2)

c

)

≤

=

t=1
c
T log(1 + cσ−2)

(cid:88)T

t=1

log(1 + σ−2kt−1(xt)).

Notice that here Corollary 13 applies because 0 ≤ kτ −1(xτ ) ≤ c.

By Lemma 15, I(f (xT ); yT ) = 1
2

(cid:80)T

t=1 log(1 + σ−2kt−1(xt)) ≤ ρT , so
2cρT
T log(1 + cσ−2)

,

kτ −1(xτ ) ≤

which implies

(cid:115)

rGP-UCB
T

< η

2cρT
T log(1 + cσ−2)

+ σ2 −

√

ζ (cid:48)σ2
c + σ2

.

19

Next, we show the proof for a special case of PI with ˆf ∗, an upper bound on f , as its target value.
Again, by Corollary 6, with probability at least 1 − δ
3 ,
T = ˆf ∗ − maxt∈[T ] f (xt)
rPI

≤ ˆf ∗ − f (xτ )
≤ ˆf ∗ − µτ −1(xτ ) + µτ −1(xτ ) − f (xτ )
≤ ˆf ∗ − µτ −1(xτ ) + ζ (cid:48)(cid:112)kτ −1(xτ ),

2δ )) 1

where ζ (cid:48) = (2 log( 3
of PI, with probability at least 1 − 2δ
3 ,
ˆf ∗ − µτ −1(xτ ) < ˆf ∗ − ˆµτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2

2 and τ = arg mint∈[T ] kt−1(xt). By Lemma 11 and the selection strategy

ˆkτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2

(cid:113)

ˆf ∗ − ˆµτ −1(x∗)
ˆkτ −1(x∗)

(cid:113)

≤

≤

(cid:113)

ˆkτ −1(x∗)

ˆf ∗ − µτ −1(x∗) + ιτ −1

(cid:112)kτ −1(x∗) + σ2

(cid:113)

ˆkτ −1(xτ ) + ιτ −1

(cid:112)kτ −1(xτ ) + σ2



<

(

ˆf ∗ − µτ −1(x∗)
(cid:112)kτ −1(x∗) + σ2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+ ιτ −1)

1
2

1 + 2b

τ −1 + 2bτ −1

1 − 2b

1
2
τ −1



+ ιτ −1



(cid:112)kτ −1(xτ ) + σ2.

Hence, with probability at least 1 − δ, the best-sample simple regret of PI satisfy

(cid:115)

T < ηPI
rPI

2cρT
T log(1 + cσ−2)

+ σ2 −

ζ (cid:48)σ2
c + σ2

√
2

,

where ηPI = (

ˆf ∗−µτ −1(x∗)
√
kτ −1(x∗)+σ2

+ ιτ −1)

(cid:115)

1+2b

1
2
τ −1+2bτ −1
1
2
τ −1

1−2b

+ ιτ −1 + ζ (cid:48).

Theorem 17 (Theorem 2 in th paper). Assume there exist constant c ≥ maxx∈X k(x) and a training
dataset is available whose size is N ≥ 4 log 6
δ + T + 2. Then, with probability at least 1 − δ, the
best-sample simple regret in T iterations of meta BO with special cases of either GP-UCB or PI
satisﬁes

T < ηUCB
rUCB

T

T < ηPI

T (N )λT , λ2

T = O(ρT /T ) + σ2,

where ηU CB

T

(N ) = (m+C1)(

C1, C2, C3 > 0 are constants, and ρT = max

T (N ) = (m+C2)(
1

2 log |I + σ−2k(A)|.

√
1+m√
1−m

A∈X,|A|=T

+1)+C3, m = O(

(cid:113) 1

N −T ),

(N )λT , rPI
√
1+m√
1−m

+1), ηPI

Proof. This theorem is a condensed version of Thm. 16 with big O notations.

C Proofs for Section 4.2

Recall that we assume X is a compact set which is a subset of Rd. We only considers a special case
of GPs that assumes f (x) = Φ(x)TW , W ∼ N (u, Σ) and the basis functions Φ(x) ∈ RK are given.
The mean function and kernel are deﬁned as

µ(x) = Φ(x)Tu and k(x) = Φ(x)TΣΦ(x).

Given noisy observations Dt = {(xτ , yτ )}t

τ =1, t ≤ K, we have

µt(x) = Φ(x)Tut and kt(x, x(cid:48)) = Φ(x)TΣtΦ(x(cid:48)),

where the posterior of W ∼ N (ut, Σt) satisﬁes

ut = u + ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1(yt − Φ(xt)Tu),
Σt = Σ − ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1Φ(xt)TΣ.

20

Our estimators for ut and Σt are

ˆut = ˆu + ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1(yt − Φ(xt)Tu),

ˆΣt =

N − 1
N − t − 1

(cid:16) ˆΣ − ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1Φ(xt)T ˆΣ

(cid:17)

.

We can compute the approximated conditional mean and variance of the observation on x ∈ X to be
ˆµt(x) = Φ(x)T ˆut and ˆkt(x) = Φ(x)T ˆΣtΦ(x).

Again, we prove a bound on the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt).
The evaluated inputs xt = [xτ ]t
τ are selected either by a special case of GP-UCB using the acquisition
function

αGP-UCB
t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1

2 , with

(cid:16)

6(N − 3 + t + 2

t log 6

(cid:113)

ζt =

δ ) 1
or by a special case of PI using the acquisition function

N −t log 6

δ + 2 log 6
(1 − 2( 1

δ )/(δN (N − t − 1))
2 ) 1

2

(cid:17) 1

2

+ (2 log( 3

δ )) 1

2

, δ ∈ (0, 1),

αPI

t−1(x) =

ˆµt−1(x) − ˆf ∗
ˆkt−1(x) 1

2

.

This special case of PI assumes additional information of the upper bound on function value ˆf ∗ ≥
maxx∈X f (x).
For convenience of the notations, we deﬁne ¯σ2(x) = σ2Φ(x)T(Φ( ¯x)Φ( ¯x)T)−1Φ(x).

Corollary 18 combines Lemma 7 and basic properties of the Wishart distribution [12].
Corollary 18. Assume the matrix Φ( ¯x) ∈ RK×M has linearly independent rows. Then, ˆu and ˆΣ
are independent and

ˆu ∼ N

u,

(Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1)

, ˆΣ ∼ W

(cid:18)

1
N

(cid:18) 1

N − 1

(cid:0)Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1(cid:1) , N − 1

(cid:19)

.

For ﬁnite set of inputs x ⊂ X, ˆµ(x) and ˆk(x) are also independent; they satisfy

ˆµ(x) ∼ N

µ,

(k(x) + ¯σ2(x))

, ˆk(x) ∼ W

(cid:18)

1
N

(cid:18) 1

N − 1

(cid:0)k(x) + ¯σ2(x)(cid:1) , N − 1

(cid:19)

.

(cid:19)

(cid:19)

The proofs of Lemma 3 and Theorem 4 in the paper directly follow Corollary 18 and proofs of
Lemma 11, Theorem 16 in this appendix.

D Proofs for Section 4.3

We show that the simple regret with ˆx∗
simple regret.
Lemma 19. With probability at least 1 − δ, RT − rT ≤ 2(2 log 1

δ ) 1

2 σ.

T = xτ , τ = arg maxt∈[T ] yt is very close to the best-sample

Proof. Let τ (cid:48) = arg maxt∈[T ] f (xt) and τ = arg maxt∈[T ] yt. Note that yτ ≥ yτ (cid:48). By Corollary 6,
δ ) 1
with probability at least 1 − δ, f (xτ ) + Cσ ≥ yτ ≥ yτ (cid:48) ≥ f (xτ (cid:48)) − Cσ, where C = (2 log 1
2 .
Hence RT − rT = f (xτ (cid:48)) − f (xτ ) ≤ 2Cσ.

E Experiments

For Plain and TLSM-BO with UCB in our experiments, we used the same ζt as PEM-BO.

In the following, we include extra experiments that we performed with PI acquisition function and
matrix completion for the missing entry case in the discrete domains. The PI approach uses the
maximum function value in the training dataset ¯DN as the target value. These results show that our
approach is resilient to missing data. BO with the PI acquisition function performs similarly to UCB.

21

Figure 4: Rewards vs. Number of evals for grasp optimization, grasp, base pose, and placement
optimization, and synthetic function optimization problems (from top-left to bottom). 0.6xPEM-
BO refers to the case where we have 60 percent of the dataset missing.

22

