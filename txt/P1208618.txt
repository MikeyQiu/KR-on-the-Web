9
1
0
2
 
n
a
J
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
9
0
3
2
0
.
0
1
8
1
:
v
i
X
r
a

Learning Compressed Transforms
with Low Displacement Rank

Anna T. Thomas∗†, Albert Gu∗†, Tri Dao†, Atri Rudra‡, and Christopher Ré†

†Department of Computer Science, Stanford University
‡Department of Computer Science and Engineering, University at Buﬀalo, SUNY
{thomasat,albertgu,trid}@stanford.edu, atri@buffalo.edu,
chrismre@cs.stanford.edu

January 3, 2019

Abstract

The low displacement rank (LDR) framework for structured matrices represents a matrix through two
displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied
ﬁxed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a class
of LDR matrices with more general displacement operators, and explicitly learn over both the operators
and the low-rank component. This class generalizes several previous constructions while preserving
compression and eﬃcient computation. We prove bounds on the VC dimension of multi-layer neural
networks with structured weight matrices and show empirically that our compact parameterization can
reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional,
and recurrent neural networks for image classiﬁcation and language modeling tasks, our new classes exceed
the accuracy of existing compression approaches, and on some tasks also outperform general unstructured
layers while using more than 20x fewer parameters.

1

Introduction

Recent years have seen a surge of interest in structured representations for deep learning, motivated by
achieving compression and acceleration while maintaining generalization properties. A popular approach
for learning compact models involves constraining the weight matrices to exhibit some form of dense but
compressible structure and learning directly over the parameterization of this structure. Examples of structures
explored for the weight matrices of deep learning pipelines include low-rank matrices [15, 42], low-distortion
projections [49], (block-)circulant matrices [8, 17], Toeplitz-like matrices [34, 45], and constructions derived
from Fourier-related transforms [37]. Though they confer signiﬁcant storage and computation beneﬁts, these
constructions tend to underperform general fully-connected layers in deep learning. This raises the question of
whether broader classes of structured matrices can achieve superior downstream performance while retaining
compression guarantees.

Our approach leverages the low displacement rank (LDR) framework (Section 2), which encodes
structure through two sparse displacement operators and a low-rank residual term [27]. Previous work
studying neural networks with LDR weight matrices assumes ﬁxed displacement operators and learns only
over the residual [45, 50]. The only case attempted in practice that explicitly employs the LDR framework
uses ﬁxed operators encoding shift invariance, producing weight matrices which were found to achieve superior
downstream quality than several other compression approaches [45]. Unlike previous work, we consider
learning the displacement operators jointly with the low-rank residual. Building upon recent progress on
structured dense matrix-vector multiplication [14], we introduce a more general class of LDR matrices and

∗These authors contributed equally.

1

develop practical algorithms for using these matrices in deep learning architectures. We show that the
resulting class of matrices subsumes many previously used structured layers, including constructions that
did not explicitly use the LDR framework [17, 37]. When compressing weight matrices in fully-connected,
convolutional, and recurrent neural networks, we empirically demonstrate improved accuracy over existing
approaches. Furthermore, on several tasks our constructions achieve higher accuracy than general unstructured
layers while using an order of magnitude fewer parameters.

To shed light on the empirical success of LDR matrices in machine learning, we draw connections to
recent work on learning equivariant representations, and hope to motivate further investigations of this link.
Notably, many successful previous methods for compression apply classes of structured matrices related
to convolutions [8, 17, 45]; while their explicit aim is to accelerate training and reduce memory costs, this
constraint implicitly encodes a shift-invariant structure that is well-suited for image and audio data. We
observe that the LDR construction enforces a natural notion of approximate equivariance to transformations
governed by the displacement operators, suggesting that, in contrast, our approach of learning the operators
allows for modeling and learning more general latent structures in data that may not be precisely known in
advance.

Despite their increased expressiveness, our new classes retain the storage and computational beneﬁts of
conventional structured representations. Our construction provides guaranteed compression (from quadratic
to linear parameters) and matrix-vector multiplication algorithms that are quasi-linear in the number of
parameters. We additionally provide the ﬁrst analysis of the sample complexity of learning neural networks
with LDR weight matrices, which extends to low-rank, Toeplitz-like and other previously explored ﬁxed
classes of LDR matrices. More generally, our analysis applies to structured matrices whose parameters can
interact multiplicatively with high degree. We prove that the class of neural networks constructed from these
matrices retains VC dimension almost linear in the number of parameters, which implies that LDR matrices
with learned displacement operators are still eﬃciently recoverable from data. This is consistent with our
empirical results, which suggest that constraining weight layers to our broad class of LDR matrices can reduce
the sample complexity of learning compared to unstructured weights.

We provide a detailed review of previous work and connections to our approach in Appendix B.

Summary of contributions:

•

•

•

We introduce a rich class of LDR matrices where the displacement operators are explicitly learned from
data, and provide multiplication algorithms implemented in PyTorch (Section 3).1

We prove that the VC dimension of multi-layer neural networks with LDR weight matrices, which
encompasses a broad class of previously explored approaches including the low-rank and Toeplitz-like
classes, is quasi-linear in the number of parameters (Section 4).

We empirically demonstrate that our construction improves downstream quality when compressing
weight layers in fully-connected, convolutional, and recurrent neural networks compared to previous
compression approaches, and on some tasks can even outperform general unstructured layers (Section 5).

2 Background: displacement rank

The generic term structured matrix refers to an m
n matrix that can be represented in much fewer than mn
parameters, and admits fast operations such as matrix-vector multiplication. The displacement rank approach
n)
represents a structured matrix M
deﬁning a linear map

n through displacement operators (A
×
MB on matrices, and a residual R, so that if

∈
AM

m, B

Rm

Rm

Rn

×

∈

∈

×

×

A,B : M

∇

(cid:55)→

−

AM

MB = R

−

(1)

then M can be manipulated solely through the compressed representation (A, B, R). We assume that A and
B have disjoint eigenvalues, which guarantees that M can be recovered from A, B, R (c.f. Theorem 4.3.2,
Pan [40]). The rank of R (also denoted

A,B[M]) is called the displacement rank of M w.r.t. (A, B).2

∇

1Our code is available at https://github.com/HazyResearch/structured-nets.
2Throughout this paper, we use square matrices for simplicity, but LDR is well-deﬁned for rectangular.

2

The displacement approach was originally introduced to describe the Toeplitz-like matrices, which are
not perfectly Toeplitz but still have shift-invariant structure [27]. These matrices have LDR with respect

to shift/cycle operators. A standard formulation uses A = Z1, B = Z

1, where Zf =

1(cid:21)
denotes the matrix with 1 on the subdiagonal and f in the top-right corner. The Toeplitz-like matrices have
previously been applied in deep learning and kernel approximation, and in several cases have performed
signiﬁcantly better than competing compressed approaches [10, 34, 45]. Figure 1 illustrates the displacement (1)
for a Toeplitz matrix, showing how the shift invariant structure of the matrix leads to a residual of rank at
most 2.

(cid:20)

1)

−

−

×

−

01

1)

(n
×
In

−
1

f

0(n

Figure 1: Displacement equation for a Toeplitz matrix with respect to shift operators Z1, Z−1.

A few distinct classes of useful matrices are known to satisfy a displacement property: the classic types are
the Toeplitz-, Hankel-, Vandermonde-, and Cauchy-like matrices (Appendix C, Table 5), which are ubiquitous
in other disciplines [40]. These classes have ﬁxed operators consisting of diagonal or shift matrices, and LDR
properties have traditionally been analyzed in detail only for these special cases. Nonetheless, a few elegant
properties hold for generic operators, stating that certain combinations of (and operations on) LDR matrices
preserve low displacement rank. We call these closure properties, and introduce an additional block closure
property that is related to convolutional ﬁlter channels (Section 5.2).

We use the notation

r
A,B to refer to the matrices of displacement rank

r with respect to (A, B).

D

≤

Proposition 1. LDR matrices are closed under the following operations:

r
BT ,AT and M−

1

r
B,A.

∈ D

A,B, then MT
r

∈ D
s
A,B, then M + N

∈ D

s
B,C, then MN

r+s
A,B.

∈ D

r+s
A,C.

∈ D

(a) Transpose/Inverse If M

(b) Sum If M

∈ D
(c) Product If M

r
A,B and N

∈ D
r
A,B and N

∈ D

(d) Block Let Mij satisfy Mij ∈ D
has displacement rank rk(cid:96).

∈ D
r
Ai,Bj

Proposition 1 is proved in Appendix C.

for i = 1 . . . k, j = 1 . . . (cid:96). Then the k

(cid:96) block matrix (Mij)ij

×

3 Learning displacement operators

We consider two classes of new displacement operators. These operators are ﬁxed to be matrices with
particular sparsity patterns, where the entries are treated as learnable parameters.

The ﬁrst operator class consists of subdiagonal (plus corner) matrices: Ai+1,i, along with the corner
1, are the only possible non-zero entries. As Zf is a special case matching this sparsity pattern, this

A0,n
class is the most direct generalization of Toeplitz-like matrices with learnable operators.

−

1 and An

The second class of operators are tridiagonal (plus corner) matrices: with the exception of the outer
corners A0,n
1. Figure 2 shows the displacement operators
i
for the Toeplitz-like class and our more general operators. We henceforth let LDR-SD and LDR-TD denote
the classes of matrices with low displacement rank with respect to subdiagonal and tridiagonal operators,
respectively. Note that LDR-TD contains LDR-SD.

1,0, Ai,j can only be non-zero if

| ≤

−

−

−

j

|

3

0

1

...





0


0



· · ·

. . .
. . .
. . .

0

1
. . .
0

0
. . .

. . .
1

f

0
...

0












0

x1
...

0
0












· · ·

0
. . .

. . .
. . .
. . .
. . . xn

1

−

0

x2
. . .
0

x0

0
...

0












b0 a0
c0
b1
...

c1

0











0
t

· · ·
a1
. . .
. . .
. . .

0

. . .

s
0
...

bn
cn

−

−

1 an
bn
2

2

−
1

−











Figure 2: The Zf operator (left), and our learnable subdiagonal (center) and tridiagonal (right) operators, corresponding
to our proposed LDR-SD and LDR-TD classes.

Expressiveness The matrices we introduce can model rich structure and subsume many types of linear
transformations used in machine learning. We list some of the structured matrices that have LDR with
respect to tridiagonal displacement operators:

Proposition 2. The LDR-TD matrices contain:

(a) Toeplitz-like matrices, which themselves include many Toeplitz and circulant variants (including standard

convolutional ﬁlters - see Section 5.2 and Appendix C, Corollary 1) [8, 17, 45].

(b) low-rank matrices.

(c) the other classic displacement structures: Hankel-like, Vandermonde-like, and Cauchy-like matrices.

(d) orthogonal polynomial transforms, including the Discrete Fourier and Cosine Transforms.

(e) combinations and derivatives of these classes via the closure properties (Proposition 1), including
structured classes previously used in machine learning such as ACDC [37] and block circulant layers [17].

These reductions are stated more formally and proved in Appendix C.1. We also include a diagram of the

structured matrix classes included by the proposed LDR-TD class in Figure 5 in Appendix C.1.

Our parameterization Given the parameters A, B, R, the operation that must ultimately be performed
1
is matrix-vector multiplication by M =
A,B[R]. Several schemes for explicitly reconstructing M from
−
its displacement parameters are known for speciﬁc cases [41, 44], but do not always apply to our general
operators. Instead, we use A, B, R to implicitly construct a slightly diﬀerent matrix with at most double the
displacement rank, which is simpler to work with.

∇

Proposition 3. Let
vectors g1, . . . , gr, h1, . . . , hr ∈

K

Rn, then the matrix

×

(A, v) denote the n

n Krylov matrix, deﬁned to have i-th column Aiv. For any

r

K

i=1
(cid:88)

(A, gi)

(BT , hi)T

K

(2)

has displacement rank at most 2r with respect to A−

1, B.

Thus our representation stores the parameters A, B, G, H, where A, B are either subdiagonal or tridiagonal
r. These parameters implicitly deﬁne the

Rn

operators (containing n or 3n parameters), and G, H
matrix (2), which is the LDR weight layer we use.

∈

×

Algorithms for LDR-SD Generic and near-linear time algorithms for matrix-vector multiplication by
LDR matrices with even more general operators, including both the LDR-TD and LDR-SD classes, were
recently shown to exist [14]. However, complete algorithms were not provided, as they relied on theoretical
results such as the transposition principle [6] that only imply the existence of algorithms. Additionally, the
recursive polynomial-based algorithms are diﬃcult to implement eﬃciently. For LDR-SD, we provide explicit

4

and complete near-linear time algorithms for multiplication by (2), as well as substantially simplify them to
be useful in practical settings and implementable with standard library operations. We empirically compare
the eﬃciency of our implementation and unstructured matrix-vector multiplication in Figure 8 and Table 14
in Appendix E, showing that LDR-SD accelerates inference by 3.34-46.06x for n
4096. We also show
results for the low-rank and Toeplitz-like classes, which have a lower computational cost. For LDR-TD, we
(BT , hi) matrices for i = 1, ..., r from Proposition 3 and then apply
explicitly construct the
the standard O(n2) matrix-vector multiplication algorithm. Eﬃcient implementations of near-linear time
algorithms for LDR-TD are an interesting area of future work.

(A, gi) and

≥

K

K

Theorem 1. Deﬁne the simultaneous computation of k Fast Fourier Transforms (FFT), each with size m,
to be a batched FFT with total size km.
Consider any subdiagonal matrix A

(A, g) can be
multiplied by any vector x by computing 8 log2(n) batched FFTs, each of total size 2n. The total number of
computations is O(n log2 n).

n and vectors g, h

(A, g)T or

Rn. Then

Rn

K

K

∈

∈

×

These algorithms are also automatically diﬀerentiable, which we use to compute the gradients when

learning. More complete descriptions of these algorithms are presented in Appendix C.

4 Theoretical properties of structured matrices

Complexity of LDR neural networks The matrices we use (2) are unusual in that the parameters
interact multiplicatively (namely in Ai, Bi) to implicitly deﬁne the actual layer. In contrast, fully-connected
layers are linear and other structured layers, such as Fastfood and ACDC [31, 37, 49], are constant degree in
their parameters. However, we can prove that this does not signiﬁcantly change the learnability of our classes:

Theorem 2. Let
linear activations. Let sign
The VC dimension of this class is

F

F

denote the class of neural networks with L LDR layers, W total parameters, and piecewise
.

denote the corresponding classiﬁcation functions, i.e.

sign f (x) : f

x
{

(cid:55)→

∈ F}

VCdim(sign

) = O(LW log W ).

F

Theorem 2 matches the standard bound for unconstrained weight matrices [4, 24]. This immediately
implies a standard PAC-learnable guarantee [47]. Theorem 2 holds for even more general activations and
matrices that for example include the broad classes of [14]. The proof is in Appendix D, and we empirically
validate the generalization and sample complexity properties of our class in Section 5.3.

Displacement rank and equivariance We observe that displacement rank is related to a line of work
outside the resource-constrained learning community, speciﬁcally on building equivariant (also called
covariant in some contexts [5, 35]) feature representations that transform in predictable ways when the input
is transformed. An equivariant feature map Φ satisﬁes

Φ(B(x)) = A(Φ(x))

(3)

for transformations A, B (invariance is the special case when A is the identity) [16, 33, 43]. This means that
perturbing the input by a transformation B before passing through the map Φ is equivalent to ﬁrst ﬁnding
the features Φ then transforming by A.

−

Intuitively, LDR matrices are a suitable choice for modeling approximately equivariant linear maps, since
ΦB of (3) has low complexity. Furthermore, approximately equivariant maps should
the residual AΦ
retain the compositional properties of equivariance, which LDR satisﬁes via Proposition 1. For example,
Proposition 1(c) formalizes the notion that the composition of two approximately equivariant maps is still
approximately equivariant. Using this intuition, the displacement representation (1) of a matrix decomposes
into two parts: the operators A, B deﬁne transformations to which the model is approximately equivariant,
and the low complexity residual R controls standard model capacity.

Equivariance has been used in several ways in the context of machine learning. One formulation, used for
example to model ego-motions, supposes that (3) holds only approximately, and uses a ﬁxed transformation

5

B along with data for (3) to learn an appropriate A [1, 33]. Another line of work uses the representation
theory formalization of equivariant maps [12, 28]. We describe this formulation in more detail and show how
LDR satisﬁes this deﬁnition as well in Appendix C.3, Proposition 7. In contrast to previous settings, which
ﬁx one or both of A, B, our formulation stipulates that Φ can be uniquely determined from A, B, and learns
the latter as part of an end-to-end model. In Section 5.4 we include a visual example of latent structure that
our displacement operators learn, where they recover centering information about objects from a 2D image
dataset.

5 Empirical evaluation

Overview In Section 5.1 we consider a standard setting of compressing a single hidden layer (SHL) neural
network and the fully-connected (FC) layer of a CNN for image classiﬁcation tasks. Following previous
work [7, 45], we test on two challenging MNIST variants [30], and include two additional datasets with more
realistic objects (CIFAR-10 [29] and NORB [32]). Since SHL models take a single channel as input, we
converted CIFAR-10 to grayscale for this task. Our classes and the structured baselines are tested across
diﬀerent parameter budgets in order to show tradeoﬀs between compression and accuracy. As shown in
Table 1, in the SHL model, our methods consistently have higher test accuracy than baselines for compressed
training and inference, by 3.14, 2.70, 3.55, and 3.37 accuracy points on MNIST-bg-rot, MNIST-noise,
CIFAR-10, and NORB respectively. In the CNN model, as shown in Table 1 in Appendix E, we found
improvements of 5.56, 0.95, and 1.98 accuracy points over baselines on MNIST-bg-rot, MNIST-noise, and
NORB respectively. Additionally, to explore whether learning the displacement operators can facilitate
adaptation to other domains, we replace the input-hidden weights in an LSTM for a language modeling task,
and show improvements of 0.81-30.47 perplexity points compared to baselines at several parameter budgets.
In addition to experiments on replacing fully-connected layers, in Section 5.2 we also replace the con-
volutional layer of a simple CNN while preserving performance within 1.05 accuracy points on CIFAR-10.
In Section 5.3, we consider the eﬀect of a higher parameter budget. By increasing the rank to just 16, the
LDR-SD class meets or exceeds the accuracy of the unstructured FC layer in all datasets we tested on, for
both SHL and CNN.3 Appendix F includes more experimental details and protocols. Our PyTorch code is
publicly available at github.com/HazyResearch/structured-nets.

5.1 Compressing fully-connected layers

Image classiﬁcation Sindhwani et al. [45] showed that for a ﬁxed parameter budget, the Toeplitz-like
class signiﬁcantly outperforms several other compression approaches, including Random Edge Removal [11],
Low Rank Decomposition [15], Dark Knowledge [25], HashedNets [7], and HashedNets with Dark Knowledge.
Following previous experimental settings [7, 45], Table 1 compares our proposed classes to several baselines
using dense structured matrices to compress the hidden layer of a single hidden layer neural network.
In addition to Toeplitz-like, we implement and compare to other classic LDR types, Hankel-like and
Vandermonde-like, which were previously indicated as an unexplored possibility [45, 50]. We also show results
when compressing the FC layer of a 7-layer CNN based on LeNet in Appendix E, Table 7. In Appendix E, we
show comparisons to additional baselines at multiple budgets, including network pruning [23] and a baseline
used in [7], in which the number of hidden units is adjusted to meet the parameter budget.

At rank one (the most compressed setting), our classes with learned operators achieve higher accuracy
than the ﬁxed operator classes, and on the MNIST-bg-rot, MNIST-noise, and NORB datasets even improve
on FC layers of the same dimensions, by 1.73, 13.30, and 2.92 accuracy points respectively on the SHL task,
as shown in Table 1. On the CNN task, our classes improve upon unstructured fully-connected layers by 0.85
and 2.25 accuracy points on the MNIST-bg-rot and MNIST-noise datasets (shown in Table 7 in Appendix E).
As noted above, at higher ranks our classes meet or improve upon the accuracy of FC layers on all datasets
in both the SHL and CNN architectures.

parameters between LDR-SD and the Toeplitz-like or low-rank is r+1

Additionally, in Figure 3 we evaluate the performance of LDR-SD at higher ranks. Note that the ratio of
r , which becomes negligible at higher
3In addition to the results reported in Table 1, Figure 3 and Table 7 in Appendix E, we also found that at rank 16 the
LDR-SD class on the CNN architecture achieved test accuracies of 68.48% and 75.45% on CIFAR-10 and NORB respectively.

6

Table 1: Test accuracy when replacing the hidden layer with structured classes. Where applicable, rank (r) is in
parentheses, and the number of parameters in the architecture is in italics below each method. Comparisons to
previously unexplored classic LDR types as well as additional structured baselines are included, with the ranks
adjusted to match the parameter count of LDR-TD where possible. The Fastfood [49] and Circulant [8] methods
do not have rank parameters, and the parameter count for these methods cannot be exactly controlled. Additional
results when replacing the FC layer of a CNN are in Appendix E. Details for all experiments are in Appendix F.

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Method

Unstructured

LDR-TD (r = 1)

Toeplitz-like [45] (r = 4)

Hankel-like (r = 4)

Vandermonde-like (r = 4)

Low-rank [15] (r = 4)

Fastfood [49]

Circulant [8]

44.08
622506
45.81
14122

42.67
14122

42.23
14122

37.14
14122

35.67
14122

38.13
10202

34.46
8634

65.15
622506
78.45
14122

75.75
14122

73.65
14122

59.80
14122

52.25
14122

63.55
10202

65.35
8634

46.03
1058826
45.33
18442

59.83
1054726
62.75
14342

41.78
18442

41.40
18442

33.93
18442

32.28
18442

39.64
13322

34.28
11274

59.38
14342

60.09
14342

48.98
14342

43.66
14342

59.02
9222

46.45
7174

ranks. Figure 3 shows that at just rank 16, the LDR-SD class meets or exceeds the performance of the FC
layer on all four datasets, by 5.87, 15.05, 0.74, and 6.86 accuracy points on MNIST-bg-rot, MNIST-noise,
CIFAR-10, and NORB respectively, while still maintaining at least 20x fewer parameters.

Of particular note is the poor performance of low-rank matrices. As mentioned in Section 2, every
ﬁxed-operator class has the same parameterization (a low-rank matrix). We hypothesize that the main
contribution to their marked performance diﬀerence is the eﬀect of the learned displacement operator modeling
latent invariances in the data, and that the improvement in the displacement rank classes—from low-rank
to Toeplitz-like to our learned operators—comes from more accurate representations of these invariances.
As shown in Figure 3, broadening the operator class (from Toeplitz-like at r = 1 to LDR-SD at r = 1) is
consistently a more eﬀective use of parameters than increasing the displacement rank (from Toeplitz-like at
r = 1 to r = 2). Note that LDR-SD (r = 1) and Toeplitz-like (r = 2) have the same parameter count.

For the rest of our experiments outside Section 5.1 we use the algorithms in Appendix C speciﬁcally for

LDR-SD matrices, and focus on further evaluation of this class on more expensive models.

Language modeling Here, we replace the input-hidden weights in a single layer long short-term memory
network (LSTM) for a language modeling task. We evaluate on the WikiText-2 dataset, consisting of 2M
training tokens and a vocabulary size of 33K [36]. We compare to Toeplitz-like and low-rank baselines, both
previously investigated for compressing recurrent nets [34]. As shown in Table 2, LDR-SD improves upon the
baselines for each budget tested. Though our class does not outperform the unstructured model, we did ﬁnd
that it achieves a signiﬁcantly lower perplexity than the ﬁxed Toeplitz-like class (by 19.94-42.92 perplexity
points), suggesting that learning the displacement operator can help adapt to diﬀerent domains.

7

Figure 3: Test accuracy vs. rank for unstructured, LDR-SD, Toeplitz-like, low-rank classes. On each dataset, LDR-SD
meets or exceeds the accuracy of the unstructured FC baseline at higher ranks. At rank 16, the compression ratio of
an LDR-SD layer compared to the unstructured layer ranges from 23 to 30. Shaded regions represent two standard
deviations from the mean, computed over ﬁve trials with randomly initialized weights.

Table 2: Test perplexity when replacing input-hidden matrices of an LSTM with structured classes on WikiText-2.
An unconstrained layer, with 65536 parameters, has perplexity 117.74. Parameter budgets correspond to ranks
1,2,4,8,16,24 for LDR-SD. Lower is better.

Num. Parameters LDR-SD Toeplitz-like Low-rank

2048
3072
5120
9216
17408
25600

166.97
154.51
141.91
143.60
132.43
129.46

186.91
177.60
178.07
186.52
162.58
155.73

205.72
179.46
172.38
144.41
135.65
133.37

5.2 Replacing convolutional layers

Convolutional layers of CNNs are a prominent example of equivariant feature maps.4 It has been noted that
convolutions are a subcase of Toeplitz-like matrices with a particular sparsity pattern5 [8, 45]. As channels
are simply block matrices6, the block closure property implies that multi-channel convolutional ﬁlters are
simply a Toeplitz-like matrix of higher rank (see Appendix C, Corollary 1). In light of the interpretation of
LDR of an approximately equivariant linear map (as discussed in Section 4), we investigate whether replacing
convolutional layers with more general representations can recover similar performance, without needing the
hand-crafted sparsity pattern.

Brieﬂy, we test the simplest multi-channel CNN model on the CIFAR-10 dataset, consisting of one layer
of convolutional channels (3 in/out channels), followed by a FC layer, followed by the softmax layer. The
ﬁnal accuracies are listed in Table 3. The most striking result is for the simple architecture consisting of
two layers of a single structured matrix. This comes within 1.05 accuracy points of the highly specialized
architecture consisting of convolutional channels + pooling + FC layer, while using fewer layers, hidden units,
and parameters. The full details are in Appendix F.

4Convolutions are designed to be shift equivariant, i.e. shifting the input is equivalent to shifting the output.
5E.g. a 3 × 3 convolutional ﬁlter on an n × n matrix has a Toeplitz weight matrix supported on diagonals −1, 0, 1, n − 1, n, n +

6A layer consisting of k in-channels and (cid:96) out-channels, each of which is connected by a weight matrix of class C, is the same

1, 2n − 1, . . . .

as a k × (cid:96) block matrix.

8

Table 3: Replacing a ﬁve-layer CNN consisting of convolutional channels, max pooling, and FC layers with two generic
LDR matrices results in only slight test accuracy decrease while containing fewer layers, hidden units, and parameters.
Rank (r) is in parentheses.

First hidden layer(s)

Last hidden layer Hidden units Parameters Test Acc.

3 Convolutional Channels (CC) FC
FC
3CC + Max Pool
FC
4CC + Max Pool

1573089
3072, 512
3072, 768, 512
393441
4096, 1024, 512 524588

Toeplitz-like (r = 16) channels Toeplitz-like (r = 16) 3072, 512
LDR-SD (r = 16)
LDR-SD (r = 16) channels
3072, 512
Toeplitz-like (r = 16) 3072, 512
Toeplitz-like (r = 48) matrix
3072, 512
LDR-SD (r = 16)
LDR-SD (r = 48) matrix

393216
417792
393216
405504

54.59
55.14
60.05

57.29
59.36
55.29
59.00

5.3 Generalization and sample complexity

Theorem 2 states that the theoretical sample complexity of neural networks with structured weight matrices
scales almost linearly in the total number of parameters, matching the results for networks with fully-connected
layers [4, 24]. As LDR matrices have far fewer parameters, the VC dimension bound for LDR networks
are correspondingly lower than that of general unstructured networks. Though the VC dimension bounds
are suﬃcient but not necessary for learnability, one might still expect to be able to learn over compressed
networks with fewer samples than over unstructured networks. We empirically investigate this result using
the same experimental setting as Table 1 and Figure 3. As shown in Table 12 (Appendix E), the structured
classes consistently have lower generalization error (measured by the diﬀerence between training and test
error) than the unstructured baseline.

Reducing sample complexity We investigate whether LDR models with learned displacement operators
require fewer samples to achieve the same test error, compared to unstructured weights, in both the single
hidden layer and CNN architectures. Tables 10 and 11 in Appendix E show our results. In the single hidden
layer architecture, when using only 25% of the training data the LDR-TD class exceeds the performance
of an unstructured model trained on the full MNIST-noise dataset. On the CNN model, only 50% of the
training data is suﬃcient for the LDR-TD to exceed the performance of an unstructured layer trained on the
full dataset.

5.4 Visualizing learned weights

×

∈

R784

Finally, we examine the actual structures that our models learn. Figure 4(a,b) shows the heat map of the
784 for the Toeplitz-like and LDR-SD classes, trained on MNIST-bg-rot with a
weight matrix W
single hidden layer model. As is convention, the input is ﬂattened to a vector in R784. The Toeplitz-like class
is unable to determine that the input is actually a 28
28 image instead of a vector. In contrast, LDR-SD
class is able to pick up regularity in the input, as the weight matrix displays grid-like periodicity of size 28.
Figure 4(c) reveals why the weight matrix displays this pattern. The equivariance interpretation (Section 4)
predicts that B should encode a meaningful transformation of the inputs. The entries of the learned subdiagonal
are in fact recovering a latent invariant of the 2D domain: when visualized as an image, the pixel intensities
correspond to how the inputs are centered in the dataset (Figure 4(d)). Figure 6 in Appendix E shows a
similar ﬁgure for the NORB dataset, which has smaller objects, and we found that the subdiagonal learns a
correspondingly smaller circle.

×

6 Conclusion

We generalize the class of low displacement rank matrices explored in machine learning by considering classes
of LDR matrices with displacement operators that can be learned from data. We show these matrices can
improve performance on downstream tasks compared to compression baselines and, on some tasks, general

9

(a) Toeplitz-like

(b) LDR-SD

(c) Subdiagonal of B

(d) Input examples

Figure 4: The learned weight matrices (a,b) of models trained on MNIST-bg-rot. Unlike the Toeplitz-like matrix,
the LDR-SD matrix displays grid-like periodicity corresponding to the 2D input. Figure (c) shows the values of the
subdiagonal of B, reshaped as an image. The size and location of the circle roughly corresponds to the location of
objects of interest in the 2D inputs. A similar centering phenomenon was found on the NORB dataset, shown in
Figure 6 in Appendix E.

unstructured weight layers. We hope this work inspires additional ways of using structure to achieve both
more compact and higher quality representations, especially for deep learning models, which are commonly
acknowledged to be overparameterized.

Acknowledgments

We thank Taco Cohen, Jared Dunnmon, Braden Hancock, Tatsunori Hashimoto, Fred Sala, Virginia Smith,
James Thomas, Mary Wootters, Paroma Varma, and Jian Zhang for helpful discussions and feedback.

We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore
Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture,
Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, and members
of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, and
VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or
endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.

References

[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the

IEEE International Conference on Computer Vision, pages 37–45. IEEE, 2015.

[2] Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio.
Unsupervised learning of invariant representations. Theor. Comput. Sci., 633(C):112–121, June 2016.
ISSN 0304-3975. doi: 10.1016/j.tcs.2015.06.048. URL https://doi.org/10.1016/j.tcs.2015.06.048.

[3] Martin Anthony and Peter L Bartlett. Neural network learning: theoretical foundations. Cambridge

University Press, 2009.

[4] Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems, pages 190–196, 1999.

[5] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

10

[6] Peter Bürgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity theory, volume

315. Springer Science & Business Media, 2013.

[7] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural
networks with the hashing trick. In Francis Bach and David Blei, editors, Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pages 2285–2294, Lille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/
chenc15.html.

[8] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An
exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the
IEEE International Conference on Computer Vision, pages 2857–2865, 2015.

[9] T.S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publica-

tions, 2011. ISBN 9780486479293. URL https://books.google.com/books?id=IkCJSQAACAAJ.

[10] Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear
time kernel expansions.
In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings
of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pages 2502–2510, New York, New York, USA, 20–22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/choromanski16.html.

[11] Dan C Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber.
Flexible, high performance convolutional neural networks for image classiﬁcation. In IJCAI Proceedings-
International Joint Conference on Artiﬁcial Intelligence, volume 22, page 1237. Barcelona, Spain, 2011.

[12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Conference

on Machine Learning, pages 2990–2999, 2016.

[13] Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hkbd5xZRb.

[14] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 1060–1079. SIAM, 2018.

[15] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep

learning. In Advances in Neural Information Processing Systems, pages 2148–2156, 2013.

[16] Sander Dieleman, Jeﬀrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional
neural networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research,
pages 1889–1898, New York, New York, USA, 20–22 Jun 2016. PMLR. URL http://proceedings.mlr.
press/v48/dieleman16.html.

[17] Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian,
Yu Bai, Geng Yuan, et al. CirCNN: accelerating and compressing deep neural networks using block-
circulant weight matrices. In Proceedings of the 50th Annual IEEE/ACM International Symposium on
Microarchitecture, pages 395–408. ACM, 2017.

[18] Sebastian Egner and Markus Püschel. Automatic generation of fast discrete signal transforms. IEEE

Transactions on Signal Processing, 49(9):1992–2002, 2001.

[19] Sebastian Egner and Markus Püschel. Symmetry-based matrix factorization. Journal of Symbolic

Computation, 37(2):157–186, 2004.

[20] Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information

Processing Systems, pages 2537–2545, 2014.

11

[21] C. Lee Giles and Tom Maxwell. Learning, invariance, and generalization in high-order neural networks.
Appl. Opt., 26(23):4972–4978, Dec 1987. doi: 10.1364/AO.26.004972. URL http://ao.osa.org/
abstract.cfm?URI=ao-26-23-4972.

[22] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic
diﬀerentiation. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, second edition,
2008. ISBN 0898716594, 9780898716597.

[23] Song Han, Jeﬀ Pool, John Tran, and William Dally. Learning both weights and connections for eﬃcient
neural network. In Advances in Neural Information Processing Systems, pages 1135–1143, 2015.

[24] Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise
linear neural networks. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on
Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1064–1068, Amsterdam,
Netherlands, 07–10 Jul 2017. PMLR. URL http://proceedings.mlr.press/v65/harvey17a.html.

[25] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. NIPS Deep

Learning Workshop, 2015.

[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer

networks. In Advances in Neural Information Processing Systems, pages 2017–2025, 2015.

[27] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations.

Journal of Mathematical Analysis and Applications, 68(2):395–407, 1979.

[28] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 2752–2760, 2018.
URL http://proceedings.mlr.press/v80/kondor18a.html.

[29] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s Thesis,

Department of Computer Science, University of Toronto, 2009.

[30] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical
evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th
International Conference on Machine Learning, ICML ’07, pages 473–480, New York, NY, USA, 2007.
ACM. ISBN 978-1-59593-793-3. doi: 10.1145/1273496.1273556. URL http://doi.acm.org/10.1145/
1273496.1273556.

[31] Quoc Le, Tamas Sarlos, and Alexander Smola. Fastfood - computing Hilbert space expansions in loglinear
time. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference
on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 244–252, Atlanta,
Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/le13.html.

[32] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of the IEEE International Conference on Computer
Vision, volume 2, pages II–104. IEEE, 2004.

[33] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance
and equivalence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 991–999, 2015.

[34] Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. Learning compact recurrent neural networks. In
Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages
5960–5964. IEEE, 2016.

[35] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector ﬁeld
networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 5058–5067,
2017.

12

[36] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?
id=Byj72udxe.

[37] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. ACDC: a structured eﬃcient

linear layer. In International Conference on Learning Representations, 2016.

[38] Samet Oymak. Learning compact neural networks with regularization. In Jennifer Dy and Andreas
Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 3966–3975, Stockholmsmässan, Stockholm, Sweden,
10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/oymak18a.html.

[39] Dipan K Pal and Marios Savvides. Non-parametric transformation networks.

arXiv preprint

arXiv:1801.04520, 2018.

Business Media, 2012.

[40] Victor Y Pan. Structured matrices and polynomials: uniﬁed superfast algorithms. Springer Science &

[41] Victor Y Pan and Xinmao Wang. Inversion of displacement operators. SIAM Journal on Matrix Analysis

and Applications, 24(3):660–677, 2003.

[42] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training with high-dimensional output targets. In Proceedings
of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659.
IEEE, 2013.

[43] Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant
descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2050–2057. IEEE, 2012.

[44] Valeria Simoncini. Computational methods for linear matrix equations. SIAM Review, 58(3):377–441,

2016.

[45] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep

learning. In Advances in Neural Information Processing Systems, pages 3088–3096, 2015.

[46] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant
classiﬁers. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on
Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages
1094–1103, Fort Lauderdale, FL, USA, 20–22 Apr 2017. PMLR. URL http://proceedings.mlr.press/
v54/sokolic17a.html.

[47] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

[48] Hugh E Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American

Mathematical Society, 133(1):167–178, 1968.

[49] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang.
Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pages
1476–1483, 2015.

[50] Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, and Bo Yuan. Theoretical properties for neural
networks with weight matrices of low displacement rank. In Doina Precup and Yee Whye Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 4082–4090, International Convention Centre, Sydney, Australia, 06–11
Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/zhao17b.html.

13

A Symbols and abbreviations

Table 4: Symbols and abbreviations used in this paper.

Symbol
LDR

Used For
low displacement rank

LDR-SD matrices with low displacement rank with respect to subdiagonal operators
LDR-TD matrices with low displacement rank with respect to tridiagonal operators
(A, B)
A,B[M]
r
(G, H)
Zf
(A, v) Krylov matrix, with ith column Aiv

displacement operators
Sylvester displacement, AM
(displacement) rank
parameters which deﬁne the rank r residual matrix GHT , where G, H
unit-f-circulant matrix, deﬁned as Zf =

e2, e3, ..., en, f e1

MB

−

∈

∇

Rn

r

×

r with respect to (A, B)

(cid:2)

(cid:3)

K

D

r
A,B
Φ
CC
FC

matrices of displacement rank
feature map
convolutional channels
fully-connected

≤

B Related work

Our study of the potential for structured matrices for compressing deep learning pipelines was motivated by
exciting work along these lines from Sindhwani et al. [45], the ﬁrst to suggest the use of low displacement
rank (LDR) matrices in deep learning. They speciﬁcally explored applications of the Toeplitz-like class, and
empirically show that this class is competitive against many other baselines for compressing neural networks
on image and speech domains. Toeplitz-like matrices were similarly found to be eﬀective at compressing RNN
and LSTM architectures on a voice search task [34]. Another special case of LDR matrices are the circulant
(or block-circulant) matrices, which have also been used for compressing CNNs [8]; more recently, these have
also been further developed and shown to achieve state-of-the-art results on FPGA and ASIC platforms [17].
Earlier works on compressing deep learning pipelines investigated the use of low-rank matrices [15, 42]—
perhaps the most canonical type of dense structured matrix—which are also encompassed by our framework,
as shown in Proposition 2. Outside of deep learning, Choromanski and Sindhwani [10] examined a structured
matrix class that includes Toeplitz-like, circulant, and Hankel matrices (which are all LDR matrices) in the
context of kernel approximation.

On the theoretical side, Zhao et al. [50] study properties of neural networks with LDR weight matrices,
proving results including a universal approximation property and error bounds. However, they retain
the standard paradigm of ﬁxing the displacement operators and varying the low-rank portion. Another
natural theoretical question that arises with these models is whether the resulting hypothesis class is still
eﬃciently learnable, especially when learning the structured class (as opposed to these previous ﬁxed classes).
Recently, Oymak [38] proved a Rademacher complexity bound for one layer neural networks with low-rank
weight matrices. To the best of our knowledge, Theorem 2 provides the ﬁrst sample complexity bounds for
neural networks with a broad class of structured weight matrices including low-rank, our LDR classes, and
other general structured matrices [14].

In Section 3 we suggest that the LDR representation enforces a natural notion of approximate equivariance
and satisﬁes closure properties that one would expect of equivariant representations. The study of equivariant
feature maps is of broad interest for constructing more eﬀective representations when known symmetries exist
in underlying data. Equivariant linear maps have long been used in algebraic signal processing to derive eﬃcient
transform algorithms [18, 19]. The fact that convolutional networks induce equivariant representations, and
the importance of this eﬀect on sample complexity and generalization, has been well-analyzed [2, 12, 21, 46].
Building upon the observation that convolutional ﬁlters are simply linear maps constructed to be translation

14

equivariant7, exciting recent progress has been made on crafting representations invariant to more complex
symmetries such as the spherical rotation group [13] and egomotions [1]. Generally, however, underlying
assumptions are made about the domain and invariances present in order to construct feature maps for
each application. A few works have explored the possibility of learning invariances automatically from
data, and design deep architectures that are in principle capable of modeling and learning more general
symmetries [20, 26, 39].

C Properties of displacement rank

Displacement rank has traditionally been used to describe the Toeplitz-like, Hankel-like, Vandermonde-like,
and Cauchy-like matrices, which are ubiquitous in disciplines such as engineering, coding theory, and computer
algebra. Their associated displacement representations are shown in Table 5.

Table 5: Traditional classes of structured matrices analyzed with displacement rank. In the Vandermonde and Cauchy
cases, the displacement operators are parameterized by v ∈ Rn and s, t ∈ Rn respectively.

B
Structured Matrix M A
Z
Z1
Toeplitz
−
ZT
Z1
Hankel
0
diag(v) Z0
Vandermonde
Cauchy
diag(s)

1

diag(t)

2
2
1
1

≤
≤
≤
≤

Displacement Rank r

Proof of Proposition 1. The following identities are easily veriﬁed:

Transpose

Inverse

Sum

Product

Block The remainder

is the block matrix

∇BT ,AT MT =

(

−

∇

A,BM)T

B,AM−

1 =

M−

1 (

1
A,BM) M−

−

∇

A,B(M + N) =

A,BM +

A,BN

∇

∇

A,CMN = (

A,BM)N + M (

B,CN)

∇

∇

diag(A1, . . . , Ak)M

M diag(B1, . . . , B(cid:96))

∇

∇

∇

−

(cid:96).
This is the sum of k(cid:96) matrices of rank r and thus has rank rk(cid:96).

Ai,Bj Mij)1

(
∇

j
≤

i
≤

k,1

≤

≤

Corollary 1. A k
Toeplitz-like with displacement rank rk(cid:96) + 2k + 2(cid:96).

×

(cid:96) block matrix M, where each block is a Toeplitz-like matrix of displacement rank r, is

Proof. Apply Proposition (d) where each Ak, Bk has the form Zf . Let A = diag(A1, . . . , Ak) and B =
diag(B1, . . . , B(cid:96)). Note that A and Z1 (of the same size as A) diﬀer only in 2k entries, and similarly B and
Z

1 diﬀer in 2(cid:96) entries. Since an s-sparse matrix also has rank at most s,

−

−
has rank at most rk(cid:96) + 2k + 2(cid:96).

Z1M

MZ

1 = AM

−

MB + (Z1 −

−

A)M

M(Z

−

B)

1 −
−

7Shifting the input to a convolutional feature map is the same as shifting the output.

15

Proof of Proposition 3. First consider the rank one case, R = ghT . It is easy to check that
will only be non-empty in the ﬁrst column, hence
Proposition 1(a) implies
K
The rank r case follows directly from Theorem 1(b).

1
ZT ,B. Then Theorem 1(c) implies that

1
A−1,ZT . Similarly,

K
(A, g)

(BT , h)T

(A, g)

∈ D

∈ D

K

K

K

(BT , h)

K

(A, g)
∇A−1,ZT
1
BT ,Z and
∈ D
2
(B, h)T
A,B.

∈ D

C.1 Expressiveness

Expanding on the claim in Section 3, we formally show that these structured matrices are contained in the
tridiagonal (plus corners) LDR class. This includes several types previously used in similar works.

A storage Av compute

LDR-TD

O(nr) O(nr log2 n)

Toeplitz-like

O(nr) O(nr log n)

Circulant

O(n) O(n log n)

Convolutional ﬁlters

O(n) O(n log n)

Low-rank

O(nr) O(nr)

Orthogonal polynomial
transforms
O(n) O(n log2 n)

Figure 5: Our proposed LDR-TD structured matrix class contains a number of other classes including Toeplitz-
like [45] (and other classic displacement types, such as Hankel-like, Vandermonde-like, and Cauchy-like), low-rank [15],
circulant [8], standard convolutional ﬁlters, and orthogonal polynomial transforms, including the Discrete Fourier and
Cosine Transforms. Captions for each class show storage cost and operation count for matrix-vector multiplication.

Classic displacement rank The Toeplitz-like, Hankel-like, Vandermonde-like, and Cauchy-like matrices
where D is the set of diagonal matrices [40].
are deﬁned as having LDR with respect to A, B
(For example, [45] deﬁnes the Toeplitz-like matrices as (A, B) = (Z1, Z
1).) All of these operator choices are
only non-zero along the three main diagonals or opposite corners, and hence these classic displacement types
belong to the LDR-TD class.

Zf , ZT

f , D

∈ {

}

−

Low-rank A rank r matrix R trivially has displacement rank r with respect to (A, B) = (I, 0). It also has
displacement rank r with respect to (A, B) = (Z1, 0), since Z1 is full rank (it is a permutation matrix) and so
rank(Z1R) = rank(R) = r. Thus low-rank matrices are contained in both the LDR-TD and LDR-SD classes.

Orthogonal polynomial transforms The polynomial transform matrix M with respect to polynomials
1) is deﬁned by Mij = pi(λj). When the pi(X) are a family of
(p0(X), . . . , pm

1(X)) and nodes (λ0, . . . , λn

−

−

16

orthogonal polynomials, it is called an orthogonal polynomial transform.

Proposition 4. Orthogonal polynomial transforms have displacement rank 1 with respect to tridiagonal
operators.

Proof. Every orthogonal polynomial family satisﬁes a three-term recurrence

pi+1(X) = (aiX + bi)pi(X) + cipi

1(X)

−

(4)

where ai > 0 [9]. Let M be an orthogonal polynomial transform with respect to polynomials (pi(X))0
and nodes (λj)0

j<n. Deﬁne the tridiagonal and diagonal matrix

≤

i<m

≤

0
0
0
...

A =

0
1
a1
b1
a1

b0
a0
−
c1
a1 −
−
0
−
...
0

1
a0
b1
a1
c1
a1 −
...
0

0

0

. . .
. . .
. . .
. . .
. . .

. . .

...
0

0

−














B = diag(λ0, . . . , λn

1).

0
0
0
...














bm−2
am−2
cm−1
am−1 −

1
am−2
bm−1
am−1

−

−

For any i

0, . . . , m

and any j, consider entry ij of AM

MB. This is

∈ {

2
}

−

1
ai

[

−

cipi

1(λj)

bipi(λj) + pi+1(λj)

λjpi(λj)]

−

−

which is 0 by plugging λj into (4).

Thus

A,BM can only non-zero in the last row, so M

−

−

1
A,B.

∈ D

Fourier-like transforms Orthogonal polynomial transforms include many special cases. We single out
the Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT) for their ubiquity.

The N

N DFT and DCT (type II) are deﬁned as matrix multiplication by the matrices

∇

×

F =

2π ij
N

e−

C =

cos

(cid:16)

(cid:16)

π
N

(cid:104)

ij

(cid:17)
i(j + 1/2)

ij

(cid:105)(cid:17)

Ti

cos

(j +

π
N

1
2

,

)
(cid:21)(cid:19)(cid:19)ij

Tn(X) = cos(n arccos x).

respectively.

The former is a special type of Vandermonde matrix, which were already shown to be in LDR-TD. Also
j)ij are themselves orthogonal polynomial transforms with pi(X) = X i.

note that Vandermonde matrices (λi
The latter can be written as

(cid:18)
where Ti are the Chebyshev polynomials (of the ﬁrst kind) deﬁned such that

(cid:18)

(cid:20)

Thus this is an orthogonal polynomial transform with respect to the Chebyshev polynomials.

Other constructions From these basic building blocks, interesting constructions belonging to LDR-TD can
be found via the closure properties. For example, several types of structured layers inspired by convolutions,
including Toeplitz [45], circulant [8] and block-circulant [17] matrices, are special instances of Toeplitz-like
matrices. We also point out a more sophisticated layer [37] in the tridiagonal LDR class, which requires more
deliberate use of Proposition 1 to show.

17

Proposition 5. The ACDC−
Transform [37], has displacement rank 2 with respect to tridiagonal operators.

1 layer, where A, D are diagonal matrices and C is the Discrete Cosine

Proof. Let T, Λ be the tridiagonal and diagonal matrix such that C
also tridiagonal. Note that A
application of the inverse closure rule yields C

∈ D
0
S,T by construction. Also note that D

∈ D

1
T,Λ. Deﬁne S = ATA−

1, which is
0
Λ,Λ since Λ is diagonal. An

∈ D
1
Λ,T. Finally, the product closure property implies that

∈ D

ACDC−

1

2
S,T.

∈ D

C.2 Algorithm derivation and details

De Sa et al. recently showed that a very general class of LDR matrices have asymptotically fast matrix-vector
multiplication algorithms [14]. However, parts of the argument are left to existential results. Building
upon De Sa et al. [14], we derive a simpliﬁed and self-contained algorithm for multiplication by LDR matrices
with subdiagonal operators.

Since these matrices can be represented by the Krylov product formula (2), it suﬃces to show multiplication

algorithms separately for matrix-vector multiplication by

(A, v)T and

(A, v).

K

K

Krylov transpose multiplication Let A
possible non-zero entries. Let u, v
n is a power of 2.

∈

Following [14], the vector

Rn, we wish to compute the product

n be a subdiagonal matrix, i.e. Ai+1,i are the only
(A, v)T u. For simplicity assume

∈

×

Rn

is the coeﬃcient vector of the polynomial in X

(cid:2)

uT

K

(A, v) =

uv uAv . . . uAn

1v

−

K

(cid:3)

uv + uAvX +

+ uAn

1vX n

−

−

1

∞

=

i=0
(cid:88)
= u(I

· · ·

uAiX iv

AX)−

1v,

−

where we use the observation that An = 0.

By partitioning A into n/2

n/2 blocks, it has the form

×

matrices of half the size, a is a scalar, and ei are basis vectors. Let also u0, u1 ∈
the ﬁrst and second halves of u, v.

A0
ae1eT
(cid:20)

0
n/2 A1

, where A0, A1 are subdiagonal
(cid:21)

Rn/2 denote

Rn/2, v0, v1 ∈

By block matrix inversion for triangular matrices

, this can be written

A 0
C B

1

−

=

(cid:20)

(cid:21)

(cid:20)

−

1
A−
1CA−

0
1 B−

1

B−

(cid:21)

as

uT (I

AX)−

1v =

0 uT
uT
1

−

= uT

0 (I

A0X)−

(cid:2)
1v0 + uT

(cid:3)
1 (I

A1X)−

1
n/2X)(I

(I
1(

A0X)−
−
ae1eT
−
1v1 + aX

uT

1 (I

−
A1X)−

−

(I

(cid:20)

−
−
A1X)−

−

A0X)−

1

(I

0
1
A1X)−

1e1

eT
n/2(I

A0X)−

−

−

Therefore uT (I

AX)−

1v can be computed from

(cid:0)

(cid:1) (cid:16)

v0
v1(cid:21)

(cid:17)

(cid:21) (cid:20)
1v0

−

−

uT
0 (I
uT
1 (I

−

−

A0X)−
A1X)−

1v0
1e1

uT
1 (I
−
eT
n/2(I

−

A1X)−

A0X)−

1v1
1v0

with an additional polynomial multiplication and 3 polynomial addition/subtractions.

18

A modiﬁcation of this reduction shows that the 2

2 matrix of polynomials

×

can be computed from

u0 en

T

(I

1
A0X)−

v0 e1

−

u en

T

(I

AX)−

1

v e1

−

(cid:2)

(cid:3)

u1 en

T

(I

A1X)−

1

v1 e1

(cid:2)

−

(cid:3)

(cid:2)

with an additional constant number of polynomial multiplications and additions.

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:3)

The complete recursive algorithm is provided in Algorithm 1, where subroutine R computes the above

matrix of polynomials. For convenience, Algorithm 1 uses Python indexing notation.

∈

n, u, v

×

Rn)

∈

←

s
return R(s, u, v)

Algorithm 1 Krylov Transpose (Recursive)
1: function Krylov_Transpose(A
Rn
subdiagonal(A)
2:
3:
4: end function
5: function R(s
S0 ←
6:
S1 ←
7:
L
←

Rn
−
∈
R(s[0 : n/2
R(s[n/2 : n

1, u, v)

s[n/2

−

8:

−
−

1], u[0 : n/2], v[0 : n/2])
1], u[n/2 : n], v[n/2 : n])
S0[1, 0] S1[0, 1]
S1[0, 1]
S0[1, 0] S1[1, 1]
S1[1, 1]
(cid:20)

S0[1, 1]
S0[1, 1]
L[0, 0] + S0[0, 0] + S1[0, 0] L[0, 1] + S0[0, 1]

1]X

·
·

·
·

·

L[1, 0] + S1[1, 0]

L[1, 1]

9:

return

10: end function

(cid:20)

(cid:21)

(cid:21)

A polynomial multiplication of degree m in Step 8 can be computed as a convolution of size 2m. This
reduces to two Fast Fourier Transform (FFT) calls, an elementwise multiplication in the frequency domain,
and an inverse FFT. The total number of calls can be further reduced to 4 FFTs and 4 inverse FFTs.

Algorithm 1 deﬁnes a recursion tree, and in practice we compute this breadth ﬁrst bottom-up to avoid
recursive overhead. This also allows the FFT operations to be batched and computed in parallel. Thus the
d-th layer of the algorithm (starting from the leaves) performs n

2d FFT computations of size 2d+1.

This completes the proof of Theorem 1.
We note several optimizations that are useful for implementation:

n (I

1. The polynomial eT

1e1 for i = 0, 1 are in fact monomials, which can be shown inductively. To
use the notation of Algorithm 1, S0[1, 1], S1[1, 1], and L[1, 1] are monomials. Therefore the polynomial
multiplication with S0[1, 1] and S1[1, 1] can be done directly by coeﬃcient-wise multiplication instead
of using the FFT.

AiX)−

−

0 (I

A0X)−

2. We don’t need the polynomials uT

1v1 separately, we only need their
1v0 and uT
sum. To use the notation of Algorithm 1, we don’t need S0[0, 0] and S1[0, 0] separately, we only need
their sum. In fact, by tracing the algorithm from the leaves of the recursion tree to the root, we see
that across the same depth d, only the sum of the terms S0[0, 0] + S1[0, 0] of the n/2d subproblems
is required, not the individual terms. Therefore, when computing polynomial multiplication at depth
d, we can perform the FFT of size 2d+1 and the pointwise multiplication, then sum across the n/2d
problems before performing the inverse FFT of size 2d+1.

A1X)−

1 (I

−

−

×

b and there are r vectors v1, . . . , vr, and we wish to compute

Eﬃcient batching with respect to input vector and rank. Optimization 2 is especially important
for eﬃcient multiplication with respect to batched input u and higher rank v. Suppose that u has size
(A, vi)T u. Naively performing
n
Algorithm 1 on each of the b inputs and each of the r vectors then summing the results, takes O(brn log2 n)
time. The bottleneck of the algorithm is the polynomial multiplication S1[0, 1]
S0[1, 0]. At depth d, there are
n/2d subproblems, and in each of those, S1[0, 1] consists of b polynomials of degree at most 2d, while S0[1, 0]
consists of r polynomials of degree at most 2d. If we apply optimization 2, we ﬁrst perform the FFT of size
2d+1 on these (b + r)n/2d polynomials, then pointwise multiplication in the frequency domain to get brn/2d
vectors of size 2d+1 each. Next we sum across the n/2d problems to get br vectors, before performing the

r
i=1 K

(cid:80)

·

19

inverse FFT of size 2d+1 to these br vectors. The summing step allows us to reduce the number of inverse
FFTs from brn/2d to br. The total running time over all depth d is then O((b + r)n log2 n + brn log n) instead
of O(brn log2 n).

Krylov multiplication De Sa et al. [14] do not provide explicit algorithms for the more complicated problem
of multiplication by
(A, v), instead justifying the existence of such an algorithm with the transposition
principle. Traditional proofs of the transposition principle use circuit based arguments involving reversing
arrows in the arithmetic circuit deﬁning the algorithm’s computation graph [6].

K

Here we show an alternative simple way to implement the transpose algorithm using any automatic
diﬀerentiation (AD) implementation, which all modern deep learning frameworks include. AD states that for
any computation, its derivative can be computed with only a constant factor more operations [22].

Proposition 6 (Transposition Principle). If the matrix M
any vector in N operations, then MT admits matrix-vector multiplication in O(N + n) operations.

n admits matrix-vector multiplication by

∈

×

Rn

Proof. Note that for any x and y, the scalar yT Mx = y

(Mx) can be computed in N + n operations.

The statement follows from applying reverse-mode AD to compute MT y = ∂
Additionally, the algorithm can be optimized by choosing x = 0 to construct the forward graph.

∂x (yT Mx).

·

To perform the optimization mentioned in Proposition 6, and avoid needing second-order derivatives when
computing backprop for gradient descent, we provide an explicit implementation of non-transpose Krylov
multiplication

(A, v). This was found by using Proposition 6 to hand-diﬀerentiate Algorithm 1.

Finally, we comment on multiplication by the LDR-TD class. Desa et al.[14] showed that these matrices
also have asymptotically eﬃcient multiplication algorithms, of the order O(rn log3 n) operations. However,
these algorithms are even more complicated and involve operations such as inverting matrices of polynomials
in a modulus. Practical algorithms for this class similar to the one we provide for LDR-SD matrices require
more work to derive.

K

C.3 Displacement rank and equivariance

Here we discuss in more detail the connection between LDR and equivariance. One line of work [12, 28] has
used the group representation theory formalization of equivariant maps, in which the model is equivariant
to a set of transformations which form a group G. Each transformation g
G acts on an input x via a
corresponding linear map Tg. For example, elements of the rotation group in two and three dimensions,
SO(2) and SO(3), can be represented by 2D and 3D rotation matrices respectively. Formally, a feature map
Φ is equivariant if it satisﬁes

∈

Φ(Tgx) = T (cid:48)g(Φ(x))

(5)

for representations T, T (cid:48) of G [12, 28]. This means that perturbing the input x by a transformation g
G
before computing the map Φ is equivalent to ﬁrst ﬁnding the features Φ and then applying the transformation.
Group equivariant convolutional neural networks (G-CNNs) are a particular realization where Φ has a speciﬁc
Rd, and T, T (cid:48) are chosen in advance [12]. We use the notation Φ to distinguish our setting, where
form G
the input x is ﬁnite dimensional and Φ is linear.

→

∈

Proposition 7. If Φ has displacement rank 0 with respect to invertible A, B, then Φ is equivariant as deﬁned
by (5).

Proof. Note that if AΦ = ΦB for invertible matrices A, B (i.e. if a matrix Φ has displacement rank 0 with
Z. Also note that the set of powers of any invertible
respect to A and B), then AiΦ = ΦBi also holds for i
∈
matrix forms a cyclic group, where the group operation is multiplication. The statement follows directly from
this fact, where the group G is Z, and the representations T and T (cid:48) of G correspond to the cyclic groups
generated by A and B, respectively consisting of Ai and Bi for all i

Z.

∈

More generally, a feature map Φ satisfying (5) for a set of generators S =

is equivariant with respect
to the free group generated by S. Proposition 7 follows from the speciﬁc case of a single generator, i.e.
S =

gi}
{

.

1
}
{

20

D Bound on VC dimension and sample complexity

In this section we upper bound the VC dimension of a neural network where all the weight matrices are
LDR matrices and the activation functions are piecewise polynomials. In particular, the VC dimension is
almost linear in the number of parameters, which is much smaller than the VC dimension of a network with
unstructured layers. The bound on the VC dimension allows us to bound the sample complexity to learn
an LDR network that performs well among LDR networks. This formalizes the intuition that compressed
parameterization reduces the complexity of the class.

Neural network model Consider a neural network architecture with W parameters, arranged in L layers.
Each layer l, has output dimension nl, where n0 is the dimension of the input data and the output dimension
Rnl be the input to the l-th layer. The input to the (l + 1)-th layer is
is nL = 1. For l = 1, . . . , L, let il ∈
exactly the output of the l-th layer. The activation functions φl are piecewise polynomials with at most p + 1
Rn1, and the output of the
pieces and degree at most k
1. The input to the ﬁrst layer is the data i1 = x
last layer is a real number iL+1 ∈
il+1 = φl(Mlil + bl)

R. The intermediate layer computation has the form:

(applied elementwise),

Rnl−1

Rnl .

≥

∈

×

where Ml ∈

nl , bl ∈

We assume the activation function of the ﬁnal layer is the identity.

Each weight matrix Ml is deﬁned through some set of parameters; for example, traditional unconstrained
matrices are parametrized by their entries, and our formulation (2) is parametrized by the entries of some
operator matrices Al, Bl and low-rank matrix GlHT
l . We collectively refer to all the parameters of the neural
network (including the biases bl) as θ

RW , where W is the number of parameters.

∈

Rn

Bounding the polynomial degree The crux of the proof of the VC dimension bound is that the entries
m are polynomials in terms of the entries of its parameters (A, B, G, and H). of total degree at
of M
most c1mc2 for universal constants c1, c2. This allows us to bound the total degree of all of the layers and
apply Warren’s lemma to bound the VC dimension.

∈

×

We will ﬁrst show this for the speciﬁc class of matrices that we use, where the matrix M is deﬁned through

equation (2).

Lemma 1. Suppose that M

Rm

×

m is deﬁned as

∈

M =

(A, gi)

(BT , hi).

K

r

K

i=1
(cid:88)

Then the entries of M are polynomials of the entries of A, B, G, H with total degree at most 2m.

K

gi Agi

. . . Am

(A, gi) =

1gi
, and each entry of Ak is a polynomial of the entries of A
Proof. Since
−
(A, gi) are polynomials of the entries of A and gi with total
with total degree at most k, the entries of
(cid:3)
K
(BT , hi) are polynomials of the entries of B and hi with total
degree at most m. Similarly the entries of
K
(BT , hi) are polynomials of the entries of A, B, G, H with
degree at most m. Hence the entries of
(A, gi)
total degree at most 2m. We then conclude that the entries of M are polynomials of the entries of A, B, G, H
with total degree at most 2m.

K

K

(cid:2)

Lemma 2. Suppose that the LDR weight matrices Ml of a neural network have entries that are polynomials
in their parameters with total degree at most c1nc2
0. For a ﬁxed data
l
−
point x, at the l-th layer of a neural network with LDR weight matrices, each entry of Mlil + bl is a piecewise
polynomial of the network parameters θ, with total degree at most dl, where

1 for some universal constants c1, c2 ≥

Thus entries of the output φl(Mlil + bl) are piecewise polynomials of θ with total degree at most kdl. Moreover,

d0 = 0,

dl = kdl

1 + c1nc2
l
−

1

−

for l = 1, . . . , L.

c1kl

−

1

dl ≤

nc2
j .

1

l

−

j=0
(cid:88)

21

(6)

By Lemma 1, Lemma 2 applies to the speciﬁc class of matrices that we use, for c1 = 2 and c2 = 1. As we

will see, it also applies to very general classes of structured matrices.

Proof. We induct on l. For l = 1, since i1 = x is ﬁxed, the entries of M1 are polynomials of θ of degree
at most c1nc2
0 , and so the entries of M1i1 + b1 are polynomials of θ with total degree at most d1 = c1nc2
0 .
As φ is a piecewise polynomials of degree at most k, each entry the output φ1(M1i1 + b1) is a piecewise
polynomial of θ with total degree at most 2n0k. The bound (6) holds trivially.

Suppose that the lemma is true for some l

θ with total degree at most kdl
the entries of Mlil + bl are piecewise polynomials of θ with total degree at most dl = kdl
φl(Mlil + bl) have entries that are piecewise polynomials of θ with total degree at most kdl.

1. Since the entries of il are piecewise polynomials of
1 and entries of Ml are polynomials of θ with total degree at most c1nc2
1,
l
−
1 + c1nc2
−
1. Thus
l
−

−

≥

1

−

We can bound

dl = kdl

1 + c1nc2
l
−

−

1 ≤

kc1kl

−

2

j + c1nc2
nc2

l

c1kl

−

1

nc2
j ,

1 ≤

−

1

l

−

j=0
(cid:88)

2

l

−

j=0
(cid:88)
c1kl

where we have used the fact that k

1, so c1nc2
l
−

1 ≤

≥

−

1nc2
l
−

1. This concludes the proof.

Bounding the VC dimension Now we are ready to bound the VC dimension of the neural network.

Theorem 3. For input x
F
. Let Wl be
be the class of functions
}
the number of parameters up to layer l (i.e., the total number of parameters in layer 1, 2, . . . , l). Deﬁne the
eﬀective depth as

RW , let f (x, θ) denote the output of the network. Let

and parameter θ
RW
f (x, θ) : θ

∈
. Denote sign
}

sign f (x, θ) : θ

∈ X
x
→

x
{

RW

:=

→

F

∈

∈

{

¯L :=

1
W

L

(cid:88)l=1

Wl,

U :=

nl.

L

(cid:88)l=0

and the total number of computation units (including the input dimension) as

Then

In particular, if k = 1 (corresponding to piecewise linear networks) then

VCdim(sign

) = O( ¯LW log(pU ) + ¯LLW log k).

VCdim(sign

) = O( ¯LW log(pU )) = O(LW log W ).

F

F

We adapt the proof of the upper bound from Bartlett et al. [4], Harvey et al. [24]. The main technical
tool is Warren’s lemma [48], which bounds the growth function of a set of polynomials. We state a slightly
improved form here from Anthony and Bartlett [3, Theorem 8.3].

Lemma 3. Let p1, . . . , pm be polynomials of degree at most d in n

m variables. Deﬁne

≤
(sign(p1(x)), . . . , sign(pm(x)) : x

K :=

|{

Rn

,
}|

∈

i.e., K is the number of possible sign vectors given by the polynomials. Then K

2(2emd/n)n.

Proof of Theorem 3. Fixed some large integer m and some inputs x1, . . . , xm. We want to bound the number
of sign patterns that the neural network can output for the set of input x1, . . . , xm:

≤

RW

∈

.

}

(cid:12)
(cid:12)

K :=

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

{

(cid:12)
(cid:12)

22

We want to partition the parameter space RW so that for a ﬁxed xj, the output f (xj, θ) is a polynomial
on each region in the partition. Then we can apply Warren’s lemma to bound the number of sign patterns.
Indeed, for any partition

of the parameter space RW , we have

=

P1, . . . , PN }
{

S

K

≤

N

|{

j=1
(cid:88)

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

.

Pj}|

∈

(7)

We construct the partitions iteratively layer by layer, through a sequence

S0,

S1, . . . ,

SL

−

1 of successive

reﬁnements, satisfying two properties:

1.

|S0|

= 1 and for each 1

l

L

1,

≤

≤

−

|Sl| ≤ |Sl

1|
−

2

2empnldl
Wl

Wl

,

(cid:18)
where nl is the dimension of the output of the l-th layer, dl is the bound on the total degree of Mlil + bl
as piecewise polynomials of θ as deﬁned in Lemma 2, and Wl is the number of parameters up to layer l
(i.e., the total number of parameters in layer 1, 2, . . . , l).

(cid:19)

2. For each l = 0, . . . , L

Sl, for each ﬁxed data point xj (with j = 1, . . . , m),
the entries of the output φl(Mlil + bl) when restricted to S are polynomials of θ with total degree at
most kdl

1, for each element S of

1.

−

−

We can deﬁne

are polynomials of θ of degree d0 = 0.
Suppose that we have constructed
∈ Sl

S0 = RW , which satisﬁes property 2, since at layer 1, the entries of i1 = xj (for ﬁxed xj)
[m], and
1, and we want to deﬁne
S0, . . . ,
1, let ph,xj ,S(θ) = (Mlil + bl)h|S be the h-th entry of Mlil + bl restricted to the region S. By the
1, the entries of il when restricted to S are polynomials of θ of total
1. Thus by Lemma 2, the entries of Mlil + bl when restricted to S are polynomials of θ

Sl. For any h

S
inductive hypothesis, for each S
degree at most kdl
with total degree at most kdl

1 = dl, and depends on at most Wl many variables.

[nl], j

Sl

∈

∈

−

−

−

∈ Sl
−
1 + c1nc2
l
−

−

Since the activation function is piecewise polynomial with at most p pieces, let

t1, . . . , tp}

{

be the set of

breakpoints. For any ﬁxed S

1, by Lemma 3, the polynomials

can have at most

∈ Sl

−

(cid:8)

ph,xj ,S(θ)

ti : h

[nl], j

[m], i

[p]

−

∈

∈

∈

(cid:9)

Π := 2

2e(nlmp)dl
Wl

(cid:18)

Wl

(cid:19)

∈

Fix some S(cid:48)

RW . We can then partition RW into this many regions so that within each
distinct sign patterns when θ
region, all these polynomials have the same signs. Intersecting all these regions with S yields a partition of S
Sl that satisﬁes the property 1.
into at most Π subregions. Applying this for all S
∈ Sl
∈ Sn. When θ is restricted to S(cid:48), by construction, all the polynomials
ti : h

∈
have the same sign. This means that the entries of Mlil + bl lie between two breakpoints of the activation
function, and so the entries of the output φl(Mlil + bl) are ﬁxed polynomials in Wl variables of degree at
most kdl.

1 gives a partition

By this recursive construction,
−
any input xj is a ﬁxed polynomial of θ
the activation function of the ﬁnal layer is the identity). Hence we can apply Lemma 3 again:

1 is a partition of RW such that for S
1 the network output for
∈ SL
1 + c1nc2
1 = dL (recall that we assume
L
−
−

S of degree at most kdL

ph,xj ,S(θ)

[nl], j

[m], i

SL

[p]

−

∈

∈

∈

(cid:8)

(cid:9)

−

−

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

S

|{

∈

}| ≤

2emkdL

WL (cid:19)

2

(cid:18)

WL

.

23

By property 1, we can bound the size of

1:

SL

−

|SL| ≤

L

1

−

2

(cid:89)l=1

(cid:18)

2emnlpdl
Wl

Wl

.

(cid:19)

L

2

2empnldl
Wl

Wl

.

K

≤

Combining the two bounds along with equation (7) yields

(cid:19)
We can take logarithm and apply Jensen’s inequality, with ¯W :=

(cid:89)l=1

(cid:18)

L
l=1 Wl:

(cid:80)

log2 K

L +

Wl log2

2empnldl
Wl

L

≤

(cid:88)l=1
= L + ¯W

L

(cid:88)l=1
L + ¯W log2

≤

= L + ¯W log2

Wl
¯W

log2

2empnldl
Wl

L

(cid:32)

(cid:88)l=1
2emp

Wl
¯W

2empnldl
Wl

(cid:33)

L
l=1 nldl

.

¯W
(cid:80)

(Jensen’s inequality)

We can bound

nldl using the bound on dl from Lemma 2:

(cid:80)

L

L

nldl ≤

(cid:88)l=1

(cid:88)l=1
U . Thus

≤

1

l

−

j=0
(cid:88)

nc2
j ≤

where we used the fact that L

nlc1kl

−

1

LU c1kL
−

1U c2

c1U c2+2kL,

≤

log2 K

≤

L + ¯W log2

2c1empU 2+c2 kL
¯W

.

To bound the VC-dimension, recall that by deﬁnition, if VCdim(sign

) = m then exists m data points
x1, . . . , xm such that the output of the model can have 2n sign patterns. The bound on log2 K then implies

F

VCdim(sign

)

F

≤

L + ¯W log2

We then use Lemma 4 below, noting that 2c1epU 2+c2kL

2c1epU 2+c2kLVCdim(sign
¯W
16, to conclude that

)

.

F

≥

L + ¯W log2(2c1epU 2+c2 kL log2(2c1epU 2+c2kL)) = O( ¯LW log(pU ) + ¯LLW log k),

VCdim(sign

)

≤
completing the proof.

F

A bound on the VC dimension immediate yields a bound on the sample complexity of learning from this

class of neural networks with LDR matrices [47].

Corollary 2. The class of neural network with LDR matrices as weights and piecewise linear activation is
((cid:15), δ)-PAC-learnable with a sample of size

Since the number of parameters W is around the square root of the number of parameters of a network
with unstructured layers (assuming ﬁxed rank of the LDR matrices), the sample complexity of LDR networks
is much smaller than that of general unstructured networks.

Lemma 4 (Lemma 16 of [24]). Suppose that 2m
m

t + w log2(2r log2 r).

≤

≤

2t(mr/w)w for some r

16 and m

≥

w

t

≥

≥

≥

0. Then,

LW log W + log 1
δ
(cid:15)

.

(cid:19)

O

(cid:18)

24

F

Extension to rational functions. We now show that Theorem 3 holds for matrices where the entries are
rational functions—rather than polynomials—of its parameters, incurring only a constant in the bound. To
deﬁne the function class sign

, we account for the possibility of poles by deﬁning sign(a/0) = 0.

We only need to check that Lemma 2 and Lemma 3 still hold when polynomials are replaced by rational
.
functions everywhere, and the degree of a rational function is deﬁned as the usual deg(a/b) = max
deg a, deg b
}
{
To show Lemma 2 still holds, it suﬃces that the compositional degree bound deg(f
deg(f ) deg(g) holds
for rational functions f, g, just as in the polynomial case. To show Lemma 3 in the case when pi = ai/bi
are rational functions, we note that sign(pi(x)) = sign(ai(x)bi(x)), and furthermore deg(aibi)
2 deg(pi).
Appealing to the polynomial version of Lemma 3 shows that it holds in the rational function setting with
2(4emd/n)n. This gets converted to a constant factor in the result of
a slightly weaker upper bound K
Theorem 3.

g)

≤

≤

≤

◦

Next, we extend Lemma 1 by showing that generic LDR matrices have entries which are rational functions
of their parameters. This immediately lets us conclude that neural networks built from any LDR matrices
satisfy the VC dimension bounds of Theorem 3.

Lemma 5. If M
entries of A, B, G, H with total degree at most c1mc2 for some universal constants c1, c2 > 0.

MB = GHT , then the entries of M are rational functions of the

m satisﬁes AM

−

∈

×

Rm

Proof. The vectorization of the Sylvester equation AM
⊗
where vec denotes the vectorization operation by stacking a matrix’s columns, and
product. Note that the entries of N−
n in the entries of N, and R = GH(cid:62) has degree 2 in the entries of G, H. Therefore the entries of

I) vec(M) = vec(R),
is the Kronecker
⊗
n are rational functions with degree

1 for an arbitrary matrix N

MB = R is (I

B(cid:62)

Rn

A

−

⊗

−

∈

×

have degree n2 + 2 in the entries of A, B, G, H.

vec(M) = (I

A

⊗

−

B(cid:62)

⊗

I)−

1 vec(R)

Note that many other classes of matrices satisfy this lemma. For example, a large class of matrices
satisfying a property called low recurrence width was recently introduced as a way of generalizing many known
structured matrices [14]. The low recurrence width matrices are explicitly deﬁned through a polynomial
recurrence and satisfy the bounded degree condition. Additionally, Lemma 5 holds when the parameters
A, B themselves are structured matrices with entries having polynomial degree in terms of some parameters.
This includes the case when they are quasiseparable matrices, the most general class of LDR previously
analyzed [14].

E Additional results

E.1 Additional baselines and comparisons at multiple budgets

In Tables 6 and 7 we compare to baselines at parameter budgets corresponding to both the LDR-TD and
LDR-SD classes in the SHL and CNN models. In Tables 8 and 9, we also compare to two additional baselines,
network pruning [23] and a baseline used in [7], in which the number of hidden units is reduced to meet
the parameter budget. We refer to this baseline as RHU ("reduced hidden units"). We show consistent
improvements of LDR-SD over both methods at several budgets. We note that unlike the structured matrix
methods which provide compression beneﬁts during both training and inference, pruning requires ﬁrst training
the original model, followed by retraining with a ﬁxed sparsity pattern.

E.2 Sample complexity and generalization

As shown in Tables 10 and 11, we investigated how the performance of the structured and general unstructured
fully-connected layers varied with the amount of training data. On the MNIST variants, we trained both
the single hidden layer and CNN models with random subsamples of 25%, 50%, and 75% of the training
set, with 15% of the training set used for validation in all settings. In addition, in Table 12, we compare the
generalization error of structured classes with an unstructured model, and ﬁnd that the structured classes
have consistently lower generalization error.

25

Table 6: Test accuracy when replacing the hidden layer with structured classes in the single hidden layer architecture,
at parameter budgets corresponding to LDR-TD and LDR-SD rank one. Rank is in parentheses. The ﬁrst group
of structured methods (in orange) all have compression factors (relative to a general unstructured layer) of 98 on
MNIST-bg-rot and MNIST-noise, and 128 on CIFAR-10 and NORB. The second group of structured methods (in
blue) all have compression factors of 196 on MNIST-bg-rot and MNIST-noise, and 256 on CIFAR-10 and NORB.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Table 7: Test accuracy when replacing the fully-connected layer with structured classes in the CNN architecture,
at parameter budgets corresponding to LDR-TD and LDR-SD rank one. Rank is in parentheses. The ﬁrst group
of structured methods (in orange) all have compression factors (relative to a general unstructured layer) of 98 on
MNIST-bg-rot and MNIST-noise, and 128 on CIFAR-10 and NORB. The second group of structured methods (in
blue) all have compression factors of 196 on MNIST-bg-rot and MNIST-noise, and 256 on CIFAR-10 and NORB.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Unstructured
LDR-TD (r = 1)
Toeplitz-like [45] (r = 4)
Hankel-like (r = 4)
Vandermonde-like (r = 4)
Low-rank [15] (r = 4)

LDR-SD (r = 1)
Toeplitz-like [45] (r = 2)
Hankel-like (r = 2)
Vandermonde-like (r = 2)
Low-rank [15] (r = 2)

44.08
45.81
42.67
42.23
37.14
35.67

44.74
42.07
41.01
33.56
32.64

Fully-connected
LDR-TD (r = 1)
Toeplitz-like [45] (r = 4)
Hankel-like (r = 4)
Vandermonde-like (r = 4)
Low-rank [15] (r = 4)

LDR-SD (r = 1)
Toeplitz-like [45] (r = 2)
Hankel-like (r = 2)
Vandermonde-like (r = 2)
Low-rank [15] (r = 2)

67.94
68.79
63.23
64.21
61.76
60.35

67.40
63.63
64.08
51.38
41.91

46.03
45.33
41.78
41.40
33.93
32.28

43.29
40.68
40.46
28.99
24.93

68.09
66.63
67.10
68.10
63.63
60.90

65.48
67.15
67.49
58.00
48.48

59.83
62.75
59.38
60.09
48.98
43.66

63.78
57.27
57.95
43.21
37.03

75.16
74.23
72.25
71.23
72.11
71.47

73.63
71.64
71.21
68.08
65.34

65.15
78.45
75.75
73.65
59.80
52.25

78.80
74.25
71.20
50.85
38.85

90.30
92.55
91.60
90.80
90.40
87.30

92.20
91.45
90.65
86.50
71.15

26

E.3 Additional visualizations

In Figure 6, we visualize the learned subdiagonal on NORB along with images from the dataset.

On the MNIST-bg-rot dataset [30], we note that Chen et al. [7] also tested several methods on this dataset,
including Random Edge Removal [11], Low Rank Decomposition [15], Dark Knowledge [25], HashedNets [7],
and HashedNets with Dark Knowledge, and reported test errors of 73.17, 80.63, 79.03, 77.40, 59.20, and 58.25,
where each method had 12406 parameters in the architecture. We found that our LDR-SD class, with 10986
parameters in the architecture, achieved a test error of 55.26, as shown in Table 6, outperforming all methods
evaluated by Chen et al. [7]. Sindhwani et al. [45] later also tested on this dataset, and reported test errors
of 68.4, 62.11, and 55.21 for Fastfood (10202 parameters), Circulant (8634 parameters), and Toeplitz-like,
r = 2 (10986 parameters). LDR-SD exceeds their reported results for Fastfood and Circulant [8], but not
that of Toeplitz-like. We did ﬁnd that our proposed classes consistently exceeded the performance of our own
implementation of Toeplitz-like on this dataset (Table 1, Figure 3, and Tables 6 and 7).

Table 8: On the MNIST variants, in the single hidden layer architecture, we compare LDR-SD, pruning [23], and a
baseline which reduces the number of hidden units (denoted RHU), at multiple budgets. At each budget, we adjust the
number of pruned weights or hidden units to match as closely as possible the parameter budget of LDR-SD. Parameter
counts of fully-connected layers for LDR-SD and pruning at ranks 1,2,4,8,12, and 16 are 10986, 12554, 15690, 21962,
28234, and 34506 respectively, and 11126, 12714, 15890, 22242, 28594, 34946 for RHU (for which parameter count
cannot be controlled exactly). As shown above, we ﬁnd that the classiﬁcation accuracy of LDR-SD consistently
exceeds that of both methods.

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

1
2
4
8
12
16

1
2
4
8
12
16

(a) MNIST-bg-rot

44.74
44.46
47.72
48.76
48.90
49.51

78.80
77.95
78.32
78.63
78.33
78.08

40.41
41.18
42.45
43.52
43.19
43.58

67.75
69.35
68.25
67.25
67.30
66.95

(b) MNIST-noise

37.18
37.60
37.98
39.77
40.56
40.70

62.85
62.55
63.40
64.45
63.85
66.10

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

(a) Subdiagonal of B (NORB)

(b) Images from NORB

Figure 6: We visualize the learned subdiagonal of the operator B and images from the NORB dataset. We observe a
centering phenomenon similar to that described in Figure 4.

E.4 Rectangles dataset

We provide an interesting example of a case where LDR-TD and LDR-SD do not exceed the performance
of the ﬁxed operator classes in the single hidden layer architecture. In this simple dataset from Larochelle
et al. [30], the task is to classify a binary image of a rectangle as having a greater length or width. We
show examples of the dataset in Figure 7. On this dataset, in contrast to the more challenging datasets
(MNIST-bg-rot [30], MNIST-noise [30], CIFAR-10 [29], and NORB [32]) we tested on, every structured class
outperforms an unconstrained model (622506 parameters), including the circulant class [8] which compresses
the hidden layer by 784x, and expanding the class beyond Toeplitz-like does not improve performance. We
hypothesize that this is because the Toeplitz-like class may enforce the right structure, in the sense that it is
suﬃciently expressive to ﬁt a perfect model on this dataset, but not expansive enough to lead to overﬁtting.

27

Table 9: On the MNIST variants, in the CNN architecture, we compare LDR-SD, pruning [23], and a baseline which
reduces the number of hidden units (denoted RHU), at multiple budgets. At each budget, we adjust the number of
pruned weights or hidden units to match as closely as possible the parameter budget of LDR-SD. Parameter counts of
fully-connected layers for LDR-SD and pruning at ranks 1,2,4,8,12, and 16 are 11770, 13338, 16474, 22746, 29018, and
35290 respectively, and 11935, 13525, 16705, 23065, 29425, 35785 for RHU (for which parameter count cannot be
controlled exactly). As shown above, we ﬁnd that the classiﬁcation accuracy of LDR-SD consistently exceeds that of
both methods.

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

1
2
4
8
12
16

1
2
4
8
12
16

(a) MNIST-bg-rot

67.40
67.53
67.96
67.21
68.54
67.00

92.20
92.75
91.30
91.95
92.10
93.20

64.25
64.05
65.50
64.12
65.65
65.59

90.80
91.65
90.60
91.05
90.00
90.55

(b) MNIST-noise

64.03
64.67
66.37
64.70
65.99
66.47

90.95
91.00
91.25
90.65
90.85
90.40

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

For example, while the Toeplitz-like operators model approximate shift equivariance (discussed in Section 4
and Proposition 7 in Section C.3), the additional scaling that subdiagonal operators provide is unnecessary
on these binary inputs.

Figure 7: Examples of images from the rectangles dataset [30].

E.5 Acceleration at inference time

We empirically study the acceleration obtained at inference time (on CPU) with our implementation of the
algorithms for multiplication by LDR-SD described in Appendix C.2. We generated random parameters for
each class and ran each multiplication algorithm 1000 times to compare the speedup of each class over an
unstructured multiply. Each test was repeated 10 times, and the minimum total runtime over the 10 tests was
used for each class. As shown in Figure 8 and Table 14, at n
4096, our simple Python implementation is
3.34-46.06x faster than the highly optimized unstructured matrix-vector multiply (a BLAS level 2 operation).
We also compare with two other structured classes, low-rank and Toeplitz-like, at r = 1, 2, 4, 8, 16. A batch
size of one was used in all tests. The time complexity of multiplication by low-rank and Toeplitz-like is O(nr)
and O(nr log n) respectively, compared to O(nr log2 n) for LDR-SD.

≥

28

Table 10: On the MNIST variants, in the single hidden layer architecture, we show how the number of training
samples aﬀects the performance of the unstructured model and the structured classes. Columns correspond to models
trained on 25%, 50%, 75% and 100% of the training data (randomly subsampled). LDR-TD and LDR-SD consistently
outperform the structured baselines at the tested subsampling ratios. On MNIST-bg-rot, LDR-TD only needs 75% of
the training data to outperform the unstructured model trained on 100% of the training data. On MNIST-noise, both
LDR-TD and LDR-SD only need 25% of the training data to outperform the unstructured layer. All are rank one.

Method

25% 50% 75% 100%

Method

25% 50% 75% 100%

Unstructured

34.46

38.80

43.35

44.08

Unstructured

59.30

61.85

65.35

65.15

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

34.01
35.64
33.71
21.44

39.59
39.78
36.44
23.46

44.35
42.72
39.32
23.48

45.81
44.74
41.12
25.06

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

65.45
67.90
56.15
24.25

74.60
71.15
67.75
26.20

77.45
76.95
72.30
26.85

78.45
78.80
73.95
26.40

(a) MNIST-bg-rot

(b) MNIST-noise

Table 11: On the MNIST variants, in the CNN architecture, we show how the number of training samples aﬀects the
performance of the unstructured model and the structured classes. Columns correspond to models trained on 25%,
50%, 75% and 100% of the training data (randomly subsampled). LDR-TD and LDR-SD consistently outperform the
structured baselines at the tested subsampling ratios. On MNIST-noise, both LDR-TD and LDR-SD only need 50%
of the training data to outperform the unstructured layer. All are rank one.

Method

25% 50% 75% 100%

Method

25% 50% 75% 100%

Unstructured

54.12

62.53

67.52

67.94

Unstructured

81.85

88.25

89.75

90.30

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

53.66
50.72
49.10
26.98

62.15
61.92
57.20
27.97

67.25
65.93
61.53
28.97

68.79
67.40
63.00
29.63

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

86.45
86.95
81.65
33.15

91.35
90.90
88.15
38.40

93.00
91.55
90.90
42.55

92.55
92.20
90.95
44.55

(a) MNIST-bg-rot

(b) MNIST-noise

Table 12: Generalization error for unstructured, LDR-TD, LDR-SD, Toeplitz-like, low-rank classes on the single
hidden layer architecture. Consistent with Theorem 2, the structured classes have consistently lower generalization
error than the unstructured model. All are rank one.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Unstructured

LDR-TD
LDR-SD
Toeplitz-like [45]
Low-rank [15]

55.78

13.52
12.87
7.98
8.40

21.63

11.36
12.65
15.80
0.31

34.32

7.10
6.29
5.59
0.09

40.03

9.51
8.68
7.87
2.59

F Experimental details

F.1

Image classiﬁcation

In Table 15, we provide details on the datasets we use for evaluation. For all our experiments, batch sizes
were chosen to be 50. NORB was downsampled to 32
32, and the left stereo image was used. Training
was performed with stochastic gradient descent with momentum, with the number of epochs set to 50 on all
datasets. 15% of the training data was used for the validation set on all experiments. We ﬁxed momentum at
0.9 for all methods for all experiments, and performed a grid search over learning rate. Unless otherwise
stated, for each method, we tested the learning rates {0.0002, 0.0005, 0.001, 0.002}, with three trials (with

×

29

Table 13: Test accuracy when replacing the hidden layer with structured classes on the rectangles dataset [30]. Where
applicable, rank (r) is in parentheses, and the number of parameters in the architecture is in italics below each method.

Test Accuracy

Method

Unconstrained

LDR-TD (r = 1)

LDR-SD (r = 1)

Toeplitz-like (r = 4) [45]

Hankel-like (r = 4)

Vandermonde-like (r = 4)

Low-rank (r = 4) [15]

Fastfood [49]

Circulant [8]

91.94
622506
98.53
14122

98.39
10986

99.29
14122

97.77
14122

94.11
14122

92.80
14122

92.20
10202

95.58
8634

Figure 8: Acceleration of n × n structured classes over unstructured matrix-vector multiply at inference time. At
n ≥ 4096, LDR-SD (r = 1) achieves a speedup of 3.34-46.06x over unstructured. Data for higher ranks are shown in
Table 14. The comparison to the low-rank and Toeplitz-like classes illustrates a tradeoﬀ involved in broadening the
class of structured matrices we learn over. Though LDR-SD consistently outperforms these classes on downstream
quality, its computational cost of multiplication is O(nr log2 n), compared to O(nr) and O(nr log n) for low-rank and
Toeplitz-like respectively. Experimental details are in Appendix E.5.

random initializations) per learning rate. For each trial, we test on the validation set at each epoch, and
report the test accuracy of the model with the highest validation accuracy, over all learning rates, trials, and
epochs.

In Figure 3, for each method and each of the four learning rates, we perform ﬁve trials with random
initializations and report the average and standard deviation of the test accuracy of the learning rate with
the highest average validation accuracy.

30

Table 14: Acceleration of n × n structured classes over unstructured matrix-vector multiply at inference time.
Experimental details are in Appendix E.5.

n
29
210
211
212
213
214
215

1

×
×
×
×
×
×
×

101
102
102
103
103
103
104

5.15
1.39
4.14
2.38
5.96
8.35
1.79

2

×
×
×
×
×
×
×

101
101
102
102
103
103
103

2.43
5.41
1.60
8.71
1.75
3.44
7.50

Rank

2.46
5.66
1.71
7.46
1.65
3.40
7.53

101
101
102
102
103
103
103

4

×
×
×
×
×
×
×

4

(a) Low-rank

Rank

8

×
×
×
×
×
×
×

101
101
102
102
103
103
103

2.08
4.62
1.05
4.73
1.13
2.29
4.91

16

1.81
3.43
6.90
3.59
8.86
1.74
3.70

101
101
101
102
102
103
103

×
×
×
×
×
×
×

n
29
210
211
212
213
214
215

n
29
210
211
212
213
214
215

1

2

3.06
7.34
1.90
1.23
3.34
6.96
1.49

×
×
×
×
×
×
×

1
10−
1
10−
100
101
101
101
102

2.60
6.21
1.71
1.01
2.73
5.68
1.19

×
×
×
×
×
×
×

1

1
10−
1
10−
100
101
101
101
102

2.32
5.18
1.38
7.92
2.26
4.19
9.07

10−
1
10−
100
100
101
101
101
(b) Toeplitz-like

×
×
×
×
×
×
×

8

16

1.86
4.00
1.08
5.97
1.52
3.00
5.46

×
×
×
×
×
×
×

1

10−
1
10−
100
100
101
101
101

1.61
3.28
8.46
4.62
1.23
2.26
3.82

×
×
×
×
×
×
×

1
10−
1
10−
1
10−
100
101
101
101

1

2

6.68
1.49
4.99
3.34
9.71
2.12
4.61

×
×
×
×
×
×
×

2
10−
1
10−
1
10−
100
100
101
101

4.63
1.20
4.32
2.57
6.61
1.41
2.82

×
×
×
×
×
×
×

2
10−
1
10−
1
10−
100
100
101
101

Rank

4

4.05
9.45
3.02
1.61
4.40
8.38
1.60

×
×
×
×
×
×
×

2

2

10−
10−
1
10−
100
100
100
101

(c) LDR-SD

8

16

3.10
6.73
1.94
1.06
2.46
4.35
8.58

×
×
×
×
×
×
×

2

2

10−
10−
1
10−
100
100
100
100

2.56
5.24
1.37
7.52
1.68
3.00
5.70

×
×
×
×
×
×
×

2
10−
2
10−
1
10−
1
10−
100
100
100

31

Table 15: Overview of the image classiﬁcation datasets used in this work. For all datasets, 15% of the training set was
used for the validation set.

Dataset

Training Examples Test Examples Number of Classes

MNIST-bg-rot [30]
MNIST-noise [30]
CIFAR-10 [29]
NORB [32]
Rectangles [30]

12000
12000
50000
291600
1200

50000
2000
10000
58320
50000

10
10
10
6
2

Single hidden layer architecture In these experiments, we used an architecture consisting of a fully-
connected hidden layer, followed by a fully-connected softmax layer. In order to be consistent with the
architecture used in Sindhwani et al. [45], we do not use a bias term in the hidden layer.

CNN architecture In these experiments, shown in Table 7 in Appendix E, we tested on a LeNet-based
architecture. The architecture has 2 convolution/pool layers with 6 and 16 channels respectively, followed
by a fully-connected layer, followed by fully-connected logit/softmax layer. We replaced the second to last
fully-connected layer, which was of dimensions 784
784 for the MNIST-bg-rot and MNIST-noise datasets,
and 1024

1024 for the CIFAR-10 and NORB experiments.

×

×

Replacing convolutional layers This experiment corresponds to Table 3.

Here, we investigated whether the convolutional layers of CNNs can be learned automatically. For our
experiments, we test on the simplest possible multi-channel CNN model on the CIFAR-10 dataset. The model
consists of one layer of convolutional channels (3 RGB in channels, 3 out channels, stride 5), followed by
a fully-connected layer and a ﬁnal FC+softmax layer (total of 4 layers). We replace the convolutions with
various structured matrices of the same dimensions, keeping the same 3
3 channel structure (e.g. it would
consist of 3

3 = 9 square structured matrices) and number of hidden units.8

×

The LDR classes beneﬁt from being composed with LDR matrices of the same type (due to the composition
property, Proposition 1(c)), so we additionally replace the later FC layer with the same structured matrix
type.

·

By Proposition 1(d), channels of Toeplitz-like matrices form a larger Toeplitz-like matrix of the same
size. Using this insight, we consider replacing the channel structure of the convolutional layer with either
channels of structured matrices or a single wide structured matrix. (Also, note that this is able to leverage
the asymptotic fast nature of our structured classes.)

Because it seems that convolutional layers are strongly dependent on pooling – our structured matrices
outperform them in isolation – we compare against a version of the CNN with an additional pooling layer after
the convolutional channels. Note that this comparison is the same basic four layer model with a structured
matrix vs. a ﬁve layer convolutional model with pooling. Since the architectures are quite diﬀerent and
diﬃcult to directly compare, we also experimented with adding more hidden units to the pooling model.

F.2 Language modeling

For a language modeling application9, we explored replacing weight matrices in a recurrent neural network
with structured matrices. We evaluate on a single layer LSTM architecture, deﬁned by the update equations:

8The convolutions are padded to ensure their input and output dimensions are equal.
9Code available at https://github.com/pytorch/examples/tree/master/word_language_model.

32

i = σ(Wiix + bii + Whih + bhi)
f = σ(Wif x + bif + Whf h + bhf )
g = tanh(Wigx + big + Whgh + bhg)
o = σ(Wiox + bio + Whoh + bho)
c(cid:48) = f
g
∗
∗
h(cid:48) = o tanh(c(cid:48))

c + i

In our experiments we replace the matrices Wii, Wif , Wig, Wio with structured matrices. We use a hidden
layer of size 128, and word embedding size of 128. We evaluate on the Wikitext-2 dataset, which consists of
Wikipedia articles (2,088,628 training, 217,646 validation, and 245,569 test tokens). The total vocabulary is
of size 33,278. We use the default hyperparameters and train using stochastic gradient descent with an initial
learning rate of 20. The learning rate is annealed 4x after each epoch if performance does not improve on the
validation set. Results are shown in Table 2.

33

9
1
0
2
 
n
a
J
 
1
 
 
]

G
L
.
s
c
[
 
 
3
v
9
0
3
2
0
.
0
1
8
1
:
v
i
X
r
a

Learning Compressed Transforms
with Low Displacement Rank

Anna T. Thomas∗†, Albert Gu∗†, Tri Dao†, Atri Rudra‡, and Christopher Ré†

†Department of Computer Science, Stanford University
‡Department of Computer Science and Engineering, University at Buﬀalo, SUNY
{thomasat,albertgu,trid}@stanford.edu, atri@buffalo.edu,
chrismre@cs.stanford.edu

January 3, 2019

Abstract

The low displacement rank (LDR) framework for structured matrices represents a matrix through two
displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied
ﬁxed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a class
of LDR matrices with more general displacement operators, and explicitly learn over both the operators
and the low-rank component. This class generalizes several previous constructions while preserving
compression and eﬃcient computation. We prove bounds on the VC dimension of multi-layer neural
networks with structured weight matrices and show empirically that our compact parameterization can
reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional,
and recurrent neural networks for image classiﬁcation and language modeling tasks, our new classes exceed
the accuracy of existing compression approaches, and on some tasks also outperform general unstructured
layers while using more than 20x fewer parameters.

1

Introduction

Recent years have seen a surge of interest in structured representations for deep learning, motivated by
achieving compression and acceleration while maintaining generalization properties. A popular approach
for learning compact models involves constraining the weight matrices to exhibit some form of dense but
compressible structure and learning directly over the parameterization of this structure. Examples of structures
explored for the weight matrices of deep learning pipelines include low-rank matrices [15, 42], low-distortion
projections [49], (block-)circulant matrices [8, 17], Toeplitz-like matrices [34, 45], and constructions derived
from Fourier-related transforms [37]. Though they confer signiﬁcant storage and computation beneﬁts, these
constructions tend to underperform general fully-connected layers in deep learning. This raises the question of
whether broader classes of structured matrices can achieve superior downstream performance while retaining
compression guarantees.

Our approach leverages the low displacement rank (LDR) framework (Section 2), which encodes
structure through two sparse displacement operators and a low-rank residual term [27]. Previous work
studying neural networks with LDR weight matrices assumes ﬁxed displacement operators and learns only
over the residual [45, 50]. The only case attempted in practice that explicitly employs the LDR framework
uses ﬁxed operators encoding shift invariance, producing weight matrices which were found to achieve superior
downstream quality than several other compression approaches [45]. Unlike previous work, we consider
learning the displacement operators jointly with the low-rank residual. Building upon recent progress on
structured dense matrix-vector multiplication [14], we introduce a more general class of LDR matrices and

∗These authors contributed equally.

1

develop practical algorithms for using these matrices in deep learning architectures. We show that the
resulting class of matrices subsumes many previously used structured layers, including constructions that
did not explicitly use the LDR framework [17, 37]. When compressing weight matrices in fully-connected,
convolutional, and recurrent neural networks, we empirically demonstrate improved accuracy over existing
approaches. Furthermore, on several tasks our constructions achieve higher accuracy than general unstructured
layers while using an order of magnitude fewer parameters.

To shed light on the empirical success of LDR matrices in machine learning, we draw connections to
recent work on learning equivariant representations, and hope to motivate further investigations of this link.
Notably, many successful previous methods for compression apply classes of structured matrices related
to convolutions [8, 17, 45]; while their explicit aim is to accelerate training and reduce memory costs, this
constraint implicitly encodes a shift-invariant structure that is well-suited for image and audio data. We
observe that the LDR construction enforces a natural notion of approximate equivariance to transformations
governed by the displacement operators, suggesting that, in contrast, our approach of learning the operators
allows for modeling and learning more general latent structures in data that may not be precisely known in
advance.

Despite their increased expressiveness, our new classes retain the storage and computational beneﬁts of
conventional structured representations. Our construction provides guaranteed compression (from quadratic
to linear parameters) and matrix-vector multiplication algorithms that are quasi-linear in the number of
parameters. We additionally provide the ﬁrst analysis of the sample complexity of learning neural networks
with LDR weight matrices, which extends to low-rank, Toeplitz-like and other previously explored ﬁxed
classes of LDR matrices. More generally, our analysis applies to structured matrices whose parameters can
interact multiplicatively with high degree. We prove that the class of neural networks constructed from these
matrices retains VC dimension almost linear in the number of parameters, which implies that LDR matrices
with learned displacement operators are still eﬃciently recoverable from data. This is consistent with our
empirical results, which suggest that constraining weight layers to our broad class of LDR matrices can reduce
the sample complexity of learning compared to unstructured weights.

We provide a detailed review of previous work and connections to our approach in Appendix B.

Summary of contributions:

•

•

•

We introduce a rich class of LDR matrices where the displacement operators are explicitly learned from
data, and provide multiplication algorithms implemented in PyTorch (Section 3).1

We prove that the VC dimension of multi-layer neural networks with LDR weight matrices, which
encompasses a broad class of previously explored approaches including the low-rank and Toeplitz-like
classes, is quasi-linear in the number of parameters (Section 4).

We empirically demonstrate that our construction improves downstream quality when compressing
weight layers in fully-connected, convolutional, and recurrent neural networks compared to previous
compression approaches, and on some tasks can even outperform general unstructured layers (Section 5).

2 Background: displacement rank

The generic term structured matrix refers to an m
n matrix that can be represented in much fewer than mn
parameters, and admits fast operations such as matrix-vector multiplication. The displacement rank approach
n)
represents a structured matrix M
deﬁning a linear map

n through displacement operators (A
×
MB on matrices, and a residual R, so that if

∈
AM

m, B

Rm

Rm

Rn

×

∈

∈

×

×

A,B : M

∇

(cid:55)→

−

AM

MB = R

−

(1)

then M can be manipulated solely through the compressed representation (A, B, R). We assume that A and
B have disjoint eigenvalues, which guarantees that M can be recovered from A, B, R (c.f. Theorem 4.3.2,
Pan [40]). The rank of R (also denoted

A,B[M]) is called the displacement rank of M w.r.t. (A, B).2

∇

1Our code is available at https://github.com/HazyResearch/structured-nets.
2Throughout this paper, we use square matrices for simplicity, but LDR is well-deﬁned for rectangular.

2

The displacement approach was originally introduced to describe the Toeplitz-like matrices, which are
not perfectly Toeplitz but still have shift-invariant structure [27]. These matrices have LDR with respect

to shift/cycle operators. A standard formulation uses A = Z1, B = Z

1, where Zf =

1(cid:21)
denotes the matrix with 1 on the subdiagonal and f in the top-right corner. The Toeplitz-like matrices have
previously been applied in deep learning and kernel approximation, and in several cases have performed
signiﬁcantly better than competing compressed approaches [10, 34, 45]. Figure 1 illustrates the displacement (1)
for a Toeplitz matrix, showing how the shift invariant structure of the matrix leads to a residual of rank at
most 2.

(cid:20)

1)

−

×

−

−

01

1)

(n
×
In

−
1

f

0(n

Figure 1: Displacement equation for a Toeplitz matrix with respect to shift operators Z1, Z−1.

A few distinct classes of useful matrices are known to satisfy a displacement property: the classic types are
the Toeplitz-, Hankel-, Vandermonde-, and Cauchy-like matrices (Appendix C, Table 5), which are ubiquitous
in other disciplines [40]. These classes have ﬁxed operators consisting of diagonal or shift matrices, and LDR
properties have traditionally been analyzed in detail only for these special cases. Nonetheless, a few elegant
properties hold for generic operators, stating that certain combinations of (and operations on) LDR matrices
preserve low displacement rank. We call these closure properties, and introduce an additional block closure
property that is related to convolutional ﬁlter channels (Section 5.2).

We use the notation

r
A,B to refer to the matrices of displacement rank

r with respect to (A, B).

D

≤

Proposition 1. LDR matrices are closed under the following operations:

r
BT ,AT and M−

1

r
B,A.

∈ D

A,B, then MT
r

∈ D
s
A,B, then M + N

∈ D

s
B,C, then MN

r+s
A,B.

∈ D

r+s
A,C.

∈ D

(a) Transpose/Inverse If M

(b) Sum If M

∈ D
(c) Product If M

r
A,B and N

∈ D
r
A,B and N

∈ D

(d) Block Let Mij satisfy Mij ∈ D
has displacement rank rk(cid:96).

∈ D
r
Ai,Bj

Proposition 1 is proved in Appendix C.

for i = 1 . . . k, j = 1 . . . (cid:96). Then the k

(cid:96) block matrix (Mij)ij

×

3 Learning displacement operators

We consider two classes of new displacement operators. These operators are ﬁxed to be matrices with
particular sparsity patterns, where the entries are treated as learnable parameters.

The ﬁrst operator class consists of subdiagonal (plus corner) matrices: Ai+1,i, along with the corner
1, are the only possible non-zero entries. As Zf is a special case matching this sparsity pattern, this

A0,n
class is the most direct generalization of Toeplitz-like matrices with learnable operators.

−

1 and An

The second class of operators are tridiagonal (plus corner) matrices: with the exception of the outer
corners A0,n
1. Figure 2 shows the displacement operators
i
for the Toeplitz-like class and our more general operators. We henceforth let LDR-SD and LDR-TD denote
the classes of matrices with low displacement rank with respect to subdiagonal and tridiagonal operators,
respectively. Note that LDR-TD contains LDR-SD.

1,0, Ai,j can only be non-zero if

| ≤

−

−

−

j

|

3

0

1

...





0


0



· · ·

. . .
. . .
. . .

0

1
. . .
0

0
. . .

. . .
1

f

0
...

0












0

x1
...

0
0












· · ·

0
. . .

. . .
. . .
. . .
. . . xn

1

−

0

x2
. . .
0

x0

0
...

0












b0 a0
c0
b1
...

c1

0











0
t

· · ·
a1
. . .
. . .
. . .

0

. . .

s
0
...

bn
cn

−

−

1 an
bn
2

2

−
1

−











Figure 2: The Zf operator (left), and our learnable subdiagonal (center) and tridiagonal (right) operators, corresponding
to our proposed LDR-SD and LDR-TD classes.

Expressiveness The matrices we introduce can model rich structure and subsume many types of linear
transformations used in machine learning. We list some of the structured matrices that have LDR with
respect to tridiagonal displacement operators:

Proposition 2. The LDR-TD matrices contain:

(a) Toeplitz-like matrices, which themselves include many Toeplitz and circulant variants (including standard

convolutional ﬁlters - see Section 5.2 and Appendix C, Corollary 1) [8, 17, 45].

(b) low-rank matrices.

(c) the other classic displacement structures: Hankel-like, Vandermonde-like, and Cauchy-like matrices.

(d) orthogonal polynomial transforms, including the Discrete Fourier and Cosine Transforms.

(e) combinations and derivatives of these classes via the closure properties (Proposition 1), including
structured classes previously used in machine learning such as ACDC [37] and block circulant layers [17].

These reductions are stated more formally and proved in Appendix C.1. We also include a diagram of the

structured matrix classes included by the proposed LDR-TD class in Figure 5 in Appendix C.1.

Our parameterization Given the parameters A, B, R, the operation that must ultimately be performed
1
is matrix-vector multiplication by M =
A,B[R]. Several schemes for explicitly reconstructing M from
−
its displacement parameters are known for speciﬁc cases [41, 44], but do not always apply to our general
operators. Instead, we use A, B, R to implicitly construct a slightly diﬀerent matrix with at most double the
displacement rank, which is simpler to work with.

∇

Proposition 3. Let
vectors g1, . . . , gr, h1, . . . , hr ∈

K

Rn, then the matrix

×

(A, v) denote the n

n Krylov matrix, deﬁned to have i-th column Aiv. For any

r

K

i=1
(cid:88)

(A, gi)

(BT , hi)T

K

(2)

has displacement rank at most 2r with respect to A−

1, B.

Thus our representation stores the parameters A, B, G, H, where A, B are either subdiagonal or tridiagonal
r. These parameters implicitly deﬁne the

Rn

operators (containing n or 3n parameters), and G, H
matrix (2), which is the LDR weight layer we use.

∈

×

Algorithms for LDR-SD Generic and near-linear time algorithms for matrix-vector multiplication by
LDR matrices with even more general operators, including both the LDR-TD and LDR-SD classes, were
recently shown to exist [14]. However, complete algorithms were not provided, as they relied on theoretical
results such as the transposition principle [6] that only imply the existence of algorithms. Additionally, the
recursive polynomial-based algorithms are diﬃcult to implement eﬃciently. For LDR-SD, we provide explicit

4

and complete near-linear time algorithms for multiplication by (2), as well as substantially simplify them to
be useful in practical settings and implementable with standard library operations. We empirically compare
the eﬃciency of our implementation and unstructured matrix-vector multiplication in Figure 8 and Table 14
in Appendix E, showing that LDR-SD accelerates inference by 3.34-46.06x for n
4096. We also show
results for the low-rank and Toeplitz-like classes, which have a lower computational cost. For LDR-TD, we
(BT , hi) matrices for i = 1, ..., r from Proposition 3 and then apply
explicitly construct the
the standard O(n2) matrix-vector multiplication algorithm. Eﬃcient implementations of near-linear time
algorithms for LDR-TD are an interesting area of future work.

(A, gi) and

≥

K

K

Theorem 1. Deﬁne the simultaneous computation of k Fast Fourier Transforms (FFT), each with size m,
to be a batched FFT with total size km.
Consider any subdiagonal matrix A

(A, g) can be
multiplied by any vector x by computing 8 log2(n) batched FFTs, each of total size 2n. The total number of
computations is O(n log2 n).

n and vectors g, h

(A, g)T or

Rn. Then

Rn

K

K

∈

∈

×

These algorithms are also automatically diﬀerentiable, which we use to compute the gradients when

learning. More complete descriptions of these algorithms are presented in Appendix C.

4 Theoretical properties of structured matrices

Complexity of LDR neural networks The matrices we use (2) are unusual in that the parameters
interact multiplicatively (namely in Ai, Bi) to implicitly deﬁne the actual layer. In contrast, fully-connected
layers are linear and other structured layers, such as Fastfood and ACDC [31, 37, 49], are constant degree in
their parameters. However, we can prove that this does not signiﬁcantly change the learnability of our classes:

Theorem 2. Let
linear activations. Let sign
The VC dimension of this class is

F

F

denote the class of neural networks with L LDR layers, W total parameters, and piecewise
.

denote the corresponding classiﬁcation functions, i.e.

sign f (x) : f

x
{

(cid:55)→

∈ F}

VCdim(sign

) = O(LW log W ).

F

Theorem 2 matches the standard bound for unconstrained weight matrices [4, 24]. This immediately
implies a standard PAC-learnable guarantee [47]. Theorem 2 holds for even more general activations and
matrices that for example include the broad classes of [14]. The proof is in Appendix D, and we empirically
validate the generalization and sample complexity properties of our class in Section 5.3.

Displacement rank and equivariance We observe that displacement rank is related to a line of work
outside the resource-constrained learning community, speciﬁcally on building equivariant (also called
covariant in some contexts [5, 35]) feature representations that transform in predictable ways when the input
is transformed. An equivariant feature map Φ satisﬁes

Φ(B(x)) = A(Φ(x))

(3)

for transformations A, B (invariance is the special case when A is the identity) [16, 33, 43]. This means that
perturbing the input by a transformation B before passing through the map Φ is equivalent to ﬁrst ﬁnding
the features Φ then transforming by A.

−

Intuitively, LDR matrices are a suitable choice for modeling approximately equivariant linear maps, since
ΦB of (3) has low complexity. Furthermore, approximately equivariant maps should
the residual AΦ
retain the compositional properties of equivariance, which LDR satisﬁes via Proposition 1. For example,
Proposition 1(c) formalizes the notion that the composition of two approximately equivariant maps is still
approximately equivariant. Using this intuition, the displacement representation (1) of a matrix decomposes
into two parts: the operators A, B deﬁne transformations to which the model is approximately equivariant,
and the low complexity residual R controls standard model capacity.

Equivariance has been used in several ways in the context of machine learning. One formulation, used for
example to model ego-motions, supposes that (3) holds only approximately, and uses a ﬁxed transformation

5

B along with data for (3) to learn an appropriate A [1, 33]. Another line of work uses the representation
theory formalization of equivariant maps [12, 28]. We describe this formulation in more detail and show how
LDR satisﬁes this deﬁnition as well in Appendix C.3, Proposition 7. In contrast to previous settings, which
ﬁx one or both of A, B, our formulation stipulates that Φ can be uniquely determined from A, B, and learns
the latter as part of an end-to-end model. In Section 5.4 we include a visual example of latent structure that
our displacement operators learn, where they recover centering information about objects from a 2D image
dataset.

5 Empirical evaluation

Overview In Section 5.1 we consider a standard setting of compressing a single hidden layer (SHL) neural
network and the fully-connected (FC) layer of a CNN for image classiﬁcation tasks. Following previous
work [7, 45], we test on two challenging MNIST variants [30], and include two additional datasets with more
realistic objects (CIFAR-10 [29] and NORB [32]). Since SHL models take a single channel as input, we
converted CIFAR-10 to grayscale for this task. Our classes and the structured baselines are tested across
diﬀerent parameter budgets in order to show tradeoﬀs between compression and accuracy. As shown in
Table 1, in the SHL model, our methods consistently have higher test accuracy than baselines for compressed
training and inference, by 3.14, 2.70, 3.55, and 3.37 accuracy points on MNIST-bg-rot, MNIST-noise,
CIFAR-10, and NORB respectively. In the CNN model, as shown in Table 1 in Appendix E, we found
improvements of 5.56, 0.95, and 1.98 accuracy points over baselines on MNIST-bg-rot, MNIST-noise, and
NORB respectively. Additionally, to explore whether learning the displacement operators can facilitate
adaptation to other domains, we replace the input-hidden weights in an LSTM for a language modeling task,
and show improvements of 0.81-30.47 perplexity points compared to baselines at several parameter budgets.
In addition to experiments on replacing fully-connected layers, in Section 5.2 we also replace the con-
volutional layer of a simple CNN while preserving performance within 1.05 accuracy points on CIFAR-10.
In Section 5.3, we consider the eﬀect of a higher parameter budget. By increasing the rank to just 16, the
LDR-SD class meets or exceeds the accuracy of the unstructured FC layer in all datasets we tested on, for
both SHL and CNN.3 Appendix F includes more experimental details and protocols. Our PyTorch code is
publicly available at github.com/HazyResearch/structured-nets.

5.1 Compressing fully-connected layers

Image classiﬁcation Sindhwani et al. [45] showed that for a ﬁxed parameter budget, the Toeplitz-like
class signiﬁcantly outperforms several other compression approaches, including Random Edge Removal [11],
Low Rank Decomposition [15], Dark Knowledge [25], HashedNets [7], and HashedNets with Dark Knowledge.
Following previous experimental settings [7, 45], Table 1 compares our proposed classes to several baselines
using dense structured matrices to compress the hidden layer of a single hidden layer neural network.
In addition to Toeplitz-like, we implement and compare to other classic LDR types, Hankel-like and
Vandermonde-like, which were previously indicated as an unexplored possibility [45, 50]. We also show results
when compressing the FC layer of a 7-layer CNN based on LeNet in Appendix E, Table 7. In Appendix E, we
show comparisons to additional baselines at multiple budgets, including network pruning [23] and a baseline
used in [7], in which the number of hidden units is adjusted to meet the parameter budget.

At rank one (the most compressed setting), our classes with learned operators achieve higher accuracy
than the ﬁxed operator classes, and on the MNIST-bg-rot, MNIST-noise, and NORB datasets even improve
on FC layers of the same dimensions, by 1.73, 13.30, and 2.92 accuracy points respectively on the SHL task,
as shown in Table 1. On the CNN task, our classes improve upon unstructured fully-connected layers by 0.85
and 2.25 accuracy points on the MNIST-bg-rot and MNIST-noise datasets (shown in Table 7 in Appendix E).
As noted above, at higher ranks our classes meet or improve upon the accuracy of FC layers on all datasets
in both the SHL and CNN architectures.

parameters between LDR-SD and the Toeplitz-like or low-rank is r+1

Additionally, in Figure 3 we evaluate the performance of LDR-SD at higher ranks. Note that the ratio of
r , which becomes negligible at higher
3In addition to the results reported in Table 1, Figure 3 and Table 7 in Appendix E, we also found that at rank 16 the
LDR-SD class on the CNN architecture achieved test accuracies of 68.48% and 75.45% on CIFAR-10 and NORB respectively.

6

Table 1: Test accuracy when replacing the hidden layer with structured classes. Where applicable, rank (r) is in
parentheses, and the number of parameters in the architecture is in italics below each method. Comparisons to
previously unexplored classic LDR types as well as additional structured baselines are included, with the ranks
adjusted to match the parameter count of LDR-TD where possible. The Fastfood [49] and Circulant [8] methods
do not have rank parameters, and the parameter count for these methods cannot be exactly controlled. Additional
results when replacing the FC layer of a CNN are in Appendix E. Details for all experiments are in Appendix F.

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Method

Unstructured

LDR-TD (r = 1)

Toeplitz-like [45] (r = 4)

Hankel-like (r = 4)

Vandermonde-like (r = 4)

Low-rank [15] (r = 4)

Fastfood [49]

Circulant [8]

44.08
622506
45.81
14122

42.67
14122

42.23
14122

37.14
14122

35.67
14122

38.13
10202

34.46
8634

65.15
622506
78.45
14122

75.75
14122

73.65
14122

59.80
14122

52.25
14122

63.55
10202

65.35
8634

46.03
1058826
45.33
18442

59.83
1054726
62.75
14342

41.78
18442

41.40
18442

33.93
18442

32.28
18442

39.64
13322

34.28
11274

59.38
14342

60.09
14342

48.98
14342

43.66
14342

59.02
9222

46.45
7174

ranks. Figure 3 shows that at just rank 16, the LDR-SD class meets or exceeds the performance of the FC
layer on all four datasets, by 5.87, 15.05, 0.74, and 6.86 accuracy points on MNIST-bg-rot, MNIST-noise,
CIFAR-10, and NORB respectively, while still maintaining at least 20x fewer parameters.

Of particular note is the poor performance of low-rank matrices. As mentioned in Section 2, every
ﬁxed-operator class has the same parameterization (a low-rank matrix). We hypothesize that the main
contribution to their marked performance diﬀerence is the eﬀect of the learned displacement operator modeling
latent invariances in the data, and that the improvement in the displacement rank classes—from low-rank
to Toeplitz-like to our learned operators—comes from more accurate representations of these invariances.
As shown in Figure 3, broadening the operator class (from Toeplitz-like at r = 1 to LDR-SD at r = 1) is
consistently a more eﬀective use of parameters than increasing the displacement rank (from Toeplitz-like at
r = 1 to r = 2). Note that LDR-SD (r = 1) and Toeplitz-like (r = 2) have the same parameter count.

For the rest of our experiments outside Section 5.1 we use the algorithms in Appendix C speciﬁcally for

LDR-SD matrices, and focus on further evaluation of this class on more expensive models.

Language modeling Here, we replace the input-hidden weights in a single layer long short-term memory
network (LSTM) for a language modeling task. We evaluate on the WikiText-2 dataset, consisting of 2M
training tokens and a vocabulary size of 33K [36]. We compare to Toeplitz-like and low-rank baselines, both
previously investigated for compressing recurrent nets [34]. As shown in Table 2, LDR-SD improves upon the
baselines for each budget tested. Though our class does not outperform the unstructured model, we did ﬁnd
that it achieves a signiﬁcantly lower perplexity than the ﬁxed Toeplitz-like class (by 19.94-42.92 perplexity
points), suggesting that learning the displacement operator can help adapt to diﬀerent domains.

7

Figure 3: Test accuracy vs. rank for unstructured, LDR-SD, Toeplitz-like, low-rank classes. On each dataset, LDR-SD
meets or exceeds the accuracy of the unstructured FC baseline at higher ranks. At rank 16, the compression ratio of
an LDR-SD layer compared to the unstructured layer ranges from 23 to 30. Shaded regions represent two standard
deviations from the mean, computed over ﬁve trials with randomly initialized weights.

Table 2: Test perplexity when replacing input-hidden matrices of an LSTM with structured classes on WikiText-2.
An unconstrained layer, with 65536 parameters, has perplexity 117.74. Parameter budgets correspond to ranks
1,2,4,8,16,24 for LDR-SD. Lower is better.

Num. Parameters LDR-SD Toeplitz-like Low-rank

2048
3072
5120
9216
17408
25600

166.97
154.51
141.91
143.60
132.43
129.46

186.91
177.60
178.07
186.52
162.58
155.73

205.72
179.46
172.38
144.41
135.65
133.37

5.2 Replacing convolutional layers

Convolutional layers of CNNs are a prominent example of equivariant feature maps.4 It has been noted that
convolutions are a subcase of Toeplitz-like matrices with a particular sparsity pattern5 [8, 45]. As channels
are simply block matrices6, the block closure property implies that multi-channel convolutional ﬁlters are
simply a Toeplitz-like matrix of higher rank (see Appendix C, Corollary 1). In light of the interpretation of
LDR of an approximately equivariant linear map (as discussed in Section 4), we investigate whether replacing
convolutional layers with more general representations can recover similar performance, without needing the
hand-crafted sparsity pattern.

Brieﬂy, we test the simplest multi-channel CNN model on the CIFAR-10 dataset, consisting of one layer
of convolutional channels (3 in/out channels), followed by a FC layer, followed by the softmax layer. The
ﬁnal accuracies are listed in Table 3. The most striking result is for the simple architecture consisting of
two layers of a single structured matrix. This comes within 1.05 accuracy points of the highly specialized
architecture consisting of convolutional channels + pooling + FC layer, while using fewer layers, hidden units,
and parameters. The full details are in Appendix F.

4Convolutions are designed to be shift equivariant, i.e. shifting the input is equivalent to shifting the output.
5E.g. a 3 × 3 convolutional ﬁlter on an n × n matrix has a Toeplitz weight matrix supported on diagonals −1, 0, 1, n − 1, n, n +

6A layer consisting of k in-channels and (cid:96) out-channels, each of which is connected by a weight matrix of class C, is the same

1, 2n − 1, . . . .

as a k × (cid:96) block matrix.

8

Table 3: Replacing a ﬁve-layer CNN consisting of convolutional channels, max pooling, and FC layers with two generic
LDR matrices results in only slight test accuracy decrease while containing fewer layers, hidden units, and parameters.
Rank (r) is in parentheses.

First hidden layer(s)

Last hidden layer Hidden units Parameters Test Acc.

3 Convolutional Channels (CC) FC
FC
3CC + Max Pool
FC
4CC + Max Pool

1573089
3072, 512
3072, 768, 512
393441
4096, 1024, 512 524588

Toeplitz-like (r = 16) channels Toeplitz-like (r = 16) 3072, 512
LDR-SD (r = 16)
LDR-SD (r = 16) channels
3072, 512
Toeplitz-like (r = 16) 3072, 512
Toeplitz-like (r = 48) matrix
3072, 512
LDR-SD (r = 16)
LDR-SD (r = 48) matrix

393216
417792
393216
405504

54.59
55.14
60.05

57.29
59.36
55.29
59.00

5.3 Generalization and sample complexity

Theorem 2 states that the theoretical sample complexity of neural networks with structured weight matrices
scales almost linearly in the total number of parameters, matching the results for networks with fully-connected
layers [4, 24]. As LDR matrices have far fewer parameters, the VC dimension bound for LDR networks
are correspondingly lower than that of general unstructured networks. Though the VC dimension bounds
are suﬃcient but not necessary for learnability, one might still expect to be able to learn over compressed
networks with fewer samples than over unstructured networks. We empirically investigate this result using
the same experimental setting as Table 1 and Figure 3. As shown in Table 12 (Appendix E), the structured
classes consistently have lower generalization error (measured by the diﬀerence between training and test
error) than the unstructured baseline.

Reducing sample complexity We investigate whether LDR models with learned displacement operators
require fewer samples to achieve the same test error, compared to unstructured weights, in both the single
hidden layer and CNN architectures. Tables 10 and 11 in Appendix E show our results. In the single hidden
layer architecture, when using only 25% of the training data the LDR-TD class exceeds the performance
of an unstructured model trained on the full MNIST-noise dataset. On the CNN model, only 50% of the
training data is suﬃcient for the LDR-TD to exceed the performance of an unstructured layer trained on the
full dataset.

5.4 Visualizing learned weights

×

∈

R784

Finally, we examine the actual structures that our models learn. Figure 4(a,b) shows the heat map of the
784 for the Toeplitz-like and LDR-SD classes, trained on MNIST-bg-rot with a
weight matrix W
single hidden layer model. As is convention, the input is ﬂattened to a vector in R784. The Toeplitz-like class
is unable to determine that the input is actually a 28
28 image instead of a vector. In contrast, LDR-SD
class is able to pick up regularity in the input, as the weight matrix displays grid-like periodicity of size 28.
Figure 4(c) reveals why the weight matrix displays this pattern. The equivariance interpretation (Section 4)
predicts that B should encode a meaningful transformation of the inputs. The entries of the learned subdiagonal
are in fact recovering a latent invariant of the 2D domain: when visualized as an image, the pixel intensities
correspond to how the inputs are centered in the dataset (Figure 4(d)). Figure 6 in Appendix E shows a
similar ﬁgure for the NORB dataset, which has smaller objects, and we found that the subdiagonal learns a
correspondingly smaller circle.

×

6 Conclusion

We generalize the class of low displacement rank matrices explored in machine learning by considering classes
of LDR matrices with displacement operators that can be learned from data. We show these matrices can
improve performance on downstream tasks compared to compression baselines and, on some tasks, general

9

(a) Toeplitz-like

(b) LDR-SD

(c) Subdiagonal of B

(d) Input examples

Figure 4: The learned weight matrices (a,b) of models trained on MNIST-bg-rot. Unlike the Toeplitz-like matrix,
the LDR-SD matrix displays grid-like periodicity corresponding to the 2D input. Figure (c) shows the values of the
subdiagonal of B, reshaped as an image. The size and location of the circle roughly corresponds to the location of
objects of interest in the 2D inputs. A similar centering phenomenon was found on the NORB dataset, shown in
Figure 6 in Appendix E.

unstructured weight layers. We hope this work inspires additional ways of using structure to achieve both
more compact and higher quality representations, especially for deep learning models, which are commonly
acknowledged to be overparameterized.

Acknowledgments

We thank Taco Cohen, Jared Dunnmon, Braden Hancock, Tatsunori Hashimoto, Fred Sala, Virginia Smith,
James Thomas, Mary Wootters, Paroma Varma, and Jian Zhang for helpful discussions and feedback.

We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865
(SDH), NIH under No. N000141712266 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and
CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore
Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture,
Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, and members
of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, and
VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or
endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.

References

[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the

IEEE International Conference on Computer Vision, pages 37–45. IEEE, 2015.

[2] Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio.
Unsupervised learning of invariant representations. Theor. Comput. Sci., 633(C):112–121, June 2016.
ISSN 0304-3975. doi: 10.1016/j.tcs.2015.06.048. URL https://doi.org/10.1016/j.tcs.2015.06.048.

[3] Martin Anthony and Peter L Bartlett. Neural network learning: theoretical foundations. Cambridge

University Press, 2009.

[4] Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems, pages 190–196, 1999.

[5] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

10

[6] Peter Bürgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity theory, volume

315. Springer Science & Business Media, 2013.

[7] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural
networks with the hashing trick. In Francis Bach and David Blei, editors, Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pages 2285–2294, Lille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/
chenc15.html.

[8] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An
exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the
IEEE International Conference on Computer Vision, pages 2857–2865, 2015.

[9] T.S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publica-

tions, 2011. ISBN 9780486479293. URL https://books.google.com/books?id=IkCJSQAACAAJ.

[10] Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear
time kernel expansions.
In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings
of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pages 2502–2510, New York, New York, USA, 20–22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/choromanski16.html.

[11] Dan C Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber.
Flexible, high performance convolutional neural networks for image classiﬁcation. In IJCAI Proceedings-
International Joint Conference on Artiﬁcial Intelligence, volume 22, page 1237. Barcelona, Spain, 2011.

[12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Conference

on Machine Learning, pages 2990–2999, 2016.

[13] Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hkbd5xZRb.

[14] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged
progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 1060–1079. SIAM, 2018.

[15] Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep

learning. In Advances in Neural Information Processing Systems, pages 2148–2156, 2013.

[16] Sander Dieleman, Jeﬀrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional
neural networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research,
pages 1889–1898, New York, New York, USA, 20–22 Jun 2016. PMLR. URL http://proceedings.mlr.
press/v48/dieleman16.html.

[17] Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian,
Yu Bai, Geng Yuan, et al. CirCNN: accelerating and compressing deep neural networks using block-
circulant weight matrices. In Proceedings of the 50th Annual IEEE/ACM International Symposium on
Microarchitecture, pages 395–408. ACM, 2017.

[18] Sebastian Egner and Markus Püschel. Automatic generation of fast discrete signal transforms. IEEE

Transactions on Signal Processing, 49(9):1992–2002, 2001.

[19] Sebastian Egner and Markus Püschel. Symmetry-based matrix factorization. Journal of Symbolic

Computation, 37(2):157–186, 2004.

[20] Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in Neural Information

Processing Systems, pages 2537–2545, 2014.

11

[21] C. Lee Giles and Tom Maxwell. Learning, invariance, and generalization in high-order neural networks.
Appl. Opt., 26(23):4972–4978, Dec 1987. doi: 10.1364/AO.26.004972. URL http://ao.osa.org/
abstract.cfm?URI=ao-26-23-4972.

[22] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic
diﬀerentiation. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, second edition,
2008. ISBN 0898716594, 9780898716597.

[23] Song Han, Jeﬀ Pool, John Tran, and William Dally. Learning both weights and connections for eﬃcient
neural network. In Advances in Neural Information Processing Systems, pages 1135–1143, 2015.

[24] Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise
linear neural networks. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on
Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1064–1068, Amsterdam,
Netherlands, 07–10 Jul 2017. PMLR. URL http://proceedings.mlr.press/v65/harvey17a.html.

[25] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. NIPS Deep

Learning Workshop, 2015.

[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer

networks. In Advances in Neural Information Processing Systems, pages 2017–2025, 2015.

[27] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations.

Journal of Mathematical Analysis and Applications, 68(2):395–407, 1979.

[28] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 2752–2760, 2018.
URL http://proceedings.mlr.press/v80/kondor18a.html.

[29] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s Thesis,

Department of Computer Science, University of Toronto, 2009.

[30] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical
evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th
International Conference on Machine Learning, ICML ’07, pages 473–480, New York, NY, USA, 2007.
ACM. ISBN 978-1-59593-793-3. doi: 10.1145/1273496.1273556. URL http://doi.acm.org/10.1145/
1273496.1273556.

[31] Quoc Le, Tamas Sarlos, and Alexander Smola. Fastfood - computing Hilbert space expansions in loglinear
time. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference
on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 244–252, Atlanta,
Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/le13.html.

[32] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of the IEEE International Conference on Computer
Vision, volume 2, pages II–104. IEEE, 2004.

[33] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance
and equivalence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 991–999, 2015.

[34] Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. Learning compact recurrent neural networks. In
Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages
5960–5964. IEEE, 2016.

[35] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector ﬁeld
networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 5058–5067,
2017.

12

[36] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?
id=Byj72udxe.

[37] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. ACDC: a structured eﬃcient

linear layer. In International Conference on Learning Representations, 2016.

[38] Samet Oymak. Learning compact neural networks with regularization. In Jennifer Dy and Andreas
Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 3966–3975, Stockholmsmässan, Stockholm, Sweden,
10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/oymak18a.html.

[39] Dipan K Pal and Marios Savvides. Non-parametric transformation networks.

arXiv preprint

arXiv:1801.04520, 2018.

Business Media, 2012.

[40] Victor Y Pan. Structured matrices and polynomials: uniﬁed superfast algorithms. Springer Science &

[41] Victor Y Pan and Xinmao Wang. Inversion of displacement operators. SIAM Journal on Matrix Analysis

and Applications, 24(3):660–677, 2003.

[42] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank
matrix factorization for deep neural network training with high-dimensional output targets. In Proceedings
of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659.
IEEE, 2013.

[43] Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant
descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2050–2057. IEEE, 2012.

[44] Valeria Simoncini. Computational methods for linear matrix equations. SIAM Review, 58(3):377–441,

2016.

[45] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep

learning. In Advances in Neural Information Processing Systems, pages 3088–3096, 2015.

[46] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant
classiﬁers. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on
Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages
1094–1103, Fort Lauderdale, FL, USA, 20–22 Apr 2017. PMLR. URL http://proceedings.mlr.press/
v54/sokolic17a.html.

[47] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

[48] Hugh E Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American

Mathematical Society, 133(1):167–178, 1968.

[49] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang.
Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pages
1476–1483, 2015.

[50] Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, and Bo Yuan. Theoretical properties for neural
networks with weight matrices of low displacement rank. In Doina Precup and Yee Whye Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 4082–4090, International Convention Centre, Sydney, Australia, 06–11
Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/zhao17b.html.

13

A Symbols and abbreviations

Table 4: Symbols and abbreviations used in this paper.

Symbol
LDR

Used For
low displacement rank

LDR-SD matrices with low displacement rank with respect to subdiagonal operators
LDR-TD matrices with low displacement rank with respect to tridiagonal operators
(A, B)
A,B[M]
r
(G, H)
Zf
(A, v) Krylov matrix, with ith column Aiv

displacement operators
Sylvester displacement, AM
(displacement) rank
parameters which deﬁne the rank r residual matrix GHT , where G, H
unit-f-circulant matrix, deﬁned as Zf =

e2, e3, ..., en, f e1

MB

−

∈

∇

Rn

r

×

r with respect to (A, B)

(cid:2)

(cid:3)

K

D

r
A,B
Φ
CC
FC

matrices of displacement rank
feature map
convolutional channels
fully-connected

≤

B Related work

Our study of the potential for structured matrices for compressing deep learning pipelines was motivated by
exciting work along these lines from Sindhwani et al. [45], the ﬁrst to suggest the use of low displacement
rank (LDR) matrices in deep learning. They speciﬁcally explored applications of the Toeplitz-like class, and
empirically show that this class is competitive against many other baselines for compressing neural networks
on image and speech domains. Toeplitz-like matrices were similarly found to be eﬀective at compressing RNN
and LSTM architectures on a voice search task [34]. Another special case of LDR matrices are the circulant
(or block-circulant) matrices, which have also been used for compressing CNNs [8]; more recently, these have
also been further developed and shown to achieve state-of-the-art results on FPGA and ASIC platforms [17].
Earlier works on compressing deep learning pipelines investigated the use of low-rank matrices [15, 42]—
perhaps the most canonical type of dense structured matrix—which are also encompassed by our framework,
as shown in Proposition 2. Outside of deep learning, Choromanski and Sindhwani [10] examined a structured
matrix class that includes Toeplitz-like, circulant, and Hankel matrices (which are all LDR matrices) in the
context of kernel approximation.

On the theoretical side, Zhao et al. [50] study properties of neural networks with LDR weight matrices,
proving results including a universal approximation property and error bounds. However, they retain
the standard paradigm of ﬁxing the displacement operators and varying the low-rank portion. Another
natural theoretical question that arises with these models is whether the resulting hypothesis class is still
eﬃciently learnable, especially when learning the structured class (as opposed to these previous ﬁxed classes).
Recently, Oymak [38] proved a Rademacher complexity bound for one layer neural networks with low-rank
weight matrices. To the best of our knowledge, Theorem 2 provides the ﬁrst sample complexity bounds for
neural networks with a broad class of structured weight matrices including low-rank, our LDR classes, and
other general structured matrices [14].

In Section 3 we suggest that the LDR representation enforces a natural notion of approximate equivariance
and satisﬁes closure properties that one would expect of equivariant representations. The study of equivariant
feature maps is of broad interest for constructing more eﬀective representations when known symmetries exist
in underlying data. Equivariant linear maps have long been used in algebraic signal processing to derive eﬃcient
transform algorithms [18, 19]. The fact that convolutional networks induce equivariant representations, and
the importance of this eﬀect on sample complexity and generalization, has been well-analyzed [2, 12, 21, 46].
Building upon the observation that convolutional ﬁlters are simply linear maps constructed to be translation

14

equivariant7, exciting recent progress has been made on crafting representations invariant to more complex
symmetries such as the spherical rotation group [13] and egomotions [1]. Generally, however, underlying
assumptions are made about the domain and invariances present in order to construct feature maps for
each application. A few works have explored the possibility of learning invariances automatically from
data, and design deep architectures that are in principle capable of modeling and learning more general
symmetries [20, 26, 39].

C Properties of displacement rank

Displacement rank has traditionally been used to describe the Toeplitz-like, Hankel-like, Vandermonde-like,
and Cauchy-like matrices, which are ubiquitous in disciplines such as engineering, coding theory, and computer
algebra. Their associated displacement representations are shown in Table 5.

Table 5: Traditional classes of structured matrices analyzed with displacement rank. In the Vandermonde and Cauchy
cases, the displacement operators are parameterized by v ∈ Rn and s, t ∈ Rn respectively.

B
Structured Matrix M A
Z
Z1
Toeplitz
−
ZT
Z1
Hankel
0
diag(v) Z0
Vandermonde
Cauchy
diag(s)

1

diag(t)

2
2
1
1

≤
≤
≤
≤

Displacement Rank r

Proof of Proposition 1. The following identities are easily veriﬁed:

Transpose

Inverse

Sum

Product

Block The remainder

is the block matrix

∇BT ,AT MT =

(

−

∇

A,BM)T

B,AM−

1 =

M−

1 (

1
A,BM) M−

−

∇

A,B(M + N) =

A,BM +

A,BN

∇

∇

A,CMN = (

A,BM)N + M (

B,CN)

∇

∇

diag(A1, . . . , Ak)M

M diag(B1, . . . , B(cid:96))

∇

∇

∇

−

(cid:96).
This is the sum of k(cid:96) matrices of rank r and thus has rank rk(cid:96).

Ai,Bj Mij)1

(
∇

j
≤

i
≤

k,1

≤

≤

Corollary 1. A k
Toeplitz-like with displacement rank rk(cid:96) + 2k + 2(cid:96).

×

(cid:96) block matrix M, where each block is a Toeplitz-like matrix of displacement rank r, is

Proof. Apply Proposition (d) where each Ak, Bk has the form Zf . Let A = diag(A1, . . . , Ak) and B =
diag(B1, . . . , B(cid:96)). Note that A and Z1 (of the same size as A) diﬀer only in 2k entries, and similarly B and
Z

1 diﬀer in 2(cid:96) entries. Since an s-sparse matrix also has rank at most s,

−

−
has rank at most rk(cid:96) + 2k + 2(cid:96).

Z1M

MZ

1 = AM

−

MB + (Z1 −

−

A)M

M(Z

−

B)

1 −
−

7Shifting the input to a convolutional feature map is the same as shifting the output.

15

Proof of Proposition 3. First consider the rank one case, R = ghT . It is easy to check that
will only be non-empty in the ﬁrst column, hence
Proposition 1(a) implies
K
The rank r case follows directly from Theorem 1(b).

1
ZT ,B. Then Theorem 1(c) implies that

1
A−1,ZT . Similarly,

K
(A, g)

(BT , h)T

(A, g)

∈ D

∈ D

K

K

K

(BT , h)

K

(A, g)
∇A−1,ZT
1
BT ,Z and
∈ D
2
(B, h)T
A,B.

∈ D

C.1 Expressiveness

Expanding on the claim in Section 3, we formally show that these structured matrices are contained in the
tridiagonal (plus corners) LDR class. This includes several types previously used in similar works.

A storage Av compute

LDR-TD

O(nr) O(nr log2 n)

Toeplitz-like

O(nr) O(nr log n)

Circulant

O(n) O(n log n)

Convolutional ﬁlters

O(n) O(n log n)

Low-rank

O(nr) O(nr)

Orthogonal polynomial
transforms
O(n) O(n log2 n)

Figure 5: Our proposed LDR-TD structured matrix class contains a number of other classes including Toeplitz-
like [45] (and other classic displacement types, such as Hankel-like, Vandermonde-like, and Cauchy-like), low-rank [15],
circulant [8], standard convolutional ﬁlters, and orthogonal polynomial transforms, including the Discrete Fourier and
Cosine Transforms. Captions for each class show storage cost and operation count for matrix-vector multiplication.

Classic displacement rank The Toeplitz-like, Hankel-like, Vandermonde-like, and Cauchy-like matrices
where D is the set of diagonal matrices [40].
are deﬁned as having LDR with respect to A, B
(For example, [45] deﬁnes the Toeplitz-like matrices as (A, B) = (Z1, Z
1).) All of these operator choices are
only non-zero along the three main diagonals or opposite corners, and hence these classic displacement types
belong to the LDR-TD class.

Zf , ZT

f , D

∈ {

}

−

Low-rank A rank r matrix R trivially has displacement rank r with respect to (A, B) = (I, 0). It also has
displacement rank r with respect to (A, B) = (Z1, 0), since Z1 is full rank (it is a permutation matrix) and so
rank(Z1R) = rank(R) = r. Thus low-rank matrices are contained in both the LDR-TD and LDR-SD classes.

Orthogonal polynomial transforms The polynomial transform matrix M with respect to polynomials
1) is deﬁned by Mij = pi(λj). When the pi(X) are a family of
(p0(X), . . . , pm

1(X)) and nodes (λ0, . . . , λn

−

−

16

orthogonal polynomials, it is called an orthogonal polynomial transform.

Proposition 4. Orthogonal polynomial transforms have displacement rank 1 with respect to tridiagonal
operators.

Proof. Every orthogonal polynomial family satisﬁes a three-term recurrence

pi+1(X) = (aiX + bi)pi(X) + cipi

1(X)

−

(4)

where ai > 0 [9]. Let M be an orthogonal polynomial transform with respect to polynomials (pi(X))0
and nodes (λj)0

j<n. Deﬁne the tridiagonal and diagonal matrix

≤

i<m

≤

0
0
0
...

A =

0
1
a1
b1
a1

b0
a0
−
c1
a1 −
−
0
−
...
0

1
a0
b1
a1
c1
a1 −
...
0

0

0

. . .
. . .
. . .
. . .
. . .

. . .

...
0

0

−














B = diag(λ0, . . . , λn

1).

0
0
0
...














bm−2
am−2
cm−1
am−1 −

1
am−2
bm−1
am−1

−

−

For any i

0, . . . , m

and any j, consider entry ij of AM

MB. This is

∈ {

2
}

−

1
ai

[

−

cipi

1(λj)

bipi(λj) + pi+1(λj)

λjpi(λj)]

−

−

which is 0 by plugging λj into (4).

Thus

A,BM can only non-zero in the last row, so M

−

−

1
A,B.

∈ D

Fourier-like transforms Orthogonal polynomial transforms include many special cases. We single out
the Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT) for their ubiquity.

The N

N DFT and DCT (type II) are deﬁned as matrix multiplication by the matrices

∇

×

F =

2π ij
N

e−

C =

cos

(cid:16)

(cid:16)

π
N

(cid:104)

ij

(cid:17)
i(j + 1/2)

ij

(cid:105)(cid:17)

Ti

cos

(j +

π
N

1
2

,

)
(cid:21)(cid:19)(cid:19)ij

Tn(X) = cos(n arccos x).

respectively.

The former is a special type of Vandermonde matrix, which were already shown to be in LDR-TD. Also
j)ij are themselves orthogonal polynomial transforms with pi(X) = X i.

note that Vandermonde matrices (λi
The latter can be written as

(cid:18)
where Ti are the Chebyshev polynomials (of the ﬁrst kind) deﬁned such that

(cid:18)

(cid:20)

Thus this is an orthogonal polynomial transform with respect to the Chebyshev polynomials.

Other constructions From these basic building blocks, interesting constructions belonging to LDR-TD can
be found via the closure properties. For example, several types of structured layers inspired by convolutions,
including Toeplitz [45], circulant [8] and block-circulant [17] matrices, are special instances of Toeplitz-like
matrices. We also point out a more sophisticated layer [37] in the tridiagonal LDR class, which requires more
deliberate use of Proposition 1 to show.

17

Proposition 5. The ACDC−
Transform [37], has displacement rank 2 with respect to tridiagonal operators.

1 layer, where A, D are diagonal matrices and C is the Discrete Cosine

Proof. Let T, Λ be the tridiagonal and diagonal matrix such that C
also tridiagonal. Note that A
application of the inverse closure rule yields C

∈ D
0
S,T by construction. Also note that D

∈ D

1
T,Λ. Deﬁne S = ATA−

1, which is
0
Λ,Λ since Λ is diagonal. An

∈ D
1
Λ,T. Finally, the product closure property implies that

∈ D

ACDC−

1

2
S,T.

∈ D

C.2 Algorithm derivation and details

De Sa et al. recently showed that a very general class of LDR matrices have asymptotically fast matrix-vector
multiplication algorithms [14]. However, parts of the argument are left to existential results. Building
upon De Sa et al. [14], we derive a simpliﬁed and self-contained algorithm for multiplication by LDR matrices
with subdiagonal operators.

Since these matrices can be represented by the Krylov product formula (2), it suﬃces to show multiplication

algorithms separately for matrix-vector multiplication by

(A, v)T and

(A, v).

K

K

Krylov transpose multiplication Let A
possible non-zero entries. Let u, v
n is a power of 2.

∈

Following [14], the vector

Rn, we wish to compute the product

n be a subdiagonal matrix, i.e. Ai+1,i are the only
(A, v)T u. For simplicity assume

∈

×

Rn

is the coeﬃcient vector of the polynomial in X

(cid:2)

uT

K

(A, v) =

uv uAv . . . uAn

1v

−

K

(cid:3)

uv + uAvX +

+ uAn

1vX n

−

−

1

∞

=

i=0
(cid:88)
= u(I

· · ·

uAiX iv

AX)−

1v,

−

where we use the observation that An = 0.

By partitioning A into n/2

n/2 blocks, it has the form

×

matrices of half the size, a is a scalar, and ei are basis vectors. Let also u0, u1 ∈
the ﬁrst and second halves of u, v.

A0
ae1eT
(cid:20)

0
n/2 A1

, where A0, A1 are subdiagonal
(cid:21)

Rn/2 denote

Rn/2, v0, v1 ∈

By block matrix inversion for triangular matrices

, this can be written

A 0
C B

1

−

=

(cid:20)

(cid:21)

(cid:20)

−

1
A−
1CA−

0
1 B−

1

B−

(cid:21)

as

uT (I

AX)−

1v =

0 uT
uT
1

−

= uT

0 (I

A0X)−

(cid:2)
1v0 + uT

(cid:3)
1 (I

A1X)−

1
n/2X)(I

(I
1(

A0X)−
−
ae1eT
−
1v1 + aX

uT

1 (I

−
A1X)−

−

(I

(cid:20)

−
−
A1X)−

−

A0X)−

1

(I

0
1
A1X)−

1e1

eT
n/2(I

A0X)−

−

−

Therefore uT (I

AX)−

1v can be computed from

(cid:0)

(cid:1) (cid:16)

v0
v1(cid:21)

(cid:17)

(cid:21) (cid:20)
1v0

−

−

uT
0 (I
uT
1 (I

−

−

A0X)−
A1X)−

1v0
1e1

uT
1 (I
−
eT
n/2(I

−

A1X)−

A0X)−

1v1
1v0

with an additional polynomial multiplication and 3 polynomial addition/subtractions.

18

A modiﬁcation of this reduction shows that the 2

2 matrix of polynomials

×

can be computed from

u0 en

T

(I

1
A0X)−

v0 e1

−

u en

T

(I

AX)−

1

v e1

−

(cid:2)

(cid:3)

u1 en

T

(I

A1X)−

1

v1 e1

(cid:2)

−

(cid:3)

(cid:2)

with an additional constant number of polynomial multiplications and additions.

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:3)

The complete recursive algorithm is provided in Algorithm 1, where subroutine R computes the above

matrix of polynomials. For convenience, Algorithm 1 uses Python indexing notation.

∈

n, u, v

×

Rn)

∈

←

s
return R(s, u, v)

Algorithm 1 Krylov Transpose (Recursive)
1: function Krylov_Transpose(A
Rn
subdiagonal(A)
2:
3:
4: end function
5: function R(s
S0 ←
6:
S1 ←
7:
L
←

Rn
−
∈
R(s[0 : n/2
R(s[n/2 : n

1, u, v)

s[n/2

−

8:

·

−
−

1], u[0 : n/2], v[0 : n/2])
1], u[n/2 : n], v[n/2 : n])
S0[1, 0] S1[0, 1]
S1[0, 1]
S0[1, 0] S1[1, 1]
S1[1, 1]
(cid:20)

S0[1, 1]
S0[1, 1]
L[0, 0] + S0[0, 0] + S1[0, 0] L[0, 1] + S0[0, 1]

1]X

·
·

·
·

L[1, 0] + S1[1, 0]

L[1, 1]

9:

return

10: end function

(cid:20)

(cid:21)

(cid:21)

A polynomial multiplication of degree m in Step 8 can be computed as a convolution of size 2m. This
reduces to two Fast Fourier Transform (FFT) calls, an elementwise multiplication in the frequency domain,
and an inverse FFT. The total number of calls can be further reduced to 4 FFTs and 4 inverse FFTs.

Algorithm 1 deﬁnes a recursion tree, and in practice we compute this breadth ﬁrst bottom-up to avoid
recursive overhead. This also allows the FFT operations to be batched and computed in parallel. Thus the
d-th layer of the algorithm (starting from the leaves) performs n

2d FFT computations of size 2d+1.

This completes the proof of Theorem 1.
We note several optimizations that are useful for implementation:

n (I

1. The polynomial eT

1e1 for i = 0, 1 are in fact monomials, which can be shown inductively. To
use the notation of Algorithm 1, S0[1, 1], S1[1, 1], and L[1, 1] are monomials. Therefore the polynomial
multiplication with S0[1, 1] and S1[1, 1] can be done directly by coeﬃcient-wise multiplication instead
of using the FFT.

AiX)−

−

0 (I

A0X)−

2. We don’t need the polynomials uT

1v1 separately, we only need their
1v0 and uT
sum. To use the notation of Algorithm 1, we don’t need S0[0, 0] and S1[0, 0] separately, we only need
their sum. In fact, by tracing the algorithm from the leaves of the recursion tree to the root, we see
that across the same depth d, only the sum of the terms S0[0, 0] + S1[0, 0] of the n/2d subproblems
is required, not the individual terms. Therefore, when computing polynomial multiplication at depth
d, we can perform the FFT of size 2d+1 and the pointwise multiplication, then sum across the n/2d
problems before performing the inverse FFT of size 2d+1.

A1X)−

1 (I

−

−

×

b and there are r vectors v1, . . . , vr, and we wish to compute

Eﬃcient batching with respect to input vector and rank. Optimization 2 is especially important
for eﬃcient multiplication with respect to batched input u and higher rank v. Suppose that u has size
(A, vi)T u. Naively performing
n
Algorithm 1 on each of the b inputs and each of the r vectors then summing the results, takes O(brn log2 n)
time. The bottleneck of the algorithm is the polynomial multiplication S1[0, 1]
S0[1, 0]. At depth d, there are
n/2d subproblems, and in each of those, S1[0, 1] consists of b polynomials of degree at most 2d, while S0[1, 0]
consists of r polynomials of degree at most 2d. If we apply optimization 2, we ﬁrst perform the FFT of size
2d+1 on these (b + r)n/2d polynomials, then pointwise multiplication in the frequency domain to get brn/2d
vectors of size 2d+1 each. Next we sum across the n/2d problems to get br vectors, before performing the

r
i=1 K

(cid:80)

·

19

inverse FFT of size 2d+1 to these br vectors. The summing step allows us to reduce the number of inverse
FFTs from brn/2d to br. The total running time over all depth d is then O((b + r)n log2 n + brn log n) instead
of O(brn log2 n).

Krylov multiplication De Sa et al. [14] do not provide explicit algorithms for the more complicated problem
of multiplication by
(A, v), instead justifying the existence of such an algorithm with the transposition
principle. Traditional proofs of the transposition principle use circuit based arguments involving reversing
arrows in the arithmetic circuit deﬁning the algorithm’s computation graph [6].

K

Here we show an alternative simple way to implement the transpose algorithm using any automatic
diﬀerentiation (AD) implementation, which all modern deep learning frameworks include. AD states that for
any computation, its derivative can be computed with only a constant factor more operations [22].

Proposition 6 (Transposition Principle). If the matrix M
any vector in N operations, then MT admits matrix-vector multiplication in O(N + n) operations.

n admits matrix-vector multiplication by

∈

×

Rn

Proof. Note that for any x and y, the scalar yT Mx = y

(Mx) can be computed in N + n operations.

The statement follows from applying reverse-mode AD to compute MT y = ∂
Additionally, the algorithm can be optimized by choosing x = 0 to construct the forward graph.

∂x (yT Mx).

·

To perform the optimization mentioned in Proposition 6, and avoid needing second-order derivatives when
computing backprop for gradient descent, we provide an explicit implementation of non-transpose Krylov
multiplication

(A, v). This was found by using Proposition 6 to hand-diﬀerentiate Algorithm 1.

Finally, we comment on multiplication by the LDR-TD class. Desa et al.[14] showed that these matrices
also have asymptotically eﬃcient multiplication algorithms, of the order O(rn log3 n) operations. However,
these algorithms are even more complicated and involve operations such as inverting matrices of polynomials
in a modulus. Practical algorithms for this class similar to the one we provide for LDR-SD matrices require
more work to derive.

K

C.3 Displacement rank and equivariance

Here we discuss in more detail the connection between LDR and equivariance. One line of work [12, 28] has
used the group representation theory formalization of equivariant maps, in which the model is equivariant
to a set of transformations which form a group G. Each transformation g
G acts on an input x via a
corresponding linear map Tg. For example, elements of the rotation group in two and three dimensions,
SO(2) and SO(3), can be represented by 2D and 3D rotation matrices respectively. Formally, a feature map
Φ is equivariant if it satisﬁes

∈

Φ(Tgx) = T (cid:48)g(Φ(x))

(5)

for representations T, T (cid:48) of G [12, 28]. This means that perturbing the input x by a transformation g
G
before computing the map Φ is equivalent to ﬁrst ﬁnding the features Φ and then applying the transformation.
Group equivariant convolutional neural networks (G-CNNs) are a particular realization where Φ has a speciﬁc
Rd, and T, T (cid:48) are chosen in advance [12]. We use the notation Φ to distinguish our setting, where
form G
the input x is ﬁnite dimensional and Φ is linear.

→

∈

Proposition 7. If Φ has displacement rank 0 with respect to invertible A, B, then Φ is equivariant as deﬁned
by (5).

Proof. Note that if AΦ = ΦB for invertible matrices A, B (i.e. if a matrix Φ has displacement rank 0 with
Z. Also note that the set of powers of any invertible
respect to A and B), then AiΦ = ΦBi also holds for i
∈
matrix forms a cyclic group, where the group operation is multiplication. The statement follows directly from
this fact, where the group G is Z, and the representations T and T (cid:48) of G correspond to the cyclic groups
generated by A and B, respectively consisting of Ai and Bi for all i

Z.

∈

More generally, a feature map Φ satisfying (5) for a set of generators S =

is equivariant with respect
to the free group generated by S. Proposition 7 follows from the speciﬁc case of a single generator, i.e.
S =

gi}
{

.

1
}
{

20

D Bound on VC dimension and sample complexity

In this section we upper bound the VC dimension of a neural network where all the weight matrices are
LDR matrices and the activation functions are piecewise polynomials. In particular, the VC dimension is
almost linear in the number of parameters, which is much smaller than the VC dimension of a network with
unstructured layers. The bound on the VC dimension allows us to bound the sample complexity to learn
an LDR network that performs well among LDR networks. This formalizes the intuition that compressed
parameterization reduces the complexity of the class.

Neural network model Consider a neural network architecture with W parameters, arranged in L layers.
Each layer l, has output dimension nl, where n0 is the dimension of the input data and the output dimension
Rnl be the input to the l-th layer. The input to the (l + 1)-th layer is
is nL = 1. For l = 1, . . . , L, let il ∈
exactly the output of the l-th layer. The activation functions φl are piecewise polynomials with at most p + 1
Rn1, and the output of the
pieces and degree at most k
1. The input to the ﬁrst layer is the data i1 = x
last layer is a real number iL+1 ∈
il+1 = φl(Mlil + bl)

R. The intermediate layer computation has the form:

(applied elementwise),

Rnl−1

Rnl .

≥

∈

×

where Ml ∈

nl , bl ∈

We assume the activation function of the ﬁnal layer is the identity.

Each weight matrix Ml is deﬁned through some set of parameters; for example, traditional unconstrained
matrices are parametrized by their entries, and our formulation (2) is parametrized by the entries of some
operator matrices Al, Bl and low-rank matrix GlHT
l . We collectively refer to all the parameters of the neural
network (including the biases bl) as θ

RW , where W is the number of parameters.

∈

Rn

Bounding the polynomial degree The crux of the proof of the VC dimension bound is that the entries
m are polynomials in terms of the entries of its parameters (A, B, G, and H). of total degree at
of M
most c1mc2 for universal constants c1, c2. This allows us to bound the total degree of all of the layers and
apply Warren’s lemma to bound the VC dimension.

∈

×

We will ﬁrst show this for the speciﬁc class of matrices that we use, where the matrix M is deﬁned through

equation (2).

Lemma 1. Suppose that M

Rm

×

m is deﬁned as

∈

M =

(A, gi)

(BT , hi).

K

r

K

i=1
(cid:88)

Then the entries of M are polynomials of the entries of A, B, G, H with total degree at most 2m.

K

gi Agi

. . . Am

(A, gi) =

1gi
, and each entry of Ak is a polynomial of the entries of A
Proof. Since
−
(A, gi) are polynomials of the entries of A and gi with total
with total degree at most k, the entries of
(cid:3)
K
(BT , hi) are polynomials of the entries of B and hi with total
degree at most m. Similarly the entries of
K
(BT , hi) are polynomials of the entries of A, B, G, H with
degree at most m. Hence the entries of
(A, gi)
total degree at most 2m. We then conclude that the entries of M are polynomials of the entries of A, B, G, H
with total degree at most 2m.

K

K

(cid:2)

Lemma 2. Suppose that the LDR weight matrices Ml of a neural network have entries that are polynomials
in their parameters with total degree at most c1nc2
0. For a ﬁxed data
l
−
point x, at the l-th layer of a neural network with LDR weight matrices, each entry of Mlil + bl is a piecewise
polynomial of the network parameters θ, with total degree at most dl, where

1 for some universal constants c1, c2 ≥

Thus entries of the output φl(Mlil + bl) are piecewise polynomials of θ with total degree at most kdl. Moreover,

d0 = 0,

dl = kdl

1 + c1nc2
l
−

1

−

for l = 1, . . . , L.

c1kl

−

1

dl ≤

nc2
j .

1

l

−

j=0
(cid:88)

21

(6)

By Lemma 1, Lemma 2 applies to the speciﬁc class of matrices that we use, for c1 = 2 and c2 = 1. As we

will see, it also applies to very general classes of structured matrices.

Proof. We induct on l. For l = 1, since i1 = x is ﬁxed, the entries of M1 are polynomials of θ of degree
at most c1nc2
0 , and so the entries of M1i1 + b1 are polynomials of θ with total degree at most d1 = c1nc2
0 .
As φ is a piecewise polynomials of degree at most k, each entry the output φ1(M1i1 + b1) is a piecewise
polynomial of θ with total degree at most 2n0k. The bound (6) holds trivially.

Suppose that the lemma is true for some l

θ with total degree at most kdl
the entries of Mlil + bl are piecewise polynomials of θ with total degree at most dl = kdl
φl(Mlil + bl) have entries that are piecewise polynomials of θ with total degree at most kdl.

1. Since the entries of il are piecewise polynomials of
1 and entries of Ml are polynomials of θ with total degree at most c1nc2
1,
l
−
1 + c1nc2
−
1. Thus
l
−

−

≥

1

−

We can bound

dl = kdl

1 + c1nc2
l
−

−

1 ≤

kc1kl

−

2

j + c1nc2
nc2

l

c1kl

−

1

nc2
j ,

1 ≤

−

1

l

−

j=0
(cid:88)

2

l

−

j=0
(cid:88)
c1kl

where we have used the fact that k

1, so c1nc2
l
−

1 ≤

≥

−

1nc2
l
−

1. This concludes the proof.

Bounding the VC dimension Now we are ready to bound the VC dimension of the neural network.

Theorem 3. For input x
F
. Let Wl be
be the class of functions
}
the number of parameters up to layer l (i.e., the total number of parameters in layer 1, 2, . . . , l). Deﬁne the
eﬀective depth as

RW , let f (x, θ) denote the output of the network. Let

and parameter θ
RW
f (x, θ) : θ

∈
. Denote sign
}

sign f (x, θ) : θ

∈ X
x
→

x
{

RW

:=

→

F

∈

∈

{

¯L :=

1
W

L

(cid:88)l=1

Wl,

U :=

nl.

L

(cid:88)l=0

and the total number of computation units (including the input dimension) as

Then

In particular, if k = 1 (corresponding to piecewise linear networks) then

VCdim(sign

) = O( ¯LW log(pU ) + ¯LLW log k).

VCdim(sign

) = O( ¯LW log(pU )) = O(LW log W ).

F

F

We adapt the proof of the upper bound from Bartlett et al. [4], Harvey et al. [24]. The main technical
tool is Warren’s lemma [48], which bounds the growth function of a set of polynomials. We state a slightly
improved form here from Anthony and Bartlett [3, Theorem 8.3].

Lemma 3. Let p1, . . . , pm be polynomials of degree at most d in n

m variables. Deﬁne

≤
(sign(p1(x)), . . . , sign(pm(x)) : x

K :=

|{

Rn

,
}|

∈

i.e., K is the number of possible sign vectors given by the polynomials. Then K

2(2emd/n)n.

Proof of Theorem 3. Fixed some large integer m and some inputs x1, . . . , xm. We want to bound the number
of sign patterns that the neural network can output for the set of input x1, . . . , xm:

≤

RW

∈

.

}

(cid:12)
(cid:12)

K :=

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

{

(cid:12)
(cid:12)

22

We want to partition the parameter space RW so that for a ﬁxed xj, the output f (xj, θ) is a polynomial
on each region in the partition. Then we can apply Warren’s lemma to bound the number of sign patterns.
Indeed, for any partition

of the parameter space RW , we have

=

P1, . . . , PN }
{

S

K

≤

N

|{

j=1
(cid:88)

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

.

Pj}|

∈

(7)

We construct the partitions iteratively layer by layer, through a sequence

S0,

S1, . . . ,

SL

−

1 of successive

reﬁnements, satisfying two properties:

1.

|S0|

= 1 and for each 1

l

L

1,

≤

≤

−

|Sl| ≤ |Sl

1|
−

2

2empnldl
Wl

Wl

,

(cid:18)
where nl is the dimension of the output of the l-th layer, dl is the bound on the total degree of Mlil + bl
as piecewise polynomials of θ as deﬁned in Lemma 2, and Wl is the number of parameters up to layer l
(i.e., the total number of parameters in layer 1, 2, . . . , l).

(cid:19)

2. For each l = 0, . . . , L

Sl, for each ﬁxed data point xj (with j = 1, . . . , m),
the entries of the output φl(Mlil + bl) when restricted to S are polynomials of θ with total degree at
most kdl

1, for each element S of

1.

−

−

We can deﬁne

are polynomials of θ of degree d0 = 0.
Suppose that we have constructed
∈ Sl

S0 = RW , which satisﬁes property 2, since at layer 1, the entries of i1 = xj (for ﬁxed xj)
[m], and
1, and we want to deﬁne
S0, . . . ,
1, let ph,xj ,S(θ) = (Mlil + bl)h|S be the h-th entry of Mlil + bl restricted to the region S. By the
1, the entries of il when restricted to S are polynomials of θ of total
1. Thus by Lemma 2, the entries of Mlil + bl when restricted to S are polynomials of θ

Sl. For any h

S
inductive hypothesis, for each S
degree at most kdl
with total degree at most kdl

1 = dl, and depends on at most Wl many variables.

[nl], j

Sl

∈

∈

−

−

−

∈ Sl
−
1 + c1nc2
l
−

−

Since the activation function is piecewise polynomial with at most p pieces, let

t1, . . . , tp}

{

be the set of

breakpoints. For any ﬁxed S

1, by Lemma 3, the polynomials

can have at most

∈ Sl

−

(cid:8)

ph,xj ,S(θ)

ti : h

[nl], j

[m], i

[p]

−

∈

∈

∈

(cid:9)

Π := 2

2e(nlmp)dl
Wl

(cid:18)

Wl

(cid:19)

∈

Fix some S(cid:48)

RW . We can then partition RW into this many regions so that within each
distinct sign patterns when θ
region, all these polynomials have the same signs. Intersecting all these regions with S yields a partition of S
Sl that satisﬁes the property 1.
into at most Π subregions. Applying this for all S
∈ Sl
∈ Sn. When θ is restricted to S(cid:48), by construction, all the polynomials
ti : h

∈
have the same sign. This means that the entries of Mlil + bl lie between two breakpoints of the activation
function, and so the entries of the output φl(Mlil + bl) are ﬁxed polynomials in Wl variables of degree at
most kdl.

1 gives a partition

By this recursive construction,
−
any input xj is a ﬁxed polynomial of θ
the activation function of the ﬁnal layer is the identity). Hence we can apply Lemma 3 again:

1 is a partition of RW such that for S
1 the network output for
∈ SL
1 + c1nc2
1 = dL (recall that we assume
L
−
−

S of degree at most kdL

ph,xj ,S(θ)

[nl], j

[m], i

SL

[p]

−

∈

∈

∈

(cid:8)

(cid:9)

−

−

(sign f (x1, θ), . . . , sign f (xm, θ)) : θ

S

|{

∈

}| ≤

2emkdL

WL (cid:19)

2

(cid:18)

WL

.

23

By property 1, we can bound the size of

1:

SL

−

|SL| ≤

L

1

−

2

(cid:89)l=1

(cid:18)

2emnlpdl
Wl

Wl

.

(cid:19)

L

2

2empnldl
Wl

Wl

.

K

≤

Combining the two bounds along with equation (7) yields

(cid:19)
We can take logarithm and apply Jensen’s inequality, with ¯W :=

(cid:89)l=1

(cid:18)

L
l=1 Wl:

(cid:80)

log2 K

L +

Wl log2

2empnldl
Wl

L

≤

(cid:88)l=1
= L + ¯W

L

(cid:88)l=1
L + ¯W log2

≤

= L + ¯W log2

Wl
¯W

log2

2empnldl
Wl

L

(cid:32)

(cid:88)l=1
2emp

Wl
¯W

2empnldl
Wl

(cid:33)

L
l=1 nldl

.

¯W
(cid:80)

(Jensen’s inequality)

We can bound

nldl using the bound on dl from Lemma 2:

(cid:80)

L

L

nldl ≤

(cid:88)l=1

(cid:88)l=1
U . Thus

≤

1

l

−

j=0
(cid:88)

nc2
j ≤

where we used the fact that L

nlc1kl

−

1

LU c1kL
−

1U c2

c1U c2+2kL,

≤

log2 K

≤

L + ¯W log2

2c1empU 2+c2 kL
¯W

.

To bound the VC-dimension, recall that by deﬁnition, if VCdim(sign

) = m then exists m data points
x1, . . . , xm such that the output of the model can have 2n sign patterns. The bound on log2 K then implies

F

VCdim(sign

)

F

≤

L + ¯W log2

We then use Lemma 4 below, noting that 2c1epU 2+c2kL

2c1epU 2+c2kLVCdim(sign
¯W
16, to conclude that

)

.

F

≥

L + ¯W log2(2c1epU 2+c2 kL log2(2c1epU 2+c2kL)) = O( ¯LW log(pU ) + ¯LLW log k),

VCdim(sign

)

≤
completing the proof.

F

A bound on the VC dimension immediate yields a bound on the sample complexity of learning from this

class of neural networks with LDR matrices [47].

Corollary 2. The class of neural network with LDR matrices as weights and piecewise linear activation is
((cid:15), δ)-PAC-learnable with a sample of size

Since the number of parameters W is around the square root of the number of parameters of a network
with unstructured layers (assuming ﬁxed rank of the LDR matrices), the sample complexity of LDR networks
is much smaller than that of general unstructured networks.

Lemma 4 (Lemma 16 of [24]). Suppose that 2m
m

t + w log2(2r log2 r).

≤

≤

2t(mr/w)w for some r

16 and m

≥

w

t

≥

≥

≥

0. Then,

LW log W + log 1
δ
(cid:15)

.

(cid:19)

O

(cid:18)

24

F

Extension to rational functions. We now show that Theorem 3 holds for matrices where the entries are
rational functions—rather than polynomials—of its parameters, incurring only a constant in the bound. To
deﬁne the function class sign

, we account for the possibility of poles by deﬁning sign(a/0) = 0.

We only need to check that Lemma 2 and Lemma 3 still hold when polynomials are replaced by rational
.
functions everywhere, and the degree of a rational function is deﬁned as the usual deg(a/b) = max
deg a, deg b
}
{
To show Lemma 2 still holds, it suﬃces that the compositional degree bound deg(f
deg(f ) deg(g) holds
for rational functions f, g, just as in the polynomial case. To show Lemma 3 in the case when pi = ai/bi
are rational functions, we note that sign(pi(x)) = sign(ai(x)bi(x)), and furthermore deg(aibi)
2 deg(pi).
Appealing to the polynomial version of Lemma 3 shows that it holds in the rational function setting with
2(4emd/n)n. This gets converted to a constant factor in the result of
a slightly weaker upper bound K
Theorem 3.

g)

≤

≤

≤

◦

Next, we extend Lemma 1 by showing that generic LDR matrices have entries which are rational functions
of their parameters. This immediately lets us conclude that neural networks built from any LDR matrices
satisfy the VC dimension bounds of Theorem 3.

Lemma 5. If M
entries of A, B, G, H with total degree at most c1mc2 for some universal constants c1, c2 > 0.

MB = GHT , then the entries of M are rational functions of the

m satisﬁes AM

−

∈

×

Rm

Proof. The vectorization of the Sylvester equation AM
⊗
where vec denotes the vectorization operation by stacking a matrix’s columns, and
product. Note that the entries of N−
n in the entries of N, and R = GH(cid:62) has degree 2 in the entries of G, H. Therefore the entries of

I) vec(M) = vec(R),
is the Kronecker
⊗
n are rational functions with degree

1 for an arbitrary matrix N

MB = R is (I

B(cid:62)

Rn

A

⊗

−

−

∈

×

have degree n2 + 2 in the entries of A, B, G, H.

vec(M) = (I

A

⊗

−

B(cid:62)

⊗

I)−

1 vec(R)

Note that many other classes of matrices satisfy this lemma. For example, a large class of matrices
satisfying a property called low recurrence width was recently introduced as a way of generalizing many known
structured matrices [14]. The low recurrence width matrices are explicitly deﬁned through a polynomial
recurrence and satisfy the bounded degree condition. Additionally, Lemma 5 holds when the parameters
A, B themselves are structured matrices with entries having polynomial degree in terms of some parameters.
This includes the case when they are quasiseparable matrices, the most general class of LDR previously
analyzed [14].

E Additional results

E.1 Additional baselines and comparisons at multiple budgets

In Tables 6 and 7 we compare to baselines at parameter budgets corresponding to both the LDR-TD and
LDR-SD classes in the SHL and CNN models. In Tables 8 and 9, we also compare to two additional baselines,
network pruning [23] and a baseline used in [7], in which the number of hidden units is reduced to meet
the parameter budget. We refer to this baseline as RHU ("reduced hidden units"). We show consistent
improvements of LDR-SD over both methods at several budgets. We note that unlike the structured matrix
methods which provide compression beneﬁts during both training and inference, pruning requires ﬁrst training
the original model, followed by retraining with a ﬁxed sparsity pattern.

E.2 Sample complexity and generalization

As shown in Tables 10 and 11, we investigated how the performance of the structured and general unstructured
fully-connected layers varied with the amount of training data. On the MNIST variants, we trained both
the single hidden layer and CNN models with random subsamples of 25%, 50%, and 75% of the training
set, with 15% of the training set used for validation in all settings. In addition, in Table 12, we compare the
generalization error of structured classes with an unstructured model, and ﬁnd that the structured classes
have consistently lower generalization error.

25

Table 6: Test accuracy when replacing the hidden layer with structured classes in the single hidden layer architecture,
at parameter budgets corresponding to LDR-TD and LDR-SD rank one. Rank is in parentheses. The ﬁrst group
of structured methods (in orange) all have compression factors (relative to a general unstructured layer) of 98 on
MNIST-bg-rot and MNIST-noise, and 128 on CIFAR-10 and NORB. The second group of structured methods (in
blue) all have compression factors of 196 on MNIST-bg-rot and MNIST-noise, and 256 on CIFAR-10 and NORB.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Table 7: Test accuracy when replacing the fully-connected layer with structured classes in the CNN architecture,
at parameter budgets corresponding to LDR-TD and LDR-SD rank one. Rank is in parentheses. The ﬁrst group
of structured methods (in orange) all have compression factors (relative to a general unstructured layer) of 98 on
MNIST-bg-rot and MNIST-noise, and 128 on CIFAR-10 and NORB. The second group of structured methods (in
blue) all have compression factors of 196 on MNIST-bg-rot and MNIST-noise, and 256 on CIFAR-10 and NORB.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Unstructured
LDR-TD (r = 1)
Toeplitz-like [45] (r = 4)
Hankel-like (r = 4)
Vandermonde-like (r = 4)
Low-rank [15] (r = 4)

LDR-SD (r = 1)
Toeplitz-like [45] (r = 2)
Hankel-like (r = 2)
Vandermonde-like (r = 2)
Low-rank [15] (r = 2)

44.08
45.81
42.67
42.23
37.14
35.67

44.74
42.07
41.01
33.56
32.64

Fully-connected
LDR-TD (r = 1)
Toeplitz-like [45] (r = 4)
Hankel-like (r = 4)
Vandermonde-like (r = 4)
Low-rank [15] (r = 4)

LDR-SD (r = 1)
Toeplitz-like [45] (r = 2)
Hankel-like (r = 2)
Vandermonde-like (r = 2)
Low-rank [15] (r = 2)

67.94
68.79
63.23
64.21
61.76
60.35

67.40
63.63
64.08
51.38
41.91

46.03
45.33
41.78
41.40
33.93
32.28

43.29
40.68
40.46
28.99
24.93

68.09
66.63
67.10
68.10
63.63
60.90

65.48
67.15
67.49
58.00
48.48

59.83
62.75
59.38
60.09
48.98
43.66

63.78
57.27
57.95
43.21
37.03

75.16
74.23
72.25
71.23
72.11
71.47

73.63
71.64
71.21
68.08
65.34

65.15
78.45
75.75
73.65
59.80
52.25

78.80
74.25
71.20
50.85
38.85

90.30
92.55
91.60
90.80
90.40
87.30

92.20
91.45
90.65
86.50
71.15

26

E.3 Additional visualizations

In Figure 6, we visualize the learned subdiagonal on NORB along with images from the dataset.

On the MNIST-bg-rot dataset [30], we note that Chen et al. [7] also tested several methods on this dataset,
including Random Edge Removal [11], Low Rank Decomposition [15], Dark Knowledge [25], HashedNets [7],
and HashedNets with Dark Knowledge, and reported test errors of 73.17, 80.63, 79.03, 77.40, 59.20, and 58.25,
where each method had 12406 parameters in the architecture. We found that our LDR-SD class, with 10986
parameters in the architecture, achieved a test error of 55.26, as shown in Table 6, outperforming all methods
evaluated by Chen et al. [7]. Sindhwani et al. [45] later also tested on this dataset, and reported test errors
of 68.4, 62.11, and 55.21 for Fastfood (10202 parameters), Circulant (8634 parameters), and Toeplitz-like,
r = 2 (10986 parameters). LDR-SD exceeds their reported results for Fastfood and Circulant [8], but not
that of Toeplitz-like. We did ﬁnd that our proposed classes consistently exceeded the performance of our own
implementation of Toeplitz-like on this dataset (Table 1, Figure 3, and Tables 6 and 7).

Table 8: On the MNIST variants, in the single hidden layer architecture, we compare LDR-SD, pruning [23], and a
baseline which reduces the number of hidden units (denoted RHU), at multiple budgets. At each budget, we adjust the
number of pruned weights or hidden units to match as closely as possible the parameter budget of LDR-SD. Parameter
counts of fully-connected layers for LDR-SD and pruning at ranks 1,2,4,8,12, and 16 are 10986, 12554, 15690, 21962,
28234, and 34506 respectively, and 11126, 12714, 15890, 22242, 28594, 34946 for RHU (for which parameter count
cannot be controlled exactly). As shown above, we ﬁnd that the classiﬁcation accuracy of LDR-SD consistently
exceeds that of both methods.

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

1
2
4
8
12
16

1
2
4
8
12
16

(a) MNIST-bg-rot

44.74
44.46
47.72
48.76
48.90
49.51

78.80
77.95
78.32
78.63
78.33
78.08

40.41
41.18
42.45
43.52
43.19
43.58

67.75
69.35
68.25
67.25
67.30
66.95

(b) MNIST-noise

37.18
37.60
37.98
39.77
40.56
40.70

62.85
62.55
63.40
64.45
63.85
66.10

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

(a) Subdiagonal of B (NORB)

(b) Images from NORB

Figure 6: We visualize the learned subdiagonal of the operator B and images from the NORB dataset. We observe a
centering phenomenon similar to that described in Figure 4.

E.4 Rectangles dataset

We provide an interesting example of a case where LDR-TD and LDR-SD do not exceed the performance
of the ﬁxed operator classes in the single hidden layer architecture. In this simple dataset from Larochelle
et al. [30], the task is to classify a binary image of a rectangle as having a greater length or width. We
show examples of the dataset in Figure 7. On this dataset, in contrast to the more challenging datasets
(MNIST-bg-rot [30], MNIST-noise [30], CIFAR-10 [29], and NORB [32]) we tested on, every structured class
outperforms an unconstrained model (622506 parameters), including the circulant class [8] which compresses
the hidden layer by 784x, and expanding the class beyond Toeplitz-like does not improve performance. We
hypothesize that this is because the Toeplitz-like class may enforce the right structure, in the sense that it is
suﬃciently expressive to ﬁt a perfect model on this dataset, but not expansive enough to lead to overﬁtting.

27

Table 9: On the MNIST variants, in the CNN architecture, we compare LDR-SD, pruning [23], and a baseline which
reduces the number of hidden units (denoted RHU), at multiple budgets. At each budget, we adjust the number of
pruned weights or hidden units to match as closely as possible the parameter budget of LDR-SD. Parameter counts of
fully-connected layers for LDR-SD and pruning at ranks 1,2,4,8,12, and 16 are 11770, 13338, 16474, 22746, 29018, and
35290 respectively, and 11935, 13525, 16705, 23065, 29425, 35785 for RHU (for which parameter count cannot be
controlled exactly). As shown above, we ﬁnd that the classiﬁcation accuracy of LDR-SD consistently exceeds that of
both methods.

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

1
2
4
8
12
16

1
2
4
8
12
16

(a) MNIST-bg-rot

67.40
67.53
67.96
67.21
68.54
67.00

92.20
92.75
91.30
91.95
92.10
93.20

64.25
64.05
65.50
64.12
65.65
65.59

90.80
91.65
90.60
91.05
90.00
90.55

(b) MNIST-noise

64.03
64.67
66.37
64.70
65.99
66.47

90.95
91.00
91.25
90.65
90.85
90.40

Rank of LDR-SD LDR-SD Pruning [23] RHU [7]

For example, while the Toeplitz-like operators model approximate shift equivariance (discussed in Section 4
and Proposition 7 in Section C.3), the additional scaling that subdiagonal operators provide is unnecessary
on these binary inputs.

Figure 7: Examples of images from the rectangles dataset [30].

E.5 Acceleration at inference time

We empirically study the acceleration obtained at inference time (on CPU) with our implementation of the
algorithms for multiplication by LDR-SD described in Appendix C.2. We generated random parameters for
each class and ran each multiplication algorithm 1000 times to compare the speedup of each class over an
unstructured multiply. Each test was repeated 10 times, and the minimum total runtime over the 10 tests was
used for each class. As shown in Figure 8 and Table 14, at n
4096, our simple Python implementation is
3.34-46.06x faster than the highly optimized unstructured matrix-vector multiply (a BLAS level 2 operation).
We also compare with two other structured classes, low-rank and Toeplitz-like, at r = 1, 2, 4, 8, 16. A batch
size of one was used in all tests. The time complexity of multiplication by low-rank and Toeplitz-like is O(nr)
and O(nr log n) respectively, compared to O(nr log2 n) for LDR-SD.

≥

28

Table 10: On the MNIST variants, in the single hidden layer architecture, we show how the number of training
samples aﬀects the performance of the unstructured model and the structured classes. Columns correspond to models
trained on 25%, 50%, 75% and 100% of the training data (randomly subsampled). LDR-TD and LDR-SD consistently
outperform the structured baselines at the tested subsampling ratios. On MNIST-bg-rot, LDR-TD only needs 75% of
the training data to outperform the unstructured model trained on 100% of the training data. On MNIST-noise, both
LDR-TD and LDR-SD only need 25% of the training data to outperform the unstructured layer. All are rank one.

Method

25% 50% 75% 100%

Method

25% 50% 75% 100%

Unstructured

34.46

38.80

43.35

44.08

Unstructured

59.30

61.85

65.35

65.15

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

34.01
35.64
33.71
21.44

39.59
39.78
36.44
23.46

44.35
42.72
39.32
23.48

45.81
44.74
41.12
25.06

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

65.45
67.90
56.15
24.25

74.60
71.15
67.75
26.20

77.45
76.95
72.30
26.85

78.45
78.80
73.95
26.40

(a) MNIST-bg-rot

(b) MNIST-noise

Table 11: On the MNIST variants, in the CNN architecture, we show how the number of training samples aﬀects the
performance of the unstructured model and the structured classes. Columns correspond to models trained on 25%,
50%, 75% and 100% of the training data (randomly subsampled). LDR-TD and LDR-SD consistently outperform the
structured baselines at the tested subsampling ratios. On MNIST-noise, both LDR-TD and LDR-SD only need 50%
of the training data to outperform the unstructured layer. All are rank one.

Method

25% 50% 75% 100%

Method

25% 50% 75% 100%

Unstructured

54.12

62.53

67.52

67.94

Unstructured

81.85

88.25

89.75

90.30

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

53.66
50.72
49.10
26.98

62.15
61.92
57.20
27.97

67.25
65.93
61.53
28.97

68.79
67.40
63.00
29.63

LDR-TD
LDR-SD
Toeplitz-like
Low-rank

86.45
86.95
81.65
33.15

91.35
90.90
88.15
38.40

93.00
91.55
90.90
42.55

92.55
92.20
90.95
44.55

(a) MNIST-bg-rot

(b) MNIST-noise

Table 12: Generalization error for unstructured, LDR-TD, LDR-SD, Toeplitz-like, low-rank classes on the single
hidden layer architecture. Consistent with Theorem 2, the structured classes have consistently lower generalization
error than the unstructured model. All are rank one.

Method

MNIST-bg-rot MNIST-noise CIFAR-10 NORB

Unstructured

LDR-TD
LDR-SD
Toeplitz-like [45]
Low-rank [15]

55.78

13.52
12.87
7.98
8.40

21.63

11.36
12.65
15.80
0.31

34.32

7.10
6.29
5.59
0.09

40.03

9.51
8.68
7.87
2.59

F Experimental details

F.1

Image classiﬁcation

In Table 15, we provide details on the datasets we use for evaluation. For all our experiments, batch sizes
were chosen to be 50. NORB was downsampled to 32
32, and the left stereo image was used. Training
was performed with stochastic gradient descent with momentum, with the number of epochs set to 50 on all
datasets. 15% of the training data was used for the validation set on all experiments. We ﬁxed momentum at
0.9 for all methods for all experiments, and performed a grid search over learning rate. Unless otherwise
stated, for each method, we tested the learning rates {0.0002, 0.0005, 0.001, 0.002}, with three trials (with

×

29

Table 13: Test accuracy when replacing the hidden layer with structured classes on the rectangles dataset [30]. Where
applicable, rank (r) is in parentheses, and the number of parameters in the architecture is in italics below each method.

Test Accuracy

Method

Unconstrained

LDR-TD (r = 1)

LDR-SD (r = 1)

Toeplitz-like (r = 4) [45]

Hankel-like (r = 4)

Vandermonde-like (r = 4)

Low-rank (r = 4) [15]

Fastfood [49]

Circulant [8]

91.94
622506
98.53
14122

98.39
10986

99.29
14122

97.77
14122

94.11
14122

92.80
14122

92.20
10202

95.58
8634

Figure 8: Acceleration of n × n structured classes over unstructured matrix-vector multiply at inference time. At
n ≥ 4096, LDR-SD (r = 1) achieves a speedup of 3.34-46.06x over unstructured. Data for higher ranks are shown in
Table 14. The comparison to the low-rank and Toeplitz-like classes illustrates a tradeoﬀ involved in broadening the
class of structured matrices we learn over. Though LDR-SD consistently outperforms these classes on downstream
quality, its computational cost of multiplication is O(nr log2 n), compared to O(nr) and O(nr log n) for low-rank and
Toeplitz-like respectively. Experimental details are in Appendix E.5.

random initializations) per learning rate. For each trial, we test on the validation set at each epoch, and
report the test accuracy of the model with the highest validation accuracy, over all learning rates, trials, and
epochs.

In Figure 3, for each method and each of the four learning rates, we perform ﬁve trials with random
initializations and report the average and standard deviation of the test accuracy of the learning rate with
the highest average validation accuracy.

30

Table 14: Acceleration of n × n structured classes over unstructured matrix-vector multiply at inference time.
Experimental details are in Appendix E.5.

n
29
210
211
212
213
214
215

1

×
×
×
×
×
×
×

101
102
102
103
103
103
104

5.15
1.39
4.14
2.38
5.96
8.35
1.79

2

×
×
×
×
×
×
×

101
101
102
102
103
103
103

2.43
5.41
1.60
8.71
1.75
3.44
7.50

Rank

2.46
5.66
1.71
7.46
1.65
3.40
7.53

101
101
102
102
103
103
103

4

×
×
×
×
×
×
×

4

(a) Low-rank

Rank

8

×
×
×
×
×
×
×

101
101
102
102
103
103
103

2.08
4.62
1.05
4.73
1.13
2.29
4.91

16

1.81
3.43
6.90
3.59
8.86
1.74
3.70

101
101
101
102
102
103
103

×
×
×
×
×
×
×

n
29
210
211
212
213
214
215

n
29
210
211
212
213
214
215

1

2

3.06
7.34
1.90
1.23
3.34
6.96
1.49

×
×
×
×
×
×
×

1
10−
1
10−
100
101
101
101
102

2.60
6.21
1.71
1.01
2.73
5.68
1.19

×
×
×
×
×
×
×

1

1
10−
1
10−
100
101
101
101
102

2.32
5.18
1.38
7.92
2.26
4.19
9.07

10−
1
10−
100
100
101
101
101
(b) Toeplitz-like

×
×
×
×
×
×
×

8

16

1.86
4.00
1.08
5.97
1.52
3.00
5.46

×
×
×
×
×
×
×

1

10−
1
10−
100
100
101
101
101

1.61
3.28
8.46
4.62
1.23
2.26
3.82

×
×
×
×
×
×
×

1
10−
1
10−
1
10−
100
101
101
101

1

2

6.68
1.49
4.99
3.34
9.71
2.12
4.61

×
×
×
×
×
×
×

2
10−
1
10−
1
10−
100
100
101
101

4.63
1.20
4.32
2.57
6.61
1.41
2.82

×
×
×
×
×
×
×

2
10−
1
10−
1
10−
100
100
101
101

Rank

4

4.05
9.45
3.02
1.61
4.40
8.38
1.60

×
×
×
×
×
×
×

2

2

10−
10−
1
10−
100
100
100
101

(c) LDR-SD

8

16

3.10
6.73
1.94
1.06
2.46
4.35
8.58

×
×
×
×
×
×
×

2

2

10−
10−
1
10−
100
100
100
100

2.56
5.24
1.37
7.52
1.68
3.00
5.70

×
×
×
×
×
×
×

2
10−
2
10−
1
10−
1
10−
100
100
100

31

Table 15: Overview of the image classiﬁcation datasets used in this work. For all datasets, 15% of the training set was
used for the validation set.

Dataset

Training Examples Test Examples Number of Classes

MNIST-bg-rot [30]
MNIST-noise [30]
CIFAR-10 [29]
NORB [32]
Rectangles [30]

12000
12000
50000
291600
1200

50000
2000
10000
58320
50000

10
10
10
6
2

Single hidden layer architecture In these experiments, we used an architecture consisting of a fully-
connected hidden layer, followed by a fully-connected softmax layer. In order to be consistent with the
architecture used in Sindhwani et al. [45], we do not use a bias term in the hidden layer.

CNN architecture In these experiments, shown in Table 7 in Appendix E, we tested on a LeNet-based
architecture. The architecture has 2 convolution/pool layers with 6 and 16 channels respectively, followed
by a fully-connected layer, followed by fully-connected logit/softmax layer. We replaced the second to last
fully-connected layer, which was of dimensions 784
784 for the MNIST-bg-rot and MNIST-noise datasets,
and 1024

1024 for the CIFAR-10 and NORB experiments.

×

×

Replacing convolutional layers This experiment corresponds to Table 3.

Here, we investigated whether the convolutional layers of CNNs can be learned automatically. For our
experiments, we test on the simplest possible multi-channel CNN model on the CIFAR-10 dataset. The model
consists of one layer of convolutional channels (3 RGB in channels, 3 out channels, stride 5), followed by
a fully-connected layer and a ﬁnal FC+softmax layer (total of 4 layers). We replace the convolutions with
various structured matrices of the same dimensions, keeping the same 3
3 channel structure (e.g. it would
consist of 3

3 = 9 square structured matrices) and number of hidden units.8

×

The LDR classes beneﬁt from being composed with LDR matrices of the same type (due to the composition
property, Proposition 1(c)), so we additionally replace the later FC layer with the same structured matrix
type.

·

By Proposition 1(d), channels of Toeplitz-like matrices form a larger Toeplitz-like matrix of the same
size. Using this insight, we consider replacing the channel structure of the convolutional layer with either
channels of structured matrices or a single wide structured matrix. (Also, note that this is able to leverage
the asymptotic fast nature of our structured classes.)

Because it seems that convolutional layers are strongly dependent on pooling – our structured matrices
outperform them in isolation – we compare against a version of the CNN with an additional pooling layer after
the convolutional channels. Note that this comparison is the same basic four layer model with a structured
matrix vs. a ﬁve layer convolutional model with pooling. Since the architectures are quite diﬀerent and
diﬃcult to directly compare, we also experimented with adding more hidden units to the pooling model.

F.2 Language modeling

For a language modeling application9, we explored replacing weight matrices in a recurrent neural network
with structured matrices. We evaluate on a single layer LSTM architecture, deﬁned by the update equations:

8The convolutions are padded to ensure their input and output dimensions are equal.
9Code available at https://github.com/pytorch/examples/tree/master/word_language_model.

32

i = σ(Wiix + bii + Whih + bhi)
f = σ(Wif x + bif + Whf h + bhf )
g = tanh(Wigx + big + Whgh + bhg)
o = σ(Wiox + bio + Whoh + bho)
c(cid:48) = f
g
∗
∗
h(cid:48) = o tanh(c(cid:48))

c + i

In our experiments we replace the matrices Wii, Wif , Wig, Wio with structured matrices. We use a hidden
layer of size 128, and word embedding size of 128. We evaluate on the Wikitext-2 dataset, which consists of
Wikipedia articles (2,088,628 training, 217,646 validation, and 245,569 test tokens). The total vocabulary is
of size 33,278. We use the default hyperparameters and train using stochastic gradient descent with an initial
learning rate of 20. The learning rate is annealed 4x after each epoch if performance does not improve on the
validation set. Results are shown in Table 2.

33

