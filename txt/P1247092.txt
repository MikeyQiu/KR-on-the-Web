6
1
0
2
 
p
e
S
 
6
2
 
 
]
L
C
.
s
c
[
 
 
2
v
4
6
4
5
0
.
6
0
6
1
:
v
i
X
r
a

Stance Detection with Bidirectional Conditional Encoding

Isabelle Augenstein and Tim Rockt¨aschel
Department of Computer Science
University College London
i.augenstein@ucl.ac.uk, t.rocktaschel@cs.ucl.ac.uk

Andreas Vlachos and Kalina Bontcheva
Department of Computer Science
University of Shefﬁeld
{a.vlachos, k.bontcheva}@sheffield.ac.uk

Abstract

Stance detection is the task of classifying the
attitude expressed in a text towards a target
such as Hillary Clinton to be “positive”, “neg-
ative” or “neutral”. Previous work has as-
sumed that either the target is mentioned in
the text or that training data for every tar-
get is given. This paper considers the more
challenging version of this task, where targets
are not always mentioned and no training data
is available for the test targets. We experi-
ment with conditional LSTM encoding, which
builds a representation of the tweet that is de-
pendent on the target, and demonstrate that it
outperforms encoding the tweet and the target
independently. Performance is improved fur-
ther when the conditional model is augmented
with bidirectional encoding. We evaluate our
approach on the SemEval 2016 Task 6 Twit-
ter Stance Detection corpus achieving perfor-
mance second best only to a system trained on
semi-automatically labelled tweets for the test
target. When such weak supervision is added,
our approach achieves state–of-the-art results.

1 Introduction

The goal of stance detection is to classify the at-
titude expressed in a text towards a given target,
as “positive”, ”negative”, or ”neutral”. Such in-
formation can be useful for a variety of tasks, e.g.
Mendoza et al. (2010) showed that tweets stating ac-
tual facts were afﬁrmed by 90% of the tweets re-
lated to them, while tweets conveying false in-
formation were predominantly questioned or de-
In this paper we focus on a novel stance
nied.

detection task, namely tweet stance detection to-
wards previously unseen targets (mostly entities
such as politicians or issues of public interest), as
deﬁned in the SemEval Stance Detection for Twitter
task (Mohammad et al., 2016). This task is rather
difﬁcult, ﬁrstly due to not having training data for
the targets in the test set, and secondly, due to the
targets not always being mentioned in the tweet. For
example, the tweet “@realDonaldTrump is the only
honest voice of the @GOP” expresses a positive
stance towards the target Donald Trump. However,
when stance is annotated with respect to Hillary
Clinton as the implicit target, this tweet expresses
a negative stance, since supporting candidates from
one party implies negative stance towards candidates
from other parties.

Thus the challenge is twofold. First, we need to
learn a model that interprets the tweet stance towards
a target that might not be mentioned in the tweet it-
self. Second, we need to learn such a model without
labelled training data for the target with respect to
which we are predicting the stance. In the example
above, we need to learn a model for Hillary Clinton
by only using training data for other targets. While
this renders the task more challenging, it is a more
realistic scenario, as it is unlikely that labelled train-
ing data for each target of interest will be available.

based

network

encoding

architecture

To address

these challenges we develop a
con-
neural
(Rockt¨aschel et al., 2016).
ditional
net-
A long-short
work
is
used to encode the target, followed by a second
LSTM that encodes the tweet using the encoding

(LSTM)
(Hochreiter and Schmidhuber, 1997)

term memory

on

of the target as its initial state. We show that this
approach achieves better F1 than an SVM baseline,
or an independent LSTM encoding of the tweet and
the target. Results improve further (0.4901 F1) with
a bidirectional version of our model, which takes
into account the context on either side of the word
being encoded.
In the context of the shared task,
this would have been the second best result, except
for an approach which uses automatically labelled
tweets for the test targets (F1 of 0.5628). Lastly,
when our bidirectional conditional encoding model
is trained on such data, it achieves state-of-the-art
performance (0.5803 F1).

2 Task Setup

The SemEval 2016 Stance Detection for Twitter
shared task (Mohammad et al., 2016) consists of
two subtasks, Task A and Task B. In Task A the
goal is to detect the stance of tweets towards tar-
gets given labelled training data for all test targets
(Climate Change is a Real Concern, Feminist Move-
ment, Atheism, Legalization of Abortion and Hillary
Clinton). In Task B, which is the focus of this paper,
the goal is to detect stance with respect to an un-
seen target, Donald Trump, for which labeled train-
ing/development data is not provided.

Systems need to classify the stance of each tweet
as “positive” (FAVOR), “negative” (AGAINST) or
“neutral” (NONE) towards the target. The ofﬁcial
metric reported for the shared task is F1 macro-
averaged over the classes FAVOR and AGAINST.
Although the F1 of NONE is not considered, sys-
tems still need to predict it to avoid precision errors
for the other two classes.

Even though participants were not allowed to
manually label data for the test
target Donald
Trump, they were allowed to label data automat-
ically.
The two best-performing systems sub-
mitted to Task B, pkudblab (Wei et al., 2016) and
LitisMind (Zarrella and Marsh, 2016) made use of
this, thus changing the task to weakly supervised
seen target stance detection, instead of an unseen tar-
get task. Although the goal of this paper is to present
stance detection methods for targets for which no
training data is available, we show that they can also
be used successfully in a weakly supervised frame-
work and outperform the state-of-the-art on the Se-

mEval 2016 Stance Detection for Twitter dataset.

3 Methods

to sentiment analysis

A common stance detection approach is to treat
it as a sentence-level classiﬁcation task sim-
ilar
(Pang and Lee, 2008;
Socher et al., 2013). However, such an approach
cannot capture the stance of a tweet with respect to a
particular target, unless training data is available for
each of the test targets. In such cases, we could learn
that a tweet mentioning Donald Trump in a positive
manner expresses a negative stance towards Hillary
Clinton. Despite this limitation, we use two such
baselines, one implemented with a Support Vector
Machine (SVM) classiﬁer and one with an LSTM
network, in order to assess whether we are success-
ful in incorporating the target in stance prediction.

A naive approach to incorporate the target in
stance prediction would be to generate features con-
catenating the target with words from the tweet. Ig-
noring the issue that such features would be rather
sparse, a classiﬁer could learn that some words have
target-dependent stance weights, but it still assumes
that training data is available for each target.

In order to learn how to combine the stance target
with the tweet in a way that generalises to unseen
targets, we focus on learning distributed represen-
tations and ways to combine them. The following
sections develop progressively the proposed bidirec-
tional conditional LSTM encoding model, starting
from independently encoding the tweet and the tar-
get using LSTMs.

3.1

Independent Encoding

attempt

Our
initial
resentations for
to encode the target and tweet
using
as
LSTMs (Hochreiter and Schmidhuber, 1997).

to learn distributed rep-
the tweets and the targets is
independently
two

k-dimensional

vectors

dense

(cid:20)

H =

xt
ht−1 (cid:21)
it = σ(WiH + bi)
ft = σ(Wf H + bf )
ot = σ(WoH + bo)
ct = ft ⊙ ct−1 + it ⊙ tanh(WcH + bc)
ht = ot ⊙ tanh(ct)

h←
1

h→
1

c←
1

c→
1

h←
2

h→
2

c←
2

c→
2

h←
3

h→
3

c←
3

c→
3

h←
4

h→
4

c←
4

c→
4

h←
5

h→
5

c←
5

c→
5

h←
6

h→
6

c←
6

c→
6

h←
7

h→
7

c←
7

c→
7

h←
8

h→
8

c←
8

c→
8

x1

x2

x3

x4

x5

x6

x7

x8

Legalization of Abortion

A

foetus

has

rights

too

Target

Tweet

h←
9

h→
9

c←
9

c→
9

x9

!

Figure 1: Bidirectional encoding of tweet conditioned on bidirectional encoding of target ([c→
the last forward and reversed output representations ([h→

9 h←

4 ]).

3 c←

1 ]). The stance is predicted using

Here, xt is an input vector at time step t, ct denotes
the LSTM memory, ht ∈ Rk is an output vector and
the remaining weight matrices and biases are train-
able parameters. We concatenate the two output vec-
tor representations and classify the stance using the
softmax over a non-linear projection

softmax(tanh(Wtahtarget + Wtwhtweet + b))

into the space of the three classes for stance detec-
tion where Wta, Wtw ∈ R3×k are trainable weight
matrices and b ∈ R3 is a trainable class bias. This
model learns target-independent distributed repre-
sentations for the tweets and relies on the non-
linear projection layer to incorporate the target in the
stance prediction.

3.2 Conditional Encoding

In order to learn target-dependent tweet representa-
tions, we use conditional encoding as previously ap-
plied to the task of recognising textual entailment
(Rockt¨aschel et al., 2016). We use one LSTM to en-
code the target as a ﬁxed-length vector. Then, we
encode the tweet with another LSTM, whose state
is initialised with the representation of the target.
Finally, we use the last output vector of the tweet
LSTM to predict the stance of the target-tweet pair.
Formally, let (x1, . . . , xT ) be a sequence of tar-
get word vectors, (xT +1, . . . , xN ) be a sequence of
tweet word vectors and [h0 c0] be a start state of

zeros. The two LSTMs map input vectors and a pre-
vious state to a next state as follows:

[h1 c1] = LSTMtarget(x1, h0, c0)

[hT cT ] = LSTMtarget(xT , hT −1, cT −1)

[hT +1 cT +1] = LSTMtweet(xT +1, h0, cT )

. . .

. . .

[hN cN ] = LSTMtweet(xN , hN −1, cN −1)

Finally, the stance of the tweet w.r.t.
classiﬁed using a non-linear projection

the target is

c = tanh(WhN )

where W ∈ R3×k is a trainable weight matrix.
This effectively allows the second LSTM to read the
tweet in a target-speciﬁc manner, which is crucial
since the stance of the tweet depends on the target
(recall the Donald Trump example above).

3.3 Bidirectional Conditional Encoding

Bidirectional
LSTMs
(Graves and Schmidhuber, 2005) have been shown
to learn improved representations of sequences by
encoding a sequence from left to right and from
right to left. Therefore, we adapt the conditional en-
coding model from Section 3.2 to use bidirectional
LSTMs, which represent the target and the tweet
using two vectors for each of them, one obtained

by reading the target and then the tweet left-to-right
(as in the conditional LSTM encoding) and one
obtained by reading them right-to-left. To achieve
this, we initialise the state of the bidirectional
LSTM that reads the tweet by the last state of the
forward and reversed encoding of the target (see
Figure 1). The bidirectional encoding allows the
model to construct target-dependent representations
of the tweet such that when a word is considered,
both its left- and the right-hand side context are
taken into account.

3.4 Unsupervised Pretraining

In order to counter-balance the relatively small
amount of training data available (5,628 instances
in total), we employ unsupervised pre-training
by initialising the word embeddings used in the
LSTMs with an appropriately trained word2vec
model (Mikolov et al., 2013). Note that these em-
beddings are used only for initialisation, as we allow
them to be optimised further during training.

In more detail, we train a word2vec model on a
corpus of 395,212 unlabelled tweets, collected with
the Twitter Keyword Search API1 between Novem-
ber 2015 and January 2016, plus all the tweets con-
tained in the ofﬁcial SemEval 2016 Stance Detec-
tion datasets (Mohammad et al., 2016). The unla-
belled tweets are collected so that they contain the
targets considered in the shared task, using up to
two keywords per target, namely “hillary”, “clin-
ton”, “trump”, “climate”, “femini”, “aborti”. Note
that Twitter does not allow for regular expression
search, so this is a free text search disregarding pos-
sible word boundaries. We combine this large unla-
belled corpus with the ofﬁcial training data and train
a skip-gram word2vec model (dimensionality 100, 5
min words, context window of 5).

Tweets and targets are tokenised with the Twitter-
adapted tokeniser twokenize2. Subsequently, all to-
kens are lowercased, URLs are removed, and stop-
word tokens are ﬁltered (i.e. punctuation characters,
Twitter-speciﬁc stopwords (“rt”, “#semst”, “via”).

As it will be shown in our experiments, unsuper-
vised pre-training is quite helpful, since it is difﬁcult

1https://dev.twitter.com/rest/public/

search

2https://github.com/leondz/twokenize

Corpus

Favor Against None

All

TaskA Tr+Dv
1462
TaskA Tr+Dv HC 224
TaskB Unlab
TaskB Auto-lab*
TaskB Test
Crawled Unlab*

-
4681
148
-

2684
722
-
5095
299
-

1482
332
-
4026
260
-

5628
1278
278,013
13,802
707
395,212

Table 1: Data sizes of available corpora. TaskA Tr+Dv HC
is the part of TaskA Tr+Dv with tweets for the target Hillary
Clinton only, which we use for development. TaskB Auto-
lab is an automatically labelled version of TaskB Unlab.

Crawled Unlab is an unlabelled tweet corpus collected by us.

to learn representations for all the words using only
the relatively small training datasets available.

Finally, to ensure that the proposed neural net-
work architectures contribute to the performance,
we also use the word vectors from word2vec to de-
velop a Bag-of-Word-Vectors baseline (BOWV), in
which the tweet and target representations are fed
into a logistic regression classiﬁer with L2 regular-
ization (Pedregosa et al., 2011).

4 Experiments

Experiments are performed on the SemEval 2016
Task 6 corpus for Stance Detection on Twit-
ter (Mohammad et al., 2016). We report experi-
ments for two different experimental setups: one
is the unseen target setup (Section 5), which is the
main focus of this paper, i.e. detecting the stance of
tweets towards previously unseen targets. We show
that conditional encoding, by reading the tweets in
a target-speciﬁc way, generalises to unseen targets
better than baselines which ignore the target. Next,
we compare our approach to previous work in a
weakly supervised framework (Section 6) and show
that our approach outperforms the state-of-the-art on
the SemEval 2016 Stance Detection Subtask B cor-
pus.

Table 1 lists the various corpora used in the ex-
periments and their sizes. TaskA Tr+Dv is the
ofﬁcial SemEval 2016 Twitter Stance Detection
TaskA training and development corpus, which
contain instances for the targets Legalization of
Abortion, Atheism, Feminist Movement, Climate
Change is a Real Concern and Hillary Clinton.
TaskA Tr+Dv HC is the part of the corpus which
contains only the Hillary Clinton tweets, which

we use for development purposes. TaskB Test
is the TaskB test corpus on which we report re-
sults containing Donald Trump testing instances.
TaskB Unlab is an unlabelled corpus containing
Donald Trump tweets supplied by the task organ-
isers, and TaskB Auto-lab* is an automatically la-
belled version of a small portion of the corpus for
the weakly supervised stance detection experiments
reported in Section 6. Finally, Crawled Unlab* is
a corpus we collected for unsupervised pre-training
(see Section 3.4).

Method

Stance

P

R

F1

BoWV

FAVOR

0.2444
AGAINST 0.5916

0.0940
0.8626

TweetOnly

FAVOR

0.2127
AGAINST 0.6529

0.5726
0.4020

Concat

FAVOR

0.1811
AGAINST 0.6299

0.6239
0.4504

TarCondTweet

FAVOR

0.3293
AGAINST 0.4304

0.3649
0.5686

is used.

For all experiments, the ofﬁcial task evaluation
script
Predictions are post processed
so that if the target is contained in a tweet, the
highest-scoring non-neutral stance is chosen. This
in the
was motivated by the observation that
training data most target-containing tweets express
a stance, with only 16% of them being neutral.
The code used in our experiments is available from
https://github.com/sheffieldnlp/stance-conditional.

opment setup.

TweetCondTar

BiCond

FAVOR

0.1985
AGAINST 0.6332

0.2308
0.7379

FAVOR

0.2588
AGAINST 0.7081

0.3761
0.5802

Table 2: Results for the unseen target stance detection devel-

0.1358
0.7019
0.4188

0.3102
0.4976
0.4039

0.2808
0.5252
0.4030

0.3462
0.4899
0.4180

0.2134
0.6816
0.4475

0.3066
0.6378
0.4722

Macro

Macro

Macro

Macro

Macro

Macro

4.1 Methods

We compare the following baseline methods:

• SVM trained with word and character
(SVM-ngrams-

tweet
comb) Mohammad et al. (2016)

n-grams

features

• a majority class baseline (Majority baseline),

reported in (Mohammad et al., 2016)

• bag of word vectors (BoWV) (see Section 3.4)
• independent encoding of tweet and the target
with two LSTMs (Concat) (see Section 3.1)
• encoding of the tweet only with an LSTM

(TweetOnly) (see Section 3.1)

to three versions of conditional encoding:

• target conditioned on tweet (TarCondTweet)
• tweet conditioned on target (TweetCondTar)
• a bidirectional encoding model (BiCond)

5 Unseen Target Stance Detection

models, we used the tweets labelled for Hillary Clin-
ton as a development set and the tweets for the re-
maining four targets as training. We refer to this as
the development setup, and all models are tuned us-
ing this setup. The labelled Donald Trump tweets
were only used in reporting our ﬁnal results.

For the ﬁnal results we train on all the data from
the development setup and evaluate on the ofﬁcial
Task B test set, i.e. the Donald Trump tweets. We
refer to this as our test setup.

Based on a small grid search using the develop-
ment setup, the following settings for LSTM-based
models were chosen: input layer size 100 (equal to
the word embedding dimension), hidden layer size
of 60, training for max 50 epochs with initial learn-
ing rate 1e-3 using ADAM (Kingma and Ba, 2014)
for optimisation, dropout 0.1. Models were trained
using cross-entropy loss. The use of one, relatively
small hidden layer and dropout help to avoid over-
ﬁtting.

As explained earlier,
the challenge is to learn a
model without any manually labelled training data
for the test target, but only using the data from the
Task A targets.
In order to avoid using any la-
belled data for Donald Trump, while still having a
(labelled) development set to tune and evaluate our

5.1 Results and Discussion

Results for the unseen target setting show how well
conditional encoding is suited for learning target-
dependent representations of tweets, and crucially,
how well such representations generalise to unseen
targets. The best performing method on both de-

Method

Stance

P

R

F1

BoWV

FAVOR

0.3158
AGAINST 0.4316

0.0405
0.8963

TweetOnly

FAVOR

0.2767
AGAINST 0.4225

0.3851
0.5284

Concat

FAVOR

0.3145
AGAINST 0.4452

0.5270
0.4348

TarCondTweet

FAVOR

0.2322
AGAINST 0.6712

0.4188
0.6234

TweetCondTar

FAVOR

0.3710
AGAINST 0.4633

0.5541
0.5485

BiCond

FAVOR

0.3033
AGAINST 0.6788

0.5470
0.5216

Macro

Macro

Macro

Macro

Macro

Macro

0.0719
0.5826
0.3272

0.3220
0.4695
0.3958

0.3939
0.4399
0.4169

0.2988
0.6464
0.4726

0.4444
0.5023
0.4734

0.3902
0.5899
0.4901

Table 3: Results for the unseen target stance detection test

setup.

EmbIni

NumMatr

Stance

P

R

F1

Random

PreFixed

PreCont

Sing

Sep

Sing

Sep

Sing

Sep

FAVOR

0.1982
AGAINST 0.6263

0.3846
0.5929

FAVOR

0.2278
AGAINST 0.6706

0.5043
0.4300

FAVOR

0.6000
AGAINST 0.5761

0.0513
0.9440

FAVOR

0.1429
AGAINST 0.5707

0.0342
0.9033

FAVOR

0.2588
AGAINST 0.7081

0.3761
0.5802

FAVOR

0.2243
AGAINST 0.6185

0.4103
0.5445

Macro

Macro

Macro

Macro

Macro

Macro

0.2616
0.6092
0.4354

0.3138
0.5240
0.4189

0.0945
0.7155
0.4050

0.0552
0.6995
0.3773

0.3066
0.6378
0.4722

0.2900
0.5792
0.4346

Table 4: Results for the unseen target stance detection develop-
ment setup using BiCond, with single vs separate embeddings

matrices for tweet and target and different initialisations

velopment (Table 2) and test setups (Table 3) is Bi-
Cond, which achieves an F1 of 0.4722 and 0.4901
respectively. Notably, Concat, which learns an in-
dependent encoding of the target and the tweets,
does not achieve big F1 improvements over Twee-
tOnly, which learns a representation of the tweets

only. This shows that it is not sufﬁcient to just take
the target into account, but is is important to learn
target-dependent encodings for the tweets. Models
that learn to condition the encoding of tweets on tar-
gets outperform all baselines on the test set.

It is further worth noting that the Bag-of-Word-
Vectors baseline achieves results comparable with
TweetOnly, Concat and one of the conditional en-
coding models, TarCondTweet, on the dev set,
even though it achieves signiﬁcantly lower perfor-
mance on the test set.
the
pre-trained word embeddings on their own are al-
ready very useful for stance detection. This is con-
sistent with ﬁndings of other works showing the
usefulness of such a Bag-of-Word-Vectors baseline
for the related tasks of recognising textual entail-
ment Bowman et al. (2015) and sentiment analysis
Eisner et al. (2016).

This indicates that

Our best result in the test setup with BiCond is the
second highest reported result on the Twitter Stance
Detection corpus, however the ﬁrst, third and fourth
best approaches achieved their results by automati-
cally labelling Donald Trump training data. BiCond
for the unseen target setting outperforms the third
and fourth best approaches by a large margin (5 and
7 points in Macro F1, respectively), as can be seen
in Table 7. Results for weakly supervised stance de-
tection are discussed in Section 6.

Pre-Training Table 4 shows the effect of unsu-
pervised pre-training of word embeddings with a
word2vec skip-gram model, and furthermore, the re-
sults of sharing of these representations between the
tweets and targets, on the development set. The ﬁrst
set of results is with a uniformly Random embed-
ding initialisation in [−0.1, 0.1]. PreFixed uses the
pre-trained skip-gram word embeddings, whereas
PreCont initialises the word embeddings with ones
from SkipGram and continues training them dur-
ing LSTM training. Our results show that, in the
absence of a large labelled training dataset, pre-
training of word embeddings is more helpful than
random initialisation of embeddings. Sing vs Sep
shows the difference between using shared vs two
separate embeddings matrices for looking up the
word embeddings. Sing means the word represen-
tations for tweet and target vocabularies are shared,
whereas Sep means they are different. Using shared

Method

inTwe

Stance

P

R

F1

Method

Stance

P

R

F1

Concat

TweetCondTar

BiCond

Yes

No

Yes

No

Yes

No

FAVOR

0.3153 0.6214 0.4183
AGAINST 0.7438 0.4630 0.5707
0.4945

Macro

FAVOR

0.0450 0.6429 0.0841
AGAINST 0.4793 0.4265 0.4514
0.2677

Macro

FAVOR

0.3529 0.2330 0.2807
AGAINST 0.7254 0.8327 0.7754
0.5280

Macro

FAVOR

0.0441 0.2143 0.0732
AGAINST 0.4663 0.5588 0.5084
0.2908

Macro

FAVOR

0.3585 0.3689 0.3636
AGAINST 0.7393 0.7393 0.7393
0.5515

Macro

FAVOR

0.0938 0.4286 0.1538
AGAINST 0.5846 0.2794 0.3781
0.2660

Macro

BoWV

FAVOR

0.5156
AGAINST 0.6266

0.6689
0.3311

TweetOnly

FAVOR

0.5284
AGAINST 0.5774

0.6284
0.4615

Concat

FAVOR

0.5506
AGAINST 0.5794

0.5878
0.4883

TarCondTweet

FAVOR

0.5636
AGAINST 0.5947

0.6284
0.4515

TweetCondTar

FAVOR

0.5868
AGAINST 0.5915

0.6622
0.4649

BiCond

FAVOR

0.6268
AGAINST 0.6057

0.6014
0.4983

Macro

Macro

Macro

Macro

Macro

Macro

0.5824
0.4333
0.5078

0.5741
0.5130
0.5435

0.5686
0.5299
0.5493

0.5942
0.5133
0.5538

0.6222
0.5206
0.5714

0.6138
0.5468
0.5803

Table 5: Results for the unseen target stance detection devel-

opment setup for tweets containing the target vs tweets not con-

taining the target.

Table 6: Stance Detection test results for weakly super-

vised setup, trained on automatically labelled pos+neg+neutral

Trump data, and reported on the ofﬁcial test set.

embeddings performs better, which we hypothesise
is because the tweets contain some mentions of tar-
gets that are tested.

Target in Tweet vs Not in Tweet Table 5 shows
results on the development set for BiCond, com-
pared to the best unidirectional encoding model,
TweetCondTar and the baseline model Concat,
split by tweets that contain the target and those that
do not. All three models perform well when the
target is mentioned in the tweet, but less so when
the targets are not mentioned explicitly. In the case
where the target is mentioned in the tweet, bicon-
ditional encoding outperforms unidirectional encod-
ing and unidirectional encoding outperforms Con-
cat. This shows that conditional encoding is able
to learn useful dependencies between the tweets and
the targets.

6 Weakly Supervised Stance Detection

The previous section showed the usefulness of con-
ditional encoding for unseen target stance detec-
internal base-
tion and compared results against
lines.
The goal of experiments reported in
this section is to compare against participants
in the SemEval 2016 Stance Detection Task B.

including the three highest

While we consider an unseen target setup, most
submissions,
rank-
ing ones for Task B, pkudblab (Wei et al., 2016),
LitisMind (Zarrella and Marsh, 2016) and INF-
UFRGS (Dias and Becker, 2016) considered a dif-
ferent experimental setup. They automatically anno-
tated training data for the test target Donald Trump,
thus converting the task into weakly supervised
seen target stance detection. The pkudblab sys-
tem uses a deep convolutional neural network that
learns to make 2-way predictions on automatically
labelled positive and negative training data for Don-
ald Trump. The neutral class is predicted according
to rules which are applied at test time.

Since the best performing systems which partic-
ipated in the shared task consider a weakly super-
vised setup, we further compare our proposed ap-
proach to the state-of-the-art using such a weakly
supervised setup. Note that, even though pkudblab,
LitisMind and INF-UFRGS also use regular expres-
sions to label training data automatically, the result-
ing datasets were not available to us. Therefore, we
had to develop our own automatic labelling method
and dataset, which are publicly available from our
code repository.

Weakly Supervised Test Setup For this setup, the
unlabelled Donald Trump corpus TaskB Unlab is
annotated automatically. For this purpose we cre-
ated a small set of regular expressions3, based on
inspection of the TaskB Unlab corpus, expressing
positive and negative stance towards the target. The
regular expressions for the positive stance were:

• make( ?)america( ?)great( ?)again
• trump( ?)(for|4)( ?)president
• votetrump
• trumpisright
• the truth
• #trumprules
keyphrases

negative

Method

SVM-ngrams-comb (Unseen Target)

Majority baseline (Unseen Target)

BiCond (Unseen Target)

INF-UFRGS (Weakly Supervised*)

The
#dumptrump,
idiot, ﬁred

for
#notrump,

stance were:
racist,

#trumpwatch,

LitisMind (Weakly Supervised*)

pkudblab (Weakly Supervised*)

Stance

F1

FAVOR

0.1842
AGAINST 0.3845
0.2843

Macro

FAVOR

0.0

AGAINST 0.5944
0.2972

Macro

FAVOR

0.3902
AGAINST 0.5899
0.4901

Macro

FAVOR

0.3256
AGAINST 0.5209
0.4232

Macro

FAVOR

0.3004
AGAINST 0.5928
0.4466

Macro

FAVOR

0.5739
AGAINST 0.5517
0.5628

Macro

FAVOR

0.6138
AGAINST 0.5468
0.5803

Macro

A tweet is labelled as positive if one of the posi-
tive expressions is detected, else negative if a nega-
tive expressions is detected. If neither are detected,
the tweet is annotated as neutral randomly with 2%
chance. The resulting corpus size per stance is
shown in Table 1. The same hyperparameters for
the LSTM-based models are used as for the unseen
target setup described in the previous section.

6.1 Results and Discussion

Table 6 lists our results in the weakly supervised
setting.
includ-
Table 7 shows all our results,
ing those using the unseen target setup, compared
the state-of-the-art on the stance detec-
against
tion corpus.
Table 7 further lists baselines re-
ported by Mohammad et al. (2016), namely a major-
ity class baseline (Majority baseline), and a method
using 1 to 3-gram bag-of-word and character n-gram
features (SVM-ngrams-comb), which are extracted
from the tweets and used to train a 3-way SVM clas-
siﬁer.

Bag-of-word baselines (BoWV, SVM-ngrams-
comb) achieve results comparable to the majority
baseline (F1 of 0.2972), which shows how difﬁ-
cult the task is. The baselines which only extract
features from the tweets, SVM-ngrams-comb and
TweetOnly perform worse than the baselines which
also learn representations for the targets (BoWV,
Concat). By training conditional encoding models

3Note that “|” indiates “or”, ( ?) indicates optional space

BiCond (Weakly Supervised)

Table 7: Stance Detection test results, compared against the
state of the art. SVM-ngrams-comb and Majority base-
line are reported in Mohammad et al. (2016), pkudblab in

Wei et al. (2016), LitisMind in Zarrella and Marsh (2016), INF-

UFRGS in Dias and Becker (2016)

on automatically labelled stance detection data we
achieve state-of-the-art results. The best result (F1
of 0.5803) is achieved with the bi-directional condi-
tional encoding model (BiCond). This shows that
such models are suitable for unseen, as well as seen
target stance detection.

7 Related Work

Stance Detection: Previous work mostly consid-
ered target-speciﬁc stance prediction in debates
(Hasan and Ng, 2013; Walker et al., 2012) or stu-
dent essays (Faulkner, 2014). The task considered in
this paper is more challenging than stance detection
in debates because, in addition to irregular language,
the Mohammad et al. (2016) dataset is offered with-
out any context, e.g., conversational structure or
tweet metadata. The targets are also not always men-
tioned in the tweets, which is an additional challenge
(Augenstein et al., 2016) and distinguishes this task
(Vo and Zhang, 2015;
from
and
Zhang et al., 2016; Alghunaim et al., 2015)

target-dependent

sentiment

target-dependent

(Mitchell et al., 2013;

anal-
open-domain
ysis
Zhang et al., 2015).
Related work on rumour stance detection ei-
ther
requires training data from the same ru-
target, or is
mour (Qazvinian et al., 2011),
rule-based (Liu et al., 2015) and thus potentially
hard to generalise. Finally,
the target-dependent
stance detection task tackled in this paper is dif-
ferent
from that of Ferreira and Vlachos (2016),
which while related concerned with the stance of
a statement
language towards another
statement.

in natural

i.e.,

Conditional Encoding: Conditional encoding
has been applied to the related task of recog-
nising textual entailment (Rockt¨aschel et al., 2016),
using a dataset of half a million training exam-
ples (Bowman et al., 2015) and numerous different
hypotheses. Our experiments here show that con-
ditional encoding is also successful on a relatively
small training set and when applied to an unseen
testing target. Moreover, we augment conditional
encoding with bidirectional encoding and demon-
strate the added beneﬁt of unsupervised pre-training
of word embeddings on unlabelled domain data.

8 Conclusions and Future Work

This paper showed that conditional LSTM encod-
ing is a successful approach to stance detection for
unseen targets. Our unseen target bidirectional con-
ditional encoding approach achieves the second best
results reported to date on the SemEval 2016 Twitter
Stance Detection corpus. In the weakly supervised
seen target scenario, as considered by prior work,
our approach achieves the best results to date on the
SemEval Task B dataset. We further show that in the
absence of large labelled corpora, unsupervised pre-
training can be used to learn target representations
for stance detection and improves results on the Se-
mEval corpus. Future work will investigate further
the challenge of stance detection for tweets which
do not contain explicit mentions of the target.

Acknowledgments

This work was partially supported by the European
Union under grant agreement No. 611233 PHEME4

4http://www.pheme.eu

and by Microsoft Research through its PhD Scholar-
ship Programme.

References

[Alghunaim et al.2015] Abdulaziz Alghunaim, Mitra Mo-
htarami, Scott Cyphers, and Jim Glass.
2015. A
Vector Space Approach for Aspect Based Sentiment
Analysis. In Proceedings of the 1st Workshop on Vec-
tor Space Modeling for Natural Language Processing,
pages 116–122, Denver, Colorado, June. Association
for Computational Linguistics.

[Augenstein et al.2016] Isabelle Augenstein, Andreas
Vlachos, and Kalina Bontcheva.
2016. USFD:
Any-Target Stance Detection on Twitter with Autoen-
coders. In Proceedings of the International Workshop
on Semantic Evaluation, SemEval ’16, San Diego,
California.

[Bowman et al.2015] Samuel R. Bowman, Gabor Angeli,
Christopher Potts, and Christopher D. Manning. 2015.
A large annotated corpus for learning natural language
inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 632–642, Lisbon, Portugal, September. Associ-
ation for Computational Linguistics.

[Dias and Becker2016] Marcelo Dias and Karin Becker.
2016. INF-UFRGS-OPINION-MINING at SemEval-
2016 Task 6: Automatic Generation of a Training
Corpus for Unsupervised Identiﬁcation of Stance in
Tweets. In Proceedings of the International Workshop
on Semantic Evaluation, SemEval ’16, San Diego,
California, June.

[Eisner et al.2016] Ben Eisner, Tim Rockt¨aschel, Isabelle
Augenstein, Matko Boˇsnjak, and Sebastian Riedel.
emoji2vec: Learning Emoji Representations
2016.
from their Description.
In Proceedings of the Inter-
national Workshop on Natural Language Processing
for Social Media, SocialNLP ’16, Austin, Texas.

2014.

[Faulkner2014] Adam Faulkner.

Automated
Classiﬁcation of Stance in Student Essays: An Ap-
proach Using Stance Target Information and the
In William Eberle
Wikipedia Link-Based Measure.
and Chutima Boonthum-Denecke, editors, FLAIRS
Conference. AAAI Press.

[Ferreira and Vlachos2016] William Ferreira and An-
dreas Vlachos. 2016. Emergent: a novel data-set for
stance classiﬁcation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1163–1168, San Diego, Califor-
nia, June. Association for Computational Linguistics.
[Graves and Schmidhuber2005] Alex Graves and J¨urgen
Schmidhuber. 2005. Framewise phoneme classiﬁca-

tion with bidirectional LSTM and other neural network
architectures. Neural Networks, 18(5):602–610.
[Hasan and Ng2013] Kazi Saidul Hasan and Vincent Ng.
2013. Stance Classiﬁcation of Ideological Debates:
Data, Models, Features, and Constraints. In IJCNLP,
pages 1348–1356. Asian Federation of Natural Lan-
guage Processing / ACL.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Kingma and Ba2014] Diederik P. Kingma and Jimmy
Ba. 2014. Adam: A Method for Stochastic Optimiza-
tion. CoRR, abs/1412.6980.

[Liu et al.2015] Xiaomo Liu, Armineh Nourbakhsh,
Quanzhi Li, Rui Fang, and Sameena Shah.
2015.
Real-time Rumor Debunking on Twitter. In Proceed-
ings of the 24th ACM International on Conference on
Information and Knowledge Management, CIKM ’15,
pages 1867–1870, New York, NY, USA. ACM.

[Mendoza et al.2010] Marcelo Mendoza,

Barbara
Poblete, and Carlos Castillo. 2010. Twitter Under
Crisis: Can We Trust What We RT? In Proceedings
of
the First Workshop on Social Media Analytics
(SOMA’2010), pages 71–79, New York, NY, USA.
ACM.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space. arXiv preprint
arXiv:1301.3781.

[Mitchell et al.2013] Margaret Mitchell, Jacqui Aguilar,
Theresa Wilson, and Benjamin Van Durme. 2013.
Open Domain Targeted Sentiment. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1643–1654, Seattle,
Washington, USA, October. Association for Computa-
tional Linguistics.

[Mohammad et al.2016] Saif M. Mohammad, Svetlana
Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin
2016. SemEval-2016 Task 6: Detecting
Cherry.
stance in tweets. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval ’16, San
Diego, California, June.

[Pang and Lee2008] Bo Pang and Lillian Lee.

2008.
Opinion mining and sentiment analysis. Foundations
and trends in information retrieval, 2(1-2):1–135.
[Pedregosa et al.2011] F. Pedregosa, G. Varoquaux,
A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. 2011. Scikit-learn:
Journal of Machine
Machine Learning in Python.
Learning Research, 12:2825–2830.

[Qazvinian et al.2011] Vahed Qazvinian, Emily Rosen-
gren, Dragomir R. Radev, and Qiaozhu Mei. 2011.

Identifying Misinformation in Mi-
Rumor Has It:
croblogs.
In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1589–1599.

[Rockt¨aschel et al.2016] Tim Rockt¨aschel,

Edward
Grefenstette, Karl Moritz Hermann, Tom´aˇs Koˇcisk`y,
and Phil Blunsom. 2016. Reasoning about Entailment
with Neural Attention.
In International Conference
on Learning Representations (ICLR).

[Socher et al.2013] Richard Socher, Alex Perelygin, Jean
Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. 2013. Recursive Deep
Models for Semantic Compositionality Over a Senti-
ment Treebank. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.
[Vo and Zhang2015] Duy-Tin Vo and Yue Zhang. 2015.
Target-Dependent Twitter Sentiment Classiﬁcation
with Rich Automatic Features.
In Qiang Yang and
Michael Wooldridge, editors, IJCAI, pages 1347–
1353. AAAI Press.

[Walker et al.2012] Marilyn Walker, Pranav Anand, Rob
Abbott, and Ricky Grant. 2012. Stance Classiﬁcation
using Dialogic Properties of Persuasion. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 592–596.
[Wei et al.2016] Wan Wei, Xiao Zhang, Xuqin Liu, Wei
pkudblab at
Chen, and Tengjiao Wang.
SemEval-2016 Task 6: A Speciﬁc Convolutional Neu-
ral Network System for Effective Stance Detection. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ’16, San Diego, California,
June.

2016.

[Zarrella and Marsh2016] Guido Zarrella

and Amy
2016. MITRE at SemEval-2016 Task 6:
Marsh.
Transfer Learning for Stance Detection.
In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ’16, San Diego, California,
June.

[Zhang et al.2015] Meishan Zhang, Yue Zhang, and
Duy Tin Vo. 2015. Neural Networks for Open Do-
main Targeted Sentiment. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 612–621, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

[Zhang et al.2016] Meishan Zhang, Yue Zhang, and Duy-
Tin Vo. 2016. Gated Neural Networks for Targeted
In Proceedings of the Thirtieth
Sentiment Analysis.
AAAI Conference on Artiﬁcial Intelligence, Phoenix,
Arizona, USA, February. Association for the Advance-
ment of Artiﬁcial Intelligence.

6
1
0
2
 
p
e
S
 
6
2
 
 
]
L
C
.
s
c
[
 
 
2
v
4
6
4
5
0
.
6
0
6
1
:
v
i
X
r
a

Stance Detection with Bidirectional Conditional Encoding

Isabelle Augenstein and Tim Rockt¨aschel
Department of Computer Science
University College London
i.augenstein@ucl.ac.uk, t.rocktaschel@cs.ucl.ac.uk

Andreas Vlachos and Kalina Bontcheva
Department of Computer Science
University of Shefﬁeld
{a.vlachos, k.bontcheva}@sheffield.ac.uk

Abstract

Stance detection is the task of classifying the
attitude expressed in a text towards a target
such as Hillary Clinton to be “positive”, “neg-
ative” or “neutral”. Previous work has as-
sumed that either the target is mentioned in
the text or that training data for every tar-
get is given. This paper considers the more
challenging version of this task, where targets
are not always mentioned and no training data
is available for the test targets. We experi-
ment with conditional LSTM encoding, which
builds a representation of the tweet that is de-
pendent on the target, and demonstrate that it
outperforms encoding the tweet and the target
independently. Performance is improved fur-
ther when the conditional model is augmented
with bidirectional encoding. We evaluate our
approach on the SemEval 2016 Task 6 Twit-
ter Stance Detection corpus achieving perfor-
mance second best only to a system trained on
semi-automatically labelled tweets for the test
target. When such weak supervision is added,
our approach achieves state–of-the-art results.

1 Introduction

The goal of stance detection is to classify the at-
titude expressed in a text towards a given target,
as “positive”, ”negative”, or ”neutral”. Such in-
formation can be useful for a variety of tasks, e.g.
Mendoza et al. (2010) showed that tweets stating ac-
tual facts were afﬁrmed by 90% of the tweets re-
lated to them, while tweets conveying false in-
formation were predominantly questioned or de-
In this paper we focus on a novel stance
nied.

detection task, namely tweet stance detection to-
wards previously unseen targets (mostly entities
such as politicians or issues of public interest), as
deﬁned in the SemEval Stance Detection for Twitter
task (Mohammad et al., 2016). This task is rather
difﬁcult, ﬁrstly due to not having training data for
the targets in the test set, and secondly, due to the
targets not always being mentioned in the tweet. For
example, the tweet “@realDonaldTrump is the only
honest voice of the @GOP” expresses a positive
stance towards the target Donald Trump. However,
when stance is annotated with respect to Hillary
Clinton as the implicit target, this tweet expresses
a negative stance, since supporting candidates from
one party implies negative stance towards candidates
from other parties.

Thus the challenge is twofold. First, we need to
learn a model that interprets the tweet stance towards
a target that might not be mentioned in the tweet it-
self. Second, we need to learn such a model without
labelled training data for the target with respect to
which we are predicting the stance. In the example
above, we need to learn a model for Hillary Clinton
by only using training data for other targets. While
this renders the task more challenging, it is a more
realistic scenario, as it is unlikely that labelled train-
ing data for each target of interest will be available.

based

network

encoding

architecture

To address

these challenges we develop a
con-
neural
(Rockt¨aschel et al., 2016).
ditional
net-
A long-short
work
is
used to encode the target, followed by a second
LSTM that encodes the tweet using the encoding

(LSTM)
(Hochreiter and Schmidhuber, 1997)

term memory

on

of the target as its initial state. We show that this
approach achieves better F1 than an SVM baseline,
or an independent LSTM encoding of the tweet and
the target. Results improve further (0.4901 F1) with
a bidirectional version of our model, which takes
into account the context on either side of the word
being encoded.
In the context of the shared task,
this would have been the second best result, except
for an approach which uses automatically labelled
tweets for the test targets (F1 of 0.5628). Lastly,
when our bidirectional conditional encoding model
is trained on such data, it achieves state-of-the-art
performance (0.5803 F1).

2 Task Setup

The SemEval 2016 Stance Detection for Twitter
shared task (Mohammad et al., 2016) consists of
two subtasks, Task A and Task B. In Task A the
goal is to detect the stance of tweets towards tar-
gets given labelled training data for all test targets
(Climate Change is a Real Concern, Feminist Move-
ment, Atheism, Legalization of Abortion and Hillary
Clinton). In Task B, which is the focus of this paper,
the goal is to detect stance with respect to an un-
seen target, Donald Trump, for which labeled train-
ing/development data is not provided.

Systems need to classify the stance of each tweet
as “positive” (FAVOR), “negative” (AGAINST) or
“neutral” (NONE) towards the target. The ofﬁcial
metric reported for the shared task is F1 macro-
averaged over the classes FAVOR and AGAINST.
Although the F1 of NONE is not considered, sys-
tems still need to predict it to avoid precision errors
for the other two classes.

Even though participants were not allowed to
manually label data for the test
target Donald
Trump, they were allowed to label data automat-
ically.
The two best-performing systems sub-
mitted to Task B, pkudblab (Wei et al., 2016) and
LitisMind (Zarrella and Marsh, 2016) made use of
this, thus changing the task to weakly supervised
seen target stance detection, instead of an unseen tar-
get task. Although the goal of this paper is to present
stance detection methods for targets for which no
training data is available, we show that they can also
be used successfully in a weakly supervised frame-
work and outperform the state-of-the-art on the Se-

mEval 2016 Stance Detection for Twitter dataset.

3 Methods

to sentiment analysis

A common stance detection approach is to treat
it as a sentence-level classiﬁcation task sim-
ilar
(Pang and Lee, 2008;
Socher et al., 2013). However, such an approach
cannot capture the stance of a tweet with respect to a
particular target, unless training data is available for
each of the test targets. In such cases, we could learn
that a tweet mentioning Donald Trump in a positive
manner expresses a negative stance towards Hillary
Clinton. Despite this limitation, we use two such
baselines, one implemented with a Support Vector
Machine (SVM) classiﬁer and one with an LSTM
network, in order to assess whether we are success-
ful in incorporating the target in stance prediction.

A naive approach to incorporate the target in
stance prediction would be to generate features con-
catenating the target with words from the tweet. Ig-
noring the issue that such features would be rather
sparse, a classiﬁer could learn that some words have
target-dependent stance weights, but it still assumes
that training data is available for each target.

In order to learn how to combine the stance target
with the tweet in a way that generalises to unseen
targets, we focus on learning distributed represen-
tations and ways to combine them. The following
sections develop progressively the proposed bidirec-
tional conditional LSTM encoding model, starting
from independently encoding the tweet and the tar-
get using LSTMs.

3.1

Independent Encoding

attempt

Our
initial
resentations for
to encode the target and tweet
using
as
LSTMs (Hochreiter and Schmidhuber, 1997).

to learn distributed rep-
the tweets and the targets is
independently
two

k-dimensional

vectors

dense

(cid:20)

H =

xt
ht−1 (cid:21)
it = σ(WiH + bi)
ft = σ(Wf H + bf )
ot = σ(WoH + bo)
ct = ft ⊙ ct−1 + it ⊙ tanh(WcH + bc)
ht = ot ⊙ tanh(ct)

h←
1

h→
1

c←
1

c→
1

h←
2

h→
2

c←
2

c→
2

h←
3

h→
3

c←
3

c→
3

h←
4

h→
4

c←
4

c→
4

h←
5

h→
5

c←
5

c→
5

h←
6

h→
6

c←
6

c→
6

h←
7

h→
7

c←
7

c→
7

h←
8

h→
8

c←
8

c→
8

x1

x2

x3

x4

x5

x6

x7

x8

Legalization of Abortion

A

foetus

has

rights

too

Target

Tweet

h←
9

h→
9

c←
9

c→
9

x9

!

Figure 1: Bidirectional encoding of tweet conditioned on bidirectional encoding of target ([c→
the last forward and reversed output representations ([h→

9 h←

4 ]).

3 c←

1 ]). The stance is predicted using

Here, xt is an input vector at time step t, ct denotes
the LSTM memory, ht ∈ Rk is an output vector and
the remaining weight matrices and biases are train-
able parameters. We concatenate the two output vec-
tor representations and classify the stance using the
softmax over a non-linear projection

softmax(tanh(Wtahtarget + Wtwhtweet + b))

into the space of the three classes for stance detec-
tion where Wta, Wtw ∈ R3×k are trainable weight
matrices and b ∈ R3 is a trainable class bias. This
model learns target-independent distributed repre-
sentations for the tweets and relies on the non-
linear projection layer to incorporate the target in the
stance prediction.

3.2 Conditional Encoding

In order to learn target-dependent tweet representa-
tions, we use conditional encoding as previously ap-
plied to the task of recognising textual entailment
(Rockt¨aschel et al., 2016). We use one LSTM to en-
code the target as a ﬁxed-length vector. Then, we
encode the tweet with another LSTM, whose state
is initialised with the representation of the target.
Finally, we use the last output vector of the tweet
LSTM to predict the stance of the target-tweet pair.
Formally, let (x1, . . . , xT ) be a sequence of tar-
get word vectors, (xT +1, . . . , xN ) be a sequence of
tweet word vectors and [h0 c0] be a start state of

zeros. The two LSTMs map input vectors and a pre-
vious state to a next state as follows:

[h1 c1] = LSTMtarget(x1, h0, c0)

[hT cT ] = LSTMtarget(xT , hT −1, cT −1)

[hT +1 cT +1] = LSTMtweet(xT +1, h0, cT )

. . .

. . .

[hN cN ] = LSTMtweet(xN , hN −1, cN −1)

Finally, the stance of the tweet w.r.t.
classiﬁed using a non-linear projection

the target is

c = tanh(WhN )

where W ∈ R3×k is a trainable weight matrix.
This effectively allows the second LSTM to read the
tweet in a target-speciﬁc manner, which is crucial
since the stance of the tweet depends on the target
(recall the Donald Trump example above).

3.3 Bidirectional Conditional Encoding

Bidirectional
LSTMs
(Graves and Schmidhuber, 2005) have been shown
to learn improved representations of sequences by
encoding a sequence from left to right and from
right to left. Therefore, we adapt the conditional en-
coding model from Section 3.2 to use bidirectional
LSTMs, which represent the target and the tweet
using two vectors for each of them, one obtained

by reading the target and then the tweet left-to-right
(as in the conditional LSTM encoding) and one
obtained by reading them right-to-left. To achieve
this, we initialise the state of the bidirectional
LSTM that reads the tweet by the last state of the
forward and reversed encoding of the target (see
Figure 1). The bidirectional encoding allows the
model to construct target-dependent representations
of the tweet such that when a word is considered,
both its left- and the right-hand side context are
taken into account.

3.4 Unsupervised Pretraining

In order to counter-balance the relatively small
amount of training data available (5,628 instances
in total), we employ unsupervised pre-training
by initialising the word embeddings used in the
LSTMs with an appropriately trained word2vec
model (Mikolov et al., 2013). Note that these em-
beddings are used only for initialisation, as we allow
them to be optimised further during training.

In more detail, we train a word2vec model on a
corpus of 395,212 unlabelled tweets, collected with
the Twitter Keyword Search API1 between Novem-
ber 2015 and January 2016, plus all the tweets con-
tained in the ofﬁcial SemEval 2016 Stance Detec-
tion datasets (Mohammad et al., 2016). The unla-
belled tweets are collected so that they contain the
targets considered in the shared task, using up to
two keywords per target, namely “hillary”, “clin-
ton”, “trump”, “climate”, “femini”, “aborti”. Note
that Twitter does not allow for regular expression
search, so this is a free text search disregarding pos-
sible word boundaries. We combine this large unla-
belled corpus with the ofﬁcial training data and train
a skip-gram word2vec model (dimensionality 100, 5
min words, context window of 5).

Tweets and targets are tokenised with the Twitter-
adapted tokeniser twokenize2. Subsequently, all to-
kens are lowercased, URLs are removed, and stop-
word tokens are ﬁltered (i.e. punctuation characters,
Twitter-speciﬁc stopwords (“rt”, “#semst”, “via”).

As it will be shown in our experiments, unsuper-
vised pre-training is quite helpful, since it is difﬁcult

1https://dev.twitter.com/rest/public/

search

2https://github.com/leondz/twokenize

Corpus

Favor Against None

All

TaskA Tr+Dv
1462
TaskA Tr+Dv HC 224
TaskB Unlab
TaskB Auto-lab*
TaskB Test
Crawled Unlab*

-
4681
148
-

2684
722
-
5095
299
-

1482
332
-
4026
260
-

5628
1278
278,013
13,802
707
395,212

Table 1: Data sizes of available corpora. TaskA Tr+Dv HC
is the part of TaskA Tr+Dv with tweets for the target Hillary
Clinton only, which we use for development. TaskB Auto-
lab is an automatically labelled version of TaskB Unlab.

Crawled Unlab is an unlabelled tweet corpus collected by us.

to learn representations for all the words using only
the relatively small training datasets available.

Finally, to ensure that the proposed neural net-
work architectures contribute to the performance,
we also use the word vectors from word2vec to de-
velop a Bag-of-Word-Vectors baseline (BOWV), in
which the tweet and target representations are fed
into a logistic regression classiﬁer with L2 regular-
ization (Pedregosa et al., 2011).

4 Experiments

Experiments are performed on the SemEval 2016
Task 6 corpus for Stance Detection on Twit-
ter (Mohammad et al., 2016). We report experi-
ments for two different experimental setups: one
is the unseen target setup (Section 5), which is the
main focus of this paper, i.e. detecting the stance of
tweets towards previously unseen targets. We show
that conditional encoding, by reading the tweets in
a target-speciﬁc way, generalises to unseen targets
better than baselines which ignore the target. Next,
we compare our approach to previous work in a
weakly supervised framework (Section 6) and show
that our approach outperforms the state-of-the-art on
the SemEval 2016 Stance Detection Subtask B cor-
pus.

Table 1 lists the various corpora used in the ex-
periments and their sizes. TaskA Tr+Dv is the
ofﬁcial SemEval 2016 Twitter Stance Detection
TaskA training and development corpus, which
contain instances for the targets Legalization of
Abortion, Atheism, Feminist Movement, Climate
Change is a Real Concern and Hillary Clinton.
TaskA Tr+Dv HC is the part of the corpus which
contains only the Hillary Clinton tweets, which

we use for development purposes. TaskB Test
is the TaskB test corpus on which we report re-
sults containing Donald Trump testing instances.
TaskB Unlab is an unlabelled corpus containing
Donald Trump tweets supplied by the task organ-
isers, and TaskB Auto-lab* is an automatically la-
belled version of a small portion of the corpus for
the weakly supervised stance detection experiments
reported in Section 6. Finally, Crawled Unlab* is
a corpus we collected for unsupervised pre-training
(see Section 3.4).

Method

Stance

P

R

F1

BoWV

FAVOR

0.2444
AGAINST 0.5916

0.0940
0.8626

TweetOnly

FAVOR

0.2127
AGAINST 0.6529

0.5726
0.4020

Concat

FAVOR

0.1811
AGAINST 0.6299

0.6239
0.4504

TarCondTweet

FAVOR

0.3293
AGAINST 0.4304

0.3649
0.5686

is used.

For all experiments, the ofﬁcial task evaluation
script
Predictions are post processed
so that if the target is contained in a tweet, the
highest-scoring non-neutral stance is chosen. This
in the
was motivated by the observation that
training data most target-containing tweets express
a stance, with only 16% of them being neutral.
The code used in our experiments is available from
https://github.com/sheffieldnlp/stance-conditional.

opment setup.

TweetCondTar

BiCond

FAVOR

0.1985
AGAINST 0.6332

0.2308
0.7379

FAVOR

0.2588
AGAINST 0.7081

0.3761
0.5802

Table 2: Results for the unseen target stance detection devel-

0.1358
0.7019
0.4188

0.3102
0.4976
0.4039

0.2808
0.5252
0.4030

0.3462
0.4899
0.4180

0.2134
0.6816
0.4475

0.3066
0.6378
0.4722

Macro

Macro

Macro

Macro

Macro

Macro

4.1 Methods

We compare the following baseline methods:

• SVM trained with word and character
(SVM-ngrams-

tweet
comb) Mohammad et al. (2016)

n-grams

features

• a majority class baseline (Majority baseline),

reported in (Mohammad et al., 2016)

• bag of word vectors (BoWV) (see Section 3.4)
• independent encoding of tweet and the target
with two LSTMs (Concat) (see Section 3.1)
• encoding of the tweet only with an LSTM

(TweetOnly) (see Section 3.1)

to three versions of conditional encoding:

• target conditioned on tweet (TarCondTweet)
• tweet conditioned on target (TweetCondTar)
• a bidirectional encoding model (BiCond)

5 Unseen Target Stance Detection

models, we used the tweets labelled for Hillary Clin-
ton as a development set and the tweets for the re-
maining four targets as training. We refer to this as
the development setup, and all models are tuned us-
ing this setup. The labelled Donald Trump tweets
were only used in reporting our ﬁnal results.

For the ﬁnal results we train on all the data from
the development setup and evaluate on the ofﬁcial
Task B test set, i.e. the Donald Trump tweets. We
refer to this as our test setup.

Based on a small grid search using the develop-
ment setup, the following settings for LSTM-based
models were chosen: input layer size 100 (equal to
the word embedding dimension), hidden layer size
of 60, training for max 50 epochs with initial learn-
ing rate 1e-3 using ADAM (Kingma and Ba, 2014)
for optimisation, dropout 0.1. Models were trained
using cross-entropy loss. The use of one, relatively
small hidden layer and dropout help to avoid over-
ﬁtting.

As explained earlier,
the challenge is to learn a
model without any manually labelled training data
for the test target, but only using the data from the
Task A targets.
In order to avoid using any la-
belled data for Donald Trump, while still having a
(labelled) development set to tune and evaluate our

5.1 Results and Discussion

Results for the unseen target setting show how well
conditional encoding is suited for learning target-
dependent representations of tweets, and crucially,
how well such representations generalise to unseen
targets. The best performing method on both de-

Method

Stance

P

R

F1

BoWV

FAVOR

0.3158
AGAINST 0.4316

0.0405
0.8963

TweetOnly

FAVOR

0.2767
AGAINST 0.4225

0.3851
0.5284

Concat

FAVOR

0.3145
AGAINST 0.4452

0.5270
0.4348

TarCondTweet

FAVOR

0.2322
AGAINST 0.6712

0.4188
0.6234

TweetCondTar

FAVOR

0.3710
AGAINST 0.4633

0.5541
0.5485

BiCond

FAVOR

0.3033
AGAINST 0.6788

0.5470
0.5216

Macro

Macro

Macro

Macro

Macro

Macro

0.0719
0.5826
0.3272

0.3220
0.4695
0.3958

0.3939
0.4399
0.4169

0.2988
0.6464
0.4726

0.4444
0.5023
0.4734

0.3902
0.5899
0.4901

Table 3: Results for the unseen target stance detection test

setup.

EmbIni

NumMatr

Stance

P

R

F1

Random

PreFixed

PreCont

Sing

Sep

Sing

Sep

Sing

Sep

FAVOR

0.1982
AGAINST 0.6263

0.3846
0.5929

FAVOR

0.2278
AGAINST 0.6706

0.5043
0.4300

FAVOR

0.6000
AGAINST 0.5761

0.0513
0.9440

FAVOR

0.1429
AGAINST 0.5707

0.0342
0.9033

FAVOR

0.2588
AGAINST 0.7081

0.3761
0.5802

FAVOR

0.2243
AGAINST 0.6185

0.4103
0.5445

Macro

Macro

Macro

Macro

Macro

Macro

0.2616
0.6092
0.4354

0.3138
0.5240
0.4189

0.0945
0.7155
0.4050

0.0552
0.6995
0.3773

0.3066
0.6378
0.4722

0.2900
0.5792
0.4346

Table 4: Results for the unseen target stance detection develop-
ment setup using BiCond, with single vs separate embeddings

matrices for tweet and target and different initialisations

velopment (Table 2) and test setups (Table 3) is Bi-
Cond, which achieves an F1 of 0.4722 and 0.4901
respectively. Notably, Concat, which learns an in-
dependent encoding of the target and the tweets,
does not achieve big F1 improvements over Twee-
tOnly, which learns a representation of the tweets

only. This shows that it is not sufﬁcient to just take
the target into account, but is is important to learn
target-dependent encodings for the tweets. Models
that learn to condition the encoding of tweets on tar-
gets outperform all baselines on the test set.

It is further worth noting that the Bag-of-Word-
Vectors baseline achieves results comparable with
TweetOnly, Concat and one of the conditional en-
coding models, TarCondTweet, on the dev set,
even though it achieves signiﬁcantly lower perfor-
mance on the test set.
the
pre-trained word embeddings on their own are al-
ready very useful for stance detection. This is con-
sistent with ﬁndings of other works showing the
usefulness of such a Bag-of-Word-Vectors baseline
for the related tasks of recognising textual entail-
ment Bowman et al. (2015) and sentiment analysis
Eisner et al. (2016).

This indicates that

Our best result in the test setup with BiCond is the
second highest reported result on the Twitter Stance
Detection corpus, however the ﬁrst, third and fourth
best approaches achieved their results by automati-
cally labelling Donald Trump training data. BiCond
for the unseen target setting outperforms the third
and fourth best approaches by a large margin (5 and
7 points in Macro F1, respectively), as can be seen
in Table 7. Results for weakly supervised stance de-
tection are discussed in Section 6.

Pre-Training Table 4 shows the effect of unsu-
pervised pre-training of word embeddings with a
word2vec skip-gram model, and furthermore, the re-
sults of sharing of these representations between the
tweets and targets, on the development set. The ﬁrst
set of results is with a uniformly Random embed-
ding initialisation in [−0.1, 0.1]. PreFixed uses the
pre-trained skip-gram word embeddings, whereas
PreCont initialises the word embeddings with ones
from SkipGram and continues training them dur-
ing LSTM training. Our results show that, in the
absence of a large labelled training dataset, pre-
training of word embeddings is more helpful than
random initialisation of embeddings. Sing vs Sep
shows the difference between using shared vs two
separate embeddings matrices for looking up the
word embeddings. Sing means the word represen-
tations for tweet and target vocabularies are shared,
whereas Sep means they are different. Using shared

Method

inTwe

Stance

P

R

F1

Method

Stance

P

R

F1

Concat

TweetCondTar

BiCond

Yes

No

Yes

No

Yes

No

FAVOR

0.3153 0.6214 0.4183
AGAINST 0.7438 0.4630 0.5707
0.4945

Macro

FAVOR

0.0450 0.6429 0.0841
AGAINST 0.4793 0.4265 0.4514
0.2677

Macro

FAVOR

0.3529 0.2330 0.2807
AGAINST 0.7254 0.8327 0.7754
0.5280

Macro

FAVOR

0.0441 0.2143 0.0732
AGAINST 0.4663 0.5588 0.5084
0.2908

Macro

FAVOR

0.3585 0.3689 0.3636
AGAINST 0.7393 0.7393 0.7393
0.5515

Macro

FAVOR

0.0938 0.4286 0.1538
AGAINST 0.5846 0.2794 0.3781
0.2660

Macro

BoWV

FAVOR

0.5156
AGAINST 0.6266

0.6689
0.3311

TweetOnly

FAVOR

0.5284
AGAINST 0.5774

0.6284
0.4615

Concat

FAVOR

0.5506
AGAINST 0.5794

0.5878
0.4883

TarCondTweet

FAVOR

0.5636
AGAINST 0.5947

0.6284
0.4515

TweetCondTar

FAVOR

0.5868
AGAINST 0.5915

0.6622
0.4649

BiCond

FAVOR

0.6268
AGAINST 0.6057

0.6014
0.4983

Macro

Macro

Macro

Macro

Macro

Macro

0.5824
0.4333
0.5078

0.5741
0.5130
0.5435

0.5686
0.5299
0.5493

0.5942
0.5133
0.5538

0.6222
0.5206
0.5714

0.6138
0.5468
0.5803

Table 5: Results for the unseen target stance detection devel-

opment setup for tweets containing the target vs tweets not con-

taining the target.

Table 6: Stance Detection test results for weakly super-

vised setup, trained on automatically labelled pos+neg+neutral

Trump data, and reported on the ofﬁcial test set.

embeddings performs better, which we hypothesise
is because the tweets contain some mentions of tar-
gets that are tested.

Target in Tweet vs Not in Tweet Table 5 shows
results on the development set for BiCond, com-
pared to the best unidirectional encoding model,
TweetCondTar and the baseline model Concat,
split by tweets that contain the target and those that
do not. All three models perform well when the
target is mentioned in the tweet, but less so when
the targets are not mentioned explicitly. In the case
where the target is mentioned in the tweet, bicon-
ditional encoding outperforms unidirectional encod-
ing and unidirectional encoding outperforms Con-
cat. This shows that conditional encoding is able
to learn useful dependencies between the tweets and
the targets.

6 Weakly Supervised Stance Detection

The previous section showed the usefulness of con-
ditional encoding for unseen target stance detec-
internal base-
tion and compared results against
lines.
The goal of experiments reported in
this section is to compare against participants
in the SemEval 2016 Stance Detection Task B.

including the three highest

While we consider an unseen target setup, most
submissions,
rank-
ing ones for Task B, pkudblab (Wei et al., 2016),
LitisMind (Zarrella and Marsh, 2016) and INF-
UFRGS (Dias and Becker, 2016) considered a dif-
ferent experimental setup. They automatically anno-
tated training data for the test target Donald Trump,
thus converting the task into weakly supervised
seen target stance detection. The pkudblab sys-
tem uses a deep convolutional neural network that
learns to make 2-way predictions on automatically
labelled positive and negative training data for Don-
ald Trump. The neutral class is predicted according
to rules which are applied at test time.

Since the best performing systems which partic-
ipated in the shared task consider a weakly super-
vised setup, we further compare our proposed ap-
proach to the state-of-the-art using such a weakly
supervised setup. Note that, even though pkudblab,
LitisMind and INF-UFRGS also use regular expres-
sions to label training data automatically, the result-
ing datasets were not available to us. Therefore, we
had to develop our own automatic labelling method
and dataset, which are publicly available from our
code repository.

Weakly Supervised Test Setup For this setup, the
unlabelled Donald Trump corpus TaskB Unlab is
annotated automatically. For this purpose we cre-
ated a small set of regular expressions3, based on
inspection of the TaskB Unlab corpus, expressing
positive and negative stance towards the target. The
regular expressions for the positive stance were:

• make( ?)america( ?)great( ?)again
• trump( ?)(for|4)( ?)president
• votetrump
• trumpisright
• the truth
• #trumprules
keyphrases

negative

Method

SVM-ngrams-comb (Unseen Target)

Majority baseline (Unseen Target)

BiCond (Unseen Target)

INF-UFRGS (Weakly Supervised*)

The
#dumptrump,
idiot, ﬁred

for
#notrump,

stance were:
racist,

#trumpwatch,

LitisMind (Weakly Supervised*)

pkudblab (Weakly Supervised*)

Stance

F1

FAVOR

0.1842
AGAINST 0.3845
0.2843

Macro

FAVOR

0.0

AGAINST 0.5944
0.2972

Macro

FAVOR

0.3902
AGAINST 0.5899
0.4901

Macro

FAVOR

0.3256
AGAINST 0.5209
0.4232

Macro

FAVOR

0.3004
AGAINST 0.5928
0.4466

Macro

FAVOR

0.5739
AGAINST 0.5517
0.5628

Macro

FAVOR

0.6138
AGAINST 0.5468
0.5803

Macro

A tweet is labelled as positive if one of the posi-
tive expressions is detected, else negative if a nega-
tive expressions is detected. If neither are detected,
the tweet is annotated as neutral randomly with 2%
chance. The resulting corpus size per stance is
shown in Table 1. The same hyperparameters for
the LSTM-based models are used as for the unseen
target setup described in the previous section.

6.1 Results and Discussion

Table 6 lists our results in the weakly supervised
setting.
includ-
Table 7 shows all our results,
ing those using the unseen target setup, compared
the state-of-the-art on the stance detec-
against
tion corpus.
Table 7 further lists baselines re-
ported by Mohammad et al. (2016), namely a major-
ity class baseline (Majority baseline), and a method
using 1 to 3-gram bag-of-word and character n-gram
features (SVM-ngrams-comb), which are extracted
from the tweets and used to train a 3-way SVM clas-
siﬁer.

Bag-of-word baselines (BoWV, SVM-ngrams-
comb) achieve results comparable to the majority
baseline (F1 of 0.2972), which shows how difﬁ-
cult the task is. The baselines which only extract
features from the tweets, SVM-ngrams-comb and
TweetOnly perform worse than the baselines which
also learn representations for the targets (BoWV,
Concat). By training conditional encoding models

3Note that “|” indiates “or”, ( ?) indicates optional space

BiCond (Weakly Supervised)

Table 7: Stance Detection test results, compared against the
state of the art. SVM-ngrams-comb and Majority base-
line are reported in Mohammad et al. (2016), pkudblab in

Wei et al. (2016), LitisMind in Zarrella and Marsh (2016), INF-

UFRGS in Dias and Becker (2016)

on automatically labelled stance detection data we
achieve state-of-the-art results. The best result (F1
of 0.5803) is achieved with the bi-directional condi-
tional encoding model (BiCond). This shows that
such models are suitable for unseen, as well as seen
target stance detection.

7 Related Work

Stance Detection: Previous work mostly consid-
ered target-speciﬁc stance prediction in debates
(Hasan and Ng, 2013; Walker et al., 2012) or stu-
dent essays (Faulkner, 2014). The task considered in
this paper is more challenging than stance detection
in debates because, in addition to irregular language,
the Mohammad et al. (2016) dataset is offered with-
out any context, e.g., conversational structure or
tweet metadata. The targets are also not always men-
tioned in the tweets, which is an additional challenge
(Augenstein et al., 2016) and distinguishes this task
(Vo and Zhang, 2015;
from
and
Zhang et al., 2016; Alghunaim et al., 2015)

target-dependent

sentiment

target-dependent

(Mitchell et al., 2013;

anal-
open-domain
ysis
Zhang et al., 2015).
Related work on rumour stance detection ei-
ther
requires training data from the same ru-
target, or is
mour (Qazvinian et al., 2011),
rule-based (Liu et al., 2015) and thus potentially
hard to generalise. Finally,
the target-dependent
stance detection task tackled in this paper is dif-
ferent
from that of Ferreira and Vlachos (2016),
which while related concerned with the stance of
a statement
language towards another
statement.

in natural

i.e.,

Conditional Encoding: Conditional encoding
has been applied to the related task of recog-
nising textual entailment (Rockt¨aschel et al., 2016),
using a dataset of half a million training exam-
ples (Bowman et al., 2015) and numerous different
hypotheses. Our experiments here show that con-
ditional encoding is also successful on a relatively
small training set and when applied to an unseen
testing target. Moreover, we augment conditional
encoding with bidirectional encoding and demon-
strate the added beneﬁt of unsupervised pre-training
of word embeddings on unlabelled domain data.

8 Conclusions and Future Work

This paper showed that conditional LSTM encod-
ing is a successful approach to stance detection for
unseen targets. Our unseen target bidirectional con-
ditional encoding approach achieves the second best
results reported to date on the SemEval 2016 Twitter
Stance Detection corpus. In the weakly supervised
seen target scenario, as considered by prior work,
our approach achieves the best results to date on the
SemEval Task B dataset. We further show that in the
absence of large labelled corpora, unsupervised pre-
training can be used to learn target representations
for stance detection and improves results on the Se-
mEval corpus. Future work will investigate further
the challenge of stance detection for tweets which
do not contain explicit mentions of the target.

Acknowledgments

This work was partially supported by the European
Union under grant agreement No. 611233 PHEME4

4http://www.pheme.eu

and by Microsoft Research through its PhD Scholar-
ship Programme.

References

[Alghunaim et al.2015] Abdulaziz Alghunaim, Mitra Mo-
htarami, Scott Cyphers, and Jim Glass.
2015. A
Vector Space Approach for Aspect Based Sentiment
Analysis. In Proceedings of the 1st Workshop on Vec-
tor Space Modeling for Natural Language Processing,
pages 116–122, Denver, Colorado, June. Association
for Computational Linguistics.

[Augenstein et al.2016] Isabelle Augenstein, Andreas
Vlachos, and Kalina Bontcheva.
2016. USFD:
Any-Target Stance Detection on Twitter with Autoen-
coders. In Proceedings of the International Workshop
on Semantic Evaluation, SemEval ’16, San Diego,
California.

[Bowman et al.2015] Samuel R. Bowman, Gabor Angeli,
Christopher Potts, and Christopher D. Manning. 2015.
A large annotated corpus for learning natural language
inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 632–642, Lisbon, Portugal, September. Associ-
ation for Computational Linguistics.

[Dias and Becker2016] Marcelo Dias and Karin Becker.
2016. INF-UFRGS-OPINION-MINING at SemEval-
2016 Task 6: Automatic Generation of a Training
Corpus for Unsupervised Identiﬁcation of Stance in
Tweets. In Proceedings of the International Workshop
on Semantic Evaluation, SemEval ’16, San Diego,
California, June.

[Eisner et al.2016] Ben Eisner, Tim Rockt¨aschel, Isabelle
Augenstein, Matko Boˇsnjak, and Sebastian Riedel.
emoji2vec: Learning Emoji Representations
2016.
from their Description.
In Proceedings of the Inter-
national Workshop on Natural Language Processing
for Social Media, SocialNLP ’16, Austin, Texas.

2014.

[Faulkner2014] Adam Faulkner.

Automated
Classiﬁcation of Stance in Student Essays: An Ap-
proach Using Stance Target Information and the
In William Eberle
Wikipedia Link-Based Measure.
and Chutima Boonthum-Denecke, editors, FLAIRS
Conference. AAAI Press.

[Ferreira and Vlachos2016] William Ferreira and An-
dreas Vlachos. 2016. Emergent: a novel data-set for
stance classiﬁcation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1163–1168, San Diego, Califor-
nia, June. Association for Computational Linguistics.
[Graves and Schmidhuber2005] Alex Graves and J¨urgen
Schmidhuber. 2005. Framewise phoneme classiﬁca-

tion with bidirectional LSTM and other neural network
architectures. Neural Networks, 18(5):602–610.
[Hasan and Ng2013] Kazi Saidul Hasan and Vincent Ng.
2013. Stance Classiﬁcation of Ideological Debates:
Data, Models, Features, and Constraints. In IJCNLP,
pages 1348–1356. Asian Federation of Natural Lan-
guage Processing / ACL.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Kingma and Ba2014] Diederik P. Kingma and Jimmy
Ba. 2014. Adam: A Method for Stochastic Optimiza-
tion. CoRR, abs/1412.6980.

[Liu et al.2015] Xiaomo Liu, Armineh Nourbakhsh,
Quanzhi Li, Rui Fang, and Sameena Shah.
2015.
Real-time Rumor Debunking on Twitter. In Proceed-
ings of the 24th ACM International on Conference on
Information and Knowledge Management, CIKM ’15,
pages 1867–1870, New York, NY, USA. ACM.

[Mendoza et al.2010] Marcelo Mendoza,

Barbara
Poblete, and Carlos Castillo. 2010. Twitter Under
Crisis: Can We Trust What We RT? In Proceedings
of
the First Workshop on Social Media Analytics
(SOMA’2010), pages 71–79, New York, NY, USA.
ACM.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation
of word representations in vector space. arXiv preprint
arXiv:1301.3781.

[Mitchell et al.2013] Margaret Mitchell, Jacqui Aguilar,
Theresa Wilson, and Benjamin Van Durme. 2013.
Open Domain Targeted Sentiment. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1643–1654, Seattle,
Washington, USA, October. Association for Computa-
tional Linguistics.

[Mohammad et al.2016] Saif M. Mohammad, Svetlana
Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin
2016. SemEval-2016 Task 6: Detecting
Cherry.
stance in tweets. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval ’16, San
Diego, California, June.

[Pang and Lee2008] Bo Pang and Lillian Lee.

2008.
Opinion mining and sentiment analysis. Foundations
and trends in information retrieval, 2(1-2):1–135.
[Pedregosa et al.2011] F. Pedregosa, G. Varoquaux,
A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. 2011. Scikit-learn:
Journal of Machine
Machine Learning in Python.
Learning Research, 12:2825–2830.

[Qazvinian et al.2011] Vahed Qazvinian, Emily Rosen-
gren, Dragomir R. Radev, and Qiaozhu Mei. 2011.

Identifying Misinformation in Mi-
Rumor Has It:
croblogs.
In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1589–1599.

[Rockt¨aschel et al.2016] Tim Rockt¨aschel,

Edward
Grefenstette, Karl Moritz Hermann, Tom´aˇs Koˇcisk`y,
and Phil Blunsom. 2016. Reasoning about Entailment
with Neural Attention.
In International Conference
on Learning Representations (ICLR).

[Socher et al.2013] Richard Socher, Alex Perelygin, Jean
Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. 2013. Recursive Deep
Models for Semantic Compositionality Over a Senti-
ment Treebank. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.
[Vo and Zhang2015] Duy-Tin Vo and Yue Zhang. 2015.
Target-Dependent Twitter Sentiment Classiﬁcation
with Rich Automatic Features.
In Qiang Yang and
Michael Wooldridge, editors, IJCAI, pages 1347–
1353. AAAI Press.

[Walker et al.2012] Marilyn Walker, Pranav Anand, Rob
Abbott, and Ricky Grant. 2012. Stance Classiﬁcation
using Dialogic Properties of Persuasion. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 592–596.
[Wei et al.2016] Wan Wei, Xiao Zhang, Xuqin Liu, Wei
pkudblab at
Chen, and Tengjiao Wang.
SemEval-2016 Task 6: A Speciﬁc Convolutional Neu-
ral Network System for Effective Stance Detection. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ’16, San Diego, California,
June.

2016.

[Zarrella and Marsh2016] Guido Zarrella

and Amy
2016. MITRE at SemEval-2016 Task 6:
Marsh.
Transfer Learning for Stance Detection.
In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ’16, San Diego, California,
June.

[Zhang et al.2015] Meishan Zhang, Yue Zhang, and
Duy Tin Vo. 2015. Neural Networks for Open Do-
main Targeted Sentiment. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 612–621, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

[Zhang et al.2016] Meishan Zhang, Yue Zhang, and Duy-
Tin Vo. 2016. Gated Neural Networks for Targeted
In Proceedings of the Thirtieth
Sentiment Analysis.
AAAI Conference on Artiﬁcial Intelligence, Phoenix,
Arizona, USA, February. Association for the Advance-
ment of Artiﬁcial Intelligence.

