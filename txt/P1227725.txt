DiSAN: Directional Self-Attention Network for
RNN/CNN-Free Language Understanding
Tianyi Zhou‡
Shirui Pan†

Tao Shen†
Jing Jiang†

Guodong Long†
Chengqi Zhang†

†Centre of Artiﬁcial Intelligence, FEIT, University of Technology Sydney
‡Paul G. Allen School of Computer Science & Engineering, University of Washington
tao.shen@student.uts.edu.au, tianyizh@uw.edu
{guodong.long, jing.jiang, shirui.pan, chengqi.zhang}@uts.edu.au

7
1
0
2
 
v
o
N
 
0
2
 
 
]
L
C
.
s
c
[
 
 
3
v
6
9
6
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recurrent neural nets (RNN) and convolutional neural nets
(CNN) are widely used on NLP tasks to capture the long-term
and local dependencies, respectively. Attention mechanisms
have recently attracted enormous interest due to their highly
parallelizable computation, signiﬁcantly less training time,
and ﬂexibility in modeling dependencies. We propose a novel
attention mechanism in which the attention between elements
from input sequence(s) is directional and multi-dimensional
(i.e., feature-wise). A light-weight neural net, “Directional
Self-Attention Network (DiSAN)”, is then proposed to learn
sentence embedding, based solely on the proposed attention
without any RNN/CNN structure. DiSAN is only composed
of a directional self-attention with temporal order encoded,
followed by a multi-dimensional attention that compresses
the sequence into a vector representation. Despite its simple
form, DiSAN outperforms complicated RNN models on both
prediction quality and time efﬁciency. It achieves the best
test accuracy among all sentence encoding methods and im-
proves the most recent best result by 1.02% on the Stanford
Natural Language Inference (SNLI) dataset, and shows state-
of-the-art test accuracy on the Stanford Sentiment Treebank
(SST), Multi-Genre natural language inference (MultiNLI),
Sentences Involving Compositional Knowledge (SICK), Cus-
tomer Review, MPQA, TREC question-type classiﬁcation
and Subjectivity (SUBJ) datasets.

1

Introduction

Context dependency plays a signiﬁcant role in language
understanding and provides critical information to natural
language processing (NLP) tasks. For different tasks and
data, researchers often switch between two types of deep
neural network (DNN): recurrent neural network (RNN)
with sequential architecture capturing long-range dependen-
cies (e.g., long short-term memory (LSTM) (Hochreiter and
Schmidhuber 1997) and gated recurrent unit (GRU) (Chung
et al. 2014)), and convolutional neural network (CNN) (Kim
2014) whose hierarchical structure is good at extracting lo-
cal or position-invariant features. However, which network
to choose in practice is an open question, and the choice re-
lies largely on the empirical knowledge.

Recent works have found that equipping RNN or CNN
with an attention mechanism can achieve state-of-the-art

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

performance on a large number of NLP tasks, including neu-
ral machine translation (Bahdanau, Cho, and Bengio 2015;
Luong, Pham, and Manning 2015), natural language infer-
ence (Liu et al. 2016), conversation generation (Shang, Lu,
and Li 2015), question answering (Hermann et al. 2015;
Sukhbaatar et al. 2015), machine reading comprehension
(Seo et al. 2017), and sentiment analysis (Kokkinos and
Potamianos 2017). The attention uses a hidden layer to com-
pute a categorical distribution over elements from the in-
put sequence to reﬂect their importance weights. It allows
RNN/CNN to maintain a variable-length memory, so that
elements from the input sequence can be selected by their
importance/relevance and merged into the output. In con-
trast to RNN and CNN, the attention mechanism is trained
to capture the dependencies that make signiﬁcant contribu-
tions to the task, regardless of the distance between the el-
ements in the sequence. It can thus provide complementary
information to the distance-aware dependencies modeled by
RNN/CNN. In addition, computing attention only requires
matrix multiplication, which is highly parallelizable com-
pared to the sequential computation of RNN.

In a very recent work (Vaswani et al. 2017), an atten-
tion mechanism is solely used to construct a sequence to
sequence (seq2seq) model that achieves a state-of-the-art
quality score on the neural machine translation (NMT) task.
The seq2seq model, “Transformer”, has an encoder-decoder
structure that is only composed of stacked attention net-
works, without using either recurrence or convolution. The
proposed attention, “multi-head attention”, projects the in-
put sequence to multiple subspaces, then applies scaled dot-
product attention to its representation in each subspace,
and lastly concatenates their output. By doing this, it can
combine different attentions from multiple subspaces. This
mechanism is used in Transformer to compute both the
context-aware features inside the encoder/decoder and the
bottleneck features between them.

The attention mechanism has more ﬂexibility in sequence
length than RNN/CNN, and is more task/data-driven when
modeling dependencies. Unlike sequential models, its com-
putation can be easily and signiﬁcantly accelerated by exist-
ing distributed/parallel computing schemes. However, to the
best of our knowledge, a neural net entirely based on atten-
tion has not been designed for other NLP tasks except NMT,
especially those that cannot be cast into a seq2seq problem.

Compared to RNN, a disadvantage of most attention mech-
anisms is that the temporal order information is lost, which
however might be important to the task. This explains why
positional encoding is applied to the sequence before being
processed by the attention in Transformer. How to model or-
der information within an attention is still an open problem.
The goal of this paper is to develop a uniﬁed and
RNN/CNN-free attention network that can be generally uti-
lized to learn the sentence encoding model for different NLP
tasks, such as natural language inference, sentiment analy-
sis, sentence classiﬁcation and semantic relatedness. We fo-
cus on the sentence encoding model because it is a basic
module of most DNNs used in the NLP literature.

We propose a novel attention mechanism that differs from
previous ones in that it is 1) multi-dimensional: the atten-
tion w.r.t. each pair of elements from the source(s) is a vec-
tor, where each entry is the attention computed on each fea-
ture; and 2) directional: it uses one or multiple positional
masks to model the asymmetric attention between two el-
ements. We compute feature-wise attention since each ele-
ment in a sequence is usually represented by a vector, e.g.,
word/character embedding (Kim et al. 2016), and attention
on different features can contain different information about
dependency, thus to handle the variation of contexts around
the same word. We apply positional masks to attention dis-
tribution since they can easily encode prior structure knowl-
edge such as temporal order and dependency parsing. This
design mitigates the weakness of attention in modeling order
information, and takes full advantage of parallel computing.
We then build a light-weight and RNN/CNN-free neural
network, “Directional Self-Attention Network (DiSAN)”,
for sentence encoding. This network relies entirely on the
proposed attentions and does not use any RNN/CNN struc-
ture. In DiSAN, the input sequence is processed by direc-
tional (forward and backward) self-attentions to model con-
text dependency and produce context-aware representations
for all tokens. Then, a multi-dimensional attention computes
a vector representation of the entire sequence, which can
be passed into a classiﬁcation/regression module to com-
pute the ﬁnal prediction for a particular task. Unlike Trans-
former, neither stacking of attention blocks nor an encoder-
decoder structure is required. The simple architecture of
DiSAN leads to fewer parameters, less computation and eas-
ier parallelization.

In experiments1, we compare DiSAN with the currently
popular methods on various NLP tasks, e.g., natural lan-
guage inference, sentiment analysis, sentence classiﬁcation,
etc. DiSAN achieves the highest test accuracy on the Stan-
ford Natural Language Inference (SNLI) dataset among
sentence-encoding models and improves the currently best
result by 1.02%. It also shows the state-of-the-art perfor-
mance on the Stanford Sentiment Treebank (SST), Multi-
Genre natural language inference (MultiNLI), SICK, Cus-
tomer Review, MPQA, SUBJ and TREC question-type clas-
siﬁcation datasets. Meanwhile, it has fewer parameters and
exhibits much higher computation efﬁciency than the mod-

1Codes and pre-trained models for experiments can be found at

https://github.com/taoshen58/DiSAN

els it outperforms, e.g., LSTM and tree-based models.

Annotation: 1) Lowercase denotes a vector; 2) bold low-
ercase denotes a sequence of vectors (stored as a matrix);
and 3) uppercase denotes a matrix or a tensor.

2 Background

2.1 Sentence Encoding
In the pipeline of NLP tasks, a sentence is denoted by a se-
quence of discrete tokens (e.g., words or characters) v =
[v1, v2, . . . , vn], where vi could be a one-hot vector whose
dimension length equals the number of distinct tokens N .
A pre-trained token embedding (e.g., word2vec (Mikolov
et al. 2013b) or GloVe (Pennington, Socher, and Manning
2014)) is applied to v and transforms all discrete tokens to
a sequence of low-dimensional dense vector representations
x = [x1, x2, . . . , xn] with xi ∈ Rde. This pre-process can
be written as x = W (e)v, where word embedding weight
matrix W (e) ∈ Rde×N and x ∈ Rde×n.

Most DNN sentence-encoding models for NLP tasks take
x as the input and further generate a vector representation ui
for each xi by context fusion. Then a sentence encoding is
obtained by mapping the sequence u = [u1, u2, . . . , un] to
a single vector s ∈ Rd, which is used as a compact encoding
of the entire sentence in NLP problems.

2.2 Attention
The attention is proposed to compute an alignment score be-
tween elements from two sources. In particular, given the to-
ken embeddings of a source sequence x = [x1, x2, . . . , xn]
and the vector representation of a query q, attention com-
putes the alignment score between xi and q by a compati-
bility function f (xi, q), which measures the dependency be-
tween xi and q, or the attention of q to xi. A softmax func-
tion then transforms the scores [f (xi, q)]n
i=1 to a probability
distribution p(z|x, q) by normalizing over all the n tokens
of x. Here z is an indicator of which token in x is important
to q on a speciﬁc task. That is, large p(z = i|x, q) means xi
contributes important information to q. The above process
can be summarized by the following equations.
a = [f (xi, q)]n
p(z|x, q) = softmax(a).

(1)
(2)

i=1 ,

Speciﬁcally,

p(z = i|x, q) =

exp(f (xi, q))
i=1 exp(f (xi, q))

.

(cid:80)n

(3)

The output of this attention mechanism is a weighted sum
of the embeddings for all tokens in x, where the weights
are given by p(z|x, q). It places large weights on the tokens
important to q, and can be written as the expectation of a
token sampled according to its importance, i.e.,

s =

p(z = i|x, q)xi = Ei∼p(z|x,q)(xi),

(4)

n
(cid:88)

i=1

where s ∈ Rde can be used as the sentence encoding of x.

Additive attention (or multi-layer perceptron attention)
(Bahdanau, Cho, and Bengio 2015; Shang, Lu, and Li

3.1 Multi-dimensional Attention
Multi-dimensional attention is a natural extension of addi-
tive attention (or MLP attention) at the feature level. Instead
of computing a single scalar score f (xi, q) for each token xi
as shown in Eq.(5), multi-dimensional attention computes a
feature-wise score vector for xi by replacing weight vector
w in Eq.(5) with a matrix W , i.e.,

f (xi, q) = W T σ

W (1)xi + W (2)q

(7)

(cid:17)

,

(cid:16)

where f (xi, q) ∈ Rde is a vector with the same length as xi,
and all the weight matrices W, W (1), W (2) ∈ Rde×de. We
further add two bias terms to the parts in and out activation
σ(·), i.e.,

f (xi, q) = W T σ

(cid:16)

W (1)xi + W (2)q + b(1)(cid:17)

+ b.

(8)

We then compute a categorical distribution p(zk|x, q) over
all the n tokens for each feature k ∈ [de]. A large p(zk =
i|x, q) means that feature k of token i is important to q.

We apply the same procedure Eq.(1)-(3) in traditional at-
tention to the kth dimension of f (xi, q). In particular, for
each feature k ∈ [de], we replace f (xi, q) with [f (xi, q)]k,
and change z to zk in Eq.(1)-(3). Now each feature k in each
token i has an importance weight Pki (cid:44) p(zk = i|x, q). The
output s can be written as
(cid:105)de

(cid:104)(cid:88)n

= (cid:2)Ei∼p(zk|x,q)(xki)(cid:3)de

k=1 .

(9)

Pkixki

s =

i=1

k=1

We give an illustration of traditional attention and multi-
dimensional attention in Figure 1. In the rest of this paper,
we will ignore the subscript k which indexes feature dimen-
sion for simpliﬁcation if no confusion is possible. Hence,
the output s can be written as an element-wise product
s = (cid:80)n

i=1 P·i (cid:12) xi

Remark: The word embedding usually suffers from the
polysemy in natural language. Since traditional attention
computes a single importance score for each word based on
the word embedding, it cannot distinguish the meanings of
the same word in different contexts. Multi-dimensional at-
tention, however, computes a score for each feature of each
word, so it can select the features that can best describe the
word’s speciﬁc meaning in any given context, and include
this information in the sentence encoding output s.

3.2 Two types of Multi-dimensional Self-attention
When extending multi-dimension to self-attentions, we have
two variants of multi-dimensional attention. The ﬁrst one,
called multi-dimensional “token2token” self-attention, ex-
plores the dependency between xi and xj from the same
source x, and generates context-aware coding for each el-
ement. It replaces q with xj in Eq.(8), i.e.,
(cid:16)

W (1)xi + W (2)xj + b(1)(cid:17)

f (xi, xj) = W T σ

+ b.

(10)

Similar to P in vanilla multi-dimensional attention, we com-
pute a probability matrix P j ∈ Rde×n for each xj such that
P j
ki

(cid:44) p(zk = i|x, xj). The output for xj is

sj =

P j

·i (cid:12) xi

n
(cid:88)

i=1

(11)

(a)

(b)

Figure 1: (a) Traditional (additive/multiplicative) attention
and (b) multi-dimensional attention. zi denotes alignment
score f (xi, q), which is a scalar in (a) but a vector in (b).

2015) and multiplicative attention (or dot-product attention)
(Vaswani et al. 2017; Sukhbaatar et al. 2015; Rush, Chopra,
and Weston 2015) are the two most commonly used atten-
tion mechanisms. They share the same and uniﬁed form of
attention introduced above, but are different in the compati-
bility function f (xi, q). Additive attention is associated with

f (xi, q) = wT σ(W (1)xi + W (2)q),
(5)
where σ(·) is an activation function and w ∈ Rde is a weight
vector. Multiplicative attention uses inner product or cosine
similarity for f (xi, q), i.e.,
(cid:68)
W (1)xi, W (2)q

f (xi, q) =

(6)

(cid:69)

.

In practice, additive attention often outperforms multiplica-
tive one in prediction quality, but the latter is faster and more
memory-efﬁcient due to optimized matrix multiplication.

2.3 Self-Attention
Self-Attention is a special case of the attention mechanism
introduced above. It replaces q with a token embedding xj
from the source input itself. It relates elements at different
positions from a single sequence by computing the attention
between each pair of tokens, xi and xj. It is very expres-
sive and ﬂexible for both long-range and local dependen-
cies, which used to be respectively modeled by RNN and
CNN. Moreover, it has much faster computation speed and
fewer parameters than RNN. In recent works, we have al-
ready witnessed its success across a variety of NLP tasks,
such as reading comprehension (Hu, Peng, and Qiu 2017)
and neural machine translation (Vaswani et al. 2017).

3 Two Proposed Attention Mechanisms
In this section, we introduce two novel attention mecha-
nisms, multi-dimensional attention in Section 3.1 (with two
extensions to self-attention in Section 3.2) and directional
self-attention in Section 3.3. They are the main components
of DiSAN and may be of independent interest to other neural
nets for other NLP problems in which an attention is needed.

In DiSA, we ﬁrst transform the input sequence x =
to a sequence of hidden state h =

[x1, x2, . . . , xn]
[h1, h2, . . . , hn] by a fully connected layer, i.e.,

h = σh

(cid:16)

W (h)x + b(h)(cid:17)

,

(14)

where x ∈ Rde×n, h ∈ Rdh×n, W (h) and b(h) are the learn-
able parameters, and σh(·) is an activation function.

We then apply multi-dimensional

token2token self-
attention to h, and generate context-aware vector represen-
tations s for all elements from the input sequence. We make
two modiﬁcations to Eq.(10) to reduce the number of param-
eters and make the attention directional.

First, we set W in Eq.(10) to a scalar c and divide the part
in σ(·) by c, and we use tanh(·) for σ(·), which reduces the
number of parameters. In experiments, we always set c = 5,
and obtain stable output.

Second, we apply a positional mask to Eq.(10), so the at-
tention between two elements can be asymmetric. Given a
mask M ∈ {0, −∞}n×n, we set bias b to a constant vec-
tor Mij1 in Eq.(10), where 1 is an all-one vector. Hence,
Eq.(10) is modiﬁed to

f (hi, hj) =

c · tanh

(cid:16)

(cid:17)
[W (1)hi + W (2)hj + b(1)]/c

+ Mij1.

(15)

To see why a mask can encode directional information, let
us consider a case in which Mij = −∞ and Mji = 0, which
results in [f (hi, hj)]k = −∞ and unchanged [f (hj, hi)]k.
Since the probability p(zk = i|x, xj) is computed by
softmax, [f (hi, hj)]k = −∞ leads to p(zk = i|x, xj) = 0.
This means that there is no attention of xj to xi on feature k.
On the contrary, we have p(zk = j|x, xi) > 0, which means
that attention of xi to xj exists on feature k. Therefore, prior
structure knowledge such as temporal order and dependency
parsing can be easily encoded by the mask, and explored in
generating sentence encoding. This is an important feature
of DiSA that previous attention mechanisms do not have.

For self-attention, we usually need to disable the attention
of each token to itself (Hu, Peng, and Qiu 2017). This is the
same as applying a diagonal-disabled (i.e., diag-disabled)
mask such that

M diag

ij =

(cid:26) 0,
−∞,

i (cid:54)= j
i = j

Moreover, we can use masks to encode temporal order
information into attention output. In this paper, we use two
masks, i.e., forward mask M f w and backward mask M bw,

M f w

ij =

M bw

ij =

(cid:26) 0,
−∞,

(cid:26) 0,
−∞,

i < j
otherwise

i > j
otherwise

(16)

(17)

(18)

In forward mask M f w, there is the only attention of later
token j to early token i, and vice versa in backward mask.
We show these three positional masks in Figure 3.

Given input sequence x and a mask M , we compute
f (xi, xj) according to Eq.(15), and follow the standard pro-
cedure of multi-dimensional token2token self-attention to

Figure 2: Directional self-attention (DiSA) mechanism.
Here, we use li,j to denote f (hi, hj) in Eq. (15).

The output of token2token self-attention for all elements
from x is s = [s1, s2, . . . , sn] ∈ Rde×n.

The second one, multi-dimensional “source2token” self-
attention, explores the dependency between xi and the entire
sequence x, and compresses the sequence x into a vector. It
removes q from Eq.(8), i.e.,

f (xi) = W T σ

(cid:16)

W (1)xi + b(1)(cid:17)

+ b.

(12)

The probability matrix is deﬁned as Pki (cid:44) p(zk = i|x)
and is computed in the same way as P in vanilla multi-
dimensional attention. The output s is also same, i.e.,

s =

P·i (cid:12) xi

(13)

n
(cid:88)

i=1

We will use these two types (i.e.,

token2token and
source2token) of multi-dimensional self-attention in differ-
ent parts of our sentence encoding model, DiSAN.

3.3 Directional Self-Attention

Directional self-attention (DiSA) is composed of a fully
connected layer whose input is the token embeddings x,
a “masked” multi-dimensional token2token self-attention
block to explore the dependency and temporal order, and a
fusion gate to combine the output and input of the attention
block. Its structure is shown in Figure 2. It can be used as
either a neural net or a module to compose a large network.

(a) Diag-disabled mask

(b) Forward mask

(c) Backward mask

Figure 3: Three positional masks: (a) is the diag-disabled
mask M diag; (b) and (c) are forward mask M f w and back-
ward mask M bw, respectively.

(19)

(20)

compute the probability matrix P j for each j ∈ [n]. Each
output sj in s is computed as in Eq.(11).

The ﬁnal output u ∈ Rdh×n of DiSA is obtained by com-
bining the output s and the input h of the masked multi-
dimensional token2token self-attention block. This yields a
temporal order encoded and context-aware vector represen-
tation for each element/token. The combination is accom-
plished by a dimension-wise fusion gate, i.e.,

F = sigmoid

(cid:16)

W (f 1)s + W (f 2)h + b(f )(cid:17)

u = F (cid:12) h + (1 − F ) (cid:12) s

where W (f 1), W (f 2) ∈ Rdh×dh and b(f ) ∈ Rdh are the
learnable parameters of the fusion gate.

4 Directional Self-Attention Network
We propose a light-weight network, “Directional Self-
Attention Network (DiSAN)”, for sentence encoding. Its ar-
chitecture is shown in Figure 4.

Given an input sequence of token embedding x, DiSAN
ﬁrstly applies two parameter-untied DiSA blocks with for-
ward mask M f w Eq.(17) and M bw Eq.(18), respectively.
The feed-forward procedure is given in Eq.(14)-(15) and
Eq.(19)-(20). Their outputs are denoted by uf w, ubw ∈
Rdh×n. We concatenate them vertically as [uf w; ubw] ∈
R2dh×n, and use this concatenated output as input to a multi-
dimensional source2token self-attention block, whose out-
put sdisan ∈ R2dh computed by Eq.(12)-(13) is the ﬁnal
sentence encoding result of DiSAN.

Remark: In DiSAN, forward/backward DiSA blocks
work as context fusion layers. And the multi-dimensional
source2token self-attention compresses the sequence into a

Figure 4: Directional self-attention network (DiSAN)

single vector. The idea of using both forward and backward
attentions is inspired by Bi-directional LSTM (Bi-LSTM)
(Graves, Jaitly, and Mohamed 2013), in which forward and
backward LSTMs are used to encode long-range depen-
dency from different directions. In Bi-LSTM, LSTM com-
bines the context-aware output with the input by multi-gate.
The fusion gate used in DiSA shares the similar motivation.
However, DiSAN has fewer parameters, simpler structure
and better efﬁciency.

5 Experiments
In this section, we ﬁrst apply DiSAN to natural language
inference and sentiment analysis tasks. DiSAN achieves
the state-of-the-art performance and signiﬁcantly better ef-
ﬁciency than other baseline methods on benchmark datasets
for both tasks. We also conduct experiments on other NLP
tasks and DiSAN also achieves state-of-the-art performance.
Training Setup: We use cross-entropy loss plus L2 regu-
larization penalty as optimization objective. We minimize
it by Adadelta (Zeiler 2012) (an optimizer of mini-batch
SGD) with batch size of 64. We use Adadelta rather than
Adam (Kingma and Ba 2015) because in our experiments,
DiSAN optimized by Adadelta can achieve more stable per-
formance than Adam optimized one. Initial learning rate is
set to 0.5. All weight matrices are initialized by Glorot Ini-
tialization (Glorot and Bengio 2010), and the biases are ini-
tialized with 0. We initialize the word embedding in x by
300D GloVe 6B pre-trained vectors (Pennington, Socher,
and Manning 2014). The Out-of-Vocabulary words in train-
ing set are randomly initialized by uniform distribution be-
tween (−0.05, 0.05). The word embeddings are ﬁne-tuned
during the training phrase. We use Dropout (Srivastava et
al. 2014) with keep probability 0.75 for language inference
and 0.8 for sentiment analysis. The L2 regularization de-
cay factors γ are 5 × 10−5 and 10−4 for language inference
and sentiment analysis, respectively. Note that the dropout
keep probability and γ varies with the scale of correspond-
ing dataset. Hidden units number dh is set to 300. Activa-
tion functions σ(·) are ELU (exponential linear unit) (Clev-
ert, Unterthiner, and Hochreiter 2016) if not speciﬁed. All
models are implemented with TensorFlow2 and run on sin-

2https://www.tensorﬂow.org

Model Name

|θ|

T(s)/epoch Train Accu(%) Test Accu(%)

Unlexicalized features (Bowman et al. 2015)
+ Unigram and bigram features (Bowman et al. 2015)

100D LSTM encoders (Bowman et al. 2015)
300D LSTM encoders (Bowman et al. 2016)
1024D GRU encoders (Vendrov et al. 2016)
300D Tree-based CNN encoders (Mou et al. 2016)
300D SPINN-PI encoders (Bowman et al. 2016)
600D Bi-LSTM encoders (Liu et al. 2016)
300D NTI-SLSTM-LSTM encoders (Munkhdalai and Yu 2017b)
600D Bi-LSTM encoders+intra-attention (Liu et al. 2016)
300D NSE encoders (Munkhdalai and Yu 2017a)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

Directional self-attention network (DiSAN)

0.2m
3.0m
15m
3.5m
3.7m
2.0m
4.0m
2.8m
3.0m

0.45m
0.54m
1.98m
2.88m
2.35m

2.35m

216
261
345
2080
592

587

49.4
99.7

84.8
83.9
98.8
83.3
89.2
86.4
82.5
84.5
86.2

82.39
86.22
89.58
90.39
90.18

91.08

50.4
78.2

77.6
80.6
81.4
82.1
83.2
83.3
83.4
84.2
84.6

79.81
83.12
84.17
84.98
84.66

85.62

Table 1: Experimental results for different methods on SNLI. |θ|: the number of parameters (excluding word embedding part).
T(s)/epoch: average time (second) per epoch. Train Accu(%) and Test Accu(%): the accuracy on training and test set.

gle Nvidia GTX 1080Ti graphic card.

5.1 Natural Language Inference
The goal of Natural Language Inference (NLI) is to rea-
son the semantic relationship between a premise sentence
and a corresponding hypothesis sentence. The possible rela-
tionship could be entailment, neutral or contradiction. We
compare different models on a widely used benchmark,
Stanford Natural Language Inference (SNLI)3 (Bowman et
al. 2015) dataset, which consists of 549,367/9,842/9,824
(train/dev/test) premise-hypothesis pairs with labels.

Following the standard procedure in Bowman et al.
(2016), we launch two sentence encoding models (e.g.,
DiSAN) with tied parameters for the premise sentence and
hypothesis sentence, respectively. Given the output encoding
sp for the premise and sh for the hypothesis, the represen-
tation of relationship is the concatenation of sp, sh, sp − sh
and sp (cid:12) sh, which is fed into a 300D fully connected layer
and then a 3-unit output layer with softmax to compute a
probability distribution over the three types of relationship.
For thorough comparison, besides the neural nets pro-
posed in previous works of NLI, we implement ﬁve extra
neural net baselines to compare with DiSAN. They help
us to analyze the improvement contributed by each part of
DiSAN and to verify that the two attention mechanisms pro-
posed in Section 3 can improve other networks.

• Word Embedding with additive attention.

• Word Embedding with s2t self-attention: DiSAN with

DiSA blocks removed.

• Multi-head with s2t self-attention: Multi-head attention
(Vaswani et al. 2017) (8 heads, each has 75 hidden units)
with source2token self-attention. The positional encoding

3https://nlp.stanford.edu/projects/snli/

method used in Vaswani et al. (2017) is applied to the in-
put sequence to encode temporal information. We ﬁnd our
experiments show that multi-head attention is sensitive to
hyperparameters, so we adjust keep probability of dropout
from 0.7 to 0.9 with step 0.05 and report the best result.
• Bi-LSTM with s2t self-attention: a multi-dimensional
source2token self-attention block is applied to the output
of Bi-LSTM (300D forward + 300D backward LSTMs).
• DiSAN without directions: DiSAN with the for-
ward/backward masks M f w and M bw replaced with two
diag-disabled masks M diag, i.e., DiSAN without for-
ward/backward order information.
Compared to the results from the ofﬁcial leaderboard
of SNLI in Table 1, DiSAN outperforms previous works
and improves the best latest test accuracy (achieved by a
memory-based NSE encoder network) by a remarkable mar-
gin of 1.02%. DiSAN surpasses the RNN/CNN based mod-
els with more complicated architecture and more parameters
by large margins, e.g., +2.32% to Bi-LSTM, +1.42% to Bi-
LSTM with additive attention. It even outperforms models
with the assistance of a semantic parsing tree, e.g., +3.52%
to Tree-based CNN, +2.42% to SPINN-PI.

In the results of the ﬁve baseline methods and DiSAN at
the bottom of Table 1, we demonstrate that making attention
multi-dimensional (feature-wise) or directional brings sub-
stantial improvement to different neural nets. First, a com-
parison between the ﬁrst two models shows that changing
token-wise attention to multi-dimensional/feature-wise at-
tention leads to 3.31% improvement on a word embedding
based model. Also, a comparison between the third baseline
and DiSAN shows that DiSAN can substantially outperform
multi-head attention by 1.45%. Moreover, a comparison be-
tween the forth baseline and DiSAN shows that the DiSA
block can even outperform Bi-LSTM layer in context en-
coding, improving test accuracy by 0.64%. A comparison

between the ﬁfth baseline and DiSAN shows that directional
self-attention with forward and backward masks (with tem-
poral order encoded) can bring 0.96% improvement.

Additional advantages of DiSAN shown in Table 1 are
its fewer parameters and compelling time efﬁciency. It is
×3 faster than widely used Bi-LSTM model. Compared to
other models with competitive performance, e.g., 600D Bi-
LSTM encoders with intra-attention (2.8M), 300D NSE en-
coders (3.0M) and 600D Bi-LSTM encoders with multi-
dimensional attention (2.88M), DiSAN only has 2.35M pa-
rameters.

5.2 Sentiment Analysis

Model

Test Accu

MV-RNN (Socher et al. 2013)
RNTN (Socher et al. 2013)
Bi-LSTM (Li et al. 2015)
Tree-LSTM (Tai, Socher, and Manning 2015)
CNN-non-static (Kim 2014)
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015)
NCSL (Teng, Vo, and Zhang 2016)
LR-Bi-LSTM (Qian, Huang, and Zhu 2017)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

DiSAN

44.4
45.7
49.8
51.0
48.0
51.2
51.1
50.6

47.47
48.87
49.14
49.95
49.41

51.72

Table 2: Test accuracy of ﬁne-grained sentiment analysis on
Stanford Sentiment Treebank (SST) dataset.

Sentiment analysis aims to analyze the sentiment of a
sentence or a paragraph, e.g., a movie or a product re-
view. We use Stanford Sentiment Treebank (SST)4 (Socher
et al. 2013) for the experiments, and only focus on the
ﬁne-grained movie review sentiment classiﬁcation over ﬁve
classes, i.e., very negative, negative, neutral, positive and
very positive. We use the standard train/dev/test sets split
with 8,544/1,101/2,210 samples. Similar to Section 5.1, we
employ a single sentence encoding model to obtain a sen-
tence representation s of a movie review, then pass it into
a 300D fully connected layer. Finally, a 5-unit output layer
with softmax is used to calculate a probability distribution
over the ﬁve classes.

In Table 2, we compare previous works with DiSAN on
test accuracy. To the best of our knowledge, DiSAN im-
proves the last best accuracy (given by CNN-Tensor) by
0.52%. Compared to tree-based models with heavy use of
the prior structure, e.g., MV-RNN, RNTN and Tree-LSTM,
DiSAN outperforms them by 7.32%, 6.02% and 0.72%,
respectively. Additionally, DiSAN achieves better perfor-
mance than CNN-based models. More recent works tend
to focus on lexicon-based sentiment analysis, by explor-
ing sentiment lexicons, negation words and intensity words.

4https://nlp.stanford.edu/sentiment/

Nonetheless, DiSAN still outperforms these fancy models,
such as NCSL (+0.62%) and LR-Bi-LSTM (+1.12%).

Figure 5: Fine-grained sentiment analysis accuracy vs. sen-
tence length. The results of LSTM, Bi-LSTM and Tree-
LSTM are from Tai, Socher, and Manning (2015) and the
result of DiSAN is the average over ﬁve random trials.

It is also interesting to see the performance of different
models on the sentences with different lengths. In Figure 5,
we compare LSTM, Bi-LSTM, Tree-LSTM and DiSAN on
different sentence lengths. In the range of (5, 12), the length
range for most movie review sentences, DiSAN signiﬁcantly
outperforms others. Meanwhile, DiSAN also shows impres-
sive performance for slightly longer sentences or paragraphs
in the range of (25, 38). DiSAN performs poorly when the
sentence length ≥ 38, in which however only 3.21% of total
movie review sentences lie.

5.3 Experiments on Other NLP Tasks
Multi-Genre Natural Language Inference Multi-Genre
Natural Language Inference (MultiNLI)5 (Williams, Nan-
gia, and Bowman 2017) dataset consists of 433k sentence
pairs annotated with textual entailment information. This
dataset is similar to SNLI, but it covers more genres of spo-
ken and written text, and supports a distinctive cross-genre
generalization evaluation. However, MultiNLI is a quite new
dataset, and its leaderboard does not include a session for
the sentence-encoding only model. Hence, we only compare
DiSAN with the baselines provided at the ofﬁcial website.
The results of DiSAN and two sentence-encoding models
on the leaderboard are shown in Table 3. Note that the pre-
diction accuracies of Matched and Mismatched test datasets
are obtained by submitting our test results to Kaggle open
evaluation platforms6: MultiNLI Matched Open Evaluation
and MultiNLI Mismatched Open Evaluation.

Semantic Relatedness The task of semantic relatedness
aims to predict a similarity degree of a given pair of sen-
tences. We show an experimental comparison of different

5https://www.nyu.edu/projects/bowman/multinli/
6https://inclass.kaggle.com/c/multinli-matched-open-

evaluation and https://inclass.kaggle.com/c/multinli-mismatched-
open-evaluation

Method Matched Mismatched

0.65200
cBoW
Bi-LSTM 0.67507

DiSAN

0.70977

0.64759
0.67248

0.71402

Table 3: Experimental results of prediction accuracy for dif-
ferent methods on MultiNLI.

Model
cBoWa
Skip-thoughtb
DCNNc
AdaSentd
SRUe
Wide CNNse

CR

79.9
81.3
/

MPQA

SUBJ

TREC

86.4
87.5
/

91.3
93.6
/

87.3
92.2
93.0

83.6 (1.6) 90.4 (0.7) 92.2 (1.2) 91.1 (1.0)
84.8 (1.3) 89.7 (1.1) 93.4 (0.8) 93.9 (0.6)
82.2 (2.2) 88.8 (1.2) 92.9 (0.7) 93.2 (0.5)

DiSAN

84.8 (2.0) 90.1 (0.4) 94.2 (0.6) 94.2 (0.1)

methods on Sentences Involving Compositional Knowledge
(SICK)7 dataset (Marelli et al. 2014). SICK is composed
of 9,927 sentence pairs with 4,500/500/4,927 instances for
train/dev/test. The regression module on the top of DiSAN is
introduced by Tai, Socher, and Manning (2015). The results
in Table 4 show that DiSAN outperforms the models from
previous works in terms of Pearson’s r and Spearman’s ρ
indexes.

Table 5: Experimental results for different methods on vari-
ous sentence classiﬁcation benchmarks. The reported accu-
racies on CR, MPQA and SUBJ are the mean of 10-fold
cross validation, the accuracies on TREC are the mean of
dev accuracies of ﬁve runs. All standard deviations are in
parentheses. a(Mikolov et al. 2013a), b(Kiros et al. 2015),
c(Kalchbrenner, Grefenstette, and Blunsom 2014), d(Zhao,
Lu, and Poupart 2015), e(Lei and Zhang 2017).

Pearson’s r Spearman’s ρ MSE

Model
Meaning Factorya
.7721
ECNUb
/
DT-RNNc
.7923 (.0070) .7319 (.0071) .3822 (.0137)
SDT-RNNc
.7900 (.0042) .7304 (.0042) .3848 (.0042)
Cons. Tree-LSTMd .8582 (.0038) .7966 (.0053) .2734 (.0108)
Dep. Tree-LSTMd .8676 (.0030) .8083 (.0042) .2532 (.0052)

.8268
.8414

.3224
/

DiSAN

.8695 (.0012) .8139 (.0012) .2879 (.0036)

Table 4: Experimental results for different methods on SICK
sentence relatedness dataset. The reported accuracies are the
mean of ﬁve runs (standard deviations in parentheses). Cons.
and Dep. represent Constituency and Dependency, respec-
tively. a(Bjerva et al. 2014), b(Zhao, Zhu, and Lan 2014),
c(Socher et al. 2014), d(Tai, Socher, and Manning 2015).

Sentence Classiﬁcations The goal of sentence classiﬁca-
tion is to correctly predict the class label of a given sentence
in various scenarios. We evaluate the models on four sen-
tence classiﬁcation benchmarks of various NLP tasks, such
as sentiment analysis and question-type classiﬁcation. They
are listed as follows. 1) CR: Customer review (Hu and Liu
2004) of various products (cameras, etc.), which is to pre-
dict whether the review is positive or negative; 2) MPQA:
Opinion polarity detection subtask of the MPQA dataset
(Wiebe, Wilson, and Cardie 2005); 3) SUBJ: Subjectivity
dataset (Pang and Lee 2004) whose labels indicate whether
each sentence is subjective or objective; 4) TREC: TREC
question-type classiﬁcation dataset (Li and Roth 2002). The
experimental results of DiSAN and existing methods are
shown in Table 5.

5.4 Case Study
To gain a closer view of what dependencies in a sentence
can be captured by DiSAN, we visualize the attention prob-
ability p(z = i|x, xj) or alignment score by heatmaps. In

7http://clic.cimec.unitn.it/composes/sick.html

particular, we will focus primarily on the probability in for-
ward/backward DiSA blocks (Figure 6), forward/backward
fusion gates F in Eq.(19) (Figure 7), and the probability
in multi-dimensional source2token self-attention block (Fig-
ure 8). For the ﬁrst two, we desire to demonstrate the de-
pendency at token level, but attention probability in DiSAN
is deﬁned on each feature, so we average the probabilities
along the feature dimension.

We select two sentences from SNLI test set as examples
for this case study. Sentence 1 is Families have some dogs
in front of a carousel and sentence 2 is volleyball match is in
progress between ladies.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 6: Attention probability in forward/backward DiSA
blocks for the two example sentences.

Figure 6 shows that1) semantically important words such
as nouns and verbs usually get large attention, but stop
words (am, is, are, etc.) do not; 2) globally important words,
e.g., volleyball, match, ladies in sentence 1 and dog, front,
carousel in sentence 2, get large attention from all other
words; 3) if a word is important to only some of the other
words (e.g. to constitute a phrase or sense-group), it gets
large attention only from these words, e.g., attention be-
tween progress, between in sentence1, and attention between
families, have in sentence 2.

This also shows that directional information can help to
generate context-aware word representation with temporal
order encoded. For instance, for word match in sentence 1,
its forward DiSA focuses more on word volleyball, while
its backward attention focuses more on progress and ladies,
so the representation of word match contains the essential
information of the entire sentence, and simultaneously in-
cludes the positional order information.

In addition, forward and backward DiSAs can focus on
different parts of a sentence. For example, the forward one
in sentence 2 pays attention to the word families, whereas the
backward one focuses on the word carousel. Since forward
and backward attentions are computed separately, it avoids
normalization over multiple signiﬁcant words to weaken
their weights. Note that this is a weakness of traditional at-
tention compared to RNN, especially for long sentences.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 7: Fusion Gate F in forward/backward DiSA blocks.

In Figure 7, we show that the gate value F in Eq.(19). The
gate combines the input and output of masked self-attention.
It tends to selects the input representation h instead of the
output s if the corresponding weight in F is large. This
shows that the gate values for meaningless words, especially
stop words is small. The stop words themselves cannot con-
tribute important information, so only their semantic rela-
tions to other words might help to understand the sentence.
Hence, the gate tends to use their context features given by
masked self-attention.

(a) glass in pair 1

(b) close in pair 2

Figure 8: Two pairs of attention probability comparison of
same word in difference sentence contexts.

In Figure 8, we show the two multi-dimensional
source2token self-attention score vectors of the same word
in the two sentences, by their heatmaps. The ﬁrst pair has
two sentences: one is The glass bottle is big, and another is
A man is pouring a glass of tea. They share the same word is
glass with different meanings. The second pair has two sen-
tences: one is The restaurant is about to close and another
is A biker is close to the fountain. It can be seen that the
two attention vectors for the same words are very different
due to their different meanings in different contexts. This
indicates that the multi-dimensional attention vector is not
redundant because it can encode more information than one
single score used in traditional attention and it is able to cap-
ture subtle difference of the same word in different contexts
or sentences. Additionally, it can also alleviate the weakness
of the attention over long sequence, which can avoid nor-
malization over entire sequence in traditional attention only
once.

6 Conclusion
In this paper, we propose two novel attention mechanisms,
multi-dimensional attention and directional self-attention.
The multi-dimensional attention performs a feature-wise se-
lection over the input sequence for a speciﬁc task, and the
directional self-attention uses the positional masks to pro-
duce the context-aware representations with temporal in-
formation encoded. Based on these attentions, Directional
Self-Attention Network (DiSAN) is proposed for sentence-
encoding without any recurrent or convolutional structure.
The experiment results show that DiSAN can achieve state-
of-the-art inference quality and outperform existing works
(LSTM, etc.) on a wide range of NLP tasks with fewer pa-
rameters and higher time efﬁciency.

In future work, we will explore the approaches to using
the proposed attention mechanisms on more sophisticated
tasks, e.g. question answering and reading comprehension,
to achieve better performance on various benchmarks.

7 Acknowledgments
This research was funded by the Australian Government
through the Australian Research Council (ARC) under grant
1) LP160100630 partnership with Australia Government
Department of Health, and 2) LP150100671 partnership
with Australia Research Alliance for Children and Youth
(ARACY) and Global Business College Australia (GBCA).

References
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine
translation by jointly learning to align and translate. In ICLR.
Bjerva, J.; Bos, J.; Van der Goot, R.; and Nissim, M. 2014. The
meaning factory: Formal semantics for recognizing textual entail-
ment and determining semantic similarity. In SemEval@ COLING,
642–646.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language inference. In
EMNLP.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
sentence understanding. In ACL.

Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical
evaluation of gated recurrent neural networks on sequence model-
ing. In NIPS.
Clevert, D.-A.; Unterthiner, T.; and Hochreiter, S. 2016. Fast and
accurate deep network learning by exponential linear units (elus).
In ICLR.
Glorot, X., and Bengio, Y. 2010. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings of the
Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, 249–256.
Graves, A.; Jaitly, N.; and Mohamed, A.-r. 2013. Hybrid speech
In Automatic Speech
recognition with deep bidirectional lstm.
Recognition and Understanding (ASRU), 2013 IEEE Workshop on,
273–278. IEEE.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay,
W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to
read and comprehend. In NIPS.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8):1735–1780.
Hu, M., and Liu, B. 2004. Mining and summarizing customer re-
In Proceedings of the tenth ACM SIGKDD international
views.
conference on Knowledge discovery and data mining, 168–177.
ACM.
Hu, M.; Peng, Y.; and Qiu, X. 2017. Reinforced mnemonic reader
for machine comprehension. arXiv preprint arXiv:1705.02798.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. arXiv preprint
arXiv:1404.2188.
Kim, Y.; Jernite, Y.; Sontag, D.; and Rush, A. M. 2016. Character-
aware neural language models. In AAAI.
Kim, Y. 2014. Convolutional neural networks for sentence classi-
ﬁcation. In EMNLP.
Kingma, D., and Ba, J. 2015. Adam: A method for stochastic
optimization. In ICLR.
Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun, R.;
Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In NIPS.
Kokkinos, F., and Potamianos, A.
2017. Structural attention
neural networks for improved sentiment analysis. arXiv preprint
arXiv:1701.01811.
Lei, T., and Zhang, Y. 2017. Training rnns as fast as cnns. arXiv
preprint arXiv:1709.02755.
Lei, T.; Barzilay, R.; and Jaakkola, T. 2015. Molding cnns for text:
non-linear, non-consecutive convolutions. In EMNLP.
Li, X., and Roth, D. 2002. Learning question classiﬁers. In ACL.
Li, J.; Luong, M.-T.; Jurafsky, D.; and Hovy, E. 2015. When
are tree structures necessary for deep learning of representations?
arXiv preprint arXiv:1503.00185.
Liu, Y.; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natural lan-
guage inference using bidirectional lstm model and inner-attention.
arXiv preprint arXiv:1605.09090.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effective ap-
proaches to attention-based neural machine translation. In EMNLP.
Marelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.; Bernardi, R.;
and Zamparelli, R. 2014. A sick cure for the evaluation of compo-
sitional distributional semantic models. In LREC.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Efﬁcient
estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781.

Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013b. Distributed representations of words and phrases and their
compositionality. In NIPS.
Mou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.; and Jin, Z.
2016. Natural language inference by tree-based convolution and
heuristic matching. In ACL.
Munkhdalai, T., and Yu, H. 2017a. Neural semantic encoders. In
EACL.
Munkhdalai, T., and Yu, H. 2017b. Neural tree indexers for text
understanding. In EACL.
Pang, B., and Lee, L. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on minimum cuts.
In ACL.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
Global vectors for word representation. In EMNLP.
Qian, Q.; Huang, M.; and Zhu, X. 2017. Linguistically regularized
lstms for sentiment classiﬁcation. In ACL.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention
model for abstractive sentence summarization. In EMNLP.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2017. Bidi-
rectional attention ﬂow for machine comprehension. In ICLR.
Shang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine for
short-text conversation. In ACL.
Socher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; Potts, C.; et al. 2013. Recursive deep models for seman-
tic compositionality over a sentiment treebank. In EMNLP.
Socher, R.; Karpathy, A.; Le, Q. V.; Manning, C. D.; and Ng, A. Y.
2014. Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. Transactions of the Association for
Computational Linguistics 2:207–218.
Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural
networks from overﬁtting. Journal of Machine Learning Research
15(1):1929–1958.
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end
memory networks. In NIPS.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved se-
mantic representations from tree-structured long short-term mem-
ory networks. In ACL.
Teng, Z.; Vo, D.-T.; and Zhang, Y. 2016. Context-sensitive lexicon
features for neural sentiment analysis. In EMNLP.
Vaswani, A.; Shazeer; Noam; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all
you need. In NIPS.
Vendrov, I.; Kiros, R.; Fidler, S.; and Urtasun, R. 2016. Order-
embeddings of images and language. In ICLR.
Wiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating expressions
of opinions and emotions in language. Language resources and
evaluation 39(2):165–210.
Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A broad-
coverage challenge corpus for sentence understanding through in-
ference. arXiv preprint arXiv:1704.05426.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method.
arXiv preprint arXiv:1212.5701.
Zhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hierarchical
sentence model. In IJCAI.
Zhao, J.; Zhu, T.; and Lan, M. 2014. Ecnu: One stone two birds:
Ensemble of heterogenous measures for semantic relatedness and
textual entailment. In SemEval@ COLING, 271–277.

DiSAN: Directional Self-Attention Network for
RNN/CNN-Free Language Understanding
Tianyi Zhou‡
Shirui Pan†

Tao Shen†
Jing Jiang†

Guodong Long†
Chengqi Zhang†

†Centre of Artiﬁcial Intelligence, FEIT, University of Technology Sydney
‡Paul G. Allen School of Computer Science & Engineering, University of Washington
tao.shen@student.uts.edu.au, tianyizh@uw.edu
{guodong.long, jing.jiang, shirui.pan, chengqi.zhang}@uts.edu.au

7
1
0
2
 
v
o
N
 
0
2
 
 
]
L
C
.
s
c
[
 
 
3
v
6
9
6
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recurrent neural nets (RNN) and convolutional neural nets
(CNN) are widely used on NLP tasks to capture the long-term
and local dependencies, respectively. Attention mechanisms
have recently attracted enormous interest due to their highly
parallelizable computation, signiﬁcantly less training time,
and ﬂexibility in modeling dependencies. We propose a novel
attention mechanism in which the attention between elements
from input sequence(s) is directional and multi-dimensional
(i.e., feature-wise). A light-weight neural net, “Directional
Self-Attention Network (DiSAN)”, is then proposed to learn
sentence embedding, based solely on the proposed attention
without any RNN/CNN structure. DiSAN is only composed
of a directional self-attention with temporal order encoded,
followed by a multi-dimensional attention that compresses
the sequence into a vector representation. Despite its simple
form, DiSAN outperforms complicated RNN models on both
prediction quality and time efﬁciency. It achieves the best
test accuracy among all sentence encoding methods and im-
proves the most recent best result by 1.02% on the Stanford
Natural Language Inference (SNLI) dataset, and shows state-
of-the-art test accuracy on the Stanford Sentiment Treebank
(SST), Multi-Genre natural language inference (MultiNLI),
Sentences Involving Compositional Knowledge (SICK), Cus-
tomer Review, MPQA, TREC question-type classiﬁcation
and Subjectivity (SUBJ) datasets.

1

Introduction

Context dependency plays a signiﬁcant role in language
understanding and provides critical information to natural
language processing (NLP) tasks. For different tasks and
data, researchers often switch between two types of deep
neural network (DNN): recurrent neural network (RNN)
with sequential architecture capturing long-range dependen-
cies (e.g., long short-term memory (LSTM) (Hochreiter and
Schmidhuber 1997) and gated recurrent unit (GRU) (Chung
et al. 2014)), and convolutional neural network (CNN) (Kim
2014) whose hierarchical structure is good at extracting lo-
cal or position-invariant features. However, which network
to choose in practice is an open question, and the choice re-
lies largely on the empirical knowledge.

Recent works have found that equipping RNN or CNN
with an attention mechanism can achieve state-of-the-art

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

performance on a large number of NLP tasks, including neu-
ral machine translation (Bahdanau, Cho, and Bengio 2015;
Luong, Pham, and Manning 2015), natural language infer-
ence (Liu et al. 2016), conversation generation (Shang, Lu,
and Li 2015), question answering (Hermann et al. 2015;
Sukhbaatar et al. 2015), machine reading comprehension
(Seo et al. 2017), and sentiment analysis (Kokkinos and
Potamianos 2017). The attention uses a hidden layer to com-
pute a categorical distribution over elements from the in-
put sequence to reﬂect their importance weights. It allows
RNN/CNN to maintain a variable-length memory, so that
elements from the input sequence can be selected by their
importance/relevance and merged into the output. In con-
trast to RNN and CNN, the attention mechanism is trained
to capture the dependencies that make signiﬁcant contribu-
tions to the task, regardless of the distance between the el-
ements in the sequence. It can thus provide complementary
information to the distance-aware dependencies modeled by
RNN/CNN. In addition, computing attention only requires
matrix multiplication, which is highly parallelizable com-
pared to the sequential computation of RNN.

In a very recent work (Vaswani et al. 2017), an atten-
tion mechanism is solely used to construct a sequence to
sequence (seq2seq) model that achieves a state-of-the-art
quality score on the neural machine translation (NMT) task.
The seq2seq model, “Transformer”, has an encoder-decoder
structure that is only composed of stacked attention net-
works, without using either recurrence or convolution. The
proposed attention, “multi-head attention”, projects the in-
put sequence to multiple subspaces, then applies scaled dot-
product attention to its representation in each subspace,
and lastly concatenates their output. By doing this, it can
combine different attentions from multiple subspaces. This
mechanism is used in Transformer to compute both the
context-aware features inside the encoder/decoder and the
bottleneck features between them.

The attention mechanism has more ﬂexibility in sequence
length than RNN/CNN, and is more task/data-driven when
modeling dependencies. Unlike sequential models, its com-
putation can be easily and signiﬁcantly accelerated by exist-
ing distributed/parallel computing schemes. However, to the
best of our knowledge, a neural net entirely based on atten-
tion has not been designed for other NLP tasks except NMT,
especially those that cannot be cast into a seq2seq problem.

Compared to RNN, a disadvantage of most attention mech-
anisms is that the temporal order information is lost, which
however might be important to the task. This explains why
positional encoding is applied to the sequence before being
processed by the attention in Transformer. How to model or-
der information within an attention is still an open problem.
The goal of this paper is to develop a uniﬁed and
RNN/CNN-free attention network that can be generally uti-
lized to learn the sentence encoding model for different NLP
tasks, such as natural language inference, sentiment analy-
sis, sentence classiﬁcation and semantic relatedness. We fo-
cus on the sentence encoding model because it is a basic
module of most DNNs used in the NLP literature.

We propose a novel attention mechanism that differs from
previous ones in that it is 1) multi-dimensional: the atten-
tion w.r.t. each pair of elements from the source(s) is a vec-
tor, where each entry is the attention computed on each fea-
ture; and 2) directional: it uses one or multiple positional
masks to model the asymmetric attention between two el-
ements. We compute feature-wise attention since each ele-
ment in a sequence is usually represented by a vector, e.g.,
word/character embedding (Kim et al. 2016), and attention
on different features can contain different information about
dependency, thus to handle the variation of contexts around
the same word. We apply positional masks to attention dis-
tribution since they can easily encode prior structure knowl-
edge such as temporal order and dependency parsing. This
design mitigates the weakness of attention in modeling order
information, and takes full advantage of parallel computing.
We then build a light-weight and RNN/CNN-free neural
network, “Directional Self-Attention Network (DiSAN)”,
for sentence encoding. This network relies entirely on the
proposed attentions and does not use any RNN/CNN struc-
ture. In DiSAN, the input sequence is processed by direc-
tional (forward and backward) self-attentions to model con-
text dependency and produce context-aware representations
for all tokens. Then, a multi-dimensional attention computes
a vector representation of the entire sequence, which can
be passed into a classiﬁcation/regression module to com-
pute the ﬁnal prediction for a particular task. Unlike Trans-
former, neither stacking of attention blocks nor an encoder-
decoder structure is required. The simple architecture of
DiSAN leads to fewer parameters, less computation and eas-
ier parallelization.

In experiments1, we compare DiSAN with the currently
popular methods on various NLP tasks, e.g., natural lan-
guage inference, sentiment analysis, sentence classiﬁcation,
etc. DiSAN achieves the highest test accuracy on the Stan-
ford Natural Language Inference (SNLI) dataset among
sentence-encoding models and improves the currently best
result by 1.02%. It also shows the state-of-the-art perfor-
mance on the Stanford Sentiment Treebank (SST), Multi-
Genre natural language inference (MultiNLI), SICK, Cus-
tomer Review, MPQA, SUBJ and TREC question-type clas-
siﬁcation datasets. Meanwhile, it has fewer parameters and
exhibits much higher computation efﬁciency than the mod-

1Codes and pre-trained models for experiments can be found at

https://github.com/taoshen58/DiSAN

els it outperforms, e.g., LSTM and tree-based models.

Annotation: 1) Lowercase denotes a vector; 2) bold low-
ercase denotes a sequence of vectors (stored as a matrix);
and 3) uppercase denotes a matrix or a tensor.

2 Background

2.1 Sentence Encoding
In the pipeline of NLP tasks, a sentence is denoted by a se-
quence of discrete tokens (e.g., words or characters) v =
[v1, v2, . . . , vn], where vi could be a one-hot vector whose
dimension length equals the number of distinct tokens N .
A pre-trained token embedding (e.g., word2vec (Mikolov
et al. 2013b) or GloVe (Pennington, Socher, and Manning
2014)) is applied to v and transforms all discrete tokens to
a sequence of low-dimensional dense vector representations
x = [x1, x2, . . . , xn] with xi ∈ Rde. This pre-process can
be written as x = W (e)v, where word embedding weight
matrix W (e) ∈ Rde×N and x ∈ Rde×n.

Most DNN sentence-encoding models for NLP tasks take
x as the input and further generate a vector representation ui
for each xi by context fusion. Then a sentence encoding is
obtained by mapping the sequence u = [u1, u2, . . . , un] to
a single vector s ∈ Rd, which is used as a compact encoding
of the entire sentence in NLP problems.

2.2 Attention
The attention is proposed to compute an alignment score be-
tween elements from two sources. In particular, given the to-
ken embeddings of a source sequence x = [x1, x2, . . . , xn]
and the vector representation of a query q, attention com-
putes the alignment score between xi and q by a compati-
bility function f (xi, q), which measures the dependency be-
tween xi and q, or the attention of q to xi. A softmax func-
tion then transforms the scores [f (xi, q)]n
i=1 to a probability
distribution p(z|x, q) by normalizing over all the n tokens
of x. Here z is an indicator of which token in x is important
to q on a speciﬁc task. That is, large p(z = i|x, q) means xi
contributes important information to q. The above process
can be summarized by the following equations.
a = [f (xi, q)]n
p(z|x, q) = softmax(a).

(1)
(2)

i=1 ,

Speciﬁcally,

p(z = i|x, q) =

exp(f (xi, q))
i=1 exp(f (xi, q))

.

(cid:80)n

(3)

The output of this attention mechanism is a weighted sum
of the embeddings for all tokens in x, where the weights
are given by p(z|x, q). It places large weights on the tokens
important to q, and can be written as the expectation of a
token sampled according to its importance, i.e.,

s =

p(z = i|x, q)xi = Ei∼p(z|x,q)(xi),

(4)

n
(cid:88)

i=1

where s ∈ Rde can be used as the sentence encoding of x.

Additive attention (or multi-layer perceptron attention)
(Bahdanau, Cho, and Bengio 2015; Shang, Lu, and Li

3.1 Multi-dimensional Attention
Multi-dimensional attention is a natural extension of addi-
tive attention (or MLP attention) at the feature level. Instead
of computing a single scalar score f (xi, q) for each token xi
as shown in Eq.(5), multi-dimensional attention computes a
feature-wise score vector for xi by replacing weight vector
w in Eq.(5) with a matrix W , i.e.,

f (xi, q) = W T σ

W (1)xi + W (2)q

(7)

(cid:17)

,

(cid:16)

where f (xi, q) ∈ Rde is a vector with the same length as xi,
and all the weight matrices W, W (1), W (2) ∈ Rde×de. We
further add two bias terms to the parts in and out activation
σ(·), i.e.,

f (xi, q) = W T σ

(cid:16)

W (1)xi + W (2)q + b(1)(cid:17)

+ b.

(8)

We then compute a categorical distribution p(zk|x, q) over
all the n tokens for each feature k ∈ [de]. A large p(zk =
i|x, q) means that feature k of token i is important to q.

We apply the same procedure Eq.(1)-(3) in traditional at-
tention to the kth dimension of f (xi, q). In particular, for
each feature k ∈ [de], we replace f (xi, q) with [f (xi, q)]k,
and change z to zk in Eq.(1)-(3). Now each feature k in each
token i has an importance weight Pki (cid:44) p(zk = i|x, q). The
output s can be written as
(cid:105)de

(cid:104)(cid:88)n

= (cid:2)Ei∼p(zk|x,q)(xki)(cid:3)de

k=1 .

(9)

Pkixki

s =

i=1

k=1

We give an illustration of traditional attention and multi-
dimensional attention in Figure 1. In the rest of this paper,
we will ignore the subscript k which indexes feature dimen-
sion for simpliﬁcation if no confusion is possible. Hence,
the output s can be written as an element-wise product
s = (cid:80)n

i=1 P·i (cid:12) xi

Remark: The word embedding usually suffers from the
polysemy in natural language. Since traditional attention
computes a single importance score for each word based on
the word embedding, it cannot distinguish the meanings of
the same word in different contexts. Multi-dimensional at-
tention, however, computes a score for each feature of each
word, so it can select the features that can best describe the
word’s speciﬁc meaning in any given context, and include
this information in the sentence encoding output s.

3.2 Two types of Multi-dimensional Self-attention
When extending multi-dimension to self-attentions, we have
two variants of multi-dimensional attention. The ﬁrst one,
called multi-dimensional “token2token” self-attention, ex-
plores the dependency between xi and xj from the same
source x, and generates context-aware coding for each el-
ement. It replaces q with xj in Eq.(8), i.e.,
(cid:16)

W (1)xi + W (2)xj + b(1)(cid:17)

f (xi, xj) = W T σ

+ b.

(10)

Similar to P in vanilla multi-dimensional attention, we com-
pute a probability matrix P j ∈ Rde×n for each xj such that
P j
ki

(cid:44) p(zk = i|x, xj). The output for xj is

sj =

P j

·i (cid:12) xi

n
(cid:88)

i=1

(11)

(a)

(b)

Figure 1: (a) Traditional (additive/multiplicative) attention
and (b) multi-dimensional attention. zi denotes alignment
score f (xi, q), which is a scalar in (a) but a vector in (b).

2015) and multiplicative attention (or dot-product attention)
(Vaswani et al. 2017; Sukhbaatar et al. 2015; Rush, Chopra,
and Weston 2015) are the two most commonly used atten-
tion mechanisms. They share the same and uniﬁed form of
attention introduced above, but are different in the compati-
bility function f (xi, q). Additive attention is associated with

f (xi, q) = wT σ(W (1)xi + W (2)q),
(5)
where σ(·) is an activation function and w ∈ Rde is a weight
vector. Multiplicative attention uses inner product or cosine
similarity for f (xi, q), i.e.,
(cid:68)
W (1)xi, W (2)q

f (xi, q) =

(6)

(cid:69)

.

In practice, additive attention often outperforms multiplica-
tive one in prediction quality, but the latter is faster and more
memory-efﬁcient due to optimized matrix multiplication.

2.3 Self-Attention
Self-Attention is a special case of the attention mechanism
introduced above. It replaces q with a token embedding xj
from the source input itself. It relates elements at different
positions from a single sequence by computing the attention
between each pair of tokens, xi and xj. It is very expres-
sive and ﬂexible for both long-range and local dependen-
cies, which used to be respectively modeled by RNN and
CNN. Moreover, it has much faster computation speed and
fewer parameters than RNN. In recent works, we have al-
ready witnessed its success across a variety of NLP tasks,
such as reading comprehension (Hu, Peng, and Qiu 2017)
and neural machine translation (Vaswani et al. 2017).

3 Two Proposed Attention Mechanisms
In this section, we introduce two novel attention mecha-
nisms, multi-dimensional attention in Section 3.1 (with two
extensions to self-attention in Section 3.2) and directional
self-attention in Section 3.3. They are the main components
of DiSAN and may be of independent interest to other neural
nets for other NLP problems in which an attention is needed.

In DiSA, we ﬁrst transform the input sequence x =
to a sequence of hidden state h =

[x1, x2, . . . , xn]
[h1, h2, . . . , hn] by a fully connected layer, i.e.,

h = σh

(cid:16)

W (h)x + b(h)(cid:17)

,

(14)

where x ∈ Rde×n, h ∈ Rdh×n, W (h) and b(h) are the learn-
able parameters, and σh(·) is an activation function.

We then apply multi-dimensional

token2token self-
attention to h, and generate context-aware vector represen-
tations s for all elements from the input sequence. We make
two modiﬁcations to Eq.(10) to reduce the number of param-
eters and make the attention directional.

First, we set W in Eq.(10) to a scalar c and divide the part
in σ(·) by c, and we use tanh(·) for σ(·), which reduces the
number of parameters. In experiments, we always set c = 5,
and obtain stable output.

Second, we apply a positional mask to Eq.(10), so the at-
tention between two elements can be asymmetric. Given a
mask M ∈ {0, −∞}n×n, we set bias b to a constant vec-
tor Mij1 in Eq.(10), where 1 is an all-one vector. Hence,
Eq.(10) is modiﬁed to

f (hi, hj) =

c · tanh

(cid:16)

(cid:17)
[W (1)hi + W (2)hj + b(1)]/c

+ Mij1.

(15)

To see why a mask can encode directional information, let
us consider a case in which Mij = −∞ and Mji = 0, which
results in [f (hi, hj)]k = −∞ and unchanged [f (hj, hi)]k.
Since the probability p(zk = i|x, xj) is computed by
softmax, [f (hi, hj)]k = −∞ leads to p(zk = i|x, xj) = 0.
This means that there is no attention of xj to xi on feature k.
On the contrary, we have p(zk = j|x, xi) > 0, which means
that attention of xi to xj exists on feature k. Therefore, prior
structure knowledge such as temporal order and dependency
parsing can be easily encoded by the mask, and explored in
generating sentence encoding. This is an important feature
of DiSA that previous attention mechanisms do not have.

For self-attention, we usually need to disable the attention
of each token to itself (Hu, Peng, and Qiu 2017). This is the
same as applying a diagonal-disabled (i.e., diag-disabled)
mask such that

M diag

ij =

(cid:26) 0,
−∞,

i (cid:54)= j
i = j

Moreover, we can use masks to encode temporal order
information into attention output. In this paper, we use two
masks, i.e., forward mask M f w and backward mask M bw,

M f w

ij =

M bw

ij =

(cid:26) 0,
−∞,

(cid:26) 0,
−∞,

i < j
otherwise

i > j
otherwise

(16)

(17)

(18)

In forward mask M f w, there is the only attention of later
token j to early token i, and vice versa in backward mask.
We show these three positional masks in Figure 3.

Given input sequence x and a mask M , we compute
f (xi, xj) according to Eq.(15), and follow the standard pro-
cedure of multi-dimensional token2token self-attention to

Figure 2: Directional self-attention (DiSA) mechanism.
Here, we use li,j to denote f (hi, hj) in Eq. (15).

The output of token2token self-attention for all elements
from x is s = [s1, s2, . . . , sn] ∈ Rde×n.

The second one, multi-dimensional “source2token” self-
attention, explores the dependency between xi and the entire
sequence x, and compresses the sequence x into a vector. It
removes q from Eq.(8), i.e.,

f (xi) = W T σ

(cid:16)

W (1)xi + b(1)(cid:17)

+ b.

(12)

The probability matrix is deﬁned as Pki (cid:44) p(zk = i|x)
and is computed in the same way as P in vanilla multi-
dimensional attention. The output s is also same, i.e.,

s =

P·i (cid:12) xi

(13)

n
(cid:88)

i=1

We will use these two types (i.e.,

token2token and
source2token) of multi-dimensional self-attention in differ-
ent parts of our sentence encoding model, DiSAN.

3.3 Directional Self-Attention

Directional self-attention (DiSA) is composed of a fully
connected layer whose input is the token embeddings x,
a “masked” multi-dimensional token2token self-attention
block to explore the dependency and temporal order, and a
fusion gate to combine the output and input of the attention
block. Its structure is shown in Figure 2. It can be used as
either a neural net or a module to compose a large network.

(a) Diag-disabled mask

(b) Forward mask

(c) Backward mask

Figure 3: Three positional masks: (a) is the diag-disabled
mask M diag; (b) and (c) are forward mask M f w and back-
ward mask M bw, respectively.

(19)

(20)

compute the probability matrix P j for each j ∈ [n]. Each
output sj in s is computed as in Eq.(11).

The ﬁnal output u ∈ Rdh×n of DiSA is obtained by com-
bining the output s and the input h of the masked multi-
dimensional token2token self-attention block. This yields a
temporal order encoded and context-aware vector represen-
tation for each element/token. The combination is accom-
plished by a dimension-wise fusion gate, i.e.,

F = sigmoid

(cid:16)

W (f 1)s + W (f 2)h + b(f )(cid:17)

u = F (cid:12) h + (1 − F ) (cid:12) s

where W (f 1), W (f 2) ∈ Rdh×dh and b(f ) ∈ Rdh are the
learnable parameters of the fusion gate.

4 Directional Self-Attention Network
We propose a light-weight network, “Directional Self-
Attention Network (DiSAN)”, for sentence encoding. Its ar-
chitecture is shown in Figure 4.

Given an input sequence of token embedding x, DiSAN
ﬁrstly applies two parameter-untied DiSA blocks with for-
ward mask M f w Eq.(17) and M bw Eq.(18), respectively.
The feed-forward procedure is given in Eq.(14)-(15) and
Eq.(19)-(20). Their outputs are denoted by uf w, ubw ∈
Rdh×n. We concatenate them vertically as [uf w; ubw] ∈
R2dh×n, and use this concatenated output as input to a multi-
dimensional source2token self-attention block, whose out-
put sdisan ∈ R2dh computed by Eq.(12)-(13) is the ﬁnal
sentence encoding result of DiSAN.

Remark: In DiSAN, forward/backward DiSA blocks
work as context fusion layers. And the multi-dimensional
source2token self-attention compresses the sequence into a

Figure 4: Directional self-attention network (DiSAN)

single vector. The idea of using both forward and backward
attentions is inspired by Bi-directional LSTM (Bi-LSTM)
(Graves, Jaitly, and Mohamed 2013), in which forward and
backward LSTMs are used to encode long-range depen-
dency from different directions. In Bi-LSTM, LSTM com-
bines the context-aware output with the input by multi-gate.
The fusion gate used in DiSA shares the similar motivation.
However, DiSAN has fewer parameters, simpler structure
and better efﬁciency.

5 Experiments
In this section, we ﬁrst apply DiSAN to natural language
inference and sentiment analysis tasks. DiSAN achieves
the state-of-the-art performance and signiﬁcantly better ef-
ﬁciency than other baseline methods on benchmark datasets
for both tasks. We also conduct experiments on other NLP
tasks and DiSAN also achieves state-of-the-art performance.
Training Setup: We use cross-entropy loss plus L2 regu-
larization penalty as optimization objective. We minimize
it by Adadelta (Zeiler 2012) (an optimizer of mini-batch
SGD) with batch size of 64. We use Adadelta rather than
Adam (Kingma and Ba 2015) because in our experiments,
DiSAN optimized by Adadelta can achieve more stable per-
formance than Adam optimized one. Initial learning rate is
set to 0.5. All weight matrices are initialized by Glorot Ini-
tialization (Glorot and Bengio 2010), and the biases are ini-
tialized with 0. We initialize the word embedding in x by
300D GloVe 6B pre-trained vectors (Pennington, Socher,
and Manning 2014). The Out-of-Vocabulary words in train-
ing set are randomly initialized by uniform distribution be-
tween (−0.05, 0.05). The word embeddings are ﬁne-tuned
during the training phrase. We use Dropout (Srivastava et
al. 2014) with keep probability 0.75 for language inference
and 0.8 for sentiment analysis. The L2 regularization de-
cay factors γ are 5 × 10−5 and 10−4 for language inference
and sentiment analysis, respectively. Note that the dropout
keep probability and γ varies with the scale of correspond-
ing dataset. Hidden units number dh is set to 300. Activa-
tion functions σ(·) are ELU (exponential linear unit) (Clev-
ert, Unterthiner, and Hochreiter 2016) if not speciﬁed. All
models are implemented with TensorFlow2 and run on sin-

2https://www.tensorﬂow.org

Model Name

|θ|

T(s)/epoch Train Accu(%) Test Accu(%)

Unlexicalized features (Bowman et al. 2015)
+ Unigram and bigram features (Bowman et al. 2015)

100D LSTM encoders (Bowman et al. 2015)
300D LSTM encoders (Bowman et al. 2016)
1024D GRU encoders (Vendrov et al. 2016)
300D Tree-based CNN encoders (Mou et al. 2016)
300D SPINN-PI encoders (Bowman et al. 2016)
600D Bi-LSTM encoders (Liu et al. 2016)
300D NTI-SLSTM-LSTM encoders (Munkhdalai and Yu 2017b)
600D Bi-LSTM encoders+intra-attention (Liu et al. 2016)
300D NSE encoders (Munkhdalai and Yu 2017a)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

Directional self-attention network (DiSAN)

0.2m
3.0m
15m
3.5m
3.7m
2.0m
4.0m
2.8m
3.0m

0.45m
0.54m
1.98m
2.88m
2.35m

2.35m

216
261
345
2080
592

587

49.4
99.7

84.8
83.9
98.8
83.3
89.2
86.4
82.5
84.5
86.2

82.39
86.22
89.58
90.39
90.18

91.08

50.4
78.2

77.6
80.6
81.4
82.1
83.2
83.3
83.4
84.2
84.6

79.81
83.12
84.17
84.98
84.66

85.62

Table 1: Experimental results for different methods on SNLI. |θ|: the number of parameters (excluding word embedding part).
T(s)/epoch: average time (second) per epoch. Train Accu(%) and Test Accu(%): the accuracy on training and test set.

gle Nvidia GTX 1080Ti graphic card.

5.1 Natural Language Inference
The goal of Natural Language Inference (NLI) is to rea-
son the semantic relationship between a premise sentence
and a corresponding hypothesis sentence. The possible rela-
tionship could be entailment, neutral or contradiction. We
compare different models on a widely used benchmark,
Stanford Natural Language Inference (SNLI)3 (Bowman et
al. 2015) dataset, which consists of 549,367/9,842/9,824
(train/dev/test) premise-hypothesis pairs with labels.

Following the standard procedure in Bowman et al.
(2016), we launch two sentence encoding models (e.g.,
DiSAN) with tied parameters for the premise sentence and
hypothesis sentence, respectively. Given the output encoding
sp for the premise and sh for the hypothesis, the represen-
tation of relationship is the concatenation of sp, sh, sp − sh
and sp (cid:12) sh, which is fed into a 300D fully connected layer
and then a 3-unit output layer with softmax to compute a
probability distribution over the three types of relationship.
For thorough comparison, besides the neural nets pro-
posed in previous works of NLI, we implement ﬁve extra
neural net baselines to compare with DiSAN. They help
us to analyze the improvement contributed by each part of
DiSAN and to verify that the two attention mechanisms pro-
posed in Section 3 can improve other networks.

• Word Embedding with additive attention.

• Word Embedding with s2t self-attention: DiSAN with

DiSA blocks removed.

• Multi-head with s2t self-attention: Multi-head attention
(Vaswani et al. 2017) (8 heads, each has 75 hidden units)
with source2token self-attention. The positional encoding

3https://nlp.stanford.edu/projects/snli/

method used in Vaswani et al. (2017) is applied to the in-
put sequence to encode temporal information. We ﬁnd our
experiments show that multi-head attention is sensitive to
hyperparameters, so we adjust keep probability of dropout
from 0.7 to 0.9 with step 0.05 and report the best result.
• Bi-LSTM with s2t self-attention: a multi-dimensional
source2token self-attention block is applied to the output
of Bi-LSTM (300D forward + 300D backward LSTMs).
• DiSAN without directions: DiSAN with the for-
ward/backward masks M f w and M bw replaced with two
diag-disabled masks M diag, i.e., DiSAN without for-
ward/backward order information.
Compared to the results from the ofﬁcial leaderboard
of SNLI in Table 1, DiSAN outperforms previous works
and improves the best latest test accuracy (achieved by a
memory-based NSE encoder network) by a remarkable mar-
gin of 1.02%. DiSAN surpasses the RNN/CNN based mod-
els with more complicated architecture and more parameters
by large margins, e.g., +2.32% to Bi-LSTM, +1.42% to Bi-
LSTM with additive attention. It even outperforms models
with the assistance of a semantic parsing tree, e.g., +3.52%
to Tree-based CNN, +2.42% to SPINN-PI.

In the results of the ﬁve baseline methods and DiSAN at
the bottom of Table 1, we demonstrate that making attention
multi-dimensional (feature-wise) or directional brings sub-
stantial improvement to different neural nets. First, a com-
parison between the ﬁrst two models shows that changing
token-wise attention to multi-dimensional/feature-wise at-
tention leads to 3.31% improvement on a word embedding
based model. Also, a comparison between the third baseline
and DiSAN shows that DiSAN can substantially outperform
multi-head attention by 1.45%. Moreover, a comparison be-
tween the forth baseline and DiSAN shows that the DiSA
block can even outperform Bi-LSTM layer in context en-
coding, improving test accuracy by 0.64%. A comparison

between the ﬁfth baseline and DiSAN shows that directional
self-attention with forward and backward masks (with tem-
poral order encoded) can bring 0.96% improvement.

Additional advantages of DiSAN shown in Table 1 are
its fewer parameters and compelling time efﬁciency. It is
×3 faster than widely used Bi-LSTM model. Compared to
other models with competitive performance, e.g., 600D Bi-
LSTM encoders with intra-attention (2.8M), 300D NSE en-
coders (3.0M) and 600D Bi-LSTM encoders with multi-
dimensional attention (2.88M), DiSAN only has 2.35M pa-
rameters.

5.2 Sentiment Analysis

Model

Test Accu

MV-RNN (Socher et al. 2013)
RNTN (Socher et al. 2013)
Bi-LSTM (Li et al. 2015)
Tree-LSTM (Tai, Socher, and Manning 2015)
CNN-non-static (Kim 2014)
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015)
NCSL (Teng, Vo, and Zhang 2016)
LR-Bi-LSTM (Qian, Huang, and Zhu 2017)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

DiSAN

44.4
45.7
49.8
51.0
48.0
51.2
51.1
50.6

47.47
48.87
49.14
49.95
49.41

51.72

Table 2: Test accuracy of ﬁne-grained sentiment analysis on
Stanford Sentiment Treebank (SST) dataset.

Sentiment analysis aims to analyze the sentiment of a
sentence or a paragraph, e.g., a movie or a product re-
view. We use Stanford Sentiment Treebank (SST)4 (Socher
et al. 2013) for the experiments, and only focus on the
ﬁne-grained movie review sentiment classiﬁcation over ﬁve
classes, i.e., very negative, negative, neutral, positive and
very positive. We use the standard train/dev/test sets split
with 8,544/1,101/2,210 samples. Similar to Section 5.1, we
employ a single sentence encoding model to obtain a sen-
tence representation s of a movie review, then pass it into
a 300D fully connected layer. Finally, a 5-unit output layer
with softmax is used to calculate a probability distribution
over the ﬁve classes.

In Table 2, we compare previous works with DiSAN on
test accuracy. To the best of our knowledge, DiSAN im-
proves the last best accuracy (given by CNN-Tensor) by
0.52%. Compared to tree-based models with heavy use of
the prior structure, e.g., MV-RNN, RNTN and Tree-LSTM,
DiSAN outperforms them by 7.32%, 6.02% and 0.72%,
respectively. Additionally, DiSAN achieves better perfor-
mance than CNN-based models. More recent works tend
to focus on lexicon-based sentiment analysis, by explor-
ing sentiment lexicons, negation words and intensity words.

4https://nlp.stanford.edu/sentiment/

Nonetheless, DiSAN still outperforms these fancy models,
such as NCSL (+0.62%) and LR-Bi-LSTM (+1.12%).

Figure 5: Fine-grained sentiment analysis accuracy vs. sen-
tence length. The results of LSTM, Bi-LSTM and Tree-
LSTM are from Tai, Socher, and Manning (2015) and the
result of DiSAN is the average over ﬁve random trials.

It is also interesting to see the performance of different
models on the sentences with different lengths. In Figure 5,
we compare LSTM, Bi-LSTM, Tree-LSTM and DiSAN on
different sentence lengths. In the range of (5, 12), the length
range for most movie review sentences, DiSAN signiﬁcantly
outperforms others. Meanwhile, DiSAN also shows impres-
sive performance for slightly longer sentences or paragraphs
in the range of (25, 38). DiSAN performs poorly when the
sentence length ≥ 38, in which however only 3.21% of total
movie review sentences lie.

5.3 Experiments on Other NLP Tasks
Multi-Genre Natural Language Inference Multi-Genre
Natural Language Inference (MultiNLI)5 (Williams, Nan-
gia, and Bowman 2017) dataset consists of 433k sentence
pairs annotated with textual entailment information. This
dataset is similar to SNLI, but it covers more genres of spo-
ken and written text, and supports a distinctive cross-genre
generalization evaluation. However, MultiNLI is a quite new
dataset, and its leaderboard does not include a session for
the sentence-encoding only model. Hence, we only compare
DiSAN with the baselines provided at the ofﬁcial website.
The results of DiSAN and two sentence-encoding models
on the leaderboard are shown in Table 3. Note that the pre-
diction accuracies of Matched and Mismatched test datasets
are obtained by submitting our test results to Kaggle open
evaluation platforms6: MultiNLI Matched Open Evaluation
and MultiNLI Mismatched Open Evaluation.

Semantic Relatedness The task of semantic relatedness
aims to predict a similarity degree of a given pair of sen-
tences. We show an experimental comparison of different

5https://www.nyu.edu/projects/bowman/multinli/
6https://inclass.kaggle.com/c/multinli-matched-open-

evaluation and https://inclass.kaggle.com/c/multinli-mismatched-
open-evaluation

Method Matched Mismatched

0.65200
cBoW
Bi-LSTM 0.67507

DiSAN

0.70977

0.64759
0.67248

0.71402

Table 3: Experimental results of prediction accuracy for dif-
ferent methods on MultiNLI.

Model
cBoWa
Skip-thoughtb
DCNNc
AdaSentd
SRUe
Wide CNNse

CR

79.9
81.3
/

MPQA

SUBJ

TREC

86.4
87.5
/

91.3
93.6
/

87.3
92.2
93.0

83.6 (1.6) 90.4 (0.7) 92.2 (1.2) 91.1 (1.0)
84.8 (1.3) 89.7 (1.1) 93.4 (0.8) 93.9 (0.6)
82.2 (2.2) 88.8 (1.2) 92.9 (0.7) 93.2 (0.5)

DiSAN

84.8 (2.0) 90.1 (0.4) 94.2 (0.6) 94.2 (0.1)

methods on Sentences Involving Compositional Knowledge
(SICK)7 dataset (Marelli et al. 2014). SICK is composed
of 9,927 sentence pairs with 4,500/500/4,927 instances for
train/dev/test. The regression module on the top of DiSAN is
introduced by Tai, Socher, and Manning (2015). The results
in Table 4 show that DiSAN outperforms the models from
previous works in terms of Pearson’s r and Spearman’s ρ
indexes.

Table 5: Experimental results for different methods on vari-
ous sentence classiﬁcation benchmarks. The reported accu-
racies on CR, MPQA and SUBJ are the mean of 10-fold
cross validation, the accuracies on TREC are the mean of
dev accuracies of ﬁve runs. All standard deviations are in
parentheses. a(Mikolov et al. 2013a), b(Kiros et al. 2015),
c(Kalchbrenner, Grefenstette, and Blunsom 2014), d(Zhao,
Lu, and Poupart 2015), e(Lei and Zhang 2017).

Pearson’s r Spearman’s ρ MSE

Model
Meaning Factorya
.7721
ECNUb
/
DT-RNNc
.7923 (.0070) .7319 (.0071) .3822 (.0137)
SDT-RNNc
.7900 (.0042) .7304 (.0042) .3848 (.0042)
Cons. Tree-LSTMd .8582 (.0038) .7966 (.0053) .2734 (.0108)
Dep. Tree-LSTMd .8676 (.0030) .8083 (.0042) .2532 (.0052)

.8268
.8414

.3224
/

DiSAN

.8695 (.0012) .8139 (.0012) .2879 (.0036)

Table 4: Experimental results for different methods on SICK
sentence relatedness dataset. The reported accuracies are the
mean of ﬁve runs (standard deviations in parentheses). Cons.
and Dep. represent Constituency and Dependency, respec-
tively. a(Bjerva et al. 2014), b(Zhao, Zhu, and Lan 2014),
c(Socher et al. 2014), d(Tai, Socher, and Manning 2015).

Sentence Classiﬁcations The goal of sentence classiﬁca-
tion is to correctly predict the class label of a given sentence
in various scenarios. We evaluate the models on four sen-
tence classiﬁcation benchmarks of various NLP tasks, such
as sentiment analysis and question-type classiﬁcation. They
are listed as follows. 1) CR: Customer review (Hu and Liu
2004) of various products (cameras, etc.), which is to pre-
dict whether the review is positive or negative; 2) MPQA:
Opinion polarity detection subtask of the MPQA dataset
(Wiebe, Wilson, and Cardie 2005); 3) SUBJ: Subjectivity
dataset (Pang and Lee 2004) whose labels indicate whether
each sentence is subjective or objective; 4) TREC: TREC
question-type classiﬁcation dataset (Li and Roth 2002). The
experimental results of DiSAN and existing methods are
shown in Table 5.

5.4 Case Study
To gain a closer view of what dependencies in a sentence
can be captured by DiSAN, we visualize the attention prob-
ability p(z = i|x, xj) or alignment score by heatmaps. In

7http://clic.cimec.unitn.it/composes/sick.html

particular, we will focus primarily on the probability in for-
ward/backward DiSA blocks (Figure 6), forward/backward
fusion gates F in Eq.(19) (Figure 7), and the probability
in multi-dimensional source2token self-attention block (Fig-
ure 8). For the ﬁrst two, we desire to demonstrate the de-
pendency at token level, but attention probability in DiSAN
is deﬁned on each feature, so we average the probabilities
along the feature dimension.

We select two sentences from SNLI test set as examples
for this case study. Sentence 1 is Families have some dogs
in front of a carousel and sentence 2 is volleyball match is in
progress between ladies.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 6: Attention probability in forward/backward DiSA
blocks for the two example sentences.

Figure 6 shows that1) semantically important words such
as nouns and verbs usually get large attention, but stop
words (am, is, are, etc.) do not; 2) globally important words,
e.g., volleyball, match, ladies in sentence 1 and dog, front,
carousel in sentence 2, get large attention from all other
words; 3) if a word is important to only some of the other
words (e.g. to constitute a phrase or sense-group), it gets
large attention only from these words, e.g., attention be-
tween progress, between in sentence1, and attention between
families, have in sentence 2.

This also shows that directional information can help to
generate context-aware word representation with temporal
order encoded. For instance, for word match in sentence 1,
its forward DiSA focuses more on word volleyball, while
its backward attention focuses more on progress and ladies,
so the representation of word match contains the essential
information of the entire sentence, and simultaneously in-
cludes the positional order information.

In addition, forward and backward DiSAs can focus on
different parts of a sentence. For example, the forward one
in sentence 2 pays attention to the word families, whereas the
backward one focuses on the word carousel. Since forward
and backward attentions are computed separately, it avoids
normalization over multiple signiﬁcant words to weaken
their weights. Note that this is a weakness of traditional at-
tention compared to RNN, especially for long sentences.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 7: Fusion Gate F in forward/backward DiSA blocks.

In Figure 7, we show that the gate value F in Eq.(19). The
gate combines the input and output of masked self-attention.
It tends to selects the input representation h instead of the
output s if the corresponding weight in F is large. This
shows that the gate values for meaningless words, especially
stop words is small. The stop words themselves cannot con-
tribute important information, so only their semantic rela-
tions to other words might help to understand the sentence.
Hence, the gate tends to use their context features given by
masked self-attention.

(a) glass in pair 1

(b) close in pair 2

Figure 8: Two pairs of attention probability comparison of
same word in difference sentence contexts.

In Figure 8, we show the two multi-dimensional
source2token self-attention score vectors of the same word
in the two sentences, by their heatmaps. The ﬁrst pair has
two sentences: one is The glass bottle is big, and another is
A man is pouring a glass of tea. They share the same word is
glass with different meanings. The second pair has two sen-
tences: one is The restaurant is about to close and another
is A biker is close to the fountain. It can be seen that the
two attention vectors for the same words are very different
due to their different meanings in different contexts. This
indicates that the multi-dimensional attention vector is not
redundant because it can encode more information than one
single score used in traditional attention and it is able to cap-
ture subtle difference of the same word in different contexts
or sentences. Additionally, it can also alleviate the weakness
of the attention over long sequence, which can avoid nor-
malization over entire sequence in traditional attention only
once.

6 Conclusion
In this paper, we propose two novel attention mechanisms,
multi-dimensional attention and directional self-attention.
The multi-dimensional attention performs a feature-wise se-
lection over the input sequence for a speciﬁc task, and the
directional self-attention uses the positional masks to pro-
duce the context-aware representations with temporal in-
formation encoded. Based on these attentions, Directional
Self-Attention Network (DiSAN) is proposed for sentence-
encoding without any recurrent or convolutional structure.
The experiment results show that DiSAN can achieve state-
of-the-art inference quality and outperform existing works
(LSTM, etc.) on a wide range of NLP tasks with fewer pa-
rameters and higher time efﬁciency.

In future work, we will explore the approaches to using
the proposed attention mechanisms on more sophisticated
tasks, e.g. question answering and reading comprehension,
to achieve better performance on various benchmarks.

7 Acknowledgments
This research was funded by the Australian Government
through the Australian Research Council (ARC) under grant
1) LP160100630 partnership with Australia Government
Department of Health, and 2) LP150100671 partnership
with Australia Research Alliance for Children and Youth
(ARACY) and Global Business College Australia (GBCA).

References
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine
translation by jointly learning to align and translate. In ICLR.
Bjerva, J.; Bos, J.; Van der Goot, R.; and Nissim, M. 2014. The
meaning factory: Formal semantics for recognizing textual entail-
ment and determining semantic similarity. In SemEval@ COLING,
642–646.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language inference. In
EMNLP.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
sentence understanding. In ACL.

Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical
evaluation of gated recurrent neural networks on sequence model-
ing. In NIPS.
Clevert, D.-A.; Unterthiner, T.; and Hochreiter, S. 2016. Fast and
accurate deep network learning by exponential linear units (elus).
In ICLR.
Glorot, X., and Bengio, Y. 2010. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings of the
Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, 249–256.
Graves, A.; Jaitly, N.; and Mohamed, A.-r. 2013. Hybrid speech
In Automatic Speech
recognition with deep bidirectional lstm.
Recognition and Understanding (ASRU), 2013 IEEE Workshop on,
273–278. IEEE.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay,
W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to
read and comprehend. In NIPS.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8):1735–1780.
Hu, M., and Liu, B. 2004. Mining and summarizing customer re-
In Proceedings of the tenth ACM SIGKDD international
views.
conference on Knowledge discovery and data mining, 168–177.
ACM.
Hu, M.; Peng, Y.; and Qiu, X. 2017. Reinforced mnemonic reader
for machine comprehension. arXiv preprint arXiv:1705.02798.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. arXiv preprint
arXiv:1404.2188.
Kim, Y.; Jernite, Y.; Sontag, D.; and Rush, A. M. 2016. Character-
aware neural language models. In AAAI.
Kim, Y. 2014. Convolutional neural networks for sentence classi-
ﬁcation. In EMNLP.
Kingma, D., and Ba, J. 2015. Adam: A method for stochastic
optimization. In ICLR.
Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun, R.;
Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In NIPS.
Kokkinos, F., and Potamianos, A.
2017. Structural attention
neural networks for improved sentiment analysis. arXiv preprint
arXiv:1701.01811.
Lei, T., and Zhang, Y. 2017. Training rnns as fast as cnns. arXiv
preprint arXiv:1709.02755.
Lei, T.; Barzilay, R.; and Jaakkola, T. 2015. Molding cnns for text:
non-linear, non-consecutive convolutions. In EMNLP.
Li, X., and Roth, D. 2002. Learning question classiﬁers. In ACL.
Li, J.; Luong, M.-T.; Jurafsky, D.; and Hovy, E. 2015. When
are tree structures necessary for deep learning of representations?
arXiv preprint arXiv:1503.00185.
Liu, Y.; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natural lan-
guage inference using bidirectional lstm model and inner-attention.
arXiv preprint arXiv:1605.09090.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effective ap-
proaches to attention-based neural machine translation. In EMNLP.
Marelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.; Bernardi, R.;
and Zamparelli, R. 2014. A sick cure for the evaluation of compo-
sitional distributional semantic models. In LREC.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Efﬁcient
estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781.

Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013b. Distributed representations of words and phrases and their
compositionality. In NIPS.
Mou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.; and Jin, Z.
2016. Natural language inference by tree-based convolution and
heuristic matching. In ACL.
Munkhdalai, T., and Yu, H. 2017a. Neural semantic encoders. In
EACL.
Munkhdalai, T., and Yu, H. 2017b. Neural tree indexers for text
understanding. In EACL.
Pang, B., and Lee, L. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on minimum cuts.
In ACL.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
Global vectors for word representation. In EMNLP.
Qian, Q.; Huang, M.; and Zhu, X. 2017. Linguistically regularized
lstms for sentiment classiﬁcation. In ACL.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention
model for abstractive sentence summarization. In EMNLP.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2017. Bidi-
rectional attention ﬂow for machine comprehension. In ICLR.
Shang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine for
short-text conversation. In ACL.
Socher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; Potts, C.; et al. 2013. Recursive deep models for seman-
tic compositionality over a sentiment treebank. In EMNLP.
Socher, R.; Karpathy, A.; Le, Q. V.; Manning, C. D.; and Ng, A. Y.
2014. Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. Transactions of the Association for
Computational Linguistics 2:207–218.
Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural
networks from overﬁtting. Journal of Machine Learning Research
15(1):1929–1958.
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end
memory networks. In NIPS.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved se-
mantic representations from tree-structured long short-term mem-
ory networks. In ACL.
Teng, Z.; Vo, D.-T.; and Zhang, Y. 2016. Context-sensitive lexicon
features for neural sentiment analysis. In EMNLP.
Vaswani, A.; Shazeer; Noam; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all
you need. In NIPS.
Vendrov, I.; Kiros, R.; Fidler, S.; and Urtasun, R. 2016. Order-
embeddings of images and language. In ICLR.
Wiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating expressions
of opinions and emotions in language. Language resources and
evaluation 39(2):165–210.
Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A broad-
coverage challenge corpus for sentence understanding through in-
ference. arXiv preprint arXiv:1704.05426.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method.
arXiv preprint arXiv:1212.5701.
Zhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hierarchical
sentence model. In IJCAI.
Zhao, J.; Zhu, T.; and Lan, M. 2014. Ecnu: One stone two birds:
Ensemble of heterogenous measures for semantic relatedness and
textual entailment. In SemEval@ COLING, 271–277.

DiSAN: Directional Self-Attention Network for
RNN/CNN-Free Language Understanding
Tianyi Zhou‡
Shirui Pan†

Tao Shen†
Jing Jiang†

Guodong Long†
Chengqi Zhang†

†Centre of Artiﬁcial Intelligence, FEIT, University of Technology Sydney
‡Paul G. Allen School of Computer Science & Engineering, University of Washington
tao.shen@student.uts.edu.au, tianyizh@uw.edu
{guodong.long, jing.jiang, shirui.pan, chengqi.zhang}@uts.edu.au

7
1
0
2
 
v
o
N
 
0
2
 
 
]
L
C
.
s
c
[
 
 
3
v
6
9
6
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recurrent neural nets (RNN) and convolutional neural nets
(CNN) are widely used on NLP tasks to capture the long-term
and local dependencies, respectively. Attention mechanisms
have recently attracted enormous interest due to their highly
parallelizable computation, signiﬁcantly less training time,
and ﬂexibility in modeling dependencies. We propose a novel
attention mechanism in which the attention between elements
from input sequence(s) is directional and multi-dimensional
(i.e., feature-wise). A light-weight neural net, “Directional
Self-Attention Network (DiSAN)”, is then proposed to learn
sentence embedding, based solely on the proposed attention
without any RNN/CNN structure. DiSAN is only composed
of a directional self-attention with temporal order encoded,
followed by a multi-dimensional attention that compresses
the sequence into a vector representation. Despite its simple
form, DiSAN outperforms complicated RNN models on both
prediction quality and time efﬁciency. It achieves the best
test accuracy among all sentence encoding methods and im-
proves the most recent best result by 1.02% on the Stanford
Natural Language Inference (SNLI) dataset, and shows state-
of-the-art test accuracy on the Stanford Sentiment Treebank
(SST), Multi-Genre natural language inference (MultiNLI),
Sentences Involving Compositional Knowledge (SICK), Cus-
tomer Review, MPQA, TREC question-type classiﬁcation
and Subjectivity (SUBJ) datasets.

1

Introduction

Context dependency plays a signiﬁcant role in language
understanding and provides critical information to natural
language processing (NLP) tasks. For different tasks and
data, researchers often switch between two types of deep
neural network (DNN): recurrent neural network (RNN)
with sequential architecture capturing long-range dependen-
cies (e.g., long short-term memory (LSTM) (Hochreiter and
Schmidhuber 1997) and gated recurrent unit (GRU) (Chung
et al. 2014)), and convolutional neural network (CNN) (Kim
2014) whose hierarchical structure is good at extracting lo-
cal or position-invariant features. However, which network
to choose in practice is an open question, and the choice re-
lies largely on the empirical knowledge.

Recent works have found that equipping RNN or CNN
with an attention mechanism can achieve state-of-the-art

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

performance on a large number of NLP tasks, including neu-
ral machine translation (Bahdanau, Cho, and Bengio 2015;
Luong, Pham, and Manning 2015), natural language infer-
ence (Liu et al. 2016), conversation generation (Shang, Lu,
and Li 2015), question answering (Hermann et al. 2015;
Sukhbaatar et al. 2015), machine reading comprehension
(Seo et al. 2017), and sentiment analysis (Kokkinos and
Potamianos 2017). The attention uses a hidden layer to com-
pute a categorical distribution over elements from the in-
put sequence to reﬂect their importance weights. It allows
RNN/CNN to maintain a variable-length memory, so that
elements from the input sequence can be selected by their
importance/relevance and merged into the output. In con-
trast to RNN and CNN, the attention mechanism is trained
to capture the dependencies that make signiﬁcant contribu-
tions to the task, regardless of the distance between the el-
ements in the sequence. It can thus provide complementary
information to the distance-aware dependencies modeled by
RNN/CNN. In addition, computing attention only requires
matrix multiplication, which is highly parallelizable com-
pared to the sequential computation of RNN.

In a very recent work (Vaswani et al. 2017), an atten-
tion mechanism is solely used to construct a sequence to
sequence (seq2seq) model that achieves a state-of-the-art
quality score on the neural machine translation (NMT) task.
The seq2seq model, “Transformer”, has an encoder-decoder
structure that is only composed of stacked attention net-
works, without using either recurrence or convolution. The
proposed attention, “multi-head attention”, projects the in-
put sequence to multiple subspaces, then applies scaled dot-
product attention to its representation in each subspace,
and lastly concatenates their output. By doing this, it can
combine different attentions from multiple subspaces. This
mechanism is used in Transformer to compute both the
context-aware features inside the encoder/decoder and the
bottleneck features between them.

The attention mechanism has more ﬂexibility in sequence
length than RNN/CNN, and is more task/data-driven when
modeling dependencies. Unlike sequential models, its com-
putation can be easily and signiﬁcantly accelerated by exist-
ing distributed/parallel computing schemes. However, to the
best of our knowledge, a neural net entirely based on atten-
tion has not been designed for other NLP tasks except NMT,
especially those that cannot be cast into a seq2seq problem.

Compared to RNN, a disadvantage of most attention mech-
anisms is that the temporal order information is lost, which
however might be important to the task. This explains why
positional encoding is applied to the sequence before being
processed by the attention in Transformer. How to model or-
der information within an attention is still an open problem.
The goal of this paper is to develop a uniﬁed and
RNN/CNN-free attention network that can be generally uti-
lized to learn the sentence encoding model for different NLP
tasks, such as natural language inference, sentiment analy-
sis, sentence classiﬁcation and semantic relatedness. We fo-
cus on the sentence encoding model because it is a basic
module of most DNNs used in the NLP literature.

We propose a novel attention mechanism that differs from
previous ones in that it is 1) multi-dimensional: the atten-
tion w.r.t. each pair of elements from the source(s) is a vec-
tor, where each entry is the attention computed on each fea-
ture; and 2) directional: it uses one or multiple positional
masks to model the asymmetric attention between two el-
ements. We compute feature-wise attention since each ele-
ment in a sequence is usually represented by a vector, e.g.,
word/character embedding (Kim et al. 2016), and attention
on different features can contain different information about
dependency, thus to handle the variation of contexts around
the same word. We apply positional masks to attention dis-
tribution since they can easily encode prior structure knowl-
edge such as temporal order and dependency parsing. This
design mitigates the weakness of attention in modeling order
information, and takes full advantage of parallel computing.
We then build a light-weight and RNN/CNN-free neural
network, “Directional Self-Attention Network (DiSAN)”,
for sentence encoding. This network relies entirely on the
proposed attentions and does not use any RNN/CNN struc-
ture. In DiSAN, the input sequence is processed by direc-
tional (forward and backward) self-attentions to model con-
text dependency and produce context-aware representations
for all tokens. Then, a multi-dimensional attention computes
a vector representation of the entire sequence, which can
be passed into a classiﬁcation/regression module to com-
pute the ﬁnal prediction for a particular task. Unlike Trans-
former, neither stacking of attention blocks nor an encoder-
decoder structure is required. The simple architecture of
DiSAN leads to fewer parameters, less computation and eas-
ier parallelization.

In experiments1, we compare DiSAN with the currently
popular methods on various NLP tasks, e.g., natural lan-
guage inference, sentiment analysis, sentence classiﬁcation,
etc. DiSAN achieves the highest test accuracy on the Stan-
ford Natural Language Inference (SNLI) dataset among
sentence-encoding models and improves the currently best
result by 1.02%. It also shows the state-of-the-art perfor-
mance on the Stanford Sentiment Treebank (SST), Multi-
Genre natural language inference (MultiNLI), SICK, Cus-
tomer Review, MPQA, SUBJ and TREC question-type clas-
siﬁcation datasets. Meanwhile, it has fewer parameters and
exhibits much higher computation efﬁciency than the mod-

1Codes and pre-trained models for experiments can be found at

https://github.com/taoshen58/DiSAN

els it outperforms, e.g., LSTM and tree-based models.

Annotation: 1) Lowercase denotes a vector; 2) bold low-
ercase denotes a sequence of vectors (stored as a matrix);
and 3) uppercase denotes a matrix or a tensor.

2 Background

2.1 Sentence Encoding
In the pipeline of NLP tasks, a sentence is denoted by a se-
quence of discrete tokens (e.g., words or characters) v =
[v1, v2, . . . , vn], where vi could be a one-hot vector whose
dimension length equals the number of distinct tokens N .
A pre-trained token embedding (e.g., word2vec (Mikolov
et al. 2013b) or GloVe (Pennington, Socher, and Manning
2014)) is applied to v and transforms all discrete tokens to
a sequence of low-dimensional dense vector representations
x = [x1, x2, . . . , xn] with xi ∈ Rde. This pre-process can
be written as x = W (e)v, where word embedding weight
matrix W (e) ∈ Rde×N and x ∈ Rde×n.

Most DNN sentence-encoding models for NLP tasks take
x as the input and further generate a vector representation ui
for each xi by context fusion. Then a sentence encoding is
obtained by mapping the sequence u = [u1, u2, . . . , un] to
a single vector s ∈ Rd, which is used as a compact encoding
of the entire sentence in NLP problems.

2.2 Attention
The attention is proposed to compute an alignment score be-
tween elements from two sources. In particular, given the to-
ken embeddings of a source sequence x = [x1, x2, . . . , xn]
and the vector representation of a query q, attention com-
putes the alignment score between xi and q by a compati-
bility function f (xi, q), which measures the dependency be-
tween xi and q, or the attention of q to xi. A softmax func-
tion then transforms the scores [f (xi, q)]n
i=1 to a probability
distribution p(z|x, q) by normalizing over all the n tokens
of x. Here z is an indicator of which token in x is important
to q on a speciﬁc task. That is, large p(z = i|x, q) means xi
contributes important information to q. The above process
can be summarized by the following equations.
a = [f (xi, q)]n
p(z|x, q) = softmax(a).

(1)
(2)

i=1 ,

Speciﬁcally,

p(z = i|x, q) =

exp(f (xi, q))
i=1 exp(f (xi, q))

.

(cid:80)n

(3)

The output of this attention mechanism is a weighted sum
of the embeddings for all tokens in x, where the weights
are given by p(z|x, q). It places large weights on the tokens
important to q, and can be written as the expectation of a
token sampled according to its importance, i.e.,

s =

p(z = i|x, q)xi = Ei∼p(z|x,q)(xi),

(4)

n
(cid:88)

i=1

where s ∈ Rde can be used as the sentence encoding of x.

Additive attention (or multi-layer perceptron attention)
(Bahdanau, Cho, and Bengio 2015; Shang, Lu, and Li

3.1 Multi-dimensional Attention
Multi-dimensional attention is a natural extension of addi-
tive attention (or MLP attention) at the feature level. Instead
of computing a single scalar score f (xi, q) for each token xi
as shown in Eq.(5), multi-dimensional attention computes a
feature-wise score vector for xi by replacing weight vector
w in Eq.(5) with a matrix W , i.e.,

f (xi, q) = W T σ

W (1)xi + W (2)q

(7)

(cid:17)

,

(cid:16)

where f (xi, q) ∈ Rde is a vector with the same length as xi,
and all the weight matrices W, W (1), W (2) ∈ Rde×de. We
further add two bias terms to the parts in and out activation
σ(·), i.e.,

f (xi, q) = W T σ

(cid:16)

W (1)xi + W (2)q + b(1)(cid:17)

+ b.

(8)

We then compute a categorical distribution p(zk|x, q) over
all the n tokens for each feature k ∈ [de]. A large p(zk =
i|x, q) means that feature k of token i is important to q.

We apply the same procedure Eq.(1)-(3) in traditional at-
tention to the kth dimension of f (xi, q). In particular, for
each feature k ∈ [de], we replace f (xi, q) with [f (xi, q)]k,
and change z to zk in Eq.(1)-(3). Now each feature k in each
token i has an importance weight Pki (cid:44) p(zk = i|x, q). The
output s can be written as
(cid:105)de

(cid:104)(cid:88)n

= (cid:2)Ei∼p(zk|x,q)(xki)(cid:3)de

k=1 .

(9)

Pkixki

s =

i=1

k=1

We give an illustration of traditional attention and multi-
dimensional attention in Figure 1. In the rest of this paper,
we will ignore the subscript k which indexes feature dimen-
sion for simpliﬁcation if no confusion is possible. Hence,
the output s can be written as an element-wise product
s = (cid:80)n

i=1 P·i (cid:12) xi

Remark: The word embedding usually suffers from the
polysemy in natural language. Since traditional attention
computes a single importance score for each word based on
the word embedding, it cannot distinguish the meanings of
the same word in different contexts. Multi-dimensional at-
tention, however, computes a score for each feature of each
word, so it can select the features that can best describe the
word’s speciﬁc meaning in any given context, and include
this information in the sentence encoding output s.

3.2 Two types of Multi-dimensional Self-attention
When extending multi-dimension to self-attentions, we have
two variants of multi-dimensional attention. The ﬁrst one,
called multi-dimensional “token2token” self-attention, ex-
plores the dependency between xi and xj from the same
source x, and generates context-aware coding for each el-
ement. It replaces q with xj in Eq.(8), i.e.,
(cid:16)

W (1)xi + W (2)xj + b(1)(cid:17)

f (xi, xj) = W T σ

+ b.

(10)

Similar to P in vanilla multi-dimensional attention, we com-
pute a probability matrix P j ∈ Rde×n for each xj such that
P j
ki

(cid:44) p(zk = i|x, xj). The output for xj is

sj =

P j

·i (cid:12) xi

n
(cid:88)

i=1

(11)

(a)

(b)

Figure 1: (a) Traditional (additive/multiplicative) attention
and (b) multi-dimensional attention. zi denotes alignment
score f (xi, q), which is a scalar in (a) but a vector in (b).

2015) and multiplicative attention (or dot-product attention)
(Vaswani et al. 2017; Sukhbaatar et al. 2015; Rush, Chopra,
and Weston 2015) are the two most commonly used atten-
tion mechanisms. They share the same and uniﬁed form of
attention introduced above, but are different in the compati-
bility function f (xi, q). Additive attention is associated with

f (xi, q) = wT σ(W (1)xi + W (2)q),
(5)
where σ(·) is an activation function and w ∈ Rde is a weight
vector. Multiplicative attention uses inner product or cosine
similarity for f (xi, q), i.e.,
(cid:68)
W (1)xi, W (2)q

f (xi, q) =

(6)

(cid:69)

.

In practice, additive attention often outperforms multiplica-
tive one in prediction quality, but the latter is faster and more
memory-efﬁcient due to optimized matrix multiplication.

2.3 Self-Attention
Self-Attention is a special case of the attention mechanism
introduced above. It replaces q with a token embedding xj
from the source input itself. It relates elements at different
positions from a single sequence by computing the attention
between each pair of tokens, xi and xj. It is very expres-
sive and ﬂexible for both long-range and local dependen-
cies, which used to be respectively modeled by RNN and
CNN. Moreover, it has much faster computation speed and
fewer parameters than RNN. In recent works, we have al-
ready witnessed its success across a variety of NLP tasks,
such as reading comprehension (Hu, Peng, and Qiu 2017)
and neural machine translation (Vaswani et al. 2017).

3 Two Proposed Attention Mechanisms
In this section, we introduce two novel attention mecha-
nisms, multi-dimensional attention in Section 3.1 (with two
extensions to self-attention in Section 3.2) and directional
self-attention in Section 3.3. They are the main components
of DiSAN and may be of independent interest to other neural
nets for other NLP problems in which an attention is needed.

In DiSA, we ﬁrst transform the input sequence x =
to a sequence of hidden state h =

[x1, x2, . . . , xn]
[h1, h2, . . . , hn] by a fully connected layer, i.e.,

h = σh

(cid:16)

W (h)x + b(h)(cid:17)

,

(14)

where x ∈ Rde×n, h ∈ Rdh×n, W (h) and b(h) are the learn-
able parameters, and σh(·) is an activation function.

We then apply multi-dimensional

token2token self-
attention to h, and generate context-aware vector represen-
tations s for all elements from the input sequence. We make
two modiﬁcations to Eq.(10) to reduce the number of param-
eters and make the attention directional.

First, we set W in Eq.(10) to a scalar c and divide the part
in σ(·) by c, and we use tanh(·) for σ(·), which reduces the
number of parameters. In experiments, we always set c = 5,
and obtain stable output.

Second, we apply a positional mask to Eq.(10), so the at-
tention between two elements can be asymmetric. Given a
mask M ∈ {0, −∞}n×n, we set bias b to a constant vec-
tor Mij1 in Eq.(10), where 1 is an all-one vector. Hence,
Eq.(10) is modiﬁed to

f (hi, hj) =

c · tanh

(cid:16)

(cid:17)
[W (1)hi + W (2)hj + b(1)]/c

+ Mij1.

(15)

To see why a mask can encode directional information, let
us consider a case in which Mij = −∞ and Mji = 0, which
results in [f (hi, hj)]k = −∞ and unchanged [f (hj, hi)]k.
Since the probability p(zk = i|x, xj) is computed by
softmax, [f (hi, hj)]k = −∞ leads to p(zk = i|x, xj) = 0.
This means that there is no attention of xj to xi on feature k.
On the contrary, we have p(zk = j|x, xi) > 0, which means
that attention of xi to xj exists on feature k. Therefore, prior
structure knowledge such as temporal order and dependency
parsing can be easily encoded by the mask, and explored in
generating sentence encoding. This is an important feature
of DiSA that previous attention mechanisms do not have.

For self-attention, we usually need to disable the attention
of each token to itself (Hu, Peng, and Qiu 2017). This is the
same as applying a diagonal-disabled (i.e., diag-disabled)
mask such that

M diag

ij =

(cid:26) 0,
−∞,

i (cid:54)= j
i = j

Moreover, we can use masks to encode temporal order
information into attention output. In this paper, we use two
masks, i.e., forward mask M f w and backward mask M bw,

M f w

ij =

M bw

ij =

(cid:26) 0,
−∞,

(cid:26) 0,
−∞,

i < j
otherwise

i > j
otherwise

(16)

(17)

(18)

In forward mask M f w, there is the only attention of later
token j to early token i, and vice versa in backward mask.
We show these three positional masks in Figure 3.

Given input sequence x and a mask M , we compute
f (xi, xj) according to Eq.(15), and follow the standard pro-
cedure of multi-dimensional token2token self-attention to

Figure 2: Directional self-attention (DiSA) mechanism.
Here, we use li,j to denote f (hi, hj) in Eq. (15).

The output of token2token self-attention for all elements
from x is s = [s1, s2, . . . , sn] ∈ Rde×n.

The second one, multi-dimensional “source2token” self-
attention, explores the dependency between xi and the entire
sequence x, and compresses the sequence x into a vector. It
removes q from Eq.(8), i.e.,

f (xi) = W T σ

(cid:16)

W (1)xi + b(1)(cid:17)

+ b.

(12)

The probability matrix is deﬁned as Pki (cid:44) p(zk = i|x)
and is computed in the same way as P in vanilla multi-
dimensional attention. The output s is also same, i.e.,

s =

P·i (cid:12) xi

(13)

n
(cid:88)

i=1

We will use these two types (i.e.,

token2token and
source2token) of multi-dimensional self-attention in differ-
ent parts of our sentence encoding model, DiSAN.

3.3 Directional Self-Attention

Directional self-attention (DiSA) is composed of a fully
connected layer whose input is the token embeddings x,
a “masked” multi-dimensional token2token self-attention
block to explore the dependency and temporal order, and a
fusion gate to combine the output and input of the attention
block. Its structure is shown in Figure 2. It can be used as
either a neural net or a module to compose a large network.

(a) Diag-disabled mask

(b) Forward mask

(c) Backward mask

Figure 3: Three positional masks: (a) is the diag-disabled
mask M diag; (b) and (c) are forward mask M f w and back-
ward mask M bw, respectively.

(19)

(20)

compute the probability matrix P j for each j ∈ [n]. Each
output sj in s is computed as in Eq.(11).

The ﬁnal output u ∈ Rdh×n of DiSA is obtained by com-
bining the output s and the input h of the masked multi-
dimensional token2token self-attention block. This yields a
temporal order encoded and context-aware vector represen-
tation for each element/token. The combination is accom-
plished by a dimension-wise fusion gate, i.e.,

F = sigmoid

(cid:16)

W (f 1)s + W (f 2)h + b(f )(cid:17)

u = F (cid:12) h + (1 − F ) (cid:12) s

where W (f 1), W (f 2) ∈ Rdh×dh and b(f ) ∈ Rdh are the
learnable parameters of the fusion gate.

4 Directional Self-Attention Network
We propose a light-weight network, “Directional Self-
Attention Network (DiSAN)”, for sentence encoding. Its ar-
chitecture is shown in Figure 4.

Given an input sequence of token embedding x, DiSAN
ﬁrstly applies two parameter-untied DiSA blocks with for-
ward mask M f w Eq.(17) and M bw Eq.(18), respectively.
The feed-forward procedure is given in Eq.(14)-(15) and
Eq.(19)-(20). Their outputs are denoted by uf w, ubw ∈
Rdh×n. We concatenate them vertically as [uf w; ubw] ∈
R2dh×n, and use this concatenated output as input to a multi-
dimensional source2token self-attention block, whose out-
put sdisan ∈ R2dh computed by Eq.(12)-(13) is the ﬁnal
sentence encoding result of DiSAN.

Remark: In DiSAN, forward/backward DiSA blocks
work as context fusion layers. And the multi-dimensional
source2token self-attention compresses the sequence into a

Figure 4: Directional self-attention network (DiSAN)

single vector. The idea of using both forward and backward
attentions is inspired by Bi-directional LSTM (Bi-LSTM)
(Graves, Jaitly, and Mohamed 2013), in which forward and
backward LSTMs are used to encode long-range depen-
dency from different directions. In Bi-LSTM, LSTM com-
bines the context-aware output with the input by multi-gate.
The fusion gate used in DiSA shares the similar motivation.
However, DiSAN has fewer parameters, simpler structure
and better efﬁciency.

5 Experiments
In this section, we ﬁrst apply DiSAN to natural language
inference and sentiment analysis tasks. DiSAN achieves
the state-of-the-art performance and signiﬁcantly better ef-
ﬁciency than other baseline methods on benchmark datasets
for both tasks. We also conduct experiments on other NLP
tasks and DiSAN also achieves state-of-the-art performance.
Training Setup: We use cross-entropy loss plus L2 regu-
larization penalty as optimization objective. We minimize
it by Adadelta (Zeiler 2012) (an optimizer of mini-batch
SGD) with batch size of 64. We use Adadelta rather than
Adam (Kingma and Ba 2015) because in our experiments,
DiSAN optimized by Adadelta can achieve more stable per-
formance than Adam optimized one. Initial learning rate is
set to 0.5. All weight matrices are initialized by Glorot Ini-
tialization (Glorot and Bengio 2010), and the biases are ini-
tialized with 0. We initialize the word embedding in x by
300D GloVe 6B pre-trained vectors (Pennington, Socher,
and Manning 2014). The Out-of-Vocabulary words in train-
ing set are randomly initialized by uniform distribution be-
tween (−0.05, 0.05). The word embeddings are ﬁne-tuned
during the training phrase. We use Dropout (Srivastava et
al. 2014) with keep probability 0.75 for language inference
and 0.8 for sentiment analysis. The L2 regularization de-
cay factors γ are 5 × 10−5 and 10−4 for language inference
and sentiment analysis, respectively. Note that the dropout
keep probability and γ varies with the scale of correspond-
ing dataset. Hidden units number dh is set to 300. Activa-
tion functions σ(·) are ELU (exponential linear unit) (Clev-
ert, Unterthiner, and Hochreiter 2016) if not speciﬁed. All
models are implemented with TensorFlow2 and run on sin-

2https://www.tensorﬂow.org

Model Name

|θ|

T(s)/epoch Train Accu(%) Test Accu(%)

Unlexicalized features (Bowman et al. 2015)
+ Unigram and bigram features (Bowman et al. 2015)

100D LSTM encoders (Bowman et al. 2015)
300D LSTM encoders (Bowman et al. 2016)
1024D GRU encoders (Vendrov et al. 2016)
300D Tree-based CNN encoders (Mou et al. 2016)
300D SPINN-PI encoders (Bowman et al. 2016)
600D Bi-LSTM encoders (Liu et al. 2016)
300D NTI-SLSTM-LSTM encoders (Munkhdalai and Yu 2017b)
600D Bi-LSTM encoders+intra-attention (Liu et al. 2016)
300D NSE encoders (Munkhdalai and Yu 2017a)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

Directional self-attention network (DiSAN)

0.2m
3.0m
15m
3.5m
3.7m
2.0m
4.0m
2.8m
3.0m

0.45m
0.54m
1.98m
2.88m
2.35m

2.35m

216
261
345
2080
592

587

49.4
99.7

84.8
83.9
98.8
83.3
89.2
86.4
82.5
84.5
86.2

82.39
86.22
89.58
90.39
90.18

91.08

50.4
78.2

77.6
80.6
81.4
82.1
83.2
83.3
83.4
84.2
84.6

79.81
83.12
84.17
84.98
84.66

85.62

Table 1: Experimental results for different methods on SNLI. |θ|: the number of parameters (excluding word embedding part).
T(s)/epoch: average time (second) per epoch. Train Accu(%) and Test Accu(%): the accuracy on training and test set.

gle Nvidia GTX 1080Ti graphic card.

5.1 Natural Language Inference
The goal of Natural Language Inference (NLI) is to rea-
son the semantic relationship between a premise sentence
and a corresponding hypothesis sentence. The possible rela-
tionship could be entailment, neutral or contradiction. We
compare different models on a widely used benchmark,
Stanford Natural Language Inference (SNLI)3 (Bowman et
al. 2015) dataset, which consists of 549,367/9,842/9,824
(train/dev/test) premise-hypothesis pairs with labels.

Following the standard procedure in Bowman et al.
(2016), we launch two sentence encoding models (e.g.,
DiSAN) with tied parameters for the premise sentence and
hypothesis sentence, respectively. Given the output encoding
sp for the premise and sh for the hypothesis, the represen-
tation of relationship is the concatenation of sp, sh, sp − sh
and sp (cid:12) sh, which is fed into a 300D fully connected layer
and then a 3-unit output layer with softmax to compute a
probability distribution over the three types of relationship.
For thorough comparison, besides the neural nets pro-
posed in previous works of NLI, we implement ﬁve extra
neural net baselines to compare with DiSAN. They help
us to analyze the improvement contributed by each part of
DiSAN and to verify that the two attention mechanisms pro-
posed in Section 3 can improve other networks.

• Word Embedding with additive attention.

• Word Embedding with s2t self-attention: DiSAN with

DiSA blocks removed.

• Multi-head with s2t self-attention: Multi-head attention
(Vaswani et al. 2017) (8 heads, each has 75 hidden units)
with source2token self-attention. The positional encoding

3https://nlp.stanford.edu/projects/snli/

method used in Vaswani et al. (2017) is applied to the in-
put sequence to encode temporal information. We ﬁnd our
experiments show that multi-head attention is sensitive to
hyperparameters, so we adjust keep probability of dropout
from 0.7 to 0.9 with step 0.05 and report the best result.
• Bi-LSTM with s2t self-attention: a multi-dimensional
source2token self-attention block is applied to the output
of Bi-LSTM (300D forward + 300D backward LSTMs).
• DiSAN without directions: DiSAN with the for-
ward/backward masks M f w and M bw replaced with two
diag-disabled masks M diag, i.e., DiSAN without for-
ward/backward order information.
Compared to the results from the ofﬁcial leaderboard
of SNLI in Table 1, DiSAN outperforms previous works
and improves the best latest test accuracy (achieved by a
memory-based NSE encoder network) by a remarkable mar-
gin of 1.02%. DiSAN surpasses the RNN/CNN based mod-
els with more complicated architecture and more parameters
by large margins, e.g., +2.32% to Bi-LSTM, +1.42% to Bi-
LSTM with additive attention. It even outperforms models
with the assistance of a semantic parsing tree, e.g., +3.52%
to Tree-based CNN, +2.42% to SPINN-PI.

In the results of the ﬁve baseline methods and DiSAN at
the bottom of Table 1, we demonstrate that making attention
multi-dimensional (feature-wise) or directional brings sub-
stantial improvement to different neural nets. First, a com-
parison between the ﬁrst two models shows that changing
token-wise attention to multi-dimensional/feature-wise at-
tention leads to 3.31% improvement on a word embedding
based model. Also, a comparison between the third baseline
and DiSAN shows that DiSAN can substantially outperform
multi-head attention by 1.45%. Moreover, a comparison be-
tween the forth baseline and DiSAN shows that the DiSA
block can even outperform Bi-LSTM layer in context en-
coding, improving test accuracy by 0.64%. A comparison

between the ﬁfth baseline and DiSAN shows that directional
self-attention with forward and backward masks (with tem-
poral order encoded) can bring 0.96% improvement.

Additional advantages of DiSAN shown in Table 1 are
its fewer parameters and compelling time efﬁciency. It is
×3 faster than widely used Bi-LSTM model. Compared to
other models with competitive performance, e.g., 600D Bi-
LSTM encoders with intra-attention (2.8M), 300D NSE en-
coders (3.0M) and 600D Bi-LSTM encoders with multi-
dimensional attention (2.88M), DiSAN only has 2.35M pa-
rameters.

5.2 Sentiment Analysis

Model

Test Accu

MV-RNN (Socher et al. 2013)
RNTN (Socher et al. 2013)
Bi-LSTM (Li et al. 2015)
Tree-LSTM (Tai, Socher, and Manning 2015)
CNN-non-static (Kim 2014)
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015)
NCSL (Teng, Vo, and Zhang 2016)
LR-Bi-LSTM (Qian, Huang, and Zhu 2017)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

DiSAN

44.4
45.7
49.8
51.0
48.0
51.2
51.1
50.6

47.47
48.87
49.14
49.95
49.41

51.72

Table 2: Test accuracy of ﬁne-grained sentiment analysis on
Stanford Sentiment Treebank (SST) dataset.

Sentiment analysis aims to analyze the sentiment of a
sentence or a paragraph, e.g., a movie or a product re-
view. We use Stanford Sentiment Treebank (SST)4 (Socher
et al. 2013) for the experiments, and only focus on the
ﬁne-grained movie review sentiment classiﬁcation over ﬁve
classes, i.e., very negative, negative, neutral, positive and
very positive. We use the standard train/dev/test sets split
with 8,544/1,101/2,210 samples. Similar to Section 5.1, we
employ a single sentence encoding model to obtain a sen-
tence representation s of a movie review, then pass it into
a 300D fully connected layer. Finally, a 5-unit output layer
with softmax is used to calculate a probability distribution
over the ﬁve classes.

In Table 2, we compare previous works with DiSAN on
test accuracy. To the best of our knowledge, DiSAN im-
proves the last best accuracy (given by CNN-Tensor) by
0.52%. Compared to tree-based models with heavy use of
the prior structure, e.g., MV-RNN, RNTN and Tree-LSTM,
DiSAN outperforms them by 7.32%, 6.02% and 0.72%,
respectively. Additionally, DiSAN achieves better perfor-
mance than CNN-based models. More recent works tend
to focus on lexicon-based sentiment analysis, by explor-
ing sentiment lexicons, negation words and intensity words.

4https://nlp.stanford.edu/sentiment/

Nonetheless, DiSAN still outperforms these fancy models,
such as NCSL (+0.62%) and LR-Bi-LSTM (+1.12%).

Figure 5: Fine-grained sentiment analysis accuracy vs. sen-
tence length. The results of LSTM, Bi-LSTM and Tree-
LSTM are from Tai, Socher, and Manning (2015) and the
result of DiSAN is the average over ﬁve random trials.

It is also interesting to see the performance of different
models on the sentences with different lengths. In Figure 5,
we compare LSTM, Bi-LSTM, Tree-LSTM and DiSAN on
different sentence lengths. In the range of (5, 12), the length
range for most movie review sentences, DiSAN signiﬁcantly
outperforms others. Meanwhile, DiSAN also shows impres-
sive performance for slightly longer sentences or paragraphs
in the range of (25, 38). DiSAN performs poorly when the
sentence length ≥ 38, in which however only 3.21% of total
movie review sentences lie.

5.3 Experiments on Other NLP Tasks
Multi-Genre Natural Language Inference Multi-Genre
Natural Language Inference (MultiNLI)5 (Williams, Nan-
gia, and Bowman 2017) dataset consists of 433k sentence
pairs annotated with textual entailment information. This
dataset is similar to SNLI, but it covers more genres of spo-
ken and written text, and supports a distinctive cross-genre
generalization evaluation. However, MultiNLI is a quite new
dataset, and its leaderboard does not include a session for
the sentence-encoding only model. Hence, we only compare
DiSAN with the baselines provided at the ofﬁcial website.
The results of DiSAN and two sentence-encoding models
on the leaderboard are shown in Table 3. Note that the pre-
diction accuracies of Matched and Mismatched test datasets
are obtained by submitting our test results to Kaggle open
evaluation platforms6: MultiNLI Matched Open Evaluation
and MultiNLI Mismatched Open Evaluation.

Semantic Relatedness The task of semantic relatedness
aims to predict a similarity degree of a given pair of sen-
tences. We show an experimental comparison of different

5https://www.nyu.edu/projects/bowman/multinli/
6https://inclass.kaggle.com/c/multinli-matched-open-

evaluation and https://inclass.kaggle.com/c/multinli-mismatched-
open-evaluation

Method Matched Mismatched

0.65200
cBoW
Bi-LSTM 0.67507

DiSAN

0.70977

0.64759
0.67248

0.71402

Table 3: Experimental results of prediction accuracy for dif-
ferent methods on MultiNLI.

Model
cBoWa
Skip-thoughtb
DCNNc
AdaSentd
SRUe
Wide CNNse

CR

79.9
81.3
/

MPQA

SUBJ

TREC

86.4
87.5
/

91.3
93.6
/

87.3
92.2
93.0

83.6 (1.6) 90.4 (0.7) 92.2 (1.2) 91.1 (1.0)
84.8 (1.3) 89.7 (1.1) 93.4 (0.8) 93.9 (0.6)
82.2 (2.2) 88.8 (1.2) 92.9 (0.7) 93.2 (0.5)

DiSAN

84.8 (2.0) 90.1 (0.4) 94.2 (0.6) 94.2 (0.1)

methods on Sentences Involving Compositional Knowledge
(SICK)7 dataset (Marelli et al. 2014). SICK is composed
of 9,927 sentence pairs with 4,500/500/4,927 instances for
train/dev/test. The regression module on the top of DiSAN is
introduced by Tai, Socher, and Manning (2015). The results
in Table 4 show that DiSAN outperforms the models from
previous works in terms of Pearson’s r and Spearman’s ρ
indexes.

Table 5: Experimental results for different methods on vari-
ous sentence classiﬁcation benchmarks. The reported accu-
racies on CR, MPQA and SUBJ are the mean of 10-fold
cross validation, the accuracies on TREC are the mean of
dev accuracies of ﬁve runs. All standard deviations are in
parentheses. a(Mikolov et al. 2013a), b(Kiros et al. 2015),
c(Kalchbrenner, Grefenstette, and Blunsom 2014), d(Zhao,
Lu, and Poupart 2015), e(Lei and Zhang 2017).

Pearson’s r Spearman’s ρ MSE

Model
Meaning Factorya
.7721
ECNUb
/
DT-RNNc
.7923 (.0070) .7319 (.0071) .3822 (.0137)
SDT-RNNc
.7900 (.0042) .7304 (.0042) .3848 (.0042)
Cons. Tree-LSTMd .8582 (.0038) .7966 (.0053) .2734 (.0108)
Dep. Tree-LSTMd .8676 (.0030) .8083 (.0042) .2532 (.0052)

.8268
.8414

.3224
/

DiSAN

.8695 (.0012) .8139 (.0012) .2879 (.0036)

Table 4: Experimental results for different methods on SICK
sentence relatedness dataset. The reported accuracies are the
mean of ﬁve runs (standard deviations in parentheses). Cons.
and Dep. represent Constituency and Dependency, respec-
tively. a(Bjerva et al. 2014), b(Zhao, Zhu, and Lan 2014),
c(Socher et al. 2014), d(Tai, Socher, and Manning 2015).

Sentence Classiﬁcations The goal of sentence classiﬁca-
tion is to correctly predict the class label of a given sentence
in various scenarios. We evaluate the models on four sen-
tence classiﬁcation benchmarks of various NLP tasks, such
as sentiment analysis and question-type classiﬁcation. They
are listed as follows. 1) CR: Customer review (Hu and Liu
2004) of various products (cameras, etc.), which is to pre-
dict whether the review is positive or negative; 2) MPQA:
Opinion polarity detection subtask of the MPQA dataset
(Wiebe, Wilson, and Cardie 2005); 3) SUBJ: Subjectivity
dataset (Pang and Lee 2004) whose labels indicate whether
each sentence is subjective or objective; 4) TREC: TREC
question-type classiﬁcation dataset (Li and Roth 2002). The
experimental results of DiSAN and existing methods are
shown in Table 5.

5.4 Case Study
To gain a closer view of what dependencies in a sentence
can be captured by DiSAN, we visualize the attention prob-
ability p(z = i|x, xj) or alignment score by heatmaps. In

7http://clic.cimec.unitn.it/composes/sick.html

particular, we will focus primarily on the probability in for-
ward/backward DiSA blocks (Figure 6), forward/backward
fusion gates F in Eq.(19) (Figure 7), and the probability
in multi-dimensional source2token self-attention block (Fig-
ure 8). For the ﬁrst two, we desire to demonstrate the de-
pendency at token level, but attention probability in DiSAN
is deﬁned on each feature, so we average the probabilities
along the feature dimension.

We select two sentences from SNLI test set as examples
for this case study. Sentence 1 is Families have some dogs
in front of a carousel and sentence 2 is volleyball match is in
progress between ladies.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 6: Attention probability in forward/backward DiSA
blocks for the two example sentences.

Figure 6 shows that1) semantically important words such
as nouns and verbs usually get large attention, but stop
words (am, is, are, etc.) do not; 2) globally important words,
e.g., volleyball, match, ladies in sentence 1 and dog, front,
carousel in sentence 2, get large attention from all other
words; 3) if a word is important to only some of the other
words (e.g. to constitute a phrase or sense-group), it gets
large attention only from these words, e.g., attention be-
tween progress, between in sentence1, and attention between
families, have in sentence 2.

This also shows that directional information can help to
generate context-aware word representation with temporal
order encoded. For instance, for word match in sentence 1,
its forward DiSA focuses more on word volleyball, while
its backward attention focuses more on progress and ladies,
so the representation of word match contains the essential
information of the entire sentence, and simultaneously in-
cludes the positional order information.

In addition, forward and backward DiSAs can focus on
different parts of a sentence. For example, the forward one
in sentence 2 pays attention to the word families, whereas the
backward one focuses on the word carousel. Since forward
and backward attentions are computed separately, it avoids
normalization over multiple signiﬁcant words to weaken
their weights. Note that this is a weakness of traditional at-
tention compared to RNN, especially for long sentences.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 7: Fusion Gate F in forward/backward DiSA blocks.

In Figure 7, we show that the gate value F in Eq.(19). The
gate combines the input and output of masked self-attention.
It tends to selects the input representation h instead of the
output s if the corresponding weight in F is large. This
shows that the gate values for meaningless words, especially
stop words is small. The stop words themselves cannot con-
tribute important information, so only their semantic rela-
tions to other words might help to understand the sentence.
Hence, the gate tends to use their context features given by
masked self-attention.

(a) glass in pair 1

(b) close in pair 2

Figure 8: Two pairs of attention probability comparison of
same word in difference sentence contexts.

In Figure 8, we show the two multi-dimensional
source2token self-attention score vectors of the same word
in the two sentences, by their heatmaps. The ﬁrst pair has
two sentences: one is The glass bottle is big, and another is
A man is pouring a glass of tea. They share the same word is
glass with different meanings. The second pair has two sen-
tences: one is The restaurant is about to close and another
is A biker is close to the fountain. It can be seen that the
two attention vectors for the same words are very different
due to their different meanings in different contexts. This
indicates that the multi-dimensional attention vector is not
redundant because it can encode more information than one
single score used in traditional attention and it is able to cap-
ture subtle difference of the same word in different contexts
or sentences. Additionally, it can also alleviate the weakness
of the attention over long sequence, which can avoid nor-
malization over entire sequence in traditional attention only
once.

6 Conclusion
In this paper, we propose two novel attention mechanisms,
multi-dimensional attention and directional self-attention.
The multi-dimensional attention performs a feature-wise se-
lection over the input sequence for a speciﬁc task, and the
directional self-attention uses the positional masks to pro-
duce the context-aware representations with temporal in-
formation encoded. Based on these attentions, Directional
Self-Attention Network (DiSAN) is proposed for sentence-
encoding without any recurrent or convolutional structure.
The experiment results show that DiSAN can achieve state-
of-the-art inference quality and outperform existing works
(LSTM, etc.) on a wide range of NLP tasks with fewer pa-
rameters and higher time efﬁciency.

In future work, we will explore the approaches to using
the proposed attention mechanisms on more sophisticated
tasks, e.g. question answering and reading comprehension,
to achieve better performance on various benchmarks.

7 Acknowledgments
This research was funded by the Australian Government
through the Australian Research Council (ARC) under grant
1) LP160100630 partnership with Australia Government
Department of Health, and 2) LP150100671 partnership
with Australia Research Alliance for Children and Youth
(ARACY) and Global Business College Australia (GBCA).

References
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine
translation by jointly learning to align and translate. In ICLR.
Bjerva, J.; Bos, J.; Van der Goot, R.; and Nissim, M. 2014. The
meaning factory: Formal semantics for recognizing textual entail-
ment and determining semantic similarity. In SemEval@ COLING,
642–646.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language inference. In
EMNLP.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
sentence understanding. In ACL.

Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical
evaluation of gated recurrent neural networks on sequence model-
ing. In NIPS.
Clevert, D.-A.; Unterthiner, T.; and Hochreiter, S. 2016. Fast and
accurate deep network learning by exponential linear units (elus).
In ICLR.
Glorot, X., and Bengio, Y. 2010. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings of the
Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, 249–256.
Graves, A.; Jaitly, N.; and Mohamed, A.-r. 2013. Hybrid speech
In Automatic Speech
recognition with deep bidirectional lstm.
Recognition and Understanding (ASRU), 2013 IEEE Workshop on,
273–278. IEEE.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay,
W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to
read and comprehend. In NIPS.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8):1735–1780.
Hu, M., and Liu, B. 2004. Mining and summarizing customer re-
In Proceedings of the tenth ACM SIGKDD international
views.
conference on Knowledge discovery and data mining, 168–177.
ACM.
Hu, M.; Peng, Y.; and Qiu, X. 2017. Reinforced mnemonic reader
for machine comprehension. arXiv preprint arXiv:1705.02798.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. arXiv preprint
arXiv:1404.2188.
Kim, Y.; Jernite, Y.; Sontag, D.; and Rush, A. M. 2016. Character-
aware neural language models. In AAAI.
Kim, Y. 2014. Convolutional neural networks for sentence classi-
ﬁcation. In EMNLP.
Kingma, D., and Ba, J. 2015. Adam: A method for stochastic
optimization. In ICLR.
Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun, R.;
Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In NIPS.
Kokkinos, F., and Potamianos, A.
2017. Structural attention
neural networks for improved sentiment analysis. arXiv preprint
arXiv:1701.01811.
Lei, T., and Zhang, Y. 2017. Training rnns as fast as cnns. arXiv
preprint arXiv:1709.02755.
Lei, T.; Barzilay, R.; and Jaakkola, T. 2015. Molding cnns for text:
non-linear, non-consecutive convolutions. In EMNLP.
Li, X., and Roth, D. 2002. Learning question classiﬁers. In ACL.
Li, J.; Luong, M.-T.; Jurafsky, D.; and Hovy, E. 2015. When
are tree structures necessary for deep learning of representations?
arXiv preprint arXiv:1503.00185.
Liu, Y.; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natural lan-
guage inference using bidirectional lstm model and inner-attention.
arXiv preprint arXiv:1605.09090.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effective ap-
proaches to attention-based neural machine translation. In EMNLP.
Marelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.; Bernardi, R.;
and Zamparelli, R. 2014. A sick cure for the evaluation of compo-
sitional distributional semantic models. In LREC.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Efﬁcient
estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781.

Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013b. Distributed representations of words and phrases and their
compositionality. In NIPS.
Mou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.; and Jin, Z.
2016. Natural language inference by tree-based convolution and
heuristic matching. In ACL.
Munkhdalai, T., and Yu, H. 2017a. Neural semantic encoders. In
EACL.
Munkhdalai, T., and Yu, H. 2017b. Neural tree indexers for text
understanding. In EACL.
Pang, B., and Lee, L. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on minimum cuts.
In ACL.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
Global vectors for word representation. In EMNLP.
Qian, Q.; Huang, M.; and Zhu, X. 2017. Linguistically regularized
lstms for sentiment classiﬁcation. In ACL.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention
model for abstractive sentence summarization. In EMNLP.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2017. Bidi-
rectional attention ﬂow for machine comprehension. In ICLR.
Shang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine for
short-text conversation. In ACL.
Socher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; Potts, C.; et al. 2013. Recursive deep models for seman-
tic compositionality over a sentiment treebank. In EMNLP.
Socher, R.; Karpathy, A.; Le, Q. V.; Manning, C. D.; and Ng, A. Y.
2014. Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. Transactions of the Association for
Computational Linguistics 2:207–218.
Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural
networks from overﬁtting. Journal of Machine Learning Research
15(1):1929–1958.
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end
memory networks. In NIPS.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved se-
mantic representations from tree-structured long short-term mem-
ory networks. In ACL.
Teng, Z.; Vo, D.-T.; and Zhang, Y. 2016. Context-sensitive lexicon
features for neural sentiment analysis. In EMNLP.
Vaswani, A.; Shazeer; Noam; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all
you need. In NIPS.
Vendrov, I.; Kiros, R.; Fidler, S.; and Urtasun, R. 2016. Order-
embeddings of images and language. In ICLR.
Wiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating expressions
of opinions and emotions in language. Language resources and
evaluation 39(2):165–210.
Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A broad-
coverage challenge corpus for sentence understanding through in-
ference. arXiv preprint arXiv:1704.05426.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method.
arXiv preprint arXiv:1212.5701.
Zhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hierarchical
sentence model. In IJCAI.
Zhao, J.; Zhu, T.; and Lan, M. 2014. Ecnu: One stone two birds:
Ensemble of heterogenous measures for semantic relatedness and
textual entailment. In SemEval@ COLING, 271–277.

DiSAN: Directional Self-Attention Network for
RNN/CNN-Free Language Understanding
Tianyi Zhou‡
Shirui Pan†

Tao Shen†
Jing Jiang†

Guodong Long†
Chengqi Zhang†

†Centre of Artiﬁcial Intelligence, FEIT, University of Technology Sydney
‡Paul G. Allen School of Computer Science & Engineering, University of Washington
tao.shen@student.uts.edu.au, tianyizh@uw.edu
{guodong.long, jing.jiang, shirui.pan, chengqi.zhang}@uts.edu.au

7
1
0
2
 
v
o
N
 
0
2
 
 
]
L
C
.
s
c
[
 
 
3
v
6
9
6
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recurrent neural nets (RNN) and convolutional neural nets
(CNN) are widely used on NLP tasks to capture the long-term
and local dependencies, respectively. Attention mechanisms
have recently attracted enormous interest due to their highly
parallelizable computation, signiﬁcantly less training time,
and ﬂexibility in modeling dependencies. We propose a novel
attention mechanism in which the attention between elements
from input sequence(s) is directional and multi-dimensional
(i.e., feature-wise). A light-weight neural net, “Directional
Self-Attention Network (DiSAN)”, is then proposed to learn
sentence embedding, based solely on the proposed attention
without any RNN/CNN structure. DiSAN is only composed
of a directional self-attention with temporal order encoded,
followed by a multi-dimensional attention that compresses
the sequence into a vector representation. Despite its simple
form, DiSAN outperforms complicated RNN models on both
prediction quality and time efﬁciency. It achieves the best
test accuracy among all sentence encoding methods and im-
proves the most recent best result by 1.02% on the Stanford
Natural Language Inference (SNLI) dataset, and shows state-
of-the-art test accuracy on the Stanford Sentiment Treebank
(SST), Multi-Genre natural language inference (MultiNLI),
Sentences Involving Compositional Knowledge (SICK), Cus-
tomer Review, MPQA, TREC question-type classiﬁcation
and Subjectivity (SUBJ) datasets.

1

Introduction

Context dependency plays a signiﬁcant role in language
understanding and provides critical information to natural
language processing (NLP) tasks. For different tasks and
data, researchers often switch between two types of deep
neural network (DNN): recurrent neural network (RNN)
with sequential architecture capturing long-range dependen-
cies (e.g., long short-term memory (LSTM) (Hochreiter and
Schmidhuber 1997) and gated recurrent unit (GRU) (Chung
et al. 2014)), and convolutional neural network (CNN) (Kim
2014) whose hierarchical structure is good at extracting lo-
cal or position-invariant features. However, which network
to choose in practice is an open question, and the choice re-
lies largely on the empirical knowledge.

Recent works have found that equipping RNN or CNN
with an attention mechanism can achieve state-of-the-art

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

performance on a large number of NLP tasks, including neu-
ral machine translation (Bahdanau, Cho, and Bengio 2015;
Luong, Pham, and Manning 2015), natural language infer-
ence (Liu et al. 2016), conversation generation (Shang, Lu,
and Li 2015), question answering (Hermann et al. 2015;
Sukhbaatar et al. 2015), machine reading comprehension
(Seo et al. 2017), and sentiment analysis (Kokkinos and
Potamianos 2017). The attention uses a hidden layer to com-
pute a categorical distribution over elements from the in-
put sequence to reﬂect their importance weights. It allows
RNN/CNN to maintain a variable-length memory, so that
elements from the input sequence can be selected by their
importance/relevance and merged into the output. In con-
trast to RNN and CNN, the attention mechanism is trained
to capture the dependencies that make signiﬁcant contribu-
tions to the task, regardless of the distance between the el-
ements in the sequence. It can thus provide complementary
information to the distance-aware dependencies modeled by
RNN/CNN. In addition, computing attention only requires
matrix multiplication, which is highly parallelizable com-
pared to the sequential computation of RNN.

In a very recent work (Vaswani et al. 2017), an atten-
tion mechanism is solely used to construct a sequence to
sequence (seq2seq) model that achieves a state-of-the-art
quality score on the neural machine translation (NMT) task.
The seq2seq model, “Transformer”, has an encoder-decoder
structure that is only composed of stacked attention net-
works, without using either recurrence or convolution. The
proposed attention, “multi-head attention”, projects the in-
put sequence to multiple subspaces, then applies scaled dot-
product attention to its representation in each subspace,
and lastly concatenates their output. By doing this, it can
combine different attentions from multiple subspaces. This
mechanism is used in Transformer to compute both the
context-aware features inside the encoder/decoder and the
bottleneck features between them.

The attention mechanism has more ﬂexibility in sequence
length than RNN/CNN, and is more task/data-driven when
modeling dependencies. Unlike sequential models, its com-
putation can be easily and signiﬁcantly accelerated by exist-
ing distributed/parallel computing schemes. However, to the
best of our knowledge, a neural net entirely based on atten-
tion has not been designed for other NLP tasks except NMT,
especially those that cannot be cast into a seq2seq problem.

Compared to RNN, a disadvantage of most attention mech-
anisms is that the temporal order information is lost, which
however might be important to the task. This explains why
positional encoding is applied to the sequence before being
processed by the attention in Transformer. How to model or-
der information within an attention is still an open problem.
The goal of this paper is to develop a uniﬁed and
RNN/CNN-free attention network that can be generally uti-
lized to learn the sentence encoding model for different NLP
tasks, such as natural language inference, sentiment analy-
sis, sentence classiﬁcation and semantic relatedness. We fo-
cus on the sentence encoding model because it is a basic
module of most DNNs used in the NLP literature.

We propose a novel attention mechanism that differs from
previous ones in that it is 1) multi-dimensional: the atten-
tion w.r.t. each pair of elements from the source(s) is a vec-
tor, where each entry is the attention computed on each fea-
ture; and 2) directional: it uses one or multiple positional
masks to model the asymmetric attention between two el-
ements. We compute feature-wise attention since each ele-
ment in a sequence is usually represented by a vector, e.g.,
word/character embedding (Kim et al. 2016), and attention
on different features can contain different information about
dependency, thus to handle the variation of contexts around
the same word. We apply positional masks to attention dis-
tribution since they can easily encode prior structure knowl-
edge such as temporal order and dependency parsing. This
design mitigates the weakness of attention in modeling order
information, and takes full advantage of parallel computing.
We then build a light-weight and RNN/CNN-free neural
network, “Directional Self-Attention Network (DiSAN)”,
for sentence encoding. This network relies entirely on the
proposed attentions and does not use any RNN/CNN struc-
ture. In DiSAN, the input sequence is processed by direc-
tional (forward and backward) self-attentions to model con-
text dependency and produce context-aware representations
for all tokens. Then, a multi-dimensional attention computes
a vector representation of the entire sequence, which can
be passed into a classiﬁcation/regression module to com-
pute the ﬁnal prediction for a particular task. Unlike Trans-
former, neither stacking of attention blocks nor an encoder-
decoder structure is required. The simple architecture of
DiSAN leads to fewer parameters, less computation and eas-
ier parallelization.

In experiments1, we compare DiSAN with the currently
popular methods on various NLP tasks, e.g., natural lan-
guage inference, sentiment analysis, sentence classiﬁcation,
etc. DiSAN achieves the highest test accuracy on the Stan-
ford Natural Language Inference (SNLI) dataset among
sentence-encoding models and improves the currently best
result by 1.02%. It also shows the state-of-the-art perfor-
mance on the Stanford Sentiment Treebank (SST), Multi-
Genre natural language inference (MultiNLI), SICK, Cus-
tomer Review, MPQA, SUBJ and TREC question-type clas-
siﬁcation datasets. Meanwhile, it has fewer parameters and
exhibits much higher computation efﬁciency than the mod-

1Codes and pre-trained models for experiments can be found at

https://github.com/taoshen58/DiSAN

els it outperforms, e.g., LSTM and tree-based models.

Annotation: 1) Lowercase denotes a vector; 2) bold low-
ercase denotes a sequence of vectors (stored as a matrix);
and 3) uppercase denotes a matrix or a tensor.

2 Background

2.1 Sentence Encoding
In the pipeline of NLP tasks, a sentence is denoted by a se-
quence of discrete tokens (e.g., words or characters) v =
[v1, v2, . . . , vn], where vi could be a one-hot vector whose
dimension length equals the number of distinct tokens N .
A pre-trained token embedding (e.g., word2vec (Mikolov
et al. 2013b) or GloVe (Pennington, Socher, and Manning
2014)) is applied to v and transforms all discrete tokens to
a sequence of low-dimensional dense vector representations
x = [x1, x2, . . . , xn] with xi ∈ Rde. This pre-process can
be written as x = W (e)v, where word embedding weight
matrix W (e) ∈ Rde×N and x ∈ Rde×n.

Most DNN sentence-encoding models for NLP tasks take
x as the input and further generate a vector representation ui
for each xi by context fusion. Then a sentence encoding is
obtained by mapping the sequence u = [u1, u2, . . . , un] to
a single vector s ∈ Rd, which is used as a compact encoding
of the entire sentence in NLP problems.

2.2 Attention
The attention is proposed to compute an alignment score be-
tween elements from two sources. In particular, given the to-
ken embeddings of a source sequence x = [x1, x2, . . . , xn]
and the vector representation of a query q, attention com-
putes the alignment score between xi and q by a compati-
bility function f (xi, q), which measures the dependency be-
tween xi and q, or the attention of q to xi. A softmax func-
tion then transforms the scores [f (xi, q)]n
i=1 to a probability
distribution p(z|x, q) by normalizing over all the n tokens
of x. Here z is an indicator of which token in x is important
to q on a speciﬁc task. That is, large p(z = i|x, q) means xi
contributes important information to q. The above process
can be summarized by the following equations.
a = [f (xi, q)]n
p(z|x, q) = softmax(a).

(1)
(2)

i=1 ,

Speciﬁcally,

p(z = i|x, q) =

exp(f (xi, q))
i=1 exp(f (xi, q))

.

(cid:80)n

(3)

The output of this attention mechanism is a weighted sum
of the embeddings for all tokens in x, where the weights
are given by p(z|x, q). It places large weights on the tokens
important to q, and can be written as the expectation of a
token sampled according to its importance, i.e.,

s =

p(z = i|x, q)xi = Ei∼p(z|x,q)(xi),

(4)

n
(cid:88)

i=1

where s ∈ Rde can be used as the sentence encoding of x.

Additive attention (or multi-layer perceptron attention)
(Bahdanau, Cho, and Bengio 2015; Shang, Lu, and Li

3.1 Multi-dimensional Attention
Multi-dimensional attention is a natural extension of addi-
tive attention (or MLP attention) at the feature level. Instead
of computing a single scalar score f (xi, q) for each token xi
as shown in Eq.(5), multi-dimensional attention computes a
feature-wise score vector for xi by replacing weight vector
w in Eq.(5) with a matrix W , i.e.,

f (xi, q) = W T σ

W (1)xi + W (2)q

(7)

(cid:17)

,

(cid:16)

where f (xi, q) ∈ Rde is a vector with the same length as xi,
and all the weight matrices W, W (1), W (2) ∈ Rde×de. We
further add two bias terms to the parts in and out activation
σ(·), i.e.,

f (xi, q) = W T σ

(cid:16)

W (1)xi + W (2)q + b(1)(cid:17)

+ b.

(8)

We then compute a categorical distribution p(zk|x, q) over
all the n tokens for each feature k ∈ [de]. A large p(zk =
i|x, q) means that feature k of token i is important to q.

We apply the same procedure Eq.(1)-(3) in traditional at-
tention to the kth dimension of f (xi, q). In particular, for
each feature k ∈ [de], we replace f (xi, q) with [f (xi, q)]k,
and change z to zk in Eq.(1)-(3). Now each feature k in each
token i has an importance weight Pki (cid:44) p(zk = i|x, q). The
output s can be written as
(cid:105)de

(cid:104)(cid:88)n

= (cid:2)Ei∼p(zk|x,q)(xki)(cid:3)de

k=1 .

(9)

Pkixki

s =

i=1

k=1

We give an illustration of traditional attention and multi-
dimensional attention in Figure 1. In the rest of this paper,
we will ignore the subscript k which indexes feature dimen-
sion for simpliﬁcation if no confusion is possible. Hence,
the output s can be written as an element-wise product
s = (cid:80)n

i=1 P·i (cid:12) xi

Remark: The word embedding usually suffers from the
polysemy in natural language. Since traditional attention
computes a single importance score for each word based on
the word embedding, it cannot distinguish the meanings of
the same word in different contexts. Multi-dimensional at-
tention, however, computes a score for each feature of each
word, so it can select the features that can best describe the
word’s speciﬁc meaning in any given context, and include
this information in the sentence encoding output s.

3.2 Two types of Multi-dimensional Self-attention
When extending multi-dimension to self-attentions, we have
two variants of multi-dimensional attention. The ﬁrst one,
called multi-dimensional “token2token” self-attention, ex-
plores the dependency between xi and xj from the same
source x, and generates context-aware coding for each el-
ement. It replaces q with xj in Eq.(8), i.e.,
(cid:16)

W (1)xi + W (2)xj + b(1)(cid:17)

f (xi, xj) = W T σ

+ b.

(10)

Similar to P in vanilla multi-dimensional attention, we com-
pute a probability matrix P j ∈ Rde×n for each xj such that
P j
ki

(cid:44) p(zk = i|x, xj). The output for xj is

sj =

P j

·i (cid:12) xi

n
(cid:88)

i=1

(11)

(a)

(b)

Figure 1: (a) Traditional (additive/multiplicative) attention
and (b) multi-dimensional attention. zi denotes alignment
score f (xi, q), which is a scalar in (a) but a vector in (b).

2015) and multiplicative attention (or dot-product attention)
(Vaswani et al. 2017; Sukhbaatar et al. 2015; Rush, Chopra,
and Weston 2015) are the two most commonly used atten-
tion mechanisms. They share the same and uniﬁed form of
attention introduced above, but are different in the compati-
bility function f (xi, q). Additive attention is associated with

f (xi, q) = wT σ(W (1)xi + W (2)q),
(5)
where σ(·) is an activation function and w ∈ Rde is a weight
vector. Multiplicative attention uses inner product or cosine
similarity for f (xi, q), i.e.,
(cid:68)
W (1)xi, W (2)q

f (xi, q) =

(6)

(cid:69)

.

In practice, additive attention often outperforms multiplica-
tive one in prediction quality, but the latter is faster and more
memory-efﬁcient due to optimized matrix multiplication.

2.3 Self-Attention
Self-Attention is a special case of the attention mechanism
introduced above. It replaces q with a token embedding xj
from the source input itself. It relates elements at different
positions from a single sequence by computing the attention
between each pair of tokens, xi and xj. It is very expres-
sive and ﬂexible for both long-range and local dependen-
cies, which used to be respectively modeled by RNN and
CNN. Moreover, it has much faster computation speed and
fewer parameters than RNN. In recent works, we have al-
ready witnessed its success across a variety of NLP tasks,
such as reading comprehension (Hu, Peng, and Qiu 2017)
and neural machine translation (Vaswani et al. 2017).

3 Two Proposed Attention Mechanisms
In this section, we introduce two novel attention mecha-
nisms, multi-dimensional attention in Section 3.1 (with two
extensions to self-attention in Section 3.2) and directional
self-attention in Section 3.3. They are the main components
of DiSAN and may be of independent interest to other neural
nets for other NLP problems in which an attention is needed.

In DiSA, we ﬁrst transform the input sequence x =
to a sequence of hidden state h =

[x1, x2, . . . , xn]
[h1, h2, . . . , hn] by a fully connected layer, i.e.,

h = σh

(cid:16)

W (h)x + b(h)(cid:17)

,

(14)

where x ∈ Rde×n, h ∈ Rdh×n, W (h) and b(h) are the learn-
able parameters, and σh(·) is an activation function.

We then apply multi-dimensional

token2token self-
attention to h, and generate context-aware vector represen-
tations s for all elements from the input sequence. We make
two modiﬁcations to Eq.(10) to reduce the number of param-
eters and make the attention directional.

First, we set W in Eq.(10) to a scalar c and divide the part
in σ(·) by c, and we use tanh(·) for σ(·), which reduces the
number of parameters. In experiments, we always set c = 5,
and obtain stable output.

Second, we apply a positional mask to Eq.(10), so the at-
tention between two elements can be asymmetric. Given a
mask M ∈ {0, −∞}n×n, we set bias b to a constant vec-
tor Mij1 in Eq.(10), where 1 is an all-one vector. Hence,
Eq.(10) is modiﬁed to

f (hi, hj) =

c · tanh

(cid:16)

(cid:17)
[W (1)hi + W (2)hj + b(1)]/c

+ Mij1.

(15)

To see why a mask can encode directional information, let
us consider a case in which Mij = −∞ and Mji = 0, which
results in [f (hi, hj)]k = −∞ and unchanged [f (hj, hi)]k.
Since the probability p(zk = i|x, xj) is computed by
softmax, [f (hi, hj)]k = −∞ leads to p(zk = i|x, xj) = 0.
This means that there is no attention of xj to xi on feature k.
On the contrary, we have p(zk = j|x, xi) > 0, which means
that attention of xi to xj exists on feature k. Therefore, prior
structure knowledge such as temporal order and dependency
parsing can be easily encoded by the mask, and explored in
generating sentence encoding. This is an important feature
of DiSA that previous attention mechanisms do not have.

For self-attention, we usually need to disable the attention
of each token to itself (Hu, Peng, and Qiu 2017). This is the
same as applying a diagonal-disabled (i.e., diag-disabled)
mask such that

M diag

ij =

(cid:26) 0,
−∞,

i (cid:54)= j
i = j

Moreover, we can use masks to encode temporal order
information into attention output. In this paper, we use two
masks, i.e., forward mask M f w and backward mask M bw,

M f w

ij =

M bw

ij =

(cid:26) 0,
−∞,

(cid:26) 0,
−∞,

i < j
otherwise

i > j
otherwise

(16)

(17)

(18)

In forward mask M f w, there is the only attention of later
token j to early token i, and vice versa in backward mask.
We show these three positional masks in Figure 3.

Given input sequence x and a mask M , we compute
f (xi, xj) according to Eq.(15), and follow the standard pro-
cedure of multi-dimensional token2token self-attention to

Figure 2: Directional self-attention (DiSA) mechanism.
Here, we use li,j to denote f (hi, hj) in Eq. (15).

The output of token2token self-attention for all elements
from x is s = [s1, s2, . . . , sn] ∈ Rde×n.

The second one, multi-dimensional “source2token” self-
attention, explores the dependency between xi and the entire
sequence x, and compresses the sequence x into a vector. It
removes q from Eq.(8), i.e.,

f (xi) = W T σ

(cid:16)

W (1)xi + b(1)(cid:17)

+ b.

(12)

The probability matrix is deﬁned as Pki (cid:44) p(zk = i|x)
and is computed in the same way as P in vanilla multi-
dimensional attention. The output s is also same, i.e.,

s =

P·i (cid:12) xi

(13)

n
(cid:88)

i=1

We will use these two types (i.e.,

token2token and
source2token) of multi-dimensional self-attention in differ-
ent parts of our sentence encoding model, DiSAN.

3.3 Directional Self-Attention

Directional self-attention (DiSA) is composed of a fully
connected layer whose input is the token embeddings x,
a “masked” multi-dimensional token2token self-attention
block to explore the dependency and temporal order, and a
fusion gate to combine the output and input of the attention
block. Its structure is shown in Figure 2. It can be used as
either a neural net or a module to compose a large network.

(a) Diag-disabled mask

(b) Forward mask

(c) Backward mask

Figure 3: Three positional masks: (a) is the diag-disabled
mask M diag; (b) and (c) are forward mask M f w and back-
ward mask M bw, respectively.

(19)

(20)

compute the probability matrix P j for each j ∈ [n]. Each
output sj in s is computed as in Eq.(11).

The ﬁnal output u ∈ Rdh×n of DiSA is obtained by com-
bining the output s and the input h of the masked multi-
dimensional token2token self-attention block. This yields a
temporal order encoded and context-aware vector represen-
tation for each element/token. The combination is accom-
plished by a dimension-wise fusion gate, i.e.,

F = sigmoid

(cid:16)

W (f 1)s + W (f 2)h + b(f )(cid:17)

u = F (cid:12) h + (1 − F ) (cid:12) s

where W (f 1), W (f 2) ∈ Rdh×dh and b(f ) ∈ Rdh are the
learnable parameters of the fusion gate.

4 Directional Self-Attention Network
We propose a light-weight network, “Directional Self-
Attention Network (DiSAN)”, for sentence encoding. Its ar-
chitecture is shown in Figure 4.

Given an input sequence of token embedding x, DiSAN
ﬁrstly applies two parameter-untied DiSA blocks with for-
ward mask M f w Eq.(17) and M bw Eq.(18), respectively.
The feed-forward procedure is given in Eq.(14)-(15) and
Eq.(19)-(20). Their outputs are denoted by uf w, ubw ∈
Rdh×n. We concatenate them vertically as [uf w; ubw] ∈
R2dh×n, and use this concatenated output as input to a multi-
dimensional source2token self-attention block, whose out-
put sdisan ∈ R2dh computed by Eq.(12)-(13) is the ﬁnal
sentence encoding result of DiSAN.

Remark: In DiSAN, forward/backward DiSA blocks
work as context fusion layers. And the multi-dimensional
source2token self-attention compresses the sequence into a

Figure 4: Directional self-attention network (DiSAN)

single vector. The idea of using both forward and backward
attentions is inspired by Bi-directional LSTM (Bi-LSTM)
(Graves, Jaitly, and Mohamed 2013), in which forward and
backward LSTMs are used to encode long-range depen-
dency from different directions. In Bi-LSTM, LSTM com-
bines the context-aware output with the input by multi-gate.
The fusion gate used in DiSA shares the similar motivation.
However, DiSAN has fewer parameters, simpler structure
and better efﬁciency.

5 Experiments
In this section, we ﬁrst apply DiSAN to natural language
inference and sentiment analysis tasks. DiSAN achieves
the state-of-the-art performance and signiﬁcantly better ef-
ﬁciency than other baseline methods on benchmark datasets
for both tasks. We also conduct experiments on other NLP
tasks and DiSAN also achieves state-of-the-art performance.
Training Setup: We use cross-entropy loss plus L2 regu-
larization penalty as optimization objective. We minimize
it by Adadelta (Zeiler 2012) (an optimizer of mini-batch
SGD) with batch size of 64. We use Adadelta rather than
Adam (Kingma and Ba 2015) because in our experiments,
DiSAN optimized by Adadelta can achieve more stable per-
formance than Adam optimized one. Initial learning rate is
set to 0.5. All weight matrices are initialized by Glorot Ini-
tialization (Glorot and Bengio 2010), and the biases are ini-
tialized with 0. We initialize the word embedding in x by
300D GloVe 6B pre-trained vectors (Pennington, Socher,
and Manning 2014). The Out-of-Vocabulary words in train-
ing set are randomly initialized by uniform distribution be-
tween (−0.05, 0.05). The word embeddings are ﬁne-tuned
during the training phrase. We use Dropout (Srivastava et
al. 2014) with keep probability 0.75 for language inference
and 0.8 for sentiment analysis. The L2 regularization de-
cay factors γ are 5 × 10−5 and 10−4 for language inference
and sentiment analysis, respectively. Note that the dropout
keep probability and γ varies with the scale of correspond-
ing dataset. Hidden units number dh is set to 300. Activa-
tion functions σ(·) are ELU (exponential linear unit) (Clev-
ert, Unterthiner, and Hochreiter 2016) if not speciﬁed. All
models are implemented with TensorFlow2 and run on sin-

2https://www.tensorﬂow.org

Model Name

|θ|

T(s)/epoch Train Accu(%) Test Accu(%)

Unlexicalized features (Bowman et al. 2015)
+ Unigram and bigram features (Bowman et al. 2015)

100D LSTM encoders (Bowman et al. 2015)
300D LSTM encoders (Bowman et al. 2016)
1024D GRU encoders (Vendrov et al. 2016)
300D Tree-based CNN encoders (Mou et al. 2016)
300D SPINN-PI encoders (Bowman et al. 2016)
600D Bi-LSTM encoders (Liu et al. 2016)
300D NTI-SLSTM-LSTM encoders (Munkhdalai and Yu 2017b)
600D Bi-LSTM encoders+intra-attention (Liu et al. 2016)
300D NSE encoders (Munkhdalai and Yu 2017a)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

Directional self-attention network (DiSAN)

0.2m
3.0m
15m
3.5m
3.7m
2.0m
4.0m
2.8m
3.0m

0.45m
0.54m
1.98m
2.88m
2.35m

2.35m

216
261
345
2080
592

587

49.4
99.7

84.8
83.9
98.8
83.3
89.2
86.4
82.5
84.5
86.2

82.39
86.22
89.58
90.39
90.18

91.08

50.4
78.2

77.6
80.6
81.4
82.1
83.2
83.3
83.4
84.2
84.6

79.81
83.12
84.17
84.98
84.66

85.62

Table 1: Experimental results for different methods on SNLI. |θ|: the number of parameters (excluding word embedding part).
T(s)/epoch: average time (second) per epoch. Train Accu(%) and Test Accu(%): the accuracy on training and test set.

gle Nvidia GTX 1080Ti graphic card.

5.1 Natural Language Inference
The goal of Natural Language Inference (NLI) is to rea-
son the semantic relationship between a premise sentence
and a corresponding hypothesis sentence. The possible rela-
tionship could be entailment, neutral or contradiction. We
compare different models on a widely used benchmark,
Stanford Natural Language Inference (SNLI)3 (Bowman et
al. 2015) dataset, which consists of 549,367/9,842/9,824
(train/dev/test) premise-hypothesis pairs with labels.

Following the standard procedure in Bowman et al.
(2016), we launch two sentence encoding models (e.g.,
DiSAN) with tied parameters for the premise sentence and
hypothesis sentence, respectively. Given the output encoding
sp for the premise and sh for the hypothesis, the represen-
tation of relationship is the concatenation of sp, sh, sp − sh
and sp (cid:12) sh, which is fed into a 300D fully connected layer
and then a 3-unit output layer with softmax to compute a
probability distribution over the three types of relationship.
For thorough comparison, besides the neural nets pro-
posed in previous works of NLI, we implement ﬁve extra
neural net baselines to compare with DiSAN. They help
us to analyze the improvement contributed by each part of
DiSAN and to verify that the two attention mechanisms pro-
posed in Section 3 can improve other networks.

• Word Embedding with additive attention.

• Word Embedding with s2t self-attention: DiSAN with

DiSA blocks removed.

• Multi-head with s2t self-attention: Multi-head attention
(Vaswani et al. 2017) (8 heads, each has 75 hidden units)
with source2token self-attention. The positional encoding

3https://nlp.stanford.edu/projects/snli/

method used in Vaswani et al. (2017) is applied to the in-
put sequence to encode temporal information. We ﬁnd our
experiments show that multi-head attention is sensitive to
hyperparameters, so we adjust keep probability of dropout
from 0.7 to 0.9 with step 0.05 and report the best result.
• Bi-LSTM with s2t self-attention: a multi-dimensional
source2token self-attention block is applied to the output
of Bi-LSTM (300D forward + 300D backward LSTMs).
• DiSAN without directions: DiSAN with the for-
ward/backward masks M f w and M bw replaced with two
diag-disabled masks M diag, i.e., DiSAN without for-
ward/backward order information.
Compared to the results from the ofﬁcial leaderboard
of SNLI in Table 1, DiSAN outperforms previous works
and improves the best latest test accuracy (achieved by a
memory-based NSE encoder network) by a remarkable mar-
gin of 1.02%. DiSAN surpasses the RNN/CNN based mod-
els with more complicated architecture and more parameters
by large margins, e.g., +2.32% to Bi-LSTM, +1.42% to Bi-
LSTM with additive attention. It even outperforms models
with the assistance of a semantic parsing tree, e.g., +3.52%
to Tree-based CNN, +2.42% to SPINN-PI.

In the results of the ﬁve baseline methods and DiSAN at
the bottom of Table 1, we demonstrate that making attention
multi-dimensional (feature-wise) or directional brings sub-
stantial improvement to different neural nets. First, a com-
parison between the ﬁrst two models shows that changing
token-wise attention to multi-dimensional/feature-wise at-
tention leads to 3.31% improvement on a word embedding
based model. Also, a comparison between the third baseline
and DiSAN shows that DiSAN can substantially outperform
multi-head attention by 1.45%. Moreover, a comparison be-
tween the forth baseline and DiSAN shows that the DiSA
block can even outperform Bi-LSTM layer in context en-
coding, improving test accuracy by 0.64%. A comparison

between the ﬁfth baseline and DiSAN shows that directional
self-attention with forward and backward masks (with tem-
poral order encoded) can bring 0.96% improvement.

Additional advantages of DiSAN shown in Table 1 are
its fewer parameters and compelling time efﬁciency. It is
×3 faster than widely used Bi-LSTM model. Compared to
other models with competitive performance, e.g., 600D Bi-
LSTM encoders with intra-attention (2.8M), 300D NSE en-
coders (3.0M) and 600D Bi-LSTM encoders with multi-
dimensional attention (2.88M), DiSAN only has 2.35M pa-
rameters.

5.2 Sentiment Analysis

Model

Test Accu

MV-RNN (Socher et al. 2013)
RNTN (Socher et al. 2013)
Bi-LSTM (Li et al. 2015)
Tree-LSTM (Tai, Socher, and Manning 2015)
CNN-non-static (Kim 2014)
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015)
NCSL (Teng, Vo, and Zhang 2016)
LR-Bi-LSTM (Qian, Huang, and Zhu 2017)

Word Embedding with additive attention
Word Embedding with s2t self-attention
Multi-head with s2t self-attention
Bi-LSTM with s2t self-attention
DiSAN without directions

DiSAN

44.4
45.7
49.8
51.0
48.0
51.2
51.1
50.6

47.47
48.87
49.14
49.95
49.41

51.72

Table 2: Test accuracy of ﬁne-grained sentiment analysis on
Stanford Sentiment Treebank (SST) dataset.

Sentiment analysis aims to analyze the sentiment of a
sentence or a paragraph, e.g., a movie or a product re-
view. We use Stanford Sentiment Treebank (SST)4 (Socher
et al. 2013) for the experiments, and only focus on the
ﬁne-grained movie review sentiment classiﬁcation over ﬁve
classes, i.e., very negative, negative, neutral, positive and
very positive. We use the standard train/dev/test sets split
with 8,544/1,101/2,210 samples. Similar to Section 5.1, we
employ a single sentence encoding model to obtain a sen-
tence representation s of a movie review, then pass it into
a 300D fully connected layer. Finally, a 5-unit output layer
with softmax is used to calculate a probability distribution
over the ﬁve classes.

In Table 2, we compare previous works with DiSAN on
test accuracy. To the best of our knowledge, DiSAN im-
proves the last best accuracy (given by CNN-Tensor) by
0.52%. Compared to tree-based models with heavy use of
the prior structure, e.g., MV-RNN, RNTN and Tree-LSTM,
DiSAN outperforms them by 7.32%, 6.02% and 0.72%,
respectively. Additionally, DiSAN achieves better perfor-
mance than CNN-based models. More recent works tend
to focus on lexicon-based sentiment analysis, by explor-
ing sentiment lexicons, negation words and intensity words.

4https://nlp.stanford.edu/sentiment/

Nonetheless, DiSAN still outperforms these fancy models,
such as NCSL (+0.62%) and LR-Bi-LSTM (+1.12%).

Figure 5: Fine-grained sentiment analysis accuracy vs. sen-
tence length. The results of LSTM, Bi-LSTM and Tree-
LSTM are from Tai, Socher, and Manning (2015) and the
result of DiSAN is the average over ﬁve random trials.

It is also interesting to see the performance of different
models on the sentences with different lengths. In Figure 5,
we compare LSTM, Bi-LSTM, Tree-LSTM and DiSAN on
different sentence lengths. In the range of (5, 12), the length
range for most movie review sentences, DiSAN signiﬁcantly
outperforms others. Meanwhile, DiSAN also shows impres-
sive performance for slightly longer sentences or paragraphs
in the range of (25, 38). DiSAN performs poorly when the
sentence length ≥ 38, in which however only 3.21% of total
movie review sentences lie.

5.3 Experiments on Other NLP Tasks
Multi-Genre Natural Language Inference Multi-Genre
Natural Language Inference (MultiNLI)5 (Williams, Nan-
gia, and Bowman 2017) dataset consists of 433k sentence
pairs annotated with textual entailment information. This
dataset is similar to SNLI, but it covers more genres of spo-
ken and written text, and supports a distinctive cross-genre
generalization evaluation. However, MultiNLI is a quite new
dataset, and its leaderboard does not include a session for
the sentence-encoding only model. Hence, we only compare
DiSAN with the baselines provided at the ofﬁcial website.
The results of DiSAN and two sentence-encoding models
on the leaderboard are shown in Table 3. Note that the pre-
diction accuracies of Matched and Mismatched test datasets
are obtained by submitting our test results to Kaggle open
evaluation platforms6: MultiNLI Matched Open Evaluation
and MultiNLI Mismatched Open Evaluation.

Semantic Relatedness The task of semantic relatedness
aims to predict a similarity degree of a given pair of sen-
tences. We show an experimental comparison of different

5https://www.nyu.edu/projects/bowman/multinli/
6https://inclass.kaggle.com/c/multinli-matched-open-

evaluation and https://inclass.kaggle.com/c/multinli-mismatched-
open-evaluation

Method Matched Mismatched

0.65200
cBoW
Bi-LSTM 0.67507

DiSAN

0.70977

0.64759
0.67248

0.71402

Table 3: Experimental results of prediction accuracy for dif-
ferent methods on MultiNLI.

Model
cBoWa
Skip-thoughtb
DCNNc
AdaSentd
SRUe
Wide CNNse

CR

79.9
81.3
/

MPQA

SUBJ

TREC

86.4
87.5
/

91.3
93.6
/

87.3
92.2
93.0

83.6 (1.6) 90.4 (0.7) 92.2 (1.2) 91.1 (1.0)
84.8 (1.3) 89.7 (1.1) 93.4 (0.8) 93.9 (0.6)
82.2 (2.2) 88.8 (1.2) 92.9 (0.7) 93.2 (0.5)

DiSAN

84.8 (2.0) 90.1 (0.4) 94.2 (0.6) 94.2 (0.1)

methods on Sentences Involving Compositional Knowledge
(SICK)7 dataset (Marelli et al. 2014). SICK is composed
of 9,927 sentence pairs with 4,500/500/4,927 instances for
train/dev/test. The regression module on the top of DiSAN is
introduced by Tai, Socher, and Manning (2015). The results
in Table 4 show that DiSAN outperforms the models from
previous works in terms of Pearson’s r and Spearman’s ρ
indexes.

Table 5: Experimental results for different methods on vari-
ous sentence classiﬁcation benchmarks. The reported accu-
racies on CR, MPQA and SUBJ are the mean of 10-fold
cross validation, the accuracies on TREC are the mean of
dev accuracies of ﬁve runs. All standard deviations are in
parentheses. a(Mikolov et al. 2013a), b(Kiros et al. 2015),
c(Kalchbrenner, Grefenstette, and Blunsom 2014), d(Zhao,
Lu, and Poupart 2015), e(Lei and Zhang 2017).

Pearson’s r Spearman’s ρ MSE

Model
Meaning Factorya
.7721
ECNUb
/
DT-RNNc
.7923 (.0070) .7319 (.0071) .3822 (.0137)
SDT-RNNc
.7900 (.0042) .7304 (.0042) .3848 (.0042)
Cons. Tree-LSTMd .8582 (.0038) .7966 (.0053) .2734 (.0108)
Dep. Tree-LSTMd .8676 (.0030) .8083 (.0042) .2532 (.0052)

.8268
.8414

.3224
/

DiSAN

.8695 (.0012) .8139 (.0012) .2879 (.0036)

Table 4: Experimental results for different methods on SICK
sentence relatedness dataset. The reported accuracies are the
mean of ﬁve runs (standard deviations in parentheses). Cons.
and Dep. represent Constituency and Dependency, respec-
tively. a(Bjerva et al. 2014), b(Zhao, Zhu, and Lan 2014),
c(Socher et al. 2014), d(Tai, Socher, and Manning 2015).

Sentence Classiﬁcations The goal of sentence classiﬁca-
tion is to correctly predict the class label of a given sentence
in various scenarios. We evaluate the models on four sen-
tence classiﬁcation benchmarks of various NLP tasks, such
as sentiment analysis and question-type classiﬁcation. They
are listed as follows. 1) CR: Customer review (Hu and Liu
2004) of various products (cameras, etc.), which is to pre-
dict whether the review is positive or negative; 2) MPQA:
Opinion polarity detection subtask of the MPQA dataset
(Wiebe, Wilson, and Cardie 2005); 3) SUBJ: Subjectivity
dataset (Pang and Lee 2004) whose labels indicate whether
each sentence is subjective or objective; 4) TREC: TREC
question-type classiﬁcation dataset (Li and Roth 2002). The
experimental results of DiSAN and existing methods are
shown in Table 5.

5.4 Case Study
To gain a closer view of what dependencies in a sentence
can be captured by DiSAN, we visualize the attention prob-
ability p(z = i|x, xj) or alignment score by heatmaps. In

7http://clic.cimec.unitn.it/composes/sick.html

particular, we will focus primarily on the probability in for-
ward/backward DiSA blocks (Figure 6), forward/backward
fusion gates F in Eq.(19) (Figure 7), and the probability
in multi-dimensional source2token self-attention block (Fig-
ure 8). For the ﬁrst two, we desire to demonstrate the de-
pendency at token level, but attention probability in DiSAN
is deﬁned on each feature, so we average the probabilities
along the feature dimension.

We select two sentences from SNLI test set as examples
for this case study. Sentence 1 is Families have some dogs
in front of a carousel and sentence 2 is volleyball match is in
progress between ladies.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 6: Attention probability in forward/backward DiSA
blocks for the two example sentences.

Figure 6 shows that1) semantically important words such
as nouns and verbs usually get large attention, but stop
words (am, is, are, etc.) do not; 2) globally important words,
e.g., volleyball, match, ladies in sentence 1 and dog, front,
carousel in sentence 2, get large attention from all other
words; 3) if a word is important to only some of the other
words (e.g. to constitute a phrase or sense-group), it gets
large attention only from these words, e.g., attention be-
tween progress, between in sentence1, and attention between
families, have in sentence 2.

This also shows that directional information can help to
generate context-aware word representation with temporal
order encoded. For instance, for word match in sentence 1,
its forward DiSA focuses more on word volleyball, while
its backward attention focuses more on progress and ladies,
so the representation of word match contains the essential
information of the entire sentence, and simultaneously in-
cludes the positional order information.

In addition, forward and backward DiSAs can focus on
different parts of a sentence. For example, the forward one
in sentence 2 pays attention to the word families, whereas the
backward one focuses on the word carousel. Since forward
and backward attentions are computed separately, it avoids
normalization over multiple signiﬁcant words to weaken
their weights. Note that this is a weakness of traditional at-
tention compared to RNN, especially for long sentences.

(a) Sentence 1, forward

(b) Sentence 1, backward

(c) Sentence 2, forward

(d) Sentence 2, backward

Figure 7: Fusion Gate F in forward/backward DiSA blocks.

In Figure 7, we show that the gate value F in Eq.(19). The
gate combines the input and output of masked self-attention.
It tends to selects the input representation h instead of the
output s if the corresponding weight in F is large. This
shows that the gate values for meaningless words, especially
stop words is small. The stop words themselves cannot con-
tribute important information, so only their semantic rela-
tions to other words might help to understand the sentence.
Hence, the gate tends to use their context features given by
masked self-attention.

(a) glass in pair 1

(b) close in pair 2

Figure 8: Two pairs of attention probability comparison of
same word in difference sentence contexts.

In Figure 8, we show the two multi-dimensional
source2token self-attention score vectors of the same word
in the two sentences, by their heatmaps. The ﬁrst pair has
two sentences: one is The glass bottle is big, and another is
A man is pouring a glass of tea. They share the same word is
glass with different meanings. The second pair has two sen-
tences: one is The restaurant is about to close and another
is A biker is close to the fountain. It can be seen that the
two attention vectors for the same words are very different
due to their different meanings in different contexts. This
indicates that the multi-dimensional attention vector is not
redundant because it can encode more information than one
single score used in traditional attention and it is able to cap-
ture subtle difference of the same word in different contexts
or sentences. Additionally, it can also alleviate the weakness
of the attention over long sequence, which can avoid nor-
malization over entire sequence in traditional attention only
once.

6 Conclusion
In this paper, we propose two novel attention mechanisms,
multi-dimensional attention and directional self-attention.
The multi-dimensional attention performs a feature-wise se-
lection over the input sequence for a speciﬁc task, and the
directional self-attention uses the positional masks to pro-
duce the context-aware representations with temporal in-
formation encoded. Based on these attentions, Directional
Self-Attention Network (DiSAN) is proposed for sentence-
encoding without any recurrent or convolutional structure.
The experiment results show that DiSAN can achieve state-
of-the-art inference quality and outperform existing works
(LSTM, etc.) on a wide range of NLP tasks with fewer pa-
rameters and higher time efﬁciency.

In future work, we will explore the approaches to using
the proposed attention mechanisms on more sophisticated
tasks, e.g. question answering and reading comprehension,
to achieve better performance on various benchmarks.

7 Acknowledgments
This research was funded by the Australian Government
through the Australian Research Council (ARC) under grant
1) LP160100630 partnership with Australia Government
Department of Health, and 2) LP150100671 partnership
with Australia Research Alliance for Children and Youth
(ARACY) and Global Business College Australia (GBCA).

References
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine
translation by jointly learning to align and translate. In ICLR.
Bjerva, J.; Bos, J.; Van der Goot, R.; and Nissim, M. 2014. The
meaning factory: Formal semantics for recognizing textual entail-
ment and determining semantic similarity. In SemEval@ COLING,
642–646.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language inference. In
EMNLP.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
sentence understanding. In ACL.

Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empirical
evaluation of gated recurrent neural networks on sequence model-
ing. In NIPS.
Clevert, D.-A.; Unterthiner, T.; and Hochreiter, S. 2016. Fast and
accurate deep network learning by exponential linear units (elus).
In ICLR.
Glorot, X., and Bengio, Y. 2010. Understanding the difﬁculty of
training deep feedforward neural networks. In Proceedings of the
Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, 249–256.
Graves, A.; Jaitly, N.; and Mohamed, A.-r. 2013. Hybrid speech
In Automatic Speech
recognition with deep bidirectional lstm.
Recognition and Understanding (ASRU), 2013 IEEE Workshop on,
273–278. IEEE.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay,
W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to
read and comprehend. In NIPS.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8):1735–1780.
Hu, M., and Liu, B. 2004. Mining and summarizing customer re-
In Proceedings of the tenth ACM SIGKDD international
views.
conference on Knowledge discovery and data mining, 168–177.
ACM.
Hu, M.; Peng, Y.; and Qiu, X. 2017. Reinforced mnemonic reader
for machine comprehension. arXiv preprint arXiv:1705.02798.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. arXiv preprint
arXiv:1404.2188.
Kim, Y.; Jernite, Y.; Sontag, D.; and Rush, A. M. 2016. Character-
aware neural language models. In AAAI.
Kim, Y. 2014. Convolutional neural networks for sentence classi-
ﬁcation. In EMNLP.
Kingma, D., and Ba, J. 2015. Adam: A method for stochastic
optimization. In ICLR.
Kiros, R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun, R.;
Torralba, A.; and Fidler, S. 2015. Skip-thought vectors. In NIPS.
Kokkinos, F., and Potamianos, A.
2017. Structural attention
neural networks for improved sentiment analysis. arXiv preprint
arXiv:1701.01811.
Lei, T., and Zhang, Y. 2017. Training rnns as fast as cnns. arXiv
preprint arXiv:1709.02755.
Lei, T.; Barzilay, R.; and Jaakkola, T. 2015. Molding cnns for text:
non-linear, non-consecutive convolutions. In EMNLP.
Li, X., and Roth, D. 2002. Learning question classiﬁers. In ACL.
Li, J.; Luong, M.-T.; Jurafsky, D.; and Hovy, E. 2015. When
are tree structures necessary for deep learning of representations?
arXiv preprint arXiv:1503.00185.
Liu, Y.; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natural lan-
guage inference using bidirectional lstm model and inner-attention.
arXiv preprint arXiv:1605.09090.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effective ap-
proaches to attention-based neural machine translation. In EMNLP.
Marelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.; Bernardi, R.;
and Zamparelli, R. 2014. A sick cure for the evaluation of compo-
sitional distributional semantic models. In LREC.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Efﬁcient
estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781.

Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013b. Distributed representations of words and phrases and their
compositionality. In NIPS.
Mou, L.; Men, R.; Li, G.; Xu, Y.; Zhang, L.; Yan, R.; and Jin, Z.
2016. Natural language inference by tree-based convolution and
heuristic matching. In ACL.
Munkhdalai, T., and Yu, H. 2017a. Neural semantic encoders. In
EACL.
Munkhdalai, T., and Yu, H. 2017b. Neural tree indexers for text
understanding. In EACL.
Pang, B., and Lee, L. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on minimum cuts.
In ACL.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:
Global vectors for word representation. In EMNLP.
Qian, Q.; Huang, M.; and Zhu, X. 2017. Linguistically regularized
lstms for sentiment classiﬁcation. In ACL.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention
model for abstractive sentence summarization. In EMNLP.
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2017. Bidi-
rectional attention ﬂow for machine comprehension. In ICLR.
Shang, L.; Lu, Z.; and Li, H. 2015. Neural responding machine for
short-text conversation. In ACL.
Socher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Manning, C. D.;
Ng, A. Y.; Potts, C.; et al. 2013. Recursive deep models for seman-
tic compositionality over a sentiment treebank. In EMNLP.
Socher, R.; Karpathy, A.; Le, Q. V.; Manning, C. D.; and Ng, A. Y.
2014. Grounded compositional semantics for ﬁnding and describ-
ing images with sentences. Transactions of the Association for
Computational Linguistics 2:207–218.
Srivastava, N.; Hinton, G. E.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural
networks from overﬁtting. Journal of Machine Learning Research
15(1):1929–1958.
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end
memory networks. In NIPS.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved se-
mantic representations from tree-structured long short-term mem-
ory networks. In ACL.
Teng, Z.; Vo, D.-T.; and Zhang, Y. 2016. Context-sensitive lexicon
features for neural sentiment analysis. In EMNLP.
Vaswani, A.; Shazeer; Noam; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all
you need. In NIPS.
Vendrov, I.; Kiros, R.; Fidler, S.; and Urtasun, R. 2016. Order-
embeddings of images and language. In ICLR.
Wiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating expressions
of opinions and emotions in language. Language resources and
evaluation 39(2):165–210.
Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A broad-
coverage challenge corpus for sentence understanding through in-
ference. arXiv preprint arXiv:1704.05426.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method.
arXiv preprint arXiv:1212.5701.
Zhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hierarchical
sentence model. In IJCAI.
Zhao, J.; Zhu, T.; and Lan, M. 2014. Ecnu: One stone two birds:
Ensemble of heterogenous measures for semantic relatedness and
textual entailment. In SemEval@ COLING, 271–277.

