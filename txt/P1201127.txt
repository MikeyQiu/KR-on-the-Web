8
1
0
2
 
v
o
N
3

 

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
9
5
1
1
0
.
1
1
8
1
:
v
i
X
r
a

Understanding and Comparing Scalable Gaussian Process
Regression for Big Data

Haitao Liua,∗, Jianfei Caib, Yew-Soon Ongb,c, Yi Wangd

aRolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore 637460
bSchool of Computer Science and Engineering, Nanyang Technological University, Singapore 639798
cData Science and Artiﬁcial Intelligence Research Center, Nanyang Technological University,
Singapore 639798
dCentral Technology Group, Rolls-Royce Singapore, 1 Seletar Aerospace Crescent, Singapore 797565

Abstract

As a non-parametric Bayesian model which produces informative predictive distribution,
Gaussian process (GP) has been widely used in various ﬁelds, like regression, classiﬁ-
cation and optimization. The cubic complexity of standard GP however leads to poor
scalability, which poses challenges in the era of big data. Hence, various scalable GPs
have been developed in the literature in order to improve the scalability while retaining
desirable prediction accuracy. This paper devotes to investigating the methodological
characteristics and performance of representative global and local scalable GPs including
sparse approximations and local aggregations from four main perspectives: scalability,
capability, controllability and robustness. The numerical experiments on two toy exam-
ples and ﬁve real-world datasets with up to 250K points oﬀer the following ﬁndings. In
terms of scalability, most of the scalable GPs own a time complexity that is linear to the
training size. In terms of capability, the sparse approximations capture the long-term
spatial correlations, the local aggregations capture the local patterns but suﬀer from
over-ﬁtting in some scenarios. In terms of controllability, we could improve the perfor-
mance of sparse approximations by simply increasing the inducing size. But this is not
the case for local aggregations. In terms of robustness, local aggregations are robust to
various initializations of hyperparameters due to the local attention mechanism. Finally,
we highlight that the proper hybrid of global and local scalable GPs may be a promising
way to improve both the model capability and scalability for big data.

Keywords: Gaussian process, big data, sparse approximations, local aggregations

1. Introduction

Surrogate-assisted modeling and optimization have been extensively deployed to fa-
cilitate modern aeroengine design [1, 2, 3, 4] due to the representational capability of

∗Corresponding author
Email addresses: htliu@ntu.edu.sg (Haitao Liu), ASJFCai@ntu.edu.sg (Jianfei Cai),

ASYSOng@ntu.edu.sg (Yew-Soon Ong), Yi.Wang4@Rolls-Royce.com (Yi Wang)

Preprint submitted to Elsevier

November 6, 2018

complex features. Among current surrogates (also known as machine learning models),
as a non-parametric Bayesian model, Gaussian process (GP) [5] (also known as Kriging
or emulator), has gained popularity.

Given n training data points X = {xi ∈ Rd}n

i=1 and the relevant observations
y = {yi ∈ R}n
i=1, GP intends to infer the latent function f : Rd 7→ R, which is drawn
from a Gaussian process, to describe the data pattern. Compared to other popular ma-
chine learning models, e.g., random forest [6] and deep neural networks [7], the Bayesian
perspective allows GP to provide not only the predictions at unseen points but also the
uncertainties about the predictions.1 The informative predictive distribution enables GP
to be deployed in various scenarios, e.g., active learning [9], time-series forecast [10] and
Bayesian optimization [11]. However, an inherent disadvantage of GP is that it scales
poorly with the training size n, since as a kernel method it handles the data in a one-shot
fashion. The standard implementation of GP needs to invert an n × n covariance matrix,
resulting in O(n2) complexity in storage and O(n3) complexity in time. Hence, the full
GP becomes intractable when the training size is greater than O(104) [12].

In the era of big data,2 numerous data brings vast quantity of information to enhance
the analysis, learning and exploration in the machine learning community. Hence, there
is a great demand for improving the scalability of standard GP while retaining desirable
prediction accuracy.

In current literature, various scalable GPs have been developed for handing big data

in diﬀerent ways. They usually can be classiﬁed into two main categories:

• Global approximations that summarize the whole training data using a small subset.
The simplest way is the subset-of-data (SoD) which trains GP on a random subset
of the training data D = {X, y} [13]. The performance of SoD however is limited
due to the ignorance of the remaining data. The most commonly used global ap-
proximations are the so-called sparse approximations [14, 15, 16, 17], which employ
a set of global inducing variables to approximate the joint prior or the interested
posterior. By using m inducing points, sparse approximations reduce the time
complexity of full GP to O(nm2) [15, 16] and more remarkably, O(m3) [17]. The
advantages of sparse approximations are that they are usually derived in a unify-
ing and elegant Bayesian framework, thus yielding a complete probabilistic model.
The limitation however is that the modeling performance is limited by the small set
of global inducing points, i.e., it is diﬃcult for them to capture the quick-varying
features, especially in high dimensions [18].

• Local approximations that employ the idea of divide-and-conquer (D&C) to dis-
tribute the whole training process over multiple local GP experts. By partitioning
the data D into M local subsets {Di = {Xi, yi}}M
i=1, each of which is assumed to
have the same training size m0 = n/M , local approximations yield the time com-
plexity of O(nm2
0). The beneﬁts brought by the D&C idea are that (i) it enables
capturing local patterns; and (ii) it scales local approximations up to arbitrary

1It has been pointed out that GP is equivalent to a shallow but inﬁnitely wide neural network with

Gaussian weights [8].

expensive to be stored and analyzed.

2Big data here mainly refers to the datasets with extremely large sample size n such that they are

2

dataset due to the straightforward parallel/distributed structure. The simplest lo-
cal approximation is to training individual GP experts on multiple local subsets,
which however suﬀers from discontinuous predictions and local over-ﬁtting. Hence,
local aggregations have been presented to smooth the predictions by aggregating
the predictions from multiple local experts [12, 19, 20, 21, 22]. The limitation of
local aggregations however is that they cannot provide a complete probabilistic
model, which results in the so-called Kolmogorov inconsistency [23].

This paper intends to comprehensively investigate the characteristics and perfor-
mance of representative scalable GPs on real-world large-scale datasets. As for the
global approximations, we mainly introduce and compare the sparse approximations.
Particularly, the sparse approximations are classiﬁed into two categories: the prior ap-
proximations [15, 24, 25] which approximate the prior but perform exact inference, and
the posterior approximations [16, 17, 26] which retain the exact prior but perform ap-
proximate inference. We do not investigate some particular sparse approximations, e.g.,
the structured kernel interpolation [27] which exploits the inducing set with Kronecker
structure, since the inducing size increases exponentially with the dimensionality d, and
the training quickly becomes intractable when d > 5.

As for the local approximations, we introduce and compare the pure local GPs and
the local aggregations using product-of-experts and Bayesian committe machine [12, 19,
20, 21, 22]. We here do not investigate the mixture-of-experts [28, 29] since it is mainly
designed for capturing non-stationary features and suﬀers from intractable inference.

The comparative study introduces and investigates the characteristics of representa-

tive scalable GPs from four main perspectives:

• Scalability that indicates the time complexity of scalable GPs to handle big data;

• Capability that means the representational ability of scalable GPs to handle various

tasks;

• Controllability that indicates whether we can easily improve the performance of
scalable GPs by tuning model parameters, e.g., the inducing size or the number of
experts;

• Robustness means the sensitivity of scalable GPs to various initializations of hy-

perparameters.

The remainder of the paper is organized as follows. Section 2 brieﬂy introduces
the standard GP. Thereby, Sections 3 and 4 respectively introduce the global and local
approximations. Thereafter, Section 5 comprehensively investigates the characteristics
and performance of these scalable GPs on several toy examples and ﬁve real-world large-
scale datasets with up to 250K data points. Finally, Section 6 provides concluding
remarks.

2. Gaussian processes regression

As a non-parametric Bayesian model, the Gaussian process places a GP prior over

the latent function f : Rd 7→ R as

f (x) = GP(m(x), k(x, x′)),
3

(1)

where m(.) is the mean function which is usually taken as zero without loss of generality,
and k(., .) is the kernel function which controls the model smoothness. Popularly, we
use the well-known squared exponential (SE) kernel function with automatic relevance
determination (ARD) as

k(x, x′) = σ2

f exp

d

−

1
2

 

i)2

(xi − x′
l2
i

,

!

(2)

i=1
X
f is the output signal, and li is the input length-scale along the ith dimension.

where σ2
Fig. 1(a) illustrates several samples drawn from the zero-mean GP prior.

Figure 1: Samples (red curves) drawn from (a) the zero-mean GP prior and (b) the posterior after
observing four data points.

Let us consider a regression task y(x) = f (x) + ǫ where ǫ ∼ N (0, σ2

ǫ ) is the iid
ǫ ). Typically, the GP can be trained by
ǫ } through maximizing the log

f , l1, · · · , ld, σ2

noise, the likelihood writes p(y|f ) = N (y|f, σ2
ﬁnding the optimal hyperparameters θ = {σ2
marginal likelihood (model evidence) as

log p(y) = −

log(2π) −

log |Knn + σ2

ǫ I| −

(Knn + σ2

ǫ I)−1y,

(3)

n
2

1
2

yT

1
2

where Knn = k(X, X) is the n × n kernel matrix. Conditioned on the training data and
the inferred hyperparameters, we derive the predictive (posterior) distribution p(f∗|D, x∗)
at a test point x∗ with the mean and variance respectively given as

µ∗ =k∗n[Knn + σ2
ǫ I]−1y,
∗ =k∗∗ − k∗n[Knn + σ2
σ2

ǫ I]−1kT
∗n,

(4a)

(4b)

where k∗∗ = k(x∗, x∗) and k∗n = k(x∗, X). Hence, the predictive distribution of y∗
has p(y∗|D, x∗) = N (y∗|µ∗, σ2
ǫ ). Fig. 1(b) depicts several samples drawn from the
posterior after observing four data points.

∗ + σ2

It is found in (3) that the computational bottleneck in the full GP inference is the
ǫ I|, both of which require the

inversion (Knn + σ2
time complexity of O(n3) through standard Cholesky decomposition.

ǫ I)−1 and the determinant |Knn + σ2

The cubic complexity poses urgent demand of improving the scalability of standard
GP for handling big data. In what follows, we will introduce representative scalable GPs
including

• global approximations, particularly the sparse approximations, which distillate the

training data through m inducing points; and

4

• local approximations, particularly the local aggregations, which follow the idea of

D&C to aggregate multiple GP experts for boosting predictions.

3. Global approximations

The simplest global approximation is the subset-of-data (SoD) which trains GP on a
random subset of the training data. Given m (m ≪ n) subset points, the time complexity
of SoD is substantially reduced to O(m3) in comparison to the full GP. The SoD however
may produce over-conﬁdent predictions by ignoring the remaining data.

To date, the popular global approximations are sparse approximations [14] motivated
by the Nystr¨om approximation [30]. Sparse approximations employ an inducing set Xm
which includes m points to summarize the whole training data, thus reducing the time
complexity to O(nm2).3 The latent inducing variables fm akin to f follow the same GP
prior p(fm) = N (fm|0, Kmm).

In what follows, current sparse approximations are classiﬁed into two main categories:
the prior approximations which approximate the prior but perform exact inference, and
the posterior approximations which retain the exact prior but perform approximate in-
ference.

3.1. Prior approximations

by marginalizing out fm as

Given the independence assumption f ⊥ f∗|fm, the joint prior p(f , f∗) can be derived

p(f , f∗) =

p(f |fm)p(f∗|fm)p(fm)dfm.

Z

Given the Nystr¨om notation Qab = KamK −1
in (5) are respectively expressed as

mmKmb, the training and test conditionals

p(f |fm) =N (f |KnmK −1
p(f∗|fm) =N (f∗|k∗mK −1

mmfm, Knn − Qnn),
mmfm, k∗∗ − Q∗∗).

Now we clearly see that fm is called inducing variables since it induces the dependence
between the independent fm and f∗.

To achieve computational gains, prior approximations further modify the joint prior

as

p(f , f∗) ≈ q(f , f∗) =

q(f |fm)q(f∗|fm)p(fm)dfm.

(7)

Compared to (5), the training and test conditionals are respectively approximated in (7)
through replacing the co-variance matrices as

q(f |fm) =N (f |KnmK −1
q(f∗|fm) =N (f∗|k∗mK −1

mmfm, ˜Qnn),
mmfm, ˜Q∗∗).

(5)

(6a)

(6b)

(8a)

(8b)

3The inducing points in Xm are usually regarded as hyperparameters to be inferred.

Z

5

(10a)

(10b)

(11a)
(11b)

Thereafter, we marginalize out all the latent variables to approximate the log marginal
likelihood log p(y) as

log q(y) = −

log(2π) −

log | ˜Qnn + Qnn + σ2

ǫ I| −

( ˜Qnn + Qnn + σ2

ǫ I)−1y. (9)

n
2

1
2

yT

1
2

Through particular selections of ˜Qnn, we can eﬃciently calculate the determinant | ˜Qnn +
ǫ I)−1 in (9) by only depending on K −1
Qnn + σ2
mm.
For instance, the subset-of-regressors (SoR) [24, 31, 32] imposes ˜Qnn = 0 and ˜Q∗∗ = 0

ǫ I| and the inversion ( ˜Qnn + Qnn + σ2

to the training and test conditionals such that

qSoR(f |fm) = N (f |KnmK −1
qSoR(f∗|fm) = N (f∗|k∗mK −1

mmfm, 0),
mmfm, 0).

This is equivalent to applying the Nystr¨om approximation to both training and test data.
With this consistent assumption, the SoR is equivalent to a degenerate4 GP with the
mmkT(xj, Xm). However,
rank (at most) m kernel function kSoR(xi, xj) = k(xi, Xm)K −1
due to the limited m freedoms in kSoR, the SoR suﬀers from over-conﬁdent prediction
variance when leaving the training data. In contrast to SoR, the deterministic training
conditional (DTC) [25, 33] imposes ˜Qnn = 0 but retains the exact test conditional as

qDTC(f |fm) = N (f |KnmK −1
qDTC(f∗|fm) = p(f∗|fm).

mmfm, 0),

Hence, the prediction mean µDTC(x∗) is the same as that of SoR, but the prediction
variance σ2
DTC(x∗) is always larger than that of SoR. Notably, due to the inconsistent
approximations in (11), the DTC is not an exact GP.

Moreover, the fully independent training conditional (FITC) [15] imposes another

independence assumption to remove the dependency among the latent variables {fi}n
such that the training conditional factorizes as

i=1

qFITC(f |fm) =

p(fi|fm) = N (f |KnmK −1

mmfm, diag[Knn − Qnn]),

(12)

n

i=1
Y

whereas the test conditional retains exact. It is found that the variances of qFITC(f |fm)
are identical to that of p(f |fm) due to the diagonal correction term ˜Qnn = diag[Knn −
Qnn]. Hence, compared to SoR and DTC which completely omit the uncertainty in (10)
and (11), FITC partially retains it, resulting in a closer approximation to the prior
p(f , f∗).

The extensions of FITC include for example the fully independent conditional (FIC) [14]

which additionally applies the independence assumption to the test conditional q(f∗|fm)
in order to obtain a degenerate GP with the covariance function kFIC(xi, xj) = kSoR(xi, xj)+
δij [k(xi, xj) − kSoR(xi, xj)], where δij is the Kronecker’s delta. Besides, the partially
independent training conditional (PITC) [14] factorizes q(f |fm) over M independent
subsets (blocks) {Di}M
i=1, thus taking into account the joint distribution of fi in each

4“degenerate” means the GP employs a kernel with a ﬁnite rank.

6

subset. However, it is argued in [34] that though providing a closer approximation to
p(f |fm), the PITC brings little improvements over FITC.

Diﬀerently, the partially independent conditional (PIC) [34] retains the conditional
independence assumption, i.e., f ⊥ f∗|fm, for all the blocks except the one containing
the test point x∗. Suppose that x∗ ∈ Dj, the training conditional is

qPIC(f , f∗|fm) = p(fj, f∗|fm)

p(fi|fm),

(13)

M

i6=j
Y

which corresponds to an exact GP with kPIC(xi, xj) = kSoR(xi, xj) + ψij [k(xi, xj) −
kSoR(xi, xj)], where ψij = 1 when xi and xj belong to the same block; otherwise ψij = 0.
The PIC is regarded as a hybrid approximation since when we take all the block sizes
to one, it recovers FIC; when we take the number of inducing points to zero, it recovers
the pure local GP.

3.2. Posterior approximations

In contrast to prior approximations, posterior approximations, e.g., variational free
energy (VFE) [16], directly approximate the posterior p(f , fm|y) by a free variational
distribution q(f , fm|y). The discrepancy between q(f , fm|y) and p(f , fm|y) can be
quantiﬁed by the Kullback-Leibler (KL) divergence

KL(q(f , fm|y)||p(f , fm|y)) =

q(f , fm|y) log

df dfm + log p(y)

(14)

q(f , fm|y)
p(f , fm, y)

Z

= − Fq + log p(y).

It is observed that minimizing the non-negative KL(q||p) is equivalent to maximizing Fq,
since log p(y) is ﬁxed w.r.t. q(f , fm|y). Thus, Fq is called the lower bound of log p(y) or
the variational free energy, which permits joint optimization of the variational parameters
and hyperparameters.

Since we have p(f , fm|y) = p(f |fm)p(fm|y), the variational distribution factorizes
similarly as q(f , fm|y) = p(f |fm)q(fm|y), where q(fm|y) = N (fm|m, S). Fortunately,
the optimal variational distribution q∗(fm|y) can be found using the calculus of varia-
tions. Taking the derivative of Fq w.r.t. q(fm|y) to zero, we have

q∗(fm|y) = N (fm|σ−2

ǫ KmmΣKmny, KmmΣKmm),

(15)

where Σ = (Kmm + σ−2
“collapsed” bound which is independent of q(fm|y) as

ǫ KmnKnm)−1.

Inserting q∗(fm|y) back into Fq, we have a

FVFE = −

n
2
1
2

−

log |Qnn + σ2

ǫ I|

log(2π) −

1
2
ǫ I)−1y −
(Qnn + σ2

yT

1
2σ2
ǫ
ǫ Tr(Knn − Qnn).

= log qDTC(y) − 0.5σ−2

Tr(Knn − Qnn)

(16)

It is found that compared to the log marginal likelihood of DTC, the VFE bound (16)
involves an additional trace term Tr(Knn − Qnn) ≥ 0 which represents the total variance
7

of the training conditional p(f |fm). Particularly, Tr(Knn − Qnn) = 0 means that fm =
f and we reproduce the full GP. Hence, the trace term helps VFE (i) guard against
over-ﬁtting; (ii) choose a good inducing set; and (iii) improve FVFE with increasing
m [16, 35, 36, 37].

To further improve the scalability of VFE, unlike the bound (16) that “integrates”
out the variational distribution, q(fm|y) now is retained in the bound Fq in order to
obtain a full factorization over data points as [17]

Fq = hlog p(y|f )ip(f |fm)q(fm|y) − KL(q(fm|y)||p(fm)),

(17)

where h.iq(.) represents the expectation over the distribution q(.). The newly organized
bound Fq, which is looser than FVFE, has a key property that it can be written as
n
the sum of n terms because of the likelihood p(y|f ) =
i=1 p(yi|fi). Due to (i) the
full factorization and (ii) the diﬃculty of optimizing the variational parameters m and S
since they are deﬁned in a non-Euclidean space, one can employ the Stochastic Variational
Inference (SVI) [38] to infer the variational parameters in the natural gradient space via
eﬃcient stochastic gradient descent (SGD) algorithms [39, 40], resulting in the greatly
reduced complexity of O(bm3) where b is the mini-batch size in SGD. Therefore, the
stochastic variational GP (SVGP) adopts the SGD to train a sparse GP at any time
with a small batch of the training data in each iteration [41].5

Q

4. Local approximations

Following the idea of D&C, the simplest local approximation is the pure local GPs
which ﬁrst partition the training data into M subsets for example through clustering
techniques, and then train a GP expert on each subset using individual parameters. The
D&C idea scales local approximations up to arbitrary dataset due to the straightforward
parallel/distributed fashion. Besides, in comparison to the global approximations, the
naive local approximation is capable of describing non-stationary features (local patterns)
for improving predictions at the cost of however suﬀering from discontinuous predictions,
inaccurate uncertainties and local over-ﬁtting.

To smooth and boost predictions while retaining computation gains from D&C, the
local aggregations, which are inspired by the idea of model averaging, were presented to
aggregate the predictions from multiple experts. Particularly, given a partition {Di}M
i=1
of D, the aggregation methods follow the product rule [19], i.e., assuming independent
GP experts, such that the marginal likelihood p(y) factorizes as

p(y|X, θ) ≈

pi(yi|Xi, θi),

(18)

i=1
Y
where θi is a vector comprising the hyperparameters for the ith expert Mi, and θ =
{θ1, · · · , θM }. Similar to the sparse approximations, the aggregations reduce the time

5Unlike the traditional conjugate gradient descent (CGD) algorithm which takes all the training data
to calculate the gradients, the SGD only takes b (b ≪ n) out of the n data points to calculate the
gradients in each iteration, thus enhancing large-scale learning.

M

8

complexity of full GP to O(nm2
n/M , but requiring no additional inducing points or variational parameters.

0) when all the experts have an equal training size m0 =

After training the experts {Mi}M

i=1, we combine their
predictions together at the test point x∗ by employing some aggregation criteria. For
instance, under the independence assumption, the product-of-experts (PoE) [19, 21, 42,
43, 44, 45] conducts the aggregation as

i=1 on the relevant subsets {Di}M

pA(y∗|D, x∗) =

pβi
i (y∗|Di, x∗),

(19)

i=1
Y
where βi is a weight quantifying the contribution of Mi at x∗. The product of Gaussian
distributions in (19) results in another Gaussian distribution with the mean and variance
analytically expressed as

M

M

µA(x∗) =σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

σ−2
A (x∗) =

i=1
X
βiσ−2
i

(x∗),

M

i=1
X

where µi(x∗) and σ2
i (x∗) are respectively the prediction mean and variance of Mi at x∗.
By taking a constant weight βi = 1, the original PoE quantiﬁes the contribution of each
expert by the prediction precision [19]. However, the naive sum of experts’ prediction
precisions in (20b) will make the aggregated variance vanish quickly with increasing M ,
i.e., producing seriously over-conﬁdent prediction variance [22, 46]. Hence, the general-
ized PoE (GPoE) [21] introduces a varying weight βi = 0.5(log σ2
i (x∗)), which is
deﬁned as the diﬀerence in the diﬀerential entropy between the prior and the posterior,
to weaken the votes of those poor experts with large uncertainty. This ﬂexible weight,
however, produces an explosive prediction variance when x∗ is far away from X [22]. To
M
address this issue, we could either impose a constraint
i=1 βi = 1 [21] or simply employ
βi = 1/M [12].

∗∗ − log σ2

To improve the performance of PoEs, the Bayesian committee machine (BCM) [12,
20, 22, 47] additionally takes the prior p(y∗|x∗) into account and imposes a conditional
M
independence assumption p(y|f∗, X) =
i=1 p(yi|f∗, Xi). Consequently, the aggregated
predictive distribution derived from the Bayes rule is

P

pA(y∗|D, x∗) =

M

i=1 pβi
pP
Q

i (y∗|Di, x∗)
i=1 βi−1(y∗|x∗)

M

,

with the prediction mean and variance analytically given as

Q

M

µA(x∗) =σ2

A(x∗)

βiσ−2
i

(x∗)µi(x∗),

σ−2
A (x∗) =

i=1
X
βiσ−2
i

M

i=1
X

9

(x∗) +

1 −

 

βi

σ−2
∗∗ .

!

M

i=1
X

(20a)

(20b)

(21)

(22a)

(22b)

Table 1: Comparison of the space and time complexity of representative global and local scalable GPs,
where m is the inducing size for sparse approximations and m0 is the training size of each expert for
local aggregations.

Model
Test
SoR, DTC, FITC, VFE O(nm) O(nm2) O(m2)
O(m2)
SVGP
0) O(nm0)
(G)PoE, (R)BCM

O(bm2) O(bm3)
O(nm0) O(nm2

Storage Training

Compared to (20b), the prior correlation in (22b) helps BCMs recover the GP prior when
leaving the training data. The original BCM [20] takes βi = 1, and the newly developed
robust BCM (RBCM) [12] employs the varying βi like GPoE in order to produce robust
predictions within X. The BCMs however are found to suﬀer from weak experts when
leaving X [12, 22].

Regarding the model capability, the PoEs in (19) allow the GP experts to own indi-
vidual parameters in order to capture non-stationary features, whereas the BCMs in (21)
cannot due to the shared prior p(y∗|x∗) across experts. Besides, for the BCMs using ex-
perts with shared hyperparameters, the direct aggregation of {pi(y∗|Di, x∗)}M
i=1 induces
seriously over-conﬁdent prediction variance with increasing n [22]. To alleviate this is-
sue, we could use the newly proposed generalized RBCM framework [22]; or simply, we
aggregate {pi(f∗|Di, x∗)}M
i=1, and ﬁnally add the estimated
noise variance [12].

i=1 instead of {pi(y∗|Di, x∗)}M

Finally, Table 1 summarizes the training and test time complexity of representative
global and local scalable GPs.
It is found that if m = m0, all the scalable GPs ex-
cept SVGP own the same training complexity. Besides, the local aggregations have a
higher test complexity since they need the predictions of all the experts at x∗. Finally,
note that the computations in the scalable GPs listed above can be sped up through
distributed/parallel computing, see [12, 48, 49].

5. Numerical experiments

This section ﬁrst employs several toy examples to illustrate the methodological char-
acteristics of global and local scalable GPs, and then applies some of them to ﬁve real-
world datasets with up to 250K data points. The goal of this comparative study is to
enhance the understanding of representative scalable GPs and investigate their usability
by addressing the issues as: (i) what are the features of these scalable GPs and in what
scenarios they are useful? (ii) are the scalable GPs controllable to model parameters?
and (iii) are the scalable GPs robust to various initializations of hyperparameters?

In the comparative study below, we implement the scalable GPs based on the GPML
toolbox6, the GPstuﬀ toolbox7 and the GPy toolbox8. Before model training, the data
pre-processing is performed by normalizing y and each column of X to N (0, 1).
In
modeling, we employ the SE kernel in (2), and initialize the length-scales l1, · · · , ld as

6http://www.gaussianprocess.org/gpml/code/matlab/doc/
7https://github.com/gpstuff-dev/gpstuff
8https://github.com/SheffieldML/GPy

10

f as 1.0, and the noise variance σ2

0.5, the signal variance σ2
ǫ as 0.1. All the scalable GPs
except SVGP employ the CGD for inference with the maximum number of iterations as
100. The particular SVGP employs the Adadelta SGD algorithm [39] for inference with
the step rate as 0.1, the momentum as 0.9, and the maximum number of iterations as
1000. All the codes are executed on a personal computer with four 3.70 GHz cores and
16 GB RAM.

Finally, given n∗ test points {X∗, y∗}, we assess the prediction accuracy of scalable

GPs using the standardized mean square error (SMSE) deﬁned as

SMSE =

n∗

j=1 (y∗j − µ∗j)2
n∗ × var(y)

.

P

(23)

The SMSE criterion quantiﬁes the discrepancy between the predictions and the exact
function values; particularly, it equals to one when the model always predicts the mean
of y. Besides, to quantify the quality of predictive distribution that considers both
prediction mean and variance, we employ the mean standardized log loss (MSLL) deﬁned
as

MSLL =

[log N (y∗j|y, var(y)) − log p(y∗j|D, x∗j)] ,

(24)

j=1
X
where log p(y∗j|D, x∗j) = −0.5
. The MSLL will be neg-
ative for high quality models, and particularly, it will be zero when the model always
predicts the mean and variance of y.

∗j) + (y∗j − µ∗j)2/σ2
∗j

log(2πσ2

(cid:3)

(cid:2)

n∗

1
n∗

5.1. Toy examples

5.1.1. Characteristics of sparse approximations

This section attempts to study various sparse approximations via a toy example

expressed as

y(x) = sinc(x) + ǫ,

x ∈ [−4, 4],

(25)

where ǫ = N (0, 0.04). We randomly draw 120 training points from this generative
function; besides, we generate 300 test points in [−7, 7]. The sparse approximations
include SoR, DTC, FITC, PIC, VFE and SVGP. As for model conﬁgurations, we choose
15 initial inducing points equally spaced in [−4, 4]; particularly, we partition the training
data into M = 10 disjoint subsets for the PIC approximation; we use the batch size
of b = 30 for SVGP. Fig. 2 depicts the predictions of the six sparse approximations,
respectively, on the toy example. For the purpose of comparison, the results of full GP
are involved in the ﬁgure.

It is ﬁrst observed that the two posterior approximations, VFE and its variant SVGP,
provide the best approximation to the full GP, since they directly approximate the poste-
rior with no modiﬁcation to the joint prior. If we gradually increase the inducing size m,
they will ﬁnally converge to the full GP. To verify this, Fig. 3(a) depicts the convergence
curves of VFE with diﬀerent m values. The horizontal axis represents the number of
optimization iterations, and the vertical axis represents the negative log marginal likeli-
hood (NLML). The black dash line indicates the converged NLML value of full GP. It is
observed that when using a small inducing set (m = 5), the VFE converges with a larger
NLML value than that of full GP; but with the increase of m, the NLML of VFE quickly

11

1

0

-1

1

0

-1

1

0

-1

103

102

0

1

0

-1

1

0

-1

1

0

-1

200

180

160

140

120

100

0

-6

-4

-2

0

2

4

6

-6

-4

-2

0

2

4

6

-6

-4

-2

0

2

4

6

-6

-4

-2

0

2

4

6

-6

-4

-2

0

2

4

6

-6

-4

-2

0

2

4

6

Figure 2: A toy example to illustrate the characteristics of various sparse approximations. The crosses
represent the training points. The green dot curve represents the prediction mean of full GP. The two
green curves represent 95% conﬁdence interval of the full GP prediction. The red curve represents the
prediction mean of a sparse approximation. The shadow region represents 95% conﬁdence interval of the
sparse prediction. The top circles and bottom triangles represent the positions of initial and optimized
inducing points, respectively.

10

20

30

40

50

100

200

300

400

Figure 3: The convergence curves of (a) VFE and FITC using CGD with diﬀerent inducing sizes, and
(b) SVGP using SGD with diﬀerent batch sizes on the toy example.

converges to that of full GP. Note that because of the simplicity of this toy example, the
VFE with m = 15 has provided a very close approximation to the full GP.

The diﬀerences between VFE and SVGP are that (i) SVGP employs a less tight
bound (17) deﬁned in an augmented probabilistic space;9 and (ii) SVGP employs the
SGD for optimization. Fig. 3(b) shows the convergence curves of SVGP using diﬀerent
batch sizes for SGD. It is observed that compared to the deterministic CGD, the SGD

9Due to the explicit variational distribution q(fm|y) in (17), the SVGP should consider m+m(m+1)/2

additional variational parameters.

12

(i) produces many ﬂuctuations with a small batch size; and (ii) converges more slowly
even by using b = 120, because of the relaxed bound Fq (17) and the huge parameter
space. But the superiority of SGD for big data is that (i) it greatly reduces the compu-
tational complexity by running in a batch mode; and (ii) it can achieve better solutions
for complicated functions by easily escaping from local optima and saddle points. For
example, the SVGP with b = 5 in Fig. 3(b) ﬁnds some smaller NLML values than that
of full GP.

As for the four prior approximations, it is observed in Fig. 2 that the SoR produces
severely over-conﬁdent prediction variance when leaving X. This is because the SoR im-
poses too restrictive assumptions to the training and test data in (10). Though equipped
with the same prediction expressions to that of VFE, the DTC produces poorer pre-
dictions due to the restrictive marginal likelihood. Diﬀerent from SoR and DTC, the
FITC is capable of capturing the heteroscedastic noise. Let us look inside the prediction
variance of FITC

σ2(x∗) = k∗∗ − Q∗∗ + k∗mΣmmkm∗,

where Σmm = (Kmm + KmnΛ−1Knm)−1 and Λ = diag[Knn − Qnn] + σ2
ǫ In. It is found
that the diagonal term diag[Knn − Qnn], which represents the input-dependent variances
of the training conditional p(f |fm), enables FITC to capture the heteroscedasticity of
noise at the cost of (i) producing an invalid estimation (nearly zero) of the noise variance
σ2
ǫ , (ii) worsening the accuracy of prediction mean, and (iii) producing overlapped in-
ducing points [36]. Finally, the hybrid PIC approximation, which is capable of capturing
local patterns, is found to produce discontinuous predictions and conservative variances.
As discussed before, the posterior approximations VFE and SVGP can be regarded as
the approximation to full GP. But this is not the case for the four prior approximations.
Prior approximations seek to achieve good prediction accuracy at a low computational
cost, rather than faithfully converging to the full GP with increasing m. This can be
reﬂected by two observations: (i) some of the optimized inducing points in Fig. 2 overlap
with each other, especially for FITC;10 and (ii) rather than converging to the NLML of
full GP with the increase of m, the FITC tends to produce a diﬀerent, smaller NLML
value in Fig. 3(a).

5.1.2. Characteristics of local approximations

Diﬀerent from the sparse approximations which capture long-term spatial correlations
based on the global inducing set, the local approximations focus on subspace learning via
local experts, thus capturing local patterns (non-stationary features). As an illustration,
we apply the pure local GPs to the toy example (25) in Fig. 4. Particularly, we partition
the 120 training points into M = 10 local subsets and train individual GP experts. It
is found that the naive local approximation captures local patterns at the cost of (i)
producing discontinuity on the boundaries of sub-regions, and (ii) risking over-ﬁtting in
some local regions.

The ﬁrst issue could be addressed by the model averaging strategies like PoE and
BCM. As shown in Fig. 4, the PoE and GPoE yield continuous predictions. But the
original product rule makes PoE produce over-conﬁdent prediction variance and deteri-
orated prediction mean, which are not preferred in practice. Instead, the GPoE employs

10The reason for FITC to produce overlapped inducing points has been theoretically analyzed in [36].
13

Figure 4: A toy example to illustrate the characteristics of various local approximations.

a varying weight βi to weaken the votes of poor experts, resulting in sensible prediction
mean and variance.

The second issue however is directly inherited by PoE and GPoE, since they have no
mechanism to avoid local over-ﬁtting. To alleviate the over-ﬁtting issue, we could increase
the training size for each expert (i.e., small M value) in order to take into account the
long-term spatial correlation at the cost of degrading the capability of capturing local
patterns and increasing the complexity. Besides, the hybrid approximations [34, 50, 51]
may be a promising solution to this issue, since they inherit the advantages of both
global and local approximations.11 Finally, another alternative way to guard against
local over-ﬁtting is to sharing hyperparameters across experts, like the RBCM in Fig. 4.
It is observed that the RBCM has conservative prediction variances. Note that though
sharing the hyperparameters, the local structure itself could help RBCM capture local
patterns, which will be shown in next section.

5.1.3. Global vs. local approximations

Here we take the time-series solar dataset [52] which contains 391 data points with
quick-varying features for comparing global and local approximations. The study at-
tempts to investigate whether global and local approximations could handle complicated
tasks using limited computational resources.

Among the four local approximations, we take the RBCM for example and use the
k -means technique to partition the training data into M = 10 disjoint subsets, resulting
in approximately 39 data points for each expert. Among the six global approximations,
we employ the VFE and use m = 40 inducing points. Particularly, in order to model the
quick-varying features in this dataset, we initialize the length-scales in the SE kernel (2)
with a small value of 0.0442 after data normalization.12

11The current hybrid approximations, e.g., the PIC in Fig. 2, however still suﬀer from the discontinuity

12In the original input space [1600, 2000] of the solar dataset, the initial length-scale is equal to 5,

issue.

which is a very small value.

14

1367

1366

1365

1364

1363

1367

1366

1365

1364

1363

1600

1650

1700

1750

1800

1850

1900

1950

2000

1600

1650

1700

1750

1800

1850

1900

1950

2000

Figure 5: A solar example involving quick-varying features to illustrate the characteristics of RBCM
and VFE.

Fig. 5 depicts the modeling results of RBCM and VFE on the solar dataset. It turns
out that though sharing the hyperparameters across experts, the RBCM captures the
quick-varying features successfully. This is because the localized structure helps RBCM
equipped with local attention to take into account the local patterns when estimating the
shared hyperparameters. On the contrary, even by using such small initial length-scales,
the VFE fails to capture the quick-varying features due to the small set of global inducing
points. The performance of VFE indeed could be improved by increasing the number
of inducing points, which however becomes unattractive in terms of computational com-
plexity.

Besides, the local attention mechanism may help improve the robustness of RBCM to
various settings. To verify this, we study an extreme case wherein the length-scales in the
SE kernel are initialized as 2.0 which in the original space is 226. By simply increasing
the number of experts to M = 40, the RBCM again is enabled to capture the quick-
varying feature at, interestingly, lower computational cost. But it is notable that the
increase of M does not always correspond to good predictions, since practical datasets
often do not follow the iid noise assumption. That means too much localized experts for
RBCM may degrade the generalization capability (indicated by severely over-conﬁdent
prediction variance), which will be observed in next section.

5.2. Real-world datasets

This section seeks to assess the representative global and local scalable GPs on
ﬁve real-world datasets with diﬀerent characteristics, e.g., regular/clustering inputs,
noise/noiseless observations, and homoscedastic/heteroscedastic noise, see Table 2. The
airfoil dataset [53] comprises diﬀerent sizes of NACA 0012 airfoils at various wind tun-
nel speeds and angles of attack, and the output is the scaled sound pressure level. The
protein dataset [53] describes the physicochemical properties of the protein tertiary struc-
ture. The sarcos dataset [5] describes the inverse kinematics of a robot arm. The chem
dataset [54] concerns the physical simulations relating to electron energies in molecules.
Finally, the sdss dataset [55] comes from the Sloan Digital Sky Surveys 12th Data Re-
lease. Note that each conﬁguration of n and n∗ in Table 2 has ten random instances in
order to comprehensively evaluate the performance of scalable GPs.

15

dataset
airfoil
protein
sarcos
chem
sdss

Table 2: Characteristics of ﬁve real-world datasets.
remark
regular inputs
clustering inputs
nearly noiseless
heteroscedastic, tiny noise
heteroscedastic noise

n
1,200
35,000
40,000
60,000
250,000

n∗
303
10,730
8,933
11,969
50,000

d
5
9
21
15
10

Table 3: Model parameters of scalable GPs for the ﬁve real-world datasets. m is the inducing size for
VFE, SVGP and FITC, b is the batch size for SVGP, and M is the number of experts for GPoE and
RBCM.

Parameter
m
b
M

airfoil
60
50
20

protein
400
2,500
35

sarcos
400
2,500
50

chem sdss
250
300
9,000
3,000
500
80

Among the six global scalable GPs in Fig. 3, we select the VFE and SVGP due
to the high approximation quality, and the FITC due to the capability of capturing
heteroscedastic noise; the SoR, DTC and PIC are not included due to their poor or
discontinuous predictions. Among the four local scalable GPs in Fig. 4, we choose the
RBCM trained on experts with shared hyperparameters and the GPoE trained on experts
with individual hyperparameters; the pure local GPs and PoE are not considered due to
their less competitive performance.

5.2.1. Comparison of global and local scalable GPs

We ﬁrst study the scalability and capability of global and local scalable GPs. Table 3
oﬀers the model parameters of scalable GPs, including the inducing size m, the batch size
b and the number M of experts, for the ﬁve real-world datasets. During the comparison
study, the inducing points are initialized by the centroids of clusters partitioned by the
k -means technique; the disjoint local subsets are partitioned by the k -means technique
as well. We choose the model parameters in Table 3 such that these scalable GPs have
comparable running time. The data pre-processing and optimization conﬁgurations are
consistent to that in Section 5.1.

Fig. 6 depicts the modeling results of ﬁve scalable GPs over ten runs on the ﬁve
datasets in terms of SMSE and MSLL. The horizontal axis represents the sum of training
and predicting time. Note that due to the small training size, we include the results of
full GP on the airfoil dataset for comparison.

As for the three global scalable GPs, the VFE and SVGP produce similar results since
they are derived in the same posterior approximation framework. However, the SVGP
is more potential in terms of eﬃciency for large-scale learning since it allows using the
SGD optimization, see the results on the sdss dataset. Diﬀerent from VFE and SVGP
which follow a constant noise assumption, the FITC is capable of describing possible
heteroscedastic noise variances, which are indicated by the smaller MSLL values in the
ﬁgure. But this superiority of FITC comes at the cost of worsening the accuracy of
prediction mean, resulting in larger SMSE values.

16

0.8

0.6

0.4

0.2

0

0

6

4

2

0

-2

0

airfoil

protein

sarcos

chem

sdss

10

20

400

500

600

700

400

600

400

450

500

1000

2000

3000

airfoil

protein

sarcos

chem

sdss

0.08

0.06

0.04

0.02

0

-1.5

-2

-2.5

-3

-3.5

-4

0.2

0.15

0.1

0.05

0
350

4

2

0

-2

-4

0.14

0.12

0.1

0.08

0.06

-1.3

-1.4

-1.5

-1.6

-1.7

-1.8

10

20

400

500

600

700

400

600

350

400

450

500

1000

2000

3000

Figure 6: Comparative results of VFE, SVGP, FITC, RBCM and GPoE on the ﬁve real-world datasets.
The results of full GP are provided for the airfoil dataset with n = 1200. Note that the GPoE has
no MSLL symbols on the protein dataset since it produces invalid MSLL values in all the ten runs.
Similarly, we only show ﬁve success runs of GPoE in terms of MSLL on the chem dataset.

0.6

0.55

0.5

0.45

0.4

0.35

-0.3

-0.4

-0.5

-0.6

-0.7

0

-10

-20

-30

Figure 7: The boxplots of log σ2

∗ estimated by FITC and GPoE in a run on the protein dataset.

FITC

GPoE

As for the two local scalable GPs, due to the individual experts, the GPoE captures
better predictive distributions on three out of the ﬁve datasets, and produces more ac-
curate predictions on the protein dataset. However, it is observed that the GPoE yields
invalid prediction variances on the protein and chem datasets due to the existence of local
over-ﬁtting. For instance, Fig. 7 depicts the log σ2
∗ values estimated respectively by FITC
and GPoE, since both of which can capture the heteroscedasticity in noise variance, on
the protein dataset. It is observed that in comparison to FITC, the GPoE produces many
extremely small prediction variances due to local over-ﬁtting. These extreme prediction
variances of GPoE, as low as nearly e−35, in turn bring invalid MSLL with the value up
to 6.59 × 1010. Besides, it is observed that compared to the RBCM, the individual treat-
ment of GP experts in GPoE often induces ill-conditioned kernel matrix, especially in the
scenario with many experts. This prohibits the model inference. The local over-ﬁtting
and the ill-conditioned phenomenon of GPoE could be alleviated by using a small M ,
i.e., large experts that can take into account the the spatial correlations. But this would
degrade the model capability and improve the complexity of GPoE. Hence, in practice

17

we prefer RBCM, unless there exist some tricks to guard against local over-ﬁtting and
ill-conditioned kernel matrix in GPoE.

Finally, the comparison between global and local scalable GPs shows that the local
attention mechanism enables RBCM to produce smaller MSLL values than that of VFE
and SVGP in four out of the ﬁve datasets, and smaller SMSE values in three out of the
ﬁve datasets.

5.2.2. Impact of model parameters

It is found that the model parameters, e.g., the inducing size m and the number M
of experts, aﬀect the performance of scalable GPs. More inducing points bring better
distillation of training data for sparse approximations, while more experts enhance the
localization by taking into account more local patterns for local aggregations. Hence,
this section seeks to run global and local scalable GPs using various model parameters in
order to investigate their controllability. i.e., explicitly controlling the model performance
by tuning m or M .

VFE

SVGP

FITC

RBCM

0.2

0.15

0.1

-0.8

-1

0.25

0.2

0.15

-0.7
-0.8
-0.9

0.3
0.25
0.2
0.15

-1

-1.2

-1.4

0.14
0.12
0.1
0.08
0.06
0.04

-1.2

-1.4

-1.6

20

40

60

80

20

40

60

80

20

40

60

80

15

20

25

30

VFE

SVGP

FITC

RBCM

20

40

60

80

20

40

60

80

20

40

60

80

15

20

25

30

Figure 8: Impact of varying inducing size m or number M of experts on the performance of scalable GPs
on the airfoil dataset. The shaded region represents two times the standard deviation over ten runs.

Fig. 8 depicts the modeling results of scalable GPs using diﬀerent m or M values
on the airfoil dataset. The results of scalable GPs on the remaining four datasets are
provided in the Appendix. Note that the GPoE is not included in the comparison due to
the unstable performance induced by local over-ﬁtting and ill-conditioned kernel matrix.
The results in Fig. 8 and the Appendix indicate that VFE, SVGP and FITC provide
It is because more inducing
better predictions with the increase of inducing size m.
points bring closer approximation to the full GP. This on the other hand indicates their
good controllability: we could improve the predictions by simply increasing m.

However, it is interesting to observe that the RBCM provides similar predictions with
diﬀerent M values on not only the airfoil dataset but also the protein and sarcos datasets
in the Appendix. This is caused by the local attention mechanism which considers local
features, thus enabling RBCM to provide better and more robust estimation of the
hyperparameters in comparison to VFE and FITC. More precisely, Fig. 9 depicts the
estimated noise variance σ2
ǫ of VFE and RBCM using diﬀerent model parameters on the
airfoil dataset. As the ground truth, the noise variance estimated by the full GP on this
dataset has an average value of σ2
ǫ = 0.0218. It is observed that by using diﬀerent M
values, the RBCM is always capable of providing a small σ2
ǫ close to that of full GP.
On the contrary, the VFE provides a too conservative σ2
ǫ when using a small inducing
size. The estimated σ2
ǫ becomes closer to that of full GP with increasing m at the cost
of higher computing complexity.

18

Figure 9: The estimated noise variance σ2
airfoil dataset. The shaded region represents two times the standard deviation over ten runs.

ǫ of VFE and RBCM using diﬀerent model parameters on the

In comparison to the conservative VFE, SVGP and FITC, the local attention helps
RBCM quickly obtain a good estimation of σ2
ǫ , which in turn encourages better predic-
tions on some datasets. However, this is often not the case for complicated datasets with
heteroscedastic noise, like the chem and sdss datasets. In these datasets, some subre-
gions with small noise variances favor a small estimation of σ2
ǫ , which however is not
beneﬁcial for other subregions with large noise variances. This inbalance becomes more
serious when M is large, since now we are forced to handle more localized regions. For
example, as shown in Fig. 14 in the Appendix, the RBCM provides poorer predictions
with the increase of M .

The inconsistent performance of RBCM w.r.t.

the number of experts on the ﬁve
datasets indicates a poor controllability of RBCM, since it is unclear how to improve its
performance by tuning M .

5.2.3. Impact of the initialization of hyperparameters

It is known that we train the GP by maximizing the marginal likelihood p(y), which
however is usually a non-convex optimization problem. Hence, this section investigates
the robustness of scalable GPs, including VFE, FITC and RBCM, to the initialization of
hyperparameters, including the SE kernel parameters {l1, · · · , ld, σ2
f } and the noise vari-
ance σ2
ǫ , on the airfoil dataset. Particularly, we randomly initialize the kernel parameters
and the noise variance as li ∼ random(0, 1), σ2
ǫ ∼ random(0, 0.5)
in order to produce 100 instances.

f ∼ random(0, 1), and σ2

1

0.8

0.6

0.4

0.2

0

-0.5

-1

-1.5

VFE

FITC

RBCM

VFE

FITC

RBCM

Figure 10: Impact of the initialization of hyperparameters on the performance of VFE, FITC and RBCM
on the airfoil dataset.

Fig. 10 depicts the boxplots of the three scalable GPs with respect to 100 initializa-
tions of hyperparameters on the airfoil dataset. Some of the outliers in the boxplots
19

represent the failure runs since they provide poor SMSE (close to one) and MSLL (close
to zero). Here, we deﬁne a failure run wherein the SMSE is larger than 0.8 and the
MSLL value is larger than -0.3. It is observed that the VFE fails in 19 out of the 100
runs, the FITC fails in 7 runs, and the RBCM has no failure run. The results indicate
that the RBCM is robust to various initializations of hyperparameters, because (i) as
explained before, it uses the local attention to help estimate the hyperparameters well;
and (ii) compared to VFE and FITC which have a large parameter space by considering
the additional inducing parameters, the RBCM has a narrow parameter space due to the
sharing of hyperparameters across experts.

6. Conclusions

This paper studies representative scalable GPs including the sparse approximations
and the local aggregations on two toy examples and ﬁve large-scale real-world datasets.
We summarize below their characteristics in terms of scalability, capability, robustness
and controllability.

For sparse approximations including the prior and posterior approximations, we have

the following ﬁndings from the numerical experiments:

• In terms of scalability, all the sparse approximations except SVGP have the same
time complexity of O(nm2). The SVGP reorganizes the variational lower bound
such that it factorizes over data points, thus reducing the complexity to O(bm3)
via SGD;

• In terms of capability, most of the prior approximations provide poorer predictions
than the posterior counterparts. Particularly, the FITC captures heteroscedastic
noise at the cost of worsening the accuracy of prediction mean. The posterior
approximations including VFE and SVGP are preferred since they are faithful
approximations of full GP.

• In terms of robustness, the sparse approximations are sensitive to the initialization
of hyperparameters, because of the augmented parameter space by considering
inducing and variational parameters.

• In terms of controllability, it is observed that VFE, SVGP and FITC generally oﬀer

better predictions with increasing m.

For local aggregations including GPoE and RBCM, we have the following ﬁndings

from the numerical experiments:

• In terms of scalability, the training complexity of local aggregations is the same
as most sparse approximations when we have the training size m0 = m for each
expert. But the test complexity is a bit higher since we need the predictions from
M experts.

• In terms of capability, the GPoE that allows individual hyperparameters for experts
is capable of capturing non-stationary features. However, with increasing M , the
local over-ﬁtting and the possible ill-conditioned kernel matrix would signiﬁcantly
degrade GPoE’s generalization capability, rendering it impractical. Contrarily, the

20

RBCM (i) shares hyperparameters across experts to guard against over-ﬁtting and
(ii) estimates the hyperparameters well due to the local attention mechanism.

• In terms of robustness, the RBCM is robust to the initialization of hyperparameters

on the airfoil dataset due to the local attention mechanism.

• In terms of controllability, it is unclear how to tune the number M of experts for

RBCM.

capability

VFE
SVGP
FITC
RBCM

1.0

0.8

0.6

0.4

0.2

robustness

scalability

controllability

Figure 11: The radar plot to illustrate the representative global and local scalable GPs in terms of
scalability, capability, robustness and controllability.

According to the above conclusions, Fig. 11 summarizes the characteristics of four
successful global/local scalable GPs including VFE, SVGP, FITC and RBCM in terms
of scalability, capability, robustness and controllability. To further improve the model
capability while retaining the scalability, alternatively, we may combine sparse and local
approximations together such that the hybrid could (i) guard against local over-ﬁtting,
and (ii) capture non-stationary features. Another promising avenue is the combination
of scalable GPs and the well-known feature extractor, deep neural networks, to further
boost the representational capability and scalability for big data [56, 57].

Acknowledgments

This work was conducted within the Rolls-Royce@NTU Corporate Lab with support
from the National Research Foundation (NRF) Singapore under the Corp Lab@University
Scheme. It is also partially supported by the Data Science and Artiﬁcial Intelligence Re-
search Center (DSAIR) and the School of Computer Science and Engineering at Nanyang
Technological University.

21

A. Appendix

Fig.s 12-15 below depict the impact of varying inducing size m or number M of experts
on the performance of scalable GPs on the protein, sarcos, chem and sdss datasets,
respectively.

VFE

SVGP

FITC

RBCM

200

300

400

500

200

300

400

500

200

300

400

500

20

40

60

80

100

VFE

SVGP

FITC

RBCM

200

300

400

500

200

300

400

500

200

300

400

500

20

40

60

80

100

Figure 12: Impact of varying inducing size m or number M of experts on the performance of scalable
GPs on the protein dataset.

VFE

SVGP

FITC

10-3

RBCM

200

300

400

500

200

300

400

500

200

300

400

500

60

80

100

120

VFE

SVGP

FITC

RBCM

200

300

400

500

200

300

400

500

200

300

400

500

40

60

80

100

120

Figure 13: Impact of varying inducing size m or number M of experts on the performance of scalable
GPs on the sarcos dataset.

VFE

SVGP

FITC

RBCM

100 200 300 400 500 600

100 200 300 400 500 600

100 200 300 400 500 600

50

100

150

200

VFE

SVGP

FITC

RBCM

100 200 300 400 500 600

100 200 300 400 500 600

100 200 300 400 500 600

50

100

150

200

Figure 14: Impact of varying inducing size m or number M of experts on the performance of scalable
GPs on the chem dataset.

0.55
0.5
0.45

-0.6

-0.7

10

9.5

9
40

-2.28
-2.3
-2.32
-2.34
-2.36
-2.38

0.04

0.02

6
4
2
0
-2

0.5

0.45

-0.35

-0.4

0.022
0.02
0.018

-1.9
-1.95
-2

0.15

0.1

0.05

-1

-1.5

0.48
0.46
0.44
0.42

-0.5

-0.55

-0.6

0.028
0.026
0.024
0.022
0.02
0.018

-2.1
-2.15
-2.2
-2.25

0.2

0.1

-1

-1.5

-2

0.5

0.45

-0.35

-0.4

0.2

0.1

0

-1
-1.5
-2

0.25
0.2
0.15
0.1
0.05

-1

-1.5

References

References

[1] F. Duchaine, T. Morel, L. Gicquel, Computational-ﬂuid-dynamics-based Kriging optimization tool

for aeronautical combustion chambers, AIAA Journal 47 (3) (2009) 631–645.

22

VFE

SVGP

FITC

RBCM

0.07

0.065

-1.35

-1.4

0.07

0.065

-1.35

-1.4

0.08

0.07

-1.72
-1.74
-1.76
-1.78
-1.8
-1.82

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

400

600

800

VFE

SVGP

FITC

RBCM

0.088
0.086
0.084
0.082

-1.46
-1.48
-1.5
-1.52
-1.54

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

400

600

800

Figure 15: Impact of varying inducing size m or number M of experts on the performance of scalable
GPs on the sdss dataset.

[2] X. Liu, Q. Zhu, H. Lu, Modeling multiresponse surfaces for airfoil design with multiple-output-

Gaussian-process regression, Journal of Aircraft 51 (3) (2014) 740–747.

[3] A. Amrit, L. Leifsson, S. Koziel, Y. A. Tesfahunegn, Eﬃcient multi-objective aerodynamic optimiza-
tion by design space dimension reduction and co-Kriging, in: 17th AIAA/ISSMO Multidisciplinary
Analysis and Optimization Conference, AIAA, 2016, pp. AIAA 2016–3515.

[4] N. Wagle, E. W. Frew, Forward adaptive transfer of Gaussian process regression, Journal of

Aerospace Information Systems 14 (4) (2017) 214–231.

[5] C. E. Rasmussen, C. K. Williams, Gaussian processes for machine learning, MIT Press, 2006.
[6] A. Liaw, M. Wiener, et al., Classiﬁcation and regression by randomForest, R news 2 (3) (2002)

[7] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (May) (2015) 436–444.
[8] R. M. Neal, Bayesian learning for neural networks, Vol. 118, Springer Science & Business Media,

18–22.

2012.

[9] H. Liu, Y.-S. Ong, J. Cai, A survey of adaptive sampling for global metamodeling in support of
simulation-based complex engineering design, Structural and Multidisciplinary Optimization 57 (1)
(2018) 393–416.

[10] D. Foreman-Mackey, E. Agol, S. Ambikasaran, R. Angus, Fast and scalable Gaussian process mod-
eling with applications to astronomical time series, The Astronomical Journal 154 (6) (2017) 220.
[11] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Prabhat,
R. Adams, Scalable Bayesian optimization using deep neural networks, in: International Conference
on Machine Learning, 2015, pp. 2171–2180.

[12] M. P. Deisenroth, J. W. Ng, Distributed Gaussian processes, in: International Conference on Ma-

chine Learning, JMLR. org, 2015, pp. 1481–1490.

[13] K. Chalupka, C. K. Williams, I. Murray, A framework for evaluating approximation methods for
Gaussian process regression, Journal of Machine Learning Research 14 (Feb) (2013) 333–350.
[14] J. Qui˜nonero-Candela, C. E. Rasmussen, A unifying view of sparse approximate Gaussian process

regression, Journal of Machine Learning Research 6 (Dec) (2005) 1939–1959.

[15] E. Snelson, Z. Ghahramani, Sparse Gaussian processes using pseudo-inputs, in: Advances in Neural

Information Processing Systems, 2006, pp. 1257–1264.

[16] M. K. Titsias, Variational learning of inducing variables in sparse Gaussian processes, in: Artiﬁcial

[17] J. Hensman, N. Fusi, N. D. Lawrence, Gaussian processes for big data, in: Uncertainty in Artiﬁcial

Intelligence and Statistics, 2009, pp. 567–574.

Intelligence, AUAI Press, 2013, pp. 282–290.

[18] T. D. Bui, R. E. Turner, Tree-structured Gaussian process approximations, in: Advances in Neural

Information Processing Systems, 2014, pp. 2213–2221.

[19] G. E. Hinton, Training products of experts by minimizing contrastive divergence, Neural Compu-

tation 14 (8) (2002) 1771–1800.

[20] V. Tresp, A Bayesian committee machine, Neural Computation 12 (11) (2000) 2719–2741.
[21] Y. Cao, D. J. Fleet, Generalized product of experts for automatic and principled fusion of Gaussian

process predictions, arXiv preprint arXiv:1410.7827.

[22] H. Liu, J. Cai, Y. Ong, Y. Wang, Generalized robust Bayesian committee machine for large-scale
Gaussian process regression, in: International Conference on Machine Learning, JMLR. org, 2018,
pp. 1–10.

23

[23] Y.-L. K. Samo, S. J. Roberts, String and membrane Gaussian processes, Journal of Machine Learn-

ing Research 17 (1) (2016) 4485–4571.

[24] A. J. Smola, P. L. Bartlett, Sparse greedy Gaussian process regression, in: Advances in Neural

Information Processing Systems, 2001, pp. 619–625.

[25] M. Seeger, C. Williams, N. Lawrence, Fast forward selection to speed up sparse Gaussian process
regression, in: Artiﬁcial Intelligence and Statistics, PMLR, 2003, pp. EPFL–CONF–161318.
[26] A. Dezfouli, E. V. Bonilla, Scalable inference for Gaussian process models with black-box likelihoods,

in: Advances in Neural Information Processing Systems, 2015, pp. 1414–1422.

[27] A. Wilson, H. Nickisch, Kernel interpolation for scalable structured Gaussian processes (KISS-GP),

in: International Conference on Machine Learning, 2015, pp. 1775–1784.

[28] C. E. Rasmussen, Z. Ghahramani, Inﬁnite mixtures of Gaussian process experts, in: Advances in

Neural Information Processing Systems, 2002, pp. 881–888.

[29] S. E. Yuksel, J. N. Wilson, P. D. Gader, Twenty years of mixture of experts, IEEE Transactions on

Neural Networks and Learning Systems 23 (8) (2012) 1177–1193.

[30] C. K. Williams, M. Seeger, Using the Nystr¨om method to speed up kernel machines, in: Advances

in Neural Information Processing Systems, 2001, pp. 682–688.

[31] B. W. Silverman, Some aspects of the spline smoothing approach to non-parametric regression curve
ﬁtting, Journal of the Royal Statistical Society. Series B (Methodological) 47 (1) (1985) 1–52.
[32] G. Wahba, X. Lin, F. Gao, D. Xiang, R. Klein, B. Klein, The bias-variance tradeoﬀ and the

randomized GACV, in: Advances in Neural Information Processing Systems, 1999, pp. 620–626.

[33] L. Csat´o, M. Opper, Sparse on-line Gaussian processes, Neural Computation 14 (3) (2002) 641–668.
[34] E. Snelson, Z. Ghahramani, Local and global sparse Gaussian process approximations, in: Artiﬁcial

Intelligence and Statistics, PMLR, 2007, pp. 524–531.

[35] M. K. Titsias, Variational model selection for sparse Gaussian process regression, Tech. rep., Uni-

versity of Manchester (2009).

[36] M. Bauer, M. van der Wilk, C. E. Rasmussen, Understanding probabilistic sparse Gaussian process
approximations, in: Advances in Neural Information Processing Systems, 2016, pp. 1533–1541.
[37] A. G. d. G. Matthews, J. Hensman, R. Turner, Z. Ghahramani, On sparse variational methods
and the Kullback-Leibler divergence between stochastic processes, Journal of Machine Learning
Research 51 (2016) 231–239.

[38] M. D. Hoﬀman, D. M. Blei, C. Wang, J. Paisley, Stochastic variational inference, Journal of Machine

Learning Research 14 (1) (2013) 1303–1347.

[39] M. D. Zeiler, ADADELTA: An adaptive learning rate method, arXiv preprint arXiv:1212.5701.
[40] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980.
[41] T. N. Hoang, Q. M. Hoang, B. K. H. Low, A unifying framework of anytime sparse Gaussian process
regression models with stochastic variational inference for big data, in: International Conference on
Machine Learning, 2015, pp. 569–578.

[42] T. Chen, J. Ren, Bagging for Gaussian process regression, Neurocomputing 72 (7-9) (2009) 1605–

1610.

[43] Y. Okadome, Y. Nakamura, Y. Shikauchi, S. Ishii, H. Ishiguro, Fast approximation method for
Gaussian process regression using hash function for non-uniformly distributed data, in: Interna-
tional Conference on Artiﬁcial Neural Networks, Springer, 2013, pp. 17–25.

[44] B. van Stein, H. Wang, W. Kowalczyk, T. B¨ack, M. Emmerich, Optimally weighted cluster Kriging
for big data regression, in: International Symposium on Intelligent Data Analysis, Springer, 2015,
pp. 310–321.

[45] B. van Stein, H. Wang, W. Kowalczyk, M. Emmerich, T. B¨ack, Cluster-based Kriging approximation

algorithms for complexity reduction, arXiv preprint arXiv:1702.01313.

[46] B. Szabo, H. van Zanten, An asymptotic analysis of distributed nonparametric methods, arXiv

[47] S. Mair, U. Brefeld, Distributed robust Gaussian process regression, Knowledge and Information

preprint arXiv:1711.03149.

Systems 55 (2) (2018) 415–435.

[48] Y. Gal, M. van der Wilk, C. E. Rasmussen, Distributed variational inference in sparse Gaussian pro-
cess regression and latent variable models, in: Advances in Neural Information Processing Systems,
Curran Associates, Inc., 2014, pp. 3257–3265.

[49] Z. Dai, A. Damianou, J. Hensman, N. Lawrence, Gaussian process models with parallelization and

GPU acceleration, arXiv preprint arXiv:1410.4984.

[50] J. Vanhatalo, V. Pietil¨ainen, A. Vehtari, Approximate inference for disease mapping with sparse

Gaussian processes, Statistics in Medicine 29 (15) (2010) 1580–1607.

[51] B.-J. Lee, J. Lee, K.-E. Kim, Hierarchically-partitioned Gaussian process approximation, in: Arti-
24

ﬁcial Intelligence and Statistics, 2017, pp. 822–831.

[52] J. Hensman, N. Durrande, A. Solin, Variational fourier features for Gaussian processes, arXiv

preprint arXiv:1611.06740.

[53] D. Dheeru, E. Karra Taniskidou, UCI machine learning repository (2017).

URL http://archive.ics.uci.edu/ml

[54] M. Malshe, L. Raﬀ, M. Rockley, M. Hagan, P. M. Agrawal, R. Komanduri, Theoretical investigation
of the dissociation dynamics of vibrationally excited vinyl bromide on an ab initio potential-energy
surface obtained using modiﬁed novelty sampling and feedforward neural networks. II. Numerical
application of the method, The Journal of Chemical Physics 127 (13) (2007) 134105.

[55] I. A. Almosallam, M. J. Jarvis, S. J. Roberts, GPz: non-stationary sparse Gaussian processes
for heteroscedastic uncertainty estimation in photometric redshifts, Monthly Notices of the Royal
Astronomical Society 462 (1) (2016) 726–739.

[56] W.-b. Huang, D. Zhao, F. Sun, H. Liu, E. Y. Chang, Scalable Gaussian process regression using deep
neural networks., in: International Joint Conference on Artiﬁcial Intelligence, 2015, pp. 3576–3582.
[57] A. G. Wilson, Z. Hu, R. Salakhutdinov, E. P. Xing, Deep kernel learning, in: Artiﬁcial Intelligence

and Statistics, 2016, pp. 370–378.

25

