0
2
0
2
 
n
a
J
 
0
3
 
 
]

D
S
.
s
c
[
 
 
1
v
3
6
2
1
1
.
1
0
0
2
:
v
i
X
r
a

Sound ﬁeld reconstruction in rooms: inpainting
meets superresolution

Francesc Llu´ıs,1, 2, 3, a) Pablo Mart´ınez-Nuevo,1 Martin Bo Møller,1 and Sven Ewan Shepstone1
1R&D Acoustics, Bang & Olufsen, Struer, 7600, Denmark
2Department of Music Acoustics, University of Music and Performing Arts Vienna, Austria
3Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain

(Dated: 31 January 2020)

In this paper a deep-learning-based method for sound ﬁeld reconstruction is proposed. It is
shown the possibility to reconstruct the magnitude of the sound pressure in the frequency
band 30-300 Hz for an entire room by using a very low number of irregularly distributed
microphones arbitrarily arranged. In particular, the presented approach uses a limited num-
ber of arbitrary discrete measurements of the magnitude of the sound ﬁeld pressure in order
to extrapolate this ﬁeld to a higher-resolution grid of discrete points in space with a low
computational complexity. The method is based on a U-net-like neural network with partial
convolutions trained solely on simulated data, i.e. the dataset is constructed from numerical
simulations of the Green’s function across thousands of common rectangular rooms. Although
extensible to three dimensions, the method focuses on reconstructing a two-dimensional plane
of the room from measurements of the three-dimensional sound ﬁeld. Experiments using sim-
ulated data together with an experimental validation in a real listening room are shown. The
results suggest a performance—in terms of mean squared error and structural similarity—
which may exceed conventional reconstruction techniques for a low number of microphones
and computational requirements.
c(cid:13)2020 Acoustical Society of America.

[http://dx.doi.org(DOI number)]

Pages: 1–11

[XYZ]

I. INTRODUCTION

The functions describing sound propagation, such as
sound pressure or particle velocity, operate scalar and
vector values respectively which vary across the tem-
poral and spatial dimensions. There exist many appli-
cations where knowledge of the spatial variation of the
sound ﬁeld is of paramount interest. For example, sound
ﬁeld navigation for virtual reality environments1,2, ac-
curate spatial sound ﬁeld reproduction over predeﬁned
regions of space3–5, or sound ﬁeld control in reverberant
environments6,7.

The diﬀerent reconstruction scenarios are deter-
mined by the type of information gathered from the
sound ﬁeld. Depending on the type of acquisition, several
techniques are used, ranging for example, from acous-
tic holography8, acousto-optic methods9,10, or traditional
discrete sets of spatial samples11. The latter is particu-
larly convenient in practice since it requires simple mi-
crophones.

In the case of sound ﬁeld reconstruction in rooms,
there exist several methods in the literature. In partic-
ular, model-based approaches based on samples of the
sound pressure at a discrete set of locations tend to
dominate the area. Results using classical sampling11—
i.e. based on bandwidth analysis—build upon the image

a)lluis-salvado@mdw.ac.at

source method to characterize the sound ﬁeld in a room in
order to derive bounds on the aliasing error for a given
sampling density. This leads to an impractically high
density of microphones for an acceptable reconstruction
error. Another approach to simplify the model and the
number of measurements is based on parameterizing the
room impulse response as a pole-zero system12.

Compressive sensing approaches have been eﬀective
in reducing the number of measurements compared to
these previous methods. They inherently require an un-
derlying assumption of the sparsity of the chosen room
acoustics model. Utilizing the modal theory, it is pos-
sible to consider a plane wave approximation of the
sound ﬁeld13 in a room in order to describe it spa-
tially as a sparse linear combination of damped complex
exponentials14–16. Dictionaries tend to be large, perfor-
mance degrades at high frequencies, and the interpolated
location should be, in general, in the far ﬁeld with respect
to the source. Under the image source method, estima-
tion of the early part of the room impulse response is
also possible assuming a few dominant image sources17.
These techniques are in general sensitive to the choice
of sampling scheme used in order to guarantee mean-
ingful solutions and well-conditioned problems. Empiri-
cal methods for the latter are commonly adopted leading
to some restrictions in the arrangement of microphones.
Exploiting information about the modal frequencies may
allow a more general microphone arrangement18 at the
expense of sensitivity to source location, modal density,

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

1

and accurate modal frequencies estimation. Addition-
ally, ﬁnding solutions to these sparse inverse problems is
typically computationally demanding19.

In this paper, we adopt a data-driven approach to
the problem of sound ﬁeld sampling and reconstruction,
which, for the present application, appears to be unex-
plored. For clarity of exposition, we focus on a two-
dimensional horizontal plane of three-dimensional rect-
angular rooms. We consider a very low number of irregu-
larly and arbitrarily distributed measurements to recover
the magnitude of the sound pressure in a room across
the spatial dimension for the frequency range 30-300 Hz.
The goal of the paper is aimed in three directions: use a
very low number of microphones, accommodate irregular
microphone distributions, and eﬃcient inference.

We ﬁrst view this ﬁeld as a two-dimensional discrete
It can be interpreted that the acquisition step
signal.
produces a low-resolution signal with missing samples.
Then, the recovery step consists of ﬁlling the missing data
of a high-resolution two-dimensional signal. We show
how this process can be viewed as jointly performing
inpainting20,21 and superresolution22,23, both well-known
techniques in image processing with a good performance
using deep learning methods. In particular, we use a U-
net neural network24 with partial convolutions21 trained
on simulated data that simultaneously performs inpaint-
ing and superresolution. Under this framework, we show
how it is possible to recover a high-resolution ﬁeld from a
very low number of microphones with low computational
complexity in the inference process.

The paper is organized as follows: Section II estab-
lishes the conceptual framework under which the recon-
struction problem is addressed, i.e. as a learning algo-
rithm drawing upon inpainting and superresolution tech-
niques. The details about the neural network architec-
ture and the training procedure used for recovery are
explained in Section III. Section IV presents results con-
cerning the reconstruction accuracy of the proposed algo-
rithm both in simulated and experimental settings—i.e.
in real rooms.

II. PROBLEM DESCRIPTION

We frame the problem of sound ﬁeld reconstruction
within a data-driven approach, i.e. we aim at devel-
oping a recovery algorithm that directly and progres-
sively learns from raw sound ﬁeld data. The machine
learning methods that have been particularly successful
in this regard fall under deep learning systems. These
have signiﬁcantly outperformed model-based approaches
in tasks such as, but not limited to, image classiﬁcation,
analysis, and restoration25–27; or speech recognition and
synthesis27,28.

The novelty of the present approach lies in the ob-
servation that the magnitude of the sound pressure in
a room can be interpreted as a two-dimensional dis-
crete function deﬁned on a rectangular grid of points
in space—i.e.
in the same way a raster image is rep-
resented by a rectangular grid of pixels. This allows us

(1)

(2)

to exploit the eﬀectiveness of deep learning techniques
in image processing. Although the principles governing
the proposed algorithm can, in principle, be extended to
three-dimensional regions, we focus on reconstructing the
three-dimensional ﬁeld in a two-dimensional plane for the
sake of simplicity. We further assume that the enclosures
of interest consist of rectangular rooms corresponding to
domestic standards29.

In particular, the function that we sample and recon-
struct is a discrete version of the magnitude of the Fourier
transform of the sound ﬁeld in a given frequency band.
We show in the following how reconstructing this func-
tion is connected to the well-known concepts of image
inpainting and superresolution. Let us ﬁrst denote the
spatio-temporal sound ﬁeld in a three-dimensional rect-
angular room as p(r, t) where R = (0, a) × (0, b) × (0, c)
for some a, b, c > 0 and r ∈ R. The magnitude of its
Fourier transform is given by

s(r, ω) :=

(cid:90)

(cid:12)
(cid:12)
(cid:12)

R

(cid:12)
p(r, t)e−jωtdt
(cid:12)
(cid:12)

for ω ∈ R and r ∈ R.

Initially, given a room, we can deﬁne the following
rectangular grid as a set on an arbitrary two-dimensional
plane, i.e.

Do :=

(cid:110)(cid:16)
i

a
I

, j

, zo

b
J

(cid:17)(cid:111)

i,j

for zo ∈ (0, c), i = 0, . . . , I, j = 0, . . . , J, and some inte-
gers I, J ≥ 2. Then, the available spatial sample points,
denoted as So, consist of a subset of Do.
It is impor-
tant to observe that there is no constraint whatsoever
with regard to the pattern that So has to form within
Do. This allows us to have, for example, irregularly dis-
tributed spatial sample points within the room. For a
given excitation frequency, the available samples can then
be expressed as follows

{s(r, ω)}r∈So⊆Do.

(3)

Note that the problem of interpolating s(r, ω) to the en-
tire domain Do from known values in So can be viewed
as image inpainting—i.e. ﬁlling in the missing holes of a
raster image. This is motivated by the irregular nature
of the sampling pattern.

However, we are interested in reconstruction on an
even ﬁner rectangular grid in order to capture the small-
scale spatial variations of the sound ﬁeld. In order to do
so, we eventually interpolate the sound ﬁeld to a grid of
points corresponding to an upsampled version of the set
Do, i.e.

DL,P
o

:=

(cid:110)(cid:16)
i

a
IL

, j

b
JP

(cid:17)(cid:111)

, zo

i,j

(4)

where i = 0, . . . , IL, j = 0, . . . , JP , and some integers
L, P ≥ 1.
In the signal processing community, recon-
structing a function on the domain DL,P
o —high resolu-
tion signal—from knowledge of the function on Do—low
resolution signal—is known as superresolution. Fig. 1 il-
lustrates how the diﬀerent sets—Do, DL,P
, and So—are

o

2

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

FIG. 1. Illustration of the spatial points considered for recon-
struction of the function s(r, ω) for a given frequency. The
set Do consists of the solid black and gray circles where the
former, for example, can be interpreted as So. The set DL,P
is then given by all the points depicted where inpainting and
superresolution is jointly performed from knowledge of the
function in So. Note that here L = P = 4.

o

(a)

(b)

placed under the inpainting and superresolution frame-
work.

In summary, we aim at designing an estimator gw
with the structure of a neural network where its param-
eters are real-valued weights w learned from simulated
data. In particular, for a given set of frequencies of in-
terest {ωk}K

k=1, the estimator is deﬁned as follows

gw : R|Do|×K

→ R|DL,P

o

|×K

{s(r, ωk)}r∈Do,k (cid:55)→ {ˆs(r, ωk)}r∈DL,P

,k.

o

(5)

The goal is then that the error

FIG. 2. Recovery of the magnitude of the Room Transfer
Function (RTF) in a rectangular room. (a) Microphones avail-
able for measurements. (b) Locations of the predictions pro-
vided by the learning algorithm.

into a tensor S ∈ RIL×JP ×K whose elements are given
by

Si+1,j+1,k := s

(cid:16)

i

a
IL

, j

b
JP

(cid:17)

, ωk

(7)

(cid:80)

r∈DL,P
o
(cid:80)

|s(r, ωk) − ˆs(r, ωk)|2
|s(r, ωk)|2

r∈DL,P
o

III. APPROACH

(6)

is reduced for each frequency point.

It is important to note that the available input val-
ues to the estimator are of dimension |So| × K, thus the
network should also be provided with information about
the available sample values and their relative locations
within Do. We address this in the following sections by
means of a mask on the original grid. Interestingly, the
relative distances among the microphone locations will
not be needed as an input to the algorithm. Irrespective
of the room dimensions, we assume that our algorithm
accepts measurements from a rectangular grid—whose
absolute size depends on the room size—in the same way
an image reconstruction algorithm would learn to recover
zoomed in or out input images.

We occasionally use tensors further below in order
to represent function values on discrete spatial and fre-
quency domains and as the data structure for the neu-
ral network operations. In particular, tensors, irrespec-
tive of their order, are denoted by bold uppercase let-
ters, e.g. matrices can be denoted by A ∈ Rn1×n2 for
n1, n2 ∈ N. Regarding function values, we interchange-
ably use the tensor representation. For example, consider
{s(r, ωk)}r∈DL,P
,k, then it possible to arrange its values

o

We propose a learning algorithm capable of estimat-
ing the magnitude of the spatial sound ﬁeld, for a given
frequency range, at a predeﬁned number of locations
based on very few measurements from irregularly dis-
tributed microphones. From the concepts introduced in
the previous section, a practical scenario could be de-
scribed as in Fig. 2. The microphones are assumed to
provide the room transfer functions (RTFs) at those par-
ticular locations for a given frequency range.
It is as-
sumed that these microphones are located in a rectangu-
lar grid with a predeﬁned number of points irrespective of
the room size. Then, the prediction algorithm provides
an estimate of the corresponding RTFs at the desired
locations, e.g. Fig. 2(b).

The approach is to train an artiﬁcial neural network
that learns the structure of these sound ﬁelds from thou-
sands of diﬀerent examples of common domestic rectan-
gular rooms.

The main parts of the algorithm, which we describe
in detail in the following sections and illustrate in Fig. 3,
can be brieﬂy summarized as follows:

• Dataset: we simulate three-dimensional sound
ﬁelds, in the frequency band [30,300] Hz, for thou-
sands of common rectangular rooms. The magni-

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

3

tude of the pressure in the available spatial sample
points So serves as input to the network after a pre-
processing step. The magnitude of the pressure in
the ﬁner rectangular grid, i.e. {s(r, ωk)}r∈DL,P
,k,
is then used to train the network in a supervised
manner.

o

• Data Preprocessing:

from {s(r, ωk)}r∈So,k, we
generate a grid version, deﬁned on DL,P
, consisting
o
of the observed samples and a mask that encodes
the information about the locations of these mea-
surements. This preprocessing step involves com-
pletion, scaling, and upsampling operations.

• Neural Network: The architecture learns to pre-
dict a scaled version of the two-dimensional func-
tion {s(r, ωk)}r∈DL,P
,k from the preprocessed ob-
served sample values {s(r, ωk)}r∈So,k and the mask.

o

• Data Postprocessing: Estimates the appropriate
scaling in order to restore the predicted values to
the range of the source data.

The data and code of the proposed algorithm is freely

available online30.

A. Dataset

o

We use the Green’s function31 to simulate point
source radiation in 5 000 rectangular rooms. Room size
and room proportions are randomly created following the
standards speciﬁed in ITU - R BS.1116 - 329. For each
room, the source is placed on the ﬂoor at a random lo-
cation, i.e. (xo, yo, 0)xo∈(0,a),yo∈(0,b). The magnitude of
the sound ﬁeld pressure is acquired in the ﬁner rectan-
gular grid DL,P
with L = P = 4, I = J = 8 and
zo = 0. This essentially divides the room into a grid
of 32 by 32 uniformly-spaced points. Given the domes-
tic standards29, this corresponds to an average distance
between grid samples of 14 cm in the x-axis and 26 cm
in the y-axis. We analyze the results with 1/12th oc-
tave frequency resolution in the range [30, 300] Hz. This
gives K = 40 frequency points. The sound ﬁelds gener-
ated using this technique are referred to as ground truth
sound ﬁelds, i.e. sGT (r, ωk) := s(r, ωk) for r ∈ DL,P
and k = 1, . . . K. A subset of sGT (r, ωk) contain-
ing the observed samples captured by the microphones,
{sGT (r, ωk)}r∈So,k, is used in the preprocessing part.

o

B. Preprocessing

This part addresses the processing stage necessary to
handle the arbitrary nature of the sampling distribution.
In particular, the raw input data is allowed to be variable
in size and sampling location. In order to address this,
we complete the input data to take values on Do. This is
followed by a scaling operation in order to generalize the
predictions for arbitrary sources and receivers. The ac-
tual information of where the samples are located within
DL,P
is encoded into a mask-like function. An upsam-
o

pled version of this processed input data together with
this mask comprise the ﬁnal input to the network.

1. Completion

We assume that the possible observed pressure values
correspond to locations within the coarser grid Do, which
also covers the whole room area. In this paper, the choice
of parameters results in Do being a grid of 8 by 8 points
with an average distance between samples of 56 cm in the
x-axis and 104 cm in the y-axis. The samples observed
are then given by {sGT (r, ωk)}r∈So,k. Irrespective of the
structure of So—i.e. the number and pattern of observed
samples—, the neural network is designed so that the
size of the input data is ﬁxed. In order to address this,
we introduce a function deﬁned on Do that, in a sense,
completes the acquired data, i.e.

(cid:40)

sc(r, ωk) :=

sGT (r, ωk)
if r ∈ So
maxr∈So sGT (r, ωk) if r (cid:54)∈ So.

(8)

for each ωk. In other words, for the locations where no
samples are provided—i.e. no microphone is present—,
sc is chosen to take the maximum value.

2. Scaling

We want the proposed method to be independent
of the gain in the measurement equipment and the re-
production system. Thus, we introduce a scaling for the
sample values sc in such a way that the range is restricted
to [0,1], i.e.

ss(r, ωk) :=

sc(r, ωk) − minr∈So sc(r, ωk)
maxr∈So sc(r, ωk) − minr∈So sc(r, ωk)

(9)

for each ωk. Consequently, the neural network will learn
to predict the sound ﬁeld values in [0,1]. A postpro-
cessing stage will be added so that the predictions are
restored to the original range.

3. Upsampling

Since we are interested in predicting values in the
, we transform ss ∈ R8×8×40
ﬁner rectangular grid, DL,P
to a function sirr ∈ R32×32×40 by means of an upsam-
pling operation. In particular, we have that

o

(cid:40)

sirr(r, ωk) :=

ss(r, ωk) if r ∈ Do
1

if r ∈ DL,P

o

\ Do

(10)

for each ωk. The original measurements are incorporated
into sc, however, the actual input values to the network
are given by sirr.

4. Mask generator

The function sirr does not provide any information
about which values have been originally observed. Thus,
we simultaneously generate a mask, deﬁned on the ﬁner

4

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

FIG. 3. Diagram showing the diﬀerent steps of the algorithm design. The data is assumed to be represented as third-order
tensors in order to include the frequency dimension and the spatial dimensions; however, for the sake of illustration, the former
is not shown. The preprocessing stage generates the input mask together with an upsampled and scaled version of the observed
samples. The training examples are also scaled. For our choice of parameters, the two input tensors and the training examples
take values in [0, 1]32×32×40. During training, the observed sample values are drawn from our simulated dataset of sound ﬁelds
in rooms. (Color online)

o

grid DL,P
, that carries information about the spatial lo-
cations of the measurements. This mask takes the value
1 at each available spatial sample point and 0 otherwise,
i.e.

m(r, ωk) :=

(cid:40)

1 if r ∈ So
0 if r ∈ DL,P

o

\ So

(11)

for all ωk.

5. Input

The input data to the network consists of third-
order tensors representing the frequency dimension and
the two spatial dimensions, i.e. M ∈ [0, 1]32×32×40 and
Sirr ∈ [0, 1]32×32×40. It is important to emphasize that
the network performs convolutions considering the three
dimensions in order to learn the relationships within and
between frequency and space.

C. Neural Network

1. Architecture

We propose a U-Net-like deep neural network32 with
partial convolutions33 in order to predict the magnitude
of the sound ﬁeld pressure in a room. The U-Net encoder-
decoder structure can learn multi-resolution features of

it
the sound ﬁeld in the frequency-space domain, i.e.
can capture the sound ﬁeld variations at diﬀerent scales
in both domains. This is carried out by the encoder
downsampling by 2 the feature map at each layer and
doubling the ﬁlter size. The decoder then reverses this
procedure by upsampling the feature map and down-
sampling by 2 the ﬁlter size. Moreover, the decoder—
via concatenation—incorporates at the same hierarchi-
cal level the feature maps and masks computed by the
encoder. In other words, the features from diﬀerent res-
olutions in the frequency-space domain are also utilized
as an input in the upsampling layers of the decoder. Fi-
nally, a 1×1 convolution projects the last feature map
to generate the predicted sound ﬁeld ˆSp. Fig. 4 shows a
schematic diagram of the architecture.

2. Partial Convolutions

Unlike

traditional

convolutions,

partial
convolutions33 allow us to compute the output fea-
ture maps based solely on the available spatial sample
points from the input feature maps. This provides the
necessary ﬂexibility to use any number of microphones
at irregularly distributed locations. Let w be the sliding
convolutional window with size kh ×kw. Consider further
Iw ∈ Rkh×kw×C and Mw ∈ [0, 1]kh×kw×C as correspond-

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

5

(15)

(16)

As a loss function, we use two terms in order to dis-
tinguish between predicted values in the available spatial
sample points So and its complement under DL,P
. We
ﬁrst deﬁne

o

LSo :=

sum(cid:0)(cid:12)

(cid:12)M (cid:12) (ˆSp − ¯S(cid:1)(cid:12)
(cid:12)
IL × JP × K

(cid:1)

and then

LDL,P

o

\So

:=

sum(cid:0)(cid:12)

(cid:12)(1 − M) (cid:12) (ˆSp − ¯SGT )(cid:12)
(cid:12)

(cid:1)

IL × JP × K

where 1 ∈ R32×32×40 with all entries equal to 1, and
sum(| · |) acting on a tensor is the summation of the ab-
solute value of its elements. The combined loss function
ﬁnally takes the form

L := LSo + 12LDL,P

o

\So

.

(17)

The best performing loss weights were found after hyper-
parameter search on 1000 validation rooms.

4. Training Procedure

The model is trained in two diﬀerent stages using su-
pervised learning. We use 75% of the dataset for training
purposes and the remaining 25% is used for validation.
In the ﬁrst stage, the learning rate is set to 2 · 10−4 and
batch normalization is enabled in all layers. For the sec-
ond stage, the learning rate is set to 5 · 10−5 with batch
normalization disabled in all encoding layers. Training
the model in multiple stages helps to overcome the er-
ror generated when computing, in the ﬁrst stage, the
mean and variance for all input values—corresponding
to known and unknown locations—by batch normaliza-
tion. In addition, faster convergence is achieved.

D. Postprocessing

We use linear regression to restore the output of the
neural network ˆsp to its original range. Thus, the rescaled
version takes the form

ˆs(r, ωk) = ak · ˆsp(r, ωk) + bk

(18)

for all r ∈ DL,P
and k = 1, . . . , K, where the values
ak, bk ∈ R are determined through the following opti-
mization problem

o

min
ak,bk∈R

(cid:88)

r∈So

|ak · ˆsp(r, ωk) + bk − sc(r, ωk)|2

(19)

for each k = 1, . . . , K. Note that the rescaling oper-
ation could be implemented as another neural network
that learns the mapping function. However, experiments
showed that linear regression provided reasonable perfor-
mance.

FIG. 4. Schematic diagram of the neural network architecture
proposed in this paper. (Color online)

ing to the C-channel
input feature maps and mask
within w respectively. The tensor W ∈ Rkh×kw×C(cid:48)×C
respresents the ﬁlter weights and b ∈ RC(cid:48)
is the bias.
Partial convolution computes each spatial location value
o(cid:48) ∈ RC(cid:48)
(cid:40)

in the C (cid:48)-channel output feature maps as

sum(Mw) + b if sum(Mw) > 0

W · (Iw (cid:12) Mw) sum(1)
0

otherwise

o(cid:48) :=

(12)
where sum(·) receives a tensor as an argument and
provides the summation of
its elements, (cid:12) is the
Hadamard product, and · is a combination—in diﬀer-
ent dimensions—of matrix dot products and element-
sum(1)
wise summations21. The scaling factor
sum(Mw) can be
interpreted as a measure of the amount of known infor-
mation in the input feature maps. Then, the mask Mw
is updated at each spatial location m(cid:48) ∈ RC(cid:48)
as follows:
(cid:40)

m(cid:48) =

1 if sum(Mw) > 0
0 otherwise.

(13)

3. Loss Function

In order to train the model in a supervised manner,
we also use a scaled version of the ground truth in order to
be consistent with the output data before postprocessing.
The assumption is that this process may also assist the
learning process. The scaling is given by

¯sGT :=

sGT (r, ωk) − min sGT (r, ωk)
max sGT (r, ωk) − min sGT (r, ωk)

(14)

IV. RESULTS

A. Evaluation Metrics

for r ∈ DL,P
¯sGT (r, ωk) ∈ [0, 1].

o

and k = 1, . . . , K.

It is clear then that

We use two diﬀerent measures of performance for
the proposed method. First, we consider the normalized

6

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

mean square error (NMSE) computed for each frequency
point, i.e.

NMSEk =

(cid:80)

r∈DL,P
o
(cid:80)

|s(r, ωk) − ˆs(r, ωk)|2
|s(r, ωk)|2

.

r∈DL,P
o

(20)

The NMSE mainly provides an average absolute squared
error over all locations between the reconstructed and
the original signals. As a consequence, a high NMSE
value may result from a poor performance locally while
performing individually well in the remaining spatial lo-
cations.

Therefore, we use the concept of mean structural
similarity34 (MSSIM) from image processing. This eval-
uates how the model predicts the overall shape of the
pressure distribution for each frequency point. Moreover,
it also provides a measure of performance that is inde-
pendent of the scaling chosen. Let us ﬁrst introduce the
structural similarity index (SSIM) between two matrices
A, B ∈ Rn×n as follows

SSIM(A, B) =

(2µAµB + c1)(2σAB + c2)
A + µ2

B + c1)(σ2

A + σ2

B + c2)

(µ2

(21)

where µ is the mean of the corresponding matrix entries,
σ2 the estimate of the variance of the entries, and σAB
is the covariance estimate between the entries of A and
B. The constants c1 = (h1R)2 and c2 = (h2R)2, where
R is the dynamic range of the entry values, are meant to
stabilize the division with a weak denominator. We set
h1 and h2 to 0.01 and 0.03 respectively.

In our scenario, we consider the individual matri-
ces Sk ∈ RIL×JP , i.e.
the k-th matrix of tensor S ∈
RIL×JP ×K. Now, let {Sn
k (η)}N
n=1 denote the set of all
possible windowed versions of Sk of size η × η. The mean
structural similarity is then given by

MSSIM(Sk, ˆSk) :=

SSIM(Sn

k (η), ˆSn

k (η))

(22)

1
N

N
(cid:88)

n=1

for each frequency point.
have used η = 7.

In the results presented, we

B. Simulated Data

We asses the reconstruction performance of the pro-
posed method—i.e.
the generalization error—by using
sound ﬁelds simulated in 30 diﬀerent rooms. We are in-
terested in evaluating the performance with regard to
the number of irregularly placed microphones, denoted
by nmic. Thus, given nmic, we analyze the reconstruc-
tion in each room placing the microphones in 10 000 dif-
ferent arrangements, i.e. each realization corresponds to
a diﬀerent So. Figures 5 and 6 show, as a function of
frequency, the average NMSE in dB and MSSIM for all
rooms and locations tested and diﬀerent number of avail-
able microphones.

Results show a general

improved performance in
sound ﬁeld reconstruction as the number of available mi-
crophones is increased. At the same time, performance

FIG. 5. Normalized mean squared error (NMSE) estimated
from simulated data. The results are reported for diﬀerent
number of microphone observations nmic, i.e. (
):nmic = 5,
):nmic = 55. (Color
(
online)

):nmic = 35, and (

):nmic = 15, (

FIG. 6. Mean structural similarity index (MSSIM) estimated
from simulated data. The results are reported for diﬀerent
number of microphone observations nmic, i.e. (
):nmic = 5,
(
):nmic = 55. (Color
online)

):nmic = 35, and (

):nmic = 15, (

degrades as the frequency increases. This is in agreement
with theoretical results that, given a maximum frequency
content, require a higher sampling density for a more
robust reconstruction and, given a reconstruction error,
the sampling density constraints also increase whenever
higher frequency content is available11,35. This suggests
that the neural network capacity is subject to the same
physical limitations as classical methods when learning
the spatial variations of the pressure distribution, i.e. at
high frequencies is hindered by undersampling and it also
requires more observations to improve robustness. For
example, the relative improvement as the number of mi-
crophones increase is higher at lower frequencies as op-
posed to the high-frequency range where more observa-
tions do not provide a big impact on performance. How-
ever, the requirements in terms of sampling density for
a particular performance seem to be less stringent than
other methods present in the literature. For example,

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

7

FIG. 7. Normalized mean square error (NMSE) in dB esti-
mated from experimental data. Top and bottom plots cor-
respond to diﬀerent source locations. The results are re-
ported for diﬀerent number of microphone observations nmic,
):nmic = 35, and
i.e.
(

):nmic = 55. (Color online)

):nmic = 15, (

):nmic = 5, (

(

FIG. 8. Mean structural similarity (MSSIM) estimated from
experimental data. Top and bottom plots correspond to dif-
ferent source locations. The results are reported for diﬀerent
number of microphone observations nmic, i.e. (
):nmic = 5,
):nmic = 55. (Color
(
online)

):nmic = 35, and (

):nmic = 15, (

only nmic = 5 microphones are able to provide an NMSE
below −5 dB for the frequency range considered in com-
mon domestic rooms.

It is also important to observe that the L1 loss used
during the training stage is suitable for prediction at
low frequencies but it underperforms at high frequencies.
This could be interpreted as the L1 loss resulting in pre-
dictions emphasizing the median value in order to reduce
the overall error. This can explain, in the frequency range
100-300 Hz, the more abrupt changes in performance of
the MSSIM as opposed to the NMSE.

C. Experimental Data

We test the model optimized for simulated data in
a real listening room. The RTFs are estimated for two
diﬀerent source locations on a two-dimensional grid con-
sisting of 32 by 32 points uniformly spaced along the cor-
responding dimensions. In particular, impulse response
measurements were conducted from two 10” loudspeak-
ers on a grid one meter above the ﬂoor in a rectangular
room of dimensions 4.16 × 6.46 × 2.3 m. The measure-
ments were performed using 4-second duration exponen-
tial sweeps from 0.1 Hz to 24 kHz at a sampling frequency
of 48 kHz36. These measurements were performed with

two microphones, each covering roughly half of the grid.
The microphones were a Br¨uel & Kjær (B&K) 4192 and a
B&K 4133 1
2 ” condenser microphone connected to a B&K
Nexus conditioning ampliﬁer and recorded with an RME
Fireface UFX+ sound card. Both microphones were level
calibrated at 1 kHz using a B&K 4231 calibrator prior to
the measurements. The reverberation time of the room,
speciﬁed as the arithmetic average of the 1/3 octave T20
estimates37 in the range of 32 Hz to 316 Hz, was 0.46 s.
Similar to the previous scenario, we investigate the
performance of the model with regard to the number of
microphones placed in the room. We are particularly in-
terested in assessing the performance when using very
few observations. Thus, for each predeﬁned source loca-
tion, we also use here 5, 15, 35, and 55 microphones in
10 000 diﬀerent arrangements and analyze the mean per-
formance with a 95% conﬁdence interval. These results
are reported in Figures 7 and 8.

It is important to emphasize that the model to be
evaluated with experimental data was trained using sim-
ulated data. Moreover, the horizontal plane, relative to
the ﬂoor, in the experimental data does not correspond
It can be ob-
to its counterpart in the training data.
served that, given nmic, the NMSE improves for decreas-
ing frequencies as a general trend although there exist

8

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

FIG. 9. Visualization of the model reconstruction when using 5 microphones arbitrarily placed. The results are shown for
diﬀerent frequencies in a real room where the source location is the same as the top plots in Figures 7 and 8. (Color online)

inconsistencies at a local level, i.e. adjacent frequencies
may present abrupt changes in performance. The same
interpretation applies to the MSSIM. In particular, there
are two speciﬁc frequencies acting as outliers, i.e. 82 Hz
and 157 Hz for the two diﬀerent source locations. In this
case, this is likely to be caused by the sources being po-
sitioned at nulls of the room modes. Fig. 9 depicts a rep-
resentation of the magnitude of the sound ﬁeld when the
reconstruction is performed using only 5 microphones.

Despite the mismatch between the training and test
scenarios, the network shows promising results under un-
seen data. Note that the simulated dataset used is usu-
ally an oversimpliﬁed model of the sound ﬁeld behavior
in a real room. It has been shown that the structure of
convolutional neural networks already represents a prior
conditioning the network to perform well in image-like
signals38. The magnitude of the spatial sound ﬁeld nat-
urally ﬁts the latter. Further, it can also be interpreted
as a transfer learning39 approach where the architecture
itself helps to generalize well in the experimental scenario
from weights only learned with simulated data.

D. Computational Complexity

Apart from the reduced number of microphones used,
another advantage of the proposed method is the com-
putational complexity regarding the inference operation.
The training stage is usually time consuming, but it can
often be run oﬄine. The model size is relatively small
with 3.9 million parameters resulting in a determinis-
tic inference time of approximately 0.05 s on a Nvidia
GeForce GTX 1080 Ti GPU—value estimated from 100
diﬀerent room predictions.

FIG. 10. Best and worst performing sampling distributions
for 6 microphones in terms of NMSE performance. The results
are shown for diﬀerent frequencies in a real room where the
source location is the same as the top plots in Figures 7 and
8. Symbol ( ) represents the microphone locations. (Color
online)

It can be observed that a better reconstruction at a spe-
ciﬁc frequency is achieved when the microphones capture
the maximum variation of the pressure values. On the
contrary, if the observations consist solely of the dip-like
part of the room modes, the reconstruction degrades sig-
niﬁcantly. Evidently, this eﬀect is frequency dependent,
thus there is not a microphone setup that performs well
across all frequencies. However, this also suggests that
an unstructured microphone arrangement may be more
likely to avoid these sampling issues caused by the modal
structure.

V. CONCLUSIONS

E. Microphone Distribution

In our analysis, we have mainly focused on the per-
formance based on the number of observations. However,
we are also interested in studying the impact that partic-
ular microphone distributions have on the performance.
Fig. 10 shows an illustration of the best and worst per-
forming microphone distributions in terms of the NMSE.

In this paper, a deep-learning-based method for
sound ﬁeld reconstruction in rectangular rooms has been
proposed and examined. The method jointly performs in-
painting and superresolution in order to reconstruct the
magnitude of the sound pressure in a two-dimensional
plane of a three-dimensional room. The focus of this work
is threefold: use a very low number of microphones, ac-
commodate irregular microphone distributions, and eﬃ-

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

9

cient inference. The results suggest a performance which
oﬀers advantages in these three directions, e.g. even us-
ing 5 microphones arbitrarily placed the method provides
an acceptable reconstruction error with a low inference
time.

Regarding future work, the study of generative ad-
versarial networks as discriminators may help to increase
In addition, us-
the performance at high frequencies.
ing more complex acoustic simulation models during the
training stage could overcome performance inconsisten-
cies at a local level as well as providing a lower general-
ization error when using experimental data.

ACKNOWLEDGMENTS

This project has received funding from the Euro-
pean Union’s Horizon 2020 research and innovation pro-
gramme under the Marie Sk(cid:32)lodowska-Curie grant agree-
ment No 812719.

1J. G. Tylka and E. Choueiri, “Comparison of techniques for bin-
aural navigation of higher-order ambisonic soundﬁelds,” in Audio
Engineering Society Convention 139, Audio Engineering Society
(2015).
2J. G. Tylka and E. Choueiri, “Soundﬁeld navigation using an
array of higher-order ambisonics microphones,” in Audio Engi-
neering Society Conference: 2016 AES International Conference
on Audio for Virtual and Augmented Reality, Audio Engineering
Society (2016).
3A. J. Berkhout, D. de Vries, and P. Vogel, “Acoustic control by
wave ﬁeld synthesis,” The Journal of the Acoustical Society of
America 93(5), 2764–2778 (1993).
4W. Druyvesteyn and J. Garas, “Personal sound,” Journal of the
Audio Engineering Society 45(9), 685–701 (1997).
5D. B. Ward and T. D. Abhayapala, “Reproduction of a plane-
wave sound ﬁeld using an array of loudspeakers,” IEEE Trans-
actions on speech and audio processing 9(6), 697–707 (2001).
6T. Betlehem and T. D. Abhayapala, “Theory and design of sound
ﬁeld reproduction in reverberant rooms,” The Journal of the
Acoustical Society of America 117(4), 2100–2111 (2005).
7B. D. Radlovic, R. C. Williamson, and R. A. Kennedy, “Equal-
ization in an acoustic reverberant environment: Robustness re-
sults,” IEEE Transactions on Speech and Audio Processing 8(3),
311–319 (2000).
8E. G. Williams, Fourier acoustics: sound radiation and nearﬁeld
acoustical holography (Elsevier, 1999).
9A. Torras-Rosell, S. Barrera-Figueroa, and F. Jacobsen, “Sound
ﬁeld reconstruction using acousto-optic tomography,” The Jour-
nal of the Acoustical Society of America 131(5), 3786–3793
(2012).

10E. Fernandez-Grande, A. Torras-Rosell, and F. Jacobsen, “Holo-
graphic reconstruction of sound ﬁelds based on the acousto-
optic eﬀect,” in INTER-NOISE and NOISE-CON Congress and
Conference Proceedings, Institute of Noise Control Engineering
(2013), Vol. 247, pp. 3181–3190.

11T. Ajdler, L. Sbaiz, and M. Vetterli, “The plenacoustic func-
tion and its sampling,” IEEE transactions on Signal Processing
54(10), 3790–3804 (2006).

12Y. Haneda, Y. Kaneda, and N. Kitawaki, “Common-acoustical-
pole and residue model and its application to spatial interpolation
and extrapolation of a room transfer function,” IEEE Transac-
tions on Speech and Audio Processing 7(6), 709–717 (1999).
13A. Moiola, R. Hiptmair, and I. Perugia, “Plane wave approxima-
tion of homogeneous helmholtz solutions,” Zeitschrift f¨ur ange-
wandte Mathematik und Physik 62(5), 809 (2011).

14R. Mignot, G. Chardon, and L. Daudet, “Low frequency inter-
polation of room impulse responses using compressed sensing,”
IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing 22(1), 205–216 (2013).

15N. Antonello, E. De Sena, M. Moonen, P. A. Naylor, and
T. van Waterschoot, “Room impulse response interpolation us-
ing a sparse spatio-temporal representation of the sound ﬁeld,”
IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing 25(10), 1929–1941 (2017).

16S. A. Verburg and E. Fernandez-Grande, “Reconstruction of the
sound ﬁeld in a room using compressive sensing,” The Journal of
the Acoustical Society of America 143(6), 3770–3779 (2018).
17R. Mignot, L. Daudet, and F. Ollivier, “Room reverberation re-
construction: Interpolation of the early part using compressed
sensing,” IEEE Transactions on Audio, Speech, and Language
Processing 21(11), 2301–2312 (2013).

18E. F. Grande, “Sound ﬁeld reconstruction in a room from spa-
tially distributed measurements,” in 23rd International Congress
on Acoustics, German Acoustical Society (DEGA) (2019), pp.
4961–68.

19S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky,
“An interior-point method for large-scale (cid:96)1-regularized least
squares,” IEEE journal of selected topics in signal processing
1(4), 606–617 (2007).

20M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester, “Image in-
painting,” in Proceedings of the 27th annual conference on Com-
puter graphics and interactive techniques, ACM Press/Addison-
Wesley Publishing Co. (2000), pp. 417–424.

21G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and
B. Catanzaro, “Image inpainting for irregular holes using par-
tial convolutions,” in Proceedings of the European Conference
on Computer Vision (ECCV) (2018), pp. 85–100.

22W. T. Freeman, T. R. Jones, and E. C. Pasztor, “Example-
based super-resolution,” IEEE Computer graphics and Applica-
tions 22(2), 56–65 (2002).

23Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual
dense network for image super-resolution,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition
(2018), pp. 2472–2481.

24O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
networks for biomedical image segmentation,” in International
Conference on Medical image computing and computer-assisted
intervention, Springer (2015), pp. 234–241.

25K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition (2016), pp. 770–778.
26F. Chollet, “Xception: Deep learning with depthwise separable
convolutions,” in Proceedings of the IEEE conference on com-
puter vision and pattern recognition (2017), pp. 1251–1258.
27Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss,
N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., “Tacotron:
Towards end-to-end speech synthesis,” Proc. Interspeech 2017
4006–4010 (2017).

28A. B. Nassif, I. Shahin, I. Attili, M. Azzeh, and K. Shaalan,
“Speech recognition using deep neural networks: A systematic
review,” IEEE Access 7, 19143–19165 (2019).

29I. T. Union, “Recommendation itu-r bs. 1116-3: Methods for the
subjective assessment of small impairments in audio systems,”
(2015).

30See supplementary material at github.com/francesclluis/

sound-field-neural-network.

31F. Jacobsen and P. M. Juhl, Fundamentals of general

linear

acoustics (John Wiley & Sons, 2013).

32O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
networks for biomedical image segmentation,” in International
Conference on Medical image computing and computer-assisted
intervention, Springer (2015), pp. 234–241.

33G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and
B. Catanzaro, “Image inpainting for irregular holes using par-

10

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

tial convolutions,” in Proceedings of the European Conference
on Computer Vision (ECCV) (2018), pp. 85–100.

34Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, et al., “Im-
age quality assessment: from error visibility to structural simi-
larity,” IEEE transactions on image processing 13(4), 600–612
(2004).

35H. J. Landau, “Necessary density conditions for sampling and
interpolation of certain entire functions,” Acta Mathematica
117(1), 37–52 (1967).

36A. Farina, “Simultaneous measurement of impulse response and
distortion with a swept-sine technique,” in Audio Engineering
Society Convention 108, Audio Engineering Society (2000).
37I. 3382-2, “Acoustics measurement of room acoustic parameters

part 2: Reverberation time in ordinary rooms,” (2008).

38D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,”
in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (2018), pp. 9446–9454.

39I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning (MIT

Press, 2016), http://www.deeplearningbook.org.

J. Acoust. Soc. Am. / 31 January 2020

JASA/Sample JASA Article

11

