Automated Phrase Mining from Massive Text Corpora

Jingbo Shang1, Jialu Liu2, Meng Jiang1, Xiang Ren1, Clare R Voss3, Jiawei Han1
1Computer Science Department, University of Illinois at Urbana-Champaign, IL, USA
2Google Research, New York City, NY, USA
3Computational & Information Sciences Directorate, Army Research Laboratory
2jialu@google.com

1{shang7, mjiang89, xren7, hanj}@illinois.edu

3clare.r.voss.civ@mail.mil

7
1
0
2
 
r
a

M
 
1
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
5
4
4
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
As one of the fundamental tasks in text analysis, phrase
mining aims at extracting quality phrases from a text corpus.
Phrase mining is important in various tasks such as informa-
tion extraction/retrieval, taxonomy construction, and topic
modeling. Most existing methods rely on complex, trained
linguistic analyzers, and thus likely have unsatisfactory per-
formance on text corpora of new domains and genres without
extra but expensive adaption. Recently, a few data-driven
methods have been developed successfully for extraction of
phrases from massive domain-speciﬁc text. However, none
of the state-of-the-art models is fully automated because
they require human experts for designing rules or labeling
phrases.

Since one can easily obtain many quality phrases from
public knowledge bases to a scale that is much larger than
that produced by human experts, in this paper, we propose
a novel framework for automated phrase mining, AutoPhrase,
which leverages this large amount of high-quality phrases in
an eﬀective way and achieves better performance compared
to limited human labeled phrases. In addition, we develop a
POS-guided phrasal segmentation model, which incorporates
the shallow syntactic information in part-of-speech (POS)
tags to further enhance the performance, when a POS tagger
is available. Note that, AutoPhrase can support any language
as long as a general knowledge base (e.g., Wikipedia) in that
language is available, while beneﬁting from, but not requiring,
a POS tagger. Compared to the state-of-the-art methods,
the new method has shown signiﬁcant improvements in eﬀec-
tiveness on ﬁve real-world datasets across diﬀerent domains
and languages.

1.

INTRODUCTION

Phrase mining refers to the process of automatic extrac-
tion of high-quality phrases (e.g., scientiﬁc terms and general
entity names) in a given corpus (e.g., research papers and
news). Representing the text with quality phrases instead

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

of n-grams can improve computational models for applica-
tions such as information extraction/retrieval, taxonomy
construction, and topic modeling.

Almost all the state-of-the-art methods, however, require
human experts at certain levels. Most existing methods [9,
20, 25] rely on complex, trained linguistic analyzers (e.g.,
dependency parsers) to locate phrase mentions, and thus
may have unsatisfactory performance on text corpora of new
domains and genres without extra but expensive adaption.
Our latest domain-independent method SegPhrase [13] out-
performs many other approaches [9, 20, 25, 5, 19, 6], but
still needs domain experts to ﬁrst carefully select hundreds
of varying-quality phrases from millions of candidates, and
then annotate them with binary labels.

Such reliance on manual eﬀorts by domain and linguis-
tic experts becomes an impediment for timely analysis of
massive, emerging text corpora in speciﬁc domains. An
ideal automated phrase mining method is supposed to be
domain-independent, with minimal human eﬀort or reliance
on linguistic analyzers1. Bearing this in mind, we propose
a novel automated phrase mining framework AutoPhrase in
this paper, going beyond SegPhrase, to further get rid of ad-
ditional manual labeling eﬀort and enhance the performance,
mainly using the following two new techniques.
1. Robust Positive-Only Distant Training.

In fact, many
high-quality phrases are freely available in general knowl-
edge bases, and they can be easily obtained to a scale
that is much larger than that produced by human ex-
perts. Domain-speciﬁc corpora usually contain some qual-
ity phrases also encoded in general knowledge bases, even
when there may be no other domain-speciﬁc knowledge
bases. Therefore, for distant training, we leverage the
existing high-quality phrases, as available from general
knowledge bases, such as Wikipedia and Freebase, to get
rid of additional manual labeling eﬀort. We independently
build samples of positive labels from general knowledge
bases and negative labels from the given domain corpora,
and train a number of base classiﬁers. We then aggregate
the predictions from these classiﬁers, whose independence
helps reduce the noise from negative labels.

2. POS-Guided Phrasal Segmentation. There is a trade-
oﬀ between the performance and domain-independence
when incorporating linguistic processors in the phrase
mining method. On the domain independence side, the

1The phrase “minimal human eﬀort” indicates using only
existing general knowledge bases without any other human
eﬀort.

accuracy might be limited without linguistic knowledge. It
is diﬃcult to support multiple languages, if the method is
completely language-blind. On the accuracy side, relying
on complex, trained linguistic analyzers may hurt the
domain-independence of the phrase mining method. For
example, it is expensive to adapt dependency parsers to
special domains like clinical reports. As a compromise, we
propose to incorporate a pre-trained part-of-speech (POS)
tagger to further enhance the performance, when it is
available for the language of the document collection. The
POS-guided phrasal segmentation leverages the shallow
syntactic information in POS tags to guide the phrasal
segmentation model locating the boundaries of phrases
more accurately.
In principle, AutoPhrase can support any language as long
as a general knowledge base in that language is available. In
fact, at least 58 languages have more than 100,000 articles
in Wikipedia as of Feb, 20172. Moreover, since pre-trained
part-of-speech (POS) taggers are widely available in many
languages (e.g., more than 20 languages in TreeTagger [22]3),
the POS-guided phrasal segmentation can be applied in many
scenarios. It is worth mentioning that for domain-speciﬁc
knowledge bases (e.g., MeSH terms in the biomedical domain)
and trained POS taggers, the same paradigm applies. In
this study, without loss of generality, we only assume the
availability of a general knowledge base together with a
pre-trained POS tagger.

As demonstrated in our experiments, AutoPhrase not only
works eﬀectively in multiple domains like scientiﬁc papers,
business reviews, and Wikipedia articles, but also supports
multiple languages, such as English, Spanish, and Chinese.

Our main contributions are highlighted as follows:

• We study an important problem, automated phrase mining,

and analyze its major challenges as above.

• We propose a robust positive-only distant training method
for phrase quality estimation to minimize the human eﬀort.
• We develop a novel phrasal segmentation model to leverage
POS tags to achieve further improvement, when a POS
tagger is available.

• We demonstrate the robustness and accuracy of our method
and show improvements over prior methods, with results
of experiments conducted on ﬁve real-world datasets in
diﬀerent domains (scientiﬁc papers, business reviews, and
Wikipedia articles) and diﬀerent languages (English, Span-
ish, and Chinese).

The rest of the paper is organized as follows. Section 2
positions our work relative to existing works. Section 3
deﬁnes basic concepts including four requirements of phrases.
The details of our method are covered in Section 4. Extensive
experiments and case studies are presented in Section 5. We
conclude the study in Section 7.

2. RELATED WORK

Identifying quality phrases eﬃciently has become ever
more central and critical for eﬀective handling of massively
increasing-size text datasets. In contrast to keyphrase ex-
traction [17, 23, 14], this task goes beyond the scope of
single documents and provides useful cross-document sig-
nals. The natural language processing (NLP) community

2https://meta.wikimedia.org/wiki/List_of_Wikipedias
3http://www.cis.uni-muenchen.de/~schmid/tools/
TreeTagger/

has conducted extensive studies typically referred to as au-
tomatic term recognition [9, 20, 25], for the computational
task of extracting terms (such as technical phrases). This
topic also attracts attention in the information retrieval (IR)
community [7, 19] since selecting appropriate indexing terms
is critical to the improvement of search engines where the
ideal indexing units represent the main concepts in a corpus,
not just literal bag-of-words.

Text indexing algorithms typically ﬁlter out stop words and
restrict candidate terms to noun phrases. With pre-deﬁned
part-of-speech (POS) rules, one can identify noun phrases
as term candidates in POS-tagged documents. Supervised
noun phrase chunking techniques [21, 24, 3] exploit such
tagged documents to automatically learn rules for identifying
noun phrase boundaries. Other methods may utilize more
sophisticated NLP technologies such as dependency parsing
to further enhance the precision [11, 16]. With candidate
terms collected, the next step is to leverage certain statistical
measures derived from the corpus to estimate phrase quality.
Some methods rely on other reference corpora for the calibra-
tion of “termhood” [25]. The dependency on these various
kinds of linguistic analyzers, domain-dependent language
rules, and expensive human labeling, makes it challenging to
extend these approaches to emerging, big, and unrestricted
corpora, which may include many diﬀerent domains, topics,
and languages.

To overcome this limitation, data-driven approaches opt
instead to make use of frequency statistics in the corpus to
address both candidate generation and quality estimation [5,
19, 6, 13]. They do not rely on complex linguistic feature
generation, domain-speciﬁc rules or extensive labeling eﬀorts.
Instead, they rely on large corpora containing hundreds
of thousands of documents to help deliver superior perfor-
mance [13]. In [19], several indicators, including frequency
and comparison to super/sub-sequences, were proposed to
extract n-grams that reliably indicate frequent, concise con-
cepts. Deane [5] proposed a heuristic metric over frequency
distribution based on Zipﬁan ranks, to measure lexical associ-
ation for phrase candidates. As a preprocessing step towards
topical phrase extraction, El-Kishky et al. [6] proposed to
mine signiﬁcant phrases based on frequency as well as doc-
ument context following a bottom-up fashion, which only
considers a part of quality phrase criteria, popularity and
concordance. Our previous work [13] succeeded at integrat-
ing phrase quality estimation with phrasal segmentation to
further rectify the initial set of statistical features, based on
local occurrence context. Unlike previous methods which
are purely unsupervised,
[13] required a small set of phrase
labels to train its phrase quality estimator. It is worth men-
tioning that all these approaches still depend on the human
eﬀort (e.g., setting domain-sensitive thresholds). Therefore,
extending them to work automatically is challenging.

3. PRELIMINARIES

The goal of this paper is to develop an automated phrase
mining method to extract quality phrases from a large collec-
tion of documents without human labeling eﬀort, and with
only limited, shallow linguistic analysis. The main input to
the automated phrase mining task is a corpus and a knowl-
edge base. The input corpus is a textual word sequence
in a particular language and a speciﬁc domain, of arbitrary
length. The output is a ranked list of phrases with decreasing
quality.

Figure 1: The overview of AutoPhrase. The two novel techniques developed in this paper are highlighted.

The AutoPhrase framework is shown in Figure 1. The
work ﬂow is completely diﬀerent form our previous domain-
independent phrase mining method requiring human ef-
fort [13], although the phrase candidates and the features
used during phrase quality (re-)estimation are the same. In
this paper, we propose a robust positive-only distant train-
ing to minimize the human eﬀort and develop a POS-guided
phrasal segmentation model to improve the model perfor-
mance. In this section, we brieﬂy introduce basic concepts
and components as preliminaries.

A phrase is deﬁned as a sequence of words that appear
consecutively in the text, forming a complete semantic unit
in certain contexts of the given documents [8]. The phrase
quality is deﬁned to be the probability of a word sequence
being a complete semantic unit, meeting the following crite-
ria [13]:
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Concordance: The collocation of tokens in quality phrases
occurs with signiﬁcantly higher probability than expected
due to chance.

• Informativeness: A phrase is informative if it is indica-

tive of a speciﬁc topic or concept.

• Completeness: Long frequent phrases and their subse-
quences within those phrases may both satisfy the 3 criteria
above. A phrase is deemed complete when it can be inter-
preted as a complete semantic unit in some given document
context. Note that a phrase and a subphrase contained
within it, may both be deemed complete, depending on
the context in which they appear. For example, “relational
database system”, “relational database” and “database sys-
tem” can all be valid in certain context.

AutoPhrase will estimate the phrase quality based on the
positive and negative pools twice, once before and once
after the POS-guided phrasal segmentation. That is, the
POS-guided phrasal segmentation requires an initial set of
phrase quality scores; we estimate the scores based on raw
frequencies beforehand; and then once the feature values
have been rectiﬁed, we re-estimate the scores.

Only the phrases satisfying all above requirements are

recognized as quality phrases.

Example 1. “strong tea” is a quality phrase while “heavy
tea” fails to be due to concordance. “this paper” is a popular
and concordant phrase, but is not informative in research
publication corpus. “NP-complete in the strong sense” is a
quality phrase while “NP-complete in the strong” fails to be
due to completeness. (cid:3)

To automatically mine these quality phrases, the ﬁrst phase
of AutoPhrase (see leftmost box in Figure 1) establishes the
set of phrase candidates that contains all n-grams over

the minimum support threshold τ (e.g., 30) in the corpus.
Here, this threshold refers to raw frequency of the n-grams
calculated by string matching.
In practice, one can also
set a phrase length threshold (e.g., n ≤ 6) to restrict the
number of words in any phrase. Given a phrase candidate
w1w2 . . . wn, its phrase quality is:

Q(w1w2 . . . wn) = p(dw1w2 . . . wnc|w1w2 . . . wn) ∈ [0, 1]
where dw1w2 . . . wnc refers to the event that these words
constitute a phrase. Q(·), also known as the phrase quality
estimator, is initially learned from data based on statistical
features4, such as point-wise mutual information, point-wise
KL divergence, and inverse document frequency, designed
to model concordance and informativeness mentioned above.
Note the phrase quality estimator is computed independent
of POS tags. For unigrams, we simply set their phrase quality
as 1.

Example 2. A good quality estimator will return Q(this paper) ≈

0 and Q(relational database system) ≈ 1. (cid:3)

Then, to address the completeness criterion, the phrasal
segmentation ﬁnds the best segmentation for each sentence.

Example 3. Ideal phrasal segmentation results are as fol-

lows.

(cid:3)

#1:
... / the / Great Firewall / is / ...
#2: This / is / a / great / ﬁrewall software/ .
#3: The / discriminative classiﬁer / SVM / is / ...

During the phrase quality re-estimation, related sta-
tistical features will be re-computed based on the rectiﬁed
frequency of phrases, which means the number of times that
a phrase becomes a complete semantic unit in the identiﬁed
segmentation. After incorporating the rectiﬁed frequency, the
phrase quality estimator Q(·) also models the completeness
in addition to concordance and informativeness.

Example 4. Continuing the previous example, the raw
frequency of the phrase “great ﬁrewall” is 2, but its rectiﬁed
frequency is 1. Both the raw frequency and the rectiﬁed
frequency of the phrase “ﬁrewall software” are 1. The raw
frequency of the phrase “classiﬁer SVM” is 1, but its rectiﬁed
frequency is 0. (cid:3)

4. METHODOLOGY

In this section, we focus on introducing our two new tech-

niques.
4See https://github.com/shangjingbo1226/AutoPhrase for
further details

Figure 2: The illustration of each base classiﬁer. In
each base classiﬁer, we ﬁrst randomly sample K pos-
itive and negative labels from the pools respectively.
There might be δ quality phrases among the K neg-
ative labels. An unpruned decision tree is trained
based on this perturbed training set.
4.1 Robust Positive-Only Distant Training

To estimate the phrase quality score for each phrase can-
didate, our previous work [13] required domain experts to
ﬁrst carefully select hundreds of varying-quality phrases from
millions of candidates, and then annotate them with binary
labels. For example, for computer science papers, our domain
experts provided hundreds of positive labels (e.g., “spanning
tree” and “computer science”) and negative labels (e.g., “pa-
per focuses” and “important form of ”). However, creating
such a label set is expensive, especially in specialized do-
mains like clinical reports and business reviews, because this
approach provides no clues for how to identify the phrase
candidates to be labeled.
In this paper, we introduce a
method that only utilizes existing general knowledge bases
without any other human eﬀort.

4.1.1 Label Pools

Public knowledge bases (e.g., Wikipedia) usually encode
a considerable number of high-quality phrases in the titles,
keywords, and internal links of pages. For example, by ana-
lyzing the internal links and synonyms5 in English Wikipedia,
more than a hundred thousand high-quality phrases were
discovered. As a result, we place these phrases in a positive
pool.

Knowledge bases, however, rarely, if ever, identify phrases
that fail to meet our criteria, what we call inferior phrases.
An important observation is that the number of phrase can-
didates, based on n-grams (recall leftmost box of Figure 1),
is huge and the majority of them are actually of of inferior
quality (e.g., “Francisco opera and”).
In practice, based
on our experiments, among millions of phrase candidates,
usually, only about 10% are in good quality. Therefore,
phrase candidates that are derived from the given corpus
but that fail to match any high-quality phrase derived from
the given knowledge base, are used to populate a large but
noisy negative pool.

4.1.2 Noise Reduction

Directly training a classiﬁer based on the noisy label pools
is not a wise choice: some phrases of high quality from
the given corpus may have been missed (i.e., inaccurately
binned into the negative pool) simply because they were
Instead, we propose
not present in the knowledge base.
to utilize an ensemble classiﬁer that averages the results
of T independently trained base classiﬁers. As shown in
Figure 2, for each base classiﬁer, we randomly draw K phrase
candidates with replacement from the positive pool and the
negative pool respectively (considering a canonical balanced
classiﬁcation scenario). This size-2K subset of the full set of

5https://github.com/kno10/WikipediaEntities

all phrase candidates is called a perturbed training set [2],
because the labels of some (δ in the ﬁgure) quality phrases
In order for the
are switched from positive to negative.
ensemble classiﬁer to alleviate the eﬀect of such noise, we
need to use base classiﬁers with the lowest possible training
errors. We grow an unpruned decision tree to the point of
separating all phrases to meet this requirement.
In fact,
such decision tree will always reach 100% training accuracy
when no two positive and negative phrases share identical
feature values in the perturbed training set. In this case,
δ
its ideal error is
2K , which approximately equals to the
proportion of switched labels among all phrase candidates
δ
2K ≈ 10%). Therefore, the value of K is not sensitive
(i.e.,
to the accuracy of the unpruned decision tree and is ﬁxed as
100 in our implementation. Assuming the extracted features
are distinguishable between quality and inferior phrases, the
empirical error evaluated on all phrase candidates, p, should
be relatively small as well.

An interesting property of this sampling procedure is that
the random selection of phrase candidates for building per-
turbed training sets creates classiﬁers that have statistically
independent errors and similar erring probability [2, 15].
Therefore, we naturally adopt random forest [10], which is
veriﬁed, in the statistics literature, to be robust and eﬃcient.
The phrase quality score of a particular phrase is computed
as the proportion of all decision trees that predict that phrase
is a quality phrase. Suppose there are T trees in the random
forest, the ensemble error can be estimated as the probability
of having more than half of the classiﬁers misclassifying a
given phrase candidate as follows.

ensemble_ error(T ) =

pt(1 − p)T −t

T
X

(cid:19)

(cid:18)T
t

t=b1+T /2c

From Figure 3, one can
easily observe that the en-
semble error is approach-
ing 0 when T grows. In
practice, T needs to be
set larger due to the ad-
ditional error brought by
model bias. Empirical
studies can be found in
Figure 7.

Figure 3: Ensemble er-
rors of diﬀerent p’s vary-
ing T .

4.2 POS-Guided Phrasal Segmentation

Phrasal segmentation addresses the challenge of measuring
completeness (our fourth criterion) by locating all phrase
mentions in the corpus and rectifying their frequencies ob-
tained originally via string matching.

The corpus is processed as a length-n POS-tagged word
sequence Ω = Ω1Ω2 . . . Ωn, where Ωi refers to a pair con-
sisting of a word and its POS tag hwi, tii. A POS-guided
phrasal segmentation is a partition of this sequence into
m segments induced by a boundary index sequence B =
{b1, b2, . . . , bm+1} satisfying 1 = b1 < b2 < . . . < bm+1 =
n+1. The i-th segment refers to Ωbi Ωbi+1 . . . Ωbi+1−1.

Compared to the phrasal segmentation in our previous
work [13], the POS-guided phrasal segmentation addresses
the completeness requirement in a context-aware way, instead
of equivalently penalizing phrase candidates of the same
length. In addition, POS tags provide shallow, language-
speciﬁc knowledge, which may help boost phrase detection

Algorithm 1: POS-Guided Phrasal Segmentation (PGPS)

Input: Corpus Ω = Ω1Ω2 . . . Ωn, phrase quality Q,
parameters θu and δ(tx, ty).
Output: Optimal boundary index sequence B.
// hi ≡ maxB p(Ω1Ω2 . . . Ωi−1, B|Q, θ, δ)
h1 ← 1, hi ← 0 (1 < i ≤ n + 1)
for i = 1 to n do

for j = i + 1 to min(i + length threshold, n + 1) do

// Efficiently implemented via Trie.
if there is no phrase starting with w[i,j) then

break

// In practice, log and addition are used

to avoid underflow.

if hi × p(j, dw[i,j)c|i, t[i,j)) > hj then
hj ← hi × p(j, dw[i,j)c|i, t[i,j))
gj ← i
j ← n + 1, m ← 0
while j > 1 do
m ← m + 1
bm ← j
j ← gj

return B ← 1, bm, bm−1, . . . , b1

accuracy, especially at syntactic constituent boundaries for
that language.

Given the POS tag sequence for the full n-length corpus
is t = t1t2 . . . tn, containing the tag subsequence tl . . . tr−1
(denote as t[l,r) for clarity), the POS quality score for that
tag subsequence is deﬁned to be the conditional probability of
its corresponding word sequence being a complete semantic
unit. Formally, we have

T (t[l,r)) = p(dwl . . . wrc|t) ∈ [0, 1]

The POS quality score T (·) is designed to reward the phrases
with their correctly identiﬁed POS sequences, as follows.

Example 5. Suppose the whole POS tag sequence is “NN
NN NN VB DT NN”. A good POS sequence quality estima-
tor might return T (NN NN NN) ≈ 1 and T (NN VB) ≈ 0,
where NN refers to singular or mass noun (e.g., database),
VB means verb in the base form (e.g., is), and DT is for
determiner (e.g., the). (cid:3)

The particular form of T (·) we have chosen is:

T (t[l,r)) = (1 − δ(tbr −1, tbr )) ×

δ(tj−1, tj)

r−1
Y

j=l+1

where, δ(tx, ty) is the probability that the POS tag tx is
adjacent and precedes POS tag ty within a phrase in the given
document collection. In this formula, the ﬁrst term indicates
the probability that there is a phrase boundary between the
words indexed r − 1 and r, while the latter product indicates
the probability that all POS tags within t[l,r) are in the
same phrase. This POS quality score can naturally counter
the bias to longer segments because ∀i > 1, exactly one of
δ(ti−1, ti) and (1 − δ(ti−1, ti)) is always multiplied no matter
how the corpus is segmented. Note that the length penalty
model in our previous work [13] is a special case when all
values of δ(tx, ty) are the same.

Mathematically, δ(tx, ty) is deﬁned as:

δ(tx, ty) = p(d. . . w1w2 . . .c|Ω, tag(w1) = tx ∧ tag(w2) = ty)

As it depends on how documents are segmented into phrases,
δ(tx, ty) is initialized uniformly and will be learned during
the phrasal segmentation.

Now, after we have both phrase quality Q(·) and POS
quality T (·) ready, we are able to formally deﬁne the POS-
guided phrasal segmentation model. The joint probability of
a POS tagged sequence Ω and a boundary index sequence
B = {b1, b2, . . . , bm+1} is factorized as:

p(Ω, B) =

p

bi+1, dw[bi,bi+1)c

(cid:17)

(cid:12)
(cid:12)
(cid:12)bi, t

m
Y

(cid:16)

i=1

where p(bi+1, dw[bi,bi+1)c|bi, t) is the probability of observing
a word sequence w[bi,bi+1) as the i-th quality segment given
the previous boundary index bi and the whole POS tag
sequence t.

Since the phrase segments function as a constituent in the
syntax of a sentence, they usually have weak dependence on
each other [8, 13]. As a result, we assume these segments in
the word sequence are generated one by one for the sake of
both eﬃciency and simplicity.

For each segment, given the POS tag sequence t and the

start index bi, the generative process is deﬁned as follows.
1. Generate the end index bi+1, according to its POS quality

p(bi+1|bi, t) = T (t[bi,bi+1))

2. Given the two ends bi and bi+1, generate the word se-
quence w[bi,bi+1) according to a multinomial distribution
over all segments of length-(bi+1 − bi).

p(w[bi,bi+1)|bi, bi+1) = p(w[bi,bi+1)|bi+1 − bi)

3. Finally, we generate an indicator whether w[bi,bi+1) forms

a quality segment according to its quality

p(dw[bi,bi+1)c|w[bi,bi+1)) = Q(w[bi,bi+1))
We denote p(w[bi,bi+1)|bi+1 −bi) as θw[bi,bi+1) for convenience.
Integrating the above three generative steps together, we
have the following probabilistic factorization:

p(bi+1, dw[bi,bi+1)c|bi, t)

=p(bi+1|bi, t)p(w[bi,bi+1)|bi, bi+1)p(dw[bi,bi+1)c|w[bi,bi+1))
=T (t[bi,bi+1))θw[bi,bi+1) Q(w[bi,bi+1))

Therefore, there are three subproblems:

1. Learn θu for each word and phrase candidate u;
2. Learn δ(tx, ty) for every POS tag pair; and
3. Infer B when θu and δ(tx, ty) are ﬁxed.

We employ the maximum a posterior principle and maxi-

mize the joint log likelihood:

log p(Ω, B) =

log p

(cid:16)

(cid:12)
(cid:12)
(cid:12)bt, t
bi+1, dw[bi,bi+1)c

(cid:17)

(1)

m
X

i=1

Given θu and δ(tx, ty), to ﬁnd the best segmentation that
maximizes Equation (1), we develop an eﬃcient dynamic
programming algorithm for the POS-guided phrasal segmen-
tation as shown in Algorithm 1.

When the segmentation S and the parameter θ are ﬁxed,

the closed-form solution of δ(tx, ty) is:

δ(tx, ty) =

Pm

i=1

Pbi+1−2
j=bi

Pn−1

1(tj = tx ∧ tj+1 = ty)

(2)

i=1 1(ti = tx ∧ ti+1 = ty)

Algorithm 2: Viterbi Training (VT)

Input: Corpus Ω and phrase quality Q.
Output: θu and δ(tx, ty).
initialize θ with normalized raw frequencies in the corpus
while θu does not converge do

while δ(tx, ty) does not converge do

B ← best segmentation via Alg. 1
update δ(tx, ty) using B according to Eq. (2)

B ← best segmentation via Alg. 1
update θu using B according to Eq. (3)

return θu and δ(tx, ty)

where 1(·) denotes the identity indicator. δ(tx, ty) is the
unsegmented ratio among all htx, tyi pairs in the given corpus.
Similarly, once the segmentation S and the parameter δ
are ﬁxed, the closed-form solution of θu can be derived as:

θu =

Pm
Pm

i=1 1(w[bi,bi+1) = u)
i=1 1(bi+1 − bi = |u|)

(3)

We can see that θu is the times that u becomes a com-
plete segment normalized by the number of the length-|u|
segments.

As shown in Algorithm 2, we choose Viterbi Training, or
Hard EM in literature [1] to iteratively optimize parameters,
because Viterbi Training converges fast and results in sparse
and simple models for Hidden Markov Model-like tasks [1].

4.3 Complexity Analysis

The time complexity of the most time consuming compo-
nents in our framework, such as frequent n-gram, feature
extraction, POS-guided phrasal segmentation, are all O(|Ω|)
with the assumption that the maximum number of words in
a phrase is a small constant (e.g., n ≤ 6), where |Ω| is the
total number of words in the corpus. Therefore, AutoPhrase
is linear to the corpus size and thus being very eﬃcient and
scalable. Meanwhile, every component can be parallelized
in an almost lock-free way grouping by either phrases or
sentences.

5. EXPERIMENTS

In this section, we will apply the proposed method to mine
quality phrases from ﬁve massive text corpora across three
domains (scientiﬁc papers, business reviews, and Wikipedia
articles) and in three languages (English, Spanish, and Chi-
nese). We compare the proposed method with many other
methods to demonstrate its high performance. Then we
explore the robustness of the proposed positive-only distant
training and its performance against expert labeling. The
advantage of incorporating POS tags in phrasal segmentation
will also be proved. In the end, we present case studies.

5.1 Datasets

To validate that the proposed positive-only distant train-
ing can eﬀectively work in diﬀerent domains and the POS-
guided phrasal segmentation can support multiple languages
eﬀectively, we have ﬁve large collections of text in diﬀerent
domains and languages, as shown in Table 1: Abstracts
of English computer science papers from DBLP6, English

6https://aminer.org/citation

Table 1: Five real-world massive text corpora in dif-
|Ω| is the
ferent domains and multiple languages.
total number of words. sizep is the size of positive
pool.

Dataset
DBLP
Yelp
EN
ES
CN

Domain
Scientiﬁc Paper
Business Review
Wikipedia Article
Wikipedia Article
Wikipedia Article

Language
English
English
English
Spanish
Chinese

|Ω|

sizep
File size
29K
91.6M 618MB
145.1M 749MB
22K
808.0M 3.94GB 184K
65K
791.2M 4.06GB
29K
371.9M 1.56GB

business reviews from Yelp7, Wikipedia articles8 in English
(EN), Spanish (ES), and Chinese (CN). From the exist-
ing general knowledge base Wikipedia, we extract popular
mentions of entities by analyzing intra-Wiki citations within
Wiki content9. On each dataset, the intersection between the
extracted popular mentions and the generated phrase candi-
dates forms the positive pool. Therefore, the size of positive
pool may vary in diﬀerent datasets of the same language.

5.2 Compared Methods

We compare AutoPhrase with three lines of methods as

follows. Every method returns a ranked list of phrases.
SegPhrase10/WrapSegPhrae11: In English domain-speciﬁc
text corpora, our latest work SegPhrase outperformed phrase
mining [6], keyphrase extraction [23, 19], and noun phrase
chunking methods. WrapSegPhrase extends SegPhrase to dif-
ferent languages by adding an encoding preprocessing to
ﬁrst transform non-English corpus using English characters
and punctuation as well as a decoding postprocessing to
later translate them back to the original language. Both
methods require domain expert labors. For each dataset, we
ask domain experts to annotate a representative set of 300
quality/interior phrases.
Parser-based Phrase Extraction: Using complicated lin-
guistic processors, such as parsers, we can extract minimum
phrase units (e.g., NP) from the parsing trees as phrase
candidates. Parsers of all three languages are available in
Stanford NLP tools [18, 4, 12]. Two ranking heuristics are
considered:
• TF-IDF ranks the extracted phrases by their term fre-
quency and inverse document frequency in the given docu-
ments;

• TextRank: An unsupervised graph-based ranking model

for keyword extraction [17].

Pre-trained Chinese Segmentation Models: Diﬀerent
from English and Spanish, phrasal segmentation in Chi-
nese has been intensively studied because there is no space
between Chinese words. The most eﬀective and popular
segmentation methods are:
• AnsjSeg12 is a popular text segmentation algorithm for
Chinese corpus. It ensembles statistical modeling methods
of Conditional Random Fields (CRF) and Hidden Markov
Models (HMMs) based on the n-gram setting;

• JiebaPSeg13 is a Chinese text segmentation method im-

7https://www.yelp.com/dataset_challenge
8https://dumps.wikimedia.org/
9https://github.com/kno10/WikipediaEntities
10https://github.com/shangjingbo1226/SegPhrase
11https://github.com/remenberl/SegPhrase-MultiLingual
12https://github.com/NLPchina/ansj_seg
13https://github.com/fxsjy/jieba

(a) DBLP

(b) Yelp

(c) EN

(d) ES

(e) CN

Figure 4: Overall Performance Evaluation: Precision-recall curves of all methods evaluated by human anno-
tation.

plemented in Python. It builds a directed acyclic graph
for all possible phrase combinations based on a preﬁx
dictionary structure to achieve eﬃcient phrase graph scan-
ning. Then it uses dynamic programming to ﬁnd the most
probable combination based on the phrase frequency. For
unknown phrases, an HMM-based model is used with the
Viterbi algorithm.

Note that all parser-based phrase extraction and Chinese
segmentation models are pre-trained based on general corpus.
To study the eﬀectiveness of the POS-guided segmenta-
tion, AutoSegPhrase adopts the length penalty instead of
δ(tx, ty), while other components are the same as AutoPhrase.
AutoSegPhrase is useful when there is no POS tagger.

5.3 Experimental Settings
Implementation. The preprocessing includes tokenizers
from Lucene and Stanford NLP as well as the POS tagger
from TreeTagger. Our documented code package has been
released and maintained in GitHub14.
Default Parameters. We set the minimum support thresh-
old σ as 30. The maximum number of words in a phrase is
set as 6 according to labels from domain experts. These are
two parameters required by all methods. Other parameters
required by compared methods were set according to the
open-source tools or the original papers.
Human Annotation. We rely on human evaluators to
judge the quality of the phrases which cannot be identiﬁed
through any knowledge base. More speciﬁcally, on each
dataset, we randomly sample 500 such phrases from the
predicted phrases of each method in the experiments. These
selected phrases are shuﬄed in a shared pool and evaluated
by 3 reviewers independently. We allow reviewers to use
search engines when unfamiliar phrases encountered. By the
rule of majority voting, phrases in this pool received at least
two positive annotations are quality phrases. The intra-class
correlations (ICCs) are all more than 0.9 on all ﬁve datasets,
which shows the agreement.
Evaluation Metrics. For a list of phrases, precision is
deﬁned as the number of true quality phrases divided by the
number of predicted quality phrases; recall is deﬁned as the
number of true quality phrases divided by the total number
of quality phrases. We retrieve the ranked list of the pool
from the outcome of each method. When a new true quality
phrase encountered, we evaluate the precision and recall of
this ranked list.
In the end, we plot the precision-recall
curves. In addition, area under the curve (AUC) is adopted
as another quantitative measure. AUC in this paper refers
to the area under the precision-recall curve.

14https://github.com/shangjingbo1226/AutoPhrase

5.4 Overall Performance

Figures 4 presents the precision-recall curves of all com-
pared methods evaluated by human annotation on ﬁve datasets.
Overall, AutoPhrase performs the best, in terms of both pre-
cision and recall. Signiﬁcant advantages can be observed,
especially on two non-English datasets ES and CN. For
example, on the ES dataset, the recall of AutoPhrase is
about 20% higher than the second best method (SegPhrase)
in absolute value. Meanwhile, there is a visible precision
gap between AutoPhrase and the best baseline. The phrase
chunking-based methods TF-IDF and TextRank work poorly,
because the extraction and ranking are modeled separately
and the pre-trained complex linguistic analyzers fail to ex-
tend to domain-speciﬁc corpora. TextRank usually starts
with a higher precision than TF-IDF, but its recall is very
low because of the sparsity of the constructed co-occurrence
graph. TF-IDF achieves a reasonable recall but unsatisfac-
tory precision. On the CN dataset, the pre-trained Chinese
segmentation models, JiebaSeg and AnsjSeg, are very com-
petitive, because they not only leverage training data for
segmentations, but also exhaust the engineering work, in-
cluding a huge dictionary for popular Chinese entity names
and speciﬁc rules for certain types of entities. As a conse-
quence, they can easily extract tons of well-known terms
and people/location names. Outperforming such a strong
baseline further conﬁrms the eﬀectiveness of AutoPhrase.

The comparison among the English datasets across three
domains (i.e., scientiﬁc papers, business reviews, and Wikipedia
articles) demonstrates that AutoPhrase is reasonably domain-
independent. The performance of parser-based methods
TF-IDF and TextRank depends on the rigorous degree of
the documents. For example, it works well on the DBLP
dataset but poorly on the Yelp dataset. However, without
any human eﬀort, AutoPhrase can work eﬀectively on domain-
speciﬁc datasets, and even outperforms SegPhrase, which is
supervised by the domain experts.

The comparison among the Wikipedia article datasets in
three languages (i.e., EN, ES, and CN ) shows that, ﬁrst
of all, AutoPhrase supports multiple languages. Secondly,
the advantage of AutoPhrase over SegPhrase/WrapSegPhrase
is more obvious on two non-English datasets ES and CN
than the EN dataset, which proves that the helpfulness of
introducing the POS tagger.

As conclusions, AutoPhrase is able to support diﬀerent
domains and support multiple languages with minimal human
eﬀort.

5.5 Distant Training Exploration

To compare the distant training and domain expert label-
ing, we experiment with the domain-speciﬁc datasets DBLP

(a) DBLP

(b) Yelp

(a) DBLP

(b) Yelp

Figure 5: AUC curves of four variants when we have
enough positive labels in the positive pool EP.

Figure 6: AUC curves of four variants after we ex-
haust positive labels in the positive pool EP.

and Yelp. To be fair, all the conﬁgurations in the classiﬁers
are the same except for the label selection process. More
speciﬁcally, we come up with four training pools:
1. EP means that domain experts give the positive pool.
2. DP means that a sampled subset from existing general

knowledge forms the positive pool.

3. EN means that domain experts give the negative pool.
4. DN means that all unlabeled (i.e., not in the positive

pool) phrase candidates form the negative pool.

By combining any pair of the positive and negative pools,
we have four variants, EPEN (in SegPhrase), DPDN (in
AutoPhrase), EPDN, and DPEN.

First of all, we evaluate the performance diﬀerence in the
two positive pools. Compared to EPEN, DPEN adopts a
positive pool sampled from knowledge bases instead of the
well-designed positive pool given by domain experts. The
negative pool EN is shared. As shown in Figure 5, we vary the
size of the positive pool and plot their AUC curves. We can
ﬁnd that EPEN outperforms DPEN and the trends of curves
on both datasets are similar. Therefore, we conclude that the
positive pool generated from knowledge bases has reasonable
quality, although its corresponding quality estimator works
slightly worse.

Secondly, we verify that whether the proposed noise re-
duction mechanism works properly. Compared to EPEN,
EPDN adopts a negative pool of all unlabeled phrase can-
didates instead of the well-designed negative pool given by
domain experts. The positive pool EP is shared. In Fig-
ure 5, the clear gap between them and the similar trends on
both datasets show that the noisy negative pool is slightly
worse than the well-designed negative pool, but it still works
eﬀectively.

As illustrated in Figure 5, DPDN has the worst perfor-
mance when the size of positive pools are limited. However,
distant training can generate much larger positive pools,
which may signiﬁcantly beyond the ability of domain experts
considering the high expense of labeling. Consequently, we
are curious whether the distant training can ﬁnally beat do-
main experts when positive pool sizes become large enough.
We call the size at this tipping point as the ideal number.
We increase positive pool sizes and plot AUC curves of
DPEN and DPDN, while EPEN and EPDN are degenerated
as dashed lines due to the limited domain expert abilities.
As shown in Figure 6, with a large enough positive pool,
distant training is able to beat expert labeling. On the
DBLP dataset, the ideal number is about 700, while on the
Yelp dataset, it becomes around 1600. Our guess is that the
ideal training size is proportional to the number of words

(a) EN

(b) ES

(c) CN

Figure 8: Precision-recall curves of AutoPhrase and
AutoSegPhrase.

(e.g., 91.6M in DBLP and 145.1M in Yelp). We notice that
compared to the corpus size, the ideal number is relatively
small, which implies the distant training should be eﬀective
in many domain-speciﬁc corpora as if they overlap with
Wikipedia.

Besides, Figure 6 shows that when the positive pool size
continues growing, the AUC score increases but the slope
becomes smaller. The performance of distant training will
be ﬁnally stable when a relatively large number of quality
phrases were fed.

We are curious how
many trees (i.e., T ) is
enough for DPDN. We
increase T and plot AUC
As
curves of DPDN.
shown in Figure 7, on
both datasets,
as T
grows, the AUC scores
ﬁrst increase rapidly and
the speed slows
later
down gradually, which is
consistent with the theoretical analysis in Section 4.1.2.

Figure 7: AUC curves of
DPDN varying T .

5.6 POS-guided Phrasal Segmentation

We are also interested in how much performance gain we
can obtain from incorporating POS tags in this segmentation
model, especially for diﬀerent languages. We select Wikipedia
article datasets in three diﬀerent languages: EN, ES, and
CN.

Figure 8 compares the results of AutoPhrase and AutoSegPhrase,

with the best baseline methods as references. AutoPhrase
outperforms AutoSegPhrase even on the English dataset EN,
though it has been shown the length penalty works rea-
sonably well in English [13]. The Spanish dataset ES has
similar observation. Moreover, the advantage of AutoPhrase
becomes more signiﬁcant on the CN dataset, indicating the
poor generality of length penalty.

(a) Running Time (b) Peak Memory (c) Multi-threading

Figure 9: Eﬃciency of AutoPhrase.

tive of a speciﬁc topic or concept.

Considering the criteria of quality phrases, because single-
word phrases cannot be decomposed into two or more parts,
the concordance and completeness are no longer deﬁnable.
Therefore, we revise the requirements for quality single-
word phrases as below.
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Informativeness: A phrase is informative if it is indica-

• Independence: A quality single-word phrase is more likely

a complete semantic unit in the given documents.

Only single-word phrases satisfying all popularity, indepen-
dence, and informativeness requirements are recognized as
quality single-word phrases.

Example 6. “UIUC” is a quality single-word phrase. “this”
is not a quality phrase due to its low informativeness. “united”,
usually occurring within other quality multi-word phrases such
as “United States”, “United Kingdom”, “United Airlines”,
and “United Parcel Service”, is not a quality single-word
phrase, because its independence is not enough.

After the phrasal segmentation, in replacement of con-
cordance features, the independence feature is added for
single-word phrases. Formally, it is the ratio of the rectiﬁed
frequency of a single-word phrase given the phrasal segmen-
tation over its raw frequency. Quality single-word phrases
are expected to have large values. For example, “united” is
likely to an almost zero ratio.

We use AutoPhrase+ to denote the extended AutoPhrase

with quality single-word phrase estimation.

6.2 Experiments

We have a similar human annotation as that in the pa-
per. Diﬀerently, we randomly sampled 500 Wiki-uncovered
phrases from the returned phrases (both single-word and
multi-word phrases) of each method in experiments of the
paper. Therefore, we have new pools on the EN, ES, and CN
datasets. The intra-class correlations (ICCs) are all more
than 0.9, which shows the agreement.

Figure 10 compare all methods based these new pools.
Note that all methods except for SegPhrase/WrapSegPhrase
extract single-word phrases as well.

Signiﬁcant recall advantages can be always observed on
all EN, ES, and CN datasets. The recall diﬀerences be-
tween AutoPhrase+ and AutoPhrase, ranging from 10% to
30% sheds light on the importance of modeling single-word
phrases. Across two Latin language datasets, EN and ES,
AutoPhrase+ and AutoPhrase overlaps in the beginning, but
later, the precision of AutoPhrase drops earlier and has a
lower recall due to the lack of single-word phrases. On the
CN dataset, AutoPhrase+ and AutoPhrase has a clear gap
even in the very beginning, which is diﬀerent from the trends
on the EN and ES datasets, which reﬂects that single-word
phrases are more important in Chinese. The major reason
behind is that there are a considerable number of high-quality
phrases (e.g., person names) in Chinese have only one token
after tokenization.

7. CONCLUSIONS

In this paper, we present an automated phrase mining
framework with two novel techniques: the robust positive-
only distant training and the POS-guided phrasal segmenta-
tion incorporating part-of-speech (POS) tags, for the develop-

In summary, thanks to the extra context information and
syntactic information for the particular language, incorpo-
rating POS tags during the phrasal segmentation can work
better than equally penalizing phrases of the same length.

5.7 Case Study

We present a case study about the extracted phrases
as shown in Table 2. The top ranked phrases are mostly
named entities, which makes sense for the Wikipedia article
datasets. Even in the long tail part, there are still many high-
quality phrases. For example, we have the dgreat spotted
woodpeckerc (a type of birds) and d计算机 科学技术c (i.e.,
Computer Science and Technology) ranked about 100,000.
In fact, we have more than 345K and 116K phrases with a
phrase quality higher than 0.5 on the EN and CN datasets
respectively.

5.8 Efﬁciency Evaluation

To study both time and memory eﬃciency, we choose the

three largest datasets: EN, ES, and CN.

Figures 9(a) and 9(b) evaluate the running time and the
peak memory usage of AutoPhrase using 10 threads on dif-
ferent proportions of three datasets respectively. Both time
and memory are linear to the size of text corpora. Moreover,
AutoPhrase can also be parallelized in an almost lock-free
way and shows a linear speedup in Figure 10(c).

Besides, compared to the previous state-of-the-art phrase
mining method SegPhrase and its variants WrapSegPhrase
on three datasets, as shown in Table 3, AutoPhrase achieves
about 8 to 11 times speedup and about 5 to 7 times memory
usage improvement. These improvements are made by a
more eﬃcient indexing and a more thorough parallelization.

6. SINGLE-WORD PHRASES

AutoPhrase can be extended to model single-word phrases,
which can gain about 10% to 30% recall improvements on
diﬀerent datasets. To study the eﬀect of modeling quality
single-word phrases, we choose the three Wikipedia article
datasets in diﬀerent languages: EN, ES, and CN.

6.1 Quality Estimation

In the paper, the deﬁnition of quality phrases and the
evaluation only focus on multi-word phrases. In linguistic
analysis, however, a phrase is not only a group of multiple
words, but also possibly a single word, as long as it functions
as a constituent in the syntax of a sentence [8]. As a great
portion (ranging from 10% to 30% on diﬀerent datasets based
on our experiments) of high-quality phrases, we should take
single-word phrases (e.g., dUIUCc, dIllinoisc, and dUSAc)
into consideration as well as multi-word phrases to achieve a
high recall in phrase mining.

Table 2: The results of AutoPhrase on the EN and CN datasets, with translations and explanations for Chinese
phrases. The whitespaces on the CN dataset are inserted by the Chinese tokenizer.

EN

CN

Sacramento Bee

Phrase
Elf Aquitaine
Arnold Sommerfeld
Eugene Wigner
Tarpon Springs
Sean Astin
. . .

Rank
1
2
3
4
5
. . .
20,001 ECAC Hockey
20,002
20,003 Bering Strait
20,004
Jacknife Lee
20,005 WXYZ-TV
. . .
. . .
99,994
John Gregson
99,995 white-tailed eagle
99,996

Translation (Explanation)
(the name of a soccer team)
Absinthe
(the name of a novel/TV-series)
notebook computer, laptop
Secretary of Party Committee
. . .
African countries
The Left (German: Die Linke)
Fraser Valley
Hippocampus
Mitsuki Saiga (a voice actress)
. . .

Phrase
江苏 舜 天
苦 艾 酒
白发 魔 女
笔记 型 电脑
党委 书记
. . .
非洲 国家
左翼 党
菲 沙 河谷
海马 体
斋 贺光希
. . .
计算机 科学技术 Computer Science and Technology
恒 天然
中国 作家 协会 The Vice President of Writers
副 主席
great spotted woodpecker 维他命 b
舆论 导向
. . .

Association of China
Vitamin B
controlled guidance of the media
. . .

99,997
99,998 David Manners
. . .

rhombic dodecahedron

Fonterra (a company)

. . .

Table 3: Eﬃciency Comparison between AutoPhrase and SegPhrase/WrapSegPhrase utilizing 10 threads.

EN
Time Memory
(mins)
32.77
369.53
11.27

(GB)
13.77
97.72
86%

ES
Time Memory
(mins)
54.05
452.85
8.37

(GB)
16.42
92.47
82%

CN
Time Memory
(mins)
9.43
108.58
11.50

(GB)
5.74
35.38
83%

AutoPhrase
(Wrap)SegPhrase
Speedup/Saving

(a) EN

(b) ES

(c) CN

Figure 10: Precision-recall curves evaluated by human annotation with both single-word and multi-word
phrases in pools.

ment of an automated phrase mining framework AutoPhrase.
Our extensive experiments show that AutoPhrase is domain-
independent, outperforms other phrase mining methods, and
supports multiple languages (e.g., English, Spanish, and
Chinese) eﬀectively, with minimal human eﬀort.

Besides, the inclusion of quality single-word phrases (e.g.,
dUIUCc and dUSAc) which leads to about 10% to 30% in-
creased recall and the exploration of better indexing strate-
gies and more thorough parallelization, which leads to about
8 to 11 times running time speedup and about 80% to 86%

memory usage saving over SegPhrase. Interested readers may
try our released code at GitHub.

For future work, it is interesting to (1) reﬁne quality
phrases to entity mentions, (2) apply AutoPhrase to more lan-
guages, such as Japanese, and (3) for those languages without
general knowledge bases, seek an unsupervised method to
generate the positive pool from the corpus, even with some
noise.

8. REFERENCES
[1] A. Allahverdyan and A. Galstyan. Comparative

A. Rajaraman. Towards the web of concepts:
Extracting concepts from large datasets. Proceedings of
the Very Large Data Bases Conference (VLDB),
3((1-2)), September 2010.

[20] Y. Park, R. J. Byrd, and B. K. Boguraev. Automatic

glossary extraction: beyond terminology identiﬁcation.
In COLING, 2002.

[21] V. Punyakanok and D. Roth. The use of classiﬁers in

sequential inference. In NIPS, 2001.

[22] H. Schmid. Treetagger| a language independent

part-of-speech tagger. Institut für Maschinelle
Sprachverarbeitung, Universität Stuttgart, 43:28, 1995.
[23] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. Kea: Practical automatic
keyphrase extraction. In Proceedings of the fourth ACM
conference on Digital libraries, pages 254–255. ACM,
1999.

[24] E. Xun, C. Huang, and M. Zhou. A uniﬁed statistical
model for the identiﬁcation of english basenp. In ACL,
2000.

[25] Z. Zhang, J. Iria, C. A. Brewster, and F. Ciravegna. A
comparative evaluation of term recognition algorithms.
LREC, 2008.

analysis of viterbi training and maximum likelihood
estimation for hmms. In NIPS, pages 1674–1682, 2011.

[2] L. Breiman. Randomizing outputs to increase

prediction accuracy. Machine Learning, 40(3):229–242,
2000.

[3] K.-h. Chen and H.-H. Chen. Extracting noun phrases
from large-scale texts: A hybrid approach and its
automatic evaluation. In ACL, 1994.

[4] M.-C. De Marneﬀe, B. MacCartney, C. D. Manning,

et al. Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454, 2006.

[5] P. Deane. A nonparametric method for extraction of

candidate phrasal terms. In ACL, 2005.

[6] A. El-Kishky, Y. Song, C. Wang, C. R. Voss, and
J. Han. Scalable topical phrase mining from text
corpora. VLDB, 8(3), Aug. 2015.

[7] D. A. Evans and C. Zhai. Noun-phrase analysis in
unrestricted text for information retrieval. In
Proceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 17–24.
Association for Computational Linguistics, 1996.
[8] G. Finch. Linguistic terms and concepts. Macmillan

Press Limited, 2000.

[9] K. Frantzi, S. Ananiadou, and H. Mima. Automatic

recognition of multi-word terms:. the c-value/nc-value
method. JODL, 3(2):115–130, 2000.

[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely

randomized trees. Machine learning, 63(1):3–42, 2006.

[11] T. Koo, X. Carreras, and M. Collins. Simple

semi-supervised dependency parsing. ACL-HLT, 2008.
[12] R. Levy and C. Manning. Is it harder to parse chinese,
or the chinese treebank? In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 439–446. Association for
Computational Linguistics, 2003.

[13] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. Mining

quality phrases from massive text corpora. In
Proceedings of 2015 ACM SIGMOD International
Conference on Management of Data, 2015.

[14] Z. Liu, X. Chen, Y. Zheng, and M. Sun. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
135–144. Association for Computational Linguistics,
2011.

[15] G. Martínez-Muñoz and A. Suárez. Switching class
labels to generate classiﬁcation ensembles. Pattern
Recognition, 38(10):1483–1494, 2005.

[16] R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.

Non-projective dependency parsing using spanning tree
algorithms. In EMNLP, 2005.

[17] R. Mihalcea and P. Tarau. Textrank: Bringing order

into texts. In ACL, 2004.

[18] J. Nivre, M.-C. de Marneﬀe, F. Ginter, Y. Goldberg,
J. Hajic, C. D. Manning, R. McDonald, S. Petrov,
S. Pyysalo, N. Silveira, et al. Universal dependencies
v1: A multilingual treebank collection. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016), 2016.

[19] A. Parameswaran, H. Garcia-Molina, and

Automated Phrase Mining from Massive Text Corpora

Jingbo Shang1, Jialu Liu2, Meng Jiang1, Xiang Ren1, Clare R Voss3, Jiawei Han1
1Computer Science Department, University of Illinois at Urbana-Champaign, IL, USA
2Google Research, New York City, NY, USA
3Computational & Information Sciences Directorate, Army Research Laboratory
2jialu@google.com

1{shang7, mjiang89, xren7, hanj}@illinois.edu

3clare.r.voss.civ@mail.mil

7
1
0
2
 
r
a

M
 
1
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
5
4
4
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
As one of the fundamental tasks in text analysis, phrase
mining aims at extracting quality phrases from a text corpus.
Phrase mining is important in various tasks such as informa-
tion extraction/retrieval, taxonomy construction, and topic
modeling. Most existing methods rely on complex, trained
linguistic analyzers, and thus likely have unsatisfactory per-
formance on text corpora of new domains and genres without
extra but expensive adaption. Recently, a few data-driven
methods have been developed successfully for extraction of
phrases from massive domain-speciﬁc text. However, none
of the state-of-the-art models is fully automated because
they require human experts for designing rules or labeling
phrases.

Since one can easily obtain many quality phrases from
public knowledge bases to a scale that is much larger than
that produced by human experts, in this paper, we propose
a novel framework for automated phrase mining, AutoPhrase,
which leverages this large amount of high-quality phrases in
an eﬀective way and achieves better performance compared
to limited human labeled phrases. In addition, we develop a
POS-guided phrasal segmentation model, which incorporates
the shallow syntactic information in part-of-speech (POS)
tags to further enhance the performance, when a POS tagger
is available. Note that, AutoPhrase can support any language
as long as a general knowledge base (e.g., Wikipedia) in that
language is available, while beneﬁting from, but not requiring,
a POS tagger. Compared to the state-of-the-art methods,
the new method has shown signiﬁcant improvements in eﬀec-
tiveness on ﬁve real-world datasets across diﬀerent domains
and languages.

1.

INTRODUCTION

Phrase mining refers to the process of automatic extrac-
tion of high-quality phrases (e.g., scientiﬁc terms and general
entity names) in a given corpus (e.g., research papers and
news). Representing the text with quality phrases instead

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

of n-grams can improve computational models for applica-
tions such as information extraction/retrieval, taxonomy
construction, and topic modeling.

Almost all the state-of-the-art methods, however, require
human experts at certain levels. Most existing methods [9,
20, 25] rely on complex, trained linguistic analyzers (e.g.,
dependency parsers) to locate phrase mentions, and thus
may have unsatisfactory performance on text corpora of new
domains and genres without extra but expensive adaption.
Our latest domain-independent method SegPhrase [13] out-
performs many other approaches [9, 20, 25, 5, 19, 6], but
still needs domain experts to ﬁrst carefully select hundreds
of varying-quality phrases from millions of candidates, and
then annotate them with binary labels.

Such reliance on manual eﬀorts by domain and linguis-
tic experts becomes an impediment for timely analysis of
massive, emerging text corpora in speciﬁc domains. An
ideal automated phrase mining method is supposed to be
domain-independent, with minimal human eﬀort or reliance
on linguistic analyzers1. Bearing this in mind, we propose
a novel automated phrase mining framework AutoPhrase in
this paper, going beyond SegPhrase, to further get rid of ad-
ditional manual labeling eﬀort and enhance the performance,
mainly using the following two new techniques.
1. Robust Positive-Only Distant Training.

In fact, many
high-quality phrases are freely available in general knowl-
edge bases, and they can be easily obtained to a scale
that is much larger than that produced by human ex-
perts. Domain-speciﬁc corpora usually contain some qual-
ity phrases also encoded in general knowledge bases, even
when there may be no other domain-speciﬁc knowledge
bases. Therefore, for distant training, we leverage the
existing high-quality phrases, as available from general
knowledge bases, such as Wikipedia and Freebase, to get
rid of additional manual labeling eﬀort. We independently
build samples of positive labels from general knowledge
bases and negative labels from the given domain corpora,
and train a number of base classiﬁers. We then aggregate
the predictions from these classiﬁers, whose independence
helps reduce the noise from negative labels.

2. POS-Guided Phrasal Segmentation. There is a trade-
oﬀ between the performance and domain-independence
when incorporating linguistic processors in the phrase
mining method. On the domain independence side, the

1The phrase “minimal human eﬀort” indicates using only
existing general knowledge bases without any other human
eﬀort.

accuracy might be limited without linguistic knowledge. It
is diﬃcult to support multiple languages, if the method is
completely language-blind. On the accuracy side, relying
on complex, trained linguistic analyzers may hurt the
domain-independence of the phrase mining method. For
example, it is expensive to adapt dependency parsers to
special domains like clinical reports. As a compromise, we
propose to incorporate a pre-trained part-of-speech (POS)
tagger to further enhance the performance, when it is
available for the language of the document collection. The
POS-guided phrasal segmentation leverages the shallow
syntactic information in POS tags to guide the phrasal
segmentation model locating the boundaries of phrases
more accurately.
In principle, AutoPhrase can support any language as long
as a general knowledge base in that language is available. In
fact, at least 58 languages have more than 100,000 articles
in Wikipedia as of Feb, 20172. Moreover, since pre-trained
part-of-speech (POS) taggers are widely available in many
languages (e.g., more than 20 languages in TreeTagger [22]3),
the POS-guided phrasal segmentation can be applied in many
scenarios. It is worth mentioning that for domain-speciﬁc
knowledge bases (e.g., MeSH terms in the biomedical domain)
and trained POS taggers, the same paradigm applies. In
this study, without loss of generality, we only assume the
availability of a general knowledge base together with a
pre-trained POS tagger.

As demonstrated in our experiments, AutoPhrase not only
works eﬀectively in multiple domains like scientiﬁc papers,
business reviews, and Wikipedia articles, but also supports
multiple languages, such as English, Spanish, and Chinese.

Our main contributions are highlighted as follows:

• We study an important problem, automated phrase mining,

and analyze its major challenges as above.

• We propose a robust positive-only distant training method
for phrase quality estimation to minimize the human eﬀort.
• We develop a novel phrasal segmentation model to leverage
POS tags to achieve further improvement, when a POS
tagger is available.

• We demonstrate the robustness and accuracy of our method
and show improvements over prior methods, with results
of experiments conducted on ﬁve real-world datasets in
diﬀerent domains (scientiﬁc papers, business reviews, and
Wikipedia articles) and diﬀerent languages (English, Span-
ish, and Chinese).

The rest of the paper is organized as follows. Section 2
positions our work relative to existing works. Section 3
deﬁnes basic concepts including four requirements of phrases.
The details of our method are covered in Section 4. Extensive
experiments and case studies are presented in Section 5. We
conclude the study in Section 7.

2. RELATED WORK

Identifying quality phrases eﬃciently has become ever
more central and critical for eﬀective handling of massively
increasing-size text datasets. In contrast to keyphrase ex-
traction [17, 23, 14], this task goes beyond the scope of
single documents and provides useful cross-document sig-
nals. The natural language processing (NLP) community

2https://meta.wikimedia.org/wiki/List_of_Wikipedias
3http://www.cis.uni-muenchen.de/~schmid/tools/
TreeTagger/

has conducted extensive studies typically referred to as au-
tomatic term recognition [9, 20, 25], for the computational
task of extracting terms (such as technical phrases). This
topic also attracts attention in the information retrieval (IR)
community [7, 19] since selecting appropriate indexing terms
is critical to the improvement of search engines where the
ideal indexing units represent the main concepts in a corpus,
not just literal bag-of-words.

Text indexing algorithms typically ﬁlter out stop words and
restrict candidate terms to noun phrases. With pre-deﬁned
part-of-speech (POS) rules, one can identify noun phrases
as term candidates in POS-tagged documents. Supervised
noun phrase chunking techniques [21, 24, 3] exploit such
tagged documents to automatically learn rules for identifying
noun phrase boundaries. Other methods may utilize more
sophisticated NLP technologies such as dependency parsing
to further enhance the precision [11, 16]. With candidate
terms collected, the next step is to leverage certain statistical
measures derived from the corpus to estimate phrase quality.
Some methods rely on other reference corpora for the calibra-
tion of “termhood” [25]. The dependency on these various
kinds of linguistic analyzers, domain-dependent language
rules, and expensive human labeling, makes it challenging to
extend these approaches to emerging, big, and unrestricted
corpora, which may include many diﬀerent domains, topics,
and languages.

To overcome this limitation, data-driven approaches opt
instead to make use of frequency statistics in the corpus to
address both candidate generation and quality estimation [5,
19, 6, 13]. They do not rely on complex linguistic feature
generation, domain-speciﬁc rules or extensive labeling eﬀorts.
Instead, they rely on large corpora containing hundreds
of thousands of documents to help deliver superior perfor-
mance [13]. In [19], several indicators, including frequency
and comparison to super/sub-sequences, were proposed to
extract n-grams that reliably indicate frequent, concise con-
cepts. Deane [5] proposed a heuristic metric over frequency
distribution based on Zipﬁan ranks, to measure lexical associ-
ation for phrase candidates. As a preprocessing step towards
topical phrase extraction, El-Kishky et al. [6] proposed to
mine signiﬁcant phrases based on frequency as well as doc-
ument context following a bottom-up fashion, which only
considers a part of quality phrase criteria, popularity and
concordance. Our previous work [13] succeeded at integrat-
ing phrase quality estimation with phrasal segmentation to
further rectify the initial set of statistical features, based on
local occurrence context. Unlike previous methods which
are purely unsupervised,
[13] required a small set of phrase
labels to train its phrase quality estimator. It is worth men-
tioning that all these approaches still depend on the human
eﬀort (e.g., setting domain-sensitive thresholds). Therefore,
extending them to work automatically is challenging.

3. PRELIMINARIES

The goal of this paper is to develop an automated phrase
mining method to extract quality phrases from a large collec-
tion of documents without human labeling eﬀort, and with
only limited, shallow linguistic analysis. The main input to
the automated phrase mining task is a corpus and a knowl-
edge base. The input corpus is a textual word sequence
in a particular language and a speciﬁc domain, of arbitrary
length. The output is a ranked list of phrases with decreasing
quality.

Figure 1: The overview of AutoPhrase. The two novel techniques developed in this paper are highlighted.

The AutoPhrase framework is shown in Figure 1. The
work ﬂow is completely diﬀerent form our previous domain-
independent phrase mining method requiring human ef-
fort [13], although the phrase candidates and the features
used during phrase quality (re-)estimation are the same. In
this paper, we propose a robust positive-only distant train-
ing to minimize the human eﬀort and develop a POS-guided
phrasal segmentation model to improve the model perfor-
mance. In this section, we brieﬂy introduce basic concepts
and components as preliminaries.

A phrase is deﬁned as a sequence of words that appear
consecutively in the text, forming a complete semantic unit
in certain contexts of the given documents [8]. The phrase
quality is deﬁned to be the probability of a word sequence
being a complete semantic unit, meeting the following crite-
ria [13]:
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Concordance: The collocation of tokens in quality phrases
occurs with signiﬁcantly higher probability than expected
due to chance.

• Informativeness: A phrase is informative if it is indica-

tive of a speciﬁc topic or concept.

• Completeness: Long frequent phrases and their subse-
quences within those phrases may both satisfy the 3 criteria
above. A phrase is deemed complete when it can be inter-
preted as a complete semantic unit in some given document
context. Note that a phrase and a subphrase contained
within it, may both be deemed complete, depending on
the context in which they appear. For example, “relational
database system”, “relational database” and “database sys-
tem” can all be valid in certain context.

AutoPhrase will estimate the phrase quality based on the
positive and negative pools twice, once before and once
after the POS-guided phrasal segmentation. That is, the
POS-guided phrasal segmentation requires an initial set of
phrase quality scores; we estimate the scores based on raw
frequencies beforehand; and then once the feature values
have been rectiﬁed, we re-estimate the scores.

Only the phrases satisfying all above requirements are

recognized as quality phrases.

Example 1. “strong tea” is a quality phrase while “heavy
tea” fails to be due to concordance. “this paper” is a popular
and concordant phrase, but is not informative in research
publication corpus. “NP-complete in the strong sense” is a
quality phrase while “NP-complete in the strong” fails to be
due to completeness. (cid:3)

To automatically mine these quality phrases, the ﬁrst phase
of AutoPhrase (see leftmost box in Figure 1) establishes the
set of phrase candidates that contains all n-grams over

the minimum support threshold τ (e.g., 30) in the corpus.
Here, this threshold refers to raw frequency of the n-grams
calculated by string matching.
In practice, one can also
set a phrase length threshold (e.g., n ≤ 6) to restrict the
number of words in any phrase. Given a phrase candidate
w1w2 . . . wn, its phrase quality is:

Q(w1w2 . . . wn) = p(dw1w2 . . . wnc|w1w2 . . . wn) ∈ [0, 1]
where dw1w2 . . . wnc refers to the event that these words
constitute a phrase. Q(·), also known as the phrase quality
estimator, is initially learned from data based on statistical
features4, such as point-wise mutual information, point-wise
KL divergence, and inverse document frequency, designed
to model concordance and informativeness mentioned above.
Note the phrase quality estimator is computed independent
of POS tags. For unigrams, we simply set their phrase quality
as 1.

Example 2. A good quality estimator will return Q(this paper) ≈

0 and Q(relational database system) ≈ 1. (cid:3)

Then, to address the completeness criterion, the phrasal
segmentation ﬁnds the best segmentation for each sentence.

Example 3. Ideal phrasal segmentation results are as fol-

lows.

(cid:3)

#1:
... / the / Great Firewall / is / ...
#2: This / is / a / great / ﬁrewall software/ .
#3: The / discriminative classiﬁer / SVM / is / ...

During the phrase quality re-estimation, related sta-
tistical features will be re-computed based on the rectiﬁed
frequency of phrases, which means the number of times that
a phrase becomes a complete semantic unit in the identiﬁed
segmentation. After incorporating the rectiﬁed frequency, the
phrase quality estimator Q(·) also models the completeness
in addition to concordance and informativeness.

Example 4. Continuing the previous example, the raw
frequency of the phrase “great ﬁrewall” is 2, but its rectiﬁed
frequency is 1. Both the raw frequency and the rectiﬁed
frequency of the phrase “ﬁrewall software” are 1. The raw
frequency of the phrase “classiﬁer SVM” is 1, but its rectiﬁed
frequency is 0. (cid:3)

4. METHODOLOGY

In this section, we focus on introducing our two new tech-

niques.
4See https://github.com/shangjingbo1226/AutoPhrase for
further details

Figure 2: The illustration of each base classiﬁer. In
each base classiﬁer, we ﬁrst randomly sample K pos-
itive and negative labels from the pools respectively.
There might be δ quality phrases among the K neg-
ative labels. An unpruned decision tree is trained
based on this perturbed training set.
4.1 Robust Positive-Only Distant Training

To estimate the phrase quality score for each phrase can-
didate, our previous work [13] required domain experts to
ﬁrst carefully select hundreds of varying-quality phrases from
millions of candidates, and then annotate them with binary
labels. For example, for computer science papers, our domain
experts provided hundreds of positive labels (e.g., “spanning
tree” and “computer science”) and negative labels (e.g., “pa-
per focuses” and “important form of ”). However, creating
such a label set is expensive, especially in specialized do-
mains like clinical reports and business reviews, because this
approach provides no clues for how to identify the phrase
candidates to be labeled.
In this paper, we introduce a
method that only utilizes existing general knowledge bases
without any other human eﬀort.

4.1.1 Label Pools

Public knowledge bases (e.g., Wikipedia) usually encode
a considerable number of high-quality phrases in the titles,
keywords, and internal links of pages. For example, by ana-
lyzing the internal links and synonyms5 in English Wikipedia,
more than a hundred thousand high-quality phrases were
discovered. As a result, we place these phrases in a positive
pool.

Knowledge bases, however, rarely, if ever, identify phrases
that fail to meet our criteria, what we call inferior phrases.
An important observation is that the number of phrase can-
didates, based on n-grams (recall leftmost box of Figure 1),
is huge and the majority of them are actually of of inferior
quality (e.g., “Francisco opera and”).
In practice, based
on our experiments, among millions of phrase candidates,
usually, only about 10% are in good quality. Therefore,
phrase candidates that are derived from the given corpus
but that fail to match any high-quality phrase derived from
the given knowledge base, are used to populate a large but
noisy negative pool.

4.1.2 Noise Reduction

Directly training a classiﬁer based on the noisy label pools
is not a wise choice: some phrases of high quality from
the given corpus may have been missed (i.e., inaccurately
binned into the negative pool) simply because they were
Instead, we propose
not present in the knowledge base.
to utilize an ensemble classiﬁer that averages the results
of T independently trained base classiﬁers. As shown in
Figure 2, for each base classiﬁer, we randomly draw K phrase
candidates with replacement from the positive pool and the
negative pool respectively (considering a canonical balanced
classiﬁcation scenario). This size-2K subset of the full set of

5https://github.com/kno10/WikipediaEntities

all phrase candidates is called a perturbed training set [2],
because the labels of some (δ in the ﬁgure) quality phrases
In order for the
are switched from positive to negative.
ensemble classiﬁer to alleviate the eﬀect of such noise, we
need to use base classiﬁers with the lowest possible training
errors. We grow an unpruned decision tree to the point of
separating all phrases to meet this requirement.
In fact,
such decision tree will always reach 100% training accuracy
when no two positive and negative phrases share identical
feature values in the perturbed training set. In this case,
δ
its ideal error is
2K , which approximately equals to the
proportion of switched labels among all phrase candidates
δ
2K ≈ 10%). Therefore, the value of K is not sensitive
(i.e.,
to the accuracy of the unpruned decision tree and is ﬁxed as
100 in our implementation. Assuming the extracted features
are distinguishable between quality and inferior phrases, the
empirical error evaluated on all phrase candidates, p, should
be relatively small as well.

An interesting property of this sampling procedure is that
the random selection of phrase candidates for building per-
turbed training sets creates classiﬁers that have statistically
independent errors and similar erring probability [2, 15].
Therefore, we naturally adopt random forest [10], which is
veriﬁed, in the statistics literature, to be robust and eﬃcient.
The phrase quality score of a particular phrase is computed
as the proportion of all decision trees that predict that phrase
is a quality phrase. Suppose there are T trees in the random
forest, the ensemble error can be estimated as the probability
of having more than half of the classiﬁers misclassifying a
given phrase candidate as follows.

ensemble_ error(T ) =

pt(1 − p)T −t

T
X

(cid:19)

(cid:18)T
t

t=b1+T /2c

From Figure 3, one can
easily observe that the en-
semble error is approach-
ing 0 when T grows. In
practice, T needs to be
set larger due to the ad-
ditional error brought by
model bias. Empirical
studies can be found in
Figure 7.

Figure 3: Ensemble er-
rors of diﬀerent p’s vary-
ing T .

4.2 POS-Guided Phrasal Segmentation

Phrasal segmentation addresses the challenge of measuring
completeness (our fourth criterion) by locating all phrase
mentions in the corpus and rectifying their frequencies ob-
tained originally via string matching.

The corpus is processed as a length-n POS-tagged word
sequence Ω = Ω1Ω2 . . . Ωn, where Ωi refers to a pair con-
sisting of a word and its POS tag hwi, tii. A POS-guided
phrasal segmentation is a partition of this sequence into
m segments induced by a boundary index sequence B =
{b1, b2, . . . , bm+1} satisfying 1 = b1 < b2 < . . . < bm+1 =
n+1. The i-th segment refers to Ωbi Ωbi+1 . . . Ωbi+1−1.

Compared to the phrasal segmentation in our previous
work [13], the POS-guided phrasal segmentation addresses
the completeness requirement in a context-aware way, instead
of equivalently penalizing phrase candidates of the same
length. In addition, POS tags provide shallow, language-
speciﬁc knowledge, which may help boost phrase detection

Algorithm 1: POS-Guided Phrasal Segmentation (PGPS)

Input: Corpus Ω = Ω1Ω2 . . . Ωn, phrase quality Q,
parameters θu and δ(tx, ty).
Output: Optimal boundary index sequence B.
// hi ≡ maxB p(Ω1Ω2 . . . Ωi−1, B|Q, θ, δ)
h1 ← 1, hi ← 0 (1 < i ≤ n + 1)
for i = 1 to n do

for j = i + 1 to min(i + length threshold, n + 1) do

// Efficiently implemented via Trie.
if there is no phrase starting with w[i,j) then

break

// In practice, log and addition are used

to avoid underflow.

if hi × p(j, dw[i,j)c|i, t[i,j)) > hj then
hj ← hi × p(j, dw[i,j)c|i, t[i,j))
gj ← i
j ← n + 1, m ← 0
while j > 1 do
m ← m + 1
bm ← j
j ← gj

return B ← 1, bm, bm−1, . . . , b1

accuracy, especially at syntactic constituent boundaries for
that language.

Given the POS tag sequence for the full n-length corpus
is t = t1t2 . . . tn, containing the tag subsequence tl . . . tr−1
(denote as t[l,r) for clarity), the POS quality score for that
tag subsequence is deﬁned to be the conditional probability of
its corresponding word sequence being a complete semantic
unit. Formally, we have

T (t[l,r)) = p(dwl . . . wrc|t) ∈ [0, 1]

The POS quality score T (·) is designed to reward the phrases
with their correctly identiﬁed POS sequences, as follows.

Example 5. Suppose the whole POS tag sequence is “NN
NN NN VB DT NN”. A good POS sequence quality estima-
tor might return T (NN NN NN) ≈ 1 and T (NN VB) ≈ 0,
where NN refers to singular or mass noun (e.g., database),
VB means verb in the base form (e.g., is), and DT is for
determiner (e.g., the). (cid:3)

The particular form of T (·) we have chosen is:

T (t[l,r)) = (1 − δ(tbr −1, tbr )) ×

δ(tj−1, tj)

r−1
Y

j=l+1

where, δ(tx, ty) is the probability that the POS tag tx is
adjacent and precedes POS tag ty within a phrase in the given
document collection. In this formula, the ﬁrst term indicates
the probability that there is a phrase boundary between the
words indexed r − 1 and r, while the latter product indicates
the probability that all POS tags within t[l,r) are in the
same phrase. This POS quality score can naturally counter
the bias to longer segments because ∀i > 1, exactly one of
δ(ti−1, ti) and (1 − δ(ti−1, ti)) is always multiplied no matter
how the corpus is segmented. Note that the length penalty
model in our previous work [13] is a special case when all
values of δ(tx, ty) are the same.

Mathematically, δ(tx, ty) is deﬁned as:

δ(tx, ty) = p(d. . . w1w2 . . .c|Ω, tag(w1) = tx ∧ tag(w2) = ty)

As it depends on how documents are segmented into phrases,
δ(tx, ty) is initialized uniformly and will be learned during
the phrasal segmentation.

Now, after we have both phrase quality Q(·) and POS
quality T (·) ready, we are able to formally deﬁne the POS-
guided phrasal segmentation model. The joint probability of
a POS tagged sequence Ω and a boundary index sequence
B = {b1, b2, . . . , bm+1} is factorized as:

p(Ω, B) =

p

bi+1, dw[bi,bi+1)c

(cid:17)

(cid:12)
(cid:12)
(cid:12)bi, t

m
Y

(cid:16)

i=1

where p(bi+1, dw[bi,bi+1)c|bi, t) is the probability of observing
a word sequence w[bi,bi+1) as the i-th quality segment given
the previous boundary index bi and the whole POS tag
sequence t.

Since the phrase segments function as a constituent in the
syntax of a sentence, they usually have weak dependence on
each other [8, 13]. As a result, we assume these segments in
the word sequence are generated one by one for the sake of
both eﬃciency and simplicity.

For each segment, given the POS tag sequence t and the

start index bi, the generative process is deﬁned as follows.
1. Generate the end index bi+1, according to its POS quality

p(bi+1|bi, t) = T (t[bi,bi+1))

2. Given the two ends bi and bi+1, generate the word se-
quence w[bi,bi+1) according to a multinomial distribution
over all segments of length-(bi+1 − bi).

p(w[bi,bi+1)|bi, bi+1) = p(w[bi,bi+1)|bi+1 − bi)

3. Finally, we generate an indicator whether w[bi,bi+1) forms

a quality segment according to its quality

p(dw[bi,bi+1)c|w[bi,bi+1)) = Q(w[bi,bi+1))
We denote p(w[bi,bi+1)|bi+1 −bi) as θw[bi,bi+1) for convenience.
Integrating the above three generative steps together, we
have the following probabilistic factorization:

p(bi+1, dw[bi,bi+1)c|bi, t)

=p(bi+1|bi, t)p(w[bi,bi+1)|bi, bi+1)p(dw[bi,bi+1)c|w[bi,bi+1))
=T (t[bi,bi+1))θw[bi,bi+1) Q(w[bi,bi+1))

Therefore, there are three subproblems:

1. Learn θu for each word and phrase candidate u;
2. Learn δ(tx, ty) for every POS tag pair; and
3. Infer B when θu and δ(tx, ty) are ﬁxed.

We employ the maximum a posterior principle and maxi-

mize the joint log likelihood:

log p(Ω, B) =

log p

(cid:16)

(cid:12)
(cid:12)
(cid:12)bt, t
bi+1, dw[bi,bi+1)c

(cid:17)

(1)

m
X

i=1

Given θu and δ(tx, ty), to ﬁnd the best segmentation that
maximizes Equation (1), we develop an eﬃcient dynamic
programming algorithm for the POS-guided phrasal segmen-
tation as shown in Algorithm 1.

When the segmentation S and the parameter θ are ﬁxed,

the closed-form solution of δ(tx, ty) is:

δ(tx, ty) =

Pm

i=1

Pbi+1−2
j=bi

Pn−1

1(tj = tx ∧ tj+1 = ty)

(2)

i=1 1(ti = tx ∧ ti+1 = ty)

Algorithm 2: Viterbi Training (VT)

Input: Corpus Ω and phrase quality Q.
Output: θu and δ(tx, ty).
initialize θ with normalized raw frequencies in the corpus
while θu does not converge do

while δ(tx, ty) does not converge do

B ← best segmentation via Alg. 1
update δ(tx, ty) using B according to Eq. (2)

B ← best segmentation via Alg. 1
update θu using B according to Eq. (3)

return θu and δ(tx, ty)

where 1(·) denotes the identity indicator. δ(tx, ty) is the
unsegmented ratio among all htx, tyi pairs in the given corpus.
Similarly, once the segmentation S and the parameter δ
are ﬁxed, the closed-form solution of θu can be derived as:

θu =

Pm
Pm

i=1 1(w[bi,bi+1) = u)
i=1 1(bi+1 − bi = |u|)

(3)

We can see that θu is the times that u becomes a com-
plete segment normalized by the number of the length-|u|
segments.

As shown in Algorithm 2, we choose Viterbi Training, or
Hard EM in literature [1] to iteratively optimize parameters,
because Viterbi Training converges fast and results in sparse
and simple models for Hidden Markov Model-like tasks [1].

4.3 Complexity Analysis

The time complexity of the most time consuming compo-
nents in our framework, such as frequent n-gram, feature
extraction, POS-guided phrasal segmentation, are all O(|Ω|)
with the assumption that the maximum number of words in
a phrase is a small constant (e.g., n ≤ 6), where |Ω| is the
total number of words in the corpus. Therefore, AutoPhrase
is linear to the corpus size and thus being very eﬃcient and
scalable. Meanwhile, every component can be parallelized
in an almost lock-free way grouping by either phrases or
sentences.

5. EXPERIMENTS

In this section, we will apply the proposed method to mine
quality phrases from ﬁve massive text corpora across three
domains (scientiﬁc papers, business reviews, and Wikipedia
articles) and in three languages (English, Spanish, and Chi-
nese). We compare the proposed method with many other
methods to demonstrate its high performance. Then we
explore the robustness of the proposed positive-only distant
training and its performance against expert labeling. The
advantage of incorporating POS tags in phrasal segmentation
will also be proved. In the end, we present case studies.

5.1 Datasets

To validate that the proposed positive-only distant train-
ing can eﬀectively work in diﬀerent domains and the POS-
guided phrasal segmentation can support multiple languages
eﬀectively, we have ﬁve large collections of text in diﬀerent
domains and languages, as shown in Table 1: Abstracts
of English computer science papers from DBLP6, English

6https://aminer.org/citation

Table 1: Five real-world massive text corpora in dif-
|Ω| is the
ferent domains and multiple languages.
total number of words. sizep is the size of positive
pool.

Dataset
DBLP
Yelp
EN
ES
CN

Domain
Scientiﬁc Paper
Business Review
Wikipedia Article
Wikipedia Article
Wikipedia Article

Language
English
English
English
Spanish
Chinese

|Ω|

sizep
File size
29K
91.6M 618MB
145.1M 749MB
22K
808.0M 3.94GB 184K
65K
791.2M 4.06GB
29K
371.9M 1.56GB

business reviews from Yelp7, Wikipedia articles8 in English
(EN), Spanish (ES), and Chinese (CN). From the exist-
ing general knowledge base Wikipedia, we extract popular
mentions of entities by analyzing intra-Wiki citations within
Wiki content9. On each dataset, the intersection between the
extracted popular mentions and the generated phrase candi-
dates forms the positive pool. Therefore, the size of positive
pool may vary in diﬀerent datasets of the same language.

5.2 Compared Methods

We compare AutoPhrase with three lines of methods as

follows. Every method returns a ranked list of phrases.
SegPhrase10/WrapSegPhrae11: In English domain-speciﬁc
text corpora, our latest work SegPhrase outperformed phrase
mining [6], keyphrase extraction [23, 19], and noun phrase
chunking methods. WrapSegPhrase extends SegPhrase to dif-
ferent languages by adding an encoding preprocessing to
ﬁrst transform non-English corpus using English characters
and punctuation as well as a decoding postprocessing to
later translate them back to the original language. Both
methods require domain expert labors. For each dataset, we
ask domain experts to annotate a representative set of 300
quality/interior phrases.
Parser-based Phrase Extraction: Using complicated lin-
guistic processors, such as parsers, we can extract minimum
phrase units (e.g., NP) from the parsing trees as phrase
candidates. Parsers of all three languages are available in
Stanford NLP tools [18, 4, 12]. Two ranking heuristics are
considered:
• TF-IDF ranks the extracted phrases by their term fre-
quency and inverse document frequency in the given docu-
ments;

• TextRank: An unsupervised graph-based ranking model

for keyword extraction [17].

Pre-trained Chinese Segmentation Models: Diﬀerent
from English and Spanish, phrasal segmentation in Chi-
nese has been intensively studied because there is no space
between Chinese words. The most eﬀective and popular
segmentation methods are:
• AnsjSeg12 is a popular text segmentation algorithm for
Chinese corpus. It ensembles statistical modeling methods
of Conditional Random Fields (CRF) and Hidden Markov
Models (HMMs) based on the n-gram setting;

• JiebaPSeg13 is a Chinese text segmentation method im-

7https://www.yelp.com/dataset_challenge
8https://dumps.wikimedia.org/
9https://github.com/kno10/WikipediaEntities
10https://github.com/shangjingbo1226/SegPhrase
11https://github.com/remenberl/SegPhrase-MultiLingual
12https://github.com/NLPchina/ansj_seg
13https://github.com/fxsjy/jieba

(a) DBLP

(b) Yelp

(c) EN

(d) ES

(e) CN

Figure 4: Overall Performance Evaluation: Precision-recall curves of all methods evaluated by human anno-
tation.

plemented in Python. It builds a directed acyclic graph
for all possible phrase combinations based on a preﬁx
dictionary structure to achieve eﬃcient phrase graph scan-
ning. Then it uses dynamic programming to ﬁnd the most
probable combination based on the phrase frequency. For
unknown phrases, an HMM-based model is used with the
Viterbi algorithm.

Note that all parser-based phrase extraction and Chinese
segmentation models are pre-trained based on general corpus.
To study the eﬀectiveness of the POS-guided segmenta-
tion, AutoSegPhrase adopts the length penalty instead of
δ(tx, ty), while other components are the same as AutoPhrase.
AutoSegPhrase is useful when there is no POS tagger.

5.3 Experimental Settings
Implementation. The preprocessing includes tokenizers
from Lucene and Stanford NLP as well as the POS tagger
from TreeTagger. Our documented code package has been
released and maintained in GitHub14.
Default Parameters. We set the minimum support thresh-
old σ as 30. The maximum number of words in a phrase is
set as 6 according to labels from domain experts. These are
two parameters required by all methods. Other parameters
required by compared methods were set according to the
open-source tools or the original papers.
Human Annotation. We rely on human evaluators to
judge the quality of the phrases which cannot be identiﬁed
through any knowledge base. More speciﬁcally, on each
dataset, we randomly sample 500 such phrases from the
predicted phrases of each method in the experiments. These
selected phrases are shuﬄed in a shared pool and evaluated
by 3 reviewers independently. We allow reviewers to use
search engines when unfamiliar phrases encountered. By the
rule of majority voting, phrases in this pool received at least
two positive annotations are quality phrases. The intra-class
correlations (ICCs) are all more than 0.9 on all ﬁve datasets,
which shows the agreement.
Evaluation Metrics. For a list of phrases, precision is
deﬁned as the number of true quality phrases divided by the
number of predicted quality phrases; recall is deﬁned as the
number of true quality phrases divided by the total number
of quality phrases. We retrieve the ranked list of the pool
from the outcome of each method. When a new true quality
phrase encountered, we evaluate the precision and recall of
this ranked list.
In the end, we plot the precision-recall
curves. In addition, area under the curve (AUC) is adopted
as another quantitative measure. AUC in this paper refers
to the area under the precision-recall curve.

14https://github.com/shangjingbo1226/AutoPhrase

5.4 Overall Performance

Figures 4 presents the precision-recall curves of all com-
pared methods evaluated by human annotation on ﬁve datasets.
Overall, AutoPhrase performs the best, in terms of both pre-
cision and recall. Signiﬁcant advantages can be observed,
especially on two non-English datasets ES and CN. For
example, on the ES dataset, the recall of AutoPhrase is
about 20% higher than the second best method (SegPhrase)
in absolute value. Meanwhile, there is a visible precision
gap between AutoPhrase and the best baseline. The phrase
chunking-based methods TF-IDF and TextRank work poorly,
because the extraction and ranking are modeled separately
and the pre-trained complex linguistic analyzers fail to ex-
tend to domain-speciﬁc corpora. TextRank usually starts
with a higher precision than TF-IDF, but its recall is very
low because of the sparsity of the constructed co-occurrence
graph. TF-IDF achieves a reasonable recall but unsatisfac-
tory precision. On the CN dataset, the pre-trained Chinese
segmentation models, JiebaSeg and AnsjSeg, are very com-
petitive, because they not only leverage training data for
segmentations, but also exhaust the engineering work, in-
cluding a huge dictionary for popular Chinese entity names
and speciﬁc rules for certain types of entities. As a conse-
quence, they can easily extract tons of well-known terms
and people/location names. Outperforming such a strong
baseline further conﬁrms the eﬀectiveness of AutoPhrase.

The comparison among the English datasets across three
domains (i.e., scientiﬁc papers, business reviews, and Wikipedia
articles) demonstrates that AutoPhrase is reasonably domain-
independent. The performance of parser-based methods
TF-IDF and TextRank depends on the rigorous degree of
the documents. For example, it works well on the DBLP
dataset but poorly on the Yelp dataset. However, without
any human eﬀort, AutoPhrase can work eﬀectively on domain-
speciﬁc datasets, and even outperforms SegPhrase, which is
supervised by the domain experts.

The comparison among the Wikipedia article datasets in
three languages (i.e., EN, ES, and CN ) shows that, ﬁrst
of all, AutoPhrase supports multiple languages. Secondly,
the advantage of AutoPhrase over SegPhrase/WrapSegPhrase
is more obvious on two non-English datasets ES and CN
than the EN dataset, which proves that the helpfulness of
introducing the POS tagger.

As conclusions, AutoPhrase is able to support diﬀerent
domains and support multiple languages with minimal human
eﬀort.

5.5 Distant Training Exploration

To compare the distant training and domain expert label-
ing, we experiment with the domain-speciﬁc datasets DBLP

(a) DBLP

(b) Yelp

(a) DBLP

(b) Yelp

Figure 5: AUC curves of four variants when we have
enough positive labels in the positive pool EP.

Figure 6: AUC curves of four variants after we ex-
haust positive labels in the positive pool EP.

and Yelp. To be fair, all the conﬁgurations in the classiﬁers
are the same except for the label selection process. More
speciﬁcally, we come up with four training pools:
1. EP means that domain experts give the positive pool.
2. DP means that a sampled subset from existing general

knowledge forms the positive pool.

3. EN means that domain experts give the negative pool.
4. DN means that all unlabeled (i.e., not in the positive

pool) phrase candidates form the negative pool.

By combining any pair of the positive and negative pools,
we have four variants, EPEN (in SegPhrase), DPDN (in
AutoPhrase), EPDN, and DPEN.

First of all, we evaluate the performance diﬀerence in the
two positive pools. Compared to EPEN, DPEN adopts a
positive pool sampled from knowledge bases instead of the
well-designed positive pool given by domain experts. The
negative pool EN is shared. As shown in Figure 5, we vary the
size of the positive pool and plot their AUC curves. We can
ﬁnd that EPEN outperforms DPEN and the trends of curves
on both datasets are similar. Therefore, we conclude that the
positive pool generated from knowledge bases has reasonable
quality, although its corresponding quality estimator works
slightly worse.

Secondly, we verify that whether the proposed noise re-
duction mechanism works properly. Compared to EPEN,
EPDN adopts a negative pool of all unlabeled phrase can-
didates instead of the well-designed negative pool given by
domain experts. The positive pool EP is shared. In Fig-
ure 5, the clear gap between them and the similar trends on
both datasets show that the noisy negative pool is slightly
worse than the well-designed negative pool, but it still works
eﬀectively.

As illustrated in Figure 5, DPDN has the worst perfor-
mance when the size of positive pools are limited. However,
distant training can generate much larger positive pools,
which may signiﬁcantly beyond the ability of domain experts
considering the high expense of labeling. Consequently, we
are curious whether the distant training can ﬁnally beat do-
main experts when positive pool sizes become large enough.
We call the size at this tipping point as the ideal number.
We increase positive pool sizes and plot AUC curves of
DPEN and DPDN, while EPEN and EPDN are degenerated
as dashed lines due to the limited domain expert abilities.
As shown in Figure 6, with a large enough positive pool,
distant training is able to beat expert labeling. On the
DBLP dataset, the ideal number is about 700, while on the
Yelp dataset, it becomes around 1600. Our guess is that the
ideal training size is proportional to the number of words

(a) EN

(b) ES

(c) CN

Figure 8: Precision-recall curves of AutoPhrase and
AutoSegPhrase.

(e.g., 91.6M in DBLP and 145.1M in Yelp). We notice that
compared to the corpus size, the ideal number is relatively
small, which implies the distant training should be eﬀective
in many domain-speciﬁc corpora as if they overlap with
Wikipedia.

Besides, Figure 6 shows that when the positive pool size
continues growing, the AUC score increases but the slope
becomes smaller. The performance of distant training will
be ﬁnally stable when a relatively large number of quality
phrases were fed.

We are curious how
many trees (i.e., T ) is
enough for DPDN. We
increase T and plot AUC
As
curves of DPDN.
shown in Figure 7, on
both datasets,
as T
grows, the AUC scores
ﬁrst increase rapidly and
the speed slows
later
down gradually, which is
consistent with the theoretical analysis in Section 4.1.2.

Figure 7: AUC curves of
DPDN varying T .

5.6 POS-guided Phrasal Segmentation

We are also interested in how much performance gain we
can obtain from incorporating POS tags in this segmentation
model, especially for diﬀerent languages. We select Wikipedia
article datasets in three diﬀerent languages: EN, ES, and
CN.

Figure 8 compares the results of AutoPhrase and AutoSegPhrase,

with the best baseline methods as references. AutoPhrase
outperforms AutoSegPhrase even on the English dataset EN,
though it has been shown the length penalty works rea-
sonably well in English [13]. The Spanish dataset ES has
similar observation. Moreover, the advantage of AutoPhrase
becomes more signiﬁcant on the CN dataset, indicating the
poor generality of length penalty.

(a) Running Time (b) Peak Memory (c) Multi-threading

Figure 9: Eﬃciency of AutoPhrase.

tive of a speciﬁc topic or concept.

Considering the criteria of quality phrases, because single-
word phrases cannot be decomposed into two or more parts,
the concordance and completeness are no longer deﬁnable.
Therefore, we revise the requirements for quality single-
word phrases as below.
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Informativeness: A phrase is informative if it is indica-

• Independence: A quality single-word phrase is more likely

a complete semantic unit in the given documents.

Only single-word phrases satisfying all popularity, indepen-
dence, and informativeness requirements are recognized as
quality single-word phrases.

Example 6. “UIUC” is a quality single-word phrase. “this”
is not a quality phrase due to its low informativeness. “united”,
usually occurring within other quality multi-word phrases such
as “United States”, “United Kingdom”, “United Airlines”,
and “United Parcel Service”, is not a quality single-word
phrase, because its independence is not enough.

After the phrasal segmentation, in replacement of con-
cordance features, the independence feature is added for
single-word phrases. Formally, it is the ratio of the rectiﬁed
frequency of a single-word phrase given the phrasal segmen-
tation over its raw frequency. Quality single-word phrases
are expected to have large values. For example, “united” is
likely to an almost zero ratio.

We use AutoPhrase+ to denote the extended AutoPhrase

with quality single-word phrase estimation.

6.2 Experiments

We have a similar human annotation as that in the pa-
per. Diﬀerently, we randomly sampled 500 Wiki-uncovered
phrases from the returned phrases (both single-word and
multi-word phrases) of each method in experiments of the
paper. Therefore, we have new pools on the EN, ES, and CN
datasets. The intra-class correlations (ICCs) are all more
than 0.9, which shows the agreement.

Figure 10 compare all methods based these new pools.
Note that all methods except for SegPhrase/WrapSegPhrase
extract single-word phrases as well.

Signiﬁcant recall advantages can be always observed on
all EN, ES, and CN datasets. The recall diﬀerences be-
tween AutoPhrase+ and AutoPhrase, ranging from 10% to
30% sheds light on the importance of modeling single-word
phrases. Across two Latin language datasets, EN and ES,
AutoPhrase+ and AutoPhrase overlaps in the beginning, but
later, the precision of AutoPhrase drops earlier and has a
lower recall due to the lack of single-word phrases. On the
CN dataset, AutoPhrase+ and AutoPhrase has a clear gap
even in the very beginning, which is diﬀerent from the trends
on the EN and ES datasets, which reﬂects that single-word
phrases are more important in Chinese. The major reason
behind is that there are a considerable number of high-quality
phrases (e.g., person names) in Chinese have only one token
after tokenization.

7. CONCLUSIONS

In this paper, we present an automated phrase mining
framework with two novel techniques: the robust positive-
only distant training and the POS-guided phrasal segmenta-
tion incorporating part-of-speech (POS) tags, for the develop-

In summary, thanks to the extra context information and
syntactic information for the particular language, incorpo-
rating POS tags during the phrasal segmentation can work
better than equally penalizing phrases of the same length.

5.7 Case Study

We present a case study about the extracted phrases
as shown in Table 2. The top ranked phrases are mostly
named entities, which makes sense for the Wikipedia article
datasets. Even in the long tail part, there are still many high-
quality phrases. For example, we have the dgreat spotted
woodpeckerc (a type of birds) and d计算机 科学技术c (i.e.,
Computer Science and Technology) ranked about 100,000.
In fact, we have more than 345K and 116K phrases with a
phrase quality higher than 0.5 on the EN and CN datasets
respectively.

5.8 Efﬁciency Evaluation

To study both time and memory eﬃciency, we choose the

three largest datasets: EN, ES, and CN.

Figures 9(a) and 9(b) evaluate the running time and the
peak memory usage of AutoPhrase using 10 threads on dif-
ferent proportions of three datasets respectively. Both time
and memory are linear to the size of text corpora. Moreover,
AutoPhrase can also be parallelized in an almost lock-free
way and shows a linear speedup in Figure 10(c).

Besides, compared to the previous state-of-the-art phrase
mining method SegPhrase and its variants WrapSegPhrase
on three datasets, as shown in Table 3, AutoPhrase achieves
about 8 to 11 times speedup and about 5 to 7 times memory
usage improvement. These improvements are made by a
more eﬃcient indexing and a more thorough parallelization.

6. SINGLE-WORD PHRASES

AutoPhrase can be extended to model single-word phrases,
which can gain about 10% to 30% recall improvements on
diﬀerent datasets. To study the eﬀect of modeling quality
single-word phrases, we choose the three Wikipedia article
datasets in diﬀerent languages: EN, ES, and CN.

6.1 Quality Estimation

In the paper, the deﬁnition of quality phrases and the
evaluation only focus on multi-word phrases. In linguistic
analysis, however, a phrase is not only a group of multiple
words, but also possibly a single word, as long as it functions
as a constituent in the syntax of a sentence [8]. As a great
portion (ranging from 10% to 30% on diﬀerent datasets based
on our experiments) of high-quality phrases, we should take
single-word phrases (e.g., dUIUCc, dIllinoisc, and dUSAc)
into consideration as well as multi-word phrases to achieve a
high recall in phrase mining.

Table 2: The results of AutoPhrase on the EN and CN datasets, with translations and explanations for Chinese
phrases. The whitespaces on the CN dataset are inserted by the Chinese tokenizer.

EN

CN

Sacramento Bee

Phrase
Elf Aquitaine
Arnold Sommerfeld
Eugene Wigner
Tarpon Springs
Sean Astin
. . .

Rank
1
2
3
4
5
. . .
20,001 ECAC Hockey
20,002
20,003 Bering Strait
20,004
Jacknife Lee
20,005 WXYZ-TV
. . .
. . .
99,994
John Gregson
99,995 white-tailed eagle
99,996

Translation (Explanation)
(the name of a soccer team)
Absinthe
(the name of a novel/TV-series)
notebook computer, laptop
Secretary of Party Committee
. . .
African countries
The Left (German: Die Linke)
Fraser Valley
Hippocampus
Mitsuki Saiga (a voice actress)
. . .

Phrase
江苏 舜 天
苦 艾 酒
白发 魔 女
笔记 型 电脑
党委 书记
. . .
非洲 国家
左翼 党
菲 沙 河谷
海马 体
斋 贺光希
. . .
计算机 科学技术 Computer Science and Technology
恒 天然
中国 作家 协会 The Vice President of Writers
副 主席
great spotted woodpecker 维他命 b
舆论 导向
. . .

Association of China
Vitamin B
controlled guidance of the media
. . .

99,997
99,998 David Manners
. . .

rhombic dodecahedron

Fonterra (a company)

. . .

Table 3: Eﬃciency Comparison between AutoPhrase and SegPhrase/WrapSegPhrase utilizing 10 threads.

EN
Time Memory
(mins)
32.77
369.53
11.27

(GB)
13.77
97.72
86%

ES
Time Memory
(mins)
54.05
452.85
8.37

(GB)
16.42
92.47
82%

CN
Time Memory
(mins)
9.43
108.58
11.50

(GB)
5.74
35.38
83%

AutoPhrase
(Wrap)SegPhrase
Speedup/Saving

(a) EN

(b) ES

(c) CN

Figure 10: Precision-recall curves evaluated by human annotation with both single-word and multi-word
phrases in pools.

ment of an automated phrase mining framework AutoPhrase.
Our extensive experiments show that AutoPhrase is domain-
independent, outperforms other phrase mining methods, and
supports multiple languages (e.g., English, Spanish, and
Chinese) eﬀectively, with minimal human eﬀort.

Besides, the inclusion of quality single-word phrases (e.g.,
dUIUCc and dUSAc) which leads to about 10% to 30% in-
creased recall and the exploration of better indexing strate-
gies and more thorough parallelization, which leads to about
8 to 11 times running time speedup and about 80% to 86%

memory usage saving over SegPhrase. Interested readers may
try our released code at GitHub.

For future work, it is interesting to (1) reﬁne quality
phrases to entity mentions, (2) apply AutoPhrase to more lan-
guages, such as Japanese, and (3) for those languages without
general knowledge bases, seek an unsupervised method to
generate the positive pool from the corpus, even with some
noise.

8. REFERENCES
[1] A. Allahverdyan and A. Galstyan. Comparative

A. Rajaraman. Towards the web of concepts:
Extracting concepts from large datasets. Proceedings of
the Very Large Data Bases Conference (VLDB),
3((1-2)), September 2010.

[20] Y. Park, R. J. Byrd, and B. K. Boguraev. Automatic

glossary extraction: beyond terminology identiﬁcation.
In COLING, 2002.

[21] V. Punyakanok and D. Roth. The use of classiﬁers in

sequential inference. In NIPS, 2001.

[22] H. Schmid. Treetagger| a language independent

part-of-speech tagger. Institut für Maschinelle
Sprachverarbeitung, Universität Stuttgart, 43:28, 1995.
[23] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. Kea: Practical automatic
keyphrase extraction. In Proceedings of the fourth ACM
conference on Digital libraries, pages 254–255. ACM,
1999.

[24] E. Xun, C. Huang, and M. Zhou. A uniﬁed statistical
model for the identiﬁcation of english basenp. In ACL,
2000.

[25] Z. Zhang, J. Iria, C. A. Brewster, and F. Ciravegna. A
comparative evaluation of term recognition algorithms.
LREC, 2008.

analysis of viterbi training and maximum likelihood
estimation for hmms. In NIPS, pages 1674–1682, 2011.

[2] L. Breiman. Randomizing outputs to increase

prediction accuracy. Machine Learning, 40(3):229–242,
2000.

[3] K.-h. Chen and H.-H. Chen. Extracting noun phrases
from large-scale texts: A hybrid approach and its
automatic evaluation. In ACL, 1994.

[4] M.-C. De Marneﬀe, B. MacCartney, C. D. Manning,

et al. Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454, 2006.

[5] P. Deane. A nonparametric method for extraction of

candidate phrasal terms. In ACL, 2005.

[6] A. El-Kishky, Y. Song, C. Wang, C. R. Voss, and
J. Han. Scalable topical phrase mining from text
corpora. VLDB, 8(3), Aug. 2015.

[7] D. A. Evans and C. Zhai. Noun-phrase analysis in
unrestricted text for information retrieval. In
Proceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 17–24.
Association for Computational Linguistics, 1996.
[8] G. Finch. Linguistic terms and concepts. Macmillan

Press Limited, 2000.

[9] K. Frantzi, S. Ananiadou, and H. Mima. Automatic

recognition of multi-word terms:. the c-value/nc-value
method. JODL, 3(2):115–130, 2000.

[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely

randomized trees. Machine learning, 63(1):3–42, 2006.

[11] T. Koo, X. Carreras, and M. Collins. Simple

semi-supervised dependency parsing. ACL-HLT, 2008.
[12] R. Levy and C. Manning. Is it harder to parse chinese,
or the chinese treebank? In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 439–446. Association for
Computational Linguistics, 2003.

[13] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. Mining

quality phrases from massive text corpora. In
Proceedings of 2015 ACM SIGMOD International
Conference on Management of Data, 2015.

[14] Z. Liu, X. Chen, Y. Zheng, and M. Sun. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
135–144. Association for Computational Linguistics,
2011.

[15] G. Martínez-Muñoz and A. Suárez. Switching class
labels to generate classiﬁcation ensembles. Pattern
Recognition, 38(10):1483–1494, 2005.

[16] R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.

Non-projective dependency parsing using spanning tree
algorithms. In EMNLP, 2005.

[17] R. Mihalcea and P. Tarau. Textrank: Bringing order

into texts. In ACL, 2004.

[18] J. Nivre, M.-C. de Marneﬀe, F. Ginter, Y. Goldberg,
J. Hajic, C. D. Manning, R. McDonald, S. Petrov,
S. Pyysalo, N. Silveira, et al. Universal dependencies
v1: A multilingual treebank collection. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016), 2016.

[19] A. Parameswaran, H. Garcia-Molina, and

Automated Phrase Mining from Massive Text Corpora

Jingbo Shang1, Jialu Liu2, Meng Jiang1, Xiang Ren1, Clare R Voss3, Jiawei Han1
1Computer Science Department, University of Illinois at Urbana-Champaign, IL, USA
2Google Research, New York City, NY, USA
3Computational & Information Sciences Directorate, Army Research Laboratory
2jialu@google.com

1{shang7, mjiang89, xren7, hanj}@illinois.edu

3clare.r.voss.civ@mail.mil

7
1
0
2
 
r
a

M
 
1
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
5
4
4
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
As one of the fundamental tasks in text analysis, phrase
mining aims at extracting quality phrases from a text corpus.
Phrase mining is important in various tasks such as informa-
tion extraction/retrieval, taxonomy construction, and topic
modeling. Most existing methods rely on complex, trained
linguistic analyzers, and thus likely have unsatisfactory per-
formance on text corpora of new domains and genres without
extra but expensive adaption. Recently, a few data-driven
methods have been developed successfully for extraction of
phrases from massive domain-speciﬁc text. However, none
of the state-of-the-art models is fully automated because
they require human experts for designing rules or labeling
phrases.

Since one can easily obtain many quality phrases from
public knowledge bases to a scale that is much larger than
that produced by human experts, in this paper, we propose
a novel framework for automated phrase mining, AutoPhrase,
which leverages this large amount of high-quality phrases in
an eﬀective way and achieves better performance compared
to limited human labeled phrases. In addition, we develop a
POS-guided phrasal segmentation model, which incorporates
the shallow syntactic information in part-of-speech (POS)
tags to further enhance the performance, when a POS tagger
is available. Note that, AutoPhrase can support any language
as long as a general knowledge base (e.g., Wikipedia) in that
language is available, while beneﬁting from, but not requiring,
a POS tagger. Compared to the state-of-the-art methods,
the new method has shown signiﬁcant improvements in eﬀec-
tiveness on ﬁve real-world datasets across diﬀerent domains
and languages.

1.

INTRODUCTION

Phrase mining refers to the process of automatic extrac-
tion of high-quality phrases (e.g., scientiﬁc terms and general
entity names) in a given corpus (e.g., research papers and
news). Representing the text with quality phrases instead

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

of n-grams can improve computational models for applica-
tions such as information extraction/retrieval, taxonomy
construction, and topic modeling.

Almost all the state-of-the-art methods, however, require
human experts at certain levels. Most existing methods [9,
20, 25] rely on complex, trained linguistic analyzers (e.g.,
dependency parsers) to locate phrase mentions, and thus
may have unsatisfactory performance on text corpora of new
domains and genres without extra but expensive adaption.
Our latest domain-independent method SegPhrase [13] out-
performs many other approaches [9, 20, 25, 5, 19, 6], but
still needs domain experts to ﬁrst carefully select hundreds
of varying-quality phrases from millions of candidates, and
then annotate them with binary labels.

Such reliance on manual eﬀorts by domain and linguis-
tic experts becomes an impediment for timely analysis of
massive, emerging text corpora in speciﬁc domains. An
ideal automated phrase mining method is supposed to be
domain-independent, with minimal human eﬀort or reliance
on linguistic analyzers1. Bearing this in mind, we propose
a novel automated phrase mining framework AutoPhrase in
this paper, going beyond SegPhrase, to further get rid of ad-
ditional manual labeling eﬀort and enhance the performance,
mainly using the following two new techniques.
1. Robust Positive-Only Distant Training.

In fact, many
high-quality phrases are freely available in general knowl-
edge bases, and they can be easily obtained to a scale
that is much larger than that produced by human ex-
perts. Domain-speciﬁc corpora usually contain some qual-
ity phrases also encoded in general knowledge bases, even
when there may be no other domain-speciﬁc knowledge
bases. Therefore, for distant training, we leverage the
existing high-quality phrases, as available from general
knowledge bases, such as Wikipedia and Freebase, to get
rid of additional manual labeling eﬀort. We independently
build samples of positive labels from general knowledge
bases and negative labels from the given domain corpora,
and train a number of base classiﬁers. We then aggregate
the predictions from these classiﬁers, whose independence
helps reduce the noise from negative labels.

2. POS-Guided Phrasal Segmentation. There is a trade-
oﬀ between the performance and domain-independence
when incorporating linguistic processors in the phrase
mining method. On the domain independence side, the

1The phrase “minimal human eﬀort” indicates using only
existing general knowledge bases without any other human
eﬀort.

accuracy might be limited without linguistic knowledge. It
is diﬃcult to support multiple languages, if the method is
completely language-blind. On the accuracy side, relying
on complex, trained linguistic analyzers may hurt the
domain-independence of the phrase mining method. For
example, it is expensive to adapt dependency parsers to
special domains like clinical reports. As a compromise, we
propose to incorporate a pre-trained part-of-speech (POS)
tagger to further enhance the performance, when it is
available for the language of the document collection. The
POS-guided phrasal segmentation leverages the shallow
syntactic information in POS tags to guide the phrasal
segmentation model locating the boundaries of phrases
more accurately.
In principle, AutoPhrase can support any language as long
as a general knowledge base in that language is available. In
fact, at least 58 languages have more than 100,000 articles
in Wikipedia as of Feb, 20172. Moreover, since pre-trained
part-of-speech (POS) taggers are widely available in many
languages (e.g., more than 20 languages in TreeTagger [22]3),
the POS-guided phrasal segmentation can be applied in many
scenarios. It is worth mentioning that for domain-speciﬁc
knowledge bases (e.g., MeSH terms in the biomedical domain)
and trained POS taggers, the same paradigm applies. In
this study, without loss of generality, we only assume the
availability of a general knowledge base together with a
pre-trained POS tagger.

As demonstrated in our experiments, AutoPhrase not only
works eﬀectively in multiple domains like scientiﬁc papers,
business reviews, and Wikipedia articles, but also supports
multiple languages, such as English, Spanish, and Chinese.

Our main contributions are highlighted as follows:

• We study an important problem, automated phrase mining,

and analyze its major challenges as above.

• We propose a robust positive-only distant training method
for phrase quality estimation to minimize the human eﬀort.
• We develop a novel phrasal segmentation model to leverage
POS tags to achieve further improvement, when a POS
tagger is available.

• We demonstrate the robustness and accuracy of our method
and show improvements over prior methods, with results
of experiments conducted on ﬁve real-world datasets in
diﬀerent domains (scientiﬁc papers, business reviews, and
Wikipedia articles) and diﬀerent languages (English, Span-
ish, and Chinese).

The rest of the paper is organized as follows. Section 2
positions our work relative to existing works. Section 3
deﬁnes basic concepts including four requirements of phrases.
The details of our method are covered in Section 4. Extensive
experiments and case studies are presented in Section 5. We
conclude the study in Section 7.

2. RELATED WORK

Identifying quality phrases eﬃciently has become ever
more central and critical for eﬀective handling of massively
increasing-size text datasets. In contrast to keyphrase ex-
traction [17, 23, 14], this task goes beyond the scope of
single documents and provides useful cross-document sig-
nals. The natural language processing (NLP) community

2https://meta.wikimedia.org/wiki/List_of_Wikipedias
3http://www.cis.uni-muenchen.de/~schmid/tools/
TreeTagger/

has conducted extensive studies typically referred to as au-
tomatic term recognition [9, 20, 25], for the computational
task of extracting terms (such as technical phrases). This
topic also attracts attention in the information retrieval (IR)
community [7, 19] since selecting appropriate indexing terms
is critical to the improvement of search engines where the
ideal indexing units represent the main concepts in a corpus,
not just literal bag-of-words.

Text indexing algorithms typically ﬁlter out stop words and
restrict candidate terms to noun phrases. With pre-deﬁned
part-of-speech (POS) rules, one can identify noun phrases
as term candidates in POS-tagged documents. Supervised
noun phrase chunking techniques [21, 24, 3] exploit such
tagged documents to automatically learn rules for identifying
noun phrase boundaries. Other methods may utilize more
sophisticated NLP technologies such as dependency parsing
to further enhance the precision [11, 16]. With candidate
terms collected, the next step is to leverage certain statistical
measures derived from the corpus to estimate phrase quality.
Some methods rely on other reference corpora for the calibra-
tion of “termhood” [25]. The dependency on these various
kinds of linguistic analyzers, domain-dependent language
rules, and expensive human labeling, makes it challenging to
extend these approaches to emerging, big, and unrestricted
corpora, which may include many diﬀerent domains, topics,
and languages.

To overcome this limitation, data-driven approaches opt
instead to make use of frequency statistics in the corpus to
address both candidate generation and quality estimation [5,
19, 6, 13]. They do not rely on complex linguistic feature
generation, domain-speciﬁc rules or extensive labeling eﬀorts.
Instead, they rely on large corpora containing hundreds
of thousands of documents to help deliver superior perfor-
mance [13]. In [19], several indicators, including frequency
and comparison to super/sub-sequences, were proposed to
extract n-grams that reliably indicate frequent, concise con-
cepts. Deane [5] proposed a heuristic metric over frequency
distribution based on Zipﬁan ranks, to measure lexical associ-
ation for phrase candidates. As a preprocessing step towards
topical phrase extraction, El-Kishky et al. [6] proposed to
mine signiﬁcant phrases based on frequency as well as doc-
ument context following a bottom-up fashion, which only
considers a part of quality phrase criteria, popularity and
concordance. Our previous work [13] succeeded at integrat-
ing phrase quality estimation with phrasal segmentation to
further rectify the initial set of statistical features, based on
local occurrence context. Unlike previous methods which
are purely unsupervised,
[13] required a small set of phrase
labels to train its phrase quality estimator. It is worth men-
tioning that all these approaches still depend on the human
eﬀort (e.g., setting domain-sensitive thresholds). Therefore,
extending them to work automatically is challenging.

3. PRELIMINARIES

The goal of this paper is to develop an automated phrase
mining method to extract quality phrases from a large collec-
tion of documents without human labeling eﬀort, and with
only limited, shallow linguistic analysis. The main input to
the automated phrase mining task is a corpus and a knowl-
edge base. The input corpus is a textual word sequence
in a particular language and a speciﬁc domain, of arbitrary
length. The output is a ranked list of phrases with decreasing
quality.

Figure 1: The overview of AutoPhrase. The two novel techniques developed in this paper are highlighted.

The AutoPhrase framework is shown in Figure 1. The
work ﬂow is completely diﬀerent form our previous domain-
independent phrase mining method requiring human ef-
fort [13], although the phrase candidates and the features
used during phrase quality (re-)estimation are the same. In
this paper, we propose a robust positive-only distant train-
ing to minimize the human eﬀort and develop a POS-guided
phrasal segmentation model to improve the model perfor-
mance. In this section, we brieﬂy introduce basic concepts
and components as preliminaries.

A phrase is deﬁned as a sequence of words that appear
consecutively in the text, forming a complete semantic unit
in certain contexts of the given documents [8]. The phrase
quality is deﬁned to be the probability of a word sequence
being a complete semantic unit, meeting the following crite-
ria [13]:
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Concordance: The collocation of tokens in quality phrases
occurs with signiﬁcantly higher probability than expected
due to chance.

• Informativeness: A phrase is informative if it is indica-

tive of a speciﬁc topic or concept.

• Completeness: Long frequent phrases and their subse-
quences within those phrases may both satisfy the 3 criteria
above. A phrase is deemed complete when it can be inter-
preted as a complete semantic unit in some given document
context. Note that a phrase and a subphrase contained
within it, may both be deemed complete, depending on
the context in which they appear. For example, “relational
database system”, “relational database” and “database sys-
tem” can all be valid in certain context.

AutoPhrase will estimate the phrase quality based on the
positive and negative pools twice, once before and once
after the POS-guided phrasal segmentation. That is, the
POS-guided phrasal segmentation requires an initial set of
phrase quality scores; we estimate the scores based on raw
frequencies beforehand; and then once the feature values
have been rectiﬁed, we re-estimate the scores.

Only the phrases satisfying all above requirements are

recognized as quality phrases.

Example 1. “strong tea” is a quality phrase while “heavy
tea” fails to be due to concordance. “this paper” is a popular
and concordant phrase, but is not informative in research
publication corpus. “NP-complete in the strong sense” is a
quality phrase while “NP-complete in the strong” fails to be
due to completeness. (cid:3)

To automatically mine these quality phrases, the ﬁrst phase
of AutoPhrase (see leftmost box in Figure 1) establishes the
set of phrase candidates that contains all n-grams over

the minimum support threshold τ (e.g., 30) in the corpus.
Here, this threshold refers to raw frequency of the n-grams
calculated by string matching.
In practice, one can also
set a phrase length threshold (e.g., n ≤ 6) to restrict the
number of words in any phrase. Given a phrase candidate
w1w2 . . . wn, its phrase quality is:

Q(w1w2 . . . wn) = p(dw1w2 . . . wnc|w1w2 . . . wn) ∈ [0, 1]
where dw1w2 . . . wnc refers to the event that these words
constitute a phrase. Q(·), also known as the phrase quality
estimator, is initially learned from data based on statistical
features4, such as point-wise mutual information, point-wise
KL divergence, and inverse document frequency, designed
to model concordance and informativeness mentioned above.
Note the phrase quality estimator is computed independent
of POS tags. For unigrams, we simply set their phrase quality
as 1.

Example 2. A good quality estimator will return Q(this paper) ≈

0 and Q(relational database system) ≈ 1. (cid:3)

Then, to address the completeness criterion, the phrasal
segmentation ﬁnds the best segmentation for each sentence.

Example 3. Ideal phrasal segmentation results are as fol-

lows.

(cid:3)

#1:
... / the / Great Firewall / is / ...
#2: This / is / a / great / ﬁrewall software/ .
#3: The / discriminative classiﬁer / SVM / is / ...

During the phrase quality re-estimation, related sta-
tistical features will be re-computed based on the rectiﬁed
frequency of phrases, which means the number of times that
a phrase becomes a complete semantic unit in the identiﬁed
segmentation. After incorporating the rectiﬁed frequency, the
phrase quality estimator Q(·) also models the completeness
in addition to concordance and informativeness.

Example 4. Continuing the previous example, the raw
frequency of the phrase “great ﬁrewall” is 2, but its rectiﬁed
frequency is 1. Both the raw frequency and the rectiﬁed
frequency of the phrase “ﬁrewall software” are 1. The raw
frequency of the phrase “classiﬁer SVM” is 1, but its rectiﬁed
frequency is 0. (cid:3)

4. METHODOLOGY

In this section, we focus on introducing our two new tech-

niques.
4See https://github.com/shangjingbo1226/AutoPhrase for
further details

Figure 2: The illustration of each base classiﬁer. In
each base classiﬁer, we ﬁrst randomly sample K pos-
itive and negative labels from the pools respectively.
There might be δ quality phrases among the K neg-
ative labels. An unpruned decision tree is trained
based on this perturbed training set.
4.1 Robust Positive-Only Distant Training

To estimate the phrase quality score for each phrase can-
didate, our previous work [13] required domain experts to
ﬁrst carefully select hundreds of varying-quality phrases from
millions of candidates, and then annotate them with binary
labels. For example, for computer science papers, our domain
experts provided hundreds of positive labels (e.g., “spanning
tree” and “computer science”) and negative labels (e.g., “pa-
per focuses” and “important form of ”). However, creating
such a label set is expensive, especially in specialized do-
mains like clinical reports and business reviews, because this
approach provides no clues for how to identify the phrase
candidates to be labeled.
In this paper, we introduce a
method that only utilizes existing general knowledge bases
without any other human eﬀort.

4.1.1 Label Pools

Public knowledge bases (e.g., Wikipedia) usually encode
a considerable number of high-quality phrases in the titles,
keywords, and internal links of pages. For example, by ana-
lyzing the internal links and synonyms5 in English Wikipedia,
more than a hundred thousand high-quality phrases were
discovered. As a result, we place these phrases in a positive
pool.

Knowledge bases, however, rarely, if ever, identify phrases
that fail to meet our criteria, what we call inferior phrases.
An important observation is that the number of phrase can-
didates, based on n-grams (recall leftmost box of Figure 1),
is huge and the majority of them are actually of of inferior
quality (e.g., “Francisco opera and”).
In practice, based
on our experiments, among millions of phrase candidates,
usually, only about 10% are in good quality. Therefore,
phrase candidates that are derived from the given corpus
but that fail to match any high-quality phrase derived from
the given knowledge base, are used to populate a large but
noisy negative pool.

4.1.2 Noise Reduction

Directly training a classiﬁer based on the noisy label pools
is not a wise choice: some phrases of high quality from
the given corpus may have been missed (i.e., inaccurately
binned into the negative pool) simply because they were
Instead, we propose
not present in the knowledge base.
to utilize an ensemble classiﬁer that averages the results
of T independently trained base classiﬁers. As shown in
Figure 2, for each base classiﬁer, we randomly draw K phrase
candidates with replacement from the positive pool and the
negative pool respectively (considering a canonical balanced
classiﬁcation scenario). This size-2K subset of the full set of

5https://github.com/kno10/WikipediaEntities

all phrase candidates is called a perturbed training set [2],
because the labels of some (δ in the ﬁgure) quality phrases
In order for the
are switched from positive to negative.
ensemble classiﬁer to alleviate the eﬀect of such noise, we
need to use base classiﬁers with the lowest possible training
errors. We grow an unpruned decision tree to the point of
separating all phrases to meet this requirement.
In fact,
such decision tree will always reach 100% training accuracy
when no two positive and negative phrases share identical
feature values in the perturbed training set. In this case,
δ
its ideal error is
2K , which approximately equals to the
proportion of switched labels among all phrase candidates
δ
2K ≈ 10%). Therefore, the value of K is not sensitive
(i.e.,
to the accuracy of the unpruned decision tree and is ﬁxed as
100 in our implementation. Assuming the extracted features
are distinguishable between quality and inferior phrases, the
empirical error evaluated on all phrase candidates, p, should
be relatively small as well.

An interesting property of this sampling procedure is that
the random selection of phrase candidates for building per-
turbed training sets creates classiﬁers that have statistically
independent errors and similar erring probability [2, 15].
Therefore, we naturally adopt random forest [10], which is
veriﬁed, in the statistics literature, to be robust and eﬃcient.
The phrase quality score of a particular phrase is computed
as the proportion of all decision trees that predict that phrase
is a quality phrase. Suppose there are T trees in the random
forest, the ensemble error can be estimated as the probability
of having more than half of the classiﬁers misclassifying a
given phrase candidate as follows.

ensemble_ error(T ) =

pt(1 − p)T −t

T
X

(cid:19)

(cid:18)T
t

t=b1+T /2c

From Figure 3, one can
easily observe that the en-
semble error is approach-
ing 0 when T grows. In
practice, T needs to be
set larger due to the ad-
ditional error brought by
model bias. Empirical
studies can be found in
Figure 7.

Figure 3: Ensemble er-
rors of diﬀerent p’s vary-
ing T .

4.2 POS-Guided Phrasal Segmentation

Phrasal segmentation addresses the challenge of measuring
completeness (our fourth criterion) by locating all phrase
mentions in the corpus and rectifying their frequencies ob-
tained originally via string matching.

The corpus is processed as a length-n POS-tagged word
sequence Ω = Ω1Ω2 . . . Ωn, where Ωi refers to a pair con-
sisting of a word and its POS tag hwi, tii. A POS-guided
phrasal segmentation is a partition of this sequence into
m segments induced by a boundary index sequence B =
{b1, b2, . . . , bm+1} satisfying 1 = b1 < b2 < . . . < bm+1 =
n+1. The i-th segment refers to Ωbi Ωbi+1 . . . Ωbi+1−1.

Compared to the phrasal segmentation in our previous
work [13], the POS-guided phrasal segmentation addresses
the completeness requirement in a context-aware way, instead
of equivalently penalizing phrase candidates of the same
length. In addition, POS tags provide shallow, language-
speciﬁc knowledge, which may help boost phrase detection

Algorithm 1: POS-Guided Phrasal Segmentation (PGPS)

Input: Corpus Ω = Ω1Ω2 . . . Ωn, phrase quality Q,
parameters θu and δ(tx, ty).
Output: Optimal boundary index sequence B.
// hi ≡ maxB p(Ω1Ω2 . . . Ωi−1, B|Q, θ, δ)
h1 ← 1, hi ← 0 (1 < i ≤ n + 1)
for i = 1 to n do

for j = i + 1 to min(i + length threshold, n + 1) do

// Efficiently implemented via Trie.
if there is no phrase starting with w[i,j) then

break

// In practice, log and addition are used

to avoid underflow.

if hi × p(j, dw[i,j)c|i, t[i,j)) > hj then
hj ← hi × p(j, dw[i,j)c|i, t[i,j))
gj ← i
j ← n + 1, m ← 0
while j > 1 do
m ← m + 1
bm ← j
j ← gj

return B ← 1, bm, bm−1, . . . , b1

accuracy, especially at syntactic constituent boundaries for
that language.

Given the POS tag sequence for the full n-length corpus
is t = t1t2 . . . tn, containing the tag subsequence tl . . . tr−1
(denote as t[l,r) for clarity), the POS quality score for that
tag subsequence is deﬁned to be the conditional probability of
its corresponding word sequence being a complete semantic
unit. Formally, we have

T (t[l,r)) = p(dwl . . . wrc|t) ∈ [0, 1]

The POS quality score T (·) is designed to reward the phrases
with their correctly identiﬁed POS sequences, as follows.

Example 5. Suppose the whole POS tag sequence is “NN
NN NN VB DT NN”. A good POS sequence quality estima-
tor might return T (NN NN NN) ≈ 1 and T (NN VB) ≈ 0,
where NN refers to singular or mass noun (e.g., database),
VB means verb in the base form (e.g., is), and DT is for
determiner (e.g., the). (cid:3)

The particular form of T (·) we have chosen is:

T (t[l,r)) = (1 − δ(tbr −1, tbr )) ×

δ(tj−1, tj)

r−1
Y

j=l+1

where, δ(tx, ty) is the probability that the POS tag tx is
adjacent and precedes POS tag ty within a phrase in the given
document collection. In this formula, the ﬁrst term indicates
the probability that there is a phrase boundary between the
words indexed r − 1 and r, while the latter product indicates
the probability that all POS tags within t[l,r) are in the
same phrase. This POS quality score can naturally counter
the bias to longer segments because ∀i > 1, exactly one of
δ(ti−1, ti) and (1 − δ(ti−1, ti)) is always multiplied no matter
how the corpus is segmented. Note that the length penalty
model in our previous work [13] is a special case when all
values of δ(tx, ty) are the same.

Mathematically, δ(tx, ty) is deﬁned as:

δ(tx, ty) = p(d. . . w1w2 . . .c|Ω, tag(w1) = tx ∧ tag(w2) = ty)

As it depends on how documents are segmented into phrases,
δ(tx, ty) is initialized uniformly and will be learned during
the phrasal segmentation.

Now, after we have both phrase quality Q(·) and POS
quality T (·) ready, we are able to formally deﬁne the POS-
guided phrasal segmentation model. The joint probability of
a POS tagged sequence Ω and a boundary index sequence
B = {b1, b2, . . . , bm+1} is factorized as:

p(Ω, B) =

p

bi+1, dw[bi,bi+1)c

(cid:17)

(cid:12)
(cid:12)
(cid:12)bi, t

m
Y

(cid:16)

i=1

where p(bi+1, dw[bi,bi+1)c|bi, t) is the probability of observing
a word sequence w[bi,bi+1) as the i-th quality segment given
the previous boundary index bi and the whole POS tag
sequence t.

Since the phrase segments function as a constituent in the
syntax of a sentence, they usually have weak dependence on
each other [8, 13]. As a result, we assume these segments in
the word sequence are generated one by one for the sake of
both eﬃciency and simplicity.

For each segment, given the POS tag sequence t and the

start index bi, the generative process is deﬁned as follows.
1. Generate the end index bi+1, according to its POS quality

p(bi+1|bi, t) = T (t[bi,bi+1))

2. Given the two ends bi and bi+1, generate the word se-
quence w[bi,bi+1) according to a multinomial distribution
over all segments of length-(bi+1 − bi).

p(w[bi,bi+1)|bi, bi+1) = p(w[bi,bi+1)|bi+1 − bi)

3. Finally, we generate an indicator whether w[bi,bi+1) forms

a quality segment according to its quality

p(dw[bi,bi+1)c|w[bi,bi+1)) = Q(w[bi,bi+1))
We denote p(w[bi,bi+1)|bi+1 −bi) as θw[bi,bi+1) for convenience.
Integrating the above three generative steps together, we
have the following probabilistic factorization:

p(bi+1, dw[bi,bi+1)c|bi, t)

=p(bi+1|bi, t)p(w[bi,bi+1)|bi, bi+1)p(dw[bi,bi+1)c|w[bi,bi+1))
=T (t[bi,bi+1))θw[bi,bi+1) Q(w[bi,bi+1))

Therefore, there are three subproblems:

1. Learn θu for each word and phrase candidate u;
2. Learn δ(tx, ty) for every POS tag pair; and
3. Infer B when θu and δ(tx, ty) are ﬁxed.

We employ the maximum a posterior principle and maxi-

mize the joint log likelihood:

log p(Ω, B) =

log p

(cid:16)

(cid:12)
(cid:12)
(cid:12)bt, t
bi+1, dw[bi,bi+1)c

(cid:17)

(1)

m
X

i=1

Given θu and δ(tx, ty), to ﬁnd the best segmentation that
maximizes Equation (1), we develop an eﬃcient dynamic
programming algorithm for the POS-guided phrasal segmen-
tation as shown in Algorithm 1.

When the segmentation S and the parameter θ are ﬁxed,

the closed-form solution of δ(tx, ty) is:

δ(tx, ty) =

Pm

i=1

Pbi+1−2
j=bi

Pn−1

1(tj = tx ∧ tj+1 = ty)

(2)

i=1 1(ti = tx ∧ ti+1 = ty)

Algorithm 2: Viterbi Training (VT)

Input: Corpus Ω and phrase quality Q.
Output: θu and δ(tx, ty).
initialize θ with normalized raw frequencies in the corpus
while θu does not converge do

while δ(tx, ty) does not converge do

B ← best segmentation via Alg. 1
update δ(tx, ty) using B according to Eq. (2)

B ← best segmentation via Alg. 1
update θu using B according to Eq. (3)

return θu and δ(tx, ty)

where 1(·) denotes the identity indicator. δ(tx, ty) is the
unsegmented ratio among all htx, tyi pairs in the given corpus.
Similarly, once the segmentation S and the parameter δ
are ﬁxed, the closed-form solution of θu can be derived as:

θu =

Pm
Pm

i=1 1(w[bi,bi+1) = u)
i=1 1(bi+1 − bi = |u|)

(3)

We can see that θu is the times that u becomes a com-
plete segment normalized by the number of the length-|u|
segments.

As shown in Algorithm 2, we choose Viterbi Training, or
Hard EM in literature [1] to iteratively optimize parameters,
because Viterbi Training converges fast and results in sparse
and simple models for Hidden Markov Model-like tasks [1].

4.3 Complexity Analysis

The time complexity of the most time consuming compo-
nents in our framework, such as frequent n-gram, feature
extraction, POS-guided phrasal segmentation, are all O(|Ω|)
with the assumption that the maximum number of words in
a phrase is a small constant (e.g., n ≤ 6), where |Ω| is the
total number of words in the corpus. Therefore, AutoPhrase
is linear to the corpus size and thus being very eﬃcient and
scalable. Meanwhile, every component can be parallelized
in an almost lock-free way grouping by either phrases or
sentences.

5. EXPERIMENTS

In this section, we will apply the proposed method to mine
quality phrases from ﬁve massive text corpora across three
domains (scientiﬁc papers, business reviews, and Wikipedia
articles) and in three languages (English, Spanish, and Chi-
nese). We compare the proposed method with many other
methods to demonstrate its high performance. Then we
explore the robustness of the proposed positive-only distant
training and its performance against expert labeling. The
advantage of incorporating POS tags in phrasal segmentation
will also be proved. In the end, we present case studies.

5.1 Datasets

To validate that the proposed positive-only distant train-
ing can eﬀectively work in diﬀerent domains and the POS-
guided phrasal segmentation can support multiple languages
eﬀectively, we have ﬁve large collections of text in diﬀerent
domains and languages, as shown in Table 1: Abstracts
of English computer science papers from DBLP6, English

6https://aminer.org/citation

Table 1: Five real-world massive text corpora in dif-
|Ω| is the
ferent domains and multiple languages.
total number of words. sizep is the size of positive
pool.

Dataset
DBLP
Yelp
EN
ES
CN

Domain
Scientiﬁc Paper
Business Review
Wikipedia Article
Wikipedia Article
Wikipedia Article

Language
English
English
English
Spanish
Chinese

|Ω|

sizep
File size
29K
91.6M 618MB
145.1M 749MB
22K
808.0M 3.94GB 184K
65K
791.2M 4.06GB
29K
371.9M 1.56GB

business reviews from Yelp7, Wikipedia articles8 in English
(EN), Spanish (ES), and Chinese (CN). From the exist-
ing general knowledge base Wikipedia, we extract popular
mentions of entities by analyzing intra-Wiki citations within
Wiki content9. On each dataset, the intersection between the
extracted popular mentions and the generated phrase candi-
dates forms the positive pool. Therefore, the size of positive
pool may vary in diﬀerent datasets of the same language.

5.2 Compared Methods

We compare AutoPhrase with three lines of methods as

follows. Every method returns a ranked list of phrases.
SegPhrase10/WrapSegPhrae11: In English domain-speciﬁc
text corpora, our latest work SegPhrase outperformed phrase
mining [6], keyphrase extraction [23, 19], and noun phrase
chunking methods. WrapSegPhrase extends SegPhrase to dif-
ferent languages by adding an encoding preprocessing to
ﬁrst transform non-English corpus using English characters
and punctuation as well as a decoding postprocessing to
later translate them back to the original language. Both
methods require domain expert labors. For each dataset, we
ask domain experts to annotate a representative set of 300
quality/interior phrases.
Parser-based Phrase Extraction: Using complicated lin-
guistic processors, such as parsers, we can extract minimum
phrase units (e.g., NP) from the parsing trees as phrase
candidates. Parsers of all three languages are available in
Stanford NLP tools [18, 4, 12]. Two ranking heuristics are
considered:
• TF-IDF ranks the extracted phrases by their term fre-
quency and inverse document frequency in the given docu-
ments;

• TextRank: An unsupervised graph-based ranking model

for keyword extraction [17].

Pre-trained Chinese Segmentation Models: Diﬀerent
from English and Spanish, phrasal segmentation in Chi-
nese has been intensively studied because there is no space
between Chinese words. The most eﬀective and popular
segmentation methods are:
• AnsjSeg12 is a popular text segmentation algorithm for
Chinese corpus. It ensembles statistical modeling methods
of Conditional Random Fields (CRF) and Hidden Markov
Models (HMMs) based on the n-gram setting;

• JiebaPSeg13 is a Chinese text segmentation method im-

7https://www.yelp.com/dataset_challenge
8https://dumps.wikimedia.org/
9https://github.com/kno10/WikipediaEntities
10https://github.com/shangjingbo1226/SegPhrase
11https://github.com/remenberl/SegPhrase-MultiLingual
12https://github.com/NLPchina/ansj_seg
13https://github.com/fxsjy/jieba

(a) DBLP

(b) Yelp

(c) EN

(d) ES

(e) CN

Figure 4: Overall Performance Evaluation: Precision-recall curves of all methods evaluated by human anno-
tation.

plemented in Python. It builds a directed acyclic graph
for all possible phrase combinations based on a preﬁx
dictionary structure to achieve eﬃcient phrase graph scan-
ning. Then it uses dynamic programming to ﬁnd the most
probable combination based on the phrase frequency. For
unknown phrases, an HMM-based model is used with the
Viterbi algorithm.

Note that all parser-based phrase extraction and Chinese
segmentation models are pre-trained based on general corpus.
To study the eﬀectiveness of the POS-guided segmenta-
tion, AutoSegPhrase adopts the length penalty instead of
δ(tx, ty), while other components are the same as AutoPhrase.
AutoSegPhrase is useful when there is no POS tagger.

5.3 Experimental Settings
Implementation. The preprocessing includes tokenizers
from Lucene and Stanford NLP as well as the POS tagger
from TreeTagger. Our documented code package has been
released and maintained in GitHub14.
Default Parameters. We set the minimum support thresh-
old σ as 30. The maximum number of words in a phrase is
set as 6 according to labels from domain experts. These are
two parameters required by all methods. Other parameters
required by compared methods were set according to the
open-source tools or the original papers.
Human Annotation. We rely on human evaluators to
judge the quality of the phrases which cannot be identiﬁed
through any knowledge base. More speciﬁcally, on each
dataset, we randomly sample 500 such phrases from the
predicted phrases of each method in the experiments. These
selected phrases are shuﬄed in a shared pool and evaluated
by 3 reviewers independently. We allow reviewers to use
search engines when unfamiliar phrases encountered. By the
rule of majority voting, phrases in this pool received at least
two positive annotations are quality phrases. The intra-class
correlations (ICCs) are all more than 0.9 on all ﬁve datasets,
which shows the agreement.
Evaluation Metrics. For a list of phrases, precision is
deﬁned as the number of true quality phrases divided by the
number of predicted quality phrases; recall is deﬁned as the
number of true quality phrases divided by the total number
of quality phrases. We retrieve the ranked list of the pool
from the outcome of each method. When a new true quality
phrase encountered, we evaluate the precision and recall of
this ranked list.
In the end, we plot the precision-recall
curves. In addition, area under the curve (AUC) is adopted
as another quantitative measure. AUC in this paper refers
to the area under the precision-recall curve.

14https://github.com/shangjingbo1226/AutoPhrase

5.4 Overall Performance

Figures 4 presents the precision-recall curves of all com-
pared methods evaluated by human annotation on ﬁve datasets.
Overall, AutoPhrase performs the best, in terms of both pre-
cision and recall. Signiﬁcant advantages can be observed,
especially on two non-English datasets ES and CN. For
example, on the ES dataset, the recall of AutoPhrase is
about 20% higher than the second best method (SegPhrase)
in absolute value. Meanwhile, there is a visible precision
gap between AutoPhrase and the best baseline. The phrase
chunking-based methods TF-IDF and TextRank work poorly,
because the extraction and ranking are modeled separately
and the pre-trained complex linguistic analyzers fail to ex-
tend to domain-speciﬁc corpora. TextRank usually starts
with a higher precision than TF-IDF, but its recall is very
low because of the sparsity of the constructed co-occurrence
graph. TF-IDF achieves a reasonable recall but unsatisfac-
tory precision. On the CN dataset, the pre-trained Chinese
segmentation models, JiebaSeg and AnsjSeg, are very com-
petitive, because they not only leverage training data for
segmentations, but also exhaust the engineering work, in-
cluding a huge dictionary for popular Chinese entity names
and speciﬁc rules for certain types of entities. As a conse-
quence, they can easily extract tons of well-known terms
and people/location names. Outperforming such a strong
baseline further conﬁrms the eﬀectiveness of AutoPhrase.

The comparison among the English datasets across three
domains (i.e., scientiﬁc papers, business reviews, and Wikipedia
articles) demonstrates that AutoPhrase is reasonably domain-
independent. The performance of parser-based methods
TF-IDF and TextRank depends on the rigorous degree of
the documents. For example, it works well on the DBLP
dataset but poorly on the Yelp dataset. However, without
any human eﬀort, AutoPhrase can work eﬀectively on domain-
speciﬁc datasets, and even outperforms SegPhrase, which is
supervised by the domain experts.

The comparison among the Wikipedia article datasets in
three languages (i.e., EN, ES, and CN ) shows that, ﬁrst
of all, AutoPhrase supports multiple languages. Secondly,
the advantage of AutoPhrase over SegPhrase/WrapSegPhrase
is more obvious on two non-English datasets ES and CN
than the EN dataset, which proves that the helpfulness of
introducing the POS tagger.

As conclusions, AutoPhrase is able to support diﬀerent
domains and support multiple languages with minimal human
eﬀort.

5.5 Distant Training Exploration

To compare the distant training and domain expert label-
ing, we experiment with the domain-speciﬁc datasets DBLP

(a) DBLP

(b) Yelp

(a) DBLP

(b) Yelp

Figure 5: AUC curves of four variants when we have
enough positive labels in the positive pool EP.

Figure 6: AUC curves of four variants after we ex-
haust positive labels in the positive pool EP.

and Yelp. To be fair, all the conﬁgurations in the classiﬁers
are the same except for the label selection process. More
speciﬁcally, we come up with four training pools:
1. EP means that domain experts give the positive pool.
2. DP means that a sampled subset from existing general

knowledge forms the positive pool.

3. EN means that domain experts give the negative pool.
4. DN means that all unlabeled (i.e., not in the positive

pool) phrase candidates form the negative pool.

By combining any pair of the positive and negative pools,
we have four variants, EPEN (in SegPhrase), DPDN (in
AutoPhrase), EPDN, and DPEN.

First of all, we evaluate the performance diﬀerence in the
two positive pools. Compared to EPEN, DPEN adopts a
positive pool sampled from knowledge bases instead of the
well-designed positive pool given by domain experts. The
negative pool EN is shared. As shown in Figure 5, we vary the
size of the positive pool and plot their AUC curves. We can
ﬁnd that EPEN outperforms DPEN and the trends of curves
on both datasets are similar. Therefore, we conclude that the
positive pool generated from knowledge bases has reasonable
quality, although its corresponding quality estimator works
slightly worse.

Secondly, we verify that whether the proposed noise re-
duction mechanism works properly. Compared to EPEN,
EPDN adopts a negative pool of all unlabeled phrase can-
didates instead of the well-designed negative pool given by
domain experts. The positive pool EP is shared. In Fig-
ure 5, the clear gap between them and the similar trends on
both datasets show that the noisy negative pool is slightly
worse than the well-designed negative pool, but it still works
eﬀectively.

As illustrated in Figure 5, DPDN has the worst perfor-
mance when the size of positive pools are limited. However,
distant training can generate much larger positive pools,
which may signiﬁcantly beyond the ability of domain experts
considering the high expense of labeling. Consequently, we
are curious whether the distant training can ﬁnally beat do-
main experts when positive pool sizes become large enough.
We call the size at this tipping point as the ideal number.
We increase positive pool sizes and plot AUC curves of
DPEN and DPDN, while EPEN and EPDN are degenerated
as dashed lines due to the limited domain expert abilities.
As shown in Figure 6, with a large enough positive pool,
distant training is able to beat expert labeling. On the
DBLP dataset, the ideal number is about 700, while on the
Yelp dataset, it becomes around 1600. Our guess is that the
ideal training size is proportional to the number of words

(a) EN

(b) ES

(c) CN

Figure 8: Precision-recall curves of AutoPhrase and
AutoSegPhrase.

(e.g., 91.6M in DBLP and 145.1M in Yelp). We notice that
compared to the corpus size, the ideal number is relatively
small, which implies the distant training should be eﬀective
in many domain-speciﬁc corpora as if they overlap with
Wikipedia.

Besides, Figure 6 shows that when the positive pool size
continues growing, the AUC score increases but the slope
becomes smaller. The performance of distant training will
be ﬁnally stable when a relatively large number of quality
phrases were fed.

We are curious how
many trees (i.e., T ) is
enough for DPDN. We
increase T and plot AUC
As
curves of DPDN.
shown in Figure 7, on
both datasets,
as T
grows, the AUC scores
ﬁrst increase rapidly and
the speed slows
later
down gradually, which is
consistent with the theoretical analysis in Section 4.1.2.

Figure 7: AUC curves of
DPDN varying T .

5.6 POS-guided Phrasal Segmentation

We are also interested in how much performance gain we
can obtain from incorporating POS tags in this segmentation
model, especially for diﬀerent languages. We select Wikipedia
article datasets in three diﬀerent languages: EN, ES, and
CN.

Figure 8 compares the results of AutoPhrase and AutoSegPhrase,

with the best baseline methods as references. AutoPhrase
outperforms AutoSegPhrase even on the English dataset EN,
though it has been shown the length penalty works rea-
sonably well in English [13]. The Spanish dataset ES has
similar observation. Moreover, the advantage of AutoPhrase
becomes more signiﬁcant on the CN dataset, indicating the
poor generality of length penalty.

(a) Running Time (b) Peak Memory (c) Multi-threading

Figure 9: Eﬃciency of AutoPhrase.

tive of a speciﬁc topic or concept.

Considering the criteria of quality phrases, because single-
word phrases cannot be decomposed into two or more parts,
the concordance and completeness are no longer deﬁnable.
Therefore, we revise the requirements for quality single-
word phrases as below.
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Informativeness: A phrase is informative if it is indica-

• Independence: A quality single-word phrase is more likely

a complete semantic unit in the given documents.

Only single-word phrases satisfying all popularity, indepen-
dence, and informativeness requirements are recognized as
quality single-word phrases.

Example 6. “UIUC” is a quality single-word phrase. “this”
is not a quality phrase due to its low informativeness. “united”,
usually occurring within other quality multi-word phrases such
as “United States”, “United Kingdom”, “United Airlines”,
and “United Parcel Service”, is not a quality single-word
phrase, because its independence is not enough.

After the phrasal segmentation, in replacement of con-
cordance features, the independence feature is added for
single-word phrases. Formally, it is the ratio of the rectiﬁed
frequency of a single-word phrase given the phrasal segmen-
tation over its raw frequency. Quality single-word phrases
are expected to have large values. For example, “united” is
likely to an almost zero ratio.

We use AutoPhrase+ to denote the extended AutoPhrase

with quality single-word phrase estimation.

6.2 Experiments

We have a similar human annotation as that in the pa-
per. Diﬀerently, we randomly sampled 500 Wiki-uncovered
phrases from the returned phrases (both single-word and
multi-word phrases) of each method in experiments of the
paper. Therefore, we have new pools on the EN, ES, and CN
datasets. The intra-class correlations (ICCs) are all more
than 0.9, which shows the agreement.

Figure 10 compare all methods based these new pools.
Note that all methods except for SegPhrase/WrapSegPhrase
extract single-word phrases as well.

Signiﬁcant recall advantages can be always observed on
all EN, ES, and CN datasets. The recall diﬀerences be-
tween AutoPhrase+ and AutoPhrase, ranging from 10% to
30% sheds light on the importance of modeling single-word
phrases. Across two Latin language datasets, EN and ES,
AutoPhrase+ and AutoPhrase overlaps in the beginning, but
later, the precision of AutoPhrase drops earlier and has a
lower recall due to the lack of single-word phrases. On the
CN dataset, AutoPhrase+ and AutoPhrase has a clear gap
even in the very beginning, which is diﬀerent from the trends
on the EN and ES datasets, which reﬂects that single-word
phrases are more important in Chinese. The major reason
behind is that there are a considerable number of high-quality
phrases (e.g., person names) in Chinese have only one token
after tokenization.

7. CONCLUSIONS

In this paper, we present an automated phrase mining
framework with two novel techniques: the robust positive-
only distant training and the POS-guided phrasal segmenta-
tion incorporating part-of-speech (POS) tags, for the develop-

In summary, thanks to the extra context information and
syntactic information for the particular language, incorpo-
rating POS tags during the phrasal segmentation can work
better than equally penalizing phrases of the same length.

5.7 Case Study

We present a case study about the extracted phrases
as shown in Table 2. The top ranked phrases are mostly
named entities, which makes sense for the Wikipedia article
datasets. Even in the long tail part, there are still many high-
quality phrases. For example, we have the dgreat spotted
woodpeckerc (a type of birds) and d计算机 科学技术c (i.e.,
Computer Science and Technology) ranked about 100,000.
In fact, we have more than 345K and 116K phrases with a
phrase quality higher than 0.5 on the EN and CN datasets
respectively.

5.8 Efﬁciency Evaluation

To study both time and memory eﬃciency, we choose the

three largest datasets: EN, ES, and CN.

Figures 9(a) and 9(b) evaluate the running time and the
peak memory usage of AutoPhrase using 10 threads on dif-
ferent proportions of three datasets respectively. Both time
and memory are linear to the size of text corpora. Moreover,
AutoPhrase can also be parallelized in an almost lock-free
way and shows a linear speedup in Figure 10(c).

Besides, compared to the previous state-of-the-art phrase
mining method SegPhrase and its variants WrapSegPhrase
on three datasets, as shown in Table 3, AutoPhrase achieves
about 8 to 11 times speedup and about 5 to 7 times memory
usage improvement. These improvements are made by a
more eﬃcient indexing and a more thorough parallelization.

6. SINGLE-WORD PHRASES

AutoPhrase can be extended to model single-word phrases,
which can gain about 10% to 30% recall improvements on
diﬀerent datasets. To study the eﬀect of modeling quality
single-word phrases, we choose the three Wikipedia article
datasets in diﬀerent languages: EN, ES, and CN.

6.1 Quality Estimation

In the paper, the deﬁnition of quality phrases and the
evaluation only focus on multi-word phrases. In linguistic
analysis, however, a phrase is not only a group of multiple
words, but also possibly a single word, as long as it functions
as a constituent in the syntax of a sentence [8]. As a great
portion (ranging from 10% to 30% on diﬀerent datasets based
on our experiments) of high-quality phrases, we should take
single-word phrases (e.g., dUIUCc, dIllinoisc, and dUSAc)
into consideration as well as multi-word phrases to achieve a
high recall in phrase mining.

Table 2: The results of AutoPhrase on the EN and CN datasets, with translations and explanations for Chinese
phrases. The whitespaces on the CN dataset are inserted by the Chinese tokenizer.

EN

CN

Sacramento Bee

Phrase
Elf Aquitaine
Arnold Sommerfeld
Eugene Wigner
Tarpon Springs
Sean Astin
. . .

Rank
1
2
3
4
5
. . .
20,001 ECAC Hockey
20,002
20,003 Bering Strait
20,004
Jacknife Lee
20,005 WXYZ-TV
. . .
. . .
99,994
John Gregson
99,995 white-tailed eagle
99,996

Translation (Explanation)
(the name of a soccer team)
Absinthe
(the name of a novel/TV-series)
notebook computer, laptop
Secretary of Party Committee
. . .
African countries
The Left (German: Die Linke)
Fraser Valley
Hippocampus
Mitsuki Saiga (a voice actress)
. . .

Phrase
江苏 舜 天
苦 艾 酒
白发 魔 女
笔记 型 电脑
党委 书记
. . .
非洲 国家
左翼 党
菲 沙 河谷
海马 体
斋 贺光希
. . .
计算机 科学技术 Computer Science and Technology
恒 天然
中国 作家 协会 The Vice President of Writers
副 主席
great spotted woodpecker 维他命 b
舆论 导向
. . .

Association of China
Vitamin B
controlled guidance of the media
. . .

99,997
99,998 David Manners
. . .

rhombic dodecahedron

Fonterra (a company)

. . .

Table 3: Eﬃciency Comparison between AutoPhrase and SegPhrase/WrapSegPhrase utilizing 10 threads.

EN
Time Memory
(mins)
32.77
369.53
11.27

(GB)
13.77
97.72
86%

ES
Time Memory
(mins)
54.05
452.85
8.37

(GB)
16.42
92.47
82%

CN
Time Memory
(mins)
9.43
108.58
11.50

(GB)
5.74
35.38
83%

AutoPhrase
(Wrap)SegPhrase
Speedup/Saving

(a) EN

(b) ES

(c) CN

Figure 10: Precision-recall curves evaluated by human annotation with both single-word and multi-word
phrases in pools.

ment of an automated phrase mining framework AutoPhrase.
Our extensive experiments show that AutoPhrase is domain-
independent, outperforms other phrase mining methods, and
supports multiple languages (e.g., English, Spanish, and
Chinese) eﬀectively, with minimal human eﬀort.

Besides, the inclusion of quality single-word phrases (e.g.,
dUIUCc and dUSAc) which leads to about 10% to 30% in-
creased recall and the exploration of better indexing strate-
gies and more thorough parallelization, which leads to about
8 to 11 times running time speedup and about 80% to 86%

memory usage saving over SegPhrase. Interested readers may
try our released code at GitHub.

For future work, it is interesting to (1) reﬁne quality
phrases to entity mentions, (2) apply AutoPhrase to more lan-
guages, such as Japanese, and (3) for those languages without
general knowledge bases, seek an unsupervised method to
generate the positive pool from the corpus, even with some
noise.

8. REFERENCES
[1] A. Allahverdyan and A. Galstyan. Comparative

A. Rajaraman. Towards the web of concepts:
Extracting concepts from large datasets. Proceedings of
the Very Large Data Bases Conference (VLDB),
3((1-2)), September 2010.

[20] Y. Park, R. J. Byrd, and B. K. Boguraev. Automatic

glossary extraction: beyond terminology identiﬁcation.
In COLING, 2002.

[21] V. Punyakanok and D. Roth. The use of classiﬁers in

sequential inference. In NIPS, 2001.

[22] H. Schmid. Treetagger| a language independent

part-of-speech tagger. Institut für Maschinelle
Sprachverarbeitung, Universität Stuttgart, 43:28, 1995.
[23] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. Kea: Practical automatic
keyphrase extraction. In Proceedings of the fourth ACM
conference on Digital libraries, pages 254–255. ACM,
1999.

[24] E. Xun, C. Huang, and M. Zhou. A uniﬁed statistical
model for the identiﬁcation of english basenp. In ACL,
2000.

[25] Z. Zhang, J. Iria, C. A. Brewster, and F. Ciravegna. A
comparative evaluation of term recognition algorithms.
LREC, 2008.

analysis of viterbi training and maximum likelihood
estimation for hmms. In NIPS, pages 1674–1682, 2011.

[2] L. Breiman. Randomizing outputs to increase

prediction accuracy. Machine Learning, 40(3):229–242,
2000.

[3] K.-h. Chen and H.-H. Chen. Extracting noun phrases
from large-scale texts: A hybrid approach and its
automatic evaluation. In ACL, 1994.

[4] M.-C. De Marneﬀe, B. MacCartney, C. D. Manning,

et al. Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454, 2006.

[5] P. Deane. A nonparametric method for extraction of

candidate phrasal terms. In ACL, 2005.

[6] A. El-Kishky, Y. Song, C. Wang, C. R. Voss, and
J. Han. Scalable topical phrase mining from text
corpora. VLDB, 8(3), Aug. 2015.

[7] D. A. Evans and C. Zhai. Noun-phrase analysis in
unrestricted text for information retrieval. In
Proceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 17–24.
Association for Computational Linguistics, 1996.
[8] G. Finch. Linguistic terms and concepts. Macmillan

Press Limited, 2000.

[9] K. Frantzi, S. Ananiadou, and H. Mima. Automatic

recognition of multi-word terms:. the c-value/nc-value
method. JODL, 3(2):115–130, 2000.

[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely

randomized trees. Machine learning, 63(1):3–42, 2006.

[11] T. Koo, X. Carreras, and M. Collins. Simple

semi-supervised dependency parsing. ACL-HLT, 2008.
[12] R. Levy and C. Manning. Is it harder to parse chinese,
or the chinese treebank? In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 439–446. Association for
Computational Linguistics, 2003.

[13] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. Mining

quality phrases from massive text corpora. In
Proceedings of 2015 ACM SIGMOD International
Conference on Management of Data, 2015.

[14] Z. Liu, X. Chen, Y. Zheng, and M. Sun. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
135–144. Association for Computational Linguistics,
2011.

[15] G. Martínez-Muñoz and A. Suárez. Switching class
labels to generate classiﬁcation ensembles. Pattern
Recognition, 38(10):1483–1494, 2005.

[16] R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.

Non-projective dependency parsing using spanning tree
algorithms. In EMNLP, 2005.

[17] R. Mihalcea and P. Tarau. Textrank: Bringing order

into texts. In ACL, 2004.

[18] J. Nivre, M.-C. de Marneﬀe, F. Ginter, Y. Goldberg,
J. Hajic, C. D. Manning, R. McDonald, S. Petrov,
S. Pyysalo, N. Silveira, et al. Universal dependencies
v1: A multilingual treebank collection. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016), 2016.

[19] A. Parameswaran, H. Garcia-Molina, and

Automated Phrase Mining from Massive Text Corpora

Jingbo Shang1, Jialu Liu2, Meng Jiang1, Xiang Ren1, Clare R Voss3, Jiawei Han1
1Computer Science Department, University of Illinois at Urbana-Champaign, IL, USA
2Google Research, New York City, NY, USA
3Computational & Information Sciences Directorate, Army Research Laboratory
2jialu@google.com

1{shang7, mjiang89, xren7, hanj}@illinois.edu

3clare.r.voss.civ@mail.mil

7
1
0
2
 
r
a

M
 
1
1
 
 
]
L
C
.
s
c
[
 
 
2
v
7
5
4
4
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
As one of the fundamental tasks in text analysis, phrase
mining aims at extracting quality phrases from a text corpus.
Phrase mining is important in various tasks such as informa-
tion extraction/retrieval, taxonomy construction, and topic
modeling. Most existing methods rely on complex, trained
linguistic analyzers, and thus likely have unsatisfactory per-
formance on text corpora of new domains and genres without
extra but expensive adaption. Recently, a few data-driven
methods have been developed successfully for extraction of
phrases from massive domain-speciﬁc text. However, none
of the state-of-the-art models is fully automated because
they require human experts for designing rules or labeling
phrases.

Since one can easily obtain many quality phrases from
public knowledge bases to a scale that is much larger than
that produced by human experts, in this paper, we propose
a novel framework for automated phrase mining, AutoPhrase,
which leverages this large amount of high-quality phrases in
an eﬀective way and achieves better performance compared
to limited human labeled phrases. In addition, we develop a
POS-guided phrasal segmentation model, which incorporates
the shallow syntactic information in part-of-speech (POS)
tags to further enhance the performance, when a POS tagger
is available. Note that, AutoPhrase can support any language
as long as a general knowledge base (e.g., Wikipedia) in that
language is available, while beneﬁting from, but not requiring,
a POS tagger. Compared to the state-of-the-art methods,
the new method has shown signiﬁcant improvements in eﬀec-
tiveness on ﬁve real-world datasets across diﬀerent domains
and languages.

1.

INTRODUCTION

Phrase mining refers to the process of automatic extrac-
tion of high-quality phrases (e.g., scientiﬁc terms and general
entity names) in a given corpus (e.g., research papers and
news). Representing the text with quality phrases instead

ACM ISBN 978-1-4503-2138-9.
DOI: 10.1145/1235

of n-grams can improve computational models for applica-
tions such as information extraction/retrieval, taxonomy
construction, and topic modeling.

Almost all the state-of-the-art methods, however, require
human experts at certain levels. Most existing methods [9,
20, 25] rely on complex, trained linguistic analyzers (e.g.,
dependency parsers) to locate phrase mentions, and thus
may have unsatisfactory performance on text corpora of new
domains and genres without extra but expensive adaption.
Our latest domain-independent method SegPhrase [13] out-
performs many other approaches [9, 20, 25, 5, 19, 6], but
still needs domain experts to ﬁrst carefully select hundreds
of varying-quality phrases from millions of candidates, and
then annotate them with binary labels.

Such reliance on manual eﬀorts by domain and linguis-
tic experts becomes an impediment for timely analysis of
massive, emerging text corpora in speciﬁc domains. An
ideal automated phrase mining method is supposed to be
domain-independent, with minimal human eﬀort or reliance
on linguistic analyzers1. Bearing this in mind, we propose
a novel automated phrase mining framework AutoPhrase in
this paper, going beyond SegPhrase, to further get rid of ad-
ditional manual labeling eﬀort and enhance the performance,
mainly using the following two new techniques.
1. Robust Positive-Only Distant Training.

In fact, many
high-quality phrases are freely available in general knowl-
edge bases, and they can be easily obtained to a scale
that is much larger than that produced by human ex-
perts. Domain-speciﬁc corpora usually contain some qual-
ity phrases also encoded in general knowledge bases, even
when there may be no other domain-speciﬁc knowledge
bases. Therefore, for distant training, we leverage the
existing high-quality phrases, as available from general
knowledge bases, such as Wikipedia and Freebase, to get
rid of additional manual labeling eﬀort. We independently
build samples of positive labels from general knowledge
bases and negative labels from the given domain corpora,
and train a number of base classiﬁers. We then aggregate
the predictions from these classiﬁers, whose independence
helps reduce the noise from negative labels.

2. POS-Guided Phrasal Segmentation. There is a trade-
oﬀ between the performance and domain-independence
when incorporating linguistic processors in the phrase
mining method. On the domain independence side, the

1The phrase “minimal human eﬀort” indicates using only
existing general knowledge bases without any other human
eﬀort.

accuracy might be limited without linguistic knowledge. It
is diﬃcult to support multiple languages, if the method is
completely language-blind. On the accuracy side, relying
on complex, trained linguistic analyzers may hurt the
domain-independence of the phrase mining method. For
example, it is expensive to adapt dependency parsers to
special domains like clinical reports. As a compromise, we
propose to incorporate a pre-trained part-of-speech (POS)
tagger to further enhance the performance, when it is
available for the language of the document collection. The
POS-guided phrasal segmentation leverages the shallow
syntactic information in POS tags to guide the phrasal
segmentation model locating the boundaries of phrases
more accurately.
In principle, AutoPhrase can support any language as long
as a general knowledge base in that language is available. In
fact, at least 58 languages have more than 100,000 articles
in Wikipedia as of Feb, 20172. Moreover, since pre-trained
part-of-speech (POS) taggers are widely available in many
languages (e.g., more than 20 languages in TreeTagger [22]3),
the POS-guided phrasal segmentation can be applied in many
scenarios. It is worth mentioning that for domain-speciﬁc
knowledge bases (e.g., MeSH terms in the biomedical domain)
and trained POS taggers, the same paradigm applies. In
this study, without loss of generality, we only assume the
availability of a general knowledge base together with a
pre-trained POS tagger.

As demonstrated in our experiments, AutoPhrase not only
works eﬀectively in multiple domains like scientiﬁc papers,
business reviews, and Wikipedia articles, but also supports
multiple languages, such as English, Spanish, and Chinese.

Our main contributions are highlighted as follows:

• We study an important problem, automated phrase mining,

and analyze its major challenges as above.

• We propose a robust positive-only distant training method
for phrase quality estimation to minimize the human eﬀort.
• We develop a novel phrasal segmentation model to leverage
POS tags to achieve further improvement, when a POS
tagger is available.

• We demonstrate the robustness and accuracy of our method
and show improvements over prior methods, with results
of experiments conducted on ﬁve real-world datasets in
diﬀerent domains (scientiﬁc papers, business reviews, and
Wikipedia articles) and diﬀerent languages (English, Span-
ish, and Chinese).

The rest of the paper is organized as follows. Section 2
positions our work relative to existing works. Section 3
deﬁnes basic concepts including four requirements of phrases.
The details of our method are covered in Section 4. Extensive
experiments and case studies are presented in Section 5. We
conclude the study in Section 7.

2. RELATED WORK

Identifying quality phrases eﬃciently has become ever
more central and critical for eﬀective handling of massively
increasing-size text datasets. In contrast to keyphrase ex-
traction [17, 23, 14], this task goes beyond the scope of
single documents and provides useful cross-document sig-
nals. The natural language processing (NLP) community

2https://meta.wikimedia.org/wiki/List_of_Wikipedias
3http://www.cis.uni-muenchen.de/~schmid/tools/
TreeTagger/

has conducted extensive studies typically referred to as au-
tomatic term recognition [9, 20, 25], for the computational
task of extracting terms (such as technical phrases). This
topic also attracts attention in the information retrieval (IR)
community [7, 19] since selecting appropriate indexing terms
is critical to the improvement of search engines where the
ideal indexing units represent the main concepts in a corpus,
not just literal bag-of-words.

Text indexing algorithms typically ﬁlter out stop words and
restrict candidate terms to noun phrases. With pre-deﬁned
part-of-speech (POS) rules, one can identify noun phrases
as term candidates in POS-tagged documents. Supervised
noun phrase chunking techniques [21, 24, 3] exploit such
tagged documents to automatically learn rules for identifying
noun phrase boundaries. Other methods may utilize more
sophisticated NLP technologies such as dependency parsing
to further enhance the precision [11, 16]. With candidate
terms collected, the next step is to leverage certain statistical
measures derived from the corpus to estimate phrase quality.
Some methods rely on other reference corpora for the calibra-
tion of “termhood” [25]. The dependency on these various
kinds of linguistic analyzers, domain-dependent language
rules, and expensive human labeling, makes it challenging to
extend these approaches to emerging, big, and unrestricted
corpora, which may include many diﬀerent domains, topics,
and languages.

To overcome this limitation, data-driven approaches opt
instead to make use of frequency statistics in the corpus to
address both candidate generation and quality estimation [5,
19, 6, 13]. They do not rely on complex linguistic feature
generation, domain-speciﬁc rules or extensive labeling eﬀorts.
Instead, they rely on large corpora containing hundreds
of thousands of documents to help deliver superior perfor-
mance [13]. In [19], several indicators, including frequency
and comparison to super/sub-sequences, were proposed to
extract n-grams that reliably indicate frequent, concise con-
cepts. Deane [5] proposed a heuristic metric over frequency
distribution based on Zipﬁan ranks, to measure lexical associ-
ation for phrase candidates. As a preprocessing step towards
topical phrase extraction, El-Kishky et al. [6] proposed to
mine signiﬁcant phrases based on frequency as well as doc-
ument context following a bottom-up fashion, which only
considers a part of quality phrase criteria, popularity and
concordance. Our previous work [13] succeeded at integrat-
ing phrase quality estimation with phrasal segmentation to
further rectify the initial set of statistical features, based on
local occurrence context. Unlike previous methods which
are purely unsupervised,
[13] required a small set of phrase
labels to train its phrase quality estimator. It is worth men-
tioning that all these approaches still depend on the human
eﬀort (e.g., setting domain-sensitive thresholds). Therefore,
extending them to work automatically is challenging.

3. PRELIMINARIES

The goal of this paper is to develop an automated phrase
mining method to extract quality phrases from a large collec-
tion of documents without human labeling eﬀort, and with
only limited, shallow linguistic analysis. The main input to
the automated phrase mining task is a corpus and a knowl-
edge base. The input corpus is a textual word sequence
in a particular language and a speciﬁc domain, of arbitrary
length. The output is a ranked list of phrases with decreasing
quality.

Figure 1: The overview of AutoPhrase. The two novel techniques developed in this paper are highlighted.

The AutoPhrase framework is shown in Figure 1. The
work ﬂow is completely diﬀerent form our previous domain-
independent phrase mining method requiring human ef-
fort [13], although the phrase candidates and the features
used during phrase quality (re-)estimation are the same. In
this paper, we propose a robust positive-only distant train-
ing to minimize the human eﬀort and develop a POS-guided
phrasal segmentation model to improve the model perfor-
mance. In this section, we brieﬂy introduce basic concepts
and components as preliminaries.

A phrase is deﬁned as a sequence of words that appear
consecutively in the text, forming a complete semantic unit
in certain contexts of the given documents [8]. The phrase
quality is deﬁned to be the probability of a word sequence
being a complete semantic unit, meeting the following crite-
ria [13]:
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Concordance: The collocation of tokens in quality phrases
occurs with signiﬁcantly higher probability than expected
due to chance.

• Informativeness: A phrase is informative if it is indica-

tive of a speciﬁc topic or concept.

• Completeness: Long frequent phrases and their subse-
quences within those phrases may both satisfy the 3 criteria
above. A phrase is deemed complete when it can be inter-
preted as a complete semantic unit in some given document
context. Note that a phrase and a subphrase contained
within it, may both be deemed complete, depending on
the context in which they appear. For example, “relational
database system”, “relational database” and “database sys-
tem” can all be valid in certain context.

AutoPhrase will estimate the phrase quality based on the
positive and negative pools twice, once before and once
after the POS-guided phrasal segmentation. That is, the
POS-guided phrasal segmentation requires an initial set of
phrase quality scores; we estimate the scores based on raw
frequencies beforehand; and then once the feature values
have been rectiﬁed, we re-estimate the scores.

Only the phrases satisfying all above requirements are

recognized as quality phrases.

Example 1. “strong tea” is a quality phrase while “heavy
tea” fails to be due to concordance. “this paper” is a popular
and concordant phrase, but is not informative in research
publication corpus. “NP-complete in the strong sense” is a
quality phrase while “NP-complete in the strong” fails to be
due to completeness. (cid:3)

To automatically mine these quality phrases, the ﬁrst phase
of AutoPhrase (see leftmost box in Figure 1) establishes the
set of phrase candidates that contains all n-grams over

the minimum support threshold τ (e.g., 30) in the corpus.
Here, this threshold refers to raw frequency of the n-grams
calculated by string matching.
In practice, one can also
set a phrase length threshold (e.g., n ≤ 6) to restrict the
number of words in any phrase. Given a phrase candidate
w1w2 . . . wn, its phrase quality is:

Q(w1w2 . . . wn) = p(dw1w2 . . . wnc|w1w2 . . . wn) ∈ [0, 1]
where dw1w2 . . . wnc refers to the event that these words
constitute a phrase. Q(·), also known as the phrase quality
estimator, is initially learned from data based on statistical
features4, such as point-wise mutual information, point-wise
KL divergence, and inverse document frequency, designed
to model concordance and informativeness mentioned above.
Note the phrase quality estimator is computed independent
of POS tags. For unigrams, we simply set their phrase quality
as 1.

Example 2. A good quality estimator will return Q(this paper) ≈

0 and Q(relational database system) ≈ 1. (cid:3)

Then, to address the completeness criterion, the phrasal
segmentation ﬁnds the best segmentation for each sentence.

Example 3. Ideal phrasal segmentation results are as fol-

lows.

(cid:3)

#1:
... / the / Great Firewall / is / ...
#2: This / is / a / great / ﬁrewall software/ .
#3: The / discriminative classiﬁer / SVM / is / ...

During the phrase quality re-estimation, related sta-
tistical features will be re-computed based on the rectiﬁed
frequency of phrases, which means the number of times that
a phrase becomes a complete semantic unit in the identiﬁed
segmentation. After incorporating the rectiﬁed frequency, the
phrase quality estimator Q(·) also models the completeness
in addition to concordance and informativeness.

Example 4. Continuing the previous example, the raw
frequency of the phrase “great ﬁrewall” is 2, but its rectiﬁed
frequency is 1. Both the raw frequency and the rectiﬁed
frequency of the phrase “ﬁrewall software” are 1. The raw
frequency of the phrase “classiﬁer SVM” is 1, but its rectiﬁed
frequency is 0. (cid:3)

4. METHODOLOGY

In this section, we focus on introducing our two new tech-

niques.
4See https://github.com/shangjingbo1226/AutoPhrase for
further details

Figure 2: The illustration of each base classiﬁer. In
each base classiﬁer, we ﬁrst randomly sample K pos-
itive and negative labels from the pools respectively.
There might be δ quality phrases among the K neg-
ative labels. An unpruned decision tree is trained
based on this perturbed training set.
4.1 Robust Positive-Only Distant Training

To estimate the phrase quality score for each phrase can-
didate, our previous work [13] required domain experts to
ﬁrst carefully select hundreds of varying-quality phrases from
millions of candidates, and then annotate them with binary
labels. For example, for computer science papers, our domain
experts provided hundreds of positive labels (e.g., “spanning
tree” and “computer science”) and negative labels (e.g., “pa-
per focuses” and “important form of ”). However, creating
such a label set is expensive, especially in specialized do-
mains like clinical reports and business reviews, because this
approach provides no clues for how to identify the phrase
candidates to be labeled.
In this paper, we introduce a
method that only utilizes existing general knowledge bases
without any other human eﬀort.

4.1.1 Label Pools

Public knowledge bases (e.g., Wikipedia) usually encode
a considerable number of high-quality phrases in the titles,
keywords, and internal links of pages. For example, by ana-
lyzing the internal links and synonyms5 in English Wikipedia,
more than a hundred thousand high-quality phrases were
discovered. As a result, we place these phrases in a positive
pool.

Knowledge bases, however, rarely, if ever, identify phrases
that fail to meet our criteria, what we call inferior phrases.
An important observation is that the number of phrase can-
didates, based on n-grams (recall leftmost box of Figure 1),
is huge and the majority of them are actually of of inferior
quality (e.g., “Francisco opera and”).
In practice, based
on our experiments, among millions of phrase candidates,
usually, only about 10% are in good quality. Therefore,
phrase candidates that are derived from the given corpus
but that fail to match any high-quality phrase derived from
the given knowledge base, are used to populate a large but
noisy negative pool.

4.1.2 Noise Reduction

Directly training a classiﬁer based on the noisy label pools
is not a wise choice: some phrases of high quality from
the given corpus may have been missed (i.e., inaccurately
binned into the negative pool) simply because they were
Instead, we propose
not present in the knowledge base.
to utilize an ensemble classiﬁer that averages the results
of T independently trained base classiﬁers. As shown in
Figure 2, for each base classiﬁer, we randomly draw K phrase
candidates with replacement from the positive pool and the
negative pool respectively (considering a canonical balanced
classiﬁcation scenario). This size-2K subset of the full set of

5https://github.com/kno10/WikipediaEntities

all phrase candidates is called a perturbed training set [2],
because the labels of some (δ in the ﬁgure) quality phrases
In order for the
are switched from positive to negative.
ensemble classiﬁer to alleviate the eﬀect of such noise, we
need to use base classiﬁers with the lowest possible training
errors. We grow an unpruned decision tree to the point of
separating all phrases to meet this requirement.
In fact,
such decision tree will always reach 100% training accuracy
when no two positive and negative phrases share identical
feature values in the perturbed training set. In this case,
δ
its ideal error is
2K , which approximately equals to the
proportion of switched labels among all phrase candidates
δ
2K ≈ 10%). Therefore, the value of K is not sensitive
(i.e.,
to the accuracy of the unpruned decision tree and is ﬁxed as
100 in our implementation. Assuming the extracted features
are distinguishable between quality and inferior phrases, the
empirical error evaluated on all phrase candidates, p, should
be relatively small as well.

An interesting property of this sampling procedure is that
the random selection of phrase candidates for building per-
turbed training sets creates classiﬁers that have statistically
independent errors and similar erring probability [2, 15].
Therefore, we naturally adopt random forest [10], which is
veriﬁed, in the statistics literature, to be robust and eﬃcient.
The phrase quality score of a particular phrase is computed
as the proportion of all decision trees that predict that phrase
is a quality phrase. Suppose there are T trees in the random
forest, the ensemble error can be estimated as the probability
of having more than half of the classiﬁers misclassifying a
given phrase candidate as follows.

ensemble_ error(T ) =

pt(1 − p)T −t

T
X

(cid:19)

(cid:18)T
t

t=b1+T /2c

From Figure 3, one can
easily observe that the en-
semble error is approach-
ing 0 when T grows. In
practice, T needs to be
set larger due to the ad-
ditional error brought by
model bias. Empirical
studies can be found in
Figure 7.

Figure 3: Ensemble er-
rors of diﬀerent p’s vary-
ing T .

4.2 POS-Guided Phrasal Segmentation

Phrasal segmentation addresses the challenge of measuring
completeness (our fourth criterion) by locating all phrase
mentions in the corpus and rectifying their frequencies ob-
tained originally via string matching.

The corpus is processed as a length-n POS-tagged word
sequence Ω = Ω1Ω2 . . . Ωn, where Ωi refers to a pair con-
sisting of a word and its POS tag hwi, tii. A POS-guided
phrasal segmentation is a partition of this sequence into
m segments induced by a boundary index sequence B =
{b1, b2, . . . , bm+1} satisfying 1 = b1 < b2 < . . . < bm+1 =
n+1. The i-th segment refers to Ωbi Ωbi+1 . . . Ωbi+1−1.

Compared to the phrasal segmentation in our previous
work [13], the POS-guided phrasal segmentation addresses
the completeness requirement in a context-aware way, instead
of equivalently penalizing phrase candidates of the same
length. In addition, POS tags provide shallow, language-
speciﬁc knowledge, which may help boost phrase detection

Algorithm 1: POS-Guided Phrasal Segmentation (PGPS)

Input: Corpus Ω = Ω1Ω2 . . . Ωn, phrase quality Q,
parameters θu and δ(tx, ty).
Output: Optimal boundary index sequence B.
// hi ≡ maxB p(Ω1Ω2 . . . Ωi−1, B|Q, θ, δ)
h1 ← 1, hi ← 0 (1 < i ≤ n + 1)
for i = 1 to n do

for j = i + 1 to min(i + length threshold, n + 1) do

// Efficiently implemented via Trie.
if there is no phrase starting with w[i,j) then

break

// In practice, log and addition are used

to avoid underflow.

if hi × p(j, dw[i,j)c|i, t[i,j)) > hj then
hj ← hi × p(j, dw[i,j)c|i, t[i,j))
gj ← i
j ← n + 1, m ← 0
while j > 1 do
m ← m + 1
bm ← j
j ← gj

return B ← 1, bm, bm−1, . . . , b1

accuracy, especially at syntactic constituent boundaries for
that language.

Given the POS tag sequence for the full n-length corpus
is t = t1t2 . . . tn, containing the tag subsequence tl . . . tr−1
(denote as t[l,r) for clarity), the POS quality score for that
tag subsequence is deﬁned to be the conditional probability of
its corresponding word sequence being a complete semantic
unit. Formally, we have

T (t[l,r)) = p(dwl . . . wrc|t) ∈ [0, 1]

The POS quality score T (·) is designed to reward the phrases
with their correctly identiﬁed POS sequences, as follows.

Example 5. Suppose the whole POS tag sequence is “NN
NN NN VB DT NN”. A good POS sequence quality estima-
tor might return T (NN NN NN) ≈ 1 and T (NN VB) ≈ 0,
where NN refers to singular or mass noun (e.g., database),
VB means verb in the base form (e.g., is), and DT is for
determiner (e.g., the). (cid:3)

The particular form of T (·) we have chosen is:

T (t[l,r)) = (1 − δ(tbr −1, tbr )) ×

δ(tj−1, tj)

r−1
Y

j=l+1

where, δ(tx, ty) is the probability that the POS tag tx is
adjacent and precedes POS tag ty within a phrase in the given
document collection. In this formula, the ﬁrst term indicates
the probability that there is a phrase boundary between the
words indexed r − 1 and r, while the latter product indicates
the probability that all POS tags within t[l,r) are in the
same phrase. This POS quality score can naturally counter
the bias to longer segments because ∀i > 1, exactly one of
δ(ti−1, ti) and (1 − δ(ti−1, ti)) is always multiplied no matter
how the corpus is segmented. Note that the length penalty
model in our previous work [13] is a special case when all
values of δ(tx, ty) are the same.

Mathematically, δ(tx, ty) is deﬁned as:

δ(tx, ty) = p(d. . . w1w2 . . .c|Ω, tag(w1) = tx ∧ tag(w2) = ty)

As it depends on how documents are segmented into phrases,
δ(tx, ty) is initialized uniformly and will be learned during
the phrasal segmentation.

Now, after we have both phrase quality Q(·) and POS
quality T (·) ready, we are able to formally deﬁne the POS-
guided phrasal segmentation model. The joint probability of
a POS tagged sequence Ω and a boundary index sequence
B = {b1, b2, . . . , bm+1} is factorized as:

p(Ω, B) =

p

bi+1, dw[bi,bi+1)c

(cid:17)

(cid:12)
(cid:12)
(cid:12)bi, t

m
Y

(cid:16)

i=1

where p(bi+1, dw[bi,bi+1)c|bi, t) is the probability of observing
a word sequence w[bi,bi+1) as the i-th quality segment given
the previous boundary index bi and the whole POS tag
sequence t.

Since the phrase segments function as a constituent in the
syntax of a sentence, they usually have weak dependence on
each other [8, 13]. As a result, we assume these segments in
the word sequence are generated one by one for the sake of
both eﬃciency and simplicity.

For each segment, given the POS tag sequence t and the

start index bi, the generative process is deﬁned as follows.
1. Generate the end index bi+1, according to its POS quality

p(bi+1|bi, t) = T (t[bi,bi+1))

2. Given the two ends bi and bi+1, generate the word se-
quence w[bi,bi+1) according to a multinomial distribution
over all segments of length-(bi+1 − bi).

p(w[bi,bi+1)|bi, bi+1) = p(w[bi,bi+1)|bi+1 − bi)

3. Finally, we generate an indicator whether w[bi,bi+1) forms

a quality segment according to its quality

p(dw[bi,bi+1)c|w[bi,bi+1)) = Q(w[bi,bi+1))
We denote p(w[bi,bi+1)|bi+1 −bi) as θw[bi,bi+1) for convenience.
Integrating the above three generative steps together, we
have the following probabilistic factorization:

p(bi+1, dw[bi,bi+1)c|bi, t)

=p(bi+1|bi, t)p(w[bi,bi+1)|bi, bi+1)p(dw[bi,bi+1)c|w[bi,bi+1))
=T (t[bi,bi+1))θw[bi,bi+1) Q(w[bi,bi+1))

Therefore, there are three subproblems:

1. Learn θu for each word and phrase candidate u;
2. Learn δ(tx, ty) for every POS tag pair; and
3. Infer B when θu and δ(tx, ty) are ﬁxed.

We employ the maximum a posterior principle and maxi-

mize the joint log likelihood:

log p(Ω, B) =

log p

(cid:16)

(cid:12)
(cid:12)
(cid:12)bt, t
bi+1, dw[bi,bi+1)c

(cid:17)

(1)

m
X

i=1

Given θu and δ(tx, ty), to ﬁnd the best segmentation that
maximizes Equation (1), we develop an eﬃcient dynamic
programming algorithm for the POS-guided phrasal segmen-
tation as shown in Algorithm 1.

When the segmentation S and the parameter θ are ﬁxed,

the closed-form solution of δ(tx, ty) is:

δ(tx, ty) =

Pm

i=1

Pbi+1−2
j=bi

Pn−1

1(tj = tx ∧ tj+1 = ty)

(2)

i=1 1(ti = tx ∧ ti+1 = ty)

Algorithm 2: Viterbi Training (VT)

Input: Corpus Ω and phrase quality Q.
Output: θu and δ(tx, ty).
initialize θ with normalized raw frequencies in the corpus
while θu does not converge do

while δ(tx, ty) does not converge do

B ← best segmentation via Alg. 1
update δ(tx, ty) using B according to Eq. (2)

B ← best segmentation via Alg. 1
update θu using B according to Eq. (3)

return θu and δ(tx, ty)

where 1(·) denotes the identity indicator. δ(tx, ty) is the
unsegmented ratio among all htx, tyi pairs in the given corpus.
Similarly, once the segmentation S and the parameter δ
are ﬁxed, the closed-form solution of θu can be derived as:

θu =

Pm
Pm

i=1 1(w[bi,bi+1) = u)
i=1 1(bi+1 − bi = |u|)

(3)

We can see that θu is the times that u becomes a com-
plete segment normalized by the number of the length-|u|
segments.

As shown in Algorithm 2, we choose Viterbi Training, or
Hard EM in literature [1] to iteratively optimize parameters,
because Viterbi Training converges fast and results in sparse
and simple models for Hidden Markov Model-like tasks [1].

4.3 Complexity Analysis

The time complexity of the most time consuming compo-
nents in our framework, such as frequent n-gram, feature
extraction, POS-guided phrasal segmentation, are all O(|Ω|)
with the assumption that the maximum number of words in
a phrase is a small constant (e.g., n ≤ 6), where |Ω| is the
total number of words in the corpus. Therefore, AutoPhrase
is linear to the corpus size and thus being very eﬃcient and
scalable. Meanwhile, every component can be parallelized
in an almost lock-free way grouping by either phrases or
sentences.

5. EXPERIMENTS

In this section, we will apply the proposed method to mine
quality phrases from ﬁve massive text corpora across three
domains (scientiﬁc papers, business reviews, and Wikipedia
articles) and in three languages (English, Spanish, and Chi-
nese). We compare the proposed method with many other
methods to demonstrate its high performance. Then we
explore the robustness of the proposed positive-only distant
training and its performance against expert labeling. The
advantage of incorporating POS tags in phrasal segmentation
will also be proved. In the end, we present case studies.

5.1 Datasets

To validate that the proposed positive-only distant train-
ing can eﬀectively work in diﬀerent domains and the POS-
guided phrasal segmentation can support multiple languages
eﬀectively, we have ﬁve large collections of text in diﬀerent
domains and languages, as shown in Table 1: Abstracts
of English computer science papers from DBLP6, English

6https://aminer.org/citation

Table 1: Five real-world massive text corpora in dif-
|Ω| is the
ferent domains and multiple languages.
total number of words. sizep is the size of positive
pool.

Dataset
DBLP
Yelp
EN
ES
CN

Domain
Scientiﬁc Paper
Business Review
Wikipedia Article
Wikipedia Article
Wikipedia Article

Language
English
English
English
Spanish
Chinese

|Ω|

sizep
File size
29K
91.6M 618MB
145.1M 749MB
22K
808.0M 3.94GB 184K
65K
791.2M 4.06GB
29K
371.9M 1.56GB

business reviews from Yelp7, Wikipedia articles8 in English
(EN), Spanish (ES), and Chinese (CN). From the exist-
ing general knowledge base Wikipedia, we extract popular
mentions of entities by analyzing intra-Wiki citations within
Wiki content9. On each dataset, the intersection between the
extracted popular mentions and the generated phrase candi-
dates forms the positive pool. Therefore, the size of positive
pool may vary in diﬀerent datasets of the same language.

5.2 Compared Methods

We compare AutoPhrase with three lines of methods as

follows. Every method returns a ranked list of phrases.
SegPhrase10/WrapSegPhrae11: In English domain-speciﬁc
text corpora, our latest work SegPhrase outperformed phrase
mining [6], keyphrase extraction [23, 19], and noun phrase
chunking methods. WrapSegPhrase extends SegPhrase to dif-
ferent languages by adding an encoding preprocessing to
ﬁrst transform non-English corpus using English characters
and punctuation as well as a decoding postprocessing to
later translate them back to the original language. Both
methods require domain expert labors. For each dataset, we
ask domain experts to annotate a representative set of 300
quality/interior phrases.
Parser-based Phrase Extraction: Using complicated lin-
guistic processors, such as parsers, we can extract minimum
phrase units (e.g., NP) from the parsing trees as phrase
candidates. Parsers of all three languages are available in
Stanford NLP tools [18, 4, 12]. Two ranking heuristics are
considered:
• TF-IDF ranks the extracted phrases by their term fre-
quency and inverse document frequency in the given docu-
ments;

• TextRank: An unsupervised graph-based ranking model

for keyword extraction [17].

Pre-trained Chinese Segmentation Models: Diﬀerent
from English and Spanish, phrasal segmentation in Chi-
nese has been intensively studied because there is no space
between Chinese words. The most eﬀective and popular
segmentation methods are:
• AnsjSeg12 is a popular text segmentation algorithm for
Chinese corpus. It ensembles statistical modeling methods
of Conditional Random Fields (CRF) and Hidden Markov
Models (HMMs) based on the n-gram setting;

• JiebaPSeg13 is a Chinese text segmentation method im-

7https://www.yelp.com/dataset_challenge
8https://dumps.wikimedia.org/
9https://github.com/kno10/WikipediaEntities
10https://github.com/shangjingbo1226/SegPhrase
11https://github.com/remenberl/SegPhrase-MultiLingual
12https://github.com/NLPchina/ansj_seg
13https://github.com/fxsjy/jieba

(a) DBLP

(b) Yelp

(c) EN

(d) ES

(e) CN

Figure 4: Overall Performance Evaluation: Precision-recall curves of all methods evaluated by human anno-
tation.

plemented in Python. It builds a directed acyclic graph
for all possible phrase combinations based on a preﬁx
dictionary structure to achieve eﬃcient phrase graph scan-
ning. Then it uses dynamic programming to ﬁnd the most
probable combination based on the phrase frequency. For
unknown phrases, an HMM-based model is used with the
Viterbi algorithm.

Note that all parser-based phrase extraction and Chinese
segmentation models are pre-trained based on general corpus.
To study the eﬀectiveness of the POS-guided segmenta-
tion, AutoSegPhrase adopts the length penalty instead of
δ(tx, ty), while other components are the same as AutoPhrase.
AutoSegPhrase is useful when there is no POS tagger.

5.3 Experimental Settings
Implementation. The preprocessing includes tokenizers
from Lucene and Stanford NLP as well as the POS tagger
from TreeTagger. Our documented code package has been
released and maintained in GitHub14.
Default Parameters. We set the minimum support thresh-
old σ as 30. The maximum number of words in a phrase is
set as 6 according to labels from domain experts. These are
two parameters required by all methods. Other parameters
required by compared methods were set according to the
open-source tools or the original papers.
Human Annotation. We rely on human evaluators to
judge the quality of the phrases which cannot be identiﬁed
through any knowledge base. More speciﬁcally, on each
dataset, we randomly sample 500 such phrases from the
predicted phrases of each method in the experiments. These
selected phrases are shuﬄed in a shared pool and evaluated
by 3 reviewers independently. We allow reviewers to use
search engines when unfamiliar phrases encountered. By the
rule of majority voting, phrases in this pool received at least
two positive annotations are quality phrases. The intra-class
correlations (ICCs) are all more than 0.9 on all ﬁve datasets,
which shows the agreement.
Evaluation Metrics. For a list of phrases, precision is
deﬁned as the number of true quality phrases divided by the
number of predicted quality phrases; recall is deﬁned as the
number of true quality phrases divided by the total number
of quality phrases. We retrieve the ranked list of the pool
from the outcome of each method. When a new true quality
phrase encountered, we evaluate the precision and recall of
this ranked list.
In the end, we plot the precision-recall
curves. In addition, area under the curve (AUC) is adopted
as another quantitative measure. AUC in this paper refers
to the area under the precision-recall curve.

14https://github.com/shangjingbo1226/AutoPhrase

5.4 Overall Performance

Figures 4 presents the precision-recall curves of all com-
pared methods evaluated by human annotation on ﬁve datasets.
Overall, AutoPhrase performs the best, in terms of both pre-
cision and recall. Signiﬁcant advantages can be observed,
especially on two non-English datasets ES and CN. For
example, on the ES dataset, the recall of AutoPhrase is
about 20% higher than the second best method (SegPhrase)
in absolute value. Meanwhile, there is a visible precision
gap between AutoPhrase and the best baseline. The phrase
chunking-based methods TF-IDF and TextRank work poorly,
because the extraction and ranking are modeled separately
and the pre-trained complex linguistic analyzers fail to ex-
tend to domain-speciﬁc corpora. TextRank usually starts
with a higher precision than TF-IDF, but its recall is very
low because of the sparsity of the constructed co-occurrence
graph. TF-IDF achieves a reasonable recall but unsatisfac-
tory precision. On the CN dataset, the pre-trained Chinese
segmentation models, JiebaSeg and AnsjSeg, are very com-
petitive, because they not only leverage training data for
segmentations, but also exhaust the engineering work, in-
cluding a huge dictionary for popular Chinese entity names
and speciﬁc rules for certain types of entities. As a conse-
quence, they can easily extract tons of well-known terms
and people/location names. Outperforming such a strong
baseline further conﬁrms the eﬀectiveness of AutoPhrase.

The comparison among the English datasets across three
domains (i.e., scientiﬁc papers, business reviews, and Wikipedia
articles) demonstrates that AutoPhrase is reasonably domain-
independent. The performance of parser-based methods
TF-IDF and TextRank depends on the rigorous degree of
the documents. For example, it works well on the DBLP
dataset but poorly on the Yelp dataset. However, without
any human eﬀort, AutoPhrase can work eﬀectively on domain-
speciﬁc datasets, and even outperforms SegPhrase, which is
supervised by the domain experts.

The comparison among the Wikipedia article datasets in
three languages (i.e., EN, ES, and CN ) shows that, ﬁrst
of all, AutoPhrase supports multiple languages. Secondly,
the advantage of AutoPhrase over SegPhrase/WrapSegPhrase
is more obvious on two non-English datasets ES and CN
than the EN dataset, which proves that the helpfulness of
introducing the POS tagger.

As conclusions, AutoPhrase is able to support diﬀerent
domains and support multiple languages with minimal human
eﬀort.

5.5 Distant Training Exploration

To compare the distant training and domain expert label-
ing, we experiment with the domain-speciﬁc datasets DBLP

(a) DBLP

(b) Yelp

(a) DBLP

(b) Yelp

Figure 5: AUC curves of four variants when we have
enough positive labels in the positive pool EP.

Figure 6: AUC curves of four variants after we ex-
haust positive labels in the positive pool EP.

and Yelp. To be fair, all the conﬁgurations in the classiﬁers
are the same except for the label selection process. More
speciﬁcally, we come up with four training pools:
1. EP means that domain experts give the positive pool.
2. DP means that a sampled subset from existing general

knowledge forms the positive pool.

3. EN means that domain experts give the negative pool.
4. DN means that all unlabeled (i.e., not in the positive

pool) phrase candidates form the negative pool.

By combining any pair of the positive and negative pools,
we have four variants, EPEN (in SegPhrase), DPDN (in
AutoPhrase), EPDN, and DPEN.

First of all, we evaluate the performance diﬀerence in the
two positive pools. Compared to EPEN, DPEN adopts a
positive pool sampled from knowledge bases instead of the
well-designed positive pool given by domain experts. The
negative pool EN is shared. As shown in Figure 5, we vary the
size of the positive pool and plot their AUC curves. We can
ﬁnd that EPEN outperforms DPEN and the trends of curves
on both datasets are similar. Therefore, we conclude that the
positive pool generated from knowledge bases has reasonable
quality, although its corresponding quality estimator works
slightly worse.

Secondly, we verify that whether the proposed noise re-
duction mechanism works properly. Compared to EPEN,
EPDN adopts a negative pool of all unlabeled phrase can-
didates instead of the well-designed negative pool given by
domain experts. The positive pool EP is shared. In Fig-
ure 5, the clear gap between them and the similar trends on
both datasets show that the noisy negative pool is slightly
worse than the well-designed negative pool, but it still works
eﬀectively.

As illustrated in Figure 5, DPDN has the worst perfor-
mance when the size of positive pools are limited. However,
distant training can generate much larger positive pools,
which may signiﬁcantly beyond the ability of domain experts
considering the high expense of labeling. Consequently, we
are curious whether the distant training can ﬁnally beat do-
main experts when positive pool sizes become large enough.
We call the size at this tipping point as the ideal number.
We increase positive pool sizes and plot AUC curves of
DPEN and DPDN, while EPEN and EPDN are degenerated
as dashed lines due to the limited domain expert abilities.
As shown in Figure 6, with a large enough positive pool,
distant training is able to beat expert labeling. On the
DBLP dataset, the ideal number is about 700, while on the
Yelp dataset, it becomes around 1600. Our guess is that the
ideal training size is proportional to the number of words

(a) EN

(b) ES

(c) CN

Figure 8: Precision-recall curves of AutoPhrase and
AutoSegPhrase.

(e.g., 91.6M in DBLP and 145.1M in Yelp). We notice that
compared to the corpus size, the ideal number is relatively
small, which implies the distant training should be eﬀective
in many domain-speciﬁc corpora as if they overlap with
Wikipedia.

Besides, Figure 6 shows that when the positive pool size
continues growing, the AUC score increases but the slope
becomes smaller. The performance of distant training will
be ﬁnally stable when a relatively large number of quality
phrases were fed.

We are curious how
many trees (i.e., T ) is
enough for DPDN. We
increase T and plot AUC
As
curves of DPDN.
shown in Figure 7, on
both datasets,
as T
grows, the AUC scores
ﬁrst increase rapidly and
the speed slows
later
down gradually, which is
consistent with the theoretical analysis in Section 4.1.2.

Figure 7: AUC curves of
DPDN varying T .

5.6 POS-guided Phrasal Segmentation

We are also interested in how much performance gain we
can obtain from incorporating POS tags in this segmentation
model, especially for diﬀerent languages. We select Wikipedia
article datasets in three diﬀerent languages: EN, ES, and
CN.

Figure 8 compares the results of AutoPhrase and AutoSegPhrase,

with the best baseline methods as references. AutoPhrase
outperforms AutoSegPhrase even on the English dataset EN,
though it has been shown the length penalty works rea-
sonably well in English [13]. The Spanish dataset ES has
similar observation. Moreover, the advantage of AutoPhrase
becomes more signiﬁcant on the CN dataset, indicating the
poor generality of length penalty.

(a) Running Time (b) Peak Memory (c) Multi-threading

Figure 9: Eﬃciency of AutoPhrase.

tive of a speciﬁc topic or concept.

Considering the criteria of quality phrases, because single-
word phrases cannot be decomposed into two or more parts,
the concordance and completeness are no longer deﬁnable.
Therefore, we revise the requirements for quality single-
word phrases as below.
• Popularity: Quality phrases should occur with suﬃcient

frequency in the given document collection.

• Informativeness: A phrase is informative if it is indica-

• Independence: A quality single-word phrase is more likely

a complete semantic unit in the given documents.

Only single-word phrases satisfying all popularity, indepen-
dence, and informativeness requirements are recognized as
quality single-word phrases.

Example 6. “UIUC” is a quality single-word phrase. “this”
is not a quality phrase due to its low informativeness. “united”,
usually occurring within other quality multi-word phrases such
as “United States”, “United Kingdom”, “United Airlines”,
and “United Parcel Service”, is not a quality single-word
phrase, because its independence is not enough.

After the phrasal segmentation, in replacement of con-
cordance features, the independence feature is added for
single-word phrases. Formally, it is the ratio of the rectiﬁed
frequency of a single-word phrase given the phrasal segmen-
tation over its raw frequency. Quality single-word phrases
are expected to have large values. For example, “united” is
likely to an almost zero ratio.

We use AutoPhrase+ to denote the extended AutoPhrase

with quality single-word phrase estimation.

6.2 Experiments

We have a similar human annotation as that in the pa-
per. Diﬀerently, we randomly sampled 500 Wiki-uncovered
phrases from the returned phrases (both single-word and
multi-word phrases) of each method in experiments of the
paper. Therefore, we have new pools on the EN, ES, and CN
datasets. The intra-class correlations (ICCs) are all more
than 0.9, which shows the agreement.

Figure 10 compare all methods based these new pools.
Note that all methods except for SegPhrase/WrapSegPhrase
extract single-word phrases as well.

Signiﬁcant recall advantages can be always observed on
all EN, ES, and CN datasets. The recall diﬀerences be-
tween AutoPhrase+ and AutoPhrase, ranging from 10% to
30% sheds light on the importance of modeling single-word
phrases. Across two Latin language datasets, EN and ES,
AutoPhrase+ and AutoPhrase overlaps in the beginning, but
later, the precision of AutoPhrase drops earlier and has a
lower recall due to the lack of single-word phrases. On the
CN dataset, AutoPhrase+ and AutoPhrase has a clear gap
even in the very beginning, which is diﬀerent from the trends
on the EN and ES datasets, which reﬂects that single-word
phrases are more important in Chinese. The major reason
behind is that there are a considerable number of high-quality
phrases (e.g., person names) in Chinese have only one token
after tokenization.

7. CONCLUSIONS

In this paper, we present an automated phrase mining
framework with two novel techniques: the robust positive-
only distant training and the POS-guided phrasal segmenta-
tion incorporating part-of-speech (POS) tags, for the develop-

In summary, thanks to the extra context information and
syntactic information for the particular language, incorpo-
rating POS tags during the phrasal segmentation can work
better than equally penalizing phrases of the same length.

5.7 Case Study

We present a case study about the extracted phrases
as shown in Table 2. The top ranked phrases are mostly
named entities, which makes sense for the Wikipedia article
datasets. Even in the long tail part, there are still many high-
quality phrases. For example, we have the dgreat spotted
woodpeckerc (a type of birds) and d计算机 科学技术c (i.e.,
Computer Science and Technology) ranked about 100,000.
In fact, we have more than 345K and 116K phrases with a
phrase quality higher than 0.5 on the EN and CN datasets
respectively.

5.8 Efﬁciency Evaluation

To study both time and memory eﬃciency, we choose the

three largest datasets: EN, ES, and CN.

Figures 9(a) and 9(b) evaluate the running time and the
peak memory usage of AutoPhrase using 10 threads on dif-
ferent proportions of three datasets respectively. Both time
and memory are linear to the size of text corpora. Moreover,
AutoPhrase can also be parallelized in an almost lock-free
way and shows a linear speedup in Figure 10(c).

Besides, compared to the previous state-of-the-art phrase
mining method SegPhrase and its variants WrapSegPhrase
on three datasets, as shown in Table 3, AutoPhrase achieves
about 8 to 11 times speedup and about 5 to 7 times memory
usage improvement. These improvements are made by a
more eﬃcient indexing and a more thorough parallelization.

6. SINGLE-WORD PHRASES

AutoPhrase can be extended to model single-word phrases,
which can gain about 10% to 30% recall improvements on
diﬀerent datasets. To study the eﬀect of modeling quality
single-word phrases, we choose the three Wikipedia article
datasets in diﬀerent languages: EN, ES, and CN.

6.1 Quality Estimation

In the paper, the deﬁnition of quality phrases and the
evaluation only focus on multi-word phrases. In linguistic
analysis, however, a phrase is not only a group of multiple
words, but also possibly a single word, as long as it functions
as a constituent in the syntax of a sentence [8]. As a great
portion (ranging from 10% to 30% on diﬀerent datasets based
on our experiments) of high-quality phrases, we should take
single-word phrases (e.g., dUIUCc, dIllinoisc, and dUSAc)
into consideration as well as multi-word phrases to achieve a
high recall in phrase mining.

Table 2: The results of AutoPhrase on the EN and CN datasets, with translations and explanations for Chinese
phrases. The whitespaces on the CN dataset are inserted by the Chinese tokenizer.

EN

CN

Sacramento Bee

Phrase
Elf Aquitaine
Arnold Sommerfeld
Eugene Wigner
Tarpon Springs
Sean Astin
. . .

Rank
1
2
3
4
5
. . .
20,001 ECAC Hockey
20,002
20,003 Bering Strait
20,004
Jacknife Lee
20,005 WXYZ-TV
. . .
. . .
99,994
John Gregson
99,995 white-tailed eagle
99,996

Translation (Explanation)
(the name of a soccer team)
Absinthe
(the name of a novel/TV-series)
notebook computer, laptop
Secretary of Party Committee
. . .
African countries
The Left (German: Die Linke)
Fraser Valley
Hippocampus
Mitsuki Saiga (a voice actress)
. . .

Phrase
江苏 舜 天
苦 艾 酒
白发 魔 女
笔记 型 电脑
党委 书记
. . .
非洲 国家
左翼 党
菲 沙 河谷
海马 体
斋 贺光希
. . .
计算机 科学技术 Computer Science and Technology
恒 天然
中国 作家 协会 The Vice President of Writers
副 主席
great spotted woodpecker 维他命 b
舆论 导向
. . .

Association of China
Vitamin B
controlled guidance of the media
. . .

99,997
99,998 David Manners
. . .

rhombic dodecahedron

Fonterra (a company)

. . .

Table 3: Eﬃciency Comparison between AutoPhrase and SegPhrase/WrapSegPhrase utilizing 10 threads.

EN
Time Memory
(mins)
32.77
369.53
11.27

(GB)
13.77
97.72
86%

ES
Time Memory
(mins)
54.05
452.85
8.37

(GB)
16.42
92.47
82%

CN
Time Memory
(mins)
9.43
108.58
11.50

(GB)
5.74
35.38
83%

AutoPhrase
(Wrap)SegPhrase
Speedup/Saving

(a) EN

(b) ES

(c) CN

Figure 10: Precision-recall curves evaluated by human annotation with both single-word and multi-word
phrases in pools.

ment of an automated phrase mining framework AutoPhrase.
Our extensive experiments show that AutoPhrase is domain-
independent, outperforms other phrase mining methods, and
supports multiple languages (e.g., English, Spanish, and
Chinese) eﬀectively, with minimal human eﬀort.

Besides, the inclusion of quality single-word phrases (e.g.,
dUIUCc and dUSAc) which leads to about 10% to 30% in-
creased recall and the exploration of better indexing strate-
gies and more thorough parallelization, which leads to about
8 to 11 times running time speedup and about 80% to 86%

memory usage saving over SegPhrase. Interested readers may
try our released code at GitHub.

For future work, it is interesting to (1) reﬁne quality
phrases to entity mentions, (2) apply AutoPhrase to more lan-
guages, such as Japanese, and (3) for those languages without
general knowledge bases, seek an unsupervised method to
generate the positive pool from the corpus, even with some
noise.

8. REFERENCES
[1] A. Allahverdyan and A. Galstyan. Comparative

A. Rajaraman. Towards the web of concepts:
Extracting concepts from large datasets. Proceedings of
the Very Large Data Bases Conference (VLDB),
3((1-2)), September 2010.

[20] Y. Park, R. J. Byrd, and B. K. Boguraev. Automatic

glossary extraction: beyond terminology identiﬁcation.
In COLING, 2002.

[21] V. Punyakanok and D. Roth. The use of classiﬁers in

sequential inference. In NIPS, 2001.

[22] H. Schmid. Treetagger| a language independent

part-of-speech tagger. Institut für Maschinelle
Sprachverarbeitung, Universität Stuttgart, 43:28, 1995.
[23] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. Kea: Practical automatic
keyphrase extraction. In Proceedings of the fourth ACM
conference on Digital libraries, pages 254–255. ACM,
1999.

[24] E. Xun, C. Huang, and M. Zhou. A uniﬁed statistical
model for the identiﬁcation of english basenp. In ACL,
2000.

[25] Z. Zhang, J. Iria, C. A. Brewster, and F. Ciravegna. A
comparative evaluation of term recognition algorithms.
LREC, 2008.

analysis of viterbi training and maximum likelihood
estimation for hmms. In NIPS, pages 1674–1682, 2011.

[2] L. Breiman. Randomizing outputs to increase

prediction accuracy. Machine Learning, 40(3):229–242,
2000.

[3] K.-h. Chen and H.-H. Chen. Extracting noun phrases
from large-scale texts: A hybrid approach and its
automatic evaluation. In ACL, 1994.

[4] M.-C. De Marneﬀe, B. MacCartney, C. D. Manning,

et al. Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454, 2006.

[5] P. Deane. A nonparametric method for extraction of

candidate phrasal terms. In ACL, 2005.

[6] A. El-Kishky, Y. Song, C. Wang, C. R. Voss, and
J. Han. Scalable topical phrase mining from text
corpora. VLDB, 8(3), Aug. 2015.

[7] D. A. Evans and C. Zhai. Noun-phrase analysis in
unrestricted text for information retrieval. In
Proceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 17–24.
Association for Computational Linguistics, 1996.
[8] G. Finch. Linguistic terms and concepts. Macmillan

Press Limited, 2000.

[9] K. Frantzi, S. Ananiadou, and H. Mima. Automatic

recognition of multi-word terms:. the c-value/nc-value
method. JODL, 3(2):115–130, 2000.

[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely

randomized trees. Machine learning, 63(1):3–42, 2006.

[11] T. Koo, X. Carreras, and M. Collins. Simple

semi-supervised dependency parsing. ACL-HLT, 2008.
[12] R. Levy and C. Manning. Is it harder to parse chinese,
or the chinese treebank? In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 439–446. Association for
Computational Linguistics, 2003.

[13] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. Mining

quality phrases from massive text corpora. In
Proceedings of 2015 ACM SIGMOD International
Conference on Management of Data, 2015.

[14] Z. Liu, X. Chen, Y. Zheng, and M. Sun. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
135–144. Association for Computational Linguistics,
2011.

[15] G. Martínez-Muñoz and A. Suárez. Switching class
labels to generate classiﬁcation ensembles. Pattern
Recognition, 38(10):1483–1494, 2005.

[16] R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.

Non-projective dependency parsing using spanning tree
algorithms. In EMNLP, 2005.

[17] R. Mihalcea and P. Tarau. Textrank: Bringing order

into texts. In ACL, 2004.

[18] J. Nivre, M.-C. de Marneﬀe, F. Ginter, Y. Goldberg,
J. Hajic, C. D. Manning, R. McDonald, S. Petrov,
S. Pyysalo, N. Silveira, et al. Universal dependencies
v1: A multilingual treebank collection. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016), 2016.

[19] A. Parameswaran, H. Garcia-Molina, and

