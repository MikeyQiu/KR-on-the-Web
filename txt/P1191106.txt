8
1
0
2
 
n
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
1
8
0
1
.
5
0
7
1
:
v
i
X
r
a

Surface Networks

Ilya Kostrikov1, Zhongshi Jiang1, Daniele Panozzo ∗1, Denis Zorin †1, and Joan Bruna‡1,2

1Courant Institute of Mathematical Sciences, New York University
2Center for Data Science, New York University

Abstract

We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used
to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs,
namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the
Laplacian operator.

Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric
deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to
GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In
particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions — this is in stark
contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface
Networks (SN).

We prove that these models deﬁne shape representations that are stable to deformation and to discretization, and we
demonstrate the efﬁciency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under
non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by
SNs.

1. Introduction

3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering
to computer animation to medical imaging. Despite the vast amount of high-quality 3D geometric data available, data-
driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data
representation regularity which is required for traditional convolutional neural network approaches. While in computer vision
problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more
complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar
chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical
properties.

Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike
the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the
inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary
features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations.
Extrinsic representations rely on the speciﬁc embedding of surfaces within a three-dimensional ambient space, whereas
intrinsic representations only capture geometric properties speciﬁc to the surface, irrespective of its parametrization. Whereas
the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local
deformations and invariance to global transformations.

∗DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.
†DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.
‡JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding

author: bruna@cims.nyu.edu

1

A particularly simple and popular extrinsic method [36, 37] represents shapes as point clouds in R3 of variable size,
and leverages recent deep learning models that operate on input sets [44, 43]. Despite its advantages in terms of ease
of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classiﬁcation
and segmentation tasks, one may wonder whether this simpliﬁcation comes at a loss of precision as one considers more
challenging prediction tasks.

In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on
geometric deep learning. These models provide data-driven intrinsic graph and manifold representations with inductive
biases analogous to CNNs on natural images. Models based on Graph Neural Networks [40] and their spectral variants
[6, 11, 26] have been successfully applied to geometry processing tasks such as shape correspondence [32]. In their basic
form, these models learn a deep representation over the discretized surface by combining a latent representation at a given
node with a local linear combination of its neighbors’ latent representations, and a point-wise nonlinearity. Different models
vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to
spectral interpretations of those models.

Our contributions are three-fold. First, we extend the model to support extrinsic features. More speciﬁcally, we exploit the
fact that surfaces in R3 admit a ﬁrst-order differential operator, the Dirac operator, that is stable to discretization, provides a
direct generalization of Laplacian-based propagation models, and is able to detect principal curvature directions [9, 18]. Next,
we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization,
two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the
variational autoencoder framework [25, 39], that is able to exploit non-Euclidean geometric regularity.

By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation
that we apply to several challenging tasks. The resulting Surface Networks – using either the Dirac or the Laplacian, inherit
the stability and invariance properties of these operators, thus providing data-driven representations with certiﬁed stability to
deformations. We demonstrate the model efﬁciency on a temporal prediction task of complex dynamics, based on a physical
simulation of elastic shells, which conﬁrms that whenever geometric information (in the form of a mesh) is available, it can
be leveraged to signiﬁcantly outperform point-cloud based models.

Our main contributions are summarized as follows:

• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear

dynamics, motivating the use of geometric shape information.

• We prove that Surface Networks deﬁne shape representations that are stable to deformation and to discretization.

• We introduce a generative model for 3D surfaces based on the variational autoencoder.

2. Related Work

Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research.
[40] introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained
by backpropagation. Subsequent works [27, 43] have relaxed the model by untying the recurrent layer weights and proposed
several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convo-
lutional networks to non-Euclidean graphs. [6, 17] proposed to learn smooth spectral multipliers of the graph Laplacian,
albeit with high computational cost, and [11, 26] resolved the computational bottleneck by learning polynomials of the graph
Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to
[5] for an exhaustive literature review on the topic. GNNs are ﬁnding application in many different domains. [2, 7] develop
graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics.
[12, 22] study molecular ﬁngerprints using variants of the GNN architecture, and [14] further develop the model by combin-
ing it with set representations [44], showing state-of-the-art results on molecular prediction. The resulting models, so-called
Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model
on general graphs.

In the context of computer graphics, [30] developed the ﬁrst CNN model on meshed surfaces using intrinsic patch rep-
resentations, and further generalized in [4] and [32]. This last work allows for ﬂexible representations via the so-called
pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode ﬁrst-
order differential information. These intrinsic models contrast with Euclidean models such as [48, 46], that have higher
sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models

A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/SurfaceNetworks.

are increasingly popular to model 3d objects due to their simplicity and versatility. [36, 37] use set-invariant representations
from [44, 43] to solve shape segmentation and classiﬁcation tasks. More recently, [29] proposes to learn surface convolutional
network from a canonical representation of planar ﬂat-torus, with excellent performance on shape segmentation and classi-
ﬁcation, although such canonical representations may introduce exponential scale changes that can introduce instabilities.
Finally, [13] proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but
does not encode geometrical information as our shape generative model. Learning variational deformations is an important
problem for graphics applications, since it enables negligible and ﬁxed per-frame cost [35], but it is currently limited to 2D
deformations using point handles. In constrast, our method easily generalizes to 3D and learns dynamic behaviours.

3. Surface Networks

This section presents our surface neural network model and its basic properties. We start by introducing the problem setup
and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section
3.2).

3.1. Laplacian Surface Networks

Our ﬁrst goal is to deﬁne a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where
V = (vi ∈ R3)i≤N contains the node coordinates, E = (ei,j) corresponds to edges, and F is the set of triangular faces. We
denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see [5] for details).
This operator can be interpreted as a local, linear high-pass ﬁlter in M that acts on signals x ∈ Rd×|V | deﬁned on the
vertices as a simple matrix multiplication ˜x = ∆x. By combining ∆ with an all-pass ﬁlter and learning generic linear
combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators
in M that update a feature map from layer k to layer k + 1 using trainable parameters Ak and Bk:

xk+1 = ρ (cid:0)Ak∆xk + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk .

By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization,
this model is a speciﬁc instance of the graph neural network [40, 5, 26] and a generalization of the spectrum-free Laplacian
networks from [11]. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening
or pooling layers.

In contrast to general graphs, meshes contain a low-dimensional Euclidean embedding that contains potentially useful
information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A
simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as
input channels to the network: x1 := V ∈ R|V |×3. The mean curvature can be computed by applying the Laplace operator
to the coordinates of the vertices:

∆x1 = −2Hn ,

where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian
neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph
neural network models is related to the use of generalized coordinates from [32]. By cascading K layers of the form (1) we
obtain a representation Φ∆(M) that contains generic features at each node location. When the number of layers K is of
the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate
information across the whole surface.

Equation (2) illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to
the mean variations across all directions. Although in general graphs there is no well-deﬁned procedure to recover anisotropic
local variations, in the case of surfaces some authors ([4, 1, 32]) have considered anisotropic extensions. We describe next
a particularly simple procedure to increase the expressive power of the network using a related operator from quantum
mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation [9] and shape
analysis [18].

(1)

(2)

3.2. Dirac Surface Networks

The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the
gradient (a ﬁrst-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to
these ﬁrst-order differential operators separately, enabling oriented high-pass ﬁlters.

For convenience, we embed R3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for
details). The Dirac operator is then deﬁned as a matrix D ∈ H|F |×|V | that maps (quaternion) signals on the nodes to signals
on the faces. In coordinates,

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area (see Appendix A) using counter-clockwise
orientations on all faces.

To apply the Dirac operator deﬁned in quaternions to signals in vertices and faces deﬁned in real numbers, we write the
feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a
quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The
Dirac operator provides ﬁrst-order differential information and is sensitive to local orientations. Moreover, one can verify [9]
that

Re D∗D = ∆ ,
where D∗ is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as
D∗ = M −1
V DH MF where DH is a conjugate transpose of D and MV , MF are diagonal mass matrices with one third of
areas of triangles incident to a vertex and face areas respectively.

The Dirac operator can be used to deﬁne a new neural surface representation that alternates layers with signals deﬁned
over nodes with layers deﬁned over faces. Given a d-dimensional feature representation over the nodes xk ∈ Rd×|V |, and the
faces of the mesh, yk ∈ Rd×|F |, we deﬁne a d(cid:48)-dimensional mapping to a face representation as

yk+1 = ρ (cid:0)CkDxk + Ekyk(cid:1) , Ck, Ek ∈ Rdk+1×dk ,

(3)

where Ck, Ek are trainable parameters. Similarly, we deﬁne the adjoint layer that maps back to a ˜d-dimensional signal over
nodes as

xk+1 = ρ (cid:0)AkD∗yk+1 + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk ,
where Ak, Bk are trainable parameters. A surface neural network layer is thus determined by parameters {A, B, C, E} using
equations (3) and (4) to deﬁne xk+1 ∈ Rdk+1×|V |. We denote by ΦD(M) the mesh representation resulting from applying
K such layers (that we assume ﬁxed for the purpose of exposition).

(4)

The Dirac-based surface network is related to edge feature transforms proposed on general graphs in [14], although these
edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no
notion of square root of ∆ that recovers oriented ﬁrst-order derivatives.

4. Stability of Surface Networks

Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise
under the model. Given a continuous surface S ⊂ R3 or a discrete mesh M, and a smooth deformation ﬁeld τ : R3 → R3,
we are particularly interested in two forms of stability:

• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that (cid:107)Φ(M) −

Φ(τ (M))(cid:107) is small if (cid:107)∇τ (∇τ )∗ − I(cid:107) is small, i.e when the deformation is nearly rigid; see Theorem 4.1.

• Given two discretizations M1 and M2 of the same underlying surface S, we would like to control (cid:107)Φ(M1)−Φ(M2)(cid:107)

in terms of the resolution of the meshes; see Theorem 4.2.

These stability properties are important in applications, since most tasks we are interested in are stable to deformation and
to discretization. We shall see that the ﬁrst property is a simple consequence of the fact that the mesh Laplacian and Dirac
operators are themselves stable to deformations. The second property will require us to specify under which conditions the
discrete mesh Laplacian ∆M converges to the Laplace-Beltrami operator ∆S on S. Unless it is clear from the context, in the
following ∆ will denote the discrete Laplacian.

Theorem 4.1 Let M be a N -node mesh and x, x(cid:48) ∈ R|V |×d be input signals deﬁned on the nodes. Assume the nonlinearity
ρ( · ) is non-expansive (|ρ(z) − ρ(z(cid:48))| ≤ |z − z(cid:48)|). Then

(a) (cid:107)Φ∆(M; x) − Φ∆(M; x(cid:48))(cid:107) ≤ α∆(cid:107)x − x(cid:48)(cid:107) , where α∆ depends only on the trained weights and the mesh.

(b) (cid:107)ΦD(M; x) − ΦD(M; x(cid:48))(cid:107) ≤ αD(cid:107)x − x(cid:48)(cid:107) , where αD depends only on the trained weights and the mesh.

(c) Let |∇τ |∞ := supu (cid:107)∇τ (u)(∇τ (u))∗ − 1(cid:107), where ∇τ (u) is the Jacobian matrix of u (cid:55)→ τ (u). Then (cid:107)Φ∆(M; x) −

Φ∆(τ (M); x)(cid:107) ≤ β∆|∇τ |∞(cid:107)x(cid:107) , where β∆ is independent of τ and x.

(d) Denote by (cid:103)|∇τ |∞ := supu (cid:107)∇τ (u) − 1(cid:107). Then (cid:107)ΦD(M; x) − ΦD(τ (M); x)(cid:107) ≤ βD (cid:103)|∇τ |∞(cid:107)x(cid:107) , where βD is

independent of τ and x.

Properties (a) and (b) are not speciﬁc to surface representations, and are a simple consequence of the non-expansive property
of our chosen nonlinearities. The constant α is controlled by the product of (cid:96)2 norms of the network weights at each layer and
the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators
are themselves stable to deformations, a property that depends on two key aspects: ﬁrst, the Laplacian/Dirac is localized in
space, and next, that it is a high-pass ﬁlter and therefore only depends on relative changes in position.

One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the
reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral
radius of ∆M diverges as the mesh size N increases.

In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incom-
ing to the surface network. This can be measured with Sobolev norms deﬁned using the spectrum of the Laplacian operator.
Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆M, consider
the spectral decomposition of ∆M (a symmetric, positive deﬁnite operator):

∆M =

λkekeT

k , ek ∈ RN , 0 ≤ λ1 ≤ λ2 · · · ≤ λN .

(cid:88)

k≤N

Under normal uniform convergence 1 [45], the spectrum of ∆M converges to the spectrum of the Laplace-Beltrami operator
∆S of S. If S is bounded, it is known from the Weyl law [47] that there exists γ > 0 such that k−γ(S) (cid:46) λ−1
k , so the
eigenvalues λk do not grow too fast. The smoothness of a signal x ∈ R|V |×d deﬁned in M is captured by how fast its
H := (cid:80)
spectral decomposition ˆx(k) = eT
k λ(k)2(cid:107)ˆx(k)(cid:107)2 is Sobolev norm, and
β(x, S) > 1 as the largest rate such that its spectral decomposition coefﬁcients satisfy

k x ∈ Rd decays [42]. We deﬁne (cid:107)x(cid:107)2

(cid:107)ˆx(k)(cid:107) (cid:46) k−β , (k → ∞) .
(5)
If x ∈ R|V |×d is the input to the Laplace Surface Network of R layers, we denote by (β0, β1, . . . , βR−1) the smoothness
rates of the feature maps x(r) deﬁned at each layer r ≤ R.

Theorem 4.2 Consider a surface S and a ﬁnite-mesh approximation MN of N points, and Φ∆ a Laplace Surface Network
with parameters {(Ar, Br)}r≤R. Denote by d(S, MN ) the uniform normal distance, and let x1, x2 be piece-wise polyhedral
approximations of ¯x(t), t ∈ S in MN , with (cid:107)¯x(cid:107)H(S) < ∞. Assume (cid:107)¯x(r)(cid:107)H(S) < ∞ for r ≤ R.

(a) If x1, x2 are two functions such that the R feature maps x(r)

have rates (β0, β1, . . . , βR−1), then

l
(cid:107)Φ∆(x1; MN ) − Φ∆(x2; MN )(cid:107)2 ≤ C(β)(cid:107)x1 − x2(cid:107)h(β) ,

(6)

with h(β) = (cid:81)R

βr−1
βr−1/2 , and where C(β) does not depend upon N .

r=1

(b) If τ is a smooth deformation ﬁeld, then (cid:107)Φ∆(x; MN ) − Φ∆(x; τ (MN ))(cid:107) ≤ C|∇τ |∞

h(β) , where C does not depend

upon N .

(c) Let M and M(cid:48) be N -point discretizations of S, If max(d(M, S), d(M(cid:48), S)) ≤ (cid:15), then (cid:107)Φ∆(M; x) − Φ∆(M(cid:48), x(cid:48))(cid:107) ≤

C(cid:15)h(β) , where C is independent of N .

This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases,
the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac
operator also has a well-deﬁned continuous counterpart [9] that generalizes the gradient operator in quaternion space. Also,
our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in
terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x).
These extensions are left for future work. Finally, a speciﬁc setup that we use in experiments is to use as input signal the
canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields

Corollary 4.3 Denote Φ(M) := ΦM(V ), where V are the node coordinates of M. Then, if A1 = 0,

(cid:107)Φ(M) − Φ(τ (M))(cid:107) ≤ κ max(|∇τ |∞, (cid:107)∇2τ (cid:107))h(β) .

(7)

1which controls how the normals of the mesh align with the surface normals; see [45].

Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R3 (right) is expressed in terms of a “sampling” 2D irregular mesh
˜M ⊂ R2 (left) and a depth scalar ﬁeld f : ˜M → R over ˜M (center).

5. Generative Surface Models

State-of-the-art generative models for images, such as generative adversarial networks [38], pixel autoregressive networks
[34], or variational autoencoders [25], exploit the locality and stationarity of natural images in their probabilistic models, in
the sense that the model satisﬁes pθ(x) ≈ pθ(xτ ) by construction, where xτ is a small deformation of a given input x. This
property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric
stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains
two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface,
which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas
the former contains the essential semantic information, the latter is not informative, and to some extent independent of the
shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as
height-ﬁeld meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as ( ˜M, f ( ˜M)), where ˜M = ( ˜V , ˜E, ˜F ) is
now a 2D mesh and f : ˜V → R is a depth-map encoding the original node locations V , as shown in Figure 1.

In this work, we consider the variational autoencoder framework [25, 39].

It considers a mixture model of the form
p(M) = (cid:82) pθ(M | h)p0(h)dh , where h ∈ R|S| is a vector of latent variables. We train this model by optimizing the
variational lower bound of the data log-likelihood:

min
θ,ψ

1
L

(cid:88)

l≤L

−Eh∼qψ (h | Ml) log pθ(Ml | h) + DKL(qψ(h | Ml) || p0(h)) .

(8)

We thus need to specify a conditional generative model pθ(M | h), a prior distribution p0(h) and a variational approximation
to the posterior qψ(h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the
height-ﬁeld representation, we choose for simplicity a separable model of the form pθ(M | h) = pθ(f | h, ˜M) · p( ˜M) ,
where ˜M ∼ p( ˜M) is a homogeneous Poisson point process, and f ∼ pθ(f | h, ˜M) is a normal distribution with mean and
isotropic covariance parameters given by a SN:

pθ(f | h, ˜M) = N (µ(h, ˜M), σ2(h, ˜M)1) ,

with [µ(h, ˜M), σ2(h, ˜M)] = ΦD( ˜M ; h) . The generation step thus proceeds as follows. We ﬁrst sample a 2D mesh ˜M
independent of the latent variable h, and then sample a depth ﬁeld over ˜M conditioned on h from the output of a decoder
network ΦD( ˜M ; h). Finally, the variational family qψ is also a Normal distribution whose parameters are obtained from an
encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization: qψ(h | M) =
N (¯µ, ¯σ21) , with [¯µ, ¯σ] = ¯ΦD(M) .

6. Experiments

6.1. MeshMNIST

For experimental evaluation, we compare models built using ResNet-v2 blocks [16], where convolutions are replaced with
the appropriate operators (see Fig. 2): (i) a point cloud based model from [43] that aggregates global information by averaging
features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical
coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant
of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).

For this task, we construct a MeshMNIST database with only height-ﬁeld meshes (Sec. 5). First, we sample points on
a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a
Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each

Figure 2. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to
the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch
Normalization (blue) and ‘1 × 1’ convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language
and denote by xk+1 the output of this 2-layer block.

Receptive ﬁeld Number of parameters

Model
MLP
PointCloud
Laplace
Dirac

1
-
16
8

519672
1018872
1018872
1018872

Smooth L1-loss (mean per sequence (std))
64.56 (0.62)
23.64 (0.21)
17.34 (0.52)
16.84 (0.16)

Table 1. Evaluation of different models on the temporal task

point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to deﬁne a sampling process
over 3D height-ﬁeld meshes.

We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder
converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to
obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a speciﬁc 3D
mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does
not depend on input data. We trained the model for 75 epochs using Adam optimizer [24] with learning rate 10−3, weight
decay 10−5 and batch size 32. Figures 3,4 illustrate samples from the model. The geometric encoder is able to leverage the
local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to
the speciﬁc sampled grid, as opposed to set-based generative models.

Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of dis-
cretization/triangulation (Poisson disk sampling with p=1.5).

Figure 4. Meshes from the dataset (ﬁrst ﬁve). And meshes generated by our model (last ﬁve).

6.2. Spatio-Temporal Predictions

One speciﬁc task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1, X 2, . . . , X n,

the task is to predict the following frames Y = Y 1 = X n+1, Y 2, . . . , Y m = X n+m. As in [31], we use a simple non-
recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on
n = 2 frames and predict the next m = 40 frames. In order to generate data, we ﬁrst extracted 10k patches from the
MPI-Faust dataset[3], by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of
the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground.
We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique [41], with additional
gravitational forces [20]. Libigl [21] has been used for the mesh processing tasks. Sequences with patches from the ﬁrst 80
subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on
request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial
frames. Thus, we can train models by minimizing smooth-L1 loss [15] between target frames and output of our models.

Ground Truth

MLP

PointCloud

Laplace

Dirac

Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set.
Boxes indicate distinctive features. For larger crops, see Figure 6

.

We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and
Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned
frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce
120-dimensional outputs. We trained all models using the Adam optimizer [24] with learning rate 10−3, weight decay 10−5,
and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained
for 110k steps in overall.

Ground Truth

Laplace

Dirac

Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Table 1 reports quantitative prediction performance of different models, and Figure 5 displays samples from the prediction

Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between
ground truth and prediction: green - smaller error, red - larger error.

Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

models at speciﬁc frames. The set-to-set model [44, 43], corresponding to a point-cloud representation used also in [36],
already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between
this model and Laplace-/Dirac-based models is signiﬁcant, both visually and quantitatively. Dirac-based model outperforms
Laplace-based model despite the smaller receptive ﬁeld. Videos comparing the performance of different models are available
in the additional material.

Figure 6 illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since
they propagate information using an isotropic operator, have more difﬁculties at resolving corners and pointy structures than
the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit
the extrinsic information only via the input coordinates is remarkable and more computationally efﬁcient than the Dirac
counterpart. Figures 7 and 8 overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac
respectively. They conﬁrm ﬁrst that SNs outperform the point-cloud based model, which often produce excessive ﬂattening
and large deformations, and next that ﬁrst-order Dirac operators help resolve areas with high directional curvature. We refer
to the supplementary material for additional qualitative results.

7. Conclusions

We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the non-Euclidean
geometry of surfaces. We have shown how a ﬁrst-order differential operator (the Dirac operator) can detect and adapt to
geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction
is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments.
That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting
to instead learn the operator, akin to recent Message-Passing NNs [14] and explore whether Dirac is recovered.

Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly
superior performance to point-cloud based models. While there are not many such datasets currently available, we expect
them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated
in consumer devices. SNs provide efﬁcient inference, with predictable runtime, which makes them appealing across many
areas of computer graphics, where a ﬁxed, per-frame cost is required to ensure a stable framerate, especially in VR applica-
tions. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and
generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.

References

2014. 3

[1] M. Andreux, E. Rodol`a, M. Aubry, and D. Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In Proc. NORDIA,

[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In

Advances in Neural Information Processing Systems, pages 4502–4510, 2016. 2

[3] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 3794–3801, 2014. 7, 18, 20

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks.

In Advances in Neural Information Processing Systems, pages 3189–3197, 2016. 2, 3

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv

preprint arXiv:1611.08097, 2016. 2, 3, 14

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013. 2
[7] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics.

[8] D. Chen and J. R. Gilbert. Obtaining bounds on the two norm of a matrix from the splitting lemma. Electronic Transactions on

[9] K. Crane, U. Pinkall, and P. Schr¨oder. Spin transformations of discrete surfaces. In ACM Transactions on Graphics (TOG). ACM,

ICLR, 2016. 2

2011. 2, 3, 4, 5

Numerical Analysis, 21:28–46, 2005. 14

and Its Applications, 427(1):55–69, 2007. 12

[10] K. C. Das. Extremal graph characterization from the upper bound of the laplacian spectral radius of weighted graphs. Linear Algebra

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In

Advances in Neural Information Processing Systems, pages 3837–3845, 2016. 2, 3

[12] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convo-

lutional networks on graphs for learning molecular ﬁngerprints. In Neural Information Processing Systems, 2015. 2

[13] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. arXiv preprint

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. 7
[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision,

arXiv:1612.00603, 2016. 3

arXiv:1704.01212, 2017. 2, 4, 10

pages 630–645. Springer, 2016. 6

[17] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015. 2
[18] K. C. Hsueh-Ti Derek Liu, Alec Jacobson. A dirac operator for extrinsic shape analysis. Computer Graphics Forum, 2017. 2, 3
[19] Y. Hu, Q. Zhou, X. Gao, A. Jacobson, D. Zorin, and D. Panozzo. Tetrahedral meshing in the wild. Submitted to ACM Transaction on

Graphics, 2018. 18

[20] A. Jacobson. Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes. PhD thesis, ETH, Z¨urich, 2013. 7
[21] A. Jacobson, D. Panozzo, et al. libigl: A simple C++ geometry processing library, 2016. http://libigl.github.io/libigl/. 7
[22] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal

of computer-aided molecular design, 2016. 2

[23] V. G. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic maps. In ACM Transactions on Graphics (TOG), volume 30, page 79.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representation, 2015.

[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 6
[26] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907,

ACM, 2011. 19

7, 8

2016. 2, 3

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. 2
[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape

correspondence. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5660–5668, 2017. 21

[29] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. Kim, and Y. Lipman. Convolutional neural networks on surfaces

via seamless toric covers. In SIGGRAPH, 2017. 3

[30] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In

Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. 2

[31] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error.

arXiv preprint

arXiv:1511.05440, 2015. 7

[32] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds

using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. 2, 3

[33] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna. A note on learning algorithms for quadratic assignment with graph neural networks.

arXiv preprint arXiv:1706.07450, 2017. 18

[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. 6
[35] R. Poranne and Y. Lipman. Simple approximations of planar deformation operators. Technical report, ETHZ, 2015. 3
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. arXiv preprint

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint

arXiv:1612.00593, 2016. 2, 3, 9

arXiv:1706.02413, 2017. 2, 3

arXiv preprint arXiv:1511.06434, 2015. 6

Neural Networks, 20(1):61–80, 2009. 2, 3

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks.

[39] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint arXiv:1505.05770, 2015. 2, 6
[40] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on

[41] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, 2007. 7
[42] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE

[43] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation.

In Advances in Neural Information

Symposium on, pages 29–38. IEEE, 2007. 5

Processing Systems, pages 2244–2252, 2016. 2, 3, 6, 9

[44] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015. 2, 3, 9
[45] M. Wardetzky. Convergence of the cotangent formula: An overview. In Discrete Differential Geometry, pages 275–286. 2008. 5, 12,

17

[46] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolutional networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1544–1553, 2016. 2

[47] H. Weyl.

¨Uber die asymptotische verteilung der eigenwerte. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottingen,

Mathematisch-Physikalische Klasse, 1911:110–117, 1911. 5

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 2

A. The Dirac Operator

The quaternions H is an extension of complex numbers. A quaternion q ∈ H can be represented in a form q = a+bi+cj +

dk where a, b, c, d are real numbers and i, j, k are quaternion units that satisfy the relationship i2 = j2 = k2 = ijk = −1.

As mentioned in Section 3.1, the Dirac operator used in the model can be conveniently represented as a quaternion matrix:

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area, as illustrated in Fig. A, using counter-
clockwise orientations on all faces.

vj

f

ej







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c













b
a
−b
a
−c −d
−d

d
c
d −c
b
a
a
c −b







.

The Deep Learning library PyTorch that we used to implement the models does not support quaternions. Nevertheless,
quaternion-valued matrix multiplication can be replaced with real-valued matrix multiplication where each entry q = a +
bi + cj + dk is represented as a 4 × 4 block

and the conjugate q∗ = a − bi − cj − dk is a transpose of this real-valued matrix:

B. Theorem 4.1

B.1. Proof of (a)

Laplacian ∆ of M is

We ﬁrst show the result for the mapping x (cid:55)→ ρ (Ax + B∆x), corresponding to one layer of Φ∆. By deﬁnition, the

∆ = diag( ¯A)−1(U − W ) ,
where ¯Aj is one third of the total area of triangles incident to node j, and W = (wi,j) contains the cotangent weights [45],
and U = diag(W 1) contains the node aggregated weights in its diagonal.

From [10] we verify that

(cid:107)U − W (cid:107) ≤

√

2 max
i

(cid:115)






U 2

i + Ui

Ujwi,j

(cid:88)

i∼j






√

√

≤ 2

2 sup
i,j

wi,j sup

dj

j

≤ 2

2 cot(αmin)dmax ,

(cid:107)∆(cid:107) ≤ C

cot(αmin)Smax
inf j ¯Aj

:= LM ,

where dj denotes the degree (number of neighbors) of node j, αmin is the smallest angle in the triangulation of M and Smax
the largest number of incident triangles. It results that

which depends uniquely on the mesh M and is ﬁnite for non-degenerate meshes. Moreover, since ρ( · ) is non-expansive, we
have

(cid:107)ρ (Ax + B∆x) − ρ (Ax(cid:48) + B∆x(cid:48))(cid:107) ≤ (cid:107)A(x − x(cid:48)) + B∆(x − x(cid:48))(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)LM)(cid:107)x − x(cid:48)(cid:107) .

By cascading (10) across the K layers of the network, we obtain

(cid:107)Φ(M; x) − Φ(M; x(cid:48))(cid:107) ≤

((cid:107)Ak(cid:107) + (cid:107)Bk(cid:107)LM)


 (cid:107)x − x(cid:48)(cid:107) ,





(cid:89)

k≤K

(9)

(10)

which proves (a). (cid:3)

B.2. Proof of (b)

B.3. Proof of (c)

The proof is analogous, by observing that (cid:107)D(cid:107) = (cid:112)(cid:107)∆(cid:107) and therefore

(cid:107)D(cid:107) ≤

(cid:112)

LM . (cid:3)

To establish (c) we ﬁrst observe that given three points p, q, r ∈ R3 forming any of the triangles of M,

(cid:107)p − q(cid:107)2(1 − |∇τ |∞)2

≤ (cid:107)τ (p) − τ (q)(cid:107)2 ≤

(cid:107)p − q(cid:107)2(1 + |∇τ |∞)2

A(p, q, r)2(1 − |∇τ |∞Cα−2

min − o(|∇τ |∞

2) ≤ A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2(1 + |∇τ |∞Cα−2

min + o(|∇τ |∞

2)) .

(11)

(12)

Indeed, (11) is a direct consequence of the lower and upper Lipschitz constants of τ (u), which are bounded respectively by
1 − |∇τ |∞ and 1 + |∇τ |∞. As for (12), we use the Heron formula

A(p, q, r)2 = s(s − (cid:107)p − q(cid:107))(s − (cid:107)p − r(cid:107))(s − (cid:107)r − q(cid:107)) ,

with s = 1
determined by the deformed points τ (p), τ (q), τ (r), we have that

2 ((cid:107)p − q(cid:107) + (cid:107)p − r(cid:107) + (cid:107)r − q(cid:107)) being the half-perimeter. By denoting sτ the corresponding half-perimeter

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≤ s(1 + |∇τ |∞) − (cid:107)p − q(cid:107)(1 − |∇τ |∞) = s − (cid:107)p − q(cid:107) + |∇τ |∞(s + (cid:107)p − q(cid:107)) and

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≥ s(1 − |∇τ |∞) − (cid:107)p − q(cid:107)(1 + |∇τ |∞) = s − (cid:107)p − q(cid:107) − |∇τ |∞(s + (cid:107)p − q(cid:107)) ,

and similarly for the (cid:107)r − q(cid:107) and (cid:107)r − p(cid:107) terms. It results in

A(τ (p), τ (q), τ (r))2 ≥ A(p, q, r)2
≥ A(p, q, r)2 (cid:104)

(cid:20)
1 − |∇τ |∞

(cid:18)

1 − C|∇τ |∞α−2

1 +

+

s + (cid:107)p − q(cid:107)
s − (cid:107)p − q(cid:107)
(cid:105)
2)

min − o(|∇τ |∞

,

s + (cid:107)p − r(cid:107)
s − (cid:107)p − r(cid:107)

+

s + (cid:107)r − q(cid:107)
s − (cid:107)r − q(cid:107)

(cid:19)

− o(|∇τ |∞

(cid:21)
2)

and similarly

A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2 (cid:104)

1 + C|∇τ |∞α−2

min − o(|∇τ |∞

(cid:105)

2)

.

By noting that the cotangent Laplacian weights can be written (see Fig. 9) as

wi,j =

−(cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
A(i, j, k)

+

−(cid:96)2

jh + (cid:96)2
ih

ij + (cid:96)2
A(i, j, h)

,

we have from the previous Bilipschitz bounds that

τ (wi,j) ≤ wi,j

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

+ 2|∇τ |∞

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

τ (wi,j) ≥ wi,j

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

− 2|∇τ |∞

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

+

+

(cid:33)

(cid:33)

,

,

which proves that, up to second order terms, the cotangent weights are Lipschitz continuous to deformations.

Finally, since the mesh Laplacian operator is constructed as diag( ¯A)−1(U − W ), with ¯Ai,i = 1
3

(cid:80)

j,k;(i,j,k)∈F A(i, j, k),

and U = diag(W 1), let us show how to bound (cid:107)∆ − τ (∆)(cid:107) from

¯Ai,i(1 − αM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ ( ¯Ai,i) ≤ ¯Ai,i(1 + αM|∇τ |∞ + o(|∇τ |∞

2))

and

wi,j(1 − βM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ (wi,j) ≤ wi,j(1 + βM|∇τ |∞ + o(|∇τ |∞

2)) .

(13)

(14)

k

αij

aijk

j

(cid:96)ij

ai

i

βij

h

Figure 9. Triangular mesh and Cotangent Laplacian (ﬁgure reproduced from [5])

Using the fact that ¯A, τ ( ¯A) are diagonal, and using the spectral bound for k × m sparse matrices from [8], Lemma 5.12,

(cid:107)Y (cid:107)2 ≤ max

i

(cid:88)

j; Yi,j (cid:54)=0

|Yi,j|

|Yr,j|

,

(cid:33)

(cid:32) l

(cid:88)

r=1

the bounds (13) and (14) yield respectively

τ ( ¯A) = ¯A(1 + (cid:15)τ ) , with (cid:107)(cid:15)τ (cid:107) = o(|∇τ |∞) , and
τ (U − W ) = U − W + ητ , with (cid:107)ητ (cid:107) = o(|∇τ |∞) .

It results that, up to second order terms,
(cid:13)τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )(cid:13)
(cid:107)∆ − τ (∆)(cid:107) = (cid:13)
(cid:13)
(cid:13)
(cid:0) ¯A[1 + (cid:15)τ ](cid:1)−1
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
=
(cid:13)
= (cid:13)
(cid:13)(cid:15)τ ∆ + ¯A−1ητ
= o(|τ |∞) ,

(cid:13)
(cid:13) + o(|∇τ |∞

1 − (cid:15)τ + o(|∇τ |∞

2)

=

[U − W + ητ ] − ¯A−1(U − W )

(cid:13)
(cid:13)
(cid:13)

(cid:17) ¯A−1(U − W + ητ ) − ¯A−1(U − W )
2)

(cid:13)
(cid:13)
(cid:13)

which shows that the Laplacian is stable to deformations in operator norm. Finally, by denoting ˜xτ a layer of the deformed
Laplacian network

it follows that

Also,

˜xτ = ρ(Ax + Bτ (∆)x) ,

(cid:107)˜x − ˜xτ (cid:107) ≤ (cid:107)B(∆ − τ (∆)x(cid:107)
≤ C(cid:107)B(cid:107)|∇τ |∞(cid:107)x(cid:107) .

(cid:107)˜x − ˜yτ (cid:107) ≤ (cid:107)A(x − y) + B(∆x − τ (∆)y)(cid:107)

≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))(cid:107)x − y(cid:107) + (cid:107)∆ − τ (∆)(cid:107)(cid:107)x(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))
(cid:125)

(cid:107)x − y(cid:107) + C|∇τ |∞
(cid:124) (cid:123)(cid:122) (cid:125)
δ2

(cid:107)x(cid:107) ,

(cid:123)(cid:122)
δ1

(cid:124)

and therefore, by plugging (17) with y = ˜xτ , K layers of the Laplacian network satisfy

(cid:107)Φ(x; ∆) − Φ(x; τ (∆)(cid:107) ≤





(cid:89)

j≤K−1








δ1(j)

 (cid:107)˜x − ˜xτ (cid:107) +



(cid:88)

(cid:89)

j<K−1

j(cid:48)≤j







δ1(j(cid:48))δ2(j)

 |∇τ |∞(cid:107)x(cid:107)

≤

C



δ1(j)

 (cid:107)B(cid:107) +



(cid:89)

j≤K−1

(cid:88)

(cid:89)

δ1(j(cid:48))δ2(j)

j<K−1

j(cid:48)≤j






 |∇τ |∞(cid:107)x(cid:107) . (cid:3) .

(15)
(16)

(17)

B.4. Proof of (d)

The proof is also analogous to the proof of (c), with the difference that now the Dirac operator is no longer invariant to

orthogonal transformations, only to translations. Given two points p, q, we verify that

(cid:107)p − q − τ (p) − τ (q)(cid:107) ≤ (cid:102)|τ |∞(cid:107)p − q(cid:107) ,

(cid:107)D − τ (D)(cid:107) = o((cid:102)|τ |∞) .

which, following the previous argument, leads to

C. Theorem 4.2

C.1. Proof of part (a)

The proof is based on the following lemma:

Lemma C.1 Let xN , yN ∈ H(MN ) such that ∀ N , (cid:107)xN (cid:107)H ≤ c,(cid:107)yN (cid:107)H ≤ c. Let ˆxN = EN (xN ), where EN is the
eigendecomposition of the Laplacian operator ∆N on MN , , with associated eigenvalues λ1 . . . λN in increasing order. Let
γ > 0 and β be deﬁned as in (5) for xN and yN . If β > 1 and (cid:107)xN − yN (cid:107) ≤ (cid:15) for all N ,

where C is a constant independent of (cid:15) and N .

(cid:107)∆N (xN − yN )(cid:107)2 ≤ C(cid:15)2− 1

β−1/2 ,

One layer of the network will transform the difference x1 − x2 into ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2). We verify that

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + (cid:107)B(cid:107)(cid:107)∆(x1 − x2)(cid:107) .

We now apply Lemma C.1 to obtain

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + C(cid:107)B(cid:107)(cid:107)x1 − x2(cid:107)

β−1
β−1/2

≤ (cid:107)x1 − x2(cid:107)

β−1
β−1/2

(cid:16)

(cid:107)A(cid:107)(cid:107)x1 − x2(cid:107)(2β−1)−1

+ C(cid:107)B(cid:107)

(cid:17)

≤ C((cid:107)A(cid:107) + (cid:107)B(cid:107))(cid:107)x1 − x2(cid:107)

β−1
β−1/2 ,

where we redeﬁne C to account for the fact that (cid:107)x1 − x2(cid:107)(2β−1)−1

is bounded. We have just showed that

with fr = C((cid:107)Ar(cid:107) + (cid:107)Br(cid:107)) and gr = βr−1

βr−1/2 . By cascading (20) for each of the R layers we thus obtain

(cid:107)x(r+1)
1

− x(r+1)
2

(cid:107) ≤ fr(cid:107)x(r)

1 − x(r)

2 (cid:107)gr

(cid:107)Φ∆(x1) − Φ∆(x2)(cid:107) ≤

(cid:35)

(cid:81)
f
r

r(cid:48)>r gr(cid:48)

(cid:107)x1 − x2(cid:107)

(cid:81)R

r=1 gr ,

(cid:34) R
(cid:89)

r=1

which proves (6) (cid:3).

Proof of (19): Let {e1, . . . , eN } be the eigendecomposition of ∆N . For simplicity, we drop the subindex N in the signals
from now on. Let ˆx(k) = (cid:104)x, ek(cid:105) and ˜x(k) = λk ˆx(k); and analogously for y. From the Parseval identity we have that
(cid:107)x(cid:107)2 = (cid:107)ˆx(cid:107)2. We express (cid:107)∆(x − y)(cid:107) as

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 .
λ2

(cid:88)

k≤N

The basic principle of the proof is to cut the spectral sum (22) in two parts, chosen to exploit the decay of ˜x(k). Let

F (x)(k) =

(cid:80)

k(cid:48)≥k ˜x(k)2
(cid:107)x(cid:107)2
H

=

(cid:80)

k(cid:48)≥k ˜x(k)2
k(cid:48) ˜x(k)2 =
(cid:80)

(cid:80)

k(cid:48)≥k λ2
(cid:80)
k(cid:48) λ2

k ˆx(k)2
k ˆx(k)2 ≤ 1 ,

(18)

(19)

(20)

(21)

(22)

and analogously for y. For any cutoff k∗ ≤ N we have

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 +
λ2

k(ˆx(k) − ˆy(k))2
λ2

(cid:88)

(cid:88)

k≤k∗

≤ λ2
k∗
≤ λ2
k∗
≤ λ2
k∗
where we denote for simplicity F (k∗) = max(F (x)(k∗), F (y)(k∗)). By assumption, we have λ2
k
F (k) (cid:46) (cid:88)

(cid:15)2 + 2(F (x)(k∗)(cid:107)x(cid:107)2
(cid:15)2 + 2F (k∗)((cid:107)x(cid:107)2
(cid:15)2 + 4F (k∗)D2 ,

k2(γ−β) (cid:39) k1+2(γ−β) .

H + (cid:107)y(cid:107)2

H)

H)

k>k∗
H + F (y)(k∗)(cid:107)y(cid:107)2

(cid:46) k2γ and

By denoting ˜β = β − γ − 1/2, it follows that

k(cid:48)≥k

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2k2γ

∗ + 4D2k−2 ˜β

∗

(cid:15)22γk2γ−1 − 2 ˜β4D2k−2 ˜β−1 = 0, thus
(cid:20) 4βD2
γ(cid:15)2

k∗ =

(cid:21) 1

2γ+2 ˜β

.

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2− 1

γ+ ˜β = (cid:15)2− 1

β−1/2 ,

Optimizing for k∗ yields

which proves part (a) (cid:3).

C.2. Proof of part (b)

We will use the following lemma:

By plugging (25) back into (24) and dropping all constants independent of N and (cid:15), this leads to

Lemma C.2 Let M = (V, E, F ) is a non-degenerate mesh, and deﬁne

η1(M) = sup

, η2(M) = sup

, η3(M) = αmin .

¯Ai
¯Aj

(i,j)∈E

ij + (cid:96)2
(cid:96)2

jk + (cid:96)2
ik

(i,j,k)∈F

A(i, j, k)

Then, given a smooth deformation τ and x deﬁned in M, we have

(cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) ,

where C depends only upon η1, η2 and η3.

In that case, we need to control the difference ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x). We verify that

(cid:107)ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x)(cid:107) ≤ (cid:107)B(cid:107)(cid:107)(∆ − τ (∆))x(cid:107) .

By Lemma C.2 it follows that (cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) and therefore, by denoting x(1)
x(1)
2 = ρ(Ax + Bτ (∆)x), we have

1 = ρ(Ax + B∆x) and

2 (cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) = C|∇τ |∞(cid:107)x(cid:107)H .

(28)

1 − x(1)
By applying again Lemma C.1, we also have that

(cid:107)x(1)

(cid:107)∆x(1)

1 − τ (∆)x(1)

2 (cid:107) = (cid:107)∆x(1)
= (cid:107)∆(x(1)
≤ C(cid:107)x(1)

1 − (∆ + τ (∆) − ∆)x(1)
2 (cid:107)
2 ) + (τ (∆) − ∆)x(1)
1 − x(1)
2 (cid:107)
1 − x(1)
β1−1/2 + |∇τ |∞(cid:107)x(1)
2 (cid:107)
β1−1
β1−1/2 ,

β1−1

(cid:46) C|∇τ |∞

2 (cid:107)H

(23)

(24)

(25)

(26)

(27)

which, by combining it with (28) and repeating through the R layers yields

(cid:107)Φ∆(x, M) − Φ∆(x, τ (M)(cid:107) ≤ C|∇τ |∞

(cid:81)R

r=1

βr −1
βr −1/2 ,

(29)

which concludes the proof (cid:3).

Proof of (27): The proof follows closely the proof of Theorem 4.1, part (c). From (13) and (14) we have that

τ ( ¯A) = ¯A(I + Gτ ) , with |Gτ |∞ ≤ C(η2, η3)|∇τ |∞ , and
τ (U − W ) = (I + Hτ )(U − W ) , with |Hτ |∞ ≤ C(η2, η3)|∇τ |∞ .

It follows that, up to second order o(|∇τ |∞

2) terms,

τ (∆) − ∆ = τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )

= (cid:0) ¯A[1 + Gτ ](cid:1)−1
(cid:39) ¯A−1Hτ (U − W ) + Gτ ∆ .

[(I + Hτ )(U − W )] − ¯A−1(U − W )

(30)

By writing ¯A−1Hτ = (cid:102)Hτ ¯A−1, and since ¯A is diagonal, we verify that

( (cid:102)Hτ )i,j = (Hτ )i,j

, with

Ai,i
Aj,j

Ai,i
Aj,j

≤ η1, and hence that

¯A−1Hτ (U − W ) = (cid:102)Hτ ∆ , with | (cid:102)Hτ |∞ ≤ C(η1, η2, η3)|∇τ |∞ .

(31)

We conclude by combining (30) and (31) into

(cid:107)(∆ − τ (∆))x(cid:107) = (cid:107)(Gτ + (cid:102)Hτ )∆x(cid:107)

≤ C (cid:48)(η1, η2, η3)|∇τ |∞(cid:107)∆x(cid:107) ,

which proves (27) (cid:3)

C.3. Proof of part (c)

This result is a consequence of the consistency of the cotangent Laplacian to the Laplace-Beltrami operator on S [45]:

Theorem C.3 ([45], Thm 3.4) Let M be a compact polyhedral surface which is a normal graph over a smooth surface S
with distortion tensor T , and let ¯T = (det T )1/2T −1. If the normal ﬁeld uniform distance d(T , 1) = (cid:107) ¯T − 1(cid:107)∞ satisﬁes
d(T , 1) ≤ (cid:15), then

(cid:107)∆M − ∆S(cid:107) ≤ (cid:15) .

(32)

If ∆M converges uniformly to ∆S, in particular we verify that

(cid:107)x(cid:107)H(M) → (cid:107)x(cid:107)H(S) .

Thus, given two meshes M, M(cid:48) approximating a smooth surface S in terms of uniform normal distance, and the corre-

sponding irregular sampling x and x(cid:48) of an underlying function ¯x : S → R, we have

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x − x(cid:48)(cid:107) + (cid:107)B(cid:107)(cid:107)∆Mx − ∆M(cid:48)x(cid:48)(cid:107) .

(33)

Since M and M(cid:48) both converge uniformly normally to S and ¯x is Lipschitz on S, it results that

thus (cid:107)x − x(cid:48)(cid:107) ≤ 2L(cid:15). Also, thanks to the uniform normal convergence, we also have convergence in the Sobolev sense:

(cid:107)x − ¯x(cid:107) ≤ L(cid:15) , and (cid:107)x(cid:48) − ¯x(cid:107) ≤ L(cid:15) ,

(cid:107)x − ¯x(cid:107)H (cid:46) (cid:15) , (cid:107)x(cid:48) − ¯x(cid:107)H (cid:46) (cid:15) ,

which implies in particular that

From (33) and (34) it follows that

(cid:107)x − x(cid:48)(cid:107)H (cid:46) (cid:15) .

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ 2(cid:107)A(cid:107)L(cid:15) +

(34)

(35)

+(cid:107)B(cid:107)(cid:107)∆Mx − ∆S ¯x + ∆S ¯x − ∆M(cid:48)x(cid:48)(cid:107)

≤ 2(cid:15) ((cid:107)A(cid:107)L + (cid:107)B(cid:107)) .

By applying again Lemma C.1 to ˜x = ρ(Ax + B∆Mx), ˜x(cid:48) = ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48)), we have

We conclude by retracing the same argument as before, reapplying Lemma C.1 at each layer to obtain

(cid:107)˜x − ˜x(cid:48)(cid:107)H ≤ C(cid:107)˜x − ˜x(cid:48)(cid:107)

β1−1
β1−1/2 (cid:46) (cid:15)

β1−1
β1−1/2 .

(cid:107)ΦM(x) − ΦM(cid:48)(x(cid:48))(cid:107) ≤ C(cid:15)

(cid:81)R

r=1

βr −1
βr −1/2 . (cid:3) .

D. Proof of Corollary 4.3

We verify that

(cid:107)ρ(B∆x) − ρ(Bτ (∆)τ (x))(cid:107) ≤ (cid:107)B(cid:107)(cid:107)∆x − τ (∆)τ (x)(cid:107)

≤ (cid:107)B(cid:107)(cid:107)∆(x − τ (x)) + (∆ − τ (∆))(τ (x))(cid:107)
≤ (cid:107)B(cid:107)((cid:107)∆(x − τ (x))(cid:107) + (cid:107)(∆ − τ (∆))(τ (x))(cid:107) .

The second term is o(|∇τ |∞) from Lemma C.2. The ﬁrst term is

(cid:107)x − τ (x)(cid:107)H ≤ (cid:107)∆(I − τ )(cid:107)(cid:107)x(cid:107) ≤ (cid:107)∇2τ (cid:107)(cid:107)x(cid:107) ,

where (cid:107)∇2τ (cid:107) is the uniform Hessian norm of τ . The result follows from applying the cascading argument from last section.
(cid:3)

E. Preliminary Study: Metric Learning for Dense Correspondence

As an interesting extension, we apply the architecture we built in Experiments 6.2 directly to a dense shape correspondence

problem.

Similarly as the graph correspondence model from [33], we consider a Siamese Surface Network, consisting of two
identical models with the same architecture and sharing parameters. For a pair of input surfaces M1, M2 of N1, N2 points
respectively, the network produces embeddings E1 ∈ RN1×d and E2 ∈ RN2×d. These embeddings deﬁne a trainable
similarity between points given by

si,j =

e(cid:104)E1,i,E2,j (cid:105)
j(cid:48) e(cid:104)E1,i,E2,j(cid:48) (cid:105)

,

(cid:80)

(36)

which can be trained by minimizing the cross-entropy relative to ground truth pairs. A diagram of the architecture is

provided in Figure 10.

In general, dense shape correspondence is a task that requires a blend of intrinsic and extrinsic information, motivating
the use of data-driven models that can obtain such tradeoffs automatically. Following the setup in Experiment 6.2, we use
models with 15 ResNet-v2 blocks with 128 output features each, and alternate Laplace and Dirac based models with Average
Pooling blocks to cover a larger context: The input to our network consists of vertex positions only.

We tested our architecture on a reconstructed (i.e. changing the mesh connectivity) version of the real scan of FAUST
dataset[3]. The FAUST dataset contains 100 real scans and their corresponding ground truth registrations. The ground truth
is based on a deformable template mesh with the same ordering and connectivity, which is ﬁtted to the scans. In order to
eliminate the bias of using the same template connectivity, as well as the need of a single connected component, the scans
are reconstructed again with [19]. To foster replicability, we release the processed dataset in the additional material. In our
experiment, we use 80 models for training and 20 models for testing.

Figure 10. Siamese network pipeline: the two networks take vertex coordinates of the input models and generate a high dimensional feature
vector, which are then used to deﬁne a map from M1 to M2. Here, the map is visualized by taking a color map on M2, and transferring
it on M1

Figure 11. Additional results from our setup. Plot in the middle shows rate of correct correspondence with respect to geodesic error [23].
We observe that Laplace is performing similarly to Dirac in this scenario. We believe that the reason is that the FAUST dataset contains
only isometric deformations, and thus the two operators have access to the same information. We also provide visual comparison, with the
transfer of a higher frequency colormap from the reference shape to another pose.

Since the ground truth correspondence is implied only through the common template mesh, we compute the correspon-
dence between our meshes with a nearest neighbor search between the point cloud and the reconstructed mesh. Consequently,

Figure 12. Heat map illustrating the point-wise geodesic difference between predicted correspondence point and the ground truth. The unit
is proportional to the geodesic diameter, and saturated at 10%.

Figure 13. A failure case of applying the Laplace network to a new pose in the FAUST benchmark dataset. The network confuses between
left and right arms. We show the correspondence visualization for front and back of this pair.

due to the drastic change in vertex replacement after the remeshing, only 60-70 percent of labeled matches are used. Although
making it more challenging, we believe this setup is close to a real case scenario, where acquisition noise and occlusions are
unavoidable.

Our preliminary results are reported in Figure 11. For simplicity, we generate predicted correspondences by simply taking
the mode of the softmax distribution for each reference node i: ˆj(i) = arg maxj si,j, thus avoiding a reﬁnement step that is
standard in other shape correspondence pipelines. The MLP model uses no context whatsoever and provides a baseline that
captures the prior information from input coordinates alone. Using contextual information (even extrinsically as in point-
cloud model) brings signiﬁcative improvments, but these results may be substantially improved by encoding further prior
knowledge. An example of the current failure of our model is depitcted in Figure 13, illustrating that our current architecture
does not have sufﬁciently large spatial context to disambiguate between locally similar (but globally inconsistent) parts.

We postulate that the FAUST dataset [3] is not an ideal ﬁt for our contribution for two reasons: (1) it is small (100

models), and (2) it contains only near-isometric deformations, which do not require the generality offered by our network. As
demonstrated in [28], the correspondence performances can be dramatically improved by constructing basis that are invariant
to the deformations. We look forward to the emergence of new geometric datasets, and we are currently developing a capture
setup that will allow us to acquire a more challenging dataset for this task.

F. Further Numerical Experiments

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 14. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 15. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 16. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 17. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

Laplace

Dirac

Figure 18. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Figure 19. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

Figure 20. From left to right: set-to-set, ground truth and Dirac based model. Color corresponds to mean squared error between ground
truth and prediction: green - smaller error, red - larger error.

8
1
0
2
 
n
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
1
8
0
1
.
5
0
7
1
:
v
i
X
r
a

Surface Networks

Ilya Kostrikov1, Zhongshi Jiang1, Daniele Panozzo ∗1, Denis Zorin †1, and Joan Bruna‡1,2

1Courant Institute of Mathematical Sciences, New York University
2Center for Data Science, New York University

Abstract

We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used
to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs,
namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the
Laplacian operator.

Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric
deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to
GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In
particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions — this is in stark
contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface
Networks (SN).

We prove that these models deﬁne shape representations that are stable to deformation and to discretization, and we
demonstrate the efﬁciency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under
non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by
SNs.

1. Introduction

3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering
to computer animation to medical imaging. Despite the vast amount of high-quality 3D geometric data available, data-
driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data
representation regularity which is required for traditional convolutional neural network approaches. While in computer vision
problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more
complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar
chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical
properties.

Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike
the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the
inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary
features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations.
Extrinsic representations rely on the speciﬁc embedding of surfaces within a three-dimensional ambient space, whereas
intrinsic representations only capture geometric properties speciﬁc to the surface, irrespective of its parametrization. Whereas
the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local
deformations and invariance to global transformations.

∗DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.
†DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.
‡JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding

author: bruna@cims.nyu.edu

1

A particularly simple and popular extrinsic method [36, 37] represents shapes as point clouds in R3 of variable size,
and leverages recent deep learning models that operate on input sets [44, 43]. Despite its advantages in terms of ease
of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classiﬁcation
and segmentation tasks, one may wonder whether this simpliﬁcation comes at a loss of precision as one considers more
challenging prediction tasks.

In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on
geometric deep learning. These models provide data-driven intrinsic graph and manifold representations with inductive
biases analogous to CNNs on natural images. Models based on Graph Neural Networks [40] and their spectral variants
[6, 11, 26] have been successfully applied to geometry processing tasks such as shape correspondence [32]. In their basic
form, these models learn a deep representation over the discretized surface by combining a latent representation at a given
node with a local linear combination of its neighbors’ latent representations, and a point-wise nonlinearity. Different models
vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to
spectral interpretations of those models.

Our contributions are three-fold. First, we extend the model to support extrinsic features. More speciﬁcally, we exploit the
fact that surfaces in R3 admit a ﬁrst-order differential operator, the Dirac operator, that is stable to discretization, provides a
direct generalization of Laplacian-based propagation models, and is able to detect principal curvature directions [9, 18]. Next,
we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization,
two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the
variational autoencoder framework [25, 39], that is able to exploit non-Euclidean geometric regularity.

By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation
that we apply to several challenging tasks. The resulting Surface Networks – using either the Dirac or the Laplacian, inherit
the stability and invariance properties of these operators, thus providing data-driven representations with certiﬁed stability to
deformations. We demonstrate the model efﬁciency on a temporal prediction task of complex dynamics, based on a physical
simulation of elastic shells, which conﬁrms that whenever geometric information (in the form of a mesh) is available, it can
be leveraged to signiﬁcantly outperform point-cloud based models.

Our main contributions are summarized as follows:

• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear

dynamics, motivating the use of geometric shape information.

• We prove that Surface Networks deﬁne shape representations that are stable to deformation and to discretization.

• We introduce a generative model for 3D surfaces based on the variational autoencoder.

2. Related Work

Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research.
[40] introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained
by backpropagation. Subsequent works [27, 43] have relaxed the model by untying the recurrent layer weights and proposed
several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convo-
lutional networks to non-Euclidean graphs. [6, 17] proposed to learn smooth spectral multipliers of the graph Laplacian,
albeit with high computational cost, and [11, 26] resolved the computational bottleneck by learning polynomials of the graph
Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to
[5] for an exhaustive literature review on the topic. GNNs are ﬁnding application in many different domains. [2, 7] develop
graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics.
[12, 22] study molecular ﬁngerprints using variants of the GNN architecture, and [14] further develop the model by combin-
ing it with set representations [44], showing state-of-the-art results on molecular prediction. The resulting models, so-called
Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model
on general graphs.

In the context of computer graphics, [30] developed the ﬁrst CNN model on meshed surfaces using intrinsic patch rep-
resentations, and further generalized in [4] and [32]. This last work allows for ﬂexible representations via the so-called
pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode ﬁrst-
order differential information. These intrinsic models contrast with Euclidean models such as [48, 46], that have higher
sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models

A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/SurfaceNetworks.

are increasingly popular to model 3d objects due to their simplicity and versatility. [36, 37] use set-invariant representations
from [44, 43] to solve shape segmentation and classiﬁcation tasks. More recently, [29] proposes to learn surface convolutional
network from a canonical representation of planar ﬂat-torus, with excellent performance on shape segmentation and classi-
ﬁcation, although such canonical representations may introduce exponential scale changes that can introduce instabilities.
Finally, [13] proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but
does not encode geometrical information as our shape generative model. Learning variational deformations is an important
problem for graphics applications, since it enables negligible and ﬁxed per-frame cost [35], but it is currently limited to 2D
deformations using point handles. In constrast, our method easily generalizes to 3D and learns dynamic behaviours.

3. Surface Networks

This section presents our surface neural network model and its basic properties. We start by introducing the problem setup
and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section
3.2).

3.1. Laplacian Surface Networks

Our ﬁrst goal is to deﬁne a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where
V = (vi ∈ R3)i≤N contains the node coordinates, E = (ei,j) corresponds to edges, and F is the set of triangular faces. We
denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see [5] for details).
This operator can be interpreted as a local, linear high-pass ﬁlter in M that acts on signals x ∈ Rd×|V | deﬁned on the
vertices as a simple matrix multiplication ˜x = ∆x. By combining ∆ with an all-pass ﬁlter and learning generic linear
combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators
in M that update a feature map from layer k to layer k + 1 using trainable parameters Ak and Bk:

xk+1 = ρ (cid:0)Ak∆xk + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk .

By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization,
this model is a speciﬁc instance of the graph neural network [40, 5, 26] and a generalization of the spectrum-free Laplacian
networks from [11]. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening
or pooling layers.

In contrast to general graphs, meshes contain a low-dimensional Euclidean embedding that contains potentially useful
information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A
simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as
input channels to the network: x1 := V ∈ R|V |×3. The mean curvature can be computed by applying the Laplace operator
to the coordinates of the vertices:

∆x1 = −2Hn ,

where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian
neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph
neural network models is related to the use of generalized coordinates from [32]. By cascading K layers of the form (1) we
obtain a representation Φ∆(M) that contains generic features at each node location. When the number of layers K is of
the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate
information across the whole surface.

Equation (2) illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to
the mean variations across all directions. Although in general graphs there is no well-deﬁned procedure to recover anisotropic
local variations, in the case of surfaces some authors ([4, 1, 32]) have considered anisotropic extensions. We describe next
a particularly simple procedure to increase the expressive power of the network using a related operator from quantum
mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation [9] and shape
analysis [18].

(1)

(2)

3.2. Dirac Surface Networks

The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the
gradient (a ﬁrst-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to
these ﬁrst-order differential operators separately, enabling oriented high-pass ﬁlters.

For convenience, we embed R3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for
details). The Dirac operator is then deﬁned as a matrix D ∈ H|F |×|V | that maps (quaternion) signals on the nodes to signals
on the faces. In coordinates,

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area (see Appendix A) using counter-clockwise
orientations on all faces.

To apply the Dirac operator deﬁned in quaternions to signals in vertices and faces deﬁned in real numbers, we write the
feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a
quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The
Dirac operator provides ﬁrst-order differential information and is sensitive to local orientations. Moreover, one can verify [9]
that

Re D∗D = ∆ ,
where D∗ is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as
D∗ = M −1
V DH MF where DH is a conjugate transpose of D and MV , MF are diagonal mass matrices with one third of
areas of triangles incident to a vertex and face areas respectively.

The Dirac operator can be used to deﬁne a new neural surface representation that alternates layers with signals deﬁned
over nodes with layers deﬁned over faces. Given a d-dimensional feature representation over the nodes xk ∈ Rd×|V |, and the
faces of the mesh, yk ∈ Rd×|F |, we deﬁne a d(cid:48)-dimensional mapping to a face representation as

yk+1 = ρ (cid:0)CkDxk + Ekyk(cid:1) , Ck, Ek ∈ Rdk+1×dk ,

(3)

where Ck, Ek are trainable parameters. Similarly, we deﬁne the adjoint layer that maps back to a ˜d-dimensional signal over
nodes as

xk+1 = ρ (cid:0)AkD∗yk+1 + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk ,
where Ak, Bk are trainable parameters. A surface neural network layer is thus determined by parameters {A, B, C, E} using
equations (3) and (4) to deﬁne xk+1 ∈ Rdk+1×|V |. We denote by ΦD(M) the mesh representation resulting from applying
K such layers (that we assume ﬁxed for the purpose of exposition).

(4)

The Dirac-based surface network is related to edge feature transforms proposed on general graphs in [14], although these
edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no
notion of square root of ∆ that recovers oriented ﬁrst-order derivatives.

4. Stability of Surface Networks

Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise
under the model. Given a continuous surface S ⊂ R3 or a discrete mesh M, and a smooth deformation ﬁeld τ : R3 → R3,
we are particularly interested in two forms of stability:

• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that (cid:107)Φ(M) −

Φ(τ (M))(cid:107) is small if (cid:107)∇τ (∇τ )∗ − I(cid:107) is small, i.e when the deformation is nearly rigid; see Theorem 4.1.

• Given two discretizations M1 and M2 of the same underlying surface S, we would like to control (cid:107)Φ(M1)−Φ(M2)(cid:107)

in terms of the resolution of the meshes; see Theorem 4.2.

These stability properties are important in applications, since most tasks we are interested in are stable to deformation and
to discretization. We shall see that the ﬁrst property is a simple consequence of the fact that the mesh Laplacian and Dirac
operators are themselves stable to deformations. The second property will require us to specify under which conditions the
discrete mesh Laplacian ∆M converges to the Laplace-Beltrami operator ∆S on S. Unless it is clear from the context, in the
following ∆ will denote the discrete Laplacian.

Theorem 4.1 Let M be a N -node mesh and x, x(cid:48) ∈ R|V |×d be input signals deﬁned on the nodes. Assume the nonlinearity
ρ( · ) is non-expansive (|ρ(z) − ρ(z(cid:48))| ≤ |z − z(cid:48)|). Then

(a) (cid:107)Φ∆(M; x) − Φ∆(M; x(cid:48))(cid:107) ≤ α∆(cid:107)x − x(cid:48)(cid:107) , where α∆ depends only on the trained weights and the mesh.

(b) (cid:107)ΦD(M; x) − ΦD(M; x(cid:48))(cid:107) ≤ αD(cid:107)x − x(cid:48)(cid:107) , where αD depends only on the trained weights and the mesh.

(c) Let |∇τ |∞ := supu (cid:107)∇τ (u)(∇τ (u))∗ − 1(cid:107), where ∇τ (u) is the Jacobian matrix of u (cid:55)→ τ (u). Then (cid:107)Φ∆(M; x) −

Φ∆(τ (M); x)(cid:107) ≤ β∆|∇τ |∞(cid:107)x(cid:107) , where β∆ is independent of τ and x.

(d) Denote by (cid:103)|∇τ |∞ := supu (cid:107)∇τ (u) − 1(cid:107). Then (cid:107)ΦD(M; x) − ΦD(τ (M); x)(cid:107) ≤ βD (cid:103)|∇τ |∞(cid:107)x(cid:107) , where βD is

independent of τ and x.

Properties (a) and (b) are not speciﬁc to surface representations, and are a simple consequence of the non-expansive property
of our chosen nonlinearities. The constant α is controlled by the product of (cid:96)2 norms of the network weights at each layer and
the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators
are themselves stable to deformations, a property that depends on two key aspects: ﬁrst, the Laplacian/Dirac is localized in
space, and next, that it is a high-pass ﬁlter and therefore only depends on relative changes in position.

One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the
reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral
radius of ∆M diverges as the mesh size N increases.

In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incom-
ing to the surface network. This can be measured with Sobolev norms deﬁned using the spectrum of the Laplacian operator.
Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆M, consider
the spectral decomposition of ∆M (a symmetric, positive deﬁnite operator):

∆M =

λkekeT

k , ek ∈ RN , 0 ≤ λ1 ≤ λ2 · · · ≤ λN .

(cid:88)

k≤N

Under normal uniform convergence 1 [45], the spectrum of ∆M converges to the spectrum of the Laplace-Beltrami operator
∆S of S. If S is bounded, it is known from the Weyl law [47] that there exists γ > 0 such that k−γ(S) (cid:46) λ−1
k , so the
eigenvalues λk do not grow too fast. The smoothness of a signal x ∈ R|V |×d deﬁned in M is captured by how fast its
H := (cid:80)
spectral decomposition ˆx(k) = eT
k λ(k)2(cid:107)ˆx(k)(cid:107)2 is Sobolev norm, and
β(x, S) > 1 as the largest rate such that its spectral decomposition coefﬁcients satisfy

k x ∈ Rd decays [42]. We deﬁne (cid:107)x(cid:107)2

(cid:107)ˆx(k)(cid:107) (cid:46) k−β , (k → ∞) .
(5)
If x ∈ R|V |×d is the input to the Laplace Surface Network of R layers, we denote by (β0, β1, . . . , βR−1) the smoothness
rates of the feature maps x(r) deﬁned at each layer r ≤ R.

Theorem 4.2 Consider a surface S and a ﬁnite-mesh approximation MN of N points, and Φ∆ a Laplace Surface Network
with parameters {(Ar, Br)}r≤R. Denote by d(S, MN ) the uniform normal distance, and let x1, x2 be piece-wise polyhedral
approximations of ¯x(t), t ∈ S in MN , with (cid:107)¯x(cid:107)H(S) < ∞. Assume (cid:107)¯x(r)(cid:107)H(S) < ∞ for r ≤ R.

(a) If x1, x2 are two functions such that the R feature maps x(r)

have rates (β0, β1, . . . , βR−1), then

l
(cid:107)Φ∆(x1; MN ) − Φ∆(x2; MN )(cid:107)2 ≤ C(β)(cid:107)x1 − x2(cid:107)h(β) ,

(6)

with h(β) = (cid:81)R

βr−1
βr−1/2 , and where C(β) does not depend upon N .

r=1

(b) If τ is a smooth deformation ﬁeld, then (cid:107)Φ∆(x; MN ) − Φ∆(x; τ (MN ))(cid:107) ≤ C|∇τ |∞

h(β) , where C does not depend

upon N .

(c) Let M and M(cid:48) be N -point discretizations of S, If max(d(M, S), d(M(cid:48), S)) ≤ (cid:15), then (cid:107)Φ∆(M; x) − Φ∆(M(cid:48), x(cid:48))(cid:107) ≤

C(cid:15)h(β) , where C is independent of N .

This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases,
the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac
operator also has a well-deﬁned continuous counterpart [9] that generalizes the gradient operator in quaternion space. Also,
our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in
terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x).
These extensions are left for future work. Finally, a speciﬁc setup that we use in experiments is to use as input signal the
canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields

Corollary 4.3 Denote Φ(M) := ΦM(V ), where V are the node coordinates of M. Then, if A1 = 0,

(cid:107)Φ(M) − Φ(τ (M))(cid:107) ≤ κ max(|∇τ |∞, (cid:107)∇2τ (cid:107))h(β) .

(7)

1which controls how the normals of the mesh align with the surface normals; see [45].

Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R3 (right) is expressed in terms of a “sampling” 2D irregular mesh
˜M ⊂ R2 (left) and a depth scalar ﬁeld f : ˜M → R over ˜M (center).

5. Generative Surface Models

State-of-the-art generative models for images, such as generative adversarial networks [38], pixel autoregressive networks
[34], or variational autoencoders [25], exploit the locality and stationarity of natural images in their probabilistic models, in
the sense that the model satisﬁes pθ(x) ≈ pθ(xτ ) by construction, where xτ is a small deformation of a given input x. This
property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric
stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains
two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface,
which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas
the former contains the essential semantic information, the latter is not informative, and to some extent independent of the
shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as
height-ﬁeld meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as ( ˜M, f ( ˜M)), where ˜M = ( ˜V , ˜E, ˜F ) is
now a 2D mesh and f : ˜V → R is a depth-map encoding the original node locations V , as shown in Figure 1.

In this work, we consider the variational autoencoder framework [25, 39].

It considers a mixture model of the form
p(M) = (cid:82) pθ(M | h)p0(h)dh , where h ∈ R|S| is a vector of latent variables. We train this model by optimizing the
variational lower bound of the data log-likelihood:

min
θ,ψ

1
L

(cid:88)

l≤L

−Eh∼qψ (h | Ml) log pθ(Ml | h) + DKL(qψ(h | Ml) || p0(h)) .

(8)

We thus need to specify a conditional generative model pθ(M | h), a prior distribution p0(h) and a variational approximation
to the posterior qψ(h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the
height-ﬁeld representation, we choose for simplicity a separable model of the form pθ(M | h) = pθ(f | h, ˜M) · p( ˜M) ,
where ˜M ∼ p( ˜M) is a homogeneous Poisson point process, and f ∼ pθ(f | h, ˜M) is a normal distribution with mean and
isotropic covariance parameters given by a SN:

pθ(f | h, ˜M) = N (µ(h, ˜M), σ2(h, ˜M)1) ,

with [µ(h, ˜M), σ2(h, ˜M)] = ΦD( ˜M ; h) . The generation step thus proceeds as follows. We ﬁrst sample a 2D mesh ˜M
independent of the latent variable h, and then sample a depth ﬁeld over ˜M conditioned on h from the output of a decoder
network ΦD( ˜M ; h). Finally, the variational family qψ is also a Normal distribution whose parameters are obtained from an
encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization: qψ(h | M) =
N (¯µ, ¯σ21) , with [¯µ, ¯σ] = ¯ΦD(M) .

6. Experiments

6.1. MeshMNIST

For experimental evaluation, we compare models built using ResNet-v2 blocks [16], where convolutions are replaced with
the appropriate operators (see Fig. 2): (i) a point cloud based model from [43] that aggregates global information by averaging
features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical
coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant
of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).

For this task, we construct a MeshMNIST database with only height-ﬁeld meshes (Sec. 5). First, we sample points on
a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a
Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each

Figure 2. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to
the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch
Normalization (blue) and ‘1 × 1’ convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language
and denote by xk+1 the output of this 2-layer block.

Receptive ﬁeld Number of parameters

Model
MLP
PointCloud
Laplace
Dirac

1
-
16
8

519672
1018872
1018872
1018872

Smooth L1-loss (mean per sequence (std))
64.56 (0.62)
23.64 (0.21)
17.34 (0.52)
16.84 (0.16)

Table 1. Evaluation of different models on the temporal task

point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to deﬁne a sampling process
over 3D height-ﬁeld meshes.

We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder
converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to
obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a speciﬁc 3D
mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does
not depend on input data. We trained the model for 75 epochs using Adam optimizer [24] with learning rate 10−3, weight
decay 10−5 and batch size 32. Figures 3,4 illustrate samples from the model. The geometric encoder is able to leverage the
local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to
the speciﬁc sampled grid, as opposed to set-based generative models.

Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of dis-
cretization/triangulation (Poisson disk sampling with p=1.5).

Figure 4. Meshes from the dataset (ﬁrst ﬁve). And meshes generated by our model (last ﬁve).

6.2. Spatio-Temporal Predictions

One speciﬁc task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1, X 2, . . . , X n,

the task is to predict the following frames Y = Y 1 = X n+1, Y 2, . . . , Y m = X n+m. As in [31], we use a simple non-
recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on
n = 2 frames and predict the next m = 40 frames. In order to generate data, we ﬁrst extracted 10k patches from the
MPI-Faust dataset[3], by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of
the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground.
We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique [41], with additional
gravitational forces [20]. Libigl [21] has been used for the mesh processing tasks. Sequences with patches from the ﬁrst 80
subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on
request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial
frames. Thus, we can train models by minimizing smooth-L1 loss [15] between target frames and output of our models.

Ground Truth

MLP

PointCloud

Laplace

Dirac

Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set.
Boxes indicate distinctive features. For larger crops, see Figure 6

.

We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and
Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned
frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce
120-dimensional outputs. We trained all models using the Adam optimizer [24] with learning rate 10−3, weight decay 10−5,
and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained
for 110k steps in overall.

Ground Truth

Laplace

Dirac

Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Table 1 reports quantitative prediction performance of different models, and Figure 5 displays samples from the prediction

Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between
ground truth and prediction: green - smaller error, red - larger error.

Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

models at speciﬁc frames. The set-to-set model [44, 43], corresponding to a point-cloud representation used also in [36],
already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between
this model and Laplace-/Dirac-based models is signiﬁcant, both visually and quantitatively. Dirac-based model outperforms
Laplace-based model despite the smaller receptive ﬁeld. Videos comparing the performance of different models are available
in the additional material.

Figure 6 illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since
they propagate information using an isotropic operator, have more difﬁculties at resolving corners and pointy structures than
the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit
the extrinsic information only via the input coordinates is remarkable and more computationally efﬁcient than the Dirac
counterpart. Figures 7 and 8 overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac
respectively. They conﬁrm ﬁrst that SNs outperform the point-cloud based model, which often produce excessive ﬂattening
and large deformations, and next that ﬁrst-order Dirac operators help resolve areas with high directional curvature. We refer
to the supplementary material for additional qualitative results.

7. Conclusions

We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the non-Euclidean
geometry of surfaces. We have shown how a ﬁrst-order differential operator (the Dirac operator) can detect and adapt to
geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction
is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments.
That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting
to instead learn the operator, akin to recent Message-Passing NNs [14] and explore whether Dirac is recovered.

Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly
superior performance to point-cloud based models. While there are not many such datasets currently available, we expect
them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated
in consumer devices. SNs provide efﬁcient inference, with predictable runtime, which makes them appealing across many
areas of computer graphics, where a ﬁxed, per-frame cost is required to ensure a stable framerate, especially in VR applica-
tions. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and
generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.

References

2014. 3

[1] M. Andreux, E. Rodol`a, M. Aubry, and D. Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In Proc. NORDIA,

[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In

Advances in Neural Information Processing Systems, pages 4502–4510, 2016. 2

[3] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 3794–3801, 2014. 7, 18, 20

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks.

In Advances in Neural Information Processing Systems, pages 3189–3197, 2016. 2, 3

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv

preprint arXiv:1611.08097, 2016. 2, 3, 14

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013. 2
[7] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics.

[8] D. Chen and J. R. Gilbert. Obtaining bounds on the two norm of a matrix from the splitting lemma. Electronic Transactions on

[9] K. Crane, U. Pinkall, and P. Schr¨oder. Spin transformations of discrete surfaces. In ACM Transactions on Graphics (TOG). ACM,

ICLR, 2016. 2

2011. 2, 3, 4, 5

Numerical Analysis, 21:28–46, 2005. 14

and Its Applications, 427(1):55–69, 2007. 12

[10] K. C. Das. Extremal graph characterization from the upper bound of the laplacian spectral radius of weighted graphs. Linear Algebra

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In

Advances in Neural Information Processing Systems, pages 3837–3845, 2016. 2, 3

[12] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convo-

lutional networks on graphs for learning molecular ﬁngerprints. In Neural Information Processing Systems, 2015. 2

[13] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. arXiv preprint

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. 7
[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision,

arXiv:1612.00603, 2016. 3

arXiv:1704.01212, 2017. 2, 4, 10

pages 630–645. Springer, 2016. 6

[17] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015. 2
[18] K. C. Hsueh-Ti Derek Liu, Alec Jacobson. A dirac operator for extrinsic shape analysis. Computer Graphics Forum, 2017. 2, 3
[19] Y. Hu, Q. Zhou, X. Gao, A. Jacobson, D. Zorin, and D. Panozzo. Tetrahedral meshing in the wild. Submitted to ACM Transaction on

Graphics, 2018. 18

[20] A. Jacobson. Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes. PhD thesis, ETH, Z¨urich, 2013. 7
[21] A. Jacobson, D. Panozzo, et al. libigl: A simple C++ geometry processing library, 2016. http://libigl.github.io/libigl/. 7
[22] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal

of computer-aided molecular design, 2016. 2

[23] V. G. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic maps. In ACM Transactions on Graphics (TOG), volume 30, page 79.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representation, 2015.

[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 6
[26] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907,

ACM, 2011. 19

7, 8

2016. 2, 3

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. 2
[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape

correspondence. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5660–5668, 2017. 21

[29] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. Kim, and Y. Lipman. Convolutional neural networks on surfaces

via seamless toric covers. In SIGGRAPH, 2017. 3

[30] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In

Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. 2

[31] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error.

arXiv preprint

arXiv:1511.05440, 2015. 7

[32] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds

using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. 2, 3

[33] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna. A note on learning algorithms for quadratic assignment with graph neural networks.

arXiv preprint arXiv:1706.07450, 2017. 18

[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. 6
[35] R. Poranne and Y. Lipman. Simple approximations of planar deformation operators. Technical report, ETHZ, 2015. 3
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. arXiv preprint

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint

arXiv:1612.00593, 2016. 2, 3, 9

arXiv:1706.02413, 2017. 2, 3

arXiv preprint arXiv:1511.06434, 2015. 6

Neural Networks, 20(1):61–80, 2009. 2, 3

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks.

[39] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint arXiv:1505.05770, 2015. 2, 6
[40] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on

[41] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, 2007. 7
[42] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE

[43] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation.

In Advances in Neural Information

Symposium on, pages 29–38. IEEE, 2007. 5

Processing Systems, pages 2244–2252, 2016. 2, 3, 6, 9

[44] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015. 2, 3, 9
[45] M. Wardetzky. Convergence of the cotangent formula: An overview. In Discrete Differential Geometry, pages 275–286. 2008. 5, 12,

17

[46] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolutional networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1544–1553, 2016. 2

[47] H. Weyl.

¨Uber die asymptotische verteilung der eigenwerte. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottingen,

Mathematisch-Physikalische Klasse, 1911:110–117, 1911. 5

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 2

A. The Dirac Operator

The quaternions H is an extension of complex numbers. A quaternion q ∈ H can be represented in a form q = a+bi+cj +

dk where a, b, c, d are real numbers and i, j, k are quaternion units that satisfy the relationship i2 = j2 = k2 = ijk = −1.

As mentioned in Section 3.1, the Dirac operator used in the model can be conveniently represented as a quaternion matrix:

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area, as illustrated in Fig. A, using counter-
clockwise orientations on all faces.

vj

f

ej







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c













b
a
−b
a
−c −d
−d

d
c
d −c
b
a
a
c −b







.

The Deep Learning library PyTorch that we used to implement the models does not support quaternions. Nevertheless,
quaternion-valued matrix multiplication can be replaced with real-valued matrix multiplication where each entry q = a +
bi + cj + dk is represented as a 4 × 4 block

and the conjugate q∗ = a − bi − cj − dk is a transpose of this real-valued matrix:

B. Theorem 4.1

B.1. Proof of (a)

Laplacian ∆ of M is

We ﬁrst show the result for the mapping x (cid:55)→ ρ (Ax + B∆x), corresponding to one layer of Φ∆. By deﬁnition, the

∆ = diag( ¯A)−1(U − W ) ,
where ¯Aj is one third of the total area of triangles incident to node j, and W = (wi,j) contains the cotangent weights [45],
and U = diag(W 1) contains the node aggregated weights in its diagonal.

From [10] we verify that

(cid:107)U − W (cid:107) ≤

√

2 max
i

(cid:115)






U 2

i + Ui

Ujwi,j

(cid:88)

i∼j






√

√

≤ 2

2 sup
i,j

wi,j sup

dj

j

≤ 2

2 cot(αmin)dmax ,

(cid:107)∆(cid:107) ≤ C

cot(αmin)Smax
inf j ¯Aj

:= LM ,

where dj denotes the degree (number of neighbors) of node j, αmin is the smallest angle in the triangulation of M and Smax
the largest number of incident triangles. It results that

which depends uniquely on the mesh M and is ﬁnite for non-degenerate meshes. Moreover, since ρ( · ) is non-expansive, we
have

(cid:107)ρ (Ax + B∆x) − ρ (Ax(cid:48) + B∆x(cid:48))(cid:107) ≤ (cid:107)A(x − x(cid:48)) + B∆(x − x(cid:48))(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)LM)(cid:107)x − x(cid:48)(cid:107) .

By cascading (10) across the K layers of the network, we obtain

(cid:107)Φ(M; x) − Φ(M; x(cid:48))(cid:107) ≤

((cid:107)Ak(cid:107) + (cid:107)Bk(cid:107)LM)


 (cid:107)x − x(cid:48)(cid:107) ,





(cid:89)

k≤K

(9)

(10)

which proves (a). (cid:3)

B.2. Proof of (b)

B.3. Proof of (c)

The proof is analogous, by observing that (cid:107)D(cid:107) = (cid:112)(cid:107)∆(cid:107) and therefore

(cid:107)D(cid:107) ≤

(cid:112)

LM . (cid:3)

To establish (c) we ﬁrst observe that given three points p, q, r ∈ R3 forming any of the triangles of M,

(cid:107)p − q(cid:107)2(1 − |∇τ |∞)2

≤ (cid:107)τ (p) − τ (q)(cid:107)2 ≤

(cid:107)p − q(cid:107)2(1 + |∇τ |∞)2

A(p, q, r)2(1 − |∇τ |∞Cα−2

min − o(|∇τ |∞

2) ≤ A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2(1 + |∇τ |∞Cα−2

min + o(|∇τ |∞

2)) .

(11)

(12)

Indeed, (11) is a direct consequence of the lower and upper Lipschitz constants of τ (u), which are bounded respectively by
1 − |∇τ |∞ and 1 + |∇τ |∞. As for (12), we use the Heron formula

A(p, q, r)2 = s(s − (cid:107)p − q(cid:107))(s − (cid:107)p − r(cid:107))(s − (cid:107)r − q(cid:107)) ,

with s = 1
determined by the deformed points τ (p), τ (q), τ (r), we have that

2 ((cid:107)p − q(cid:107) + (cid:107)p − r(cid:107) + (cid:107)r − q(cid:107)) being the half-perimeter. By denoting sτ the corresponding half-perimeter

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≤ s(1 + |∇τ |∞) − (cid:107)p − q(cid:107)(1 − |∇τ |∞) = s − (cid:107)p − q(cid:107) + |∇τ |∞(s + (cid:107)p − q(cid:107)) and

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≥ s(1 − |∇τ |∞) − (cid:107)p − q(cid:107)(1 + |∇τ |∞) = s − (cid:107)p − q(cid:107) − |∇τ |∞(s + (cid:107)p − q(cid:107)) ,

and similarly for the (cid:107)r − q(cid:107) and (cid:107)r − p(cid:107) terms. It results in

A(τ (p), τ (q), τ (r))2 ≥ A(p, q, r)2
≥ A(p, q, r)2 (cid:104)

(cid:20)
1 − |∇τ |∞

(cid:18)

1 − C|∇τ |∞α−2

1 +

+

s + (cid:107)p − q(cid:107)
s − (cid:107)p − q(cid:107)
(cid:105)
2)

min − o(|∇τ |∞

,

s + (cid:107)p − r(cid:107)
s − (cid:107)p − r(cid:107)

+

s + (cid:107)r − q(cid:107)
s − (cid:107)r − q(cid:107)

(cid:19)

− o(|∇τ |∞

(cid:21)
2)

and similarly

A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2 (cid:104)

1 + C|∇τ |∞α−2

min − o(|∇τ |∞

(cid:105)

2)

.

By noting that the cotangent Laplacian weights can be written (see Fig. 9) as

wi,j =

−(cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
A(i, j, k)

+

−(cid:96)2

jh + (cid:96)2
ih

ij + (cid:96)2
A(i, j, h)

,

we have from the previous Bilipschitz bounds that

τ (wi,j) ≤ wi,j

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

+ 2|∇τ |∞

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

τ (wi,j) ≥ wi,j

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

− 2|∇τ |∞

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

+

+

(cid:33)

(cid:33)

,

,

which proves that, up to second order terms, the cotangent weights are Lipschitz continuous to deformations.

Finally, since the mesh Laplacian operator is constructed as diag( ¯A)−1(U − W ), with ¯Ai,i = 1
3

(cid:80)

j,k;(i,j,k)∈F A(i, j, k),

and U = diag(W 1), let us show how to bound (cid:107)∆ − τ (∆)(cid:107) from

¯Ai,i(1 − αM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ ( ¯Ai,i) ≤ ¯Ai,i(1 + αM|∇τ |∞ + o(|∇τ |∞

2))

and

wi,j(1 − βM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ (wi,j) ≤ wi,j(1 + βM|∇τ |∞ + o(|∇τ |∞

2)) .

(13)

(14)

k

αij

aijk

j

(cid:96)ij

ai

i

βij

h

Figure 9. Triangular mesh and Cotangent Laplacian (ﬁgure reproduced from [5])

Using the fact that ¯A, τ ( ¯A) are diagonal, and using the spectral bound for k × m sparse matrices from [8], Lemma 5.12,

(cid:107)Y (cid:107)2 ≤ max

i

(cid:88)

j; Yi,j (cid:54)=0

|Yi,j|

|Yr,j|

,

(cid:33)

(cid:32) l

(cid:88)

r=1

the bounds (13) and (14) yield respectively

τ ( ¯A) = ¯A(1 + (cid:15)τ ) , with (cid:107)(cid:15)τ (cid:107) = o(|∇τ |∞) , and
τ (U − W ) = U − W + ητ , with (cid:107)ητ (cid:107) = o(|∇τ |∞) .

It results that, up to second order terms,
(cid:13)τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )(cid:13)
(cid:107)∆ − τ (∆)(cid:107) = (cid:13)
(cid:13)
(cid:13)
(cid:0) ¯A[1 + (cid:15)τ ](cid:1)−1
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
=
(cid:13)
= (cid:13)
(cid:13)(cid:15)τ ∆ + ¯A−1ητ
= o(|τ |∞) ,

(cid:13)
(cid:13) + o(|∇τ |∞

1 − (cid:15)τ + o(|∇τ |∞

2)

=

[U − W + ητ ] − ¯A−1(U − W )

(cid:13)
(cid:13)
(cid:13)

(cid:17) ¯A−1(U − W + ητ ) − ¯A−1(U − W )
2)

(cid:13)
(cid:13)
(cid:13)

which shows that the Laplacian is stable to deformations in operator norm. Finally, by denoting ˜xτ a layer of the deformed
Laplacian network

it follows that

Also,

˜xτ = ρ(Ax + Bτ (∆)x) ,

(cid:107)˜x − ˜xτ (cid:107) ≤ (cid:107)B(∆ − τ (∆)x(cid:107)
≤ C(cid:107)B(cid:107)|∇τ |∞(cid:107)x(cid:107) .

(cid:107)˜x − ˜yτ (cid:107) ≤ (cid:107)A(x − y) + B(∆x − τ (∆)y)(cid:107)

≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))(cid:107)x − y(cid:107) + (cid:107)∆ − τ (∆)(cid:107)(cid:107)x(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))
(cid:125)

(cid:107)x − y(cid:107) + C|∇τ |∞
(cid:124) (cid:123)(cid:122) (cid:125)
δ2

(cid:107)x(cid:107) ,

(cid:123)(cid:122)
δ1

(cid:124)

and therefore, by plugging (17) with y = ˜xτ , K layers of the Laplacian network satisfy

(cid:107)Φ(x; ∆) − Φ(x; τ (∆)(cid:107) ≤





(cid:89)

j≤K−1








δ1(j)

 (cid:107)˜x − ˜xτ (cid:107) +



(cid:88)

(cid:89)

j<K−1

j(cid:48)≤j







δ1(j(cid:48))δ2(j)

 |∇τ |∞(cid:107)x(cid:107)

≤

C



δ1(j)

 (cid:107)B(cid:107) +



(cid:89)

j≤K−1

(cid:88)

(cid:89)

δ1(j(cid:48))δ2(j)

j<K−1

j(cid:48)≤j






 |∇τ |∞(cid:107)x(cid:107) . (cid:3) .

(15)
(16)

(17)

B.4. Proof of (d)

The proof is also analogous to the proof of (c), with the difference that now the Dirac operator is no longer invariant to

orthogonal transformations, only to translations. Given two points p, q, we verify that

(cid:107)p − q − τ (p) − τ (q)(cid:107) ≤ (cid:102)|τ |∞(cid:107)p − q(cid:107) ,

(cid:107)D − τ (D)(cid:107) = o((cid:102)|τ |∞) .

which, following the previous argument, leads to

C. Theorem 4.2

C.1. Proof of part (a)

The proof is based on the following lemma:

Lemma C.1 Let xN , yN ∈ H(MN ) such that ∀ N , (cid:107)xN (cid:107)H ≤ c,(cid:107)yN (cid:107)H ≤ c. Let ˆxN = EN (xN ), where EN is the
eigendecomposition of the Laplacian operator ∆N on MN , , with associated eigenvalues λ1 . . . λN in increasing order. Let
γ > 0 and β be deﬁned as in (5) for xN and yN . If β > 1 and (cid:107)xN − yN (cid:107) ≤ (cid:15) for all N ,

where C is a constant independent of (cid:15) and N .

(cid:107)∆N (xN − yN )(cid:107)2 ≤ C(cid:15)2− 1

β−1/2 ,

One layer of the network will transform the difference x1 − x2 into ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2). We verify that

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + (cid:107)B(cid:107)(cid:107)∆(x1 − x2)(cid:107) .

We now apply Lemma C.1 to obtain

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + C(cid:107)B(cid:107)(cid:107)x1 − x2(cid:107)

β−1
β−1/2

≤ (cid:107)x1 − x2(cid:107)

β−1
β−1/2

(cid:16)

(cid:107)A(cid:107)(cid:107)x1 − x2(cid:107)(2β−1)−1

+ C(cid:107)B(cid:107)

(cid:17)

≤ C((cid:107)A(cid:107) + (cid:107)B(cid:107))(cid:107)x1 − x2(cid:107)

β−1
β−1/2 ,

where we redeﬁne C to account for the fact that (cid:107)x1 − x2(cid:107)(2β−1)−1

is bounded. We have just showed that

with fr = C((cid:107)Ar(cid:107) + (cid:107)Br(cid:107)) and gr = βr−1

βr−1/2 . By cascading (20) for each of the R layers we thus obtain

(cid:107)x(r+1)
1

− x(r+1)
2

(cid:107) ≤ fr(cid:107)x(r)

1 − x(r)

2 (cid:107)gr

(cid:107)Φ∆(x1) − Φ∆(x2)(cid:107) ≤

(cid:35)

(cid:81)
f
r

r(cid:48)>r gr(cid:48)

(cid:107)x1 − x2(cid:107)

(cid:81)R

r=1 gr ,

(cid:34) R
(cid:89)

r=1

which proves (6) (cid:3).

Proof of (19): Let {e1, . . . , eN } be the eigendecomposition of ∆N . For simplicity, we drop the subindex N in the signals
from now on. Let ˆx(k) = (cid:104)x, ek(cid:105) and ˜x(k) = λk ˆx(k); and analogously for y. From the Parseval identity we have that
(cid:107)x(cid:107)2 = (cid:107)ˆx(cid:107)2. We express (cid:107)∆(x − y)(cid:107) as

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 .
λ2

(cid:88)

k≤N

The basic principle of the proof is to cut the spectral sum (22) in two parts, chosen to exploit the decay of ˜x(k). Let

F (x)(k) =

(cid:80)

k(cid:48)≥k ˜x(k)2
(cid:107)x(cid:107)2
H

=

(cid:80)

k(cid:48)≥k ˜x(k)2
k(cid:48) ˜x(k)2 =
(cid:80)

(cid:80)

k(cid:48)≥k λ2
(cid:80)
k(cid:48) λ2

k ˆx(k)2
k ˆx(k)2 ≤ 1 ,

(18)

(19)

(20)

(21)

(22)

and analogously for y. For any cutoff k∗ ≤ N we have

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 +
λ2

k(ˆx(k) − ˆy(k))2
λ2

(cid:88)

(cid:88)

k≤k∗

≤ λ2
k∗
≤ λ2
k∗
≤ λ2
k∗
where we denote for simplicity F (k∗) = max(F (x)(k∗), F (y)(k∗)). By assumption, we have λ2
k
F (k) (cid:46) (cid:88)

(cid:15)2 + 2(F (x)(k∗)(cid:107)x(cid:107)2
(cid:15)2 + 2F (k∗)((cid:107)x(cid:107)2
(cid:15)2 + 4F (k∗)D2 ,

k2(γ−β) (cid:39) k1+2(γ−β) .

H + (cid:107)y(cid:107)2

H)

H)

k>k∗
H + F (y)(k∗)(cid:107)y(cid:107)2

(cid:46) k2γ and

By denoting ˜β = β − γ − 1/2, it follows that

k(cid:48)≥k

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2k2γ

∗ + 4D2k−2 ˜β

∗

(cid:15)22γk2γ−1 − 2 ˜β4D2k−2 ˜β−1 = 0, thus
(cid:20) 4βD2
γ(cid:15)2

k∗ =

(cid:21) 1

2γ+2 ˜β

.

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2− 1

γ+ ˜β = (cid:15)2− 1

β−1/2 ,

Optimizing for k∗ yields

which proves part (a) (cid:3).

C.2. Proof of part (b)

We will use the following lemma:

By plugging (25) back into (24) and dropping all constants independent of N and (cid:15), this leads to

Lemma C.2 Let M = (V, E, F ) is a non-degenerate mesh, and deﬁne

η1(M) = sup

, η2(M) = sup

, η3(M) = αmin .

¯Ai
¯Aj

(i,j)∈E

ij + (cid:96)2
(cid:96)2

jk + (cid:96)2
ik

(i,j,k)∈F

A(i, j, k)

Then, given a smooth deformation τ and x deﬁned in M, we have

(cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) ,

where C depends only upon η1, η2 and η3.

In that case, we need to control the difference ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x). We verify that

(cid:107)ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x)(cid:107) ≤ (cid:107)B(cid:107)(cid:107)(∆ − τ (∆))x(cid:107) .

By Lemma C.2 it follows that (cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) and therefore, by denoting x(1)
x(1)
2 = ρ(Ax + Bτ (∆)x), we have

1 = ρ(Ax + B∆x) and

2 (cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) = C|∇τ |∞(cid:107)x(cid:107)H .

(28)

1 − x(1)
By applying again Lemma C.1, we also have that

(cid:107)x(1)

(cid:107)∆x(1)

1 − τ (∆)x(1)

2 (cid:107) = (cid:107)∆x(1)
= (cid:107)∆(x(1)
≤ C(cid:107)x(1)

1 − (∆ + τ (∆) − ∆)x(1)
2 (cid:107)
2 ) + (τ (∆) − ∆)x(1)
1 − x(1)
2 (cid:107)
1 − x(1)
β1−1/2 + |∇τ |∞(cid:107)x(1)
2 (cid:107)
β1−1
β1−1/2 ,

β1−1

(cid:46) C|∇τ |∞

2 (cid:107)H

(23)

(24)

(25)

(26)

(27)

which, by combining it with (28) and repeating through the R layers yields

(cid:107)Φ∆(x, M) − Φ∆(x, τ (M)(cid:107) ≤ C|∇τ |∞

(cid:81)R

r=1

βr −1
βr −1/2 ,

(29)

which concludes the proof (cid:3).

Proof of (27): The proof follows closely the proof of Theorem 4.1, part (c). From (13) and (14) we have that

τ ( ¯A) = ¯A(I + Gτ ) , with |Gτ |∞ ≤ C(η2, η3)|∇τ |∞ , and
τ (U − W ) = (I + Hτ )(U − W ) , with |Hτ |∞ ≤ C(η2, η3)|∇τ |∞ .

It follows that, up to second order o(|∇τ |∞

2) terms,

τ (∆) − ∆ = τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )

= (cid:0) ¯A[1 + Gτ ](cid:1)−1
(cid:39) ¯A−1Hτ (U − W ) + Gτ ∆ .

[(I + Hτ )(U − W )] − ¯A−1(U − W )

(30)

By writing ¯A−1Hτ = (cid:102)Hτ ¯A−1, and since ¯A is diagonal, we verify that

( (cid:102)Hτ )i,j = (Hτ )i,j

, with

Ai,i
Aj,j

Ai,i
Aj,j

≤ η1, and hence that

¯A−1Hτ (U − W ) = (cid:102)Hτ ∆ , with | (cid:102)Hτ |∞ ≤ C(η1, η2, η3)|∇τ |∞ .

(31)

We conclude by combining (30) and (31) into

(cid:107)(∆ − τ (∆))x(cid:107) = (cid:107)(Gτ + (cid:102)Hτ )∆x(cid:107)

≤ C (cid:48)(η1, η2, η3)|∇τ |∞(cid:107)∆x(cid:107) ,

which proves (27) (cid:3)

C.3. Proof of part (c)

This result is a consequence of the consistency of the cotangent Laplacian to the Laplace-Beltrami operator on S [45]:

Theorem C.3 ([45], Thm 3.4) Let M be a compact polyhedral surface which is a normal graph over a smooth surface S
with distortion tensor T , and let ¯T = (det T )1/2T −1. If the normal ﬁeld uniform distance d(T , 1) = (cid:107) ¯T − 1(cid:107)∞ satisﬁes
d(T , 1) ≤ (cid:15), then

(cid:107)∆M − ∆S(cid:107) ≤ (cid:15) .

(32)

If ∆M converges uniformly to ∆S, in particular we verify that

(cid:107)x(cid:107)H(M) → (cid:107)x(cid:107)H(S) .

Thus, given two meshes M, M(cid:48) approximating a smooth surface S in terms of uniform normal distance, and the corre-

sponding irregular sampling x and x(cid:48) of an underlying function ¯x : S → R, we have

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x − x(cid:48)(cid:107) + (cid:107)B(cid:107)(cid:107)∆Mx − ∆M(cid:48)x(cid:48)(cid:107) .

(33)

Since M and M(cid:48) both converge uniformly normally to S and ¯x is Lipschitz on S, it results that

thus (cid:107)x − x(cid:48)(cid:107) ≤ 2L(cid:15). Also, thanks to the uniform normal convergence, we also have convergence in the Sobolev sense:

(cid:107)x − ¯x(cid:107) ≤ L(cid:15) , and (cid:107)x(cid:48) − ¯x(cid:107) ≤ L(cid:15) ,

(cid:107)x − ¯x(cid:107)H (cid:46) (cid:15) , (cid:107)x(cid:48) − ¯x(cid:107)H (cid:46) (cid:15) ,

which implies in particular that

From (33) and (34) it follows that

(cid:107)x − x(cid:48)(cid:107)H (cid:46) (cid:15) .

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ 2(cid:107)A(cid:107)L(cid:15) +

(34)

(35)

+(cid:107)B(cid:107)(cid:107)∆Mx − ∆S ¯x + ∆S ¯x − ∆M(cid:48)x(cid:48)(cid:107)

≤ 2(cid:15) ((cid:107)A(cid:107)L + (cid:107)B(cid:107)) .

By applying again Lemma C.1 to ˜x = ρ(Ax + B∆Mx), ˜x(cid:48) = ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48)), we have

We conclude by retracing the same argument as before, reapplying Lemma C.1 at each layer to obtain

(cid:107)˜x − ˜x(cid:48)(cid:107)H ≤ C(cid:107)˜x − ˜x(cid:48)(cid:107)

β1−1
β1−1/2 (cid:46) (cid:15)

β1−1
β1−1/2 .

(cid:107)ΦM(x) − ΦM(cid:48)(x(cid:48))(cid:107) ≤ C(cid:15)

(cid:81)R

r=1

βr −1
βr −1/2 . (cid:3) .

D. Proof of Corollary 4.3

We verify that

(cid:107)ρ(B∆x) − ρ(Bτ (∆)τ (x))(cid:107) ≤ (cid:107)B(cid:107)(cid:107)∆x − τ (∆)τ (x)(cid:107)

≤ (cid:107)B(cid:107)(cid:107)∆(x − τ (x)) + (∆ − τ (∆))(τ (x))(cid:107)
≤ (cid:107)B(cid:107)((cid:107)∆(x − τ (x))(cid:107) + (cid:107)(∆ − τ (∆))(τ (x))(cid:107) .

The second term is o(|∇τ |∞) from Lemma C.2. The ﬁrst term is

(cid:107)x − τ (x)(cid:107)H ≤ (cid:107)∆(I − τ )(cid:107)(cid:107)x(cid:107) ≤ (cid:107)∇2τ (cid:107)(cid:107)x(cid:107) ,

where (cid:107)∇2τ (cid:107) is the uniform Hessian norm of τ . The result follows from applying the cascading argument from last section.
(cid:3)

E. Preliminary Study: Metric Learning for Dense Correspondence

As an interesting extension, we apply the architecture we built in Experiments 6.2 directly to a dense shape correspondence

problem.

Similarly as the graph correspondence model from [33], we consider a Siamese Surface Network, consisting of two
identical models with the same architecture and sharing parameters. For a pair of input surfaces M1, M2 of N1, N2 points
respectively, the network produces embeddings E1 ∈ RN1×d and E2 ∈ RN2×d. These embeddings deﬁne a trainable
similarity between points given by

si,j =

e(cid:104)E1,i,E2,j (cid:105)
j(cid:48) e(cid:104)E1,i,E2,j(cid:48) (cid:105)

,

(cid:80)

(36)

which can be trained by minimizing the cross-entropy relative to ground truth pairs. A diagram of the architecture is

provided in Figure 10.

In general, dense shape correspondence is a task that requires a blend of intrinsic and extrinsic information, motivating
the use of data-driven models that can obtain such tradeoffs automatically. Following the setup in Experiment 6.2, we use
models with 15 ResNet-v2 blocks with 128 output features each, and alternate Laplace and Dirac based models with Average
Pooling blocks to cover a larger context: The input to our network consists of vertex positions only.

We tested our architecture on a reconstructed (i.e. changing the mesh connectivity) version of the real scan of FAUST
dataset[3]. The FAUST dataset contains 100 real scans and their corresponding ground truth registrations. The ground truth
is based on a deformable template mesh with the same ordering and connectivity, which is ﬁtted to the scans. In order to
eliminate the bias of using the same template connectivity, as well as the need of a single connected component, the scans
are reconstructed again with [19]. To foster replicability, we release the processed dataset in the additional material. In our
experiment, we use 80 models for training and 20 models for testing.

Figure 10. Siamese network pipeline: the two networks take vertex coordinates of the input models and generate a high dimensional feature
vector, which are then used to deﬁne a map from M1 to M2. Here, the map is visualized by taking a color map on M2, and transferring
it on M1

Figure 11. Additional results from our setup. Plot in the middle shows rate of correct correspondence with respect to geodesic error [23].
We observe that Laplace is performing similarly to Dirac in this scenario. We believe that the reason is that the FAUST dataset contains
only isometric deformations, and thus the two operators have access to the same information. We also provide visual comparison, with the
transfer of a higher frequency colormap from the reference shape to another pose.

Since the ground truth correspondence is implied only through the common template mesh, we compute the correspon-
dence between our meshes with a nearest neighbor search between the point cloud and the reconstructed mesh. Consequently,

Figure 12. Heat map illustrating the point-wise geodesic difference between predicted correspondence point and the ground truth. The unit
is proportional to the geodesic diameter, and saturated at 10%.

Figure 13. A failure case of applying the Laplace network to a new pose in the FAUST benchmark dataset. The network confuses between
left and right arms. We show the correspondence visualization for front and back of this pair.

due to the drastic change in vertex replacement after the remeshing, only 60-70 percent of labeled matches are used. Although
making it more challenging, we believe this setup is close to a real case scenario, where acquisition noise and occlusions are
unavoidable.

Our preliminary results are reported in Figure 11. For simplicity, we generate predicted correspondences by simply taking
the mode of the softmax distribution for each reference node i: ˆj(i) = arg maxj si,j, thus avoiding a reﬁnement step that is
standard in other shape correspondence pipelines. The MLP model uses no context whatsoever and provides a baseline that
captures the prior information from input coordinates alone. Using contextual information (even extrinsically as in point-
cloud model) brings signiﬁcative improvments, but these results may be substantially improved by encoding further prior
knowledge. An example of the current failure of our model is depitcted in Figure 13, illustrating that our current architecture
does not have sufﬁciently large spatial context to disambiguate between locally similar (but globally inconsistent) parts.

We postulate that the FAUST dataset [3] is not an ideal ﬁt for our contribution for two reasons: (1) it is small (100

models), and (2) it contains only near-isometric deformations, which do not require the generality offered by our network. As
demonstrated in [28], the correspondence performances can be dramatically improved by constructing basis that are invariant
to the deformations. We look forward to the emergence of new geometric datasets, and we are currently developing a capture
setup that will allow us to acquire a more challenging dataset for this task.

F. Further Numerical Experiments

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 14. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 15. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 16. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 17. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

Laplace

Dirac

Figure 18. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Figure 19. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

Figure 20. From left to right: set-to-set, ground truth and Dirac based model. Color corresponds to mean squared error between ground
truth and prediction: green - smaller error, red - larger error.

8
1
0
2
 
n
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
1
8
0
1
.
5
0
7
1
:
v
i
X
r
a

Surface Networks

Ilya Kostrikov1, Zhongshi Jiang1, Daniele Panozzo ∗1, Denis Zorin †1, and Joan Bruna‡1,2

1Courant Institute of Mathematical Sciences, New York University
2Center for Data Science, New York University

Abstract

We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used
to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs,
namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the
Laplacian operator.

Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric
deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to
GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In
particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions — this is in stark
contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface
Networks (SN).

We prove that these models deﬁne shape representations that are stable to deformation and to discretization, and we
demonstrate the efﬁciency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under
non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by
SNs.

1. Introduction

3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering
to computer animation to medical imaging. Despite the vast amount of high-quality 3D geometric data available, data-
driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data
representation regularity which is required for traditional convolutional neural network approaches. While in computer vision
problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more
complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar
chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical
properties.

Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike
the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the
inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary
features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations.
Extrinsic representations rely on the speciﬁc embedding of surfaces within a three-dimensional ambient space, whereas
intrinsic representations only capture geometric properties speciﬁc to the surface, irrespective of its parametrization. Whereas
the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local
deformations and invariance to global transformations.

∗DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.
†DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.
‡JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding

author: bruna@cims.nyu.edu

1

A particularly simple and popular extrinsic method [36, 37] represents shapes as point clouds in R3 of variable size,
and leverages recent deep learning models that operate on input sets [44, 43]. Despite its advantages in terms of ease
of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classiﬁcation
and segmentation tasks, one may wonder whether this simpliﬁcation comes at a loss of precision as one considers more
challenging prediction tasks.

In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on
geometric deep learning. These models provide data-driven intrinsic graph and manifold representations with inductive
biases analogous to CNNs on natural images. Models based on Graph Neural Networks [40] and their spectral variants
[6, 11, 26] have been successfully applied to geometry processing tasks such as shape correspondence [32]. In their basic
form, these models learn a deep representation over the discretized surface by combining a latent representation at a given
node with a local linear combination of its neighbors’ latent representations, and a point-wise nonlinearity. Different models
vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to
spectral interpretations of those models.

Our contributions are three-fold. First, we extend the model to support extrinsic features. More speciﬁcally, we exploit the
fact that surfaces in R3 admit a ﬁrst-order differential operator, the Dirac operator, that is stable to discretization, provides a
direct generalization of Laplacian-based propagation models, and is able to detect principal curvature directions [9, 18]. Next,
we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization,
two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the
variational autoencoder framework [25, 39], that is able to exploit non-Euclidean geometric regularity.

By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation
that we apply to several challenging tasks. The resulting Surface Networks – using either the Dirac or the Laplacian, inherit
the stability and invariance properties of these operators, thus providing data-driven representations with certiﬁed stability to
deformations. We demonstrate the model efﬁciency on a temporal prediction task of complex dynamics, based on a physical
simulation of elastic shells, which conﬁrms that whenever geometric information (in the form of a mesh) is available, it can
be leveraged to signiﬁcantly outperform point-cloud based models.

Our main contributions are summarized as follows:

• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear

dynamics, motivating the use of geometric shape information.

• We prove that Surface Networks deﬁne shape representations that are stable to deformation and to discretization.

• We introduce a generative model for 3D surfaces based on the variational autoencoder.

2. Related Work

Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research.
[40] introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained
by backpropagation. Subsequent works [27, 43] have relaxed the model by untying the recurrent layer weights and proposed
several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convo-
lutional networks to non-Euclidean graphs. [6, 17] proposed to learn smooth spectral multipliers of the graph Laplacian,
albeit with high computational cost, and [11, 26] resolved the computational bottleneck by learning polynomials of the graph
Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to
[5] for an exhaustive literature review on the topic. GNNs are ﬁnding application in many different domains. [2, 7] develop
graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics.
[12, 22] study molecular ﬁngerprints using variants of the GNN architecture, and [14] further develop the model by combin-
ing it with set representations [44], showing state-of-the-art results on molecular prediction. The resulting models, so-called
Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model
on general graphs.

In the context of computer graphics, [30] developed the ﬁrst CNN model on meshed surfaces using intrinsic patch rep-
resentations, and further generalized in [4] and [32]. This last work allows for ﬂexible representations via the so-called
pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode ﬁrst-
order differential information. These intrinsic models contrast with Euclidean models such as [48, 46], that have higher
sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models

A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/SurfaceNetworks.

are increasingly popular to model 3d objects due to their simplicity and versatility. [36, 37] use set-invariant representations
from [44, 43] to solve shape segmentation and classiﬁcation tasks. More recently, [29] proposes to learn surface convolutional
network from a canonical representation of planar ﬂat-torus, with excellent performance on shape segmentation and classi-
ﬁcation, although such canonical representations may introduce exponential scale changes that can introduce instabilities.
Finally, [13] proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but
does not encode geometrical information as our shape generative model. Learning variational deformations is an important
problem for graphics applications, since it enables negligible and ﬁxed per-frame cost [35], but it is currently limited to 2D
deformations using point handles. In constrast, our method easily generalizes to 3D and learns dynamic behaviours.

3. Surface Networks

This section presents our surface neural network model and its basic properties. We start by introducing the problem setup
and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section
3.2).

3.1. Laplacian Surface Networks

Our ﬁrst goal is to deﬁne a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where
V = (vi ∈ R3)i≤N contains the node coordinates, E = (ei,j) corresponds to edges, and F is the set of triangular faces. We
denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see [5] for details).
This operator can be interpreted as a local, linear high-pass ﬁlter in M that acts on signals x ∈ Rd×|V | deﬁned on the
vertices as a simple matrix multiplication ˜x = ∆x. By combining ∆ with an all-pass ﬁlter and learning generic linear
combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators
in M that update a feature map from layer k to layer k + 1 using trainable parameters Ak and Bk:

xk+1 = ρ (cid:0)Ak∆xk + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk .

By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization,
this model is a speciﬁc instance of the graph neural network [40, 5, 26] and a generalization of the spectrum-free Laplacian
networks from [11]. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening
or pooling layers.

In contrast to general graphs, meshes contain a low-dimensional Euclidean embedding that contains potentially useful
information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A
simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as
input channels to the network: x1 := V ∈ R|V |×3. The mean curvature can be computed by applying the Laplace operator
to the coordinates of the vertices:

∆x1 = −2Hn ,

where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian
neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph
neural network models is related to the use of generalized coordinates from [32]. By cascading K layers of the form (1) we
obtain a representation Φ∆(M) that contains generic features at each node location. When the number of layers K is of
the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate
information across the whole surface.

Equation (2) illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to
the mean variations across all directions. Although in general graphs there is no well-deﬁned procedure to recover anisotropic
local variations, in the case of surfaces some authors ([4, 1, 32]) have considered anisotropic extensions. We describe next
a particularly simple procedure to increase the expressive power of the network using a related operator from quantum
mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation [9] and shape
analysis [18].

(1)

(2)

3.2. Dirac Surface Networks

The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the
gradient (a ﬁrst-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to
these ﬁrst-order differential operators separately, enabling oriented high-pass ﬁlters.

For convenience, we embed R3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for
details). The Dirac operator is then deﬁned as a matrix D ∈ H|F |×|V | that maps (quaternion) signals on the nodes to signals
on the faces. In coordinates,

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area (see Appendix A) using counter-clockwise
orientations on all faces.

To apply the Dirac operator deﬁned in quaternions to signals in vertices and faces deﬁned in real numbers, we write the
feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a
quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The
Dirac operator provides ﬁrst-order differential information and is sensitive to local orientations. Moreover, one can verify [9]
that

Re D∗D = ∆ ,
where D∗ is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as
D∗ = M −1
V DH MF where DH is a conjugate transpose of D and MV , MF are diagonal mass matrices with one third of
areas of triangles incident to a vertex and face areas respectively.

The Dirac operator can be used to deﬁne a new neural surface representation that alternates layers with signals deﬁned
over nodes with layers deﬁned over faces. Given a d-dimensional feature representation over the nodes xk ∈ Rd×|V |, and the
faces of the mesh, yk ∈ Rd×|F |, we deﬁne a d(cid:48)-dimensional mapping to a face representation as

yk+1 = ρ (cid:0)CkDxk + Ekyk(cid:1) , Ck, Ek ∈ Rdk+1×dk ,

(3)

where Ck, Ek are trainable parameters. Similarly, we deﬁne the adjoint layer that maps back to a ˜d-dimensional signal over
nodes as

xk+1 = ρ (cid:0)AkD∗yk+1 + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk ,
where Ak, Bk are trainable parameters. A surface neural network layer is thus determined by parameters {A, B, C, E} using
equations (3) and (4) to deﬁne xk+1 ∈ Rdk+1×|V |. We denote by ΦD(M) the mesh representation resulting from applying
K such layers (that we assume ﬁxed for the purpose of exposition).

(4)

The Dirac-based surface network is related to edge feature transforms proposed on general graphs in [14], although these
edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no
notion of square root of ∆ that recovers oriented ﬁrst-order derivatives.

4. Stability of Surface Networks

Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise
under the model. Given a continuous surface S ⊂ R3 or a discrete mesh M, and a smooth deformation ﬁeld τ : R3 → R3,
we are particularly interested in two forms of stability:

• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that (cid:107)Φ(M) −

Φ(τ (M))(cid:107) is small if (cid:107)∇τ (∇τ )∗ − I(cid:107) is small, i.e when the deformation is nearly rigid; see Theorem 4.1.

• Given two discretizations M1 and M2 of the same underlying surface S, we would like to control (cid:107)Φ(M1)−Φ(M2)(cid:107)

in terms of the resolution of the meshes; see Theorem 4.2.

These stability properties are important in applications, since most tasks we are interested in are stable to deformation and
to discretization. We shall see that the ﬁrst property is a simple consequence of the fact that the mesh Laplacian and Dirac
operators are themselves stable to deformations. The second property will require us to specify under which conditions the
discrete mesh Laplacian ∆M converges to the Laplace-Beltrami operator ∆S on S. Unless it is clear from the context, in the
following ∆ will denote the discrete Laplacian.

Theorem 4.1 Let M be a N -node mesh and x, x(cid:48) ∈ R|V |×d be input signals deﬁned on the nodes. Assume the nonlinearity
ρ( · ) is non-expansive (|ρ(z) − ρ(z(cid:48))| ≤ |z − z(cid:48)|). Then

(a) (cid:107)Φ∆(M; x) − Φ∆(M; x(cid:48))(cid:107) ≤ α∆(cid:107)x − x(cid:48)(cid:107) , where α∆ depends only on the trained weights and the mesh.

(b) (cid:107)ΦD(M; x) − ΦD(M; x(cid:48))(cid:107) ≤ αD(cid:107)x − x(cid:48)(cid:107) , where αD depends only on the trained weights and the mesh.

(c) Let |∇τ |∞ := supu (cid:107)∇τ (u)(∇τ (u))∗ − 1(cid:107), where ∇τ (u) is the Jacobian matrix of u (cid:55)→ τ (u). Then (cid:107)Φ∆(M; x) −

Φ∆(τ (M); x)(cid:107) ≤ β∆|∇τ |∞(cid:107)x(cid:107) , where β∆ is independent of τ and x.

(d) Denote by (cid:103)|∇τ |∞ := supu (cid:107)∇τ (u) − 1(cid:107). Then (cid:107)ΦD(M; x) − ΦD(τ (M); x)(cid:107) ≤ βD (cid:103)|∇τ |∞(cid:107)x(cid:107) , where βD is

independent of τ and x.

Properties (a) and (b) are not speciﬁc to surface representations, and are a simple consequence of the non-expansive property
of our chosen nonlinearities. The constant α is controlled by the product of (cid:96)2 norms of the network weights at each layer and
the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators
are themselves stable to deformations, a property that depends on two key aspects: ﬁrst, the Laplacian/Dirac is localized in
space, and next, that it is a high-pass ﬁlter and therefore only depends on relative changes in position.

One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the
reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral
radius of ∆M diverges as the mesh size N increases.

In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incom-
ing to the surface network. This can be measured with Sobolev norms deﬁned using the spectrum of the Laplacian operator.
Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆M, consider
the spectral decomposition of ∆M (a symmetric, positive deﬁnite operator):

∆M =

λkekeT

k , ek ∈ RN , 0 ≤ λ1 ≤ λ2 · · · ≤ λN .

(cid:88)

k≤N

Under normal uniform convergence 1 [45], the spectrum of ∆M converges to the spectrum of the Laplace-Beltrami operator
∆S of S. If S is bounded, it is known from the Weyl law [47] that there exists γ > 0 such that k−γ(S) (cid:46) λ−1
k , so the
eigenvalues λk do not grow too fast. The smoothness of a signal x ∈ R|V |×d deﬁned in M is captured by how fast its
H := (cid:80)
spectral decomposition ˆx(k) = eT
k λ(k)2(cid:107)ˆx(k)(cid:107)2 is Sobolev norm, and
β(x, S) > 1 as the largest rate such that its spectral decomposition coefﬁcients satisfy

k x ∈ Rd decays [42]. We deﬁne (cid:107)x(cid:107)2

(cid:107)ˆx(k)(cid:107) (cid:46) k−β , (k → ∞) .
(5)
If x ∈ R|V |×d is the input to the Laplace Surface Network of R layers, we denote by (β0, β1, . . . , βR−1) the smoothness
rates of the feature maps x(r) deﬁned at each layer r ≤ R.

Theorem 4.2 Consider a surface S and a ﬁnite-mesh approximation MN of N points, and Φ∆ a Laplace Surface Network
with parameters {(Ar, Br)}r≤R. Denote by d(S, MN ) the uniform normal distance, and let x1, x2 be piece-wise polyhedral
approximations of ¯x(t), t ∈ S in MN , with (cid:107)¯x(cid:107)H(S) < ∞. Assume (cid:107)¯x(r)(cid:107)H(S) < ∞ for r ≤ R.

(a) If x1, x2 are two functions such that the R feature maps x(r)

have rates (β0, β1, . . . , βR−1), then

l
(cid:107)Φ∆(x1; MN ) − Φ∆(x2; MN )(cid:107)2 ≤ C(β)(cid:107)x1 − x2(cid:107)h(β) ,

(6)

with h(β) = (cid:81)R

βr−1
βr−1/2 , and where C(β) does not depend upon N .

r=1

(b) If τ is a smooth deformation ﬁeld, then (cid:107)Φ∆(x; MN ) − Φ∆(x; τ (MN ))(cid:107) ≤ C|∇τ |∞

h(β) , where C does not depend

upon N .

(c) Let M and M(cid:48) be N -point discretizations of S, If max(d(M, S), d(M(cid:48), S)) ≤ (cid:15), then (cid:107)Φ∆(M; x) − Φ∆(M(cid:48), x(cid:48))(cid:107) ≤

C(cid:15)h(β) , where C is independent of N .

This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases,
the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac
operator also has a well-deﬁned continuous counterpart [9] that generalizes the gradient operator in quaternion space. Also,
our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in
terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x).
These extensions are left for future work. Finally, a speciﬁc setup that we use in experiments is to use as input signal the
canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields

Corollary 4.3 Denote Φ(M) := ΦM(V ), where V are the node coordinates of M. Then, if A1 = 0,

(cid:107)Φ(M) − Φ(τ (M))(cid:107) ≤ κ max(|∇τ |∞, (cid:107)∇2τ (cid:107))h(β) .

(7)

1which controls how the normals of the mesh align with the surface normals; see [45].

Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R3 (right) is expressed in terms of a “sampling” 2D irregular mesh
˜M ⊂ R2 (left) and a depth scalar ﬁeld f : ˜M → R over ˜M (center).

5. Generative Surface Models

State-of-the-art generative models for images, such as generative adversarial networks [38], pixel autoregressive networks
[34], or variational autoencoders [25], exploit the locality and stationarity of natural images in their probabilistic models, in
the sense that the model satisﬁes pθ(x) ≈ pθ(xτ ) by construction, where xτ is a small deformation of a given input x. This
property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric
stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains
two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface,
which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas
the former contains the essential semantic information, the latter is not informative, and to some extent independent of the
shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as
height-ﬁeld meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as ( ˜M, f ( ˜M)), where ˜M = ( ˜V , ˜E, ˜F ) is
now a 2D mesh and f : ˜V → R is a depth-map encoding the original node locations V , as shown in Figure 1.

In this work, we consider the variational autoencoder framework [25, 39].

It considers a mixture model of the form
p(M) = (cid:82) pθ(M | h)p0(h)dh , where h ∈ R|S| is a vector of latent variables. We train this model by optimizing the
variational lower bound of the data log-likelihood:

min
θ,ψ

1
L

(cid:88)

l≤L

−Eh∼qψ (h | Ml) log pθ(Ml | h) + DKL(qψ(h | Ml) || p0(h)) .

(8)

We thus need to specify a conditional generative model pθ(M | h), a prior distribution p0(h) and a variational approximation
to the posterior qψ(h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the
height-ﬁeld representation, we choose for simplicity a separable model of the form pθ(M | h) = pθ(f | h, ˜M) · p( ˜M) ,
where ˜M ∼ p( ˜M) is a homogeneous Poisson point process, and f ∼ pθ(f | h, ˜M) is a normal distribution with mean and
isotropic covariance parameters given by a SN:

pθ(f | h, ˜M) = N (µ(h, ˜M), σ2(h, ˜M)1) ,

with [µ(h, ˜M), σ2(h, ˜M)] = ΦD( ˜M ; h) . The generation step thus proceeds as follows. We ﬁrst sample a 2D mesh ˜M
independent of the latent variable h, and then sample a depth ﬁeld over ˜M conditioned on h from the output of a decoder
network ΦD( ˜M ; h). Finally, the variational family qψ is also a Normal distribution whose parameters are obtained from an
encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization: qψ(h | M) =
N (¯µ, ¯σ21) , with [¯µ, ¯σ] = ¯ΦD(M) .

6. Experiments

6.1. MeshMNIST

For experimental evaluation, we compare models built using ResNet-v2 blocks [16], where convolutions are replaced with
the appropriate operators (see Fig. 2): (i) a point cloud based model from [43] that aggregates global information by averaging
features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical
coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant
of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).

For this task, we construct a MeshMNIST database with only height-ﬁeld meshes (Sec. 5). First, we sample points on
a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a
Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each

Figure 2. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to
the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch
Normalization (blue) and ‘1 × 1’ convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language
and denote by xk+1 the output of this 2-layer block.

Receptive ﬁeld Number of parameters

Model
MLP
PointCloud
Laplace
Dirac

1
-
16
8

519672
1018872
1018872
1018872

Smooth L1-loss (mean per sequence (std))
64.56 (0.62)
23.64 (0.21)
17.34 (0.52)
16.84 (0.16)

Table 1. Evaluation of different models on the temporal task

point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to deﬁne a sampling process
over 3D height-ﬁeld meshes.

We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder
converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to
obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a speciﬁc 3D
mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does
not depend on input data. We trained the model for 75 epochs using Adam optimizer [24] with learning rate 10−3, weight
decay 10−5 and batch size 32. Figures 3,4 illustrate samples from the model. The geometric encoder is able to leverage the
local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to
the speciﬁc sampled grid, as opposed to set-based generative models.

Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of dis-
cretization/triangulation (Poisson disk sampling with p=1.5).

Figure 4. Meshes from the dataset (ﬁrst ﬁve). And meshes generated by our model (last ﬁve).

6.2. Spatio-Temporal Predictions

One speciﬁc task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1, X 2, . . . , X n,

the task is to predict the following frames Y = Y 1 = X n+1, Y 2, . . . , Y m = X n+m. As in [31], we use a simple non-
recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on
n = 2 frames and predict the next m = 40 frames. In order to generate data, we ﬁrst extracted 10k patches from the
MPI-Faust dataset[3], by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of
the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground.
We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique [41], with additional
gravitational forces [20]. Libigl [21] has been used for the mesh processing tasks. Sequences with patches from the ﬁrst 80
subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on
request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial
frames. Thus, we can train models by minimizing smooth-L1 loss [15] between target frames and output of our models.

Ground Truth

MLP

PointCloud

Laplace

Dirac

Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set.
Boxes indicate distinctive features. For larger crops, see Figure 6

.

We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and
Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned
frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce
120-dimensional outputs. We trained all models using the Adam optimizer [24] with learning rate 10−3, weight decay 10−5,
and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained
for 110k steps in overall.

Ground Truth

Laplace

Dirac

Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Table 1 reports quantitative prediction performance of different models, and Figure 5 displays samples from the prediction

Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between
ground truth and prediction: green - smaller error, red - larger error.

Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

models at speciﬁc frames. The set-to-set model [44, 43], corresponding to a point-cloud representation used also in [36],
already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between
this model and Laplace-/Dirac-based models is signiﬁcant, both visually and quantitatively. Dirac-based model outperforms
Laplace-based model despite the smaller receptive ﬁeld. Videos comparing the performance of different models are available
in the additional material.

Figure 6 illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since
they propagate information using an isotropic operator, have more difﬁculties at resolving corners and pointy structures than
the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit
the extrinsic information only via the input coordinates is remarkable and more computationally efﬁcient than the Dirac
counterpart. Figures 7 and 8 overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac
respectively. They conﬁrm ﬁrst that SNs outperform the point-cloud based model, which often produce excessive ﬂattening
and large deformations, and next that ﬁrst-order Dirac operators help resolve areas with high directional curvature. We refer
to the supplementary material for additional qualitative results.

7. Conclusions

We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the non-Euclidean
geometry of surfaces. We have shown how a ﬁrst-order differential operator (the Dirac operator) can detect and adapt to
geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction
is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments.
That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting
to instead learn the operator, akin to recent Message-Passing NNs [14] and explore whether Dirac is recovered.

Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly
superior performance to point-cloud based models. While there are not many such datasets currently available, we expect
them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated
in consumer devices. SNs provide efﬁcient inference, with predictable runtime, which makes them appealing across many
areas of computer graphics, where a ﬁxed, per-frame cost is required to ensure a stable framerate, especially in VR applica-
tions. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and
generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.

References

2014. 3

[1] M. Andreux, E. Rodol`a, M. Aubry, and D. Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In Proc. NORDIA,

[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In

Advances in Neural Information Processing Systems, pages 4502–4510, 2016. 2

[3] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 3794–3801, 2014. 7, 18, 20

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks.

In Advances in Neural Information Processing Systems, pages 3189–3197, 2016. 2, 3

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv

preprint arXiv:1611.08097, 2016. 2, 3, 14

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013. 2
[7] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics.

[8] D. Chen and J. R. Gilbert. Obtaining bounds on the two norm of a matrix from the splitting lemma. Electronic Transactions on

[9] K. Crane, U. Pinkall, and P. Schr¨oder. Spin transformations of discrete surfaces. In ACM Transactions on Graphics (TOG). ACM,

ICLR, 2016. 2

2011. 2, 3, 4, 5

Numerical Analysis, 21:28–46, 2005. 14

and Its Applications, 427(1):55–69, 2007. 12

[10] K. C. Das. Extremal graph characterization from the upper bound of the laplacian spectral radius of weighted graphs. Linear Algebra

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In

Advances in Neural Information Processing Systems, pages 3837–3845, 2016. 2, 3

[12] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convo-

lutional networks on graphs for learning molecular ﬁngerprints. In Neural Information Processing Systems, 2015. 2

[13] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. arXiv preprint

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. 7
[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision,

arXiv:1612.00603, 2016. 3

arXiv:1704.01212, 2017. 2, 4, 10

pages 630–645. Springer, 2016. 6

[17] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015. 2
[18] K. C. Hsueh-Ti Derek Liu, Alec Jacobson. A dirac operator for extrinsic shape analysis. Computer Graphics Forum, 2017. 2, 3
[19] Y. Hu, Q. Zhou, X. Gao, A. Jacobson, D. Zorin, and D. Panozzo. Tetrahedral meshing in the wild. Submitted to ACM Transaction on

Graphics, 2018. 18

[20] A. Jacobson. Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes. PhD thesis, ETH, Z¨urich, 2013. 7
[21] A. Jacobson, D. Panozzo, et al. libigl: A simple C++ geometry processing library, 2016. http://libigl.github.io/libigl/. 7
[22] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal

of computer-aided molecular design, 2016. 2

[23] V. G. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic maps. In ACM Transactions on Graphics (TOG), volume 30, page 79.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representation, 2015.

[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 6
[26] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907,

ACM, 2011. 19

7, 8

2016. 2, 3

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. 2
[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape

correspondence. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5660–5668, 2017. 21

[29] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. Kim, and Y. Lipman. Convolutional neural networks on surfaces

via seamless toric covers. In SIGGRAPH, 2017. 3

[30] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In

Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. 2

[31] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error.

arXiv preprint

arXiv:1511.05440, 2015. 7

[32] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds

using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. 2, 3

[33] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna. A note on learning algorithms for quadratic assignment with graph neural networks.

arXiv preprint arXiv:1706.07450, 2017. 18

[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. 6
[35] R. Poranne and Y. Lipman. Simple approximations of planar deformation operators. Technical report, ETHZ, 2015. 3
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. arXiv preprint

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint

arXiv:1612.00593, 2016. 2, 3, 9

arXiv:1706.02413, 2017. 2, 3

arXiv preprint arXiv:1511.06434, 2015. 6

Neural Networks, 20(1):61–80, 2009. 2, 3

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks.

[39] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint arXiv:1505.05770, 2015. 2, 6
[40] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on

[41] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, 2007. 7
[42] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE

[43] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation.

In Advances in Neural Information

Symposium on, pages 29–38. IEEE, 2007. 5

Processing Systems, pages 2244–2252, 2016. 2, 3, 6, 9

[44] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015. 2, 3, 9
[45] M. Wardetzky. Convergence of the cotangent formula: An overview. In Discrete Differential Geometry, pages 275–286. 2008. 5, 12,

17

[46] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolutional networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1544–1553, 2016. 2

[47] H. Weyl.

¨Uber die asymptotische verteilung der eigenwerte. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottingen,

Mathematisch-Physikalische Klasse, 1911:110–117, 1911. 5

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 2

A. The Dirac Operator

The quaternions H is an extension of complex numbers. A quaternion q ∈ H can be represented in a form q = a+bi+cj +

dk where a, b, c, d are real numbers and i, j, k are quaternion units that satisfy the relationship i2 = j2 = k2 = ijk = −1.

As mentioned in Section 3.1, the Dirac operator used in the model can be conveniently represented as a quaternion matrix:

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area, as illustrated in Fig. A, using counter-
clockwise orientations on all faces.

vj

f

ej







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c













b
a
−b
a
−c −d
−d

d
c
d −c
b
a
a
c −b







.

The Deep Learning library PyTorch that we used to implement the models does not support quaternions. Nevertheless,
quaternion-valued matrix multiplication can be replaced with real-valued matrix multiplication where each entry q = a +
bi + cj + dk is represented as a 4 × 4 block

and the conjugate q∗ = a − bi − cj − dk is a transpose of this real-valued matrix:

B. Theorem 4.1

B.1. Proof of (a)

Laplacian ∆ of M is

We ﬁrst show the result for the mapping x (cid:55)→ ρ (Ax + B∆x), corresponding to one layer of Φ∆. By deﬁnition, the

∆ = diag( ¯A)−1(U − W ) ,
where ¯Aj is one third of the total area of triangles incident to node j, and W = (wi,j) contains the cotangent weights [45],
and U = diag(W 1) contains the node aggregated weights in its diagonal.

From [10] we verify that

(cid:107)U − W (cid:107) ≤

√

2 max
i

(cid:115)






U 2

i + Ui

Ujwi,j

(cid:88)

i∼j






√

√

≤ 2

2 sup
i,j

wi,j sup

dj

j

≤ 2

2 cot(αmin)dmax ,

(cid:107)∆(cid:107) ≤ C

cot(αmin)Smax
inf j ¯Aj

:= LM ,

where dj denotes the degree (number of neighbors) of node j, αmin is the smallest angle in the triangulation of M and Smax
the largest number of incident triangles. It results that

which depends uniquely on the mesh M and is ﬁnite for non-degenerate meshes. Moreover, since ρ( · ) is non-expansive, we
have

(cid:107)ρ (Ax + B∆x) − ρ (Ax(cid:48) + B∆x(cid:48))(cid:107) ≤ (cid:107)A(x − x(cid:48)) + B∆(x − x(cid:48))(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)LM)(cid:107)x − x(cid:48)(cid:107) .

By cascading (10) across the K layers of the network, we obtain

(cid:107)Φ(M; x) − Φ(M; x(cid:48))(cid:107) ≤

((cid:107)Ak(cid:107) + (cid:107)Bk(cid:107)LM)


 (cid:107)x − x(cid:48)(cid:107) ,





(cid:89)

k≤K

(9)

(10)

which proves (a). (cid:3)

B.2. Proof of (b)

B.3. Proof of (c)

The proof is analogous, by observing that (cid:107)D(cid:107) = (cid:112)(cid:107)∆(cid:107) and therefore

(cid:107)D(cid:107) ≤

(cid:112)

LM . (cid:3)

To establish (c) we ﬁrst observe that given three points p, q, r ∈ R3 forming any of the triangles of M,

(cid:107)p − q(cid:107)2(1 − |∇τ |∞)2

≤ (cid:107)τ (p) − τ (q)(cid:107)2 ≤

(cid:107)p − q(cid:107)2(1 + |∇τ |∞)2

A(p, q, r)2(1 − |∇τ |∞Cα−2

min − o(|∇τ |∞

2) ≤ A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2(1 + |∇τ |∞Cα−2

min + o(|∇τ |∞

2)) .

(11)

(12)

Indeed, (11) is a direct consequence of the lower and upper Lipschitz constants of τ (u), which are bounded respectively by
1 − |∇τ |∞ and 1 + |∇τ |∞. As for (12), we use the Heron formula

A(p, q, r)2 = s(s − (cid:107)p − q(cid:107))(s − (cid:107)p − r(cid:107))(s − (cid:107)r − q(cid:107)) ,

with s = 1
determined by the deformed points τ (p), τ (q), τ (r), we have that

2 ((cid:107)p − q(cid:107) + (cid:107)p − r(cid:107) + (cid:107)r − q(cid:107)) being the half-perimeter. By denoting sτ the corresponding half-perimeter

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≤ s(1 + |∇τ |∞) − (cid:107)p − q(cid:107)(1 − |∇τ |∞) = s − (cid:107)p − q(cid:107) + |∇τ |∞(s + (cid:107)p − q(cid:107)) and

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≥ s(1 − |∇τ |∞) − (cid:107)p − q(cid:107)(1 + |∇τ |∞) = s − (cid:107)p − q(cid:107) − |∇τ |∞(s + (cid:107)p − q(cid:107)) ,

and similarly for the (cid:107)r − q(cid:107) and (cid:107)r − p(cid:107) terms. It results in

A(τ (p), τ (q), τ (r))2 ≥ A(p, q, r)2
≥ A(p, q, r)2 (cid:104)

(cid:20)
1 − |∇τ |∞

(cid:18)

1 − C|∇τ |∞α−2

1 +

+

s + (cid:107)p − q(cid:107)
s − (cid:107)p − q(cid:107)
(cid:105)
2)

min − o(|∇τ |∞

,

s + (cid:107)p − r(cid:107)
s − (cid:107)p − r(cid:107)

+

s + (cid:107)r − q(cid:107)
s − (cid:107)r − q(cid:107)

(cid:19)

− o(|∇τ |∞

(cid:21)
2)

and similarly

A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2 (cid:104)

1 + C|∇τ |∞α−2

min − o(|∇τ |∞

(cid:105)

2)

.

By noting that the cotangent Laplacian weights can be written (see Fig. 9) as

wi,j =

−(cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
A(i, j, k)

+

−(cid:96)2

jh + (cid:96)2
ih

ij + (cid:96)2
A(i, j, h)

,

we have from the previous Bilipschitz bounds that

τ (wi,j) ≤ wi,j

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

+ 2|∇τ |∞

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

τ (wi,j) ≥ wi,j

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

− 2|∇τ |∞

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

+

+

(cid:33)

(cid:33)

,

,

which proves that, up to second order terms, the cotangent weights are Lipschitz continuous to deformations.

Finally, since the mesh Laplacian operator is constructed as diag( ¯A)−1(U − W ), with ¯Ai,i = 1
3

(cid:80)

j,k;(i,j,k)∈F A(i, j, k),

and U = diag(W 1), let us show how to bound (cid:107)∆ − τ (∆)(cid:107) from

¯Ai,i(1 − αM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ ( ¯Ai,i) ≤ ¯Ai,i(1 + αM|∇τ |∞ + o(|∇τ |∞

2))

and

wi,j(1 − βM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ (wi,j) ≤ wi,j(1 + βM|∇τ |∞ + o(|∇τ |∞

2)) .

(13)

(14)

k

αij

aijk

j

(cid:96)ij

ai

i

βij

h

Figure 9. Triangular mesh and Cotangent Laplacian (ﬁgure reproduced from [5])

Using the fact that ¯A, τ ( ¯A) are diagonal, and using the spectral bound for k × m sparse matrices from [8], Lemma 5.12,

(cid:107)Y (cid:107)2 ≤ max

i

(cid:88)

j; Yi,j (cid:54)=0

|Yi,j|

|Yr,j|

,

(cid:33)

(cid:32) l

(cid:88)

r=1

the bounds (13) and (14) yield respectively

τ ( ¯A) = ¯A(1 + (cid:15)τ ) , with (cid:107)(cid:15)τ (cid:107) = o(|∇τ |∞) , and
τ (U − W ) = U − W + ητ , with (cid:107)ητ (cid:107) = o(|∇τ |∞) .

It results that, up to second order terms,
(cid:13)τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )(cid:13)
(cid:107)∆ − τ (∆)(cid:107) = (cid:13)
(cid:13)
(cid:13)
(cid:0) ¯A[1 + (cid:15)τ ](cid:1)−1
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
=
(cid:13)
= (cid:13)
(cid:13)(cid:15)τ ∆ + ¯A−1ητ
= o(|τ |∞) ,

(cid:13)
(cid:13) + o(|∇τ |∞

1 − (cid:15)τ + o(|∇τ |∞

2)

=

[U − W + ητ ] − ¯A−1(U − W )

(cid:13)
(cid:13)
(cid:13)

(cid:17) ¯A−1(U − W + ητ ) − ¯A−1(U − W )
2)

(cid:13)
(cid:13)
(cid:13)

which shows that the Laplacian is stable to deformations in operator norm. Finally, by denoting ˜xτ a layer of the deformed
Laplacian network

it follows that

Also,

˜xτ = ρ(Ax + Bτ (∆)x) ,

(cid:107)˜x − ˜xτ (cid:107) ≤ (cid:107)B(∆ − τ (∆)x(cid:107)
≤ C(cid:107)B(cid:107)|∇τ |∞(cid:107)x(cid:107) .

(cid:107)˜x − ˜yτ (cid:107) ≤ (cid:107)A(x − y) + B(∆x − τ (∆)y)(cid:107)

≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))(cid:107)x − y(cid:107) + (cid:107)∆ − τ (∆)(cid:107)(cid:107)x(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))
(cid:125)

(cid:107)x − y(cid:107) + C|∇τ |∞
(cid:124) (cid:123)(cid:122) (cid:125)
δ2

(cid:107)x(cid:107) ,

(cid:123)(cid:122)
δ1

(cid:124)

and therefore, by plugging (17) with y = ˜xτ , K layers of the Laplacian network satisfy

(cid:107)Φ(x; ∆) − Φ(x; τ (∆)(cid:107) ≤





(cid:89)

j≤K−1








δ1(j)

 (cid:107)˜x − ˜xτ (cid:107) +



(cid:88)

(cid:89)

j<K−1

j(cid:48)≤j







δ1(j(cid:48))δ2(j)

 |∇τ |∞(cid:107)x(cid:107)

≤

C



δ1(j)

 (cid:107)B(cid:107) +



(cid:89)

j≤K−1

(cid:88)

(cid:89)

δ1(j(cid:48))δ2(j)

j<K−1

j(cid:48)≤j






 |∇τ |∞(cid:107)x(cid:107) . (cid:3) .

(15)
(16)

(17)

B.4. Proof of (d)

The proof is also analogous to the proof of (c), with the difference that now the Dirac operator is no longer invariant to

orthogonal transformations, only to translations. Given two points p, q, we verify that

(cid:107)p − q − τ (p) − τ (q)(cid:107) ≤ (cid:102)|τ |∞(cid:107)p − q(cid:107) ,

(cid:107)D − τ (D)(cid:107) = o((cid:102)|τ |∞) .

which, following the previous argument, leads to

C. Theorem 4.2

C.1. Proof of part (a)

The proof is based on the following lemma:

Lemma C.1 Let xN , yN ∈ H(MN ) such that ∀ N , (cid:107)xN (cid:107)H ≤ c,(cid:107)yN (cid:107)H ≤ c. Let ˆxN = EN (xN ), where EN is the
eigendecomposition of the Laplacian operator ∆N on MN , , with associated eigenvalues λ1 . . . λN in increasing order. Let
γ > 0 and β be deﬁned as in (5) for xN and yN . If β > 1 and (cid:107)xN − yN (cid:107) ≤ (cid:15) for all N ,

where C is a constant independent of (cid:15) and N .

(cid:107)∆N (xN − yN )(cid:107)2 ≤ C(cid:15)2− 1

β−1/2 ,

One layer of the network will transform the difference x1 − x2 into ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2). We verify that

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + (cid:107)B(cid:107)(cid:107)∆(x1 − x2)(cid:107) .

We now apply Lemma C.1 to obtain

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + C(cid:107)B(cid:107)(cid:107)x1 − x2(cid:107)

β−1
β−1/2

≤ (cid:107)x1 − x2(cid:107)

β−1
β−1/2

(cid:16)

(cid:107)A(cid:107)(cid:107)x1 − x2(cid:107)(2β−1)−1

+ C(cid:107)B(cid:107)

(cid:17)

≤ C((cid:107)A(cid:107) + (cid:107)B(cid:107))(cid:107)x1 − x2(cid:107)

β−1
β−1/2 ,

where we redeﬁne C to account for the fact that (cid:107)x1 − x2(cid:107)(2β−1)−1

is bounded. We have just showed that

with fr = C((cid:107)Ar(cid:107) + (cid:107)Br(cid:107)) and gr = βr−1

βr−1/2 . By cascading (20) for each of the R layers we thus obtain

(cid:107)x(r+1)
1

− x(r+1)
2

(cid:107) ≤ fr(cid:107)x(r)

1 − x(r)

2 (cid:107)gr

(cid:107)Φ∆(x1) − Φ∆(x2)(cid:107) ≤

(cid:35)

(cid:81)
f
r

r(cid:48)>r gr(cid:48)

(cid:107)x1 − x2(cid:107)

(cid:81)R

r=1 gr ,

(cid:34) R
(cid:89)

r=1

which proves (6) (cid:3).

Proof of (19): Let {e1, . . . , eN } be the eigendecomposition of ∆N . For simplicity, we drop the subindex N in the signals
from now on. Let ˆx(k) = (cid:104)x, ek(cid:105) and ˜x(k) = λk ˆx(k); and analogously for y. From the Parseval identity we have that
(cid:107)x(cid:107)2 = (cid:107)ˆx(cid:107)2. We express (cid:107)∆(x − y)(cid:107) as

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 .
λ2

(cid:88)

k≤N

The basic principle of the proof is to cut the spectral sum (22) in two parts, chosen to exploit the decay of ˜x(k). Let

F (x)(k) =

(cid:80)

k(cid:48)≥k ˜x(k)2
(cid:107)x(cid:107)2
H

=

(cid:80)

k(cid:48)≥k ˜x(k)2
k(cid:48) ˜x(k)2 =
(cid:80)

(cid:80)

k(cid:48)≥k λ2
(cid:80)
k(cid:48) λ2

k ˆx(k)2
k ˆx(k)2 ≤ 1 ,

(18)

(19)

(20)

(21)

(22)

and analogously for y. For any cutoff k∗ ≤ N we have

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 +
λ2

k(ˆx(k) − ˆy(k))2
λ2

(cid:88)

(cid:88)

k≤k∗

≤ λ2
k∗
≤ λ2
k∗
≤ λ2
k∗
where we denote for simplicity F (k∗) = max(F (x)(k∗), F (y)(k∗)). By assumption, we have λ2
k
F (k) (cid:46) (cid:88)

(cid:15)2 + 2(F (x)(k∗)(cid:107)x(cid:107)2
(cid:15)2 + 2F (k∗)((cid:107)x(cid:107)2
(cid:15)2 + 4F (k∗)D2 ,

k2(γ−β) (cid:39) k1+2(γ−β) .

H + (cid:107)y(cid:107)2

H)

H)

k>k∗
H + F (y)(k∗)(cid:107)y(cid:107)2

(cid:46) k2γ and

By denoting ˜β = β − γ − 1/2, it follows that

k(cid:48)≥k

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2k2γ

∗ + 4D2k−2 ˜β

∗

(cid:15)22γk2γ−1 − 2 ˜β4D2k−2 ˜β−1 = 0, thus
(cid:20) 4βD2
γ(cid:15)2

k∗ =

(cid:21) 1

2γ+2 ˜β

.

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2− 1

γ+ ˜β = (cid:15)2− 1

β−1/2 ,

Optimizing for k∗ yields

which proves part (a) (cid:3).

C.2. Proof of part (b)

We will use the following lemma:

By plugging (25) back into (24) and dropping all constants independent of N and (cid:15), this leads to

Lemma C.2 Let M = (V, E, F ) is a non-degenerate mesh, and deﬁne

η1(M) = sup

, η2(M) = sup

, η3(M) = αmin .

¯Ai
¯Aj

(i,j)∈E

ij + (cid:96)2
(cid:96)2

jk + (cid:96)2
ik

(i,j,k)∈F

A(i, j, k)

Then, given a smooth deformation τ and x deﬁned in M, we have

(cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) ,

where C depends only upon η1, η2 and η3.

In that case, we need to control the difference ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x). We verify that

(cid:107)ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x)(cid:107) ≤ (cid:107)B(cid:107)(cid:107)(∆ − τ (∆))x(cid:107) .

By Lemma C.2 it follows that (cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) and therefore, by denoting x(1)
x(1)
2 = ρ(Ax + Bτ (∆)x), we have

1 = ρ(Ax + B∆x) and

2 (cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) = C|∇τ |∞(cid:107)x(cid:107)H .

(28)

1 − x(1)
By applying again Lemma C.1, we also have that

(cid:107)x(1)

(cid:107)∆x(1)

1 − τ (∆)x(1)

2 (cid:107) = (cid:107)∆x(1)
= (cid:107)∆(x(1)
≤ C(cid:107)x(1)

1 − (∆ + τ (∆) − ∆)x(1)
2 (cid:107)
2 ) + (τ (∆) − ∆)x(1)
1 − x(1)
2 (cid:107)
1 − x(1)
β1−1/2 + |∇τ |∞(cid:107)x(1)
2 (cid:107)
β1−1
β1−1/2 ,

β1−1

(cid:46) C|∇τ |∞

2 (cid:107)H

(23)

(24)

(25)

(26)

(27)

which, by combining it with (28) and repeating through the R layers yields

(cid:107)Φ∆(x, M) − Φ∆(x, τ (M)(cid:107) ≤ C|∇τ |∞

(cid:81)R

r=1

βr −1
βr −1/2 ,

(29)

which concludes the proof (cid:3).

Proof of (27): The proof follows closely the proof of Theorem 4.1, part (c). From (13) and (14) we have that

τ ( ¯A) = ¯A(I + Gτ ) , with |Gτ |∞ ≤ C(η2, η3)|∇τ |∞ , and
τ (U − W ) = (I + Hτ )(U − W ) , with |Hτ |∞ ≤ C(η2, η3)|∇τ |∞ .

It follows that, up to second order o(|∇τ |∞

2) terms,

τ (∆) − ∆ = τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )

= (cid:0) ¯A[1 + Gτ ](cid:1)−1
(cid:39) ¯A−1Hτ (U − W ) + Gτ ∆ .

[(I + Hτ )(U − W )] − ¯A−1(U − W )

(30)

By writing ¯A−1Hτ = (cid:102)Hτ ¯A−1, and since ¯A is diagonal, we verify that

( (cid:102)Hτ )i,j = (Hτ )i,j

, with

Ai,i
Aj,j

Ai,i
Aj,j

≤ η1, and hence that

¯A−1Hτ (U − W ) = (cid:102)Hτ ∆ , with | (cid:102)Hτ |∞ ≤ C(η1, η2, η3)|∇τ |∞ .

(31)

We conclude by combining (30) and (31) into

(cid:107)(∆ − τ (∆))x(cid:107) = (cid:107)(Gτ + (cid:102)Hτ )∆x(cid:107)

≤ C (cid:48)(η1, η2, η3)|∇τ |∞(cid:107)∆x(cid:107) ,

which proves (27) (cid:3)

C.3. Proof of part (c)

This result is a consequence of the consistency of the cotangent Laplacian to the Laplace-Beltrami operator on S [45]:

Theorem C.3 ([45], Thm 3.4) Let M be a compact polyhedral surface which is a normal graph over a smooth surface S
with distortion tensor T , and let ¯T = (det T )1/2T −1. If the normal ﬁeld uniform distance d(T , 1) = (cid:107) ¯T − 1(cid:107)∞ satisﬁes
d(T , 1) ≤ (cid:15), then

(cid:107)∆M − ∆S(cid:107) ≤ (cid:15) .

(32)

If ∆M converges uniformly to ∆S, in particular we verify that

(cid:107)x(cid:107)H(M) → (cid:107)x(cid:107)H(S) .

Thus, given two meshes M, M(cid:48) approximating a smooth surface S in terms of uniform normal distance, and the corre-

sponding irregular sampling x and x(cid:48) of an underlying function ¯x : S → R, we have

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x − x(cid:48)(cid:107) + (cid:107)B(cid:107)(cid:107)∆Mx − ∆M(cid:48)x(cid:48)(cid:107) .

(33)

Since M and M(cid:48) both converge uniformly normally to S and ¯x is Lipschitz on S, it results that

thus (cid:107)x − x(cid:48)(cid:107) ≤ 2L(cid:15). Also, thanks to the uniform normal convergence, we also have convergence in the Sobolev sense:

(cid:107)x − ¯x(cid:107) ≤ L(cid:15) , and (cid:107)x(cid:48) − ¯x(cid:107) ≤ L(cid:15) ,

(cid:107)x − ¯x(cid:107)H (cid:46) (cid:15) , (cid:107)x(cid:48) − ¯x(cid:107)H (cid:46) (cid:15) ,

which implies in particular that

From (33) and (34) it follows that

(cid:107)x − x(cid:48)(cid:107)H (cid:46) (cid:15) .

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ 2(cid:107)A(cid:107)L(cid:15) +

(34)

(35)

+(cid:107)B(cid:107)(cid:107)∆Mx − ∆S ¯x + ∆S ¯x − ∆M(cid:48)x(cid:48)(cid:107)

≤ 2(cid:15) ((cid:107)A(cid:107)L + (cid:107)B(cid:107)) .

By applying again Lemma C.1 to ˜x = ρ(Ax + B∆Mx), ˜x(cid:48) = ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48)), we have

We conclude by retracing the same argument as before, reapplying Lemma C.1 at each layer to obtain

(cid:107)˜x − ˜x(cid:48)(cid:107)H ≤ C(cid:107)˜x − ˜x(cid:48)(cid:107)

β1−1
β1−1/2 (cid:46) (cid:15)

β1−1
β1−1/2 .

(cid:107)ΦM(x) − ΦM(cid:48)(x(cid:48))(cid:107) ≤ C(cid:15)

(cid:81)R

r=1

βr −1
βr −1/2 . (cid:3) .

D. Proof of Corollary 4.3

We verify that

(cid:107)ρ(B∆x) − ρ(Bτ (∆)τ (x))(cid:107) ≤ (cid:107)B(cid:107)(cid:107)∆x − τ (∆)τ (x)(cid:107)

≤ (cid:107)B(cid:107)(cid:107)∆(x − τ (x)) + (∆ − τ (∆))(τ (x))(cid:107)
≤ (cid:107)B(cid:107)((cid:107)∆(x − τ (x))(cid:107) + (cid:107)(∆ − τ (∆))(τ (x))(cid:107) .

The second term is o(|∇τ |∞) from Lemma C.2. The ﬁrst term is

(cid:107)x − τ (x)(cid:107)H ≤ (cid:107)∆(I − τ )(cid:107)(cid:107)x(cid:107) ≤ (cid:107)∇2τ (cid:107)(cid:107)x(cid:107) ,

where (cid:107)∇2τ (cid:107) is the uniform Hessian norm of τ . The result follows from applying the cascading argument from last section.
(cid:3)

E. Preliminary Study: Metric Learning for Dense Correspondence

As an interesting extension, we apply the architecture we built in Experiments 6.2 directly to a dense shape correspondence

problem.

Similarly as the graph correspondence model from [33], we consider a Siamese Surface Network, consisting of two
identical models with the same architecture and sharing parameters. For a pair of input surfaces M1, M2 of N1, N2 points
respectively, the network produces embeddings E1 ∈ RN1×d and E2 ∈ RN2×d. These embeddings deﬁne a trainable
similarity between points given by

si,j =

e(cid:104)E1,i,E2,j (cid:105)
j(cid:48) e(cid:104)E1,i,E2,j(cid:48) (cid:105)

,

(cid:80)

(36)

which can be trained by minimizing the cross-entropy relative to ground truth pairs. A diagram of the architecture is

provided in Figure 10.

In general, dense shape correspondence is a task that requires a blend of intrinsic and extrinsic information, motivating
the use of data-driven models that can obtain such tradeoffs automatically. Following the setup in Experiment 6.2, we use
models with 15 ResNet-v2 blocks with 128 output features each, and alternate Laplace and Dirac based models with Average
Pooling blocks to cover a larger context: The input to our network consists of vertex positions only.

We tested our architecture on a reconstructed (i.e. changing the mesh connectivity) version of the real scan of FAUST
dataset[3]. The FAUST dataset contains 100 real scans and their corresponding ground truth registrations. The ground truth
is based on a deformable template mesh with the same ordering and connectivity, which is ﬁtted to the scans. In order to
eliminate the bias of using the same template connectivity, as well as the need of a single connected component, the scans
are reconstructed again with [19]. To foster replicability, we release the processed dataset in the additional material. In our
experiment, we use 80 models for training and 20 models for testing.

Figure 10. Siamese network pipeline: the two networks take vertex coordinates of the input models and generate a high dimensional feature
vector, which are then used to deﬁne a map from M1 to M2. Here, the map is visualized by taking a color map on M2, and transferring
it on M1

Figure 11. Additional results from our setup. Plot in the middle shows rate of correct correspondence with respect to geodesic error [23].
We observe that Laplace is performing similarly to Dirac in this scenario. We believe that the reason is that the FAUST dataset contains
only isometric deformations, and thus the two operators have access to the same information. We also provide visual comparison, with the
transfer of a higher frequency colormap from the reference shape to another pose.

Since the ground truth correspondence is implied only through the common template mesh, we compute the correspon-
dence between our meshes with a nearest neighbor search between the point cloud and the reconstructed mesh. Consequently,

Figure 12. Heat map illustrating the point-wise geodesic difference between predicted correspondence point and the ground truth. The unit
is proportional to the geodesic diameter, and saturated at 10%.

Figure 13. A failure case of applying the Laplace network to a new pose in the FAUST benchmark dataset. The network confuses between
left and right arms. We show the correspondence visualization for front and back of this pair.

due to the drastic change in vertex replacement after the remeshing, only 60-70 percent of labeled matches are used. Although
making it more challenging, we believe this setup is close to a real case scenario, where acquisition noise and occlusions are
unavoidable.

Our preliminary results are reported in Figure 11. For simplicity, we generate predicted correspondences by simply taking
the mode of the softmax distribution for each reference node i: ˆj(i) = arg maxj si,j, thus avoiding a reﬁnement step that is
standard in other shape correspondence pipelines. The MLP model uses no context whatsoever and provides a baseline that
captures the prior information from input coordinates alone. Using contextual information (even extrinsically as in point-
cloud model) brings signiﬁcative improvments, but these results may be substantially improved by encoding further prior
knowledge. An example of the current failure of our model is depitcted in Figure 13, illustrating that our current architecture
does not have sufﬁciently large spatial context to disambiguate between locally similar (but globally inconsistent) parts.

We postulate that the FAUST dataset [3] is not an ideal ﬁt for our contribution for two reasons: (1) it is small (100

models), and (2) it contains only near-isometric deformations, which do not require the generality offered by our network. As
demonstrated in [28], the correspondence performances can be dramatically improved by constructing basis that are invariant
to the deformations. We look forward to the emergence of new geometric datasets, and we are currently developing a capture
setup that will allow us to acquire a more challenging dataset for this task.

F. Further Numerical Experiments

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 14. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 15. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 16. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 17. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

Laplace

Dirac

Figure 18. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Figure 19. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

Figure 20. From left to right: set-to-set, ground truth and Dirac based model. Color corresponds to mean squared error between ground
truth and prediction: green - smaller error, red - larger error.

8
1
0
2
 
n
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
1
8
0
1
.
5
0
7
1
:
v
i
X
r
a

Surface Networks

Ilya Kostrikov1, Zhongshi Jiang1, Daniele Panozzo ∗1, Denis Zorin †1, and Joan Bruna‡1,2

1Courant Institute of Mathematical Sciences, New York University
2Center for Data Science, New York University

Abstract

We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used
to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs,
namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the
Laplacian operator.

Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric
deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to
GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In
particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions — this is in stark
contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface
Networks (SN).

We prove that these models deﬁne shape representations that are stable to deformation and to discretization, and we
demonstrate the efﬁciency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under
non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by
SNs.

1. Introduction

3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering
to computer animation to medical imaging. Despite the vast amount of high-quality 3D geometric data available, data-
driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data
representation regularity which is required for traditional convolutional neural network approaches. While in computer vision
problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more
complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar
chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical
properties.

Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike
the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the
inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary
features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations.
Extrinsic representations rely on the speciﬁc embedding of surfaces within a three-dimensional ambient space, whereas
intrinsic representations only capture geometric properties speciﬁc to the surface, irrespective of its parametrization. Whereas
the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local
deformations and invariance to global transformations.

∗DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.
†DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.
‡JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding

author: bruna@cims.nyu.edu

1

A particularly simple and popular extrinsic method [36, 37] represents shapes as point clouds in R3 of variable size,
and leverages recent deep learning models that operate on input sets [44, 43]. Despite its advantages in terms of ease
of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classiﬁcation
and segmentation tasks, one may wonder whether this simpliﬁcation comes at a loss of precision as one considers more
challenging prediction tasks.

In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on
geometric deep learning. These models provide data-driven intrinsic graph and manifold representations with inductive
biases analogous to CNNs on natural images. Models based on Graph Neural Networks [40] and their spectral variants
[6, 11, 26] have been successfully applied to geometry processing tasks such as shape correspondence [32]. In their basic
form, these models learn a deep representation over the discretized surface by combining a latent representation at a given
node with a local linear combination of its neighbors’ latent representations, and a point-wise nonlinearity. Different models
vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to
spectral interpretations of those models.

Our contributions are three-fold. First, we extend the model to support extrinsic features. More speciﬁcally, we exploit the
fact that surfaces in R3 admit a ﬁrst-order differential operator, the Dirac operator, that is stable to discretization, provides a
direct generalization of Laplacian-based propagation models, and is able to detect principal curvature directions [9, 18]. Next,
we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization,
two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the
variational autoencoder framework [25, 39], that is able to exploit non-Euclidean geometric regularity.

By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation
that we apply to several challenging tasks. The resulting Surface Networks – using either the Dirac or the Laplacian, inherit
the stability and invariance properties of these operators, thus providing data-driven representations with certiﬁed stability to
deformations. We demonstrate the model efﬁciency on a temporal prediction task of complex dynamics, based on a physical
simulation of elastic shells, which conﬁrms that whenever geometric information (in the form of a mesh) is available, it can
be leveraged to signiﬁcantly outperform point-cloud based models.

Our main contributions are summarized as follows:

• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear

dynamics, motivating the use of geometric shape information.

• We prove that Surface Networks deﬁne shape representations that are stable to deformation and to discretization.

• We introduce a generative model for 3D surfaces based on the variational autoencoder.

2. Related Work

Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research.
[40] introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained
by backpropagation. Subsequent works [27, 43] have relaxed the model by untying the recurrent layer weights and proposed
several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convo-
lutional networks to non-Euclidean graphs. [6, 17] proposed to learn smooth spectral multipliers of the graph Laplacian,
albeit with high computational cost, and [11, 26] resolved the computational bottleneck by learning polynomials of the graph
Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to
[5] for an exhaustive literature review on the topic. GNNs are ﬁnding application in many different domains. [2, 7] develop
graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics.
[12, 22] study molecular ﬁngerprints using variants of the GNN architecture, and [14] further develop the model by combin-
ing it with set representations [44], showing state-of-the-art results on molecular prediction. The resulting models, so-called
Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model
on general graphs.

In the context of computer graphics, [30] developed the ﬁrst CNN model on meshed surfaces using intrinsic patch rep-
resentations, and further generalized in [4] and [32]. This last work allows for ﬂexible representations via the so-called
pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode ﬁrst-
order differential information. These intrinsic models contrast with Euclidean models such as [48, 46], that have higher
sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models

A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/SurfaceNetworks.

are increasingly popular to model 3d objects due to their simplicity and versatility. [36, 37] use set-invariant representations
from [44, 43] to solve shape segmentation and classiﬁcation tasks. More recently, [29] proposes to learn surface convolutional
network from a canonical representation of planar ﬂat-torus, with excellent performance on shape segmentation and classi-
ﬁcation, although such canonical representations may introduce exponential scale changes that can introduce instabilities.
Finally, [13] proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but
does not encode geometrical information as our shape generative model. Learning variational deformations is an important
problem for graphics applications, since it enables negligible and ﬁxed per-frame cost [35], but it is currently limited to 2D
deformations using point handles. In constrast, our method easily generalizes to 3D and learns dynamic behaviours.

3. Surface Networks

This section presents our surface neural network model and its basic properties. We start by introducing the problem setup
and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section
3.2).

3.1. Laplacian Surface Networks

Our ﬁrst goal is to deﬁne a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where
V = (vi ∈ R3)i≤N contains the node coordinates, E = (ei,j) corresponds to edges, and F is the set of triangular faces. We
denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see [5] for details).
This operator can be interpreted as a local, linear high-pass ﬁlter in M that acts on signals x ∈ Rd×|V | deﬁned on the
vertices as a simple matrix multiplication ˜x = ∆x. By combining ∆ with an all-pass ﬁlter and learning generic linear
combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators
in M that update a feature map from layer k to layer k + 1 using trainable parameters Ak and Bk:

xk+1 = ρ (cid:0)Ak∆xk + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk .

By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization,
this model is a speciﬁc instance of the graph neural network [40, 5, 26] and a generalization of the spectrum-free Laplacian
networks from [11]. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening
or pooling layers.

In contrast to general graphs, meshes contain a low-dimensional Euclidean embedding that contains potentially useful
information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A
simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as
input channels to the network: x1 := V ∈ R|V |×3. The mean curvature can be computed by applying the Laplace operator
to the coordinates of the vertices:

∆x1 = −2Hn ,

where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian
neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph
neural network models is related to the use of generalized coordinates from [32]. By cascading K layers of the form (1) we
obtain a representation Φ∆(M) that contains generic features at each node location. When the number of layers K is of
the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate
information across the whole surface.

Equation (2) illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to
the mean variations across all directions. Although in general graphs there is no well-deﬁned procedure to recover anisotropic
local variations, in the case of surfaces some authors ([4, 1, 32]) have considered anisotropic extensions. We describe next
a particularly simple procedure to increase the expressive power of the network using a related operator from quantum
mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation [9] and shape
analysis [18].

(1)

(2)

3.2. Dirac Surface Networks

The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the
gradient (a ﬁrst-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to
these ﬁrst-order differential operators separately, enabling oriented high-pass ﬁlters.

For convenience, we embed R3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for
details). The Dirac operator is then deﬁned as a matrix D ∈ H|F |×|V | that maps (quaternion) signals on the nodes to signals
on the faces. In coordinates,

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area (see Appendix A) using counter-clockwise
orientations on all faces.

To apply the Dirac operator deﬁned in quaternions to signals in vertices and faces deﬁned in real numbers, we write the
feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a
quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The
Dirac operator provides ﬁrst-order differential information and is sensitive to local orientations. Moreover, one can verify [9]
that

Re D∗D = ∆ ,
where D∗ is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as
D∗ = M −1
V DH MF where DH is a conjugate transpose of D and MV , MF are diagonal mass matrices with one third of
areas of triangles incident to a vertex and face areas respectively.

The Dirac operator can be used to deﬁne a new neural surface representation that alternates layers with signals deﬁned
over nodes with layers deﬁned over faces. Given a d-dimensional feature representation over the nodes xk ∈ Rd×|V |, and the
faces of the mesh, yk ∈ Rd×|F |, we deﬁne a d(cid:48)-dimensional mapping to a face representation as

yk+1 = ρ (cid:0)CkDxk + Ekyk(cid:1) , Ck, Ek ∈ Rdk+1×dk ,

(3)

where Ck, Ek are trainable parameters. Similarly, we deﬁne the adjoint layer that maps back to a ˜d-dimensional signal over
nodes as

xk+1 = ρ (cid:0)AkD∗yk+1 + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk ,
where Ak, Bk are trainable parameters. A surface neural network layer is thus determined by parameters {A, B, C, E} using
equations (3) and (4) to deﬁne xk+1 ∈ Rdk+1×|V |. We denote by ΦD(M) the mesh representation resulting from applying
K such layers (that we assume ﬁxed for the purpose of exposition).

(4)

The Dirac-based surface network is related to edge feature transforms proposed on general graphs in [14], although these
edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no
notion of square root of ∆ that recovers oriented ﬁrst-order derivatives.

4. Stability of Surface Networks

Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise
under the model. Given a continuous surface S ⊂ R3 or a discrete mesh M, and a smooth deformation ﬁeld τ : R3 → R3,
we are particularly interested in two forms of stability:

• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that (cid:107)Φ(M) −

Φ(τ (M))(cid:107) is small if (cid:107)∇τ (∇τ )∗ − I(cid:107) is small, i.e when the deformation is nearly rigid; see Theorem 4.1.

• Given two discretizations M1 and M2 of the same underlying surface S, we would like to control (cid:107)Φ(M1)−Φ(M2)(cid:107)

in terms of the resolution of the meshes; see Theorem 4.2.

These stability properties are important in applications, since most tasks we are interested in are stable to deformation and
to discretization. We shall see that the ﬁrst property is a simple consequence of the fact that the mesh Laplacian and Dirac
operators are themselves stable to deformations. The second property will require us to specify under which conditions the
discrete mesh Laplacian ∆M converges to the Laplace-Beltrami operator ∆S on S. Unless it is clear from the context, in the
following ∆ will denote the discrete Laplacian.

Theorem 4.1 Let M be a N -node mesh and x, x(cid:48) ∈ R|V |×d be input signals deﬁned on the nodes. Assume the nonlinearity
ρ( · ) is non-expansive (|ρ(z) − ρ(z(cid:48))| ≤ |z − z(cid:48)|). Then

(a) (cid:107)Φ∆(M; x) − Φ∆(M; x(cid:48))(cid:107) ≤ α∆(cid:107)x − x(cid:48)(cid:107) , where α∆ depends only on the trained weights and the mesh.

(b) (cid:107)ΦD(M; x) − ΦD(M; x(cid:48))(cid:107) ≤ αD(cid:107)x − x(cid:48)(cid:107) , where αD depends only on the trained weights and the mesh.

(c) Let |∇τ |∞ := supu (cid:107)∇τ (u)(∇τ (u))∗ − 1(cid:107), where ∇τ (u) is the Jacobian matrix of u (cid:55)→ τ (u). Then (cid:107)Φ∆(M; x) −

Φ∆(τ (M); x)(cid:107) ≤ β∆|∇τ |∞(cid:107)x(cid:107) , where β∆ is independent of τ and x.

(d) Denote by (cid:103)|∇τ |∞ := supu (cid:107)∇τ (u) − 1(cid:107). Then (cid:107)ΦD(M; x) − ΦD(τ (M); x)(cid:107) ≤ βD (cid:103)|∇τ |∞(cid:107)x(cid:107) , where βD is

independent of τ and x.

Properties (a) and (b) are not speciﬁc to surface representations, and are a simple consequence of the non-expansive property
of our chosen nonlinearities. The constant α is controlled by the product of (cid:96)2 norms of the network weights at each layer and
the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators
are themselves stable to deformations, a property that depends on two key aspects: ﬁrst, the Laplacian/Dirac is localized in
space, and next, that it is a high-pass ﬁlter and therefore only depends on relative changes in position.

One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the
reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral
radius of ∆M diverges as the mesh size N increases.

In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incom-
ing to the surface network. This can be measured with Sobolev norms deﬁned using the spectrum of the Laplacian operator.
Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆M, consider
the spectral decomposition of ∆M (a symmetric, positive deﬁnite operator):

∆M =

λkekeT

k , ek ∈ RN , 0 ≤ λ1 ≤ λ2 · · · ≤ λN .

(cid:88)

k≤N

Under normal uniform convergence 1 [45], the spectrum of ∆M converges to the spectrum of the Laplace-Beltrami operator
∆S of S. If S is bounded, it is known from the Weyl law [47] that there exists γ > 0 such that k−γ(S) (cid:46) λ−1
k , so the
eigenvalues λk do not grow too fast. The smoothness of a signal x ∈ R|V |×d deﬁned in M is captured by how fast its
H := (cid:80)
spectral decomposition ˆx(k) = eT
k λ(k)2(cid:107)ˆx(k)(cid:107)2 is Sobolev norm, and
β(x, S) > 1 as the largest rate such that its spectral decomposition coefﬁcients satisfy

k x ∈ Rd decays [42]. We deﬁne (cid:107)x(cid:107)2

(cid:107)ˆx(k)(cid:107) (cid:46) k−β , (k → ∞) .
(5)
If x ∈ R|V |×d is the input to the Laplace Surface Network of R layers, we denote by (β0, β1, . . . , βR−1) the smoothness
rates of the feature maps x(r) deﬁned at each layer r ≤ R.

Theorem 4.2 Consider a surface S and a ﬁnite-mesh approximation MN of N points, and Φ∆ a Laplace Surface Network
with parameters {(Ar, Br)}r≤R. Denote by d(S, MN ) the uniform normal distance, and let x1, x2 be piece-wise polyhedral
approximations of ¯x(t), t ∈ S in MN , with (cid:107)¯x(cid:107)H(S) < ∞. Assume (cid:107)¯x(r)(cid:107)H(S) < ∞ for r ≤ R.

(a) If x1, x2 are two functions such that the R feature maps x(r)

have rates (β0, β1, . . . , βR−1), then

l
(cid:107)Φ∆(x1; MN ) − Φ∆(x2; MN )(cid:107)2 ≤ C(β)(cid:107)x1 − x2(cid:107)h(β) ,

(6)

with h(β) = (cid:81)R

βr−1
βr−1/2 , and where C(β) does not depend upon N .

r=1

(b) If τ is a smooth deformation ﬁeld, then (cid:107)Φ∆(x; MN ) − Φ∆(x; τ (MN ))(cid:107) ≤ C|∇τ |∞

h(β) , where C does not depend

upon N .

(c) Let M and M(cid:48) be N -point discretizations of S, If max(d(M, S), d(M(cid:48), S)) ≤ (cid:15), then (cid:107)Φ∆(M; x) − Φ∆(M(cid:48), x(cid:48))(cid:107) ≤

C(cid:15)h(β) , where C is independent of N .

This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases,
the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac
operator also has a well-deﬁned continuous counterpart [9] that generalizes the gradient operator in quaternion space. Also,
our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in
terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x).
These extensions are left for future work. Finally, a speciﬁc setup that we use in experiments is to use as input signal the
canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields

Corollary 4.3 Denote Φ(M) := ΦM(V ), where V are the node coordinates of M. Then, if A1 = 0,

(cid:107)Φ(M) − Φ(τ (M))(cid:107) ≤ κ max(|∇τ |∞, (cid:107)∇2τ (cid:107))h(β) .

(7)

1which controls how the normals of the mesh align with the surface normals; see [45].

Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R3 (right) is expressed in terms of a “sampling” 2D irregular mesh
˜M ⊂ R2 (left) and a depth scalar ﬁeld f : ˜M → R over ˜M (center).

5. Generative Surface Models

State-of-the-art generative models for images, such as generative adversarial networks [38], pixel autoregressive networks
[34], or variational autoencoders [25], exploit the locality and stationarity of natural images in their probabilistic models, in
the sense that the model satisﬁes pθ(x) ≈ pθ(xτ ) by construction, where xτ is a small deformation of a given input x. This
property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric
stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains
two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface,
which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas
the former contains the essential semantic information, the latter is not informative, and to some extent independent of the
shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as
height-ﬁeld meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as ( ˜M, f ( ˜M)), where ˜M = ( ˜V , ˜E, ˜F ) is
now a 2D mesh and f : ˜V → R is a depth-map encoding the original node locations V , as shown in Figure 1.

In this work, we consider the variational autoencoder framework [25, 39].

It considers a mixture model of the form
p(M) = (cid:82) pθ(M | h)p0(h)dh , where h ∈ R|S| is a vector of latent variables. We train this model by optimizing the
variational lower bound of the data log-likelihood:

min
θ,ψ

1
L

(cid:88)

l≤L

−Eh∼qψ (h | Ml) log pθ(Ml | h) + DKL(qψ(h | Ml) || p0(h)) .

(8)

We thus need to specify a conditional generative model pθ(M | h), a prior distribution p0(h) and a variational approximation
to the posterior qψ(h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the
height-ﬁeld representation, we choose for simplicity a separable model of the form pθ(M | h) = pθ(f | h, ˜M) · p( ˜M) ,
where ˜M ∼ p( ˜M) is a homogeneous Poisson point process, and f ∼ pθ(f | h, ˜M) is a normal distribution with mean and
isotropic covariance parameters given by a SN:

pθ(f | h, ˜M) = N (µ(h, ˜M), σ2(h, ˜M)1) ,

with [µ(h, ˜M), σ2(h, ˜M)] = ΦD( ˜M ; h) . The generation step thus proceeds as follows. We ﬁrst sample a 2D mesh ˜M
independent of the latent variable h, and then sample a depth ﬁeld over ˜M conditioned on h from the output of a decoder
network ΦD( ˜M ; h). Finally, the variational family qψ is also a Normal distribution whose parameters are obtained from an
encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization: qψ(h | M) =
N (¯µ, ¯σ21) , with [¯µ, ¯σ] = ¯ΦD(M) .

6. Experiments

6.1. MeshMNIST

For experimental evaluation, we compare models built using ResNet-v2 blocks [16], where convolutions are replaced with
the appropriate operators (see Fig. 2): (i) a point cloud based model from [43] that aggregates global information by averaging
features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical
coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant
of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).

For this task, we construct a MeshMNIST database with only height-ﬁeld meshes (Sec. 5). First, we sample points on
a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a
Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each

Figure 2. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to
the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch
Normalization (blue) and ‘1 × 1’ convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language
and denote by xk+1 the output of this 2-layer block.

Receptive ﬁeld Number of parameters

Model
MLP
PointCloud
Laplace
Dirac

1
-
16
8

519672
1018872
1018872
1018872

Smooth L1-loss (mean per sequence (std))
64.56 (0.62)
23.64 (0.21)
17.34 (0.52)
16.84 (0.16)

Table 1. Evaluation of different models on the temporal task

point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to deﬁne a sampling process
over 3D height-ﬁeld meshes.

We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder
converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to
obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a speciﬁc 3D
mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does
not depend on input data. We trained the model for 75 epochs using Adam optimizer [24] with learning rate 10−3, weight
decay 10−5 and batch size 32. Figures 3,4 illustrate samples from the model. The geometric encoder is able to leverage the
local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to
the speciﬁc sampled grid, as opposed to set-based generative models.

Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of dis-
cretization/triangulation (Poisson disk sampling with p=1.5).

Figure 4. Meshes from the dataset (ﬁrst ﬁve). And meshes generated by our model (last ﬁve).

6.2. Spatio-Temporal Predictions

One speciﬁc task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1, X 2, . . . , X n,

the task is to predict the following frames Y = Y 1 = X n+1, Y 2, . . . , Y m = X n+m. As in [31], we use a simple non-
recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on
n = 2 frames and predict the next m = 40 frames. In order to generate data, we ﬁrst extracted 10k patches from the
MPI-Faust dataset[3], by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of
the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground.
We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique [41], with additional
gravitational forces [20]. Libigl [21] has been used for the mesh processing tasks. Sequences with patches from the ﬁrst 80
subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on
request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial
frames. Thus, we can train models by minimizing smooth-L1 loss [15] between target frames and output of our models.

Ground Truth

MLP

PointCloud

Laplace

Dirac

Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set.
Boxes indicate distinctive features. For larger crops, see Figure 6

.

We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and
Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned
frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce
120-dimensional outputs. We trained all models using the Adam optimizer [24] with learning rate 10−3, weight decay 10−5,
and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained
for 110k steps in overall.

Ground Truth

Laplace

Dirac

Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Table 1 reports quantitative prediction performance of different models, and Figure 5 displays samples from the prediction

Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between
ground truth and prediction: green - smaller error, red - larger error.

Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

models at speciﬁc frames. The set-to-set model [44, 43], corresponding to a point-cloud representation used also in [36],
already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between
this model and Laplace-/Dirac-based models is signiﬁcant, both visually and quantitatively. Dirac-based model outperforms
Laplace-based model despite the smaller receptive ﬁeld. Videos comparing the performance of different models are available
in the additional material.

Figure 6 illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since
they propagate information using an isotropic operator, have more difﬁculties at resolving corners and pointy structures than
the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit
the extrinsic information only via the input coordinates is remarkable and more computationally efﬁcient than the Dirac
counterpart. Figures 7 and 8 overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac
respectively. They conﬁrm ﬁrst that SNs outperform the point-cloud based model, which often produce excessive ﬂattening
and large deformations, and next that ﬁrst-order Dirac operators help resolve areas with high directional curvature. We refer
to the supplementary material for additional qualitative results.

7. Conclusions

We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the non-Euclidean
geometry of surfaces. We have shown how a ﬁrst-order differential operator (the Dirac operator) can detect and adapt to
geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction
is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments.
That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting
to instead learn the operator, akin to recent Message-Passing NNs [14] and explore whether Dirac is recovered.

Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly
superior performance to point-cloud based models. While there are not many such datasets currently available, we expect
them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated
in consumer devices. SNs provide efﬁcient inference, with predictable runtime, which makes them appealing across many
areas of computer graphics, where a ﬁxed, per-frame cost is required to ensure a stable framerate, especially in VR applica-
tions. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and
generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.

References

2014. 3

[1] M. Andreux, E. Rodol`a, M. Aubry, and D. Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In Proc. NORDIA,

[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In

Advances in Neural Information Processing Systems, pages 4502–4510, 2016. 2

[3] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 3794–3801, 2014. 7, 18, 20

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks.

In Advances in Neural Information Processing Systems, pages 3189–3197, 2016. 2, 3

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv

preprint arXiv:1611.08097, 2016. 2, 3, 14

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013. 2
[7] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics.

[8] D. Chen and J. R. Gilbert. Obtaining bounds on the two norm of a matrix from the splitting lemma. Electronic Transactions on

[9] K. Crane, U. Pinkall, and P. Schr¨oder. Spin transformations of discrete surfaces. In ACM Transactions on Graphics (TOG). ACM,

ICLR, 2016. 2

2011. 2, 3, 4, 5

Numerical Analysis, 21:28–46, 2005. 14

and Its Applications, 427(1):55–69, 2007. 12

[10] K. C. Das. Extremal graph characterization from the upper bound of the laplacian spectral radius of weighted graphs. Linear Algebra

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In

Advances in Neural Information Processing Systems, pages 3837–3845, 2016. 2, 3

[12] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convo-

lutional networks on graphs for learning molecular ﬁngerprints. In Neural Information Processing Systems, 2015. 2

[13] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. arXiv preprint

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. 7
[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision,

arXiv:1612.00603, 2016. 3

arXiv:1704.01212, 2017. 2, 4, 10

pages 630–645. Springer, 2016. 6

[17] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015. 2
[18] K. C. Hsueh-Ti Derek Liu, Alec Jacobson. A dirac operator for extrinsic shape analysis. Computer Graphics Forum, 2017. 2, 3
[19] Y. Hu, Q. Zhou, X. Gao, A. Jacobson, D. Zorin, and D. Panozzo. Tetrahedral meshing in the wild. Submitted to ACM Transaction on

Graphics, 2018. 18

[20] A. Jacobson. Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes. PhD thesis, ETH, Z¨urich, 2013. 7
[21] A. Jacobson, D. Panozzo, et al. libigl: A simple C++ geometry processing library, 2016. http://libigl.github.io/libigl/. 7
[22] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal

of computer-aided molecular design, 2016. 2

[23] V. G. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic maps. In ACM Transactions on Graphics (TOG), volume 30, page 79.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representation, 2015.

[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 6
[26] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907,

ACM, 2011. 19

7, 8

2016. 2, 3

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. 2
[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape

correspondence. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5660–5668, 2017. 21

[29] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. Kim, and Y. Lipman. Convolutional neural networks on surfaces

via seamless toric covers. In SIGGRAPH, 2017. 3

[30] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In

Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. 2

[31] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error.

arXiv preprint

arXiv:1511.05440, 2015. 7

[32] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds

using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. 2, 3

[33] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna. A note on learning algorithms for quadratic assignment with graph neural networks.

arXiv preprint arXiv:1706.07450, 2017. 18

[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. 6
[35] R. Poranne and Y. Lipman. Simple approximations of planar deformation operators. Technical report, ETHZ, 2015. 3
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. arXiv preprint

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint

arXiv:1612.00593, 2016. 2, 3, 9

arXiv:1706.02413, 2017. 2, 3

arXiv preprint arXiv:1511.06434, 2015. 6

Neural Networks, 20(1):61–80, 2009. 2, 3

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks.

[39] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint arXiv:1505.05770, 2015. 2, 6
[40] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on

[41] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, 2007. 7
[42] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE

[43] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation.

In Advances in Neural Information

Symposium on, pages 29–38. IEEE, 2007. 5

Processing Systems, pages 2244–2252, 2016. 2, 3, 6, 9

[44] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015. 2, 3, 9
[45] M. Wardetzky. Convergence of the cotangent formula: An overview. In Discrete Differential Geometry, pages 275–286. 2008. 5, 12,

17

[46] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolutional networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1544–1553, 2016. 2

[47] H. Weyl.

¨Uber die asymptotische verteilung der eigenwerte. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottingen,

Mathematisch-Physikalische Klasse, 1911:110–117, 1911. 5

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 2

A. The Dirac Operator

The quaternions H is an extension of complex numbers. A quaternion q ∈ H can be represented in a form q = a+bi+cj +

dk where a, b, c, d are real numbers and i, j, k are quaternion units that satisfy the relationship i2 = j2 = k2 = ijk = −1.

As mentioned in Section 3.1, the Dirac operator used in the model can be conveniently represented as a quaternion matrix:

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area, as illustrated in Fig. A, using counter-
clockwise orientations on all faces.

vj

f

ej







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c













b
a
−b
a
−c −d
−d

d
c
d −c
b
a
a
c −b







.

The Deep Learning library PyTorch that we used to implement the models does not support quaternions. Nevertheless,
quaternion-valued matrix multiplication can be replaced with real-valued matrix multiplication where each entry q = a +
bi + cj + dk is represented as a 4 × 4 block

and the conjugate q∗ = a − bi − cj − dk is a transpose of this real-valued matrix:

B. Theorem 4.1

B.1. Proof of (a)

Laplacian ∆ of M is

We ﬁrst show the result for the mapping x (cid:55)→ ρ (Ax + B∆x), corresponding to one layer of Φ∆. By deﬁnition, the

∆ = diag( ¯A)−1(U − W ) ,
where ¯Aj is one third of the total area of triangles incident to node j, and W = (wi,j) contains the cotangent weights [45],
and U = diag(W 1) contains the node aggregated weights in its diagonal.

From [10] we verify that

(cid:107)U − W (cid:107) ≤

√

2 max
i

(cid:115)






U 2

i + Ui

Ujwi,j

(cid:88)

i∼j






√

√

≤ 2

2 sup
i,j

wi,j sup

dj

j

≤ 2

2 cot(αmin)dmax ,

(cid:107)∆(cid:107) ≤ C

cot(αmin)Smax
inf j ¯Aj

:= LM ,

where dj denotes the degree (number of neighbors) of node j, αmin is the smallest angle in the triangulation of M and Smax
the largest number of incident triangles. It results that

which depends uniquely on the mesh M and is ﬁnite for non-degenerate meshes. Moreover, since ρ( · ) is non-expansive, we
have

(cid:107)ρ (Ax + B∆x) − ρ (Ax(cid:48) + B∆x(cid:48))(cid:107) ≤ (cid:107)A(x − x(cid:48)) + B∆(x − x(cid:48))(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)LM)(cid:107)x − x(cid:48)(cid:107) .

By cascading (10) across the K layers of the network, we obtain

(cid:107)Φ(M; x) − Φ(M; x(cid:48))(cid:107) ≤

((cid:107)Ak(cid:107) + (cid:107)Bk(cid:107)LM)


 (cid:107)x − x(cid:48)(cid:107) ,





(cid:89)

k≤K

(9)

(10)

which proves (a). (cid:3)

B.2. Proof of (b)

B.3. Proof of (c)

The proof is analogous, by observing that (cid:107)D(cid:107) = (cid:112)(cid:107)∆(cid:107) and therefore

(cid:107)D(cid:107) ≤

(cid:112)

LM . (cid:3)

To establish (c) we ﬁrst observe that given three points p, q, r ∈ R3 forming any of the triangles of M,

(cid:107)p − q(cid:107)2(1 − |∇τ |∞)2

≤ (cid:107)τ (p) − τ (q)(cid:107)2 ≤

(cid:107)p − q(cid:107)2(1 + |∇τ |∞)2

A(p, q, r)2(1 − |∇τ |∞Cα−2

min − o(|∇τ |∞

2) ≤ A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2(1 + |∇τ |∞Cα−2

min + o(|∇τ |∞

2)) .

(11)

(12)

Indeed, (11) is a direct consequence of the lower and upper Lipschitz constants of τ (u), which are bounded respectively by
1 − |∇τ |∞ and 1 + |∇τ |∞. As for (12), we use the Heron formula

A(p, q, r)2 = s(s − (cid:107)p − q(cid:107))(s − (cid:107)p − r(cid:107))(s − (cid:107)r − q(cid:107)) ,

with s = 1
determined by the deformed points τ (p), τ (q), τ (r), we have that

2 ((cid:107)p − q(cid:107) + (cid:107)p − r(cid:107) + (cid:107)r − q(cid:107)) being the half-perimeter. By denoting sτ the corresponding half-perimeter

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≤ s(1 + |∇τ |∞) − (cid:107)p − q(cid:107)(1 − |∇τ |∞) = s − (cid:107)p − q(cid:107) + |∇τ |∞(s + (cid:107)p − q(cid:107)) and

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≥ s(1 − |∇τ |∞) − (cid:107)p − q(cid:107)(1 + |∇τ |∞) = s − (cid:107)p − q(cid:107) − |∇τ |∞(s + (cid:107)p − q(cid:107)) ,

and similarly for the (cid:107)r − q(cid:107) and (cid:107)r − p(cid:107) terms. It results in

A(τ (p), τ (q), τ (r))2 ≥ A(p, q, r)2
≥ A(p, q, r)2 (cid:104)

(cid:20)
1 − |∇τ |∞

(cid:18)

1 − C|∇τ |∞α−2

1 +

+

s + (cid:107)p − q(cid:107)
s − (cid:107)p − q(cid:107)
(cid:105)
2)

min − o(|∇τ |∞

,

s + (cid:107)p − r(cid:107)
s − (cid:107)p − r(cid:107)

+

s + (cid:107)r − q(cid:107)
s − (cid:107)r − q(cid:107)

(cid:19)

− o(|∇τ |∞

(cid:21)
2)

and similarly

A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2 (cid:104)

1 + C|∇τ |∞α−2

min − o(|∇τ |∞

(cid:105)

2)

.

By noting that the cotangent Laplacian weights can be written (see Fig. 9) as

wi,j =

−(cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
A(i, j, k)

+

−(cid:96)2

jh + (cid:96)2
ih

ij + (cid:96)2
A(i, j, h)

,

we have from the previous Bilipschitz bounds that

τ (wi,j) ≤ wi,j

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

+ 2|∇τ |∞

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

τ (wi,j) ≥ wi,j

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

− 2|∇τ |∞

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

+

+

(cid:33)

(cid:33)

,

,

which proves that, up to second order terms, the cotangent weights are Lipschitz continuous to deformations.

Finally, since the mesh Laplacian operator is constructed as diag( ¯A)−1(U − W ), with ¯Ai,i = 1
3

(cid:80)

j,k;(i,j,k)∈F A(i, j, k),

and U = diag(W 1), let us show how to bound (cid:107)∆ − τ (∆)(cid:107) from

¯Ai,i(1 − αM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ ( ¯Ai,i) ≤ ¯Ai,i(1 + αM|∇τ |∞ + o(|∇τ |∞

2))

and

wi,j(1 − βM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ (wi,j) ≤ wi,j(1 + βM|∇τ |∞ + o(|∇τ |∞

2)) .

(13)

(14)

k

αij

aijk

j

(cid:96)ij

ai

i

βij

h

Figure 9. Triangular mesh and Cotangent Laplacian (ﬁgure reproduced from [5])

Using the fact that ¯A, τ ( ¯A) are diagonal, and using the spectral bound for k × m sparse matrices from [8], Lemma 5.12,

(cid:107)Y (cid:107)2 ≤ max

i

(cid:88)

j; Yi,j (cid:54)=0

|Yi,j|

|Yr,j|

,

(cid:33)

(cid:32) l

(cid:88)

r=1

the bounds (13) and (14) yield respectively

τ ( ¯A) = ¯A(1 + (cid:15)τ ) , with (cid:107)(cid:15)τ (cid:107) = o(|∇τ |∞) , and
τ (U − W ) = U − W + ητ , with (cid:107)ητ (cid:107) = o(|∇τ |∞) .

It results that, up to second order terms,
(cid:13)τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )(cid:13)
(cid:107)∆ − τ (∆)(cid:107) = (cid:13)
(cid:13)
(cid:13)
(cid:0) ¯A[1 + (cid:15)τ ](cid:1)−1
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
=
(cid:13)
= (cid:13)
(cid:13)(cid:15)τ ∆ + ¯A−1ητ
= o(|τ |∞) ,

(cid:13)
(cid:13) + o(|∇τ |∞

1 − (cid:15)τ + o(|∇τ |∞

2)

=

[U − W + ητ ] − ¯A−1(U − W )

(cid:13)
(cid:13)
(cid:13)

(cid:17) ¯A−1(U − W + ητ ) − ¯A−1(U − W )
2)

(cid:13)
(cid:13)
(cid:13)

which shows that the Laplacian is stable to deformations in operator norm. Finally, by denoting ˜xτ a layer of the deformed
Laplacian network

it follows that

Also,

˜xτ = ρ(Ax + Bτ (∆)x) ,

(cid:107)˜x − ˜xτ (cid:107) ≤ (cid:107)B(∆ − τ (∆)x(cid:107)
≤ C(cid:107)B(cid:107)|∇τ |∞(cid:107)x(cid:107) .

(cid:107)˜x − ˜yτ (cid:107) ≤ (cid:107)A(x − y) + B(∆x − τ (∆)y)(cid:107)

≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))(cid:107)x − y(cid:107) + (cid:107)∆ − τ (∆)(cid:107)(cid:107)x(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))
(cid:125)

(cid:107)x − y(cid:107) + C|∇τ |∞
(cid:124) (cid:123)(cid:122) (cid:125)
δ2

(cid:107)x(cid:107) ,

(cid:123)(cid:122)
δ1

(cid:124)

and therefore, by plugging (17) with y = ˜xτ , K layers of the Laplacian network satisfy

(cid:107)Φ(x; ∆) − Φ(x; τ (∆)(cid:107) ≤





(cid:89)

j≤K−1








δ1(j)

 (cid:107)˜x − ˜xτ (cid:107) +



(cid:88)

(cid:89)

j<K−1

j(cid:48)≤j







δ1(j(cid:48))δ2(j)

 |∇τ |∞(cid:107)x(cid:107)

≤

C



δ1(j)

 (cid:107)B(cid:107) +



(cid:89)

j≤K−1

(cid:88)

(cid:89)

δ1(j(cid:48))δ2(j)

j<K−1

j(cid:48)≤j






 |∇τ |∞(cid:107)x(cid:107) . (cid:3) .

(15)
(16)

(17)

B.4. Proof of (d)

The proof is also analogous to the proof of (c), with the difference that now the Dirac operator is no longer invariant to

orthogonal transformations, only to translations. Given two points p, q, we verify that

(cid:107)p − q − τ (p) − τ (q)(cid:107) ≤ (cid:102)|τ |∞(cid:107)p − q(cid:107) ,

(cid:107)D − τ (D)(cid:107) = o((cid:102)|τ |∞) .

which, following the previous argument, leads to

C. Theorem 4.2

C.1. Proof of part (a)

The proof is based on the following lemma:

Lemma C.1 Let xN , yN ∈ H(MN ) such that ∀ N , (cid:107)xN (cid:107)H ≤ c,(cid:107)yN (cid:107)H ≤ c. Let ˆxN = EN (xN ), where EN is the
eigendecomposition of the Laplacian operator ∆N on MN , , with associated eigenvalues λ1 . . . λN in increasing order. Let
γ > 0 and β be deﬁned as in (5) for xN and yN . If β > 1 and (cid:107)xN − yN (cid:107) ≤ (cid:15) for all N ,

where C is a constant independent of (cid:15) and N .

(cid:107)∆N (xN − yN )(cid:107)2 ≤ C(cid:15)2− 1

β−1/2 ,

One layer of the network will transform the difference x1 − x2 into ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2). We verify that

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + (cid:107)B(cid:107)(cid:107)∆(x1 − x2)(cid:107) .

We now apply Lemma C.1 to obtain

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + C(cid:107)B(cid:107)(cid:107)x1 − x2(cid:107)

β−1
β−1/2

≤ (cid:107)x1 − x2(cid:107)

β−1
β−1/2

(cid:16)

(cid:107)A(cid:107)(cid:107)x1 − x2(cid:107)(2β−1)−1

+ C(cid:107)B(cid:107)

(cid:17)

≤ C((cid:107)A(cid:107) + (cid:107)B(cid:107))(cid:107)x1 − x2(cid:107)

β−1
β−1/2 ,

where we redeﬁne C to account for the fact that (cid:107)x1 − x2(cid:107)(2β−1)−1

is bounded. We have just showed that

with fr = C((cid:107)Ar(cid:107) + (cid:107)Br(cid:107)) and gr = βr−1

βr−1/2 . By cascading (20) for each of the R layers we thus obtain

(cid:107)x(r+1)
1

− x(r+1)
2

(cid:107) ≤ fr(cid:107)x(r)

1 − x(r)

2 (cid:107)gr

(cid:107)Φ∆(x1) − Φ∆(x2)(cid:107) ≤

(cid:35)

(cid:81)
f
r

r(cid:48)>r gr(cid:48)

(cid:107)x1 − x2(cid:107)

(cid:81)R

r=1 gr ,

(cid:34) R
(cid:89)

r=1

which proves (6) (cid:3).

Proof of (19): Let {e1, . . . , eN } be the eigendecomposition of ∆N . For simplicity, we drop the subindex N in the signals
from now on. Let ˆx(k) = (cid:104)x, ek(cid:105) and ˜x(k) = λk ˆx(k); and analogously for y. From the Parseval identity we have that
(cid:107)x(cid:107)2 = (cid:107)ˆx(cid:107)2. We express (cid:107)∆(x − y)(cid:107) as

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 .
λ2

(cid:88)

k≤N

The basic principle of the proof is to cut the spectral sum (22) in two parts, chosen to exploit the decay of ˜x(k). Let

F (x)(k) =

(cid:80)

k(cid:48)≥k ˜x(k)2
(cid:107)x(cid:107)2
H

=

(cid:80)

k(cid:48)≥k ˜x(k)2
k(cid:48) ˜x(k)2 =
(cid:80)

(cid:80)

k(cid:48)≥k λ2
(cid:80)
k(cid:48) λ2

k ˆx(k)2
k ˆx(k)2 ≤ 1 ,

(18)

(19)

(20)

(21)

(22)

and analogously for y. For any cutoff k∗ ≤ N we have

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 +
λ2

k(ˆx(k) − ˆy(k))2
λ2

(cid:88)

(cid:88)

k≤k∗

≤ λ2
k∗
≤ λ2
k∗
≤ λ2
k∗
where we denote for simplicity F (k∗) = max(F (x)(k∗), F (y)(k∗)). By assumption, we have λ2
k
F (k) (cid:46) (cid:88)

(cid:15)2 + 2(F (x)(k∗)(cid:107)x(cid:107)2
(cid:15)2 + 2F (k∗)((cid:107)x(cid:107)2
(cid:15)2 + 4F (k∗)D2 ,

k2(γ−β) (cid:39) k1+2(γ−β) .

H + (cid:107)y(cid:107)2

H)

H)

k>k∗
H + F (y)(k∗)(cid:107)y(cid:107)2

(cid:46) k2γ and

By denoting ˜β = β − γ − 1/2, it follows that

k(cid:48)≥k

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2k2γ

∗ + 4D2k−2 ˜β

∗

(cid:15)22γk2γ−1 − 2 ˜β4D2k−2 ˜β−1 = 0, thus
(cid:20) 4βD2
γ(cid:15)2

k∗ =

(cid:21) 1

2γ+2 ˜β

.

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2− 1

γ+ ˜β = (cid:15)2− 1

β−1/2 ,

Optimizing for k∗ yields

which proves part (a) (cid:3).

C.2. Proof of part (b)

We will use the following lemma:

By plugging (25) back into (24) and dropping all constants independent of N and (cid:15), this leads to

Lemma C.2 Let M = (V, E, F ) is a non-degenerate mesh, and deﬁne

η1(M) = sup

, η2(M) = sup

, η3(M) = αmin .

¯Ai
¯Aj

(i,j)∈E

ij + (cid:96)2
(cid:96)2

jk + (cid:96)2
ik

(i,j,k)∈F

A(i, j, k)

Then, given a smooth deformation τ and x deﬁned in M, we have

(cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) ,

where C depends only upon η1, η2 and η3.

In that case, we need to control the difference ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x). We verify that

(cid:107)ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x)(cid:107) ≤ (cid:107)B(cid:107)(cid:107)(∆ − τ (∆))x(cid:107) .

By Lemma C.2 it follows that (cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) and therefore, by denoting x(1)
x(1)
2 = ρ(Ax + Bτ (∆)x), we have

1 = ρ(Ax + B∆x) and

2 (cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) = C|∇τ |∞(cid:107)x(cid:107)H .

(28)

1 − x(1)
By applying again Lemma C.1, we also have that

(cid:107)x(1)

(cid:107)∆x(1)

1 − τ (∆)x(1)

2 (cid:107) = (cid:107)∆x(1)
= (cid:107)∆(x(1)
≤ C(cid:107)x(1)

1 − (∆ + τ (∆) − ∆)x(1)
2 (cid:107)
2 ) + (τ (∆) − ∆)x(1)
1 − x(1)
2 (cid:107)
1 − x(1)
β1−1/2 + |∇τ |∞(cid:107)x(1)
2 (cid:107)
β1−1
β1−1/2 ,

β1−1

(cid:46) C|∇τ |∞

2 (cid:107)H

(23)

(24)

(25)

(26)

(27)

which, by combining it with (28) and repeating through the R layers yields

(cid:107)Φ∆(x, M) − Φ∆(x, τ (M)(cid:107) ≤ C|∇τ |∞

(cid:81)R

r=1

βr −1
βr −1/2 ,

(29)

which concludes the proof (cid:3).

Proof of (27): The proof follows closely the proof of Theorem 4.1, part (c). From (13) and (14) we have that

τ ( ¯A) = ¯A(I + Gτ ) , with |Gτ |∞ ≤ C(η2, η3)|∇τ |∞ , and
τ (U − W ) = (I + Hτ )(U − W ) , with |Hτ |∞ ≤ C(η2, η3)|∇τ |∞ .

It follows that, up to second order o(|∇τ |∞

2) terms,

τ (∆) − ∆ = τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )

= (cid:0) ¯A[1 + Gτ ](cid:1)−1
(cid:39) ¯A−1Hτ (U − W ) + Gτ ∆ .

[(I + Hτ )(U − W )] − ¯A−1(U − W )

(30)

By writing ¯A−1Hτ = (cid:102)Hτ ¯A−1, and since ¯A is diagonal, we verify that

( (cid:102)Hτ )i,j = (Hτ )i,j

, with

Ai,i
Aj,j

Ai,i
Aj,j

≤ η1, and hence that

¯A−1Hτ (U − W ) = (cid:102)Hτ ∆ , with | (cid:102)Hτ |∞ ≤ C(η1, η2, η3)|∇τ |∞ .

(31)

We conclude by combining (30) and (31) into

(cid:107)(∆ − τ (∆))x(cid:107) = (cid:107)(Gτ + (cid:102)Hτ )∆x(cid:107)

≤ C (cid:48)(η1, η2, η3)|∇τ |∞(cid:107)∆x(cid:107) ,

which proves (27) (cid:3)

C.3. Proof of part (c)

This result is a consequence of the consistency of the cotangent Laplacian to the Laplace-Beltrami operator on S [45]:

Theorem C.3 ([45], Thm 3.4) Let M be a compact polyhedral surface which is a normal graph over a smooth surface S
with distortion tensor T , and let ¯T = (det T )1/2T −1. If the normal ﬁeld uniform distance d(T , 1) = (cid:107) ¯T − 1(cid:107)∞ satisﬁes
d(T , 1) ≤ (cid:15), then

(cid:107)∆M − ∆S(cid:107) ≤ (cid:15) .

(32)

If ∆M converges uniformly to ∆S, in particular we verify that

(cid:107)x(cid:107)H(M) → (cid:107)x(cid:107)H(S) .

Thus, given two meshes M, M(cid:48) approximating a smooth surface S in terms of uniform normal distance, and the corre-

sponding irregular sampling x and x(cid:48) of an underlying function ¯x : S → R, we have

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x − x(cid:48)(cid:107) + (cid:107)B(cid:107)(cid:107)∆Mx − ∆M(cid:48)x(cid:48)(cid:107) .

(33)

Since M and M(cid:48) both converge uniformly normally to S and ¯x is Lipschitz on S, it results that

thus (cid:107)x − x(cid:48)(cid:107) ≤ 2L(cid:15). Also, thanks to the uniform normal convergence, we also have convergence in the Sobolev sense:

(cid:107)x − ¯x(cid:107) ≤ L(cid:15) , and (cid:107)x(cid:48) − ¯x(cid:107) ≤ L(cid:15) ,

(cid:107)x − ¯x(cid:107)H (cid:46) (cid:15) , (cid:107)x(cid:48) − ¯x(cid:107)H (cid:46) (cid:15) ,

which implies in particular that

From (33) and (34) it follows that

(cid:107)x − x(cid:48)(cid:107)H (cid:46) (cid:15) .

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ 2(cid:107)A(cid:107)L(cid:15) +

(34)

(35)

+(cid:107)B(cid:107)(cid:107)∆Mx − ∆S ¯x + ∆S ¯x − ∆M(cid:48)x(cid:48)(cid:107)

≤ 2(cid:15) ((cid:107)A(cid:107)L + (cid:107)B(cid:107)) .

By applying again Lemma C.1 to ˜x = ρ(Ax + B∆Mx), ˜x(cid:48) = ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48)), we have

We conclude by retracing the same argument as before, reapplying Lemma C.1 at each layer to obtain

(cid:107)˜x − ˜x(cid:48)(cid:107)H ≤ C(cid:107)˜x − ˜x(cid:48)(cid:107)

β1−1
β1−1/2 (cid:46) (cid:15)

β1−1
β1−1/2 .

(cid:107)ΦM(x) − ΦM(cid:48)(x(cid:48))(cid:107) ≤ C(cid:15)

(cid:81)R

r=1

βr −1
βr −1/2 . (cid:3) .

D. Proof of Corollary 4.3

We verify that

(cid:107)ρ(B∆x) − ρ(Bτ (∆)τ (x))(cid:107) ≤ (cid:107)B(cid:107)(cid:107)∆x − τ (∆)τ (x)(cid:107)

≤ (cid:107)B(cid:107)(cid:107)∆(x − τ (x)) + (∆ − τ (∆))(τ (x))(cid:107)
≤ (cid:107)B(cid:107)((cid:107)∆(x − τ (x))(cid:107) + (cid:107)(∆ − τ (∆))(τ (x))(cid:107) .

The second term is o(|∇τ |∞) from Lemma C.2. The ﬁrst term is

(cid:107)x − τ (x)(cid:107)H ≤ (cid:107)∆(I − τ )(cid:107)(cid:107)x(cid:107) ≤ (cid:107)∇2τ (cid:107)(cid:107)x(cid:107) ,

where (cid:107)∇2τ (cid:107) is the uniform Hessian norm of τ . The result follows from applying the cascading argument from last section.
(cid:3)

E. Preliminary Study: Metric Learning for Dense Correspondence

As an interesting extension, we apply the architecture we built in Experiments 6.2 directly to a dense shape correspondence

problem.

Similarly as the graph correspondence model from [33], we consider a Siamese Surface Network, consisting of two
identical models with the same architecture and sharing parameters. For a pair of input surfaces M1, M2 of N1, N2 points
respectively, the network produces embeddings E1 ∈ RN1×d and E2 ∈ RN2×d. These embeddings deﬁne a trainable
similarity between points given by

si,j =

e(cid:104)E1,i,E2,j (cid:105)
j(cid:48) e(cid:104)E1,i,E2,j(cid:48) (cid:105)

,

(cid:80)

(36)

which can be trained by minimizing the cross-entropy relative to ground truth pairs. A diagram of the architecture is

provided in Figure 10.

In general, dense shape correspondence is a task that requires a blend of intrinsic and extrinsic information, motivating
the use of data-driven models that can obtain such tradeoffs automatically. Following the setup in Experiment 6.2, we use
models with 15 ResNet-v2 blocks with 128 output features each, and alternate Laplace and Dirac based models with Average
Pooling blocks to cover a larger context: The input to our network consists of vertex positions only.

We tested our architecture on a reconstructed (i.e. changing the mesh connectivity) version of the real scan of FAUST
dataset[3]. The FAUST dataset contains 100 real scans and their corresponding ground truth registrations. The ground truth
is based on a deformable template mesh with the same ordering and connectivity, which is ﬁtted to the scans. In order to
eliminate the bias of using the same template connectivity, as well as the need of a single connected component, the scans
are reconstructed again with [19]. To foster replicability, we release the processed dataset in the additional material. In our
experiment, we use 80 models for training and 20 models for testing.

Figure 10. Siamese network pipeline: the two networks take vertex coordinates of the input models and generate a high dimensional feature
vector, which are then used to deﬁne a map from M1 to M2. Here, the map is visualized by taking a color map on M2, and transferring
it on M1

Figure 11. Additional results from our setup. Plot in the middle shows rate of correct correspondence with respect to geodesic error [23].
We observe that Laplace is performing similarly to Dirac in this scenario. We believe that the reason is that the FAUST dataset contains
only isometric deformations, and thus the two operators have access to the same information. We also provide visual comparison, with the
transfer of a higher frequency colormap from the reference shape to another pose.

Since the ground truth correspondence is implied only through the common template mesh, we compute the correspon-
dence between our meshes with a nearest neighbor search between the point cloud and the reconstructed mesh. Consequently,

Figure 12. Heat map illustrating the point-wise geodesic difference between predicted correspondence point and the ground truth. The unit
is proportional to the geodesic diameter, and saturated at 10%.

Figure 13. A failure case of applying the Laplace network to a new pose in the FAUST benchmark dataset. The network confuses between
left and right arms. We show the correspondence visualization for front and back of this pair.

due to the drastic change in vertex replacement after the remeshing, only 60-70 percent of labeled matches are used. Although
making it more challenging, we believe this setup is close to a real case scenario, where acquisition noise and occlusions are
unavoidable.

Our preliminary results are reported in Figure 11. For simplicity, we generate predicted correspondences by simply taking
the mode of the softmax distribution for each reference node i: ˆj(i) = arg maxj si,j, thus avoiding a reﬁnement step that is
standard in other shape correspondence pipelines. The MLP model uses no context whatsoever and provides a baseline that
captures the prior information from input coordinates alone. Using contextual information (even extrinsically as in point-
cloud model) brings signiﬁcative improvments, but these results may be substantially improved by encoding further prior
knowledge. An example of the current failure of our model is depitcted in Figure 13, illustrating that our current architecture
does not have sufﬁciently large spatial context to disambiguate between locally similar (but globally inconsistent) parts.

We postulate that the FAUST dataset [3] is not an ideal ﬁt for our contribution for two reasons: (1) it is small (100

models), and (2) it contains only near-isometric deformations, which do not require the generality offered by our network. As
demonstrated in [28], the correspondence performances can be dramatically improved by constructing basis that are invariant
to the deformations. We look forward to the emergence of new geometric datasets, and we are currently developing a capture
setup that will allow us to acquire a more challenging dataset for this task.

F. Further Numerical Experiments

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 14. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 15. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 16. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 17. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

Laplace

Dirac

Figure 18. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Figure 19. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

Figure 20. From left to right: set-to-set, ground truth and Dirac based model. Color corresponds to mean squared error between ground
truth and prediction: green - smaller error, red - larger error.

8
1
0
2
 
n
u
J
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
1
8
0
1
.
5
0
7
1
:
v
i
X
r
a

Surface Networks

Ilya Kostrikov1, Zhongshi Jiang1, Daniele Panozzo ∗1, Denis Zorin †1, and Joan Bruna‡1,2

1Courant Institute of Mathematical Sciences, New York University
2Center for Data Science, New York University

Abstract

We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used
to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs,
namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the
Laplacian operator.

Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric
deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to
GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In
particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions — this is in stark
contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface
Networks (SN).

We prove that these models deﬁne shape representations that are stable to deformation and to discretization, and we
demonstrate the efﬁciency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under
non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by
SNs.

1. Introduction

3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering
to computer animation to medical imaging. Despite the vast amount of high-quality 3D geometric data available, data-
driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data
representation regularity which is required for traditional convolutional neural network approaches. While in computer vision
problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more
complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar
chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical
properties.

Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike
the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the
inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary
features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations.
Extrinsic representations rely on the speciﬁc embedding of surfaces within a three-dimensional ambient space, whereas
intrinsic representations only capture geometric properties speciﬁc to the surface, irrespective of its parametrization. Whereas
the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local
deformations and invariance to global transformations.

∗DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.
†DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.
‡JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding

author: bruna@cims.nyu.edu

1

A particularly simple and popular extrinsic method [36, 37] represents shapes as point clouds in R3 of variable size,
and leverages recent deep learning models that operate on input sets [44, 43]. Despite its advantages in terms of ease
of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classiﬁcation
and segmentation tasks, one may wonder whether this simpliﬁcation comes at a loss of precision as one considers more
challenging prediction tasks.

In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on
geometric deep learning. These models provide data-driven intrinsic graph and manifold representations with inductive
biases analogous to CNNs on natural images. Models based on Graph Neural Networks [40] and their spectral variants
[6, 11, 26] have been successfully applied to geometry processing tasks such as shape correspondence [32]. In their basic
form, these models learn a deep representation over the discretized surface by combining a latent representation at a given
node with a local linear combination of its neighbors’ latent representations, and a point-wise nonlinearity. Different models
vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to
spectral interpretations of those models.

Our contributions are three-fold. First, we extend the model to support extrinsic features. More speciﬁcally, we exploit the
fact that surfaces in R3 admit a ﬁrst-order differential operator, the Dirac operator, that is stable to discretization, provides a
direct generalization of Laplacian-based propagation models, and is able to detect principal curvature directions [9, 18]. Next,
we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization,
two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the
variational autoencoder framework [25, 39], that is able to exploit non-Euclidean geometric regularity.

By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation
that we apply to several challenging tasks. The resulting Surface Networks – using either the Dirac or the Laplacian, inherit
the stability and invariance properties of these operators, thus providing data-driven representations with certiﬁed stability to
deformations. We demonstrate the model efﬁciency on a temporal prediction task of complex dynamics, based on a physical
simulation of elastic shells, which conﬁrms that whenever geometric information (in the form of a mesh) is available, it can
be leveraged to signiﬁcantly outperform point-cloud based models.

Our main contributions are summarized as follows:

• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear

dynamics, motivating the use of geometric shape information.

• We prove that Surface Networks deﬁne shape representations that are stable to deformation and to discretization.

• We introduce a generative model for 3D surfaces based on the variational autoencoder.

2. Related Work

Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research.
[40] introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained
by backpropagation. Subsequent works [27, 43] have relaxed the model by untying the recurrent layer weights and proposed
several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convo-
lutional networks to non-Euclidean graphs. [6, 17] proposed to learn smooth spectral multipliers of the graph Laplacian,
albeit with high computational cost, and [11, 26] resolved the computational bottleneck by learning polynomials of the graph
Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to
[5] for an exhaustive literature review on the topic. GNNs are ﬁnding application in many different domains. [2, 7] develop
graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics.
[12, 22] study molecular ﬁngerprints using variants of the GNN architecture, and [14] further develop the model by combin-
ing it with set representations [44], showing state-of-the-art results on molecular prediction. The resulting models, so-called
Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model
on general graphs.

In the context of computer graphics, [30] developed the ﬁrst CNN model on meshed surfaces using intrinsic patch rep-
resentations, and further generalized in [4] and [32]. This last work allows for ﬂexible representations via the so-called
pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode ﬁrst-
order differential information. These intrinsic models contrast with Euclidean models such as [48, 46], that have higher
sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models

A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/SurfaceNetworks.

are increasingly popular to model 3d objects due to their simplicity and versatility. [36, 37] use set-invariant representations
from [44, 43] to solve shape segmentation and classiﬁcation tasks. More recently, [29] proposes to learn surface convolutional
network from a canonical representation of planar ﬂat-torus, with excellent performance on shape segmentation and classi-
ﬁcation, although such canonical representations may introduce exponential scale changes that can introduce instabilities.
Finally, [13] proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but
does not encode geometrical information as our shape generative model. Learning variational deformations is an important
problem for graphics applications, since it enables negligible and ﬁxed per-frame cost [35], but it is currently limited to 2D
deformations using point handles. In constrast, our method easily generalizes to 3D and learns dynamic behaviours.

3. Surface Networks

This section presents our surface neural network model and its basic properties. We start by introducing the problem setup
and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section
3.2).

3.1. Laplacian Surface Networks

Our ﬁrst goal is to deﬁne a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where
V = (vi ∈ R3)i≤N contains the node coordinates, E = (ei,j) corresponds to edges, and F is the set of triangular faces. We
denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see [5] for details).
This operator can be interpreted as a local, linear high-pass ﬁlter in M that acts on signals x ∈ Rd×|V | deﬁned on the
vertices as a simple matrix multiplication ˜x = ∆x. By combining ∆ with an all-pass ﬁlter and learning generic linear
combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators
in M that update a feature map from layer k to layer k + 1 using trainable parameters Ak and Bk:

xk+1 = ρ (cid:0)Ak∆xk + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk .

By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization,
this model is a speciﬁc instance of the graph neural network [40, 5, 26] and a generalization of the spectrum-free Laplacian
networks from [11]. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening
or pooling layers.

In contrast to general graphs, meshes contain a low-dimensional Euclidean embedding that contains potentially useful
information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A
simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as
input channels to the network: x1 := V ∈ R|V |×3. The mean curvature can be computed by applying the Laplace operator
to the coordinates of the vertices:

∆x1 = −2Hn ,

where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian
neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph
neural network models is related to the use of generalized coordinates from [32]. By cascading K layers of the form (1) we
obtain a representation Φ∆(M) that contains generic features at each node location. When the number of layers K is of
the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate
information across the whole surface.

Equation (2) illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to
the mean variations across all directions. Although in general graphs there is no well-deﬁned procedure to recover anisotropic
local variations, in the case of surfaces some authors ([4, 1, 32]) have considered anisotropic extensions. We describe next
a particularly simple procedure to increase the expressive power of the network using a related operator from quantum
mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation [9] and shape
analysis [18].

(1)

(2)

3.2. Dirac Surface Networks

The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the
gradient (a ﬁrst-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to
these ﬁrst-order differential operators separately, enabling oriented high-pass ﬁlters.

For convenience, we embed R3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for
details). The Dirac operator is then deﬁned as a matrix D ∈ H|F |×|V | that maps (quaternion) signals on the nodes to signals
on the faces. In coordinates,

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area (see Appendix A) using counter-clockwise
orientations on all faces.

To apply the Dirac operator deﬁned in quaternions to signals in vertices and faces deﬁned in real numbers, we write the
feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a
quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The
Dirac operator provides ﬁrst-order differential information and is sensitive to local orientations. Moreover, one can verify [9]
that

Re D∗D = ∆ ,
where D∗ is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as
D∗ = M −1
V DH MF where DH is a conjugate transpose of D and MV , MF are diagonal mass matrices with one third of
areas of triangles incident to a vertex and face areas respectively.

The Dirac operator can be used to deﬁne a new neural surface representation that alternates layers with signals deﬁned
over nodes with layers deﬁned over faces. Given a d-dimensional feature representation over the nodes xk ∈ Rd×|V |, and the
faces of the mesh, yk ∈ Rd×|F |, we deﬁne a d(cid:48)-dimensional mapping to a face representation as

yk+1 = ρ (cid:0)CkDxk + Ekyk(cid:1) , Ck, Ek ∈ Rdk+1×dk ,

(3)

where Ck, Ek are trainable parameters. Similarly, we deﬁne the adjoint layer that maps back to a ˜d-dimensional signal over
nodes as

xk+1 = ρ (cid:0)AkD∗yk+1 + Bkxk(cid:1) , Ak, Bk ∈ Rdk+1×dk ,
where Ak, Bk are trainable parameters. A surface neural network layer is thus determined by parameters {A, B, C, E} using
equations (3) and (4) to deﬁne xk+1 ∈ Rdk+1×|V |. We denote by ΦD(M) the mesh representation resulting from applying
K such layers (that we assume ﬁxed for the purpose of exposition).

(4)

The Dirac-based surface network is related to edge feature transforms proposed on general graphs in [14], although these
edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no
notion of square root of ∆ that recovers oriented ﬁrst-order derivatives.

4. Stability of Surface Networks

Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise
under the model. Given a continuous surface S ⊂ R3 or a discrete mesh M, and a smooth deformation ﬁeld τ : R3 → R3,
we are particularly interested in two forms of stability:

• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that (cid:107)Φ(M) −

Φ(τ (M))(cid:107) is small if (cid:107)∇τ (∇τ )∗ − I(cid:107) is small, i.e when the deformation is nearly rigid; see Theorem 4.1.

• Given two discretizations M1 and M2 of the same underlying surface S, we would like to control (cid:107)Φ(M1)−Φ(M2)(cid:107)

in terms of the resolution of the meshes; see Theorem 4.2.

These stability properties are important in applications, since most tasks we are interested in are stable to deformation and
to discretization. We shall see that the ﬁrst property is a simple consequence of the fact that the mesh Laplacian and Dirac
operators are themselves stable to deformations. The second property will require us to specify under which conditions the
discrete mesh Laplacian ∆M converges to the Laplace-Beltrami operator ∆S on S. Unless it is clear from the context, in the
following ∆ will denote the discrete Laplacian.

Theorem 4.1 Let M be a N -node mesh and x, x(cid:48) ∈ R|V |×d be input signals deﬁned on the nodes. Assume the nonlinearity
ρ( · ) is non-expansive (|ρ(z) − ρ(z(cid:48))| ≤ |z − z(cid:48)|). Then

(a) (cid:107)Φ∆(M; x) − Φ∆(M; x(cid:48))(cid:107) ≤ α∆(cid:107)x − x(cid:48)(cid:107) , where α∆ depends only on the trained weights and the mesh.

(b) (cid:107)ΦD(M; x) − ΦD(M; x(cid:48))(cid:107) ≤ αD(cid:107)x − x(cid:48)(cid:107) , where αD depends only on the trained weights and the mesh.

(c) Let |∇τ |∞ := supu (cid:107)∇τ (u)(∇τ (u))∗ − 1(cid:107), where ∇τ (u) is the Jacobian matrix of u (cid:55)→ τ (u). Then (cid:107)Φ∆(M; x) −

Φ∆(τ (M); x)(cid:107) ≤ β∆|∇τ |∞(cid:107)x(cid:107) , where β∆ is independent of τ and x.

(d) Denote by (cid:103)|∇τ |∞ := supu (cid:107)∇τ (u) − 1(cid:107). Then (cid:107)ΦD(M; x) − ΦD(τ (M); x)(cid:107) ≤ βD (cid:103)|∇τ |∞(cid:107)x(cid:107) , where βD is

independent of τ and x.

Properties (a) and (b) are not speciﬁc to surface representations, and are a simple consequence of the non-expansive property
of our chosen nonlinearities. The constant α is controlled by the product of (cid:96)2 norms of the network weights at each layer and
the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators
are themselves stable to deformations, a property that depends on two key aspects: ﬁrst, the Laplacian/Dirac is localized in
space, and next, that it is a high-pass ﬁlter and therefore only depends on relative changes in position.

One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the
reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral
radius of ∆M diverges as the mesh size N increases.

In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incom-
ing to the surface network. This can be measured with Sobolev norms deﬁned using the spectrum of the Laplacian operator.
Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆M, consider
the spectral decomposition of ∆M (a symmetric, positive deﬁnite operator):

∆M =

λkekeT

k , ek ∈ RN , 0 ≤ λ1 ≤ λ2 · · · ≤ λN .

(cid:88)

k≤N

Under normal uniform convergence 1 [45], the spectrum of ∆M converges to the spectrum of the Laplace-Beltrami operator
∆S of S. If S is bounded, it is known from the Weyl law [47] that there exists γ > 0 such that k−γ(S) (cid:46) λ−1
k , so the
eigenvalues λk do not grow too fast. The smoothness of a signal x ∈ R|V |×d deﬁned in M is captured by how fast its
H := (cid:80)
spectral decomposition ˆx(k) = eT
k λ(k)2(cid:107)ˆx(k)(cid:107)2 is Sobolev norm, and
β(x, S) > 1 as the largest rate such that its spectral decomposition coefﬁcients satisfy

k x ∈ Rd decays [42]. We deﬁne (cid:107)x(cid:107)2

(cid:107)ˆx(k)(cid:107) (cid:46) k−β , (k → ∞) .
(5)
If x ∈ R|V |×d is the input to the Laplace Surface Network of R layers, we denote by (β0, β1, . . . , βR−1) the smoothness
rates of the feature maps x(r) deﬁned at each layer r ≤ R.

Theorem 4.2 Consider a surface S and a ﬁnite-mesh approximation MN of N points, and Φ∆ a Laplace Surface Network
with parameters {(Ar, Br)}r≤R. Denote by d(S, MN ) the uniform normal distance, and let x1, x2 be piece-wise polyhedral
approximations of ¯x(t), t ∈ S in MN , with (cid:107)¯x(cid:107)H(S) < ∞. Assume (cid:107)¯x(r)(cid:107)H(S) < ∞ for r ≤ R.

(a) If x1, x2 are two functions such that the R feature maps x(r)

have rates (β0, β1, . . . , βR−1), then

l
(cid:107)Φ∆(x1; MN ) − Φ∆(x2; MN )(cid:107)2 ≤ C(β)(cid:107)x1 − x2(cid:107)h(β) ,

(6)

with h(β) = (cid:81)R

βr−1
βr−1/2 , and where C(β) does not depend upon N .

r=1

(b) If τ is a smooth deformation ﬁeld, then (cid:107)Φ∆(x; MN ) − Φ∆(x; τ (MN ))(cid:107) ≤ C|∇τ |∞

h(β) , where C does not depend

upon N .

(c) Let M and M(cid:48) be N -point discretizations of S, If max(d(M, S), d(M(cid:48), S)) ≤ (cid:15), then (cid:107)Φ∆(M; x) − Φ∆(M(cid:48), x(cid:48))(cid:107) ≤

C(cid:15)h(β) , where C is independent of N .

This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases,
the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac
operator also has a well-deﬁned continuous counterpart [9] that generalizes the gradient operator in quaternion space. Also,
our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in
terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x).
These extensions are left for future work. Finally, a speciﬁc setup that we use in experiments is to use as input signal the
canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields

Corollary 4.3 Denote Φ(M) := ΦM(V ), where V are the node coordinates of M. Then, if A1 = 0,

(cid:107)Φ(M) − Φ(τ (M))(cid:107) ≤ κ max(|∇τ |∞, (cid:107)∇2τ (cid:107))h(β) .

(7)

1which controls how the normals of the mesh align with the surface normals; see [45].

Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R3 (right) is expressed in terms of a “sampling” 2D irregular mesh
˜M ⊂ R2 (left) and a depth scalar ﬁeld f : ˜M → R over ˜M (center).

5. Generative Surface Models

State-of-the-art generative models for images, such as generative adversarial networks [38], pixel autoregressive networks
[34], or variational autoencoders [25], exploit the locality and stationarity of natural images in their probabilistic models, in
the sense that the model satisﬁes pθ(x) ≈ pθ(xτ ) by construction, where xτ is a small deformation of a given input x. This
property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric
stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains
two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface,
which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas
the former contains the essential semantic information, the latter is not informative, and to some extent independent of the
shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as
height-ﬁeld meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as ( ˜M, f ( ˜M)), where ˜M = ( ˜V , ˜E, ˜F ) is
now a 2D mesh and f : ˜V → R is a depth-map encoding the original node locations V , as shown in Figure 1.

In this work, we consider the variational autoencoder framework [25, 39].

It considers a mixture model of the form
p(M) = (cid:82) pθ(M | h)p0(h)dh , where h ∈ R|S| is a vector of latent variables. We train this model by optimizing the
variational lower bound of the data log-likelihood:

min
θ,ψ

1
L

(cid:88)

l≤L

−Eh∼qψ (h | Ml) log pθ(Ml | h) + DKL(qψ(h | Ml) || p0(h)) .

(8)

We thus need to specify a conditional generative model pθ(M | h), a prior distribution p0(h) and a variational approximation
to the posterior qψ(h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the
height-ﬁeld representation, we choose for simplicity a separable model of the form pθ(M | h) = pθ(f | h, ˜M) · p( ˜M) ,
where ˜M ∼ p( ˜M) is a homogeneous Poisson point process, and f ∼ pθ(f | h, ˜M) is a normal distribution with mean and
isotropic covariance parameters given by a SN:

pθ(f | h, ˜M) = N (µ(h, ˜M), σ2(h, ˜M)1) ,

with [µ(h, ˜M), σ2(h, ˜M)] = ΦD( ˜M ; h) . The generation step thus proceeds as follows. We ﬁrst sample a 2D mesh ˜M
independent of the latent variable h, and then sample a depth ﬁeld over ˜M conditioned on h from the output of a decoder
network ΦD( ˜M ; h). Finally, the variational family qψ is also a Normal distribution whose parameters are obtained from an
encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization: qψ(h | M) =
N (¯µ, ¯σ21) , with [¯µ, ¯σ] = ¯ΦD(M) .

6. Experiments

6.1. MeshMNIST

For experimental evaluation, we compare models built using ResNet-v2 blocks [16], where convolutions are replaced with
the appropriate operators (see Fig. 2): (i) a point cloud based model from [43] that aggregates global information by averaging
features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical
coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant
of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).

For this task, we construct a MeshMNIST database with only height-ﬁeld meshes (Sec. 5). First, we sample points on
a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a
Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each

Figure 2. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to
the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch
Normalization (blue) and ‘1 × 1’ convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language
and denote by xk+1 the output of this 2-layer block.

Receptive ﬁeld Number of parameters

Model
MLP
PointCloud
Laplace
Dirac

1
-
16
8

519672
1018872
1018872
1018872

Smooth L1-loss (mean per sequence (std))
64.56 (0.62)
23.64 (0.21)
17.34 (0.52)
16.84 (0.16)

Table 1. Evaluation of different models on the temporal task

point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to deﬁne a sampling process
over 3D height-ﬁeld meshes.

We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder
converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to
obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a speciﬁc 3D
mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does
not depend on input data. We trained the model for 75 epochs using Adam optimizer [24] with learning rate 10−3, weight
decay 10−5 and batch size 32. Figures 3,4 illustrate samples from the model. The geometric encoder is able to leverage the
local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to
the speciﬁc sampled grid, as opposed to set-based generative models.

Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of dis-
cretization/triangulation (Poisson disk sampling with p=1.5).

Figure 4. Meshes from the dataset (ﬁrst ﬁve). And meshes generated by our model (last ﬁve).

6.2. Spatio-Temporal Predictions

One speciﬁc task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1, X 2, . . . , X n,

the task is to predict the following frames Y = Y 1 = X n+1, Y 2, . . . , Y m = X n+m. As in [31], we use a simple non-
recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on
n = 2 frames and predict the next m = 40 frames. In order to generate data, we ﬁrst extracted 10k patches from the
MPI-Faust dataset[3], by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of
the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground.
We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique [41], with additional
gravitational forces [20]. Libigl [21] has been used for the mesh processing tasks. Sequences with patches from the ﬁrst 80
subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on
request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial
frames. Thus, we can train models by minimizing smooth-L1 loss [15] between target frames and output of our models.

Ground Truth

MLP

PointCloud

Laplace

Dirac

Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set.
Boxes indicate distinctive features. For larger crops, see Figure 6

.

We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and
Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned
frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce
120-dimensional outputs. We trained all models using the Adam optimizer [24] with learning rate 10−3, weight decay 10−5,
and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained
for 110k steps in overall.

Ground Truth

Laplace

Dirac

Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Table 1 reports quantitative prediction performance of different models, and Figure 5 displays samples from the prediction

Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between
ground truth and prediction: green - smaller error, red - larger error.

Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

models at speciﬁc frames. The set-to-set model [44, 43], corresponding to a point-cloud representation used also in [36],
already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between
this model and Laplace-/Dirac-based models is signiﬁcant, both visually and quantitatively. Dirac-based model outperforms
Laplace-based model despite the smaller receptive ﬁeld. Videos comparing the performance of different models are available
in the additional material.

Figure 6 illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since
they propagate information using an isotropic operator, have more difﬁculties at resolving corners and pointy structures than
the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit
the extrinsic information only via the input coordinates is remarkable and more computationally efﬁcient than the Dirac
counterpart. Figures 7 and 8 overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac
respectively. They conﬁrm ﬁrst that SNs outperform the point-cloud based model, which often produce excessive ﬂattening
and large deformations, and next that ﬁrst-order Dirac operators help resolve areas with high directional curvature. We refer
to the supplementary material for additional qualitative results.

7. Conclusions

We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the non-Euclidean
geometry of surfaces. We have shown how a ﬁrst-order differential operator (the Dirac operator) can detect and adapt to
geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction
is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments.
That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting
to instead learn the operator, akin to recent Message-Passing NNs [14] and explore whether Dirac is recovered.

Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly
superior performance to point-cloud based models. While there are not many such datasets currently available, we expect
them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated
in consumer devices. SNs provide efﬁcient inference, with predictable runtime, which makes them appealing across many
areas of computer graphics, where a ﬁxed, per-frame cost is required to ensure a stable framerate, especially in VR applica-
tions. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and
generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.

References

2014. 3

[1] M. Andreux, E. Rodol`a, M. Aubry, and D. Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In Proc. NORDIA,

[2] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In

Advances in Neural Information Processing Systems, pages 4502–4510, 2016. 2

[3] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 3794–3801, 2014. 7, 18, 20

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks.

In Advances in Neural Information Processing Systems, pages 3189–3197, 2016. 2, 3

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv

preprint arXiv:1611.08097, 2016. 2, 3, 14

[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013. 2
[7] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics.

[8] D. Chen and J. R. Gilbert. Obtaining bounds on the two norm of a matrix from the splitting lemma. Electronic Transactions on

[9] K. Crane, U. Pinkall, and P. Schr¨oder. Spin transformations of discrete surfaces. In ACM Transactions on Graphics (TOG). ACM,

ICLR, 2016. 2

2011. 2, 3, 4, 5

Numerical Analysis, 21:28–46, 2005. 14

and Its Applications, 427(1):55–69, 2007. 12

[10] K. C. Das. Extremal graph characterization from the upper bound of the laplacian spectral radius of weighted graphs. Linear Algebra

[11] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In

Advances in Neural Information Processing Systems, pages 3837–3845, 2016. 2, 3

[12] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convo-

lutional networks on graphs for learning molecular ﬁngerprints. In Neural Information Processing Systems, 2015. 2

[13] H. Fan, H. Su, and L. Guibas. A point set generation network for 3d object reconstruction from a single image. arXiv preprint

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. arXiv preprint

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. 7
[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision,

arXiv:1612.00603, 2016. 3

arXiv:1704.01212, 2017. 2, 4, 10

pages 630–645. Springer, 2016. 6

[17] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015. 2
[18] K. C. Hsueh-Ti Derek Liu, Alec Jacobson. A dirac operator for extrinsic shape analysis. Computer Graphics Forum, 2017. 2, 3
[19] Y. Hu, Q. Zhou, X. Gao, A. Jacobson, D. Zorin, and D. Panozzo. Tetrahedral meshing in the wild. Submitted to ACM Transaction on

Graphics, 2018. 18

[20] A. Jacobson. Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes. PhD thesis, ETH, Z¨urich, 2013. 7
[21] A. Jacobson, D. Panozzo, et al. libigl: A simple C++ geometry processing library, 2016. http://libigl.github.io/libigl/. 7
[22] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal

of computer-aided molecular design, 2016. 2

[23] V. G. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic maps. In ACM Transactions on Graphics (TOG), volume 30, page 79.

[24] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representation, 2015.

[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 6
[26] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907,

ACM, 2011. 19

7, 8

2016. 2, 3

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. 2
[28] O. Litany, T. Remez, E. Rodol`a, A. M. Bronstein, and M. M. Bronstein. Deep functional maps: Structured prediction for dense shape

correspondence. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5660–5668, 2017. 21

[29] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. Kim, and Y. Lipman. Convolutional neural networks on surfaces

via seamless toric covers. In SIGGRAPH, 2017. 3

[30] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In

Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. 2

[31] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error.

arXiv preprint

arXiv:1511.05440, 2015. 7

[32] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds

using mixture model cnns. arXiv preprint arXiv:1611.08402, 2016. 2, 3

[33] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna. A note on learning algorithms for quadratic assignment with graph neural networks.

arXiv preprint arXiv:1706.07450, 2017. 18

[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. 6
[35] R. Poranne and Y. Lipman. Simple approximations of planar deformation operators. Technical report, ETHZ, 2015. 3
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. arXiv preprint

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint

arXiv:1612.00593, 2016. 2, 3, 9

arXiv:1706.02413, 2017. 2, 3

arXiv preprint arXiv:1511.06434, 2015. 6

Neural Networks, 20(1):61–80, 2009. 2, 3

[38] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks.

[39] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint arXiv:1505.05770, 2015. 2, 6
[40] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on

[41] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, 2007. 7
[42] D. A. Spielman. Spectral graph theory and its applications. In Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE

[43] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation.

In Advances in Neural Information

Symposium on, pages 29–38. IEEE, 2007. 5

Processing Systems, pages 2244–2252, 2016. 2, 3, 6, 9

[44] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015. 2, 3, 9
[45] M. Wardetzky. Convergence of the cotangent formula: An overview. In Discrete Differential Geometry, pages 275–286. 2008. 5, 12,

17

[46] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense human body correspondences using convolutional networks. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1544–1553, 2016. 2

[47] H. Weyl.

¨Uber die asymptotische verteilung der eigenwerte. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottingen,

Mathematisch-Physikalische Klasse, 1911:110–117, 1911. 5

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912–1920, 2015. 2

A. The Dirac Operator

The quaternions H is an extension of complex numbers. A quaternion q ∈ H can be represented in a form q = a+bi+cj +

dk where a, b, c, d are real numbers and i, j, k are quaternion units that satisfy the relationship i2 = j2 = k2 = ijk = −1.

As mentioned in Section 3.1, the Dirac operator used in the model can be conveniently represented as a quaternion matrix:

Df,j =

ej , f ∈ F, j ∈ V ,

−1
2|Af |

where ej is the opposing edge vector of node j in the face f , and Af is the area, as illustrated in Fig. A, using counter-
clockwise orientations on all faces.

vj

f

ej







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c













b
a
−b
a
−c −d
−d

d
c
d −c
b
a
a
c −b







.

The Deep Learning library PyTorch that we used to implement the models does not support quaternions. Nevertheless,
quaternion-valued matrix multiplication can be replaced with real-valued matrix multiplication where each entry q = a +
bi + cj + dk is represented as a 4 × 4 block

and the conjugate q∗ = a − bi − cj − dk is a transpose of this real-valued matrix:

B. Theorem 4.1

B.1. Proof of (a)

Laplacian ∆ of M is

We ﬁrst show the result for the mapping x (cid:55)→ ρ (Ax + B∆x), corresponding to one layer of Φ∆. By deﬁnition, the

∆ = diag( ¯A)−1(U − W ) ,
where ¯Aj is one third of the total area of triangles incident to node j, and W = (wi,j) contains the cotangent weights [45],
and U = diag(W 1) contains the node aggregated weights in its diagonal.

From [10] we verify that

(cid:107)U − W (cid:107) ≤

√

2 max
i

(cid:115)






U 2

i + Ui

Ujwi,j

(cid:88)

i∼j






√

√

≤ 2

2 sup
i,j

wi,j sup

dj

j

≤ 2

2 cot(αmin)dmax ,

(cid:107)∆(cid:107) ≤ C

cot(αmin)Smax
inf j ¯Aj

:= LM ,

where dj denotes the degree (number of neighbors) of node j, αmin is the smallest angle in the triangulation of M and Smax
the largest number of incident triangles. It results that

which depends uniquely on the mesh M and is ﬁnite for non-degenerate meshes. Moreover, since ρ( · ) is non-expansive, we
have

(cid:107)ρ (Ax + B∆x) − ρ (Ax(cid:48) + B∆x(cid:48))(cid:107) ≤ (cid:107)A(x − x(cid:48)) + B∆(x − x(cid:48))(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)LM)(cid:107)x − x(cid:48)(cid:107) .

By cascading (10) across the K layers of the network, we obtain

(cid:107)Φ(M; x) − Φ(M; x(cid:48))(cid:107) ≤

((cid:107)Ak(cid:107) + (cid:107)Bk(cid:107)LM)


 (cid:107)x − x(cid:48)(cid:107) ,





(cid:89)

k≤K

(9)

(10)

which proves (a). (cid:3)

B.2. Proof of (b)

B.3. Proof of (c)

The proof is analogous, by observing that (cid:107)D(cid:107) = (cid:112)(cid:107)∆(cid:107) and therefore

(cid:107)D(cid:107) ≤

(cid:112)

LM . (cid:3)

To establish (c) we ﬁrst observe that given three points p, q, r ∈ R3 forming any of the triangles of M,

(cid:107)p − q(cid:107)2(1 − |∇τ |∞)2

≤ (cid:107)τ (p) − τ (q)(cid:107)2 ≤

(cid:107)p − q(cid:107)2(1 + |∇τ |∞)2

A(p, q, r)2(1 − |∇τ |∞Cα−2

min − o(|∇τ |∞

2) ≤ A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2(1 + |∇τ |∞Cα−2

min + o(|∇τ |∞

2)) .

(11)

(12)

Indeed, (11) is a direct consequence of the lower and upper Lipschitz constants of τ (u), which are bounded respectively by
1 − |∇τ |∞ and 1 + |∇τ |∞. As for (12), we use the Heron formula

A(p, q, r)2 = s(s − (cid:107)p − q(cid:107))(s − (cid:107)p − r(cid:107))(s − (cid:107)r − q(cid:107)) ,

with s = 1
determined by the deformed points τ (p), τ (q), τ (r), we have that

2 ((cid:107)p − q(cid:107) + (cid:107)p − r(cid:107) + (cid:107)r − q(cid:107)) being the half-perimeter. By denoting sτ the corresponding half-perimeter

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≤ s(1 + |∇τ |∞) − (cid:107)p − q(cid:107)(1 − |∇τ |∞) = s − (cid:107)p − q(cid:107) + |∇τ |∞(s + (cid:107)p − q(cid:107)) and

sτ − (cid:107)τ (p) − τ (q)(cid:107) ≥ s(1 − |∇τ |∞) − (cid:107)p − q(cid:107)(1 + |∇τ |∞) = s − (cid:107)p − q(cid:107) − |∇τ |∞(s + (cid:107)p − q(cid:107)) ,

and similarly for the (cid:107)r − q(cid:107) and (cid:107)r − p(cid:107) terms. It results in

A(τ (p), τ (q), τ (r))2 ≥ A(p, q, r)2
≥ A(p, q, r)2 (cid:104)

(cid:20)
1 − |∇τ |∞

(cid:18)

1 − C|∇τ |∞α−2

1 +

+

s + (cid:107)p − q(cid:107)
s − (cid:107)p − q(cid:107)
(cid:105)
2)

min − o(|∇τ |∞

,

s + (cid:107)p − r(cid:107)
s − (cid:107)p − r(cid:107)

+

s + (cid:107)r − q(cid:107)
s − (cid:107)r − q(cid:107)

(cid:19)

− o(|∇τ |∞

(cid:21)
2)

and similarly

A(τ (p), τ (q), τ (r))2 ≤ A(p, q, r)2 (cid:104)

1 + C|∇τ |∞α−2

min − o(|∇τ |∞

(cid:105)

2)

.

By noting that the cotangent Laplacian weights can be written (see Fig. 9) as

wi,j =

−(cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
A(i, j, k)

+

−(cid:96)2

jh + (cid:96)2
ih

ij + (cid:96)2
A(i, j, h)

,

we have from the previous Bilipschitz bounds that

τ (wi,j) ≤ wi,j

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

+ 2|∇τ |∞

(cid:2)1 − C|∇τ |∞α−2

min

(cid:3)−1

τ (wi,j) ≥ wi,j

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

− 2|∇τ |∞

(cid:2)1 + C|∇τ |∞α−2

min

(cid:3)−1

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

(cid:32) (cid:96)2

ij + (cid:96)2

jk + (cid:96)2
ik

ij + (cid:96)2
(cid:96)2

jh + (cid:96)2
ih

A(i, j, k)

A(i, j, h)

+

+

(cid:33)

(cid:33)

,

,

which proves that, up to second order terms, the cotangent weights are Lipschitz continuous to deformations.

Finally, since the mesh Laplacian operator is constructed as diag( ¯A)−1(U − W ), with ¯Ai,i = 1
3

(cid:80)

j,k;(i,j,k)∈F A(i, j, k),

and U = diag(W 1), let us show how to bound (cid:107)∆ − τ (∆)(cid:107) from

¯Ai,i(1 − αM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ ( ¯Ai,i) ≤ ¯Ai,i(1 + αM|∇τ |∞ + o(|∇τ |∞

2))

and

wi,j(1 − βM|∇τ |∞ − o(|∇τ |∞

2)) ≤ τ (wi,j) ≤ wi,j(1 + βM|∇τ |∞ + o(|∇τ |∞

2)) .

(13)

(14)

k

αij

aijk

j

(cid:96)ij

ai

i

βij

h

Figure 9. Triangular mesh and Cotangent Laplacian (ﬁgure reproduced from [5])

Using the fact that ¯A, τ ( ¯A) are diagonal, and using the spectral bound for k × m sparse matrices from [8], Lemma 5.12,

(cid:107)Y (cid:107)2 ≤ max

i

(cid:88)

j; Yi,j (cid:54)=0

|Yi,j|

|Yr,j|

,

(cid:33)

(cid:32) l

(cid:88)

r=1

the bounds (13) and (14) yield respectively

τ ( ¯A) = ¯A(1 + (cid:15)τ ) , with (cid:107)(cid:15)τ (cid:107) = o(|∇τ |∞) , and
τ (U − W ) = U − W + ητ , with (cid:107)ητ (cid:107) = o(|∇τ |∞) .

It results that, up to second order terms,
(cid:13)τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )(cid:13)
(cid:107)∆ − τ (∆)(cid:107) = (cid:13)
(cid:13)
(cid:13)
(cid:0) ¯A[1 + (cid:15)τ ](cid:1)−1
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
=
(cid:13)
= (cid:13)
(cid:13)(cid:15)τ ∆ + ¯A−1ητ
= o(|τ |∞) ,

(cid:13)
(cid:13) + o(|∇τ |∞

1 − (cid:15)τ + o(|∇τ |∞

2)

=

[U − W + ητ ] − ¯A−1(U − W )

(cid:13)
(cid:13)
(cid:13)

(cid:17) ¯A−1(U − W + ητ ) − ¯A−1(U − W )
2)

(cid:13)
(cid:13)
(cid:13)

which shows that the Laplacian is stable to deformations in operator norm. Finally, by denoting ˜xτ a layer of the deformed
Laplacian network

it follows that

Also,

˜xτ = ρ(Ax + Bτ (∆)x) ,

(cid:107)˜x − ˜xτ (cid:107) ≤ (cid:107)B(∆ − τ (∆)x(cid:107)
≤ C(cid:107)B(cid:107)|∇τ |∞(cid:107)x(cid:107) .

(cid:107)˜x − ˜yτ (cid:107) ≤ (cid:107)A(x − y) + B(∆x − τ (∆)y)(cid:107)

≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))(cid:107)x − y(cid:107) + (cid:107)∆ − τ (∆)(cid:107)(cid:107)x(cid:107)
≤ ((cid:107)A(cid:107) + (cid:107)B(cid:107)(cid:107)∆(cid:107))
(cid:125)

(cid:107)x − y(cid:107) + C|∇τ |∞
(cid:124) (cid:123)(cid:122) (cid:125)
δ2

(cid:107)x(cid:107) ,

(cid:123)(cid:122)
δ1

(cid:124)

and therefore, by plugging (17) with y = ˜xτ , K layers of the Laplacian network satisfy

(cid:107)Φ(x; ∆) − Φ(x; τ (∆)(cid:107) ≤





(cid:89)

j≤K−1








δ1(j)

 (cid:107)˜x − ˜xτ (cid:107) +



(cid:88)

(cid:89)

j<K−1

j(cid:48)≤j







δ1(j(cid:48))δ2(j)

 |∇τ |∞(cid:107)x(cid:107)

≤

C



δ1(j)

 (cid:107)B(cid:107) +



(cid:89)

j≤K−1

(cid:88)

(cid:89)

δ1(j(cid:48))δ2(j)

j<K−1

j(cid:48)≤j






 |∇τ |∞(cid:107)x(cid:107) . (cid:3) .

(15)
(16)

(17)

B.4. Proof of (d)

The proof is also analogous to the proof of (c), with the difference that now the Dirac operator is no longer invariant to

orthogonal transformations, only to translations. Given two points p, q, we verify that

(cid:107)p − q − τ (p) − τ (q)(cid:107) ≤ (cid:102)|τ |∞(cid:107)p − q(cid:107) ,

(cid:107)D − τ (D)(cid:107) = o((cid:102)|τ |∞) .

which, following the previous argument, leads to

C. Theorem 4.2

C.1. Proof of part (a)

The proof is based on the following lemma:

Lemma C.1 Let xN , yN ∈ H(MN ) such that ∀ N , (cid:107)xN (cid:107)H ≤ c,(cid:107)yN (cid:107)H ≤ c. Let ˆxN = EN (xN ), where EN is the
eigendecomposition of the Laplacian operator ∆N on MN , , with associated eigenvalues λ1 . . . λN in increasing order. Let
γ > 0 and β be deﬁned as in (5) for xN and yN . If β > 1 and (cid:107)xN − yN (cid:107) ≤ (cid:15) for all N ,

where C is a constant independent of (cid:15) and N .

(cid:107)∆N (xN − yN )(cid:107)2 ≤ C(cid:15)2− 1

β−1/2 ,

One layer of the network will transform the difference x1 − x2 into ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2). We verify that

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + (cid:107)B(cid:107)(cid:107)∆(x1 − x2)(cid:107) .

We now apply Lemma C.1 to obtain

(cid:107)ρ(Ax1 + B∆x1) − ρ(Ax2 + B∆x2)(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x1 − x2(cid:107) + C(cid:107)B(cid:107)(cid:107)x1 − x2(cid:107)

β−1
β−1/2

≤ (cid:107)x1 − x2(cid:107)

β−1
β−1/2

(cid:16)

(cid:107)A(cid:107)(cid:107)x1 − x2(cid:107)(2β−1)−1

+ C(cid:107)B(cid:107)

(cid:17)

≤ C((cid:107)A(cid:107) + (cid:107)B(cid:107))(cid:107)x1 − x2(cid:107)

β−1
β−1/2 ,

where we redeﬁne C to account for the fact that (cid:107)x1 − x2(cid:107)(2β−1)−1

is bounded. We have just showed that

with fr = C((cid:107)Ar(cid:107) + (cid:107)Br(cid:107)) and gr = βr−1

βr−1/2 . By cascading (20) for each of the R layers we thus obtain

(cid:107)x(r+1)
1

− x(r+1)
2

(cid:107) ≤ fr(cid:107)x(r)

1 − x(r)

2 (cid:107)gr

(cid:107)Φ∆(x1) − Φ∆(x2)(cid:107) ≤

(cid:35)

(cid:81)
f
r

r(cid:48)>r gr(cid:48)

(cid:107)x1 − x2(cid:107)

(cid:81)R

r=1 gr ,

(cid:34) R
(cid:89)

r=1

which proves (6) (cid:3).

Proof of (19): Let {e1, . . . , eN } be the eigendecomposition of ∆N . For simplicity, we drop the subindex N in the signals
from now on. Let ˆx(k) = (cid:104)x, ek(cid:105) and ˜x(k) = λk ˆx(k); and analogously for y. From the Parseval identity we have that
(cid:107)x(cid:107)2 = (cid:107)ˆx(cid:107)2. We express (cid:107)∆(x − y)(cid:107) as

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 .
λ2

(cid:88)

k≤N

The basic principle of the proof is to cut the spectral sum (22) in two parts, chosen to exploit the decay of ˜x(k). Let

F (x)(k) =

(cid:80)

k(cid:48)≥k ˜x(k)2
(cid:107)x(cid:107)2
H

=

(cid:80)

k(cid:48)≥k ˜x(k)2
k(cid:48) ˜x(k)2 =
(cid:80)

(cid:80)

k(cid:48)≥k λ2
(cid:80)
k(cid:48) λ2

k ˆx(k)2
k ˆx(k)2 ≤ 1 ,

(18)

(19)

(20)

(21)

(22)

and analogously for y. For any cutoff k∗ ≤ N we have

(cid:107)∆(x − y)(cid:107)2 =

k(ˆx(k) − ˆy(k))2 +
λ2

k(ˆx(k) − ˆy(k))2
λ2

(cid:88)

(cid:88)

k≤k∗

≤ λ2
k∗
≤ λ2
k∗
≤ λ2
k∗
where we denote for simplicity F (k∗) = max(F (x)(k∗), F (y)(k∗)). By assumption, we have λ2
k
F (k) (cid:46) (cid:88)

(cid:15)2 + 2(F (x)(k∗)(cid:107)x(cid:107)2
(cid:15)2 + 2F (k∗)((cid:107)x(cid:107)2
(cid:15)2 + 4F (k∗)D2 ,

k2(γ−β) (cid:39) k1+2(γ−β) .

H + (cid:107)y(cid:107)2

H)

H)

k>k∗
H + F (y)(k∗)(cid:107)y(cid:107)2

(cid:46) k2γ and

By denoting ˜β = β − γ − 1/2, it follows that

k(cid:48)≥k

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2k2γ

∗ + 4D2k−2 ˜β

∗

(cid:15)22γk2γ−1 − 2 ˜β4D2k−2 ˜β−1 = 0, thus
(cid:20) 4βD2
γ(cid:15)2

k∗ =

(cid:21) 1

2γ+2 ˜β

.

(cid:107)∆(x − y)(cid:107)2 (cid:46) (cid:15)2− 1

γ+ ˜β = (cid:15)2− 1

β−1/2 ,

Optimizing for k∗ yields

which proves part (a) (cid:3).

C.2. Proof of part (b)

We will use the following lemma:

By plugging (25) back into (24) and dropping all constants independent of N and (cid:15), this leads to

Lemma C.2 Let M = (V, E, F ) is a non-degenerate mesh, and deﬁne

η1(M) = sup

, η2(M) = sup

, η3(M) = αmin .

¯Ai
¯Aj

(i,j)∈E

ij + (cid:96)2
(cid:96)2

jk + (cid:96)2
ik

(i,j,k)∈F

A(i, j, k)

Then, given a smooth deformation τ and x deﬁned in M, we have

(cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) ,

where C depends only upon η1, η2 and η3.

In that case, we need to control the difference ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x). We verify that

(cid:107)ρ(Ax + B∆x) − ρ(Ax + Bτ (∆)x)(cid:107) ≤ (cid:107)B(cid:107)(cid:107)(∆ − τ (∆))x(cid:107) .

By Lemma C.2 it follows that (cid:107)(∆ − τ (∆))x(cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) and therefore, by denoting x(1)
x(1)
2 = ρ(Ax + Bτ (∆)x), we have

1 = ρ(Ax + B∆x) and

2 (cid:107) ≤ C|∇τ |∞(cid:107)∆x(cid:107) = C|∇τ |∞(cid:107)x(cid:107)H .

(28)

1 − x(1)
By applying again Lemma C.1, we also have that

(cid:107)x(1)

(cid:107)∆x(1)

1 − τ (∆)x(1)

2 (cid:107) = (cid:107)∆x(1)
= (cid:107)∆(x(1)
≤ C(cid:107)x(1)

1 − (∆ + τ (∆) − ∆)x(1)
2 (cid:107)
2 ) + (τ (∆) − ∆)x(1)
1 − x(1)
2 (cid:107)
1 − x(1)
β1−1/2 + |∇τ |∞(cid:107)x(1)
2 (cid:107)
β1−1
β1−1/2 ,

β1−1

(cid:46) C|∇τ |∞

2 (cid:107)H

(23)

(24)

(25)

(26)

(27)

which, by combining it with (28) and repeating through the R layers yields

(cid:107)Φ∆(x, M) − Φ∆(x, τ (M)(cid:107) ≤ C|∇τ |∞

(cid:81)R

r=1

βr −1
βr −1/2 ,

(29)

which concludes the proof (cid:3).

Proof of (27): The proof follows closely the proof of Theorem 4.1, part (c). From (13) and (14) we have that

τ ( ¯A) = ¯A(I + Gτ ) , with |Gτ |∞ ≤ C(η2, η3)|∇τ |∞ , and
τ (U − W ) = (I + Hτ )(U − W ) , with |Hτ |∞ ≤ C(η2, η3)|∇τ |∞ .

It follows that, up to second order o(|∇τ |∞

2) terms,

τ (∆) − ∆ = τ ( ¯A)−1(τ (U ) − τ (W )) − ¯A−1(U − W )

= (cid:0) ¯A[1 + Gτ ](cid:1)−1
(cid:39) ¯A−1Hτ (U − W ) + Gτ ∆ .

[(I + Hτ )(U − W )] − ¯A−1(U − W )

(30)

By writing ¯A−1Hτ = (cid:102)Hτ ¯A−1, and since ¯A is diagonal, we verify that

( (cid:102)Hτ )i,j = (Hτ )i,j

, with

Ai,i
Aj,j

Ai,i
Aj,j

≤ η1, and hence that

¯A−1Hτ (U − W ) = (cid:102)Hτ ∆ , with | (cid:102)Hτ |∞ ≤ C(η1, η2, η3)|∇τ |∞ .

(31)

We conclude by combining (30) and (31) into

(cid:107)(∆ − τ (∆))x(cid:107) = (cid:107)(Gτ + (cid:102)Hτ )∆x(cid:107)

≤ C (cid:48)(η1, η2, η3)|∇τ |∞(cid:107)∆x(cid:107) ,

which proves (27) (cid:3)

C.3. Proof of part (c)

This result is a consequence of the consistency of the cotangent Laplacian to the Laplace-Beltrami operator on S [45]:

Theorem C.3 ([45], Thm 3.4) Let M be a compact polyhedral surface which is a normal graph over a smooth surface S
with distortion tensor T , and let ¯T = (det T )1/2T −1. If the normal ﬁeld uniform distance d(T , 1) = (cid:107) ¯T − 1(cid:107)∞ satisﬁes
d(T , 1) ≤ (cid:15), then

(cid:107)∆M − ∆S(cid:107) ≤ (cid:15) .

(32)

If ∆M converges uniformly to ∆S, in particular we verify that

(cid:107)x(cid:107)H(M) → (cid:107)x(cid:107)H(S) .

Thus, given two meshes M, M(cid:48) approximating a smooth surface S in terms of uniform normal distance, and the corre-

sponding irregular sampling x and x(cid:48) of an underlying function ¯x : S → R, we have

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ (cid:107)A(cid:107)(cid:107)x − x(cid:48)(cid:107) + (cid:107)B(cid:107)(cid:107)∆Mx − ∆M(cid:48)x(cid:48)(cid:107) .

(33)

Since M and M(cid:48) both converge uniformly normally to S and ¯x is Lipschitz on S, it results that

thus (cid:107)x − x(cid:48)(cid:107) ≤ 2L(cid:15). Also, thanks to the uniform normal convergence, we also have convergence in the Sobolev sense:

(cid:107)x − ¯x(cid:107) ≤ L(cid:15) , and (cid:107)x(cid:48) − ¯x(cid:107) ≤ L(cid:15) ,

(cid:107)x − ¯x(cid:107)H (cid:46) (cid:15) , (cid:107)x(cid:48) − ¯x(cid:107)H (cid:46) (cid:15) ,

which implies in particular that

From (33) and (34) it follows that

(cid:107)x − x(cid:48)(cid:107)H (cid:46) (cid:15) .

(cid:107)ρ(Ax + B∆Mx) − ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48))(cid:107) ≤ 2(cid:107)A(cid:107)L(cid:15) +

(34)

(35)

+(cid:107)B(cid:107)(cid:107)∆Mx − ∆S ¯x + ∆S ¯x − ∆M(cid:48)x(cid:48)(cid:107)

≤ 2(cid:15) ((cid:107)A(cid:107)L + (cid:107)B(cid:107)) .

By applying again Lemma C.1 to ˜x = ρ(Ax + B∆Mx), ˜x(cid:48) = ρ(Ax(cid:48) + B∆M(cid:48)x(cid:48)), we have

We conclude by retracing the same argument as before, reapplying Lemma C.1 at each layer to obtain

(cid:107)˜x − ˜x(cid:48)(cid:107)H ≤ C(cid:107)˜x − ˜x(cid:48)(cid:107)

β1−1
β1−1/2 (cid:46) (cid:15)

β1−1
β1−1/2 .

(cid:107)ΦM(x) − ΦM(cid:48)(x(cid:48))(cid:107) ≤ C(cid:15)

(cid:81)R

r=1

βr −1
βr −1/2 . (cid:3) .

D. Proof of Corollary 4.3

We verify that

(cid:107)ρ(B∆x) − ρ(Bτ (∆)τ (x))(cid:107) ≤ (cid:107)B(cid:107)(cid:107)∆x − τ (∆)τ (x)(cid:107)

≤ (cid:107)B(cid:107)(cid:107)∆(x − τ (x)) + (∆ − τ (∆))(τ (x))(cid:107)
≤ (cid:107)B(cid:107)((cid:107)∆(x − τ (x))(cid:107) + (cid:107)(∆ − τ (∆))(τ (x))(cid:107) .

The second term is o(|∇τ |∞) from Lemma C.2. The ﬁrst term is

(cid:107)x − τ (x)(cid:107)H ≤ (cid:107)∆(I − τ )(cid:107)(cid:107)x(cid:107) ≤ (cid:107)∇2τ (cid:107)(cid:107)x(cid:107) ,

where (cid:107)∇2τ (cid:107) is the uniform Hessian norm of τ . The result follows from applying the cascading argument from last section.
(cid:3)

E. Preliminary Study: Metric Learning for Dense Correspondence

As an interesting extension, we apply the architecture we built in Experiments 6.2 directly to a dense shape correspondence

problem.

Similarly as the graph correspondence model from [33], we consider a Siamese Surface Network, consisting of two
identical models with the same architecture and sharing parameters. For a pair of input surfaces M1, M2 of N1, N2 points
respectively, the network produces embeddings E1 ∈ RN1×d and E2 ∈ RN2×d. These embeddings deﬁne a trainable
similarity between points given by

si,j =

e(cid:104)E1,i,E2,j (cid:105)
j(cid:48) e(cid:104)E1,i,E2,j(cid:48) (cid:105)

,

(cid:80)

(36)

which can be trained by minimizing the cross-entropy relative to ground truth pairs. A diagram of the architecture is

provided in Figure 10.

In general, dense shape correspondence is a task that requires a blend of intrinsic and extrinsic information, motivating
the use of data-driven models that can obtain such tradeoffs automatically. Following the setup in Experiment 6.2, we use
models with 15 ResNet-v2 blocks with 128 output features each, and alternate Laplace and Dirac based models with Average
Pooling blocks to cover a larger context: The input to our network consists of vertex positions only.

We tested our architecture on a reconstructed (i.e. changing the mesh connectivity) version of the real scan of FAUST
dataset[3]. The FAUST dataset contains 100 real scans and their corresponding ground truth registrations. The ground truth
is based on a deformable template mesh with the same ordering and connectivity, which is ﬁtted to the scans. In order to
eliminate the bias of using the same template connectivity, as well as the need of a single connected component, the scans
are reconstructed again with [19]. To foster replicability, we release the processed dataset in the additional material. In our
experiment, we use 80 models for training and 20 models for testing.

Figure 10. Siamese network pipeline: the two networks take vertex coordinates of the input models and generate a high dimensional feature
vector, which are then used to deﬁne a map from M1 to M2. Here, the map is visualized by taking a color map on M2, and transferring
it on M1

Figure 11. Additional results from our setup. Plot in the middle shows rate of correct correspondence with respect to geodesic error [23].
We observe that Laplace is performing similarly to Dirac in this scenario. We believe that the reason is that the FAUST dataset contains
only isometric deformations, and thus the two operators have access to the same information. We also provide visual comparison, with the
transfer of a higher frequency colormap from the reference shape to another pose.

Since the ground truth correspondence is implied only through the common template mesh, we compute the correspon-
dence between our meshes with a nearest neighbor search between the point cloud and the reconstructed mesh. Consequently,

Figure 12. Heat map illustrating the point-wise geodesic difference between predicted correspondence point and the ground truth. The unit
is proportional to the geodesic diameter, and saturated at 10%.

Figure 13. A failure case of applying the Laplace network to a new pose in the FAUST benchmark dataset. The network confuses between
left and right arms. We show the correspondence visualization for front and back of this pair.

due to the drastic change in vertex replacement after the remeshing, only 60-70 percent of labeled matches are used. Although
making it more challenging, we believe this setup is close to a real case scenario, where acquisition noise and occlusions are
unavoidable.

Our preliminary results are reported in Figure 11. For simplicity, we generate predicted correspondences by simply taking
the mode of the softmax distribution for each reference node i: ˆj(i) = arg maxj si,j, thus avoiding a reﬁnement step that is
standard in other shape correspondence pipelines. The MLP model uses no context whatsoever and provides a baseline that
captures the prior information from input coordinates alone. Using contextual information (even extrinsically as in point-
cloud model) brings signiﬁcative improvments, but these results may be substantially improved by encoding further prior
knowledge. An example of the current failure of our model is depitcted in Figure 13, illustrating that our current architecture
does not have sufﬁciently large spatial context to disambiguate between locally similar (but globally inconsistent) parts.

We postulate that the FAUST dataset [3] is not an ideal ﬁt for our contribution for two reasons: (1) it is small (100

models), and (2) it contains only near-isometric deformations, which do not require the generality offered by our network. As
demonstrated in [28], the correspondence performances can be dramatically improved by constructing basis that are invariant
to the deformations. We look forward to the emergence of new geometric datasets, and we are currently developing a capture
setup that will allow us to acquire a more challenging dataset for this task.

F. Further Numerical Experiments

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 14. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 15. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 16. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

MLP

AvgPool

Laplace

Dirac

Figure 17. Qualitative comparison of different models. We plot 1th, 10th, 20th, 30th and 40th predicted frame correspondingly.

Ground Truth

Laplace

Dirac

Figure 18. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.

Figure 19. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth
and prediction: green - smaller error, red - larger error.

Figure 20. From left to right: set-to-set, ground truth and Dirac based model. Color corresponds to mean squared error between ground
truth and prediction: green - smaller error, red - larger error.

