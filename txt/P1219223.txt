9
1
0
2
 
g
u
A
 
6
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
8
4
1
0
.
5
0
9
1
:
v
i
X
r
a

WoodScape: A multi-task, multi-camera ﬁsheye dataset for autonomous driving

Senthil Yogamani, Ciar´an Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek O’Dea,
Michal Uˇriˇc´aˇr, Stefan Milz, Martin Simon, Karl Amende, Christian Witt, Hazem Rashed,
Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier Perrotton, Patrick P´erez

https://github.com/valeoai/WoodScape

firstname.lastname@valeo.com

Figure 1: We introduce WoodScape, the ﬁrst ﬁsheye image dataset dedicated to autonomous driving. It contains four cameras
covering 360° accompanied by a HD laser scanner, IMU and GNSS. Annotations are made available for nine tasks, notably
3D object detection, depth estimation (overlaid on front camera) and semantic segmentation as illustrated here.

Abstract

Fisheye cameras are commonly employed for obtaining a
large ﬁeld of view in surveillance, augmented reality and in
particular automotive applications. In spite of their preva-
lence, there are few public datasets for detailed evaluation
of computer vision algorithms on ﬁsheye images. We re-
lease the ﬁrst extensive ﬁsheye automotive dataset, Wood-
Scape, named after Robert Wood who invented the ﬁsheye
camera in 1906. WoodScape comprises of four surround
view cameras and nine tasks including segmentation, depth
estimation, 3D bounding box detection and soiling detec-
tion. Semantic annotation of 40 classes at the instance level
is provided for over 10,000 images and annotation for other
tasks are provided for over 100,000 images. With Wood-
Scape, we would like to encourage the community to adapt
computer vision models for ﬁsheye camera instead of using
na¨ıve rectiﬁcation.

1. Introduction

Fisheye lenses provide a large ﬁeld of view (FOV) us-
ing a highly non-linear mapping instead of the standard per-
spective projection. However, it comes at the cost of strong
radial distortion. Fisheye cameras are so-named because

they relate to the 180° view of the world that a ﬁsh has ob-
serving the water surface from below, a phenomenon known
as Snell’s window. Robert Wood originally coined the term
in 1906 [58], and constructed a basic ﬁsheye camera by tak-
ing a pin-hole camera and ﬁlling it with water. It was later
replaced with a hemispherical lens [3]. To pay homage to
the original inventor and coiner of the term “ﬁsheye”, we
have named our dataset WoodScape.

Large FOV cameras are necessary for various computer
vision application domains, including video surveillance
[28] and augmented reality [46], and have been of particular
interest in autonomous driving [23].
In automotive, rear-
view ﬁsheye cameras are commonly deployed in existing
vehicles for dashboard viewing and reverse parking. While
commercial autonomous driving systems typically make
use of narrow FOV forward facing cameras at present,
full 360° perception is now investigated for handling
more complex use cases. In spite of this growing interest,
there is relatively little literature and datasets available.
Some examples of the few datasets that have ﬁsheye are:
Visual SLAM ground truth for indoor scenes with omni-
directional cameras in [7], SphereNet [9] containing 1200
labelled images of parked cars using 360° cameras (not
strictly ﬁsheye) and, in automotive, the Oxford Robotcar

1

Figure 2: Sample images from the surround-view camera
network showing wide ﬁeld of view and 360◦ coverage.

dataset [37] containing a large scale relocalization dataset.

WoodScape is a comprehensive dataset for 360° sensing
around a vehicle using the four ﬁsheye cameras shown in
Figure 2. It aims at complementing the range of already ex-
isting automotive datasets where only narrow FOV image
data is present: among those, KITTI [17] was the ﬁrst pio-
neering dataset with a variety of tasks, which drove a lot of
research for autonomous driving; Cityscapes [10] provided
the ﬁrst comprehensive semantic segmentation dataset and
Mapillary [39] provided a signiﬁcantly larger dataset; Apol-
loscape [24] and BDD100k [59] are more recent datasets
that push the annotation scale further. WoodScape is unique
in that it provides ﬁsheye image data, along with a compre-
hensive range of annotation types. A comparative summary
of these different datasets is provided in Table 1. The main
contributions of WoodScape are as follows:

1. First ﬁsheye dataset comprising of over 10,000 images

containing instance level semantic annotation.

2. Four-camera nine-task dataset designed to encourage

uniﬁed multi-task and multi-camera models.

3. Introduction of a novel soiling detection task and re-

lease of ﬁrst dataset of its kind.

4. Proposal of an efﬁcient metric for the 3D box detection

task which improves training time by 95x.

The paper is organized as follows. Section 2 provides
an overview of ﬁsheye camera model, undistortion meth-
ods and ﬁsheye adaption of vision algorithms. Section 3
discusses the details of the dataset including goals, capture
infrastructure and dataset design. Section 4 presents the list
of supported tasks and baseline experiments. Finally, Sec-
tion 5 summarizes and concludes the paper.

2. Overview of Fisheye Camera Projections

Fisheye cameras offer a distinct advantage for automo-
tive applications. Given their extremely wide ﬁeld of view,
they can observe the full surrounding of a vehicle with a
minimal number of sensors, with just four cameras typi-

Figure 3: Comparison of ﬁsheye models.

cally being required for full 360◦ coverage (Figure 2). This
advantage comes with some drawbacks in the signiﬁcantly
more complex projection geometry that ﬁsheye cameras ex-
hibit. That is, images from ﬁsheye cameras display severe
distortion.

Typical camera datasets consist of narrow FOV camera
data where a simple pinhole projection model is commonly
employed. In case of ﬁsheye camera images, it is imperative
that the appropriate camera model is well understood either
to handle distortion in the algorithm or to warp the image
prior to processing. This section is intended to highlight to
the reader that the ﬁsheye camera model requires speciﬁc
attention. We provide a brief overview and references for
further details, and discuss the merits of operating on the
raw ﬁsheye versus undistortion of the image.

2.1. Fisheye Camera Models

Fisheye distortion is modelled by a radial mapping func-
tion r(θ), where r(θ) is the distance on the image from the
centre of distortion, and is a function of the angle θ of the
incident ray against the optical axis of the camera system.
The centre of distortion is the intersection of the optical axis
with the image plane, and is the origin of the radial mapping
function r(θ). Stereographic projection [22] is the simplest
model which uses a mapping from a sphere to a plane. More
recent projection models are Uniﬁed Camera Model (UCM)
[1, 7] and eUCM (Enhanced UCM) [27]. More detailed
analysis of accuracy of various projection models is dis-
cussed in [25]. These models are not a perfect ﬁt for ﬁsheye
cameras as they encode a speciﬁc geometry (e.g. spherical
projection), and errors arising in the model are compensated
by using an added distortion correction component.

In WoodScape, we provide model parameters for a more
generic ﬁsheye intrinsic calibration that is independent of
any speciﬁc projection model, and does not require the
added step of distortion correction. Our model is based on
a fourth order polynomial mapping incident angle to image
radius in pixels (r(θ) = a1θ + a2θ2 + a3θ3 + a4θ4). In

The missing FOV can be resolved by multiple linear
viewports as shown in Figure 4 (b). However there are is-
sues in the transition region from one plane to another. This
can be viewed as a piecewise linear approximation of the
ﬁsheye lens manifold. Figure 4 (c) demonstrates a quasi-
linear correction using a cylindrical viewport, where it is
linear in vertical direction and straight vertical objects like
pedestrians are preserved. However, there is a quadratic dis-
tortion along the horizontal axis. In many scenarios, it pro-
vides a reasonable trade-off but it still has limitations. In
case of learning algorithms, a parametric transform can be
optimized for optimal performance of the target application
accuracy.

Because of fundamental limitations of undistortion, an
alternate approach of adapting the algorithm incorporating
ﬁsheye model discussed in previous section could be an op-
timal solution. In case of classical geometric algorithms, an
analytical version of non-linear projection can be incorpo-
rated. For example, Kukelova et al. [32] extend homogra-
phy estimation by incorporating radial distortion model. In
case of deep learning algorithms, a possible solution could
be to train the CNN model to learn the distortion. How-
ever, the translation invariance assumption of CNN funda-
mentally breaks down due to spatially variant distortion and
thus it is not efﬁcient to let the network learn it implicitly.
This had led to several adaptations of CNN to handle spher-
ical images such as [52] and [9]. However, spherical models
do not provide an accurate ﬁt for ﬁsheye lenses and it is an
open problem.

3. Overview of WoodScape Dataset

3.1. High-Level Goals

Fisheye: One of the main goals of this dataset is to
encourage the research community to develop vision al-
gorithms natively on ﬁsheye images without undistortion.
There are very few public ﬁsheye datasets and none of them
provide semantic segmentation annotation. Fisheye is par-
ticularly beneﬁcial to automotive low speed manoeuvring
scenarios such as parking [21] where accurate full coverage
near ﬁeld sensing can be achieved with just four cameras.

Multi-camera: Surround view systems have at least
four cameras rigidly connected to the body of the car. Pless
[42] did pioneering work in deriving a framework for mod-
eling a network of cameras as one, this approach is useful
for geometric vision algorithms like visual odometry. How-
ever, for semantic segmentation algorithms, there is no lit-
erature on joint modeling of rigidly connected cameras.

Multi-task: Autonomous driving has various vision
tasks and most of the work has been focused on solving
individual tasks independently. However, there is a recent
trend [30, 53, 51, 8] to solve tasks using a single multi-task
model to enable efﬁcient reuse of encoder features and also

Figure 4: Undistorting the ﬁsheye image: (a) Rectilinear
correction; (b) Piecewise linear correction; (c) Cylindrical
correction. Left: raw image; Right: undistorted image.

our experience, higher orders provide no additional accu-
racy. Each video sequence in the dataset is provided with
parameters for the fourth order polynomial model of ﬁsh-
eye intrinsics.

As a comparison, to give the reader an understanding of
how different models behave, Figure 3 shows the mapping
function r(θ) for ﬁve different projection models, which are
Polynomial, Rectilinear, Stereographic, UCM and eUCM.
The parameters of the fourth order polynomial are taken
from a calibration of our ﬁsheye lens. We optimized the pa-
rameters for the other models to match this model in a range
of 0◦ to 120◦ (i.e. up to FOV of 240◦). The plot indicates
that the difference to the original fourth order polynomial is
about four pixels for UCM and one pixel for eUCM for low
incident angles. For larger incident angles, these models are
less precise.

2.2. Image Undistortion vs. Model Adaptation

Standard computer vision models do not generalize eas-
ily to ﬁsheye cameras because of large non-linear distortion.
For example, translation invariance is lost for a standard
convolutional neural net (CNN). The na¨ıve way to develop
algorithms for ﬁsheye cameras is to perform rectilinear cor-
rection so that standard models can be applied. The sim-
plest undistortion is to re-warp pixels to a rectilinear image
as shown in Figure 4 (a). But there are two major issues.
Firstly, the FOV is greater than 180◦, hence there are rays
incident from behind the camera and it is not possible to es-
tablish a complete mapping to a rectilinear viewport. This
leads to a loss of FOV, this is seen via the missing yellow
pillars in the corrected image. Secondly, there is an issue of
resampling distortion, which is more pronounced near the
periphery of the image where a smaller region gets mapped
to a larger region.

Figure 5: SLAM point cloud top-view of a parking lot.
Height of the objects is color coded (green for high value,
blue for medium value and grayscape for low value).

provide regularization while learning multiple tasks. How-
ever, in these cases, only the encoder is shared and there
is no synergy among decoders. Existing datasets are pri-
marily designed to facilitate task-speciﬁc learning and they
don’t provide simultaneous annotation for all the tasks. We
have designed our dataset so that simultaneous annotation
is provided for various tasks with some exceptions due to
practical limitations of optimal dataset design for each task.

3.2. Dataset Acquisition

Our diverse dataset originates from three distinct geo-
graphical locations: USA, Europe, and China. While the
majority of data was obtained from saloon vehicles there
is a signiﬁcant subset from a sports utility vehicle ensuring
a strong mix in sensor mechanical conﬁgurations. Driving
scenarios are divided across the highway, urban driving and
parking use cases. Intrinsic and extrinsic calibrations are
provided for all sensors as well as timestamp ﬁles to allow
synchronization of the data. Relevant vehicle’s mechanical
data (e.g. wheel circumference, wheel base) are included.
High-quality data is ensured via quality checks at all stages
of the data collection process. Annotation data undergoes a
rigorous quality assurance by highly skilled reviewers. The
sensors recorded for this dataset are listed below:

• 4x 1MPx RGB ﬁsheye cameras (190◦ horizontal FOV)
• 1x LiDAR rotating at 20Hz (Velodyne HDL-64E)
• 1x GNSS/IMU (NovAtel Propak6 & SPAN-IGM-A1)
• 1x GNSS Positioning with SPS (Garmin 18x)
• Odometry signals from the vehicle bus.

Our WoodScape dataset provides labels for several au-
tonomous driving tasks including semantic segmentation,
monocular depth estimation, object detection (2D & 3D
bounding boxes), visual odometry, visual SLAM, mo-
tion segmentation, soiling detection and end-to-end driving
(driving controls). In Table 1, we compare several proper-
ties of popular datasets against WoodScape. In addition to
providing ﬁsheye data, we provide data for many more tasks

Figure 6: Distribution of instances of semantic segmenta-
tion classes in WoodScape. Minimum size is 300 pixels.

than is typical (nine in total), providing completely novel
tasks such as soiled lens detection.
Images are provided
at 1MPx 24-bit resolution and videos are uncompressed at
30fps ranging in duration from 30s to 120s. The dataset also
provides a set of synthetic data using accurate models of
the real cameras, enabling investigations of additional tasks.
The camera has a HDR sensor with a rolling shutter and a
dynamic range of 120 dB. It has features including black
level correction, auto-exposure control, auto-gain control,
lens shading (optical vignetting) compensation, gamma cor-
rection and automatic white balance for color correction.

The laser scanner point cloud provided in our data set
is accurately preprocessed using a commercial SLAM algo-
rithm to provide a denser point cloud ground truth for tasks
such as depth estimation and visual SLAM, as shown in Fig-
ure 5. In terms of recognition tasks, we provide labels for
forty classes, the distribution of the main classes is shown
in Figure 6. Note, that for the purposes of display in this
paper, we have merged some of the classes in Figure 6 (e.g.
‘two wheelers’ is a merge of ‘bicycle’ and ‘motorcycle’).

3.3. Dataset Design

The design of a dataset for machine learning is a very
complex task. Unfortunately, due to the overwhelming
success of deep learning, recently it does not get as much
attention as it still deserves in our opinion. However, at
the same time, it was shown that careful inspection of
the training sets for outliers improves the robustness of
deep neural networks [36], especially with regards to the
adversarial examples. Therefore, we believe that whenever
a new dataset is released, there should be a signiﬁcant
effort spent not only on the data acquisition but also on the
careful consistency check and on the database splitting for
the needs of training, model selection and testing.

Sampling strategy: Let us deﬁne some notation and nam-

Table 1: Summary of various autonomous driving datasets containing semantic annotation

Task/Info

Capture Information

Quantity

Year
State/cities

Cityscapes
[10]
2016
2/50

Mapillary
[39]
2017
50+/100+

KITTI
[17]
2012/14/15
1/1

1 LiDAR
GPS

Other sensors

-

Segmentation

3D Bounding Box

Camera Information

2D Bounding Box1

Cameras
Tasks
Classes
Frames
Classes
Frames
Classes
Frames
Frames
Depth Estimation
Frames
Motion Segmentation
Soiling Detection
Frames
Visual SLAM/Odometry Videos
Videos
End-to-end Driving
Synthetic Data
Frames
12D box annotation can be obtained for other datasets from instance segmentation.

4
6
8
400
3
15k
3
15k
93k
1.6k
-
33
-
-

2
1
30
5k
-
-
-
-
-
-
-
-
-
-

ing conventions, which we will refer to ﬁrst (we follow the
deﬁnitions provided in [4]). A population is a set of all ex-
isting feature vectors. A subset of the population collected
during some process is called a sample set S. A representa-
tive set S ∗ is signiﬁcantly smaller than S, while capturing
most of the information from S (compared to any different
subset of the same size), and has low redundancy among the
representatives it contains.

In an ideal world, we would like our training set to be
equal to S ∗. This is extremely difﬁcult to achieve in prac-
tice. One approach to approximate this is the concept of the
minimal consistent subset of a training set, where, given a
training set T , we are interested in a subset T ∗, being the
smallest set such that Acc(T ∗) = Acc(T ), where Acc(·)
denotes the selected accuracy measure (e.g. the Jaccard in-
dex). Note, that computation of accuracy implies having
the ground truth labels. The purpose is to reduce the size
of the training set by removing non-informative samples,
which do not contribute to improving the learned model,
and therefore put some ease on the annotation efforts.

There are several ways of obtaining T ∗. One frequently
used approach is instance selection [40, 35, 26]. There
are two main groups of instance selection: wrappers and
ﬁlters. The wrapper based methods use a selection criterion
based on the constructed classiﬁer’s accuracy. Filter based
methods, on the other hand, use a selection criterion which
is based on an unrelated selection function. The concept of
a minimal consistent subset is crucial for our setup, where
we record image data from video cameras. Collecting
frames at a frame rate of 30fps, particularly at low speeds,
ultimately leads to signiﬁcant image overlap,
therefore,
having an effective sampling strategy to distill the dataset

nuScenes
[6]
2018
2/2
1 LiDAR
GPS, IMU
5 RADAR
6
1
-
-
-
-
25
40k
-
-
-
-
-
-

ApolloScape
[24]
2018
1/4
2 LiDAR
GNSS
IMU
6
4
25
140k
-
-
1
5k+
-
-
-
-
-
-

BDD100k
[59]
2018
1/4

1 GPS
IMU

1
2
40
5.7k
10
5.7k
-
-
-
-
-
-
-
-

WoodScape
Ours
2018/19
5+/10+
1 LiDAR
GNSS
IMU
4
9
40
10k
7
10k
3
10k
400k
10k
5k
50
500
10k

-

-
1
66
25k
-
-
-
-
-
-
-
-
-
-

is critical. We used a combination of a wrapper method
using selection criterion based on the classiﬁer’s accuracy
[40] and a simple ﬁlter based on the image similarity
measurement.

Data splitting and class balancing: The dataset is split
into three chunks in ratio of 6 : 1 : 3, namely training,
validation, and testing. For classical algorithms, all the data
can be used for testing. As the names suggest, the training
part will serve for training purposes only, the validation
part can be either joined with the training set (e.g. when the
sought model does not require hyper-parameter selection)
or be used for model selection, and ﬁnally, the testing set is
used for model evaluation purposes only. The dataset sup-
ports correct hypothesis evaluation [55], therefore multiple
splits are provided (5 in total). Depending on the particular
task (see Section 4, for the full list), the class imbalance
may be an issue [19], therefore, task-speciﬁc splits are
also provided. Full control of the splitting mechanism is
provided allowing for each class to be represented equally
within each split (i.e. stratiﬁed sampling).

GDPR challenges: The recent General Data Protection
Regulation (GDPR) regulation in Europe has given rise to
challenges in making our data publicly available. More
than one third of our dataset is recorded in Europe and is
therefore GDPR sensitive due to visible faces of pedestri-
ans and license plates. There are three primary ways to han-
dle privacy namely (1) Manual blurring, (2) GAN based re-
targeting and (3) Stringent data-handling license agreement.
Blurring is the commonly used approach wherein privacy
sensitive regions in the image are manually blurred. There

is also the possibility of using GAN based re-targeting
wherein faces are exchanged by automatically generated
ones [31]. In the recent EuroCity persons dataset [5], the au-
thors argued that any anonymization measure will introduce
a bias. Thus they released their dataset with original data
and a license agreement which enforces the user to strictly
adhere to GDPR. We will follow a similar approach.

4. Tasks, Metrics and Baseline experiments

Due to limited space, we brieﬂy describe the metrics and
baseline experiments for each task and they are summarized
in Table 2. Test dataset for each task consists of 30% of the
respective number of annotated samples listed in Table 1.
Code is available on WoodScape GitHub and sample video
results are shared in supplementary material.

4.1. Semantic Segmentation

Semantic segmentation networks for autonomous driv-
ing [47] have been successfully trained directly on ﬁsheye
images in [12, 45]. Due to absence of ﬁsheye datasets,
they make use of artiﬁcially warped images of Cityscapes
for training and testing was performed on ﬁsheye images.
However, the artiﬁcial images cannot increase the orig-
inally captured FOV. Our semantic segmentation dataset
provides pixel-wise labels for 40 object categories, com-
paratively Cityscapes dataset [10] provides 30 for example.
Figure 6 illustrates the distribution of main classes. We use
ENet [41] to generate our baseline results. We ﬁne-tune
their model for our dataset by training with categorical cross
entropy loss and Adam [29] optimizer. We chose Intersec-
tion over Union (IoU) metric [16] to report the baseline re-
sults shown in Table 2. We acheive a mean IoU of 51.4 on
this test set. Figure 7 shows sample results of segmenta-
tion on ﬁsheye images from our test set. The four camera
images are treated the same, however it would be interest-
ing to explore customization of the model for each camera.
The dataset also provides instance segmentation labels to
explore panoptic segmentation models [34].

4.2. 2D Bounding Box Detection

Our 2D object detection dataset is obtained by extract-
ing bounding boxes from instance segmentation labels for 7
different object categories including pedestrians, vehicles,
cyclist and motorcyclist. We use Faster R-CNN [43] with
ResNet101 [20] as encoder. We initialize the network with
ImageNet [11] pre-trained weights. We ﬁne-tune our detec-
tion network by training on both KITTI [18] and our object
detection datasets. Performance of 2D object detection is
reported in terms of mean average precision (mAP) when
IoU≥ 0.5 between predicted and ground truth bounding
boxes. We achieve a mAP score of 31 which is signiﬁcantly
less than the accuracy achieved in other datasets. This was
expected as bounding box detection is a difﬁcult task on

Table 2: Summary of results of baseline experiments.

Task
Segmentation
2D Bounding Box
Soiling Detection
Depth Estimation
Motion Segmentation

Model
ENet [41]
Faster R-CNN [43]
ResNet10 [20]
Eigen [14]
MODNet [49]

Visual Odometry

ResNet50 [20]

Visual SLAM

LSD SLAM [15]

Metric
IoU
mAP (IoU>0.5)
Category (%)
RMSE
IoU
Translation (<5mm)
Rotation (<0.1°)
Relocalization (%)

3D Bounding Box Detection - Complex YOLO [50]

Value
51.4
31
84.5
7.7
45
51
71
61

Metric for Training
3D-IoU
Ssrt

AP (%)
64.38
62.46

AOS (%)
85.60
88.43

Runtime (ms)
95
1

ﬁsheye (the orientation of objects in the periphery of im-
ages being very different from central region). To quantify
this better, we tested a pre-trained network for person class,
and a poor mAP score of 12 was achieved compared to our
dataset trained value of 45. Sample results of the ﬁsheye
trained model are illustrated in Figure 7. We observe that it
is necessary to incorporate the ﬁsheye geometry explicitly,
which is an open research problem.

4.3. Camera Soiling Detection

The task of soiling detection was to our best knowledge
ﬁrst deﬁned in [56]. Unlike the front camera which is be-
hind the windshield, the surround view cameras are usually
directly exposed to the adverse environmental conditions,
and thus prone to becoming soiled or water drops forming
on the lens. As the functionality of visual perception de-
grades signiﬁcantly, detection of soiled cameras is neces-
sary for achieving higher levels of automated driving. As it
is a novel task, we discuss it in more detail below.

We treat the camera soiling detection task as a mixed
multilabel-categorical classiﬁcation problem, i.e. we are in-
terested in a classiﬁer, which jointly classiﬁes a single im-
age with a binary indicator array, where each 0 or 1 corre-
sponds to missing or present class, respectively and simulta-
neously assigns a categorical label. The classes to detect are
{opaque, transparent}. Typically, opaque soiling arises
from mud and dust (Figure 8 right image), and transpar-
ent soiling arises from water and ice (Figure 8 left image).
However, in practice it is common to see water producing
“opaque” regions in the camera image.

Annotation for 5k images is performed by drawing poly-
gons to separate soiled from unsoiled regions, so that it can
be modeled as a segmentation task if necessary. We evaluate
the soiling classiﬁer’s performance via an example-based
accuracy measure for each task separately, i.e. the average
|Yi∩Zi|
Jaccard index of the testing set: 1
|Yi∪Zi| , where
n
Yi ∈ Y = {0, 1}k denotes the label for the i-th testing
sample, Zi denotes the classiﬁer’s prediction and n denotes
the cardinality of the testing set and k the length of the label
vector. We use a small baseline network (ResNet10 encoder
+ 3-layer decoder) and achieved a precision of 84.5% for the
multilabel classiﬁcation.

(cid:80)n

i=1

Figure 7: Qualitative results of Segmentation using ENet [41] (top) and Object detection using Faster RCNN [43] (bottom)

4.4. 3D Bounding Box Detection

3D box annotation is provided for 10k frames with 3
classes namely ‘pedestrian’, ‘vehicle’ and ‘cyclist’. In gen-
eral, 3D IoU [18] is used to evaluate 3D bounding box pre-
dictions, but there are drawbacks, especially for rotated ob-
jects. Two boxes can reach a good 3D IoU score, while
overlapping in total with an opposite heading. Addition-
ally, an exact calculation in 3D space is a time consum-
ing task. To avoid those problems, we introduce a new
evaluation metric called Scaling-Rotation-Translation score
(SRTs). SRT is based on the idea that two non-overlapping
3D boxes can easily be transformed with respect to each
other by using independent rigid transformations: transla-
tion St, rotation Sr and scaling Ss. Hence, Ssrt is com-
posed by:

(cid:32)

Ss = 1 − min

|1 − sx| + |1 − sy| + |1 − sz|
ws

, 1

(cid:33)

(cid:16)

Sr = max

0, 1 −

(cid:17)

θ
wrπ

(cid:16)

St = max

0,

r1 + r2 − t
r1 + r2

(cid:17)

r1/2 =

d1/2 · wt
2

wt, wr, ws ∈ (0, 1]

where sx,y,z denotes size ratios in x, y, z directions, θ de-
termines the difference of the yaw angles and t deﬁnes the
Euclidean distance between the two box centers. St is cal-
culated with respect to the size of the two objects based on
the length of the diagonals d1/2 of both objects that are used
to calculate two radii r1/2. Based on the penalty term pt we
deﬁne the full metric by:

Ssrt = pt · (α Ss + β St + γ Sr) α + β + γ = 1

(cid:40)

pt =

if r1 + r2 < t

0,
1, otherwise

ws, wt and wr can be used to prioritize individual proper-
ties (e.g. ws → size, wt → angle). For our baseline exper-
iments we used ws = 0.3, wt = 1, wr = 0.5, γ = 0.4 and

α = β = 0.3 to add more weight to the angle, because our
experiments have shown that translation or scaling is eas-
ier to learn. For baseline, we trained Complex-YOLO [50]
for a single class (‘car’). We repeated training two times,
ﬁrst optimized on 3D-IoU [18] and second optimized on
Ssrt using a ﬁxed 50:50 split for training and validation.
For comparison, we present 3D-IoU, orientation and run-
time following [18] on moderate difﬁculty, see Table 2.
Runtime is the average runtime of all box comparisons for
each input during training. Even though this comparison
uses 3D-IoU, we achieve similar performance for average
precision (3D-IoU), with better angle orientation similarity
(AOS) and much faster computation time.

4.5. Monocular Depth Estimation

Monocular Depth estimation is an important task for de-
tecting generic obstacles. We provide more than 100k im-
ages of all four cameras (totaling 400k) using ground truth
provided by LiDAR. Figure 1 shows a colored example
where blue to red indicates the distance for the front cam-
era. As the depth obtained is sparse, we also provide denser
point cloud based on SLAM’d static scenes as shown in Fig-
ure 5. The ground truth 3D points are projected onto the
camera images using our proposed model discussed in Sec-
tion 2.1. We also apply occlusion correction to handle dif-
ference in perspective of LiDAR and camera similar to the
method proposed in [33]. We run the semi-supervised ap-
proach from [33] using the model proposed by Eigen [14] as
baseline on our much larger dataset and obtained an RMSE
(Root Mean Square Error) value of 7.7.

4.6. Motion Segmentation

In automotive, motion is a strong cue due to ego-motion
of the cameras on the moving vehicle and dynamic objects
around the vehicle are the critical interacting agents. Ad-
ditionally, it is helpful to detect generic objects based on

trated in Figure 9 and accuracies are provided in Table 2.

4.8. Synthetic Data Domain Transfer

Synthetic data is crucial for autonomous driving for
many reasons. Firstly, it provides a mechanism to do rig-
orous corner case testing for diverse scenarios. Secondly,
there are legal restrictions like recording videos of a child.
Finally, synthetic data is the only way to obtain dense depth
and optical ﬂow annotation. There are several popular syn-
thetic datasets like SYNTHIA [44] and CARLA [13]. We
will provide a synthetic version of our ﬁsheye surround
view dataset, as shown in Figure 10. The main goal is to
explore domain transfer from synthetic to real domain for
semantic segmentation and depth estimation tasks.

4.9. End-to-End Steering/Braking

Bojarski et al. demonstrated end-to-end learning [2] for
steering and recently it was applied to ﬁsheye cameras [54].
Although this approach is currently not mature for deploy-
ment, it can be either used as a parallel model for redun-
dancy or as an auxiliary task to improve accuracy of other
In the traditional approach, perception is indepen-
tasks.
dently designed and it is probably a more complex interme-
diate problem to solve than what is needed for a small action
space driving task. Thus we have added end-to-end steering
and braking tasks to encourage modular end-to-end archi-
tectures and to explore optimized perception for the control
task. The latter is analogous to hand-eye co-ordination of
human drivers where perception is optimized for driving.

5. Conclusions

In this paper, we provide an extensive multi-camera ﬁsh-
eye dataset for autonomous driving with annotation for nine
tasks. We hope that the release of the dataset encourages
development of native ﬁsheye models instead of undistort-
ing ﬁsheye images and applying standard models. In case
of deep learning algorithms, it can help understand whether
spatial distortion can be learned or it has to be explicitly
modeled. In future work, we plan to explore and compare
various methods of undistortion and explicit incorporation
of ﬁsheye geometry in CNN models. We also plan to design
a uniﬁed multi-task model for all the listed tasks.

Acknowledgement

We would like to thank our colleagues Nivedita Tripathi,
Mihai Ilie, Philippe Lafon, Marie Yahiaoui, Sugirtha Thay-
alan, Jose Luis Fernandez and Pantelis Ermilios for sup-
porting the creation of the dataset, and to thank our part-
ners MightyAI for providing high-quality semantic segmen-
tation annotation services and Next Limit for providing syn-
thetic data using ANYVERSE platform.

Figure 8: Soiling annotation

Figure 9: Visual SLAM baseline results (left) based on raw
ﬁsheye images (right)

Figure 10: Synthetic images modelling ﬁsheye optics

motion cues rather than appearance cues as there will al-
ways be rare objects like kangaroos or construction trucks.
This has been explored in [49, 57, 48] for narrow angle
cameras. In our dataset, we provide motion masks anno-
tation for moving classes such as vehicles, pedestrians and
cyclists for over 10k images. We also provide previous and
next images for exploring multi-stream models like MOD-
Net [49]. Motion segmentation is treated as a binary seg-
mentation problem and IoU is used as the metric. Using
MODNet as baseline network, we achieve an IoU of 45.

4.7. Visual Odometry/SLAM

Visual Odometry (VO) is necessary for creating a map
from the objects detected [38]. We make use of our GNSS
and IMU to provide annotation in centimetre level accu-
racy. The ground truth contains all the six degrees of free-
dom upto scale and the metric used is percentage of frames
within a tolerance level of translation and rotation error. Ro-
bustness could be added to the visual odometry by perform-
ing a joint estimation from all four cameras. We provide
50 video sequences comprising of over 100k frames with
ground truth. The video sequences can also be used for Vi-
sual SLAM where we focus on relocalization of a mapped
trajectory and the metric is same as VO. We use a ﬁsheye
adapted LSD-SLAM [15] as our baseline model as illus-

References

[1] Joao P Barreto. Unifying image plane liftings for central
catadioptric and dioptric cameras. Imaging Beyond the Pin-
hole Camera, pages 21–38, 2006. 2

[2] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,
Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.
End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316, 2016. 8

[3] WN Bond. A wide angle lens for cloud recording. The Lon-
don, Edinburgh, and Dublin Philosophical Magazine and
Journal of Science, 44(263):999–1001, 1922. 1

[4] Tomas Borovicka, Marcel Jirina Jr., Pavel Kordik, and Mar-
cel Jirina. Selecting representative data sets. In Adem Kara-
hoca, editor, Advances in Data Mining Knowledge Discovery
and Applications, chapter 2. IntechOpen, Rijeka, 2012. 5
[5] Markus Braun, Sebastian Krebs, Fabian Flohr, and Dariu
Gavrila. EuroCity persons: A novel benchmark for person
IEEE transactions on pattern
detection in trafﬁc scenes.
analysis and machine intelligence, 2019. 6

[6] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuScenes: A mul-
arXiv preprint
timodal dataset for autonomous driving.
arXiv:1903.11027, 2019. 5

[7] David Caruso, Jakob Engel, and Daniel Cremers. Large-
scale direct SLAM for omnidirectional cameras. In Proceed-
insg of the IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 141–148. IEEE, 2015. 1,
2

[8] Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, and
Samir Rawashdeh. Auxnet: Auxiliary tasks enhanced se-
mantic segmentation for automated driving. In Proceedings
of the 14th International Joint Conference on Computer Vi-
sion, Imaging and Computer Graphics Theory and Applica-
tions (VISAPP), pages 645–652, 2019. 3

[9] Benjamin Coors, Alexandru Paul Condurache, and Andreas
Geiger. SphereNet: Learning spherical representations for
detection and classiﬁcation in omnidirectional images.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 518–533, 2018. 1, 3

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The Cityscapes
Dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3213–3223, 2016. 2, 5, 6
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 248–
255. Ieee, 2009. 6

[12] Liuyuan Deng, Ming Yang, Yeqiang Qian, Chunxiang Wang,
and Bing Wang. CNN based semantic segmentation for ur-
ban trafﬁc scenes using ﬁsheye camera. In 2017 IEEE Intel-
ligent Vehicles Symposium (IV), pages 231–236. IEEE, 2017.
6

[13] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
Lopez, and Vladlen Koltun. CARLA: An open urban driving
simulator. In Proceedings of the 1st Annual Conference on
Robot Learning, pages 1–16, 2017. 8

[14] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale con-
In Proceedings of the IEEE inter-
volutional architecture.
national conference on computer vision, pages 2650–2658,
2015. 6, 7

[15] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. LSD-
SLAM: Large-scale direct monocular SLAM. In European
conference on computer vision, pages 834–849. Springer,
2014. 6, 8

[16] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The PASCAL visual ob-
ject classes (VOC) challenge. International Journal of Com-
puter Vision, 88(2):303–338, 2010. 6

[17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The KITTI dataset. The
International Journal of Robotics Research, 32(11):1231–
1237, 2013. 2, 5

[18] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for Autonomous Driving? The KITTI Vision Bench-
mark Suite. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2012. 6, 7
[19] Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang,
and Guang-Tong Zhou. On the class imbalance problem. In
Proceedings of the Fourth International Conference on Nat-
ural Computation (ICNC), pages 192–201, 2008. 5

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016. 6

[21] Markus Heimberger, Jonathan Horgan, Ciar´an Hughes, John
McDonald, and Senthil Yogamani. Computer vision in au-
tomated parking systems: Design, implementation and chal-
lenges. Image and Vision Computing, 68:88–101, 2017. 3

[22] Thomas J Herbert. Area projections of ﬁsheye photographic
lenses. Agricultural and Forest Meteorology, 39(2-3):215–
223, 1987. 2

[23] Jonathan Horgan, Ciar´an Hughes, John McDonald, and
Senthil Yogamani. Vision-based driver assistance systems:
Survey, taxonomy and advances. In 2015 IEEE 18th Inter-
national Conference on Intelligent Transportation Systems,
pages 2032–2039. IEEE, 2015. 1

[24] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao,
Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang.
In Pro-
The ApolloScape dataset for autonomous driving.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, pages 954–960,
2018. 2, 5

[25] Ciar´an Hughes, Patrick Denny, Edward Jones, and Martin
Glavin. Accuracy of ﬁsh-eye lens models. Applied Optics,
49(17):3338–3347, 2010. 2

[26] Norbert Jankowski and Marek Grochowski. Comparison of
instances selection algorithms I. Algorithms survey. In Pro-
ceedings of the 7th International Conference on Artiﬁcial

Intelligence and Soft Computing ICAISC, pages 598–603,
2004. 5

[27] Bogdan Khomutenko, Gaetan Garcia, and Philippe Martinet.
An enhanced uniﬁed camera model. IEEE Robotics and Au-
tomation Letters, 1(1):137–144, 2016. 2

[28] Hyungtae Kim, Jaehoon Jung, and Joonki Paik. Fisheye
lens camera based surveillance system for wide ﬁeld of view
monitoring. Optik, 127(14):5636–5646, 2016. 1

[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization, 2014. 6

[30] Iasonas Kokkinos. Ubernet: Training a universal convolu-
tional neural network for low-, mid-, and high-level vision
using diverse datasets and limited memory. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6129–6138, 2017. 3

[31] Pavel Korshunov and S´ebastien Marcel. Deepfakes: a new
threat to face recognition? assessment and detection. arXiv
preprint arXiv:1812.08685, 2018. 6

[32] Zuzana Kukelova, Jan Heller, Martin Bujnak, and Tomas Pa-
jdla. Radial distortion homography. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 639–647, 2015. 3

[33] Varun Ravi Kumar, Stefan Milz, Christian Witt, Martin Si-
mon, Karl Amende, Johannes Petzold, Senthil Yogamani,
and Timo Pech. Near-ﬁeld depth estimation using monoc-
ular ﬁsheye camera: A semi-supervised learning approach
using sparse LiDAR data. In CVPR Workshop, 2018. 7
[34] Qizhu Li, Anurag Arnab, and Philip HS Torr. Weakly-
and semi-supervised panoptic segmentation. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 102–118, 2018. 6

[35] Huan Liu and Hiroshi Motoda. On issues of instance selec-
tion. Data Mining and Knowledge Discovery, 6(2):115–130,
2002. 5

[36] Yongshuai Liu, Jiyu Chen, and Hao Chen. Less is more:
Culling the training set to improve robustness of deep neural
networks. In In Proceedings of the 9th International Confer-
ence on Decision and Game Theory for Security (GameSec),
pages 102–114, 2018. 4

[37] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul
Newman. 1 year, 1000 km: The Oxford RobotCar dataset.
The International Journal of Robotics Research, 36(1):3–15,
2017. 2

[38] Stefan Milz, Georg Arbeiter, Christian Witt, Bassam Abdal-
lah, and Senthil Yogamani. Visual slam for automated driv-
ing: Exploring the applications of deep learning. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 247–257, 2018. 8

[39] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and
Peter Kontschieder. The Mapillary Vistas dataset for seman-
In Proceedings of the
tic understanding of street scenes.
IEEE International Conference on Computer Vision (ICCV),
pages 4990–4999, 2017. 2, 5

[40] Jos´e Arturo Olvera-L´opez, Jes´us Ariel Carrasco-Ochoa,
Jos’e Francisco Mart´ınez Trinidad, and Josef Kittler. A re-
view of instance selection methods. Artiﬁcial Intelligence
Review, 34(2):133–143, 2010. 5

[41] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Euge-
nio Culurciello. ENet: A deep neural network architecture
for real-time semantic segmentation, 2016. 6, 7

[42] Robert Pless. Using many cameras as one.

In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2003. 3

[43] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Advances in Neural Information
Processing Systems, pages 91–99, 2015. 6, 7

[44] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M Lopez. The SYNTHIA Dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
3234–3243, 2016. 8

[45] Alvaro S´aez, Luis M Bergasa, Eduardo Romeral, Elena
L´opez, Rafael Barea, and Rafael Sanz. CNN-based ﬁsheye
In 2018 IEEE In-
image real-time semantic segmentation.
telligent Vehicles Symposium (IV), pages 1039–1044. IEEE,
2018. 6

[46] Dieter Schmalstieg and Tobias Hollerer. Augmented reality:
principles and practice. Addison-Wesley Professional, 2016.
1

[47] Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, and
Senthil Yogamani. Deep semantic segmentation for auto-
mated driving: Taxonomy, roadmap and challenges. In 2017
IEEE 20th International Conference on Intelligent Trans-
portation Systems (ITSC), pages 1–8. IEEE, 2017. 6
[48] Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek,
Senthil Yogamani, and Martin Jagersand. RTSeg: Real-
time semantic segmentation comparative study. In 2018 25th
IEEE International Conference on Image Processing (ICIP).
IEEE, 2018. 8

[49] Mennatullah Siam, Heba Mahgoub, Mohamed Zahran,
Senthil Yogamani, Martin Jagersand, and Ahmad El-Sallab.
MODNet: Motion and appearance based moving object de-
tection network for autonomous driving. In Proceedings of
the 21st International Conference on Intelligent Transporta-
tion Systems (ITSC), pages 2859–2864, 2018. 6, 8

[50] Martin Simon, Stefan Milz, Karl Amende, and Horst-
Michael Gross. Complex-YOLO: An Euler-region-proposal
for real-time 3D object detection on point clouds. In The Eu-
ropean Conference on Computer Vision (ECCV) Workshops,
September 2018. 6, 7

[51] Ganesh Sistu, Isabelle Leang, Sumanth Chennupati, Stefan
Milz, Senthil Yogamani, and Samir Rawashdeh. NeurAll:
Towards a uniﬁed model for visual perception in automated
In International Conference on Intelligent Trans-
driving.
portation Systems (ITSC). IEEE, 2019. 3

[52] Yu-Chuan Su and Kristen Grauman. Kernel transformer net-
In Proceedings
works for compact spherical convolution.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 9442–9451, 2019. 3

[53] M. Teichmann, M. Weber, M. Z¨ollner, R. Cipolla, and R.
Urtasun. MultiNet: Real-time joint semantic reasoning for
autonomous driving. In Proceedings of the IEEE Intelligent
Vehicles Symposium (IV), pages 1013–1020, June 2018. 3

[54] Marin Toromanoff, Emilie Wirbel, Fr´ed´eric Wilhelm,
Camilo Vejarano, Xavier Perrotton, and Fabien Moutarde.
End to end vehicle lateral control using a single ﬁsheye cam-
era. In Proceedings of the IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS), pages 3613–
3619, 2018. 8

[55] Michal Uˇriˇc´aˇr, David Hurych, Pavel Kˇr´ıˇzek, and Senthil Yo-
gamani. Challenges in designing datasets and validation for
In Proceedings of the 14th Interna-
autonomous driving.
tional Joint Conference on Computer Vision, Imaging and
Computer Graphics Theory and Applications - Volume 5:
VISAPP,, pages 653–659. INSTICC, SciTePress, 2019. 5
[56] Michal Uˇriˇc´aˇr, Pavel Kˇr´ıˇzek, David Hurych, Ibrahim Sobh,
Senthil Yogamani, and Patrick Denny. Yes, we GAN: ap-
plying adversarial techniques for autonomous driving. Elec-

tronic Imaging, 2019(15):48–1–48–17, 2019. 6

[57] Johan Vertens, Abhinav Valada, and Wolfram Burgard. Sm-
snet: Semantic motion segmentation using deep convolu-
tional neural networks. In Proceedings of the IEEE Interna-
tional Conference on Intelligent Robots and Systems (IROS),
Vancouver, Canada, 2017. 8

[58] Robert W Wood. Fish-eye views, and vision under water.
The London, Edinburgh, and Dublin Philosophical Maga-
zine and Journal of Science, 12(68):159–162, 1906. 1

[59] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike
Liao, Vashisht Madhavan, and Trevor Darrell. BDD100K: A
diverse driving video database with scalable annotation tool-

ing. arXiv preprint arXiv:1805.04687, 2018. 2, 5

