Partial Membership Latent Dirichlet Allocation for
Image Segmentation

Chao Chen
Electrical & Computer Engineering
University of Missouri
Email: ccwwf@mail.missouri.edu

Alina Zare
Electrical & Computer Engineering
University of Missouri
Email: zarea@missouri.edu

J. Tory Cobb
Naval Surface Warfare Center
Panama City, FL
Email: james.cobb@navy.mil

6
1
0
2
 
r
p
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
2
8
2
0
.
1
1
5
1
:
v
i
X
r
a

Abstract—Topic models (e.g., pLSA, LDA, SLDA) have been
widely used for segmenting imagery. These models are conﬁned
to crisp segmentation. Yet, there are many images in which
some regions cannot be assigned a crisp label (e.g., transition
regions between a foggy sky and the ground or between sand
and water at a beach). In these cases, a visual word is best
represented with partial memberships across multiple topics. To
address this, we present a partial membership latent Dirichlet
allocation (PM-LDA) model and associated parameter estimation
algorithms. Experimental results on two natural image datasets
and one SONAR image dataset show that PM-LDA can produce
both crisp and soft semantic image segmentations; a capability
existing methods do not have.

I. INTRODUCTION

The goal of unsupervised semantic image segmentation is
to divide an image into semantically distinct coherent regions,
i.e., regions corresponding to objects or parts of objects. It
plays an important role in a wide range of computer vision
applications, such as object recognition and tracking and image
retrieval [1]–[4]. Yet, in many images, widely used image
segmentation methods fail to perform. Speciﬁcally, imagery
in which there are smooth gradients and transition regions
are poorly-addressed with the many crisp image segmentation
methods in the literature. For example, consider the photograph
in Fig. 1a where the gradually thinning fog blurs the boundary
between the foggy sky and the mountain, a sharp boundary
between the “fog” and “mountain” topics does not exist.
Similarly, in Fig. 1b consider the gradually fading sunlight or
in Fig. 1c consider the gradually vanishing sand ripples shown
in the Synthetic Aperture SONAR image of the sea ﬂoor. In
both of these cases, sharp boundaries between the “sun” and
“sky” topics or “sand ripple” and “ﬂat sand” topics do not exist.
In this paper, we present a Partial Membership Latent Dirichlet
Allocation (PM-LDA) to address image segmentation in the
case of gradients and regions of transition.

Inspired by the success of Latent Dirichlet Allocation
topics
(LDA) [5] in discovering semantically meaningful
from document collections, many have successfully applied
LDA or its variants to image segmentation [1], [6]–[9].
These unsupervised semantic image segmentation methods
differ from traditional (i.e., non-hierarchical/ﬂat) segmentation
methods (e.g., normalized cuts algorithm [2]) by estimating
and describing additional inter-segment relationships. Namely,
unsupervised semantic image segmentation methods over-

(a)

(b)

(c)

Fig. 1: Imagery with regions of gradual transition. (a) Image
with gradual transition from fog to mountain. (b) Sunset image
with gradual transition from sun to sky. (c) SONAR image
with gradually vanishing sand ripples.

segment imagery and then group these “visual words” into
topic clusters such that small segments from the same object
class can be combined into a complete object and provide a
comprehensive organization of the larger scene. In other words,
unsupervised semantic image segmentation methods cluster
imagery hierarchically in which the lower level corresponds
to an over-segmentation of the imagery and the higher level
groups the over-segmented pieces into topic clusters.

However, under these existing topic models, a visual word
is only assigned to one topic (i.e., the word-topic assignment
is a binary indicator). Yet, in many images, it is impossible
to assign a crisp boundary between topics. In this paper, we
generalize LDA to allow for partial memberships.

Partial membership models and algorithms have been
previously developed in the literature. One prevalent par-
tial membership approach is the Fuzzy C-means algorithm
(FCM) [10]. More recently, Heller et al.[11] and Glenn
et al.[12] proposed Bayesian partial membership models
(i.e., probabilistic interpretations of FCM) along with their
associated generative processes. Glenn et al.[12] proposed
a Bayesian Fuzzy Clustering (BFC) model and associated
algorithms to bridge and extend probabilistic clustering and
fuzzy clustering methods. Heller et al.[11] derived a Bayesian
partial membership model (BPM) by extending the standard
mixture model. The generative processes for both the BFC
and BPM models are similar. The main difference between the
BPM and BFC models is that the BFC uses both a ﬁxed
fuzziﬁer parameter and a scaling parameter to control the
degree of mixing between topics. In contrast, in the BPM,
the degree of mixing between topics is controlled only through
a scaling hyper-parameter, s, found in the prior distribution on

(a) BPM

(b) LDA

(c) PM-LDA

Fig. 2: (a) Graphical model for BPM. (b) Graphical model for LDA. (c) Graphical model for PM-LDA

partial membership values. These existing partial membership
methods are non-hierarchical approaches. The proposed PM-
LDA extends these to hierarchical approaches to allow for
semantic image segmentation. PM-LDA is an extension of
both the LDA model proposed by Blei et al.[5] and the partial
membership model proposed by Heller et al.[11].

II. BAYESIAN PARTIAL MEMBERSHIP MODEL
In a ﬁnite mixture model, the data likelihood of xn, is
K
(cid:88)

p(xn|β) =

πkpk(xn|βk),

(1)

k=1

k=1

where {πk}K
are the mixture weights and β =
{β1, β2, ..., βK} are the mixture component parameters.
pk(xn|βk) is the kth mixture component with parameters βk.
In this model, a data point is assumed to come from one
(and only one) of the K mixture components. Thus, given
its component assignment, zn, the probability of a data point,
xn, is deﬁned as p(xn|zn, β) = (cid:81)K
k=1 pk(xn|βk)znk where
znk ∈ {0, 1}, (cid:80)K
k=1 znk = 1, and zn = [zn1, zn2, ..., znK] is
the binary membership vector. If znk = 1, the data point xn
is assumed to have been drawn from mixture component k.

In order to obtain a model allowing multiple cluster mem-
berships for a data point, the constraint znk ∈ {0, 1} is relaxed
to znk ∈ [0, 1]. The modiﬁed constraints and the inclusion of
prior distributions for several key parameters results in the
Bayesian Partial Membership model [11],

p(π, s, zn, xn|α, λ, β) = p(π|α)p(s|λ)p(zn|πs)

K
(cid:89)

k=1

pk(xn|βk)znk ,

(2)

where znk ∈ [0, 1], (cid:80)K
k=1 znk = 1, and π is the cluster
mixing proportion assumed to be distributed according to a
Dirichlet distribution with parameter α, i.e., π ∼ Dir(α). The
scaling factor, s, determines the level of cluster mixing and is
distributed according to an exponential distribution with mean
1/λ, s ∼ exp(λ), and zn ∼ Dir(πs) is the membership vector
for data point xn.

As shown in [11], if each of the mixture components
are exponential family distributions of the same type, then
p(xn|zn, β) = (cid:81)K
k=1 pk(xn|βk)znk with znk ∈ [0, 1] and
(cid:80)K

k=1 znk = 1, can be written as:

p(xn|zn, β) = Expon

znkηk

.

(3)

(cid:33)

(cid:32)

(cid:88)

k

This indicates that the data generating distribution for xn is
of the same exponential family distribution as the original K
clusters, but with new natural parameters (cid:80)
k znkηk. The new
parameters are a convex combination of the natural parameters,
ηk, of the original clusters weighted by znk. This provides the
powerful (and convenient) ability to sample directly from the
unique mixture distribution for each data point if the natural
parameters of the original clusters and the membership vector
for the data point are known. A graphical model of BPM is
shown in Fig. 2a.

III. PARTIAL MEMBERSHIP LDA

In the BPM, data points are organized at only one level,
where each data point is indexed by its corresponding com-
ponent distribution. In our proposed model, PM-LDA, (and
in LDA) data is organized at two levels: the word level and
the document level as illustrated in Fig. 2b and 2c. In the
proposed PM-LDA model, the random variable associated with
a data point is assumed to be distributed according to multiple
topics with a continuous partial membership in each topic.
Speciﬁcally, the PM-LDA model is

p(πd, sd, zd

n, xd

n|α, λ, β) = p(πd|α)p(sd|λ)p(zd
K
(cid:89)

pk(xd

n|βk)zd

nk

n|πdsd)

(4)

k=1

n is the nth word in document d, zd

where xd
n is the partial
membership vector of xd
n, πd ∼ Dir(α) and sd ∼ exp(λ)
are the topic proportion and the level of topic mixing in
document d, respectively. The parameter α gives the topic
composition across a document. For example, in Fig. 1c, the
image may be composed of 40% “sand ripple” topic and 60%
“ﬂat sand” topic (i.e., α = [0.4, 0.6]). The parameter λ controls
how similar the partial membership vector of each word is
expected to be to the topic distribution of the document. For
example, a small λ would correspond to most words in an
document to have partial membership vectors very close to πd.
During image segmentation, a small λ generally corresponds
to large transition regions (e.g., transition from “ﬂat sand”
to “sand ripple” comprises most of the image). For a large
λ, the partial membership vectors for each word can vary
signiﬁcantly from the document mixing proportions. In general,
a large λ corresponds to very narrow (tending towards crisp)
transition regions during image segmentation (e.g., the SAS
image may have 39% of the visual words as pure “sand ripple”,
59% as pure “ﬂat sand”, and only 2% mixed). The vector

(a)

(b)

Fig. 3: Partial membership data generating distributions. In (a),
two Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6], Σ1 =
Σ2 = I. In (b), two Gaussian topics with µ1 = [−4, −4]; µ2 =
[6, 6], Σ1 = [4, 0; 0, 1], Σ2 = [1, 0; 0, 4] [11].

n ∼ Dir(πdsd) represents the partial memberships of data
zd
point xd
n in each of the K topics. If each topic distribution is
assumed to be of the exponential family, pk(·|βk) = Expon(ηk),
k zd
then using the result in (3), p(xd
nkηk).
The graphical model for PM-LDA is shown in Fig. 2c.

n, β) = Expon((cid:80)

n|zd

In PM-LDA, the membership zd

n is drawn from a Dirichlet
distribution which is in contrast to a multinomial distribution
as used in LDA. With the inﬁnite number of possible values
for zd
n, the word generating distributions in PM-LDA are
expanded from only K generating distributions (as in LDA)
to inﬁnitely many. Fig. 3 illustrates this using two Gaussian
topic distributions, where the membership value to one topic
is varied from 0 to 1 with an increment 0.1. The two original
topics are shown as the Gaussian distributions at either end.
In LDA, words are generated from only the two original topic
distributions. In PM-LDA, words can be generated from any
(of the inﬁnitely many) convex combinations of the topic
distributions. As the scaling factor s → 0, the PM-LDA model
will degrade to the LDA model.

Given the hyperparameters Ψ = {α, λ, β}, the full PM-LDA

model over all words in the dth document is:

p(πd, sd, Zd, Xd|α, λ, β)

(5)

= p(πd|α)p(sd|λ)

p(xd

n|zd

n, β)p(zd

n|πdsd).

N d
(cid:89)

n=1

n}N d

where πd are the topic proportions, scaling factor sd, partial
n=1 and a set of N d words
membership vectors Zd = {zd
Xd for document d. The log of (5) when considering the
speciﬁc forms chosen in our model, is shown in (6). During
parameter estimation, our goal is to maximize L = (cid:80)D
d=1 Ld
by estimating all the model parameters πd, sd, zd
n, β. In this
paper, we employ an Metropolis within Gibbs [12], [13]
sampling approach.

IV. PARAMETER ESTIMATION FOR PM-LDA

The goal of parameter estimation is to maximize the

following posterior distribution,

p(Π, S, M, β|D, α, λ) ∝ p(Π, S, M, D|α, λ, β),
(7)
where D = (cid:8)X1, X2, ..., XD(cid:9) includes all training documents
and Π, S, M include all of the topic proportions, scaling factors
and membership vectors, respectively.

A Metropolis within Gibbs sampler is employed to perform
the MAP inference which can generate samples from the
posterior distribution in (7), [12], [13]. An outline of the
sampler is provided in Alg. 1. The sampler is simple and
straight-forward implement composed only of a series of
draws from candidate distributions for each parameter and then
evaluation of the candidate in the appropriate acceptance ratio.
Our implementation of the sampler has been posted online.1 In
our current implementation, we consider the topic distributions
to be Gaussian with different means, µk, but identical diagonal
and isotropic covariance matrices, Σk = σ2I.

Algorithm 1 Metropolis-within-Gibbs Sampling Method for
Parameter Estimation
Input: A corpus D, the number of topics K, and the number

of sampling iterations T

Output: Collection of all samples: Π(t), S(t), M(t), β(t) =

2 , Σ(t)

2 ..., µ(t)

K , Σ(t)

K

(cid:111)
.

(cid:110)

1 , Σ(t)
1 , µ(t)
µ(t)
1: for t = 1 : T do
2:
3:

for d = 1 : D do

(cid:111)

(cid:110)
1, p(π†,s(t−1),Z(t−1),X|Ψ)p(π(t−1)|α)
p(π(t−1),s(t−1),Z(t−1),X|Ψ)p(π†|α)

Sample πd: Draw candidate: π† ∼ Dir(α)
Accept candidate with probability:
aπ = min
Sample sd: Draw candidate: s† ∼ exp(λ)
Accept candidate with probability:
as = min
for n = 1 : N d do

(cid:110)
1, p(π(t),s†,Z(t−1),X|Ψ)p(s(t−1)|λ)
p(π(t),s(t−1),Z(t−1),X|Ψ)p(s†|λ)

(cid:111)

n: Draw candidate: z†

Sample zd
Accept candidate with probability:
n,xn|Ψ)
az = min

p(π(t),s(t),z†
p(π(t),s(t),z(t−1)

1,

(cid:110)

,xn|Ψ)

n

(cid:111)

n ∼ Dir(1K)

end for

end for
for k = 1 : K do

Sample µk: Draw proposal: µ†
k ∼ N (·|µD, f ΣD)
µD and ΣD are mean and covariance of the data
Accept candidate with probability:
(cid:17)

(cid:16)

p

p

(cid:16)

Π(t),S(t),M(t),D|µ†
k
(t−1)
Π(t),S(t),M(t),D|µ
k

N (µ
(cid:17)

(t−1)
k
N (µ†

|µD,ΣD)

(cid:27)

k|µD,ΣD)

ak = min

1,

(cid:26)

end for
Sample covariance matrices Σ = σ2I:
Draw candidate from:
σ2 = 1
2
Accept candidate with probability:

(cid:26)

aΣ = min

1,

p(Π(t),S(t),M(t),D|Σ†)
p(Π(t),S(t),M(t),D|Σ(t−1))

(cid:27)

.

(cid:8)maxxn d2(xn − µD) − minxn d2(xn − µD)(cid:9)

4:

5:
6:

7:
8:
9:
10:

11:
12:

13: end for

The proposed Metropolis within Gibbs scheme will return the
full distribution of parameter values given the desired posterior.
We use the MAP sample (i.e., the sample with the largest log
posterior value) as the ﬁnal estimate, {Π∗, S∗, M∗, µ∗, Σ∗}.

1Code can be found at: https://github.com/TigerSense/PMLDA

Ld = ln(p(πd, sd, Zd, Xd|α, λ, β)) = ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πd

k + ln λ − λsd

(cid:32) K
(cid:88)

(cid:33)

k=1

(cid:33)

K
(cid:88)

k=1

K
(cid:88)

k=1

N d
(cid:88)

(cid:26)

n=1

(cid:32) K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

+

ln p(xd

n|zd

n, β) +

ln Γ

sdπd
k

−

ln Γ(sdπd

k) +

(sdπd

k − 1) ln zd
nk

(6)

(cid:27)

N d
(cid:88)

n=1

V. DATA & EXPERIMENTAL RESULTS

In this section, we show results of image segmentation on
three datasets: (i) Synthetic Aperture SONAR (SAS) imagery,
(ii) Sunset imagery, and the (iii) MSRC dataset [14].

a) Synthetic Aperture Sonar (SAS) Imagery Dataset:

From our SAS image dataset, we selected 4 images (shown in
the ﬁrst column in Fig. 4) and compute the average intensity
value and entropy within a 21 × 21 window as feature values.
The average intensity value is scaled (×10) to roughly the same
magnitude of the average entropy value. Each image is divided
into multiple documents using a sliding window approach. A
document consists of all of the feature vectors associated with
each pixel (i.e., visual words) in the window. The number of
topics in this dataset is set to 3. For LDA, a dictionary of
size 100 is built by clustering all the computed feature values
using the K-means. FCM results with m = 1.5. Parameters
for LDA and FCM were selected manually to provide the best
results. Due to the lack of ground-truth, qualitative segmentation
results in Fig. 4 is provided. In the ﬁrst row, Subﬁgures (b),
(c), and (d) show the partial membership maps in the “dark
ﬂat sand” , “sand ripple” , and “bright ﬂat sand” topics using
PM-LDA, respectively. Subﬁgures (f), (g), and (h) show the
partial membership maps in each of the three clusters using
FCM, respectively. In (b) - (d) and (f) - (h), the color indicates
the degree of membership of a visual word in a topic where
red corresponds to a full membership of 1 and dark blue color
corresponds to a membership value of 0. The LDA result is
shown in (e) where color indicates topic assignment. Subﬁgures
in Row 2-4 follow the same subﬁgure captions in Row 1.

From the experimental results, we can see that PM-LDA
achieves much better results than FCM and LDA. As shown
in Fig. 4c and 4k, the segmentation results of PM-LDA show
a gradual change from “sand ripple” to “dark ﬂat sand”. FCM
captures the gradual transition to some extent but is not able to
clearly differentiate between clusters. For example, as shown
in Fig. 4o - 4p and Fig. 4w - 4x, using FCM, the rippled
region in Images 2 and 3 is assigned to 2 clusters with nearly
equal partial memberships. As LDA cannot generate partial
memberships, in Fig. 4e and 4m, Image 1 and 2 are simply
partitioned into different topics using LDA. Yet, by comparing
Fig. 4u with 4t and Fig. 4ac with 4ab, we can see that on
Image 3 and 4 that do not contain transition regions, LDA
achieves similar segmentation result to PM-LDA.

As discussed in Section III, the scaling factor sd determines
the similarity of the partial membership vector of each word, zd
n,
to the topic proportion πd. In this experiment, we investigated
the effect of s by estimating the memberships and topics

with ﬁxed topic proportion. A subregion consisting of three
superpixels [15] are used in this experiment and shown in
Fig. 5. Each superpixel is treated as a document. The topic
proportion πd is set to be [1, 1, 1]/3 and the scaling factor s
is varied to be 3, 10, 300, 30000. The membership estimation
results are shown in Figure 6. As can be seen, as the scaling
factor s increases, the partial memberships gradually approach
the topic proportion [1, 1, 1]/3 and become more smooth.

Fig. 5: A subregion of three superpixels

(a) s = 3

(a) s = 10

(a) s = 300

(b)

(b)

(b)

(c)

(c)

(c)

(c)

(a) s = 30000

(b)

Fig. 6: Partial membership maps with varying s. Each row
shows the estimated membership maps of the three estimated
topics. The black contour indicates the superpixel boundary.
The superpixels are results published in [15].

b) Sunset Dataset: Experimental results on Sunset dataset
show the ability of PM-LDA to perform partial membership
segmentation given visual natural imagery. Two sunset themed
images from Flickr (with the necessary permissions)2 3 were
used in this experiment. For each visual word, the ﬁrst order
Gaussian gradient (σ = 2) with respect to y-axis and R and
B channels are used as the feature vectors. The number of
topics is set to be 3. Experiments are run on each image
individually and results are shown in Fig. 7. Columns 2-4
show the segmentation results of PM-LDA. Column 5 is the
LDA results with 3 topics. Comparing Column 3 and Column

2Photo can be found at: https://www.ﬂickr.com/photos/aoa-/6104409480/
3Photo can be found at: https://www.ﬂickr.com/photos/frenchdave/8482336933/

(a) Image 1

(b) PM-LDA:1 (c) PM-LDA:2 (d) PM-LDA:3

(e) LDA

(f) FCM:1

(g) FCM:2

(h) FCM:3

(i) Image 2

(j) PM-LDA:1 (k) PM-LDA:2 (l) PM-LDA:3

(m) LDA

(n) FCM:1

(o) FCM:2

(p) FCM:3

(q) Image 3

(r) PM-LDA:1 (s) PM-LDA:2 (t) PM-LDA:3

(u) LDA

(v) FCM:1

(w) FCM:2

(x) FCM:3

(y) Image 4

(z) PM-LDA:1 (aa) PM-LDA:2 (ab) PM-LDA:3

(ac) LDA

(ad) FCM:1

(ae) FCM:2

(af) FCM:3

Fig. 4: Segmentation results of Image 1 - 4 using PM-LDA, FCM and LDA. (a): SAS Image. (b)-(d): PM-LDA partial
membership map in the “dark ﬂat sand,” “sand ripple,” “bright ﬂat sand” topics, respectively. (e): LDA result where color
indicates topic label. (f)-(h): FCM partial membership map in the ﬁrst, second, and third cluster, respectively. Subﬁgure captions
in Row 2 - 4 follow those in Row 1. In PM-LDA and FCM results, color indicates the degree of membership of a visual word
in a topic or cluster.

5 in Fig. 7, we can see that PM-LDA can generate continuous
partial membership according to the extent to which the sky
is colored by sunlight. The partial membership map illustrates
how the topic gradually shift from one to the other. In contrast,
LDA can only produce 0-1 segmentation.

(a)

(b)

(c)

(d)

(e)

Fig. 9: ROC curve of PM-LDA for cow detection evaluated at
the pixel level. The red star represents the LDA cow detection
results.

(f)

(g)

(h)

(i)

(j)

Fig. 7: Examples of segmentation result on Sunset dataset. (a):
Sunset Image 1. (f): Sunset Image 2. (b)-(d) and (g)-(i) are
the PM-LDA partial membership maps in the estimated three
topics for Sunset Image 1 and Sunset Image 2, respectively.
The color indicates the degree of membership of a visual word
in a topic or cluster. (e) and (j) are the LDA results where
color indicates the topic.

c) Microsoft Research Cambridge data set version one
(MSRCv1): The MSRCv1 database consists of 240, 213 × 320
pixel images. A subset from this database consisting of all
images that include the “grass”, “cow”, and “sky” topics was
used in this experiment. The local descriptors proposed in [16],
the output of a set of ﬁlter bank responses made of 3 Gaussians,
4 Laplacian of Gaussians (LoG) and 4 ﬁrst order derivatives of

Gaussians were used as the feature vectors. The ﬁlter window
size used was 15 × 15. In this experiment, instead of using
each image as a document, we apply normalized cuts method
to get 40 super-pixel segments from each image and treat each
super-pixel as a document. The topic number is set to be 3,
and for LDA, we densely sample the ﬁlter bank output and
build a dictionary of size 200. Quantitative comparison results
on accurate detection of the “cow” class are shown in Fig. 9.
ROC curve analysis of PM-LDA using the “cow” membership
map was conducted. The red star indicates the quantitative
LDA result (a ROC curve cannot be generated due to the crisp
segmentation of the LDA method). Example results are shown
in Fig. 8. In Subﬁgure (e), transition regions are highlighted
by indicating the pixels with at least one membership value
in range [0.4, 0.6]. As shown in (e), these partial membership
values mostly occur at the boundary between two topics. Thus,

(a) cow1

(b) “grass”

(c) “sky”

(d) “cow”

(e) transition

(f) max

(g) LDA

(a) cow2

(t) “grass”

(c) “sky”

(d) “cow”

(e) transition

(f) max

(g) LDA

Fig. 8: Example of PM-LDA and LDA results. (a) Original image. (b)-(d) Results of PM-LDA, the partial membership maps in
“grass”, “sky”, and “cow” topics, respectively. The color indicates the degree of membership in a topic. (e) Transition regions
consisting of visual words with at lease one partial membership value in range [0.4, 0.6] (f) Modiﬁed segmentation result of
PM-LDA by assigning each visual word to the topic with the largest membership. The colors indicate topics. (g) Result of
LDA. The colors indicate topics.

[11] K. A. Heller, S. Williamson, and Z. Ghahramani, “Statistical models for
partial membership,” in International Conference on Machine Learning,
2008, pp. 392–399. 1, 2, 3

[12] T. Glenn, A. Zare, and P. Gader, “Bayesian fuzzy clustering,” IEEE
Transaction on Fuzzy Systems, no. 8, pp. 1545–1561, 2015. 1, 3
[13] C. Robert and G. Casella, Monte Carlo statistical methods. Springer

Science & Business Media, 2013. 3

[14] A. Criminisi, “Microsoft research cambridge object recognition image

database (version 1.0 and 2.0), 2004.” 4

[15] J. T. Cobb and A. Zare, “Multi-image texton selection for sonar image
seabed co-segmentation,” in SPIE, vol. 8709, no. 87090H, June 2013. 4
[16] J. Winn, A. Criminisi, and T. Minka, “Object categorization by learned
universal visual dictionary,” in IEEE International Conference on
Computer Vision, 2005, pp. 1800–1807. 5

PM-LDA is able to identify when the feature vector contains
information from multiple topics (as the feature vector is
being computed over a window that contains more than one
topic). This is a powerful result showing the effectiveness
of PM-LDA to provide semantic image understanding. For
comparison with LDA, we modiﬁed the segmentation result of
PM-LDA by assigning each visual word to the topic with the
largest membership. As shown in (f) and (g), PM-LDA can
achieve similar results to LDA. So on these images with crisp
boundaries, PM-LDA can generate binary membership values,
and learn the three semantic topics comparable to LDA. Thus,
PM-LDA also is effective for use in crisp labeling problems.

REFERENCES

[1] B. Zhao, L. Fei-Fei, and E. P. Xing, “Image segmentation with topic
random ﬁeld,” in European Conference on Computer Vision, 2010, pp.
785–798. 1

[2] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8,
pp. 888–905, 2000. 1

[3] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature
space analysis,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, no. 5, pp. 603–619, 2002. 1

[4] P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient graph-based image
segmentation,” International Journal of Computer Vision, vol. 59, no. 2,
pp. 167–181, 2004. 1

[5] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
Journal of Machine Learning Research, vol. 3, pp. 993–1022, 2003. 1, 2
[6] B. C. Russell, W. T. Freeman, A. Efros, J. Sivic, and A. Zisserman,
“Using multiple segmentations to discover objects and their extent in
image collections,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2006, pp. 1605–1614. 1

[7] L. Cao and L. Fei-Fei, “Spatially coherent latent topic model for
concurrent segmentation and classiﬁcation of objects and scenes,” in
IEEE International Conference on Computer Vision, 2007, pp. 1–8. 1
[8] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in Advances
in Neural Information Processing Systems, 2008, pp. 1577–1584. 1
[9] M. Andreetto, L. Zelnik-Manor, and P. Perona, “Unsupervised learning
of categorical segments in image collections,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1842–1855,
2012. 1

[10] J. C. Bezdek, R. Ehrlich, and W. Full, “Fcm: The fuzzy c-means clustering
algorithm,” Computers & Geosciences, vol. 10, no. 2, pp. 191–203, 1984.
1

Partial Membership Latent Dirichlet Allocation for
Image Segmentation

Chao Chen
Electrical & Computer Engineering
University of Missouri
Email: ccwwf@mail.missouri.edu

Alina Zare
Electrical & Computer Engineering
University of Missouri
Email: zarea@missouri.edu

J. Tory Cobb
Naval Surface Warfare Center
Panama City, FL
Email: james.cobb@navy.mil

6
1
0
2
 
r
p
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
2
8
2
0
.
1
1
5
1
:
v
i
X
r
a

Abstract—Topic models (e.g., pLSA, LDA, SLDA) have been
widely used for segmenting imagery. These models are conﬁned
to crisp segmentation. Yet, there are many images in which
some regions cannot be assigned a crisp label (e.g., transition
regions between a foggy sky and the ground or between sand
and water at a beach). In these cases, a visual word is best
represented with partial memberships across multiple topics. To
address this, we present a partial membership latent Dirichlet
allocation (PM-LDA) model and associated parameter estimation
algorithms. Experimental results on two natural image datasets
and one SONAR image dataset show that PM-LDA can produce
both crisp and soft semantic image segmentations; a capability
existing methods do not have.

I. INTRODUCTION

The goal of unsupervised semantic image segmentation is
to divide an image into semantically distinct coherent regions,
i.e., regions corresponding to objects or parts of objects. It
plays an important role in a wide range of computer vision
applications, such as object recognition and tracking and image
retrieval [1]–[4]. Yet, in many images, widely used image
segmentation methods fail to perform. Speciﬁcally, imagery
in which there are smooth gradients and transition regions
are poorly-addressed with the many crisp image segmentation
methods in the literature. For example, consider the photograph
in Fig. 1a where the gradually thinning fog blurs the boundary
between the foggy sky and the mountain, a sharp boundary
between the “fog” and “mountain” topics does not exist.
Similarly, in Fig. 1b consider the gradually fading sunlight or
in Fig. 1c consider the gradually vanishing sand ripples shown
in the Synthetic Aperture SONAR image of the sea ﬂoor. In
both of these cases, sharp boundaries between the “sun” and
“sky” topics or “sand ripple” and “ﬂat sand” topics do not exist.
In this paper, we present a Partial Membership Latent Dirichlet
Allocation (PM-LDA) to address image segmentation in the
case of gradients and regions of transition.

Inspired by the success of Latent Dirichlet Allocation
topics
(LDA) [5] in discovering semantically meaningful
from document collections, many have successfully applied
LDA or its variants to image segmentation [1], [6]–[9].
These unsupervised semantic image segmentation methods
differ from traditional (i.e., non-hierarchical/ﬂat) segmentation
methods (e.g., normalized cuts algorithm [2]) by estimating
and describing additional inter-segment relationships. Namely,
unsupervised semantic image segmentation methods over-

(a)

(b)

(c)

Fig. 1: Imagery with regions of gradual transition. (a) Image
with gradual transition from fog to mountain. (b) Sunset image
with gradual transition from sun to sky. (c) SONAR image
with gradually vanishing sand ripples.

segment imagery and then group these “visual words” into
topic clusters such that small segments from the same object
class can be combined into a complete object and provide a
comprehensive organization of the larger scene. In other words,
unsupervised semantic image segmentation methods cluster
imagery hierarchically in which the lower level corresponds
to an over-segmentation of the imagery and the higher level
groups the over-segmented pieces into topic clusters.

However, under these existing topic models, a visual word
is only assigned to one topic (i.e., the word-topic assignment
is a binary indicator). Yet, in many images, it is impossible
to assign a crisp boundary between topics. In this paper, we
generalize LDA to allow for partial memberships.

Partial membership models and algorithms have been
previously developed in the literature. One prevalent par-
tial membership approach is the Fuzzy C-means algorithm
(FCM) [10]. More recently, Heller et al.[11] and Glenn
et al.[12] proposed Bayesian partial membership models
(i.e., probabilistic interpretations of FCM) along with their
associated generative processes. Glenn et al.[12] proposed
a Bayesian Fuzzy Clustering (BFC) model and associated
algorithms to bridge and extend probabilistic clustering and
fuzzy clustering methods. Heller et al.[11] derived a Bayesian
partial membership model (BPM) by extending the standard
mixture model. The generative processes for both the BFC
and BPM models are similar. The main difference between the
BPM and BFC models is that the BFC uses both a ﬁxed
fuzziﬁer parameter and a scaling parameter to control the
degree of mixing between topics. In contrast, in the BPM,
the degree of mixing between topics is controlled only through
a scaling hyper-parameter, s, found in the prior distribution on

(a) BPM

(b) LDA

(c) PM-LDA

Fig. 2: (a) Graphical model for BPM. (b) Graphical model for LDA. (c) Graphical model for PM-LDA

partial membership values. These existing partial membership
methods are non-hierarchical approaches. The proposed PM-
LDA extends these to hierarchical approaches to allow for
semantic image segmentation. PM-LDA is an extension of
both the LDA model proposed by Blei et al.[5] and the partial
membership model proposed by Heller et al.[11].

II. BAYESIAN PARTIAL MEMBERSHIP MODEL
In a ﬁnite mixture model, the data likelihood of xn, is
K
(cid:88)

p(xn|β) =

πkpk(xn|βk),

(1)

k=1

k=1

where {πk}K
are the mixture weights and β =
{β1, β2, ..., βK} are the mixture component parameters.
pk(xn|βk) is the kth mixture component with parameters βk.
In this model, a data point is assumed to come from one
(and only one) of the K mixture components. Thus, given
its component assignment, zn, the probability of a data point,
xn, is deﬁned as p(xn|zn, β) = (cid:81)K
k=1 pk(xn|βk)znk where
znk ∈ {0, 1}, (cid:80)K
k=1 znk = 1, and zn = [zn1, zn2, ..., znK] is
the binary membership vector. If znk = 1, the data point xn
is assumed to have been drawn from mixture component k.

In order to obtain a model allowing multiple cluster mem-
berships for a data point, the constraint znk ∈ {0, 1} is relaxed
to znk ∈ [0, 1]. The modiﬁed constraints and the inclusion of
prior distributions for several key parameters results in the
Bayesian Partial Membership model [11],

p(π, s, zn, xn|α, λ, β) = p(π|α)p(s|λ)p(zn|πs)

K
(cid:89)

k=1

pk(xn|βk)znk ,

(2)

where znk ∈ [0, 1], (cid:80)K
k=1 znk = 1, and π is the cluster
mixing proportion assumed to be distributed according to a
Dirichlet distribution with parameter α, i.e., π ∼ Dir(α). The
scaling factor, s, determines the level of cluster mixing and is
distributed according to an exponential distribution with mean
1/λ, s ∼ exp(λ), and zn ∼ Dir(πs) is the membership vector
for data point xn.

As shown in [11], if each of the mixture components
are exponential family distributions of the same type, then
p(xn|zn, β) = (cid:81)K
k=1 pk(xn|βk)znk with znk ∈ [0, 1] and
(cid:80)K

k=1 znk = 1, can be written as:

p(xn|zn, β) = Expon

znkηk

.

(3)

(cid:33)

(cid:32)

(cid:88)

k

This indicates that the data generating distribution for xn is
of the same exponential family distribution as the original K
clusters, but with new natural parameters (cid:80)
k znkηk. The new
parameters are a convex combination of the natural parameters,
ηk, of the original clusters weighted by znk. This provides the
powerful (and convenient) ability to sample directly from the
unique mixture distribution for each data point if the natural
parameters of the original clusters and the membership vector
for the data point are known. A graphical model of BPM is
shown in Fig. 2a.

III. PARTIAL MEMBERSHIP LDA

In the BPM, data points are organized at only one level,
where each data point is indexed by its corresponding com-
ponent distribution. In our proposed model, PM-LDA, (and
in LDA) data is organized at two levels: the word level and
the document level as illustrated in Fig. 2b and 2c. In the
proposed PM-LDA model, the random variable associated with
a data point is assumed to be distributed according to multiple
topics with a continuous partial membership in each topic.
Speciﬁcally, the PM-LDA model is

p(πd, sd, zd

n, xd

n|α, λ, β) = p(πd|α)p(sd|λ)p(zd
K
(cid:89)

pk(xd

n|βk)zd

nk

n|πdsd)

(4)

k=1

n is the nth word in document d, zd

where xd
n is the partial
membership vector of xd
n, πd ∼ Dir(α) and sd ∼ exp(λ)
are the topic proportion and the level of topic mixing in
document d, respectively. The parameter α gives the topic
composition across a document. For example, in Fig. 1c, the
image may be composed of 40% “sand ripple” topic and 60%
“ﬂat sand” topic (i.e., α = [0.4, 0.6]). The parameter λ controls
how similar the partial membership vector of each word is
expected to be to the topic distribution of the document. For
example, a small λ would correspond to most words in an
document to have partial membership vectors very close to πd.
During image segmentation, a small λ generally corresponds
to large transition regions (e.g., transition from “ﬂat sand”
to “sand ripple” comprises most of the image). For a large
λ, the partial membership vectors for each word can vary
signiﬁcantly from the document mixing proportions. In general,
a large λ corresponds to very narrow (tending towards crisp)
transition regions during image segmentation (e.g., the SAS
image may have 39% of the visual words as pure “sand ripple”,
59% as pure “ﬂat sand”, and only 2% mixed). The vector

(a)

(b)

Fig. 3: Partial membership data generating distributions. In (a),
two Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6], Σ1 =
Σ2 = I. In (b), two Gaussian topics with µ1 = [−4, −4]; µ2 =
[6, 6], Σ1 = [4, 0; 0, 1], Σ2 = [1, 0; 0, 4] [11].

n ∼ Dir(πdsd) represents the partial memberships of data
zd
point xd
n in each of the K topics. If each topic distribution is
assumed to be of the exponential family, pk(·|βk) = Expon(ηk),
k zd
then using the result in (3), p(xd
nkηk).
The graphical model for PM-LDA is shown in Fig. 2c.

n, β) = Expon((cid:80)

n|zd

In PM-LDA, the membership zd

n is drawn from a Dirichlet
distribution which is in contrast to a multinomial distribution
as used in LDA. With the inﬁnite number of possible values
for zd
n, the word generating distributions in PM-LDA are
expanded from only K generating distributions (as in LDA)
to inﬁnitely many. Fig. 3 illustrates this using two Gaussian
topic distributions, where the membership value to one topic
is varied from 0 to 1 with an increment 0.1. The two original
topics are shown as the Gaussian distributions at either end.
In LDA, words are generated from only the two original topic
distributions. In PM-LDA, words can be generated from any
(of the inﬁnitely many) convex combinations of the topic
distributions. As the scaling factor s → 0, the PM-LDA model
will degrade to the LDA model.

Given the hyperparameters Ψ = {α, λ, β}, the full PM-LDA

model over all words in the dth document is:

p(πd, sd, Zd, Xd|α, λ, β)

(5)

= p(πd|α)p(sd|λ)

p(xd

n|zd

n, β)p(zd

n|πdsd).

N d
(cid:89)

n=1

n}N d

where πd are the topic proportions, scaling factor sd, partial
n=1 and a set of N d words
membership vectors Zd = {zd
Xd for document d. The log of (5) when considering the
speciﬁc forms chosen in our model, is shown in (6). During
parameter estimation, our goal is to maximize L = (cid:80)D
d=1 Ld
by estimating all the model parameters πd, sd, zd
n, β. In this
paper, we employ an Metropolis within Gibbs [12], [13]
sampling approach.

IV. PARAMETER ESTIMATION FOR PM-LDA

The goal of parameter estimation is to maximize the

following posterior distribution,

p(Π, S, M, β|D, α, λ) ∝ p(Π, S, M, D|α, λ, β),
(7)
where D = (cid:8)X1, X2, ..., XD(cid:9) includes all training documents
and Π, S, M include all of the topic proportions, scaling factors
and membership vectors, respectively.

A Metropolis within Gibbs sampler is employed to perform
the MAP inference which can generate samples from the
posterior distribution in (7), [12], [13]. An outline of the
sampler is provided in Alg. 1. The sampler is simple and
straight-forward implement composed only of a series of
draws from candidate distributions for each parameter and then
evaluation of the candidate in the appropriate acceptance ratio.
Our implementation of the sampler has been posted online.1 In
our current implementation, we consider the topic distributions
to be Gaussian with different means, µk, but identical diagonal
and isotropic covariance matrices, Σk = σ2I.

Algorithm 1 Metropolis-within-Gibbs Sampling Method for
Parameter Estimation
Input: A corpus D, the number of topics K, and the number

of sampling iterations T

Output: Collection of all samples: Π(t), S(t), M(t), β(t) =

2 , Σ(t)

2 ..., µ(t)

K , Σ(t)

K

(cid:111)
.

(cid:110)

1 , Σ(t)
1 , µ(t)
µ(t)
1: for t = 1 : T do
2:
3:

for d = 1 : D do

(cid:111)

(cid:110)
1, p(π†,s(t−1),Z(t−1),X|Ψ)p(π(t−1)|α)
p(π(t−1),s(t−1),Z(t−1),X|Ψ)p(π†|α)

Sample πd: Draw candidate: π† ∼ Dir(α)
Accept candidate with probability:
aπ = min
Sample sd: Draw candidate: s† ∼ exp(λ)
Accept candidate with probability:
as = min
for n = 1 : N d do

(cid:110)
1, p(π(t),s†,Z(t−1),X|Ψ)p(s(t−1)|λ)
p(π(t),s(t−1),Z(t−1),X|Ψ)p(s†|λ)

(cid:111)

n: Draw candidate: z†

Sample zd
Accept candidate with probability:
n,xn|Ψ)
az = min

p(π(t),s(t),z†
p(π(t),s(t),z(t−1)

1,

(cid:110)

,xn|Ψ)

n

(cid:111)

n ∼ Dir(1K)

end for

end for
for k = 1 : K do

Sample µk: Draw proposal: µ†
k ∼ N (·|µD, f ΣD)
µD and ΣD are mean and covariance of the data
Accept candidate with probability:
(cid:17)

(cid:16)

p

p

(cid:16)

Π(t),S(t),M(t),D|µ†
k
(t−1)
Π(t),S(t),M(t),D|µ
k

N (µ
(cid:17)

(t−1)
k
N (µ†

|µD,ΣD)

(cid:27)

k|µD,ΣD)

ak = min

1,

(cid:26)

end for
Sample covariance matrices Σ = σ2I:
Draw candidate from:
σ2 = 1
2
Accept candidate with probability:

(cid:26)

aΣ = min

1,

p(Π(t),S(t),M(t),D|Σ†)
p(Π(t),S(t),M(t),D|Σ(t−1))

(cid:27)

.

(cid:8)maxxn d2(xn − µD) − minxn d2(xn − µD)(cid:9)

4:

5:
6:

7:
8:
9:
10:

11:
12:

13: end for

The proposed Metropolis within Gibbs scheme will return the
full distribution of parameter values given the desired posterior.
We use the MAP sample (i.e., the sample with the largest log
posterior value) as the ﬁnal estimate, {Π∗, S∗, M∗, µ∗, Σ∗}.

1Code can be found at: https://github.com/TigerSense/PMLDA

Ld = ln(p(πd, sd, Zd, Xd|α, λ, β)) = ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πd

k + ln λ − λsd

(cid:32) K
(cid:88)

(cid:33)

k=1

(cid:33)

K
(cid:88)

k=1

K
(cid:88)

k=1

N d
(cid:88)

(cid:26)

n=1

(cid:32) K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

+

ln p(xd

n|zd

n, β) +

ln Γ

sdπd
k

−

ln Γ(sdπd

k) +

(sdπd

k − 1) ln zd
nk

(6)

(cid:27)

N d
(cid:88)

n=1

V. DATA & EXPERIMENTAL RESULTS

In this section, we show results of image segmentation on
three datasets: (i) Synthetic Aperture SONAR (SAS) imagery,
(ii) Sunset imagery, and the (iii) MSRC dataset [14].

a) Synthetic Aperture Sonar (SAS) Imagery Dataset:

From our SAS image dataset, we selected 4 images (shown in
the ﬁrst column in Fig. 4) and compute the average intensity
value and entropy within a 21 × 21 window as feature values.
The average intensity value is scaled (×10) to roughly the same
magnitude of the average entropy value. Each image is divided
into multiple documents using a sliding window approach. A
document consists of all of the feature vectors associated with
each pixel (i.e., visual words) in the window. The number of
topics in this dataset is set to 3. For LDA, a dictionary of
size 100 is built by clustering all the computed feature values
using the K-means. FCM results with m = 1.5. Parameters
for LDA and FCM were selected manually to provide the best
results. Due to the lack of ground-truth, qualitative segmentation
results in Fig. 4 is provided. In the ﬁrst row, Subﬁgures (b),
(c), and (d) show the partial membership maps in the “dark
ﬂat sand” , “sand ripple” , and “bright ﬂat sand” topics using
PM-LDA, respectively. Subﬁgures (f), (g), and (h) show the
partial membership maps in each of the three clusters using
FCM, respectively. In (b) - (d) and (f) - (h), the color indicates
the degree of membership of a visual word in a topic where
red corresponds to a full membership of 1 and dark blue color
corresponds to a membership value of 0. The LDA result is
shown in (e) where color indicates topic assignment. Subﬁgures
in Row 2-4 follow the same subﬁgure captions in Row 1.

From the experimental results, we can see that PM-LDA
achieves much better results than FCM and LDA. As shown
in Fig. 4c and 4k, the segmentation results of PM-LDA show
a gradual change from “sand ripple” to “dark ﬂat sand”. FCM
captures the gradual transition to some extent but is not able to
clearly differentiate between clusters. For example, as shown
in Fig. 4o - 4p and Fig. 4w - 4x, using FCM, the rippled
region in Images 2 and 3 is assigned to 2 clusters with nearly
equal partial memberships. As LDA cannot generate partial
memberships, in Fig. 4e and 4m, Image 1 and 2 are simply
partitioned into different topics using LDA. Yet, by comparing
Fig. 4u with 4t and Fig. 4ac with 4ab, we can see that on
Image 3 and 4 that do not contain transition regions, LDA
achieves similar segmentation result to PM-LDA.

As discussed in Section III, the scaling factor sd determines
the similarity of the partial membership vector of each word, zd
n,
to the topic proportion πd. In this experiment, we investigated
the effect of s by estimating the memberships and topics

with ﬁxed topic proportion. A subregion consisting of three
superpixels [15] are used in this experiment and shown in
Fig. 5. Each superpixel is treated as a document. The topic
proportion πd is set to be [1, 1, 1]/3 and the scaling factor s
is varied to be 3, 10, 300, 30000. The membership estimation
results are shown in Figure 6. As can be seen, as the scaling
factor s increases, the partial memberships gradually approach
the topic proportion [1, 1, 1]/3 and become more smooth.

Fig. 5: A subregion of three superpixels

(a) s = 3

(a) s = 10

(a) s = 300

(b)

(b)

(b)

(c)

(c)

(c)

(c)

(a) s = 30000

(b)

Fig. 6: Partial membership maps with varying s. Each row
shows the estimated membership maps of the three estimated
topics. The black contour indicates the superpixel boundary.
The superpixels are results published in [15].

b) Sunset Dataset: Experimental results on Sunset dataset
show the ability of PM-LDA to perform partial membership
segmentation given visual natural imagery. Two sunset themed
images from Flickr (with the necessary permissions)2 3 were
used in this experiment. For each visual word, the ﬁrst order
Gaussian gradient (σ = 2) with respect to y-axis and R and
B channels are used as the feature vectors. The number of
topics is set to be 3. Experiments are run on each image
individually and results are shown in Fig. 7. Columns 2-4
show the segmentation results of PM-LDA. Column 5 is the
LDA results with 3 topics. Comparing Column 3 and Column

2Photo can be found at: https://www.ﬂickr.com/photos/aoa-/6104409480/
3Photo can be found at: https://www.ﬂickr.com/photos/frenchdave/8482336933/

(a) Image 1

(b) PM-LDA:1 (c) PM-LDA:2 (d) PM-LDA:3

(e) LDA

(f) FCM:1

(g) FCM:2

(h) FCM:3

(i) Image 2

(j) PM-LDA:1 (k) PM-LDA:2 (l) PM-LDA:3

(m) LDA

(n) FCM:1

(o) FCM:2

(p) FCM:3

(q) Image 3

(r) PM-LDA:1 (s) PM-LDA:2 (t) PM-LDA:3

(u) LDA

(v) FCM:1

(w) FCM:2

(x) FCM:3

(y) Image 4

(z) PM-LDA:1 (aa) PM-LDA:2 (ab) PM-LDA:3

(ac) LDA

(ad) FCM:1

(ae) FCM:2

(af) FCM:3

Fig. 4: Segmentation results of Image 1 - 4 using PM-LDA, FCM and LDA. (a): SAS Image. (b)-(d): PM-LDA partial
membership map in the “dark ﬂat sand,” “sand ripple,” “bright ﬂat sand” topics, respectively. (e): LDA result where color
indicates topic label. (f)-(h): FCM partial membership map in the ﬁrst, second, and third cluster, respectively. Subﬁgure captions
in Row 2 - 4 follow those in Row 1. In PM-LDA and FCM results, color indicates the degree of membership of a visual word
in a topic or cluster.

5 in Fig. 7, we can see that PM-LDA can generate continuous
partial membership according to the extent to which the sky
is colored by sunlight. The partial membership map illustrates
how the topic gradually shift from one to the other. In contrast,
LDA can only produce 0-1 segmentation.

(a)

(b)

(c)

(d)

(e)

Fig. 9: ROC curve of PM-LDA for cow detection evaluated at
the pixel level. The red star represents the LDA cow detection
results.

(f)

(g)

(h)

(i)

(j)

Fig. 7: Examples of segmentation result on Sunset dataset. (a):
Sunset Image 1. (f): Sunset Image 2. (b)-(d) and (g)-(i) are
the PM-LDA partial membership maps in the estimated three
topics for Sunset Image 1 and Sunset Image 2, respectively.
The color indicates the degree of membership of a visual word
in a topic or cluster. (e) and (j) are the LDA results where
color indicates the topic.

c) Microsoft Research Cambridge data set version one
(MSRCv1): The MSRCv1 database consists of 240, 213 × 320
pixel images. A subset from this database consisting of all
images that include the “grass”, “cow”, and “sky” topics was
used in this experiment. The local descriptors proposed in [16],
the output of a set of ﬁlter bank responses made of 3 Gaussians,
4 Laplacian of Gaussians (LoG) and 4 ﬁrst order derivatives of

Gaussians were used as the feature vectors. The ﬁlter window
size used was 15 × 15. In this experiment, instead of using
each image as a document, we apply normalized cuts method
to get 40 super-pixel segments from each image and treat each
super-pixel as a document. The topic number is set to be 3,
and for LDA, we densely sample the ﬁlter bank output and
build a dictionary of size 200. Quantitative comparison results
on accurate detection of the “cow” class are shown in Fig. 9.
ROC curve analysis of PM-LDA using the “cow” membership
map was conducted. The red star indicates the quantitative
LDA result (a ROC curve cannot be generated due to the crisp
segmentation of the LDA method). Example results are shown
in Fig. 8. In Subﬁgure (e), transition regions are highlighted
by indicating the pixels with at least one membership value
in range [0.4, 0.6]. As shown in (e), these partial membership
values mostly occur at the boundary between two topics. Thus,

(a) cow1

(b) “grass”

(c) “sky”

(d) “cow”

(e) transition

(f) max

(g) LDA

(a) cow2

(t) “grass”

(c) “sky”

(d) “cow”

(e) transition

(f) max

(g) LDA

Fig. 8: Example of PM-LDA and LDA results. (a) Original image. (b)-(d) Results of PM-LDA, the partial membership maps in
“grass”, “sky”, and “cow” topics, respectively. The color indicates the degree of membership in a topic. (e) Transition regions
consisting of visual words with at lease one partial membership value in range [0.4, 0.6] (f) Modiﬁed segmentation result of
PM-LDA by assigning each visual word to the topic with the largest membership. The colors indicate topics. (g) Result of
LDA. The colors indicate topics.

[11] K. A. Heller, S. Williamson, and Z. Ghahramani, “Statistical models for
partial membership,” in International Conference on Machine Learning,
2008, pp. 392–399. 1, 2, 3

[12] T. Glenn, A. Zare, and P. Gader, “Bayesian fuzzy clustering,” IEEE
Transaction on Fuzzy Systems, no. 8, pp. 1545–1561, 2015. 1, 3
[13] C. Robert and G. Casella, Monte Carlo statistical methods. Springer

Science & Business Media, 2013. 3

[14] A. Criminisi, “Microsoft research cambridge object recognition image

database (version 1.0 and 2.0), 2004.” 4

[15] J. T. Cobb and A. Zare, “Multi-image texton selection for sonar image
seabed co-segmentation,” in SPIE, vol. 8709, no. 87090H, June 2013. 4
[16] J. Winn, A. Criminisi, and T. Minka, “Object categorization by learned
universal visual dictionary,” in IEEE International Conference on
Computer Vision, 2005, pp. 1800–1807. 5

PM-LDA is able to identify when the feature vector contains
information from multiple topics (as the feature vector is
being computed over a window that contains more than one
topic). This is a powerful result showing the effectiveness
of PM-LDA to provide semantic image understanding. For
comparison with LDA, we modiﬁed the segmentation result of
PM-LDA by assigning each visual word to the topic with the
largest membership. As shown in (f) and (g), PM-LDA can
achieve similar results to LDA. So on these images with crisp
boundaries, PM-LDA can generate binary membership values,
and learn the three semantic topics comparable to LDA. Thus,
PM-LDA also is effective for use in crisp labeling problems.

REFERENCES

[1] B. Zhao, L. Fei-Fei, and E. P. Xing, “Image segmentation with topic
random ﬁeld,” in European Conference on Computer Vision, 2010, pp.
785–798. 1

[2] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8,
pp. 888–905, 2000. 1

[3] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature
space analysis,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, no. 5, pp. 603–619, 2002. 1

[4] P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient graph-based image
segmentation,” International Journal of Computer Vision, vol. 59, no. 2,
pp. 167–181, 2004. 1

[5] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
Journal of Machine Learning Research, vol. 3, pp. 993–1022, 2003. 1, 2
[6] B. C. Russell, W. T. Freeman, A. Efros, J. Sivic, and A. Zisserman,
“Using multiple segmentations to discover objects and their extent in
image collections,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2006, pp. 1605–1614. 1

[7] L. Cao and L. Fei-Fei, “Spatially coherent latent topic model for
concurrent segmentation and classiﬁcation of objects and scenes,” in
IEEE International Conference on Computer Vision, 2007, pp. 1–8. 1
[8] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in Advances
in Neural Information Processing Systems, 2008, pp. 1577–1584. 1
[9] M. Andreetto, L. Zelnik-Manor, and P. Perona, “Unsupervised learning
of categorical segments in image collections,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1842–1855,
2012. 1

[10] J. C. Bezdek, R. Ehrlich, and W. Full, “Fcm: The fuzzy c-means clustering
algorithm,” Computers & Geosciences, vol. 10, no. 2, pp. 191–203, 1984.
1

