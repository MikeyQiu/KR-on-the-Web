Real-Time Hair Rendering using Sequential
Adversarial Networks

Lingyu Wei1,2[0000−0001−7278−4228], Liwen Hu1,2,
Vladimir Kim3, Ersin Yumer4, and Hao Li1,2

1 Pinscreen Inc., Los Angeles CA 90025, USA
2 University of Southern California, Los Angeles CA 90089, USA
3 Adobe Research, San Jose CA 95110, USA
4 Argo AI, Pittsburgh PA 15222, USA

reference image reference hair model

rendering results

reference image

reference hair model

rendering results

Fig. 1. We propose a real-time hair rendering method. Given a reference image, we
can render a 3D hair model with the referenced color and lighting in real-time. Faces
in this paper are obfuscated to avoid copyright infringement

Abstract. We present an adversarial network for rendering photoreal-
istic hair as an alternative to conventional computer graphics pipelines.
Our deep learning approach does not require low-level parameter tun-
ing nor ad-hoc asset design. Our method simply takes a strand-based
3D hair model as input and provides intuitive user-control for color and
lighting through reference images. To handle the diversity of hairstyles
and its appearance complexity, we disentangle hair structure, color, and
illumination properties using a sequential GAN architecture and a semi-
supervised training approach. We also introduce an intermediate edge
activation map to orientation ﬁeld conversion step to ensure a successful
CG-to-photoreal transition, while preserving the hair structures of the
original input data. As we only require a feed-forward pass through the
network, our rendering performs in real-time. We demonstrate the syn-
thesis of photorealistic hair images on a wide range of intricate hairstyles
and compare our technique with state-of-the-art hair rendering methods.

Keywords: Hair rendering · GAN

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

2

1

Introduction

Computer-generated (CG) characters are widely used in visual eﬀects and games,
and are becoming increasingly prevalent in photo manipulation and in virtual
reality applications. Hair is an essential visual component of virtual charac-
ters. However, while signiﬁcant advancements in hair rendering have been made
in the computer graphics community, the production of aesthetically realistic
and desirable hair renderings still relies on a careful design of strand models,
shaders, lights, and composites, generally created by experienced look develop-
ment artists. Due to the geometric complexity and volumetric structure of hair,
modern hair rendering pipelines often combine the use of eﬃcient hair repre-
sentation, physically-based shading models, shadow mapping techniques, and
scattering approximations, which not only increase the computational cost, but
also the diﬃculty for tweaking parameters. In high-end ﬁlm production, it is not
unusual that a single frame for a photorealistic hair on a rendering farm takes
several minutes to generate. While compelling real-time techniques have been
introduced recently, including commercial solutions (e.g., NVIDIA HairWorks,
Unity Hair Tools), the results often appear synthetic and are diﬃcult to author,
even by skilled digital artists. For instance, several weeks are often necessary
to produce individualized hair geometries, textures, and shaders for hero hair
assets in modern games, such as Uncharted 4 and Call of Duty: Ghosts.

Inspired by recent advances in generative adversarial networks (GANs), we
introduce the ﬁrst deep learning-based technique for rendering photorealistic
hair. Our method takes a 3D hair model as input in strand representation and
uses an example input photograph to specify the desired hair color and lighting.
In addition to our intuitive user controls, we also demonstrate real-time perfor-
mance, which makes our approach suitable for interactive hair visualization and
manipulation, as well as 3D avatar rendering.

Compared to conventional graphics rendering pipelines, which are grounded
on complex parametric models, reﬂectance properties, and light transport sim-
ulation, deep learning-based image synthesis techniques have proven to be a
promising alternative for the eﬃcient generation of photorealistic images. Suc-
cessful image generations have been demonstrated on a wide range of data in-
cluding urban scenes, faces, and rooms, but ﬁne level controls remain diﬃcult
to implement. For instance, when conditioned on a semantic input, arbitrary
image content and visual artifacts often appear, and variations are also diﬃcult
to handle due to limited training samples. This problem is further challenged
by the diversity of hairstyles, the geometric intricacy of hair, and the aesthetic
complexity of hair in natural environments. For a viable photorealistic hair ren-
dering solution, we need to preserve the intended strand structures of a given
3D hair model, as well as provide controls such as color and lighting.

Furthermore, the link between CG and real-world images poses another chal-
lenge, since such training data is diﬃcult to obtain for supervised learning. Pho-
toreal simulated renderings are time-consuming to generate and often diﬃcult to
match with real-world images. In addition, capturing photorealistic hair models
is hard to scale, despite advances in hair digitization techniques.

Real-Time Hair Rendering using Sequential Adversarial Networks

3

In this work, we present an approach, based on a sequential processing of
a rendered input hair models using multiple GANs, that converts a semantic
strand representation into a photorealistic image (See Fig. 1 and Section 5). Color
and lighting parameters are speciﬁed at intermediate stages. The input 3D hair
model is ﬁrst rendered without any shading information, but strand colors are
randomized to reveal the desired hair structures. We then compute an edge acti-
vation map, which is an important intermediate representation based on adaptive
thresholding, which allows us to connect the strand features between our input
CG representation and a photoreal output for eﬀective semi-supervised training.
A conditional GAN is then used to translate this edge activation map into a
dense orientation map that is consistent with those obtained from real-world
hair photographs. We then concatenate two multi-modal image translation net-
works to disentangle color and lighting control parameters in latent space. These
high-level controls are speciﬁed using reference hair images as input, which al-
lows us to describe complex hair color variations and natural lighting conditions
intuitively. We provide extensive evaluations of our technique and demonstrate
its eﬀectiveness on a wide range of hairstyles. We compare our rendered images
to ground truth photographs and renderings obtained from state-of-the-art com-
puter graphics solutions. We also conduct a user study to validate the achieved
level of realism.

Contributions: We demonstrate that a photorealistic and directable render-
ing of hair is possible using a sequential GAN architecture and an intermediate
conversion from edge activation map to orientation ﬁeld. Our network decou-
ples color, illumination, and hair structure information using a semi-supervised
approach and does not require synthetic images for training. Our approach in-
fers parameters from input examples without tedious explicit low-level modeling
speciﬁcations. We show that color and lighting parameters can be smoothly
interpolated in latent space to enable ﬁne-level and user-friendly control. Com-
pared to conventional hair rendering techniques, our method does not require
any low-level parameter tweaking or ad-hoc texture design. Our rendering is
computed in a feed forward pass through the network, which is fast enough for
real-time applications. Our method is also signiﬁcantly easier to implement than
traditional global illumination techniques. We plan to release the code and data
to the public.‡

2 Related Work

In this section we provide an overview of state-of-the-art techniques for hair
rendering and image manipulation and synthesis.

Fiber-level hair renderings produce highly realistic output, but incurs sub-
stantial computational cost [31, 8, 43, 42, 44], but also require some level of ex-
pertise for asset preparation and parameter tuning by a digital artist. Various
simpliﬁed models have been proposed recently, such as dual scattering [54], but

‡ Available on project page: http://cosimo.cn/#hair render

4

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

its real-time variant have a rather plastic and solid appearance. Real-time ren-
dering techniques generally avoid physically-based models, and instead rely on
approximations that only mimics its appearance, by modeling hair as paramet-
ric surfaces [25, 24], meshes [46, 18], textured morphable models [2], or multiple
semi-transparent layers [40, 45]. Choosing the right parametric model and setting
the parameters for the desired appearance requires substantial artist expertise.
Converting across diﬀerent hair models can be casted as a challenging optimiza-
tion or learning problem [43]. Instead, in this work, we demonstrate that one
can directly learn a representation for hair structure, appearance, and illumina-
tion using a sequence of GANs, and that this representation can be intuitively
manipulated by using example images.

Substantial eﬀorts have been dedicated to estimating hair structures from
natural images, such as with multi-view hair capturing methods [47, 17, 15, 29,
[3, 5, 6, 4, 16]
28, 30, 35, 34]. Recently, single-view hair reconstruction methods
are becoming increasingly important because of the popularity in manipulating
internet portraits and selﬁes. We view our work as complementary to these
hair capturing methods, since they rely on existing rendering techniques and do
not estimate the appearance and illumination for the hair. Our method can be
used in similar applications, such as hair manipulation in images [5, 6], but with
simpler control over the rendering parameters.

Neural networks are increasingly used for the manipulation and synthesis of
visual data such as faces [39, 19], object views [50], and materials [27]. Recently,
Nalbach et al. [32] proposed how to render RGB images using CNN, but it
requires aligned attributes e.g. normal and reﬂectance per pixel, which are not
well-deﬁned for hair strands with sub-pixel details. Generative models with an
adversary [12, 36] can successfully learn a data representation without explicit
supervision. To enable more control these models have been further modiﬁed to
consider user input [51] or to condition on a guiding image [20]. While the latter
provides a powerful manipulation tool via image-to-image translation, it requires
strong supervision in the form of paired images. This limitation has been further
addressed by enforcing cycle-consistency across unaligned datasets [52]. Another
limitation of the original image translation architecture is that it does not handle
multimodal distributions which are common in synthesis tasks. This is addressed
by encouraging bijections between the output and latent spaces [53] in a recently
introduced architecture known as BicycleGAN. We assess this architecture as
part of our sequential GAN for hair rendering.

Our method is also related to unsupervised learning methods that remove
part of the input signal, such as color [48], and then try to recover it via an
auto-encoder architecture. However, in this work we focus on high quality hair
renderings instead of generic image analysis, and unlike SplitBrain [48], we use
image processing to connect two unrelated domains (CG hair models and real
images).

Variants of these models have been applied to many applications including
image compositing [26], font synthesis [1], texture synthesis [41], facial texture
synthesis [33], sketch colorization [38], makeup transfer [7], and many more.

Real-Time Hair Rendering using Sequential Adversarial Networks

5

Hair has a more intricate appearance model due to its thin semi-transparent
structures, inter-reﬂections, scattering eﬀects, and very detailed geometry.

3 Method

Fig. 2. Overview of our method. Given a natural image, we ﬁrst use simple image
processing to strip it oﬀ salient details such as color, lighting, and ﬁber-level structure,
leaving only coarse structure captured in activation map (top row). We encode each
simpliﬁed image into its own latent space which are further used by generators. The
CG hair is rendered in a style mimicing the extracted coarse structure, and generators
are applied in inverse order to add the details from real hair encoded in the latent
space yielding the realistic reconstruction (bottom row)

We propose a semi-supervised approach to train our hair rendering network
using only real hair photographs during training. The key idea of our approach is
to gradually reduce the amount of information by processing the image, eventu-
ally bringing it to a simple low dimensional representation, edge activation map.
This representation can also be trivially derived from a CG hair model, enabling
us to connect two domains. The encoder-decoder architecture is applied to each

6

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

simpliﬁed representation, where the encoder captures the information removed
by the image processing and the decoder recovers it (see Fig. 2).
Given a 2D image I1 we deﬁne image processing ﬁlters {Fi}3

i=1 that generate
intermediate simpliﬁed images Ii+1 := Fi(Ii). Each intermediate image Ii is
ﬁrst encoded by a network Ei(Ii) to a feature vector zi and then decoded with
a generator Gi(zi, Ii). The decoder is trained to recover the information, that
is lost in a particular image processing step. We use a conditional variational
autoencoder GAN [53] for each encoder-decoder pair.

Our sequential image processing operates in the following three steps. First,
I2 := F1(I1) desaturates a segmented hair region of an input photograph to
produce a grayscale image. Second, I3 := F2(I2) is the orientation map using the
maximal response of a rotating DoG ﬁlter [28]. Third, F3(I3) is an edge activation
map obtained using adaptive thresholding, and each pixel contains only the
values 1 or -1 indicating if it is activated (response higher than its neighboring
pixels) or not. This edge activation map provides a basic representation for
describing hair structure from a particular viewpoint, and the edge activation
map derived from natural images or rendering of CG model with random strand
colors can be processed equally well with our generators. Fig. 3 demonstrates
some examples of our processing pipeline applied to real hair.

At the inference time, we are given an input 3D hair model in strand rep-
resentation (100 vertices each) and we render it with randomized strand colors
from a desired viewpoint. We apply the full image processing stack F3 ◦ F2 ◦ F1
to obtain the edge activation map. We then can use the generators G1 ◦ G2 ◦ G3
to recover the realistic looking image from the the edge activation map. Note
that these generators rely on encoded features zi to recover the desired details,
which provides an eﬀective tool for controlling rendering attributes such as color
and lighting. We demonstrate that our method can eﬀectively transfer these at-
tributes by encoding an example image and by feeding the resulting vector to
the generator (where ﬁne-grained hair structure is encoded in z3, natural illumi-
nation and hair appearance properties are encoded in z2, and detailed hair color
is encoded in z1).

Fig. 3. Examples of our training data. For each set of images from left to right, we
show input image; segmented hair; gray image; orientation map; and edge activation
map

Real-Time Hair Rendering using Sequential Adversarial Networks

7

4

Implementation

Given a pair of input/output images for each stage in the rendering pipeline
(e.g. the segmented color image I1 and its grayscale version I2) we train both
the encoder network and generator network together. The encoder E1 extracts
the color information z1 := E1(I1), and the generator reconstructs a color image
identical to I1 using only the gray scaled image I2 and the parameter z1, I1 ≈
G1(z1, I2). These pairs of encoder and generator networks enable us to extract
information available in higher dimensional image (e.g., color), represent it with
a vector z1, and then use it convert an image in the lower-dimensional domain
(e.g., grayscale) back to the higher-dimensional representation.

We train three sets of networks (Ei, Gi) in a similar manner, using the
training images Ii, Ii+1 derived from the input image I via ﬁlters Fi. Since
these ﬁlters are ﬁxed, we can treat these three sets of networks independently
and train them in parallel. For the rest of this section, we focus on training
only a single encoder and generator pair, and thus use a simpliﬁed notation:
G := Gi, E := Ei, I := Ii, I ′ := Ii+1 = Fi(Ii), z := zi = Ei(Ii).

Fig. 4. Our entire network is composed of three encoder and generator pairs (Ei, Di)
with the same cVAE-GAN architecture depicted in this ﬁgure

4.1 Architecture

We train the encoder and generator network pair ((E, G)) using the conditional
variational autoencoder GAN (cVAE-GAN) architecture [53] (see Fig. 4). A
ground truth image I is being processed by an encoder E, producing the latent
vector z. This z and the ﬁltered input image I ′ are both inputs to the generator
G. Our loss function is composed of three terms:

LVAE
1

(G, E) = kI − G(E(I), I ′)k1

8

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

penalizes the reconstruction error between I and the generated image produced
by G(z, I ′).

LKL(E) = DKL(E(I)kN (0, 1))

favors z = E(I) to come from the normal distribution in the latent space, where
DKL is the KL-divergence of two probability distributions. This loss preserves the
diversity of z and allows us to eﬃciently re-sample a random z from the normal
distribution. Finally, an adversarial loss is introduced by training a patch-based
discriminator [21] D. The discriminator takes either I or G(z, I ′) and classiﬁes
whether the input comes from real data or from the generator. A path-based
discriminator will learn to distinguish the local feature from its receptive ﬁeld,
and penalize artifacts produced in the local regions of G(z, I ′).

We use the add to input method from the work of [53] to replicate z. For
a tensor I ′ with size H × W × C and z with size 1 × Z, we copy value of z,
extending it to a H × W × Z tensor, and concatenate this with the tensor I ′
on the channel dimension, resulting a H × W × (Z + C) tensor. This tensor
is used as G’s input. We considered additional constraints, such as providing a
randomly drawn latent vector to the generator [10, 11], but we did not achieve
visible improvements by adding more terms to our loss function. We provide a
comparison between cVAE-GAN and the BicycleGAN architecture in the results
section.

4.2 Data Preparation

Since we are only focusing on hair synthesis, we mask non-hair regions to ignore
their eﬀect, and set their pixel values to black. To avoid manual mask annota-
tion, we train Pyramid Scene Parsing Network [49] to perform automatic hair
segmentation. We annotate hair masks for 3000 random images from CelebA-HQ
dataset [23], and train our network on this data. We use the network to compute
masks for the entire 30,000 images in CelebA-HQ dataset, and manually remove
images with wrong segmentation, yielding about 27,000 segmented hair images.‡
We randomly sampled 5,000 images from this dataset to use as our training data.
We apply same deterministic ﬁlters Fi on each image in the training data,
to obtain the corresponding gray image, orientation maps, and edge activation
maps.

4.3 Training

We apply data augmentation including random rotation, translation, and color
perturbation (only for input RGB images) to add more variations to the training
set. Scaling is not applied, as the orientation map depends on the scale of the
texture details from the gray-scaled image. We choose the U-net [37] architecture
for generator G, which has an encoder-decoder architecture with symmetric skip
connections allowing generation of pixel-level details as well as preserving the
global information. ResNet [13] is used for encoder E, which consists of 6 groups
of residual blocks. In all experiments, we use a ﬁxed resolution 512 × 512 for

Real-Time Hair Rendering using Sequential Adversarial Networks

9

all images, and the dimension of z is 8 in each transformation, following the
choice from the work of Zhu et al. [53]. We train each set of networks from 5000
images for 100 epochs, with a learning rate gradually decreasing from 0.0001
to zero. The training time for each set is around 24 hours. Lastly, we also add
random Gaussian noise withdrawn from N (0, σ1) with gradually decreasing σ
to the image, before feeding them to D, to stabilize the GAN training.

5 Results

Geometric hair models used in Fig. 1, Fig. 6 and Fig. 9 are generated using
various hair modeling techniques [16, 17, 15]. The traditional computer graphic
models used for comparison in Fig. 10 are manually created in Maya with XGen.
We use the model from USC Hair Salon [14] in Fig. 7.

Real-Time Rendering System. To demonstrate the utility of our method, we
developed a real-time rendering interface for hair (see Fig. 5 and supplemen-
tal video). The user can load a CG hair model, pick the desired viewpoint,

Fig. 5. Our interactive interface

and then provide an example image for color and lighting speciﬁcation. This
example-based approach is user friendly as it is often diﬃcult to describe hair
colors using a single RGB value as they might have dyed highlights or natural
variations in follicle pigmentations. Fig. 1 and Fig. 6 demonstrate the rendering
results with our method. Note that our method successfully handles a diverse
set of hairstyles, complex hair textures, and natural lighting conditions. One can
further reﬁne the color and lighting by providing additional examples for these
attributes and interpolate between them in latent space (Fig. 7). This feature
provides an intuitive user control when a desired input example is not available.

10

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

reference image

reference 
hair model

reconstructed
orientation map

rendering result

other views

Fig. 6. Rendering results for diﬀerent CG hair models where color and lighting condi-
tions are extracted from a reference image

Comparison. We compare our sequential network to running BicycleGAN to
directly render the colored image from the orientation ﬁeld without using the
sequential network pairs (Fig. 8). Even if we double the number of parameters
and training time, we still notice that only the lighting, but not color, is accu-
rately captured. We believe that the lighting parameters are much harder to be
captured than the color parameters, hence the combined network may always
try to minimize the error brought by lighting changes ﬁrst, without considering
the change of color as an equally important factor. To qualitatively evaluate,
our system we compare to state-of-the-art rendering techniques. Unfortunately,
these rendering methods do not take reference images as input, and thus lack
similar level of control. We asked a professional artist to tweak the lighting and
shading parameters to match our reference images. We used an enhanced ver-
sion of Unity’s Hair Tool, a high-quality real-time hair rendering plugin based on

Real-Time Hair Rendering using Sequential Adversarial Networks

11

l

r
o
o
c

g
n
i
t
h
g

i
l

s
s
e
n
h
g
u
o
r
 
r
a
u
c
e
p
s

l

reference image A

result A

interpolation results

result B

reference image B

Fig. 7. We demonstrate rendering of the same CG hair model where material attributes
and lighting conditions are extracted from diﬀerent reference images, and demonstrate
how the appearance can be further reﬁned by interpolating between these attributes
in latent space

(a)

(b)

(c)

Fig. 8. Comparisons between our method with BicycleGAN (i.e., no sequential net-
work). From left to right, we show (a) reference image; (b) rendering result with Bicy-
cleGAN; (c) rendering result with our sequential pipeline

Kajiya-Kay reﬂection model [22] (Fig. 9). Note that a similar real-time shading
technique that approximates multiple scattering components was used in the
state-of-the-art real-time avatar digitization work of [18]. Our approach appears
less synthetic and contains more strand-scale texture variations. Our artist used
the default hair shader in Solid Angle’s Arnold renderer, which is a commercial
implementation of a hybrid shaded based on the works of [9] and [54], to match
the reference image. This is an oﬄine system and it takes about 5 minutes to
render a single frame (Fig. 10) on an 8-core AMD Ryzen 1800x machine with
32 GB RAM. While our renderer may not reach the level of ﬁdelity obtained
from high-end oﬄine rendering systems, it oﬀers real time performance, and in-
stantaneously, produces a realistic result. It also matching the lighting and hair
color of a reference image, a task that took our experienced artist over an hour
to perform.

User Study. We further conduct a user study to evaluate the quality of our
renderings in comparison to real hair extracted from images. We presented our

12

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

Fig. 9. Comparisons between our method with real-time rendering technique in Unity

Fig. 10. Comparisons between our method with oﬄine rendering Arnold

synthesized result and an image crop to MTurk workers side by side for 1 second,
and asked them to pick an image that contained the real hair. We tested this with
edge activation ﬁelds coming from either CG model or a reference image (in both
cases we used a latent space from another image). If the synthetic images are not
distinguishable from real images, the expected fraction of MTurk workers being
“fooled” and think they are real is 50%. This is the same evaluation method as
Zhu et al.’s work [53], and we showed 120 sets of randomly generated subjects
to over over 100 testers (a subset of 10 subjects is provided to each person). We
present qualitative results in Fig. 11 and the rate of selecting the synthetic images
in Table 1. Note that in all cases, the users had a hard time distinguishing our
results from real hair, and that our system successfully rendered hair produced
from a CG model or a real image. We further test how human judgments change
if people spend more time (3 seconds) evaluating our rendering of a CG model.
We found that crowd workers become more accurate at identifying generated
results, but still mislabel a signiﬁcant number of examples as being real.

Real-Time Hair Rendering using Sequential Adversarial Networks

13

(a)

(b)

(c)

Fig. 11. User study example. From left to right, we show (a) real image, which is used
to infer latent space z; (b) rendering result generated with z and the activation map
of another real image; (c) rendering result of z and a CG model

Table 1. MTurk workers were provided two images in a random order and were asked
to select one that looked more real to them. Either CG models or images were used to
provide activation ﬁelds. Note that in all cases MTurk users had a hard time distin-
guishing our results from real hair, even if we extend the time that images are visible
to 3 seconds

Hair Source AMT Fooling Rate [%]

Image
CG Model
CG Model(3 sec)

49.9%
48.5%
45.6%

Performance. We measure the performance of our feed forward networks on an
real-time hair rendering application. We use a desktop with 2 Titan Xp each
with 12GB of GPU memory. All online processing steps including F1, F2, F3 and
all generator are running per frame. The average amount of time used in F2
is 9ms for looping over all possible rotating angles, and the computation time
for F1 and F3 are negligible. The three networks have identical architecture and
thus runs in consistent speed, each taking around 15ms. For a single GPU, our
demo runs at around 15fps with GPU memory consumption 2.7GB. Running the
demo on multiple GPUs allows real-time performance with fps varying between
24 to 27fps. Please refer to the video included in the supplemental material.

6 Discussion

We presented the ﬁrst deep learning approach for rendering photorealistic hair,
which performs in real-time. We have shown that our sequential GAN architec-
ture and semi-supervised training approach can eﬀectively disentangle strand-
level structures, appearance, and illumination properties from the highly com-
plex and diverse range of hairstyles. In particular, our evaluations show that
without our sequential architecture, the lighting parameter would dominate over

14

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

color, and color speciﬁcation would no longer be eﬀective. Moreover, our trained
latent space is smooth, which allows us to interpolate continuously between arbi-
trary color and lighting samples. Our evaluations also suggests that there are no
signiﬁcant diﬀerences between a vanilla conditional GAN and a state-of-the-art
network such as bicycleGAN, which uses additional smoothness constraints in
the training. Our experiments further indicate that a direct conversion from a
CG rendering to a photoreal image using existing adversarial networks would
lead to signiﬁcant artifacts or unwanted hairstyles. Our intermediate conversion
step from edge activation to orientation map has proven to be an eﬀective way
for semi-supervised training and transitioning from synthetic input to photoreal
output while ensuring that the intended hairstyle structure is preserved.

Limitations and Future Work. As shown in the video demo, the hair render-
ing is not entirely temporally coherent when rotating the view. While the per
frame predictions are reasonable and most strand structure are consistent be-
tween frames, there are still visible ﬂickering artifacts. We believe that temporal
consistency could be trained with augmentations with 3D rotations, or video
training data.

We believe that our sequential GAN architecture for parameter separation
and our intermediate representation for CG-to-photoreal conversion could be
generalized for the rendering of other objects and scenes beyond hair. Our
method presents an interesting alternative and complementary solution for many
applications, such as hair modeling with interactive visual feedback, photo ma-
nipulation, and image-based 3D avatar rendering.

While we do not provide the same level of ﬁne-grained control as conventional
graphics pipelines, our eﬃcient approach is signiﬁcantly simpler and generates
more realistic output without any tedious ﬁne tuning. Nevertheless, we would
like to explore the ability to specify precise lighting conﬁgurations and advanced
shading parameters for a seamless integration of our hair rendering into vir-
tual environments and game engines. We believe that additional training with
controlled simulations and captured hair data would be necessary.

Like other GAN techniques, our results are also not fully indistinguishable
from real ones for a trained eye and an extended period of observation, but we are
conﬁdent that our proposed approach would beneﬁt from future advancements
in GANs.

Acknowledgments. This work was supported in part by the ONR YIP grant
N00014-17-S-FO14, the CONIX Research Center, one of six centers in JUMP,
a Semiconductor Research Corporation (SRC) program sponsored by DARPA,
the Andrew and Erna Viterbi Early Career Chair, the U.S. Army Research
Laboratory (ARL) under contract number W911NF-14-D-0005, and Adobe. The
content of the information does not necessarily reﬂect the position or the policy
of the Government, and no oﬃcial endorsement should be inferred. We thank
Radom´ır Mˇech for insightful discussions.

Real-Time Hair Rendering using Sequential Adversarial Networks

15

References

1. Azadi, S., Fisher, M., Kim, V.G., Wang, Z., Shechtman, E., Darrell, T.: Multi-

content gan for few-shot font style transfer. CVPR (2018)

2. Cao, C., Wu, H., Weng, Y., Shao, T., Zhou, K.: Real-time

facial
avatars. ACM Trans. Graph.
https://doi.org/10.1145/2897824.2925873,

animation with
35(4),
http://doi.acm.org/10.1145/2897824.2925873

image-based
(Jul

126:1–126:12

dynamic

2016).

3. Chai, M., Luo, L., Sunkavalli, K., Carr, N., Hadap, S., Zhou, K.: High-quality
hair modeling from a single portrait photo. ACM Transactions on Graphics (Proc.
SIGGRAPH Asia) 34(6) (Nov 2015)

4. Chai, M., Shao, T., Wu, H., Weng, Y., Zhou, K.: Autohair: Fully automatic hair
modeling from a single image. ACM Transactions on Graphics (TOG) 35(4), 116
(2016)

5. Chai, M., Wang,
hair manipulation
32(4),
http://doi.acm.org/10.1145/2461912.2461990

Zhou, K.: Dynamic
Jin, X.,
L., Weng, Y.,
and
images
in
videos. ACM Trans. Graph.
https://doi.org/10.1145/2461912.2461990,
2013).
(Jul

75:1–75:8

6. Chai, M., Wang, L., Weng, Y., Yu, Y., Guo, B., Zhou, K.: Single-view hair modeling
for portrait manipulation. ACM Transactions on Graphics (TOG) 31(4), 116
(2012)

7. Chang, H., Lu, J., Yu, F., Finkelstein, A.: Makeupgan: Makeup transfer via cycle-

consistent adversarial networks. CVPR (2018)

8. d’Eon, E., Francois, G., Hill, M., Letteri, J., Aubry, J.M.: An energy-conserving
hair reﬂectance model. In: Proceedings of the Twenty-second Eurographics Con-
ference on Rendering. pp. 1181–1187. EGSR ’11, Eurographics Association,
Aire-la-Ville, Switzerland, Switzerland (2011). https://doi.org/10.1111/j.1467-
8659.2011.01976.x, http://dx.doi.org/10.1111/j.1467-8659.2011.01976.x

9. d’Eon, E., Marschner, S., Hanika, J.: Importance sampling for physically-based
hair ﬁber models. In: SIGGRAPH Asia 2013 Technical Briefs. pp. 25:1–25:4. SA
’13, ACM, New York, NY, USA (2013). https://doi.org/10.1145/2542355.2542386,
http://doi.acm.org/10.1145/2542355.2542386

10. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. CoRR

abs/1605.09782 (2016), http://arxiv.org/abs/1605.09782

11. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,

Courville, A.C.: Adversarially learned inference. CoRR abs/1606.00704 (2016)

12. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Proceedings of
the 27th International Conference on Neural Information Processing Systems -
Volume 2. pp. 2672–2680. NIPS’14, MIT Press, Cambridge, MA, USA (2014),
http://dl.acm.org/citation.cfm?id=2969033.2969125

13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

CoRR abs/1512.03385 (2015), http://arxiv.org/abs/1512.03385
14. Hu, L.: http://www-scf.usc.edu/ liwenhu/shm/database.html (2015)
15. Hu, L., Ma, C., Luo, L., Li, H.: Robust hair capture using simulated examples.

ACM Transactions on Graphics (Proc. SIGGRAPH) 33(4) (Jul 2014)

16. Hu, L., Ma, C., Luo, L., Li, H.: Single-view hair modeling using a hairstyle
database. ACM Transactions on Graphics (Proc. SIGGRAPH) 34(4) (Aug 2015)
17. Hu, L., Ma, C., Luo, L., Wei, L.Y., Li, H.: Capturing braided hairstyles. ACM

Trans. Graph. 33(6), 225:1–225:9 (2014)

16

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

Saito,

S., Wei, L., Nagano, K.,

18. Hu, L.,
Sadeghi,
a
single
195:1–195:14
http://doi.acm.org/10.1145/3130800.31310887

J.,
I., Sun, C., Chen, Y.C., Li, H.: Avatar digitization from
rendering. ACM Trans. Graph. 36(6),
image
https://doi.org/10.1145/3130800.31310887,

real-time
2017).

J., Fursund,

for
(Nov

Seo,

19. Huynh, L., Chen, W., Saito, S., Xing, J., Nagano, K., Jones, A., Debevec, P., Li, H.:
Photorealistic facial texture inference using deep neural networks. In: Computer
Vision and Pattern Recognition (CVPR). pp. –. IEEE (2018)

20. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-

tional adversarial networks. CVPR (2016)

21. Isola, P., Zhu, J., Zhou, T., Efros, A.A.:

translation
adversarial networks. CoRR abs/1611.07004 (2016),

Image-to-image

with conditional
http://arxiv.org/abs/1611.07004

fur with three dimensional

22. Kajiya,

J.T., Kay, T.L.: Rendering
SIGGRAPH Comput. Graph.

23(3),

271–280

tures.
https://doi.org/10.1145/74334.74361, http://doi.acm.org/10.1145/74334.74361
23. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for
improved quality, stability, and variation. In: International Conference on Learning
Representations (2018), https://openreview.net/forum?id=Hk99zCeAb
U.:
ACM

hair
620–
https://doi.org/10.1145/566654.566627,

multiresolution
21(3),

modeling
629
http://doi.acm.org/10.1145/566654.566627

Interactive
Trans.

Neumann,
editing.

and
(Jul

24. Kim,

Graph.

2002).

T.Y.,

(Jul

tex-
1989).

25. Lee, D.W., Ko, H.S.: Natural hairstyle modeling and animation. Graph.
Models 63(2), 67–85 (Mar 2001). https://doi.org/10.1006/gmod.2001.0547,
http://dx.doi.org/10.1006/gmod.2001.0547

26. Lin, C., Lucey, S., Yumer, E., Wang, O., Shechtman, E.: St-gan: Spatial transformer
generative adversarial networks for image compositing. In: Computer Vision and
Pattern Recognition, 2018. CVPR 2018. IEEE Conference on. pp. –. - (2018)
27. Liu, G., Ceylan, D., Yumer, E., Yang, J., Lien, J.M.: Material editing using a

physically based rendering network. In: ICCV (2017)

28. Luo, L., Li, H., Paris, S., Weise, T., Pauly, M., Rusinkiewicz, S.: Multi-view hair
capture using orientation ﬁelds. In: Computer Vision and Pattern Recognition
(CVPR) (Jun 2012)

29. Luo, L., Li, H., Rusinkiewicz, S.: Structure-aware hair capture. ACM Transactions

on Graphics (Proc. SIGGRAPH) 32(4) (Jul 2013)

30. Luo, L., Zhang, C., Zhang, Z., Rusinkiewicz, S.: Wide-baseline hair capture using
strand-based reﬁnement. In: Computer Vision and Pattern Recognition (CVPR)
(Jun 2013)

31. Marschner, S.R.,
han, P.: Light
22(3),
http://doi.acm.org/10.1145/882262.882345

Jensen, H.W., Cammarano, M., Worley, S., Hanra-
scattering from human hair ﬁbers. ACM Trans. Graph.
https://doi.org/10.1145/882262.882345,

780–791

2003).

(Jul

32. Nalbach, O., Arabadzhiyska, E., Mehta, D., Seidel, H.P., Ritschel, T.: Deep shad-
ing: convolutional neural networks for screen space shading. In: Computer graphics
forum. vol. 36, pp. 65–78. Wiley Online Library (2017)

33. Olszewski, K., Li, Z., Yang, C., Zhou, Y., Yu, R., Huang, Z., Xiang, S., Saito, S.,
Kohli, P., Li, H.: Realistic dynamic facial textures from a single image using gans.
ICCV (2017)

Real-Time Hair Rendering using Sequential Adversarial Networks

17

34. Paris, S., Brice˜no, H.M., Sillion, F.X.: Capture of hair geometry from multiple
images. In: ACM Transactions on Graphics (TOG). vol. 23, pp. 712–719. ACM
(2004)

35. Paris, S., Chang, W., Kozhushnyan, O.I., Jarosz, W., Matusik, W., Zwicker,
M., Durand, F.: Hair photobooth: geometric and photometric acquisition of real
hairstyles. In: ACM Transactions on Graphics (TOG). vol. 27, p. 30. ACM (2008)
36. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. ICLR (2016)

37. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks
(2015),

segmentation. CoRR abs/1505.04597

biomedical

image

for
http://arxiv.org/abs/1505.04597

38. Sangkloy, P., Lu, J., Fang, C., Yu, F., Hays, J.: Scribbler: Controlling deep image
synthesis with sketch and color. Computer Vision and Pattern Recognition, CVPR
(2017)

39. Shu, Z., Yumer, E., Hadap, S., Sunkavalli, K., Shechtman, E., Samaras, D.: Neu-
ral face editing with intrinsic image disentangling. In: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (July 2017)

shadowing and transparency depth
40. Sintorn, E., Assarsson, U.: Hair
the 2009 Sympo-
ordering using occupancy maps.
I3D ’09,
sium on Interactive
ACM, New York, NY, USA (2009). https://doi.org/10.1145/1507149.1507160,
http://doi.acm.org/10.1145/1507149.1507160

self
In: Proceedings of

and Games. pp.

3D Graphics

67–74.

41. Xian, W., Sangkloy, P., Agrawal, V., Raj, A., Lu, J., Fang, C., Yu, F., Hays, J.:
Texturegan: Controlling deep image synthesis with texture patches. CVPR (2018)
42. Yan, L.Q., Jensen, H.W., Ramamoorthi, R.: An eﬃcient and practical near and
far ﬁeld fur reﬂectance model. ACM Transactions on Graphics (Proceedings of
SIGGRAPH 2017) 36(4) (2017)

43. Yan, L.Q., Sun, W., Jensen, H.W., Ramamoorthi, R.: A bssrdf model for eﬃcient
rendering of fur with global illumination. ACM Transactions on Graphics (Pro-
ceedings of SIGGRAPH Asia 2017) (2017)

44. Yan, L.Q., Tseng, C.W., Jensen, H.W., Ramamoorthi, R.: Physically-accurate fur
reﬂectance: Modeling, measurement and rendering. ACM Transactions on Graphics
(Proceedings of SIGGRAPH Asia 2015) 34(6) (2015)

45. Yu, X., Yang, J.C., Hensley, J., Harada, T., Yu, J.: A framework for ren-
dering complex scattering eﬀects on hair. In: Proceedings of the ACM SIG-
GRAPH Symposium on Interactive 3D Graphics and Games. pp. 111–118. I3D
’12, ACM, New York, NY, USA (2012). https://doi.org/10.1145/2159616.2159635,
http://doi.acm.org/10.1145/2159616.2159635

46. Yuksel, C.,
on
actions
28(5),
166:1–166:7
http://doi.acm.org/10.1145/1661412.1618512

Schaefer,
Graphics

S., Keyser,

(Proceedings

(2009).

J.: Hair meshes. ACM Trans-
2009)
https://doi.org/10.1145/1661412.1618512,

SIGGRAPH

Asia

of

47. Zhang, M., Chai, M., Wu, H., Yang, H., Zhou, K.: A data-driven approach to four-
view image-based hair modeling. ACM Transactions on Graphics (TOG) 36(4),
156 (2017)

48. Zhang, R., Isola, P., Efros, A.A.: Split-brain autoencoders: Unsupervised learning

by cross-channel prediction. In: CVPR (2017)

49. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp.
6230–6239 (July 2017). https://doi.org/10.1109/CVPR.2017.660

18

L. Wei, L. Hu, V. Kim, E. Yumer, and H. Li

50. Zhou, T., Tulsiani, S., Sun, W., Malik, J., Efros, A.A.: View synthesis by appear-

ance ﬂow. In: European Conference on Computer Vision (2016)

51. Zhu, J.Y., Kr¨ahenb¨uhl, P., Shechtman, E., Efros, A.A.: Generative visual manipu-
lation on the natural image manifold. In: Proceedings of European Conference on
Computer Vision (ECCV) (2016)

52. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation

using cycle-consistent adversarial networks. ICCV (2017)

53. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,
E.: Toward multimodal image-to-image translation. In: Advances in Neural Infor-
mation Processing Systems 30 (2017)

54. Zinke, A., Yuksel,

C., Weber, A., Keyser,

approximation

ing
Transactions
27(3),
http://doi.acm.org/10.1145/1360612.1360631

for
Graphics

fast multiple

32:1–32:10

(2008).

on

J.: Dual

scatter-
ACM
2008)
https://doi.org/10.1145/1360612.1360631,

hair.
SIGGRAPH

scattering
of

in

(Proceedings

