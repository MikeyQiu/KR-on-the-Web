Robust Subspace Clustering via Tighter Rank
Approximation

Zhao Kang
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
zhao.kang@siu.edu

Chong Peng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
pchong@siu.edu

Qiang Cheng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
qcheng@cs.siu.edu

5
1
0
2
 
t
c
O
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
1
7
9
8
0
.
0
1
5
1
:
v
i
X
r
a

ABSTRACT
Matrix rank minimization problem is in general NP-hard.
The nuclear norm is used to substitute the rank function
in many recent studies. Nevertheless, the nuclear norm ap-
proximation adds all singular values together and the ap-
proximation error may depend heavily on the magnitudes of
singular values. This might restrict its capability in dealing
In this paper, an arctan-
with many practical problems.
gent function is used as a tighter approximation to the rank
function. We use it on the challenging subspace clustering
problem. For this nonconvex minimization problem, we de-
velop an eﬀective optimization procedure based on a type
of augmented Lagrange multipliers (ALM) method. Exten-
sive experiments on face clustering and motion segmentation
show that the proposed method is eﬀective for rank approx-
imation.

Categories and Subject Descriptors
I.5 [Pattern recognition]: Clustering—Algorithm; G.1.6
[Optimization]: Constrained optimization

Keywords
Subspace Clustering; Rank Minimization; Nuclear Norm;
Nonconvex Optimization

INTRODUCTION

1.
Matrix rank minimization arises in control, machine learn-
ing, signal processing and other areas [43].
It is diﬃcult
to solve due to the discontinuity and nonconvexity of the
rank function. Existing algorithms are largely based on
the nuclear norm heuristic, i.e., to replace the rank by the
nuclear norm [11]. The nuclear norm of a matrix X, de-
noted by (cid:107)X(cid:107)∗, is the sum of all its singular values, i.e.,
(cid:107)X(cid:107)∗ = (cid:80)
i σi(X). Under some conditions, the solution to
the nuclear norm heuristic coincides with the minimum rank
solution [31, 32]. However, since the nuclear norm is the con-
vex envelop of rank(X) over the unit ball {X : (cid:107)X(cid:107)2 ≤ 1}, it
may deviate from the rank of X in many circumstances [4, 3].

The rank function counts the number of nonvanishing singu-
lar values, while the nuclear norm sums their amplitudes. As
a result, the nuclear norm may be dominated by a few very
large singular values. Variations of standard nuclear norm
are shown to be promising in some recent research [14, 2,
29]. A number of nonconvex surrogate functions have come
up to better approximate the rank function, such as Loga-
rithm Determinant [11, 16], Schatten-p norm [26], truncated
nuclear norm [14] and others [24]. In general, they are to
solve the following low-rank minimization problem:

min
Z

min(m,n)
(cid:88)

i=1

h(σi(Z)) + λg(Z),

(1)

where σi(Z) denotes the i-th singular value of Z ∈ Rm×n,
h(·) is a potentially nonconvex, nonsmooth function, and
g(·) is a loss function. By choosing h(z) = z, the summation
of the ﬁrst term in (1) goes back to the nuclear norm (cid:107)Z(cid:107)∗,
problem (1) becomes the well known convex relaxation of
the rank minimization problem:

min
Z

(cid:107)Z(cid:107)∗ + λg(Z).

(2)

In this paper, we will propose a new nonconvex rank ap-
proximation and consider subspace clustering as a speciﬁc
application.

1.1 Previous Work on Subspace Clustering
In many real-world applications, high-dimensional data re-
side in a union of multiple low-dimensional subspaces rather
than one single low-dimensional subspace [7]. Subspace clus-
tering deals with exactly this structure by clustering data
points according to their underlying subspaces. It has nu-
merous applications in computer vision [30] and image pro-
cessing [25]. Therefore subspace clustering has drawn sig-
niﬁcant attention in recent years [36]. In practice, the un-
derlying subspace structure is often corrupted by noise and
outliers, and thus the data may deviate from the original
subspaces. It is necessary to develop robust estimation tech-
niques.

A number of approaches to subspace clustering have been
proposed in the past two decades. According to the sur-
vey in [36], they can be roughly divided into four cate-
gories: 1) algebraic methods; 2) iterative methods; 3) sta-
tistical methods; and 4) spectral clustering-based methods.
Among them, spectral clustering-based methods have ob-
tained state-of-the-art results, including sparse subspace clus-
tering (SSC) [9], and low rank representation (LRR) [21].

They perform subspace clustering in two steps: ﬁrst, learn-
ing an aﬃnity matrix that encodes the subspace member-
ship information, and then applying spectral clustering al-
gorithms [33, 28] to the learned aﬃnity matrix to obtain
the ﬁnal clustering results. Their main diﬀerence is how to
obtain a good aﬃnity matrix.

SSC assumes that each data point can be represented as
a sparse linear combination of other points. The popular
l1-norm heuristic is used to capture the sparsity. It enjoys
great performance for face clustering and motion segmenta-
tion data. Now we have a good theoretical understanding
about SSC. For instance, [8] shows that disjoint subspaces
can be exactly recovered under certain conditions; geometric
analysis of SSC [34] signiﬁcantly broadens the scope of SSC
to intersecting subspaces. However, the data points are as-
sumed to be lying exactly in the subspace. This assumption
may be violated in the presence of corrupted data. [38] ex-
tends SSC by adding adversarial or random noise. However,
SSC’s solution might be too sparse, thus the aﬃnity graph
from a single subspace will not be a fully connected body
[27]. To address the above issue, another regularization term
is introduced to promote connectivity of the graph [9].

LRR also represents each data point as a linear combination
of other points. It is to ﬁnd the lowest rank representation
Z of all data points jointly, where the nuclear norm is used
as a common surrogate of the rank function. In the presence
of noise or outliers, LRR solves the following problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ (cid:107)E(cid:107)l

s.t. X = XZ + E,

(3)

where λ balances the eﬀects of the low rank representa-
tion and errors, X = [x1, · · · , xn] ∈ Rm×n is a set of m-
dimensional data vectors drawn from the union of k sub-
spaces {Si}k
i=1, and (cid:107)·(cid:107)l characterizes certain corruptions
E. For example, when E represents Gaussian noise, squared
n
(cid:80)
Frobenius norm (cid:107)E(cid:107)2
j=1

ij is used; when E denotes

m
(cid:80)
i=1

F =

E2

random corruptions, l1 norm (cid:107)E(cid:107)1 =

|Eij| is appro-

priate; when E indicates the sample-speciﬁc corruptions, l2,1

norm is adopted, where (cid:107)E(cid:107)2,1 =

E2

ij. The low

rank as well as sparsity requirement may help counteract
corruptions. A variant of LRR works even in the presence
of some arbitrarily large outliers [23]. However, LRR has
never been shown to succeed other than under strong “inde-
pendent subspace” condition [15].

m
(cid:80)
i=1

n
(cid:80)
j=1

(cid:115) m
(cid:80)
i=1

n
(cid:80)
j=1

In view of the issues with the nuclear norm mentioned in
the beginning, we propose the use of an arctangent func-
tion instead in this work. We demonstrate the enhanced
performance of the proposed algorithm on benchmark data
sets.

• More accurate rank approximation is proposed to ob-
tain the low-rank representation of high-dimensional
data.

• An eﬃcient optimization procedure is developed for
arctangent rank minimization (ARM) problem. Theo-
retical analysis shows that our algorithm converges to
a stationary point.

• The superiority of the proposed method to various
state-of-the-art subspace clustering algorithms is veri-
ﬁed with signiﬁcantly and consistently lower error rates
of ARM on popular datasets.

Figure 1: Comparison of approximation for rank 2.

2. SUBSPACE CLUSTERING BY ARM
In this work, we demonstrate the application of F (Z) =
f ◦ σ(Z), where f (x) = (cid:80)
i arctan(|xi|) for any x ∈ Rn
(for high dimensional data usually m >> n which is the
case we suppose in the paper), as a rank approximation of
matrix Z in subspace clustering setting. There are three
advantages of this approximation function. First, it approx-
imates rank(Z) much better than the nuclear norm does,
i.e., as σi ∈ [0, ∞], 2
π arctan(σi) ∈ [0, 1]. Figure 1 shows the
rank approximation value of the two approaches for rank 2
situation. We can clearly see that arctangent reﬂects the
real rank pretty well on a broad range of singular values.
Second, f is diﬀerentiable, concave and monotonically in-
creasing on [0, ∞]n, by deﬁning the gradient of f at 0 as
∂
= 1. Third, F is unitarily in-
∂xi
variant and f is absolutely symmetric, i.e., f (x) is invariant
under arbitrary permutation and sign changes of the compo-
nents of x. Based on these properties, we have the following
theorems, which is proved in Appendix A.

f (0) = limxi→0+

1
1+x2
i

Theorem 2.1. For µ > 0 and A ∈ Rm×n, the following

problem

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(4)

(5)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σA)V T .

1.2 Our Contributions
In summary, the main contributions of this paper are three-
fold:

2.1 Arctangent Rank Minimization
To demonstrate the eﬀectiveness of the arctangent rank ap-
proximation, we consider its application in the challenging

subspace clustering problem. We propose the following arc-
tangent rank minimization (ARM) problem:

min
Z,E

n
(cid:88)

i=1

min
Z,E,J

n
(cid:88)

i=1

arctan(σi(Z)) + λ (cid:107)E(cid:107)l s.t. X = XZ + E.

(6)

It is diﬃcult to solve (6) directly because the objective func-
tion is neither convex nor concave. We convert it to the
following equivalent problem:

arctan(σi(J)) + λ (cid:107)E(cid:107)l s.t.X = XZ + E, Z = J.

(7)
Now we resort to a type of augmented Lagrange multipliers
(ALM) [20] method to solve (7). For simplicity of notation,
we denote σi = σi(J) and σt
i = σi(J t). The corresponding
augmented Lagrangian function is:

Algorithm 1 Arctan Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0,
and ρ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Update Z by (10).
2: Solve (11).
3: Solve E by either (15), (16) or (17) according to l.
4: Update Y1 by (18) and Y2 by (19).
5: Update µ by µt+1 = ρµt.
UNTIL stopping criterion is met.

where ωk = ∂f (σk) is the gradient of f (·) at σk and the
SVD of Z t+1 − Y t
is U diag{σA}V T . Finally, it converges
2
µt
to a local optimal point σ∗. Then J t+1 = U diag{σ∗}V T .

n
(cid:88)

i=1

L(E, J, Y1, Y2, Z, µ) =

arctan(σi) + λ (cid:107)E(cid:107)l

+ T r(Y T
µ
2

1 (X − XZ − E)) + T r(Y T
F + (cid:107)J − Z(cid:107)2

((cid:107)X − XZ − E(cid:107)2

+

F ),

2 (J − Z))

For Et+1, we have the following subproblem:

(8)

Et+1 = arg min

λ (cid:107)E(cid:107)l +

(cid:107)X − XZ t+1 − E(cid:107)2
F

E

µt
2

+ T r[(Y t

1 )T (X − XZ t+1 − E)].

Depending on diﬀerent regularization strategies, we have dif-
ferent closed-form solutions. For squared Forbenius norm,
it is again a quadratic problem,

Et+1 =

1 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1 and l2,1 norm, we use the lemmas from Appendix B.
Let Q = X − XZ t+1 + Y t
µt , we can solve E element-wisely
as below:

1

(9)

(cid:26) Qij − λ

µt sgn(Qij),

Et+1

ij =

0,

if |Qij| < λ
µt ;
otherwise.

In the case of l2,1 norm, we have



(cid:107)Q:,i(cid:107)2

− λ
µt

Q:,i,

[Et+1]:,i =

(cid:107)Q:,i(cid:107)2



0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The update of Lagrange multipliers is:

Y t+1
1 = Y t
2 = Y t
Y t+1

1 + µt(X − XZ t+1 − Et+1),
2 + µt(J t+1 − Z t+1).

(14)

(15)

(16)

(17)

(18)

(19)

where µ > 0 is a penalty parameter and Y1, Y2 are La-
grangian multipliers. The variables E, J, and Z can be
updated alternatively, one at each step, while keeping the
other two ﬁxed. For the (t + 1)th iteration, the iterative
scheme is given as follows.

For Z t+1, by ﬁxing Et, J t, Y t

1 and Y t

2 , we have:

Z t+1 = arg min

T r[(Y t

1 )T (X − XZ − Et)]+

Z
2 )T (J t − Z)]+
T r[(Y t
µt
(cid:13)X − XZ − Et(cid:13)
((cid:13)
(cid:13)
2

2

F

+ (cid:13)

(cid:13)J t − Z(cid:13)
(cid:13)

2

F

).

It is evident that the objective function of (9) is a strongly
convex quadratic function which can be solved directly. By
setting the ﬁrst derivative of it to zero, we have:

Z t+1 = (I +X T X)−1[X T (X −Et)+J t+

X T Y t
1 + Y t
2
µt

], (10)

where I ∈ Rn×n is the identity matrix.

J t+1 = arg min

arctan(σi) +

(cid:107)J − (Z t+1 −

µt
2

1
µt Y t

2 )(cid:107)2
F .

n
(cid:88)

i=1

J

(11)
Then we can convert it to problem (5). The ﬁrst term in
(5) is concave while the second term convex in σ, so we can
apply diﬀerence of convex (DC) [13] (vector) optimization
method. A linear approximation is used at each iteration of
DC programing. At iteration k + 1,

σk+1 = arg min

(cid:104)wk, σ(cid:105) +

(cid:107)σ − σA(cid:107)2
2,

(12)

σ≥0

µt
2

whose closed-form solution is

σk+1 = (σA −

ωk
µt )+,

2.2 Afﬁnity Graph Construction
After obtaining optimal Z ∗, we can build the similarity
graph matrix W . As argued in [9], some postprocessing

(13)

Figure 2: Sample face images in Extended Yale B.

For J t+1, we have:

The procedure is outlined in Algorithm 1.

Algorithm 2 Subspace Clustering by ARM
Input: data matrix X, number of subspaces k.
Do
1: Obtain optimal Z ∗ by solving (7).
2: Compute the skinny SVD Z ∗ = U ΣV T .
3: Calculate (cid:101)U = U (Σ)1/2.
4: Construct the aﬃnity graph matrix W by (20).
5: Perform NCuts on W .

of the coeﬃcient matrix can improve the clustering perfor-
mance. Following the angular information based technique
of [21], we deﬁne (cid:101)U = U Σ1/2, where U and Σ are from the
skinny SV D of Z ∗ = U ΣV T . Inspired by [17], we deﬁne W
as follows:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2α,

mality condition,

(20)

where (cid:101)ui and (cid:101)uj denote the i-th and j-th columns of (cid:101)U , and
α ∈ N ∗ controls the sharpness of the aﬃnity between two
points. Increasing the power α enhances the separation abil-
ity in the presence of noise. However, an excessively large α
would break aﬃnities between points of the same group. In
order to compare with LRR1, we use α = 2 in our experi-
ments, then we have the same postprocessing procedure as
LRR. After obtaining W , we directly utilize a spectral clus-
tering algorithm NCuts [33] to cluster the samples. Algo-
rithm 2 summarizes the complete subspace clustering steps
of the proposed method.

Figure 3: Aﬃnity graph matrix W with ﬁve and
ten subjects.

3. CONVERGENCE ANALYSIS
Since the objective function (6) is nonconvex, it would not
be easy to prove the convergence in theory. In this paper,
we mathematically prove that our optimization algorithm
has at least a convergent subsequence which converges to an
accumulation point, and moreover, any accumulation point
of our algorithm is a stationary point. Although the ﬁnal so-
lution might be a local optimum, our results are superior to
the global optimal solution from convex approaches. Some
previous work also reports similar observations [39, 12, 42].

1As we conﬁrmed with an author of [21], the power 2 of its
equation (12) is a typo, which should be 4.

We will show the proof in the case of (cid:107)E(cid:107)1. Let’s ﬁrst re-
formulate our objective function:

G(J, Z, E) = F (J) + λ(cid:107)E(cid:107)1

s.t. Z = J, X = XZ + E,

L(J, Z, E, Y1, Y2, µ) = G(J, Z, E) + (cid:104)Y1, X − XZ − E(cid:105)

+ (cid:104)Y2, J − Z(cid:105) +

((cid:107)J − Z(cid:107)2

F + (cid:107)X − XZ − E(cid:107)2

F ).

µ
2

(21)

(22)

Lemma 3.1. The sequences of {Y t

1 } and {Y t

2 } are bounded.

Proof. J t+1 satisﬁes the ﬁrst-order necessary local opti-

∂J L (cid:0)J, Z t+1, Et, Y t
1 , Y t
(cid:18)

2 , µt(cid:1) |J t+1

=∂J F (J) |J t+1 + µt

J t+1 − Z t+1 +

(cid:19)

Y t
2
µt

(23)

=∂J F (J) |J t+1 + Y t+1
=0.

2

Let’s deﬁne θi =
cording to (41) in Appendix B,

1

1+(σi)2 if σi (cid:54)= 0; otherwise, it is 1. Ac-

U diag(θ)V T = ∂J F (J) |J t+1 ,

(24)

and 0 <

1
1+(σt+1
i
(23), we conclude that Y t+1

2

is bounded.

)2 ≤ 1, ∂J F (J) |J t+1 is bounded. From

Similarly, for Et+1

1 , Y t

2 , µt(cid:1) |Et+1

0 ∈ ∂EL (cid:0)J t+1, Z t+1, E, Y t
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 − Y t
− µt (cid:0)X − XZ t+1 − Et+1(cid:1)
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 + Y t+1

1

1

.

(25)

Here ∂E denotes the subgradient operator [6]. Because ||E||1
is nonsmooth only at Eij = 0, we deﬁne [∂E(cid:107)E(cid:107)1]ij = 0 if
Eij = 0. Then 0 ≤ (cid:107)∂E(cid:107)E(cid:107)1(cid:107)2
F ≤ mn is bounded. There-
fore, {Y t+1

} is bounded.

1

Lemma 3.2. {J t}, {Et} and {Z t} are bounded if (cid:80) µt+1

(µt)2 <

∞, (cid:80) 1

µt < ∞ and X T X is invertible.

Proof.

2 , µt(cid:1)

1

1 , Y t

((cid:107)J t − Z t(cid:107)2

L (cid:0)J t, Z t, Et, Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt − µt−1
2
+ T r((Y t
+ T r((Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1
2 − Y t−1
2

1 − Y t−1
1

1

, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)X − XZ t − Et(cid:107)2

F )

)(X − XZ t − Et))
)(J t − Z t))
, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

(26)

2 , µt(cid:1)
2 , µt)

L (cid:0)J t+1, Z t+1, Et+1, Y t
≤ L(J t+1, Z t+1, Et, Y t
1 , Y t
≤ L(J t, Z t+1, Et, Y t
≤ L (cid:0)J t, Z t, Et, Y t
1 , Y t
≤ L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1

1 , Y t
1 , Y t
2 , µt)
2 , µt(cid:1)
, Y t−1
2

1

µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

Iterating the inequality (27) gives that

Under the given conditions on {µt}, both terms on the
right-hand side of the above inequality are bounded, thus
L (cid:0)J t+1, Z t+1, Et+1, Y t

1 , Y t

2 , µt(cid:1) is bounded. In addition,
2 , µt(cid:1) +

1 , Y t

L(J t+1, Z t+1, Et+1, Y t
≤ L (cid:0)J 1, Z 1, E1, Y 0
1 , Y 0

2 , µt)
1 , Y t
2 , µ0(cid:1) +

t
(cid:88)

µi + µi−1
2(µi−1)2 ((cid:107)Y i
2 − Y i−1
2

(cid:107)2
F ).

i=1
+ (cid:107)Y i

1 − Y i−1
1

(cid:107)2
F

F )

2 (cid:107)2

1 (cid:107)2

F + (cid:107)Y t

L (cid:0)J t+1, Z t+1, Et+1, Y t
1
2µt ((cid:107)Y t
= F (J t+1) + λ(cid:107)Et+1(cid:107)1+
µt
2
µt
2

Y t
µt (cid:107)2
2
(cid:107)X − Et+1 − XZ t+1 +

(cid:107)J t+1 − Z t+1 +

F +

Y t
µt (cid:107)2
1
F .

(28)

(29)

The left-hand side in the above equation is bounded and
each term on the right-hand side is nonnegative, so each
term is bounded. Therefore, Et+1 is bounded. XZ t+1 is
bounded according to the last term on the right-hand side
of (29), and thus after multiplying a constant matrix X T , we
have X T XZ t+1 is bounded. Under the condition that X T X
is invertible, by multiplying a constant matrix (X T X)−1,
we have that (X T X)−1X T XZ t+1 = Z t+1 is bounded. Fi-
nally, J t+1 is bounded because the second to the last term is
bounded. Therefore, {J t}, {Et} and {Z t} are bounded.

1 , Y t

Theorem 3.1. The sequence {J t, Et, Z t, Y t

2 } gener-
ated by Algorithm 1 has at least one accumulation point, un-
der the conditions that (cid:80) µt+1
µt < ∞ and X T X
is invertible. For any accumulation point {J ∗, E∗, Z ∗, Y ∗
1 , Y ∗
{J ∗, E∗, Z ∗} is a stationary point of optimization problem
(21), under the conditions that µt(J t+1 − J t) → 0, and
µt(Et+1 − Et) → 0.

(µt)2 < ∞, (cid:80) 1

2 },

1 , Y t

Proof. Based on the conditions on the penalty parame-
ter sequence {µt} and X T X, Algorithm 1 generates a bounded
sequence {J t, Et, Z t, Y t
2 } by Lemma (3.1) and (3.2) . By
the Bolzano-Weierstrass theorem, at least one accumulation
point exists, e.g., {J ∗, E∗, Z ∗, Y ∗
2 }. Without loss of gen-
1 , Y t
erality, we assume that {J t, Et, Z t, Y t
2 } itself converges
to {J ∗, E∗, Z ∗, Y ∗
2 }. As shown below, {J ∗, E∗, Z ∗} is
1 , Y ∗
a stationary point of problem (21), under additional condi-
tions that µt(Et+1 − Et) → 0 and µt(J t+1 − J t) → 0.

1 , Y ∗

Since J t −Z t = Y t
Therefore, J ∗ = Z ∗.

2 −Y t−1
2
µt−1

and µt → ∞, we have J t −Z t → 0.

(27)

Similarly, by X − XZ t − Et = Y t
X − XZ ∗.

1 −Y t−1
1
µt−1

, we have E∗ =

For Z t+1, the ﬁrst-order optimality condition is

1 , Y t

2 − X T Y t

2 − X T Y t

2 , µt(cid:1) |Zt+1

1 − µt (cid:0)J t − Z t+1(cid:1)

∇Z L (cid:0)J t, Z, Et, Y t
= −Y t
− µtX T (cid:0)X − Et − XZ t+1(cid:1)
= −Y t
− µtX T (cid:0)X − Et+1 − XZ t+1 − Et + Et+1(cid:1)
2 + µt (cid:0)J t+1 − J t(cid:1) − X T Y t+1
= −Y t+1
− µtX T (cid:0)Et+1 − Et(cid:1)
= 0.

1

1 − µt (cid:0)J t+1 − Z t+1 + J t − J t+1(cid:1)

2 + Y t

1 → 0, i.e., −X T Y ∗

If µt(J t+1 − J t) → 0 and µt(Et+1 − Et) → 0, we have
X T Y t
1 . It is easy to verify
∂EL (J, Z, E, Y1, Y2, µ) |E∗ = 0. Therefore, {J ∗, E∗, Z ∗, Y ∗
satisﬁes the KKT conditions of L(J, E, Z, Y1, Y2) and thus
{J ∗, E∗, Z ∗} is a stationary point of (21).

2 = Y ∗

1 , Y ∗
2 }

4. EXPERIMENTS
This section presents experiments with the proposed algo-
rithm on the Extended Yale B (EYaleB) [18] and Hopkins
155 databases [35]. They are standard tests for robust sub-
space clustering algorithms. As shown in [9], the challenge
in the Hopkins 155 dataset is due to the small principal an-
gles between subspaces. For EYaleB, the challenge lies in the
small principal angles and another factor that data points
from diﬀerent subspaces are close. Our results are compared
with several state-of-the-art subspace clustering algorithms,
including LRR [21], SSC [9], LRSC [10, 37], spectral curva-
ture clustering (SCC) [5], and local subspace aﬃnity (LSA)
[40], in terms of misclassiﬁcation rate2. For fair compari-
son, we follow the experimental setup in [9] and obtain the
results.

As other methods do, we tune our parameters to obtain the
In general, the value of λ depends on prior
best results.
If the noise is
knowledge of the noise level of the data.
heavy, a small λ should be adopted. µ0 and ρ aﬀect the
convergence speed. The larger their values are, the fewer
iterations are required for the algorithm to converge, but
meanwhile we may lose some precision of the ﬁnal objective
In the literature, the value of ρ is often
function value.
chosen between 1 and 1.1. The iteration stops at a relative
normed diﬀerence of 10−5 between two successive iterations,
or a maximum of 150 iterations.

4.1 Face Clustering
Face clustering refers to partitioning a set of face images
from multiple individuals to multiple subspaces according to
the identity of each individual. The face images are heavily
contaminated by sparse gross errors due to varying lighting
conditions, as shown in Figure 2. Therefore, (cid:107)E(cid:107)1 is used to
2The implementation of our algorithm is available at:
https://github.com/sckangz/arctangent.

model the errors in our experiment. The EYaleB database
contains cropped face images of 38 individuals taken under
64 diﬀerent illumination conditions. The 38 subjects were
divided into four groups as follows: subjects 1 to 10, 11 to 20,
21 to 30, and 31 to 38. All choices of n ∈ {2, 3, 5, 8, 10} are
considered for each of the ﬁrst three groups, and all choices of
n ∈ {2, 3, 5, 8} are considered for the last group. As a result,
there are {163, 416, 812, 136, 3} combinations corresponding
to diﬀerent n. Each image is downsampled to 48 × 42 and is
vectorized to a 2016-dimensional vector. λ = 10−5, µ0 = 1.7
and γ = 1.03 are used in this experiment.

addition, the error of LSA is large maybe because LSA is
based on MSE. Since the MSE is quite sensitive to outliers,
LSA will fail to deal with large outliers.

Figure 5: Convergence curve of the objective func-
tion value in (6).

Figure 4: Recovery results of two face images. The
three columns from left to right are the original im-
age (X), the error matrix (E) and the recovered im-
age (XZ), respectively.

Table 1: Clustering error rates (%) on the EYaleB
database.

Algorithm
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

LRR

SSC

LSA LRSC SCC ARM

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

16.62
7.82

38.16
39.06

58.02
56.87

12.24
11.25

58.90
59.38

59.19
58.59

23.72
28.03

66.11
64.65

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

73.02
75.78

1.51
0.78

2.26
1.56

3.06
2.50

3.70
3.32

3.85
2.97

Table 1 provides the best performance of each method. As
shown in the table, our proposed method has the lowest
mean clustering error rates in all ﬁve settings. In particu-
lar, in the most challenging case of 10 subjects, the mean
clustering error rate is as low as 3.85%. The improvement
is signiﬁcant compared with other low rank representation
based subspace clustering, i.e., LRR and LRSC. For exam-
ple, 19% and 11% improvement over LRR can be observed
in the cases of 10 and 8 subjects, respectively. This demon-
strates the importance of accurate rank approximation. In

Figure 6: Average computational time (sec) of the
algorithms on the EYaleB database as a function of
the number of subjects.

Figure 3 shows the obtained aﬃnity graph matrix W for the
ﬁve and ten subjects scenarios. We can see a distinct block-
diagonal structure, which means that each cluster becomes
highly compact and diﬀerent subjects are well separated.

In Figure 4, we present the recovery results of some sam-
ple faces from the 10-subject clustering case. We can see
that the proposed algorithm has the beneﬁt of removing the
corruptions in data.

Figure 5 plots the progress of objective function values of
(6). It is observed that with more iterations, the value of
objective function decreases monotonically. This empirically
veriﬁes the convergence of our optimization method.

rithms in mean error rate. Especially, its all mean error rates
are around 1.5%. This again demonstrates the eﬀectiveness
of using arctangent as a rank approximation.

Figure 8: The inﬂuence of parameter λ of ARM on
clustering error of Hopkins 155 database.

Figure 8 shows the culstering error rate of ARM for diﬀerent
λ over all 155 sequences. When λ is between 1 and 3, the
clustering error rate varies between 1.48% and 2.19%. This
demonstrates that ARM performs well under a pretty wide
range of values of λ. This is another advantage of ARM over
LRR [21].

5. CONCLUSION
In this work, we propose to use arctangent as a concave
rank approximation function.
It has some nice properties
compared with the standard nuclear norm. We apply this
function to the low rank representation-based subspace clus-
tering problem and develop an iterative algorithm for opti-
mizing the associated objective function. Extensive experi-
mental results demonstrate that, compared to many state-
of-the-art algorithms, the proposed algorithm gives the low-
est clustering error rates on many benchmark datasets. This
fully demonstrates the signiﬁcance of accurate rank approx-
imation. Interesting future work includes other applications
of the arctangent rank approximation; for example, matrix
completion. Since LRR can only ensure its validity for in-
dependent subspace segmentation, it is worthwhile to inves-
tigate somewhat dependent yet possibly disjoint subspace
clustering.

Figure 7: Example frames from two video sequences
of the Hopkins 155 database with traced feature
points.

We compare the average computational time of LRR, SSC,
and ARM as a function of the number of subjects in Figure 6.
All the experiments are conducted and timed on the same
machine with an Intel Xeon E3-1240 3.40GHz CPU that
has 4 cores and 8GB memory, running Ubuntu and Matlab
(R2014a). We can observe that the computational time of
SSC is higher than LRR and ARM, while ARM is a little
slower than LRR in most cases.

4.2 Motion Segmentation

Table 2: Segmentation error rates (%) on the Hop-
kins 155 Dataset.

4.23
0.56

1.52
0.00

2.13
0.00

Algorithm LRR SSC LSA LRSC SCC ARM
2 Motions
Mean
Median
3 Motions
Mean
Median
All
Mean
Median

2.89
0.00

8.25
0.24

1.48
0.00

4.40
0.56

4.59
0.60

3.69
0.29

7.69
3.80

4.86
0.89

4.03
1.43

7.02
1.45

1.49
0.84

2.56
0.00

4.10
0.00

2.18
0.00

1.48
0.00

Motion segmentation involves segmenting a video sequence
of multiple moving objects into multiple spatiotemporal re-
gions corresponding to diﬀerent motions. These motion se-
quences can be divided into three main categories: checker-
board, traﬃc, and articulated or non-rigid motion sequences.
The Hopkins 155 dataset includes 155 video sequences of 2
or 3 motions, corresponding to 2 or 3 low-dimensional sub-
spaces of the ambient space. Each sequence represents a
data set and so there are 155 motion segmentation prob-
lems in total. Several example frames are shown in Figure 7.
The trajectories are extracted automatically by a tracker, so
they are slightly corrupted by noise. As in [21, 22], (cid:107)E(cid:107)2,1
is adopted in the model. In this experiment, λ = 2, µ0 = 10
and γ = 1.05.

We use the original 2F-dimensional feature trajectories in
our experiment. We show the clustering error rates of dif-
ferent algorithms in Table 2. ARM outperforms other algo-

6. ACKNOWLEDGMENTS
This work is supported by US National Science Foundation
Grants IIS 1218712. The corresponding author is Qiang
Cheng.

7. REFERENCES
[1] A. Beck and M. Teboulle. A fast iterative

shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences,
2(1):183–202, 2009.

[2] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value
thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2010.
[3] E. J. Cand`es and B. Recht. Exact matrix completion

via convex optimization. Foundations of
Computational mathematics, 9(6):717–772, 2009.

[4] E. J. Cand`es and T. Tao. The power of convex
relaxation: Near-optimal matrix completion.
Information Theory, IEEE Transactions on,
56(5):2053–2080, 2010.

[5] G. Chen and G. Lerman. Spectral curvature clustering

(scc). International Journal of Computer Vision,
81(3):317–330, 2009.

[6] F. H. Clarke. Optimization and nonsmooth analysis,

volume 5. Siam, 1990.

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering.
In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 2790–2797.
IEEE, 2009.

[8] E. Elhamifar and R. Vidal. Clustering disjoint

subspaces via sparse representation. In Acoustics
Speech and Signal Processing (ICASSP), 2010 IEEE
International Conference on, pages 1926–1929. IEEE,
2010.

[9] E. Elhamifar and R. Vidal. Sparse subspace clustering:
Algorithm, theory, and applications. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
35(11):2765–2781, 2013.

[10] P. Favaro, R. Vidal, and A. Ravichandran. A closed
form solution to robust subspace estimation and
clustering. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on,
pages 1801–1807. IEEE, 2011.

[11] M. Fazel. Matrix rank minimization with applications.
PhD thesis, PhD thesis, Stanford University, 2002.

[12] P. Gong, J. Ye, and C.-s. Zhang. Multi-stage

multi-task feature learning. In Advances in Neural
Information Processing Systems, pages 1988–1996,
2012.

[13] R. Horst and N. V. Thoai. Dc programming:

overview. Journal of Optimization Theory and
Applications, 103(1):1–43, 1999.

[14] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and

accurate matrix completion via truncated nuclear
norm regularization. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(9):2117–2130,
2013.

[15] K. Kanatani. Motion segmentation by subspace

separation and model selection. In Computer Vision,
2001. ICCV 2001. Proceedings. Eighth IEEE
International Conference on, volume 2, pages 586–591,
2001.

[16] Z. Kang, C. Peng, J. Cheng, and Q. Cheng. Logdet
rank minimization with application to subspace
clustering. Computational Intelligence and
Neuroscience, 2015, 2015.

[17] F. Lauer and C. Schnorr. Spectral clustering of linear
subspaces for motion segmentation. In Computer
Vision, 2009 IEEE 12th International Conference on,
pages 678–685. IEEE, 2009.

[18] K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear
subspaces for face recognition under variable lighting.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 27(5):684–698, 2005.

[19] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of

singular values. part i: Theory. Set-Valued Analysis,
13(3):213–241, 2005.

[20] Z. Lin, R. Liu, and Z. Su. Linearized alternating

direction method with adaptive penalty for low-rank
representation. In Advances in neural information
processing systems, pages 612–620, 2011.

[21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma.

Robust recovery of subspace structures by low-rank
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(1):171–184,
2013.

[22] G. Liu, Z. Lin, and Y. Yu. Robust subspace
segmentation by low-rank representation. In
Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 663–670, 2010.

[23] G. Liu, H. Xu, and S. Yan. Exact subspace

segmentation and outlier detection by low-rank
representation. In AISTATS, pages 703–711, 2012.
[24] C. Lu, J. Tang, S. Y. Yan, and Z. Lin. Generalized

nonconvex nonsmooth low-rank minimization. In
IEEE International Conference on Computer Vision
and Pattern Recognition. IEEE, 2014.

[25] Y. Ma, H. Derksen, W. Hong, and J. Wright.

Segmentation of multivariate mixed data via lossy
data coding and compression. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
29(9):1546–1562, 2007.

[26] K. Mohan and M. Fazel. Iterative reweighted

algorithms for matrix rank minimization. The Journal
of Machine Learning Research, 13(1):3441–3473, 2012.

[27] B. Nasihatkon and R. Hartley. Graph connectivity in

sparse subspace clustering. In Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference
on, pages 2137–2144. IEEE, 2011.

[28] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral
clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849–856,
2002.

[29] F. Nie, H. Huang, and C. H. Ding. Low-rank matrix

recovery via eﬃcient schatten p-norm minimization. In
AAAI, 2012.

[30] S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion

segmentation in the presence of outlying, incomplete,
or corrupted trajectories. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
32(10):1832–1845, 2010.

[31] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed

minimum-rank solutions of linear matrix equations via

same singular values as Z, we have

F (Z) +

(cid:107)Z − A(cid:107)2
F

µ
2

(cid:107)X − ΣA(cid:107)2
F ,

(cid:0)(cid:107)X(cid:107)2

= F (X) +

(cid:0)(cid:107)ΣX (cid:107)2

= F (ΣX ) +

≥ F (ΣX ) +

µ
2
= F (ΣX ) +

(cid:107)X − ΣA(cid:107)2
F ,
µ
2
µ
2
µ
2
µ
2
µ
= F (ΣZ ) +
2
µ
(cid:107)σ − σA(cid:107)2
2,
2
µ
≥ f (σ∗) +
2

(cid:107)σ∗ − σA(cid:107)2
2.

= F (ΣX ) +

= f (σ) +

(cid:107)ΣZ − ΣA(cid:107)2
F ,

(cid:107)ΣX − ΣA(cid:107)2
F ,

F + (cid:107)ΣA(cid:107)2

F + (cid:107)ΣA(cid:107)2

F − 2 (cid:104)X, ΣA(cid:105)(cid:1) ,
F − 2 (cid:104)ΣX , ΣA(cid:105)(cid:1) ,

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

In the above, (33) holds because the Frobenius norm is uni-
tarily invariant; (34) holds because F (X) is unitarily invari-
ant; (36) is true by von Neumann’s trace inequality; and
(38) holds because of the deﬁnition of X. Therefore, (38)
is a lower bound of (32). Note that the equality in (36) is
attained if X = ΣX . Because ΣZ = ΣX = X = U T ZV , the
SVD of Z is Z = U ΣZ V T . By minimizing (39), we get σ∗.
Therefore, eventually we get Z ∗ = U diag(σ∗)V T , which is
the minimizer of problem (30).

nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

[32] B. Recht, W. Xu, and B. Hassibi. Null space

conditions and thresholds for rank minimization.
Mathematical programming, 127(1):175–202, 2011.

[33] J. Shi and J. Malik. Normalized cuts and image
segmentation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 22(8):888–905,
2000.

[34] M. Soltanolkotabi, E. J. Candes, et al. A geometric

analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
[35] R. Tron and R. Vidal. A benchmark for the

comparison of 3-d motion segmentation algorithms. In
Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE,
2007.

[36] R. Vidal. A tutorial on subspace clustering. IEEE
Signal Processing Magazine, 28(2):52–68, 2010.

[37] R. Vidal and P. Favaro. Low rank subspace clustering
(lrsc). Pattern Recognition Letters, 43:47–61, 2014.

[38] Y.-X. Wang and H. Xu. Noisy sparse subspace

clustering. In Proceedings of The 30th International
Conference on Machine Learning, pages 89–97, 2013.
[39] S. Xiang, X. Tong, and J. Ye. Eﬃcient sparse group
feature selection via nonconvex optimization. In
Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 284–292, 2013.

[40] J. Yan and M. Pollefeys. A general framework for

motion segmentation: Independent, articulated, rigid,
non-rigid, degenerate and non-degenerate. In
Computer Vision–ECCV 2006, pages 94–106.
Springer, 2006.

[41] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast

algorithm for edge-preserving variational multichannel
image restoration. SIAM Journal on Imaging
Sciences, 2(2):569–592, 2009.

[42] Z. Zhang and B. Tu. Nonconvex penalization using
laplace exponents and concave conjugates. In
Advances in Neural Information Processing Systems,
pages 611–619, 2012.

[43] Y.-B. Zhao. An approximation theory of matrix rank

minimization and its application to quadratic
equations. Linear Algebra and its Applications,
437(1):77–93, 2012.

APPENDIX
A. PROOF

problem

Theorem A.1. For µ > 0 and A ∈ Rm×n, the following

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

(30)

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(31)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σ∗

A)V T .

Proof. Let A = U ΣAV T be the skinny SVD of A, then
ΣA = U T AV . Denoting X = U T ZV which has exactly the

B. THEOREM AND LEMMAS

Theorem B.1. [19] Suppose F : Rm×n → R is repre-
sented as F (X) = f ◦ σ(X), and f : Rn → R is absolutely
symmetric and diﬀerentiable, where X ∈ Rm×n with SVD
X = U diag(σ)V T , the gradient of F (X) at X is

∂F (X)
∂X

= U diag(θ)V T ,

(41)

where θ = ∂f (y)

∂y |y=σ(X).

Lemma B.1. [1] For µ > 0, and K ∈ Rs×t, the solution

of the problem

min
L

µ(cid:107)L(cid:107)1 +

(cid:107)L − K(cid:107)2
F ,

1
2

is given by Lµ(K), which is deﬁned component-wisely by

[Lµ(K)]ij = max{|Kij| − µ, 0} · sign(Kij).

Lemma B.2. [41] Let H be a given matrix. If the optimal

solution to

min
W

α (cid:107)W (cid:107)2,1 +

1
2

(cid:107)W − H(cid:107)2
F

is W ∗, then the i-th column of W ∗ is

[W ∗]:,i =






0,

−α

(cid:107)H:,i(cid:107)2
(cid:107)H:,i(cid:107)2

H:,i,

if (cid:107)H:,i(cid:107)2 > α;
otherwise.

Robust Subspace Clustering via Tighter Rank
Approximation

Zhao Kang
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
zhao.kang@siu.edu

Chong Peng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
pchong@siu.edu

Qiang Cheng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
qcheng@cs.siu.edu

5
1
0
2
 
t
c
O
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
1
7
9
8
0
.
0
1
5
1
:
v
i
X
r
a

ABSTRACT
Matrix rank minimization problem is in general NP-hard.
The nuclear norm is used to substitute the rank function
in many recent studies. Nevertheless, the nuclear norm ap-
proximation adds all singular values together and the ap-
proximation error may depend heavily on the magnitudes of
singular values. This might restrict its capability in dealing
In this paper, an arctan-
with many practical problems.
gent function is used as a tighter approximation to the rank
function. We use it on the challenging subspace clustering
problem. For this nonconvex minimization problem, we de-
velop an eﬀective optimization procedure based on a type
of augmented Lagrange multipliers (ALM) method. Exten-
sive experiments on face clustering and motion segmentation
show that the proposed method is eﬀective for rank approx-
imation.

Categories and Subject Descriptors
I.5 [Pattern recognition]: Clustering—Algorithm; G.1.6
[Optimization]: Constrained optimization

Keywords
Subspace Clustering; Rank Minimization; Nuclear Norm;
Nonconvex Optimization

INTRODUCTION

1.
Matrix rank minimization arises in control, machine learn-
ing, signal processing and other areas [43].
It is diﬃcult
to solve due to the discontinuity and nonconvexity of the
rank function. Existing algorithms are largely based on
the nuclear norm heuristic, i.e., to replace the rank by the
nuclear norm [11]. The nuclear norm of a matrix X, de-
noted by (cid:107)X(cid:107)∗, is the sum of all its singular values, i.e.,
(cid:107)X(cid:107)∗ = (cid:80)
i σi(X). Under some conditions, the solution to
the nuclear norm heuristic coincides with the minimum rank
solution [31, 32]. However, since the nuclear norm is the con-
vex envelop of rank(X) over the unit ball {X : (cid:107)X(cid:107)2 ≤ 1}, it
may deviate from the rank of X in many circumstances [4, 3].

The rank function counts the number of nonvanishing singu-
lar values, while the nuclear norm sums their amplitudes. As
a result, the nuclear norm may be dominated by a few very
large singular values. Variations of standard nuclear norm
are shown to be promising in some recent research [14, 2,
29]. A number of nonconvex surrogate functions have come
up to better approximate the rank function, such as Loga-
rithm Determinant [11, 16], Schatten-p norm [26], truncated
nuclear norm [14] and others [24]. In general, they are to
solve the following low-rank minimization problem:

min
Z

min(m,n)
(cid:88)

i=1

h(σi(Z)) + λg(Z),

(1)

where σi(Z) denotes the i-th singular value of Z ∈ Rm×n,
h(·) is a potentially nonconvex, nonsmooth function, and
g(·) is a loss function. By choosing h(z) = z, the summation
of the ﬁrst term in (1) goes back to the nuclear norm (cid:107)Z(cid:107)∗,
problem (1) becomes the well known convex relaxation of
the rank minimization problem:

min
Z

(cid:107)Z(cid:107)∗ + λg(Z).

(2)

In this paper, we will propose a new nonconvex rank ap-
proximation and consider subspace clustering as a speciﬁc
application.

1.1 Previous Work on Subspace Clustering
In many real-world applications, high-dimensional data re-
side in a union of multiple low-dimensional subspaces rather
than one single low-dimensional subspace [7]. Subspace clus-
tering deals with exactly this structure by clustering data
points according to their underlying subspaces. It has nu-
merous applications in computer vision [30] and image pro-
cessing [25]. Therefore subspace clustering has drawn sig-
niﬁcant attention in recent years [36]. In practice, the un-
derlying subspace structure is often corrupted by noise and
outliers, and thus the data may deviate from the original
subspaces. It is necessary to develop robust estimation tech-
niques.

A number of approaches to subspace clustering have been
proposed in the past two decades. According to the sur-
vey in [36], they can be roughly divided into four cate-
gories: 1) algebraic methods; 2) iterative methods; 3) sta-
tistical methods; and 4) spectral clustering-based methods.
Among them, spectral clustering-based methods have ob-
tained state-of-the-art results, including sparse subspace clus-
tering (SSC) [9], and low rank representation (LRR) [21].

They perform subspace clustering in two steps: ﬁrst, learn-
ing an aﬃnity matrix that encodes the subspace member-
ship information, and then applying spectral clustering al-
gorithms [33, 28] to the learned aﬃnity matrix to obtain
the ﬁnal clustering results. Their main diﬀerence is how to
obtain a good aﬃnity matrix.

SSC assumes that each data point can be represented as
a sparse linear combination of other points. The popular
l1-norm heuristic is used to capture the sparsity. It enjoys
great performance for face clustering and motion segmenta-
tion data. Now we have a good theoretical understanding
about SSC. For instance, [8] shows that disjoint subspaces
can be exactly recovered under certain conditions; geometric
analysis of SSC [34] signiﬁcantly broadens the scope of SSC
to intersecting subspaces. However, the data points are as-
sumed to be lying exactly in the subspace. This assumption
may be violated in the presence of corrupted data. [38] ex-
tends SSC by adding adversarial or random noise. However,
SSC’s solution might be too sparse, thus the aﬃnity graph
from a single subspace will not be a fully connected body
[27]. To address the above issue, another regularization term
is introduced to promote connectivity of the graph [9].

LRR also represents each data point as a linear combination
of other points. It is to ﬁnd the lowest rank representation
Z of all data points jointly, where the nuclear norm is used
as a common surrogate of the rank function. In the presence
of noise or outliers, LRR solves the following problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ (cid:107)E(cid:107)l

s.t. X = XZ + E,

(3)

where λ balances the eﬀects of the low rank representa-
tion and errors, X = [x1, · · · , xn] ∈ Rm×n is a set of m-
dimensional data vectors drawn from the union of k sub-
spaces {Si}k
i=1, and (cid:107)·(cid:107)l characterizes certain corruptions
E. For example, when E represents Gaussian noise, squared
n
(cid:80)
Frobenius norm (cid:107)E(cid:107)2
j=1

ij is used; when E denotes

m
(cid:80)
i=1

F =

E2

random corruptions, l1 norm (cid:107)E(cid:107)1 =

|Eij| is appro-

priate; when E indicates the sample-speciﬁc corruptions, l2,1

norm is adopted, where (cid:107)E(cid:107)2,1 =

E2

ij. The low

rank as well as sparsity requirement may help counteract
corruptions. A variant of LRR works even in the presence
of some arbitrarily large outliers [23]. However, LRR has
never been shown to succeed other than under strong “inde-
pendent subspace” condition [15].

m
(cid:80)
i=1

n
(cid:80)
j=1

(cid:115) m
(cid:80)
i=1

n
(cid:80)
j=1

In view of the issues with the nuclear norm mentioned in
the beginning, we propose the use of an arctangent func-
tion instead in this work. We demonstrate the enhanced
performance of the proposed algorithm on benchmark data
sets.

• More accurate rank approximation is proposed to ob-
tain the low-rank representation of high-dimensional
data.

• An eﬃcient optimization procedure is developed for
arctangent rank minimization (ARM) problem. Theo-
retical analysis shows that our algorithm converges to
a stationary point.

• The superiority of the proposed method to various
state-of-the-art subspace clustering algorithms is veri-
ﬁed with signiﬁcantly and consistently lower error rates
of ARM on popular datasets.

Figure 1: Comparison of approximation for rank 2.

2. SUBSPACE CLUSTERING BY ARM
In this work, we demonstrate the application of F (Z) =
f ◦ σ(Z), where f (x) = (cid:80)
i arctan(|xi|) for any x ∈ Rn
(for high dimensional data usually m >> n which is the
case we suppose in the paper), as a rank approximation of
matrix Z in subspace clustering setting. There are three
advantages of this approximation function. First, it approx-
imates rank(Z) much better than the nuclear norm does,
i.e., as σi ∈ [0, ∞], 2
π arctan(σi) ∈ [0, 1]. Figure 1 shows the
rank approximation value of the two approaches for rank 2
situation. We can clearly see that arctangent reﬂects the
real rank pretty well on a broad range of singular values.
Second, f is diﬀerentiable, concave and monotonically in-
creasing on [0, ∞]n, by deﬁning the gradient of f at 0 as
∂
= 1. Third, F is unitarily in-
∂xi
variant and f is absolutely symmetric, i.e., f (x) is invariant
under arbitrary permutation and sign changes of the compo-
nents of x. Based on these properties, we have the following
theorems, which is proved in Appendix A.

f (0) = limxi→0+

1
1+x2
i

Theorem 2.1. For µ > 0 and A ∈ Rm×n, the following

problem

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(4)

(5)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σA)V T .

1.2 Our Contributions
In summary, the main contributions of this paper are three-
fold:

2.1 Arctangent Rank Minimization
To demonstrate the eﬀectiveness of the arctangent rank ap-
proximation, we consider its application in the challenging

subspace clustering problem. We propose the following arc-
tangent rank minimization (ARM) problem:

min
Z,E

n
(cid:88)

i=1

min
Z,E,J

n
(cid:88)

i=1

arctan(σi(Z)) + λ (cid:107)E(cid:107)l s.t. X = XZ + E.

(6)

It is diﬃcult to solve (6) directly because the objective func-
tion is neither convex nor concave. We convert it to the
following equivalent problem:

arctan(σi(J)) + λ (cid:107)E(cid:107)l s.t.X = XZ + E, Z = J.

(7)
Now we resort to a type of augmented Lagrange multipliers
(ALM) [20] method to solve (7). For simplicity of notation,
we denote σi = σi(J) and σt
i = σi(J t). The corresponding
augmented Lagrangian function is:

Algorithm 1 Arctan Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0,
and ρ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Update Z by (10).
2: Solve (11).
3: Solve E by either (15), (16) or (17) according to l.
4: Update Y1 by (18) and Y2 by (19).
5: Update µ by µt+1 = ρµt.
UNTIL stopping criterion is met.

where ωk = ∂f (σk) is the gradient of f (·) at σk and the
SVD of Z t+1 − Y t
is U diag{σA}V T . Finally, it converges
2
µt
to a local optimal point σ∗. Then J t+1 = U diag{σ∗}V T .

n
(cid:88)

i=1

L(E, J, Y1, Y2, Z, µ) =

arctan(σi) + λ (cid:107)E(cid:107)l

+ T r(Y T
µ
2

1 (X − XZ − E)) + T r(Y T
F + (cid:107)J − Z(cid:107)2

((cid:107)X − XZ − E(cid:107)2

+

F ),

2 (J − Z))

For Et+1, we have the following subproblem:

(8)

Et+1 = arg min

λ (cid:107)E(cid:107)l +

(cid:107)X − XZ t+1 − E(cid:107)2
F

E

µt
2

+ T r[(Y t

1 )T (X − XZ t+1 − E)].

Depending on diﬀerent regularization strategies, we have dif-
ferent closed-form solutions. For squared Forbenius norm,
it is again a quadratic problem,

Et+1 =

1 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1 and l2,1 norm, we use the lemmas from Appendix B.
Let Q = X − XZ t+1 + Y t
µt , we can solve E element-wisely
as below:

1

(9)

(cid:26) Qij − λ

µt sgn(Qij),

Et+1

ij =

0,

if |Qij| < λ
µt ;
otherwise.

In the case of l2,1 norm, we have



(cid:107)Q:,i(cid:107)2

− λ
µt

Q:,i,

[Et+1]:,i =

(cid:107)Q:,i(cid:107)2



0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The update of Lagrange multipliers is:

Y t+1
1 = Y t
2 = Y t
Y t+1

1 + µt(X − XZ t+1 − Et+1),
2 + µt(J t+1 − Z t+1).

(14)

(15)

(16)

(17)

(18)

(19)

where µ > 0 is a penalty parameter and Y1, Y2 are La-
grangian multipliers. The variables E, J, and Z can be
updated alternatively, one at each step, while keeping the
other two ﬁxed. For the (t + 1)th iteration, the iterative
scheme is given as follows.

For Z t+1, by ﬁxing Et, J t, Y t

1 and Y t

2 , we have:

Z t+1 = arg min

T r[(Y t

1 )T (X − XZ − Et)]+

Z
2 )T (J t − Z)]+
T r[(Y t
µt
(cid:13)X − XZ − Et(cid:13)
((cid:13)
(cid:13)
2

2

F

+ (cid:13)

(cid:13)J t − Z(cid:13)
(cid:13)

2

F

).

It is evident that the objective function of (9) is a strongly
convex quadratic function which can be solved directly. By
setting the ﬁrst derivative of it to zero, we have:

Z t+1 = (I +X T X)−1[X T (X −Et)+J t+

X T Y t
1 + Y t
2
µt

], (10)

where I ∈ Rn×n is the identity matrix.

J t+1 = arg min

arctan(σi) +

(cid:107)J − (Z t+1 −

µt
2

1
µt Y t

2 )(cid:107)2
F .

n
(cid:88)

i=1

J

(11)
Then we can convert it to problem (5). The ﬁrst term in
(5) is concave while the second term convex in σ, so we can
apply diﬀerence of convex (DC) [13] (vector) optimization
method. A linear approximation is used at each iteration of
DC programing. At iteration k + 1,

σk+1 = arg min

(cid:104)wk, σ(cid:105) +

(cid:107)σ − σA(cid:107)2
2,

(12)

σ≥0

µt
2

whose closed-form solution is

σk+1 = (σA −

ωk
µt )+,

2.2 Afﬁnity Graph Construction
After obtaining optimal Z ∗, we can build the similarity
graph matrix W . As argued in [9], some postprocessing

(13)

Figure 2: Sample face images in Extended Yale B.

For J t+1, we have:

The procedure is outlined in Algorithm 1.

Algorithm 2 Subspace Clustering by ARM
Input: data matrix X, number of subspaces k.
Do
1: Obtain optimal Z ∗ by solving (7).
2: Compute the skinny SVD Z ∗ = U ΣV T .
3: Calculate (cid:101)U = U (Σ)1/2.
4: Construct the aﬃnity graph matrix W by (20).
5: Perform NCuts on W .

of the coeﬃcient matrix can improve the clustering perfor-
mance. Following the angular information based technique
of [21], we deﬁne (cid:101)U = U Σ1/2, where U and Σ are from the
skinny SV D of Z ∗ = U ΣV T . Inspired by [17], we deﬁne W
as follows:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2α,

mality condition,

(20)

where (cid:101)ui and (cid:101)uj denote the i-th and j-th columns of (cid:101)U , and
α ∈ N ∗ controls the sharpness of the aﬃnity between two
points. Increasing the power α enhances the separation abil-
ity in the presence of noise. However, an excessively large α
would break aﬃnities between points of the same group. In
order to compare with LRR1, we use α = 2 in our experi-
ments, then we have the same postprocessing procedure as
LRR. After obtaining W , we directly utilize a spectral clus-
tering algorithm NCuts [33] to cluster the samples. Algo-
rithm 2 summarizes the complete subspace clustering steps
of the proposed method.

Figure 3: Aﬃnity graph matrix W with ﬁve and
ten subjects.

3. CONVERGENCE ANALYSIS
Since the objective function (6) is nonconvex, it would not
be easy to prove the convergence in theory. In this paper,
we mathematically prove that our optimization algorithm
has at least a convergent subsequence which converges to an
accumulation point, and moreover, any accumulation point
of our algorithm is a stationary point. Although the ﬁnal so-
lution might be a local optimum, our results are superior to
the global optimal solution from convex approaches. Some
previous work also reports similar observations [39, 12, 42].

1As we conﬁrmed with an author of [21], the power 2 of its
equation (12) is a typo, which should be 4.

We will show the proof in the case of (cid:107)E(cid:107)1. Let’s ﬁrst re-
formulate our objective function:

G(J, Z, E) = F (J) + λ(cid:107)E(cid:107)1

s.t. Z = J, X = XZ + E,

L(J, Z, E, Y1, Y2, µ) = G(J, Z, E) + (cid:104)Y1, X − XZ − E(cid:105)

+ (cid:104)Y2, J − Z(cid:105) +

((cid:107)J − Z(cid:107)2

F + (cid:107)X − XZ − E(cid:107)2

F ).

µ
2

(21)

(22)

Lemma 3.1. The sequences of {Y t

1 } and {Y t

2 } are bounded.

Proof. J t+1 satisﬁes the ﬁrst-order necessary local opti-

∂J L (cid:0)J, Z t+1, Et, Y t
1 , Y t
(cid:18)

2 , µt(cid:1) |J t+1

=∂J F (J) |J t+1 + µt

J t+1 − Z t+1 +

(cid:19)

Y t
2
µt

(23)

=∂J F (J) |J t+1 + Y t+1
=0.

2

Let’s deﬁne θi =
cording to (41) in Appendix B,

1

1+(σi)2 if σi (cid:54)= 0; otherwise, it is 1. Ac-

U diag(θ)V T = ∂J F (J) |J t+1 ,

(24)

and 0 <

1
1+(σt+1
i
(23), we conclude that Y t+1

2

is bounded.

)2 ≤ 1, ∂J F (J) |J t+1 is bounded. From

Similarly, for Et+1

1 , Y t

2 , µt(cid:1) |Et+1

0 ∈ ∂EL (cid:0)J t+1, Z t+1, E, Y t
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 − Y t
− µt (cid:0)X − XZ t+1 − Et+1(cid:1)
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 + Y t+1

1

1

.

(25)

Here ∂E denotes the subgradient operator [6]. Because ||E||1
is nonsmooth only at Eij = 0, we deﬁne [∂E(cid:107)E(cid:107)1]ij = 0 if
Eij = 0. Then 0 ≤ (cid:107)∂E(cid:107)E(cid:107)1(cid:107)2
F ≤ mn is bounded. There-
fore, {Y t+1

} is bounded.

1

Lemma 3.2. {J t}, {Et} and {Z t} are bounded if (cid:80) µt+1

(µt)2 <

∞, (cid:80) 1

µt < ∞ and X T X is invertible.

Proof.

2 , µt(cid:1)

1

1 , Y t

((cid:107)J t − Z t(cid:107)2

L (cid:0)J t, Z t, Et, Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt − µt−1
2
+ T r((Y t
+ T r((Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1
2 − Y t−1
2

1 − Y t−1
1

1

, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)X − XZ t − Et(cid:107)2

F )

)(X − XZ t − Et))
)(J t − Z t))
, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

(26)

2 , µt(cid:1)
2 , µt)

L (cid:0)J t+1, Z t+1, Et+1, Y t
≤ L(J t+1, Z t+1, Et, Y t
1 , Y t
≤ L(J t, Z t+1, Et, Y t
≤ L (cid:0)J t, Z t, Et, Y t
1 , Y t
≤ L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1

1 , Y t
1 , Y t
2 , µt)
2 , µt(cid:1)
, Y t−1
2

1

µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

Iterating the inequality (27) gives that

Under the given conditions on {µt}, both terms on the
right-hand side of the above inequality are bounded, thus
L (cid:0)J t+1, Z t+1, Et+1, Y t

1 , Y t

2 , µt(cid:1) is bounded. In addition,
2 , µt(cid:1) +

1 , Y t

L(J t+1, Z t+1, Et+1, Y t
≤ L (cid:0)J 1, Z 1, E1, Y 0
1 , Y 0

2 , µt)
1 , Y t
2 , µ0(cid:1) +

t
(cid:88)

µi + µi−1
2(µi−1)2 ((cid:107)Y i
2 − Y i−1
2

(cid:107)2
F ).

i=1
+ (cid:107)Y i

1 − Y i−1
1

(cid:107)2
F

F )

2 (cid:107)2

1 (cid:107)2

F + (cid:107)Y t

L (cid:0)J t+1, Z t+1, Et+1, Y t
1
2µt ((cid:107)Y t
= F (J t+1) + λ(cid:107)Et+1(cid:107)1+
µt
2
µt
2

Y t
µt (cid:107)2
2
(cid:107)X − Et+1 − XZ t+1 +

(cid:107)J t+1 − Z t+1 +

F +

Y t
µt (cid:107)2
1
F .

(28)

(29)

The left-hand side in the above equation is bounded and
each term on the right-hand side is nonnegative, so each
term is bounded. Therefore, Et+1 is bounded. XZ t+1 is
bounded according to the last term on the right-hand side
of (29), and thus after multiplying a constant matrix X T , we
have X T XZ t+1 is bounded. Under the condition that X T X
is invertible, by multiplying a constant matrix (X T X)−1,
we have that (X T X)−1X T XZ t+1 = Z t+1 is bounded. Fi-
nally, J t+1 is bounded because the second to the last term is
bounded. Therefore, {J t}, {Et} and {Z t} are bounded.

1 , Y t

Theorem 3.1. The sequence {J t, Et, Z t, Y t

2 } gener-
ated by Algorithm 1 has at least one accumulation point, un-
der the conditions that (cid:80) µt+1
µt < ∞ and X T X
is invertible. For any accumulation point {J ∗, E∗, Z ∗, Y ∗
1 , Y ∗
{J ∗, E∗, Z ∗} is a stationary point of optimization problem
(21), under the conditions that µt(J t+1 − J t) → 0, and
µt(Et+1 − Et) → 0.

(µt)2 < ∞, (cid:80) 1

2 },

1 , Y t

Proof. Based on the conditions on the penalty parame-
ter sequence {µt} and X T X, Algorithm 1 generates a bounded
sequence {J t, Et, Z t, Y t
2 } by Lemma (3.1) and (3.2) . By
the Bolzano-Weierstrass theorem, at least one accumulation
point exists, e.g., {J ∗, E∗, Z ∗, Y ∗
2 }. Without loss of gen-
1 , Y t
erality, we assume that {J t, Et, Z t, Y t
2 } itself converges
to {J ∗, E∗, Z ∗, Y ∗
2 }. As shown below, {J ∗, E∗, Z ∗} is
1 , Y ∗
a stationary point of problem (21), under additional condi-
tions that µt(Et+1 − Et) → 0 and µt(J t+1 − J t) → 0.

1 , Y ∗

Since J t −Z t = Y t
Therefore, J ∗ = Z ∗.

2 −Y t−1
2
µt−1

and µt → ∞, we have J t −Z t → 0.

(27)

Similarly, by X − XZ t − Et = Y t
X − XZ ∗.

1 −Y t−1
1
µt−1

, we have E∗ =

For Z t+1, the ﬁrst-order optimality condition is

1 , Y t

2 − X T Y t

2 − X T Y t

2 , µt(cid:1) |Zt+1

1 − µt (cid:0)J t − Z t+1(cid:1)

∇Z L (cid:0)J t, Z, Et, Y t
= −Y t
− µtX T (cid:0)X − Et − XZ t+1(cid:1)
= −Y t
− µtX T (cid:0)X − Et+1 − XZ t+1 − Et + Et+1(cid:1)
2 + µt (cid:0)J t+1 − J t(cid:1) − X T Y t+1
= −Y t+1
− µtX T (cid:0)Et+1 − Et(cid:1)
= 0.

1

1 − µt (cid:0)J t+1 − Z t+1 + J t − J t+1(cid:1)

2 + Y t

1 → 0, i.e., −X T Y ∗

If µt(J t+1 − J t) → 0 and µt(Et+1 − Et) → 0, we have
X T Y t
1 . It is easy to verify
∂EL (J, Z, E, Y1, Y2, µ) |E∗ = 0. Therefore, {J ∗, E∗, Z ∗, Y ∗
satisﬁes the KKT conditions of L(J, E, Z, Y1, Y2) and thus
{J ∗, E∗, Z ∗} is a stationary point of (21).

2 = Y ∗

1 , Y ∗
2 }

4. EXPERIMENTS
This section presents experiments with the proposed algo-
rithm on the Extended Yale B (EYaleB) [18] and Hopkins
155 databases [35]. They are standard tests for robust sub-
space clustering algorithms. As shown in [9], the challenge
in the Hopkins 155 dataset is due to the small principal an-
gles between subspaces. For EYaleB, the challenge lies in the
small principal angles and another factor that data points
from diﬀerent subspaces are close. Our results are compared
with several state-of-the-art subspace clustering algorithms,
including LRR [21], SSC [9], LRSC [10, 37], spectral curva-
ture clustering (SCC) [5], and local subspace aﬃnity (LSA)
[40], in terms of misclassiﬁcation rate2. For fair compari-
son, we follow the experimental setup in [9] and obtain the
results.

As other methods do, we tune our parameters to obtain the
In general, the value of λ depends on prior
best results.
If the noise is
knowledge of the noise level of the data.
heavy, a small λ should be adopted. µ0 and ρ aﬀect the
convergence speed. The larger their values are, the fewer
iterations are required for the algorithm to converge, but
meanwhile we may lose some precision of the ﬁnal objective
In the literature, the value of ρ is often
function value.
chosen between 1 and 1.1. The iteration stops at a relative
normed diﬀerence of 10−5 between two successive iterations,
or a maximum of 150 iterations.

4.1 Face Clustering
Face clustering refers to partitioning a set of face images
from multiple individuals to multiple subspaces according to
the identity of each individual. The face images are heavily
contaminated by sparse gross errors due to varying lighting
conditions, as shown in Figure 2. Therefore, (cid:107)E(cid:107)1 is used to
2The implementation of our algorithm is available at:
https://github.com/sckangz/arctangent.

model the errors in our experiment. The EYaleB database
contains cropped face images of 38 individuals taken under
64 diﬀerent illumination conditions. The 38 subjects were
divided into four groups as follows: subjects 1 to 10, 11 to 20,
21 to 30, and 31 to 38. All choices of n ∈ {2, 3, 5, 8, 10} are
considered for each of the ﬁrst three groups, and all choices of
n ∈ {2, 3, 5, 8} are considered for the last group. As a result,
there are {163, 416, 812, 136, 3} combinations corresponding
to diﬀerent n. Each image is downsampled to 48 × 42 and is
vectorized to a 2016-dimensional vector. λ = 10−5, µ0 = 1.7
and γ = 1.03 are used in this experiment.

addition, the error of LSA is large maybe because LSA is
based on MSE. Since the MSE is quite sensitive to outliers,
LSA will fail to deal with large outliers.

Figure 5: Convergence curve of the objective func-
tion value in (6).

Figure 4: Recovery results of two face images. The
three columns from left to right are the original im-
age (X), the error matrix (E) and the recovered im-
age (XZ), respectively.

Table 1: Clustering error rates (%) on the EYaleB
database.

Algorithm
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

LRR

SSC

LSA LRSC SCC ARM

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

16.62
7.82

38.16
39.06

58.02
56.87

12.24
11.25

58.90
59.38

59.19
58.59

23.72
28.03

66.11
64.65

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

73.02
75.78

1.51
0.78

2.26
1.56

3.06
2.50

3.70
3.32

3.85
2.97

Table 1 provides the best performance of each method. As
shown in the table, our proposed method has the lowest
mean clustering error rates in all ﬁve settings. In particu-
lar, in the most challenging case of 10 subjects, the mean
clustering error rate is as low as 3.85%. The improvement
is signiﬁcant compared with other low rank representation
based subspace clustering, i.e., LRR and LRSC. For exam-
ple, 19% and 11% improvement over LRR can be observed
in the cases of 10 and 8 subjects, respectively. This demon-
strates the importance of accurate rank approximation. In

Figure 6: Average computational time (sec) of the
algorithms on the EYaleB database as a function of
the number of subjects.

Figure 3 shows the obtained aﬃnity graph matrix W for the
ﬁve and ten subjects scenarios. We can see a distinct block-
diagonal structure, which means that each cluster becomes
highly compact and diﬀerent subjects are well separated.

In Figure 4, we present the recovery results of some sam-
ple faces from the 10-subject clustering case. We can see
that the proposed algorithm has the beneﬁt of removing the
corruptions in data.

Figure 5 plots the progress of objective function values of
(6). It is observed that with more iterations, the value of
objective function decreases monotonically. This empirically
veriﬁes the convergence of our optimization method.

rithms in mean error rate. Especially, its all mean error rates
are around 1.5%. This again demonstrates the eﬀectiveness
of using arctangent as a rank approximation.

Figure 8: The inﬂuence of parameter λ of ARM on
clustering error of Hopkins 155 database.

Figure 8 shows the culstering error rate of ARM for diﬀerent
λ over all 155 sequences. When λ is between 1 and 3, the
clustering error rate varies between 1.48% and 2.19%. This
demonstrates that ARM performs well under a pretty wide
range of values of λ. This is another advantage of ARM over
LRR [21].

5. CONCLUSION
In this work, we propose to use arctangent as a concave
rank approximation function.
It has some nice properties
compared with the standard nuclear norm. We apply this
function to the low rank representation-based subspace clus-
tering problem and develop an iterative algorithm for opti-
mizing the associated objective function. Extensive experi-
mental results demonstrate that, compared to many state-
of-the-art algorithms, the proposed algorithm gives the low-
est clustering error rates on many benchmark datasets. This
fully demonstrates the signiﬁcance of accurate rank approx-
imation. Interesting future work includes other applications
of the arctangent rank approximation; for example, matrix
completion. Since LRR can only ensure its validity for in-
dependent subspace segmentation, it is worthwhile to inves-
tigate somewhat dependent yet possibly disjoint subspace
clustering.

Figure 7: Example frames from two video sequences
of the Hopkins 155 database with traced feature
points.

We compare the average computational time of LRR, SSC,
and ARM as a function of the number of subjects in Figure 6.
All the experiments are conducted and timed on the same
machine with an Intel Xeon E3-1240 3.40GHz CPU that
has 4 cores and 8GB memory, running Ubuntu and Matlab
(R2014a). We can observe that the computational time of
SSC is higher than LRR and ARM, while ARM is a little
slower than LRR in most cases.

4.2 Motion Segmentation

Table 2: Segmentation error rates (%) on the Hop-
kins 155 Dataset.

4.23
0.56

1.52
0.00

2.13
0.00

Algorithm LRR SSC LSA LRSC SCC ARM
2 Motions
Mean
Median
3 Motions
Mean
Median
All
Mean
Median

8.25
0.24

2.89
0.00

1.48
0.00

4.40
0.56

4.59
0.60

3.69
0.29

7.69
3.80

4.86
0.89

4.03
1.43

7.02
1.45

1.49
0.84

2.56
0.00

4.10
0.00

2.18
0.00

1.48
0.00

Motion segmentation involves segmenting a video sequence
of multiple moving objects into multiple spatiotemporal re-
gions corresponding to diﬀerent motions. These motion se-
quences can be divided into three main categories: checker-
board, traﬃc, and articulated or non-rigid motion sequences.
The Hopkins 155 dataset includes 155 video sequences of 2
or 3 motions, corresponding to 2 or 3 low-dimensional sub-
spaces of the ambient space. Each sequence represents a
data set and so there are 155 motion segmentation prob-
lems in total. Several example frames are shown in Figure 7.
The trajectories are extracted automatically by a tracker, so
they are slightly corrupted by noise. As in [21, 22], (cid:107)E(cid:107)2,1
is adopted in the model. In this experiment, λ = 2, µ0 = 10
and γ = 1.05.

We use the original 2F-dimensional feature trajectories in
our experiment. We show the clustering error rates of dif-
ferent algorithms in Table 2. ARM outperforms other algo-

6. ACKNOWLEDGMENTS
This work is supported by US National Science Foundation
Grants IIS 1218712. The corresponding author is Qiang
Cheng.

7. REFERENCES
[1] A. Beck and M. Teboulle. A fast iterative

shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences,
2(1):183–202, 2009.

[2] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value
thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2010.
[3] E. J. Cand`es and B. Recht. Exact matrix completion

via convex optimization. Foundations of
Computational mathematics, 9(6):717–772, 2009.

[4] E. J. Cand`es and T. Tao. The power of convex
relaxation: Near-optimal matrix completion.
Information Theory, IEEE Transactions on,
56(5):2053–2080, 2010.

[5] G. Chen and G. Lerman. Spectral curvature clustering

(scc). International Journal of Computer Vision,
81(3):317–330, 2009.

[6] F. H. Clarke. Optimization and nonsmooth analysis,

volume 5. Siam, 1990.

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering.
In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 2790–2797.
IEEE, 2009.

[8] E. Elhamifar and R. Vidal. Clustering disjoint

subspaces via sparse representation. In Acoustics
Speech and Signal Processing (ICASSP), 2010 IEEE
International Conference on, pages 1926–1929. IEEE,
2010.

[9] E. Elhamifar and R. Vidal. Sparse subspace clustering:
Algorithm, theory, and applications. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
35(11):2765–2781, 2013.

[10] P. Favaro, R. Vidal, and A. Ravichandran. A closed
form solution to robust subspace estimation and
clustering. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on,
pages 1801–1807. IEEE, 2011.

[11] M. Fazel. Matrix rank minimization with applications.
PhD thesis, PhD thesis, Stanford University, 2002.

[12] P. Gong, J. Ye, and C.-s. Zhang. Multi-stage

multi-task feature learning. In Advances in Neural
Information Processing Systems, pages 1988–1996,
2012.

[13] R. Horst and N. V. Thoai. Dc programming:

overview. Journal of Optimization Theory and
Applications, 103(1):1–43, 1999.

[14] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and

accurate matrix completion via truncated nuclear
norm regularization. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(9):2117–2130,
2013.

[15] K. Kanatani. Motion segmentation by subspace

separation and model selection. In Computer Vision,
2001. ICCV 2001. Proceedings. Eighth IEEE
International Conference on, volume 2, pages 586–591,
2001.

[16] Z. Kang, C. Peng, J. Cheng, and Q. Cheng. Logdet
rank minimization with application to subspace
clustering. Computational Intelligence and
Neuroscience, 2015, 2015.

[17] F. Lauer and C. Schnorr. Spectral clustering of linear
subspaces for motion segmentation. In Computer
Vision, 2009 IEEE 12th International Conference on,
pages 678–685. IEEE, 2009.

[18] K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear
subspaces for face recognition under variable lighting.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 27(5):684–698, 2005.

[19] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of

singular values. part i: Theory. Set-Valued Analysis,
13(3):213–241, 2005.

[20] Z. Lin, R. Liu, and Z. Su. Linearized alternating

direction method with adaptive penalty for low-rank
representation. In Advances in neural information
processing systems, pages 612–620, 2011.

[21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma.

Robust recovery of subspace structures by low-rank
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(1):171–184,
2013.

[22] G. Liu, Z. Lin, and Y. Yu. Robust subspace
segmentation by low-rank representation. In
Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 663–670, 2010.

[23] G. Liu, H. Xu, and S. Yan. Exact subspace

segmentation and outlier detection by low-rank
representation. In AISTATS, pages 703–711, 2012.
[24] C. Lu, J. Tang, S. Y. Yan, and Z. Lin. Generalized

nonconvex nonsmooth low-rank minimization. In
IEEE International Conference on Computer Vision
and Pattern Recognition. IEEE, 2014.

[25] Y. Ma, H. Derksen, W. Hong, and J. Wright.

Segmentation of multivariate mixed data via lossy
data coding and compression. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
29(9):1546–1562, 2007.

[26] K. Mohan and M. Fazel. Iterative reweighted

algorithms for matrix rank minimization. The Journal
of Machine Learning Research, 13(1):3441–3473, 2012.

[27] B. Nasihatkon and R. Hartley. Graph connectivity in

sparse subspace clustering. In Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference
on, pages 2137–2144. IEEE, 2011.

[28] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral
clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849–856,
2002.

[29] F. Nie, H. Huang, and C. H. Ding. Low-rank matrix

recovery via eﬃcient schatten p-norm minimization. In
AAAI, 2012.

[30] S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion

segmentation in the presence of outlying, incomplete,
or corrupted trajectories. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
32(10):1832–1845, 2010.

[31] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed

minimum-rank solutions of linear matrix equations via

same singular values as Z, we have

F (Z) +

(cid:107)Z − A(cid:107)2
F

µ
2

(cid:107)X − ΣA(cid:107)2
F ,

(cid:0)(cid:107)X(cid:107)2

= F (X) +

(cid:0)(cid:107)ΣX (cid:107)2

= F (ΣX ) +

≥ F (ΣX ) +

µ
2
= F (ΣX ) +

(cid:107)X − ΣA(cid:107)2
F ,
µ
2
µ
2
µ
2
µ
2
µ
= F (ΣZ ) +
2
µ
(cid:107)σ − σA(cid:107)2
2,
2
µ
≥ f (σ∗) +
2

(cid:107)σ∗ − σA(cid:107)2
2.

= F (ΣX ) +

= f (σ) +

(cid:107)ΣZ − ΣA(cid:107)2
F ,

(cid:107)ΣX − ΣA(cid:107)2
F ,

F + (cid:107)ΣA(cid:107)2

F + (cid:107)ΣA(cid:107)2

F − 2 (cid:104)X, ΣA(cid:105)(cid:1) ,
F − 2 (cid:104)ΣX , ΣA(cid:105)(cid:1) ,

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

In the above, (33) holds because the Frobenius norm is uni-
tarily invariant; (34) holds because F (X) is unitarily invari-
ant; (36) is true by von Neumann’s trace inequality; and
(38) holds because of the deﬁnition of X. Therefore, (38)
is a lower bound of (32). Note that the equality in (36) is
attained if X = ΣX . Because ΣZ = ΣX = X = U T ZV , the
SVD of Z is Z = U ΣZ V T . By minimizing (39), we get σ∗.
Therefore, eventually we get Z ∗ = U diag(σ∗)V T , which is
the minimizer of problem (30).

nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

[32] B. Recht, W. Xu, and B. Hassibi. Null space

conditions and thresholds for rank minimization.
Mathematical programming, 127(1):175–202, 2011.

[33] J. Shi and J. Malik. Normalized cuts and image
segmentation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 22(8):888–905,
2000.

[34] M. Soltanolkotabi, E. J. Candes, et al. A geometric

analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
[35] R. Tron and R. Vidal. A benchmark for the

comparison of 3-d motion segmentation algorithms. In
Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE,
2007.

[36] R. Vidal. A tutorial on subspace clustering. IEEE
Signal Processing Magazine, 28(2):52–68, 2010.

[37] R. Vidal and P. Favaro. Low rank subspace clustering
(lrsc). Pattern Recognition Letters, 43:47–61, 2014.

[38] Y.-X. Wang and H. Xu. Noisy sparse subspace

clustering. In Proceedings of The 30th International
Conference on Machine Learning, pages 89–97, 2013.
[39] S. Xiang, X. Tong, and J. Ye. Eﬃcient sparse group
feature selection via nonconvex optimization. In
Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 284–292, 2013.

[40] J. Yan and M. Pollefeys. A general framework for

motion segmentation: Independent, articulated, rigid,
non-rigid, degenerate and non-degenerate. In
Computer Vision–ECCV 2006, pages 94–106.
Springer, 2006.

[41] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast

algorithm for edge-preserving variational multichannel
image restoration. SIAM Journal on Imaging
Sciences, 2(2):569–592, 2009.

[42] Z. Zhang and B. Tu. Nonconvex penalization using
laplace exponents and concave conjugates. In
Advances in Neural Information Processing Systems,
pages 611–619, 2012.

[43] Y.-B. Zhao. An approximation theory of matrix rank

minimization and its application to quadratic
equations. Linear Algebra and its Applications,
437(1):77–93, 2012.

APPENDIX
A. PROOF

problem

Theorem A.1. For µ > 0 and A ∈ Rm×n, the following

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

(30)

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(31)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σ∗

A)V T .

Proof. Let A = U ΣAV T be the skinny SVD of A, then
ΣA = U T AV . Denoting X = U T ZV which has exactly the

B. THEOREM AND LEMMAS

Theorem B.1. [19] Suppose F : Rm×n → R is repre-
sented as F (X) = f ◦ σ(X), and f : Rn → R is absolutely
symmetric and diﬀerentiable, where X ∈ Rm×n with SVD
X = U diag(σ)V T , the gradient of F (X) at X is

∂F (X)
∂X

= U diag(θ)V T ,

(41)

where θ = ∂f (y)

∂y |y=σ(X).

Lemma B.1. [1] For µ > 0, and K ∈ Rs×t, the solution

of the problem

min
L

µ(cid:107)L(cid:107)1 +

(cid:107)L − K(cid:107)2
F ,

1
2

is given by Lµ(K), which is deﬁned component-wisely by

[Lµ(K)]ij = max{|Kij| − µ, 0} · sign(Kij).

Lemma B.2. [41] Let H be a given matrix. If the optimal

solution to

min
W

α (cid:107)W (cid:107)2,1 +

1
2

(cid:107)W − H(cid:107)2
F

is W ∗, then the i-th column of W ∗ is

[W ∗]:,i =






0,

−α

(cid:107)H:,i(cid:107)2
(cid:107)H:,i(cid:107)2

H:,i,

if (cid:107)H:,i(cid:107)2 > α;
otherwise.

Robust Subspace Clustering via Tighter Rank
Approximation

Zhao Kang
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
zhao.kang@siu.edu

Chong Peng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
pchong@siu.edu

Qiang Cheng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
qcheng@cs.siu.edu

5
1
0
2
 
t
c
O
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
1
7
9
8
0
.
0
1
5
1
:
v
i
X
r
a

ABSTRACT
Matrix rank minimization problem is in general NP-hard.
The nuclear norm is used to substitute the rank function
in many recent studies. Nevertheless, the nuclear norm ap-
proximation adds all singular values together and the ap-
proximation error may depend heavily on the magnitudes of
singular values. This might restrict its capability in dealing
In this paper, an arctan-
with many practical problems.
gent function is used as a tighter approximation to the rank
function. We use it on the challenging subspace clustering
problem. For this nonconvex minimization problem, we de-
velop an eﬀective optimization procedure based on a type
of augmented Lagrange multipliers (ALM) method. Exten-
sive experiments on face clustering and motion segmentation
show that the proposed method is eﬀective for rank approx-
imation.

Categories and Subject Descriptors
I.5 [Pattern recognition]: Clustering—Algorithm; G.1.6
[Optimization]: Constrained optimization

Keywords
Subspace Clustering; Rank Minimization; Nuclear Norm;
Nonconvex Optimization

INTRODUCTION

1.
Matrix rank minimization arises in control, machine learn-
ing, signal processing and other areas [43].
It is diﬃcult
to solve due to the discontinuity and nonconvexity of the
rank function. Existing algorithms are largely based on
the nuclear norm heuristic, i.e., to replace the rank by the
nuclear norm [11]. The nuclear norm of a matrix X, de-
noted by (cid:107)X(cid:107)∗, is the sum of all its singular values, i.e.,
(cid:107)X(cid:107)∗ = (cid:80)
i σi(X). Under some conditions, the solution to
the nuclear norm heuristic coincides with the minimum rank
solution [31, 32]. However, since the nuclear norm is the con-
vex envelop of rank(X) over the unit ball {X : (cid:107)X(cid:107)2 ≤ 1}, it
may deviate from the rank of X in many circumstances [4, 3].

The rank function counts the number of nonvanishing singu-
lar values, while the nuclear norm sums their amplitudes. As
a result, the nuclear norm may be dominated by a few very
large singular values. Variations of standard nuclear norm
are shown to be promising in some recent research [14, 2,
29]. A number of nonconvex surrogate functions have come
up to better approximate the rank function, such as Loga-
rithm Determinant [11, 16], Schatten-p norm [26], truncated
nuclear norm [14] and others [24]. In general, they are to
solve the following low-rank minimization problem:

min
Z

min(m,n)
(cid:88)

i=1

h(σi(Z)) + λg(Z),

(1)

where σi(Z) denotes the i-th singular value of Z ∈ Rm×n,
h(·) is a potentially nonconvex, nonsmooth function, and
g(·) is a loss function. By choosing h(z) = z, the summation
of the ﬁrst term in (1) goes back to the nuclear norm (cid:107)Z(cid:107)∗,
problem (1) becomes the well known convex relaxation of
the rank minimization problem:

min
Z

(cid:107)Z(cid:107)∗ + λg(Z).

(2)

In this paper, we will propose a new nonconvex rank ap-
proximation and consider subspace clustering as a speciﬁc
application.

1.1 Previous Work on Subspace Clustering
In many real-world applications, high-dimensional data re-
side in a union of multiple low-dimensional subspaces rather
than one single low-dimensional subspace [7]. Subspace clus-
tering deals with exactly this structure by clustering data
points according to their underlying subspaces. It has nu-
merous applications in computer vision [30] and image pro-
cessing [25]. Therefore subspace clustering has drawn sig-
niﬁcant attention in recent years [36]. In practice, the un-
derlying subspace structure is often corrupted by noise and
outliers, and thus the data may deviate from the original
subspaces. It is necessary to develop robust estimation tech-
niques.

A number of approaches to subspace clustering have been
proposed in the past two decades. According to the sur-
vey in [36], they can be roughly divided into four cate-
gories: 1) algebraic methods; 2) iterative methods; 3) sta-
tistical methods; and 4) spectral clustering-based methods.
Among them, spectral clustering-based methods have ob-
tained state-of-the-art results, including sparse subspace clus-
tering (SSC) [9], and low rank representation (LRR) [21].

They perform subspace clustering in two steps: ﬁrst, learn-
ing an aﬃnity matrix that encodes the subspace member-
ship information, and then applying spectral clustering al-
gorithms [33, 28] to the learned aﬃnity matrix to obtain
the ﬁnal clustering results. Their main diﬀerence is how to
obtain a good aﬃnity matrix.

SSC assumes that each data point can be represented as
a sparse linear combination of other points. The popular
l1-norm heuristic is used to capture the sparsity. It enjoys
great performance for face clustering and motion segmenta-
tion data. Now we have a good theoretical understanding
about SSC. For instance, [8] shows that disjoint subspaces
can be exactly recovered under certain conditions; geometric
analysis of SSC [34] signiﬁcantly broadens the scope of SSC
to intersecting subspaces. However, the data points are as-
sumed to be lying exactly in the subspace. This assumption
may be violated in the presence of corrupted data. [38] ex-
tends SSC by adding adversarial or random noise. However,
SSC’s solution might be too sparse, thus the aﬃnity graph
from a single subspace will not be a fully connected body
[27]. To address the above issue, another regularization term
is introduced to promote connectivity of the graph [9].

LRR also represents each data point as a linear combination
of other points. It is to ﬁnd the lowest rank representation
Z of all data points jointly, where the nuclear norm is used
as a common surrogate of the rank function. In the presence
of noise or outliers, LRR solves the following problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ (cid:107)E(cid:107)l

s.t. X = XZ + E,

(3)

where λ balances the eﬀects of the low rank representa-
tion and errors, X = [x1, · · · , xn] ∈ Rm×n is a set of m-
dimensional data vectors drawn from the union of k sub-
spaces {Si}k
i=1, and (cid:107)·(cid:107)l characterizes certain corruptions
E. For example, when E represents Gaussian noise, squared
n
(cid:80)
Frobenius norm (cid:107)E(cid:107)2
j=1

ij is used; when E denotes

m
(cid:80)
i=1

F =

E2

random corruptions, l1 norm (cid:107)E(cid:107)1 =

|Eij| is appro-

priate; when E indicates the sample-speciﬁc corruptions, l2,1

norm is adopted, where (cid:107)E(cid:107)2,1 =

E2

ij. The low

rank as well as sparsity requirement may help counteract
corruptions. A variant of LRR works even in the presence
of some arbitrarily large outliers [23]. However, LRR has
never been shown to succeed other than under strong “inde-
pendent subspace” condition [15].

m
(cid:80)
i=1

n
(cid:80)
j=1

(cid:115) m
(cid:80)
i=1

n
(cid:80)
j=1

In view of the issues with the nuclear norm mentioned in
the beginning, we propose the use of an arctangent func-
tion instead in this work. We demonstrate the enhanced
performance of the proposed algorithm on benchmark data
sets.

• More accurate rank approximation is proposed to ob-
tain the low-rank representation of high-dimensional
data.

• An eﬃcient optimization procedure is developed for
arctangent rank minimization (ARM) problem. Theo-
retical analysis shows that our algorithm converges to
a stationary point.

• The superiority of the proposed method to various
state-of-the-art subspace clustering algorithms is veri-
ﬁed with signiﬁcantly and consistently lower error rates
of ARM on popular datasets.

Figure 1: Comparison of approximation for rank 2.

2. SUBSPACE CLUSTERING BY ARM
In this work, we demonstrate the application of F (Z) =
f ◦ σ(Z), where f (x) = (cid:80)
i arctan(|xi|) for any x ∈ Rn
(for high dimensional data usually m >> n which is the
case we suppose in the paper), as a rank approximation of
matrix Z in subspace clustering setting. There are three
advantages of this approximation function. First, it approx-
imates rank(Z) much better than the nuclear norm does,
i.e., as σi ∈ [0, ∞], 2
π arctan(σi) ∈ [0, 1]. Figure 1 shows the
rank approximation value of the two approaches for rank 2
situation. We can clearly see that arctangent reﬂects the
real rank pretty well on a broad range of singular values.
Second, f is diﬀerentiable, concave and monotonically in-
creasing on [0, ∞]n, by deﬁning the gradient of f at 0 as
∂
= 1. Third, F is unitarily in-
∂xi
variant and f is absolutely symmetric, i.e., f (x) is invariant
under arbitrary permutation and sign changes of the compo-
nents of x. Based on these properties, we have the following
theorems, which is proved in Appendix A.

f (0) = limxi→0+

1
1+x2
i

Theorem 2.1. For µ > 0 and A ∈ Rm×n, the following

problem

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(4)

(5)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σA)V T .

1.2 Our Contributions
In summary, the main contributions of this paper are three-
fold:

2.1 Arctangent Rank Minimization
To demonstrate the eﬀectiveness of the arctangent rank ap-
proximation, we consider its application in the challenging

subspace clustering problem. We propose the following arc-
tangent rank minimization (ARM) problem:

min
Z,E

n
(cid:88)

i=1

min
Z,E,J

n
(cid:88)

i=1

arctan(σi(Z)) + λ (cid:107)E(cid:107)l s.t. X = XZ + E.

(6)

It is diﬃcult to solve (6) directly because the objective func-
tion is neither convex nor concave. We convert it to the
following equivalent problem:

arctan(σi(J)) + λ (cid:107)E(cid:107)l s.t.X = XZ + E, Z = J.

(7)
Now we resort to a type of augmented Lagrange multipliers
(ALM) [20] method to solve (7). For simplicity of notation,
we denote σi = σi(J) and σt
i = σi(J t). The corresponding
augmented Lagrangian function is:

Algorithm 1 Arctan Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0,
and ρ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Update Z by (10).
2: Solve (11).
3: Solve E by either (15), (16) or (17) according to l.
4: Update Y1 by (18) and Y2 by (19).
5: Update µ by µt+1 = ρµt.
UNTIL stopping criterion is met.

where ωk = ∂f (σk) is the gradient of f (·) at σk and the
SVD of Z t+1 − Y t
is U diag{σA}V T . Finally, it converges
2
µt
to a local optimal point σ∗. Then J t+1 = U diag{σ∗}V T .

n
(cid:88)

i=1

L(E, J, Y1, Y2, Z, µ) =

arctan(σi) + λ (cid:107)E(cid:107)l

+ T r(Y T
µ
2

1 (X − XZ − E)) + T r(Y T
F + (cid:107)J − Z(cid:107)2

((cid:107)X − XZ − E(cid:107)2

+

F ),

2 (J − Z))

For Et+1, we have the following subproblem:

(8)

Et+1 = arg min

λ (cid:107)E(cid:107)l +

(cid:107)X − XZ t+1 − E(cid:107)2
F

E

µt
2

+ T r[(Y t

1 )T (X − XZ t+1 − E)].

Depending on diﬀerent regularization strategies, we have dif-
ferent closed-form solutions. For squared Forbenius norm,
it is again a quadratic problem,

Et+1 =

1 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1 and l2,1 norm, we use the lemmas from Appendix B.
Let Q = X − XZ t+1 + Y t
µt , we can solve E element-wisely
as below:

1

(9)

(cid:26) Qij − λ

µt sgn(Qij),

Et+1

ij =

0,

if |Qij| < λ
µt ;
otherwise.

In the case of l2,1 norm, we have



(cid:107)Q:,i(cid:107)2

− λ
µt

Q:,i,

[Et+1]:,i =

(cid:107)Q:,i(cid:107)2



0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The update of Lagrange multipliers is:

Y t+1
1 = Y t
2 = Y t
Y t+1

1 + µt(X − XZ t+1 − Et+1),
2 + µt(J t+1 − Z t+1).

(14)

(15)

(16)

(17)

(18)

(19)

where µ > 0 is a penalty parameter and Y1, Y2 are La-
grangian multipliers. The variables E, J, and Z can be
updated alternatively, one at each step, while keeping the
other two ﬁxed. For the (t + 1)th iteration, the iterative
scheme is given as follows.

For Z t+1, by ﬁxing Et, J t, Y t

1 and Y t

2 , we have:

Z t+1 = arg min

T r[(Y t

1 )T (X − XZ − Et)]+

Z
2 )T (J t − Z)]+
T r[(Y t
µt
(cid:13)X − XZ − Et(cid:13)
((cid:13)
(cid:13)
2

2

F

+ (cid:13)

(cid:13)J t − Z(cid:13)
(cid:13)

2

F

).

It is evident that the objective function of (9) is a strongly
convex quadratic function which can be solved directly. By
setting the ﬁrst derivative of it to zero, we have:

Z t+1 = (I +X T X)−1[X T (X −Et)+J t+

X T Y t
1 + Y t
2
µt

], (10)

where I ∈ Rn×n is the identity matrix.

J t+1 = arg min

arctan(σi) +

(cid:107)J − (Z t+1 −

µt
2

1
µt Y t

2 )(cid:107)2
F .

n
(cid:88)

i=1

J

(11)
Then we can convert it to problem (5). The ﬁrst term in
(5) is concave while the second term convex in σ, so we can
apply diﬀerence of convex (DC) [13] (vector) optimization
method. A linear approximation is used at each iteration of
DC programing. At iteration k + 1,

σk+1 = arg min

(cid:104)wk, σ(cid:105) +

(cid:107)σ − σA(cid:107)2
2,

(12)

σ≥0

µt
2

whose closed-form solution is

σk+1 = (σA −

ωk
µt )+,

2.2 Afﬁnity Graph Construction
After obtaining optimal Z ∗, we can build the similarity
graph matrix W . As argued in [9], some postprocessing

(13)

Figure 2: Sample face images in Extended Yale B.

For J t+1, we have:

The procedure is outlined in Algorithm 1.

Algorithm 2 Subspace Clustering by ARM
Input: data matrix X, number of subspaces k.
Do
1: Obtain optimal Z ∗ by solving (7).
2: Compute the skinny SVD Z ∗ = U ΣV T .
3: Calculate (cid:101)U = U (Σ)1/2.
4: Construct the aﬃnity graph matrix W by (20).
5: Perform NCuts on W .

of the coeﬃcient matrix can improve the clustering perfor-
mance. Following the angular information based technique
of [21], we deﬁne (cid:101)U = U Σ1/2, where U and Σ are from the
skinny SV D of Z ∗ = U ΣV T . Inspired by [17], we deﬁne W
as follows:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2α,

mality condition,

(20)

where (cid:101)ui and (cid:101)uj denote the i-th and j-th columns of (cid:101)U , and
α ∈ N ∗ controls the sharpness of the aﬃnity between two
points. Increasing the power α enhances the separation abil-
ity in the presence of noise. However, an excessively large α
would break aﬃnities between points of the same group. In
order to compare with LRR1, we use α = 2 in our experi-
ments, then we have the same postprocessing procedure as
LRR. After obtaining W , we directly utilize a spectral clus-
tering algorithm NCuts [33] to cluster the samples. Algo-
rithm 2 summarizes the complete subspace clustering steps
of the proposed method.

Figure 3: Aﬃnity graph matrix W with ﬁve and
ten subjects.

3. CONVERGENCE ANALYSIS
Since the objective function (6) is nonconvex, it would not
be easy to prove the convergence in theory. In this paper,
we mathematically prove that our optimization algorithm
has at least a convergent subsequence which converges to an
accumulation point, and moreover, any accumulation point
of our algorithm is a stationary point. Although the ﬁnal so-
lution might be a local optimum, our results are superior to
the global optimal solution from convex approaches. Some
previous work also reports similar observations [39, 12, 42].

1As we conﬁrmed with an author of [21], the power 2 of its
equation (12) is a typo, which should be 4.

We will show the proof in the case of (cid:107)E(cid:107)1. Let’s ﬁrst re-
formulate our objective function:

G(J, Z, E) = F (J) + λ(cid:107)E(cid:107)1

s.t. Z = J, X = XZ + E,

L(J, Z, E, Y1, Y2, µ) = G(J, Z, E) + (cid:104)Y1, X − XZ − E(cid:105)

+ (cid:104)Y2, J − Z(cid:105) +

((cid:107)J − Z(cid:107)2

F + (cid:107)X − XZ − E(cid:107)2

F ).

µ
2

(21)

(22)

Lemma 3.1. The sequences of {Y t

1 } and {Y t

2 } are bounded.

Proof. J t+1 satisﬁes the ﬁrst-order necessary local opti-

∂J L (cid:0)J, Z t+1, Et, Y t
1 , Y t
(cid:18)

2 , µt(cid:1) |J t+1

=∂J F (J) |J t+1 + µt

J t+1 − Z t+1 +

(cid:19)

Y t
2
µt

(23)

=∂J F (J) |J t+1 + Y t+1
=0.

2

Let’s deﬁne θi =
cording to (41) in Appendix B,

1

1+(σi)2 if σi (cid:54)= 0; otherwise, it is 1. Ac-

U diag(θ)V T = ∂J F (J) |J t+1 ,

(24)

and 0 <

1
1+(σt+1
i
(23), we conclude that Y t+1

2

is bounded.

)2 ≤ 1, ∂J F (J) |J t+1 is bounded. From

Similarly, for Et+1

1 , Y t

2 , µt(cid:1) |Et+1

0 ∈ ∂EL (cid:0)J t+1, Z t+1, E, Y t
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 − Y t
− µt (cid:0)X − XZ t+1 − Et+1(cid:1)
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 + Y t+1

1

1

.

(25)

Here ∂E denotes the subgradient operator [6]. Because ||E||1
is nonsmooth only at Eij = 0, we deﬁne [∂E(cid:107)E(cid:107)1]ij = 0 if
Eij = 0. Then 0 ≤ (cid:107)∂E(cid:107)E(cid:107)1(cid:107)2
F ≤ mn is bounded. There-
fore, {Y t+1

} is bounded.

1

Lemma 3.2. {J t}, {Et} and {Z t} are bounded if (cid:80) µt+1

(µt)2 <

∞, (cid:80) 1

µt < ∞ and X T X is invertible.

Proof.

2 , µt(cid:1)

1

1 , Y t

((cid:107)J t − Z t(cid:107)2

L (cid:0)J t, Z t, Et, Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt − µt−1
2
+ T r((Y t
+ T r((Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1
2 − Y t−1
2

1 − Y t−1
1

1

, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)X − XZ t − Et(cid:107)2

F )

)(X − XZ t − Et))
)(J t − Z t))
, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

(26)

2 , µt(cid:1)
2 , µt)

L (cid:0)J t+1, Z t+1, Et+1, Y t
≤ L(J t+1, Z t+1, Et, Y t
1 , Y t
≤ L(J t, Z t+1, Et, Y t
≤ L (cid:0)J t, Z t, Et, Y t
1 , Y t
≤ L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1

1 , Y t
1 , Y t
2 , µt)
2 , µt(cid:1)
, Y t−1
2

1

µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

Iterating the inequality (27) gives that

Under the given conditions on {µt}, both terms on the
right-hand side of the above inequality are bounded, thus
L (cid:0)J t+1, Z t+1, Et+1, Y t

1 , Y t

2 , µt(cid:1) is bounded. In addition,
2 , µt(cid:1) +

1 , Y t

L(J t+1, Z t+1, Et+1, Y t
≤ L (cid:0)J 1, Z 1, E1, Y 0
1 , Y 0

2 , µt)
1 , Y t
2 , µ0(cid:1) +

t
(cid:88)

µi + µi−1
2(µi−1)2 ((cid:107)Y i
2 − Y i−1
2

(cid:107)2
F ).

i=1
+ (cid:107)Y i

1 − Y i−1
1

(cid:107)2
F

F )

2 (cid:107)2

1 (cid:107)2

F + (cid:107)Y t

L (cid:0)J t+1, Z t+1, Et+1, Y t
1
2µt ((cid:107)Y t
= F (J t+1) + λ(cid:107)Et+1(cid:107)1+
µt
2
µt
2

Y t
µt (cid:107)2
2
(cid:107)X − Et+1 − XZ t+1 +

(cid:107)J t+1 − Z t+1 +

F +

Y t
µt (cid:107)2
1
F .

(28)

(29)

The left-hand side in the above equation is bounded and
each term on the right-hand side is nonnegative, so each
term is bounded. Therefore, Et+1 is bounded. XZ t+1 is
bounded according to the last term on the right-hand side
of (29), and thus after multiplying a constant matrix X T , we
have X T XZ t+1 is bounded. Under the condition that X T X
is invertible, by multiplying a constant matrix (X T X)−1,
we have that (X T X)−1X T XZ t+1 = Z t+1 is bounded. Fi-
nally, J t+1 is bounded because the second to the last term is
bounded. Therefore, {J t}, {Et} and {Z t} are bounded.

1 , Y t

Theorem 3.1. The sequence {J t, Et, Z t, Y t

2 } gener-
ated by Algorithm 1 has at least one accumulation point, un-
der the conditions that (cid:80) µt+1
µt < ∞ and X T X
is invertible. For any accumulation point {J ∗, E∗, Z ∗, Y ∗
1 , Y ∗
{J ∗, E∗, Z ∗} is a stationary point of optimization problem
(21), under the conditions that µt(J t+1 − J t) → 0, and
µt(Et+1 − Et) → 0.

(µt)2 < ∞, (cid:80) 1

2 },

1 , Y t

Proof. Based on the conditions on the penalty parame-
ter sequence {µt} and X T X, Algorithm 1 generates a bounded
sequence {J t, Et, Z t, Y t
2 } by Lemma (3.1) and (3.2) . By
the Bolzano-Weierstrass theorem, at least one accumulation
point exists, e.g., {J ∗, E∗, Z ∗, Y ∗
2 }. Without loss of gen-
1 , Y t
erality, we assume that {J t, Et, Z t, Y t
2 } itself converges
to {J ∗, E∗, Z ∗, Y ∗
2 }. As shown below, {J ∗, E∗, Z ∗} is
1 , Y ∗
a stationary point of problem (21), under additional condi-
tions that µt(Et+1 − Et) → 0 and µt(J t+1 − J t) → 0.

1 , Y ∗

Since J t −Z t = Y t
Therefore, J ∗ = Z ∗.

2 −Y t−1
2
µt−1

and µt → ∞, we have J t −Z t → 0.

(27)

Similarly, by X − XZ t − Et = Y t
X − XZ ∗.

1 −Y t−1
1
µt−1

, we have E∗ =

For Z t+1, the ﬁrst-order optimality condition is

1 , Y t

2 − X T Y t

2 − X T Y t

2 , µt(cid:1) |Zt+1

1 − µt (cid:0)J t − Z t+1(cid:1)

∇Z L (cid:0)J t, Z, Et, Y t
= −Y t
− µtX T (cid:0)X − Et − XZ t+1(cid:1)
= −Y t
− µtX T (cid:0)X − Et+1 − XZ t+1 − Et + Et+1(cid:1)
2 + µt (cid:0)J t+1 − J t(cid:1) − X T Y t+1
= −Y t+1
− µtX T (cid:0)Et+1 − Et(cid:1)
= 0.

1

1 − µt (cid:0)J t+1 − Z t+1 + J t − J t+1(cid:1)

2 + Y t

1 → 0, i.e., −X T Y ∗

If µt(J t+1 − J t) → 0 and µt(Et+1 − Et) → 0, we have
X T Y t
1 . It is easy to verify
∂EL (J, Z, E, Y1, Y2, µ) |E∗ = 0. Therefore, {J ∗, E∗, Z ∗, Y ∗
satisﬁes the KKT conditions of L(J, E, Z, Y1, Y2) and thus
{J ∗, E∗, Z ∗} is a stationary point of (21).

2 = Y ∗

1 , Y ∗
2 }

4. EXPERIMENTS
This section presents experiments with the proposed algo-
rithm on the Extended Yale B (EYaleB) [18] and Hopkins
155 databases [35]. They are standard tests for robust sub-
space clustering algorithms. As shown in [9], the challenge
in the Hopkins 155 dataset is due to the small principal an-
gles between subspaces. For EYaleB, the challenge lies in the
small principal angles and another factor that data points
from diﬀerent subspaces are close. Our results are compared
with several state-of-the-art subspace clustering algorithms,
including LRR [21], SSC [9], LRSC [10, 37], spectral curva-
ture clustering (SCC) [5], and local subspace aﬃnity (LSA)
[40], in terms of misclassiﬁcation rate2. For fair compari-
son, we follow the experimental setup in [9] and obtain the
results.

As other methods do, we tune our parameters to obtain the
In general, the value of λ depends on prior
best results.
If the noise is
knowledge of the noise level of the data.
heavy, a small λ should be adopted. µ0 and ρ aﬀect the
convergence speed. The larger their values are, the fewer
iterations are required for the algorithm to converge, but
meanwhile we may lose some precision of the ﬁnal objective
In the literature, the value of ρ is often
function value.
chosen between 1 and 1.1. The iteration stops at a relative
normed diﬀerence of 10−5 between two successive iterations,
or a maximum of 150 iterations.

4.1 Face Clustering
Face clustering refers to partitioning a set of face images
from multiple individuals to multiple subspaces according to
the identity of each individual. The face images are heavily
contaminated by sparse gross errors due to varying lighting
conditions, as shown in Figure 2. Therefore, (cid:107)E(cid:107)1 is used to
2The implementation of our algorithm is available at:
https://github.com/sckangz/arctangent.

model the errors in our experiment. The EYaleB database
contains cropped face images of 38 individuals taken under
64 diﬀerent illumination conditions. The 38 subjects were
divided into four groups as follows: subjects 1 to 10, 11 to 20,
21 to 30, and 31 to 38. All choices of n ∈ {2, 3, 5, 8, 10} are
considered for each of the ﬁrst three groups, and all choices of
n ∈ {2, 3, 5, 8} are considered for the last group. As a result,
there are {163, 416, 812, 136, 3} combinations corresponding
to diﬀerent n. Each image is downsampled to 48 × 42 and is
vectorized to a 2016-dimensional vector. λ = 10−5, µ0 = 1.7
and γ = 1.03 are used in this experiment.

addition, the error of LSA is large maybe because LSA is
based on MSE. Since the MSE is quite sensitive to outliers,
LSA will fail to deal with large outliers.

Figure 5: Convergence curve of the objective func-
tion value in (6).

Figure 4: Recovery results of two face images. The
three columns from left to right are the original im-
age (X), the error matrix (E) and the recovered im-
age (XZ), respectively.

Table 1: Clustering error rates (%) on the EYaleB
database.

Algorithm
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

LRR

SSC

LSA LRSC SCC ARM

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

16.62
7.82

38.16
39.06

58.02
56.87

12.24
11.25

58.90
59.38

59.19
58.59

23.72
28.03

66.11
64.65

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

73.02
75.78

1.51
0.78

2.26
1.56

3.06
2.50

3.70
3.32

3.85
2.97

Table 1 provides the best performance of each method. As
shown in the table, our proposed method has the lowest
mean clustering error rates in all ﬁve settings. In particu-
lar, in the most challenging case of 10 subjects, the mean
clustering error rate is as low as 3.85%. The improvement
is signiﬁcant compared with other low rank representation
based subspace clustering, i.e., LRR and LRSC. For exam-
ple, 19% and 11% improvement over LRR can be observed
in the cases of 10 and 8 subjects, respectively. This demon-
strates the importance of accurate rank approximation. In

Figure 6: Average computational time (sec) of the
algorithms on the EYaleB database as a function of
the number of subjects.

Figure 3 shows the obtained aﬃnity graph matrix W for the
ﬁve and ten subjects scenarios. We can see a distinct block-
diagonal structure, which means that each cluster becomes
highly compact and diﬀerent subjects are well separated.

In Figure 4, we present the recovery results of some sam-
ple faces from the 10-subject clustering case. We can see
that the proposed algorithm has the beneﬁt of removing the
corruptions in data.

Figure 5 plots the progress of objective function values of
(6). It is observed that with more iterations, the value of
objective function decreases monotonically. This empirically
veriﬁes the convergence of our optimization method.

rithms in mean error rate. Especially, its all mean error rates
are around 1.5%. This again demonstrates the eﬀectiveness
of using arctangent as a rank approximation.

Figure 8: The inﬂuence of parameter λ of ARM on
clustering error of Hopkins 155 database.

Figure 8 shows the culstering error rate of ARM for diﬀerent
λ over all 155 sequences. When λ is between 1 and 3, the
clustering error rate varies between 1.48% and 2.19%. This
demonstrates that ARM performs well under a pretty wide
range of values of λ. This is another advantage of ARM over
LRR [21].

5. CONCLUSION
In this work, we propose to use arctangent as a concave
rank approximation function.
It has some nice properties
compared with the standard nuclear norm. We apply this
function to the low rank representation-based subspace clus-
tering problem and develop an iterative algorithm for opti-
mizing the associated objective function. Extensive experi-
mental results demonstrate that, compared to many state-
of-the-art algorithms, the proposed algorithm gives the low-
est clustering error rates on many benchmark datasets. This
fully demonstrates the signiﬁcance of accurate rank approx-
imation. Interesting future work includes other applications
of the arctangent rank approximation; for example, matrix
completion. Since LRR can only ensure its validity for in-
dependent subspace segmentation, it is worthwhile to inves-
tigate somewhat dependent yet possibly disjoint subspace
clustering.

Figure 7: Example frames from two video sequences
of the Hopkins 155 database with traced feature
points.

We compare the average computational time of LRR, SSC,
and ARM as a function of the number of subjects in Figure 6.
All the experiments are conducted and timed on the same
machine with an Intel Xeon E3-1240 3.40GHz CPU that
has 4 cores and 8GB memory, running Ubuntu and Matlab
(R2014a). We can observe that the computational time of
SSC is higher than LRR and ARM, while ARM is a little
slower than LRR in most cases.

4.2 Motion Segmentation

Table 2: Segmentation error rates (%) on the Hop-
kins 155 Dataset.

4.23
0.56

1.52
0.00

2.13
0.00

Algorithm LRR SSC LSA LRSC SCC ARM
2 Motions
Mean
Median
3 Motions
Mean
Median
All
Mean
Median

8.25
0.24

2.89
0.00

1.48
0.00

4.40
0.56

4.59
0.60

3.69
0.29

7.69
3.80

4.86
0.89

4.03
1.43

7.02
1.45

1.49
0.84

2.56
0.00

4.10
0.00

2.18
0.00

1.48
0.00

Motion segmentation involves segmenting a video sequence
of multiple moving objects into multiple spatiotemporal re-
gions corresponding to diﬀerent motions. These motion se-
quences can be divided into three main categories: checker-
board, traﬃc, and articulated or non-rigid motion sequences.
The Hopkins 155 dataset includes 155 video sequences of 2
or 3 motions, corresponding to 2 or 3 low-dimensional sub-
spaces of the ambient space. Each sequence represents a
data set and so there are 155 motion segmentation prob-
lems in total. Several example frames are shown in Figure 7.
The trajectories are extracted automatically by a tracker, so
they are slightly corrupted by noise. As in [21, 22], (cid:107)E(cid:107)2,1
is adopted in the model. In this experiment, λ = 2, µ0 = 10
and γ = 1.05.

We use the original 2F-dimensional feature trajectories in
our experiment. We show the clustering error rates of dif-
ferent algorithms in Table 2. ARM outperforms other algo-

6. ACKNOWLEDGMENTS
This work is supported by US National Science Foundation
Grants IIS 1218712. The corresponding author is Qiang
Cheng.

7. REFERENCES
[1] A. Beck and M. Teboulle. A fast iterative

shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences,
2(1):183–202, 2009.

[2] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value
thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2010.
[3] E. J. Cand`es and B. Recht. Exact matrix completion

via convex optimization. Foundations of
Computational mathematics, 9(6):717–772, 2009.

[4] E. J. Cand`es and T. Tao. The power of convex
relaxation: Near-optimal matrix completion.
Information Theory, IEEE Transactions on,
56(5):2053–2080, 2010.

[5] G. Chen and G. Lerman. Spectral curvature clustering

(scc). International Journal of Computer Vision,
81(3):317–330, 2009.

[6] F. H. Clarke. Optimization and nonsmooth analysis,

volume 5. Siam, 1990.

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering.
In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 2790–2797.
IEEE, 2009.

[8] E. Elhamifar and R. Vidal. Clustering disjoint

subspaces via sparse representation. In Acoustics
Speech and Signal Processing (ICASSP), 2010 IEEE
International Conference on, pages 1926–1929. IEEE,
2010.

[9] E. Elhamifar and R. Vidal. Sparse subspace clustering:
Algorithm, theory, and applications. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
35(11):2765–2781, 2013.

[10] P. Favaro, R. Vidal, and A. Ravichandran. A closed
form solution to robust subspace estimation and
clustering. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on,
pages 1801–1807. IEEE, 2011.

[11] M. Fazel. Matrix rank minimization with applications.
PhD thesis, PhD thesis, Stanford University, 2002.

[12] P. Gong, J. Ye, and C.-s. Zhang. Multi-stage

multi-task feature learning. In Advances in Neural
Information Processing Systems, pages 1988–1996,
2012.

[13] R. Horst and N. V. Thoai. Dc programming:

overview. Journal of Optimization Theory and
Applications, 103(1):1–43, 1999.

[14] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and

accurate matrix completion via truncated nuclear
norm regularization. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(9):2117–2130,
2013.

[15] K. Kanatani. Motion segmentation by subspace

separation and model selection. In Computer Vision,
2001. ICCV 2001. Proceedings. Eighth IEEE
International Conference on, volume 2, pages 586–591,
2001.

[16] Z. Kang, C. Peng, J. Cheng, and Q. Cheng. Logdet
rank minimization with application to subspace
clustering. Computational Intelligence and
Neuroscience, 2015, 2015.

[17] F. Lauer and C. Schnorr. Spectral clustering of linear
subspaces for motion segmentation. In Computer
Vision, 2009 IEEE 12th International Conference on,
pages 678–685. IEEE, 2009.

[18] K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear
subspaces for face recognition under variable lighting.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 27(5):684–698, 2005.

[19] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of

singular values. part i: Theory. Set-Valued Analysis,
13(3):213–241, 2005.

[20] Z. Lin, R. Liu, and Z. Su. Linearized alternating

direction method with adaptive penalty for low-rank
representation. In Advances in neural information
processing systems, pages 612–620, 2011.

[21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma.

Robust recovery of subspace structures by low-rank
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(1):171–184,
2013.

[22] G. Liu, Z. Lin, and Y. Yu. Robust subspace
segmentation by low-rank representation. In
Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 663–670, 2010.

[23] G. Liu, H. Xu, and S. Yan. Exact subspace

segmentation and outlier detection by low-rank
representation. In AISTATS, pages 703–711, 2012.
[24] C. Lu, J. Tang, S. Y. Yan, and Z. Lin. Generalized

nonconvex nonsmooth low-rank minimization. In
IEEE International Conference on Computer Vision
and Pattern Recognition. IEEE, 2014.

[25] Y. Ma, H. Derksen, W. Hong, and J. Wright.

Segmentation of multivariate mixed data via lossy
data coding and compression. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
29(9):1546–1562, 2007.

[26] K. Mohan and M. Fazel. Iterative reweighted

algorithms for matrix rank minimization. The Journal
of Machine Learning Research, 13(1):3441–3473, 2012.

[27] B. Nasihatkon and R. Hartley. Graph connectivity in

sparse subspace clustering. In Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference
on, pages 2137–2144. IEEE, 2011.

[28] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral
clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849–856,
2002.

[29] F. Nie, H. Huang, and C. H. Ding. Low-rank matrix

recovery via eﬃcient schatten p-norm minimization. In
AAAI, 2012.

[30] S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion

segmentation in the presence of outlying, incomplete,
or corrupted trajectories. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
32(10):1832–1845, 2010.

[31] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed

minimum-rank solutions of linear matrix equations via

same singular values as Z, we have

F (Z) +

(cid:107)Z − A(cid:107)2
F

µ
2

(cid:107)X − ΣA(cid:107)2
F ,

(cid:0)(cid:107)X(cid:107)2

= F (X) +

(cid:0)(cid:107)ΣX (cid:107)2

= F (ΣX ) +

≥ F (ΣX ) +

µ
2
= F (ΣX ) +

(cid:107)X − ΣA(cid:107)2
F ,
µ
2
µ
2
µ
2
µ
2
µ
= F (ΣZ ) +
2
µ
(cid:107)σ − σA(cid:107)2
2,
2
µ
≥ f (σ∗) +
2

(cid:107)σ∗ − σA(cid:107)2
2.

= F (ΣX ) +

= f (σ) +

(cid:107)ΣZ − ΣA(cid:107)2
F ,

(cid:107)ΣX − ΣA(cid:107)2
F ,

F + (cid:107)ΣA(cid:107)2

F + (cid:107)ΣA(cid:107)2

F − 2 (cid:104)X, ΣA(cid:105)(cid:1) ,
F − 2 (cid:104)ΣX , ΣA(cid:105)(cid:1) ,

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

In the above, (33) holds because the Frobenius norm is uni-
tarily invariant; (34) holds because F (X) is unitarily invari-
ant; (36) is true by von Neumann’s trace inequality; and
(38) holds because of the deﬁnition of X. Therefore, (38)
is a lower bound of (32). Note that the equality in (36) is
attained if X = ΣX . Because ΣZ = ΣX = X = U T ZV , the
SVD of Z is Z = U ΣZ V T . By minimizing (39), we get σ∗.
Therefore, eventually we get Z ∗ = U diag(σ∗)V T , which is
the minimizer of problem (30).

nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

[32] B. Recht, W. Xu, and B. Hassibi. Null space

conditions and thresholds for rank minimization.
Mathematical programming, 127(1):175–202, 2011.

[33] J. Shi and J. Malik. Normalized cuts and image
segmentation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 22(8):888–905,
2000.

[34] M. Soltanolkotabi, E. J. Candes, et al. A geometric

analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
[35] R. Tron and R. Vidal. A benchmark for the

comparison of 3-d motion segmentation algorithms. In
Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE,
2007.

[36] R. Vidal. A tutorial on subspace clustering. IEEE
Signal Processing Magazine, 28(2):52–68, 2010.

[37] R. Vidal and P. Favaro. Low rank subspace clustering
(lrsc). Pattern Recognition Letters, 43:47–61, 2014.

[38] Y.-X. Wang and H. Xu. Noisy sparse subspace

clustering. In Proceedings of The 30th International
Conference on Machine Learning, pages 89–97, 2013.
[39] S. Xiang, X. Tong, and J. Ye. Eﬃcient sparse group
feature selection via nonconvex optimization. In
Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 284–292, 2013.

[40] J. Yan and M. Pollefeys. A general framework for

motion segmentation: Independent, articulated, rigid,
non-rigid, degenerate and non-degenerate. In
Computer Vision–ECCV 2006, pages 94–106.
Springer, 2006.

[41] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast

algorithm for edge-preserving variational multichannel
image restoration. SIAM Journal on Imaging
Sciences, 2(2):569–592, 2009.

[42] Z. Zhang and B. Tu. Nonconvex penalization using
laplace exponents and concave conjugates. In
Advances in Neural Information Processing Systems,
pages 611–619, 2012.

[43] Y.-B. Zhao. An approximation theory of matrix rank

minimization and its application to quadratic
equations. Linear Algebra and its Applications,
437(1):77–93, 2012.

APPENDIX
A. PROOF

problem

Theorem A.1. For µ > 0 and A ∈ Rm×n, the following

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

(30)

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(31)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σ∗

A)V T .

Proof. Let A = U ΣAV T be the skinny SVD of A, then
ΣA = U T AV . Denoting X = U T ZV which has exactly the

B. THEOREM AND LEMMAS

Theorem B.1. [19] Suppose F : Rm×n → R is repre-
sented as F (X) = f ◦ σ(X), and f : Rn → R is absolutely
symmetric and diﬀerentiable, where X ∈ Rm×n with SVD
X = U diag(σ)V T , the gradient of F (X) at X is

∂F (X)
∂X

= U diag(θ)V T ,

(41)

where θ = ∂f (y)

∂y |y=σ(X).

Lemma B.1. [1] For µ > 0, and K ∈ Rs×t, the solution

of the problem

min
L

µ(cid:107)L(cid:107)1 +

(cid:107)L − K(cid:107)2
F ,

1
2

is given by Lµ(K), which is deﬁned component-wisely by

[Lµ(K)]ij = max{|Kij| − µ, 0} · sign(Kij).

Lemma B.2. [41] Let H be a given matrix. If the optimal

solution to

min
W

α (cid:107)W (cid:107)2,1 +

1
2

(cid:107)W − H(cid:107)2
F

is W ∗, then the i-th column of W ∗ is

[W ∗]:,i =






0,

−α

(cid:107)H:,i(cid:107)2
(cid:107)H:,i(cid:107)2

H:,i,

if (cid:107)H:,i(cid:107)2 > α;
otherwise.

Robust Subspace Clustering via Tighter Rank
Approximation

Zhao Kang
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
zhao.kang@siu.edu

Chong Peng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
pchong@siu.edu

Qiang Cheng
Computer Science Dept.
Southern Illinois University
Carbondale, IL, USA
qcheng@cs.siu.edu

5
1
0
2
 
t
c
O
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
1
7
9
8
0
.
0
1
5
1
:
v
i
X
r
a

ABSTRACT
Matrix rank minimization problem is in general NP-hard.
The nuclear norm is used to substitute the rank function
in many recent studies. Nevertheless, the nuclear norm ap-
proximation adds all singular values together and the ap-
proximation error may depend heavily on the magnitudes of
singular values. This might restrict its capability in dealing
In this paper, an arctan-
with many practical problems.
gent function is used as a tighter approximation to the rank
function. We use it on the challenging subspace clustering
problem. For this nonconvex minimization problem, we de-
velop an eﬀective optimization procedure based on a type
of augmented Lagrange multipliers (ALM) method. Exten-
sive experiments on face clustering and motion segmentation
show that the proposed method is eﬀective for rank approx-
imation.

Categories and Subject Descriptors
I.5 [Pattern recognition]: Clustering—Algorithm; G.1.6
[Optimization]: Constrained optimization

Keywords
Subspace Clustering; Rank Minimization; Nuclear Norm;
Nonconvex Optimization

INTRODUCTION

1.
Matrix rank minimization arises in control, machine learn-
ing, signal processing and other areas [43].
It is diﬃcult
to solve due to the discontinuity and nonconvexity of the
rank function. Existing algorithms are largely based on
the nuclear norm heuristic, i.e., to replace the rank by the
nuclear norm [11]. The nuclear norm of a matrix X, de-
noted by (cid:107)X(cid:107)∗, is the sum of all its singular values, i.e.,
(cid:107)X(cid:107)∗ = (cid:80)
i σi(X). Under some conditions, the solution to
the nuclear norm heuristic coincides with the minimum rank
solution [31, 32]. However, since the nuclear norm is the con-
vex envelop of rank(X) over the unit ball {X : (cid:107)X(cid:107)2 ≤ 1}, it
may deviate from the rank of X in many circumstances [4, 3].

The rank function counts the number of nonvanishing singu-
lar values, while the nuclear norm sums their amplitudes. As
a result, the nuclear norm may be dominated by a few very
large singular values. Variations of standard nuclear norm
are shown to be promising in some recent research [14, 2,
29]. A number of nonconvex surrogate functions have come
up to better approximate the rank function, such as Loga-
rithm Determinant [11, 16], Schatten-p norm [26], truncated
nuclear norm [14] and others [24]. In general, they are to
solve the following low-rank minimization problem:

min
Z

min(m,n)
(cid:88)

i=1

h(σi(Z)) + λg(Z),

(1)

where σi(Z) denotes the i-th singular value of Z ∈ Rm×n,
h(·) is a potentially nonconvex, nonsmooth function, and
g(·) is a loss function. By choosing h(z) = z, the summation
of the ﬁrst term in (1) goes back to the nuclear norm (cid:107)Z(cid:107)∗,
problem (1) becomes the well known convex relaxation of
the rank minimization problem:

min
Z

(cid:107)Z(cid:107)∗ + λg(Z).

(2)

In this paper, we will propose a new nonconvex rank ap-
proximation and consider subspace clustering as a speciﬁc
application.

1.1 Previous Work on Subspace Clustering
In many real-world applications, high-dimensional data re-
side in a union of multiple low-dimensional subspaces rather
than one single low-dimensional subspace [7]. Subspace clus-
tering deals with exactly this structure by clustering data
points according to their underlying subspaces. It has nu-
merous applications in computer vision [30] and image pro-
cessing [25]. Therefore subspace clustering has drawn sig-
niﬁcant attention in recent years [36]. In practice, the un-
derlying subspace structure is often corrupted by noise and
outliers, and thus the data may deviate from the original
subspaces. It is necessary to develop robust estimation tech-
niques.

A number of approaches to subspace clustering have been
proposed in the past two decades. According to the sur-
vey in [36], they can be roughly divided into four cate-
gories: 1) algebraic methods; 2) iterative methods; 3) sta-
tistical methods; and 4) spectral clustering-based methods.
Among them, spectral clustering-based methods have ob-
tained state-of-the-art results, including sparse subspace clus-
tering (SSC) [9], and low rank representation (LRR) [21].

They perform subspace clustering in two steps: ﬁrst, learn-
ing an aﬃnity matrix that encodes the subspace member-
ship information, and then applying spectral clustering al-
gorithms [33, 28] to the learned aﬃnity matrix to obtain
the ﬁnal clustering results. Their main diﬀerence is how to
obtain a good aﬃnity matrix.

SSC assumes that each data point can be represented as
a sparse linear combination of other points. The popular
l1-norm heuristic is used to capture the sparsity. It enjoys
great performance for face clustering and motion segmenta-
tion data. Now we have a good theoretical understanding
about SSC. For instance, [8] shows that disjoint subspaces
can be exactly recovered under certain conditions; geometric
analysis of SSC [34] signiﬁcantly broadens the scope of SSC
to intersecting subspaces. However, the data points are as-
sumed to be lying exactly in the subspace. This assumption
may be violated in the presence of corrupted data. [38] ex-
tends SSC by adding adversarial or random noise. However,
SSC’s solution might be too sparse, thus the aﬃnity graph
from a single subspace will not be a fully connected body
[27]. To address the above issue, another regularization term
is introduced to promote connectivity of the graph [9].

LRR also represents each data point as a linear combination
of other points. It is to ﬁnd the lowest rank representation
Z of all data points jointly, where the nuclear norm is used
as a common surrogate of the rank function. In the presence
of noise or outliers, LRR solves the following problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ (cid:107)E(cid:107)l

s.t. X = XZ + E,

(3)

where λ balances the eﬀects of the low rank representa-
tion and errors, X = [x1, · · · , xn] ∈ Rm×n is a set of m-
dimensional data vectors drawn from the union of k sub-
spaces {Si}k
i=1, and (cid:107)·(cid:107)l characterizes certain corruptions
E. For example, when E represents Gaussian noise, squared
n
(cid:80)
Frobenius norm (cid:107)E(cid:107)2
j=1

ij is used; when E denotes

m
(cid:80)
i=1

F =

E2

random corruptions, l1 norm (cid:107)E(cid:107)1 =

|Eij| is appro-

priate; when E indicates the sample-speciﬁc corruptions, l2,1

norm is adopted, where (cid:107)E(cid:107)2,1 =

E2

ij. The low

rank as well as sparsity requirement may help counteract
corruptions. A variant of LRR works even in the presence
of some arbitrarily large outliers [23]. However, LRR has
never been shown to succeed other than under strong “inde-
pendent subspace” condition [15].

m
(cid:80)
i=1

n
(cid:80)
j=1

(cid:115) m
(cid:80)
i=1

n
(cid:80)
j=1

In view of the issues with the nuclear norm mentioned in
the beginning, we propose the use of an arctangent func-
tion instead in this work. We demonstrate the enhanced
performance of the proposed algorithm on benchmark data
sets.

• More accurate rank approximation is proposed to ob-
tain the low-rank representation of high-dimensional
data.

• An eﬃcient optimization procedure is developed for
arctangent rank minimization (ARM) problem. Theo-
retical analysis shows that our algorithm converges to
a stationary point.

• The superiority of the proposed method to various
state-of-the-art subspace clustering algorithms is veri-
ﬁed with signiﬁcantly and consistently lower error rates
of ARM on popular datasets.

Figure 1: Comparison of approximation for rank 2.

2. SUBSPACE CLUSTERING BY ARM
In this work, we demonstrate the application of F (Z) =
f ◦ σ(Z), where f (x) = (cid:80)
i arctan(|xi|) for any x ∈ Rn
(for high dimensional data usually m >> n which is the
case we suppose in the paper), as a rank approximation of
matrix Z in subspace clustering setting. There are three
advantages of this approximation function. First, it approx-
imates rank(Z) much better than the nuclear norm does,
i.e., as σi ∈ [0, ∞], 2
π arctan(σi) ∈ [0, 1]. Figure 1 shows the
rank approximation value of the two approaches for rank 2
situation. We can clearly see that arctangent reﬂects the
real rank pretty well on a broad range of singular values.
Second, f is diﬀerentiable, concave and monotonically in-
creasing on [0, ∞]n, by deﬁning the gradient of f at 0 as
∂
= 1. Third, F is unitarily in-
∂xi
variant and f is absolutely symmetric, i.e., f (x) is invariant
under arbitrary permutation and sign changes of the compo-
nents of x. Based on these properties, we have the following
theorems, which is proved in Appendix A.

f (0) = limxi→0+

1
1+x2
i

Theorem 2.1. For µ > 0 and A ∈ Rm×n, the following

problem

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(4)

(5)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σA)V T .

1.2 Our Contributions
In summary, the main contributions of this paper are three-
fold:

2.1 Arctangent Rank Minimization
To demonstrate the eﬀectiveness of the arctangent rank ap-
proximation, we consider its application in the challenging

subspace clustering problem. We propose the following arc-
tangent rank minimization (ARM) problem:

min
Z,E

n
(cid:88)

i=1

min
Z,E,J

n
(cid:88)

i=1

arctan(σi(Z)) + λ (cid:107)E(cid:107)l s.t. X = XZ + E.

(6)

It is diﬃcult to solve (6) directly because the objective func-
tion is neither convex nor concave. We convert it to the
following equivalent problem:

arctan(σi(J)) + λ (cid:107)E(cid:107)l s.t.X = XZ + E, Z = J.

(7)
Now we resort to a type of augmented Lagrange multipliers
(ALM) [20] method to solve (7). For simplicity of notation,
we denote σi = σi(J) and σt
i = σi(J t). The corresponding
augmented Lagrangian function is:

Algorithm 1 Arctan Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0,
and ρ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Update Z by (10).
2: Solve (11).
3: Solve E by either (15), (16) or (17) according to l.
4: Update Y1 by (18) and Y2 by (19).
5: Update µ by µt+1 = ρµt.
UNTIL stopping criterion is met.

where ωk = ∂f (σk) is the gradient of f (·) at σk and the
SVD of Z t+1 − Y t
is U diag{σA}V T . Finally, it converges
2
µt
to a local optimal point σ∗. Then J t+1 = U diag{σ∗}V T .

n
(cid:88)

i=1

L(E, J, Y1, Y2, Z, µ) =

arctan(σi) + λ (cid:107)E(cid:107)l

+ T r(Y T
µ
2

1 (X − XZ − E)) + T r(Y T
F + (cid:107)J − Z(cid:107)2

((cid:107)X − XZ − E(cid:107)2

+

F ),

2 (J − Z))

For Et+1, we have the following subproblem:

(8)

Et+1 = arg min

λ (cid:107)E(cid:107)l +

(cid:107)X − XZ t+1 − E(cid:107)2
F

E

µt
2

+ T r[(Y t

1 )T (X − XZ t+1 − E)].

Depending on diﬀerent regularization strategies, we have dif-
ferent closed-form solutions. For squared Forbenius norm,
it is again a quadratic problem,

Et+1 =

1 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1 and l2,1 norm, we use the lemmas from Appendix B.
Let Q = X − XZ t+1 + Y t
µt , we can solve E element-wisely
as below:

1

(9)

(cid:26) Qij − λ

µt sgn(Qij),

Et+1

ij =

0,

if |Qij| < λ
µt ;
otherwise.

In the case of l2,1 norm, we have



(cid:107)Q:,i(cid:107)2

− λ
µt

Q:,i,

[Et+1]:,i =

(cid:107)Q:,i(cid:107)2



0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The update of Lagrange multipliers is:

Y t+1
1 = Y t
2 = Y t
Y t+1

1 + µt(X − XZ t+1 − Et+1),
2 + µt(J t+1 − Z t+1).

(14)

(15)

(16)

(17)

(18)

(19)

where µ > 0 is a penalty parameter and Y1, Y2 are La-
grangian multipliers. The variables E, J, and Z can be
updated alternatively, one at each step, while keeping the
other two ﬁxed. For the (t + 1)th iteration, the iterative
scheme is given as follows.

For Z t+1, by ﬁxing Et, J t, Y t

1 and Y t

2 , we have:

Z t+1 = arg min

T r[(Y t

1 )T (X − XZ − Et)]+

Z
2 )T (J t − Z)]+
T r[(Y t
µt
(cid:13)X − XZ − Et(cid:13)
((cid:13)
(cid:13)
2

2

F

+ (cid:13)

(cid:13)J t − Z(cid:13)
(cid:13)

2

F

).

It is evident that the objective function of (9) is a strongly
convex quadratic function which can be solved directly. By
setting the ﬁrst derivative of it to zero, we have:

Z t+1 = (I +X T X)−1[X T (X −Et)+J t+

X T Y t
1 + Y t
2
µt

], (10)

where I ∈ Rn×n is the identity matrix.

J t+1 = arg min

arctan(σi) +

(cid:107)J − (Z t+1 −

µt
2

1
µt Y t

2 )(cid:107)2
F .

n
(cid:88)

i=1

J

(11)
Then we can convert it to problem (5). The ﬁrst term in
(5) is concave while the second term convex in σ, so we can
apply diﬀerence of convex (DC) [13] (vector) optimization
method. A linear approximation is used at each iteration of
DC programing. At iteration k + 1,

σk+1 = arg min

(cid:104)wk, σ(cid:105) +

(cid:107)σ − σA(cid:107)2
2,

(12)

σ≥0

µt
2

whose closed-form solution is

σk+1 = (σA −

ωk
µt )+,

2.2 Afﬁnity Graph Construction
After obtaining optimal Z ∗, we can build the similarity
graph matrix W . As argued in [9], some postprocessing

(13)

Figure 2: Sample face images in Extended Yale B.

For J t+1, we have:

The procedure is outlined in Algorithm 1.

Algorithm 2 Subspace Clustering by ARM
Input: data matrix X, number of subspaces k.
Do
1: Obtain optimal Z ∗ by solving (7).
2: Compute the skinny SVD Z ∗ = U ΣV T .
3: Calculate (cid:101)U = U (Σ)1/2.
4: Construct the aﬃnity graph matrix W by (20).
5: Perform NCuts on W .

of the coeﬃcient matrix can improve the clustering perfor-
mance. Following the angular information based technique
of [21], we deﬁne (cid:101)U = U Σ1/2, where U and Σ are from the
skinny SV D of Z ∗ = U ΣV T . Inspired by [17], we deﬁne W
as follows:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2α,

mality condition,

(20)

where (cid:101)ui and (cid:101)uj denote the i-th and j-th columns of (cid:101)U , and
α ∈ N ∗ controls the sharpness of the aﬃnity between two
points. Increasing the power α enhances the separation abil-
ity in the presence of noise. However, an excessively large α
would break aﬃnities between points of the same group. In
order to compare with LRR1, we use α = 2 in our experi-
ments, then we have the same postprocessing procedure as
LRR. After obtaining W , we directly utilize a spectral clus-
tering algorithm NCuts [33] to cluster the samples. Algo-
rithm 2 summarizes the complete subspace clustering steps
of the proposed method.

Figure 3: Aﬃnity graph matrix W with ﬁve and
ten subjects.

3. CONVERGENCE ANALYSIS
Since the objective function (6) is nonconvex, it would not
be easy to prove the convergence in theory. In this paper,
we mathematically prove that our optimization algorithm
has at least a convergent subsequence which converges to an
accumulation point, and moreover, any accumulation point
of our algorithm is a stationary point. Although the ﬁnal so-
lution might be a local optimum, our results are superior to
the global optimal solution from convex approaches. Some
previous work also reports similar observations [39, 12, 42].

1As we conﬁrmed with an author of [21], the power 2 of its
equation (12) is a typo, which should be 4.

We will show the proof in the case of (cid:107)E(cid:107)1. Let’s ﬁrst re-
formulate our objective function:

G(J, Z, E) = F (J) + λ(cid:107)E(cid:107)1

s.t. Z = J, X = XZ + E,

L(J, Z, E, Y1, Y2, µ) = G(J, Z, E) + (cid:104)Y1, X − XZ − E(cid:105)

+ (cid:104)Y2, J − Z(cid:105) +

((cid:107)J − Z(cid:107)2

F + (cid:107)X − XZ − E(cid:107)2

F ).

µ
2

(21)

(22)

Lemma 3.1. The sequences of {Y t

1 } and {Y t

2 } are bounded.

Proof. J t+1 satisﬁes the ﬁrst-order necessary local opti-

∂J L (cid:0)J, Z t+1, Et, Y t
1 , Y t
(cid:18)

2 , µt(cid:1) |J t+1

=∂J F (J) |J t+1 + µt

J t+1 − Z t+1 +

(cid:19)

Y t
2
µt

(23)

=∂J F (J) |J t+1 + Y t+1
=0.

2

Let’s deﬁne θi =
cording to (41) in Appendix B,

1

1+(σi)2 if σi (cid:54)= 0; otherwise, it is 1. Ac-

U diag(θ)V T = ∂J F (J) |J t+1 ,

(24)

and 0 <

1
1+(σt+1
i
(23), we conclude that Y t+1

2

is bounded.

)2 ≤ 1, ∂J F (J) |J t+1 is bounded. From

Similarly, for Et+1

1 , Y t

2 , µt(cid:1) |Et+1

0 ∈ ∂EL (cid:0)J t+1, Z t+1, E, Y t
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 − Y t
− µt (cid:0)X − XZ t+1 − Et+1(cid:1)
= ∂Eλ((cid:107)E(cid:107)1)|Et+1 + Y t+1

1

1

.

(25)

Here ∂E denotes the subgradient operator [6]. Because ||E||1
is nonsmooth only at Eij = 0, we deﬁne [∂E(cid:107)E(cid:107)1]ij = 0 if
Eij = 0. Then 0 ≤ (cid:107)∂E(cid:107)E(cid:107)1(cid:107)2
F ≤ mn is bounded. There-
fore, {Y t+1

} is bounded.

1

Lemma 3.2. {J t}, {Et} and {Z t} are bounded if (cid:80) µt+1

(µt)2 <

∞, (cid:80) 1

µt < ∞ and X T X is invertible.

Proof.

2 , µt(cid:1)

1

1 , Y t

((cid:107)J t − Z t(cid:107)2

L (cid:0)J t, Z t, Et, Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt − µt−1
2
+ T r((Y t
+ T r((Y t
= L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1
2 − Y t−1
2

1 − Y t−1
1

1

, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)X − XZ t − Et(cid:107)2

F )

)(X − XZ t − Et))
)(J t − Z t))
, Y t−1
2

, µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

(26)

2 , µt(cid:1)
2 , µt)

L (cid:0)J t+1, Z t+1, Et+1, Y t
≤ L(J t+1, Z t+1, Et, Y t
1 , Y t
≤ L(J t, Z t+1, Et, Y t
≤ L (cid:0)J t, Z t, Et, Y t
1 , Y t
≤ L (cid:0)J t, Z t, Et, Y t−1
µt + µt−1
2(µt−1)2 ((cid:107)Y t

1 − Y t−1
1

1 , Y t
1 , Y t
2 , µt)
2 , µt(cid:1)
, Y t−1
2

1

µt−1(cid:1) +

F + (cid:107)Y t
(cid:107)2

2 − Y t−1
2

(cid:107)2
F ).

Iterating the inequality (27) gives that

Under the given conditions on {µt}, both terms on the
right-hand side of the above inequality are bounded, thus
L (cid:0)J t+1, Z t+1, Et+1, Y t

1 , Y t

2 , µt(cid:1) is bounded. In addition,
2 , µt(cid:1) +

1 , Y t

L(J t+1, Z t+1, Et+1, Y t
≤ L (cid:0)J 1, Z 1, E1, Y 0
1 , Y 0

2 , µt)
1 , Y t
2 , µ0(cid:1) +

t
(cid:88)

µi + µi−1
2(µi−1)2 ((cid:107)Y i
2 − Y i−1
2

(cid:107)2
F ).

i=1
+ (cid:107)Y i

1 − Y i−1
1

(cid:107)2
F

F )

2 (cid:107)2

1 (cid:107)2

F + (cid:107)Y t

L (cid:0)J t+1, Z t+1, Et+1, Y t
1
2µt ((cid:107)Y t
= F (J t+1) + λ(cid:107)Et+1(cid:107)1+
µt
2
µt
2

Y t
µt (cid:107)2
2
(cid:107)X − Et+1 − XZ t+1 +

(cid:107)J t+1 − Z t+1 +

F +

Y t
µt (cid:107)2
1
F .

(28)

(29)

The left-hand side in the above equation is bounded and
each term on the right-hand side is nonnegative, so each
term is bounded. Therefore, Et+1 is bounded. XZ t+1 is
bounded according to the last term on the right-hand side
of (29), and thus after multiplying a constant matrix X T , we
have X T XZ t+1 is bounded. Under the condition that X T X
is invertible, by multiplying a constant matrix (X T X)−1,
we have that (X T X)−1X T XZ t+1 = Z t+1 is bounded. Fi-
nally, J t+1 is bounded because the second to the last term is
bounded. Therefore, {J t}, {Et} and {Z t} are bounded.

1 , Y t

Theorem 3.1. The sequence {J t, Et, Z t, Y t

2 } gener-
ated by Algorithm 1 has at least one accumulation point, un-
der the conditions that (cid:80) µt+1
µt < ∞ and X T X
is invertible. For any accumulation point {J ∗, E∗, Z ∗, Y ∗
1 , Y ∗
{J ∗, E∗, Z ∗} is a stationary point of optimization problem
(21), under the conditions that µt(J t+1 − J t) → 0, and
µt(Et+1 − Et) → 0.

(µt)2 < ∞, (cid:80) 1

2 },

1 , Y t

Proof. Based on the conditions on the penalty parame-
ter sequence {µt} and X T X, Algorithm 1 generates a bounded
sequence {J t, Et, Z t, Y t
2 } by Lemma (3.1) and (3.2) . By
the Bolzano-Weierstrass theorem, at least one accumulation
point exists, e.g., {J ∗, E∗, Z ∗, Y ∗
2 }. Without loss of gen-
1 , Y t
erality, we assume that {J t, Et, Z t, Y t
2 } itself converges
to {J ∗, E∗, Z ∗, Y ∗
2 }. As shown below, {J ∗, E∗, Z ∗} is
1 , Y ∗
a stationary point of problem (21), under additional condi-
tions that µt(Et+1 − Et) → 0 and µt(J t+1 − J t) → 0.

1 , Y ∗

Since J t −Z t = Y t
Therefore, J ∗ = Z ∗.

2 −Y t−1
2
µt−1

and µt → ∞, we have J t −Z t → 0.

(27)

Similarly, by X − XZ t − Et = Y t
X − XZ ∗.

1 −Y t−1
1
µt−1

, we have E∗ =

For Z t+1, the ﬁrst-order optimality condition is

1 , Y t

2 − X T Y t

2 − X T Y t

2 , µt(cid:1) |Zt+1

1 − µt (cid:0)J t − Z t+1(cid:1)

∇Z L (cid:0)J t, Z, Et, Y t
= −Y t
− µtX T (cid:0)X − Et − XZ t+1(cid:1)
= −Y t
− µtX T (cid:0)X − Et+1 − XZ t+1 − Et + Et+1(cid:1)
2 + µt (cid:0)J t+1 − J t(cid:1) − X T Y t+1
= −Y t+1
− µtX T (cid:0)Et+1 − Et(cid:1)
= 0.

1

1 − µt (cid:0)J t+1 − Z t+1 + J t − J t+1(cid:1)

2 + Y t

1 → 0, i.e., −X T Y ∗

If µt(J t+1 − J t) → 0 and µt(Et+1 − Et) → 0, we have
X T Y t
1 . It is easy to verify
∂EL (J, Z, E, Y1, Y2, µ) |E∗ = 0. Therefore, {J ∗, E∗, Z ∗, Y ∗
satisﬁes the KKT conditions of L(J, E, Z, Y1, Y2) and thus
{J ∗, E∗, Z ∗} is a stationary point of (21).

2 = Y ∗

1 , Y ∗
2 }

4. EXPERIMENTS
This section presents experiments with the proposed algo-
rithm on the Extended Yale B (EYaleB) [18] and Hopkins
155 databases [35]. They are standard tests for robust sub-
space clustering algorithms. As shown in [9], the challenge
in the Hopkins 155 dataset is due to the small principal an-
gles between subspaces. For EYaleB, the challenge lies in the
small principal angles and another factor that data points
from diﬀerent subspaces are close. Our results are compared
with several state-of-the-art subspace clustering algorithms,
including LRR [21], SSC [9], LRSC [10, 37], spectral curva-
ture clustering (SCC) [5], and local subspace aﬃnity (LSA)
[40], in terms of misclassiﬁcation rate2. For fair compari-
son, we follow the experimental setup in [9] and obtain the
results.

As other methods do, we tune our parameters to obtain the
In general, the value of λ depends on prior
best results.
If the noise is
knowledge of the noise level of the data.
heavy, a small λ should be adopted. µ0 and ρ aﬀect the
convergence speed. The larger their values are, the fewer
iterations are required for the algorithm to converge, but
meanwhile we may lose some precision of the ﬁnal objective
In the literature, the value of ρ is often
function value.
chosen between 1 and 1.1. The iteration stops at a relative
normed diﬀerence of 10−5 between two successive iterations,
or a maximum of 150 iterations.

4.1 Face Clustering
Face clustering refers to partitioning a set of face images
from multiple individuals to multiple subspaces according to
the identity of each individual. The face images are heavily
contaminated by sparse gross errors due to varying lighting
conditions, as shown in Figure 2. Therefore, (cid:107)E(cid:107)1 is used to
2The implementation of our algorithm is available at:
https://github.com/sckangz/arctangent.

model the errors in our experiment. The EYaleB database
contains cropped face images of 38 individuals taken under
64 diﬀerent illumination conditions. The 38 subjects were
divided into four groups as follows: subjects 1 to 10, 11 to 20,
21 to 30, and 31 to 38. All choices of n ∈ {2, 3, 5, 8, 10} are
considered for each of the ﬁrst three groups, and all choices of
n ∈ {2, 3, 5, 8} are considered for the last group. As a result,
there are {163, 416, 812, 136, 3} combinations corresponding
to diﬀerent n. Each image is downsampled to 48 × 42 and is
vectorized to a 2016-dimensional vector. λ = 10−5, µ0 = 1.7
and γ = 1.03 are used in this experiment.

addition, the error of LSA is large maybe because LSA is
based on MSE. Since the MSE is quite sensitive to outliers,
LSA will fail to deal with large outliers.

Figure 5: Convergence curve of the objective func-
tion value in (6).

Figure 4: Recovery results of two face images. The
three columns from left to right are the original im-
age (X), the error matrix (E) and the recovered im-
age (XZ), respectively.

Table 1: Clustering error rates (%) on the EYaleB
database.

Algorithm
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

LRR

SSC

LSA LRSC SCC ARM

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

16.62
7.82

38.16
39.06

58.02
56.87

12.24
11.25

58.90
59.38

59.19
58.59

23.72
28.03

66.11
64.65

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

73.02
75.78

1.51
0.78

2.26
1.56

3.06
2.50

3.70
3.32

3.85
2.97

Table 1 provides the best performance of each method. As
shown in the table, our proposed method has the lowest
mean clustering error rates in all ﬁve settings. In particu-
lar, in the most challenging case of 10 subjects, the mean
clustering error rate is as low as 3.85%. The improvement
is signiﬁcant compared with other low rank representation
based subspace clustering, i.e., LRR and LRSC. For exam-
ple, 19% and 11% improvement over LRR can be observed
in the cases of 10 and 8 subjects, respectively. This demon-
strates the importance of accurate rank approximation. In

Figure 6: Average computational time (sec) of the
algorithms on the EYaleB database as a function of
the number of subjects.

Figure 3 shows the obtained aﬃnity graph matrix W for the
ﬁve and ten subjects scenarios. We can see a distinct block-
diagonal structure, which means that each cluster becomes
highly compact and diﬀerent subjects are well separated.

In Figure 4, we present the recovery results of some sam-
ple faces from the 10-subject clustering case. We can see
that the proposed algorithm has the beneﬁt of removing the
corruptions in data.

Figure 5 plots the progress of objective function values of
(6). It is observed that with more iterations, the value of
objective function decreases monotonically. This empirically
veriﬁes the convergence of our optimization method.

rithms in mean error rate. Especially, its all mean error rates
are around 1.5%. This again demonstrates the eﬀectiveness
of using arctangent as a rank approximation.

Figure 8: The inﬂuence of parameter λ of ARM on
clustering error of Hopkins 155 database.

Figure 8 shows the culstering error rate of ARM for diﬀerent
λ over all 155 sequences. When λ is between 1 and 3, the
clustering error rate varies between 1.48% and 2.19%. This
demonstrates that ARM performs well under a pretty wide
range of values of λ. This is another advantage of ARM over
LRR [21].

5. CONCLUSION
In this work, we propose to use arctangent as a concave
rank approximation function.
It has some nice properties
compared with the standard nuclear norm. We apply this
function to the low rank representation-based subspace clus-
tering problem and develop an iterative algorithm for opti-
mizing the associated objective function. Extensive experi-
mental results demonstrate that, compared to many state-
of-the-art algorithms, the proposed algorithm gives the low-
est clustering error rates on many benchmark datasets. This
fully demonstrates the signiﬁcance of accurate rank approx-
imation. Interesting future work includes other applications
of the arctangent rank approximation; for example, matrix
completion. Since LRR can only ensure its validity for in-
dependent subspace segmentation, it is worthwhile to inves-
tigate somewhat dependent yet possibly disjoint subspace
clustering.

Figure 7: Example frames from two video sequences
of the Hopkins 155 database with traced feature
points.

We compare the average computational time of LRR, SSC,
and ARM as a function of the number of subjects in Figure 6.
All the experiments are conducted and timed on the same
machine with an Intel Xeon E3-1240 3.40GHz CPU that
has 4 cores and 8GB memory, running Ubuntu and Matlab
(R2014a). We can observe that the computational time of
SSC is higher than LRR and ARM, while ARM is a little
slower than LRR in most cases.

4.2 Motion Segmentation

Table 2: Segmentation error rates (%) on the Hop-
kins 155 Dataset.

4.23
0.56

1.52
0.00

2.13
0.00

Algorithm LRR SSC LSA LRSC SCC ARM
2 Motions
Mean
Median
3 Motions
Mean
Median
All
Mean
Median

8.25
0.24

2.89
0.00

1.48
0.00

4.40
0.56

4.59
0.60

3.69
0.29

7.69
3.80

4.86
0.89

4.03
1.43

7.02
1.45

1.49
0.84

2.56
0.00

4.10
0.00

2.18
0.00

1.48
0.00

Motion segmentation involves segmenting a video sequence
of multiple moving objects into multiple spatiotemporal re-
gions corresponding to diﬀerent motions. These motion se-
quences can be divided into three main categories: checker-
board, traﬃc, and articulated or non-rigid motion sequences.
The Hopkins 155 dataset includes 155 video sequences of 2
or 3 motions, corresponding to 2 or 3 low-dimensional sub-
spaces of the ambient space. Each sequence represents a
data set and so there are 155 motion segmentation prob-
lems in total. Several example frames are shown in Figure 7.
The trajectories are extracted automatically by a tracker, so
they are slightly corrupted by noise. As in [21, 22], (cid:107)E(cid:107)2,1
is adopted in the model. In this experiment, λ = 2, µ0 = 10
and γ = 1.05.

We use the original 2F-dimensional feature trajectories in
our experiment. We show the clustering error rates of dif-
ferent algorithms in Table 2. ARM outperforms other algo-

6. ACKNOWLEDGMENTS
This work is supported by US National Science Foundation
Grants IIS 1218712. The corresponding author is Qiang
Cheng.

7. REFERENCES
[1] A. Beck and M. Teboulle. A fast iterative

shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences,
2(1):183–202, 2009.

[2] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value
thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2010.
[3] E. J. Cand`es and B. Recht. Exact matrix completion

via convex optimization. Foundations of
Computational mathematics, 9(6):717–772, 2009.

[4] E. J. Cand`es and T. Tao. The power of convex
relaxation: Near-optimal matrix completion.
Information Theory, IEEE Transactions on,
56(5):2053–2080, 2010.

[5] G. Chen and G. Lerman. Spectral curvature clustering

(scc). International Journal of Computer Vision,
81(3):317–330, 2009.

[6] F. H. Clarke. Optimization and nonsmooth analysis,

volume 5. Siam, 1990.

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering.
In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 2790–2797.
IEEE, 2009.

[8] E. Elhamifar and R. Vidal. Clustering disjoint

subspaces via sparse representation. In Acoustics
Speech and Signal Processing (ICASSP), 2010 IEEE
International Conference on, pages 1926–1929. IEEE,
2010.

[9] E. Elhamifar and R. Vidal. Sparse subspace clustering:
Algorithm, theory, and applications. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
35(11):2765–2781, 2013.

[10] P. Favaro, R. Vidal, and A. Ravichandran. A closed
form solution to robust subspace estimation and
clustering. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on,
pages 1801–1807. IEEE, 2011.

[11] M. Fazel. Matrix rank minimization with applications.
PhD thesis, PhD thesis, Stanford University, 2002.

[12] P. Gong, J. Ye, and C.-s. Zhang. Multi-stage

multi-task feature learning. In Advances in Neural
Information Processing Systems, pages 1988–1996,
2012.

[13] R. Horst and N. V. Thoai. Dc programming:

overview. Journal of Optimization Theory and
Applications, 103(1):1–43, 1999.

[14] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and

accurate matrix completion via truncated nuclear
norm regularization. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(9):2117–2130,
2013.

[15] K. Kanatani. Motion segmentation by subspace

separation and model selection. In Computer Vision,
2001. ICCV 2001. Proceedings. Eighth IEEE
International Conference on, volume 2, pages 586–591,
2001.

[16] Z. Kang, C. Peng, J. Cheng, and Q. Cheng. Logdet
rank minimization with application to subspace
clustering. Computational Intelligence and
Neuroscience, 2015, 2015.

[17] F. Lauer and C. Schnorr. Spectral clustering of linear
subspaces for motion segmentation. In Computer
Vision, 2009 IEEE 12th International Conference on,
pages 678–685. IEEE, 2009.

[18] K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear
subspaces for face recognition under variable lighting.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 27(5):684–698, 2005.

[19] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of

singular values. part i: Theory. Set-Valued Analysis,
13(3):213–241, 2005.

[20] Z. Lin, R. Liu, and Z. Su. Linearized alternating

direction method with adaptive penalty for low-rank
representation. In Advances in neural information
processing systems, pages 612–620, 2011.

[21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma.

Robust recovery of subspace structures by low-rank
representation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 35(1):171–184,
2013.

[22] G. Liu, Z. Lin, and Y. Yu. Robust subspace
segmentation by low-rank representation. In
Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 663–670, 2010.

[23] G. Liu, H. Xu, and S. Yan. Exact subspace

segmentation and outlier detection by low-rank
representation. In AISTATS, pages 703–711, 2012.
[24] C. Lu, J. Tang, S. Y. Yan, and Z. Lin. Generalized

nonconvex nonsmooth low-rank minimization. In
IEEE International Conference on Computer Vision
and Pattern Recognition. IEEE, 2014.

[25] Y. Ma, H. Derksen, W. Hong, and J. Wright.

Segmentation of multivariate mixed data via lossy
data coding and compression. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
29(9):1546–1562, 2007.

[26] K. Mohan and M. Fazel. Iterative reweighted

algorithms for matrix rank minimization. The Journal
of Machine Learning Research, 13(1):3441–3473, 2012.

[27] B. Nasihatkon and R. Hartley. Graph connectivity in

sparse subspace clustering. In Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference
on, pages 2137–2144. IEEE, 2011.

[28] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral
clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849–856,
2002.

[29] F. Nie, H. Huang, and C. H. Ding. Low-rank matrix

recovery via eﬃcient schatten p-norm minimization. In
AAAI, 2012.

[30] S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion

segmentation in the presence of outlying, incomplete,
or corrupted trajectories. Pattern Analysis and
Machine Intelligence, IEEE Transactions on,
32(10):1832–1845, 2010.

[31] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed

minimum-rank solutions of linear matrix equations via

same singular values as Z, we have

F (Z) +

(cid:107)Z − A(cid:107)2
F

µ
2

(cid:107)X − ΣA(cid:107)2
F ,

(cid:0)(cid:107)X(cid:107)2

= F (X) +

(cid:0)(cid:107)ΣX (cid:107)2

= F (ΣX ) +

≥ F (ΣX ) +

µ
2
= F (ΣX ) +

(cid:107)X − ΣA(cid:107)2
F ,
µ
2
µ
2
µ
2
µ
2
µ
= F (ΣZ ) +
2
µ
(cid:107)σ − σA(cid:107)2
2,
2
µ
≥ f (σ∗) +
2

(cid:107)σ∗ − σA(cid:107)2
2.

= F (ΣX ) +

= f (σ) +

(cid:107)ΣZ − ΣA(cid:107)2
F ,

(cid:107)ΣX − ΣA(cid:107)2
F ,

F + (cid:107)ΣA(cid:107)2

F + (cid:107)ΣA(cid:107)2

F − 2 (cid:104)X, ΣA(cid:105)(cid:1) ,
F − 2 (cid:104)ΣX , ΣA(cid:105)(cid:1) ,

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

In the above, (33) holds because the Frobenius norm is uni-
tarily invariant; (34) holds because F (X) is unitarily invari-
ant; (36) is true by von Neumann’s trace inequality; and
(38) holds because of the deﬁnition of X. Therefore, (38)
is a lower bound of (32). Note that the equality in (36) is
attained if X = ΣX . Because ΣZ = ΣX = X = U T ZV , the
SVD of Z is Z = U ΣZ V T . By minimizing (39), we get σ∗.
Therefore, eventually we get Z ∗ = U diag(σ∗)V T , which is
the minimizer of problem (30).

nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

[32] B. Recht, W. Xu, and B. Hassibi. Null space

conditions and thresholds for rank minimization.
Mathematical programming, 127(1):175–202, 2011.

[33] J. Shi and J. Malik. Normalized cuts and image
segmentation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 22(8):888–905,
2000.

[34] M. Soltanolkotabi, E. J. Candes, et al. A geometric

analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
[35] R. Tron and R. Vidal. A benchmark for the

comparison of 3-d motion segmentation algorithms. In
Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE,
2007.

[36] R. Vidal. A tutorial on subspace clustering. IEEE
Signal Processing Magazine, 28(2):52–68, 2010.

[37] R. Vidal and P. Favaro. Low rank subspace clustering
(lrsc). Pattern Recognition Letters, 43:47–61, 2014.

[38] Y.-X. Wang and H. Xu. Noisy sparse subspace

clustering. In Proceedings of The 30th International
Conference on Machine Learning, pages 89–97, 2013.
[39] S. Xiang, X. Tong, and J. Ye. Eﬃcient sparse group
feature selection via nonconvex optimization. In
Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 284–292, 2013.

[40] J. Yan and M. Pollefeys. A general framework for

motion segmentation: Independent, articulated, rigid,
non-rigid, degenerate and non-degenerate. In
Computer Vision–ECCV 2006, pages 94–106.
Springer, 2006.

[41] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast

algorithm for edge-preserving variational multichannel
image restoration. SIAM Journal on Imaging
Sciences, 2(2):569–592, 2009.

[42] Z. Zhang and B. Tu. Nonconvex penalization using
laplace exponents and concave conjugates. In
Advances in Neural Information Processing Systems,
pages 611–619, 2012.

[43] Y.-B. Zhao. An approximation theory of matrix rank

minimization and its application to quadratic
equations. Linear Algebra and its Applications,
437(1):77–93, 2012.

APPENDIX
A. PROOF

problem

Theorem A.1. For µ > 0 and A ∈ Rm×n, the following

Z ∗ = arg min

F (Z) +

(cid:107)Z − A(cid:107)2
F

(30)

Z

µ
2

is solved by the vector minimization
µ
2

σ∗ = arg min

f (σ) +

σ≥0

(cid:107)σ − σA(cid:107)2
2,

(31)

so that Z ∗ = U diag(σ∗)V T with the SVD of A being
U diag(σ∗

A)V T .

Proof. Let A = U ΣAV T be the skinny SVD of A, then
ΣA = U T AV . Denoting X = U T ZV which has exactly the

B. THEOREM AND LEMMAS

Theorem B.1. [19] Suppose F : Rm×n → R is repre-
sented as F (X) = f ◦ σ(X), and f : Rn → R is absolutely
symmetric and diﬀerentiable, where X ∈ Rm×n with SVD
X = U diag(σ)V T , the gradient of F (X) at X is

∂F (X)
∂X

= U diag(θ)V T ,

(41)

where θ = ∂f (y)

∂y |y=σ(X).

Lemma B.1. [1] For µ > 0, and K ∈ Rs×t, the solution

of the problem

min
L

µ(cid:107)L(cid:107)1 +

(cid:107)L − K(cid:107)2
F ,

1
2

is given by Lµ(K), which is deﬁned component-wisely by

[Lµ(K)]ij = max{|Kij| − µ, 0} · sign(Kij).

Lemma B.2. [41] Let H be a given matrix. If the optimal

solution to

min
W

α (cid:107)W (cid:107)2,1 +

1
2

(cid:107)W − H(cid:107)2
F

is W ∗, then the i-th column of W ∗ is

[W ∗]:,i =






0,

−α

(cid:107)H:,i(cid:107)2
(cid:107)H:,i(cid:107)2

H:,i,

if (cid:107)H:,i(cid:107)2 > α;
otherwise.

