AGA : Attribute-Guided Augmentation

Mandar Dixit
UC San Diego
mdixit@ucsd.edu

Roland Kwitt
University of Salzburg
rkwitt@gmx.at

Marc Niethammer
UNC Chapel Hill
mn@cs.unc.edu

Nuno Vasconcelos
UC San Diego
nvasconcelos@ucsd.edu

Abstract

We consider the problem of data augmentation, i.e., gen-
erating artiﬁcial samples to extend a given corpus of train-
ing data. Speciﬁcally, we propose attributed-guided aug-
mentation (AGA) which learns a mapping that allows syn-
thesis of data such that an attribute of a synthesized sample
is at a desired value or strength. This is particularly inter-
esting in situations where little data with no attribute an-
notation is available for learning, but we have access to an
external corpus of heavily annotated samples. While prior
works primarily augment in the space of images, we pro-
pose to perform augmentation in feature space instead. We
implement our approach as a deep encoder-decoder archi-
tecture that learns the synthesis function in an end-to-end
manner. We demonstrate the utility of our approach on the
problems of (1) one-shot object recognition in a transfer-
learning setting where we have no prior knowledge of the
new classes, as well as (2) object-based one-shot scene
recognition. As external data, we leverage 3D depth and
pose information from the SUN RGB-D dataset. Our ex-
periments show that attribute-guided augmentation of high-
level CNN features considerably improves one-shot recog-
nition performance on both problems.

1. Introduction

Convolutional neural networks (CNNs), trained on large
scale data, have signiﬁcantly advanced the state-of-the-
art on traditional vision problems such as object recogni-
tion [20, 30, 34] and object detection [14, 27]. Success
of these networks is mainly due to their high selectivity
for semantically meaningful visual concepts, e.g., objects
and object parts [29]. In addition to ensuring good perfor-
mance on the problem of interest, this property of CNNs
also allows for transfer of knowledge to several other vi-
sion tasks [9, 15, 6, 8]. The object recognition network
of [20], e.g., has been successfully used for object detec-
tion [14, 27], scene classiﬁcation [15, 8], texture classiﬁca-
tion [6] and domain adaptation [9], using various transfer
mechanisms.

Tables with depth in the range of 1-2 [m]

Tables

Training
data

Chairs

learn φ3
(by using γ)

[1,2]

γ(ˆx) ≈ 3[m]

x 7→ ˆx = φ3

[1,2](x)

x

γ(x) = 1.3 [m]

Instance x from a
new class

Training
data
(e.g., RCNN features)

X ⊂ RD

γ ... Attribute (strength) predictor
(trained separately)

Chairs with depth in the range of 1-2 [m]

Figure 1: Given a predictor γ : X → R+ of some object attribute (e.g.,
depth or pose), we propose to learn a mapping of object features x ∈ X ,
such that (1) the new synthetic feature ˆx is “close” to x (to preserve object
identity) and (2) the predicted attribute value γ(ˆx) = ˆt of ˆx matches a
desired object attribute value t, i.e., t − ˆt is small.
In this illustration,
we learn a mapping for features with associated depth values in the range
of 1-2 [m] to t = 3 [m] and apply this mapping to an instance of a new
object class. In our approach, this mapping is learned in an object-agnostic
manner. With respect to our example, this means that all training data from
‘chairs’ and ‘tables’ is used to a learn feature synthesis function φ.

CNN-based transfer is generally achieved either by ﬁne-
tuning a pre-trained network, such as in [20], on a new im-
age dataset or by designing a new image representation on
such a dataset based on the activations of the pre-trained
network layers [9, 15, 8, 6]. Recent proposals of trans-
fer have shown highly competitive performance on differ-
ent predictive tasks with a modest amount of new data (as
few as 50 images per class). The effectiveness of transfer-
based methods, however, has not yet been tested under more
severe constraints such as in a few-shot or a one-shot learn-
ing scenario. In these problems, the number of examples
available for learning may be as few as one per class. Fine-
tuning a pre-trained CNN with millions of parameters to

17455

such inadequate datasets is clearly not a viable option. A
one-shot classiﬁer trained on CNN activations will also be
prone to over-ﬁtting due to the high dimensionality of the
feature space. The only way to solve the problem of limited
data is to augment the training corpus by obtaining more
examples for the given classes.

While augmentation techniques can be as simple as ﬂip-
ping, rotating, adding noise, or extracting random crops
from images [20, 5, 37], task-speciﬁc, or guided augmen-
tation strategies [4, 16, 28, 25] have the potential to gen-
erate more realistic synthetic samples. This is a particu-
larly important issue, since performance of CNNs heavily
relies on sufﬁcient coverage of the variability that we ex-
pect in unseen testing data. In scene recognition, we desire,
for example, sufﬁcient variability in the constellation and
transient states of scene categories (c.f . [21]), whereas in
object recognition, we desire variability in the speciﬁc in-
carnations of certain objects, lighting conditions, pose, or
depth, just to name a few. Unfortunately, this variability
is often dataset-speciﬁc and can cause substantial bias in
recognition results [35].

An important observation in the context of our work is
that augmentation is typically performed on the image, or
video level. While this is not a problem with simple tech-
niques, such as ﬂipping or cropping, it can become compu-
tationally expensive if more elaborate augmentation tech-
niques are used. We argue that, in speciﬁc problem settings,
augmentation might as well be performed in feature space,
especially in situations where features are input to subse-
quent learning steps. This is common, e.g., in recognition
tasks, where the softmax output of trained CNNs is often
not used directly, but activations at earlier layers are input
to an external discriminant classiﬁer.
Contribution. We propose an approach to augment the
training set with feature descriptors instead of images.
Speciﬁcally, we advocate an augmentation technique that
learns to synthesize features, guided by desired values for a
set of object attributes, such as depth or pose. An illustra-
tion of this concept is shown in Fig. 1. We ﬁrst train a fast
RCNN [14] detector to identify objects in 2D images. This
is followed by training a neural network regressor which
predicts the 3D attributes of a detected object, namely its
depth from the camera plane and pose. An encoder-decoder
network is then trained which, for a detected object at a cer-
tain depth and pose, will “hallucinate” the changes in its
RCNN features for a set of desired depths/poses. Using this
architecture, for a new image, we are able to augment ex-
isting feature descriptors by an auxiliary set of features that
correspond to the object changing its 3D position. Since our
framework relies on object attributes to guide augmentation,
we refer to it as attribute-guided augmentation (AGA).
Organization. Sec. 2 reviews prior work. Sec. 3 introduces
the proposed encoder-decoder architecture for attribute-

guided augmentation. Sec. 4 studies the building blocks of
this approach in detail and demonstrates that AGA in fea-
ture space improves one-shot object recognition and object-
based scene recognition performance on previously unseen
classes. Sec. 5 concludes the paper with a discussion and an
outlook on potential future directions.

2. Related work

Our review of related work primarily focuses on data
augmentation strategies. While many techniques have been
proposed in the context of training deep neural networks
to avoid over-ﬁtting and to increase variability in the data,
other (sometimes closely related) techniques have previ-
ously appeared in the context of one-shot and transfer learn-
ing. We can roughly group existing techniques into (1)
generic, computationally cheap approaches and (2) task-
speciﬁc, or guided approaches that are typically more com-
putationally involved.

As a representative of the ﬁrst group, Krizhevsky et al.
[20] leverage a set of label-preserving transformations, such
as patch extraction + reﬂections, and PCA-based intensity
transformations, to increase training sample size. Simi-
lar techniques are used by Zeiler and Fergus [37]. In [5],
Chatﬁeld and Zisserman demonstrate that the augmentation
techniques of [20] are not only beneﬁcial for training deep
architectures, but shallow learning approaches equally ben-
eﬁt from such simple and generic schemes.

In the second category of guided-augmentation tech-
niques, many approaches have recently been proposed.
In [4], e.g., Charalambous and Bharath employ guided-
augmentation in the context of gait recognition. The au-
thors suggest to simulate synthetic gait video data (obtained
from avatars) with respect to various confounding factors
to extend the training cor-
(such as clothing, hair, etc.)
pus. Similar in spirit, Rogez and Schmid [28] propose an
image-based synthesis engine for augmenting existing 2D
human pose data by photorealistic images with greater pose
variability. This is done by leveraging 3D motion capture
(MoCap) data. In [25], Peng et al. also use 3D data, in the
form of CAD models, to render synthetic images of objects
(with varying pose, texture, background) that are then used
It is shown that syn-
to train CNNs for object detection.
thetic data is beneﬁcial, especially in situations where few
(or no) training instances are available, but 3D CAD mod-
els are. Su et al. [33] follow a similar pipeline of rendering
images from 3D models for viewpoint estimation, however,
with substantially more synthetic data obtained, e.g., by de-
forming existing 3D models before rendering.

Another (data-driven) guided augmentation technique is
introduced by Hauberg et al. [16]. The authors propose to
learn class-speciﬁc transformations from external training
data, instead of manually specifying transformations as in
[20, 37, 5]. The learned transformations are then applied to

7456

the samples of each class. Speciﬁcally, diffeomorphisms are
learned from data and encouraging results are demonstrated
in the context of digit recognition on MNIST. Notably, this
strategy is conceptually similar to earlier work by Miller
et al. [23] on one-shot learning, where the authors synthe-
size additional data for digit images via an iterative process,
called congealing. During that process, external images of
a given category are aligned by optimizing over a class of
geometric transforms (e.g., afﬁne transforms). These trans-
formations are then applied to single instances of the new
classes to increase data for one-shot learning.

Marginally related to our work, we remark that alterna-
tive approaches to implicitly learn spatial transformations
have been proposed. For instance, Jaderberg et al. [18] in-
troduce spatial transformer modules that can be injected
into existing deep architectures to implicitly capture spa-
tial transformations inherent in the data, thereby improving
invariance to this class of transformations.

While all previously discussed methods essentially pro-
pose image-level augmentation to train CNNs, our approach
is different in that we perform augmentation in feature
space. Along these lines, the approach of Kwitt et al.
[21] is conceptually similar to our work. In detail, the au-
thors suggest to learn how features change as a function of
the strength of certain transient attributes (such as sunny,
cloudy, or foggy) in a scene-recognition context. These
models are then transferred to previously unseen data for
one-shot recognition. There are, however, two key differ-
ences between their approach and ours. First, they require
datasets labeled with attribute trajectories, i.e., all varia-
tions of an attribute for every instance of a class. We, on the
other hand, make use of conventional datasets that seldom
carry such extensive labeling. Second, their augmenters
are simple linear regressors trained in a scene-class speciﬁc
manner. In contrast, we learn deep non-linear models in a
class-agnostic manner which enables a straightforward ap-
plication to recognition in transfer settings.

3. Architecture

Notation. To describe our architecture, we let X denote
our feature space, x ∈ X ⊂ RD denotes a feature descrip-
tor (e.g., a representation of an object) and A denotes a set
of attributes that are available for objects in the external
training corpus. Further, we let s ∈ R+ denote the value
of an attribute A ∈ A, associated with x. We assume (1)
that this attribute can be predicted by an attribute regressor
γ : X → R+ and (2) that its range can be divided into
I intervals [li, hi], where li, hi denote the lower and upper
bounds of the i-th interval. The set of desired object at-
tribute values is {t1, . . . , tT }.
Objective. On a conceptual level, we aim for a synthesis
function φ which, given a desired value t for some object
attribute A, transforms the object features x ∈ X such that

the attribute strength changes in a controlled manner to t.
More formally, we aim to learn

φ : X × R+ → X , (x, t) 7→ ˆx,

s.t. γ(ˆx) ≈ t .

(1)

Since, the formulation in Eq. (1) is overly generic, we con-
strain the problem to the case where we learn different φk
i
for a selection of intervals [li, hi] within the range of at-
tribute A and a selection of T desired object attribute val-
ues tk. In our illustration of Fig. 1, e.g., we have one in-
terval [l, h] = [1, 2] and one attribute (depth) with target
value 3[m]. While learning separate synthesis functions
simpliﬁes the problem, it requires a good a-priori attribute
(strength) predictor, since, otherwise, we could not decide
which φk
i to use. During testing, we (1) predict the object’s
attribute value from its original feature x, i.e., γ(x) = ˆt,
i (x)
and then (2) synthesize additional features as ˆx = φk
for k = 1, . . . , T . If ˆt ∈ [li, hi] ∧ tk /∈ [li, hi], φk
i is used.
Next, we discuss each component of this approach in detail.

3.1. Attribute regression

An essential part of our architecture is the attribute re-
gressor γ : X → R+ for a given attribute A. This regressor
takes as input a feature x and predicts its strength or value,
i.e., γ(x) = ˆt. While γ could, in principle, be implemented
by a variety of approaches, such as support vector regres-
sion [10] or Gaussian processes [3], we use a two-layer
neural network instead, to accomplish this task. This is not
an arbitrary choice, as it will later enable us to easily re-
use this building block in the learning stage of the synthesis
function(s) φk
i . The architecture of the attribute regressor is
shown in Fig. 2, consisting of two linear layers, interleaved
by batch normalization (BN) [17] and rectiﬁed linear units
(ReLU) [24]. While this architecture is admittedly simple,
adding more layers did not lead to signiﬁcantly better re-
sults in our experiments. Nevertheless, the design of this
component is problem-speciﬁc and could easily be replaced
by more complex variants, depending on the characteristics
of the attributes that need to be predicted.

D
R
⊂

X
∈
x

:
t
u
p
n
I

]
M
,
D
[

.
n
i
L

U
L
e
R
+
N
B

]
1
,
M
[

.
n
i
L

γ

U
L
e
R

Conﬁg.:
D = 4096
A = 64

Lin. [D,M]:

+
R
∈
ˆt

:
t
u
p
t
u
O

x|{z}
∈RD

7→ Ax + b
| {z }
∈RM

Figure 2: Architecture of the attribute regressor γ.

Learning. The attribute regressor can easily be trained from
a collection of N training tuples {(xi, si)}N
i=1 for each at-
tribute. As the task of the attribute regressor is to predict in
which interval the original feature x resides, we do not need
to organize the training data into intervals in this step.

7457

L(x, t; φ) = (γ(φ(x)) − t)2 ,

(2)

Attribute regressor
(frozen during network training)

min
φ∈C

3.2. Feature regression

To implement1 φ, we design an encoder-decoder archi-
tecture, reminiscent of a conventional autoencoder [1]. Our
objective, however, is not to encode and then reconstruct
the input, but to produce an output that resembles a feature
descriptor of an object at a desired attribute value.

In other words, the encoder essentially learns to extract
the essence of features; the decoder then takes the encod-
ing and decodes it to the desired result. In general, we can
formulate the optimization problem as

where the minimization is over a suitable class of functions
C. Notably, when implementing φ as an encoder-decoder
network with an appended (pre-trained) attribute predictor
(see Fig. 3) and loss (γ(φ(x)) − t)2, we have little con-
trol over the decoding result in the sense that we cannot
guarantee that the identity of the input is preserved. This
means that features from a particular object class might map
to features that are no longer recognizable as this class, as
the encoder-decoder will only learn to “fool” the attribute
predictor γ. For that reason, we add a regularizer to the
objective of Eq. (2), i.e., we require the decoding result to
be close, e.g., in the 2-norm, to the input. This changes the
optimization problem of Eq. (2) to

min
φ∈C

L(x, t; φ) = (γ(φ(x)) − t)2
}
|

{z
Mismatch penalty

+λ kφ(x) − xk2
}

|

{z
Regularizer

. (3)

Interpreted differently, this resembles the loss of an au-
toencoder network with an added target attribute mismatch
penalty. The encoder-decoder network that implements the
function class C to learn φ is shown in Fig. 3. The core
building block is a combination of a linear layer, batch nor-
malization, ELU [7], followed by dropout [32]. After the ﬁ-
nal linear layer, we add one ReLU layer to enforce ˆx ∈ RD
+ .
Learning. Training the encoder-decoder network of Fig. 3
requires an a-priori trained attribute regressor γ for each
given attribute A ∈ A. During training, this attribute regres-
sor is appended to the network and its weights are frozen.
Hence, only the encoder-decoder weights are updated. To
train one φk
i for each interval [li, hi] of the object attribute
range and a desired object attribute value tk, we partition the
training data from the external corpus into subsets Si, such
that ∀(xn, sn) ∈ Si : sn ∈ [li, hi]. One φk
i is learned from
Si for each desired object attribute value tk. As training
is in feature space X , we have no convolutional layers and
consequently training is computationally cheap. For test-
ing, the attribute regressor is removed and only the trained
encoder-decoder network (implementing φk
i ) is used to syn-
thesize features. Consequently, given |A| attributes, I inter-

1We omit the sub-/superscripts for readability.

Block(A,B)

Input: x ∈ X ⊂ RD

Lin. [A,B]

Block(D,A)

BN+ELU

Dropout

)
ki
φ

g
n
i
t
n
e
m
e
l
p
m

i
(

k
r
o
w
t
e
N

Block(A,B)

Block(B,A)

Lin. [A,D]

ReLU

γ

Conﬁg.:
D = 4096
A = 256
B = 32

kφk
i (x) − xk2
Regularizer

(γ(φk
i (x)) − t)2
Mismatch penalty
(for the desired object attribute value)

Figure 3: Illustration of the proposed encoder-decoder network for AGA.
During training, the attribute regressor γ is appended to the network,
whereas, for testing (i.e., feature synthesis) this part is removed. When
learning φk
i , the input x is such that the associated attribute value s is
within [li, hi] and one φk

i is learned per desired attribute value tk.

vals per attribute and T target values for an object attribute,
we obtain |A| · I · T synthesis functions.

4. Experiments

We ﬁrst discuss the generation of adequate training data
for the encoder-decoder network, then evaluate every com-
ponent of our architecture separately and eventually demon-
strate its utility on (1) one-shot object recognition in a trans-
fer learning setting and (2) one-shot scene recognition.

Dataset. We use the SUN RGB-D dataset from Song et al.
[31]. This dataset contains 10335 RGB images with depth
maps, as well as detailed annotations for more than 1000
objects in the form of 2D and 3D bounding boxes. In our
setup, we use object depth and pose as our attributes, i.e.,
A = {Depth, Pose}. For each ground-truth 3D bounding
box, we extract the depth value at its centroid and obtain
pose information as the rotation of the 3D bounding box
about the vertical y-axis. In all experiments, we use the ﬁrst
5335 images as our external database, i.e., the database for
which we assume availability of attribute annotations. The
remaining 5000 images are used for testing; more details
are given in the speciﬁc experiments.

Training data. Notably, in SUN RGB-D, the number of
instances of each object class are not evenly distributed,
simply because this dataset was not speciﬁcally designed
for object recognition tasks. Consequently, images are also
not object-centric, meaning that there is substantial varia-
tion in the location of objects, as well as the depth and pose
at which they occur. This makes it difﬁcult to extract a suf-
ﬁcient and balanced number of feature descriptors per ob-

7458

Garbage bin

Ground-truth

RCNN detection(s)

Chair

D: d
P: α◦

D: d
P: α◦

Monitor

Printer

Figure 4: Illustration of training data generation. First, we obtain fast
RCNN [14] activations (FC7 layer) of Selective Search [36] proposals that
overlap with 2D ground-truth bounding boxes (IoU > 0.5) and scores >
0.7 (for a particular object class) to generate a sufﬁcient amount of train-
ing data. Second, attribute values (i.e., depth D and pose P) of the corre-
sponding 3D ground-truth bounding boxes are associated with the propos-
als (best-viewed in color).

ject class, if we would only use the ground-truth bounding
boxes to extract training data. We circumvent this problem
by leveraging the fast RCNN detector of [14] with object
proposals generated by Selective Search [36]. In detail, we
ﬁnetune the ImageNet model from [14] to SUN RGB-D,
using the same 19 objects as in [31]. We then run the de-
tector on all images from our training split and keep the
proposals with detection scores > 0.7 and a sufﬁcient over-
lap (measured by the IoU >0.5) with the 2D ground-truth
bounding boxes. This is a simple augmentation technique
to increase the amount of available training data. The asso-
ciated RCNN activations (at the FC7 layer) are then used as
our features x. Each proposal that remains after overlap and
score thresholding is annotated by the attribute information
of the corresponding ground-truth bounding box in 3D. As
this strategy generates a larger number of descriptors (com-
pared to the number of ground-truth bounding boxes), we
can evenly balance the training data in the sense that we
can select an equal number of detections per object class to
train (1) the attribute regressor and (2) the encoder-decoder
network. Training data generation is illustrated in Fig. 4 on
four example images.

Implementation. The attribute regressor and the encoder-
decoder network are implemented in Torch. All models
are trained using Adam [19]. For the attribute regressor, we
train for 30 epochs with a batch size of 300 and a learning
rate of 0.001. The encoder-decoder network is also trained
for 30 epochs with the same learning rate, but with a batch
size of 128. The dropout probability during training is set
to 0.25. No dropout is used for testing. For our classiﬁca-

Object

bathtub
bed
bookshelf
box
chair
counter
desk
door
dresser
garbage bin
lamp
monitor
night stand
pillow
sink
sofa
table
tv
toilet
∅

D (MAE [m])

P (MAE [deg])

per-object
0.23
0.39
0.57
0.55
0.37
0.54
0.41
0.49
0.32
0.36
0.42
0.24
0.56
0.38
0.20
0.40
0.37
0.35
0.26
0.39

agnostic
0.94
0.30
0.43
0.51
0.31
0.62
0.36
1.91
0.41
0.32
0.69
0.22
0.65
0.43
0.19
0.33
0.33
0.48
0.20
0.51

per-object
37.97
44.36
52.95
27.05
37.90
40.16
48.63
52.73
67.88
47.51
25.93
34.04
23.80
32.56
56.52
34.36
41.31
35.29
25.32
40.33

agnostic
46.85
42.59
41.41
38.14
32.86
52.35
41.71
102.23
70.92
45.26
23.91
25.85
20.21
35.64
45.75
34.51
37.30
24.23
19.59
41.12

Table 1: Median-Absolute-Error (MAE), for depth / pose, of the attribute
regressor, evaluated on 19 objects from [31]. In our setup, the pose esti-
mation error quantiﬁes the error in predicting a rotation around the z-axis.
D indicates Depth, P indicates Pose. For reference, the range of of the
object attributes in the training data is [0.2m, 7.5m] for Depth and [0◦,
180◦] for Pose. Results are averaged over 5 training / evaluation runs.

tion experiments, we use a linear C-SVM, as implemented
in liblinear [11]. On a Linux system, running Ubuntu
16.04, with 128 GB of memory and one NVIDIA Titan X,
training one model (i.e., one φk
i ) takes ≈ 30 seconds. The
relatively low demand on computational resources high-
lights the advantage of AGA in feature space, as no convolu-
tional layers need to be trained. All trained models+source
code are publicly available online2.

4.1. Attribute regression

While our strategy, AGA, to data augmentation is ag-
nostic to the object classes, in both the training and testing
dataset, it is interesting to compare attribute prediction per-
formance to the case where we train object-speciﬁc regres-
sors. In other words, we compare object-agnostic training
to training one regressor γj, j ∈ {1, . . . , |S|} for each ob-
ject class in S. This allows us to quantify the potential loss
in prediction performance in the object-agnostic setting.

Table 1 lists the median-absolute-error (MAE) of depth
(in [m]) and pose (in [deg]) prediction per object. We train
on instances of 19 object classes (S) in our training split
of SUN RGB-D and test on instances of the same object
classes, but extracted from the testing split. As we can
see, training in an object-speciﬁc manner leads to a lower
MAE overall, both for depth and pose. This is not sur-
prising, since the training data is more specialized to each
particular object, which essentially amounts to solving sim-
pler sub-problems. However, in many cases, especially for
depth, the object-agnostic regressor performs on par, except
for object classes with fewer training samples (e.g., door).

2https://github.com/rkwitt/GuidedAugmentation

7459

We also remark that, in general, pose estimation from 2D
data is a substantially harder problem than depth estimation
(which works remarkably well, even on a per-pixel level,
c.f . [22]). Nevertheless, our recognition experiments (in
Secs. 4.3 and 4.4) show that even with mediocre perfor-
mance of the pose predictor (due to symmetry issues, etc.),
augmentation along this dimension is still beneﬁcial.

4.2. Feature regression

We assess the performance of our regressor(s) φk

i , shown
in Fig. 3, that are used for synthetic feature generation. In
all experiments, we use an overlapping sliding window to
bin the range of each attribute A ∈ A into I intervals
In case of Depth, we set [l0, h0] = [0, 1] and
[li, hi].
shift each interval by 0.5 meter; in case of Pose, we set
[l0, h0] = [0◦, 45◦] and shift by 25◦. We generate as many
intervals as needed to cover the full range of the attribute
values in the training data. The bin-width / step-size were
set to ensure a roughly equal number of features in each
bin. For augmentation, we choose 0.5, 1, . . . , max(Depth)
as target attribute values for Depth and 45◦, 70◦, . . . , 180◦
for Pose. This results in T = 11 target values for Depth
and T = 7 for Pose.

We use two separate evaluation metrics to assess the per-
formance of φk
i . First, we are interested in how well the
feature regressor can generate features that correspond to
the desired attribute target values. To accomplish this, we
run each synthetic feature ˆx through the attribute predictor
and assess the MAE, i.e., |γ(ˆx)−t|, over all attribute targets
t. Table 2 lists the average MAE, per object, for (1) features
from object classes that were seen in the training data and
(2) features from objects that we have never seen before.
As wee can see from Table 2, MAE’s for seen and unseen
objects are similar, indicating that the encoder-decoder has
learned to synthesize features, such that γ(ˆx) ≈ t.

Second, we are interested in how much synthesized fea-
tures differ from original features. While we cannot eval-
uate this directly (as we do not have data from one partic-
ular object instance at multiple depths and poses), we can
assess how “close” synthesized features are to the original
features. The intuition here is that closeness in feature space
is indicative of an object-identity preserving synthesis. In
i (x) − xk2, how-
principle, we could simply evaluate kφk
ever, the 2-norm is hard to interpret. Instead, we compute
the Pearson correlation coefﬁcient ρ between each original
feature and its synthesized variants, i.e., ρ(x, φk
i (x)). As
ρ ranges from [−1, 1], high values indicate a strong linear
relationship to the original features. Results are reported
in Table 2. Similar to our previous results for MAE, we
observe that ρ, when averaged over all objects, is slightly
lower for objects that did not appear in the training data.
This decrease in correlation, however, is relatively small.

In summary, we conclude that these results warrant the

Object
bathtub
bed
bookshelf
box
chair
counter
desk
door
dresser
garbage bin
lamp
monitor
night stand
pillow
sink
sofa
table
tv
toilet

picture
ottoman
whiteboard
fridge
counter
books
stove
cabinet
printer
computer

ρ
0.75
0.81
0.80
0.74
0.73
0.76
0.75
0.67
0.79
0.76
0.82
0.82
0.80
0.80
0.75
0.78
0.75
0.78
0.80
∅ 0.77
0.67
0.70
0.67
0.69
0.76
0.74
0.71
0.74
0.73
0.81
∅ 0.72

D (MAE [m])
0.10
0.07
0.06
0.08
0.07
0.08
0.07
0.10
0.08
0.07
0.08
0.06
0.07
0.08
0.11
0.08
0.07
0.08
0.10
0.08
0.08
0.09
0.12
0.10
0.08
0.08
0.10
0.09
0.08
0.06
0.09

ρ
0.68
0.82
0.79
0.74
0.71
0.77
0.74
0.63
0.77
0.76
0.79
0.80
0.78
0.81
0.76
0.78
0.74
0.72
0.81
0.76
0.65
0.70
0.65
0.68
0.77
0.73
0.71
0.72
0.72
0.80
0.71

P (MAE [deg])
3.99
3.30
3.36
4.44
3.93
3.90
3.93
4.71
4.12
5.30
4.83
3.34
4.00
3.87
4.00
4.29
4.10
4.66
3.70
4.10
5.13
4.41
4.43
4.48
3.98
4.26
4.50
3.99
4.59
3.73
4.35

1
e
l
b
a
T
e
e
s

,
s
t
c
e
j
b
o
n
e
e
S

)
1
T
(

s
t
c
e
j
b
o

n
e
e
s
n
U

Table 2: Assessment of φk
i w.r.t. (1) Pearson correlation (ρ) of synthe-
sized and original features and (2) mean MAE of predicted attribute values
of synthesized features, γ(φk
i (x)), w.r.t. the desired attribute values t. D
indicates Depth-aug. features (MAE in [m]); P indicates Pose-aug. fea-
tures (MAE in [deg]).

use of φk
i on feature descriptors from object classes that
have not appeared in the training corpus. This enables us
to test φk
i in transfer learning setups, as we will see in the
following one-shot experiments of Secs. 4.3 and 4.4.

4.3. One shot object recognition

First, we demonstrate the utility of our approach on the
task of one-shot object recognition in a transfer learning
setup. Speciﬁcally, we aim to learn attribute-guided aug-
menters φk
i from instances of object classes that are avail-
able in an external, annotated database (in our case, SUN
RGB-D). We denote this collection of object classes as our
source classes S. Given one instance from a collection of
completely different object classes, denoted as the target
classes T , we aim to train a discriminant classiﬁer C on T ,
i.e., C : X → {1, . . . , |T |}. In this setting, S ∩ T = ∅.
Note that no attribute annotations for instances of object
classes in T are available. This can be considered a vari-
ant of transfer learning, since we transfer knowledge from
object classes in S to instances of object classes in T , with-
out any prior knowledge about T .
Setup. We evaluate one-shot object recognition perfor-
mance on three collections of previously unseen object
classes in the following setup: First, we randomly select
two sets of 10 object classes and ensure that each object
class has at least 100 samples in the testing split of SUN

7460

Baseline

AGA+D

AGA+P

AGA+D+P

T1 (10)
T2 (10)
T3 (20)

T1 (10)
T2 (10)
T3 (20)

33.74
23.76
22.84

50.03
36.76
37.37

One-shot
38.32 X 37.25 X
28.49 X 27.15 X
25.52 X 24.34 X
Five-shot
55.04 X 53.83 X
44.57 X 42.68 X
40.46 X 39.36 X

39.10 X
30.12 X
26.67 X

56.92 X
47.04 X
42.87 X

Table 3: Recognition accuracy (over 500 trials) for three object recogni-
tion tasks; top: one-shot, bottom: ﬁve-shot. Numbers in parentheses indi-
cate the #classes. A ’X’ indicates that the result is statistically different (at
5% sig.) from the Baseline. +D indicates adding Depth-aug. features to
the one-shot instances; +P indicates addition of Pose-aug. features and
+D, P denotes adding a combination of Depth-/Pose-aug. features.

RGB-D. We further ensure that no object class is in S. This
guarantees (1) that we have never seen the image, nor (2) the
object class during training. Since, SUN RGB-D does not
have object-centric images, we use the ground-truth bound-
ing boxes to obtain the actual object crops. This allows us to
tease out the beneﬁt of augmentation without having to deal
with confounding factors such as background noise. The
4. We ad-
two sets of object classes are denoted T1
ditionally compile a third set of target classes T3 = T1 ∪ T2
and remark that T1 ∩ T2 = ∅. Consequently, we have two
10-class problems and one 20-class problem. For each ob-
ject image in Ti, we then collect RCNN FC7 features.

3 and T2

As a Baseline, we “train” a linear C-SVM (on 1-norm
normalized features) using only the single instances of each
object class in Ti (SVM cost ﬁxed to 10). Exactly the same
parameter settings of the SVM are then used to train on the
single instances + features synthesized by AGA. We repeat
the selection of one-shot instances 500 times and report the
average recognition accuracy. For comparison, we addition-
ally list 5-shot recognition results in the same setup.
Remark. The design of this experiment is similar to [25,
Section 4.3.], with the exceptions that we (1) do not detect
objects, (2) augmentation is performed in feature space and
(3) no object-speciﬁc information is available. The latter
is important, since [25] assumes the existence of 3D CAD
models for objects in Ti from which synthetic images can
In our case, augmentation does not require
be rendered.
any a-priori information about the objects classes.
Results. Table 3 lists the classiﬁcation accuracy for the dif-
ferent sets of one-shot training data. First, using original
one-shot instances augmented by Depth-guided features
(+D); second, using original features + Pose-guided fea-
tures (+P) and third, a combination of both (+D, P); In gen-
eral, we observe that adding AGA-synthesized features im-
proves recognition accuracy over the Baseline in all cases.

Computer @ 2.6 [m]

Abs. gradient diﬀ.: 3 [m] vs. 4 [m]

Abs. gradient diﬀ.: 3 [m] vs. 4.5 [m]

Figure 5: Illustration of the difference in gradient magnitude when back-
propagating (through RCNN) the 2-norm of the difference between an
original and a synthesized feature vector for an increasing desired change
in depth, i.e., 3[m] vs. 4[m] (middle) and 3[m] vs. 4.5[m] (right).

For Depth-augmented features, gains range from 3-5 per-
centage points, for Pose-augmented features, gains range
from 2-4 percentage points on average. We attribute this ef-
fect to the difﬁculty in predicting object pose from 2D data,
as can be seen from Table 1. Nevertheless, in both augmen-
tation settings, the gains are statistically signiﬁcant (w.r.t.
the Baseline), as evaluated by a Wilcoxn rank sum test for
equal medians [13] at 5% signiﬁcance (indicated by ’X’ in
Table 3). Adding both Depth- and Pose-augmented fea-
tures to the original one-shot features achieves the greatest
improvement in recognition accuracy, ranging from 4-6 per-
centage points. This indicates that information from depth
and pose is complementary and allows for better coverage
of the feature space. Notably, we also experimented with
the metric-learning approach of Fink [12] which only led to
negligible gains over the Baseline (e.g., 33.85% on T1).
Feature analysis/visualization. To assess the nature of fea-
ture synthesis, we backpropagate through RCNN layers the
gradient w.r.t. the 2-norm between an original and a syn-
thesized feature vector. The strength of the input gradient
indicates how much each pixel of the object must change to
produce a proportional change in depth/pose of the sample.
As can be seen in the example of Fig. 5, a greater desired
change in depth invokes a stronger gradient on the monitor.
Second, we ran a retrieval experiment: we sampled 1300
instances of 10 (unseen) object classes (T1) and synthesized
features for each instance w.r.t. depth. Synthesized features
were then used for retrieval on the original 1300 features.
This allows to assess if synthesized features (1) allow to re-
trieve instances of the same class (Top-1 acc.) and (2) of
the desired attribute value. The latter is measured by the co-
efﬁcient of determination (R2). As seen in Table 4, the R2
scores indicate that we can actually retrieve instances with
the desired attribute values. Notably, even in cases where
R2 ≈ 0 (i.e., the linear model does not explain the variabil-
ity), the results still show decent Top-1 acc., revealing that
synthesis does not alter class membership.

4.4. Object based one shot scene recognition

3T1 = {picture, whiteboard, fridge, counter, books, stove,

cabinet, printer, computer, ottoman}

4T2 = {mug, telephone, bowl, bottle, scanner, microwave,

coffee table, recycle bin, cart, bench}

Motivation. We can also use AGA for a different type of
transfer, namely the transfer from object detection networks
to one-shot scene recognition. Although, object detection is

7461

Object
picture
fridge
books
cabinet
computer

Top-1
0.33
0.26
0.52
0.57
0.94

2

R
0.36
0.08
0.07
0.27
0.26

Object
whiteboard
counter
stove
printer
ottoman

Top-1
0.12
0.64
0.20
0.31
0.60

2

R
0.30
0.18
0.13
0.02
0.12

Table 4: Retrieval results for unseen objects (T1) when querying with
synthesized features of varying depth. Larger R2 values indicate a stronger
linear relationship (R2 ∈ [0, 1]) to the depth values of retrieved instances.

Method
max. pool (Baseline)
AGA FV (+D)
AGA FV (+P)
AGA CL-1 (+D, max.)
AGA CL-2 (+P, max.)
AGA CL-3 (+D, +P, max.)
Sem-FV [8]
AGA Sem-FV
Places [38]
AGA Places

Accuracy [%]
13.97
15.13
14.63
16.04
15.52
16.32
32.75
34.36
51.28
52.11

Table 5:
One-shot classiﬁcation on 25 indoor scene classes [26]:
{auditorium, bakery, bedroom, bookstore, children room, classroom, com-
puter room, concert hall, corridor, dental ofﬁce, dining room, hospital
room, laboratory, library, living room, lobby, meeting room, movie the-
ater, nursery, ofﬁce, operating room, pantry, restaurant}. For Sem-FV [8],
we use ImageNet CNN features extracted at one image scale.

a challenging task in itself, signiﬁcant progress is made, ev-
ery year, in competitions such as the ImageNet challenge.
Extending the gains in object detection to other related
problems, such as scene recognition, is therefore quite ap-
pealing. A system that uses an accurate object detector such
as an RCNN [14] to perform scene recognition, could gen-
erate comprehensive annotations for an image in one for-
ward pass. An object detector that supports one-shot scene
recognition could do so with the least amount of additional
data. It must be noted that such systems are different from
object recognition based methods such as [15, 8, 6], where
explicit detection of objects is not necessary. They apply
ﬁlters from object recognition CNNs to several regions of
images and extract features from all of them, whether or not
an object is found. The data available to them is therefore
enough to learn complex descriptors such as Fisher vectors
(FVs). A detector, on the other hand, may produce very
few features from an image, based on the number of ob-
jects found. AGA is tailor-made for such scenarios where
features from an RCNN-detected object can be augmented.

Setup. To evaluate AGA in this setting, we select a 25-class
subset of MIT Indoor [26], which may contain objects that
the RCNN is trained for. The reason for this choice is our
reliance on a detection CNN, which has a vocabulary of 19
objects from SUN RGB-D. At present, this is the largest
such dataset that provides objects and their 3D attributes.
The system can be extended easily to accommodate more
scene classes if a larger RGB-D object dataset becomes
available. As the RCNN produces very few detections per
scene image, the best approach, without augmentation, is

to perform pooling of RCNN features from proposals into
a ﬁxed-size representation. We used max-pooling as our
baseline. Upon augmentation, using predicted depth/ pose,
an image has enough RCNN features to compute a GMM-
based FV. For this, we use the experimental settings in [8].
The FVs are denoted as AGA FV(+D) and AGA FV(+P),
based on the attribute used to guide the augmentation. As
classiﬁer, we use a linear C-SVM with ﬁxed parameter (C).
Results. Table 5 lists the avgerage one-shot recognition ac-
curacy over multiple iterations. The beneﬁts of AGA are
clear, as both aug. FVs perform better than the max-pooling
baseline by 0.5-1% points. Training on a combination (con-
catenated vector) of the augmented FVs and max-pooling,
denoted as AGA CL-1, AGA CL-2 and AGA CL-3 fur-
ther improves by about 1-2% points. Finally, we com-
bined our aug. FVs with the state-of-the-art semantic FV
of [8] and Places CNN features [38] for one-shot classiﬁca-
tion. Both combinations, denoted AGA Sem-FV and AGA
Places, improved by a non-trivial margin (∼1% points).

5. Discussion

We presented an approach toward attribute-guided aug-
mentation in feature space. Experiments show that object
attributes, such as pose / depth, are beneﬁcial in the con-
text of one-shot recognition, i.e., an extreme case of lim-
ited training data. Notably, even in case of mediocre per-
formance of the attribute regressor (e.g., on pose), results
indicate that synthesized features can still supply useful in-
formation to the classiﬁcation process. While we do use
bounding boxes to extract object crops from SUN RGB-D
in our object-recognition experiments, this is only done to
clearly tease out the effect of augmentation. In principle, as
our encoder-decoder is trained in an object-agnostic man-
ner, no external knowledge about classes is required.

As SUN RGB-D exhibits high variability in the range of
both attributes, augmentation along these dimensions can
indeed help classiﬁer training. However, when variability is
limited, e.g., under controlled acquisition settings, the gains
may be less apparent. In that case, augmentation with re-
spect to other object attributes might be required.

Two aspects are speciﬁcally interesting for future work.
First, replacing the attribute regressor for pose with a specif-
ically tailored component will potentially improve learning
of the synthesis function(s) φk
i and lead to more realistic
synthetic samples. Second, we conjecture that, as additional
data with more annotated object classes and attributes be-
comes available (e.g., [2]), the encoder-decoder can lever-
age more diverse samples and thus model feature changes
with respect to the attribute values more accurately.

Acknowledgments. This work is supported by NSF awards
IIS-1208522, CCF-0830535, ECCS-1148870 and a gener-
ous donation of GPUs from Nvidia.

7462

[22] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ﬁelds
for depth estimation from a single image. In CVPR, 2015. 6
[23] E. Miller, N. Matsakis, and P. Viola. Learning from one-
example through shared density transforms. In CVPR, 2000.
3

[24] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 3

[25] X. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep ob-

ject detectors from 3d models. In ICCV, 2015. 2, 7

[26] A. Quattoni and A. Torralba. Recognizing indoor scenes. In

CVPR, 2009. 8

[27] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-

wards real-time object detection. In NIPS, 2015. 1

[28] G. Rogez and C. Schmid. MoCap-guided data augmentation
for 3D pose estimation in the wild. CoRR, abs/1607.02046,
2016. 2

[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and
Y. LeCun. OverFeat: Integrated recognition, localization and
detection using convolutional networks. In ICLR, 2014. 1

[30] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 1

[31] S. Song, S. Lichtenberg, and J. Xiao. SUN RGB-D: A RGB-
D scene understanding benchmark suite. In CVPR, 2015. 4,
5

[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. JMLR, 15:19291958, 2014. 4
[33] H. Su, C. Qi, Y. Li, and L. Guibas. Render for CNN: View-
point estimation in images using cnns trained with rendered
3d model views. In ICCV, 2015. 2

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[35] A. Torralba and A. Efros. Unbiased look at dataset bias. In

CVPR, 2011. 2

[36] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.
Selective search for object recognition. IJCV, 104(2):154–
171, 2013. 5

[37] M. Zeiler and R. Fergus. Visualizing and understanding con-

volutional networks. In ECCV, 2014. 2

[38] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using Places
database. In NIPS, 2014. 8

References

[1] Y. Bengio. Learning deep architectures for AI. Found. Trends

Mach. Learn., 2(1):1–127, 2009. 4

[2] A. Borji, S. Izadi, and L. Itti. iLab-20M: A large-scale con-
trolled object dataset to investigate deep learning. In CVPR,
2016. 8

[3] C. W. C.E. Rasmussen. Gaussian Processes for Machine

Learning. The MIT Press, 2005. 3

[4] C. Charalambous and A. Bharath. A data augmentation
methodology for training machine/deep learning gait recog-
nition algorithms. In BMVC, 2016. 2

[5] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2

[6] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for
texture recognition and segmentation. In CVPR, 2015. 1, 8
[7] D.-A. Clevert, T. Unterhiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(ELUs). In ICLR, 2016. 4

[8] M. Dixit, S. Chen, D. Gao, N. Rasiwasia, and N. Vascon-
celos. Scene classiﬁcation with semantic Fisher vectors. In
CVPR, 2015. 1, 8

[9] J. Donahue, Y. Jia, O. Vinyals, J. Huffman, N. Zhang,
E. Tzeng, and T. Darrell. DeCAF: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, 2014.
1

[10] H. Drucker, C. Burges, L. Kaufman, and A. Smola. Support

vector regression machines. In NIPS, 1997. 3

[11] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C. J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
JMLR, 9(8):1871–1874, 2008. 5

[12] M. Fink. Object classiﬁcation from a single example utiliz-

ing relevance metrics. In NIPS, 2004. 7

[13] J. Gibbons and S. Chakraborti. Nonparametric Statistical
Inference. Chapman & Hall/CRC Press, 5th edition, 2011. 7

[14] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 5, 8
[15] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale
orderless pooling of deep convolutional activation features.
In ECCV, 2014. 1, 8

[16] S. Hauberg, O. Freifeld, A. Boensen, L. Larsen, J. F. III, and
L. Hansen. Dreaming more data: Class-dependent distribu-
tions over diffeomorphisms for learned data augmentation.
In AISTATS, 2016. 2

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 3

[18] M.

Jaderberg, K. Simonyan, A. Zisserman,
transformer networks.

Spatial

and
In

K. Kavukcuoglu.
NIPS, 2015. 3

[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 5

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[21] R. Kwitt, S. Hegenbart, and M. Niethammer. One-shot learn-
ing of scene locations via feature trajectory transfer.
In
CVPR, 2016. 2, 3

7463

AGA : Attribute-Guided Augmentation

Mandar Dixit
UC San Diego
mdixit@ucsd.edu

Roland Kwitt
University of Salzburg
rkwitt@gmx.at

Marc Niethammer
UNC Chapel Hill
mn@cs.unc.edu

Nuno Vasconcelos
UC San Diego
nvasconcelos@ucsd.edu

Abstract

We consider the problem of data augmentation, i.e., gen-
erating artiﬁcial samples to extend a given corpus of train-
ing data. Speciﬁcally, we propose attributed-guided aug-
mentation (AGA) which learns a mapping that allows syn-
thesis of data such that an attribute of a synthesized sample
is at a desired value or strength. This is particularly inter-
esting in situations where little data with no attribute an-
notation is available for learning, but we have access to an
external corpus of heavily annotated samples. While prior
works primarily augment in the space of images, we pro-
pose to perform augmentation in feature space instead. We
implement our approach as a deep encoder-decoder archi-
tecture that learns the synthesis function in an end-to-end
manner. We demonstrate the utility of our approach on the
problems of (1) one-shot object recognition in a transfer-
learning setting where we have no prior knowledge of the
new classes, as well as (2) object-based one-shot scene
recognition. As external data, we leverage 3D depth and
pose information from the SUN RGB-D dataset. Our ex-
periments show that attribute-guided augmentation of high-
level CNN features considerably improves one-shot recog-
nition performance on both problems.

1. Introduction

Convolutional neural networks (CNNs), trained on large
scale data, have signiﬁcantly advanced the state-of-the-
art on traditional vision problems such as object recogni-
tion [20, 30, 34] and object detection [14, 27]. Success
of these networks is mainly due to their high selectivity
for semantically meaningful visual concepts, e.g., objects
and object parts [29]. In addition to ensuring good perfor-
mance on the problem of interest, this property of CNNs
also allows for transfer of knowledge to several other vi-
sion tasks [9, 15, 6, 8]. The object recognition network
of [20], e.g., has been successfully used for object detec-
tion [14, 27], scene classiﬁcation [15, 8], texture classiﬁca-
tion [6] and domain adaptation [9], using various transfer
mechanisms.

Tables with depth in the range of 1-2 [m]

Tables

Training
data

Chairs

learn φ3
(by using γ)

[1,2]

γ(ˆx) ≈ 3[m]

x 7→ ˆx = φ3

[1,2](x)

x

γ(x) = 1.3 [m]

Instance x from a
new class

Training
data
(e.g., RCNN features)

X ⊂ RD

γ ... Attribute (strength) predictor
(trained separately)

Chairs with depth in the range of 1-2 [m]

Figure 1: Given a predictor γ : X → R+ of some object attribute (e.g.,
depth or pose), we propose to learn a mapping of object features x ∈ X ,
such that (1) the new synthetic feature ˆx is “close” to x (to preserve object
identity) and (2) the predicted attribute value γ(ˆx) = ˆt of ˆx matches a
desired object attribute value t, i.e., t − ˆt is small.
In this illustration,
we learn a mapping for features with associated depth values in the range
of 1-2 [m] to t = 3 [m] and apply this mapping to an instance of a new
object class. In our approach, this mapping is learned in an object-agnostic
manner. With respect to our example, this means that all training data from
‘chairs’ and ‘tables’ is used to a learn feature synthesis function φ.

CNN-based transfer is generally achieved either by ﬁne-
tuning a pre-trained network, such as in [20], on a new im-
age dataset or by designing a new image representation on
such a dataset based on the activations of the pre-trained
network layers [9, 15, 8, 6]. Recent proposals of trans-
fer have shown highly competitive performance on differ-
ent predictive tasks with a modest amount of new data (as
few as 50 images per class). The effectiveness of transfer-
based methods, however, has not yet been tested under more
severe constraints such as in a few-shot or a one-shot learn-
ing scenario. In these problems, the number of examples
available for learning may be as few as one per class. Fine-
tuning a pre-trained CNN with millions of parameters to

17455

such inadequate datasets is clearly not a viable option. A
one-shot classiﬁer trained on CNN activations will also be
prone to over-ﬁtting due to the high dimensionality of the
feature space. The only way to solve the problem of limited
data is to augment the training corpus by obtaining more
examples for the given classes.

While augmentation techniques can be as simple as ﬂip-
ping, rotating, adding noise, or extracting random crops
from images [20, 5, 37], task-speciﬁc, or guided augmen-
tation strategies [4, 16, 28, 25] have the potential to gen-
erate more realistic synthetic samples. This is a particu-
larly important issue, since performance of CNNs heavily
relies on sufﬁcient coverage of the variability that we ex-
pect in unseen testing data. In scene recognition, we desire,
for example, sufﬁcient variability in the constellation and
transient states of scene categories (c.f . [21]), whereas in
object recognition, we desire variability in the speciﬁc in-
carnations of certain objects, lighting conditions, pose, or
depth, just to name a few. Unfortunately, this variability
is often dataset-speciﬁc and can cause substantial bias in
recognition results [35].

An important observation in the context of our work is
that augmentation is typically performed on the image, or
video level. While this is not a problem with simple tech-
niques, such as ﬂipping or cropping, it can become compu-
tationally expensive if more elaborate augmentation tech-
niques are used. We argue that, in speciﬁc problem settings,
augmentation might as well be performed in feature space,
especially in situations where features are input to subse-
quent learning steps. This is common, e.g., in recognition
tasks, where the softmax output of trained CNNs is often
not used directly, but activations at earlier layers are input
to an external discriminant classiﬁer.
Contribution. We propose an approach to augment the
training set with feature descriptors instead of images.
Speciﬁcally, we advocate an augmentation technique that
learns to synthesize features, guided by desired values for a
set of object attributes, such as depth or pose. An illustra-
tion of this concept is shown in Fig. 1. We ﬁrst train a fast
RCNN [14] detector to identify objects in 2D images. This
is followed by training a neural network regressor which
predicts the 3D attributes of a detected object, namely its
depth from the camera plane and pose. An encoder-decoder
network is then trained which, for a detected object at a cer-
tain depth and pose, will “hallucinate” the changes in its
RCNN features for a set of desired depths/poses. Using this
architecture, for a new image, we are able to augment ex-
isting feature descriptors by an auxiliary set of features that
correspond to the object changing its 3D position. Since our
framework relies on object attributes to guide augmentation,
we refer to it as attribute-guided augmentation (AGA).
Organization. Sec. 2 reviews prior work. Sec. 3 introduces
the proposed encoder-decoder architecture for attribute-

guided augmentation. Sec. 4 studies the building blocks of
this approach in detail and demonstrates that AGA in fea-
ture space improves one-shot object recognition and object-
based scene recognition performance on previously unseen
classes. Sec. 5 concludes the paper with a discussion and an
outlook on potential future directions.

2. Related work

Our review of related work primarily focuses on data
augmentation strategies. While many techniques have been
proposed in the context of training deep neural networks
to avoid over-ﬁtting and to increase variability in the data,
other (sometimes closely related) techniques have previ-
ously appeared in the context of one-shot and transfer learn-
ing. We can roughly group existing techniques into (1)
generic, computationally cheap approaches and (2) task-
speciﬁc, or guided approaches that are typically more com-
putationally involved.

As a representative of the ﬁrst group, Krizhevsky et al.
[20] leverage a set of label-preserving transformations, such
as patch extraction + reﬂections, and PCA-based intensity
transformations, to increase training sample size. Simi-
lar techniques are used by Zeiler and Fergus [37]. In [5],
Chatﬁeld and Zisserman demonstrate that the augmentation
techniques of [20] are not only beneﬁcial for training deep
architectures, but shallow learning approaches equally ben-
eﬁt from such simple and generic schemes.

In the second category of guided-augmentation tech-
niques, many approaches have recently been proposed.
In [4], e.g., Charalambous and Bharath employ guided-
augmentation in the context of gait recognition. The au-
thors suggest to simulate synthetic gait video data (obtained
from avatars) with respect to various confounding factors
to extend the training cor-
(such as clothing, hair, etc.)
pus. Similar in spirit, Rogez and Schmid [28] propose an
image-based synthesis engine for augmenting existing 2D
human pose data by photorealistic images with greater pose
variability. This is done by leveraging 3D motion capture
(MoCap) data. In [25], Peng et al. also use 3D data, in the
form of CAD models, to render synthetic images of objects
(with varying pose, texture, background) that are then used
It is shown that syn-
to train CNNs for object detection.
thetic data is beneﬁcial, especially in situations where few
(or no) training instances are available, but 3D CAD mod-
els are. Su et al. [33] follow a similar pipeline of rendering
images from 3D models for viewpoint estimation, however,
with substantially more synthetic data obtained, e.g., by de-
forming existing 3D models before rendering.

Another (data-driven) guided augmentation technique is
introduced by Hauberg et al. [16]. The authors propose to
learn class-speciﬁc transformations from external training
data, instead of manually specifying transformations as in
[20, 37, 5]. The learned transformations are then applied to

7456

the samples of each class. Speciﬁcally, diffeomorphisms are
learned from data and encouraging results are demonstrated
in the context of digit recognition on MNIST. Notably, this
strategy is conceptually similar to earlier work by Miller
et al. [23] on one-shot learning, where the authors synthe-
size additional data for digit images via an iterative process,
called congealing. During that process, external images of
a given category are aligned by optimizing over a class of
geometric transforms (e.g., afﬁne transforms). These trans-
formations are then applied to single instances of the new
classes to increase data for one-shot learning.

Marginally related to our work, we remark that alterna-
tive approaches to implicitly learn spatial transformations
have been proposed. For instance, Jaderberg et al. [18] in-
troduce spatial transformer modules that can be injected
into existing deep architectures to implicitly capture spa-
tial transformations inherent in the data, thereby improving
invariance to this class of transformations.

While all previously discussed methods essentially pro-
pose image-level augmentation to train CNNs, our approach
is different in that we perform augmentation in feature
space. Along these lines, the approach of Kwitt et al.
[21] is conceptually similar to our work. In detail, the au-
thors suggest to learn how features change as a function of
the strength of certain transient attributes (such as sunny,
cloudy, or foggy) in a scene-recognition context. These
models are then transferred to previously unseen data for
one-shot recognition. There are, however, two key differ-
ences between their approach and ours. First, they require
datasets labeled with attribute trajectories, i.e., all varia-
tions of an attribute for every instance of a class. We, on the
other hand, make use of conventional datasets that seldom
carry such extensive labeling. Second, their augmenters
are simple linear regressors trained in a scene-class speciﬁc
manner. In contrast, we learn deep non-linear models in a
class-agnostic manner which enables a straightforward ap-
plication to recognition in transfer settings.

3. Architecture

Notation. To describe our architecture, we let X denote
our feature space, x ∈ X ⊂ RD denotes a feature descrip-
tor (e.g., a representation of an object) and A denotes a set
of attributes that are available for objects in the external
training corpus. Further, we let s ∈ R+ denote the value
of an attribute A ∈ A, associated with x. We assume (1)
that this attribute can be predicted by an attribute regressor
γ : X → R+ and (2) that its range can be divided into
I intervals [li, hi], where li, hi denote the lower and upper
bounds of the i-th interval. The set of desired object at-
tribute values is {t1, . . . , tT }.
Objective. On a conceptual level, we aim for a synthesis
function φ which, given a desired value t for some object
attribute A, transforms the object features x ∈ X such that

the attribute strength changes in a controlled manner to t.
More formally, we aim to learn

φ : X × R+ → X , (x, t) 7→ ˆx,

s.t. γ(ˆx) ≈ t .

(1)

Since, the formulation in Eq. (1) is overly generic, we con-
strain the problem to the case where we learn different φk
i
for a selection of intervals [li, hi] within the range of at-
tribute A and a selection of T desired object attribute val-
ues tk. In our illustration of Fig. 1, e.g., we have one in-
terval [l, h] = [1, 2] and one attribute (depth) with target
value 3[m]. While learning separate synthesis functions
simpliﬁes the problem, it requires a good a-priori attribute
(strength) predictor, since, otherwise, we could not decide
which φk
i to use. During testing, we (1) predict the object’s
attribute value from its original feature x, i.e., γ(x) = ˆt,
i (x)
and then (2) synthesize additional features as ˆx = φk
for k = 1, . . . , T . If ˆt ∈ [li, hi] ∧ tk /∈ [li, hi], φk
i is used.
Next, we discuss each component of this approach in detail.

3.1. Attribute regression

An essential part of our architecture is the attribute re-
gressor γ : X → R+ for a given attribute A. This regressor
takes as input a feature x and predicts its strength or value,
i.e., γ(x) = ˆt. While γ could, in principle, be implemented
by a variety of approaches, such as support vector regres-
sion [10] or Gaussian processes [3], we use a two-layer
neural network instead, to accomplish this task. This is not
an arbitrary choice, as it will later enable us to easily re-
use this building block in the learning stage of the synthesis
function(s) φk
i . The architecture of the attribute regressor is
shown in Fig. 2, consisting of two linear layers, interleaved
by batch normalization (BN) [17] and rectiﬁed linear units
(ReLU) [24]. While this architecture is admittedly simple,
adding more layers did not lead to signiﬁcantly better re-
sults in our experiments. Nevertheless, the design of this
component is problem-speciﬁc and could easily be replaced
by more complex variants, depending on the characteristics
of the attributes that need to be predicted.

D
R
⊂

X
∈
x

:
t
u
p
n
I

]
M
,
D
[

.
n
i
L

U
L
e
R
+
N
B

]
1
,
M
[

.
n
i
L

γ

U
L
e
R

Conﬁg.:
D = 4096
A = 64

Lin. [D,M]:

+
R
∈
ˆt

:
t
u
p
t
u
O

x|{z}
∈RD

7→ Ax + b
| {z }
∈RM

Figure 2: Architecture of the attribute regressor γ.

Learning. The attribute regressor can easily be trained from
a collection of N training tuples {(xi, si)}N
i=1 for each at-
tribute. As the task of the attribute regressor is to predict in
which interval the original feature x resides, we do not need
to organize the training data into intervals in this step.

7457

L(x, t; φ) = (γ(φ(x)) − t)2 ,

(2)

Attribute regressor
(frozen during network training)

min
φ∈C

3.2. Feature regression

To implement1 φ, we design an encoder-decoder archi-
tecture, reminiscent of a conventional autoencoder [1]. Our
objective, however, is not to encode and then reconstruct
the input, but to produce an output that resembles a feature
descriptor of an object at a desired attribute value.

In other words, the encoder essentially learns to extract
the essence of features; the decoder then takes the encod-
ing and decodes it to the desired result. In general, we can
formulate the optimization problem as

where the minimization is over a suitable class of functions
C. Notably, when implementing φ as an encoder-decoder
network with an appended (pre-trained) attribute predictor
(see Fig. 3) and loss (γ(φ(x)) − t)2, we have little con-
trol over the decoding result in the sense that we cannot
guarantee that the identity of the input is preserved. This
means that features from a particular object class might map
to features that are no longer recognizable as this class, as
the encoder-decoder will only learn to “fool” the attribute
predictor γ. For that reason, we add a regularizer to the
objective of Eq. (2), i.e., we require the decoding result to
be close, e.g., in the 2-norm, to the input. This changes the
optimization problem of Eq. (2) to

min
φ∈C

L(x, t; φ) = (γ(φ(x)) − t)2
}
|

{z
Mismatch penalty

+λ kφ(x) − xk2
}

|

{z
Regularizer

. (3)

Interpreted differently, this resembles the loss of an au-
toencoder network with an added target attribute mismatch
penalty. The encoder-decoder network that implements the
function class C to learn φ is shown in Fig. 3. The core
building block is a combination of a linear layer, batch nor-
malization, ELU [7], followed by dropout [32]. After the ﬁ-
nal linear layer, we add one ReLU layer to enforce ˆx ∈ RD
+ .
Learning. Training the encoder-decoder network of Fig. 3
requires an a-priori trained attribute regressor γ for each
given attribute A ∈ A. During training, this attribute regres-
sor is appended to the network and its weights are frozen.
Hence, only the encoder-decoder weights are updated. To
train one φk
i for each interval [li, hi] of the object attribute
range and a desired object attribute value tk, we partition the
training data from the external corpus into subsets Si, such
that ∀(xn, sn) ∈ Si : sn ∈ [li, hi]. One φk
i is learned from
Si for each desired object attribute value tk. As training
is in feature space X , we have no convolutional layers and
consequently training is computationally cheap. For test-
ing, the attribute regressor is removed and only the trained
encoder-decoder network (implementing φk
i ) is used to syn-
thesize features. Consequently, given |A| attributes, I inter-

1We omit the sub-/superscripts for readability.

Block(A,B)

Input: x ∈ X ⊂ RD

Lin. [A,B]

Block(D,A)

BN+ELU

Dropout

)
ki
φ

g
n
i
t
n
e
m
e
l
p
m

i
(

k
r
o
w
t
e
N

Block(A,B)

Block(B,A)

Lin. [A,D]

ReLU

γ

Conﬁg.:
D = 4096
A = 256
B = 32

kφk
i (x) − xk2
Regularizer

(γ(φk
i (x)) − t)2
Mismatch penalty
(for the desired object attribute value)

Figure 3: Illustration of the proposed encoder-decoder network for AGA.
During training, the attribute regressor γ is appended to the network,
whereas, for testing (i.e., feature synthesis) this part is removed. When
learning φk
i , the input x is such that the associated attribute value s is
within [li, hi] and one φk

i is learned per desired attribute value tk.

vals per attribute and T target values for an object attribute,
we obtain |A| · I · T synthesis functions.

4. Experiments

We ﬁrst discuss the generation of adequate training data
for the encoder-decoder network, then evaluate every com-
ponent of our architecture separately and eventually demon-
strate its utility on (1) one-shot object recognition in a trans-
fer learning setting and (2) one-shot scene recognition.

Dataset. We use the SUN RGB-D dataset from Song et al.
[31]. This dataset contains 10335 RGB images with depth
maps, as well as detailed annotations for more than 1000
objects in the form of 2D and 3D bounding boxes. In our
setup, we use object depth and pose as our attributes, i.e.,
A = {Depth, Pose}. For each ground-truth 3D bounding
box, we extract the depth value at its centroid and obtain
pose information as the rotation of the 3D bounding box
about the vertical y-axis. In all experiments, we use the ﬁrst
5335 images as our external database, i.e., the database for
which we assume availability of attribute annotations. The
remaining 5000 images are used for testing; more details
are given in the speciﬁc experiments.

Training data. Notably, in SUN RGB-D, the number of
instances of each object class are not evenly distributed,
simply because this dataset was not speciﬁcally designed
for object recognition tasks. Consequently, images are also
not object-centric, meaning that there is substantial varia-
tion in the location of objects, as well as the depth and pose
at which they occur. This makes it difﬁcult to extract a suf-
ﬁcient and balanced number of feature descriptors per ob-

7458

Garbage bin

Ground-truth

RCNN detection(s)

Chair

D: d
P: α◦

D: d
P: α◦

Monitor

Printer

Figure 4: Illustration of training data generation. First, we obtain fast
RCNN [14] activations (FC7 layer) of Selective Search [36] proposals that
overlap with 2D ground-truth bounding boxes (IoU > 0.5) and scores >
0.7 (for a particular object class) to generate a sufﬁcient amount of train-
ing data. Second, attribute values (i.e., depth D and pose P) of the corre-
sponding 3D ground-truth bounding boxes are associated with the propos-
als (best-viewed in color).

ject class, if we would only use the ground-truth bounding
boxes to extract training data. We circumvent this problem
by leveraging the fast RCNN detector of [14] with object
proposals generated by Selective Search [36]. In detail, we
ﬁnetune the ImageNet model from [14] to SUN RGB-D,
using the same 19 objects as in [31]. We then run the de-
tector on all images from our training split and keep the
proposals with detection scores > 0.7 and a sufﬁcient over-
lap (measured by the IoU >0.5) with the 2D ground-truth
bounding boxes. This is a simple augmentation technique
to increase the amount of available training data. The asso-
ciated RCNN activations (at the FC7 layer) are then used as
our features x. Each proposal that remains after overlap and
score thresholding is annotated by the attribute information
of the corresponding ground-truth bounding box in 3D. As
this strategy generates a larger number of descriptors (com-
pared to the number of ground-truth bounding boxes), we
can evenly balance the training data in the sense that we
can select an equal number of detections per object class to
train (1) the attribute regressor and (2) the encoder-decoder
network. Training data generation is illustrated in Fig. 4 on
four example images.

Implementation. The attribute regressor and the encoder-
decoder network are implemented in Torch. All models
are trained using Adam [19]. For the attribute regressor, we
train for 30 epochs with a batch size of 300 and a learning
rate of 0.001. The encoder-decoder network is also trained
for 30 epochs with the same learning rate, but with a batch
size of 128. The dropout probability during training is set
to 0.25. No dropout is used for testing. For our classiﬁca-

Object

bathtub
bed
bookshelf
box
chair
counter
desk
door
dresser
garbage bin
lamp
monitor
night stand
pillow
sink
sofa
table
tv
toilet
∅

D (MAE [m])

P (MAE [deg])

per-object
0.23
0.39
0.57
0.55
0.37
0.54
0.41
0.49
0.32
0.36
0.42
0.24
0.56
0.38
0.20
0.40
0.37
0.35
0.26
0.39

agnostic
0.94
0.30
0.43
0.51
0.31
0.62
0.36
1.91
0.41
0.32
0.69
0.22
0.65
0.43
0.19
0.33
0.33
0.48
0.20
0.51

per-object
37.97
44.36
52.95
27.05
37.90
40.16
48.63
52.73
67.88
47.51
25.93
34.04
23.80
32.56
56.52
34.36
41.31
35.29
25.32
40.33

agnostic
46.85
42.59
41.41
38.14
32.86
52.35
41.71
102.23
70.92
45.26
23.91
25.85
20.21
35.64
45.75
34.51
37.30
24.23
19.59
41.12

Table 1: Median-Absolute-Error (MAE), for depth / pose, of the attribute
regressor, evaluated on 19 objects from [31]. In our setup, the pose esti-
mation error quantiﬁes the error in predicting a rotation around the z-axis.
D indicates Depth, P indicates Pose. For reference, the range of of the
object attributes in the training data is [0.2m, 7.5m] for Depth and [0◦,
180◦] for Pose. Results are averaged over 5 training / evaluation runs.

tion experiments, we use a linear C-SVM, as implemented
in liblinear [11]. On a Linux system, running Ubuntu
16.04, with 128 GB of memory and one NVIDIA Titan X,
training one model (i.e., one φk
i ) takes ≈ 30 seconds. The
relatively low demand on computational resources high-
lights the advantage of AGA in feature space, as no convolu-
tional layers need to be trained. All trained models+source
code are publicly available online2.

4.1. Attribute regression

While our strategy, AGA, to data augmentation is ag-
nostic to the object classes, in both the training and testing
dataset, it is interesting to compare attribute prediction per-
formance to the case where we train object-speciﬁc regres-
sors. In other words, we compare object-agnostic training
to training one regressor γj, j ∈ {1, . . . , |S|} for each ob-
ject class in S. This allows us to quantify the potential loss
in prediction performance in the object-agnostic setting.

Table 1 lists the median-absolute-error (MAE) of depth
(in [m]) and pose (in [deg]) prediction per object. We train
on instances of 19 object classes (S) in our training split
of SUN RGB-D and test on instances of the same object
classes, but extracted from the testing split. As we can
see, training in an object-speciﬁc manner leads to a lower
MAE overall, both for depth and pose. This is not sur-
prising, since the training data is more specialized to each
particular object, which essentially amounts to solving sim-
pler sub-problems. However, in many cases, especially for
depth, the object-agnostic regressor performs on par, except
for object classes with fewer training samples (e.g., door).

2https://github.com/rkwitt/GuidedAugmentation

7459

We also remark that, in general, pose estimation from 2D
data is a substantially harder problem than depth estimation
(which works remarkably well, even on a per-pixel level,
c.f . [22]). Nevertheless, our recognition experiments (in
Secs. 4.3 and 4.4) show that even with mediocre perfor-
mance of the pose predictor (due to symmetry issues, etc.),
augmentation along this dimension is still beneﬁcial.

4.2. Feature regression

We assess the performance of our regressor(s) φk

i , shown
in Fig. 3, that are used for synthetic feature generation. In
all experiments, we use an overlapping sliding window to
bin the range of each attribute A ∈ A into I intervals
In case of Depth, we set [l0, h0] = [0, 1] and
[li, hi].
shift each interval by 0.5 meter; in case of Pose, we set
[l0, h0] = [0◦, 45◦] and shift by 25◦. We generate as many
intervals as needed to cover the full range of the attribute
values in the training data. The bin-width / step-size were
set to ensure a roughly equal number of features in each
bin. For augmentation, we choose 0.5, 1, . . . , max(Depth)
as target attribute values for Depth and 45◦, 70◦, . . . , 180◦
for Pose. This results in T = 11 target values for Depth
and T = 7 for Pose.

We use two separate evaluation metrics to assess the per-
formance of φk
i . First, we are interested in how well the
feature regressor can generate features that correspond to
the desired attribute target values. To accomplish this, we
run each synthetic feature ˆx through the attribute predictor
and assess the MAE, i.e., |γ(ˆx)−t|, over all attribute targets
t. Table 2 lists the average MAE, per object, for (1) features
from object classes that were seen in the training data and
(2) features from objects that we have never seen before.
As wee can see from Table 2, MAE’s for seen and unseen
objects are similar, indicating that the encoder-decoder has
learned to synthesize features, such that γ(ˆx) ≈ t.

Second, we are interested in how much synthesized fea-
tures differ from original features. While we cannot eval-
uate this directly (as we do not have data from one partic-
ular object instance at multiple depths and poses), we can
assess how “close” synthesized features are to the original
features. The intuition here is that closeness in feature space
is indicative of an object-identity preserving synthesis. In
i (x) − xk2, how-
principle, we could simply evaluate kφk
ever, the 2-norm is hard to interpret. Instead, we compute
the Pearson correlation coefﬁcient ρ between each original
feature and its synthesized variants, i.e., ρ(x, φk
i (x)). As
ρ ranges from [−1, 1], high values indicate a strong linear
relationship to the original features. Results are reported
in Table 2. Similar to our previous results for MAE, we
observe that ρ, when averaged over all objects, is slightly
lower for objects that did not appear in the training data.
This decrease in correlation, however, is relatively small.

In summary, we conclude that these results warrant the

Object
bathtub
bed
bookshelf
box
chair
counter
desk
door
dresser
garbage bin
lamp
monitor
night stand
pillow
sink
sofa
table
tv
toilet

picture
ottoman
whiteboard
fridge
counter
books
stove
cabinet
printer
computer

ρ
0.75
0.81
0.80
0.74
0.73
0.76
0.75
0.67
0.79
0.76
0.82
0.82
0.80
0.80
0.75
0.78
0.75
0.78
0.80
∅ 0.77
0.67
0.70
0.67
0.69
0.76
0.74
0.71
0.74
0.73
0.81
∅ 0.72

D (MAE [m])
0.10
0.07
0.06
0.08
0.07
0.08
0.07
0.10
0.08
0.07
0.08
0.06
0.07
0.08
0.11
0.08
0.07
0.08
0.10
0.08
0.08
0.09
0.12
0.10
0.08
0.08
0.10
0.09
0.08
0.06
0.09

ρ
0.68
0.82
0.79
0.74
0.71
0.77
0.74
0.63
0.77
0.76
0.79
0.80
0.78
0.81
0.76
0.78
0.74
0.72
0.81
0.76
0.65
0.70
0.65
0.68
0.77
0.73
0.71
0.72
0.72
0.80
0.71

P (MAE [deg])
3.99
3.30
3.36
4.44
3.93
3.90
3.93
4.71
4.12
5.30
4.83
3.34
4.00
3.87
4.00
4.29
4.10
4.66
3.70
4.10
5.13
4.41
4.43
4.48
3.98
4.26
4.50
3.99
4.59
3.73
4.35

1
e
l
b
a
T
e
e
s

,
s
t
c
e
j
b
o
n
e
e
S

)
1
T
(

s
t
c
e
j
b
o

n
e
e
s
n
U

Table 2: Assessment of φk
i w.r.t. (1) Pearson correlation (ρ) of synthe-
sized and original features and (2) mean MAE of predicted attribute values
of synthesized features, γ(φk
i (x)), w.r.t. the desired attribute values t. D
indicates Depth-aug. features (MAE in [m]); P indicates Pose-aug. fea-
tures (MAE in [deg]).

use of φk
i on feature descriptors from object classes that
have not appeared in the training corpus. This enables us
to test φk
i in transfer learning setups, as we will see in the
following one-shot experiments of Secs. 4.3 and 4.4.

4.3. One shot object recognition

First, we demonstrate the utility of our approach on the
task of one-shot object recognition in a transfer learning
setup. Speciﬁcally, we aim to learn attribute-guided aug-
menters φk
i from instances of object classes that are avail-
able in an external, annotated database (in our case, SUN
RGB-D). We denote this collection of object classes as our
source classes S. Given one instance from a collection of
completely different object classes, denoted as the target
classes T , we aim to train a discriminant classiﬁer C on T ,
i.e., C : X → {1, . . . , |T |}. In this setting, S ∩ T = ∅.
Note that no attribute annotations for instances of object
classes in T are available. This can be considered a vari-
ant of transfer learning, since we transfer knowledge from
object classes in S to instances of object classes in T , with-
out any prior knowledge about T .
Setup. We evaluate one-shot object recognition perfor-
mance on three collections of previously unseen object
classes in the following setup: First, we randomly select
two sets of 10 object classes and ensure that each object
class has at least 100 samples in the testing split of SUN

7460

Baseline

AGA+D

AGA+P

AGA+D+P

T1 (10)
T2 (10)
T3 (20)

T1 (10)
T2 (10)
T3 (20)

33.74
23.76
22.84

50.03
36.76
37.37

One-shot
38.32 X 37.25 X
28.49 X 27.15 X
25.52 X 24.34 X
Five-shot
55.04 X 53.83 X
44.57 X 42.68 X
40.46 X 39.36 X

39.10 X
30.12 X
26.67 X

56.92 X
47.04 X
42.87 X

Table 3: Recognition accuracy (over 500 trials) for three object recogni-
tion tasks; top: one-shot, bottom: ﬁve-shot. Numbers in parentheses indi-
cate the #classes. A ’X’ indicates that the result is statistically different (at
5% sig.) from the Baseline. +D indicates adding Depth-aug. features to
the one-shot instances; +P indicates addition of Pose-aug. features and
+D, P denotes adding a combination of Depth-/Pose-aug. features.

RGB-D. We further ensure that no object class is in S. This
guarantees (1) that we have never seen the image, nor (2) the
object class during training. Since, SUN RGB-D does not
have object-centric images, we use the ground-truth bound-
ing boxes to obtain the actual object crops. This allows us to
tease out the beneﬁt of augmentation without having to deal
with confounding factors such as background noise. The
4. We ad-
two sets of object classes are denoted T1
ditionally compile a third set of target classes T3 = T1 ∪ T2
and remark that T1 ∩ T2 = ∅. Consequently, we have two
10-class problems and one 20-class problem. For each ob-
ject image in Ti, we then collect RCNN FC7 features.

3 and T2

As a Baseline, we “train” a linear C-SVM (on 1-norm
normalized features) using only the single instances of each
object class in Ti (SVM cost ﬁxed to 10). Exactly the same
parameter settings of the SVM are then used to train on the
single instances + features synthesized by AGA. We repeat
the selection of one-shot instances 500 times and report the
average recognition accuracy. For comparison, we addition-
ally list 5-shot recognition results in the same setup.
Remark. The design of this experiment is similar to [25,
Section 4.3.], with the exceptions that we (1) do not detect
objects, (2) augmentation is performed in feature space and
(3) no object-speciﬁc information is available. The latter
is important, since [25] assumes the existence of 3D CAD
models for objects in Ti from which synthetic images can
In our case, augmentation does not require
be rendered.
any a-priori information about the objects classes.
Results. Table 3 lists the classiﬁcation accuracy for the dif-
ferent sets of one-shot training data. First, using original
one-shot instances augmented by Depth-guided features
(+D); second, using original features + Pose-guided fea-
tures (+P) and third, a combination of both (+D, P); In gen-
eral, we observe that adding AGA-synthesized features im-
proves recognition accuracy over the Baseline in all cases.

Computer @ 2.6 [m]

Abs. gradient diﬀ.: 3 [m] vs. 4 [m]

Abs. gradient diﬀ.: 3 [m] vs. 4.5 [m]

Figure 5: Illustration of the difference in gradient magnitude when back-
propagating (through RCNN) the 2-norm of the difference between an
original and a synthesized feature vector for an increasing desired change
in depth, i.e., 3[m] vs. 4[m] (middle) and 3[m] vs. 4.5[m] (right).

For Depth-augmented features, gains range from 3-5 per-
centage points, for Pose-augmented features, gains range
from 2-4 percentage points on average. We attribute this ef-
fect to the difﬁculty in predicting object pose from 2D data,
as can be seen from Table 1. Nevertheless, in both augmen-
tation settings, the gains are statistically signiﬁcant (w.r.t.
the Baseline), as evaluated by a Wilcoxn rank sum test for
equal medians [13] at 5% signiﬁcance (indicated by ’X’ in
Table 3). Adding both Depth- and Pose-augmented fea-
tures to the original one-shot features achieves the greatest
improvement in recognition accuracy, ranging from 4-6 per-
centage points. This indicates that information from depth
and pose is complementary and allows for better coverage
of the feature space. Notably, we also experimented with
the metric-learning approach of Fink [12] which only led to
negligible gains over the Baseline (e.g., 33.85% on T1).
Feature analysis/visualization. To assess the nature of fea-
ture synthesis, we backpropagate through RCNN layers the
gradient w.r.t. the 2-norm between an original and a syn-
thesized feature vector. The strength of the input gradient
indicates how much each pixel of the object must change to
produce a proportional change in depth/pose of the sample.
As can be seen in the example of Fig. 5, a greater desired
change in depth invokes a stronger gradient on the monitor.
Second, we ran a retrieval experiment: we sampled 1300
instances of 10 (unseen) object classes (T1) and synthesized
features for each instance w.r.t. depth. Synthesized features
were then used for retrieval on the original 1300 features.
This allows to assess if synthesized features (1) allow to re-
trieve instances of the same class (Top-1 acc.) and (2) of
the desired attribute value. The latter is measured by the co-
efﬁcient of determination (R2). As seen in Table 4, the R2
scores indicate that we can actually retrieve instances with
the desired attribute values. Notably, even in cases where
R2 ≈ 0 (i.e., the linear model does not explain the variabil-
ity), the results still show decent Top-1 acc., revealing that
synthesis does not alter class membership.

4.4. Object based one shot scene recognition

3T1 = {picture, whiteboard, fridge, counter, books, stove,

cabinet, printer, computer, ottoman}

4T2 = {mug, telephone, bowl, bottle, scanner, microwave,

coffee table, recycle bin, cart, bench}

Motivation. We can also use AGA for a different type of
transfer, namely the transfer from object detection networks
to one-shot scene recognition. Although, object detection is

7461

Object
picture
fridge
books
cabinet
computer

Top-1
0.33
0.26
0.52
0.57
0.94

2

R
0.36
0.08
0.07
0.27
0.26

Object
whiteboard
counter
stove
printer
ottoman

Top-1
0.12
0.64
0.20
0.31
0.60

2

R
0.30
0.18
0.13
0.02
0.12

Table 4: Retrieval results for unseen objects (T1) when querying with
synthesized features of varying depth. Larger R2 values indicate a stronger
linear relationship (R2 ∈ [0, 1]) to the depth values of retrieved instances.

Method
max. pool (Baseline)
AGA FV (+D)
AGA FV (+P)
AGA CL-1 (+D, max.)
AGA CL-2 (+P, max.)
AGA CL-3 (+D, +P, max.)
Sem-FV [8]
AGA Sem-FV
Places [38]
AGA Places

Accuracy [%]
13.97
15.13
14.63
16.04
15.52
16.32
32.75
34.36
51.28
52.11

Table 5:
One-shot classiﬁcation on 25 indoor scene classes [26]:
{auditorium, bakery, bedroom, bookstore, children room, classroom, com-
puter room, concert hall, corridor, dental ofﬁce, dining room, hospital
room, laboratory, library, living room, lobby, meeting room, movie the-
ater, nursery, ofﬁce, operating room, pantry, restaurant}. For Sem-FV [8],
we use ImageNet CNN features extracted at one image scale.

a challenging task in itself, signiﬁcant progress is made, ev-
ery year, in competitions such as the ImageNet challenge.
Extending the gains in object detection to other related
problems, such as scene recognition, is therefore quite ap-
pealing. A system that uses an accurate object detector such
as an RCNN [14] to perform scene recognition, could gen-
erate comprehensive annotations for an image in one for-
ward pass. An object detector that supports one-shot scene
recognition could do so with the least amount of additional
data. It must be noted that such systems are different from
object recognition based methods such as [15, 8, 6], where
explicit detection of objects is not necessary. They apply
ﬁlters from object recognition CNNs to several regions of
images and extract features from all of them, whether or not
an object is found. The data available to them is therefore
enough to learn complex descriptors such as Fisher vectors
(FVs). A detector, on the other hand, may produce very
few features from an image, based on the number of ob-
jects found. AGA is tailor-made for such scenarios where
features from an RCNN-detected object can be augmented.

Setup. To evaluate AGA in this setting, we select a 25-class
subset of MIT Indoor [26], which may contain objects that
the RCNN is trained for. The reason for this choice is our
reliance on a detection CNN, which has a vocabulary of 19
objects from SUN RGB-D. At present, this is the largest
such dataset that provides objects and their 3D attributes.
The system can be extended easily to accommodate more
scene classes if a larger RGB-D object dataset becomes
available. As the RCNN produces very few detections per
scene image, the best approach, without augmentation, is

to perform pooling of RCNN features from proposals into
a ﬁxed-size representation. We used max-pooling as our
baseline. Upon augmentation, using predicted depth/ pose,
an image has enough RCNN features to compute a GMM-
based FV. For this, we use the experimental settings in [8].
The FVs are denoted as AGA FV(+D) and AGA FV(+P),
based on the attribute used to guide the augmentation. As
classiﬁer, we use a linear C-SVM with ﬁxed parameter (C).
Results. Table 5 lists the avgerage one-shot recognition ac-
curacy over multiple iterations. The beneﬁts of AGA are
clear, as both aug. FVs perform better than the max-pooling
baseline by 0.5-1% points. Training on a combination (con-
catenated vector) of the augmented FVs and max-pooling,
denoted as AGA CL-1, AGA CL-2 and AGA CL-3 fur-
ther improves by about 1-2% points. Finally, we com-
bined our aug. FVs with the state-of-the-art semantic FV
of [8] and Places CNN features [38] for one-shot classiﬁca-
tion. Both combinations, denoted AGA Sem-FV and AGA
Places, improved by a non-trivial margin (∼1% points).

5. Discussion

We presented an approach toward attribute-guided aug-
mentation in feature space. Experiments show that object
attributes, such as pose / depth, are beneﬁcial in the con-
text of one-shot recognition, i.e., an extreme case of lim-
ited training data. Notably, even in case of mediocre per-
formance of the attribute regressor (e.g., on pose), results
indicate that synthesized features can still supply useful in-
formation to the classiﬁcation process. While we do use
bounding boxes to extract object crops from SUN RGB-D
in our object-recognition experiments, this is only done to
clearly tease out the effect of augmentation. In principle, as
our encoder-decoder is trained in an object-agnostic man-
ner, no external knowledge about classes is required.

As SUN RGB-D exhibits high variability in the range of
both attributes, augmentation along these dimensions can
indeed help classiﬁer training. However, when variability is
limited, e.g., under controlled acquisition settings, the gains
may be less apparent. In that case, augmentation with re-
spect to other object attributes might be required.

Two aspects are speciﬁcally interesting for future work.
First, replacing the attribute regressor for pose with a specif-
ically tailored component will potentially improve learning
of the synthesis function(s) φk
i and lead to more realistic
synthetic samples. Second, we conjecture that, as additional
data with more annotated object classes and attributes be-
comes available (e.g., [2]), the encoder-decoder can lever-
age more diverse samples and thus model feature changes
with respect to the attribute values more accurately.

Acknowledgments. This work is supported by NSF awards
IIS-1208522, CCF-0830535, ECCS-1148870 and a gener-
ous donation of GPUs from Nvidia.

7462

[22] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ﬁelds
for depth estimation from a single image. In CVPR, 2015. 6
[23] E. Miller, N. Matsakis, and P. Viola. Learning from one-
example through shared density transforms. In CVPR, 2000.
3

[24] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 3

[25] X. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep ob-

ject detectors from 3d models. In ICCV, 2015. 2, 7

[26] A. Quattoni and A. Torralba. Recognizing indoor scenes. In

CVPR, 2009. 8

[27] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-

wards real-time object detection. In NIPS, 2015. 1

[28] G. Rogez and C. Schmid. MoCap-guided data augmentation
for 3D pose estimation in the wild. CoRR, abs/1607.02046,
2016. 2

[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and
Y. LeCun. OverFeat: Integrated recognition, localization and
detection using convolutional networks. In ICLR, 2014. 1

[30] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 1

[31] S. Song, S. Lichtenberg, and J. Xiao. SUN RGB-D: A RGB-
D scene understanding benchmark suite. In CVPR, 2015. 4,
5

[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. JMLR, 15:19291958, 2014. 4
[33] H. Su, C. Qi, Y. Li, and L. Guibas. Render for CNN: View-
point estimation in images using cnns trained with rendered
3d model views. In ICCV, 2015. 2

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[35] A. Torralba and A. Efros. Unbiased look at dataset bias. In

CVPR, 2011. 2

[36] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.
Selective search for object recognition. IJCV, 104(2):154–
171, 2013. 5

[37] M. Zeiler and R. Fergus. Visualizing and understanding con-

volutional networks. In ECCV, 2014. 2

[38] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using Places
database. In NIPS, 2014. 8

References

[1] Y. Bengio. Learning deep architectures for AI. Found. Trends

Mach. Learn., 2(1):1–127, 2009. 4

[2] A. Borji, S. Izadi, and L. Itti. iLab-20M: A large-scale con-
trolled object dataset to investigate deep learning. In CVPR,
2016. 8

[3] C. W. C.E. Rasmussen. Gaussian Processes for Machine

Learning. The MIT Press, 2005. 3

[4] C. Charalambous and A. Bharath. A data augmentation
methodology for training machine/deep learning gait recog-
nition algorithms. In BMVC, 2016. 2

[5] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. In BMVC, 2014. 2

[6] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for
texture recognition and segmentation. In CVPR, 2015. 1, 8
[7] D.-A. Clevert, T. Unterhiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(ELUs). In ICLR, 2016. 4

[8] M. Dixit, S. Chen, D. Gao, N. Rasiwasia, and N. Vascon-
celos. Scene classiﬁcation with semantic Fisher vectors. In
CVPR, 2015. 1, 8

[9] J. Donahue, Y. Jia, O. Vinyals, J. Huffman, N. Zhang,
E. Tzeng, and T. Darrell. DeCAF: A deep convolutional acti-
vation feature for generic visual recognition. In ICML, 2014.
1

[10] H. Drucker, C. Burges, L. Kaufman, and A. Smola. Support

vector regression machines. In NIPS, 1997. 3

[11] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C. J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
JMLR, 9(8):1871–1874, 2008. 5

[12] M. Fink. Object classiﬁcation from a single example utiliz-

ing relevance metrics. In NIPS, 2004. 7

[13] J. Gibbons and S. Chakraborti. Nonparametric Statistical
Inference. Chapman & Hall/CRC Press, 5th edition, 2011. 7

[14] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 5, 8
[15] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale
orderless pooling of deep convolutional activation features.
In ECCV, 2014. 1, 8

[16] S. Hauberg, O. Freifeld, A. Boensen, L. Larsen, J. F. III, and
L. Hansen. Dreaming more data: Class-dependent distribu-
tions over diffeomorphisms for learned data augmentation.
In AISTATS, 2016. 2

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 3

[18] M.

Jaderberg, K. Simonyan, A. Zisserman,
transformer networks.

Spatial

and
In

K. Kavukcuoglu.
NIPS, 2015. 3

[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 5

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[21] R. Kwitt, S. Hegenbart, and M. Niethammer. One-shot learn-
ing of scene locations via feature trajectory transfer.
In
CVPR, 2016. 2, 3

7463

