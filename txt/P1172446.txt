5
1
0
2
 
v
o
N
5

 

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
8
7
1
0
.
7
0
5
1
:
v
i
X
r
a

Rethinking LDA: Moment Matching for Discrete ICA

Anastasia Podosinnikova

Francis Bach

Simon Lacoste-Julien

INRIA - ´Ecole normale sup´erieure Paris

Abstract

We consider moment matching techniques for estimation in latent Dirichlet allo-
cation (LDA). By drawing explicit links between LDA and discrete versions of
independent component analysis (ICA), we ﬁrst derive a new set of cumulant-
based tensors, with an improved sample complexity. Moreover, we reuse standard
ICA techniques such as joint diagonalization of tensors to improve over existing
methods based on the tensor power method. In an extensive set of experiments on
both synthetic and real datasets, we show that our new combination of tensors and
orthogonal joint diagonalization techniques outperforms existing moment match-
ing methods.

1 Introduction

Topic models have emerged as ﬂexible and important tools for the modelisation of text corpora.
While early work has focused on graphical-model approximate inference techniques such as varia-
tional inference [1] or Gibbs sampling [2], tensor-based moment matching techniques have recently
emerged as strong competitors due to their computational speed and theoretical guarantees [3, 4].
In this paper, we draw explicit links with the independent component analysis (ICA) literature
(e.g., [5] and references therein) by showing a strong relationship between latent Dirichlet allocation
(LDA) [1] and ICA [6, 7, 8]. We can then reuse standard ICA techniques and results, and derive new
tensors with better sample complexity and new algorithms based on joint diagonalization.

2 Is LDA discrete PCA or discrete ICA?

0, 1

Notation. Following the text modeling terminology, we deﬁne a corpus X =
as a
x1, . . . , xN }
{
of Ln tokens. It is
collection of N documents. Each document is a collection
wn1, . . . , wnLn }
convenient to represent the ℓ-th token of the n-th document as a 1-of-M encoding with an indicator
M with only one non-zero, where M is the vocabulary size, and each document
vector wnℓ ∈ {
}
as the count vector xn :=
In such representation, the length Ln of the n-th
to refer to topics,the
document is Ln =
}
P
index n
to refer to words from
}
the vocabulary, and the index ℓ
to refer to tokens of the n-th document. The plate
∈ {
diagrams of the models from this section are presented in Appendix A.

RM .
m xnm. We will always use the index k
to refer to documents, the index m
1, . . . , N
}
P
1, . . . , Ln}

ℓ wnℓ ∈

∈ {
1, . . . , M

1, . . . , K

∈ {

∈ {

{

Latent Dirichlet allocation [1] is a generative probabilistic model for discrete data such as text
corpora. In accordance to this model, the n-th document is modeled as an admixture over the vo-
cabulary of M words with K latent topics. Speciﬁcally, the latent variable θn, which is sampled
from the Dirichlet distribution, represents the topic mixture proportion over K topics for the n-th
θn for the ℓ-th token is sampled from the multinomial dis-
document. Given θn, the topic choice znℓ|
znℓ, θn is then sampled from the multinomial
tribution with the probability vector θn. The token wnℓ|
distribution with the probability vector dznℓ, or dk if k is the index of the non-zero element in znℓ.
This vector dk is the k-th topic, that is a vector of probabilities over the words from the vocabulary
subject to the simplex constraint, i.e., dk ∈
.
}
This generative process of a document (the index n is omitted for simplicity) can be summarized as

∆M , where ∆M :=

m dm = 1

RM : d

0,

(cid:23)

∈

d

{

P

1

θ
θ
zℓ|
zℓ, θ

∼
∼
∼

wℓ|

Dirichlet(c),
Multinomial(1, θ),
Multinomial(1, dzℓ).

(1)

One can think of the latent variables zℓ as auxiliary variables which were introduced for convenience
of inference, but can in fact be marginalized out [9], which leads to the following model

θ
θ

x
|

∼
∼

Dirichlet(c),
Multinomial(L, Dθ),

LDA model (2)

RM

×

∈

K is the topic matrix with the k-th column equal to the k-th topic dk, and c

where D
++
is the vector of parameters for the Dirichlet distribution. While a document is represented as a set
of tokens wℓ in the formulation (1), the formulation (2) instead compactly represents a document as
the count vector x. Although the two representations are equivalent, we focus on the second one in
this paper and therefore refer to it as the LDA model.

∈

RK

Importantly, the LDA model does not model the length of documents. Indeed, although the original
paper [1] proposes to model the document length as L
Poisson(λ), this is never used in practice
and, in particular, the parameter λ is not learned. Therefore, in the way that the LDA model is
typically used, it does not provide a complete generative process of a document as there is no rule to
sample L
λ. In this paper, this fact is important, as we need to model the document length in order
to make the link with discrete ICA.

∼

λ

|

|

×

∈

RM

K is a transformation matrix and σ is a parameter.

Discrete PCA. The LDA model (2) can be seen as a discretization of principal component anal-
ysis (PCA) via replacement of the normal likelihood with the multinomial one and adjusting the
prior [9] in the following probabilistic PCA model [10, 11]: θ
Normal(Dθ, σ2IM ), where D
Discrete ICA (DICA). Interestingly, a small extension of the LDA model allows its interpreta-
tion as a discrete independent component analysis model. The extension naturally arises when the
document length for the LDA model is modeled as a random variable from the gamma-Poisson
mixture (which is equivalent to a negative binomial random variable), i.e., L
Poisson(λ) and
|
k ck is the shape parameter and b > 0 is the rate parameter. The
λ
LDA model (2) with such document length is equivalent (see Appendix B.1) to
αk ∼
α
∼

Gamma(ck, b),
Poisson([Dα]m),

Normal(0, IK ) and x
|

Gamma(c0, b), where c0 :=

GP model (3)

xm|

P

∼

∼

∼

∼

λ

θ

where all α1, α2, . . . , αK are mutually independent, the parameters ck coincide with the ones of the
LDA model in (2), and the free parameter b can be seen (see Appendix B.2) as a scaling parameter
for the document length when c0 is already prescribed.

This model was introduced by Canny [12] and later named as a discrete ICA model [13]. It is more
natural, however, to name model (3) as the gamma-Poisson (GP) model and the model

α1, . . . , αK ∼
xm|
∼

α

mutually independent,
Poisson([Dα]m)

DICA model (4)

as the discrete ICA (DICA) model. The only difference between (4) and the standard ICA model [6,
7, 8] (without additive noise) is the presence of the Poisson noise which enforces discrete, instead of
continuous, values of xm. Note also that (a) the discrete ICA model is a semi-parametric model that
can adapt to any distribution on the topic intensities αk and that (b) the GP model (3) is a particular
case of both the LDA model (2) and the DICA model (4).

Thanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to
derive new efﬁcient algorithms for topic modeling.

3 Moment matching for topic modeling

The method of moments estimates latent parameters of a probabilistic model by matching theoretical
expressions of its moments with their sample estimates. Recently [3, 4], the method of moments
was applied to different latent variable models including LDA, resulting in computationally fast

2

learning algorithms with theoretical guarantees. For LDA, they (a) construct LDA moments with a
particular diagonal structure and (b) develop algorithms for estimating the parameters of the model
by exploiting this diagonal structure. In this paper, we introduce the novel GP/DICA cumulants with
a similar to the LDA moments structure. This structure allows to reapply the algorithms of [3, 4]
for the estimation of the model parameters, with the same theoretical guarantees. We also consider
another algorithm applicable to both the LDA moments and the GP/DICA cumulants.

3.1 Cumulants of the GP and DICA models

In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is
a particular case of the DICA model, all results of this section extend to the GP model.

The ﬁrst three cumulant tensors for the random vector x can be deﬁned as follows

cum(x) := E(x),

cum(x, x) := cov(x, x) = E
E(x))

cum(x, x, x) := E [(x

(x

−
(x

E(x))(x
E(x))

−

(cid:2)
⊗

−

−

E(x))⊤
(x

,
E(x))] ,
(cid:3)

−

⊗

(5)

(6)

(7)

⊗

denotes the tensor product (see some properties of cumulants in Appendix C.1). The
where
essential property of the cumulants (which does not hold for the moments) that we use in this paper
is that the cumulant tensor for a random vector with independent components is diagonal.
Let y = Dα; then for the Poisson random variable xm|
E(xm|
expectation in (5) has the following form

Poisson(ym), the expectation is
ym) = ym. Hence, by the law of total expectation and the linearity of expectation, the

ym ∼

E(x) = E(E(x
|
ym) = ym and, as x1,
Further, the variance of the Poisson random variable xm is var(xm|
x2, . . . , xM are conditionally independent given y, then their covariance matrix is diagonal, i.e.,
y) = diag(y). Therefore, by the law of total covariance, the covariance in (6) has the form
cov(x, x
|

y)) = E(y) = DE(α).

(8)

cov(x, x) = E [cov(x, x
|

y)] + cov [E(x
|
= diag [E(y)] + cov(y, y) = diag [E(x)] + Dcov(α, α)D⊤,

y), E(x
|

y)]

(9)

where the last equality follows by the multilinearity property of cumulants (see Appendix C.1).
Moving the ﬁrst term from the RHS of (9) to the LHS, we deﬁne

−
From (9) and by the independence of α1, . . . , αK (see Appendix C.3), S has the following diagonal
structure

S := cov(x, x)

diag [E(x)] .

DICA S-cum. (10)

S =

var(αk)dkd⊤k = Ddiag [var(α)] D⊤.

(11)

By analogy with the second order case, using the law of total cumulance, the multilinearity property
of cumulants, and the independence of α1, . . . , αK, we derive in Appendix C.2 the expression (24),
similar to (9), for the third cumulant (7). Moving the terms in this expression, we deﬁne a tensor T
with the following element

[T ]m1m2m3 := cum(xm1 , xm2 , xm3 ) + 2δ(m1, m2, m3)E(xm1 )

DICA T-cum. (12)

δ(m2, m3)cov(xm1 , xm2 )

δ(m1, m3)cov(xm1 , xm2 )

δ(m1, m2)cov(xm1 , xm3),

−

−

−

where δ is the Kronecker delta. By analogy with (11) (Appendix C.3), the diagonal structure of the
tensor T :

k

T =

cum(αk, αk, αk)dk ⊗
In Appendix E.1, we recall (in our notation) the matrix S (39) and the tensor T (40) for the LDA
model [3], which are analogues of the matrix S (10) and the tensor T (12) for the GP/DICA mod-
els. Slightly abusing terminology, we refer to the matrix S (39) and the tensor T (40) as the LDA
moments and to the matrix S (10) and the tensor T (12) as the GP/DICA cumulants. The diagonal
structure (41) & (42) of the LDA moments is similar to the diagonal structure (11) & (13) of the
GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of

dk ⊗

X

dk.

(13)

k

X

3

Appendix E.1. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA
cumulants and the LDA moments coincide.
The following sample complexity results apply to the sample estimates of the GP cumulants:1
Proposition 3.1. Under the GP model, the expected error for the sample estimator
GP cumulant S (10) is:

S (29) for the

b

1
√N

E

S

S

E
≤ r

S

S

2
F

O

max

∆ ¯L2, ¯c0 ¯L

,

(14)

2

h

i

k

k

≤

kF
−
2, ¯c0 := min(1, c0) and ¯L := E(L).
b

k
−
h
where ∆ := max k k
dkk
b
A high probability bound could be derived using concentration inequalities for Poisson random
variables [14]; but the expectation already gives the right order of magnitude for the error (for
example via Markov’s inequality). The expression (29) for an unbiased ﬁnite sample estimate
S of S
T of T are deﬁned2 in Appendix C.4.
and the expression (30) for an unbiased ﬁnite sample estimate
A sketch of a proof for Proposition 3.1 can be found in Appendix D.

(cid:18)

(cid:19)

i

(cid:2)

(cid:3)

b

b

k

T

T

kF ]

∆, ¯c0/ ¯L
}

By following a similar analysis as in [15], we can rephrase the topic recovery error in term of the
error on the GP cumulant. Importantly, the whitening transformation (introduced in Section 4) redi-
vides the error on S (14) by ¯L2, which is the scale of S (see Appendix D.5 for details). This means
that the contribution from ˆS to the recovery error will scale as O(1/√N max
), where
{
both ∆ and ¯c0/ ¯L are smaller than 1 and can be very small. We do not present the exact expression
for the expected squared error for the estimator of T , but due to a similar structure in the derivation,
we expect the analogous bound of E[

≤
Current sample complexity results of the LDA moments [3] can be summarized as O(1/√N ). How-
ever, the proof (which can be found in the supplementary material [15]) analyzes only the case when
ﬁnite sample estimates of the LDA moments are constructed from one triple per document, i.e.,
w3 only, and not from the U-statistics that average multiple (dependent) triples per doc-
w1 ⊗
ument as in the practical expressions (43) and (44) (Appendix F.4). Moreover, one has to be careful
when comparing upper bounds. Nevertheless, comparing the bound (14) with the current theoretical
results for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the
ℓ2-norm of the columns of the topic matrix D in the numerator, as opposed to the O(1) coefﬁcient
for the LDA moments. This norm can be signiﬁcantly smaller than 1 for vectors in the simplex
(e.g., ∆ = O(1/
dkk0) for sparse topics). This suggests that the GP/DICA cumulants may have
better ﬁnite sample convergence properties than the LDA moments and our experimental results in
Section 5.2 are indeed consistent with this statement.

∆3/2 ¯L3, ¯c3/2

1/√N max

w2 ⊗

¯L3/2

−

b

k

{

}

0

.

The GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as
they are expressed via the count vectors x (which are the sufﬁcient statistics for the model) and
not the tokens wℓ’s. Note also that the construction of the LDA moments depend on the unknown
parameter c0. Given that we are in an unsupervised setting and that moreover the evaluation of
LDA is a difﬁcult task [16], setting this parameter is non-trivial. In Appendix G.4, we observe
experimentally that the LDA moments are somewhat sensitive to the choice of c0.

4 Diagonalization algorithms

How is the diagonal structure (11) of S and (13) of T going to be helpful for the estimation of the
model parameters? This question has already been thoroughly investigated in the signal processing
(see, e.g., [17, 18, 19, 20, 21, 5] and references therein) and machine learning (see [3, 4] and refer-
ences therein) literature. We review the approach in this section. Due to similar diagonal structure,
the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants.

For simplicity, let us rewrite the expressions (11) and (13) for S and T as follows

S =

skdkd⊤k ,

k

T =

tkdk ⊗

dk ⊗

k

dk,

(15)

1Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact

X

X

and, in general, depend on the prior on αk.

2For completeness, we also present the ﬁnite sample estimates

the LDA moments (which are consistent with the ones suggested in [4]) in Appendix F.4.

S (43) and
b

T (44) of S (39) and T (40) for
b

4

D

dk := √skdk,
where sk := var(αk) and tk := cum(αk, αk, αk). Introducing the rescaled topics
D⊤. Following the same assumption from [3] that the topic vectors are
we can also rewrite S =
M of S, i.e.,
linearly independent (
D is full rank), we can compute a whitening matrix W
a matrix such that W SW ⊤ = IK where IK is the K-by-K identity matrix (see Appendix F.1 for
more details). As a result, the vectors zk := W
e
RK

dk form an orthonormal set of vectors.
RK
K of a tensor
e

Further, let us deﬁne a projection

K onto a vector u

RK:

RK

(v)

e
×

∈

∈

∈

e

e

T

K

×

×

×

(u)k1k2 :=

T

T ∈
k3 Tk1k2k3 uk3.

(16)

X
Applying the multilinear transformation (see, e.g., [4] for the deﬁnition) with W ⊤ to the tensor T
RK,
from (15) and projecting the resulting tensor
we obtain

:= T (W ⊤, W ⊤, W ⊤) onto some vector u

∈

T

T

(u) =

tk := tk/s3/2

tkh
stands for the inner product. As the
where
e
vectors zk are orthonormal, the pairs zk and λk :=
(u),
tkh
which are uniquely deﬁned if the eigenvalues λk are all different. If they are unique, we can recover
the GP/DICA (as well as LDA) model parameters via
e

·i
are the eigenpairs of the matrix

is due to the rescaling of topics and

dk = W †zk and

h·
zk, u
i

zkz⊤k ,
i

tk = λk/

zk, u

X

e

T

(17)

k

k

,

zk, u
h

.
i

e

e

b

This procedure was referred to as the spectral algorithm for LDA [3] and the fourth-order3 blind
identiﬁcation algorithm for ICA [17, 18]. Indeed, one can expect that the ﬁnite sample estimates
T (30) possess approximately the diagonal structure (11) and (13) and, therefore, the rea-
S (29) and
soning from above can be applied, assuming that the effect of the sampling error is controlled.
b
This spectral algorithm, however, is known to be quite unstable in practice (see, e.g., [22]). To over-
come this problem, other algorithms were proposed. For ICA, the most notable ones are probably
the FastICA algorithm [20] and the JADE algorithm [21]. The FastICA algorithm, with appropriate
choice of a contrast function, estimates iteratively the topics, making use of the orthonormal struc-
ture (17), and performs the deﬂation procedure at every step. The recently introduced tensor power
method (TPM) for the LDA model [4] is close to the FastICA algorithm. Alternatively, the JADE al-
gorithm modiﬁes the spectral algorithm by performing multiple projections for (17) and then jointly
diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special
case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Impor-
tantly, a fast implementation [23] of the orthogonal joint diagonalization algorithm from [24] was
proposed, which is based on closed-form iterative Jacobi updates (see, e.g., [25] for the later).

In practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see,
e.g., [26, p. 30]) or the spectral algorithm. Moreover, although the application of the JD algorithm
for the learning of topic models was mentioned in the literature [4, 27], it was never implemented in
practice. In this paper, we apply the JD algorithm for the diagonalization of the GP/DICA cumulants
as well as the LDA moments, which is described in Algorithm 1. Note that the choice of a projection
RK is important and corresponds to
RM obtained as vp =
W ⊤up for some vector up ∈
vector vp ∈
W ⊤ along the third mode. Importantly, in Algorithm 1, the
the multilinear transformation of
T with
joint diagonalization routine is performed over (P + 1) matrices of size K
K, where the number of
c
topics K is usually not too big. This makes the algorithm computationally fast (see Appendix G.1).
b
The same is true for the spectral algorithm, but not for TPM.

c

×

In Section 5.1, we compare experimentally the performance of the spectral, JD, and TPM algorithms
for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of
any experimental comparison of these algorithms in the LDA context. While already working on
this manuscript, the JD algorithm was also independently analyzed by [27] in the context of tensor
factorization for general latent variable models. However, [27] focused mostly on the comparison
of approaches for tensor factorization and their stability properties, with brief experiments using a
latent variable model related but not equivalent to LDA for community detection. In contrast, we
provide a detailed experimental comparison in the context of LDA in this paper, as well as propose
a novel cumulant-based estimator. Due to the space restriction the estimation of the topic matrix D
and the (gamma/Dirichlet) parameter c are moved to Appendix F.6.

3See Appendix C.5 for a discussion on the orders.

5

Algorithm 1 Joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)

RM

N , K, P (number of random projections); (and c0 for LDA moments)

M ((29) for GP/DICA / (43) for LDA in Appendix F)

RM

1: Input: X
∈
2: Compute sample estimate
3: Estimate whitening matrix

×

×

×
RK

S (see Appendix F.1)

S
∈
M of
W
∈
option (a): Choose vectors
u1, u2, . . . , uP } ⊆
b
{
RM for all p = 1, . . . , P
sphere and set vp =
W ⊤up ∈
c
option (b): Choose vectors
u1, u2, . . . , uP } ⊆
{
RM for all p = 1, . . . , K
RK and set vp =
c
W ⊤up ∈
RK
4: For
p, compute Bp =
W
c
5: Perform orthogonal joint diagonalization of matrices
W
{
RK
c
K and vectors
(see [24] and [23]) to ﬁnd an orthogonal matrix V
×
such that
c
diag(ap), p = 1, . . . , P

∈
W ⊤V ⊤ = IK , and V BpV ⊤

T (vp)

W ⊤

c

c

W

∈

S

S

V

∀

b

b

b

{

×

RK uniformly at random from the unit ℓ2-
(P = 1 yields the spectral algorithm)
RK as the canonical basis e1, e2, . . . , eK of

W ⊤ = IK, Bp, p = 1, . . . , P

a1, a2, . . . , aP } ⊂

}
RK

K ((52) for GP/DICA / (54) for LDA; Appendix F)

6: Estimate joint diagonalization matrix A = V
b
7: Output: Estimate of D and c as described in Appendix F.6

c

c

W and values ap, p = 1, . . . , P

≈

c

5 Experiments

In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and
(b) the spectral algorithm [3], the tensor power method [4] (TPM), the joint diagonalization (JD)
algorithm from Algorithm 1, and variational inference for LDA [1].

Real data: the associated press (AP) dataset, from D. Blei’s web page,4 with N = 2, 243 documents
L = 194; the NIPS papers
and M = 10, 473 vocabulary words and the average document length
dataset5 [28] of 2, 483 NIPS papers and 14, 036 words, and
L = 1, 321; the KOS dataset,6 from the
UCI Repository, with 3, 430 documents and 6, 906 words, and
b

Semi-synthetic data are constructed by analogy with [29]: (1) the LDA parameters D and c are
learned from the real datasets with variational inference and (2) toy data are sampled from a model
of interest with the given parameters D and c. This provides the ground truth parameters D and c.
For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are
the minimum and maximum values. For the AP data, K
topics are learned and, for
topics are learned. For larger K, the obtained topic matrix is ill-
the NIPS data, K
conditioned, which violates the identiﬁability condition for topic recovery using moment matching
techniques [3]. All the documents with less than 3 tokens are resampled.

L = 136.

10, 50

10, 90

∈ {

∈ {

b

b

}

}

Sampling techniques. All the sampling models have the parameter c which is set to c = c0¯c/
k1,
¯c
k
where ¯c is the learned c from the real dataset with variational LDA, and c0 is a parameter that we
can vary. The GP data are sampled from the gamma-Poisson model (3) with b = c0/
L so that
the expected document length is
L (see Appendix B.2). The LDA-ﬁx(L) data are sampled from the
LDA model (2) with the document length being ﬁxed to a given L. The LDA-ﬁx2(γ,L1,L2) data
γ)-portion of the documents are sampled from the LDA-ﬁx(L1) model
are sampled as follows: (1
with a given document length L1 and γ-portion of the documents are sampled from the LDA-ﬁx(L2)
model with a given document length L2.

−

b

b

Evaluation. The evaluation of topic recovery for semi-synthetic data is performed with the ℓ1-
D and true D topic matrices with the best permutation of columns:
error between the recovered
1
errℓ1(
[0, 1]. The minimization is over the possible
D, D) := minπ
dπk −
dkk1 ∈
2K
b
permutations π
D and can be efﬁciently obtained with the Hungarian
PERM of the columns of
b
b
algorithm for bipartite matching. For the evaluation of topic recovery in the real data case, we use
an approximation of the log-likelihood for held out documents as the metric [16]. See Appendix G.6
for more details.

PERM

k k

P

∈

b

∈

4http://www.cs.columbia.edu/˜blei/lda-c
5http://ai.stanford.edu/˜gal/data
6https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

6

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

JD
JD(k)
JD(f)
Spec
TPM

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

Figure 1: Comparison of the diagonalization algorithms. The topic matrix D and Dirichlet parameter c are
learned for K = 50 from AP; c is scaled to sum up to 0.5 and b is set to ﬁt the expected document length
L = 200. The semi-synthetic dataset is sampled from GP; number of documents N varies from 1, 000 to
b
50, 000. Left: GP/DICA moments. Right: LDA moments. Note: a smaller value of the ℓ1-error is better.

We use our Matlab implementation of the GP/DICA cumulants, the LDA moments, and the diag-
onalization algorithms. The datasets and the code for reproducing our experiments are available
online.7 In Appendix G.1, we discuss the complexity and implementation of the algorithms. We
explain how we initialize the parameter c0 for the LDA moments in Appendix G.3.

5.1 Comparison of the diagonalization algorithms

In Figure 1, we compare the diagonalization algorithms on the semi-synthetic AP dataset for K = 50
using the GP sampling. We compare the tensor power method (TPM) [4], the spectral algorithm
(Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm 1 with different
options to choose the random projections: JD(k) takes P = K vectors up sampled uniformly from
the unit ℓ2-sphere in RK and selects vp = W ⊤up (option (a) in Algorithm 1); JD selects the full basis
e1, . . . , eK in RK and sets vp = W ⊤ep (as JADE [21]) (option (b) in Algorithm 1); JD(f ) chooses
the full canonical basis of RM as the projection vectors (computationally expensive).

Both the GP/DICA cumulants and LDA moments are well-speciﬁed in this setup. However, the
LDA moments have a slower ﬁnite sample convergence and, hence, a larger estimation error for the
same value N . As expected, the spectral algorithm is always slightly inferior to the joint diagonal-
ization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms
demonstrate good performance, which also fulﬁlls our expectations. However, although TPM shows
almost perfect performance in the case of the GP/DICA cumulants (left), it signiﬁcantly deteriorates
for the LDA moments (right), which can be explained by the larger estimation error of the LDA
moments and lack of robustness of TPM. The running times are discussed in Appendix G.2. Over-
all, the orthogonal joint diagonalization algorithm with initialization of random projections as W ⊤
multiplied with the canonical basis in RK (JD) is both computationally efﬁcient and fast.

5.2 Comparison of the GP/DICA cumulants and the LDA moments

In Figure 2, when sampling from the GP model (top, left), both the GP/DICA cumulants and LDA
moments are well speciﬁed, which implies that the approximation error (i.e., the error w.r.t.
the
model (mis)ﬁt) is low for both. The GP/DICA cumulants achieve low values of the estimation error
already for N = 10, 000 documents independently of the number of topics, while the convergence
is slower for the LDA moments. When sampling from the LDA-ﬁx(200) model (top, right), the
GP/DICA cumulants are mis-speciﬁed and their approximation error is high, although the estimation
error is low due to the faster ﬁnite sample convergence. One reason of poor performance of the
GP/DICA cumulants, in this case, is the absence of variance in the document length. Indeed, if
documents with two different lengths are mixed by sampling from the LDA-ﬁx2(0.5,20,200) model
(bottom, left), the GP/DICA cumulants performance improves. Moreover, the experiment with a
changing fraction γ of documents (bottom, right) shows that a non-zero variance on the length
improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a
non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not
likely to happen.

7 https://github.com/anastasia-podosinnikova/dica

7

r
o
r
r
e
-
1

ℓ

r
o
r
r
e
-
1

ℓ

0
1 

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0
1 

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11.5

-12

-12.5

-13

-13.5

r
o
r
r
e
-
1

ℓ

r
o
r
r
e
-
1

ℓ

0
1 

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

0.8
0.2
Fraction of doc lengths γ

0.6

0.4

1

Figure 2: Comparison of the GP/DICA cumulants and LDA moments. Two topic matrices and parameters c1
and c2 are learned from the NIPS dataset for K = 10 and 90; c1 and c2 are scaled to sum up to c0 = 1.
Four corpora of different sizes N from 1, 000 to 50, 000: top, left: b is set to ﬁt the expected document length
L = 1300; sampling from the GP model; top, right: sampling from the LDA-ﬁx(200) model; bottom, left:
b
sampling from the LDA-ﬁx2(0.5,20,200) model. Bottom, right: the number of documents here is ﬁxed to
N = 20, 000; sampling from the LDA-ﬁx2(γ,20,200) model varying the values of the fraction γ from 0 to 1
with the step 0.1. Note: a smaller value of the ℓ1-error is better.

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 3: Experiments with real data. Left: the AP dataset. Right: the KOS dataset. Note: a higher value of
the log-likelihood is better.

5.3 Real data experiments

In Figure 3, JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI)
and with variational inference initialized with the output of JD-GP (VI-JD). We measure the held
out log-likelihood per token (see Appendix G.7 for details on the experimental setup). The orthogo-
nal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates promising
performance. In particular, the GP/DICA cumulants signiﬁcantly outperform the LDA moments.
Moreover, although variational inference performs better than the JD-GP algorithm, restarting varia-
tional inference with the output of the JD-GP algorithm systematically leads to better results. Similar
behavior has already been observed (see, e.g., [30]).

6 Conclusion

In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where
word counts are directly modelled. These moments make fewer assumptions regarding distributions,
and are theoretically and empirically more robust than previously proposed tensors for LDA, both
on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization
procedure is also more robust. Once the topic matrix has been estimated in a semi-parametric way
where topic intensities are left unspeciﬁed, it would be interesting to learn the unknown distributions
of the independent topic intensities.

Aknowledgements. This work was partially supported by the MSR-Inria Joint Center. The authors
would like to thank Christophe Dupuy for helpful discussions.

8

References
[1] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:903–1022,

[2] T. Grifﬁths. Gibbs sampling in the generative model of latent Dirichlet allocation. Technical report,

[3] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

2003.

Stanford University, 2002.

allocation. In NIPS, 2012.

[4] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning

latent variable models. J. Mach. Learn. Res., 15:2773–2832, 2014.

[5] P. Comon and C. Jutten, editors. Handbook of blind sourse separation: independent component analysis

[6] C. Jutten. Calcul neuromim´etique et traitement du signal: analyse en composantes ind´ependantes. PhD

and applications. Academic Press, 2010.

thesis, INP-USM Grenoble, 1987.

[7] C. Jutten and J. H´erault. Blind separation of sources, part I: an adaptive algorithm based on neuromimetric

architecture. Signal Process., 24:1–10, 1991.

[8] P. Comon. Independent component analysis, a new concept? Signal Process., 36:287–314, 1994.

[9] W.L. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002.

[10] M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. R. Stat. Soc., 61:611–622,

1999.

[11] S. Roweis. EM algorithms for PCA and SPCA. In NIPS, 1998.

[12] J. Canny. GaP: a factor model for discrete data. In SIGIR, 2004.

[13] W.L. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.

[14] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of inde-

[15] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

pendence. Oxford University Press, 2013.

allocation. CoRR, abs:1204.6703, 2013.

[16] H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In

[17] J.-F. Cardoso. Source separation using higher order moments. In ICASSP, 1989.

[18] J.-F. Cardoso. Eigen-structure of the fourth-order cumulant tensor with application to the blind source

separation problem. In ICASSP, 1990.

[19] J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods. In

[20] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Trans.

Neural Netw., 10(3):626–634, 1999.

[21] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. In IEE Proceedings-F,

[22] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Comput., 11:157–192,

[23] J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal.

Appl., 17(1):161–164, 1996.

[24] A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simulataneous diagonalization.

SIAM J. Matrix Anal. Appl., 14(4):927–949, 1993.

[25] J. Nocedal and S.J. Wright. Numerical optimization. Springer, 2nd edition, 2006.

[26] F.R. Bach and M.I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1–48, 2002.

[27] V. Kuleshov, A.T. Chaganty, and P. Liang. Tensor factorization via matrix factorization. In AISTATS,

[28] A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. J.

Mach. Learn. Res., 8:2265–2295, 2007.

[29] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm

for topic modeling with provable guarantees. In ICML, 2013.

[30] S. Cohen and M. Collins. A provably correct learning algorithm for latent-variable PCFGs. In ACL, 2014.

ICML, 2009.

ISCAS, 1996.

1993.

1999.

2015.

9

A Appendix. Plate diagrams for the models from Section 2

c

θn

znℓ

wnℓ

N

N

N

c

θn

xn

c

αn

N

αn

M

xnm

M

xnm

D

Ln

D

D

D

(a) LDA (18)

(b) LDA (19)

(c) GP (20)

(d) DICA (21)

Figure 4: Plate diagrams for the models from Section 2.

In Section 2, the index n, which stands for the n-th document, was omitted. For convenience, we
recall the models. The LDA model in the tokens representation:

the LDA model with the marginalized out latent variable z:

θn ∼
znℓ|
θn ∼
znℓ, θn ∼

wnℓ|

Dirichlet(c),
Multinomial(1, θn),
Multinomial(1, dznℓ);

θn ∼
θn ∼

xn|

Dirichlet(c),
Multinomial(Ln, Dθn);

αnk ∼
αn ∼

xnm|

Gamma(ck, b),
Poisson([Dαn]m);

αn1, . . . , αnK ∼
αn ∼
xnm|

mutually independent,
Poisson([Dαn]m).

the GP model:

and the DICA model:

(18)

(19)

(20)

(21)

B Appendix. The GP model

B.1 The connection between the LDA and GP models

To show that the LDA model (2) with the additional assumption that the document length is modeled
as a gamma-Poisson random variable is equivalent to the GP model (3), we show that:

- when modeling the document length L as a Poisson random variable with a parameter λ,
the count vectors x1, x2, . . . , xM are mutually independent Poisson random variables;

- the Gamma prior on λ reveals the connection αk = λθk between the Dirichlet random

variable θ and the mutually independent gamma random variables α1, α2, . . . , αK.

For completeness, we repeat
the known result
∼
Multinomial(L, Dθ) (which thus means that L =
m xm with probability one), then x1, x2,
. . . , xM are mutually independent Poisson random variables with parameters λ [Dθ]1, λ [Dθ]2, . . . ,

Poisson(λ) and x
|

if L

that

∼

L

P

10

λ [Dθ]M . Indeed, we consider the following joint probability mass function where x and L are
assumed to be non-negative integers:

p(x, L

θ, λ) =p(L

|

λ)p(x
|

|

L, θ)

L=Pm xm

exp (

λ) λL

−
✚✚L!

✚✚L!
m xm!

[Dθ]xm
m

L=Pm xm

exp(

λ

−

L=Pm xm

m
Y

Q
[Dθ]m)λPm xm

[Dθ]xm
m
xm!

exp(

m
m
Y
X
λ [Dθ]m)(λ [Dθ]m)xm
−

xm!

L=Pm xm

Poisson(xm; λ [Dθ]m),

=1

=1

=1

=1

{

{

{

{

}

}

}

}

m
Y

m
Y

where in the third equation we used the fact that

[Dθ]m =

Dmkθk =

θk

Dmk = 1.

m
X
θ, λ) = p(L

Xm,k
m p(xm|

Xk

m
X

|

{

L=Pm xm

x)
|
and p(xm|

x) is simply the deterministic
We thus have p(x, L
|
distribution 1
λ[Dθ]m) for m = 1, . . . , M are independent Poisson(λ[Dθ]m)
Q
distributions (and thus do not depend on L). Note that in the notation introduced in the paper,
Dmk = dkm. Hence, by using the construction of the Dirichlet distribution from the normalization
of independent gamma random variables, we can show that the LDA model with a gamma-Poisson
prior over the length is equivalent to the following model (recall, that c0 =

λ[Dθ]m) where p(L

}

k ck):

λ
θ
λ, θ

∼
∼
∼

xm|

Gamma(c0, b),
Dirichlet(c),
Poisson([D(λθ)]m).

P

(22)

More speciﬁcally, we complete the second part of the argument with the following properties. When
Gamma(ck, b),
α1, α2, . . . , αK are mutually independent gamma random variables, each αk ∼
k ck, b). The former is equivalent
their sum is also a gamma random variable
k αk ∼
to λ. It is known (e.g., [32]) that a Dirichlet random variable can be sampled by ﬁrst sampling
independent gamma random variables (αk) and then dividing each of them by their sum (λ): θk =
k′ αk′ , and, in other direction, the variables αk = λθk are mutually independent, giving back
αk/
the GP model (3).

Gamma(

P

P

P

B.2 The expectation and the variance of the document length for the GP model

From the drivations in Appendix B.1, it follows that the document length of the GP model (3) is a
gamma-Poisson random variable, i.e., L
Gamma(c0, b). Therefore, the
|
following follows from the law of total expectation and the law of total variance

Poisson(λ) and λ

∼

∼

λ

E(L) = E [E(L
|
var(L) = var [E(L

λ)] = E(λ) = c0/b
λ)] + E [var(L

λ)] = var(λ) + E(λ) = c0/b + c0/b2

|

|
The ﬁrst expression shows that the parameter b controls the expected document length E(L) for a
given parameter c0: the smaller b, the larger E(L). On the other hand, if we allow c0 to vary as
well, only the ratio c0/b is important for the document length. We can then interpret the role of c0
as actually controlling the concentration of the distribution for the length L (through the variance).
More speciﬁcally, we have that:

var(L)
(E(L))2 =

1
E(L)

+

1
c0

.

(23)

For a ﬁxed target document length E(L), we can increase the variance (and thus decrease the con-
centration) by using a smaller c0.

11

C Appendix. The cumulants of the GP and DICA models

C.1 Cumulants

For a random vector x

RM , the ﬁrst three cumulant tensors8 are

∈

cum(xm) = E(xm),
cum(xm1 , xm2 ) = E [(xm1 −
cum(xm1 , xm2 , xm3 ) = E [(xm1 −

E(xm1 ))(xm2 −
E(xm1 ))(xm2 −

E(xm2 )] = cov(xm1 , xm2),
E(xm3 )))] .
E(xm2 ))(xm3 −

Note that the 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not for
RM
M denotes the third order tensor with ele-
higher orders). In the following, cum(x, x, x)
ments cum(xm1 , xm2 , xm3). Some of the properties of cumulants are listed below (see [5, chap. 5]).
The most important property that motivate us to use cumulants in this paper (and the ICA literature)
is the independence property, which says that the cumulant tensor for a random vector with inde-
pendent components is diagonal (this property does not hold for the (non-central) moment tensors
of any order, and neither for the central moments of order 4 or more).

∈

M

×

×

RM are independent, then their cross-cumulants
- Independence. If the elements of x
are zero as soon as two indices are different, i.e., cum(xm1 , xm2 ) = δ(m1, m2)E[(xm1 −
E(xm1 ))3], where δ is the
Em1 ))2] and cum(xm1 , xm2 , xm3 ) = δ(m1, m2, m3)E[(xm1 −
Kronecker delta.

∈

- Multilinearity. If two random vectors y

y = Dα for some D

RM

K, then

×

∈

∈

RM and α

RK are linearly dependent, i.e.,

∈

cum(ym) =

cum(αk)Dmk,

cum(ym1, ym2) =

cum(αk1 , αk2 )Dm1k1Dm2k2,

cum(ym1 , ym2, ym3) =

cum(αk1 , αk2 , αk3 )Dm1k1 Dm2k2 Dm3k3 ,

Xk

Xk1,k2

Xk1,k2,k3

which can also be denoted9 by

E(y) = DE(α),

cov(y, y) = Dcov(α, α)D⊤,

cum(y, y, y) = cum(α, α, α)(D⊤, D⊤, D⊤).

- The law of total cumulance. For two random vectors x

RM and y

RM , it holds

∈

∈

y)] + cov [E(xm1 |

y), E(xm2 |

cum(xm) = E [E(xm|
cum(xm1 , xm2 ) = E [cov(xm1 , xm2 |
cum(xm1 , xm2 , xm3 ) = E [cum(xm1 , xm2, xm3 |

y)] ,

y), cov(xm2 , xm3 |
y), cov(xm1 , xm3 |
y), cov(xm1 , xm2 |
Note that the ﬁrst expression is also well known as the law of total expectation or the tower
property, while the second one is known as the law of total covariance.

+ cov [E(xm1 |
+ cov [E(xm2 |
+ cov [E(xm3 |

y)] + cum [E(xm1 |
y)]
y)]
y)] .

y)] ,
y), E(xm2 |

y), E(xm3 |

y)]

tX

8Strictly speaking, the (scalar) n-th cumulant κn of a random variable X is deﬁned via the cumulant-
generating function g(t), which is the natural logarithm of the moment-generating function, i.e g(t) :=
log E (cid:2)e
(cid:3). The cumulant κn is then obtained from a power series expansion of the cumulant-generating
function, that is g(t) = P

9In [4], given a tensor T ∈ RK×K×K , T (D⊤, D⊤, D⊤) is referred to as the multilinear map. In [34], the

/n! [Wikipedia].

∞
n=1 κnt

same entity is denoted by T ×1 D⊤ ×2 D⊤ ×3 D⊤, where ×n denotes the n-mode tensor-matrix product.

n

12

C.2 The third cumulant of the GP/DICA models

In this section, by analogy with Section 3.1, we derive the third GP/DICA cumulant.
As the third cumulant of a Poisson random variable xm with parameter ym is E((xm −
E(xm))3
ym) = ym, then by the independence property of cumulants from Section C.1, the cu-
mulant of x
|

y is diagonal:

|

cum(xm1 , xm2 , xm3 |

y) = δ(m1, m2, m3) ym1.

y into the law of total cumulance, we obtain

Substituting the cumulant of x
|
cum(xm1 , xm2 , xm3) = E [cum(xm1 , xm2 , xm3 |
y), E(xm2 |
+ cum [E(xm1 |
+ cov [E(xm2 |
y), cov(xm1 , xm3|
= δ(m1, m2, m3)E(ym1 ) + cum(ym1, ym2, ym3)

y), E(xm3 |

y)] + cov [E(xm3 |

y)]
y)] + cov [E(xm1 |

y), cov(xm2 , xm3 |
y)]

y), cov(xm1 , xm2|

y)]

+ δ(m2, m3)cov(ym1, ym2) + δ(m1, m3)cov(ym1, ym2) + δ(m1, m2)cov(ym1, ym3 )

= δ(m1, m2, m3)E(xm1 ) + cum(ym1 , ym2, ym3)

+ δ(m2, m3)cov(xm1 , xm2 )
+ δ(m1, m3)cov(xm1 , xm2 )
+ δ(m1, m2)cov(xm1 , xm3 )

δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )

−
−
−

= cum(ym1 , ym2, ym3)

2δ(m1, m2, m3)E(xm1 )

−

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 )

=

cum(α, α, α)(D⊤, D⊤, D⊤)

2δ(m1, m2, m3)E(xm1 )

m1m2m3 −

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 ),
(cid:2)
where, in the third equality, we used the previous result from (9) that cov(y, y) = cov(x, x)
diag(E(x)).

(cid:3)

(24)

−

C.3 The diagonal structure of the GP/DICA cumulants

In this section, we provide detailed derivation of the diagonal structure (11) of the matrix S (10) and
the diagonal structure (13) of the tensor T (12).

From the independence of α1, α2, . . . , αK and by the independence property of cumulants
it follows that cov(α, α) is a diagonal matrix and cum(α, α, α) is a
from Section C.1,
i.e., cov(αk1 , αk2 ) = δ(k1, k2)cov(αk1 , αk2 ) and cum(αk1 , αk2 , αk3 ) =
diagonal
δ(k1, k2, k3)cum(αk1 , αk1 , αk1 ). Therefore, the following holds

tensor,

cov(ym1 , ym2) =

cov(αk, αk)Dm1kDm2k,

cum(ym1 , ym2, ym3) =

cum(αk, αk, αk)Dm1kDm2kDm3k,

which we can rewrite in a matrix/tensor form as

cov(y, y) =

cov(αk, αk)dkd⊤k ,

cum(y, y, y) =

cum(αk, αk, αk)dk ⊗

dk ⊗

dk.

Moving cov(y, y) / cum(y, y, y) in the expression for cov(x, x) (9) / cum(x, x, x) (24) on one side of
M
equality and all other terms on the other side, we deﬁne matrix S
as follows

M / tensor T

RM

RM

∈

∈

M

×

×

×

S := cov(x, x)

diag (E(x)) ,
Tm1m2m3 := cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)E(xm1 )

−

Xk

Xk

Xk

Xk

(25)

(26)

δ(m2, m3)cov(xm1 , xm2 )
δ(m1, m3)cov(xm1 , xm2 )
δ(m1, m2)cov(xm1 , xm3 ).

−
−
−

13

By construction, S = cov(y, y) and T = cum(y, y, y) and, therefore, it holds that

S =

cov(αk, αk)dkd⊤k ,

Xk

(27)

(30)

(31)

T =

cum(αk, αk, αk)dk ⊗
This means that both the matrix S and the tensor T are sums of rank-1 matrices and tensors, respec-
tively10. This structure of the matrix S and the tensor T is the basis for the algorithms considered in
this paper.

dk ⊗

Xk

dk.

(28)

C.4 Unbiased ﬁnite sample estimators for the GP/DICA cumulants

Given a sample
the GP/DICA cumulants:

x1, x2, . . . , xN }
{

, we obtain a ﬁnite sample estimate

S of S (10) /

T of T (12) for

S :=

cov(x, x)

diag

−

,

E(x)
(cid:17)

(cid:16)

b

b

(29)

E(xm1 )

b

b

Tm1m2m3 :=

cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)
b
d
δ(m2, m3)
δ(m1, m3)
δ(m1, m2)

d

cov(xm1 , xm2 )
−
cov(xm1 , xm2 )
−
d
cov(xm1 , xm3 ),
−
where unbiased estimators of the ﬁrst three cumulants are
d
1
d
N

E(xm1 ) =

xnm1 ,

b

b
cov(xm1 , xm2 ) =

znm1znm2 ,

cum(xm1 , xm2, xm3 ) =

d

znm1znm2znm3 ,

2)

−

n
X

d

where the word vocabulary indexes are m1, m2, m3 = 1, 2, . . . , M and the centered documents
E(xm). (The latter is introduced only for compact representation of (31) and is
znm := xnm −
different from z in the LDA model.)
b
C.5 On the orders of cumulants

DU )(

DU )⊤ with any orthogonal K

D as one can equivalently use
Note that the factorization of S =
K matrix U . Therefore, one has to consider higher
S = (
than the second order information. Moreover, in ICA the fourth-order tensors are used, because the
third cumulant of the Gaussian distribution is zero, which is not the case in the DICA/LDA models,
where the third order information is sufﬁcient.

D⊤ does not uniquely determine

D

×

e

e

e

e

e

n
X
1

1

−

N

(N

−

n
X
N
1)(N

D Appendix. The sketch of the proof for Proposition 3.1

D.1 Expected squared error for the sample expectation

The sample expectation is

n xn is an unbiased estimator of the expectation and:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

−

E(xm)
(cid:17)

2

(cid:21)

E(x) = 1
N
E(xm)
P

E

(cid:17)

=

=

=

b
m
X
1
N 2

1
N

(cid:20)(cid:16)

b
E



m
X

E

 

n
X
(xm −
h

m
X

!



n
X

var(xm).

=n′
Xn

E(xm))2

=

1
N

i

m
X

(xnm −

E(xm))2

+ E

(xnm −

E(xm)) (xn′m −

E(xm))









10For tensors, such decomposition is also known under the names CANDECOMP/PARAFAC or, simply, the

CP decomposition (see, e.g., [34]).

14

[E(var(xm|

y)) + var(E(xm|

y))] =

[E(ym) + var(ym)]

1
N

m
X

Further, by the law of total variance:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

(cid:17)

=

=

1
N

m
X

1
N "

using the fact that

P

E(αk) +

dk, dki
h

var(αk)

,

#

Xk
m Dmk = 1 for any k.

Xk

D.2 Expected squared error for the sample covariance

The following ﬁnite sample estimator of the covariance cov(x, x) = E(xx⊤)

E(x)E(x)⊤

−

E(x)

E(x)⊤ =

1

N

1

−

n  
X

xnx⊤n −

1
N 2

xn′ x⊤n′′

!

Xn′

Xn′′

cov(x, x) =

d

N

1
N

=

1

1

−

n
X

xnx⊤n −

xnx⊤n −



N

n
X



b
xn

b
1

1

−

=n

Xn′

x⊤n′





is unbiased, i.e., E(

cov(x, x)) = cov(x, x). Its squared error is

E

d
cov(x, x)

cov(x, x)
k

−

2
F

=

E

cov(xm, xm′ )
(

E[

cov(xm, xm′ )])2

.

−

Xm,m′

h

(cid:1)

d

d

i

k
(cid:0)

d

The m, m′-th element of the sum above is equal to

xnm

xn′′m′

,

xn′mxn′m′

xn′m

xn′′′m′

=n′
Xn′′′





cov

xnm

xn′′m′ , xn′mxn′m′

−

N

1

1

−

=n

Xn′′

cov

xnmxnm′



−

N

1

1

−


cov (xnmxnm′ , xn′mxn′m′ )

=n

Xn′′

2
N 2(N

−

xn′′m′, xn′m

xn′′′m′

1)

−

Xn,n′

=n′
Xn′′′









1)2

cov

xnm



−

Xn,n′

Xn′′

cov (xnmxnm′ , xnmxnm′ )

=n

(32)







1
N 2

Xn,n′
1
N 2

Xn,n′
1
N 2(N

=

+

=

1
N 2

n
X
2
N 2(N

1
N 2(N

−

+

+

1
N 2(N

cov (xnmxn′′m′ , xnmxnm′ ) +

cov (xnmxn′m′, xn′mxn′m′ )

1) 

−



n
X

=n

Xn′′

1)2 



1)2 

−

−

n
X

Xn′′

=n Xn′′′

=n

Xn′


=n′
Xn

=n′
Xn′′′

n
X

=n

Xn′

cov (xnmxn′′m′ , xnmxn′′′m′ ) +

cov (xnmxn′m′ , xn′mxn′′′m′ ) +


cov (xnmxn′′m′ , xn′mxnm′ )


cov (xnmxn′′m′ , xn′mxn′′m′ )



,





Xn′

=n′
Xn

=n

Xn′′

Xn′

=n′
Xn

=n

Xn′′

where we used mutual independence of the observations xn in a sample
the covariance between the two expressions involving only independent variables is zero.

xn}

{

N
n=1 to conclude that

15

Further:
E

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N 2

k
(cid:0)

4
d
N 2(N

2
N 2(N

2
N 2(N

−

+

+

N (N

E(x2

(cid:1)
1)

−

N (N

(cid:0)
1)(N

2)

−

N (N

1)(N

2)

−

−

−

1)

−

Xm,m′

1)2

1)2

−

−

Xm,m′

Xm,m′

which after simpliﬁcation gives

N

E(x2
(cid:16)

Xm,m′
mxm′ )E(xm′ )

−

mx2

m′ )

[E(xmxm′ )]2

−

(cid:17)

E(xmxm′ )E(xm)E(xm′ )

[E(xm)]2 [E(xm′ )]2

(cid:1)

(cid:17)

−

m) [E(xm′ )]2

E(x2
(cid:16)
E(xmxm′ )E(xm)E(xm′ )
(cid:16)

−

[E(xm)]2 [E(xm′ )]2

+ O

1
N 2

,

(cid:18)

(cid:19)

(cid:17)

,

E

k

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N

Xm,m′
[2E(xm)E(xm′ )cov(xm, xm′ )

(cid:1)

h

(cid:0)
+

1
d
N

Xm,m′

var(xmxm′ ) + 2 [E(xm)]2 var(xm′ )
i
1
N 2

4E(xm)cov(xmxm′ , xm′ )] + O

−

(cid:18)

(cid:19)

where in the last equality, by symmetry, the summation indexes m and m′ can be exchanged. As
Poisson(ym), by the law of total expectation and law of total covariance, it follows, for
xm ∼
m

= m′ (and using the auxiliary expressions from Section D.4):
[E[xmxm′ ]]2 = E
mym′ + ymy2

var(xmxm′ ) = E(x2
mx2
my2
y2
m′ + ymym′
(cid:2)
[E(xm)]2 var(xm′ ) = [E(ym)]2 E(ym′ ) + [E(ym)]2 E(y2

m′ )
−
m′ + y2

E(x2

= E

m′ )

(cid:2)

y)]]2

mx2
m′

[E [E(xmxm′

|

−

y)
[E(ymym′)]2 ,
−
[E(ym)]2 [E(ym′ )]2 ,
(cid:3)

(cid:3)

|

E(xm)E(xm′ )cov(xm, xm′ ) = E(ymym′ )E(ym)E(ym′ )

E(xm)cov(xmxm′ , xm′ ) = E(ym)
Now, considering the m = m′ case, we have:
var(x2

E(ymym′) + E(ymy2

−
[E(ym)]2 [E(ym′ )]2 ,
m′)

E(ymym′ )E(ym′ )

−

−

.

(cid:3)

E(xm)E(xm)cov(xm, xm) = E(ym)2
m, xm) = E(ym)

E(xm)cov(x2
Substitution of ym =

2

y)]

E[E(x2

(cid:2)
m) = E[E(x4
y)]
m|
m|
−
= E
y4
m + 6y3
m + 7y2
m + ym
(cid:2)
m) + E(ym)
E(y2
h
m) + 3E(y2
E(y3

(cid:2)

2

,

E

y2
m + ym
(cid:3)
−
[E(ym)]2
(cid:3)
(cid:2)
−
m) + E(ym)

i
E(ym)

(cid:2)

,

(cid:3)(cid:3)

−

k Dmkαk gives the following
(cid:2)
1
N

=

2
F

cov(x, x)
P
k

−

Xk,k′,k′′,k′′′h

dk, dk′

dk′′ , dk′′′

ih

(cid:2)
iAkk′k′′k′′′

E

cov(x, x)

k
(cid:0)

d

E(y2

m) + E(ym)

.

dk′′ ,~1

iBkk′k′′ +

dk ◦
h

dk′ , dk′′

iEkk′k′′

(cid:3)(cid:3)

i

(cid:1)

1
N

1
N

+

+

+

Xk

ih

Xk,k′,k′′

dk, dk′
h
h
dk,~1
h
Xk,k′
h
E(αk) + O
dk,~1
i
h

ih

1
N 2

,

(cid:18)

(cid:19)

E(αkαk′ ) +
dk′ ,~1
i

dk, dk′
h

iFkk′

i

where ~1 is the vector with all the elements equal to 1 and
Akk′k′′k′′′ = E(αkαk′ αk′′ αk′′′ )

−

E(αkαk′′ )E(αk′ αk′′′ ) + 2E(αk)E(αk′ )E(αk′′ αk′′′ )

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ ) + 2E(αkαk′′ )E(αk′ )E(αk′′′ )
4E(αk)E(αk′ αk′′ αk′′′ ) + 4E(αk)E(αk′ αk′′ )E(αk′′′ ),

−

−
−

Bkk′k′′ = 2E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ )
Ekk′k′′ = 4E(αkαk′ αk′′ ) + 6E(αk)E(αk′ )E(αk′′ )
Fkk′ = 6E(αkαk′ )

5E(αk)E(αk′ ),

−

−
−

4E(αk)E(αk′ αk′′ ),
10E(αkαk′ )E(αk′′ ),

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ )

16

where we used the expressions from Section D.4.

D.3 Expected squared error of the estimator

S for the GP/DICA cumulants

As the estimator

S (29) of S (10) is unbiased, its expected squared error is

b

E

S

S

2
b
F
k

−

=E

i

k
h

b

cov(x, x)

cov(x, x)) +

diag[

E(x)]

diag [E(x)]

(
(cid:20)(cid:13)
(cid:13)
= E
(cid:13)

+ 2

E(x)

d
k
h

E
b

m
X

h(cid:16)

b

−
E(x)
k

2
F

−
E(xm)

+ E

i
(cid:2)
E(xm)
(cid:17)

−

−

(cid:16)
cov(x, x)

b

−

k
cov(xm, xm)
(
d

−

2

(cid:21)

F
(cid:17)(cid:13)
(cid:13)
(cid:13)

i

cov(x, x)
k

2
F

(cid:3)
cov(xm, xm))

.

(33)

cov(xm, xm) are unbiased, the m-th element of the last sum is equal to

d

E(xm) and
E(xm),
b
h
1
b
N 2

Xn,n′

As

cov

=

=

1
N 2

d

cov(xm, xm)
i
n′m

xnm, x2

d

cov

(cid:2)
xnm, x2

nm

(cid:3)

cov

n
X
E(x3

(cid:2)

1
N 2(N

−

2
N 2(N

−

(cid:3)

1)

−

Xn,n′,n′′

=n′

1)

=n

Xn,n′

−
[E(xm)]3

cov [xnm, xn′mxn′′m]

cov [xnm, xn′mxnm] + O

1
N 2

(cid:18)

(cid:19)

=

−

m)

1
N
1
N

m)E(xm)

E(x2
(cid:16)
[E(xm)]3 + O

2
N
2
N
≤
(cid:18)
where we neglected the negative term
from the expressions in Section D.4. Further, the fact that ym =

(cid:18)
(cid:19)
E(y3
m) + 3E(y2
h

(cid:19)
E(x2

(cid:17)
1
N

1
N 2

1
N 2

m) +

E(x3

+ O

−

−

=

m) + E(ym) + 2 [E(ym)]3

+ O

(cid:18)
m)E(xm) for the inequality, and the last equality follows

i

1
N 2

,

(cid:19)

k Dmkαk gives

cov

E(xm),
h

cov(xm, xm)
i

m
X

b

d

=

+

1
N

3
N

Xk,k′,k′′h

Xk,k′h

dk ◦

dk′ , dk′′

P
iCkk′k′′

dk, dk′

E(αkαk′ ) +
i

1
N

dk,~1
h

E(αk) + O
i

1
N 2

,

(cid:18)

(cid:19)

Xk

where

denotes the elementwise Hadamard product and

◦

Ckk′k′′ = E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ ).
Plugging this and the expressions for E(
cov(x, x)
−
k
Sections D.1 and D.2, respectively, into (33) gives

F ) and E(
k

E(x)
k

E(x)

2

cov(x, x)
k

−

2
F ) from

dk, dki
h

var(αk) +

E(αk) +

dk, dk′

dk′′ , dk′′′

ih

iAkk′ k′′k′′′



+ O

b

Xk

d

Xk,k′,k′′,k′′′h

iBkk′k′′ + 2

dk ◦
h

dk′ , dk′′

iCkk′k′′ ] +

i

≤

30c4

0 + 8c0

0 + 23c3

0 + 14c2
b4

where we used that, by the simplex constraint on the topics,
expression in more details, let us now consider the GP model, i.e., αk ∼
Xk,k′,k′′,k′′′ Akk′ k′′k′′′
Xk,k′,k′′ Ckk′k′′
Xk,k′ Fkk′

Xk,k′,k′′ Ekk′k′′

0 + 6c2
b3

0 + c0
b2

0 + c0
b2

E(αkαk′ )

0 + 2c0

Xk,k′

2c2

2c2

7c3

and

and

and

≤

≤

≤

≤

,

,

,

(1 + 6

dk, dk′
h

) E(αkαk′ ) + 2
i

Xk
= 1 for all k. To analyze this

Xk,k′
dk,~1
h

Gamma(ck, b):
6c3

0 + 10c2
b3

0 + 4c0

,

Xk,k′,k′′ Bkk′k′′

≤

12c3

0 + 10c2
b3

0 + 8c0

,

S

S

2
F

k

−

E

k
h

b
1
N 

+

=

1
N 

i

Xk

dk, dk′
h

[

Xk,k′,k′′


17

1
N 2

(cid:18)

(cid:19)


E(αk)

,





where we used the expressions from Section D.4, which gives

ν
N "

max

k k

2
2

dkk

c0
b2 +

c0
b

+

max
k,k′ h

dk, dk′

(cid:18)

i
(cid:19)

2

max

c4
0
b4 ,

c0
b4

+ max
k,k′ h

dk, dk′

max

i

c3
0
b3 ,

c0
b3

(cid:21)#

S

S

2
F

k

−

≤

i

E

k
h

+

b
ν
N

max
k,k′,k′′h

dk ◦

dk′ , dk′′

max

i
(cid:19)

c3
0
b3 ,

c0
b3

+

1 + max
k,k′ h

dk, dk′

max

(cid:20)

i
(cid:19)

(cid:21)
c2
0
b2 ,

c0
b2

(cid:20)

(cid:21)

(cid:18)

(cid:20)(cid:18)

≤
dkk

(cid:21)(cid:21)
30 is a universal constant. As, by the Cauchy-Schwarz inequality, max k,k′
where ν
2
2
2 =: ∆1 and max k,k′,k′′
max k k
max k k
2 ≤
∆1), it follows that
(note that for the topics in the simplex, ∆2 ≤
L3
¯c2
0 (cid:19)
0L2 + ¯c3

(cid:20)
i
1L4 + ¯c0∆1L3 + ¯c2
∆2

max k k
∆1 as well as ∆2

dkk∞ k
1 ≤

+ L + ∆2
1

L3
¯c2
0 (cid:21)

dk ◦
h

dk′ , dk′′

L2
¯c2
0

L2
¯c0

dkk

+ ∆2

ν
N

+ O

+ O

i ≤

∆1

0L

≤

+

+

(cid:18)

(cid:18)

2
F

E

S

S

(cid:20)

k

,

1
N 2

(cid:19)

L4
¯c3
0
1
N 2

k
h
2ν
b
N

−
1
¯c3
0

≤

(cid:2)

where ¯c0 = min(1, c0)
length. The second term ¯c0∆1L3 cannot be dominant as the system ¯c0∆1L3 > ¯c2
∆2
1L4 is infeasible. Also, with the reasonable assumption that L
¯c3
0L

(cid:19)
1 and, from Section B.2, c0 = bL where L is the expected document
0L2 and ¯c0∆1L3 >
1, we also have that the 4th term

0L2. Therefore,
¯c2

≥

≤

(cid:18)

(cid:3)

≤

(cid:20)

,

1
N 2

+ O

(cid:19)

(cid:18)
dk, dk′
i ≤
h
3
2 =: ∆2
dkk

E

S

k

−

S

2
F

k

h

≤

i

3ν
N

max

∆2

1L4, ¯c2

0L2

+ O

(cid:2)

(cid:3)

1
N 2

.

(cid:18)

(cid:19)

M
m=1 are conditionally independent given y in the DICA model (3), we have the following
= m′ and using the moments of the Poisson

b
D.4 Auxiliary expressions

{

xm}

As
expressions by using the law of total expectation for m
distribution with parameter ym:
E(xm) = E[E(xm|
m) = E[E(x2
E(x2
m|
m) = E[E(x3
E(x3
m|
m) = E[E(x4
E(x4
m|
E(xmxm′ ) = E[E(xmxm′
E(xmx2
m′ ) = E[E(xmx2
m′
|
E(x2
ym)E(x2
m′ ) = E[E(x2
mx2
m′
m|

ym)] = E(ym),
ym)] = E(y2
ym)] = E(y3
ym)] = E(y4

y)] = E[E(xm|
y)] = E[E(xm|
|

m) + E(ym),
m) + 3E(y2
m) + 6E(y3

ym′)] = E(y2

|

Moreover, the moments of αk ∼
c2
k + ck
E(αk) =
, E(α2
b2

k) =

ck
b

Gamma(ck, b) are

D.5 Analysis of the whitening and recovery error

m) + E(ym),
m) + 7E(y2

m) + E(ym),
ym′ )] = E(ymym′),
ym′ )] = E(ymy2

|

ym)E(xm′
ym)E(x2
m′
m′) + E(y2
my2

|

m′) + E(ymym′ ),

mym′) + E(ymy2

m′ ) + E(ymym′).

, E(α3

k) =

k + 3c2
c3
k + 2ck
b3

, E(α4

k) =

k + 6c3
c4

k + 11c2
b4

k + 6ck

,

etc.

We can follow a similar analysis as in Appendix C of [15] to derive the topic recovery error given the
sample estimate error. In particular, if we deﬁne the following sampling errors ES and ET :

ES,

k ≤

S

S

k

k

−
T (u)
b

−

T (u)

k ≤ k

u

k2 ET ,

then the following form of their Lemma C.2 holds for both the LDA moments and the GP/DICA
cumulants:

b

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

ν

k

−

(maxkγk)ES

ET

k ≤

"

σK

D

2 +

σK

D

,

3

#

(34)

c

b

c

c

18

(cid:0)

(cid:1)

e

(cid:0)

(cid:1)

e

where σk(
·
cases
the GP/DICA cumulants, γk takes the simpler form γk := cum(αk)/[var(αk)]3/2 = 2/√ck.

) denotes the k-th singular value of a matrix, ν is some universal constant, and in both
c0(c0+1))
D⊤. For the LDA moments, γk = 2
ck(c0+2)2 , whereas for

D was deﬁned such that S =

q

D

e

e

e

We note that the scaling for S is O(L2) for the GP/DICA cumulants, in contrast to O(1) for the LDA
moments. Thus, to compare the upper bound (34) for the two types of moments, we need to put it in
quantities which are common. In the ﬁrst section of the Appendix C of [15], it was mentioned that
c0(c0+1) σK(D) for the LDA moments, where cmin := mink ck. In contrast, for the
σK
GP/DICA cumulants, we can show that σK
c0 σK(D), where L := c0/b is the average
≥
length of a document in the GP model. Using this lower bound for the singular vector, we thus get
the following bound in the case of the GP cumulant:

L √cmin

cmin

q

D

D

≥

e

(cid:0)

(cid:1)

(cid:0)

(cid:1)

e

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

k

−

ν
c3/2
min "

ES
L2

2c2
0
D

σK

2 +

ET
L3

c3
0
[σK (D)]3

.

#

k ≤

(35)

b

c

c

c
The c3/2
min factor is common for both the LDA moment and GP cumulant, but as we mentioned
after Proposition 3.1, the sample error ES term gets divided by L2 for the GP cumulant, as ex-
pected.

(cid:1)(cid:3)

(cid:0)

(cid:2)

The recovery error bound in [15] is based on the bound (35), and thus by showing that the error
ES/L2 for the GP cumulant is lower than the ES term for the LDA moment, we expect to also
gain a similar gain for the recovery error, as the rest of the argument is the same for both types of
moments (see Appendix C.2, C.3 and C.4 in [15] for the completion).

E Appendix. The LDA moments

E.1 Our notation

The LDA moments were derived in [3]. Note that the full version of the paper with proofs appeared
in [15] and a later version of the paper also appeared in [31]. In this section, we recall the form of the
LDA moments using our notation. This section does not contain any novel results and is included
for the reader’s convenience. We also refer to this section when deriving the practical expressions
for computation of the sample estimates of the LDA moments in Appendix F.4.

≥

For deriving the LDA moments, a document is assumed to be composed of at least three tokens:
3. As the LDA generative model (1) is only deﬁned conditional on the length L, this is not
L
too problematic. But given that we present models in this paper which also model L, we mention
for clarity that we can suppose that all expectations and probabilities deﬁned below are implicitly
3.11 The theoretical LDA moments are derived only using the ﬁrst three words
conditioning on L
w1, w2 and w3 of a document. But note that since the words wℓ’s are conditionally i.i.d. given θ (for
w3) = E(wℓ1 ⊗
wℓ3 ) for any three distinct tokens
1
ℓ1, ℓ2 and ℓ3. The tensor M3 is thus symmetric, and could have been deﬁned using any distinct
ℓ1, ℓ2 and ℓ3 that are less than L. To highlight this arbitrary choice and to make the links with the
U-statistics estimator presented later, we thus use generic distinct ℓ1, ℓ2 and ℓ3 in the deﬁnition of
the LDA moments below, instead of ℓ1 = 1, ℓ2 = 2 and ℓ3 = 1 as in [3].

L), we have M3 := E(w1 ⊗

wℓ2 ⊗

w2 ⊗

≤

≥

≤

ℓ

11Note that another advantage of the DICA cumulants from Section 3.1 is that they do not require such a
somewhat artiﬁcial condition: they are well-deﬁned for any document length (even a document of length zero!).

19

(36)

(37)

(39)

(40)

(41)

(42)

Using this notation, then by the law of total expectation and the properties of the Dirichlet distribu-
tion, the non-central moments12 of the LDA model (1) take the form [3]:

,

M1 = E(wℓ1 ) = D

c
c0
M2 = E(wℓ1 w⊤ℓ2 ) =
M3 = E(wℓ1 ⊗
c0
c0 + 2

wℓ2 ⊗
[E(wℓ1 ⊗
2c3
0
c0(c0 + 1)(c0 + 2)

=

−

c0
c0 + 1
wℓ3)
wℓ2 ⊗

where

denotes the tensor product.

⊗

M1M ⊤1 +

1
c0(c0 + 1)

Ddiag (c) D⊤,

M1) + E(wℓ1 ⊗

M1 ⊗

M1 ⊗

M1 +

M1 ⊗

wℓ3 ) + E(M1 ⊗
2
c0(c0 + 1)(c0 + 2)

K

Xk=1

wℓ2 ⊗

wℓ3 )] ,

ckdk ⊗

dk ⊗

dk.

(38)

Similarly to the GP/DICA cumulants (as discussed in Appendix C.3), moving the terms in the non-
central moments (36), (37), (38), the following quantities are deﬁned

(P airs) = S := M2 −

c0
c0 + 1

M1M ⊤1 ,

LDA S-moment

(T riples) = T := M3 −

[E(wℓ1 ⊗

wℓ2 ⊗

M1) + E(wℓ1 ⊗

M1 ⊗

wℓ3 ) + E(M1 ⊗

wℓ2 ⊗

wℓ3 )]

M1 ⊗

M1 ⊗

M1.

LDA T-moment

c0
c0 + 2
2c2
0
(c0 + 1)(c0 + 2)

+

Slightly abusing terminology, we refer to the entities S and T as the “LDA moments”. They have
the following diagonal structure

S =

1
c0(c0 + 1)

ckdkd⊤k ,

K

Xk=1

T =

2
c0(c0 + 1)(c0 + 2)

ckdk ⊗

dk ⊗

dk.

K

Xk=1

Note however that this form of the LDA moments has a slightly different nature than the similar
form (11) and (13) of the GP/DICA cumulants. Indeed, the former is the result of properties of the
Dirichlet distribution, while the latter is the result of the independence of α’s. However, one can
think of the elements of a Dirichlet random vector as being almost independent (as, e.g., a Dirichlet
random vector can be obtained from independent gamma variables through dividing each by their
sum). Also, this closeness of the structures of the LDA moments and the GP cumulants can be
explained by the closeness of the respective models as discussed in Section 2.

E.2 Asymptotically unbiased ﬁnite sample estimators for the LDA moments

Given realizations wnℓ, n = 1, . . . , N , ℓ = 1, . . . , Ln, of the token random variable wℓ, we now
give the expressions for the ﬁnite sample estimates of S (39) and T (40) for the LDA model (and
E below to express
we rewrite them as a function of the sample counts xn).13 We use the notation
a U-statistics empirical expectation over the token within a documents, uniformly averaged over the
whole corpus. For example,
wℓ2 ⊗

M1) := 1
N

wℓ2 ⊗

wℓ1 ⊗

1
Ln(Ln

Ln
ℓ1=1

N
n=1

b

1)

−

Ln
ℓ2=1
=ℓ1
ℓ2

12Note, the difference in the notation for the LDA moments in papers [3] and [4]. In [3], M1 = E(wℓ1 ),
M2 = E(wℓ1 ⊗ wℓ2 ), and M3 = E(wℓ1 ⊗ wℓ2 ⊗ wℓ3 ). However, in [4], M2 is equivalent to S in our notation
and to P airs in the notation of [3]; similarly, M3 is T in our notation or T riples in the notation of [3].

c

13Note that because non-linear functions of

is biased, i.e., E(
unbiased. This is in contrast with the estimator for the GP/DICA moments which is easily made unbiased.

S) 6= S. The bias is small though: kE(
b

S (43) and
T (44), the estimator
b
b
S)−Sk = O(1/N ) and the estimator is asymptotically
b

M1 appear in the expression for
c

P

P

P

E(wℓ1 ⊗
b

20

M1.

c

S :=

T :=
b

b

+

M1

c0
M ⊤1 ,
M2 −
c0 + 1
c0
E(wℓ1 ⊗
M3 −
c
c
c
c0 + 2
2c2
b
c
0
M1 ⊗
(c0 + 1)(c0 + 2)
c

h

wℓ2 ⊗

M1) +

E(wℓ1 ⊗
b

M1 ⊗
c

wℓ3) +

E(

M1 ⊗
c

b

wℓ2 ⊗

wℓ3 )
i

(44)

c
where, as suggested in [4], unbiased U-statistics estimates of M1, M2 and M3 are:

M1 :=

E(wℓ) =

wnℓ =

[δ1]nxn =

Xδ1,

(45)

N

Ln

N

1
N

wnℓ1 w⊤nℓ2

Xℓ=1

1
Ln(Ln −

1)

n=1
X
Ln

Ln

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

c
M1,

M1 ⊗
c

1
N

1
Ln

N

n=1
X

n=1
X
1
N

c
M2 :=

b
E(wℓ1 w⊤ℓ2 ) =

c

b
1
N

1
N

1
N

=

=

=

N

n=1
X
N

(cid:2)

[δ2]n

xnx⊤n −

 

wnℓw⊤nℓ

!

Ln

Xℓ=1

[δ2]n

xnx⊤n −
n=1
X
(cid:0)
Xdiag(δ2)X ⊤

−

diag(xn)

(cid:1)
diag(Xδ2)

,

1
N

(cid:3)

(43)

(46)

(47)

δ3n

Ln

Xℓ=1

M

m=1
X
em1 ⊗

M3 :=

c

E(wℓ1 ⊗
b

wℓ2 ⊗

wℓ3 ) =

1
N

N

Ln

Ln

Ln

n=1
X

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

Xℓ3=1
=ℓ2
ℓ3
=ℓ1
ℓ3

wnℓ1 ⊗

wnℓ2 ⊗

wnℓ3

[δ3]n

xn ⊗

xn ⊗

xn −

 

wnℓ ⊗

wnℓ ⊗

wnℓ

=

1
N

N

n=1
X

Ln

Ln

Xℓ2=1
=ℓ1
ℓ2
N

−

Xℓ1=1

=

1
N

n=1
X

M

M

−

m1=1
X

m2=1
X

(wnℓ1 ⊗

wnℓ1 ⊗

wnℓ2 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ1 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ2 )

[δ3]n

xn ⊗

xn ⊗

 

xn + 2

xnm(em ⊗

em ⊗

em)

xnm1 xnm2 (em1 ⊗

em2 + em1 ⊗

em2 ⊗

em1 + em1 ⊗

em2 ⊗

em2)

.





!
(48)

Here, the vectors δ1, δ2 and δ3 ∈
2!
[δ2]n =

Ln
2

−

1

RN are deﬁned as [δ1]n := L−

1

n ; [δ2]n := (Ln(Ln −

1))−

1, i.e.,

is the number of times to choose an ordered pair of tokens out of Ln tokens;

(cid:1)

i
h(cid:0)
is the number of times to choose an
[δ3]n := (Ln(Ln −
ordered triple of tokens out of Ln tokens. Note that the vectors δ1, δ2, and δ3 have nothing to do
with the Kronecker delta δ.

1, i.e., [δ3]n =

1)(Ln −

2))−

Ln
3

h(cid:0)

3!

i

(cid:1)

−

1

For a vector a
matrix A

∈
RM
×

RN , we sometimes use notation [a]n to denote its n-th element. Similarly, for a
N we use notation [A]mn to denote its (m, n)-th element.

∈

21

∈

c

There is a slight abuse of notation in the expressions above as wℓ is sometimes treated as a random
E(wℓ1 w⊤ℓ2), etc.) and sometimes as its realization. However, the difference
E(wℓ),
variable (i.e., in
is clear from the context.
b

b

F Appendix. Practical aspects and implementation details

F.1 Whitening of S and dimensionality reduction

The algorithms from Section 4 require the computation of a whitening matrix W of S. Due to
the similar diagonal structure ((41) and (11)) of the matrix S for both the LDA moments (39) and
the GP/DICA cumulants (10), the computation of a whitening matrix is exactly the same in both
cases.

By a whitening matrix, we mean a matrix W
whiten S

RK
M , but also reduces its dimensionality such that14 W SW ⊤ = IK.

M (in practice, M

RM

≫

∈

×

×

K) that does not only

Let S = U ΣU ⊤ be an orthogonal eigendecomposition of the symmetric matrix S. Let Σ1:K denotes
the diagonal matrix that contains the largest K eigenvalues15 of S on its diagonal and let U1:K be a
matrix with the respective eigenvalues in its columns. Then, a whitening matrix is

(49)

(50)

W = Σ†

1/2
1:K U ⊤1:K,

b
1/2
1:K

W :=

Σ†

U ⊤1:K,

c

b

b

1/2
1:K is a diagonal matrix constructed from Σ1:K by taking the inverse and the square root

where Σ†
of its non-zero diagonal values († stands for the pseudo-inverse).

In practice, when only a ﬁnite sample estimator
W of W can be introduced
estimator

S of S is available, the following ﬁnite sample

where

S =

U

Σ

U ⊤.

b

b

b

b

F.2 Computation of the ﬁnite sample estimators of the GP/DICA cumulants

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see
Appendix C.4 for the deﬁnition of
W ⊤ for the GP/DICA models. The construction
S (29) is
of the ﬁnite sample estimator
straightforward.

W is discussed in Appendix F.1, while the computation of

T ) of

T (v)

W

c

b

c

b

c

b

14Note that such a whitening matrix W ∈ RK×M is not uniquely deﬁned as left multiplication by any orthog-
W ⊤ = V W SW ⊤V ⊤ =
f

onal matrix V ∈ RK×K does not change anything. Indeed, let
IK.

W = V W , then
f

W S
f

15We mean the largest non-negative eigenvalues. In theory, S have to be PSD. In practice, when we deal with
ﬁnite number of samples, respective estimate of S can have negative eigenvalues. However, for K sufﬁciently
small, S should have enough positive eigenvalues. Moreover, it is standard practice to use eigenvalues of S for
estimation of a good value of K, e.g., by thresholding all negative and close to zero eigenvalues.

22

By plugging the deﬁnition of the tensor
RM :
a vector, we obtain for a given v

∈

b

T (30) in the formula (16) for the projection of a tensor onto

T (v)

=

cum(xm1 , xm2 , xm3)vm3 + 2

δ(m1, m2, m3)

E(xm3 )vm3

m1m2

i

h

b

m3
X

b

d
δ(m2, m3)

cov(xm1 , xm2 )vm3

δ(m1, m3)

d
cov(xm1 , xm2 )vm3

δ(m1, m2)

d
cov(xm1 , xm3 )vm3

m3
X

m3
X

m3
X

m3
X

−

−

−

=

−

=

W k1⊤

b
c
Wk2
T (v)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

cum(xm1 , xm2 , xm3)vm3 + 2δ(m1, m2)

d

E(xm1 )vm1

m3
X
cov(xm1 , xm2 )vm2 −

d

cov(xm1 , xm2 )vm1 −

b
δ(m1, m2)

cov(xm1 , xm3 )vm3 .

m3
X

d

d
This gives the following for the expression

d

W

T (v)

W ⊤:

cum(xm1 , xm2 , xm3 )vm3

Wk1m1

Wk2m2

c

d
δ(m1, m2)

E(xm1 )vm1

c
Wk1m1

c
Wk2m2

=

b
c
m1,m2,m3
X

+ 2

m1,m2
X

−

−

−

m1,m2
X

m1,m2
X

m1,m3
X

cov(xm1 , xm2 )vm2

b

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm2 )vm1

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm3 )vm3

c
Wk1m1

c
Wk2m1 ,

d

c

c

where
Wk denotes the k-th row of
for the unbiased ﬁnite sample estimates of

cov and

cum, we further get

W as a column vector. By further plugging in the expressions (31)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

=

(N

+ 2

c

N
1)(N

2)

−
−
E(xm)vm

n D
X
Wk1m

E(x)

d

d
Wk1 , xn −
c
Wk2m

b

E(x)

Wk2 , xn −
c

b

E D

E D

v, xn −

E(x)
E

b

m
X
1

−

N

−

N

−

N

−
1

−
1

−

b
1

1

1

E D
E(x)

E(x)

c
c
Wk1 , xn −
c
v
◦

b
Wk1 , xn −
c
Wk1 ◦
c

b
Wk2 , xn −
c

b

n D
X

n D
X

n D
X

v

◦

E D
E(x)

Wk2 , xn −
c
Wk2 , xn −
c

v, xn −

E D

E(x)
E
b
E(x)
E
b
E(x)
E

b

,

N
where
denotes the elementwise Hadamard product. Introducing the counts matrix X
where each element Xmn is the count of the m-th word in the n-th document (note, the matrix X

∈

◦

×

RM

23

contain the vector xn in the n-th column), we further simplify the above expression

W

T (v)

W ⊤ =

W X)diag[X ⊤v](
(

W X)⊤

c

b

c

c
v,

E(x)

2N (

W

E(x))(
c

W

E(x))⊤

W X)(
(

W X)⊤

D
W X(X ⊤v)(

E h

b

W

E(x))⊤ +
c

b

c

b
E(x)(
W

c
W X(X ⊤v))⊤

c

i

−

c

b

c

b

c

W X)(
(
b

c

W diag(v)X)⊤ + (

W diag(v)X)(

W X)⊤ +

W diag[X(X ⊤v)]

W ⊤

E(x))(
c

W diag[v]

E(x))⊤ + (

c

W diag[v]

E(x))(
c

W

E(x))⊤
c

c
W diag[

E(x)]
b

W ⊤.

c

b

c

b

i

i

N
1)(N
N
1)(N
N
1)(N

◦

h
c
W
(

(N

+

(N

−

−

2)

−

2)

−

−

(N

−
W diag[v

+ 2

2)
−
h
E(x)]
W ⊤
c

−

+

+

1
c
N
−
N

N

N

−
N

−

1

1

1

h

D

c
v,

E(x)
b
E

A more compact way to write down expression (51) is as follows

b

c

b

c

W

T (v)

W ⊤ =

c

b

c

+

N
1)(N

2)

T1 +
−
h
T5 + T ⊤5 −
h

T6 −

(N
1

−

N

1

−

v,
h

E(x)
i

(T2 −

T3)

−

(T4 + T ⊤4 )
i

b
T ⊤6 +

W diag(a)

W ⊤

,

c

i

c

where

i

c

(51)

(52)

W X)⊤,

T1 = (

W X)diag[X ⊤v](
E(x))(

W

E(x))⊤,
c

b
E(x))⊤,

W

T4 =

T3 = (

T2 = 2N (
c
W X)⊤,
W X)(
b
c
c
W
W X(X ⊤v)(
c
W X)(
c
W diag(v)
c
c
1)[v
a = 2(N
c

T6 = (

T5 = (

◦
b

−

c
W diag(v)X)⊤,
b
c
E(x))(

W
E(x)] +
c
b

E(x))⊤,

F.3 Computational complexity of the GP/DICA T-cumulant estimator (52)

b

b

b

v,
h

E(x)
E(x)
i

−

X(X ⊤v).

When computing the T-cumulant P times with the formula above, the following terms are dominant:
O(RN K)+ O(N K 2)+ O(M K), where R is the largest number of unique words (non-zero counts)
in a document over the corpus. In practice, almost always K < M < N , which gives the overall
complexity of P computations of the estimator (52) to be equal to O(P RN K)+O(P N K 2).

F.4 Computation of the ﬁnite sample estimators of the LDA moments

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see Ap-
W ⊤ for the LDA model. Note that the construction of
pendix E.2 for the deﬁnition of
the sample estimator
W of a whitening matrix W is discussed in Appendix F.1). The computation of
S (43) is straightforward. This approach to efﬁcient implementation was discussed in [4], however,
to the best of our knowledge, the ﬁnal expressions were not explicitly stated before. All derivations
are straightforward, but quite tedious.
b

T ) of

T (v)

c

c

c

W

b

b

24

RM

M

×

×

M (44) onto some

By analogy with the GP/DICA case, a projection (16) of the tensor
vector v

T

∈

RM in the LDA is
M

∈

T (v)

=

m1m2

h

b

−

i
c0
c0 + 2

M

m3=1 h
X

m3=1 h
X
c
E(wℓ1 ⊗
b

M3

vm3 +

m1m2m3
i

2c2
0
(c0 + 1)(c0 + 2)

wℓ2 ⊗

M1) +

c

E(wℓ1 ⊗
b

M1 ⊗
c

Plugging in the expression (48) for an unbiased sample estimate

m3
X

wℓ3) +

E(

c
M1 ⊗
c

c
wℓ2 ⊗

c
wℓ3 )
i

b
M3 of M3, we get

b
M1]m1 [

[

M1]m2 [

M1]m3vm3

vm3.

m1m2m3

1
N

1
N

N

n=1
X
N

n=1
X

[δ3]n

M

M



m3=1
X

i,j=1
X


M1]m1[
[

2c2
0
(c0 + 1)(c0 + 2)

−

+

T (v)

=

m1m2

i

h

b

[δ3]n

xnm1 xnm2 h
 

xn, v

+ 2

i

δ(m1, m2, m3)xnm3 vm3

c

!

m3
X

xnixnj (ei ⊗

ei ⊗

ej + ei ⊗

ej ⊗

ei + ei ⊗

ej ⊗

ej)

vm3





m1m2m3

M1]m2

M1, v

D
M
c

E

c0
c0 + 2  

c
M2]m1m2

c
M1, v

[

−

[
m3=1 (cid:16)
X
where e1, e2, . . . , eM denote the canonical vectors of RM (i.e., the columns of the identity matrix
IM ). Further, this gives the following for the expression

M1]m2vm3 + [

M2]m1m3[

M2]m2m3[

W ⊤:

T (v)

c

c

c

c

c

c

W

+

E

D

M1]m1 vm3

,

!

(cid:17)

W

T (v)

W ⊤

=

[δ3]n

xn, v

xn,

Wk1

b
c
xn,

c
Wk2

+ 2

xnmvm

Wk1m

Wk2m

 h
M

i

D

δ3n

xnixnj

E D

c
Wk1i

c
Wk2ivj +

M

E

m=1
X

Wk1i

Wk2jvi +

Wk1i

h

c

b

c

k1k2
i

i,j=1
X
Wk1 ,

M2
h

(cid:16)
Wk2

c

(cid:16)D
2c2
0
(c0 + 1)(c0 + 2)

c

c

+

Wk1 ,

c
M2v

c
M1

Wk2

c
+

c
Wk2 ,

M1,

Wk1

i
M1,

c

E
Wk1

c

D
c
M1,

c
Wk2

E D
c
M1, v

c
,

E

D

E D

E(cid:17)

c

c

c

c

where

Wk denotes the k-th row of

c
W as a column-vector. This further simpliﬁes to

c

c

c

c

D

E D

E D

E

N

n=1
X
N

1
N

1
N

n=1
X
c0
c0 + 2

−

−

+

!

c

c
Wk2jvj

(cid:17)
M2v

1
N
1
N
1
N
1
N

−

−

−

+

W

T (v)
c

W ⊤ =

W X)diag
(

(X ⊤v)
c

◦

δ3

W X)⊤
(

c

b

c

+

W diag
c

(cid:2)
2[(Xδ3)

v]

◦

−

(cid:3)
X[(X ⊤v)
c

δ3]

W ⊤

◦

(cid:2)

W diag[v]X)diag[δ3](
(
c

W X)⊤

(cid:3)

c

W X)diag[δ3](
(
c

W diag[v]X)⊤
c

c0
c
c0 + 2

hD
2c2
0
(c0 + 1)(c0 + 2)

c

E

M1, v

W
(
c

M2

W ⊤) + (

W (

M2v))(

W

M1)⊤ + (

W

M1)(

W (

M2v))⊤

c
c
M1, v

c
W
(

c
M1)(

W

c
M1)⊤.

c

c

c

c

c

c

i

(53)

A more compact representation gives:

D

c

E

c

c

c

c

W

T (v)

W ⊤ =

c

b

c

+

1
N

T3 −

T1 + T2 −
(cid:2)
2c2
0
(c0 + 1)(c0 + 2) h

T ⊤3

−

(cid:3)
W
(
i

M1, v

M1)(

c0
c0 + 2

W
(
i

M1, v
h
h
c
M1)⊤,
W

c

c

c

M2

W ⊤) + T4 + T ⊤4

i

(54)

c

c

c
25

c

c

where

◦
v]

T1 = (

W X)diag

(X ⊤v)

δ3

W X)⊤,
(

T2 =

T3 = [

T4 = [

◦

2[(Xδ3)
(cid:2)

W diag
c
−
W diag(v)X]diag(δ3)(
c
W (
c

M2v)](

M1)⊤.

W

(cid:2)

c

X[(X ⊤v)
(cid:3)
c
W X)⊤,

◦

δ3]

W ⊤,

(cid:3)

c

F.5 Computational complexity of the LDA T-moment estimator (54)

c

c

c

c

By analogy with Appendix F.3, the computational complexity of the T-moment is O(RN K) +
O(N K 2). However, in practice we noticed that the computation of (52) is slightly faster for larger
datasets than the computation of (54) (although the code for both was equally well optimized). This
means that the constants in O(RN K) + O(N K 2) for the LDA T-moment are, probably, slightly
larger than for the GP/DICA T-cumulant.

F.6 Estimation of the model parameters for GP/DICA model

×

∈

Below we brieﬂy discuss the recovery of the model parameters for the GP/DICA and LDA mod-
RK
M estimated in Algorithm 1. This matrix has
els from a joint diagonalization matrix A
the property that AD should be approximately diagonal up to a permutation of the columns of D.
The standard approach [3] of taking the pseudo-inverse of A to get an estimate of the topic ma-
trix D has a problem that it does not preserve the simplex constraint of the topics (in particular,
the non-negativity of
D). Due to the space constraints, we do not discuss this issue here, but we
observed experimentally that this can potentially signiﬁcantly deteriorate performance of all mo-
ment matching algorithms for topic models considered in this paper. We made an attempt to solve
this problem by integrating the non-negativity constraint into the Jacobi-updates procedure of the
orthogonal joint diagonalization algorithm, but the obtained results did not lead to any signiﬁcant
improvement. Therefore, in our experiments for both GP/DICA cumulants and LDA moments, we
estimate the topic matrix by thresholding the negative values of the pseudo-inverse of A:

e

dk := τk max(0, [A†]:k)/

max(0, [A†]:k)

k

k1,

where [A†]:k is the k-th column of the pseudo-inverse A† of A, and τk = ±1 set to
1 if [A†]:k has
more negative than positive values. This might not be the best option, and we leave this issue for the
future research.

−

b

To estimate the parameters for the prior distribution over the topic intensities αk for the DICA
model (4), we use the diagonalized form of the projected tensor from (17) and relate it to the output
diagonal elements ap for the p-th projection:

[ap]k =

zk, upi

=

zk, upi
h

=

cum(αk, αk, αk)
[var(αk)]3/2

tk
s3/2
k

tkh
e

τk

dk, W ⊤up

,

(55)

D

e

E

dk = τk max(0, [A†]:k). This formula is valid for any prior on αk in the DICA model. For the
b2 and cum(αk, αk, αk) = 2ck
b3 ,
tk in (55), and solving

√ck , which enables us to estimate ck. Plugging this value of

where
GP model (3) where αk ∼
tk = 2
e
and thus
for ck gives the following expression:

Gamma(ck, b), we have that var(αk) = ck

e

e

By replacing the quantities on the RHS with their estimated ones, we get one estimate for ck per
projection. We use as our ﬁnal estimate the average estimate over the projections:

(56)

ck =

2

.

4

dk, W ⊤up
[ap]2
k

D

E

e

1
P

P

4

dk,

W ⊤up
[ap]2
k
c

D

e

p=1
X

2

.

E

ck :=

b

26

Reusing the properties of the length of documents for the GP model as described in Appendix B.2,
we ﬁnally use the following estimates for rate parameter b of the gamma distribution:

b :=

,

c0
L
b
b

(57)

where

c0 :=

ck and

k

L is the average document length in the corpus.

b

By analogy, similar formulas for the estimation of the Dirichlet parameter c of the LDA model can
be derived and are a straightforward extension of the expression in [3].

P

b

b

b

G Appendix. Complexity of algorithms and details on the experiments

G.1 Code and complexity

Our (mostly Matlab) implementations of the diagonalization algorithms (JD, Spec, and TPM) for
both the GP/DICA cumulants and LDA moments are available online.16 Moreover, all datasets and
the code for reproducing our experiments are available.17 To our knowledge, no efﬁcient implemen-
tation of these algorithms was available for LDA. Each experiment was run in a single thread.

The bottleneck for the spectral, JD, and TPM algorithms is the computation of the cumu-
lants/moments. However, the expressions (52) and (54) provide efﬁcient formulas for fast com-
putation of the GP/DICA cumulants and LDA moments (O(RN K + N K 2), where R is the largest
number of non-zeros in the count vector x over all documents, see Appendix F.3 and F.5), which
makes even the Matlab implementation fast for large datasets. Since all diagonalization algorithms
(spectral, JD, TPM) perform the whitening step once, it is sufﬁcient to compare their complexities
by the number of times the cumulants/moments are computed.

Spectral.
O(N K(R + K)) complexity and, therefore, is the fastest.

The spectral algorithm estimates the cumulants/moments only once leading to

JD. For JD, rather than estimating P cumulants/moments separately, one can jointly estimate these
values by precomputing and reusing some terms (e.g., W X). However, the complexity is still
O(P N K(R + K)), although in practice it is sufﬁcient to have P = K or even smaller.

TPM. For TPM some parts of the cumulants/moments can also be precomputed, but as TPM nor-
mally does many more iterations than P , it can be signiﬁcantly slower. In general, the complexity
of TPM can be signiﬁcantly inﬂuenced by the initialization of the parameters of the algorithm.
There are two main parameters: Ltpm is the number of random restarts within one deﬂation step
and Ntpm is the maximum number of iterations for each of Ltpm random restarts (different from
N and L). Some restarts converge very fast (in much less than Ntpm iterations), while others are
slow. Moreover, as follows from theoretical results [4] and, as we observed in practice, the restarts
which converge to a good solution converge fast, while slow restarts, normally, converge to a worse
solution. Nevertheless, in the worst case, the complexity is O(NtpmLtpmN K(R + K)).

Note that for the experiment in Figure 1, Ltpm = 10 and Ntpm = 100 and the run with the best
objective is chosen. We believe that these values are reasonable in a sense that they provide a good
5 for the norm of the difference of the vectors from the previous and the
accuracy solution (ε = 10−
current iteration) in a little number of iterations, however, they may not be the best ones.

JD implementation. For the orthogonal joint diagonalization algorithm, we implemented a faster
C++ version of the previous Matlab implementation18 by J.-F. Cardoso. Moreover, the orthogonal
joint diagonalization routine can be initialized in different ways: (a) with the K
K identity matrix
or (b) with a random orthogonal K
K matrix. We tried different options and in nearly all cases
the algorithm converged to the same solution, implying that initialization with the identity matrix is
sufﬁcient.

×

×

Whitening matrix. For the large vocabulary size M , computation of a whitening matrix can be
expensive (in terms of both memory and time). One possible solution would be to reduce the vo-
cabulary size with, e.g., TF-IDF score, which is a standard practice in the topic modeling context.

16https://github.com/anastasia-podosinnikova/dica-light
17https://github.com/anastasia-podosinnikova/dica
18http://perso.telecom-paristech.fr/˜cardoso/Algo/Joint_Diag/joint_diag_r.m

27

min
148
JD-GP
252
JD-LDA
JD(k)-GP
157
JD(k)-LDA 264
JD(f)-GP
1628
JD(f)-LDA 2545
Spec-GP
Spec-LDA
TPM-GP
TPM-LDA 12723

101
107
1734

mean max
247
192
366
284
247
190
318
290
2058
1846
2806
2649
111
107
193
140
2726
2393
19356
16460

Table 1: The running times in seconds of the algorithms from Figure 1, corresponds to the case when N =
50, 000. Each algorithm was run 5 times, so the times in the table display the minimum (min), mean, and
maximum (max) time.

Another option is using a stochastic eigendecomposition (see, e.g., [33]) to approximate the whiten-
ing matrix.

Variational inference. For variational inference, we used the code of D. Blei and modiﬁed it for the
estimation of a non-symmetric Dirichlet prior c, which is known to be important [35]. The default
values of the tolerance/maximum number of iterations parameters are used for variational inference.
The computational complexity of one iteration for one document of the variational inference algo-
rithm is O(RK), where R is the number of non-zeros in the count vector for this document, which
is then performed a signiﬁcant number of times for each document.

G.2 Runtimes of the algorithms

In Table 1, we present the running times of the algorithms from Section 5.1. JD and JD(k) are
signiﬁcantly faster than JD(f) as expected, although the performance in terms of the ℓ1-error is
nearly the same for all of them. This indicates that preference should be given to the JD or JD(k)
algorithms.

The running time of all LDA-algorithms is higher than the one of the GP/DICA-algorithms. This
indicates that the computational complexity of the LDA-moments is slightly higher than the one
of the GP/DICA-cumulants (compare, e.g., the times for the spectral algorithm which almost com-
pletely consist of the computation of the moments/cumulants). Moreover, the runtime of TPM-LDA
is signiﬁcantly higher (half an hour vs. several hours) than the one of TPM-GP/DICA. This can be
explained by the fact that the LDA-moments have more noise than the GP/DICA-cumulants and,
hence, the algorithm is slower. Interestingly, all versions of JD algorithm are not that sensitive to
noise.

Computation of a whitening matrix is roughly 30 sec (this time is the same for all algorithms and is
included in the numbers above).

G.3 Initialization of the parameter c0 for the LDA moments

The construction of the LDA moments requires the parameter c0, which is not trivial to set in the
unsupervised setting of topic modeling, especially taking into account the complexity of the evalu-
ation for topic models [16]. For the semi-synthetic experiments, the true value of c0 is provided to
the algorithms. It means that the LDA moments, in this case, have access to some oracle informa-
tion, which in practice is never available. For real data experiments, c0 is set to the value obtained
with variational inference. The experiments in Appendix G.4 show that this choice was somewhat
important. However, this requires more thorough investigation.

G.4 The LDA moments vs parameter c0

In this section, we experimentally investigate dependence of the LDA moments on the parameter c0.
In Figure 5, the joint diagonalization algorithm with the LDA moment is compared for different
values of c0 provided to the algorithm. The data is generated similarly to Figure 2. The experiment
indicates that the LDA moments are somewhat sensitive to the choice of c0. For example, the

28

0
0.01

0.1 

1   
Parameter c0

10  

100 

0
0.01

0.1 

1   
Parameter c0

10  

100 

Figure 5: Performance of the LDA moments depending on the parameter c0. D and c are learned from the
AP dataset for K = 10 and K = 50 and true c0 = 1. JD-GP(10) for K = 10 and JD-GP(50) for K = 50.
Number of sampled documents N = 20, 000. For the error bars, each dataset is resampled 5 times. Data (left):
GP sampling; (right): LDAﬁx(200) sampling. Note: a smaller value of the ℓ1-error is better.

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

JD-GP(10)

JD-LDA(10)

JD-GP(90)

JD-LDA(90)

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
2

ℓ

0
1 

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

0
1 

20

10
Number of docs in 1000s

30

40

50

20

10
Number of docs in 1000s

40

30

50

Figure 6: Comparison of the ℓ1- and ℓ2- errors on the NIPS semi-synthetic dataset as in Figure 2 (top, left).
The ℓ2 norms of the topics were normalized to [0,1] for the computation of the ℓ2 error.

recovery ℓ1-error doubles when moving from the correct choice c0 = 1 to the plausible alternative
c0 = 0.1 for K = 10 on the LDAﬁx(200) dataset (JD-LDA(10) line on the right of Figure 5).

G.5 Comparison of the ℓ1- and ℓ2-errors

The sample complexity results [3] for the spectral algorithm for the LDA moments allow straightfor-
ward extension to the GP/DICA cumulants, if the results from Proposition 3.1 are taken into account.
The analysis is, however, in terms of the ℓ2-norm. Therefore, in Figure 6, we provide experimental
comparison of the ℓ1- and ℓ2-errors to verify that they are indeed behaving similarly.

G.6 Evaluation of the real data experiments

For the evaluation of topic recovery in the real data case, we use an approximation of the log-
likelihood for held out documents as the metric. The approximation is computed using a Chib-style
method as described by [16] using the implementation by the authors.19 Importantly, this evaluation
methods is applicable for both the LDA model as well as the GP model. Indeed, as it follows from
Section 2 and Appendix B.1, the GP model is equivalent to the LDA model when conditioning on
the length of a document L (with the same ck hyper parameters), while the LDA model does not
make any assumption on the document length. For the test log-likelihood comparison, we thus treat
the GP model as a LDA model (we do not include the likelihood of the document length).

G.7 More on the real data experiments

The detailed experimental setup is as follows. Each dataset is separated into 5 training/evaluation
pairs, where the documents for evaluation are chosen randomly and non-repetitively among the
folds (600 documents are held out for KOS; 400 documents are held out for AP; 450 documents
are held out for NIPS). Then, the model parameters are learned for a different number of topics.
The evaluation of the held-out documents is performed with averaging over 5 folds. In Figure 3 and
Figure 7, on the y-axis, the predictive log-likelihood in bits averaged per token is presented.

19http://homepages.inf.ed.ac.uk/imurray2/pub/09etm

29

In addition to the experiments with AP and KOS in Figure 3, we demonstrate one more experiment
with the NIPS dataset in Figure 7 (right).

Note that, as the LDA moments require at least 3 tokens in each document, 1 document from the
NIPS dataset and 3 documents from the AP dataset, which did not fulﬁll this requirement, were
removed.

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11

-12

-13

-14

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 7: Experiments with real data. Left: the KOS dataset. Right: the NIPS dataset. Note: a higher value of
the log-likelihood is better.

Importantly, we observed that VI when initialized with the output of the JD-GP is consistently better
in terms of the predictive log-likelihood. Therefore, the new algorithm can be used for more clever
initialization of other LDA/GP inference methods.

We also observe that the joint diagonalization algorithm for the LDA moments is worse than the
spectral algorithm. This indicates that the diagonal structure (41) and (42) might not be present in the
sample estimates (43) and (44) due to either model misspeciﬁcation or to ﬁnite sample complexity
issues.

Supplementary References

[31] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for

latent Dirichlet allocation. Algorithmica, 72(1):193–214, 2015.

[32] B.A. Frigyik, A. Kapila, and M.R. Gupta. Introduction to the Dirichlet distribution and related

processes. Technical report, University of Washington, 2010.

[33] N. Halko, P.-G. Martinsson, and J.A. Tropp. Finding structure with randomness: probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288,
2011.

[34] T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):455–

[35] H.M. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: why priors matter. In NIPS,

500, 2009.

2009.

30

5
1
0
2
 
v
o
N
5

 

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
8
7
1
0
.
7
0
5
1
:
v
i
X
r
a

Rethinking LDA: Moment Matching for Discrete ICA

Anastasia Podosinnikova

Francis Bach

Simon Lacoste-Julien

INRIA - ´Ecole normale sup´erieure Paris

Abstract

We consider moment matching techniques for estimation in latent Dirichlet allo-
cation (LDA). By drawing explicit links between LDA and discrete versions of
independent component analysis (ICA), we ﬁrst derive a new set of cumulant-
based tensors, with an improved sample complexity. Moreover, we reuse standard
ICA techniques such as joint diagonalization of tensors to improve over existing
methods based on the tensor power method. In an extensive set of experiments on
both synthetic and real datasets, we show that our new combination of tensors and
orthogonal joint diagonalization techniques outperforms existing moment match-
ing methods.

1 Introduction

Topic models have emerged as ﬂexible and important tools for the modelisation of text corpora.
While early work has focused on graphical-model approximate inference techniques such as varia-
tional inference [1] or Gibbs sampling [2], tensor-based moment matching techniques have recently
emerged as strong competitors due to their computational speed and theoretical guarantees [3, 4].
In this paper, we draw explicit links with the independent component analysis (ICA) literature
(e.g., [5] and references therein) by showing a strong relationship between latent Dirichlet allocation
(LDA) [1] and ICA [6, 7, 8]. We can then reuse standard ICA techniques and results, and derive new
tensors with better sample complexity and new algorithms based on joint diagonalization.

2 Is LDA discrete PCA or discrete ICA?

0, 1

Notation. Following the text modeling terminology, we deﬁne a corpus X =
as a
x1, . . . , xN }
{
of Ln tokens. It is
collection of N documents. Each document is a collection
wn1, . . . , wnLn }
convenient to represent the ℓ-th token of the n-th document as a 1-of-M encoding with an indicator
M with only one non-zero, where M is the vocabulary size, and each document
vector wnℓ ∈ {
}
as the count vector xn :=
In such representation, the length Ln of the n-th
to refer to topics,the
document is Ln =
}
P
index n
to refer to words from
}
the vocabulary, and the index ℓ
to refer to tokens of the n-th document. The plate
∈ {
diagrams of the models from this section are presented in Appendix A.

RM .
m xnm. We will always use the index k
to refer to documents, the index m
1, . . . , N
}
P
1, . . . , Ln}

ℓ wnℓ ∈

∈ {
1, . . . , M

1, . . . , K

∈ {

∈ {

{

Latent Dirichlet allocation [1] is a generative probabilistic model for discrete data such as text
corpora. In accordance to this model, the n-th document is modeled as an admixture over the vo-
cabulary of M words with K latent topics. Speciﬁcally, the latent variable θn, which is sampled
from the Dirichlet distribution, represents the topic mixture proportion over K topics for the n-th
θn for the ℓ-th token is sampled from the multinomial dis-
document. Given θn, the topic choice znℓ|
znℓ, θn is then sampled from the multinomial
tribution with the probability vector θn. The token wnℓ|
distribution with the probability vector dznℓ, or dk if k is the index of the non-zero element in znℓ.
This vector dk is the k-th topic, that is a vector of probabilities over the words from the vocabulary
subject to the simplex constraint, i.e., dk ∈
.
}
This generative process of a document (the index n is omitted for simplicity) can be summarized as

∆M , where ∆M :=

m dm = 1

RM : d

0,

(cid:23)

∈

d

{

P

1

θ
θ
zℓ|
zℓ, θ

∼
∼
∼

wℓ|

Dirichlet(c),
Multinomial(1, θ),
Multinomial(1, dzℓ).

(1)

One can think of the latent variables zℓ as auxiliary variables which were introduced for convenience
of inference, but can in fact be marginalized out [9], which leads to the following model

θ
θ

x
|

∼
∼

Dirichlet(c),
Multinomial(L, Dθ),

LDA model (2)

RM

×

∈

K is the topic matrix with the k-th column equal to the k-th topic dk, and c

where D
++
is the vector of parameters for the Dirichlet distribution. While a document is represented as a set
of tokens wℓ in the formulation (1), the formulation (2) instead compactly represents a document as
the count vector x. Although the two representations are equivalent, we focus on the second one in
this paper and therefore refer to it as the LDA model.

∈

RK

Importantly, the LDA model does not model the length of documents. Indeed, although the original
paper [1] proposes to model the document length as L
Poisson(λ), this is never used in practice
and, in particular, the parameter λ is not learned. Therefore, in the way that the LDA model is
typically used, it does not provide a complete generative process of a document as there is no rule to
sample L
λ. In this paper, this fact is important, as we need to model the document length in order
to make the link with discrete ICA.

∼

λ

|

|

×

∈

RM

K is a transformation matrix and σ is a parameter.

Discrete PCA. The LDA model (2) can be seen as a discretization of principal component anal-
ysis (PCA) via replacement of the normal likelihood with the multinomial one and adjusting the
prior [9] in the following probabilistic PCA model [10, 11]: θ
Normal(Dθ, σ2IM ), where D
Discrete ICA (DICA). Interestingly, a small extension of the LDA model allows its interpreta-
tion as a discrete independent component analysis model. The extension naturally arises when the
document length for the LDA model is modeled as a random variable from the gamma-Poisson
mixture (which is equivalent to a negative binomial random variable), i.e., L
Poisson(λ) and
|
k ck is the shape parameter and b > 0 is the rate parameter. The
λ
LDA model (2) with such document length is equivalent (see Appendix B.1) to
αk ∼
α
∼

Gamma(ck, b),
Poisson([Dα]m),

Normal(0, IK ) and x
|

Gamma(c0, b), where c0 :=

GP model (3)

xm|

P

∼

∼

∼

∼

λ

θ

where all α1, α2, . . . , αK are mutually independent, the parameters ck coincide with the ones of the
LDA model in (2), and the free parameter b can be seen (see Appendix B.2) as a scaling parameter
for the document length when c0 is already prescribed.

This model was introduced by Canny [12] and later named as a discrete ICA model [13]. It is more
natural, however, to name model (3) as the gamma-Poisson (GP) model and the model

α1, . . . , αK ∼
xm|
∼

α

mutually independent,
Poisson([Dα]m)

DICA model (4)

as the discrete ICA (DICA) model. The only difference between (4) and the standard ICA model [6,
7, 8] (without additive noise) is the presence of the Poisson noise which enforces discrete, instead of
continuous, values of xm. Note also that (a) the discrete ICA model is a semi-parametric model that
can adapt to any distribution on the topic intensities αk and that (b) the GP model (3) is a particular
case of both the LDA model (2) and the DICA model (4).

Thanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to
derive new efﬁcient algorithms for topic modeling.

3 Moment matching for topic modeling

The method of moments estimates latent parameters of a probabilistic model by matching theoretical
expressions of its moments with their sample estimates. Recently [3, 4], the method of moments
was applied to different latent variable models including LDA, resulting in computationally fast

2

learning algorithms with theoretical guarantees. For LDA, they (a) construct LDA moments with a
particular diagonal structure and (b) develop algorithms for estimating the parameters of the model
by exploiting this diagonal structure. In this paper, we introduce the novel GP/DICA cumulants with
a similar to the LDA moments structure. This structure allows to reapply the algorithms of [3, 4]
for the estimation of the model parameters, with the same theoretical guarantees. We also consider
another algorithm applicable to both the LDA moments and the GP/DICA cumulants.

3.1 Cumulants of the GP and DICA models

In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is
a particular case of the DICA model, all results of this section extend to the GP model.

The ﬁrst three cumulant tensors for the random vector x can be deﬁned as follows

cum(x) := E(x),

cum(x, x) := cov(x, x) = E
E(x))

cum(x, x, x) := E [(x

(x

−
(x

E(x))(x
E(x))

−

(cid:2)
⊗

−

−

E(x))⊤
(x

,
E(x))] ,
(cid:3)

−

⊗

(5)

(6)

(7)

⊗

denotes the tensor product (see some properties of cumulants in Appendix C.1). The
where
essential property of the cumulants (which does not hold for the moments) that we use in this paper
is that the cumulant tensor for a random vector with independent components is diagonal.
Let y = Dα; then for the Poisson random variable xm|
E(xm|
expectation in (5) has the following form

Poisson(ym), the expectation is
ym) = ym. Hence, by the law of total expectation and the linearity of expectation, the

ym ∼

E(x) = E(E(x
|
ym) = ym and, as x1,
Further, the variance of the Poisson random variable xm is var(xm|
x2, . . . , xM are conditionally independent given y, then their covariance matrix is diagonal, i.e.,
y) = diag(y). Therefore, by the law of total covariance, the covariance in (6) has the form
cov(x, x
|

y)) = E(y) = DE(α).

(8)

cov(x, x) = E [cov(x, x
|

y)] + cov [E(x
|
= diag [E(y)] + cov(y, y) = diag [E(x)] + Dcov(α, α)D⊤,

y), E(x
|

y)]

(9)

where the last equality follows by the multilinearity property of cumulants (see Appendix C.1).
Moving the ﬁrst term from the RHS of (9) to the LHS, we deﬁne

−
From (9) and by the independence of α1, . . . , αK (see Appendix C.3), S has the following diagonal
structure

S := cov(x, x)

diag [E(x)] .

DICA S-cum. (10)

S =

var(αk)dkd⊤k = Ddiag [var(α)] D⊤.

(11)

By analogy with the second order case, using the law of total cumulance, the multilinearity property
of cumulants, and the independence of α1, . . . , αK, we derive in Appendix C.2 the expression (24),
similar to (9), for the third cumulant (7). Moving the terms in this expression, we deﬁne a tensor T
with the following element

[T ]m1m2m3 := cum(xm1 , xm2 , xm3 ) + 2δ(m1, m2, m3)E(xm1 )

DICA T-cum. (12)

δ(m2, m3)cov(xm1 , xm2 )

δ(m1, m3)cov(xm1 , xm2 )

δ(m1, m2)cov(xm1 , xm3),

−

−

−

where δ is the Kronecker delta. By analogy with (11) (Appendix C.3), the diagonal structure of the
tensor T :

k

T =

cum(αk, αk, αk)dk ⊗
In Appendix E.1, we recall (in our notation) the matrix S (39) and the tensor T (40) for the LDA
model [3], which are analogues of the matrix S (10) and the tensor T (12) for the GP/DICA mod-
els. Slightly abusing terminology, we refer to the matrix S (39) and the tensor T (40) as the LDA
moments and to the matrix S (10) and the tensor T (12) as the GP/DICA cumulants. The diagonal
structure (41) & (42) of the LDA moments is similar to the diagonal structure (11) & (13) of the
GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of

dk ⊗

X

dk.

(13)

k

X

3

Appendix E.1. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA
cumulants and the LDA moments coincide.
The following sample complexity results apply to the sample estimates of the GP cumulants:1
Proposition 3.1. Under the GP model, the expected error for the sample estimator
GP cumulant S (10) is:

S (29) for the

b

1
√N

E

S

S

E
≤ r

S

S

2
F

O

max

∆ ¯L2, ¯c0 ¯L

,

(14)

2

h

i

k

k

≤

kF
−
2, ¯c0 := min(1, c0) and ¯L := E(L).
b

k
−
h
where ∆ := max k k
dkk
b
A high probability bound could be derived using concentration inequalities for Poisson random
variables [14]; but the expectation already gives the right order of magnitude for the error (for
example via Markov’s inequality). The expression (29) for an unbiased ﬁnite sample estimate
S of S
T of T are deﬁned2 in Appendix C.4.
and the expression (30) for an unbiased ﬁnite sample estimate
A sketch of a proof for Proposition 3.1 can be found in Appendix D.

(cid:18)

(cid:19)

i

(cid:2)

(cid:3)

b

b

k

T

T

kF ]

∆, ¯c0/ ¯L
}

By following a similar analysis as in [15], we can rephrase the topic recovery error in term of the
error on the GP cumulant. Importantly, the whitening transformation (introduced in Section 4) redi-
vides the error on S (14) by ¯L2, which is the scale of S (see Appendix D.5 for details). This means
that the contribution from ˆS to the recovery error will scale as O(1/√N max
), where
{
both ∆ and ¯c0/ ¯L are smaller than 1 and can be very small. We do not present the exact expression
for the expected squared error for the estimator of T , but due to a similar structure in the derivation,
we expect the analogous bound of E[

≤
Current sample complexity results of the LDA moments [3] can be summarized as O(1/√N ). How-
ever, the proof (which can be found in the supplementary material [15]) analyzes only the case when
ﬁnite sample estimates of the LDA moments are constructed from one triple per document, i.e.,
w3 only, and not from the U-statistics that average multiple (dependent) triples per doc-
w1 ⊗
ument as in the practical expressions (43) and (44) (Appendix F.4). Moreover, one has to be careful
when comparing upper bounds. Nevertheless, comparing the bound (14) with the current theoretical
results for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the
ℓ2-norm of the columns of the topic matrix D in the numerator, as opposed to the O(1) coefﬁcient
for the LDA moments. This norm can be signiﬁcantly smaller than 1 for vectors in the simplex
(e.g., ∆ = O(1/
dkk0) for sparse topics). This suggests that the GP/DICA cumulants may have
better ﬁnite sample convergence properties than the LDA moments and our experimental results in
Section 5.2 are indeed consistent with this statement.

∆3/2 ¯L3, ¯c3/2

1/√N max

w2 ⊗

¯L3/2

−

b

k

{

}

0

.

The GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as
they are expressed via the count vectors x (which are the sufﬁcient statistics for the model) and
not the tokens wℓ’s. Note also that the construction of the LDA moments depend on the unknown
parameter c0. Given that we are in an unsupervised setting and that moreover the evaluation of
LDA is a difﬁcult task [16], setting this parameter is non-trivial. In Appendix G.4, we observe
experimentally that the LDA moments are somewhat sensitive to the choice of c0.

4 Diagonalization algorithms

How is the diagonal structure (11) of S and (13) of T going to be helpful for the estimation of the
model parameters? This question has already been thoroughly investigated in the signal processing
(see, e.g., [17, 18, 19, 20, 21, 5] and references therein) and machine learning (see [3, 4] and refer-
ences therein) literature. We review the approach in this section. Due to similar diagonal structure,
the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants.

For simplicity, let us rewrite the expressions (11) and (13) for S and T as follows

S =

skdkd⊤k ,

k

T =

tkdk ⊗

dk ⊗

k

dk,

(15)

1Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact

X

X

and, in general, depend on the prior on αk.

2For completeness, we also present the ﬁnite sample estimates

the LDA moments (which are consistent with the ones suggested in [4]) in Appendix F.4.

S (43) and
b

T (44) of S (39) and T (40) for
b

4

D

dk := √skdk,
where sk := var(αk) and tk := cum(αk, αk, αk). Introducing the rescaled topics
D⊤. Following the same assumption from [3] that the topic vectors are
we can also rewrite S =
M of S, i.e.,
linearly independent (
D is full rank), we can compute a whitening matrix W
a matrix such that W SW ⊤ = IK where IK is the K-by-K identity matrix (see Appendix F.1 for
more details). As a result, the vectors zk := W
e
RK

dk form an orthonormal set of vectors.
RK
K of a tensor
e

Further, let us deﬁne a projection

K onto a vector u

RK:

RK

(v)

e
×

∈

∈

∈

e

e

T

K

×

×

×

(u)k1k2 :=

T

T ∈
k3 Tk1k2k3 uk3.

(16)

X
Applying the multilinear transformation (see, e.g., [4] for the deﬁnition) with W ⊤ to the tensor T
RK,
from (15) and projecting the resulting tensor
we obtain

:= T (W ⊤, W ⊤, W ⊤) onto some vector u

∈

T

T

(u) =

tk := tk/s3/2

tkh
stands for the inner product. As the
where
e
vectors zk are orthonormal, the pairs zk and λk :=
(u),
tkh
which are uniquely deﬁned if the eigenvalues λk are all different. If they are unique, we can recover
the GP/DICA (as well as LDA) model parameters via
e

·i
are the eigenpairs of the matrix

is due to the rescaling of topics and

dk = W †zk and

h·
zk, u
i

zkz⊤k ,
i

tk = λk/

zk, u

X

e

T

(17)

k

k

,

zk, u
h

.
i

e

e

b

This procedure was referred to as the spectral algorithm for LDA [3] and the fourth-order3 blind
identiﬁcation algorithm for ICA [17, 18]. Indeed, one can expect that the ﬁnite sample estimates
T (30) possess approximately the diagonal structure (11) and (13) and, therefore, the rea-
S (29) and
soning from above can be applied, assuming that the effect of the sampling error is controlled.
b
This spectral algorithm, however, is known to be quite unstable in practice (see, e.g., [22]). To over-
come this problem, other algorithms were proposed. For ICA, the most notable ones are probably
the FastICA algorithm [20] and the JADE algorithm [21]. The FastICA algorithm, with appropriate
choice of a contrast function, estimates iteratively the topics, making use of the orthonormal struc-
ture (17), and performs the deﬂation procedure at every step. The recently introduced tensor power
method (TPM) for the LDA model [4] is close to the FastICA algorithm. Alternatively, the JADE al-
gorithm modiﬁes the spectral algorithm by performing multiple projections for (17) and then jointly
diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special
case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Impor-
tantly, a fast implementation [23] of the orthogonal joint diagonalization algorithm from [24] was
proposed, which is based on closed-form iterative Jacobi updates (see, e.g., [25] for the later).

In practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see,
e.g., [26, p. 30]) or the spectral algorithm. Moreover, although the application of the JD algorithm
for the learning of topic models was mentioned in the literature [4, 27], it was never implemented in
practice. In this paper, we apply the JD algorithm for the diagonalization of the GP/DICA cumulants
as well as the LDA moments, which is described in Algorithm 1. Note that the choice of a projection
RK is important and corresponds to
RM obtained as vp =
W ⊤up for some vector up ∈
vector vp ∈
W ⊤ along the third mode. Importantly, in Algorithm 1, the
the multilinear transformation of
T with
joint diagonalization routine is performed over (P + 1) matrices of size K
K, where the number of
c
topics K is usually not too big. This makes the algorithm computationally fast (see Appendix G.1).
b
The same is true for the spectral algorithm, but not for TPM.

c

×

In Section 5.1, we compare experimentally the performance of the spectral, JD, and TPM algorithms
for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of
any experimental comparison of these algorithms in the LDA context. While already working on
this manuscript, the JD algorithm was also independently analyzed by [27] in the context of tensor
factorization for general latent variable models. However, [27] focused mostly on the comparison
of approaches for tensor factorization and their stability properties, with brief experiments using a
latent variable model related but not equivalent to LDA for community detection. In contrast, we
provide a detailed experimental comparison in the context of LDA in this paper, as well as propose
a novel cumulant-based estimator. Due to the space restriction the estimation of the topic matrix D
and the (gamma/Dirichlet) parameter c are moved to Appendix F.6.

3See Appendix C.5 for a discussion on the orders.

5

Algorithm 1 Joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)

RM

N , K, P (number of random projections); (and c0 for LDA moments)

M ((29) for GP/DICA / (43) for LDA in Appendix F)

RM

1: Input: X
∈
2: Compute sample estimate
3: Estimate whitening matrix

×

×

×
RK

S (see Appendix F.1)

S
∈
M of
W
∈
option (a): Choose vectors
u1, u2, . . . , uP } ⊆
b
{
RM for all p = 1, . . . , P
sphere and set vp =
W ⊤up ∈
c
option (b): Choose vectors
u1, u2, . . . , uP } ⊆
{
RM for all p = 1, . . . , K
RK and set vp =
c
W ⊤up ∈
RK
4: For
p, compute Bp =
W
c
5: Perform orthogonal joint diagonalization of matrices
W
{
RK
c
K and vectors
(see [24] and [23]) to ﬁnd an orthogonal matrix V
×
such that
c
diag(ap), p = 1, . . . , P

∈
W ⊤V ⊤ = IK , and V BpV ⊤

T (vp)

W ⊤

c

c

W

∈

S

S

V

∀

b

b

b

{

×

RK uniformly at random from the unit ℓ2-
(P = 1 yields the spectral algorithm)
RK as the canonical basis e1, e2, . . . , eK of

W ⊤ = IK, Bp, p = 1, . . . , P

a1, a2, . . . , aP } ⊂

}
RK

K ((52) for GP/DICA / (54) for LDA; Appendix F)

6: Estimate joint diagonalization matrix A = V
b
7: Output: Estimate of D and c as described in Appendix F.6

c

c

W and values ap, p = 1, . . . , P

≈

c

5 Experiments

In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and
(b) the spectral algorithm [3], the tensor power method [4] (TPM), the joint diagonalization (JD)
algorithm from Algorithm 1, and variational inference for LDA [1].

Real data: the associated press (AP) dataset, from D. Blei’s web page,4 with N = 2, 243 documents
L = 194; the NIPS papers
and M = 10, 473 vocabulary words and the average document length
dataset5 [28] of 2, 483 NIPS papers and 14, 036 words, and
L = 1, 321; the KOS dataset,6 from the
UCI Repository, with 3, 430 documents and 6, 906 words, and
b

Semi-synthetic data are constructed by analogy with [29]: (1) the LDA parameters D and c are
learned from the real datasets with variational inference and (2) toy data are sampled from a model
of interest with the given parameters D and c. This provides the ground truth parameters D and c.
For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are
the minimum and maximum values. For the AP data, K
topics are learned and, for
topics are learned. For larger K, the obtained topic matrix is ill-
the NIPS data, K
conditioned, which violates the identiﬁability condition for topic recovery using moment matching
techniques [3]. All the documents with less than 3 tokens are resampled.

L = 136.

10, 50

10, 90

∈ {

∈ {

b

b

}

}

Sampling techniques. All the sampling models have the parameter c which is set to c = c0¯c/
k1,
¯c
k
where ¯c is the learned c from the real dataset with variational LDA, and c0 is a parameter that we
can vary. The GP data are sampled from the gamma-Poisson model (3) with b = c0/
L so that
the expected document length is
L (see Appendix B.2). The LDA-ﬁx(L) data are sampled from the
LDA model (2) with the document length being ﬁxed to a given L. The LDA-ﬁx2(γ,L1,L2) data
γ)-portion of the documents are sampled from the LDA-ﬁx(L1) model
are sampled as follows: (1
with a given document length L1 and γ-portion of the documents are sampled from the LDA-ﬁx(L2)
model with a given document length L2.

−

b

b

Evaluation. The evaluation of topic recovery for semi-synthetic data is performed with the ℓ1-
D and true D topic matrices with the best permutation of columns:
error between the recovered
1
errℓ1(
[0, 1]. The minimization is over the possible
D, D) := minπ
dπk −
dkk1 ∈
2K
b
permutations π
D and can be efﬁciently obtained with the Hungarian
PERM of the columns of
b
b
algorithm for bipartite matching. For the evaluation of topic recovery in the real data case, we use
an approximation of the log-likelihood for held out documents as the metric [16]. See Appendix G.6
for more details.

PERM

k k

P

∈

b

∈

4http://www.cs.columbia.edu/˜blei/lda-c
5http://ai.stanford.edu/˜gal/data
6https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

6

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

JD
JD(k)
JD(f)
Spec
TPM

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

Figure 1: Comparison of the diagonalization algorithms. The topic matrix D and Dirichlet parameter c are
learned for K = 50 from AP; c is scaled to sum up to 0.5 and b is set to ﬁt the expected document length
L = 200. The semi-synthetic dataset is sampled from GP; number of documents N varies from 1, 000 to
b
50, 000. Left: GP/DICA moments. Right: LDA moments. Note: a smaller value of the ℓ1-error is better.

We use our Matlab implementation of the GP/DICA cumulants, the LDA moments, and the diag-
onalization algorithms. The datasets and the code for reproducing our experiments are available
online.7 In Appendix G.1, we discuss the complexity and implementation of the algorithms. We
explain how we initialize the parameter c0 for the LDA moments in Appendix G.3.

5.1 Comparison of the diagonalization algorithms

In Figure 1, we compare the diagonalization algorithms on the semi-synthetic AP dataset for K = 50
using the GP sampling. We compare the tensor power method (TPM) [4], the spectral algorithm
(Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm 1 with different
options to choose the random projections: JD(k) takes P = K vectors up sampled uniformly from
the unit ℓ2-sphere in RK and selects vp = W ⊤up (option (a) in Algorithm 1); JD selects the full basis
e1, . . . , eK in RK and sets vp = W ⊤ep (as JADE [21]) (option (b) in Algorithm 1); JD(f ) chooses
the full canonical basis of RM as the projection vectors (computationally expensive).

Both the GP/DICA cumulants and LDA moments are well-speciﬁed in this setup. However, the
LDA moments have a slower ﬁnite sample convergence and, hence, a larger estimation error for the
same value N . As expected, the spectral algorithm is always slightly inferior to the joint diagonal-
ization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms
demonstrate good performance, which also fulﬁlls our expectations. However, although TPM shows
almost perfect performance in the case of the GP/DICA cumulants (left), it signiﬁcantly deteriorates
for the LDA moments (right), which can be explained by the larger estimation error of the LDA
moments and lack of robustness of TPM. The running times are discussed in Appendix G.2. Over-
all, the orthogonal joint diagonalization algorithm with initialization of random projections as W ⊤
multiplied with the canonical basis in RK (JD) is both computationally efﬁcient and fast.

5.2 Comparison of the GP/DICA cumulants and the LDA moments

In Figure 2, when sampling from the GP model (top, left), both the GP/DICA cumulants and LDA
moments are well speciﬁed, which implies that the approximation error (i.e., the error w.r.t.
the
model (mis)ﬁt) is low for both. The GP/DICA cumulants achieve low values of the estimation error
already for N = 10, 000 documents independently of the number of topics, while the convergence
is slower for the LDA moments. When sampling from the LDA-ﬁx(200) model (top, right), the
GP/DICA cumulants are mis-speciﬁed and their approximation error is high, although the estimation
error is low due to the faster ﬁnite sample convergence. One reason of poor performance of the
GP/DICA cumulants, in this case, is the absence of variance in the document length. Indeed, if
documents with two different lengths are mixed by sampling from the LDA-ﬁx2(0.5,20,200) model
(bottom, left), the GP/DICA cumulants performance improves. Moreover, the experiment with a
changing fraction γ of documents (bottom, right) shows that a non-zero variance on the length
improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a
non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not
likely to happen.

7 https://github.com/anastasia-podosinnikova/dica

7

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

1

0.8

0
1 

0.2

0
1 

r
o
r
r
e
-
1

ℓ

0.6

0.4

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11.5

-12

-12.5

-13

-13.5

r
o
r
r
e
-
1

ℓ

r
o
r
r
e
-
1

ℓ

0
1 

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

0.8
0.2
Fraction of doc lengths γ

0.6

0.4

1

Figure 2: Comparison of the GP/DICA cumulants and LDA moments. Two topic matrices and parameters c1
and c2 are learned from the NIPS dataset for K = 10 and 90; c1 and c2 are scaled to sum up to c0 = 1.
Four corpora of different sizes N from 1, 000 to 50, 000: top, left: b is set to ﬁt the expected document length
L = 1300; sampling from the GP model; top, right: sampling from the LDA-ﬁx(200) model; bottom, left:
b
sampling from the LDA-ﬁx2(0.5,20,200) model. Bottom, right: the number of documents here is ﬁxed to
N = 20, 000; sampling from the LDA-ﬁx2(γ,20,200) model varying the values of the fraction γ from 0 to 1
with the step 0.1. Note: a smaller value of the ℓ1-error is better.

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 3: Experiments with real data. Left: the AP dataset. Right: the KOS dataset. Note: a higher value of
the log-likelihood is better.

5.3 Real data experiments

In Figure 3, JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI)
and with variational inference initialized with the output of JD-GP (VI-JD). We measure the held
out log-likelihood per token (see Appendix G.7 for details on the experimental setup). The orthogo-
nal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates promising
performance. In particular, the GP/DICA cumulants signiﬁcantly outperform the LDA moments.
Moreover, although variational inference performs better than the JD-GP algorithm, restarting varia-
tional inference with the output of the JD-GP algorithm systematically leads to better results. Similar
behavior has already been observed (see, e.g., [30]).

6 Conclusion

In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where
word counts are directly modelled. These moments make fewer assumptions regarding distributions,
and are theoretically and empirically more robust than previously proposed tensors for LDA, both
on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization
procedure is also more robust. Once the topic matrix has been estimated in a semi-parametric way
where topic intensities are left unspeciﬁed, it would be interesting to learn the unknown distributions
of the independent topic intensities.

Aknowledgements. This work was partially supported by the MSR-Inria Joint Center. The authors
would like to thank Christophe Dupuy for helpful discussions.

8

References
[1] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:903–1022,

[2] T. Grifﬁths. Gibbs sampling in the generative model of latent Dirichlet allocation. Technical report,

[3] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

2003.

Stanford University, 2002.

allocation. In NIPS, 2012.

[4] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning

latent variable models. J. Mach. Learn. Res., 15:2773–2832, 2014.

[5] P. Comon and C. Jutten, editors. Handbook of blind sourse separation: independent component analysis

[6] C. Jutten. Calcul neuromim´etique et traitement du signal: analyse en composantes ind´ependantes. PhD

and applications. Academic Press, 2010.

thesis, INP-USM Grenoble, 1987.

[7] C. Jutten and J. H´erault. Blind separation of sources, part I: an adaptive algorithm based on neuromimetric

architecture. Signal Process., 24:1–10, 1991.

[8] P. Comon. Independent component analysis, a new concept? Signal Process., 36:287–314, 1994.

[9] W.L. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002.

[10] M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. R. Stat. Soc., 61:611–622,

1999.

[11] S. Roweis. EM algorithms for PCA and SPCA. In NIPS, 1998.

[12] J. Canny. GaP: a factor model for discrete data. In SIGIR, 2004.

[13] W.L. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.

[14] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of inde-

[15] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

pendence. Oxford University Press, 2013.

allocation. CoRR, abs:1204.6703, 2013.

[16] H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In

[17] J.-F. Cardoso. Source separation using higher order moments. In ICASSP, 1989.

[18] J.-F. Cardoso. Eigen-structure of the fourth-order cumulant tensor with application to the blind source

separation problem. In ICASSP, 1990.

[19] J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods. In

[20] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Trans.

Neural Netw., 10(3):626–634, 1999.

[21] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. In IEE Proceedings-F,

[22] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Comput., 11:157–192,

[23] J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal.

Appl., 17(1):161–164, 1996.

[24] A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simulataneous diagonalization.

SIAM J. Matrix Anal. Appl., 14(4):927–949, 1993.

[25] J. Nocedal and S.J. Wright. Numerical optimization. Springer, 2nd edition, 2006.

[26] F.R. Bach and M.I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1–48, 2002.

[27] V. Kuleshov, A.T. Chaganty, and P. Liang. Tensor factorization via matrix factorization. In AISTATS,

[28] A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. J.

Mach. Learn. Res., 8:2265–2295, 2007.

[29] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm

for topic modeling with provable guarantees. In ICML, 2013.

[30] S. Cohen and M. Collins. A provably correct learning algorithm for latent-variable PCFGs. In ACL, 2014.

ICML, 2009.

ISCAS, 1996.

1993.

1999.

2015.

9

A Appendix. Plate diagrams for the models from Section 2

c

θn

znℓ

wnℓ

N

N

N

c

θn

xn

c

αn

N

αn

M

xnm

M

xnm

D

Ln

D

D

D

(a) LDA (18)

(b) LDA (19)

(c) GP (20)

(d) DICA (21)

Figure 4: Plate diagrams for the models from Section 2.

In Section 2, the index n, which stands for the n-th document, was omitted. For convenience, we
recall the models. The LDA model in the tokens representation:

the LDA model with the marginalized out latent variable z:

θn ∼
znℓ|
θn ∼
znℓ, θn ∼

wnℓ|

Dirichlet(c),
Multinomial(1, θn),
Multinomial(1, dznℓ);

θn ∼
θn ∼

xn|

Dirichlet(c),
Multinomial(Ln, Dθn);

αnk ∼
αn ∼

xnm|

Gamma(ck, b),
Poisson([Dαn]m);

αn1, . . . , αnK ∼
αn ∼
xnm|

mutually independent,
Poisson([Dαn]m).

the GP model:

and the DICA model:

(18)

(19)

(20)

(21)

B Appendix. The GP model

B.1 The connection between the LDA and GP models

To show that the LDA model (2) with the additional assumption that the document length is modeled
as a gamma-Poisson random variable is equivalent to the GP model (3), we show that:

- when modeling the document length L as a Poisson random variable with a parameter λ,
the count vectors x1, x2, . . . , xM are mutually independent Poisson random variables;

- the Gamma prior on λ reveals the connection αk = λθk between the Dirichlet random

variable θ and the mutually independent gamma random variables α1, α2, . . . , αK.

For completeness, we repeat
the known result
∼
Multinomial(L, Dθ) (which thus means that L =
m xm with probability one), then x1, x2,
. . . , xM are mutually independent Poisson random variables with parameters λ [Dθ]1, λ [Dθ]2, . . . ,

Poisson(λ) and x
|

if L

that

∼

L

P

10

λ [Dθ]M . Indeed, we consider the following joint probability mass function where x and L are
assumed to be non-negative integers:

p(x, L

θ, λ) =p(L

|

λ)p(x
|

|

L, θ)

L=Pm xm

exp (

λ) λL

−
✚✚L!

✚✚L!
m xm!

[Dθ]xm
m

L=Pm xm

exp(

λ

−

L=Pm xm

m
Y

Q
[Dθ]m)λPm xm

[Dθ]xm
m
xm!

exp(

m
m
Y
X
λ [Dθ]m)(λ [Dθ]m)xm
−

xm!

L=Pm xm

Poisson(xm; λ [Dθ]m),

=1

=1

=1

=1

{

{

{

{

}

}

}

}

m
Y

m
Y

where in the third equation we used the fact that

[Dθ]m =

Dmkθk =

θk

Dmk = 1.

m
X
θ, λ) = p(L

Xm,k
m p(xm|

Xk

m
X

|

{

L=Pm xm

x)
|
and p(xm|

x) is simply the deterministic
We thus have p(x, L
|
distribution 1
λ[Dθ]m) for m = 1, . . . , M are independent Poisson(λ[Dθ]m)
Q
distributions (and thus do not depend on L). Note that in the notation introduced in the paper,
Dmk = dkm. Hence, by using the construction of the Dirichlet distribution from the normalization
of independent gamma random variables, we can show that the LDA model with a gamma-Poisson
prior over the length is equivalent to the following model (recall, that c0 =

λ[Dθ]m) where p(L

}

k ck):

λ
θ
λ, θ

∼
∼
∼

xm|

Gamma(c0, b),
Dirichlet(c),
Poisson([D(λθ)]m).

P

(22)

More speciﬁcally, we complete the second part of the argument with the following properties. When
Gamma(ck, b),
α1, α2, . . . , αK are mutually independent gamma random variables, each αk ∼
k ck, b). The former is equivalent
their sum is also a gamma random variable
k αk ∼
to λ. It is known (e.g., [32]) that a Dirichlet random variable can be sampled by ﬁrst sampling
independent gamma random variables (αk) and then dividing each of them by their sum (λ): θk =
k′ αk′ , and, in other direction, the variables αk = λθk are mutually independent, giving back
αk/
the GP model (3).

Gamma(

P

P

P

B.2 The expectation and the variance of the document length for the GP model

From the drivations in Appendix B.1, it follows that the document length of the GP model (3) is a
gamma-Poisson random variable, i.e., L
Gamma(c0, b). Therefore, the
|
following follows from the law of total expectation and the law of total variance

Poisson(λ) and λ

∼

∼

λ

E(L) = E [E(L
|
var(L) = var [E(L

λ)] = E(λ) = c0/b
λ)] + E [var(L

λ)] = var(λ) + E(λ) = c0/b + c0/b2

|

|
The ﬁrst expression shows that the parameter b controls the expected document length E(L) for a
given parameter c0: the smaller b, the larger E(L). On the other hand, if we allow c0 to vary as
well, only the ratio c0/b is important for the document length. We can then interpret the role of c0
as actually controlling the concentration of the distribution for the length L (through the variance).
More speciﬁcally, we have that:

var(L)
(E(L))2 =

1
E(L)

+

1
c0

.

(23)

For a ﬁxed target document length E(L), we can increase the variance (and thus decrease the con-
centration) by using a smaller c0.

11

C Appendix. The cumulants of the GP and DICA models

C.1 Cumulants

For a random vector x

RM , the ﬁrst three cumulant tensors8 are

∈

cum(xm) = E(xm),
cum(xm1 , xm2 ) = E [(xm1 −
cum(xm1 , xm2 , xm3 ) = E [(xm1 −

E(xm1 ))(xm2 −
E(xm1 ))(xm2 −

E(xm2 )] = cov(xm1 , xm2),
E(xm3 )))] .
E(xm2 ))(xm3 −

Note that the 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not for
RM
M denotes the third order tensor with ele-
higher orders). In the following, cum(x, x, x)
ments cum(xm1 , xm2 , xm3). Some of the properties of cumulants are listed below (see [5, chap. 5]).
The most important property that motivate us to use cumulants in this paper (and the ICA literature)
is the independence property, which says that the cumulant tensor for a random vector with inde-
pendent components is diagonal (this property does not hold for the (non-central) moment tensors
of any order, and neither for the central moments of order 4 or more).

∈

M

×

×

RM are independent, then their cross-cumulants
- Independence. If the elements of x
are zero as soon as two indices are different, i.e., cum(xm1 , xm2 ) = δ(m1, m2)E[(xm1 −
E(xm1 ))3], where δ is the
Em1 ))2] and cum(xm1 , xm2 , xm3 ) = δ(m1, m2, m3)E[(xm1 −
Kronecker delta.

∈

- Multilinearity. If two random vectors y

y = Dα for some D

RM

K, then

×

∈

∈

RM and α

RK are linearly dependent, i.e.,

∈

cum(ym) =

cum(αk)Dmk,

cum(ym1, ym2) =

cum(αk1 , αk2 )Dm1k1Dm2k2,

cum(ym1 , ym2, ym3) =

cum(αk1 , αk2 , αk3 )Dm1k1 Dm2k2 Dm3k3 ,

Xk

Xk1,k2

Xk1,k2,k3

which can also be denoted9 by

E(y) = DE(α),

cov(y, y) = Dcov(α, α)D⊤,

cum(y, y, y) = cum(α, α, α)(D⊤, D⊤, D⊤).

- The law of total cumulance. For two random vectors x

RM and y

RM , it holds

∈

∈

y)] + cov [E(xm1 |

y), E(xm2 |

cum(xm) = E [E(xm|
cum(xm1 , xm2 ) = E [cov(xm1 , xm2 |
cum(xm1 , xm2 , xm3 ) = E [cum(xm1 , xm2, xm3 |

y)] ,

y), cov(xm2 , xm3 |
y), cov(xm1 , xm3 |
y), cov(xm1 , xm2 |
Note that the ﬁrst expression is also well known as the law of total expectation or the tower
property, while the second one is known as the law of total covariance.

+ cov [E(xm1 |
+ cov [E(xm2 |
+ cov [E(xm3 |

y)] + cum [E(xm1 |
y)]
y)]
y)] .

y)] ,
y), E(xm2 |

y), E(xm3 |

y)]

tX

8Strictly speaking, the (scalar) n-th cumulant κn of a random variable X is deﬁned via the cumulant-
generating function g(t), which is the natural logarithm of the moment-generating function, i.e g(t) :=
log E (cid:2)e
(cid:3). The cumulant κn is then obtained from a power series expansion of the cumulant-generating
function, that is g(t) = P

9In [4], given a tensor T ∈ RK×K×K , T (D⊤, D⊤, D⊤) is referred to as the multilinear map. In [34], the

/n! [Wikipedia].

∞
n=1 κnt

same entity is denoted by T ×1 D⊤ ×2 D⊤ ×3 D⊤, where ×n denotes the n-mode tensor-matrix product.

n

12

C.2 The third cumulant of the GP/DICA models

In this section, by analogy with Section 3.1, we derive the third GP/DICA cumulant.
As the third cumulant of a Poisson random variable xm with parameter ym is E((xm −
E(xm))3
ym) = ym, then by the independence property of cumulants from Section C.1, the cu-
mulant of x
|

y is diagonal:

|

cum(xm1 , xm2 , xm3 |

y) = δ(m1, m2, m3) ym1.

y into the law of total cumulance, we obtain

Substituting the cumulant of x
|
cum(xm1 , xm2 , xm3) = E [cum(xm1 , xm2 , xm3 |
y), E(xm2 |
+ cum [E(xm1 |
+ cov [E(xm2 |
y), cov(xm1 , xm3|
= δ(m1, m2, m3)E(ym1 ) + cum(ym1, ym2, ym3)

y), E(xm3 |

y)] + cov [E(xm3 |

y)]
y)] + cov [E(xm1 |

y), cov(xm2 , xm3 |
y)]

y), cov(xm1 , xm2|

y)]

+ δ(m2, m3)cov(ym1, ym2) + δ(m1, m3)cov(ym1, ym2) + δ(m1, m2)cov(ym1, ym3 )

= δ(m1, m2, m3)E(xm1 ) + cum(ym1 , ym2, ym3)

+ δ(m2, m3)cov(xm1 , xm2 )
+ δ(m1, m3)cov(xm1 , xm2 )
+ δ(m1, m2)cov(xm1 , xm3 )

δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )

−
−
−

= cum(ym1 , ym2, ym3)

2δ(m1, m2, m3)E(xm1 )

−

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 )

=

cum(α, α, α)(D⊤, D⊤, D⊤)

2δ(m1, m2, m3)E(xm1 )

m1m2m3 −

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 ),
(cid:2)
where, in the third equality, we used the previous result from (9) that cov(y, y) = cov(x, x)
diag(E(x)).

(cid:3)

(24)

−

C.3 The diagonal structure of the GP/DICA cumulants

In this section, we provide detailed derivation of the diagonal structure (11) of the matrix S (10) and
the diagonal structure (13) of the tensor T (12).

From the independence of α1, α2, . . . , αK and by the independence property of cumulants
it follows that cov(α, α) is a diagonal matrix and cum(α, α, α) is a
from Section C.1,
i.e., cov(αk1 , αk2 ) = δ(k1, k2)cov(αk1 , αk2 ) and cum(αk1 , αk2 , αk3 ) =
diagonal
δ(k1, k2, k3)cum(αk1 , αk1 , αk1 ). Therefore, the following holds

tensor,

cov(ym1 , ym2) =

cov(αk, αk)Dm1kDm2k,

cum(ym1 , ym2, ym3) =

cum(αk, αk, αk)Dm1kDm2kDm3k,

which we can rewrite in a matrix/tensor form as

cov(y, y) =

cov(αk, αk)dkd⊤k ,

cum(y, y, y) =

cum(αk, αk, αk)dk ⊗

dk ⊗

dk.

Moving cov(y, y) / cum(y, y, y) in the expression for cov(x, x) (9) / cum(x, x, x) (24) on one side of
M
equality and all other terms on the other side, we deﬁne matrix S
as follows

M / tensor T

RM

RM

∈

∈

M

×

×

×

S := cov(x, x)

diag (E(x)) ,
Tm1m2m3 := cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)E(xm1 )

−

Xk

Xk

Xk

Xk

(25)

(26)

δ(m2, m3)cov(xm1 , xm2 )
δ(m1, m3)cov(xm1 , xm2 )
δ(m1, m2)cov(xm1 , xm3 ).

−
−
−

13

By construction, S = cov(y, y) and T = cum(y, y, y) and, therefore, it holds that

S =

cov(αk, αk)dkd⊤k ,

Xk

(27)

(30)

(31)

T =

cum(αk, αk, αk)dk ⊗
This means that both the matrix S and the tensor T are sums of rank-1 matrices and tensors, respec-
tively10. This structure of the matrix S and the tensor T is the basis for the algorithms considered in
this paper.

dk ⊗

Xk

dk.

(28)

C.4 Unbiased ﬁnite sample estimators for the GP/DICA cumulants

Given a sample
the GP/DICA cumulants:

x1, x2, . . . , xN }
{

, we obtain a ﬁnite sample estimate

S of S (10) /

T of T (12) for

S :=

cov(x, x)

diag

−

,

E(x)
(cid:17)

(cid:16)

b

b

(29)

E(xm1 )

b

b

Tm1m2m3 :=

cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)
b
d
δ(m2, m3)
δ(m1, m3)
δ(m1, m2)

d

cov(xm1 , xm2 )
−
cov(xm1 , xm2 )
−
d
cov(xm1 , xm3 ),
−
where unbiased estimators of the ﬁrst three cumulants are
d
1
d
N

E(xm1 ) =

xnm1 ,

b

b
cov(xm1 , xm2 ) =

znm1znm2 ,

cum(xm1 , xm2, xm3 ) =

d

znm1znm2znm3 ,

2)

−

n
X

d

where the word vocabulary indexes are m1, m2, m3 = 1, 2, . . . , M and the centered documents
E(xm). (The latter is introduced only for compact representation of (31) and is
znm := xnm −
different from z in the LDA model.)
b
C.5 On the orders of cumulants

DU )(

DU )⊤ with any orthogonal K

D as one can equivalently use
Note that the factorization of S =
K matrix U . Therefore, one has to consider higher
S = (
than the second order information. Moreover, in ICA the fourth-order tensors are used, because the
third cumulant of the Gaussian distribution is zero, which is not the case in the DICA/LDA models,
where the third order information is sufﬁcient.

D⊤ does not uniquely determine

D

×

e

e

e

e

e

n
X
1

1

−

N

(N

−

n
X
N
1)(N

D Appendix. The sketch of the proof for Proposition 3.1

D.1 Expected squared error for the sample expectation

The sample expectation is

n xn is an unbiased estimator of the expectation and:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

−

E(xm)
(cid:17)

2

(cid:21)

E(x) = 1
N
E(xm)
P

E

(cid:17)

=

=

=

b
m
X
1
N 2

1
N

(cid:20)(cid:16)

b
E



m
X

E

 

n
X
(xm −
h

m
X

!



n
X

var(xm).

=n′
Xn

E(xm))2

=

1
N

i

m
X

(xnm −

E(xm))2

+ E

(xnm −

E(xm)) (xn′m −

E(xm))









10For tensors, such decomposition is also known under the names CANDECOMP/PARAFAC or, simply, the

CP decomposition (see, e.g., [34]).

14

[E(var(xm|

y)) + var(E(xm|

y))] =

[E(ym) + var(ym)]

1
N

m
X

Further, by the law of total variance:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

(cid:17)

=

=

1
N

m
X

1
N "

using the fact that

P

E(αk) +

dk, dki
h

var(αk)

,

#

Xk
m Dmk = 1 for any k.

Xk

D.2 Expected squared error for the sample covariance

The following ﬁnite sample estimator of the covariance cov(x, x) = E(xx⊤)

E(x)E(x)⊤

−

E(x)

E(x)⊤ =

1

N

1

−

n  
X

xnx⊤n −

1
N 2

xn′ x⊤n′′

!

Xn′

Xn′′

cov(x, x) =

d

N

1
N

=

1

1

−

n
X

xnx⊤n −

xnx⊤n −



N

n
X



b
xn

b
1

1

−

=n

Xn′

x⊤n′





is unbiased, i.e., E(

cov(x, x)) = cov(x, x). Its squared error is

E

d
cov(x, x)

cov(x, x)
k

−

2
F

=

E

cov(xm, xm′ )
(

E[

cov(xm, xm′ )])2

.

−

Xm,m′

h

(cid:1)

d

d

i

k
(cid:0)

d

The m, m′-th element of the sum above is equal to

xnm

xn′′m′

,

xn′mxn′m′

xn′m

xn′′′m′

=n′
Xn′′′





cov

xnm

xn′′m′ , xn′mxn′m′

−

N

1

1

−

=n

Xn′′

cov

xnmxnm′



−

N

1

1

−


cov (xnmxnm′ , xn′mxn′m′ )

=n

Xn′′

2
N 2(N

−

xn′′m′, xn′m

xn′′′m′

1)

−

Xn,n′

=n′
Xn′′′









1)2

cov

xnm



−

Xn,n′

Xn′′

cov (xnmxnm′ , xnmxnm′ )

=n

(32)







1
N 2

Xn,n′
1
N 2

Xn,n′
1
N 2(N

=

+

=

1
N 2

n
X
2
N 2(N

1
N 2(N

−

+

+

1
N 2(N

cov (xnmxn′′m′ , xnmxnm′ ) +

cov (xnmxn′m′, xn′mxn′m′ )

1) 

−



n
X

=n

Xn′′

1)2 



1)2 

−

−

n
X

Xn′′

=n Xn′′′

=n

Xn′


=n′
Xn

=n′
Xn′′′

n
X

=n

Xn′

cov (xnmxn′′m′ , xnmxn′′′m′ ) +

cov (xnmxn′m′ , xn′mxn′′′m′ ) +


cov (xnmxn′′m′ , xn′mxnm′ )


cov (xnmxn′′m′ , xn′mxn′′m′ )



,





Xn′

=n′
Xn

=n

Xn′′

Xn′

=n′
Xn

=n

Xn′′

where we used mutual independence of the observations xn in a sample
the covariance between the two expressions involving only independent variables is zero.

xn}

{

N
n=1 to conclude that

15

Further:
E

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N 2

k
(cid:0)

4
d
N 2(N

2
N 2(N

2
N 2(N

−

+

+

N (N

E(x2

(cid:1)
1)

−

N (N

(cid:0)
1)(N

2)

−

N (N

1)(N

2)

−

−

−

1)

−

Xm,m′

1)2

1)2

−

−

Xm,m′

Xm,m′

which after simpliﬁcation gives

N

E(x2
(cid:16)

Xm,m′
mxm′ )E(xm′ )

−

mx2

m′ )

[E(xmxm′ )]2

−

(cid:17)

E(xmxm′ )E(xm)E(xm′ )

[E(xm)]2 [E(xm′ )]2

(cid:1)

(cid:17)

−

m) [E(xm′ )]2

E(x2
(cid:16)
E(xmxm′ )E(xm)E(xm′ )
(cid:16)

−

[E(xm)]2 [E(xm′ )]2

+ O

1
N 2

,

(cid:18)

(cid:19)

(cid:17)

,

E

k

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N

Xm,m′
[2E(xm)E(xm′ )cov(xm, xm′ )

(cid:1)

h

(cid:0)
+

1
d
N

Xm,m′

var(xmxm′ ) + 2 [E(xm)]2 var(xm′ )
i
1
N 2

4E(xm)cov(xmxm′ , xm′ )] + O

−

(cid:18)

(cid:19)

where in the last equality, by symmetry, the summation indexes m and m′ can be exchanged. As
Poisson(ym), by the law of total expectation and law of total covariance, it follows, for
xm ∼
m

= m′ (and using the auxiliary expressions from Section D.4):
[E[xmxm′ ]]2 = E
mym′ + ymy2

var(xmxm′ ) = E(x2
mx2
my2
y2
m′ + ymym′
(cid:2)
[E(xm)]2 var(xm′ ) = [E(ym)]2 E(ym′ ) + [E(ym)]2 E(y2

m′ )
−
m′ + y2

E(x2

= E

m′ )

(cid:2)

y)]]2

mx2
m′

[E [E(xmxm′

|

−

y)
[E(ymym′)]2 ,
−
[E(ym)]2 [E(ym′ )]2 ,
(cid:3)

(cid:3)

|

E(xm)E(xm′ )cov(xm, xm′ ) = E(ymym′ )E(ym)E(ym′ )

E(xm)cov(xmxm′ , xm′ ) = E(ym)
Now, considering the m = m′ case, we have:
var(x2

E(ymym′) + E(ymy2

−
[E(ym)]2 [E(ym′ )]2 ,
m′)

E(ymym′ )E(ym′ )

−

−

.

(cid:3)

E(xm)E(xm)cov(xm, xm) = E(ym)2
m, xm) = E(ym)

E(xm)cov(x2
Substitution of ym =

(cid:2)

2

y)]

E[E(x2

(cid:2)
m) = E[E(x4
y)]
m|
m|
−
= E
y4
m + 6y3
m + 7y2
m + ym
(cid:2)
m) + E(ym)
E(y2
h
m) + 3E(y2
E(y3

2

,

E

y2
m + ym
(cid:3)
−
[E(ym)]2
(cid:3)
(cid:2)
−
m) + E(ym)

i
E(ym)

(cid:2)

,

(cid:3)(cid:3)

−

k Dmkαk gives the following
(cid:2)
1
N

=

2
F

cov(x, x)
P
k

−

Xk,k′,k′′,k′′′h

dk, dk′

dk′′ , dk′′′

ih

(cid:2)
iAkk′k′′k′′′

E

cov(x, x)

k
(cid:0)

d

E(y2

m) + E(ym)

.

dk′′ ,~1

iBkk′k′′ +

dk ◦
h

dk′ , dk′′

iEkk′k′′

(cid:3)(cid:3)

i

(cid:1)

1
N

1
N

+

+

+

Xk

ih

Xk,k′,k′′

dk, dk′
h
h
dk,~1
h
Xk,k′
h
E(αk) + O
dk,~1
i
h

ih

1
N 2

,

(cid:18)

(cid:19)

E(αkαk′ ) +
dk′ ,~1
i

dk, dk′
h

iFkk′

i

where ~1 is the vector with all the elements equal to 1 and
Akk′k′′k′′′ = E(αkαk′ αk′′ αk′′′ )

−

E(αkαk′′ )E(αk′ αk′′′ ) + 2E(αk)E(αk′ )E(αk′′ αk′′′ )

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ ) + 2E(αkαk′′ )E(αk′ )E(αk′′′ )
4E(αk)E(αk′ αk′′ αk′′′ ) + 4E(αk)E(αk′ αk′′ )E(αk′′′ ),

−

−
−

Bkk′k′′ = 2E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ )
Ekk′k′′ = 4E(αkαk′ αk′′ ) + 6E(αk)E(αk′ )E(αk′′ )
Fkk′ = 6E(αkαk′ )

5E(αk)E(αk′ ),

−

−
−

4E(αk)E(αk′ αk′′ ),
10E(αkαk′ )E(αk′′ ),

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ )

16

where we used the expressions from Section D.4.

D.3 Expected squared error of the estimator

S for the GP/DICA cumulants

As the estimator

S (29) of S (10) is unbiased, its expected squared error is

b

E

S

S

2
b
F
k

−

=E

i

k
h

b

cov(x, x)

cov(x, x)) +

diag[

E(x)]

diag [E(x)]

(
(cid:20)(cid:13)
(cid:13)
= E
(cid:13)

+ 2

E(x)

d
k
h

E
b

m
X

h(cid:16)

b

−
E(x)
k

2
F

−
E(xm)

+ E

i
(cid:2)
E(xm)
(cid:17)

−

−

(cid:16)
cov(x, x)

b

−

k
cov(xm, xm)
(
d

−

2

(cid:21)

F
(cid:17)(cid:13)
(cid:13)
(cid:13)

i

cov(x, x)
k

2
F

(cid:3)
cov(xm, xm))

.

(33)

cov(xm, xm) are unbiased, the m-th element of the last sum is equal to

d

E(xm) and
E(xm),
b
h
1
b
N 2

Xn,n′

As

cov

=

=

1
N 2

d

cov(xm, xm)
i
n′m

xnm, x2

d

cov

(cid:2)
xnm, x2

nm

(cid:3)

cov

n
X
E(x3

(cid:2)

1
N 2(N

−

2
N 2(N

−

(cid:3)

1)

−

Xn,n′,n′′

=n′

1)

=n

Xn,n′

−
[E(xm)]3

cov [xnm, xn′mxn′′m]

cov [xnm, xn′mxnm] + O

1
N 2

(cid:18)

(cid:19)

=

−

m)

1
N
1
N

m)E(xm)

E(x2
(cid:16)
[E(xm)]3 + O

2
N
2
N
≤
(cid:18)
where we neglected the negative term
from the expressions in Section D.4. Further, the fact that ym =

(cid:18)
(cid:19)
E(y3
m) + 3E(y2
h

(cid:19)
E(x2

(cid:17)
1
N

1
N 2

1
N 2

m) +

E(x3

+ O

−

−

=

m) + E(ym) + 2 [E(ym)]3

+ O

(cid:18)
m)E(xm) for the inequality, and the last equality follows

i

1
N 2

,

(cid:19)

k Dmkαk gives

cov

E(xm),
h

cov(xm, xm)
i

m
X

b

d

=

+

1
N

3
N

Xk,k′,k′′h

Xk,k′h

dk ◦

dk′ , dk′′

P
iCkk′k′′

dk, dk′

E(αkαk′ ) +
i

1
N

dk,~1
h

E(αk) + O
i

1
N 2

,

(cid:18)

(cid:19)

Xk

where

denotes the elementwise Hadamard product and

◦

Ckk′k′′ = E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ ).
Plugging this and the expressions for E(
cov(x, x)
−
k
Sections D.1 and D.2, respectively, into (33) gives

F ) and E(
k

E(x)
k

E(x)

2

cov(x, x)
k

−

2
F ) from

dk, dki
h

var(αk) +

E(αk) +

dk, dk′

dk′′ , dk′′′

ih

iAkk′ k′′k′′′



+ O

b

Xk

d

Xk,k′,k′′,k′′′h

iBkk′k′′ + 2

dk ◦
h

dk′ , dk′′

iCkk′k′′ ] +

i

≤

30c4

0 + 8c0

0 + 23c3

0 + 14c2
b4

where we used that, by the simplex constraint on the topics,
expression in more details, let us now consider the GP model, i.e., αk ∼
Xk,k′,k′′,k′′′ Akk′ k′′k′′′
Xk,k′,k′′ Ckk′k′′
Xk,k′ Fkk′

Xk,k′,k′′ Ekk′k′′

0 + 6c2
b3

0 + c0
b2

0 + c0
b2

E(αkαk′ )

0 + 2c0

Xk,k′

2c2

2c2

7c3

and

and

and

≤

≤

≤

≤

,

,

,

(1 + 6

dk, dk′
h

) E(αkαk′ ) + 2
i

Xk
= 1 for all k. To analyze this

Xk,k′
dk,~1
h

Gamma(ck, b):
6c3

0 + 10c2
b3

0 + 4c0

,

Xk,k′,k′′ Bkk′k′′

≤

12c3

0 + 10c2
b3

0 + 8c0

,

S

S

2
F

k

−

E

k
h

b
1
N 

+

=

1
N 

i

Xk

dk, dk′
h

[

Xk,k′,k′′


17

1
N 2

(cid:18)

(cid:19)


E(αk)

,





where we used the expressions from Section D.4, which gives

ν
N "

max

k k

2
2

dkk

c0
b2 +

c0
b

+

max
k,k′ h

dk, dk′

(cid:18)

i
(cid:19)

2

max

c4
0
b4 ,

c0
b4

+ max
k,k′ h

dk, dk′

max

i

c3
0
b3 ,

c0
b3

(cid:21)#

S

S

2
F

k

−

≤

i

E

k
h

+

b
ν
N

max
k,k′,k′′h

dk ◦

dk′ , dk′′

max

i
(cid:19)

c3
0
b3 ,

c0
b3

+

1 + max
k,k′ h

dk, dk′

max

(cid:20)

i
(cid:19)

(cid:21)
c2
0
b2 ,

c0
b2

(cid:20)

(cid:21)

(cid:18)

(cid:20)(cid:18)

≤
dkk

(cid:21)(cid:21)
30 is a universal constant. As, by the Cauchy-Schwarz inequality, max k,k′
where ν
2
2
2 =: ∆1 and max k,k′,k′′
max k k
max k k
2 ≤
∆1), it follows that
(note that for the topics in the simplex, ∆2 ≤
L3
¯c2
0 (cid:19)
0L2 + ¯c3

(cid:20)
i
1L4 + ¯c0∆1L3 + ¯c2
∆2

max k k
∆1 as well as ∆2

dkk∞ k
1 ≤

+ L + ∆2
1

L3
¯c2
0 (cid:21)

dk ◦
h

dk′ , dk′′

L2
¯c2
0

L2
¯c0

dkk

+ ∆2

ν
N

+ O

+ O

i ≤

∆1

0L

≤

+

+

(cid:18)

(cid:18)

2
F

E

S

S

(cid:20)

k

,

1
N 2

(cid:19)

L4
¯c3
0
1
N 2

k
h
2ν
b
N

−
1
¯c3
0

≤

(cid:2)

where ¯c0 = min(1, c0)
length. The second term ¯c0∆1L3 cannot be dominant as the system ¯c0∆1L3 > ¯c2
∆2
1L4 is infeasible. Also, with the reasonable assumption that L
¯c3
0L

(cid:19)
1 and, from Section B.2, c0 = bL where L is the expected document
0L2 and ¯c0∆1L3 >
1, we also have that the 4th term

0L2. Therefore,
¯c2

≥

≤

(cid:18)

(cid:3)

≤

(cid:20)

,

1
N 2

+ O

(cid:19)

(cid:18)
dk, dk′
i ≤
h
3
2 =: ∆2
dkk

E

S

k

−

S

2
F

k

h

≤

i

3ν
N

max

∆2

1L4, ¯c2

0L2

+ O

(cid:2)

(cid:3)

1
N 2

.

(cid:18)

(cid:19)

M
m=1 are conditionally independent given y in the DICA model (3), we have the following
= m′ and using the moments of the Poisson

b
D.4 Auxiliary expressions

{

xm}

As
expressions by using the law of total expectation for m
distribution with parameter ym:
E(xm) = E[E(xm|
m) = E[E(x2
E(x2
m|
m) = E[E(x3
E(x3
m|
m) = E[E(x4
E(x4
m|
E(xmxm′ ) = E[E(xmxm′
E(xmx2
m′ ) = E[E(xmx2
m′
|
E(x2
ym)E(x2
m′ ) = E[E(x2
mx2
m′
m|

ym)] = E(ym),
ym)] = E(y2
ym)] = E(y3
ym)] = E(y4

y)] = E[E(xm|
y)] = E[E(xm|
|

m) + E(ym),
m) + 3E(y2
m) + 6E(y3

ym′)] = E(y2

|

Moreover, the moments of αk ∼
c2
k + ck
E(αk) =
, E(α2
b2

k) =

ck
b

Gamma(ck, b) are

D.5 Analysis of the whitening and recovery error

m) + E(ym),
m) + 7E(y2

m) + E(ym),
ym′ )] = E(ymym′),
ym′ )] = E(ymy2

|

ym)E(xm′
ym)E(x2
m′
m′) + E(y2
my2

|

m′) + E(ymym′ ),

mym′) + E(ymy2

m′ ) + E(ymym′).

, E(α3

k) =

k + 3c2
c3
k + 2ck
b3

, E(α4

k) =

k + 6c3
c4

k + 11c2
b4

k + 6ck

,

etc.

We can follow a similar analysis as in Appendix C of [15] to derive the topic recovery error given the
sample estimate error. In particular, if we deﬁne the following sampling errors ES and ET :

ES,

k ≤

S

S

k

k

−
T (u)
b

−

T (u)

k ≤ k

u

k2 ET ,

then the following form of their Lemma C.2 holds for both the LDA moments and the GP/DICA
cumulants:

b

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

ν

k

−

(maxkγk)ES

ET

k ≤

"

σK

D

2 +

σK

D

,

3

#

(34)

c

b

c

c

18

(cid:0)

(cid:1)

e

(cid:0)

(cid:1)

e

where σk(
·
cases
the GP/DICA cumulants, γk takes the simpler form γk := cum(αk)/[var(αk)]3/2 = 2/√ck.

) denotes the k-th singular value of a matrix, ν is some universal constant, and in both
c0(c0+1))
D⊤. For the LDA moments, γk = 2
ck(c0+2)2 , whereas for

D was deﬁned such that S =

q

D

e

e

e

We note that the scaling for S is O(L2) for the GP/DICA cumulants, in contrast to O(1) for the LDA
moments. Thus, to compare the upper bound (34) for the two types of moments, we need to put it in
quantities which are common. In the ﬁrst section of the Appendix C of [15], it was mentioned that
c0(c0+1) σK(D) for the LDA moments, where cmin := mink ck. In contrast, for the
σK
GP/DICA cumulants, we can show that σK
c0 σK(D), where L := c0/b is the average
≥
length of a document in the GP model. Using this lower bound for the singular vector, we thus get
the following bound in the case of the GP cumulant:

L √cmin

cmin

q

D

D

≥

e

(cid:0)

(cid:1)

(cid:0)

(cid:1)

e

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

k

−

ν
c3/2
min "

ES
L2

2c2
0
D

σK

2 +

ET
L3

c3
0
[σK (D)]3

.

#

k ≤

(35)

b

c

c

c
The c3/2
min factor is common for both the LDA moment and GP cumulant, but as we mentioned
after Proposition 3.1, the sample error ES term gets divided by L2 for the GP cumulant, as ex-
pected.

(cid:1)(cid:3)

(cid:0)

(cid:2)

The recovery error bound in [15] is based on the bound (35), and thus by showing that the error
ES/L2 for the GP cumulant is lower than the ES term for the LDA moment, we expect to also
gain a similar gain for the recovery error, as the rest of the argument is the same for both types of
moments (see Appendix C.2, C.3 and C.4 in [15] for the completion).

E Appendix. The LDA moments

E.1 Our notation

The LDA moments were derived in [3]. Note that the full version of the paper with proofs appeared
in [15] and a later version of the paper also appeared in [31]. In this section, we recall the form of the
LDA moments using our notation. This section does not contain any novel results and is included
for the reader’s convenience. We also refer to this section when deriving the practical expressions
for computation of the sample estimates of the LDA moments in Appendix F.4.

≥

For deriving the LDA moments, a document is assumed to be composed of at least three tokens:
3. As the LDA generative model (1) is only deﬁned conditional on the length L, this is not
L
too problematic. But given that we present models in this paper which also model L, we mention
for clarity that we can suppose that all expectations and probabilities deﬁned below are implicitly
3.11 The theoretical LDA moments are derived only using the ﬁrst three words
conditioning on L
w1, w2 and w3 of a document. But note that since the words wℓ’s are conditionally i.i.d. given θ (for
w3) = E(wℓ1 ⊗
wℓ3 ) for any three distinct tokens
1
ℓ1, ℓ2 and ℓ3. The tensor M3 is thus symmetric, and could have been deﬁned using any distinct
ℓ1, ℓ2 and ℓ3 that are less than L. To highlight this arbitrary choice and to make the links with the
U-statistics estimator presented later, we thus use generic distinct ℓ1, ℓ2 and ℓ3 in the deﬁnition of
the LDA moments below, instead of ℓ1 = 1, ℓ2 = 2 and ℓ3 = 1 as in [3].

L), we have M3 := E(w1 ⊗

wℓ2 ⊗

w2 ⊗

≤

≥

≤

ℓ

11Note that another advantage of the DICA cumulants from Section 3.1 is that they do not require such a
somewhat artiﬁcial condition: they are well-deﬁned for any document length (even a document of length zero!).

19

(36)

(37)

(39)

(40)

(41)

(42)

Using this notation, then by the law of total expectation and the properties of the Dirichlet distribu-
tion, the non-central moments12 of the LDA model (1) take the form [3]:

,

M1 = E(wℓ1 ) = D

c
c0
M2 = E(wℓ1 w⊤ℓ2 ) =
M3 = E(wℓ1 ⊗
c0
c0 + 2

wℓ2 ⊗
[E(wℓ1 ⊗
2c3
0
c0(c0 + 1)(c0 + 2)

−

=

c0
c0 + 1
wℓ3)
wℓ2 ⊗

where

denotes the tensor product.

⊗

M1M ⊤1 +

1
c0(c0 + 1)

Ddiag (c) D⊤,

M1) + E(wℓ1 ⊗

M1 ⊗

M1 ⊗

M1 +

M1 ⊗

wℓ3 ) + E(M1 ⊗
2
c0(c0 + 1)(c0 + 2)

K

Xk=1

wℓ2 ⊗

wℓ3 )] ,

ckdk ⊗

dk ⊗

dk.

(38)

Similarly to the GP/DICA cumulants (as discussed in Appendix C.3), moving the terms in the non-
central moments (36), (37), (38), the following quantities are deﬁned

(P airs) = S := M2 −

c0
c0 + 1

M1M ⊤1 ,

LDA S-moment

(T riples) = T := M3 −

[E(wℓ1 ⊗

wℓ2 ⊗

M1) + E(wℓ1 ⊗

M1 ⊗

wℓ3 ) + E(M1 ⊗

wℓ2 ⊗

wℓ3 )]

M1 ⊗

M1 ⊗

M1.

LDA T-moment

c0
c0 + 2
2c2
0
(c0 + 1)(c0 + 2)

+

Slightly abusing terminology, we refer to the entities S and T as the “LDA moments”. They have
the following diagonal structure

S =

1
c0(c0 + 1)

ckdkd⊤k ,

K

Xk=1

T =

2
c0(c0 + 1)(c0 + 2)

ckdk ⊗

dk ⊗

dk.

K

Xk=1

Note however that this form of the LDA moments has a slightly different nature than the similar
form (11) and (13) of the GP/DICA cumulants. Indeed, the former is the result of properties of the
Dirichlet distribution, while the latter is the result of the independence of α’s. However, one can
think of the elements of a Dirichlet random vector as being almost independent (as, e.g., a Dirichlet
random vector can be obtained from independent gamma variables through dividing each by their
sum). Also, this closeness of the structures of the LDA moments and the GP cumulants can be
explained by the closeness of the respective models as discussed in Section 2.

E.2 Asymptotically unbiased ﬁnite sample estimators for the LDA moments

Given realizations wnℓ, n = 1, . . . , N , ℓ = 1, . . . , Ln, of the token random variable wℓ, we now
give the expressions for the ﬁnite sample estimates of S (39) and T (40) for the LDA model (and
E below to express
we rewrite them as a function of the sample counts xn).13 We use the notation
a U-statistics empirical expectation over the token within a documents, uniformly averaged over the
whole corpus. For example,
wℓ2 ⊗

M1) := 1
N

wℓ2 ⊗

wℓ1 ⊗

1
Ln(Ln

Ln
ℓ1=1

N
n=1

b

1)

−

Ln
ℓ2=1
=ℓ1
ℓ2

12Note, the difference in the notation for the LDA moments in papers [3] and [4]. In [3], M1 = E(wℓ1 ),
M2 = E(wℓ1 ⊗ wℓ2 ), and M3 = E(wℓ1 ⊗ wℓ2 ⊗ wℓ3 ). However, in [4], M2 is equivalent to S in our notation
and to P airs in the notation of [3]; similarly, M3 is T in our notation or T riples in the notation of [3].

c

13Note that because non-linear functions of

is biased, i.e., E(
unbiased. This is in contrast with the estimator for the GP/DICA moments which is easily made unbiased.

S) 6= S. The bias is small though: kE(
b

S (43) and
T (44), the estimator
b
b
S)−Sk = O(1/N ) and the estimator is asymptotically
b

M1 appear in the expression for
c

P

P

P

E(wℓ1 ⊗
b

20

M1.

c

S :=

T :=
b

b

+

M1

c0
M ⊤1 ,
M2 −
c0 + 1
c0
E(wℓ1 ⊗
M3 −
c
c
c
c0 + 2
2c2
b
c
0
M1 ⊗
(c0 + 1)(c0 + 2)
c

h

wℓ2 ⊗

M1) +

E(wℓ1 ⊗
b

M1 ⊗
c

wℓ3) +

E(

M1 ⊗
c

b

wℓ2 ⊗

wℓ3 )
i

(44)

c
where, as suggested in [4], unbiased U-statistics estimates of M1, M2 and M3 are:

M1 :=

E(wℓ) =

wnℓ =

[δ1]nxn =

Xδ1,

(45)

N

Ln

N

1
N

wnℓ1 w⊤nℓ2

Xℓ=1

1
Ln(Ln −

1)

n=1
X
Ln

Ln

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

c
M1,

M1 ⊗
c

1
N

1
Ln

N

n=1
X

n=1
X
1
N

c
M2 :=

b
E(wℓ1 w⊤ℓ2 ) =

c

b
1
N

1
N

1
N

=

=

=

N

n=1
X
N

(cid:2)

[δ2]n

xnx⊤n −

 

wnℓw⊤nℓ

!

Ln

Xℓ=1

[δ2]n

xnx⊤n −
n=1
X
(cid:0)
Xdiag(δ2)X ⊤

−

diag(xn)

(cid:1)
diag(Xδ2)

,

1
N

(cid:3)

(43)

(46)

(47)

δ3n

Ln

Xℓ=1

M

m=1
X
em1 ⊗

M3 :=

c

E(wℓ1 ⊗
b

wℓ2 ⊗

wℓ3 ) =

1
N

N

Ln

Ln

Ln

n=1
X

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

Xℓ3=1
=ℓ2
ℓ3
=ℓ1
ℓ3

wnℓ1 ⊗

wnℓ2 ⊗

wnℓ3

[δ3]n

xn ⊗

xn ⊗

xn −

 

wnℓ ⊗

wnℓ ⊗

wnℓ

=

1
N

N

n=1
X

Ln

Ln

Xℓ2=1
=ℓ1
ℓ2
N

−

Xℓ1=1

=

1
N

n=1
X

M

M

−

m1=1
X

m2=1
X

(wnℓ1 ⊗

wnℓ1 ⊗

wnℓ2 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ1 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ2 )

[δ3]n

xn ⊗

xn ⊗

 

xn + 2

xnm(em ⊗

em ⊗

em)

xnm1 xnm2 (em1 ⊗

em2 + em1 ⊗

em2 ⊗

em1 + em1 ⊗

em2 ⊗

em2)

.





!
(48)

Here, the vectors δ1, δ2 and δ3 ∈
2!
[δ2]n =

Ln
2

−

1

RN are deﬁned as [δ1]n := L−

1

n ; [δ2]n := (Ln(Ln −

1))−

1, i.e.,

is the number of times to choose an ordered pair of tokens out of Ln tokens;

(cid:1)

i
h(cid:0)
is the number of times to choose an
[δ3]n := (Ln(Ln −
ordered triple of tokens out of Ln tokens. Note that the vectors δ1, δ2, and δ3 have nothing to do
with the Kronecker delta δ.

1, i.e., [δ3]n =

1)(Ln −

2))−

Ln
3

h(cid:0)

3!

i

(cid:1)

−

1

For a vector a
matrix A

∈
RM
×

RN , we sometimes use notation [a]n to denote its n-th element. Similarly, for a
N we use notation [A]mn to denote its (m, n)-th element.

∈

21

∈

c

There is a slight abuse of notation in the expressions above as wℓ is sometimes treated as a random
E(wℓ1 w⊤ℓ2), etc.) and sometimes as its realization. However, the difference
E(wℓ),
variable (i.e., in
is clear from the context.
b

b

F Appendix. Practical aspects and implementation details

F.1 Whitening of S and dimensionality reduction

The algorithms from Section 4 require the computation of a whitening matrix W of S. Due to
the similar diagonal structure ((41) and (11)) of the matrix S for both the LDA moments (39) and
the GP/DICA cumulants (10), the computation of a whitening matrix is exactly the same in both
cases.

By a whitening matrix, we mean a matrix W
whiten S

RK
M , but also reduces its dimensionality such that14 W SW ⊤ = IK.

M (in practice, M

RM

≫

∈

×

×

K) that does not only

Let S = U ΣU ⊤ be an orthogonal eigendecomposition of the symmetric matrix S. Let Σ1:K denotes
the diagonal matrix that contains the largest K eigenvalues15 of S on its diagonal and let U1:K be a
matrix with the respective eigenvalues in its columns. Then, a whitening matrix is

(49)

(50)

W = Σ†

1/2
1:K U ⊤1:K,

b
1/2
1:K

W :=

Σ†

U ⊤1:K,

c

b

b

1/2
1:K is a diagonal matrix constructed from Σ1:K by taking the inverse and the square root

where Σ†
of its non-zero diagonal values († stands for the pseudo-inverse).

In practice, when only a ﬁnite sample estimator
W of W can be introduced
estimator

S of S is available, the following ﬁnite sample

where

S =

U

Σ

U ⊤.

b

b

b

b

F.2 Computation of the ﬁnite sample estimators of the GP/DICA cumulants

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see
Appendix C.4 for the deﬁnition of
W ⊤ for the GP/DICA models. The construction
S (29) is
of the ﬁnite sample estimator
straightforward.

W is discussed in Appendix F.1, while the computation of

T ) of

T (v)

W

c

b

c

b

c

b

14Note that such a whitening matrix W ∈ RK×M is not uniquely deﬁned as left multiplication by any orthog-
W ⊤ = V W SW ⊤V ⊤ =
f

onal matrix V ∈ RK×K does not change anything. Indeed, let
IK.

W = V W , then
f

W S
f

15We mean the largest non-negative eigenvalues. In theory, S have to be PSD. In practice, when we deal with
ﬁnite number of samples, respective estimate of S can have negative eigenvalues. However, for K sufﬁciently
small, S should have enough positive eigenvalues. Moreover, it is standard practice to use eigenvalues of S for
estimation of a good value of K, e.g., by thresholding all negative and close to zero eigenvalues.

22

By plugging the deﬁnition of the tensor
RM :
a vector, we obtain for a given v

∈

b

T (30) in the formula (16) for the projection of a tensor onto

T (v)

=

cum(xm1 , xm2 , xm3)vm3 + 2

δ(m1, m2, m3)

E(xm3 )vm3

m1m2

i

h

b

m3
X

b

d
δ(m2, m3)

cov(xm1 , xm2 )vm3

δ(m1, m3)

d
cov(xm1 , xm2 )vm3

δ(m1, m2)

d
cov(xm1 , xm3 )vm3

m3
X

m3
X

m3
X

m3
X

−

−

−

=

−

=

W k1⊤

b
c
Wk2
T (v)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

cum(xm1 , xm2 , xm3)vm3 + 2δ(m1, m2)

d

E(xm1 )vm1

m3
X
cov(xm1 , xm2 )vm2 −

d

cov(xm1 , xm2 )vm1 −

b
δ(m1, m2)

cov(xm1 , xm3 )vm3 .

m3
X

d

d
This gives the following for the expression

d

W

T (v)

W ⊤:

cum(xm1 , xm2 , xm3 )vm3

Wk1m1

Wk2m2

c

d
δ(m1, m2)

E(xm1 )vm1

c
Wk1m1

c
Wk2m2

=

b
c
m1,m2,m3
X

+ 2

m1,m2
X

−

−

−

m1,m2
X

m1,m2
X

m1,m3
X

cov(xm1 , xm2 )vm2

b

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm2 )vm1

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm3 )vm3

c
Wk1m1

c
Wk2m1 ,

d

c

c

where
Wk denotes the k-th row of
for the unbiased ﬁnite sample estimates of

cov and

cum, we further get

W as a column vector. By further plugging in the expressions (31)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

=

(N

+ 2

c

N
1)(N

2)

−
−
E(xm)vm

n D
X
Wk1m

E(x)

d

d
Wk1 , xn −
c
Wk2m

b

E(x)

Wk2 , xn −
c

b

E D

E D

v, xn −

E(x)
E

b

m
X
1

−

N

−

N

−

N

−
1

−
1

−

b
1

1

1

E D
E(x)

E(x)

c
c
Wk1 , xn −
c
v
◦

b
Wk1 , xn −
c
Wk1 ◦
c

b
Wk2 , xn −
c

b

n D
X

n D
X

n D
X

v

◦

E D
E(x)

Wk2 , xn −
c
Wk2 , xn −
c

v, xn −

E D

E(x)
E
b
E(x)
E
b
E(x)
E

b

,

N
where
denotes the elementwise Hadamard product. Introducing the counts matrix X
where each element Xmn is the count of the m-th word in the n-th document (note, the matrix X

∈

◦

×

RM

23

contain the vector xn in the n-th column), we further simplify the above expression

W

T (v)

W ⊤ =

W X)diag[X ⊤v](
(

W X)⊤

c

b

c

c
v,

E(x)

2N (

W

E(x))(
c

W

E(x))⊤

W X)(
(

W X)⊤

D
W X(X ⊤v)(

E h

b

W

E(x))⊤ +
c

b

c

b
E(x)(
W

c
W X(X ⊤v))⊤

c

i

−

c

b

c

b

c

W X)(
(
b

c

W diag(v)X)⊤ + (

W diag(v)X)(

W X)⊤ +

W diag[X(X ⊤v)]

W ⊤

E(x))(
c

W diag[v]

E(x))⊤ + (

c

W diag[v]

E(x))(
c

W

E(x))⊤
c

c
W diag[

E(x)]
b

W ⊤.

c

b

c

b

i

i

N
1)(N
N
1)(N
N
1)(N

◦

h
c
W
(

(N

+

(N

−

−

2)

−

2)

−

−

(N

−
W diag[v

+ 2

2)
−
h
E(x)]
W ⊤
c

−

+

+

1
c
N
−
N

N

N

−
N

−

1

1

1

h

D

c
v,

E(x)
b
E

A more compact way to write down expression (51) is as follows

b

c

b

c

W

T (v)

W ⊤ =

c

b

c

+

N
1)(N

2)

T1 +
−
h
T5 + T ⊤5 −
h

T6 −

(N
1

−

N

1

−

v,
h

E(x)
i

(T2 −

T3)

−

(T4 + T ⊤4 )
i

b
T ⊤6 +

W diag(a)

W ⊤

,

c

i

c

where

i

c

(51)

(52)

W X)⊤,

T1 = (

W X)diag[X ⊤v](
E(x))(

E(x))⊤,
c

b
E(x))⊤,

W

W

T4 =

T3 = (

T2 = 2N (
c
W X)⊤,
W X)(
b
c
c
W
W X(X ⊤v)(
c
W X)(
c
W diag(v)
c
c
1)[v
a = 2(N
c

T5 = (

T6 = (

◦
b

−

c
W diag(v)X)⊤,
b
c
E(x))(

W
E(x)] +
c
b

E(x))⊤,

F.3 Computational complexity of the GP/DICA T-cumulant estimator (52)

b

b

b

v,
h

E(x)
E(x)
i

−

X(X ⊤v).

When computing the T-cumulant P times with the formula above, the following terms are dominant:
O(RN K)+ O(N K 2)+ O(M K), where R is the largest number of unique words (non-zero counts)
in a document over the corpus. In practice, almost always K < M < N , which gives the overall
complexity of P computations of the estimator (52) to be equal to O(P RN K)+O(P N K 2).

F.4 Computation of the ﬁnite sample estimators of the LDA moments

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see Ap-
W ⊤ for the LDA model. Note that the construction of
pendix E.2 for the deﬁnition of
the sample estimator
W of a whitening matrix W is discussed in Appendix F.1). The computation of
S (43) is straightforward. This approach to efﬁcient implementation was discussed in [4], however,
to the best of our knowledge, the ﬁnal expressions were not explicitly stated before. All derivations
are straightforward, but quite tedious.
b

T ) of

T (v)

c

c

c

W

b

b

24

RM

M

×

×

M (44) onto some

By analogy with the GP/DICA case, a projection (16) of the tensor
vector v

T

∈

RM in the LDA is
M

∈

T (v)

=

m1m2

h

b

−

i
c0
c0 + 2

M

m3=1 h
X

m3=1 h
X
c
E(wℓ1 ⊗
b

M3

vm3 +

m1m2m3
i

2c2
0
(c0 + 1)(c0 + 2)

wℓ2 ⊗

M1) +

c

E(wℓ1 ⊗
b

M1 ⊗
c

Plugging in the expression (48) for an unbiased sample estimate

m3
X

wℓ3) +

E(

c
M1 ⊗
c

c
wℓ2 ⊗

c
wℓ3 )
i

b
M3 of M3, we get

b
M1]m1 [

[

M1]m2 [

M1]m3vm3

vm3.

m1m2m3

1
N

1
N

N

n=1
X
N

n=1
X

[δ3]n

M

M



m3=1
X

i,j=1
X


M1]m1[
[

2c2
0
(c0 + 1)(c0 + 2)

−

+

T (v)

=

m1m2

i

h

b

[δ3]n

xnm1 xnm2 h
 

xn, v

+ 2

i

δ(m1, m2, m3)xnm3 vm3

c

!

m3
X

xnixnj (ei ⊗

ei ⊗

ej + ei ⊗

ej ⊗

ei + ei ⊗

ej ⊗

ej)

vm3





m1m2m3

M1]m2

M1, v

D
M
c

E

c0
c0 + 2  

c
M2]m1m2

c
M1, v

[

−

[
m3=1 (cid:16)
X
where e1, e2, . . . , eM denote the canonical vectors of RM (i.e., the columns of the identity matrix
IM ). Further, this gives the following for the expression

M1]m2vm3 + [

M2]m1m3[

M2]m2m3[

W ⊤:

T (v)

c

c

c

c

c

c

W

+

E

D

M1]m1 vm3

,

!

(cid:17)

W

T (v)

W ⊤

=

[δ3]n

xn, v

xn,

Wk1

b
c
xn,

c
Wk2

+ 2

xnmvm

Wk1m

Wk2m

 h
M

i

D

δ3n

xnixnj

E D

c
Wk1i

c
Wk2ivj +

M

E

m=1
X

Wk1i

Wk2jvi +

Wk1i

h

c

b

c

k1k2
i

i,j=1
X
Wk1 ,

M2
h

(cid:16)
Wk2

c

(cid:16)D
2c2
0
(c0 + 1)(c0 + 2)

c

c

+

Wk1 ,

c
M2v

c
M1

Wk2

c
+

c
Wk2 ,

M1,

Wk1

i
M1,

c

E
Wk1

c

D
c
M1,

c
Wk2

E D
c
M1, v

c
,

E

D

E D

E(cid:17)

c

c

c

c

where

Wk denotes the k-th row of

c
W as a column-vector. This further simpliﬁes to

c

c

c

c

D

E D

E D

E

N

n=1
X
N

1
N

1
N

n=1
X
c0
c0 + 2

−

−

+

!

c

c
Wk2jvj

(cid:17)
M2v

1
N
1
N
1
N
1
N

−

−

−

+

W

T (v)
c

W ⊤ =

W X)diag
(

(X ⊤v)
c

◦

δ3

W X)⊤
(

c

b

c

+

W diag
c

(cid:2)
2[(Xδ3)

v]

◦

−

(cid:3)
X[(X ⊤v)
c

δ3]

W ⊤

◦

(cid:2)

W diag[v]X)diag[δ3](
(
c

W X)⊤

(cid:3)

c

W X)diag[δ3](
(
c

W diag[v]X)⊤
c

c0
c
c0 + 2

hD
2c2
0
(c0 + 1)(c0 + 2)

c

E

M1, v

W
(
c

M2

W ⊤) + (

W (

M2v))(

W

M1)⊤ + (

W

M1)(

W (

M2v))⊤

c
c
M1, v

c
W
(

c
M1)(

W

c
M1)⊤.

c

c

c

c

c

c

i

(53)

A more compact representation gives:

D

c

E

c

c

c

c

W

T (v)

W ⊤ =

c

b

c

+

1
N

T3 −

T1 + T2 −
(cid:2)
2c2
0
(c0 + 1)(c0 + 2) h

T ⊤3

−

(cid:3)
W
(
i

M1, v

M1)(

c0
c0 + 2

W
(
i

M1, v
h
h
c
M1)⊤,
W

c

c

c

M2

W ⊤) + T4 + T ⊤4

i

(54)

c

c

c
25

c

c

where

◦
v]

T1 = (

W X)diag

(X ⊤v)

δ3

W X)⊤,
(

T2 =

T3 = [

T4 = [

◦

2[(Xδ3)
(cid:2)

W diag
c
−
W diag(v)X]diag(δ3)(
c
W (
c

M2v)](

M1)⊤.

W

(cid:2)

c

X[(X ⊤v)
(cid:3)
c
W X)⊤,

◦

δ3]

W ⊤,

(cid:3)

c

F.5 Computational complexity of the LDA T-moment estimator (54)

c

c

c

c

By analogy with Appendix F.3, the computational complexity of the T-moment is O(RN K) +
O(N K 2). However, in practice we noticed that the computation of (52) is slightly faster for larger
datasets than the computation of (54) (although the code for both was equally well optimized). This
means that the constants in O(RN K) + O(N K 2) for the LDA T-moment are, probably, slightly
larger than for the GP/DICA T-cumulant.

F.6 Estimation of the model parameters for GP/DICA model

×

∈

Below we brieﬂy discuss the recovery of the model parameters for the GP/DICA and LDA mod-
RK
M estimated in Algorithm 1. This matrix has
els from a joint diagonalization matrix A
the property that AD should be approximately diagonal up to a permutation of the columns of D.
The standard approach [3] of taking the pseudo-inverse of A to get an estimate of the topic ma-
trix D has a problem that it does not preserve the simplex constraint of the topics (in particular,
the non-negativity of
D). Due to the space constraints, we do not discuss this issue here, but we
observed experimentally that this can potentially signiﬁcantly deteriorate performance of all mo-
ment matching algorithms for topic models considered in this paper. We made an attempt to solve
this problem by integrating the non-negativity constraint into the Jacobi-updates procedure of the
orthogonal joint diagonalization algorithm, but the obtained results did not lead to any signiﬁcant
improvement. Therefore, in our experiments for both GP/DICA cumulants and LDA moments, we
estimate the topic matrix by thresholding the negative values of the pseudo-inverse of A:

e

dk := τk max(0, [A†]:k)/

max(0, [A†]:k)

k

k1,

where [A†]:k is the k-th column of the pseudo-inverse A† of A, and τk = ±1 set to
1 if [A†]:k has
more negative than positive values. This might not be the best option, and we leave this issue for the
future research.

−

b

To estimate the parameters for the prior distribution over the topic intensities αk for the DICA
model (4), we use the diagonalized form of the projected tensor from (17) and relate it to the output
diagonal elements ap for the p-th projection:

[ap]k =

zk, upi

=

zk, upi
h

=

cum(αk, αk, αk)
[var(αk)]3/2

tk
s3/2
k

tkh
e

τk

dk, W ⊤up

,

(55)

D

e

E

dk = τk max(0, [A†]:k). This formula is valid for any prior on αk in the DICA model. For the
b2 and cum(αk, αk, αk) = 2ck
b3 ,
tk in (55), and solving

√ck , which enables us to estimate ck. Plugging this value of

where
GP model (3) where αk ∼
tk = 2
e
and thus
for ck gives the following expression:

Gamma(ck, b), we have that var(αk) = ck

e

e

By replacing the quantities on the RHS with their estimated ones, we get one estimate for ck per
projection. We use as our ﬁnal estimate the average estimate over the projections:

(56)

ck =

2

.

4

dk, W ⊤up
[ap]2
k

D

E

e

1
P

P

4

dk,

W ⊤up
[ap]2
k
c

D

e

p=1
X

2

.

E

ck :=

b

26

Reusing the properties of the length of documents for the GP model as described in Appendix B.2,
we ﬁnally use the following estimates for rate parameter b of the gamma distribution:

b :=

,

c0
L
b
b

(57)

where

c0 :=

ck and

k

L is the average document length in the corpus.

b

By analogy, similar formulas for the estimation of the Dirichlet parameter c of the LDA model can
be derived and are a straightforward extension of the expression in [3].

P

b

b

b

G Appendix. Complexity of algorithms and details on the experiments

G.1 Code and complexity

Our (mostly Matlab) implementations of the diagonalization algorithms (JD, Spec, and TPM) for
both the GP/DICA cumulants and LDA moments are available online.16 Moreover, all datasets and
the code for reproducing our experiments are available.17 To our knowledge, no efﬁcient implemen-
tation of these algorithms was available for LDA. Each experiment was run in a single thread.

The bottleneck for the spectral, JD, and TPM algorithms is the computation of the cumu-
lants/moments. However, the expressions (52) and (54) provide efﬁcient formulas for fast com-
putation of the GP/DICA cumulants and LDA moments (O(RN K + N K 2), where R is the largest
number of non-zeros in the count vector x over all documents, see Appendix F.3 and F.5), which
makes even the Matlab implementation fast for large datasets. Since all diagonalization algorithms
(spectral, JD, TPM) perform the whitening step once, it is sufﬁcient to compare their complexities
by the number of times the cumulants/moments are computed.

Spectral.
O(N K(R + K)) complexity and, therefore, is the fastest.

The spectral algorithm estimates the cumulants/moments only once leading to

JD. For JD, rather than estimating P cumulants/moments separately, one can jointly estimate these
values by precomputing and reusing some terms (e.g., W X). However, the complexity is still
O(P N K(R + K)), although in practice it is sufﬁcient to have P = K or even smaller.

TPM. For TPM some parts of the cumulants/moments can also be precomputed, but as TPM nor-
mally does many more iterations than P , it can be signiﬁcantly slower. In general, the complexity
of TPM can be signiﬁcantly inﬂuenced by the initialization of the parameters of the algorithm.
There are two main parameters: Ltpm is the number of random restarts within one deﬂation step
and Ntpm is the maximum number of iterations for each of Ltpm random restarts (different from
N and L). Some restarts converge very fast (in much less than Ntpm iterations), while others are
slow. Moreover, as follows from theoretical results [4] and, as we observed in practice, the restarts
which converge to a good solution converge fast, while slow restarts, normally, converge to a worse
solution. Nevertheless, in the worst case, the complexity is O(NtpmLtpmN K(R + K)).

Note that for the experiment in Figure 1, Ltpm = 10 and Ntpm = 100 and the run with the best
objective is chosen. We believe that these values are reasonable in a sense that they provide a good
5 for the norm of the difference of the vectors from the previous and the
accuracy solution (ε = 10−
current iteration) in a little number of iterations, however, they may not be the best ones.

JD implementation. For the orthogonal joint diagonalization algorithm, we implemented a faster
C++ version of the previous Matlab implementation18 by J.-F. Cardoso. Moreover, the orthogonal
joint diagonalization routine can be initialized in different ways: (a) with the K
K identity matrix
or (b) with a random orthogonal K
K matrix. We tried different options and in nearly all cases
the algorithm converged to the same solution, implying that initialization with the identity matrix is
sufﬁcient.

×

×

Whitening matrix. For the large vocabulary size M , computation of a whitening matrix can be
expensive (in terms of both memory and time). One possible solution would be to reduce the vo-
cabulary size with, e.g., TF-IDF score, which is a standard practice in the topic modeling context.

16https://github.com/anastasia-podosinnikova/dica-light
17https://github.com/anastasia-podosinnikova/dica
18http://perso.telecom-paristech.fr/˜cardoso/Algo/Joint_Diag/joint_diag_r.m

27

min
148
JD-GP
252
JD-LDA
JD(k)-GP
157
JD(k)-LDA 264
JD(f)-GP
1628
JD(f)-LDA 2545
Spec-GP
Spec-LDA
TPM-GP
TPM-LDA 12723

101
107
1734

mean max
247
192
366
284
247
190
318
290
2058
1846
2806
2649
111
107
193
140
2726
2393
19356
16460

Table 1: The running times in seconds of the algorithms from Figure 1, corresponds to the case when N =
50, 000. Each algorithm was run 5 times, so the times in the table display the minimum (min), mean, and
maximum (max) time.

Another option is using a stochastic eigendecomposition (see, e.g., [33]) to approximate the whiten-
ing matrix.

Variational inference. For variational inference, we used the code of D. Blei and modiﬁed it for the
estimation of a non-symmetric Dirichlet prior c, which is known to be important [35]. The default
values of the tolerance/maximum number of iterations parameters are used for variational inference.
The computational complexity of one iteration for one document of the variational inference algo-
rithm is O(RK), where R is the number of non-zeros in the count vector for this document, which
is then performed a signiﬁcant number of times for each document.

G.2 Runtimes of the algorithms

In Table 1, we present the running times of the algorithms from Section 5.1. JD and JD(k) are
signiﬁcantly faster than JD(f) as expected, although the performance in terms of the ℓ1-error is
nearly the same for all of them. This indicates that preference should be given to the JD or JD(k)
algorithms.

The running time of all LDA-algorithms is higher than the one of the GP/DICA-algorithms. This
indicates that the computational complexity of the LDA-moments is slightly higher than the one
of the GP/DICA-cumulants (compare, e.g., the times for the spectral algorithm which almost com-
pletely consist of the computation of the moments/cumulants). Moreover, the runtime of TPM-LDA
is signiﬁcantly higher (half an hour vs. several hours) than the one of TPM-GP/DICA. This can be
explained by the fact that the LDA-moments have more noise than the GP/DICA-cumulants and,
hence, the algorithm is slower. Interestingly, all versions of JD algorithm are not that sensitive to
noise.

Computation of a whitening matrix is roughly 30 sec (this time is the same for all algorithms and is
included in the numbers above).

G.3 Initialization of the parameter c0 for the LDA moments

The construction of the LDA moments requires the parameter c0, which is not trivial to set in the
unsupervised setting of topic modeling, especially taking into account the complexity of the evalu-
ation for topic models [16]. For the semi-synthetic experiments, the true value of c0 is provided to
the algorithms. It means that the LDA moments, in this case, have access to some oracle informa-
tion, which in practice is never available. For real data experiments, c0 is set to the value obtained
with variational inference. The experiments in Appendix G.4 show that this choice was somewhat
important. However, this requires more thorough investigation.

G.4 The LDA moments vs parameter c0

In this section, we experimentally investigate dependence of the LDA moments on the parameter c0.
In Figure 5, the joint diagonalization algorithm with the LDA moment is compared for different
values of c0 provided to the algorithm. The data is generated similarly to Figure 2. The experiment
indicates that the LDA moments are somewhat sensitive to the choice of c0. For example, the

28

0
0.01

0.1 

1   
Parameter c0

10  

100 

0
0.01

0.1 

1   
Parameter c0

10  

100 

Figure 5: Performance of the LDA moments depending on the parameter c0. D and c are learned from the
AP dataset for K = 10 and K = 50 and true c0 = 1. JD-GP(10) for K = 10 and JD-GP(50) for K = 50.
Number of sampled documents N = 20, 000. For the error bars, each dataset is resampled 5 times. Data (left):
GP sampling; (right): LDAﬁx(200) sampling. Note: a smaller value of the ℓ1-error is better.

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

JD-GP(10)

JD-LDA(10)

JD-GP(90)

JD-LDA(90)

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
2

ℓ

0
1 

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

0
1 

20

10
Number of docs in 1000s

30

40

50

20

10
Number of docs in 1000s

40

30

50

Figure 6: Comparison of the ℓ1- and ℓ2- errors on the NIPS semi-synthetic dataset as in Figure 2 (top, left).
The ℓ2 norms of the topics were normalized to [0,1] for the computation of the ℓ2 error.

recovery ℓ1-error doubles when moving from the correct choice c0 = 1 to the plausible alternative
c0 = 0.1 for K = 10 on the LDAﬁx(200) dataset (JD-LDA(10) line on the right of Figure 5).

G.5 Comparison of the ℓ1- and ℓ2-errors

The sample complexity results [3] for the spectral algorithm for the LDA moments allow straightfor-
ward extension to the GP/DICA cumulants, if the results from Proposition 3.1 are taken into account.
The analysis is, however, in terms of the ℓ2-norm. Therefore, in Figure 6, we provide experimental
comparison of the ℓ1- and ℓ2-errors to verify that they are indeed behaving similarly.

G.6 Evaluation of the real data experiments

For the evaluation of topic recovery in the real data case, we use an approximation of the log-
likelihood for held out documents as the metric. The approximation is computed using a Chib-style
method as described by [16] using the implementation by the authors.19 Importantly, this evaluation
methods is applicable for both the LDA model as well as the GP model. Indeed, as it follows from
Section 2 and Appendix B.1, the GP model is equivalent to the LDA model when conditioning on
the length of a document L (with the same ck hyper parameters), while the LDA model does not
make any assumption on the document length. For the test log-likelihood comparison, we thus treat
the GP model as a LDA model (we do not include the likelihood of the document length).

G.7 More on the real data experiments

The detailed experimental setup is as follows. Each dataset is separated into 5 training/evaluation
pairs, where the documents for evaluation are chosen randomly and non-repetitively among the
folds (600 documents are held out for KOS; 400 documents are held out for AP; 450 documents
are held out for NIPS). Then, the model parameters are learned for a different number of topics.
The evaluation of the held-out documents is performed with averaging over 5 folds. In Figure 3 and
Figure 7, on the y-axis, the predictive log-likelihood in bits averaged per token is presented.

19http://homepages.inf.ed.ac.uk/imurray2/pub/09etm

29

In addition to the experiments with AP and KOS in Figure 3, we demonstrate one more experiment
with the NIPS dataset in Figure 7 (right).

Note that, as the LDA moments require at least 3 tokens in each document, 1 document from the
NIPS dataset and 3 documents from the AP dataset, which did not fulﬁll this requirement, were
removed.

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11

-12

-13

-14

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 7: Experiments with real data. Left: the KOS dataset. Right: the NIPS dataset. Note: a higher value of
the log-likelihood is better.

Importantly, we observed that VI when initialized with the output of the JD-GP is consistently better
in terms of the predictive log-likelihood. Therefore, the new algorithm can be used for more clever
initialization of other LDA/GP inference methods.

We also observe that the joint diagonalization algorithm for the LDA moments is worse than the
spectral algorithm. This indicates that the diagonal structure (41) and (42) might not be present in the
sample estimates (43) and (44) due to either model misspeciﬁcation or to ﬁnite sample complexity
issues.

Supplementary References

[31] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for

latent Dirichlet allocation. Algorithmica, 72(1):193–214, 2015.

[32] B.A. Frigyik, A. Kapila, and M.R. Gupta. Introduction to the Dirichlet distribution and related

processes. Technical report, University of Washington, 2010.

[33] N. Halko, P.-G. Martinsson, and J.A. Tropp. Finding structure with randomness: probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288,
2011.

[34] T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):455–

[35] H.M. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: why priors matter. In NIPS,

500, 2009.

2009.

30

5
1
0
2
 
v
o
N
5

 

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
8
7
1
0
.
7
0
5
1
:
v
i
X
r
a

Rethinking LDA: Moment Matching for Discrete ICA

Anastasia Podosinnikova

Francis Bach

Simon Lacoste-Julien

INRIA - ´Ecole normale sup´erieure Paris

Abstract

We consider moment matching techniques for estimation in latent Dirichlet allo-
cation (LDA). By drawing explicit links between LDA and discrete versions of
independent component analysis (ICA), we ﬁrst derive a new set of cumulant-
based tensors, with an improved sample complexity. Moreover, we reuse standard
ICA techniques such as joint diagonalization of tensors to improve over existing
methods based on the tensor power method. In an extensive set of experiments on
both synthetic and real datasets, we show that our new combination of tensors and
orthogonal joint diagonalization techniques outperforms existing moment match-
ing methods.

1 Introduction

Topic models have emerged as ﬂexible and important tools for the modelisation of text corpora.
While early work has focused on graphical-model approximate inference techniques such as varia-
tional inference [1] or Gibbs sampling [2], tensor-based moment matching techniques have recently
emerged as strong competitors due to their computational speed and theoretical guarantees [3, 4].
In this paper, we draw explicit links with the independent component analysis (ICA) literature
(e.g., [5] and references therein) by showing a strong relationship between latent Dirichlet allocation
(LDA) [1] and ICA [6, 7, 8]. We can then reuse standard ICA techniques and results, and derive new
tensors with better sample complexity and new algorithms based on joint diagonalization.

2 Is LDA discrete PCA or discrete ICA?

0, 1

Notation. Following the text modeling terminology, we deﬁne a corpus X =
as a
x1, . . . , xN }
{
of Ln tokens. It is
collection of N documents. Each document is a collection
wn1, . . . , wnLn }
convenient to represent the ℓ-th token of the n-th document as a 1-of-M encoding with an indicator
M with only one non-zero, where M is the vocabulary size, and each document
vector wnℓ ∈ {
}
as the count vector xn :=
In such representation, the length Ln of the n-th
to refer to topics,the
document is Ln =
}
P
index n
to refer to words from
}
the vocabulary, and the index ℓ
to refer to tokens of the n-th document. The plate
∈ {
diagrams of the models from this section are presented in Appendix A.

RM .
m xnm. We will always use the index k
to refer to documents, the index m
1, . . . , N
}
P
1, . . . , Ln}

ℓ wnℓ ∈

∈ {
1, . . . , M

1, . . . , K

∈ {

∈ {

{

Latent Dirichlet allocation [1] is a generative probabilistic model for discrete data such as text
corpora. In accordance to this model, the n-th document is modeled as an admixture over the vo-
cabulary of M words with K latent topics. Speciﬁcally, the latent variable θn, which is sampled
from the Dirichlet distribution, represents the topic mixture proportion over K topics for the n-th
θn for the ℓ-th token is sampled from the multinomial dis-
document. Given θn, the topic choice znℓ|
znℓ, θn is then sampled from the multinomial
tribution with the probability vector θn. The token wnℓ|
distribution with the probability vector dznℓ, or dk if k is the index of the non-zero element in znℓ.
This vector dk is the k-th topic, that is a vector of probabilities over the words from the vocabulary
subject to the simplex constraint, i.e., dk ∈
.
}
This generative process of a document (the index n is omitted for simplicity) can be summarized as

∆M , where ∆M :=

m dm = 1

RM : d

0,

(cid:23)

∈

d

{

P

1

θ
θ
zℓ|
zℓ, θ

∼
∼
∼

wℓ|

Dirichlet(c),
Multinomial(1, θ),
Multinomial(1, dzℓ).

(1)

One can think of the latent variables zℓ as auxiliary variables which were introduced for convenience
of inference, but can in fact be marginalized out [9], which leads to the following model

θ
θ

x
|

∼
∼

Dirichlet(c),
Multinomial(L, Dθ),

LDA model (2)

RM

×

∈

K is the topic matrix with the k-th column equal to the k-th topic dk, and c

where D
++
is the vector of parameters for the Dirichlet distribution. While a document is represented as a set
of tokens wℓ in the formulation (1), the formulation (2) instead compactly represents a document as
the count vector x. Although the two representations are equivalent, we focus on the second one in
this paper and therefore refer to it as the LDA model.

∈

RK

Importantly, the LDA model does not model the length of documents. Indeed, although the original
paper [1] proposes to model the document length as L
Poisson(λ), this is never used in practice
and, in particular, the parameter λ is not learned. Therefore, in the way that the LDA model is
typically used, it does not provide a complete generative process of a document as there is no rule to
sample L
λ. In this paper, this fact is important, as we need to model the document length in order
to make the link with discrete ICA.

∼

λ

|

|

×

∈

RM

K is a transformation matrix and σ is a parameter.

Discrete PCA. The LDA model (2) can be seen as a discretization of principal component anal-
ysis (PCA) via replacement of the normal likelihood with the multinomial one and adjusting the
prior [9] in the following probabilistic PCA model [10, 11]: θ
Normal(Dθ, σ2IM ), where D
Discrete ICA (DICA). Interestingly, a small extension of the LDA model allows its interpreta-
tion as a discrete independent component analysis model. The extension naturally arises when the
document length for the LDA model is modeled as a random variable from the gamma-Poisson
mixture (which is equivalent to a negative binomial random variable), i.e., L
Poisson(λ) and
|
k ck is the shape parameter and b > 0 is the rate parameter. The
λ
LDA model (2) with such document length is equivalent (see Appendix B.1) to
αk ∼
α
∼

Gamma(ck, b),
Poisson([Dα]m),

Normal(0, IK ) and x
|

Gamma(c0, b), where c0 :=

GP model (3)

xm|

P

∼

∼

∼

∼

λ

θ

where all α1, α2, . . . , αK are mutually independent, the parameters ck coincide with the ones of the
LDA model in (2), and the free parameter b can be seen (see Appendix B.2) as a scaling parameter
for the document length when c0 is already prescribed.

This model was introduced by Canny [12] and later named as a discrete ICA model [13]. It is more
natural, however, to name model (3) as the gamma-Poisson (GP) model and the model

α1, . . . , αK ∼
xm|
∼

α

mutually independent,
Poisson([Dα]m)

DICA model (4)

as the discrete ICA (DICA) model. The only difference between (4) and the standard ICA model [6,
7, 8] (without additive noise) is the presence of the Poisson noise which enforces discrete, instead of
continuous, values of xm. Note also that (a) the discrete ICA model is a semi-parametric model that
can adapt to any distribution on the topic intensities αk and that (b) the GP model (3) is a particular
case of both the LDA model (2) and the DICA model (4).

Thanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to
derive new efﬁcient algorithms for topic modeling.

3 Moment matching for topic modeling

The method of moments estimates latent parameters of a probabilistic model by matching theoretical
expressions of its moments with their sample estimates. Recently [3, 4], the method of moments
was applied to different latent variable models including LDA, resulting in computationally fast

2

learning algorithms with theoretical guarantees. For LDA, they (a) construct LDA moments with a
particular diagonal structure and (b) develop algorithms for estimating the parameters of the model
by exploiting this diagonal structure. In this paper, we introduce the novel GP/DICA cumulants with
a similar to the LDA moments structure. This structure allows to reapply the algorithms of [3, 4]
for the estimation of the model parameters, with the same theoretical guarantees. We also consider
another algorithm applicable to both the LDA moments and the GP/DICA cumulants.

3.1 Cumulants of the GP and DICA models

In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is
a particular case of the DICA model, all results of this section extend to the GP model.

The ﬁrst three cumulant tensors for the random vector x can be deﬁned as follows

cum(x) := E(x),

cum(x, x) := cov(x, x) = E
E(x))

cum(x, x, x) := E [(x

(x

−
(x

E(x))(x
E(x))

−

(cid:2)
⊗

−

−

E(x))⊤
(x

,
E(x))] ,
(cid:3)

−

⊗

(5)

(6)

(7)

⊗

denotes the tensor product (see some properties of cumulants in Appendix C.1). The
where
essential property of the cumulants (which does not hold for the moments) that we use in this paper
is that the cumulant tensor for a random vector with independent components is diagonal.
Let y = Dα; then for the Poisson random variable xm|
E(xm|
expectation in (5) has the following form

Poisson(ym), the expectation is
ym) = ym. Hence, by the law of total expectation and the linearity of expectation, the

ym ∼

E(x) = E(E(x
|
ym) = ym and, as x1,
Further, the variance of the Poisson random variable xm is var(xm|
x2, . . . , xM are conditionally independent given y, then their covariance matrix is diagonal, i.e.,
y) = diag(y). Therefore, by the law of total covariance, the covariance in (6) has the form
cov(x, x
|

y)) = E(y) = DE(α).

(8)

cov(x, x) = E [cov(x, x
|

y)] + cov [E(x
|
= diag [E(y)] + cov(y, y) = diag [E(x)] + Dcov(α, α)D⊤,

y), E(x
|

y)]

(9)

where the last equality follows by the multilinearity property of cumulants (see Appendix C.1).
Moving the ﬁrst term from the RHS of (9) to the LHS, we deﬁne

−
From (9) and by the independence of α1, . . . , αK (see Appendix C.3), S has the following diagonal
structure

S := cov(x, x)

diag [E(x)] .

DICA S-cum. (10)

S =

var(αk)dkd⊤k = Ddiag [var(α)] D⊤.

(11)

By analogy with the second order case, using the law of total cumulance, the multilinearity property
of cumulants, and the independence of α1, . . . , αK, we derive in Appendix C.2 the expression (24),
similar to (9), for the third cumulant (7). Moving the terms in this expression, we deﬁne a tensor T
with the following element

[T ]m1m2m3 := cum(xm1 , xm2 , xm3 ) + 2δ(m1, m2, m3)E(xm1 )

DICA T-cum. (12)

δ(m2, m3)cov(xm1 , xm2 )

δ(m1, m3)cov(xm1 , xm2 )

δ(m1, m2)cov(xm1 , xm3),

−

−

−

where δ is the Kronecker delta. By analogy with (11) (Appendix C.3), the diagonal structure of the
tensor T :

k

T =

cum(αk, αk, αk)dk ⊗
In Appendix E.1, we recall (in our notation) the matrix S (39) and the tensor T (40) for the LDA
model [3], which are analogues of the matrix S (10) and the tensor T (12) for the GP/DICA mod-
els. Slightly abusing terminology, we refer to the matrix S (39) and the tensor T (40) as the LDA
moments and to the matrix S (10) and the tensor T (12) as the GP/DICA cumulants. The diagonal
structure (41) & (42) of the LDA moments is similar to the diagonal structure (11) & (13) of the
GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of

dk ⊗

X

dk.

(13)

k

X

3

Appendix E.1. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA
cumulants and the LDA moments coincide.
The following sample complexity results apply to the sample estimates of the GP cumulants:1
Proposition 3.1. Under the GP model, the expected error for the sample estimator
GP cumulant S (10) is:

S (29) for the

b

1
√N

E

S

S

E
≤ r

S

S

2
F

O

max

∆ ¯L2, ¯c0 ¯L

,

(14)

2

h

i

k

k

≤

kF
−
2, ¯c0 := min(1, c0) and ¯L := E(L).
b

k
−
h
where ∆ := max k k
dkk
b
A high probability bound could be derived using concentration inequalities for Poisson random
variables [14]; but the expectation already gives the right order of magnitude for the error (for
example via Markov’s inequality). The expression (29) for an unbiased ﬁnite sample estimate
S of S
T of T are deﬁned2 in Appendix C.4.
and the expression (30) for an unbiased ﬁnite sample estimate
A sketch of a proof for Proposition 3.1 can be found in Appendix D.

(cid:18)

(cid:19)

i

(cid:2)

(cid:3)

b

b

k

T

T

kF ]

∆, ¯c0/ ¯L
}

By following a similar analysis as in [15], we can rephrase the topic recovery error in term of the
error on the GP cumulant. Importantly, the whitening transformation (introduced in Section 4) redi-
vides the error on S (14) by ¯L2, which is the scale of S (see Appendix D.5 for details). This means
that the contribution from ˆS to the recovery error will scale as O(1/√N max
), where
{
both ∆ and ¯c0/ ¯L are smaller than 1 and can be very small. We do not present the exact expression
for the expected squared error for the estimator of T , but due to a similar structure in the derivation,
we expect the analogous bound of E[

≤
Current sample complexity results of the LDA moments [3] can be summarized as O(1/√N ). How-
ever, the proof (which can be found in the supplementary material [15]) analyzes only the case when
ﬁnite sample estimates of the LDA moments are constructed from one triple per document, i.e.,
w3 only, and not from the U-statistics that average multiple (dependent) triples per doc-
w1 ⊗
ument as in the practical expressions (43) and (44) (Appendix F.4). Moreover, one has to be careful
when comparing upper bounds. Nevertheless, comparing the bound (14) with the current theoretical
results for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the
ℓ2-norm of the columns of the topic matrix D in the numerator, as opposed to the O(1) coefﬁcient
for the LDA moments. This norm can be signiﬁcantly smaller than 1 for vectors in the simplex
(e.g., ∆ = O(1/
dkk0) for sparse topics). This suggests that the GP/DICA cumulants may have
better ﬁnite sample convergence properties than the LDA moments and our experimental results in
Section 5.2 are indeed consistent with this statement.

∆3/2 ¯L3, ¯c3/2

1/√N max

w2 ⊗

¯L3/2

−

b

k

{

}

0

.

The GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as
they are expressed via the count vectors x (which are the sufﬁcient statistics for the model) and
not the tokens wℓ’s. Note also that the construction of the LDA moments depend on the unknown
parameter c0. Given that we are in an unsupervised setting and that moreover the evaluation of
LDA is a difﬁcult task [16], setting this parameter is non-trivial. In Appendix G.4, we observe
experimentally that the LDA moments are somewhat sensitive to the choice of c0.

4 Diagonalization algorithms

How is the diagonal structure (11) of S and (13) of T going to be helpful for the estimation of the
model parameters? This question has already been thoroughly investigated in the signal processing
(see, e.g., [17, 18, 19, 20, 21, 5] and references therein) and machine learning (see [3, 4] and refer-
ences therein) literature. We review the approach in this section. Due to similar diagonal structure,
the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants.

For simplicity, let us rewrite the expressions (11) and (13) for S and T as follows

S =

skdkd⊤k ,

k

T =

tkdk ⊗

dk ⊗

k

dk,

(15)

1Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact

X

X

and, in general, depend on the prior on αk.

2For completeness, we also present the ﬁnite sample estimates

the LDA moments (which are consistent with the ones suggested in [4]) in Appendix F.4.

S (43) and
b

T (44) of S (39) and T (40) for
b

4

D

dk := √skdk,
where sk := var(αk) and tk := cum(αk, αk, αk). Introducing the rescaled topics
D⊤. Following the same assumption from [3] that the topic vectors are
we can also rewrite S =
M of S, i.e.,
linearly independent (
D is full rank), we can compute a whitening matrix W
a matrix such that W SW ⊤ = IK where IK is the K-by-K identity matrix (see Appendix F.1 for
more details). As a result, the vectors zk := W
e
RK

dk form an orthonormal set of vectors.
RK
K of a tensor
e

Further, let us deﬁne a projection

K onto a vector u

RK:

RK

(v)

e
×

∈

∈

∈

e

e

T

K

×

×

×

(u)k1k2 :=

T

T ∈
k3 Tk1k2k3 uk3.

(16)

X
Applying the multilinear transformation (see, e.g., [4] for the deﬁnition) with W ⊤ to the tensor T
RK,
from (15) and projecting the resulting tensor
we obtain

:= T (W ⊤, W ⊤, W ⊤) onto some vector u

∈

T

T

(u) =

tk := tk/s3/2

tkh
stands for the inner product. As the
where
e
vectors zk are orthonormal, the pairs zk and λk :=
(u),
tkh
which are uniquely deﬁned if the eigenvalues λk are all different. If they are unique, we can recover
the GP/DICA (as well as LDA) model parameters via
e

·i
are the eigenpairs of the matrix

is due to the rescaling of topics and

dk = W †zk and

h·
zk, u
i

zkz⊤k ,
i

tk = λk/

zk, u

X

e

T

(17)

k

k

,

zk, u
h

.
i

e

e

b

This procedure was referred to as the spectral algorithm for LDA [3] and the fourth-order3 blind
identiﬁcation algorithm for ICA [17, 18]. Indeed, one can expect that the ﬁnite sample estimates
T (30) possess approximately the diagonal structure (11) and (13) and, therefore, the rea-
S (29) and
soning from above can be applied, assuming that the effect of the sampling error is controlled.
b
This spectral algorithm, however, is known to be quite unstable in practice (see, e.g., [22]). To over-
come this problem, other algorithms were proposed. For ICA, the most notable ones are probably
the FastICA algorithm [20] and the JADE algorithm [21]. The FastICA algorithm, with appropriate
choice of a contrast function, estimates iteratively the topics, making use of the orthonormal struc-
ture (17), and performs the deﬂation procedure at every step. The recently introduced tensor power
method (TPM) for the LDA model [4] is close to the FastICA algorithm. Alternatively, the JADE al-
gorithm modiﬁes the spectral algorithm by performing multiple projections for (17) and then jointly
diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special
case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Impor-
tantly, a fast implementation [23] of the orthogonal joint diagonalization algorithm from [24] was
proposed, which is based on closed-form iterative Jacobi updates (see, e.g., [25] for the later).

In practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see,
e.g., [26, p. 30]) or the spectral algorithm. Moreover, although the application of the JD algorithm
for the learning of topic models was mentioned in the literature [4, 27], it was never implemented in
practice. In this paper, we apply the JD algorithm for the diagonalization of the GP/DICA cumulants
as well as the LDA moments, which is described in Algorithm 1. Note that the choice of a projection
RK is important and corresponds to
RM obtained as vp =
W ⊤up for some vector up ∈
vector vp ∈
W ⊤ along the third mode. Importantly, in Algorithm 1, the
the multilinear transformation of
T with
joint diagonalization routine is performed over (P + 1) matrices of size K
K, where the number of
c
topics K is usually not too big. This makes the algorithm computationally fast (see Appendix G.1).
b
The same is true for the spectral algorithm, but not for TPM.

c

×

In Section 5.1, we compare experimentally the performance of the spectral, JD, and TPM algorithms
for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of
any experimental comparison of these algorithms in the LDA context. While already working on
this manuscript, the JD algorithm was also independently analyzed by [27] in the context of tensor
factorization for general latent variable models. However, [27] focused mostly on the comparison
of approaches for tensor factorization and their stability properties, with brief experiments using a
latent variable model related but not equivalent to LDA for community detection. In contrast, we
provide a detailed experimental comparison in the context of LDA in this paper, as well as propose
a novel cumulant-based estimator. Due to the space restriction the estimation of the topic matrix D
and the (gamma/Dirichlet) parameter c are moved to Appendix F.6.

3See Appendix C.5 for a discussion on the orders.

5

Algorithm 1 Joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)

RM

N , K, P (number of random projections); (and c0 for LDA moments)

M ((29) for GP/DICA / (43) for LDA in Appendix F)

RM

1: Input: X
∈
2: Compute sample estimate
3: Estimate whitening matrix

×

×

×
RK

S (see Appendix F.1)

S
∈
M of
W
∈
option (a): Choose vectors
u1, u2, . . . , uP } ⊆
b
{
RM for all p = 1, . . . , P
sphere and set vp =
W ⊤up ∈
c
option (b): Choose vectors
u1, u2, . . . , uP } ⊆
{
RM for all p = 1, . . . , K
RK and set vp =
c
W ⊤up ∈
RK
4: For
p, compute Bp =
W
c
5: Perform orthogonal joint diagonalization of matrices
W
{
RK
c
K and vectors
(see [24] and [23]) to ﬁnd an orthogonal matrix V
×
such that
c
diag(ap), p = 1, . . . , P

∈
W ⊤V ⊤ = IK , and V BpV ⊤

T (vp)

W ⊤

c

c

W

∈

S

S

V

∀

b

b

b

{

×

RK uniformly at random from the unit ℓ2-
(P = 1 yields the spectral algorithm)
RK as the canonical basis e1, e2, . . . , eK of

W ⊤ = IK, Bp, p = 1, . . . , P

a1, a2, . . . , aP } ⊂

}
RK

K ((52) for GP/DICA / (54) for LDA; Appendix F)

6: Estimate joint diagonalization matrix A = V
b
7: Output: Estimate of D and c as described in Appendix F.6

c

c

W and values ap, p = 1, . . . , P

≈

c

5 Experiments

In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and
(b) the spectral algorithm [3], the tensor power method [4] (TPM), the joint diagonalization (JD)
algorithm from Algorithm 1, and variational inference for LDA [1].

Real data: the associated press (AP) dataset, from D. Blei’s web page,4 with N = 2, 243 documents
L = 194; the NIPS papers
and M = 10, 473 vocabulary words and the average document length
dataset5 [28] of 2, 483 NIPS papers and 14, 036 words, and
L = 1, 321; the KOS dataset,6 from the
UCI Repository, with 3, 430 documents and 6, 906 words, and
b

Semi-synthetic data are constructed by analogy with [29]: (1) the LDA parameters D and c are
learned from the real datasets with variational inference and (2) toy data are sampled from a model
of interest with the given parameters D and c. This provides the ground truth parameters D and c.
For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are
the minimum and maximum values. For the AP data, K
topics are learned and, for
topics are learned. For larger K, the obtained topic matrix is ill-
the NIPS data, K
conditioned, which violates the identiﬁability condition for topic recovery using moment matching
techniques [3]. All the documents with less than 3 tokens are resampled.

L = 136.

10, 50

10, 90

∈ {

∈ {

b

b

}

}

Sampling techniques. All the sampling models have the parameter c which is set to c = c0¯c/
k1,
¯c
k
where ¯c is the learned c from the real dataset with variational LDA, and c0 is a parameter that we
can vary. The GP data are sampled from the gamma-Poisson model (3) with b = c0/
L so that
the expected document length is
L (see Appendix B.2). The LDA-ﬁx(L) data are sampled from the
LDA model (2) with the document length being ﬁxed to a given L. The LDA-ﬁx2(γ,L1,L2) data
γ)-portion of the documents are sampled from the LDA-ﬁx(L1) model
are sampled as follows: (1
with a given document length L1 and γ-portion of the documents are sampled from the LDA-ﬁx(L2)
model with a given document length L2.

−

b

b

Evaluation. The evaluation of topic recovery for semi-synthetic data is performed with the ℓ1-
D and true D topic matrices with the best permutation of columns:
error between the recovered
1
errℓ1(
[0, 1]. The minimization is over the possible
D, D) := minπ
dπk −
dkk1 ∈
2K
b
permutations π
D and can be efﬁciently obtained with the Hungarian
PERM of the columns of
b
b
algorithm for bipartite matching. For the evaluation of topic recovery in the real data case, we use
an approximation of the log-likelihood for held out documents as the metric [16]. See Appendix G.6
for more details.

PERM

k k

P

∈

b

∈

4http://www.cs.columbia.edu/˜blei/lda-c
5http://ai.stanford.edu/˜gal/data
6https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

6

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

JD
JD(k)
JD(f)
Spec
TPM

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

0
1 

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

Figure 1: Comparison of the diagonalization algorithms. The topic matrix D and Dirichlet parameter c are
learned for K = 50 from AP; c is scaled to sum up to 0.5 and b is set to ﬁt the expected document length
L = 200. The semi-synthetic dataset is sampled from GP; number of documents N varies from 1, 000 to
b
50, 000. Left: GP/DICA moments. Right: LDA moments. Note: a smaller value of the ℓ1-error is better.

We use our Matlab implementation of the GP/DICA cumulants, the LDA moments, and the diag-
onalization algorithms. The datasets and the code for reproducing our experiments are available
online.7 In Appendix G.1, we discuss the complexity and implementation of the algorithms. We
explain how we initialize the parameter c0 for the LDA moments in Appendix G.3.

5.1 Comparison of the diagonalization algorithms

In Figure 1, we compare the diagonalization algorithms on the semi-synthetic AP dataset for K = 50
using the GP sampling. We compare the tensor power method (TPM) [4], the spectral algorithm
(Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm 1 with different
options to choose the random projections: JD(k) takes P = K vectors up sampled uniformly from
the unit ℓ2-sphere in RK and selects vp = W ⊤up (option (a) in Algorithm 1); JD selects the full basis
e1, . . . , eK in RK and sets vp = W ⊤ep (as JADE [21]) (option (b) in Algorithm 1); JD(f ) chooses
the full canonical basis of RM as the projection vectors (computationally expensive).

Both the GP/DICA cumulants and LDA moments are well-speciﬁed in this setup. However, the
LDA moments have a slower ﬁnite sample convergence and, hence, a larger estimation error for the
same value N . As expected, the spectral algorithm is always slightly inferior to the joint diagonal-
ization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms
demonstrate good performance, which also fulﬁlls our expectations. However, although TPM shows
almost perfect performance in the case of the GP/DICA cumulants (left), it signiﬁcantly deteriorates
for the LDA moments (right), which can be explained by the larger estimation error of the LDA
moments and lack of robustness of TPM. The running times are discussed in Appendix G.2. Over-
all, the orthogonal joint diagonalization algorithm with initialization of random projections as W ⊤
multiplied with the canonical basis in RK (JD) is both computationally efﬁcient and fast.

5.2 Comparison of the GP/DICA cumulants and the LDA moments

In Figure 2, when sampling from the GP model (top, left), both the GP/DICA cumulants and LDA
moments are well speciﬁed, which implies that the approximation error (i.e., the error w.r.t.
the
model (mis)ﬁt) is low for both. The GP/DICA cumulants achieve low values of the estimation error
already for N = 10, 000 documents independently of the number of topics, while the convergence
is slower for the LDA moments. When sampling from the LDA-ﬁx(200) model (top, right), the
GP/DICA cumulants are mis-speciﬁed and their approximation error is high, although the estimation
error is low due to the faster ﬁnite sample convergence. One reason of poor performance of the
GP/DICA cumulants, in this case, is the absence of variance in the document length. Indeed, if
documents with two different lengths are mixed by sampling from the LDA-ﬁx2(0.5,20,200) model
(bottom, left), the GP/DICA cumulants performance improves. Moreover, the experiment with a
changing fraction γ of documents (bottom, right) shows that a non-zero variance on the length
improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a
non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not
likely to happen.

7 https://github.com/anastasia-podosinnikova/dica

7

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

1

0.8

0
1 

0.2

0
1 

r
o
r
r
e
-
1

ℓ

0.6

0.4

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11.5

-12

-12.5

-13

-13.5

r
o
r
r
e
-
1

ℓ

r
o
r
r
e
-
1

ℓ

0
1 

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

40
10
Number of docs in 1000s

20

30

50

0.8
0.2
Fraction of doc lengths γ

0.6

0.4

1

Figure 2: Comparison of the GP/DICA cumulants and LDA moments. Two topic matrices and parameters c1
and c2 are learned from the NIPS dataset for K = 10 and 90; c1 and c2 are scaled to sum up to c0 = 1.
Four corpora of different sizes N from 1, 000 to 50, 000: top, left: b is set to ﬁt the expected document length
L = 1300; sampling from the GP model; top, right: sampling from the LDA-ﬁx(200) model; bottom, left:
b
sampling from the LDA-ﬁx2(0.5,20,200) model. Bottom, right: the number of documents here is ﬁxed to
N = 20, 000; sampling from the LDA-ﬁx2(γ,20,200) model varying the values of the fraction γ from 0 to 1
with the step 0.1. Note: a smaller value of the ℓ1-error is better.

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 3: Experiments with real data. Left: the AP dataset. Right: the KOS dataset. Note: a higher value of
the log-likelihood is better.

5.3 Real data experiments

In Figure 3, JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI)
and with variational inference initialized with the output of JD-GP (VI-JD). We measure the held
out log-likelihood per token (see Appendix G.7 for details on the experimental setup). The orthogo-
nal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates promising
performance. In particular, the GP/DICA cumulants signiﬁcantly outperform the LDA moments.
Moreover, although variational inference performs better than the JD-GP algorithm, restarting varia-
tional inference with the output of the JD-GP algorithm systematically leads to better results. Similar
behavior has already been observed (see, e.g., [30]).

6 Conclusion

In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where
word counts are directly modelled. These moments make fewer assumptions regarding distributions,
and are theoretically and empirically more robust than previously proposed tensors for LDA, both
on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization
procedure is also more robust. Once the topic matrix has been estimated in a semi-parametric way
where topic intensities are left unspeciﬁed, it would be interesting to learn the unknown distributions
of the independent topic intensities.

Aknowledgements. This work was partially supported by the MSR-Inria Joint Center. The authors
would like to thank Christophe Dupuy for helpful discussions.

8

References
[1] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:903–1022,

[2] T. Grifﬁths. Gibbs sampling in the generative model of latent Dirichlet allocation. Technical report,

[3] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

2003.

Stanford University, 2002.

allocation. In NIPS, 2012.

[4] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning

latent variable models. J. Mach. Learn. Res., 15:2773–2832, 2014.

[5] P. Comon and C. Jutten, editors. Handbook of blind sourse separation: independent component analysis

[6] C. Jutten. Calcul neuromim´etique et traitement du signal: analyse en composantes ind´ependantes. PhD

and applications. Academic Press, 2010.

thesis, INP-USM Grenoble, 1987.

[7] C. Jutten and J. H´erault. Blind separation of sources, part I: an adaptive algorithm based on neuromimetric

architecture. Signal Process., 24:1–10, 1991.

[8] P. Comon. Independent component analysis, a new concept? Signal Process., 36:287–314, 1994.

[9] W.L. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002.

[10] M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. R. Stat. Soc., 61:611–622,

1999.

[11] S. Roweis. EM algorithms for PCA and SPCA. In NIPS, 1998.

[12] J. Canny. GaP: a factor model for discrete data. In SIGIR, 2004.

[13] W.L. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.

[14] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of inde-

[15] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet

pendence. Oxford University Press, 2013.

allocation. CoRR, abs:1204.6703, 2013.

[16] H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In

[17] J.-F. Cardoso. Source separation using higher order moments. In ICASSP, 1989.

[18] J.-F. Cardoso. Eigen-structure of the fourth-order cumulant tensor with application to the blind source

separation problem. In ICASSP, 1990.

[19] J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods. In

[20] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Trans.

Neural Netw., 10(3):626–634, 1999.

[21] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. In IEE Proceedings-F,

[22] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Comput., 11:157–192,

[23] J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal.

Appl., 17(1):161–164, 1996.

[24] A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simulataneous diagonalization.

SIAM J. Matrix Anal. Appl., 14(4):927–949, 1993.

[25] J. Nocedal and S.J. Wright. Numerical optimization. Springer, 2nd edition, 2006.

[26] F.R. Bach and M.I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1–48, 2002.

[27] V. Kuleshov, A.T. Chaganty, and P. Liang. Tensor factorization via matrix factorization. In AISTATS,

[28] A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. J.

Mach. Learn. Res., 8:2265–2295, 2007.

[29] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm

for topic modeling with provable guarantees. In ICML, 2013.

[30] S. Cohen and M. Collins. A provably correct learning algorithm for latent-variable PCFGs. In ACL, 2014.

ICML, 2009.

ISCAS, 1996.

1993.

1999.

2015.

9

A Appendix. Plate diagrams for the models from Section 2

c

θn

znℓ

wnℓ

N

N

N

c

θn

xn

c

αn

N

αn

M

xnm

M

xnm

D

Ln

D

D

D

(a) LDA (18)

(b) LDA (19)

(c) GP (20)

(d) DICA (21)

Figure 4: Plate diagrams for the models from Section 2.

In Section 2, the index n, which stands for the n-th document, was omitted. For convenience, we
recall the models. The LDA model in the tokens representation:

the LDA model with the marginalized out latent variable z:

θn ∼
znℓ|
θn ∼
znℓ, θn ∼

wnℓ|

Dirichlet(c),
Multinomial(1, θn),
Multinomial(1, dznℓ);

θn ∼
θn ∼

xn|

Dirichlet(c),
Multinomial(Ln, Dθn);

αnk ∼
αn ∼

xnm|

Gamma(ck, b),
Poisson([Dαn]m);

αn1, . . . , αnK ∼
αn ∼
xnm|

mutually independent,
Poisson([Dαn]m).

the GP model:

and the DICA model:

(18)

(19)

(20)

(21)

B Appendix. The GP model

B.1 The connection between the LDA and GP models

To show that the LDA model (2) with the additional assumption that the document length is modeled
as a gamma-Poisson random variable is equivalent to the GP model (3), we show that:

- when modeling the document length L as a Poisson random variable with a parameter λ,
the count vectors x1, x2, . . . , xM are mutually independent Poisson random variables;

- the Gamma prior on λ reveals the connection αk = λθk between the Dirichlet random

variable θ and the mutually independent gamma random variables α1, α2, . . . , αK.

For completeness, we repeat
the known result
∼
Multinomial(L, Dθ) (which thus means that L =
m xm with probability one), then x1, x2,
. . . , xM are mutually independent Poisson random variables with parameters λ [Dθ]1, λ [Dθ]2, . . . ,

Poisson(λ) and x
|

if L

that

∼

L

P

10

λ [Dθ]M . Indeed, we consider the following joint probability mass function where x and L are
assumed to be non-negative integers:

p(x, L

θ, λ) =p(L

|

λ)p(x
|

|

L, θ)

L=Pm xm

exp (

λ) λL

−
✚✚L!

✚✚L!
m xm!

[Dθ]xm
m

L=Pm xm

exp(

λ

−

L=Pm xm

m
Y

Q
[Dθ]m)λPm xm

[Dθ]xm
m
xm!

exp(

m
m
Y
X
λ [Dθ]m)(λ [Dθ]m)xm
−

xm!

L=Pm xm

Poisson(xm; λ [Dθ]m),

=1

=1

=1

=1

{

{

{

{

}

}

}

}

m
Y

m
Y

where in the third equation we used the fact that

[Dθ]m =

Dmkθk =

θk

Dmk = 1.

m
X
θ, λ) = p(L

Xm,k
m p(xm|

Xk

m
X

|

{

L=Pm xm

x)
|
and p(xm|

x) is simply the deterministic
We thus have p(x, L
|
distribution 1
λ[Dθ]m) for m = 1, . . . , M are independent Poisson(λ[Dθ]m)
Q
distributions (and thus do not depend on L). Note that in the notation introduced in the paper,
Dmk = dkm. Hence, by using the construction of the Dirichlet distribution from the normalization
of independent gamma random variables, we can show that the LDA model with a gamma-Poisson
prior over the length is equivalent to the following model (recall, that c0 =

λ[Dθ]m) where p(L

}

k ck):

λ
θ
λ, θ

∼
∼
∼

xm|

Gamma(c0, b),
Dirichlet(c),
Poisson([D(λθ)]m).

P

(22)

More speciﬁcally, we complete the second part of the argument with the following properties. When
Gamma(ck, b),
α1, α2, . . . , αK are mutually independent gamma random variables, each αk ∼
k ck, b). The former is equivalent
their sum is also a gamma random variable
k αk ∼
to λ. It is known (e.g., [32]) that a Dirichlet random variable can be sampled by ﬁrst sampling
independent gamma random variables (αk) and then dividing each of them by their sum (λ): θk =
k′ αk′ , and, in other direction, the variables αk = λθk are mutually independent, giving back
αk/
the GP model (3).

Gamma(

P

P

P

B.2 The expectation and the variance of the document length for the GP model

From the drivations in Appendix B.1, it follows that the document length of the GP model (3) is a
gamma-Poisson random variable, i.e., L
Gamma(c0, b). Therefore, the
|
following follows from the law of total expectation and the law of total variance

Poisson(λ) and λ

∼

∼

λ

E(L) = E [E(L
|
var(L) = var [E(L

λ)] = E(λ) = c0/b
λ)] + E [var(L

λ)] = var(λ) + E(λ) = c0/b + c0/b2

|

|
The ﬁrst expression shows that the parameter b controls the expected document length E(L) for a
given parameter c0: the smaller b, the larger E(L). On the other hand, if we allow c0 to vary as
well, only the ratio c0/b is important for the document length. We can then interpret the role of c0
as actually controlling the concentration of the distribution for the length L (through the variance).
More speciﬁcally, we have that:

var(L)
(E(L))2 =

1
E(L)

+

1
c0

.

(23)

For a ﬁxed target document length E(L), we can increase the variance (and thus decrease the con-
centration) by using a smaller c0.

11

C Appendix. The cumulants of the GP and DICA models

C.1 Cumulants

For a random vector x

RM , the ﬁrst three cumulant tensors8 are

∈

cum(xm) = E(xm),
cum(xm1 , xm2 ) = E [(xm1 −
cum(xm1 , xm2 , xm3 ) = E [(xm1 −

E(xm1 ))(xm2 −
E(xm1 ))(xm2 −

E(xm2 )] = cov(xm1 , xm2),
E(xm3 )))] .
E(xm2 ))(xm3 −

Note that the 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not for
RM
M denotes the third order tensor with ele-
higher orders). In the following, cum(x, x, x)
ments cum(xm1 , xm2 , xm3). Some of the properties of cumulants are listed below (see [5, chap. 5]).
The most important property that motivate us to use cumulants in this paper (and the ICA literature)
is the independence property, which says that the cumulant tensor for a random vector with inde-
pendent components is diagonal (this property does not hold for the (non-central) moment tensors
of any order, and neither for the central moments of order 4 or more).

∈

M

×

×

RM are independent, then their cross-cumulants
- Independence. If the elements of x
are zero as soon as two indices are different, i.e., cum(xm1 , xm2 ) = δ(m1, m2)E[(xm1 −
E(xm1 ))3], where δ is the
Em1 ))2] and cum(xm1 , xm2 , xm3 ) = δ(m1, m2, m3)E[(xm1 −
Kronecker delta.

∈

- Multilinearity. If two random vectors y

y = Dα for some D

RM

K, then

×

∈

∈

RM and α

RK are linearly dependent, i.e.,

∈

cum(ym) =

cum(αk)Dmk,

cum(ym1, ym2) =

cum(αk1 , αk2 )Dm1k1Dm2k2,

cum(ym1 , ym2, ym3) =

cum(αk1 , αk2 , αk3 )Dm1k1 Dm2k2 Dm3k3 ,

Xk

Xk1,k2

Xk1,k2,k3

which can also be denoted9 by

E(y) = DE(α),

cov(y, y) = Dcov(α, α)D⊤,

cum(y, y, y) = cum(α, α, α)(D⊤, D⊤, D⊤).

- The law of total cumulance. For two random vectors x

RM and y

RM , it holds

∈

∈

y)] + cov [E(xm1 |

y), E(xm2 |

cum(xm) = E [E(xm|
cum(xm1 , xm2 ) = E [cov(xm1 , xm2 |
cum(xm1 , xm2 , xm3 ) = E [cum(xm1 , xm2, xm3 |

y)] ,

y), cov(xm2 , xm3 |
y), cov(xm1 , xm3 |
y), cov(xm1 , xm2 |
Note that the ﬁrst expression is also well known as the law of total expectation or the tower
property, while the second one is known as the law of total covariance.

+ cov [E(xm1 |
+ cov [E(xm2 |
+ cov [E(xm3 |

y)] + cum [E(xm1 |
y)]
y)]
y)] .

y)] ,
y), E(xm2 |

y), E(xm3 |

y)]

tX

8Strictly speaking, the (scalar) n-th cumulant κn of a random variable X is deﬁned via the cumulant-
generating function g(t), which is the natural logarithm of the moment-generating function, i.e g(t) :=
log E (cid:2)e
(cid:3). The cumulant κn is then obtained from a power series expansion of the cumulant-generating
function, that is g(t) = P

9In [4], given a tensor T ∈ RK×K×K , T (D⊤, D⊤, D⊤) is referred to as the multilinear map. In [34], the

/n! [Wikipedia].

∞
n=1 κnt

same entity is denoted by T ×1 D⊤ ×2 D⊤ ×3 D⊤, where ×n denotes the n-mode tensor-matrix product.

n

12

C.2 The third cumulant of the GP/DICA models

In this section, by analogy with Section 3.1, we derive the third GP/DICA cumulant.
As the third cumulant of a Poisson random variable xm with parameter ym is E((xm −
E(xm))3
ym) = ym, then by the independence property of cumulants from Section C.1, the cu-
mulant of x
|

y is diagonal:

|

cum(xm1 , xm2 , xm3 |

y) = δ(m1, m2, m3) ym1.

y into the law of total cumulance, we obtain

Substituting the cumulant of x
|
cum(xm1 , xm2 , xm3) = E [cum(xm1 , xm2 , xm3 |
y), E(xm2 |
+ cum [E(xm1 |
+ cov [E(xm2 |
y), cov(xm1 , xm3|
= δ(m1, m2, m3)E(ym1 ) + cum(ym1, ym2, ym3)

y), E(xm3 |

y)] + cov [E(xm3 |

y)]
y)] + cov [E(xm1 |

y), cov(xm2 , xm3 |
y)]

y), cov(xm1 , xm2|

y)]

+ δ(m2, m3)cov(ym1, ym2) + δ(m1, m3)cov(ym1, ym2) + δ(m1, m2)cov(ym1, ym3 )

= δ(m1, m2, m3)E(xm1 ) + cum(ym1 , ym2, ym3)

+ δ(m2, m3)cov(xm1 , xm2 )
+ δ(m1, m3)cov(xm1 , xm2 )
+ δ(m1, m2)cov(xm1 , xm3 )

δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )
δ(m1, m2, m3)E(xm1 )

−
−
−

= cum(ym1 , ym2, ym3)

2δ(m1, m2, m3)E(xm1 )

−

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 )

=

cum(α, α, α)(D⊤, D⊤, D⊤)

2δ(m1, m2, m3)E(xm1 )

m1m2m3 −

+ δ(m2, m3)cov(xm1 , xm2 ) + δ(m1, m3)cov(xm1 , xm2) + δ(m1, m2)cov(xm1 , xm3 ),
(cid:2)
where, in the third equality, we used the previous result from (9) that cov(y, y) = cov(x, x)
diag(E(x)).

(cid:3)

(24)

−

C.3 The diagonal structure of the GP/DICA cumulants

In this section, we provide detailed derivation of the diagonal structure (11) of the matrix S (10) and
the diagonal structure (13) of the tensor T (12).

From the independence of α1, α2, . . . , αK and by the independence property of cumulants
it follows that cov(α, α) is a diagonal matrix and cum(α, α, α) is a
from Section C.1,
i.e., cov(αk1 , αk2 ) = δ(k1, k2)cov(αk1 , αk2 ) and cum(αk1 , αk2 , αk3 ) =
diagonal
δ(k1, k2, k3)cum(αk1 , αk1 , αk1 ). Therefore, the following holds

tensor,

cov(ym1 , ym2) =

cov(αk, αk)Dm1kDm2k,

cum(ym1 , ym2, ym3) =

cum(αk, αk, αk)Dm1kDm2kDm3k,

which we can rewrite in a matrix/tensor form as

cov(y, y) =

cov(αk, αk)dkd⊤k ,

cum(y, y, y) =

cum(αk, αk, αk)dk ⊗

dk ⊗

dk.

Moving cov(y, y) / cum(y, y, y) in the expression for cov(x, x) (9) / cum(x, x, x) (24) on one side of
M
equality and all other terms on the other side, we deﬁne matrix S
as follows

M / tensor T

RM

RM

∈

∈

M

×

×

×

S := cov(x, x)

diag (E(x)) ,
Tm1m2m3 := cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)E(xm1 )

−

Xk

Xk

Xk

Xk

(25)

(26)

δ(m2, m3)cov(xm1 , xm2 )
δ(m1, m3)cov(xm1 , xm2 )
δ(m1, m2)cov(xm1 , xm3 ).

−
−
−

13

By construction, S = cov(y, y) and T = cum(y, y, y) and, therefore, it holds that

S =

cov(αk, αk)dkd⊤k ,

Xk

(27)

(30)

(31)

T =

cum(αk, αk, αk)dk ⊗
This means that both the matrix S and the tensor T are sums of rank-1 matrices and tensors, respec-
tively10. This structure of the matrix S and the tensor T is the basis for the algorithms considered in
this paper.

dk ⊗

Xk

dk.

(28)

C.4 Unbiased ﬁnite sample estimators for the GP/DICA cumulants

Given a sample
the GP/DICA cumulants:

x1, x2, . . . , xN }
{

, we obtain a ﬁnite sample estimate

S of S (10) /

T of T (12) for

S :=

cov(x, x)

diag

−

,

E(x)
(cid:17)

(cid:16)

b

b

(29)

E(xm1 )

b

b

Tm1m2m3 :=

cum(xm1 , xm2, xm3 ) + 2δ(m1, m2, m3)
b
d
δ(m2, m3)
δ(m1, m3)
δ(m1, m2)

d

cov(xm1 , xm2 )
−
cov(xm1 , xm2 )
−
d
cov(xm1 , xm3 ),
−
where unbiased estimators of the ﬁrst three cumulants are
d
1
d
N

E(xm1 ) =

xnm1 ,

b

b
cov(xm1 , xm2 ) =

znm1znm2 ,

cum(xm1 , xm2, xm3 ) =

d

znm1znm2znm3 ,

2)

−

n
X

d

where the word vocabulary indexes are m1, m2, m3 = 1, 2, . . . , M and the centered documents
E(xm). (The latter is introduced only for compact representation of (31) and is
znm := xnm −
different from z in the LDA model.)
b
C.5 On the orders of cumulants

DU )(

DU )⊤ with any orthogonal K

D as one can equivalently use
Note that the factorization of S =
K matrix U . Therefore, one has to consider higher
S = (
than the second order information. Moreover, in ICA the fourth-order tensors are used, because the
third cumulant of the Gaussian distribution is zero, which is not the case in the DICA/LDA models,
where the third order information is sufﬁcient.

D⊤ does not uniquely determine

D

×

e

e

e

e

e

n
X
1

1

−

N

(N

−

n
X
N
1)(N

D Appendix. The sketch of the proof for Proposition 3.1

D.1 Expected squared error for the sample expectation

The sample expectation is

n xn is an unbiased estimator of the expectation and:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

−

E(xm)
(cid:17)

2

(cid:21)

E(x) = 1
N
E(xm)
P

E

(cid:17)

=

=

=

b
m
X
1
N 2

1
N

(cid:20)(cid:16)

b
E



m
X

E

 

n
X
(xm −
h

m
X

!



n
X

var(xm).

=n′
Xn

E(xm))2

=

1
N

i

m
X

(xnm −

E(xm))2

+ E

(xnm −

E(xm)) (xn′m −

E(xm))









10For tensors, such decomposition is also known under the names CANDECOMP/PARAFAC or, simply, the

CP decomposition (see, e.g., [34]).

14

[E(var(xm|

y)) + var(E(xm|

y))] =

[E(ym) + var(ym)]

1
N

m
X

Further, by the law of total variance:

E

E(x)

E(x)
k

2
2

−

k

(cid:16)

b

(cid:17)

=

=

1
N

m
X

1
N "

using the fact that

P

E(αk) +

dk, dki
h

var(αk)

,

#

Xk
m Dmk = 1 for any k.

Xk

D.2 Expected squared error for the sample covariance

The following ﬁnite sample estimator of the covariance cov(x, x) = E(xx⊤)

E(x)E(x)⊤

−

E(x)

E(x)⊤ =

1

N

1

−

n  
X

xnx⊤n −

1
N 2

xn′ x⊤n′′

!

Xn′

Xn′′

cov(x, x) =

d

N

1
N

=

1

1

−

n
X

xnx⊤n −

xnx⊤n −



N

n
X



b
xn

b
1

1

−

=n

Xn′

x⊤n′





is unbiased, i.e., E(

cov(x, x)) = cov(x, x). Its squared error is

E

d
cov(x, x)

cov(x, x)
k

−

2
F

=

E

cov(xm, xm′ )
(

E[

cov(xm, xm′ )])2

.

−

Xm,m′

h

(cid:1)

d

d

i

k
(cid:0)

d

The m, m′-th element of the sum above is equal to

xnm

xn′′m′

,

xn′mxn′m′

xn′m

xn′′′m′

=n′
Xn′′′





cov

xnm

xn′′m′ , xn′mxn′m′

−

N

1

1

−

=n

Xn′′

cov

xnmxnm′



−

N

1

1

−


cov (xnmxnm′ , xn′mxn′m′ )

=n

Xn′′

2
N 2(N

−

xn′′m′, xn′m

xn′′′m′

1)

−

Xn,n′

=n′
Xn′′′









1)2

cov

xnm



−

Xn,n′

Xn′′

cov (xnmxnm′ , xnmxnm′ )

=n

(32)







1
N 2

Xn,n′
1
N 2

Xn,n′
1
N 2(N

=

+

=

1
N 2

n
X
2
N 2(N

1
N 2(N

−

+

+

1
N 2(N

cov (xnmxn′′m′ , xnmxnm′ ) +

cov (xnmxn′m′, xn′mxn′m′ )

1) 

−



n
X

=n

Xn′′

1)2 



1)2 

−

−

n
X

Xn′′

=n Xn′′′

=n

Xn′


=n′
Xn

=n′
Xn′′′

n
X

=n

Xn′

cov (xnmxn′′m′ , xnmxn′′′m′ ) +

cov (xnmxn′m′ , xn′mxn′′′m′ ) +


cov (xnmxn′′m′ , xn′mxnm′ )


cov (xnmxn′′m′ , xn′mxn′′m′ )



,





Xn′

=n′
Xn

=n

Xn′′

Xn′

=n′
Xn

=n

Xn′′

where we used mutual independence of the observations xn in a sample
the covariance between the two expressions involving only independent variables is zero.

xn}

{

N
n=1 to conclude that

15

Further:
E

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N 2

k
(cid:0)

4
d
N 2(N

2
N 2(N

2
N 2(N

−

+

+

N (N

E(x2

(cid:1)
1)

−

N (N

(cid:0)
1)(N

2)

−

N (N

1)(N

2)

−

−

−

1)

−

Xm,m′

1)2

1)2

−

−

Xm,m′

Xm,m′

which after simpliﬁcation gives

N

E(x2
(cid:16)

Xm,m′
mxm′ )E(xm′ )

−

mx2

m′ )

[E(xmxm′ )]2

−

(cid:17)

E(xmxm′ )E(xm)E(xm′ )

[E(xm)]2 [E(xm′ )]2

(cid:1)

(cid:17)

−

m) [E(xm′ )]2

E(x2
(cid:16)
E(xmxm′ )E(xm)E(xm′ )
(cid:16)

−

[E(xm)]2 [E(xm′ )]2

+ O

1
N 2

,

(cid:18)

(cid:19)

(cid:17)

,

E

k

cov(x, x)

cov(x, x)
k

−

2
F

=

1
N

Xm,m′
[2E(xm)E(xm′ )cov(xm, xm′ )

(cid:1)

h

(cid:0)
+

1
d
N

Xm,m′

var(xmxm′ ) + 2 [E(xm)]2 var(xm′ )
i
1
N 2

4E(xm)cov(xmxm′ , xm′ )] + O

−

(cid:18)

(cid:19)

where in the last equality, by symmetry, the summation indexes m and m′ can be exchanged. As
Poisson(ym), by the law of total expectation and law of total covariance, it follows, for
xm ∼
m

= m′ (and using the auxiliary expressions from Section D.4):
[E[xmxm′ ]]2 = E
mym′ + ymy2

var(xmxm′ ) = E(x2
mx2
my2
y2
m′ + ymym′
(cid:2)
[E(xm)]2 var(xm′ ) = [E(ym)]2 E(ym′ ) + [E(ym)]2 E(y2

m′ )
−
m′ + y2

E(x2

= E

m′ )

(cid:2)

y)]]2

mx2
m′

[E [E(xmxm′

|

−

y)
[E(ymym′)]2 ,
−
[E(ym)]2 [E(ym′ )]2 ,
(cid:3)

(cid:3)

|

E(xm)E(xm′ )cov(xm, xm′ ) = E(ymym′ )E(ym)E(ym′ )

E(xm)cov(xmxm′ , xm′ ) = E(ym)
Now, considering the m = m′ case, we have:
var(x2

E(ymym′) + E(ymy2

−
[E(ym)]2 [E(ym′ )]2 ,
m′)

E(ymym′ )E(ym′ )

−

−

.

(cid:3)

E(xm)E(xm)cov(xm, xm) = E(ym)2
m, xm) = E(ym)

E(xm)cov(x2
Substitution of ym =

2

y)]

E[E(x2

(cid:2)
m) = E[E(x4
y)]
m|
m|
−
= E
y4
m + 6y3
m + 7y2
m + ym
(cid:2)
m) + E(ym)
E(y2
h
m) + 3E(y2
E(y3

(cid:2)

2

,

E

y2
m + ym
(cid:3)
−
[E(ym)]2
(cid:3)
(cid:2)
−
m) + E(ym)

i
E(ym)

(cid:2)

,

(cid:3)(cid:3)

−

k Dmkαk gives the following
(cid:2)
1
N

=

2
F

cov(x, x)
P
k

−

Xk,k′,k′′,k′′′h

dk, dk′

dk′′ , dk′′′

ih

(cid:2)
iAkk′k′′k′′′

E

cov(x, x)

k
(cid:0)

d

E(y2

m) + E(ym)

.

dk′′ ,~1

iBkk′k′′ +

dk ◦
h

dk′ , dk′′

iEkk′k′′

(cid:3)(cid:3)

i

(cid:1)

1
N

1
N

+

+

+

Xk

ih

Xk,k′,k′′

dk, dk′
h
h
dk,~1
h
Xk,k′
h
E(αk) + O
dk,~1
i
h

ih

1
N 2

,

(cid:18)

(cid:19)

E(αkαk′ ) +
dk′ ,~1
i

dk, dk′
h

iFkk′

i

where ~1 is the vector with all the elements equal to 1 and
Akk′k′′k′′′ = E(αkαk′ αk′′ αk′′′ )

−

E(αkαk′′ )E(αk′ αk′′′ ) + 2E(αk)E(αk′ )E(αk′′ αk′′′ )

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ ) + 2E(αkαk′′ )E(αk′ )E(αk′′′ )
4E(αk)E(αk′ αk′′ αk′′′ ) + 4E(αk)E(αk′ αk′′ )E(αk′′′ ),

−

−
−

Bkk′k′′ = 2E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ )
Ekk′k′′ = 4E(αkαk′ αk′′ ) + 6E(αk)E(αk′ )E(αk′′ )
Fkk′ = 6E(αkαk′ )

5E(αk)E(αk′ ),

−

−
−

4E(αk)E(αk′ αk′′ ),
10E(αkαk′ )E(αk′′ ),

2E(αk)E(αk′ )E(αk′′ )E(αk′′′ )

16

where we used the expressions from Section D.4.

D.3 Expected squared error of the estimator

S for the GP/DICA cumulants

As the estimator

S (29) of S (10) is unbiased, its expected squared error is

b

E

S

S

2
b
F
k

−

=E

i

k
h

b

cov(x, x)

cov(x, x)) +

diag[

E(x)]

diag [E(x)]

(
(cid:20)(cid:13)
(cid:13)
= E
(cid:13)

+ 2

E(x)

d
k
h

E
b

m
X

h(cid:16)

b

−
E(x)
k

2
F

−
E(xm)

+ E

i
(cid:2)
E(xm)
(cid:17)

−

−

(cid:16)
cov(x, x)

b

−

k
cov(xm, xm)
(
d

−

2

(cid:21)

F
(cid:17)(cid:13)
(cid:13)
(cid:13)

i

cov(x, x)
k

2
F

(cid:3)
cov(xm, xm))

.

(33)

cov(xm, xm) are unbiased, the m-th element of the last sum is equal to

d

E(xm) and
E(xm),
b
h
1
b
N 2

Xn,n′

As

cov

=

=

1
N 2

d

cov(xm, xm)
i
n′m

xnm, x2

d

cov

(cid:2)
xnm, x2

nm

(cid:3)

cov

n
X
E(x3

(cid:2)

1
N 2(N

−

2
N 2(N

−

(cid:3)

1)

−

Xn,n′,n′′

=n′

1)

=n

Xn,n′

−
[E(xm)]3

cov [xnm, xn′mxn′′m]

cov [xnm, xn′mxnm] + O

1
N 2

(cid:18)

(cid:19)

=

−

m)

1
N
1
N

m)E(xm)

E(x2
(cid:16)
[E(xm)]3 + O

2
N
2
N
≤
(cid:18)
where we neglected the negative term
from the expressions in Section D.4. Further, the fact that ym =

(cid:18)
(cid:19)
E(y3
m) + 3E(y2
h

(cid:19)
E(x2

(cid:17)
1
N

1
N 2

1
N 2

m) +

E(x3

+ O

−

−

=

m) + E(ym) + 2 [E(ym)]3

+ O

(cid:18)
m)E(xm) for the inequality, and the last equality follows

i

1
N 2

,

(cid:19)

k Dmkαk gives

cov

E(xm),
h

cov(xm, xm)
i

m
X

b

d

=

+

1
N

3
N

Xk,k′,k′′h

Xk,k′h

dk ◦

dk′ , dk′′

P
iCkk′k′′

dk, dk′

E(αkαk′ ) +
i

1
N

dk,~1
h

E(αk) + O
i

1
N 2

,

(cid:18)

(cid:19)

Xk

where

denotes the elementwise Hadamard product and

◦

Ckk′k′′ = E(αkαk′ αk′′ ) + 2E(αk)E(αk′ )E(αk′′ ).
Plugging this and the expressions for E(
cov(x, x)
−
k
Sections D.1 and D.2, respectively, into (33) gives

F ) and E(
k

E(x)
k

E(x)

2

cov(x, x)
k

−

2
F ) from

dk, dki
h

var(αk) +

E(αk) +

dk, dk′

dk′′ , dk′′′

ih

iAkk′ k′′k′′′



+ O

b

Xk

d

Xk,k′,k′′,k′′′h

iBkk′k′′ + 2

dk ◦
h

dk′ , dk′′

iCkk′k′′ ] +

i

≤

30c4

0 + 8c0

0 + 23c3

0 + 14c2
b4

where we used that, by the simplex constraint on the topics,
expression in more details, let us now consider the GP model, i.e., αk ∼
Xk,k′,k′′,k′′′ Akk′ k′′k′′′
Xk,k′,k′′ Ckk′k′′
Xk,k′ Fkk′

Xk,k′,k′′ Ekk′k′′

0 + 6c2
b3

0 + c0
b2

0 + c0
b2

E(αkαk′ )

0 + 2c0

Xk,k′

2c2

2c2

7c3

and

and

and

≤

≤

≤

≤

,

,

,

(1 + 6

dk, dk′
h

) E(αkαk′ ) + 2
i

Xk
= 1 for all k. To analyze this

Xk,k′
dk,~1
h

Gamma(ck, b):
6c3

0 + 10c2
b3

0 + 4c0

,

Xk,k′,k′′ Bkk′k′′

≤

12c3

0 + 10c2
b3

0 + 8c0

,

S

S

2
F

k

−

E

k
h

b
1
N 

+

=

1
N 

i

Xk

dk, dk′
h

[

Xk,k′,k′′


17

1
N 2

(cid:18)

(cid:19)


E(αk)

,





where we used the expressions from Section D.4, which gives

ν
N "

max

k k

2
2

dkk

c0
b2 +

c0
b

+

max
k,k′ h

dk, dk′

(cid:18)

i
(cid:19)

2

max

c4
0
b4 ,

c0
b4

+ max
k,k′ h

dk, dk′

max

i

c3
0
b3 ,

c0
b3

(cid:21)#

S

S

2
F

k

−

≤

i

E

k
h

+

b
ν
N

max
k,k′,k′′h

dk ◦

dk′ , dk′′

max

i
(cid:19)

c3
0
b3 ,

c0
b3

+

1 + max
k,k′ h

dk, dk′

max

(cid:20)

i
(cid:19)

(cid:21)
c2
0
b2 ,

c0
b2

(cid:20)

(cid:21)

(cid:18)

(cid:20)(cid:18)

≤
dkk

(cid:21)(cid:21)
30 is a universal constant. As, by the Cauchy-Schwarz inequality, max k,k′
where ν
2
2
2 =: ∆1 and max k,k′,k′′
max k k
max k k
2 ≤
∆1), it follows that
(note that for the topics in the simplex, ∆2 ≤
L3
¯c2
0 (cid:19)
0L2 + ¯c3

(cid:20)
i
1L4 + ¯c0∆1L3 + ¯c2
∆2

max k k
∆1 as well as ∆2

dkk∞ k
1 ≤

+ L + ∆2
1

L3
¯c2
0 (cid:21)

dk ◦
h

dk′ , dk′′

L2
¯c2
0

L2
¯c0

dkk

+ ∆2

ν
N

+ O

+ O

i ≤

∆1

0L

+

+

≤

(cid:18)

(cid:18)

2
F

E

S

S

(cid:20)

k

,

1
N 2

(cid:19)

L4
¯c3
0
1
N 2

k
h
2ν
b
N

−
1
¯c3
0

≤

(cid:2)

where ¯c0 = min(1, c0)
length. The second term ¯c0∆1L3 cannot be dominant as the system ¯c0∆1L3 > ¯c2
∆2
1L4 is infeasible. Also, with the reasonable assumption that L
¯c3
0L

(cid:19)
1 and, from Section B.2, c0 = bL where L is the expected document
0L2 and ¯c0∆1L3 >
1, we also have that the 4th term

0L2. Therefore,
¯c2

≥

≤

(cid:18)

(cid:3)

≤

(cid:20)

,

1
N 2

+ O

(cid:19)

(cid:18)
dk, dk′
i ≤
h
3
2 =: ∆2
dkk

E

S

k

−

S

2
F

k

h

≤

i

3ν
N

max

∆2

1L4, ¯c2

0L2

+ O

(cid:2)

(cid:3)

1
N 2

.

(cid:18)

(cid:19)

M
m=1 are conditionally independent given y in the DICA model (3), we have the following
= m′ and using the moments of the Poisson

b
D.4 Auxiliary expressions

{

xm}

As
expressions by using the law of total expectation for m
distribution with parameter ym:
E(xm) = E[E(xm|
m) = E[E(x2
E(x2
m|
m) = E[E(x3
E(x3
m|
m) = E[E(x4
E(x4
m|
E(xmxm′ ) = E[E(xmxm′
E(xmx2
m′ ) = E[E(xmx2
m′
|
E(x2
ym)E(x2
m′ ) = E[E(x2
mx2
m′
m|

ym)] = E(ym),
ym)] = E(y2
ym)] = E(y3
ym)] = E(y4

y)] = E[E(xm|
y)] = E[E(xm|
|

m) + E(ym),
m) + 3E(y2
m) + 6E(y3

ym′)] = E(y2

|

Moreover, the moments of αk ∼
c2
k + ck
E(αk) =
, E(α2
b2

k) =

ck
b

Gamma(ck, b) are

D.5 Analysis of the whitening and recovery error

m) + E(ym),
m) + 7E(y2

m) + E(ym),
ym′ )] = E(ymym′),
ym′ )] = E(ymy2

|

ym)E(xm′
ym)E(x2
m′
m′) + E(y2
my2

|

m′) + E(ymym′ ),

mym′) + E(ymy2

m′ ) + E(ymym′).

, E(α3

k) =

k + 3c2
c3
k + 2ck
b3

, E(α4

k) =

k + 6c3
c4

k + 11c2
b4

k + 6ck

,

etc.

We can follow a similar analysis as in Appendix C of [15] to derive the topic recovery error given the
sample estimate error. In particular, if we deﬁne the following sampling errors ES and ET :

ES,

k ≤

S

S

k

k

−
T (u)
b

−

T (u)

k ≤ k

u

k2 ET ,

then the following form of their Lemma C.2 holds for both the LDA moments and the GP/DICA
cumulants:

b

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

ν

k

−

(maxkγk)ES

ET

k ≤

"

σK

D

2 +

σK

D

,

3

#

(34)

c

b

c

c

18

(cid:0)

(cid:1)

e

(cid:0)

(cid:1)

e

where σk(
·
cases
the GP/DICA cumulants, γk takes the simpler form γk := cum(αk)/[var(αk)]3/2 = 2/√ck.

) denotes the k-th singular value of a matrix, ν is some universal constant, and in both
c0(c0+1))
D⊤. For the LDA moments, γk = 2
ck(c0+2)2 , whereas for

D was deﬁned such that S =

q

D

e

e

e

We note that the scaling for S is O(L2) for the GP/DICA cumulants, in contrast to O(1) for the LDA
moments. Thus, to compare the upper bound (34) for the two types of moments, we need to put it in
quantities which are common. In the ﬁrst section of the Appendix C of [15], it was mentioned that
c0(c0+1) σK(D) for the LDA moments, where cmin := mink ck. In contrast, for the
σK
GP/DICA cumulants, we can show that σK
c0 σK(D), where L := c0/b is the average
≥
length of a document in the GP model. Using this lower bound for the singular vector, we thus get
the following bound in the case of the GP cumulant:

L √cmin

cmin

q

D

D

≥

e

(cid:0)

(cid:1)

(cid:0)

(cid:1)

e

W

T (

W ⊤u)

W ⊤

W T (W ⊤u)W ⊤

k

−

ν
c3/2
min "

ES
L2

2c2
0
D

σK

2 +

ET
L3

c3
0
[σK (D)]3

.

#

k ≤

(35)

b

c

c

c
The c3/2
min factor is common for both the LDA moment and GP cumulant, but as we mentioned
after Proposition 3.1, the sample error ES term gets divided by L2 for the GP cumulant, as ex-
pected.

(cid:1)(cid:3)

(cid:0)

(cid:2)

The recovery error bound in [15] is based on the bound (35), and thus by showing that the error
ES/L2 for the GP cumulant is lower than the ES term for the LDA moment, we expect to also
gain a similar gain for the recovery error, as the rest of the argument is the same for both types of
moments (see Appendix C.2, C.3 and C.4 in [15] for the completion).

E Appendix. The LDA moments

E.1 Our notation

The LDA moments were derived in [3]. Note that the full version of the paper with proofs appeared
in [15] and a later version of the paper also appeared in [31]. In this section, we recall the form of the
LDA moments using our notation. This section does not contain any novel results and is included
for the reader’s convenience. We also refer to this section when deriving the practical expressions
for computation of the sample estimates of the LDA moments in Appendix F.4.

≥

For deriving the LDA moments, a document is assumed to be composed of at least three tokens:
3. As the LDA generative model (1) is only deﬁned conditional on the length L, this is not
L
too problematic. But given that we present models in this paper which also model L, we mention
for clarity that we can suppose that all expectations and probabilities deﬁned below are implicitly
3.11 The theoretical LDA moments are derived only using the ﬁrst three words
conditioning on L
w1, w2 and w3 of a document. But note that since the words wℓ’s are conditionally i.i.d. given θ (for
w3) = E(wℓ1 ⊗
wℓ3 ) for any three distinct tokens
1
ℓ1, ℓ2 and ℓ3. The tensor M3 is thus symmetric, and could have been deﬁned using any distinct
ℓ1, ℓ2 and ℓ3 that are less than L. To highlight this arbitrary choice and to make the links with the
U-statistics estimator presented later, we thus use generic distinct ℓ1, ℓ2 and ℓ3 in the deﬁnition of
the LDA moments below, instead of ℓ1 = 1, ℓ2 = 2 and ℓ3 = 1 as in [3].

L), we have M3 := E(w1 ⊗

wℓ2 ⊗

w2 ⊗

≤

≥

≤

ℓ

11Note that another advantage of the DICA cumulants from Section 3.1 is that they do not require such a
somewhat artiﬁcial condition: they are well-deﬁned for any document length (even a document of length zero!).

19

(36)

(37)

(39)

(40)

(41)

(42)

Using this notation, then by the law of total expectation and the properties of the Dirichlet distribu-
tion, the non-central moments12 of the LDA model (1) take the form [3]:

,

M1 = E(wℓ1 ) = D

c
c0
M2 = E(wℓ1 w⊤ℓ2 ) =
M3 = E(wℓ1 ⊗
c0
c0 + 2

wℓ2 ⊗
[E(wℓ1 ⊗
2c3
0
c0(c0 + 1)(c0 + 2)

−

=

c0
c0 + 1
wℓ3)
wℓ2 ⊗

where

denotes the tensor product.

⊗

M1M ⊤1 +

1
c0(c0 + 1)

Ddiag (c) D⊤,

M1) + E(wℓ1 ⊗

M1 ⊗

M1 ⊗

M1 +

M1 ⊗

wℓ3 ) + E(M1 ⊗
2
c0(c0 + 1)(c0 + 2)

K

Xk=1

wℓ2 ⊗

wℓ3 )] ,

ckdk ⊗

dk ⊗

dk.

(38)

Similarly to the GP/DICA cumulants (as discussed in Appendix C.3), moving the terms in the non-
central moments (36), (37), (38), the following quantities are deﬁned

(P airs) = S := M2 −

c0
c0 + 1

M1M ⊤1 ,

LDA S-moment

(T riples) = T := M3 −

[E(wℓ1 ⊗

wℓ2 ⊗

M1) + E(wℓ1 ⊗

M1 ⊗

wℓ3 ) + E(M1 ⊗

wℓ2 ⊗

wℓ3 )]

M1 ⊗

M1 ⊗

M1.

LDA T-moment

c0
c0 + 2
2c2
0
(c0 + 1)(c0 + 2)

+

Slightly abusing terminology, we refer to the entities S and T as the “LDA moments”. They have
the following diagonal structure

S =

1
c0(c0 + 1)

ckdkd⊤k ,

K

Xk=1

T =

2
c0(c0 + 1)(c0 + 2)

ckdk ⊗

dk ⊗

dk.

K

Xk=1

Note however that this form of the LDA moments has a slightly different nature than the similar
form (11) and (13) of the GP/DICA cumulants. Indeed, the former is the result of properties of the
Dirichlet distribution, while the latter is the result of the independence of α’s. However, one can
think of the elements of a Dirichlet random vector as being almost independent (as, e.g., a Dirichlet
random vector can be obtained from independent gamma variables through dividing each by their
sum). Also, this closeness of the structures of the LDA moments and the GP cumulants can be
explained by the closeness of the respective models as discussed in Section 2.

E.2 Asymptotically unbiased ﬁnite sample estimators for the LDA moments

Given realizations wnℓ, n = 1, . . . , N , ℓ = 1, . . . , Ln, of the token random variable wℓ, we now
give the expressions for the ﬁnite sample estimates of S (39) and T (40) for the LDA model (and
E below to express
we rewrite them as a function of the sample counts xn).13 We use the notation
a U-statistics empirical expectation over the token within a documents, uniformly averaged over the
whole corpus. For example,
wℓ2 ⊗

M1) := 1
N

wℓ2 ⊗

wℓ1 ⊗

1
Ln(Ln

Ln
ℓ1=1

N
n=1

b

1)

−

Ln
ℓ2=1
=ℓ1
ℓ2

12Note, the difference in the notation for the LDA moments in papers [3] and [4]. In [3], M1 = E(wℓ1 ),
M2 = E(wℓ1 ⊗ wℓ2 ), and M3 = E(wℓ1 ⊗ wℓ2 ⊗ wℓ3 ). However, in [4], M2 is equivalent to S in our notation
and to P airs in the notation of [3]; similarly, M3 is T in our notation or T riples in the notation of [3].

c

13Note that because non-linear functions of

is biased, i.e., E(
unbiased. This is in contrast with the estimator for the GP/DICA moments which is easily made unbiased.

S) 6= S. The bias is small though: kE(
b

S (43) and
T (44), the estimator
b
b
S)−Sk = O(1/N ) and the estimator is asymptotically
b

M1 appear in the expression for
c

P

P

P

E(wℓ1 ⊗
b

20

M1.

c

S :=

T :=
b

b

+

M1

c0
M ⊤1 ,
M2 −
c0 + 1
c0
E(wℓ1 ⊗
M3 −
c
c
c
c0 + 2
2c2
b
c
0
M1 ⊗
(c0 + 1)(c0 + 2)
c

h

wℓ2 ⊗

M1) +

E(wℓ1 ⊗
b

M1 ⊗
c

wℓ3) +

E(

M1 ⊗
c

b

wℓ2 ⊗

wℓ3 )
i

(44)

c
where, as suggested in [4], unbiased U-statistics estimates of M1, M2 and M3 are:

M1 :=

E(wℓ) =

wnℓ =

[δ1]nxn =

Xδ1,

(45)

N

Ln

N

1
N

wnℓ1 w⊤nℓ2

Xℓ=1

1
Ln(Ln −

1)

n=1
X
Ln

Ln

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

c
M1,

M1 ⊗
c

1
N

1
Ln

N

n=1
X

n=1
X
1
N

c
M2 :=

b
E(wℓ1 w⊤ℓ2 ) =

c

b
1
N

1
N

1
N

=

=

=

N

n=1
X
N

(cid:2)

[δ2]n

xnx⊤n −

 

wnℓw⊤nℓ

!

Ln

Xℓ=1

[δ2]n

xnx⊤n −
n=1
X
(cid:0)
Xdiag(δ2)X ⊤

−

diag(xn)

(cid:1)
diag(Xδ2)

,

1
N

(cid:3)

(43)

(46)

(47)

δ3n

Ln

Xℓ=1

M

m=1
X
em1 ⊗

M3 :=

c

E(wℓ1 ⊗
b

wℓ2 ⊗

wℓ3 ) =

1
N

N

Ln

Ln

Ln

n=1
X

Xℓ1=1

Xℓ2=1
=ℓ1
ℓ2

Xℓ3=1
=ℓ2
ℓ3
=ℓ1
ℓ3

wnℓ1 ⊗

wnℓ2 ⊗

wnℓ3

[δ3]n

xn ⊗

xn ⊗

xn −

 

wnℓ ⊗

wnℓ ⊗

wnℓ

=

1
N

N

n=1
X

Ln

Ln

Xℓ2=1
=ℓ1
ℓ2
N

−

Xℓ1=1

=

1
N

n=1
X

M

M

−

m1=1
X

m2=1
X

(wnℓ1 ⊗

wnℓ1 ⊗

wnℓ2 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ1 + wnℓ1 ⊗

wnℓ2 ⊗

wnℓ2 )

[δ3]n

xn ⊗

xn ⊗

 

xn + 2

xnm(em ⊗

em ⊗

em)

xnm1 xnm2 (em1 ⊗

em2 + em1 ⊗

em2 ⊗

em1 + em1 ⊗

em2 ⊗

em2)

.





!
(48)

Here, the vectors δ1, δ2 and δ3 ∈
2!
[δ2]n =

Ln
2

−

1

RN are deﬁned as [δ1]n := L−

1

n ; [δ2]n := (Ln(Ln −

1))−

1, i.e.,

is the number of times to choose an ordered pair of tokens out of Ln tokens;

(cid:1)

i
h(cid:0)
is the number of times to choose an
[δ3]n := (Ln(Ln −
ordered triple of tokens out of Ln tokens. Note that the vectors δ1, δ2, and δ3 have nothing to do
with the Kronecker delta δ.

1, i.e., [δ3]n =

1)(Ln −

2))−

Ln
3

h(cid:0)

3!

i

(cid:1)

−

1

For a vector a
matrix A

∈
RM
×

RN , we sometimes use notation [a]n to denote its n-th element. Similarly, for a
N we use notation [A]mn to denote its (m, n)-th element.

∈

21

∈

c

There is a slight abuse of notation in the expressions above as wℓ is sometimes treated as a random
E(wℓ1 w⊤ℓ2), etc.) and sometimes as its realization. However, the difference
E(wℓ),
variable (i.e., in
is clear from the context.
b

b

F Appendix. Practical aspects and implementation details

F.1 Whitening of S and dimensionality reduction

The algorithms from Section 4 require the computation of a whitening matrix W of S. Due to
the similar diagonal structure ((41) and (11)) of the matrix S for both the LDA moments (39) and
the GP/DICA cumulants (10), the computation of a whitening matrix is exactly the same in both
cases.

By a whitening matrix, we mean a matrix W
whiten S

RK
M , but also reduces its dimensionality such that14 W SW ⊤ = IK.

M (in practice, M

RM

≫

∈

×

×

K) that does not only

Let S = U ΣU ⊤ be an orthogonal eigendecomposition of the symmetric matrix S. Let Σ1:K denotes
the diagonal matrix that contains the largest K eigenvalues15 of S on its diagonal and let U1:K be a
matrix with the respective eigenvalues in its columns. Then, a whitening matrix is

(49)

(50)

W = Σ†

1/2
1:K U ⊤1:K,

b
1/2
1:K

W :=

Σ†

U ⊤1:K,

c

b

b

1/2
1:K is a diagonal matrix constructed from Σ1:K by taking the inverse and the square root

where Σ†
of its non-zero diagonal values († stands for the pseudo-inverse).

In practice, when only a ﬁnite sample estimator
W of W can be introduced
estimator

S of S is available, the following ﬁnite sample

where

S =

U

Σ

U ⊤.

b

b

b

b

F.2 Computation of the ﬁnite sample estimators of the GP/DICA cumulants

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see
Appendix C.4 for the deﬁnition of
W ⊤ for the GP/DICA models. The construction
S (29) is
of the ﬁnite sample estimator
straightforward.

W is discussed in Appendix F.1, while the computation of

T ) of

T (v)

W

c

b

c

b

c

b

14Note that such a whitening matrix W ∈ RK×M is not uniquely deﬁned as left multiplication by any orthog-
W ⊤ = V W SW ⊤V ⊤ =
f

onal matrix V ∈ RK×K does not change anything. Indeed, let
IK.

W = V W , then
f

W S
f

15We mean the largest non-negative eigenvalues. In theory, S have to be PSD. In practice, when we deal with
ﬁnite number of samples, respective estimate of S can have negative eigenvalues. However, for K sufﬁciently
small, S should have enough positive eigenvalues. Moreover, it is standard practice to use eigenvalues of S for
estimation of a good value of K, e.g., by thresholding all negative and close to zero eigenvalues.

22

By plugging the deﬁnition of the tensor
RM :
a vector, we obtain for a given v

∈

b

T (30) in the formula (16) for the projection of a tensor onto

T (v)

=

cum(xm1 , xm2 , xm3)vm3 + 2

δ(m1, m2, m3)

E(xm3 )vm3

m1m2

i

h

b

m3
X

b

d
δ(m2, m3)

cov(xm1 , xm2 )vm3

δ(m1, m3)

d
cov(xm1 , xm2 )vm3

δ(m1, m2)

d
cov(xm1 , xm3 )vm3

m3
X

m3
X

m3
X

m3
X

−

−

−

=

−

=

W k1⊤

b
c
Wk2
T (v)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

cum(xm1 , xm2 , xm3)vm3 + 2δ(m1, m2)

d

E(xm1 )vm1

m3
X
cov(xm1 , xm2 )vm2 −

d

cov(xm1 , xm2 )vm1 −

b
δ(m1, m2)

cov(xm1 , xm3 )vm3 .

m3
X

d

d
This gives the following for the expression

d

W

T (v)

W ⊤:

cum(xm1 , xm2 , xm3 )vm3

Wk1m1

Wk2m2

c

d
δ(m1, m2)

E(xm1 )vm1

c
Wk1m1

c
Wk2m2

=

b
c
m1,m2,m3
X

+ 2

m1,m2
X

−

−

−

m1,m2
X

m1,m2
X

m1,m3
X

cov(xm1 , xm2 )vm2

b

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm2 )vm1

c
Wk1m1

c
Wk2m2

d
cov(xm1 , xm3 )vm3

c
Wk1m1

c
Wk2m1 ,

d

c

c

where
Wk denotes the k-th row of
for the unbiased ﬁnite sample estimates of

cov and

cum, we further get

W as a column vector. By further plugging in the expressions (31)

c

W

T (v)

W ⊤

h

c

b

c

k1k2
i

=

(N

+ 2

c

N
1)(N

2)

−
−
E(xm)vm

n D
X
Wk1m

E(x)

d

d
Wk1 , xn −
c
Wk2m

b

E(x)

Wk2 , xn −
c

b

E D

E D

v, xn −

E(x)
E

b

m
X
1

−

N

−

N

−

N

−
1

−
1

−

b
1

1

1

E D
E(x)

E(x)

c
c
Wk1 , xn −
c
v
◦

b
Wk1 , xn −
c
Wk1 ◦
c

b
Wk2 , xn −
c

b

n D
X

n D
X

n D
X

v

◦

E D
E(x)

Wk2 , xn −
c
Wk2 , xn −
c

v, xn −

E D

E(x)
E
b
E(x)
E
b
E(x)
E

b

,

N
where
denotes the elementwise Hadamard product. Introducing the counts matrix X
where each element Xmn is the count of the m-th word in the n-th document (note, the matrix X

∈

◦

×

RM

23

contain the vector xn in the n-th column), we further simplify the above expression

W

T (v)

W ⊤ =

W X)diag[X ⊤v](
(

W X)⊤

c

b

c

c
v,

E(x)

2N (

W

E(x))(
c

W

E(x))⊤

W X)(
(

W X)⊤

D
W X(X ⊤v)(

E h

b

W

E(x))⊤ +
c

b

c

b
E(x)(
W

c
W X(X ⊤v))⊤

c

i

−

c

b

c

b

c

W X)(
(
b

c

W diag(v)X)⊤ + (

W diag(v)X)(

W X)⊤ +

W diag[X(X ⊤v)]

W ⊤

E(x))(
c

W diag[v]

E(x))⊤ + (

c

W diag[v]

E(x))(
c

W

E(x))⊤
c

c
W diag[

E(x)]
b

W ⊤.

c

b

c

b

i

i

N
1)(N
N
1)(N
N
1)(N

◦

h
c
W
(

(N

+

(N

−

−

2)

−

2)

−

−

(N

−
W diag[v

+ 2

2)
−
h
E(x)]
W ⊤
c

−

+

+

1
c
N
−
N

N

N

−
N

−

1

1

1

h

D

c
v,

E(x)
b
E

A more compact way to write down expression (51) is as follows

b

c

b

c

W

T (v)

W ⊤ =

c

b

c

+

N
1)(N

2)

T1 +
−
h
T5 + T ⊤5 −
h

T6 −

(N
1

−

N

1

−

v,
h

E(x)
i

(T2 −

T3)

−

(T4 + T ⊤4 )
i

b
T ⊤6 +

W diag(a)

W ⊤

,

c

i

c

where

i

c

(51)

(52)

W X)⊤,

T1 = (

W X)diag[X ⊤v](
E(x))(

W

E(x))⊤,
c

b
E(x))⊤,

W

T4 =

T3 = (

T2 = 2N (
c
W X)⊤,
W X)(
b
c
c
W
W X(X ⊤v)(
c
W X)(
c
W diag(v)
c
c
1)[v
a = 2(N
c

T6 = (

T5 = (

◦
b

−

c
W diag(v)X)⊤,
b
c
E(x))(

W
E(x)] +
c
b

E(x))⊤,

F.3 Computational complexity of the GP/DICA T-cumulant estimator (52)

b

b

b

v,
h

E(x)
E(x)
i

−

X(X ⊤v).

When computing the T-cumulant P times with the formula above, the following terms are dominant:
O(RN K)+ O(N K 2)+ O(M K), where R is the largest number of unique words (non-zero counts)
in a document over the corpus. In practice, almost always K < M < N , which gives the overall
complexity of P computations of the estimator (52) to be equal to O(P RN K)+O(P N K 2).

F.4 Computation of the ﬁnite sample estimators of the LDA moments

In this section, we present efﬁcient formulas for computation of the ﬁnite sample estimate (see Ap-
W ⊤ for the LDA model. Note that the construction of
pendix E.2 for the deﬁnition of
the sample estimator
W of a whitening matrix W is discussed in Appendix F.1). The computation of
S (43) is straightforward. This approach to efﬁcient implementation was discussed in [4], however,
to the best of our knowledge, the ﬁnal expressions were not explicitly stated before. All derivations
are straightforward, but quite tedious.
b

T ) of

T (v)

c

c

c

W

b

b

24

RM

M

×

×

M (44) onto some

By analogy with the GP/DICA case, a projection (16) of the tensor
vector v

T

∈

RM in the LDA is
M

∈

T (v)

=

m1m2

h

b

−

i
c0
c0 + 2

M

m3=1 h
X

m3=1 h
X
c
E(wℓ1 ⊗
b

M3

vm3 +

m1m2m3
i

2c2
0
(c0 + 1)(c0 + 2)

wℓ2 ⊗

M1) +

c

E(wℓ1 ⊗
b

M1 ⊗
c

Plugging in the expression (48) for an unbiased sample estimate

m3
X

wℓ3) +

E(

c
M1 ⊗
c

c
wℓ2 ⊗

c
wℓ3 )
i

b
M3 of M3, we get

b
M1]m1 [

[

M1]m2 [

M1]m3vm3

vm3.

m1m2m3

1
N

1
N

N

n=1
X
N

n=1
X

[δ3]n

M

M



m3=1
X

i,j=1
X


M1]m1[
[

2c2
0
(c0 + 1)(c0 + 2)

−

+

T (v)

=

m1m2

i

h

b

[δ3]n

xnm1 xnm2 h
 

xn, v

+ 2

i

δ(m1, m2, m3)xnm3 vm3

c

!

m3
X

xnixnj (ei ⊗

ei ⊗

ej + ei ⊗

ej ⊗

ei + ei ⊗

ej ⊗

ej)

vm3





m1m2m3

M1]m2

M1, v

D
M
c

E

c0
c0 + 2  

c
M2]m1m2

c
M1, v

[

−

[
m3=1 (cid:16)
X
where e1, e2, . . . , eM denote the canonical vectors of RM (i.e., the columns of the identity matrix
IM ). Further, this gives the following for the expression

M1]m2vm3 + [

M2]m1m3[

M2]m2m3[

W ⊤:

T (v)

c

c

c

c

c

c

W

+

D

E

M1]m1 vm3

,

!

(cid:17)

W

T (v)

W ⊤

=

[δ3]n

xn, v

xn,

Wk1

b
c
xn,

c
Wk2

+ 2

xnmvm

Wk1m

Wk2m

 h
M

i

D

δ3n

xnixnj

E D

c
Wk1i

c
Wk2ivj +

M

E

m=1
X

Wk1i

Wk2jvi +

Wk1i

h

c

b

c

k1k2
i

i,j=1
X
Wk1 ,

M2
h

(cid:16)
Wk2

c

(cid:16)D
2c2
0
(c0 + 1)(c0 + 2)

c

c

+

Wk1 ,

c
M2v

c
M1

Wk2

c
+

c
Wk2 ,

M1,

Wk1

i
M1,

c

E
Wk1

c

D
c
M1,

c
Wk2

E D
c
M1, v

c
,

E

D

E D

E(cid:17)

c

c

c

c

where

Wk denotes the k-th row of

c
W as a column-vector. This further simpliﬁes to

c

c

c

c

D

E D

E D

E

N

n=1
X
N

1
N

1
N

n=1
X
c0
c0 + 2

−

−

+

!

c

c
Wk2jvj

(cid:17)
M2v

1
N
1
N
1
N
1
N

−

−

−

+

W

T (v)
c

W ⊤ =

W X)diag
(

(X ⊤v)
c

◦

δ3

W X)⊤
(

c

b

c

+

W diag
c

(cid:2)
2[(Xδ3)

v]

◦

−

(cid:3)
X[(X ⊤v)
c

δ3]

W ⊤

◦

(cid:2)

W diag[v]X)diag[δ3](
(
c

W X)⊤

(cid:3)

c

W X)diag[δ3](
(
c

W diag[v]X)⊤
c

c0
c
c0 + 2

hD
2c2
0
(c0 + 1)(c0 + 2)

c

E

M1, v

W
(
c

M2

W ⊤) + (

W (

M2v))(

W

M1)⊤ + (

W

M1)(

W (

M2v))⊤

c
c
M1, v

c
W
(

c
M1)(

W

c
M1)⊤.

c

c

c

c

c

c

i

(53)

A more compact representation gives:

D

c

E

c

c

c

c

W

T (v)

W ⊤ =

c

b

c

+

1
N

T3 −

T1 + T2 −
(cid:2)
2c2
0
(c0 + 1)(c0 + 2) h

T ⊤3

−

(cid:3)
W
(
i

M1, v

M1)(

c0
c0 + 2

W
(
i

M1, v
h
h
c
M1)⊤,
W

c

c

c

M2

W ⊤) + T4 + T ⊤4

i

(54)

c

c

c
25

c

c

where

◦
v]

T1 = (

W X)diag

(X ⊤v)

δ3

W X)⊤,
(

T2 =

T3 = [

T4 = [

◦

2[(Xδ3)
(cid:2)

W diag
c
−
W diag(v)X]diag(δ3)(
c
W (
c

M2v)](

M1)⊤.

W

(cid:2)

c

X[(X ⊤v)
(cid:3)
c
W X)⊤,

◦

δ3]

W ⊤,

(cid:3)

c

F.5 Computational complexity of the LDA T-moment estimator (54)

c

c

c

c

By analogy with Appendix F.3, the computational complexity of the T-moment is O(RN K) +
O(N K 2). However, in practice we noticed that the computation of (52) is slightly faster for larger
datasets than the computation of (54) (although the code for both was equally well optimized). This
means that the constants in O(RN K) + O(N K 2) for the LDA T-moment are, probably, slightly
larger than for the GP/DICA T-cumulant.

F.6 Estimation of the model parameters for GP/DICA model

×

∈

Below we brieﬂy discuss the recovery of the model parameters for the GP/DICA and LDA mod-
RK
M estimated in Algorithm 1. This matrix has
els from a joint diagonalization matrix A
the property that AD should be approximately diagonal up to a permutation of the columns of D.
The standard approach [3] of taking the pseudo-inverse of A to get an estimate of the topic ma-
trix D has a problem that it does not preserve the simplex constraint of the topics (in particular,
the non-negativity of
D). Due to the space constraints, we do not discuss this issue here, but we
observed experimentally that this can potentially signiﬁcantly deteriorate performance of all mo-
ment matching algorithms for topic models considered in this paper. We made an attempt to solve
this problem by integrating the non-negativity constraint into the Jacobi-updates procedure of the
orthogonal joint diagonalization algorithm, but the obtained results did not lead to any signiﬁcant
improvement. Therefore, in our experiments for both GP/DICA cumulants and LDA moments, we
estimate the topic matrix by thresholding the negative values of the pseudo-inverse of A:

e

dk := τk max(0, [A†]:k)/

max(0, [A†]:k)

k

k1,

where [A†]:k is the k-th column of the pseudo-inverse A† of A, and τk = ±1 set to
1 if [A†]:k has
more negative than positive values. This might not be the best option, and we leave this issue for the
future research.

−

b

To estimate the parameters for the prior distribution over the topic intensities αk for the DICA
model (4), we use the diagonalized form of the projected tensor from (17) and relate it to the output
diagonal elements ap for the p-th projection:

[ap]k =

zk, upi

=

zk, upi
h

=

cum(αk, αk, αk)
[var(αk)]3/2

tk
s3/2
k

tkh
e

τk

dk, W ⊤up

,

(55)

D

e

E

dk = τk max(0, [A†]:k). This formula is valid for any prior on αk in the DICA model. For the
b2 and cum(αk, αk, αk) = 2ck
b3 ,
tk in (55), and solving

√ck , which enables us to estimate ck. Plugging this value of

where
GP model (3) where αk ∼
tk = 2
e
and thus
for ck gives the following expression:

Gamma(ck, b), we have that var(αk) = ck

e

e

By replacing the quantities on the RHS with their estimated ones, we get one estimate for ck per
projection. We use as our ﬁnal estimate the average estimate over the projections:

(56)

ck =

2

.

4

dk, W ⊤up
[ap]2
k

D

E

e

1
P

P

4

dk,

W ⊤up
[ap]2
k
c

D

e

p=1
X

2

.

E

ck :=

b

26

Reusing the properties of the length of documents for the GP model as described in Appendix B.2,
we ﬁnally use the following estimates for rate parameter b of the gamma distribution:

b :=

,

c0
L
b
b

(57)

where

c0 :=

ck and

k

L is the average document length in the corpus.

b

By analogy, similar formulas for the estimation of the Dirichlet parameter c of the LDA model can
be derived and are a straightforward extension of the expression in [3].

P

b

b

b

G Appendix. Complexity of algorithms and details on the experiments

G.1 Code and complexity

Our (mostly Matlab) implementations of the diagonalization algorithms (JD, Spec, and TPM) for
both the GP/DICA cumulants and LDA moments are available online.16 Moreover, all datasets and
the code for reproducing our experiments are available.17 To our knowledge, no efﬁcient implemen-
tation of these algorithms was available for LDA. Each experiment was run in a single thread.

The bottleneck for the spectral, JD, and TPM algorithms is the computation of the cumu-
lants/moments. However, the expressions (52) and (54) provide efﬁcient formulas for fast com-
putation of the GP/DICA cumulants and LDA moments (O(RN K + N K 2), where R is the largest
number of non-zeros in the count vector x over all documents, see Appendix F.3 and F.5), which
makes even the Matlab implementation fast for large datasets. Since all diagonalization algorithms
(spectral, JD, TPM) perform the whitening step once, it is sufﬁcient to compare their complexities
by the number of times the cumulants/moments are computed.

Spectral.
O(N K(R + K)) complexity and, therefore, is the fastest.

The spectral algorithm estimates the cumulants/moments only once leading to

JD. For JD, rather than estimating P cumulants/moments separately, one can jointly estimate these
values by precomputing and reusing some terms (e.g., W X). However, the complexity is still
O(P N K(R + K)), although in practice it is sufﬁcient to have P = K or even smaller.

TPM. For TPM some parts of the cumulants/moments can also be precomputed, but as TPM nor-
mally does many more iterations than P , it can be signiﬁcantly slower. In general, the complexity
of TPM can be signiﬁcantly inﬂuenced by the initialization of the parameters of the algorithm.
There are two main parameters: Ltpm is the number of random restarts within one deﬂation step
and Ntpm is the maximum number of iterations for each of Ltpm random restarts (different from
N and L). Some restarts converge very fast (in much less than Ntpm iterations), while others are
slow. Moreover, as follows from theoretical results [4] and, as we observed in practice, the restarts
which converge to a good solution converge fast, while slow restarts, normally, converge to a worse
solution. Nevertheless, in the worst case, the complexity is O(NtpmLtpmN K(R + K)).

Note that for the experiment in Figure 1, Ltpm = 10 and Ntpm = 100 and the run with the best
objective is chosen. We believe that these values are reasonable in a sense that they provide a good
5 for the norm of the difference of the vectors from the previous and the
accuracy solution (ε = 10−
current iteration) in a little number of iterations, however, they may not be the best ones.

JD implementation. For the orthogonal joint diagonalization algorithm, we implemented a faster
C++ version of the previous Matlab implementation18 by J.-F. Cardoso. Moreover, the orthogonal
joint diagonalization routine can be initialized in different ways: (a) with the K
K identity matrix
or (b) with a random orthogonal K
K matrix. We tried different options and in nearly all cases
the algorithm converged to the same solution, implying that initialization with the identity matrix is
sufﬁcient.

×

×

Whitening matrix. For the large vocabulary size M , computation of a whitening matrix can be
expensive (in terms of both memory and time). One possible solution would be to reduce the vo-
cabulary size with, e.g., TF-IDF score, which is a standard practice in the topic modeling context.

16https://github.com/anastasia-podosinnikova/dica-light
17https://github.com/anastasia-podosinnikova/dica
18http://perso.telecom-paristech.fr/˜cardoso/Algo/Joint_Diag/joint_diag_r.m

27

min
148
JD-GP
252
JD-LDA
JD(k)-GP
157
JD(k)-LDA 264
JD(f)-GP
1628
JD(f)-LDA 2545
Spec-GP
Spec-LDA
TPM-GP
TPM-LDA 12723

101
107
1734

mean max
247
192
366
284
247
190
318
290
2058
1846
2806
2649
111
107
193
140
2726
2393
19356
16460

Table 1: The running times in seconds of the algorithms from Figure 1, corresponds to the case when N =
50, 000. Each algorithm was run 5 times, so the times in the table display the minimum (min), mean, and
maximum (max) time.

Another option is using a stochastic eigendecomposition (see, e.g., [33]) to approximate the whiten-
ing matrix.

Variational inference. For variational inference, we used the code of D. Blei and modiﬁed it for the
estimation of a non-symmetric Dirichlet prior c, which is known to be important [35]. The default
values of the tolerance/maximum number of iterations parameters are used for variational inference.
The computational complexity of one iteration for one document of the variational inference algo-
rithm is O(RK), where R is the number of non-zeros in the count vector for this document, which
is then performed a signiﬁcant number of times for each document.

G.2 Runtimes of the algorithms

In Table 1, we present the running times of the algorithms from Section 5.1. JD and JD(k) are
signiﬁcantly faster than JD(f) as expected, although the performance in terms of the ℓ1-error is
nearly the same for all of them. This indicates that preference should be given to the JD or JD(k)
algorithms.

The running time of all LDA-algorithms is higher than the one of the GP/DICA-algorithms. This
indicates that the computational complexity of the LDA-moments is slightly higher than the one
of the GP/DICA-cumulants (compare, e.g., the times for the spectral algorithm which almost com-
pletely consist of the computation of the moments/cumulants). Moreover, the runtime of TPM-LDA
is signiﬁcantly higher (half an hour vs. several hours) than the one of TPM-GP/DICA. This can be
explained by the fact that the LDA-moments have more noise than the GP/DICA-cumulants and,
hence, the algorithm is slower. Interestingly, all versions of JD algorithm are not that sensitive to
noise.

Computation of a whitening matrix is roughly 30 sec (this time is the same for all algorithms and is
included in the numbers above).

G.3 Initialization of the parameter c0 for the LDA moments

The construction of the LDA moments requires the parameter c0, which is not trivial to set in the
unsupervised setting of topic modeling, especially taking into account the complexity of the evalu-
ation for topic models [16]. For the semi-synthetic experiments, the true value of c0 is provided to
the algorithms. It means that the LDA moments, in this case, have access to some oracle informa-
tion, which in practice is never available. For real data experiments, c0 is set to the value obtained
with variational inference. The experiments in Appendix G.4 show that this choice was somewhat
important. However, this requires more thorough investigation.

G.4 The LDA moments vs parameter c0

In this section, we experimentally investigate dependence of the LDA moments on the parameter c0.
In Figure 5, the joint diagonalization algorithm with the LDA moment is compared for different
values of c0 provided to the algorithm. The data is generated similarly to Figure 2. The experiment
indicates that the LDA moments are somewhat sensitive to the choice of c0. For example, the

28

0
0.01

0.1 

1   
Parameter c0

10  

100 

0
0.01

0.1 

1   
Parameter c0

10  

100 

Figure 5: Performance of the LDA moments depending on the parameter c0. D and c are learned from the
AP dataset for K = 10 and K = 50 and true c0 = 1. JD-GP(10) for K = 10 and JD-GP(50) for K = 50.
Number of sampled documents N = 20, 000. For the error bars, each dataset is resampled 5 times. Data (left):
GP sampling; (right): LDAﬁx(200) sampling. Note: a smaller value of the ℓ1-error is better.

JD-GP(10)
JD-LDA(10)
JD-GP(90)
JD-LDA(90)

JD-GP(10)

JD-LDA(10)

JD-GP(90)

JD-LDA(90)

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
2

ℓ

0
1 

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

1

0.8

0.6

0.4

0.2

r
o
r
r
e
-
1

ℓ

0
1 

20

10
Number of docs in 1000s

30

40

50

20

10
Number of docs in 1000s

40

30

50

Figure 6: Comparison of the ℓ1- and ℓ2- errors on the NIPS semi-synthetic dataset as in Figure 2 (top, left).
The ℓ2 norms of the topics were normalized to [0,1] for the computation of the ℓ2 error.

recovery ℓ1-error doubles when moving from the correct choice c0 = 1 to the plausible alternative
c0 = 0.1 for K = 10 on the LDAﬁx(200) dataset (JD-LDA(10) line on the right of Figure 5).

G.5 Comparison of the ℓ1- and ℓ2-errors

The sample complexity results [3] for the spectral algorithm for the LDA moments allow straightfor-
ward extension to the GP/DICA cumulants, if the results from Proposition 3.1 are taken into account.
The analysis is, however, in terms of the ℓ2-norm. Therefore, in Figure 6, we provide experimental
comparison of the ℓ1- and ℓ2-errors to verify that they are indeed behaving similarly.

G.6 Evaluation of the real data experiments

For the evaluation of topic recovery in the real data case, we use an approximation of the log-
likelihood for held out documents as the metric. The approximation is computed using a Chib-style
method as described by [16] using the implementation by the authors.19 Importantly, this evaluation
methods is applicable for both the LDA model as well as the GP model. Indeed, as it follows from
Section 2 and Appendix B.1, the GP model is equivalent to the LDA model when conditioning on
the length of a document L (with the same ck hyper parameters), while the LDA model does not
make any assumption on the document length. For the test log-likelihood comparison, we thus treat
the GP model as a LDA model (we do not include the likelihood of the document length).

G.7 More on the real data experiments

The detailed experimental setup is as follows. Each dataset is separated into 5 training/evaluation
pairs, where the documents for evaluation are chosen randomly and non-repetitively among the
folds (600 documents are held out for KOS; 400 documents are held out for AP; 450 documents
are held out for NIPS). Then, the model parameters are learned for a different number of topics.
The evaluation of the held-out documents is performed with averaging over 5 folds. In Figure 3 and
Figure 7, on the y-axis, the predictive log-likelihood in bits averaged per token is presented.

19http://homepages.inf.ed.ac.uk/imurray2/pub/09etm

29

In addition to the experiments with AP and KOS in Figure 3, we demonstrate one more experiment
with the NIPS dataset in Figure 7 (right).

Note that, as the LDA moments require at least 3 tokens in each document, 1 document from the
NIPS dataset and 3 documents from the AP dataset, which did not fulﬁll this requirement, were
removed.

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-10.5

-11

-11.5

-12

-12.5

JD-GP
JD-LDA
Spec-GP
Spec-LDA
VI
VI-JD

)
s
t
i
b

n
i
(

d
o
o
h

i
l
e
k
i
l
-
g
o
L

-11

-12

-13

-14

10 

50 
Topics K

100

150

10 

50 
Topics K

100

150

Figure 7: Experiments with real data. Left: the KOS dataset. Right: the NIPS dataset. Note: a higher value of
the log-likelihood is better.

Importantly, we observed that VI when initialized with the output of the JD-GP is consistently better
in terms of the predictive log-likelihood. Therefore, the new algorithm can be used for more clever
initialization of other LDA/GP inference methods.

We also observe that the joint diagonalization algorithm for the LDA moments is worse than the
spectral algorithm. This indicates that the diagonal structure (41) and (42) might not be present in the
sample estimates (43) and (44) due to either model misspeciﬁcation or to ﬁnite sample complexity
issues.

Supplementary References

[31] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for

latent Dirichlet allocation. Algorithmica, 72(1):193–214, 2015.

[32] B.A. Frigyik, A. Kapila, and M.R. Gupta. Introduction to the Dirichlet distribution and related

processes. Technical report, University of Washington, 2010.

[33] N. Halko, P.-G. Martinsson, and J.A. Tropp. Finding structure with randomness: probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288,
2011.

[34] T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):455–

[35] H.M. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: why priors matter. In NIPS,

500, 2009.

2009.

30

