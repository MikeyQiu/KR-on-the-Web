A Feature-Enriched Neural Model for Joint Chinese Word Segmentation
and Part-of-Speech Tagging

Xinchi Chen, Xipeng Qiu∗, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{xinchichen13,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Recently, neural network models for natural
language processing tasks have been increas-
ingly focused on for their ability of alleviat-
ing the burden of manual feature engineering.
However, the previous neural models cannot
extract the complicated feature compositions
as the traditional methods with discrete fea-
In this work, we propose a feature-
tures.
enriched neural model for joint Chinese word
segmentation and part-of-speech tagging task.
Speciﬁcally, to simulate the feature templates
of traditional discrete feature based models,
we use different ﬁlters to model the complex
compositional features with convolutional and
pooling layer, and then utilize long distance
dependency information with recurrent layer.
Experimental results on ﬁve different datasets
show the effectiveness of our proposed model.

1

Introduction

Chinese word segmentation and part-of-speech (POS)
tagging are two core and fundamental tasks in Chinese
natural language processing (NLP). The state-of-the-
art approaches are based on joint segmentation and tag-
ging (S&T) model, which can be regarded as character
based sequence labeling task. The joint model can alle-
viate the error propagation problem of pipeline models.
Previously, the traditional hand-crafted feature based
models have achieved great success on joint S&T task
(Jiang et al., 2008; Kruengkrai et al., 2009; Qian et al.,
2010; Zhang and Clark, 2008, 2010). Despite of their
success, their performances are easily affected by fol-
lowing two limitations.

The ﬁrst is model complexity. Since the decoding
space of joint S&T task is relatively large, the tradi-
tional models often rely on millions of discrete fea-
tures. Therefore, the efﬁciency of joint S&T models
is rather low. Moreover, these models suffer from data
sparsity. Recently, some neural models (Huang et al.,
2015; Chen et al., 2015a; Ma and Hovy, 2016) are pro-
posed to reduce the efforts of feature engineering and
the model complexity. However, these neural models

∗Corresponding author.

Figure 1: An example. CRF makes mistakes on words “re-
form” and “simplify”. The red tags with strikethrough lines
indicate the wrong predictions.

just concatenate the embeddings of the context charac-
ters, and feed them into neural network. Since the con-
catenation operation is relatively simple, it is difﬁcult
to model the complicated features as the traditional dis-
crete feature based models. Although the complicated
interactions of inputs can be modeled by the deep neu-
ral network, the previous neural models show that the
deep model cannot outperform the one with a single
non-linear model.

The second is long term dependency. Unlike pure
POS tagging task which can utilize contextual fea-
tures on word level, joint S&T task usually extracts
the contextual features on character level. Thus, the
joint model need longer dependency on character level.
As the example shown in Figure 1, conditional ran-
dom ﬁeld (CRF) model makes mistakes on words “re-
form” and “simplify” since it is hard for CRF to disam-
biguate the POS tags without using long distance infor-
mation. However, restricted by model complexity and
data sparsity, a larger window size (greater than 5) will
instead hurt the performance. Therefore, how to ex-
ploit the long distance information without increasing
the model complexity is crucial to joint S&T task.

In order to address these two problems, we propose
a feature-enriched neural model for joint S&T task,
which consists of several key components: (1) a con-
volutional layer to simulate compositional features as
complex hand-crafting features; (2) a pooling layer to
select the most valuable features; (3) a bi-directional
long short-term memory (BLSTM) layer on the top to
carry long distance dependency information. In addi-
tion, we introduce a highway layer (Srivastava et al.,
2015) to increase the depth of architecture and obtain
more sophisticated feature representation without suff-
tering from the problem of gradient vanishing, leading
to fast convergence.

7
1
0
2
 
l
u
J
 
2
 
 
]
L
C
.
s
c
[
 
 
2
v
4
8
3
5
0
.
1
1
6
1
:
v
i
X
r
a

set and d is the dimensionality of the character embed-
dings. For a given sentence c1:n of length n, the ﬁrst
step is to lookup embeddings of the characters in the
current window slide ci−(cid:98) k−1
2 (cid:101) for the current
character ci which is going to be tagged, where k is a
hyper-parameter indicating the window size. By con-
catenating the embeddings, we get the representation
xi for the current character ci.

2 (cid:99):i+(cid:100) k−1

2.2 Encoding Phase

Usually, we apply a linear transformation followed by
a non-linear function to the current input xi:

hi = g(Wh

(cid:124) × xi + bh),

(1)

where Wh ∈ Rkd×h and bh ∈ Rh is the trainable
parameters, and h is the dimensionality of the hid-
den layer, g(·) is a non-linear function which could be
sigmoid(·), tanh(·), etc.

Then, we could get the score vector pi ∈ R|T | for
each possible tags of current character ci by applying a
linear transformation layer to the hidden layer hi:

pi = Wp

(cid:124) × hi + bp,

(2)

where Wp ∈ Rh×|T | and bp ∈ R|T | is the trainable
parameters, and T is the joint tag set.

2.3 Decoding Phase

The decoding phase aims to select the best tag sequence
ˆt1:n, to maximize the reward function r(·):

r(t1:n) =

(cid:0)Ati−1ti

(cid:1) +

(pi[ti]) ,

(3)

n
(cid:88)

i=2

n
(cid:88)

i=1

ˆt1:n = arg max
t1:n∈T(c1:n)

r(t1:n),

(4)

where A ∈ R|T |×|T | is the transition parameter, in-
dicating how possible a label will transfer to another.
T(c1:n) indicates all possible tag sequences for sen-
tence c1:n.

Also, we employ the Viterbi algorithm (Forney Jr,
1973) to decode the best tag sequence in polynomial
time complexity.

3 A Feature-Enriched Neural Model for

Joint S&T

The simple neural model presented above achieves
good results on the joint S&T task. However, the sim-
ple neural model, who concatenates the embeddings of
contextual characters as features, is not as strong as
models based on the hand-crafted features. Thus, a
simple shallow neural is insufﬁcient to tackle with am-
biguous cases which rely on more sophisticated feature
combinations and long distance dependencies.

To deal with these issues, we propose a feature-
enriched neural model for joint S&T task, which con-
sists of three different types of neural layers, stacked

Figure 2: General neural network based architecture for joint
S&T. The solid arrow denotes that there is a weight matrix on
the link, while the dashed one denotes none.

Our contributions could be summaries as follows:

1. We propose a customized neural architecture for
joint S&T task, in which each component is de-
signed according to its speciﬁc requirements, in-
stead of a general deep neural model.

2. Our model can alleviate two crucial problems:
model complexity and long term dependency in
joint S&T task.

3. We evaluate our model on ﬁve different datasets.
Experimental
show that our model
achieves comparable performance to the previous
sophisticated feature based models, and outper-
forms the previous neural models.

results

2 Neural Models for Joint S&T

The joint S&T task is usually regraded as a character
based sequence labeling problem.

In this paper, we employ the {B M E S} tag set TSEG
(indicating the Begin, Middle, End of the word, and a
Single character word respectively) for word segmen-
tation and the tag set TP OS (varies from dataset to
dataset) for POS tagging.

The tag set T of our joint S&T task would be the
cross-label set of TSEG and TP OS as illustrated in Fig-
ure 1.

Conventional neural network based model for se-
quence labeling task usually consists of three phases.
Figure 2 gives the illustration.

2.1 Lookup Table Phase

In order to represent characters as distributed vectors,
we usually apply a feed-forward neural layer on the top
of the one-hot character representations. The param-
eter matrix of the neural layer is called character em-
bedding matrix E ∈ R|C|×d, where C is the character

Figure 3: The proposed feature-enriched neural model for joint S&T task. The solid arrow denotes that there is a weight matrix
on the link, while the dashed one denotes none.

one by one:
(1) Convolutional layer; (2) Highway
layer; (3) Recurrent layer. Figure 3 gives the illustra-
tion.

Hence, after convolutional layer, we would rep-
the given input sequence X ∈ Rn×d =

resent
(cid:124)
[x1, x2, . . . , xn]

as ˆX = Cov(X).

3.1 Convolutional Layer

3.2 Highway Layer

The simple neural model is just to concatenate the em-
beddings of characters in a local context, which cannot
simulate the carefully designed features in traditional
models.

To better model the complex compositional features
as conventional feature based models, we use convolu-
tion layer to separately model different n-gram features
for each character. Thus the feature of each charac-
ter is the concatenation of corresponding columns of
all different feature map sets. Then we apply a k-max
pooling layer to select the most signiﬁcant signals.

Concretely, we model uni-gram, bi-gram, . . . , Q-
gram features by generating feature map sets ˆz1, ˆz2,
. . . , ˆzQ correspondingly. Formally, the q-gram feature
map set ˆzq is:

(cid:124)

cov

ˆzq
i = tanh(Wq

2 (cid:99):i+(cid:100) q−1

×xi−(cid:98) q−1

2 (cid:101)+b), i ∈ [1, n],
(5)
cov ∈ Rqd×lq is the convolutional ﬁlter
where Wq
2 (cid:101) ∈
for q-gram feature map set, and xi−(cid:98) q−1
Rqd is the concatenation of embeddings of characters
2 (cid:101). Here, lq is the number of feature
ci−(cid:98) q−1
maps in q-gram feature map set and b ∈ Rlq is a bias
parameter. For marginal cases, we use wide convolu-
tion strategy, which means we receive the sequence in
the same length as input by padding zeros to the input.
Then, we would represent the original sentence by

2 (cid:99):i+(cid:100) q−1

2 (cid:99):i+(cid:100) q−1

concatenation operation as z ∈ Rn×(cid:80)Q

q=1 lq :

zi = ⊕Q

q=1ˆzi,

(6)

where operator ⊕ is the concatenation operation.

After taking the k-max pooling operation, the repre-
sentation of original sentence would be ˆX ∈ Rn×d =
[ˆx1, ˆx2, . . . , ˆxn]

, where ˆxi is:

(cid:124)

ˆxi = k max zi, k = d.

(7)

Highway layer (Srivastava et al., 2015) aims to keep
gradient in very deep neural network. By introducing
highway layer, we could simulate more complex com-
positional features by increasing the depth of our ar-
chitecture. In addition, highway layer speeds up con-
vergence speed and alleviates the problem of gradient
vanishing.

As described above, we would represent the input se-
quence as ˆX = Cov(X) after the convolutional layer.
By additionally adding the highway layer, the represen-
tation of the input sequence would be ˆX as:

ˆX = Cov(X) (cid:12) T (X) + X (cid:12) C(X),

(8)

where operator (cid:12) indicates the element-wise multipli-
cation operation. The T (·) is the transform gate and
C(·) is the carry gate. We adopt a simple version,
where we set C(·) = 1 − T (·). Transform gate T (·)
could be formalized as:

T (X) = σ(WT

(cid:124) × X + bT ),

(9)

where WT ∈ Rd×d and bT ∈ Rd are trainable pa-
rameters. Here σ is the sigmoid function.

3.3 Recurrent Layer

In joint S&T task, it usually relies on long distance
dependency and sophisticated features to disambiguate
lots of cases. Thus, a simple shallow neural model is
insufﬁcient to capture long distance information.

Inspired by recent works using long short-term
memory (LSTM) (Hochreiter and Schmidhuber, 1997)
neural networks, we utilize LSTM to capture the long-
term and short-term dependencies. LSTM is an exten-
sion of the recurrent neural network (RNN) (Elman,
1990), which aims to avoid the problems of gradient
vanishing and explosion, and is very suitable to carry
the long dependency information.

hi =

−→
h i ⊕

←−
h i,

li(θ) =

(13)

; θ) + ∆(t∗(i)
1:ni

, t(i)
1:ni

))

(r(t(i)
1:ni
)

max
∈T(c(i)
1:ni

t(i)
1:ni
− r(t∗(i)
1:ni

; θ).

By further adding LSTM layer on the top of
ˆX ∈ Rn×d = [ˆx1, ˆx2, . . . , ˆxn], we would repre-
sent sentence c1:n as H ∈ Rn×h = LSTM( ˆX) =
[h1, h2, . . . , hn]. Speciﬁcally, LSTM layer introduces
memory cell c ∈ Rh which controlled by input gate
i ∈ Rh, forget gate f ∈ Rh and output gate o ∈ Rh.
Thus, each output hi ∈ Rh would be calculated as:













=













σ
σ
σ
φ

ii
oi
fi
ˆci

(cid:18)

Wg

(cid:20)

(cid:124)

ˆxi
hi−1

(cid:21)

(cid:19)

+ bg

,

(10)

ci = ci−1 (cid:12) fi + ˆci (cid:12) ii,
hi = oi (cid:12) φ(ci),

(11)

(12)

where Wg ∈ R4h×(d+h) and bg ∈ R4h are trainable
parameters. Here, the hyper-parameter h is dimension-
ality of i, o, f , c and h. σ(·) is sigmoid function and
φ(·) is tanh function.

BLSTM We also employ the bi-directional LSTM
(BLSTM) neural network. Speciﬁcally, each hidden
state of BLSTM is formalized as:

where operator ⊕ indicates concatenation operation.
←−
h i are hidden states of forward and
Here,
backward LSTMs respectively.

−→
h i and

4 Training

We employ max-margin criterion (Taskar et al., 2005)
which provides an alternative to probabilistic based
methods by optimizing on the robustness of decision
boundary directly.

In the decoding phase, if the predicted tag sequence
1:ni with the maximal

for the i-th training sentence c(i)
score is ˆt(i)

1:ni:

ˆt(i)
1:ni

= arg max
∈T(c(i)
1:ni

t(i)
1:ni

)

r(t(i)
1:ni

; θ),

(14)

the goal of the max-margin criterion is to maximize
the score of the gold tag sequence t∗(i)
1:ni with
1:ni
a margin to any other possible tag sequence t(i)
∈
1:ni
T(c(i)
1:ni

= ˆt(i)

):

r(t∗(i)
1:ni

; θ) ≥ r(t(i)
1:ni

; θ) + ∆(t∗(i)
1:ni

, t(i)
1:ni

),

(15)

∆(t∗(i)
1:ni

, t(i)
1:ni

) =

η1{t∗(i)
j

(cid:54)= t(i)

j },

(16)

ni(cid:88)

j=1

CTB

PKU

NCC

Splits

Sighan 2008

Datasets Splits
Train
Test
Train
Test
Train
Test
Train 1-270, 400-1151
Dev
Test
Train
Test

301-325
271-300
1-4197
4198-4411

CTB-7

CTB-5

AVGw Nsentence
23,444
2,079
66,691
6,424
18,869
3,595
18,086
350
348
41,266
10,181

27.4
28.8
16.7
24.3
28.4
28.5
27.3
19.4
23.0
24.0
20.6

|DW |
42k
10k
55k
18k
45k
18k
37k
2k
2k
52k
21k

Table 1: Details of ﬁve datasets. DW is the dictionary of
distinct words. Nsentence indicates the number of sentences.
AVGw is the average word number in a sentence.

Thus, the object is to minimize objective function

J(θ) for m training examples (c(i)
1:ni

, t∗(i)
1:ni

)m
i=1:

J(θ) =

1
m

m
(cid:88)

i=1

li(θ) +

(cid:107)θ(cid:107)2
2,

λ
2

(17)

(18)

5 Experiments

5.1 Datasets

We evaluate proposed architecture on ﬁve datasets:
CTB, PKU, NCC, CTB-5, CTB-7. Table 1 gives the
details of ﬁve datasets. We use the ﬁrst 10% data of
shufﬂed train set as development set for CTB, PKU,
NCC and CTB-7 datasets.

• CTB, PKU and NCC datasets are from the
POS tagging task of the Fourth International Chi-
nese Language Processing Bakeoff (Jin and Chen,
2008).

• CTB-5 dataset is the version of Penn Chinese
Treebank 5.1, following the partition criterion of
(Jin and Chen, 2008; Jiang et al., 2009; Sun and
Wan, 2012)

• CTB-7 dataset is the version of Penn Chinese
Treebank 7.0. It consists of different sources of
documents (newswire, magazine articles, broad-
cast news, broadcast conversations, newsgroups
and weblogs). Since the web blogs are very dif-
ferent with news texts, we try to evaluate the ro-
bustness of our model by testing on web blogs and
training on the rest of dataset.

where ∆(t∗(i)
) is the margin function and
1:ni
hyper-parameter η is a discount parameter. Here, θ de-
notes all trainable parameters of our model.

, t(i)
1:ni

5.2 Hyper-parameters

Table 2 gives the details of hyper-parameter settings.
Note that we set window size k = 1 which means

Window size
Character embedding size
Initial learning rate
Margin loss discount
Regularization
LSTM dimensionality
Number of feature map sets

Size of each feature map set ˆf q lQ

Batch size

Table 2: Hyper-parameter settings.

k = 1
d = 50
α = 0.2
η = 0.2
λ = 10−4
h = 100
Q = 5
q=1 = 100
20

we only take the current character embedding into ac-
count instead of using window slice approach. Accord-
ing to experiment results, we ﬁnd it is a tradeoff be-
tween model performance and efﬁciency to only use {
uni-gram, bi-gram, . . . , 5-gram } convolutional feature
map sets. Besides, we set sizes of all feature map sets
consistently for simplicity. Following previous work
(Pei et al., 2014; Chen et al., 2015b), we also adopt
bigram-character embedding in this paper.

5.3 Effects of Components

We experiment several models by using different neu-
ral component layers as shown in Table 3. The model
incorporating convolutional layer, pooling layer, high-
way layer, and BLSTM layer, achieves the best perfor-
mance on F1 score (90.39) on test set of CTB dataset.
Therefore, we would like to compare our approach with
other previous works using this topology.

Notably, the conventional model using window slice
approach (Figure 2) for joint S&T task can be viewed
as a special case of our model when we only adopt a
singe convolutional layer.

5.3.1 Pooling Layer and Highway Layer

To evaluate the effectiveness of pooling layer and high-
way layer, we incrementally add pooling layer and
highway layer on the top of convolutional layer. As
shown in Table 3, by adding pooling layer, the per-
formance decrease a little for the loss of information.
However, we get the better performance on F1 score
(90.24) by additionally adding highway layer. Al-
though the performance does not beneﬁt from pooling
layer much, the pooling layer extracts the most impor-
tant features and meets the consistent dimensionality
requirement to add highway layer.
Intuitively, high-
way layer helps simulating more complex composi-
tional features by increasing the depth of architecture.

modeling features and carrying long distance informa-
tion.

By introducing convolutional

layer and highway
layer, we could further boost the performance which
beneﬁts from the feature modeling capability of convo-
lutional layer and highway layer.

5.4 Comparsion with Previous Works

We compare proposed model with several previous
works on ﬁve datasets on joint S&T task. Experimental
results are shown in Table 4.

Conditional random ﬁeld (CRF) (Lafferty et al.,
2001) is one of the most prevalent and widely used
models for sequence labeling tasks. (Qiu et al., 2013)
aims to boost the performance by exploiting datasets
with different annotation types. Multilayer perceptron
(MLP) is our implementation of (Zheng et al., 2013),
a basic neural model for joint S&T task. (Zheng et al.,
2013) is a neural model which only use one layer of
shallow feed forward neural network in the encoding
phase. Our model indicates the model with convolu-
tional layer, pooling layer, highway layer, and BLSTM
layer. “Pre-train” indicates the pre-trained character
embeddings which are trained on corresponding train
set of each dataset using word2vec toolkit (Mikolov
et al., 2013).

Result Discussion Our model outperforms the pre-
vious neural model on joint S&T task and achieves
the comparable performance with conventional hand-
crafted feature based models. As shown in Table
4, compared to other previous methods, our model
achieves the best performances on F1 scores (90.39,
90.16, 88.76, 85.31 on CTB, PKU, NCC and CTB-7
datasets respectively), and obtains comparable results
on CTB5 dataset (93.19 on F1 score). As we know, the
test set of CTB5 is very small so that previous work
might overﬁt on that dataset. In addition, according to
the experimental results, we ﬁnd that the performance
beneﬁts a lot from pre-trained character embeddings.
Intuitively, pre-trained embeddings give a more rea-
sonable initialization for the non-convex optimization
problem with huge parameter space.

It
Besides, the proposed model is quite efﬁcient.
only takes about half one hour per epoch using a small
amount of memory (to train CTB) on a single GPU.
Actually, it takes about ten hours to train our model (on
CTB).

Experiments on CTB-7, whose train set and test set
are on different domains, show the robustness of our
model.

5.3.2 Long Short-Term Memory Layer

5.5 Case Study

In this work, we introduce (B)LSTM layer to carry
the long distance dependency. To evaluate (B)LSTM
layer, we experiment different models with and without
(B)LSTM layer. As shown in Table 3, we could get a
relatively high performance by using LSTM or BLSTM
layer only, which shows the capability of (B)LSTM in

We illustrate several cases from CTB-5 dataset. As
shown in Table 5, our approach performs well on
cases with disambiguations which rely on long distance
dependency. For instance, conditional random ﬁeld

∗ (Zheng et al., 2013) only reported the results on CTB5

dataset for joint S&T task.

models

w/o CNN
CNN
CNN+Pooling
CNN+Pooling+Highway

w/o LSTM
R
-
89.16
89.00
90.34

P
-
88.24
88.51
90.14

F
-
88.70
88.76
90.24

LSTM
R
89.66
89.71
89.13
89.73

P
89.24
89.35
88.54
89.38

F
89.45
89.53
88.83
89.55

P
89.52
89.75
88.91
90.23

BLSTM
R
89.74
89.61
89.33
90.55

F
89.63
89.68
89.12
90.39

Table 3: Performances of different models on test set of CTB dataset.

P

CTB
R

NCC
R
90.51 90.23 90.37 90.00 89.12 89.56 87.93 87.24 87.58 92.85 93.24 93.05 84.64 85.86 85.24
-

CTB7
R

CTB5
R

PKU
R

F

F

F

F

P

P

P

P

F

-

-

-

-

-

(Qiu et al., 2013) 89.11 89.16 89.13 89.41 88.58 88.99

models

CRF

MLP
Ours

91.82∗ 83.60 84.53 84.06
88.11 87.29 87.69 88.22 87.74 87.98 85.80 85.66 85.73
89.48 89.63 89.56 89.82 89.55 89.68 87.30 87.76 87.53 91.78 92.88 92.33 84.02 86.26 85.13
Ours+Pre-train 90.23 90.55 90.39 90.27 90.05 90.16 88.37 89.16 88.76 92.88 93.49 93.19 84.40 86.25 85.31

93.28 93.35 93.31
-

-

Table 4: Comparisons with previous models on test sets of CTB, PKU, NCC, CTB5 and CTB7 datasets.

(CRF) model makes mistakes on words “reform” and
“simplify” since it is hard for CRF to disambiguate the
POS tags without using the long distance (wider con-
textual) information.

words. Besides, we apply pooling operation along the
feature size direction to get the most signiﬁcant fea-
tures.

6 Related Works

Recently, researches applied deep learning algorithms
on various NLP tasks and achieved impressive results,
such as chunking, POS tagging, named entity recogni-
tion for English (Collobert et al., 2011; Tsuboi, 2014;
Labeau et al., 2015; Ma and Hovy, 2016; Santos and
Zadrozny, 2014; Huang et al., 2015), and Chinese word
segmentation and POS tagging for Chinese (Zheng
et al., 2013; Pei et al., 2014; Chen et al., 2017). These
models learn features automatically which alleviate the
efforts in feature engineering. However, joint S&T is
a more difﬁcult task than Chinese word segmentation
and POS tagging since it has a larger decoding space
and need more contextual information and long dis-
tance dependency (Zhang and Clark, 2008; Jiang et al.,
2008; Kruengkrai et al., 2009; Zhang and Clark, 2010;
Sun, 2011; Qian and Liu, 2012; Zheng et al., 2013; Qiu
et al., 2013; Shen et al., 2014). Therefore, we need a
customized architecture to alleviate these problems. In
this work, we propose a feature-enriched neural model
for joint S&T task, and obtain great performance.

Besides,

there are several similar neural models
(Tsuboi, 2014; Labeau et al., 2015; Ma and Hovy,
2016; Santos and Zadrozny, 2014; Huang et al., 2015;
Kim et al., 2015) .
Instead of looking up word em-
bedding table for each word in text, they tries to di-
rectly model English words by applying convolution
layer on characters of words. Then they apply these
word presentations to other tasks, such as POS tagging,
name entity recognition, language modeling, etc. Un-
like these models, we apply convolutional operation on
sentence level, while they do within each word. There-
fore they do not capture the features involving several

7 Conclusions

In this paper, we propose a feature-enriched neural
model for joint S&T task, which better models compo-
sitional features and utilizes long distance dependency.
Experimental results show that our proposed model
outperforms the previous neural model and achieves
comparable results with previous sophisticated feature
based approaches.

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable comments. This work is partially
funded by National Natural Science Foundation of
China (No. 61532011 and 61672162), Shanghai Mu-
nicipal Science and Technology Commission on (No.
16JC1420401).

References

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In ACL.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term mem-
ory neural networks for chinese word segmentation.
In EMNLP. pages 1197–1206.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning for
chinese word segmentation. In ACL.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from

Table 5: Case study. The red tags with strikethrough lines indicate the wrong predictions, which are the results of the CRF
model. The green tags are corrected predictions made by the proposed feature-enriched neural model. The black tags are
correctly tagged by all models.

scratch. The Journal of Machine Learning Research
12:2493–2537.

Jeffrey L Elman. 1990.

Finding structure in time.

Cognitive science 14(2):179–211.

G David Forney Jr. 1973.

The viterbi algorithm.

Proceedings of the IEEE 61(3):268–278.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8):1735–
1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991 .

W. Jiang, L. Huang, and Q. Liu. 2009. Automatic adap-
tation of annotation standards: Chinese word seg-
mentation and POS tagging: a case study. In ACL.
pages 522–530.

W. Jiang, L. Huang, Q. Liu, and Y. Lu. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In ACL. Citeseer.

C. Jin and X. Chen. 2008. The fourth international
chinese language processing bakeoff: Chinese word
segmentation, named entity recognition and chi-
nese pos tagging. In Sixth SIGHAN Workshop on
Chinese Language Processing. page 69.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2015. Character-aware neural language
models. arXiv preprint arXiv:1508.06615 .

Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In ACL. Association for Computational
Linguistics, pages 513–521.

Matthieu Labeau, Kevin L¨oser, and Alexandre Al-
lauzen. 2015. Non-lexical neural architecture for
In EMNLP. Association
ﬁne-grained pos tagging.
for Computational Linguistics, Lisbon, Portugal,
pages 232–237.
http://aclweb.org/anthology/D15-
1025.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling
sequence data.
In Proceedings of the Eighteenth
International Conference on Machine Learning.

Xuezhe Ma and Eduard Hovy. 2016.

End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
arXiv preprint arXiv:1603.01354 .

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781 .

Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.

Xian Qian and Yang Liu. 2012. Joint chinese word seg-
mentation, pos tagging and parsing. In EMNLP. As-
sociation for Computational Linguistics, pages 501–
511.

Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing
Huang, and Lide Wu. 2010.
training
and decoding using virtual nodes for cascaded
segmentation and tagging tasks.
In EMNLP.
http://portal.acm.org/citation.cfm?id=1870658.1870677.

Joint

Xipeng Qiu, Jiayi Zhao, and Xuanjing Huang. 2013.
Joint chinese word segmentation and pos tagging on
heterogeneous annotated corpora with multiple task
learning. In EMNLP. pages 658–668.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
representations for part-of-speech

character-level
tagging. In ICML. pages 1818–1826.

Mo Shen, Hongxiao Liu, Daisuke Kawahara, and
Sadao Kurohashi. 2014. Chinese morphological
analysis with character-level pos tagging. In ACL.
pages 253–258.

Rupesh Kumar Srivastava, Klaus Greff, and J¨urgen
arXiv

Schmidhuber. 2015. Highway networks.
preprint arXiv:1505.00387 .

W. Sun. 2011. A stacked sub-word model for joint Chi-
nese word segmentation and part-of-speech tagging.
In ACL. pages 1385–1394.

Weiwei Sun and Xiaojun Wan. 2012. Reducing ap-
proximation and estimation errors for Chinese lex-
ical processing with heterogeneous annotations. In
ACL. pages 232–241.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In ICML.

Yuta Tsuboi. 2014.

Neural networks leverage
corpus-wide information for part-of-speech tag-
ging.
In EMNLP. Association for Computa-
tional Linguistics, Doha, Qatar, pages 938–950.
http://www.aclweb.org/anthology/D14-1101.

Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In ACL. pages 888–896.

for

Yue Zhang and Stephen Clark. 2010. A fast de-
joint word segmentation and POS-
coder
In
tagging using a single discriminative model.
EMNLP. Stroudsburg, PA, USA, pages 843–852.
http://portal.acm.org/citation.cfm?id=1870658.1870740.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for chinese word segmentation and
pos tagging. In EMNLP. pages 647–657.

