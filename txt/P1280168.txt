Scalable Gaussian Processes with Billions of Inducing Inputs via
Tensor Train Decomposition

8
1
0
2
 
n
a
J
 
7
1
 
 
]

G
L
.
s
c
[
 
 
2
v
4
2
3
7
0
.
0
1
7
1
:
v
i
X
r
a

Pavel A. Izmailov
Lomonosov Moscow State
University,
Cornell University

Alexander V. Novikov
National Research University
Higher School of Economics,
Institute of Numerical Mathematics RAS

Dmitry A. Kropotov
Lomonosov Moscow State
University

Abstract

We propose a method (TT-GP) for approx-
imate inference in Gaussian Process (GP)
models. We build on previous scalable GP
research including stochastic variational in-
ference based on inducing inputs, kernel in-
terpolation, and structure exploiting algebra.
The key idea of our method is to use Ten-
sor Train decomposition for variational pa-
rameters, which allows us to train GPs with
billions of inducing inputs and achieve state-
of-the-art results on several benchmarks. Fur-
ther, our approach allows for training kernels
based on deep neural networks without any
modiﬁcations to the underlying GP model.
A neural network learns a multidimensional
embedding for the data, which is used by
the GP to make the ﬁnal prediction. with-
out pretraining, through maximization of GP
marginal likelihood. We show the eﬃciency of
the proposed approach on several regression
and classiﬁcation benchmark datasets includ-
ing MNIST, CIFAR-10, and Airline.

1

Introduction

Gaussian processes (GPs) provide a prior over functions
and allow ﬁnding complex regularities in data. The
ability of GPs to adjust the complexity of the model
to the size of the data makes them appealing to use for
big datasets. Unfortunately, standard methods for GP
(n3) with the
regression and classiﬁcation scale as
number n of training instances and can not be applied
when n exceeds several thousands.

O

Numerous approximate inference methods have been
proposed in the literature. Many of these methods are

based on the concept of inducing inputs (Quiñonero-
Candela and Rasmussen (2005), Snelson and Ghahra-
mani (2006), Williams and Seeger (2000)). These meth-
ods build a smaller set Z of m points that serve to ap-
proximate the true posterior of the process and reduce
(nm2 + m3). Titsias (2009) pro-
the complexity to
posed to consider the values u of the Gaussian process
at the inducing inputs as latent variables and derived
a variational inference procedure to approximate the
posterior distribution of these variables. Hensman et al.
(2013) and Hensman et al. (2015) extended this frame-
work by using stochastic optimization to scale up the
method and generalized it to classiﬁcation problems.

O

Inducing input methods allow to use Gaussian processes
on datasets containing millions of examples. However,
these methods are still limited in the number of induc-
ing inputs m they can use (usually up to 104). Small
number of inducing inputs limits the ﬂexibility of the
models that can be learned with these methods, and
does not allow to learn expressive kernel functions (Wil-
son et al. (2014)). Wilson and Nickisch (2015) proposed
KISS-GP framework, which exploits the Kronecker
product structure in covariance matrices for inducing
inputs placed on a multidimensional grid in the fea-
(n + Dm1+1/D),
ture space. KISS-GP has complexity
where D is the dimensionality of the feature space.
Note however, that m is the number of points in a
D-dimensional grid and grows exponentially with D,
which makes the method impractical when the number
of features D is larger than 4.

O

In this paper, we propose TT-GP method, that can
use billions of inducing inputs and is applicable to
a much wider range of datasets compared to KISS-
GP. We achieve this by combining kernel interpolation
and Kronecker algebra of KISS-GP with a scalable
variational inference procedure. We restrict the family
of variational distributions from Hensman et al. (2013)
to have parameters in compact formats. Speciﬁcally,
we use Kronecker product format for the covariance
matrix Σ and Tensor Train format (Oseledets (2011))

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

for the expectation µ of the variational distribution
over the values u of the process at inducing inputs Z.

Nickson et al. (2015) showed that using Kronecker for-
mat for Σ does not substantially aﬀect the predictive
performance of GP regression, while allowing for com-
putational gains. The main contribution of this paper is
combining the Kronecker format for Σ with TT-format
for µ, which, together with eﬃcient inference procedure,
allows us to eﬃciently train GP models with billions
of inducing inputs.

Unlike KISS-GP the proposed method has linear com-
plexity with respect to dimensionality D of the feature
space. It means that we can apply TT-GP to datasets
that are both large and high-dimensional. Note how-
ever, that TT-GP is constructing a grid of inducing
inputs in the feature space, and tries to infer the val-
ues of the process in all points in the grid. High-
dimensional real-world datasets are believed to lie on
small-dimensional manifolds in the feature space, and it
is impractical to try to recover the complex non-linear
transformation that a Gaussian Process deﬁnes on the
whole feature space. Thus, we use TT-GP on raw fea-
tures for datasets with dimensionality up to 10. For
feature spaces with higher dimensionality we propose
to use kernels based on parametric projections, which
can be learned from data.

Wilson et al. (2016a) and Wilson et al. (2016b) demon-
strated eﬃciency of Gaussian processes with kernels
based on deep neural networks. They used subsets of
outputs of a DNN as inputs for a Gaussian process. As
the authors were using KISS-GP, they were limited to
using additive kernels, combining multiple low dimen-
sional Gaussian processes. We found that DNN-based
kernels are very eﬃcient in combination with TT-GP.
These kernels allows us to train TT-GP models on
high-dimensional datasets including computer vision
tasks. Moreover, unlike the existing deep kernel learn-
ing methods, TT-GP does not require any changes in
the GP model and allows deep kernels that produce
embeddings of dimensionality up to 10.

2 Background

2.1 Gaussian Processes

A Gaussian process is a collection of random variables,
any ﬁnite number of which have a joint normal distribu-
tion. A GP f taking place in RD is fully deﬁned by its
mean m : RD
R
functions. For every x1, x2, . . . , xn

R and covariance k : RD

RD

RD

→

→

×

f (x1), f (x2), . . . , f (xn)

(m, K),

∈

∼ N

where m = (m(x1), m(x2), . . . , m(xn))T
K

Rn, and
Rn×n is the covariance matrix with Kij =

∈

∈

k(xi, xj). Below we will use notation K(A, B) for the
matrix of pairwise values of covariance function k on
points from sets A and B.

Consider a regression problem. The dataset consists
Rn×D, and target
of n objects X = (x1, . . . , xn)T
∈
Rn. We assume that
values y = (y1, y2, . . . , yn)T
the data is generated by a latent zero-mean Gaussian
process f plus independent Gaussian noise:

∈

n
(cid:89)

p(y, f

X) = p(f
|

|

X)

p(yi

fi),
|

p(f

X) =
|
p(yi

fi) =

|

i=1
0, K(X, X)),
fi, ν2I),
|

(f

|
(yi

N

N

(1)

where fi = f (xi) is the value of the process at data
point xi and ν2 is the noise variance.

Assume that we want to predict the values of the pro-
cess f∗ at a set of test points X∗. As the joint dis-
tribution of y and f∗ is Gaussian, we can analytically
y, X, X∗) =
compute the conditional distribution p(f∗
|
ˆm, ˆK) with tractable formulas for ˆm and ˆK. The
|
N
complexity of computing ˆm and ˆK is
(n3) since it
involves calculation of the inverse of the covariance
matrix K(X, X).

(f∗

O

Covariance functions usually have a set of hyper-
parameters θ. For example, the RBF kernel

kRBF(x, x(cid:48)) = σ2

f exp (cid:0)

x

0.5
(cid:107)

−

−

x(cid:48)

2/l2(cid:1)
(cid:107)

has two hyper-parameters l and σf . In order to ﬁt
the model to the data, we can maximize the marginal
X) with respect to these
likelihood of the process p(y
|
parameters. In case of GP regression this marginal
(n3)
likelihood is tractable and can be computed in
operations.

O

fi) = 1/(1 + exp(

For two-class classiﬁcation problem we use the same
model (1) with p(yi
yifi)), where
yi
. In this case both predictive distribution
1, +1
}
and marginal likelihood are intractable. For detailed
description of GP regression and classiﬁcation see Ras-
mussen and Williams (2006).

∈ {−

−

|

2.2

Inducing Inputs

A number of approximate methods were developed to
scale up Gaussian processes. Hensman et al. (2013)
proposed a variational lower bound that factorizes over
observations for Gaussian process marginal likelihood.
We rederive this bound here.

Rm×D of m inducing inputs in the
Consider a set Z
∈
Rm representing
feature space and latent variables u
the values of the Gaussian process at these points.

∈

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Consider the augmented model

p(y, f, u) = p(y

f )p(f

|

u)p(u) =
|

p(yi

fi)p(f

u)p(u)

|

|

with

p(f

u) =
|
p(u) =

N
(u

KnmK −1
|

(f
0, Kmm),
|

N

mmu, Knn

KnmK −1

mmKmn),

n
(cid:89)

i=1

−

where Knn = K(X, X), Knm = K(X, Z), Kmn =
K(Z, X) = K T

nm, Kmm = K(Z, Z).

The standard variational lower bound is given by

log p(y)

Eq(u,f ) log

≥

p(y, f, u)
q(u, f )

=

= Eq(f ) log

p(yi

fi)
|

−

KL(q(u, f )

p(u, f )),

||

n
(cid:89)

i=1

where q(u, f ) is the variational distribution over latent
variables. Consider the following family of variational
distributions

q(u, f ) = p(f

u)
|

N

(u
|

µ, Σ),

(4)

∈

Rm and Σ

Rm×m are variational param-
where µ
eters. Then the marginal distribution over f can be
computed analytically
(cid:0)f

q(f ) =

∈

KnmK −1
mmµ,
N
Knn + KnmK −1

|

mm(Σ

Kmm)K −1

mmKmn

(cid:1) .

(5)

−

We can then rewrite (3) as

log p(y)

n
(cid:88)

i=1

≥

Eq(fi) log p(yi

fi)

|

−

KL(q(u)

p(u)). (6)

||

Note, that the lower bound (6) factorizes over observa-
tions and thus stochastic optimization can be applied
to maximize this bound with respect to both kernel
hyper-parameters θ and variational parameters µ and
Σ, as well as other parameters of the model e.g. noise
variance ν. In case of regression we can rewrite (6) in
the closed form

logp(y)

n
(cid:88)

(cid:18)

≥

i=1

1
2ν2

log |

−
(cid:18)

˜Kii

−
Kmm
Σ

|

|

|

−

1
2

−

log

(yi

i K −1
kT
|

mmµ, ν2)

−

N

1
2ν2 tr(kT

i K −1

mmΣK −1

mmki)

(cid:19)

−

m + tr(K −1

mmΣ) + µT K −1

mmµ

(cid:19)

,

(7)

where ki
∈
˜K = Knn
−

Rm is the i-th column of Kmn matrix and
KnmK −1

mmKmn.

At prediction time we can use the variational distribu-
tion as a substitute for posterior

(cid:90)

p(f∗

y) =
|

p(f∗

y)df du
f, u)p(f, u
|

|

≈

(cid:90)

≈

p(f∗

f, u)q(f, u)df du =
|

p(f∗

u)q(u)du.

|

(cid:90)

(2)

(3)

O

(nm2 +
The complexity of computing the bound (7) is
m3). Hensman et al. (2015) proposes to use Gauss-
Hermite quadratures to approximate the expectation
term in (6) for binary classiﬁcation problem to obtain
(nm2+m3). This
the same computational complexity
complexity allows to use Gaussian processes in tasks
with millions of training samples, but these methods
are limited to use small numbers of inducing inputs
m, which hurts the predictive performance and doesn’t
allow to learn expressive kernel functions.

O

2.3 KISS-GP

Saatçi (2012) noted that the covariance matrices com-
puted at points on a multidimensional grid in the fea-
ture space can be represented as a Kronecker product
if the kernel function factorizes over dimensions

k(x, x(cid:48)) = k1(x1, x(cid:48)1)

k2(x2, x(cid:48)2)

. . .

kD(xD, x(cid:48)D). (8)

·

·

·

Note, that many popular covariance functions, includ-
ing RBF, belong to this class. Kronecker structure of
covariance matrices allows to perform eﬃcient inference
for full Gaussian processes with inputs X on a grid.

Wilson and Nickisch (2015) proposed to set inducing
inputs Z on a grid:

Z = Z 1

Z 2

. . .

Z D,

Z i

Rmi

i = 1, 2, . . . , D.

×

×

×

∈

∀

The number m of inducing inputs is then given by
m = (cid:81)D

i=1 mi.

Let the covariance function satisfy (8). Then the covari-
ance matrix Kmm can be represented as a Kronecker
product over dimensions

Kmm = K 1

K 2

m1m1 ⊗

m2m2 ⊗

. . .

⊗

K D

mDmD

,

where

K i

mimi

= Ki(Zi, Zi)

Rmi×mi

∈

∀

i = 1, 2, . . . , D.

Kronecker products allow eﬃcient computation of ma-
trix inverse and determinant:

(A1
A1

⊗

A2
A2

⊗

|
where Ai

⊗

⊗

. . .

. . .

AD)−1 = A−1
1 ⊗
c1
A2
AD
· |
|
j(cid:54)=i kj,

A1

⊗
|
Rki×ki, ci = (cid:81)

=

⊗

|

∈

. . .

A−1
2 ⊗
c2
. . .
·

|

· |

⊗
AD

A−1
D ,
cD ,
|

i = 1, 2, . . . , D.

∀

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Another major idea of KISS-GP is to use interpolation
to approximate Kmn. Considering inducing inputs as
interpolation points for the function k(
, zi) we can
write

·

Kmn

KmmW,

ki

Kmmwi,

∈

≈

≈
Rm×n contains the coeﬃcients of interpo-
where W
lation, and wi is it’s i-th column. Authors of KISS-GP
suggest using cubic convolutional interpolation (Keys
(1981)), in which case the interpolation weights wi can
be represented as a Kronecker product over dimensions

(9)

wi = w1

w2

i ⊗

i ⊗

. . .

⊗

wD

i , wi

∈

Rmi

∀

i = 1, 2, . . . , D.

O

Wilson and Nickisch (2015) combine these ideas with
SOR (Silverman (1985)) in the KISS-GP method yield-
(n + Dm1+1/D) computational complexity. This
ing
complexity allows to use KISS-GP with a large number
(possibly greater than n) of inducing inputs. Note, how-
ever, that m grows exponentially with the dimension-
ality D of the feature space and the method becomes
impractical when D > 4.

2.4 Tensor Train Decomposition

Tensor Train (TT) decomposition, proposed in Os-
eledets (2011), allows to eﬃciently store tensors (multi-
dimensional arrays of data), large matrices, and vectors.
For tensors, matrices and vectors in the TT-format lin-
ear algebra operations can be implemented eﬃciently.

Consider a D-dimensional tensor

Rn1×n2×...×nD .

is said to be in the Tensor Train format if

A ∈

A

A

(i1, i2, . . . , iD) = G1[i1]
·
id

G2[i2]

. . .
1, 2, . . . , nd

GD[iD],
d,

·

·

(10)

∈ {

} ∀

Rrk−1×rk

where Gk[ik]
k, ik, r0 = rD = 1. Matrices
Gk are called TT-cores, and numbers rk are called TT-
ranks of tensor

∈

∀

.
A

In order to represent a vector in TT-format, it is re-
shaped to a multidimensional tensor (possibly with
zero padding) and then format (10) is used. We will
use TT-format for the vector µ of expectations of the
values u of the Gaussian process in points Z placed on
a multidimensional grid. In this case, µ is naturally
represented as a D-dimensional tensor.

For matrices TT format is given by

M (i1, i2, . . . , id; j1, j2, . . . , jD) =

G1[i1, j1]

G2[i2, j2]

. . .

GD[iD, jD],

·

·

·

Rrk−1×rk

where Gk[ik, jk]
k, ik, jk, r0 = rD = 1.
Note, that Kronecker product format is a special case of
the TT-matrix with TT-ranks r1 = r2 = . . . = rD = 1.

∈

∀

Rn1·n2·...·nD be vectors in TT-format with
Let u, v
TT-ranks not greater than r. Let A and B be repre-
sented as a Kronecker product

∈

A = A1

A2

. . .

AD, Ak

⊗

⊗

⊗

∈

Rnk×nk

k,

∀

and the same for B. Let n = maxk nk. Then the com-
putational complexity of computing the quadratic form
uT Av is
(Dnr3) and the complexity of computing
(Dn2). We will need these two operations
tr(AB) is
below. See Oseledets (2011) for a detailed description
of TT format and eﬃcient algorithms implementing
linear algebraic operations with it.

O
O

3 TT-GP

In the previous section we described several methods
for GP regression and classiﬁcation. All these methods
have diﬀerent limitations: standard methods are lim-
ited to small-scale datasets, KISS-GP requires small
dimensionality of the feature space, and other methods
based on inducing inputs are limited to use a small
number m of these points. In this section, we propose
the TT-GP method that can be used with big datasets
and can incorporate billions of inducing inputs. Ad-
ditionally, TT-GP allows for training expressive deep
kernels to work with structured data (e.g. images).

3.1 Variational Parameters Approximation

In section 2.2 we derived the variational lower bound
of Hensman et al. (2013). We will place the inducing
inputs Z on a multidimensional grid in the feature space
and we will assume the covariance function satisﬁes (8).
Let the number of inducing inputs in each dimension
be m0. Then the total number of inducing inputs is
m = mD
0 . As shown in Section 2.3, in this case Kmm
matrix can be expressed as a Kronecker product over
dimensions. Substituting the approximation (9) into
the lower bound (7), we obtain

log p(y)
n
(cid:18)
(cid:88)

≥

i=1
1
2

−

(cid:18)

log |

Kmm
Σ

|

|

|

−

log

(yi

wT

i µ, ν2)

N

|

−

1
2ν2

˜Kii

−

1
2ν2 tr(wT

i Σwi)

(cid:19)

−

m + tr(K −1

mmΣ) + µT K −1

mmµ

,

(cid:19)

(11)

where ˜Kii = k(xi, xi)
Note that K −1

−

wT

i Kmmwi.

0) =

(Dm3

mm and

Kmm
|

can be computed with
(Dm3/D) operations due to the Kro-
O
necker product structure. Now the most computation-
ally demanding terms are those containing variational
parameters µ and Σ.

O

|

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Let us restrict the family of variational distributions (4).
Let Σ be a Kronecker product over dimensions, and
µ be in the TT-format whith TT-rank r (r is a
hyper-parameter of our method). Then, according
to section 2.4, we can compute the lower bound (11)
(nDm1/Dr2 +
(nDm0r2 + Dm0r3 + Dm3
with
Dm1/Dr3 + Dm3/D) complexity.

0) =

O

O

The proposed TT-GP method has linear complexity
with respect to dimensionality D of the feature space,
despite the exponential growth of the number of induc-
ing inputs. Lower bound (11) can be maximized with
respect to kernel hyper-parameters θ, TT-cores of µ,
and Kronecker multipliers of Σ. Note that stochastic
optimization can be applied, as the bound (11) factor-
izes over data points.

TT format was successfully applied for diﬀerent ma-
for compressing neural
chine learning tasks, e.g.
networks (Novikov et al. (2015)) and estimating log-
partition function in probabilistic graphical models
(Novikov et al. (2014)). We explore the properties of
approximating the variational mean µ in TT format in
section 4.1.

3.2 Classiﬁcation

}

In this section we describe a generalization of the pro-
posed method for multiclass classiﬁcation. In this case
the dataset consists of features X = (x1, x2, . . . , xn)T
Rn×D and target values y = (y1, y2, . . . , yn)T
n, where C is the number of classes.
1, 2, . . . , C

{
Consider C Gaussian processes taking place in RD.
Each process corresponds to it’s own class. We will
place m = mD
0 inducing inputs Z on a grid in the
feature space, and they will be shared between all pro-
cesses. Each process has it’s own set of latent variables
representing the values of the process at data points
Rm. We will use
f c
the following model

Rn, and inducing inputs uc

∈
∈

∈

∈

p(y, f, u) =

(cid:16)

p

n
(cid:89)

i=1

yi

f 1,2,...,C
i

|

(cid:17) C
(cid:89)

c=1

p (f c

uc) p(uc),
|

where f 1,2,...,C
i
all processes 1, 2, . . . , C at data point i, p(f c
p(uc) are deﬁned as in (2) and

is the vector consisting of the values of
uc) and
|

(uc

uc)
|

µc, Σc), c = 1, 2, . . . ,
where q(f c, uc) = p(f c
|
N
C, all µc are represented in TT-format with TT-ranks
not greater than r and all Σc are represented as
Kronecker products over dimensions. Similarly to (6),
we obtain

log p(y)

q(f 1,2,...,C

i

) log p(yi

f 1,2,...,C
i

)

|

(12)

n
(cid:88)

E

≥

i=1

C
(cid:88)

c=1

−

KL(q(uc)

p(uc))

||

The second term in (12) can be computed analytically
as a sum of KL-divergences between normal distri-
butions. The ﬁrst term is intractable.
In order to
approximate the ﬁrst term we will use a lower bound.
We can rewrite

E

q(f 1,2,...,C

i

) log p(yi

f 1,2,...,C
i
|

) =

E

q(f yi

i )f yi

i −

E

q(f 1,2,...,C

i

) log

exp(f j
i )

(cid:19)

,

(cid:18) C
(cid:88)

j=1

(13)

where q(f 1,2...,C
i
i , sC
mC
i ).
|

(f C

) =

(f 1

i , s1
i )

m1
|

N

(f 2

m2, s2
i )
|

·

· N

. . .

·

N
The ﬁrst term in (13) is obviously tractable, while the
second term has to be approximated. Bouchard (2007)
discusses several lower bounds for expectations of this
type. Below we derive one of these bounds, which we
use in TT-GP.

j=1 exp(f j
i )

Concavity of logarithm implies log
≤
ϕ (cid:80)C
ϕ > 0. Taking expectation
∀
of both sides of the inequality and minimizing with
respect to ϕ, we obtain

log ϕ

j=1 exp(f j
i )

−

−

1,

(cid:16)(cid:80)C

(cid:17)

E

q(f 1,2,...,C

i

) log

(cid:19)

exp(f j
i )

(cid:18) C
(cid:88)

j=1

≤

(cid:18) C
(cid:88)

log

j=1

exp(cid:0)mj

i +

(cid:19)

.

(cid:1)

sj
i

1
2

(14)

Substituting (14) back into (12) we obtain a tractable
lower bound for multiclass classiﬁcation task, that
can be maximized with respect to kernel hyper-
parameters θc, TT-cores of µc and Kronecker factors of
Σc. The complexity of the method is C times higher,
than in regression case.

p(yi

f 1,2,...,C
i
|

) =

exp(f yi
i )
j=1 exp(f j
i )

.

(cid:80)C

3.3 Deep kernels

We will use variational distributions of the form

q(f 1, f 2, . . . , f C, u1, u2, . . . , uC) =

= q(f 1, u1)

q(f 2, u2)

. . .

q(f C, uC),

·

·

·

Wilson et al. (2016b) and Wilson et al. (2016a) showed
the eﬃciency of using expressive kernel functions based
on deep neural networks with Gaussian processes on
a variety of tasks. The proposed TT-GP method is
naturally compatible with this idea.

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Consider a covariance function k satisfying (8) and a
neural network (or in fact any parametric transform)
net. We can deﬁne a new kernel as follows

knet(x, x(cid:48)) = k(net(x), net(x(cid:48))).

We can train the neural network weights through max-
imization of GP marginal likelihood, the same way, as
we normally train kernel hyper-parameters θ. This way,
the network learns a multidimensional embedding for
the data, and GP is making the prediction working
with this embedding.

Wilson et al. (2016b) trained additive deep kernels
combining one-dimensional GPs on diﬀerent outputs
of a neural network. Training Gaussian processes on
multiple outputs of a Neural network is impractical
in their framework, because the complexity of the GP
part of their model grows exponentially with the input
dimensionality.

With methods of Hensman et al. (2013) and Hensman
et al. (2015) training Gaussian processes on multiple
outputs of a neural network also isn’t straightforward.
Indeed, with these methods we can only use up to
103–104 inducing inputs. While with standard RBF
kernels the positions of inputs of the GP are ﬁxed
and we can place the inducing inputs near the data,
with deep kernels the positions of the inputs of the GP
(outputs of the DNN) change during training to match
the positions of inducing inputs. It is thus not clear
how to set the inducing inputs in the latent feature
space, other than placing them on a multidimensional
grid, which means that the complexity of such methods
would grow exponentially with dimensionality.

On the other hand, TT-GP allows us to train Gaussian
processes on multiple DNN outputs because of it’s
ability to eﬃciently work with inducing inputs placed
on multidimensional grids.

4 Experiments

In this section we ﬁrst explore how well can we ap-
proximate variational expectations in TT format with
small ranks. Then, we compare the proposed TT-
GP method with SVI-GP (Hensman et al. (2013))
on regression tasks and KLSP-GP (Hensman et al.
(2015)) on binary classiﬁcation tasks using standard
RBF kernel functions. Then, we test the ability of
our method to learn expressive deep kernel functions
and compare it with SV-DKL (Wilson et al. (2016b)).
For TT-GP we use our implementation available at
https://github.com/izmailovpavel/TTGP, which is
based on the t3f library (Novikov et al. (2018)). For
SVI-GP and KLSP-GP we used the implementations
provided in GPfLow (Matthews et al. (2016)).

4.1 Expectation approximation

In this section we provide a numerical justiﬁcation
of using Tensor Train format for the mean µ of the
variational distribution. We use the Powerplant dataset
from UCI. This dataset consists of 7654 objects with
4 features. We place m0 = 5 inducing inputs per
dimension and form a grid, which gives us a total
of m = 625 inducing inputs. We train the standard
SVI-GP method from GPﬂow library (Matthews et al.
(2016)) with free form representations for µ and Σ.
Then we try to approximate the learned µ vector with
a TT-vector µT T with small TT-ranks.

(a) MSE

(b) Cosine similarity

Figure 1: Approximation accuracy as a function of
TT-rank.

Figure 1 shows the dependence between TT-ranks and
approximation accuracy. For TT-rank greater than 25
we can approximate the true values of µ within machine
precision. Note that for TT-rank 25 the amount of
parameters in the TT representation already exceeds
the number of entries in the tensor µ that we are
approximating. For moderate TT-ranks an accurate
approximation can still be achieved.

(a) µ

(b) µT T , r = 10

Figure 2: True variational mean and TT-approximation.
Here we reshape the 625-dimensional µ and µT T vectors
to 25

25 matrices for visualization.

×

Figure 2 shows the true variational mean µ and it’s
approximation for TT-rank 10. We can see that µT T
captures the structure of the true variational mean.

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Table 1: Experimental results for standard RBF kernels. In the table acc. stands for r2 for regression and
accuracy for classiﬁcation tasks. n is the size of the training set, D is the dimensionality of the feature space, m
is the number of inducing inputs, r is TT-ranks of µ for TT-GP; t is the time per one pass over the data (epoch)
in seconds; where provided, d is the dimensionality of linear embedding.
∗ for KLSP-GP on Airline we provide results from the original paper where the accuracy is given as a plot, and
detailed information about experiment setup and exact results is not available.

Dataset

SVI-GP / KLSP-GP

TT-GP

Name

n

D

acc. m

t (s)

acc. m

Powerplant
Protein
YearPred

Airline
svmguide1
EEG
covtype bin

7654
36584
463K 90

4
9

8
6M
4
3089
11984
14
465K 54

0.94
0.50
0.30

0.665∗
0.967
0.915
0.817

200
200
1000

-
200
1000
1000

10
45
597

-
4
18
320

0.95
0.56
0.32

0.694
0.969
0.908
0.852

354
309
106

208
204
1210
106

r

30
25
10

15
15
15
10

d

-
-
6

-
-
10
6

t (s)

5
40
105

5200
1
10
172

4.2 Standard RBF Kernels

For testing our method with standard RBF covariance
functions we used a range of classiﬁcation and regres-
sion tasks from UCI and LIBSVM archives and the
Airline dataset, that is popular for testing scalable GP
models (Hensman et al. (2013), Hensman et al. (2015),
Wilson et al. (2016b), Cutajar et al. (2016)).

For Airline dataset we provide results reported in the
original paper (Hensman et al. (2015)). For our ex-
periments, we use a cluster of Intel Xeon E5-2698B v3
CPUs having 16 cores and 230 GB of RAM.

For YearPred, EEG and covtype datasets we used a
d-dimensional linear embedding inside the RBF ker-
nel for TT-GP, as the number D of features makes
it impractical to set inducing inputs on a grid in a
D-dimensional space in this case.

Table 1 shows the results on diﬀerent regression and
classiﬁcation tasks. We can see, that TT-GP is able to
achieve better predictive quality on all datasets except
EEG. We also note that the method is able to achieve
good predictive performance with linear embedding,
which makes it practical for a wide range of datasets.

4.3 Deep Kernels

4.3.1 Representation learning

We ﬁrst explore the representation our model learns for
data on the small Digits1 dataset containing n = 1797
8 images of handwritten digits. We used a TT-
8
GP with a kernel based on a small fully-connected
neural network with two hidden layers with 50 neurons

×

1http://scikit-learn.org/stable/auto_examples/

datasets/plot_digits_last_image.html

(a) DNN with TT-GP

(b) Plain DNN

Figure 3: Learned representation for Digits dataset.

each and d = 2 neurons in the output layer to obtain
a 2-dimensional embedding. We trained the model
to classify the digits to 10 classes corresponding to
diﬀerent digits. Fig. 3 (a) shows the learned embedding.
We also trained the same network standalone, adding
another layer with 10 outputs and softmax activations.
The embedding for this network is shown in ﬁg. 3,b.

We can see that the stand-alone DNN with linear classi-
ﬁers is unable to learn a good 2-dimensional embedding.
On the other hand, using a ﬂexible GP classiﬁer that
is capable of learning non-linear transormations, our
model groups objects of the same class into compact
regions.

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Table 2: DNN architecture used in experiments with deep kernels. Here F(h) means a fully-connected layer with
h neurons; C(h
w
w) means max-pooling with h
kernel; ReLU stands for rectiﬁed linear unit and BN means batch normalization (Ioﬀe and Szegedy (2015)).

w, f ) means a convolutional layer with f h

w ﬁlters; P(h

×

×

×

×

Dataset

Architecture

Airline
CIFAR-10 C(3
C(3
C(3
C(5

MNIST

×
×
×
×

F(1000)-ReLU-F(1000)-ReLU-F(500)-ReLU-F(50)-ReLU-F(2)

3, 128)-BN-ReLU-C(3
3, 256)-BN-ReLU-P(3
3, 256)-BN-ReLU-P(3
5, 32)-ReLU-P(2

×
×
×
2)-C(5

3)-C(3

3, 128)-BN-ReLU-P(3
3)-C(3
3)-F(1536)-BN-ReLU-F(512)-BN-ReLU-F(9)
2)-F(1024)-ReLU-F(4)

3, 256)-BN-ReLU-

5, 64)-ReLU-P(2

×

×

×

3, 256)-BN-ReLU-

×

×

×

Table 3: Results of experiments with deep kernels. Here acc. is classiﬁcation accuracy; C is the number of classes;
d is the dimensionality of embedding learned by the model; t is the time per one pass over data (epoch) in seconds.

Name

Airline
CIFAR-10
MNIST

Dataset

n

D

6M 8
50K 32
60K 28

SV-DKL

DNN

TT-GP

C

2
10
10

acc.

0.781
0.770
0.992

3

32
28

×

×
×

acc.

0.780
0.915
0.993

t (s)

1055
166
23

acc.

0.788
0.908
0.9936

±
±
±

0.002
0.003
0.0004

d

2
9
10

t (s)

1375
220
64

4.3.2 Classiﬁcation tasks

×

To test our model with deep kernels we used Airline,
CIFAR-10 (Krizhevsky (2009)) and MNIST (LeCun
et al. (1998)) datasets. The corresponding DNN archi-
tectures are shown in Table 2. For CIFAR-10 dataset
we also use standard data augmentation techniques
24 parts of the image,
with random cropping of 24
horizontal ﬂipping, randomly adjusting brightness and
contrast. In all experiments we also add a BN without
trainable mean and variance after the DNN output
layer to project the outputs into the region where in-
ducing inputs are placed. We use m0 = 10 inducing
inputs per dimension placed on a regular grid from
1 to 1 and set TT-ranks of µ to r = 10 for all three
−
datasets. For experiments with convolutional neural
networks, we used Nvidia Tesla K80 GPU to train the
model.

Table 3 shows the results of the experiments for our
TT-GP with DNN kernel and SV-DKL. Note, that
the comparison is not absolutely fair on CIFAR-10
and MNIST datasets, as we didn’t use the same exact
architecture and preprocessing as Wilson et al. (2016b)
because we couldn’t ﬁnd the exact speciﬁcations of
these models. On Airline dataset we used the same
exact architecture and preprocessing as SV-DKL and
TT-GP achieves a higher accuracy on this dataset.

We also provide results of stand-alone DNNs for com-
parison. We used the same networks that were used in
TT-GP kernels with the last linear layers replaced by
layers with C outputs and softmax activations. Over-
all, we can see, that our model is able to achieve good

predictive performance, improving the results of stan-
dalone DNN on Airline and MNIST.

We train all the models from random initialization
without pretraining. We also tried using pretrained
DNNs as initialization for the kernel of our TT-GP
model, which sometimes leads to faster convergence,
but does not improve the ﬁnal accuracy.

5 Discussion

We proposed TT-GP method for scalable inference in
Gaussian process models for regression and classiﬁca-
tion. The proposed method is capable of using billions
of inducing inputs, which is impossible for existing
methods. This allows us to improve the performance
over state-of-the-art both with standard and deep ker-
nels on several benchmark datasets. Further, we believe
that our model provides a more natural way of learn-
ing deep kernel functions than the existing approaches
since it doesn’t require any speciﬁc modiﬁcations of the
GP model and allows working with high-dimensional
DNN embeddings.

Our preliminary experiments showed that TT-GP is
inferior in terms of uncertainty quantiﬁcation compared
to existing methods. We suspect that the reason for this
is restricting Kronecker structure for the covariance
matrix Σ. We hope to alleviate this limitation by
using Tensor Train format for Σ and corresponding
approximations to it’s determinant.

As a promising direction for future work we consider
training TT-GP with deep kernels incrementally, using

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

the variational approximation of posterior distribution
as a prior for new data. We also ﬁnd it interesting to try
using the low-dimensional embeddings learned by our
model for transfer learning. Finally, we are interested
in using the proposed method for structured prediction,
where TT-GP could scale up GPstruct approaches
(Bratieres et al. (2015)) and allow using deep kernels.

Acknowledgements

Alexander Novikov was supported by the Russian Sci-
ence Foundation grant 17-11-01027. Dmitry Kropotov
was supported by Samsung Research, Samsung Elec-
tronics.

References

G. Bouchard. Eﬃcient bounds for the softmax func-
tion and applications to approximate inference in
hybrid models. In NIPS 2007 workshop for approxi-
mate Bayesian inference in continuous/hybrid sys-
tems, 2007.

S. Bratieres, N. Quadrianto, and Z. Ghahramani. Gp-
struct: Bayesian structured prediction using gaussian
processes. IEEE transactions on pattern analysis and
machine intelligence, 37(7):1514–1520, 2015.

K. Cutajar, E. V. Bonilla, P. Michiardi, and M. Fil-
ippone. Practical learning of deep gaussian pro-
cesses via random fourier features. arXiv preprint
arXiv:1610.04386, 2016.

J. Hensman, N. Fusi, and N. D. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artiﬁ-
cial Intelligence, pages 282–290. AUAI Press, 2013.

J. Hensman, A. G. de G. Matthews, and Z. Ghahramani.
Scalable variational gaussian process classiﬁcation.
In AISTATS, 2015.

S. Ioﬀe and C. Szegedy. Batch normalization: Accel-
erating deep network training by reducing internal
covariate shift. In Proceedings of the 32nd Interna-
tional Conference on Machine Learning (ICML-15),
pages 448–456, 2015.

R. Keys. Cubic convolution interpolation for digital
image processing. IEEE transactions on acoustics,
speech, and signal processing, 29(6):1153–1160, 1981.

A. Krizhevsky. Learning multiple layers of features

from tiny images. 2009.

Z. Ghahramani, and J. Hensman. GPﬂow: A Gaus-
sian process library using TensorFlow. arXiv preprint
1610.08733, October 2016.

T. Nickson, T. Gunter, C. Lloyd, M. A. Osborne,
and S. Roberts. Blitzkriging: Kronecker-structured
stochastic gaussian processes.
arXiv preprint
arXiv:1510.07965, 2015.

A. Novikov, A. Rodomanov, A. Osokin, and D. Vetrov.
Putting MRFs on a tensor train. In International
Conference on Machine Learning, pages 811–819,
2014.

A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov.
Tensorizing neural networks. In Advances in Neu-
ral Information Processing Systems, pages 442–450,
2015.

A. Novikov, P. Izmailov, V. Khrulkov, M. Figurnov,
and I. Oseledets. Tensor train decomposition on
tensorﬂow (t3f). arXiv preprint, 2018.

I. V. Oseledets. Tensor-train decomposition. SIAM
Journal on Scientiﬁc Computing, 33(5):2295–2317,
2011.

J. Quiñonero-Candela and C. E. Rasmussen. A uni-
fying view of sparse approximate gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

C. E. Rasmussen and C. K. I. Williams. Gaussian

processes for machine learning, 2006.

Y. Saatçi. Scalable inference for structured Gaussian
process models. PhD thesis, University of Cambridge,
2012.

B. W. Silverman. Some aspects of the spline smoothing
approach to non-parametric regression curve ﬁtting.
Journal of the Royal Statistical Society. Series B
(Methodological), pages 1–52, 1985.

E. Snelson and Z. Ghahramani. Sparse gaussian pro-
cesses using pseudo-inputs. Advances in neural in-
formation processing systems, 18:1257, 2006.

M. K. Titsias. Variational learning of inducing variables
in sparse gaussian processes. In AISTATS, volume 5,
pages 567–574, 2009.

C. K. I. Williams and M. Seeger. Using the nyström
method to speed up kernel machines. In Proceedings
of the 13th International Conference on Neural In-
formation Processing Systems, pages 661–667. MIT
press, 2000.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324,
1998.

A. G. Wilson and H. Nickisch. Kernel interpolation
for scalable structured gaussian processes (kiss-gp).
In International Conference on Machine Learning,
pages 1775–1784, 2015.

A. G. de G. Matthews, M. van der Wilk, T. Nick-
son, K. Fujii, A. Boukouvalas, P. León-Villagrá,

A. G. Wilson, E. Gilboa, J. P. Cunningham, and A. Ne-
horai. Fast kernel learning for multidimensional pat-

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

tern extrapolation. In Advances in Neural Informa-
tion Processing Systems, pages 3626–3634, 2014.

A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing.
Deep kernel learning.
In Proceedings of the 19th
International Conference on Artiﬁcial Intelligence
and Statistics, pages 370–378, 2016a.

A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P.
Xing. Stochastic variational deep kernel learning. In
Advances in Neural Information Processing Systems,
pages 2586–2594, 2016b.

Scalable Gaussian Processes with Billions of Inducing Inputs via
Tensor Train Decomposition

8
1
0
2
 
n
a
J
 
7
1
 
 
]

G
L
.
s
c
[
 
 
2
v
4
2
3
7
0
.
0
1
7
1
:
v
i
X
r
a

Pavel A. Izmailov
Lomonosov Moscow State
University,
Cornell University

Alexander V. Novikov
National Research University
Higher School of Economics,
Institute of Numerical Mathematics RAS

Dmitry A. Kropotov
Lomonosov Moscow State
University

Abstract

We propose a method (TT-GP) for approx-
imate inference in Gaussian Process (GP)
models. We build on previous scalable GP
research including stochastic variational in-
ference based on inducing inputs, kernel in-
terpolation, and structure exploiting algebra.
The key idea of our method is to use Ten-
sor Train decomposition for variational pa-
rameters, which allows us to train GPs with
billions of inducing inputs and achieve state-
of-the-art results on several benchmarks. Fur-
ther, our approach allows for training kernels
based on deep neural networks without any
modiﬁcations to the underlying GP model.
A neural network learns a multidimensional
embedding for the data, which is used by
the GP to make the ﬁnal prediction. with-
out pretraining, through maximization of GP
marginal likelihood. We show the eﬃciency of
the proposed approach on several regression
and classiﬁcation benchmark datasets includ-
ing MNIST, CIFAR-10, and Airline.

1

Introduction

Gaussian processes (GPs) provide a prior over functions
and allow ﬁnding complex regularities in data. The
ability of GPs to adjust the complexity of the model
to the size of the data makes them appealing to use for
big datasets. Unfortunately, standard methods for GP
(n3) with the
regression and classiﬁcation scale as
number n of training instances and can not be applied
when n exceeds several thousands.

O

Numerous approximate inference methods have been
proposed in the literature. Many of these methods are

based on the concept of inducing inputs (Quiñonero-
Candela and Rasmussen (2005), Snelson and Ghahra-
mani (2006), Williams and Seeger (2000)). These meth-
ods build a smaller set Z of m points that serve to ap-
proximate the true posterior of the process and reduce
(nm2 + m3). Titsias (2009) pro-
the complexity to
posed to consider the values u of the Gaussian process
at the inducing inputs as latent variables and derived
a variational inference procedure to approximate the
posterior distribution of these variables. Hensman et al.
(2013) and Hensman et al. (2015) extended this frame-
work by using stochastic optimization to scale up the
method and generalized it to classiﬁcation problems.

O

Inducing input methods allow to use Gaussian processes
on datasets containing millions of examples. However,
these methods are still limited in the number of induc-
ing inputs m they can use (usually up to 104). Small
number of inducing inputs limits the ﬂexibility of the
models that can be learned with these methods, and
does not allow to learn expressive kernel functions (Wil-
son et al. (2014)). Wilson and Nickisch (2015) proposed
KISS-GP framework, which exploits the Kronecker
product structure in covariance matrices for inducing
inputs placed on a multidimensional grid in the fea-
(n + Dm1+1/D),
ture space. KISS-GP has complexity
where D is the dimensionality of the feature space.
Note however, that m is the number of points in a
D-dimensional grid and grows exponentially with D,
which makes the method impractical when the number
of features D is larger than 4.

O

In this paper, we propose TT-GP method, that can
use billions of inducing inputs and is applicable to
a much wider range of datasets compared to KISS-
GP. We achieve this by combining kernel interpolation
and Kronecker algebra of KISS-GP with a scalable
variational inference procedure. We restrict the family
of variational distributions from Hensman et al. (2013)
to have parameters in compact formats. Speciﬁcally,
we use Kronecker product format for the covariance
matrix Σ and Tensor Train format (Oseledets (2011))

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

for the expectation µ of the variational distribution
over the values u of the process at inducing inputs Z.

Nickson et al. (2015) showed that using Kronecker for-
mat for Σ does not substantially aﬀect the predictive
performance of GP regression, while allowing for com-
putational gains. The main contribution of this paper is
combining the Kronecker format for Σ with TT-format
for µ, which, together with eﬃcient inference procedure,
allows us to eﬃciently train GP models with billions
of inducing inputs.

Unlike KISS-GP the proposed method has linear com-
plexity with respect to dimensionality D of the feature
space. It means that we can apply TT-GP to datasets
that are both large and high-dimensional. Note how-
ever, that TT-GP is constructing a grid of inducing
inputs in the feature space, and tries to infer the val-
ues of the process in all points in the grid. High-
dimensional real-world datasets are believed to lie on
small-dimensional manifolds in the feature space, and it
is impractical to try to recover the complex non-linear
transformation that a Gaussian Process deﬁnes on the
whole feature space. Thus, we use TT-GP on raw fea-
tures for datasets with dimensionality up to 10. For
feature spaces with higher dimensionality we propose
to use kernels based on parametric projections, which
can be learned from data.

Wilson et al. (2016a) and Wilson et al. (2016b) demon-
strated eﬃciency of Gaussian processes with kernels
based on deep neural networks. They used subsets of
outputs of a DNN as inputs for a Gaussian process. As
the authors were using KISS-GP, they were limited to
using additive kernels, combining multiple low dimen-
sional Gaussian processes. We found that DNN-based
kernels are very eﬃcient in combination with TT-GP.
These kernels allows us to train TT-GP models on
high-dimensional datasets including computer vision
tasks. Moreover, unlike the existing deep kernel learn-
ing methods, TT-GP does not require any changes in
the GP model and allows deep kernels that produce
embeddings of dimensionality up to 10.

2 Background

2.1 Gaussian Processes

A Gaussian process is a collection of random variables,
any ﬁnite number of which have a joint normal distribu-
tion. A GP f taking place in RD is fully deﬁned by its
mean m : RD
R
functions. For every x1, x2, . . . , xn

R and covariance k : RD

RD

RD

→

→

×

f (x1), f (x2), . . . , f (xn)

(m, K),

∈

∼ N

where m = (m(x1), m(x2), . . . , m(xn))T
K

Rn, and
Rn×n is the covariance matrix with Kij =

∈

∈

k(xi, xj). Below we will use notation K(A, B) for the
matrix of pairwise values of covariance function k on
points from sets A and B.

Consider a regression problem. The dataset consists
Rn×D, and target
of n objects X = (x1, . . . , xn)T
∈
Rn. We assume that
values y = (y1, y2, . . . , yn)T
the data is generated by a latent zero-mean Gaussian
process f plus independent Gaussian noise:

∈

n
(cid:89)

p(y, f

X) = p(f
|

|

X)

p(yi

fi),
|

p(f

X) =
|
p(yi

fi) =

|

i=1
0, K(X, X)),
fi, ν2I),
|

(f

|
(yi

N

N

(1)

where fi = f (xi) is the value of the process at data
point xi and ν2 is the noise variance.

Assume that we want to predict the values of the pro-
cess f∗ at a set of test points X∗. As the joint dis-
tribution of y and f∗ is Gaussian, we can analytically
y, X, X∗) =
compute the conditional distribution p(f∗
|
ˆm, ˆK) with tractable formulas for ˆm and ˆK. The
|
N
complexity of computing ˆm and ˆK is
(n3) since it
involves calculation of the inverse of the covariance
matrix K(X, X).

(f∗

O

Covariance functions usually have a set of hyper-
parameters θ. For example, the RBF kernel

kRBF(x, x(cid:48)) = σ2

f exp (cid:0)

x

0.5
(cid:107)

−

−

x(cid:48)

2/l2(cid:1)
(cid:107)

has two hyper-parameters l and σf . In order to ﬁt
the model to the data, we can maximize the marginal
X) with respect to these
likelihood of the process p(y
|
parameters. In case of GP regression this marginal
(n3)
likelihood is tractable and can be computed in
operations.

O

fi) = 1/(1 + exp(

For two-class classiﬁcation problem we use the same
model (1) with p(yi
yifi)), where
yi
. In this case both predictive distribution
1, +1
}
and marginal likelihood are intractable. For detailed
description of GP regression and classiﬁcation see Ras-
mussen and Williams (2006).

∈ {−

−

|

2.2

Inducing Inputs

A number of approximate methods were developed to
scale up Gaussian processes. Hensman et al. (2013)
proposed a variational lower bound that factorizes over
observations for Gaussian process marginal likelihood.
We rederive this bound here.

Rm×D of m inducing inputs in the
Consider a set Z
∈
Rm representing
feature space and latent variables u
the values of the Gaussian process at these points.

∈

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Consider the augmented model

p(y, f, u) = p(y

f )p(f

|

u)p(u) =
|

p(yi

fi)p(f

u)p(u)

|

|

with

p(f

u) =
|
p(u) =

N
(u

KnmK −1
|

(f
0, Kmm),
|

N

mmu, Knn

KnmK −1

mmKmn),

n
(cid:89)

i=1

−

where Knn = K(X, X), Knm = K(X, Z), Kmn =
K(Z, X) = K T

nm, Kmm = K(Z, Z).

The standard variational lower bound is given by

log p(y)

Eq(u,f ) log

≥

p(y, f, u)
q(u, f )

=

= Eq(f ) log

p(yi

fi)
|

−

KL(q(u, f )

p(u, f )),

||

n
(cid:89)

i=1

where q(u, f ) is the variational distribution over latent
variables. Consider the following family of variational
distributions

q(u, f ) = p(f

u)
|

N

(u
|

µ, Σ),

(4)

∈

Rm and Σ

Rm×m are variational param-
where µ
eters. Then the marginal distribution over f can be
computed analytically
(cid:0)f

q(f ) =

∈

KnmK −1
mmµ,
N
Knn + KnmK −1

|

mm(Σ

Kmm)K −1

mmKmn

(cid:1) .

(5)

−

We can then rewrite (3) as

log p(y)

n
(cid:88)

i=1

≥

Eq(fi) log p(yi

fi)

|

−

KL(q(u)

p(u)). (6)

||

Note, that the lower bound (6) factorizes over observa-
tions and thus stochastic optimization can be applied
to maximize this bound with respect to both kernel
hyper-parameters θ and variational parameters µ and
Σ, as well as other parameters of the model e.g. noise
variance ν. In case of regression we can rewrite (6) in
the closed form

logp(y)

n
(cid:88)

(cid:18)

≥

i=1

1
2ν2

log |

−
(cid:18)

˜Kii

−
Kmm
Σ

|

|

|

−

1
2

−

log

(yi

i K −1
kT
|

mmµ, ν2)

−

N

1
2ν2 tr(kT

i K −1

mmΣK −1

mmki)

(cid:19)

−

m + tr(K −1

mmΣ) + µT K −1

mmµ

(cid:19)

,

(7)

where ki
∈
˜K = Knn
−

Rm is the i-th column of Kmn matrix and
KnmK −1

mmKmn.

At prediction time we can use the variational distribu-
tion as a substitute for posterior

(cid:90)

p(f∗

y) =
|

p(f∗

y)df du
f, u)p(f, u
|

|

≈

(cid:90)

≈

p(f∗

f, u)q(f, u)df du =
|

p(f∗

u)q(u)du.

|

(cid:90)

(2)

(3)

O

(nm2 +
The complexity of computing the bound (7) is
m3). Hensman et al. (2015) proposes to use Gauss-
Hermite quadratures to approximate the expectation
term in (6) for binary classiﬁcation problem to obtain
(nm2+m3). This
the same computational complexity
complexity allows to use Gaussian processes in tasks
with millions of training samples, but these methods
are limited to use small numbers of inducing inputs
m, which hurts the predictive performance and doesn’t
allow to learn expressive kernel functions.

O

2.3 KISS-GP

Saatçi (2012) noted that the covariance matrices com-
puted at points on a multidimensional grid in the fea-
ture space can be represented as a Kronecker product
if the kernel function factorizes over dimensions

k(x, x(cid:48)) = k1(x1, x(cid:48)1)

k2(x2, x(cid:48)2)

. . .

kD(xD, x(cid:48)D). (8)

·

·

·

Note, that many popular covariance functions, includ-
ing RBF, belong to this class. Kronecker structure of
covariance matrices allows to perform eﬃcient inference
for full Gaussian processes with inputs X on a grid.

Wilson and Nickisch (2015) proposed to set inducing
inputs Z on a grid:

Z = Z 1

Z 2

. . .

Z D,

Z i

Rmi

i = 1, 2, . . . , D.

×

×

×

∈

∀

The number m of inducing inputs is then given by
m = (cid:81)D

i=1 mi.

Let the covariance function satisfy (8). Then the covari-
ance matrix Kmm can be represented as a Kronecker
product over dimensions

Kmm = K 1

K 2

m1m1 ⊗

m2m2 ⊗

. . .

⊗

K D

mDmD

,

where

K i

mimi

= Ki(Zi, Zi)

Rmi×mi

∈

∀

i = 1, 2, . . . , D.

Kronecker products allow eﬃcient computation of ma-
trix inverse and determinant:

(A1
A1

⊗

A2
A2

⊗

|
where Ai

⊗

⊗

. . .

. . .

AD)−1 = A−1
1 ⊗
c1
A2
AD
· |
|
j(cid:54)=i kj,

A1

⊗
|
Rki×ki, ci = (cid:81)

=

⊗

|

∈

. . .

A−1
2 ⊗
c2
. . .
·

|

· |

⊗
AD

A−1
D ,
cD ,
|

i = 1, 2, . . . , D.

∀

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Another major idea of KISS-GP is to use interpolation
to approximate Kmn. Considering inducing inputs as
interpolation points for the function k(
, zi) we can
write

·

Kmn

KmmW,

ki

Kmmwi,

∈

≈

≈
Rm×n contains the coeﬃcients of interpo-
where W
lation, and wi is it’s i-th column. Authors of KISS-GP
suggest using cubic convolutional interpolation (Keys
(1981)), in which case the interpolation weights wi can
be represented as a Kronecker product over dimensions

(9)

wi = w1

w2

i ⊗

i ⊗

. . .

⊗

wD

i , wi

∈

Rmi

∀

i = 1, 2, . . . , D.

O

Wilson and Nickisch (2015) combine these ideas with
SOR (Silverman (1985)) in the KISS-GP method yield-
(n + Dm1+1/D) computational complexity. This
ing
complexity allows to use KISS-GP with a large number
(possibly greater than n) of inducing inputs. Note, how-
ever, that m grows exponentially with the dimension-
ality D of the feature space and the method becomes
impractical when D > 4.

2.4 Tensor Train Decomposition

Tensor Train (TT) decomposition, proposed in Os-
eledets (2011), allows to eﬃciently store tensors (multi-
dimensional arrays of data), large matrices, and vectors.
For tensors, matrices and vectors in the TT-format lin-
ear algebra operations can be implemented eﬃciently.

Consider a D-dimensional tensor

Rn1×n2×...×nD .

is said to be in the Tensor Train format if

A ∈

A

A

(i1, i2, . . . , iD) = G1[i1]
·
id

G2[i2]

. . .
1, 2, . . . , nd

GD[iD],
d,

·

·

(10)

∈ {

} ∀

Rrk−1×rk

where Gk[ik]
k, ik, r0 = rD = 1. Matrices
Gk are called TT-cores, and numbers rk are called TT-
ranks of tensor

∈

∀

.
A

In order to represent a vector in TT-format, it is re-
shaped to a multidimensional tensor (possibly with
zero padding) and then format (10) is used. We will
use TT-format for the vector µ of expectations of the
values u of the Gaussian process in points Z placed on
a multidimensional grid. In this case, µ is naturally
represented as a D-dimensional tensor.

For matrices TT format is given by

M (i1, i2, . . . , id; j1, j2, . . . , jD) =

G1[i1, j1]

G2[i2, j2]

. . .

GD[iD, jD],

·

·

·

Rrk−1×rk

where Gk[ik, jk]
k, ik, jk, r0 = rD = 1.
Note, that Kronecker product format is a special case of
the TT-matrix with TT-ranks r1 = r2 = . . . = rD = 1.

∈

∀

Rn1·n2·...·nD be vectors in TT-format with
Let u, v
TT-ranks not greater than r. Let A and B be repre-
sented as a Kronecker product

∈

A = A1

A2

. . .

AD, Ak

⊗

⊗

⊗

∈

Rnk×nk

k,

∀

and the same for B. Let n = maxk nk. Then the com-
putational complexity of computing the quadratic form
uT Av is
(Dnr3) and the complexity of computing
(Dn2). We will need these two operations
tr(AB) is
below. See Oseledets (2011) for a detailed description
of TT format and eﬃcient algorithms implementing
linear algebraic operations with it.

O
O

3 TT-GP

In the previous section we described several methods
for GP regression and classiﬁcation. All these methods
have diﬀerent limitations: standard methods are lim-
ited to small-scale datasets, KISS-GP requires small
dimensionality of the feature space, and other methods
based on inducing inputs are limited to use a small
number m of these points. In this section, we propose
the TT-GP method that can be used with big datasets
and can incorporate billions of inducing inputs. Ad-
ditionally, TT-GP allows for training expressive deep
kernels to work with structured data (e.g. images).

3.1 Variational Parameters Approximation

In section 2.2 we derived the variational lower bound
of Hensman et al. (2013). We will place the inducing
inputs Z on a multidimensional grid in the feature space
and we will assume the covariance function satisﬁes (8).
Let the number of inducing inputs in each dimension
be m0. Then the total number of inducing inputs is
m = mD
0 . As shown in Section 2.3, in this case Kmm
matrix can be expressed as a Kronecker product over
dimensions. Substituting the approximation (9) into
the lower bound (7), we obtain

log p(y)
n
(cid:18)
(cid:88)

≥

i=1
1
2

−

(cid:18)

log |

Kmm
Σ

|

|

|

−

log

(yi

wT

i µ, ν2)

N

|

−

1
2ν2

˜Kii

−

1
2ν2 tr(wT

i Σwi)

(cid:19)

−

m + tr(K −1

mmΣ) + µT K −1

mmµ

,

(cid:19)

(11)

where ˜Kii = k(xi, xi)
Note that K −1

−

wT

i Kmmwi.

0) =

(Dm3

mm and

Kmm
|

can be computed with
(Dm3/D) operations due to the Kro-
O
necker product structure. Now the most computation-
ally demanding terms are those containing variational
parameters µ and Σ.

O

|

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Let us restrict the family of variational distributions (4).
Let Σ be a Kronecker product over dimensions, and
µ be in the TT-format whith TT-rank r (r is a
hyper-parameter of our method). Then, according
to section 2.4, we can compute the lower bound (11)
(nDm1/Dr2 +
(nDm0r2 + Dm0r3 + Dm3
with
Dm1/Dr3 + Dm3/D) complexity.

0) =

O

O

The proposed TT-GP method has linear complexity
with respect to dimensionality D of the feature space,
despite the exponential growth of the number of induc-
ing inputs. Lower bound (11) can be maximized with
respect to kernel hyper-parameters θ, TT-cores of µ,
and Kronecker multipliers of Σ. Note that stochastic
optimization can be applied, as the bound (11) factor-
izes over data points.

TT format was successfully applied for diﬀerent ma-
for compressing neural
chine learning tasks, e.g.
networks (Novikov et al. (2015)) and estimating log-
partition function in probabilistic graphical models
(Novikov et al. (2014)). We explore the properties of
approximating the variational mean µ in TT format in
section 4.1.

3.2 Classiﬁcation

}

In this section we describe a generalization of the pro-
posed method for multiclass classiﬁcation. In this case
the dataset consists of features X = (x1, x2, . . . , xn)T
Rn×D and target values y = (y1, y2, . . . , yn)T
n, where C is the number of classes.
1, 2, . . . , C

{
Consider C Gaussian processes taking place in RD.
Each process corresponds to it’s own class. We will
place m = mD
0 inducing inputs Z on a grid in the
feature space, and they will be shared between all pro-
cesses. Each process has it’s own set of latent variables
representing the values of the process at data points
Rm. We will use
f c
the following model

Rn, and inducing inputs uc

∈
∈

∈

∈

p(y, f, u) =

(cid:16)

p

n
(cid:89)

i=1

yi

f 1,2,...,C
i

|

(cid:17) C
(cid:89)

c=1

p (f c

uc) p(uc),
|

where f 1,2,...,C
i
all processes 1, 2, . . . , C at data point i, p(f c
p(uc) are deﬁned as in (2) and

is the vector consisting of the values of
uc) and
|

(uc

uc)
|

µc, Σc), c = 1, 2, . . . ,
where q(f c, uc) = p(f c
|
N
C, all µc are represented in TT-format with TT-ranks
not greater than r and all Σc are represented as
Kronecker products over dimensions. Similarly to (6),
we obtain

log p(y)

q(f 1,2,...,C

i

) log p(yi

f 1,2,...,C
i

)

|

(12)

n
(cid:88)

E

≥

i=1

C
(cid:88)

c=1

−

KL(q(uc)

p(uc))

||

The second term in (12) can be computed analytically
as a sum of KL-divergences between normal distri-
butions. The ﬁrst term is intractable.
In order to
approximate the ﬁrst term we will use a lower bound.
We can rewrite

E

q(f 1,2,...,C

i

) log p(yi

f 1,2,...,C
i
|

) =

E

q(f yi

i )f yi

i −

E

q(f 1,2,...,C

i

) log

exp(f j
i )

(cid:19)

,

(cid:18) C
(cid:88)

j=1

(13)

where q(f 1,2...,C
i
i , sC
mC
i ).
|

(f C

) =

(f 1

i , s1
i )

m1
|

N

(f 2

m2, s2
i )
|

·

· N

. . .

·

N
The ﬁrst term in (13) is obviously tractable, while the
second term has to be approximated. Bouchard (2007)
discusses several lower bounds for expectations of this
type. Below we derive one of these bounds, which we
use in TT-GP.

j=1 exp(f j
i )

Concavity of logarithm implies log
≤
ϕ (cid:80)C
ϕ > 0. Taking expectation
∀
of both sides of the inequality and minimizing with
respect to ϕ, we obtain

log ϕ

j=1 exp(f j
i )

−

−

1,

(cid:16)(cid:80)C

(cid:17)

E

q(f 1,2,...,C

i

) log

(cid:19)

exp(f j
i )

(cid:18) C
(cid:88)

j=1

≤

(cid:18) C
(cid:88)

log

j=1

exp(cid:0)mj

i +

(cid:19)

.

(cid:1)

sj
i

1
2

(14)

Substituting (14) back into (12) we obtain a tractable
lower bound for multiclass classiﬁcation task, that
can be maximized with respect to kernel hyper-
parameters θc, TT-cores of µc and Kronecker factors of
Σc. The complexity of the method is C times higher,
than in regression case.

p(yi

f 1,2,...,C
i
|

) =

exp(f yi
i )
j=1 exp(f j
i )

.

(cid:80)C

3.3 Deep kernels

We will use variational distributions of the form

q(f 1, f 2, . . . , f C, u1, u2, . . . , uC) =

= q(f 1, u1)

q(f 2, u2)

. . .

q(f C, uC),

·

·

·

Wilson et al. (2016b) and Wilson et al. (2016a) showed
the eﬃciency of using expressive kernel functions based
on deep neural networks with Gaussian processes on
a variety of tasks. The proposed TT-GP method is
naturally compatible with this idea.

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Consider a covariance function k satisfying (8) and a
neural network (or in fact any parametric transform)
net. We can deﬁne a new kernel as follows

knet(x, x(cid:48)) = k(net(x), net(x(cid:48))).

We can train the neural network weights through max-
imization of GP marginal likelihood, the same way, as
we normally train kernel hyper-parameters θ. This way,
the network learns a multidimensional embedding for
the data, and GP is making the prediction working
with this embedding.

Wilson et al. (2016b) trained additive deep kernels
combining one-dimensional GPs on diﬀerent outputs
of a neural network. Training Gaussian processes on
multiple outputs of a Neural network is impractical
in their framework, because the complexity of the GP
part of their model grows exponentially with the input
dimensionality.

With methods of Hensman et al. (2013) and Hensman
et al. (2015) training Gaussian processes on multiple
outputs of a neural network also isn’t straightforward.
Indeed, with these methods we can only use up to
103–104 inducing inputs. While with standard RBF
kernels the positions of inputs of the GP are ﬁxed
and we can place the inducing inputs near the data,
with deep kernels the positions of the inputs of the GP
(outputs of the DNN) change during training to match
the positions of inducing inputs. It is thus not clear
how to set the inducing inputs in the latent feature
space, other than placing them on a multidimensional
grid, which means that the complexity of such methods
would grow exponentially with dimensionality.

On the other hand, TT-GP allows us to train Gaussian
processes on multiple DNN outputs because of it’s
ability to eﬃciently work with inducing inputs placed
on multidimensional grids.

4 Experiments

In this section we ﬁrst explore how well can we ap-
proximate variational expectations in TT format with
small ranks. Then, we compare the proposed TT-
GP method with SVI-GP (Hensman et al. (2013))
on regression tasks and KLSP-GP (Hensman et al.
(2015)) on binary classiﬁcation tasks using standard
RBF kernel functions. Then, we test the ability of
our method to learn expressive deep kernel functions
and compare it with SV-DKL (Wilson et al. (2016b)).
For TT-GP we use our implementation available at
https://github.com/izmailovpavel/TTGP, which is
based on the t3f library (Novikov et al. (2018)). For
SVI-GP and KLSP-GP we used the implementations
provided in GPfLow (Matthews et al. (2016)).

4.1 Expectation approximation

In this section we provide a numerical justiﬁcation
of using Tensor Train format for the mean µ of the
variational distribution. We use the Powerplant dataset
from UCI. This dataset consists of 7654 objects with
4 features. We place m0 = 5 inducing inputs per
dimension and form a grid, which gives us a total
of m = 625 inducing inputs. We train the standard
SVI-GP method from GPﬂow library (Matthews et al.
(2016)) with free form representations for µ and Σ.
Then we try to approximate the learned µ vector with
a TT-vector µT T with small TT-ranks.

(a) MSE

(b) Cosine similarity

Figure 1: Approximation accuracy as a function of
TT-rank.

Figure 1 shows the dependence between TT-ranks and
approximation accuracy. For TT-rank greater than 25
we can approximate the true values of µ within machine
precision. Note that for TT-rank 25 the amount of
parameters in the TT representation already exceeds
the number of entries in the tensor µ that we are
approximating. For moderate TT-ranks an accurate
approximation can still be achieved.

(a) µ

(b) µT T , r = 10

Figure 2: True variational mean and TT-approximation.
Here we reshape the 625-dimensional µ and µT T vectors
to 25

25 matrices for visualization.

×

Figure 2 shows the true variational mean µ and it’s
approximation for TT-rank 10. We can see that µT T
captures the structure of the true variational mean.

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

Table 1: Experimental results for standard RBF kernels. In the table acc. stands for r2 for regression and
accuracy for classiﬁcation tasks. n is the size of the training set, D is the dimensionality of the feature space, m
is the number of inducing inputs, r is TT-ranks of µ for TT-GP; t is the time per one pass over the data (epoch)
in seconds; where provided, d is the dimensionality of linear embedding.
∗ for KLSP-GP on Airline we provide results from the original paper where the accuracy is given as a plot, and
detailed information about experiment setup and exact results is not available.

Dataset

SVI-GP / KLSP-GP

TT-GP

Name

n

D

acc. m

t (s)

acc. m

Powerplant
Protein
YearPred

Airline
svmguide1
EEG
covtype bin

7654
36584
463K 90

4
9

8
6M
4
3089
11984
14
465K 54

0.94
0.50
0.30

0.665∗
0.967
0.915
0.817

200
200
1000

-
200
1000
1000

10
45
597

-
4
18
320

0.95
0.56
0.32

0.694
0.969
0.908
0.852

354
309
106

208
204
1210
106

r

30
25
10

15
15
15
10

d

-
-
6

-
-
10
6

t (s)

5
40
105

5200
1
10
172

4.2 Standard RBF Kernels

For testing our method with standard RBF covariance
functions we used a range of classiﬁcation and regres-
sion tasks from UCI and LIBSVM archives and the
Airline dataset, that is popular for testing scalable GP
models (Hensman et al. (2013), Hensman et al. (2015),
Wilson et al. (2016b), Cutajar et al. (2016)).

For Airline dataset we provide results reported in the
original paper (Hensman et al. (2015)). For our ex-
periments, we use a cluster of Intel Xeon E5-2698B v3
CPUs having 16 cores and 230 GB of RAM.

For YearPred, EEG and covtype datasets we used a
d-dimensional linear embedding inside the RBF ker-
nel for TT-GP, as the number D of features makes
it impractical to set inducing inputs on a grid in a
D-dimensional space in this case.

Table 1 shows the results on diﬀerent regression and
classiﬁcation tasks. We can see, that TT-GP is able to
achieve better predictive quality on all datasets except
EEG. We also note that the method is able to achieve
good predictive performance with linear embedding,
which makes it practical for a wide range of datasets.

4.3 Deep Kernels

4.3.1 Representation learning

We ﬁrst explore the representation our model learns for
data on the small Digits1 dataset containing n = 1797
8 images of handwritten digits. We used a TT-
8
GP with a kernel based on a small fully-connected
neural network with two hidden layers with 50 neurons

×

1http://scikit-learn.org/stable/auto_examples/

datasets/plot_digits_last_image.html

(a) DNN with TT-GP

(b) Plain DNN

Figure 3: Learned representation for Digits dataset.

each and d = 2 neurons in the output layer to obtain
a 2-dimensional embedding. We trained the model
to classify the digits to 10 classes corresponding to
diﬀerent digits. Fig. 3 (a) shows the learned embedding.
We also trained the same network standalone, adding
another layer with 10 outputs and softmax activations.
The embedding for this network is shown in ﬁg. 3,b.

We can see that the stand-alone DNN with linear classi-
ﬁers is unable to learn a good 2-dimensional embedding.
On the other hand, using a ﬂexible GP classiﬁer that
is capable of learning non-linear transormations, our
model groups objects of the same class into compact
regions.

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

Table 2: DNN architecture used in experiments with deep kernels. Here F(h) means a fully-connected layer with
h neurons; C(h
w
w) means max-pooling with h
kernel; ReLU stands for rectiﬁed linear unit and BN means batch normalization (Ioﬀe and Szegedy (2015)).

w, f ) means a convolutional layer with f h

w ﬁlters; P(h

×

×

×

×

Dataset

Architecture

Airline
CIFAR-10 C(3
C(3
C(3
C(5

MNIST

×
×
×
×

F(1000)-ReLU-F(1000)-ReLU-F(500)-ReLU-F(50)-ReLU-F(2)

3, 128)-BN-ReLU-C(3
3, 256)-BN-ReLU-P(3
3, 256)-BN-ReLU-P(3
5, 32)-ReLU-P(2

×
×
×
2)-C(5

3)-C(3

3, 128)-BN-ReLU-P(3
3)-C(3
3)-F(1536)-BN-ReLU-F(512)-BN-ReLU-F(9)
2)-F(1024)-ReLU-F(4)

3, 256)-BN-ReLU-

5, 64)-ReLU-P(2

×

×

×

3, 256)-BN-ReLU-

×

×

×

Table 3: Results of experiments with deep kernels. Here acc. is classiﬁcation accuracy; C is the number of classes;
d is the dimensionality of embedding learned by the model; t is the time per one pass over data (epoch) in seconds.

Name

Airline
CIFAR-10
MNIST

Dataset

n

D

6M 8
50K 32
60K 28

SV-DKL

DNN

TT-GP

C

2
10
10

acc.

0.781
0.770
0.992

3

32
28

×

×
×

acc.

0.780
0.915
0.993

t (s)

1055
166
23

acc.

0.788
0.908
0.9936

±
±
±

0.002
0.003
0.0004

d

2
9
10

t (s)

1375
220
64

4.3.2 Classiﬁcation tasks

×

To test our model with deep kernels we used Airline,
CIFAR-10 (Krizhevsky (2009)) and MNIST (LeCun
et al. (1998)) datasets. The corresponding DNN archi-
tectures are shown in Table 2. For CIFAR-10 dataset
we also use standard data augmentation techniques
24 parts of the image,
with random cropping of 24
horizontal ﬂipping, randomly adjusting brightness and
contrast. In all experiments we also add a BN without
trainable mean and variance after the DNN output
layer to project the outputs into the region where in-
ducing inputs are placed. We use m0 = 10 inducing
inputs per dimension placed on a regular grid from
1 to 1 and set TT-ranks of µ to r = 10 for all three
−
datasets. For experiments with convolutional neural
networks, we used Nvidia Tesla K80 GPU to train the
model.

Table 3 shows the results of the experiments for our
TT-GP with DNN kernel and SV-DKL. Note, that
the comparison is not absolutely fair on CIFAR-10
and MNIST datasets, as we didn’t use the same exact
architecture and preprocessing as Wilson et al. (2016b)
because we couldn’t ﬁnd the exact speciﬁcations of
these models. On Airline dataset we used the same
exact architecture and preprocessing as SV-DKL and
TT-GP achieves a higher accuracy on this dataset.

We also provide results of stand-alone DNNs for com-
parison. We used the same networks that were used in
TT-GP kernels with the last linear layers replaced by
layers with C outputs and softmax activations. Over-
all, we can see, that our model is able to achieve good

predictive performance, improving the results of stan-
dalone DNN on Airline and MNIST.

We train all the models from random initialization
without pretraining. We also tried using pretrained
DNNs as initialization for the kernel of our TT-GP
model, which sometimes leads to faster convergence,
but does not improve the ﬁnal accuracy.

5 Discussion

We proposed TT-GP method for scalable inference in
Gaussian process models for regression and classiﬁca-
tion. The proposed method is capable of using billions
of inducing inputs, which is impossible for existing
methods. This allows us to improve the performance
over state-of-the-art both with standard and deep ker-
nels on several benchmark datasets. Further, we believe
that our model provides a more natural way of learn-
ing deep kernel functions than the existing approaches
since it doesn’t require any speciﬁc modiﬁcations of the
GP model and allows working with high-dimensional
DNN embeddings.

Our preliminary experiments showed that TT-GP is
inferior in terms of uncertainty quantiﬁcation compared
to existing methods. We suspect that the reason for this
is restricting Kronecker structure for the covariance
matrix Σ. We hope to alleviate this limitation by
using Tensor Train format for Σ and corresponding
approximations to it’s determinant.

As a promising direction for future work we consider
training TT-GP with deep kernels incrementally, using

Pavel A. Izmailov, Alexander V. Novikov, Dmitry A. Kropotov

the variational approximation of posterior distribution
as a prior for new data. We also ﬁnd it interesting to try
using the low-dimensional embeddings learned by our
model for transfer learning. Finally, we are interested
in using the proposed method for structured prediction,
where TT-GP could scale up GPstruct approaches
(Bratieres et al. (2015)) and allow using deep kernels.

Acknowledgements

Alexander Novikov was supported by the Russian Sci-
ence Foundation grant 17-11-01027. Dmitry Kropotov
was supported by Samsung Research, Samsung Elec-
tronics.

References

G. Bouchard. Eﬃcient bounds for the softmax func-
tion and applications to approximate inference in
hybrid models. In NIPS 2007 workshop for approxi-
mate Bayesian inference in continuous/hybrid sys-
tems, 2007.

S. Bratieres, N. Quadrianto, and Z. Ghahramani. Gp-
struct: Bayesian structured prediction using gaussian
processes. IEEE transactions on pattern analysis and
machine intelligence, 37(7):1514–1520, 2015.

K. Cutajar, E. V. Bonilla, P. Michiardi, and M. Fil-
ippone. Practical learning of deep gaussian pro-
cesses via random fourier features. arXiv preprint
arXiv:1610.04386, 2016.

J. Hensman, N. Fusi, and N. D. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artiﬁ-
cial Intelligence, pages 282–290. AUAI Press, 2013.

J. Hensman, A. G. de G. Matthews, and Z. Ghahramani.
Scalable variational gaussian process classiﬁcation.
In AISTATS, 2015.

S. Ioﬀe and C. Szegedy. Batch normalization: Accel-
erating deep network training by reducing internal
covariate shift. In Proceedings of the 32nd Interna-
tional Conference on Machine Learning (ICML-15),
pages 448–456, 2015.

R. Keys. Cubic convolution interpolation for digital
image processing. IEEE transactions on acoustics,
speech, and signal processing, 29(6):1153–1160, 1981.

A. Krizhevsky. Learning multiple layers of features

from tiny images. 2009.

Z. Ghahramani, and J. Hensman. GPﬂow: A Gaus-
sian process library using TensorFlow. arXiv preprint
1610.08733, October 2016.

T. Nickson, T. Gunter, C. Lloyd, M. A. Osborne,
and S. Roberts. Blitzkriging: Kronecker-structured
stochastic gaussian processes.
arXiv preprint
arXiv:1510.07965, 2015.

A. Novikov, A. Rodomanov, A. Osokin, and D. Vetrov.
Putting MRFs on a tensor train. In International
Conference on Machine Learning, pages 811–819,
2014.

A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov.
Tensorizing neural networks. In Advances in Neu-
ral Information Processing Systems, pages 442–450,
2015.

A. Novikov, P. Izmailov, V. Khrulkov, M. Figurnov,
and I. Oseledets. Tensor train decomposition on
tensorﬂow (t3f). arXiv preprint, 2018.

I. V. Oseledets. Tensor-train decomposition. SIAM
Journal on Scientiﬁc Computing, 33(5):2295–2317,
2011.

J. Quiñonero-Candela and C. E. Rasmussen. A uni-
fying view of sparse approximate gaussian process
regression. Journal of Machine Learning Research, 6
(Dec):1939–1959, 2005.

C. E. Rasmussen and C. K. I. Williams. Gaussian

processes for machine learning, 2006.

Y. Saatçi. Scalable inference for structured Gaussian
process models. PhD thesis, University of Cambridge,
2012.

B. W. Silverman. Some aspects of the spline smoothing
approach to non-parametric regression curve ﬁtting.
Journal of the Royal Statistical Society. Series B
(Methodological), pages 1–52, 1985.

E. Snelson and Z. Ghahramani. Sparse gaussian pro-
cesses using pseudo-inputs. Advances in neural in-
formation processing systems, 18:1257, 2006.

M. K. Titsias. Variational learning of inducing variables
in sparse gaussian processes. In AISTATS, volume 5,
pages 567–574, 2009.

C. K. I. Williams and M. Seeger. Using the nyström
method to speed up kernel machines. In Proceedings
of the 13th International Conference on Neural In-
formation Processing Systems, pages 661–667. MIT
press, 2000.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324,
1998.

A. G. Wilson and H. Nickisch. Kernel interpolation
for scalable structured gaussian processes (kiss-gp).
In International Conference on Machine Learning,
pages 1775–1784, 2015.

A. G. de G. Matthews, M. van der Wilk, T. Nick-
son, K. Fujii, A. Boukouvalas, P. León-Villagrá,

A. G. Wilson, E. Gilboa, J. P. Cunningham, and A. Ne-
horai. Fast kernel learning for multidimensional pat-

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition

tern extrapolation. In Advances in Neural Informa-
tion Processing Systems, pages 3626–3634, 2014.

A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing.
Deep kernel learning.
In Proceedings of the 19th
International Conference on Artiﬁcial Intelligence
and Statistics, pages 370–378, 2016a.

A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P.
Xing. Stochastic variational deep kernel learning. In
Advances in Neural Information Processing Systems,
pages 2586–2594, 2016b.

