7
1
0
2
 
r
a

M
 
6
 
 
]

G
L
.
s
c
[
 
 
5
v
5
8
1
2
0
.
1
1
6
1
:
v
i
X
r
a

TRUSTING SVM FOR PIECEWISE LINEAR CNNS

Leonard Berrada1, Andrew Zisserman1 and M. Pawan Kumar1,2
1Department of Engineering Science
University of Oxford
2Alan Turing Institute
{lberrada,az,pawan}@robots.ox.ac.uk

ABSTRACT

We present a novel layerwise optimization algorithm for the learning objective
of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of
convolutional neural networks. Speciﬁcally, PL-CNNs employ piecewise linear
non-linearities such as the commonly used ReLU and max-pool, and an SVM
classiﬁer as the ﬁnal layer. The key observation of our approach is that the prob-
lem corresponding to the parameter estimation of a layer can be formulated as a
difference-of-convex (DC) program, which happens to be a latent structured SVM.
We optimize the DC program using the concave-convex procedure, which requires
us to iteratively solve a structured SVM problem. This allows to design an opti-
mization algorithm with an optimal learning rate that does not require any tuning.
Using the MNIST, CIFAR and ImageNet data sets, we show that our approach
always improves over the state of the art variants of backpropagation and scales to
large data and large network settings.

1

INTRODUCTION

The backpropagation algorithm is commonly employed to estimate the parameters of a convolutional
neural network (CNN) using a supervised training data set (Rumelhart et al., 1986). Part of the
appeal of backpropagation comes from the fact that it is applicable to a wide variety of networks,
namely those that have (sub-)differentiable non-linearities and employ a (sub-)differentiable learning
objective. However, the generality of backpropagation comes at the cost of a high sensitivity to its
hyperparameters such as the learning rate and momentum. Standard line-search algorithms cannot be
used on the primal objective function in this setting, as (i) there may not exist a step-size guaranteeing
a monotonic decrease because of the use of sub-gradients, and (ii) even in the smooth case, each
function evaluation requires a forward pass over the entire data set without any update, making the
approach computationally unfeasible. Choosing the learning rate thus remains an open issue, with
the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012;
Kingma & Ba, 2015). In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015)
and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the
learning rate and to prevent from overﬁtting.

With this work, we open a different line of inquiry, namely, is it possible to design more robust
optimization algorithms for special but useful classes of CNNs? To this end, we focus on the networks
that are commonly used in computer vision. Speciﬁcally, we consider CNNs with convolutional and
dense layers that apply a set of piecewise linear (PL) non-linear operations to obtain a discriminative
representation of an input image. While this assumption may sound restrictive at ﬁrst, we show that
commonly used non-linear operations such as ReLU and max-pool fall under the category of PL
functions. The representation obtained in this way is used to classify the image via a multi-class
SVM, which forms the ﬁnal layer of the network. We refer to this class of networks as PL-CNN.

We design a novel, principled algorithm to optimize the learning objective of a PL-CNN. Our
algorithm is a layerwise method, that is, it iteratively updates the parameters of one layer while
keeping the other layers ﬁxed. For this work, we use a simple schedule over the layers, namely,
repeated passes from the output layer to the input one. However, it may be possible to further improve
the accuracy and efﬁciency of our algorithm by designing more sophisticated scheduling strategies.
The key observation of our approach is that the parameter estimation of one layer of PL-CNN can be

1

formulated as a difference-of-convex (DC) program that can be viewed as a latent structured SVM
problem (Yu & Joachims, 2009). This allows us to solve the DC program using the concave-convex
procedure (CCCP) (Yuille & Rangarajan, 2002). Each iteration of CCCP requires us to solve a convex
structured SVM problem. To this end, we use the powerful block-coordinate Frank-Wolfe (BCFW)
algorithm (Lacoste-Julien et al., 2013), which solves the dual of the convex program iteratively
by computing the conditional gradients corresponding to a subset of training samples. In order to
further improve BCFW for PL-CNNs, we extend it in three important ways. First, we introduce a
trust-region term that allows us to initialize the BCFW algorithm using the current estimate of the
layer parameters. Second, we reduce the memory requirement of BCFW by an order of magnitude,
via an efﬁcient representation of the feature vectors corresponding to the dense layers. Third, we
show that, empirically, the number of constraints of the structural SVM problem can be reduced
substantially without any loss in accuracy, which allows us to signiﬁcantly reduce its time complexity.

Compared to backpropagation (Rumelhart et al., 1986) or its variants (Duchi et al., 2011; Zeiler, 2012;
Kingma & Ba, 2015), our algorithm offers three advantages. First, the CCCP algorithm provides
a monotonic decrease in the learning objective at each layer. Since layerwise optimization itself
can be viewed as a block-coordinate method, our algorithm guarantees a monotonic decrease of the
overall objective function after each layer’s parameters have been updated. Second, since the dual of
the SVM problem is a smooth convex quadratic program, each step of the BCFW algorithm (in the
inner iteration of the CCCP) provides a monotonic increase in its dual objective. Third, since the
only step-size required in our approach comes while solving the SVM dual, we can use the optimal
step-size that is computed analytically during each iteration of BCFW (Lacoste-Julien et al., 2013).
In other words, our algorithm has no learning rate, initial or not, that requires tuning.

Using standard network architectures and publicly available data sets, we show that our algorithm
provides a boost over the state of the art variants of backpropagation for learning PL-CNNs and we
demonstrate scalability of the method.

2 RELATED WORK

While some of the early successful approaches for the optimization of deep neural networks relied on
greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are
variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the
introduction.

At every iteration, backpropagation performs a forward pass and a backward pass on the network, and
updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the
choice of the learning rate critical for efﬁcient optimization. Duchi et al. (2011) have proposed the
Adagrad convex solver, which adapts the learning rate for every direction and takes into account past
updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been
observed frequently in past updates. When applied to the non-convex CNN optimization problem,
Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al.,
2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the
learning rate slower. It is worth noting that this ﬁx is empirical, and to the best of our knowledge,
provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning
rate, called Adam, which uses an online estimation of the ﬁrst and second moments of the gradients
to provide centered and normalized updates. However all these methods still require the tuning of the
initial learning rate to perform well.

Second-order and natural gradient optimization methods have also been a subject of attention. The
focus in this line of work has been to come up with appropriate approximations to make the updates
cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using ﬁnite
differences to approximate the Hessian and conjugate gradient to compute the update. Martens &
Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efﬁcient
method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on
natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of
the problem. Desjardins et al. (2015) demonstrate a scaled up natural gradient descent method by
training on the ImageNet data set (Russakovsky et al., 2015). Though providing more informative
updates and solid theoretical support than SGD-based approaches, these methods do not take into
account the structure of the problem offered by the commonly used non-linear operations.

2

Our work is also related to some of the recent developments in optimization for deep learning. For
example, Taylor et al. (2016) use ADMM for massive distribution of computation in a layer-wise
fashion, and in particular their method will yield closed-form updates for any PL-CNN. Lee et al.
(2015) propose to use targets instead of gradients to propagate information through the network,
which could help to extend our algorithm. Zhang et al. (2016) derive a convex relaxation for the
learning objective for a restricted class of CNNs, which also relies on solving an approximate convex
problem. In (Amos et al., 2016), the authors identify convex problems for the inference task, when
the neural network is a convex function of some of its inputs.

With a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU
nets with guarantees of time convergence and generalization error. Heinemann et al. (2016) show that
a subclass of neural networks can be modeled as an improper kernel, which then reduces the learning
problem to a simple SVM with the constructed kernel.

More generally, we believe that our hitherto unknown observation regarding the relationship between
PL-CNNs and latent SVMs can (i) allow the progress made in one ﬁeld to be transferred to the other
and (ii) help design a new generation of principled algorithms for deep learning optimization.

3 PIECEWISE LINEAR CONVOLUTIONAL NEURAL NETWORKS

A piecewise linear convolutional neural network (PL-CNN) consists of a series of convolutional
layers, followed by a series of dense layers, which provides a concise representation of an input image.
Each layer of the network performs two operations: a linear transformation (that is, a convolution
or a matrix multiplication), followed by a piecewise linear non-linear operation such as ReLU or
max-pool. The resulting representation of the image is used for classiﬁcation via an SVM. In the
remainder of this section, we provide a formal description of PL-CNN.

Piecewise Linear Functions. A piecewise linear (PL) function f (u) is a function of the following
form (Melzer, 1986):

f (u) = max
i∈[m]

{a(cid:62)

i u} − max
j∈[n]

{b(cid:62)

j u},

(1)

where [m] = {1, · · · , m}, and [n] = {1, · · · , n}. Each of the two maxima above is a convex function,
therefore such a function f is not generally convex, but it is rather a difference of two convex functions.
Importantly, many commonly used non-linear operations such as ReLU or max-pool are PL functions
of their input. For example, ReLU corresponds to the function R(v) = max{v, 0} where v is a
scalar. Similarly, max-pool for a D-dimensional vector u corresponds to M (u) = maxi∈[D]{e(cid:62)
i u},
where ei is a vector whose i-th element is 1 and all other elements are 0. Given a value of u,
we say that (i∗, j∗) is the activation of the PL function at u if i∗ = argmaxi∈[m]{a(cid:62)
i u} and
j∗ = argmaxj∈[n]{b(cid:62)

j u}.

PL-CNN Parameters. We denote the parameters of an L layer PL-CNN by W = {W l; l ∈ [L]}.
In other words, the parameters of the l-th layer is deﬁned as W l. The CNN deﬁnes a composite
function, that is, the output zl−1 of layer l − 1 is the input to the layer l. Given the input zl−1 to
layer l, the output is computed as zl = σl(W l · zl−1), where “·” is either a convolution or a matrix
multiplication, and σl is a PL non-linear function, such as ReLU or max-pool. The input to the ﬁrst
layer is an image x, that is, z0 = x. We denote the input to the ﬁnal layer by zL = Φ(x; W) ∈ RD.
In other words, given an image x, the convolutional and dense layers of a PL-CNN provide a D-
dimensional representation of x to the ﬁnal classiﬁcation layer. The ﬁnal layer of a PL-CNN is a C
class SVM W svm, which speciﬁes one parameter W svm

y ∈ RD for each class y ∈ Y.

Prediction. Given an image x, a PL-CNN predicts its class using the following rule:

y∗ = argmax

W svm

y Φ(x; W).

y∈Y

(2)

In other words, the dot product of the D-dimensional representation of x with the SVM parameter for
a class y provides the score for the class. The desired prediction is obtained by maximizing the score
over all possible classes.

3

Learning Objective. Given a training data set D = {(xi, yi), i ∈ [N ]}, where xi is the input
image and yi is its ground-truth class, we wish to estimate the parameters W ∪ W svm of the PL-CNN.
To this end, we minimize a regularized upper bound on the empirical risk. The risk of a prediction
y∗
i given the ground-truth yi is measured with a user-speciﬁed loss function ∆(y∗
i , yi). For example,
the standard 0 − 1 loss has a value of 0 for a correct prediction and 1 for an incorrect prediction.
Formally, the parameters of a PL-CNN are estimated using the following learning objective:

min
W,W svm

λ
2

(cid:88)

(cid:107)W l(cid:107)2

F +

l∈[L]∪{svm}

1
N

N
(cid:88)

i=1

(cid:16)

max
¯yi∈Y

∆(¯yi, yi) + (cid:0)W svm
¯yi

− W svm

yi

(cid:1)T

Φ(xi; W)

.

(3)

(cid:17)

The hyperparameter λ denotes the relative weight of the regularization compared to the upper bound of
the empirical risk. Note that, due to the presence of piecewise linear non-linearities, the representation
Φ(·; W) (and hence, the above objective) is highly non-convex in the PL-CNN parameters.

4 PARAMETER ESTIMATION FOR PL-CNN

In order to enable layerwise optimization of PL-CNNs, we show that parameter estimation of a layer
can be formulated as a difference-of-convex (DC) program (subsection 4.1). This allows us to use the
concave-convex procedure, which solves a series of convex optimization problems (subsection 4.2).
We show that each convex problem closely resembles a structured SVM objective, which can be
addressed by the powerful block-coordinate Frank-Wolfe (BCFW) algorithm. We extend BCFW to
improve its initialization, time complexity and memory requirements, thereby enabling its use in
learning PL-CNNs (subsection 4.3). For the sake of clarity, we only provide sketches of the proofs
for those propositions that are necessary for understanding the paper. The detailed proofs of the
remaining propositions are provided in the Appendix.

4.1 LAYERWISE OPTIMIZATION AS A DC PROGRAM

Given the values of the parameters for the convolutional and the dense layers (that is, W), the learning
objective (3) is the standard SVM problem in parameters W svm. In other words, it is a convex
optimization problem with several efﬁcient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009;
Shalev-Shwartz et al., 2009), including the BCFW algorithm (Lacoste-Julien et al., 2013). Hence, the
optimization of the ﬁnal layer is a computationally easy problem. In contrast, the optimization of the
parameters of a convolutional or a dense layer l does not result in a convex program. In general, this
problem can be arbitrarily hard to solve. However, in the case of PL-CNN, we show that the problem
can be formulated as a speciﬁc type of DC program, which enables efﬁcient optimization via the
iterative use of BCFW. The key property that enables our approach is the following proposition that
shows that the composition of PL functions is also a PL function.
Proposition 1. Consider PL functions g : Rm → R and gi : Rn → R, for all i ∈ [m]. Deﬁne
a function f : Rn → R as f (u) = g([g1(u), g2(u), · · · , gm(u)](cid:62)). Then f is also a PL function
(proof in Appendix A).

Using the above proposition, we can reformulate the problem of optimizing the parameters of one
layer of the network as a DC program. Speciﬁcally, the following proposition shows that the problem
can be formulated as a latent structured SVM objective (Yu & Joachims, 2009).
Proposition 2. The learning objective of a PL-CNN with respect to the parameters of the l-th layer
can be speciﬁed as follows:

min
W l

λ
2

(cid:107)W l(cid:107)2

F +

1
N

N
(cid:88)

i=1

max
hi∈H
¯yi∈Y

(cid:0)∆(¯yi, yi) + (W l)(cid:62)Ψ(xi, ¯yi, hi)(cid:1) − max
hi∈H

(cid:0)(W l)(cid:62)Ψ(xi, yi, hi)(cid:1) ,

(4)

for an appropriate choice of the latent space H and joint feature vectors Ψ(x, y, h) of the input
x, the output y and the latent variables h. In other words, parameter estimation for the l-th layer
corresponds to minimizing the sum of its Frobenius norm plus a PL function for each training sample.

Sketch of the Proof. For a given image x with the ground-truth class y, consider the input to the layer
l, which we denote by zl−1. Since all the layers except the l-th one are ﬁxed, the input zl−1 is a

4

constant vector, which only depends on the image x (that is, its value does not depend on the variables
W l). In other words, we can write zl−1 = ϕ(x).
Given the input zl−1, all the elements of the output of the l-th layer, denoted by zl, are a PL function
of W l since the layer performs a linear transformation of zl−1 according to the parameters W l,
followed by an application of PL operations such as ReLU or max-pool. The vector zl is then fed
to the (l + 1)-th layer. The output zl+1 of the (l + 1)-th layer is a vector whose elements are PL
functions of zl. Therefore, by proposition (1), the elements of zl+1 are a PL function of W l. By
applying the same argument until we reach the layer L, we can conclude that the representation
Φ(x; W) is a PL function of W l.

Next, consider the upper bound of the empirical risk, which is speciﬁed as follows:

(cid:16)

max
¯y∈Y

∆(¯y, y) + (cid:0)W svm

¯y − W svm

y

(cid:1)T

(cid:17)

Φ(x; W)

.

(5)

Once again, since W svm is ﬁxed, the above upper bound can be interpreted as a PL function of
Φ(x; W), and thus, by proposition (1), the upper bound is a PL function of W l. It only remains to
observe that the learning objective (3) also contains the Frobenius norm of W l. Thus, it follows that
the estimation of the parameters of layer l can be reformulated as minimizing the sum of its Frobenius
norm and the PL upper bound of the empirical risk over all training samples, as shown in problem (4).
Note that we have ignored the constants corresponding to the Frobenius norm of the parameters of all
the ﬁxed layers. This constitutes an existential proof of Proposition 2. In the next paragraph, we give
an intuition about the feature vectors Ψ(xi, ¯yi, hi) and the latent space H.

Feature Vectors & Latent Space. The exact form of the joint feature vectors depends on the
explicit DC decomposition of the objective function. In Appendix B, we detail the practical computa-
tions and give an example: we construct two interleaved neural networks whose outputs deﬁne the
convex and concave parts of the DC objective function. Given the explicit DC objective function, the
feature vectors are given by a subgradient and can therefore be obtained by automatic differentiation.

We now give an intuition of what the latent space H represents. Consider an input image x and
a corresponding latent variable h ∈ H. The latent variable can be viewed as a set of variables
hk, k ∈ {l + 1, · · · , L}. In other words, each subset hk of the latent variable corresponds to one of
the layers of the network that follow the layer l. Intuitively, hk represents the choice of activation at
layer k when going through the PL activation: for each neuron j of layer k, hk
j takes value i if and
only if the i-th piece of the piecewise linear activation is selected. For instance, i is the index of the
selected input in the case of a max-pooling unit.

Note that the latent space only depends on the layers that follow the current layer being optimized.
This is due to the fact that the input zl−1 to the l-th layer is a constant vector that does not depend on
the value of W l. However, the activations of all subsequent layers following the l-th one depend on
the value of the parameters W l. As a consequence, the greater the number of following layers, the
greater the size of the latent space, and this growth happens to be exponential. However, as will be
seen shortly, it is still possible to efﬁciently optimize problem (4) for all the layers of the network
despite this exponential increase.

4.2 CONCAVE-CONVEX PROCEDURE

The optimization problem (4) is a DC program in the parameters W l. This follows from the fact
that the upper bound of the empirical risk is a PL function, and can therefore be expressed as the
difference of two convex PL functions (Melzer, 1986). Furthermore, the Frobenius norm of W l
is also a convex function of W l. This observation allows us to obtain an approximate solution of
problem (4) using the iterative concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002).

Algorithm 1 describes the main steps of CCCP. In step 3, we impute the best value of the latent variable
corresponding to the ground-truth class yi for each training sample. This imputation corresponds to
the linearization step of the CCCP. The selected latent variable corresponds to a choice of activations
at each non-linear layer of the network, and therefore deﬁnes a path of activations to the ground truth.
Next, in step 4, we update the parameters by solving a convex optimization problem. This convex
problem amounts to ﬁnding the path of activations which minimizes the maximum margin violations
given the path to the ground truth deﬁned in step 3.

5

The CCCP algorithm has the desirable property of providing a monotonic decrease in the objective
function at each iteration. In other words, the objective function value of problem (4) at W l
t is
greater than or equal to its value at W l
t+1. Since layerwise optimization itself can be viewed as a
block-coordinate algorithm for minimizing the learning objective (3), our overall algorithm provides
guarantees of monotonic decrease until convergence. This is one of the main advantages of our
approach compared to backpropagation and its variants, which fail to provide similar guarantees on
the value of the objective function from one iteration to the next.

Algorithm 1 CCCP for parameter estimation of the l-th layer of the PL-CNN.
Require: Data set D = {(xi, yi), i ∈ [N ]}, ﬁxed parameters {W ∪ W svm}\W l, initial estimate W l
0.

1: t = 0
2: repeat
3:

For each sample (xi, yi), ﬁnd the best latent variable value by solving the following problem:

h∗

i = argmax

(W l

t )(cid:62)Ψ(xi, yi, h).

h∈H

(6)

4:

Update the parameters by solving the following convex optimization problem:

W l

t+1 = argmin

(cid:107)W l(cid:107)2

F +

λ
2

W l

1
N

N
(cid:88)

i=1

max
¯yi∈Y
hi∈H

(cid:0)∆(¯yi, yi) + (W l)(cid:62)Ψ(xi, ¯yi, hi)(cid:1) −

(cid:0)(W l)(cid:62)Ψ(xi, yi, h∗

i )(cid:1) .

(7)

t = t+1

5:
6: until Objective function of problem (4) cannot be improved beyond a speciﬁed tolerance.

In order to solve the convex program (7), which corresponds to a structured SVM problem, we make
use of the powerful BCFW algorithm (Lacoste-Julien et al., 2013) that solves its dual via conditional
gradients. This has two main advantages: (i) as the dual is a smooth quadratic program, each iteration
of BCFW provides a monotonic increase in its objective; and (ii) the optimal step-size at each iteration
can be computed analytically. This is once again in stark contrast to backpropagation, where the
estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma
& Ba, 2015). As shown by Lacoste-Julien et al. (2013), given the current estimate of the parameters
W l, the conditional gradient of the dual of program (7) with respect to a training sample (xi, yi) can
be obtained by solving the following problem:

(ˆyi, ˆhi) = argmax
¯y∈Y,h∈H

(W l)(cid:62)Ψ(xi, ¯y, h) + ∆(¯y, yi).

(8)

We refer the interested reader to (Lacoste-Julien et al., 2013) for further details.

The overall efﬁciency of the CCCP algorithm relies on our ability to solve problems (6) and (8). At
ﬁrst glance, these problems may appear to be computationally intractable as the latent space H can
be very large, especially for layers close to the input (of the order of millions of dimensions for a
typical network). However, the following proposition shows that both the problems can be solved
efﬁciently using the forward and backward passes that are employed in backpropagation.
Proposition 3. Given the current estimate W l of the parameters for the l-th layer, as well as the
parameter values of all the other ﬁxed layers, problems (6) and (8) can be solved using a forward
pass on the network. Furthermore, the joint feature vectors Ψ(xi, ˆyi, ˆhi) and Ψ(xi, yi, h∗
i ) can be
computed using a backward pass on the network.

Sketch of the Proof. Recall that the latent space consists of the putative activations for each PL
operation in the layers following the current one. Thus, intuitively, the maximization over the latent
variables corresponds to ﬁnding the exact activations of all such PL operations. In other words,
we need to identify the indices of the linear pieces that are used to compute the value of the PL

6

function in the current state of the network. For a ReLU operation, this corresponds to estimating
max{0, v}, where the input to the ReLU is a scalar v. Similarly, for a max-pool operation, this
corresponds to estimating maxi{e(cid:62)
i u}, where u is the input vector to the max-pool. This is precisely
the computation that the forward pass of backpropagation performs. Given the activations, the joint
feature vector is the subgradient of the sample with respect to the current layer. Once again, this is
precisely what is computed during the backward pass of the backpropagation algorithm.

An example is constructed in Appendix B to illustrate how to compute the feature vectors in practice.

4.3

IMPROVING THE BCFW ALGORITHM

As the BCFW algorithm was originally designed to solve a structured SVM problem, it requires
further extensions to be suitable for training a PL-CNN. In what follows, we present three such
extensions that improve the initialization, memory requirements and time complexity of the BCFW
algorithm respectively.

Trust-Region for Initialization. The original BCFW algorithm starts with an initial parameter
W l = 0 (that is, all the parameters are set to 0). The reason for this initialization is that it is possible
to compute the dual variables that correspond to the 0 primal variable. However, since our algorithm
visits each layer of the network several times, it would be desirable to initialize its parameters using
its current value W t
l . To this end, we introduce a trust-region in the constraints of problem (7), or
equivalently, an (cid:96)2 norm based proximal term in its objective function (Parikh & Boyd, 2014). The
following proposition shows that this has the desired effect of initializing the BCFW algorithm close
to the current parameter values.
Proposition 4. By adding a proximal term µ
F to the objective function in (7), we
can compute a feasible dual solution whose corresponding primal solution is equal to µ
λ+µ W l
t .
Furthermore, the addition of the proximal term still allows us to efﬁciently compute the conditional
gradient using a forward-backward pass (proof in Appendix D).

2 (cid:107)W l − W l

t (cid:107)2

In practice, we always choose a value of µ = 10λ: this yields an initialization of (cid:39) 0.9W l
does not signiﬁcantly change the value of the objective function.

t which

Efﬁcient Representation of Joint Feature Vectors. The BCFW algorithm requires us to store a
linear combination of the feature vectors for each mini-batch. While this requirement is not too
stringent for convolutional and multi-class SVM layers, where the dimensionality of the feature
vectors is small, it becomes prohibitively expensive for dense layers. The following proposition
prevents a blow-up in the memory requirements of BCFW.
Proposition 5. When optimizing dense layer l, if W l ∈ Rp×q, we can store a representation of the
joint feature vectors Ψ(x, y, h) with vectors of size p in problems (6) and (7). This is in contrast to
the na¨ıve approach that requires them to be of size p × q.

Sketch of the Proof. By Proposition (3), the feature vectors are subgradients of the hinge loss function,
∂zl ·(cid:0)zl−1(cid:1)T
which we loosely denote by η for this proof. Then by the chain rule: ∂η
.
Noting that zl−1 ∈ Rq is a forward pass up until layer l (independent of W l), we can store only
∂zl ∈ Rp and still reconstruct the full feature vector ∂η
∂η

∂W l by a forward pass and an outer product.

∂zl
∂W l = ∂η

∂W l = ∂η

∂zl

Reducing the Number of Constraints.
In order to reduce the amount of time required for the
BCFW algorithm to converge, we use the structure of H to simplify problem (7) to a much simpler
problem. Speciﬁcally, since H represents the activations of the network for a given sample, it has a
natural decomposition over the layers: H = H1 × ... × HL. We use this structure in the following
observation.

Observation 1. Problem (7) can be approximately solved by optimizing the dual problem on increas-
ingly large search spaces. In other words, we start with constraints of Y, followed by Y × HL, then
Y × HL × HL−1 and so on. The algorithm converges when the primal-dual gap is below tolerance.

7

The latent variables which are not optimized over are set to be the same as the ones selected for the
ground truth. Experimentally, we observe that for convolutional layers (architectures in section 5),
restricting the search space to Y yields a dual gap low enough to consider the problem has converged.
This means that in practice for these layers, problem (7) can be solved by searching directions
over the search space Y instead of the much larger Y × H. The intuition is that the norm of the
difference-of-convex decomposition grows with the number of activations selected differently in the
convex and concave parts (see Appendix A for the decomposition of piecewise linear functions).
This compels the path of activations to be the same in the convex and the concave part to avoid large
margin violations, especially for convolutional layers which are followed by numerous non-linearities
at the max-pooling layers.

5 EXPERIMENTS

Our experiments are designed to assess the ability of LW-SVM (Layer-Wise SVM, our method) and
the SGD baselines to optimize problem (3). To compare LW-SVM with the state-of-the-art variants
of backpropagation, we look at the training and testing accuracies as well as the training objective
value. Unlike dropout, which effectively learns an ensemble model, we learn a single model using
each baseline optimization algorithm. All experiments are conducted on a GPU (Nvidia Titan X)
and use Theano (Bergstra et al., 2010; Bastien et al., 2012). We compare LW-SVM with Adagrad,
Adadelta and Adam. For all data sets, we start at a good solution provided by these solvers and
ﬁne-tune it with LW-SVM. We then check whether a longer run of the SGD solver reaches the same
level of performance.

The practical use of the LW-SVM algorithm needs choices at the three following levels: how to
select the layer to optimize (i), when to stop the CCCP on each layer (ii) and when to stop the
convex optimization at each inner iteration of the CCCP (iii). These choices are detailed in the next
paragraph.

The layer-wise schedule of LW-SVM is as follows: as long as the validation accuracy increases, we
perform passes from the end of the network (SVM) to the ﬁrst layer (i). At each pass, each layer is
optimized with one outer iteration of the CCCP (ii). The inner iterations are stopped when the dual
objective function does not increase by more than 1% over an epoch (iii). We point out that the dual
objective function is cheap to compute since we are maintaining its value at all time. By contrast, to
compute the exact primal objective function requires a forward pass over the data set without any
update.

5.1 MNIST DATA SET

Data set & Architecture The training data set consists in 60,000 gray scale images of size 28 × 28
with 10 classes, which we split into 50,000 samples for training and 10,000 for validating. The
images are normalized, and we do not use any data augmentation. The architecture used for this
experiment is shown in Figure 1.

Figure 1: Network architecture for the MNIST data set.

Method The number of epochs is set to 200, 100 and 100 for Adagrad, Adadelta and Adam -
Adagrad is given more epochs as we observed it took a longer time to converge. We then use LW-
SVM and compare the results on training objective, training accuracy and testing accuracy. We also
let the solvers run to up to 500 epochs to verify that we have not stopped the optimization prematurely.
The regularization hyperparameter λ and the initial learning rate are chosen by cross-validation. λ is
set to 0.001 for all solvers, and the initial learning rates can be found in Appendix C. For LW-SVM,
λ is set to the same value as the baseline, and the proximal term µ to µ = 10λ = 0.01.

8

Table 1: Results on MNIST: we compare the performance of LW-SVM with SGD algorithms on three
metrics: training objective, training accuracy and testing accuracy. LW-SVM outperforms Adadelta
and Adam on all three metrics, with marginal improvements since those ﬁnd already very good
solutions.

Solver (epochs)

Adagrad (200)
Adagrad (500)
Adagrad (200) + LW-SVM
Adadelta (100)
Adadelta (500)
Adadelta (100) + LW-SVM
Adam (100)
Adam (500)
Adam (100) + LW-SVM

Training
Training
Objective Accuracy
99.94%
99.96%
99.94%
99.56%
99.48%
99.85%
99.76%
99.72%
99.89%

0.027
0.024
0.025
0.049
0.048
0.033
0.038
0.038
0.029

Time (s)

707
1759
707+366
124
619
124+183
333
1661
333+353

Testing
Accuracy
99.22%
99.20%
99.21%
98.96%
99.05%
99.24%
99.19%
99.23%
99.23%

Figure 2: Results on MNIST of Adagrad, Adadelta and Adam followed by LW-SVM. We verify that
switching to LW-SVM leads to better solutions than running SGD longer (shaded continued plots).

Results As Table 1 shows, LW-SVM systematically improves on all training objective, training
accuracy and testing accuracy. In particular, it obtains the best testing accuracy when combined
with Adadelta. Because each convex sub-problem is run up to sufﬁcient convergence, the objective
function of LW-SVM features of monotonic decrease at each iteration of the CCCP (blue curves in
ﬁrst row of Figure 2).

5.2 CIFAR DATA SETS

Data sets & Architectures The CIFAR-10/100 data sets are comprised of 60,000 RGB natural
images of size 32 × 32 with 10/100 classes (Krizhevsky, 2009)). We split the training set into 45,000
training samples and 5,000 validation samples in both cases. The images are centered and normalized,
and we do not use any data augmentation. To obtain a strong enough baseline, we employ (i) a
pre-training with a softmax and cross-entropy loss and (ii) Batch-Normalization (BN) layers before
each non-linearity.

We have experimentally found out that pre-training with a softmax layer followed by a cross-entropy
loss led to better behavior and results than using an SVM loss alone. The baselines are trained with
batch normalization. Once they have converged, the estimated mean and standard deviation are ﬁxed
like they would be at test time. Then batch normalization becomes a linear transformation, which
can be handled by the LW-SVM algorithm. This allows us to compare LW-SVM with a baseline
beneﬁting from batch normalization. Speciﬁcally, we use the architecture shown in Figure 3:

9

Figure 3: Network architecture for the CIFAR data sets.

Method Again, the initial learning rates and regularization weight λ are obtained by cross-
validation, and a value of 0.001 is obtained for λ for all solvers on both datasets. As before, µ
is set to 10λ. The initial learning rates are reported in Appendix C. The layer schedule and conver-
gence criteria are as described at the beginning of the section. For each SGD optimizer, we train the
network for 10 epochs with a cross-entropy loss (preceded by a softmax layer). Then it is trained with
an SVM loss (without softmax) for respectively 1000, 100 and 100 epochs for Adagrad, Adadelta and
Adam. This amount is doubled to verify that the baselines are not harmed by a premature stopping.
Results are presented in Tables 2 and 3.

Table 2: Results on CIFAR-10: LW-SVM outperforms Adam and Adadelta on all three metrics. It
improves on Adagrad, but does not outperform it - however Adagrad takes a long time to converge
and does not obtain the best generalization.

Solver (epochs)

Adagrad (1000)
Adagrad (2000)
Adagrad (1000) + LW-SVM
Adadelta (100)
Adadelta (200)
Adadelta (100) + LW-SVM
Adam (100)
Adam (200)
Adam (100) + LW-SVM

Time (h)

10.58
21.14

Training
Training
Objective Accuracy
98.42%
100.00%
100.00% 10.58+1.66
97.96%
99.83%
100.00%
98.27%
99.76%
100.00%

0.83
1.66
0.83+0.68
0.83
1.65
0.83+1.07

0.059
0.009
0.012
0.113
0.054
0.038
0.113
0.055
0.034

Testing
Accuracy
83.15%
83.84%
83.43%
84.42%
85.02%
86.62%
84.18%
82.55%
85.52%

Table 3: Results on CIFAR-100: LW-SVM improves on all other solvers and obtains the best testing
accuracy.

Solver (epochs)

Adagrad (1000)
Adagrad (2000)
Adagrad (1000) + LW-SVM
Adadelta (100)
Adadelta (200)
Adadelta (100) + LW-SVM
Adam (100)
Adam (200)
Adam (100) + LW-SVM

Training
Training
Objective Accuracy
95.36%
99.98%
99.98%
95.68%
99.90%
99.98%
95.79%
99.87%
99.98%

0.201
0.044
0.062
0.204
0.088
0.052
0.221
0.088
0.059

Time (h)

10.68
21.20
10.68+3.40
0.84
1.67
0.84+1.48
0.84
1.66
0.84+1.69

Testing
Accuracy
54.00%
54.55%
53.97%
58.71%
58.03%
61.20%
58.32%
57.81%
60.17%

It can be seen from this set of results that LW-SVM always improves over the solution of
Results
the SGD algorithm, for example on CIFAR-100, decreasing the objective value of Adam from 0.22 to
0.06, or improving the test accuracy of Adadelta from 84.4% to 86.6% on CIFAR-10. The automatic
step-size allows for a precise ﬁne-tuning to optimize the training objective, while the regularization
of the proximal term helps for better generalization.

10

Figure 4: Results on CIFAR-10 of Adagrad, Adadelta and Adam followed by LW-SVM. The successive
drops of the training objective function with LW-SVM correspond to the passes over the layers.

Figure 5: Results on CIFAR-100 of Adagrad, Adadelta and Adam followed by LW-SVM. Although
Adagrad keeps improving the training objective function, it takes much longer to converge and the
improvement on the training and testing accuracies rapidly become marginal.

5.3

IMAGENET DATA SET

We show results on the classiﬁcation task of the ImageNet data set (Russakovsky et al., 2015).
The ImageNet data set contains 1.2 million images for training and 50,000 images for validation,
each of them mapped to one of the 1,000 classes. For this experiment we use a VGG-16 network
(conﬁguration D in (Simonyan & Zisserman, 2015)). We start with a pre-trained model as publicly
available online, and we tune each of the dense layers as well as the ﬁnal SVM layer with the
LW-SVM algorithm. This experiment is designed to test the scalability of LW-SVM to large data
sets and large networks, rather than comparing with the optimization baselines as before - indeed for
any baseline, obtaining proper convergence as in previous experiments would take a very long time.
We set the hyperparameters λ to 0.001 and µ to 10λ as previously. We budget ﬁve epochs per layer,
which in total takes two days of training on a single GPU (Nvidia Titan X). At training time we used
centered crops of size 224 × 224. The evaluation method is the same as the single test scale method
described in (Simonyan & Zisserman, 2015). We report the results on the validation set in Table 4,
for the Pre-Trained model (PT) and the same model further optimized by LW-SVM (PT+LW-SVM):

11

Table 4: Results on the 1,000-way classiﬁcation challenge of ImageNet on the validation set, for the
Pre-Trained model (PT) and the same model further optimized by LW-SVM (PT+LW-SVM).

Network
VGG-16 (PT)
VGG-16 (PT + LW-SVM)

Top-1 Accuracy Top-5 Accuracy
91.33%
91.61%

73.30%
73.81%

Since the objective function penalizes the top-1 error, it is logical to observe that the improvement
is most important on the top-1 accuracy. Importantly, having an efﬁcient representation of feature
vectors proves to be essential for such large networks: for instance, in the optimization of the ﬁrst
fully connected layer with a batch-size of 100, the use of our representation lowers the memory
requirements of the BCFW algorithm from 7,600GB to 20GB, which can then ﬁt in the memory of a
powerful computer.

6 DISCUSSION

We presented a novel layerwise optimization algorithm for a large and useful class of convolutional
neural networks, which we term PL-CNNs. Our key observation is that the optimization of the
parameters of one layer of a PL-CNN is equivalent to solving a latent structured SVM problem. As
the problem is a DC program, it naturally lends itself to the iterative CCCP approach, which optimizes
a convex structured SVM objective at each iteration. This allows us to leverage the advancements
made in structured SVM optimization over the past decade to design a computationally feasible
approach for learning PL-CNNs. Speciﬁcally, we use the BCFW algorithm and extend it to improve
its initialization, memory requirements and time complexity. In particular, this allows our method
to not require the tuning of any learning rate. Using the publicly available MNIST, CIFAR-10 and
CIFAR-100 data sets, we show that our approach provides a boost for learning PL-CNNs over the
state of the art backpropagation algorithms. Furthermore, we demonstrate scalability of the method
with results on the ImageNet data set with a large network.

When the mean and standard deviation estimations of batch normalization are not ﬁxed (unlike
in our experiments with LW-SVM), batch normalization is not a piecewise linear transformation,
and therefore cannot be used in conjunction with the BCFW algorithm for SVMs. However, it is
difference-of-convex as it is a C2 function (Horst & Thoai, 1999). Incorporating a normalization
scheme into our framework will be the object of future work. With our current methodology, LW-SVM
algorithm can already be used on most standard architectures like VGG, Inception and ResNet-type
architectures.

It is worth noting that other approaches for solving structured SVM problems, such as cutting-
plane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient
descent (Shalev-Shwartz et al., 2009), also rely on the efﬁciency of estimating the conditional
gradient of the dual. Hence, all these methods are equally applicable to our setting. Indeed, the
main strength of our approach is the establishment of a hitherto unknown connection between CNNs
and latent structured SVMs. We believe that our observation will allow researchers to transfer the
substantial existing knowledge of DC programs in general, and latent SVMs speciﬁcally, to produce
the next generation of principled optimization algorithms for deep learning. In fact, there are already
several such improvements that can be readily applied in our setting, which were not explored only
due to a lack of time. This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al.,
2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al., 2016).

ACKNOWLEDGMENTS

This work was supported by the EPSRC AIMS CDT grant EP/L015987/1, the EPSRC Programme
Grant Seebibyte EP/M013774/1 and Yougov. Many thanks to A. Desmaison, R. Bunel and D.
Bouchacourt for the helpful discussions.

12

REFERENCES

arXiv:1609.07152, 2016.

Brandon Amos, Lei Xu, and J. Zico Kolter.

Input convex neural networks. arXiv preprint

Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements,
2012.

Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training of

deep networks. Conference on Neural Information Processing Systems, 2007.

James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU
math expression compiler. Python for Scientiﬁc Computing Conference (SciPy), 2010.

Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. Conference

on Neural Information Processing Systems, 2015.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 2011.

Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in

polynomial time. arXiv preprint arXiv:1611.10258, 2016.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Uri Heinemann, Roi Livni, Elad Eban, Gal Elidan, and Amir Globerson. Improper deep kernels.

International Conference on Artiﬁcial Intelligence and Statistics, 2016.

Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief

nets. Neural computation, 2006.

and Applications, 1999.

Reiner Horst and Nguyen V. Thoai. DC programming: overview. Journal of Optimization Theory

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. International Conference on Machine Learning, 2015.

Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training of structural

SVMs. Machine Learning, 2009.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

International

Conference on Learning Representations, 2015.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University

of Toronto, 2009.

Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, and Patrick Pletscher. Block-coordinate Frank-
Wolfe optimization for structural SVMs. International Conference on Machine Learning, 2013.

Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2015.

James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate

curvature. International Conference on Machine Learning, 2015.

James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free optimiza-

tion. Neural Networks: Tricks of the Trade, 2012.

D. Melzer. On the expressibility of piecewise-linear continuous functions as the difference of two

piecewise-linear convex functions. Springer Berlin Heidelberg, 1986.

Pritish Mohapatra, Puneet Dokania, CV Jawahar, and M Pawan Kumar. Partial linearization based

optimization for multi-class SVM. European Conference on Computer Vision, 2016.

13

Yann Ollivier. Riemannian metrics for neural networks. Information and Inference: a Journal of the

IMA, 2013.

Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet Dokania, and Simon Lacoste-
Julien. Minding the gaps for block Frank-Wolfe optimization of structured SVMs. Inernational
Conference on Machine Learning, 2016.

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 2014.

David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-

propagating errors. Nature, 1986.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.

Neel Shah, Vladimir Kolmogorov, and Christoph H. Lampert. A multi-plane block-coordinate
Frank-Wolfe algorithm for training structural SVMs with a costly max-oracle. Conference on
Computer Vision and Pattern Recognition, 2015.

Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient

solver for SVM. International Conference on Machine Learning, 2009.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. International Conference on Learning Representations, 2015.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 2014.

Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable ADMM approach. International Conference on
Machine Learning, 2016.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector
machine learning for interdependent and structured output spaces. International Conference on
Machine Learning, 2004.

Chun-Nam John Yu and Thorsten Joachims. Learning structural SVMs with latent variables. Interna-

tional Conference on Machine Learning, 2009.

Alan L. Yuille and Anand Rangarajan. The concave-convex procedure (CCCP). Conference on

Neural Information Processing Systems, 2002.

Matthew Zeiler. ADADELTA: an adaptive learning rate method. CoRR, 2012.

Yuchen Zhang, Percy Liang, and Martin J. Wainwright. Convexiﬁed convolutional neural networks.

arXiv preprint arXiv:1609.01000, 2016.

14

A PIECEWISE LINEAR FUNCTIONS

Proof of Proposition (1) By the deﬁnition from (Melzer, 1986), we can write each function as the
difference of two point-wise maxima of linear functions:

g(v) = max
j∈[m+]

{a(cid:62)

And ∀i ∈ [n], gi(u) = g+

{b(cid:62)

j v}

i v} − max
k∈[m−]
i (u) − g−

i (u)

Where all the g+

i , g−

i are linear point-wise maxima of linear functions. Then:

f (u) = g([g1(u), · · · , gn(u)](cid:62))
{a(cid:62)

= max
j∈[m+]

j [g1(u), · · · , gn(u)](cid:62)} − max
k∈[m−]

{b(cid:62)

k [g1(u), · · · , gn(u)](cid:62)}

= max
j∈[m+]

aj,igi(u)

− max
k∈[m−]

(cid:41)

n
(cid:88)

i=1

(cid:40) n
(cid:88)

(cid:41)

bk,igi(u)

i=1

(cid:41)

− max
k∈[m−]

(cid:40) n
(cid:88)

i=1

aj,ig+

i (u) −

aj,ig−

i (u)

bk,ig+

i (u) −

bk,ig−

i (u)

(cid:41)

n
(cid:88)

i=1

aj,ig+

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u)

(cid:88)

n
(cid:88)

aj,ig−

i (u)

j(cid:48)∈[m+]\{j}

i=1

j(cid:48)∈[m+]

i=1

= max
j∈[m+]

= max
j∈[m+]

− max
k∈[m−]

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

+

(cid:88)

n
(cid:88)

bk,ig−

i (u)

k(cid:48)∈[m−]\{k}

i=1

k(cid:48)∈[m−]

i=1

= max
j∈[m+]



−

 max
k∈[m−]

n
(cid:88)



i=1

aj,ig+

i (u) +

aj,ig−

i (u)

+

bk,ig−

i (u)

(cid:88)

n
(cid:88)

j(cid:48)∈[m+]\{j}

i=1

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

(cid:88)

n
(cid:88)



aj,ig−

i (u)



k(cid:48)∈[m−]\{k}

i=1

j(cid:48)∈[m+]

i=1

n
(cid:88)

i=1

(cid:88)

k(cid:48)∈[m−]



+



= max
j∈[m+]

aj,ig+

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

j(cid:48)∈[m+]\{j}

i=1

k(cid:48)∈[m−]

i=1

− max
k∈[m−]

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u)

k(cid:48)∈[m−]\{k}

i=1

j(cid:48)∈[m+]

i=1











(cid:40) n
(cid:88)

i=1
(cid:40) n
(cid:88)

i=1

n
(cid:88)






i=1



n
(cid:88)



i=1






n
(cid:88)

i=1



n
(cid:88)






i=1



n
(cid:88)



i=1

−
















In each line of the last equality, we recognize a pointwise maximum of a linear combination of
pointwise maxima of linear functions. This constitutes a pointwise maximum of linear functions.

This derivation also extends equation (10) to the multi-dimensional case by showing an explicit

DC decomposition of the output.

B COMPUTING THE FEATURE VECTORS

We describe here how to compute the feature vectors in practice. To this end, we show how to
construct two (intertwined) neural networks that decompose the objective function into a convex
and a concave part. We call these Difference of Convex (DC) networks. Once the DC networks are
deﬁned, a standard forward and backward pass in the two networks yields the feature vectors for the
convex and concave contribution to the objective function. First, we derive how to perform a DC
decomposition in linear and non-linear layers, and then we construct an example of DC networks.

15

2 (|W | + W ) and W − = 1

DC Decomposition in a Linear Layer Let W be the weights of a ﬁxed linear layer. We introduce
W + = 1
2 (|W | − W ). We can note that W + and W − have exclusively
non-negative weights, and that W = W + − W −. Say we have an input u with the DC decomposition
(ucvx, uccv), that is: u = ucvx − uccv, where both ucvx and uccv are convex. Then we can decompose
the output of the layer as:

W · u = (W + · ucvx + W − · uccv)
(cid:125)

(cid:124)

(cid:123)(cid:122)
convex

− (W − · ucvx + W + · uccv)
(cid:123)(cid:122)
(cid:125)
convex

(cid:124)

(9)

DC Decomposition in a Piecewise Linear Activation Layer For simplicity purposes, we consider
that the non-linear layer is a point-wise maximum across [K] scalar inputs, that is, for an input
(uk)k∈[K] ∈ RK, the output is maxk∈[K] uk (the general multi-dimensional case can be found in
Appendix A). We suppose that we have a DC decomposition (ucvx
k ) for each input k. Then we
can write the following decomposition for the output of the layer:

k , uccv

max
k∈[K]

uk = max
k∈[K]

(ucvx

k − uccv
k )


ucvx

k +

(cid:88)

uccv
i



−

i∈[K],i(cid:54)=k
(cid:123)(cid:122)
convex



(cid:125)

(cid:88)

uccv
k

k∈[K]
(cid:123)(cid:122)
(cid:124)
convex

(cid:125)

= max
k∈[K]

(cid:124)

In particular, for a ReLU, we can write:

max(ucvx − uccv, 0) = max(ucvx, uccv)
(cid:125)

(cid:124)

− uccv
(cid:124)(cid:123)(cid:122)(cid:125)
convex

(cid:123)(cid:122)
convex

And for a Max-Pooling layer, one can easily verify that equation (10) is equivalent to:

M axP ool(ucvx − uccv) = M axP ool(ucvx − uccv) + SumP ool(uccv)
(cid:125)

(cid:124)

(cid:123)(cid:122)
convex

− SumP ool(uccv)
(cid:123)(cid:122)
(cid:125)
convex

(cid:124)

(12)

An Example of DC Networks We use the previous observations to obtain a DC decomposition in
any layer. We now take the example of the neural network used for the experiments on the MNIST
data set, and we show how to construct the two neural networks when optimizing W 1, the weights of
the ﬁrst convolutional layer. First let us recall the architecture without decomposition:

(10)

(11)

Figure 6: Detailed network architecture for the MNIST data set.

We want to optimize the ﬁrst convolutional layer, therefore we ﬁx all other parameters. Then we apply
all operations as described in the previous paragraphs, which yields the DC networks in Figure 7.
The network graph in Figure 7 illustrates Proposition 3 for the optimization of W 1: suppose we are
interested in f cvx(x, W 1), the convex part of the objective function for a given sample x, and we
wish to obtain the feature vector needed to perform an update of BCFW. With a forward pass, the
oracle for the latent and label variables (ˆh, ˆy) is efﬁciently computed; and with a backward pass, we
obtain the corresponding feature vector Ψ(x, ˆy, ˆh). Indeed, we recall from problem (8) that (ˆh, ˆy)
are the latent and label variables maximizing f cvx(x, W 1). Then given x, the forward pass in the DC
networks sequentially solves the nested maximization: it maximizes the activation of the ReLU and
MaxPooling units at each layer, thereby selecting the best latent variable ˆh at each non-linear layer,
and maximizes the output of the SVM layer, thereby selecting the best label ˆy. At the end of the
forward pass, f cvx(x, W 1) is therefore available as the output of the convex network, and the feature
vector Ψ(x, ˆy, ˆh) can be computed as a subgradient of f cvx(x, W 1) with respect to W 1.

16

Figure 7: Difference of Convex Networks for the optimization of Conv1 in the MNIST architecture.
The two leftmost columns represent the DC networks. For each layer, the right column indicates the
non-decomposed corresponding operation. Note that we represent the DC decomposition of the SVM
layer as unique blocks to keep the graph simple. Given the decomposition method for linear and
non-linear layers, one can write down the explicit operations without special difﬁculty.

Linearizing the concave part is equivalent to ﬁxing the activations of the DC networks, which can be
done by using a ﬁxed copy of W 1 at the linearization point (all other weights being ﬁxed anyway).
Then one can re-use the above reasoning to obtain the feature vectors for the linearized concave part.
Altogether, this methodology allows our algorithm to be implemented in any standard deep learning
library (our implementation is available at http://github.com/oval-group/pl-cnn).

17

C EXPERIMENTAL DETAILS

Hyper-parameters The hyper-parameters are obtained by cross-validation with a search on powers
of 10. In this section, η will denote the initial learning rate. We denote the Softmax + Cross-Entropy
loss by SCE, while SVM stands for the usual Support Vector Machines loss.

Table 5: Hyper-parameters for the SGD solvers

MNIST

η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

CIFAR-10
(SCE)
η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

CIFAR-10
(SVM)
η = 0.001
λ = 0.001
η = 0.1
λ = 0.001
η = 0.0001
λ = 0.001

CIFAR-100 CIFAR-100

(SCE)
η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

(SVM)
η = 0.001
λ = 0.001
η = 0.1
λ = 0.001
η = 0.0001
λ = 0.001

Adagrad

Adadelta

Adam

One may note that the hyper-parameters are the same for both CIFAR-10 and CIFAR-100 for each
combination of solver and loss. This makes sense since the initial learning rate mainly depends on
the architecture of the network (and not so much on which particular images are fed to this network),
which is very similar for the experiments on the CIFAR-10 and CIFAR-100 data sets.

D SVM FORMULATION & DUAL DERIVATION

Multi-Class SVM Suppose we are given a data set of N samples, for which every sample i has
a feature vector φi ∈ Rd and a ground truth label yi ∈ Y. For every possible label ¯yi ∈ Y, we
introduce the augmented feature vector ψi( ¯yi) ∈ R|Y|×d containing φi at index ¯yi, −φi at index yi,
and zeros everywhere else (then ψi(yi) is just a vector of zeros). We also deﬁne ∆( ¯yi, yi) as the loss
by choosing the output ¯yi instead of the ground truth yi in our task. For classiﬁcation, this is the
zero-one loss for example.

The SVM optimization problem is formulated as:

min
w,ξi

λ
2

(cid:107)w(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

Where λ is the regularization hyperparameter. We now add a proximal term to a given starting point
w0:

min
w,ξi

λ
2

µ
2

(cid:107)w(cid:107)2 +

(cid:107)w − w0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

Factorizing the second-order polynomial in w, we obtain the equivalent problem (changed by a
constant):

min
w,ξi

λ + µ
2

(cid:107)w −

µ
λ + µ

w0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

For simplicity, we introduce the ratio ρ =

µ
λ + µ

.

18

Dual Objective function The primal problem is:

min
w,ξi

λ + µ
2

(cid:107)w − ρw0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

αi( ¯yi) (cid:0)∆(yi, ¯yi) + wT ψi( ¯yi) − ξi

(cid:1)

The dual problem can be written as:

max
α≥0

min
w,ξi

λ + µ
2

(cid:107)w − ρw0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi +

1
N

N
(cid:88)

(cid:88)

i=1

¯yi∈Y

Then we obtain the following KKT conditions:

∀i ∈ [N ],

= 0 −→

αi( ¯yi) = 1

(cid:88)

¯yi∈Y

∂·
∂ξi

∂·
∂w

= 0 −→ w = ρw0 −

αi( ¯yi)ψi( ¯yi)

1
N

1
λ + µ

(cid:124)

N
(cid:88)

(cid:88)

i=1

¯yi∈Y
(cid:123)(cid:122)
Aα

(cid:125)

We also introduce b = 1

N (∆(yi, ¯yi))i, ¯yi. We deﬁne Pn(Y) as the sample-wise probability simplex:

We inject back and simplify to:

Finally:

Where:

u ∈ Pn(Y) if: ∀i ∈ [N ], ∀ ¯yi ∈ Y, ui( ¯yi) ≥ 0

∀i ∈ [N ],

ui( ¯yi) = 1

(cid:88)

¯yi∈Y

max
α∈Pn(Y)

−(λ + µ)
2

(cid:107)Aα(cid:107)2 + µwT

0 (Aα) + αT b

min
α∈Pn(Y)

f (α)

f (α) (cid:44) λ + µ

2

(cid:107)Aα(cid:107)2 − µwT

0 (Aα) − αT b

BCFW derivation We write ∇(i)f the gradient of f w.r.t. the block (i) of variables in α, padded
with zeros on blocks (j) for j (cid:54)= i. Similarly, A(i) and b(i) contain the rows of A and the elements of
b for the block of coordinates (i) and zeros elsewhere. We can write:

∇(i)f (α) = (λ + µ)AT

(i)Aα − µA(i)w0 − b(i)

Then the search corner for the block of coordinates (i) is given by:

si = argmin

(cid:0)< s(cid:48)

i, ∇(i)f (α) >(cid:1)

s(cid:48)
i

(cid:16)

= argmin
s(cid:48)
i

(λ + µ)αT AT A(i)s(cid:48)

i − µwT

0 A(i)s(cid:48)

i − bT

(i)s(cid:48)
i

(cid:17)

We replace:

Aα = ρw0 − w
1
N

1
λ + µ

i =

A(i)s(cid:48)

(cid:88)

¯yi∈Y

s(cid:48)
i( ¯yi)ψi( ¯yi)

(i)s(cid:48)
bT

i =

s(cid:48)
i( ¯yi)∆( ¯yi, yi)

1
N

(cid:88)

¯yi∈Y

19

We then obtain:

−(w − ρw0)T (cid:88)

si = argmin

s(cid:48)
i

¯yi∈Y

i( ¯yi)ψi( ¯yi) − wT
s(cid:48)
0 ρ

s(cid:48)
i( ¯yi)ψi( ¯yi) −

s(cid:48)
i( ¯yi)∆( ¯yi, yi)



(cid:88)

¯yi∈Y



(cid:88)

¯yi∈Y



= argmax
s(cid:48)
i


wT (cid:88)

¯yi∈Y

s(cid:48)
i( ¯yi)ψi( ¯yi) +

s(cid:48)
i( ¯yi)∆( ¯yi, yi)



(cid:88)

¯yi∈Y

As expected, this maximum is obtained by setting si to one at y∗

and zeros elsewhere. We introduce the notation:

i = argmax

¯yi∈Y

(cid:0)wT ψi( ¯yi) + ∆( ¯yi, yi)(cid:1)

wi = −A(i)α(i)
li = bT
(i)α(i)
ws = −A(i)si
ls = bT

(i)si

Then we have:

ws = −

1
N

1
λ + µ

ψ(y∗

i ) = −

1
N

1
λ + µ

∂Hi(y∗
i )
∂w

ls =

∆(yi, y∗
i )

1
N

The optimal step size in the direction of the block of coordinates (i) is given by :
γ∗ = argmin

f (α + γ(si − αi))

γ

The optimal step-size is given by:

γ∗ =

< ∇(i)f (α), si − αi >
(λ + µ)(cid:107)A(si − αi)(cid:107)2

We introduce wd = −Aα = w − ρw0. Then we obtain:
(wi − ws)T (w − ρw0) + ρwT

γ∗ =

(cid:107)wi − ws(cid:107)2

0 (wi − ws) − 1

λ+µ (li − ls)

=

(wi − ws)T w − 1
(cid:107)wi − ws(cid:107)2

λ+µ (li − ls)

And the updates are the same as in standard BCFW:

Algorithm 2 BCFW with warm start

1: Let w(0) = w0,
2: Let l(0) = 0,
3: for k=0...K do
4:

∀i ∈ [N ], w(0)
l(0)
i = 0

∀i ∈ [N ],

i = 0

5:

6:

7:

8:

9:

10:

Pick i randomly in {1, .., n}

Get y∗

ls = 1

γ =

i = argmax
¯yi∈Y
N ∆(y∗
i , yi)
(wi − ws)T w − 1
(cid:107)wi − ws(cid:107)2

i + γws

w(k+1)
= (1 − γ)w(k)
i
= (1 − γ)l(k)
l(k+1)
i
w(k+1) = w(k) + w(k+1)
i
l(k+1) = l(k) + l(k+1)

i + γls

− l(k)
i

i

11:
12: end for

Hi(¯yi, w(k)) and ws = −

1
N

1
λ + µ

∂Hi(y∗

i , w(k))

∂w(k)

λ+µ (li − ls)

clipped to [0, 1]

− w(k)

i = w(k) + γ(w(k)

s − w(k)

i

)

20

In particular, we have proved Proposition (4) in this section: w is initialized to ρw0 (KKT conditions),

and the direction of the conditional gradient, ws, is given by

, which is independent of w0.

∂Hi(y∗
i )
∂w

Note that the derivation of the Lagrangian dual has introduced a dual variable αi( ¯yi) for each linear
constraint of the SVM problem (this can be replaced by αi(hi, ( ¯yi)) if we consider latent variables).
These dual variables indicate the complementary slackness not only for the output class ¯yi, but also
for each of the activation which deﬁnes a piece of the piecewise linear hinge loss. Therefore a choice
of α deﬁnes a path of activations.

E SENSITIVITY OF SGD ALGORITHMS

Here we discuss some weaknesses of the SGD-based algorithms that we have encountered in practice
for our learning objective function. These behaviors have been observed in the case of PL-CNNs,
and generally may not appear in different architectures (in particular the failure to learn with high
regularization goes away with the use of batch normalization layers).

E.1

INITIAL LEARNING RATE

As mentioned in the experiments section, the choice of the initial learning rate is critical for good
performance of all Adagrad, Adadelta and Adam. When the learning rate is too high, the network
does not learn anything and the training and validating accuracies are stuck at random level. When it
is too low, the network may take a considerably greater number of epochs to converge.

E.2 FAILURES TO LEARN

Regularization When the regularization hyper-parameter λ is set to a value of 0.01 or higher on
CIFAR-10, SGD solvers get trapped in a local minimum and fail to learn. The SGD solvers indeed
fall in the local minimum of shutting down all activations on ReLUs, which provide zero-valued
feature vector to the SVM loss layer (and a hinge loss of one). As a consequence, no information can
be back-propagated. We plot this behavior below:

Figure 8: Behavior of different algorithms for λ = 0.01. The x-axis has been rescaled to compare
the evolution of all algorithms (real training times vary between half an hour to a few hours for the
different runs).

In this situation, the network is at a bad saddle point (note that the training and validation accuracies
are stuck at random levels). Our algorithm does not fall into such bad situations, however it is not
able to get out of it either: each layer is at a pathological critical point of its own objective function,
which makes our algorithm unable to escape from it.

21

With a lower initial learning rate, the evolution is slower, but eventually the solver goes back to the
bad situation presented above.

Biases The same failing behavior as above has been observed when not using the biases in the
network. Again our algorithm is robust to this change.

22

7
1
0
2
 
r
a

M
 
6
 
 
]

G
L
.
s
c
[
 
 
5
v
5
8
1
2
0
.
1
1
6
1
:
v
i
X
r
a

TRUSTING SVM FOR PIECEWISE LINEAR CNNS

Leonard Berrada1, Andrew Zisserman1 and M. Pawan Kumar1,2
1Department of Engineering Science
University of Oxford
2Alan Turing Institute
{lberrada,az,pawan}@robots.ox.ac.uk

ABSTRACT

We present a novel layerwise optimization algorithm for the learning objective
of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of
convolutional neural networks. Speciﬁcally, PL-CNNs employ piecewise linear
non-linearities such as the commonly used ReLU and max-pool, and an SVM
classiﬁer as the ﬁnal layer. The key observation of our approach is that the prob-
lem corresponding to the parameter estimation of a layer can be formulated as a
difference-of-convex (DC) program, which happens to be a latent structured SVM.
We optimize the DC program using the concave-convex procedure, which requires
us to iteratively solve a structured SVM problem. This allows to design an opti-
mization algorithm with an optimal learning rate that does not require any tuning.
Using the MNIST, CIFAR and ImageNet data sets, we show that our approach
always improves over the state of the art variants of backpropagation and scales to
large data and large network settings.

1

INTRODUCTION

The backpropagation algorithm is commonly employed to estimate the parameters of a convolutional
neural network (CNN) using a supervised training data set (Rumelhart et al., 1986). Part of the
appeal of backpropagation comes from the fact that it is applicable to a wide variety of networks,
namely those that have (sub-)differentiable non-linearities and employ a (sub-)differentiable learning
objective. However, the generality of backpropagation comes at the cost of a high sensitivity to its
hyperparameters such as the learning rate and momentum. Standard line-search algorithms cannot be
used on the primal objective function in this setting, as (i) there may not exist a step-size guaranteeing
a monotonic decrease because of the use of sub-gradients, and (ii) even in the smooth case, each
function evaluation requires a forward pass over the entire data set without any update, making the
approach computationally unfeasible. Choosing the learning rate thus remains an open issue, with
the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012;
Kingma & Ba, 2015). In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015)
and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the
learning rate and to prevent from overﬁtting.

With this work, we open a different line of inquiry, namely, is it possible to design more robust
optimization algorithms for special but useful classes of CNNs? To this end, we focus on the networks
that are commonly used in computer vision. Speciﬁcally, we consider CNNs with convolutional and
dense layers that apply a set of piecewise linear (PL) non-linear operations to obtain a discriminative
representation of an input image. While this assumption may sound restrictive at ﬁrst, we show that
commonly used non-linear operations such as ReLU and max-pool fall under the category of PL
functions. The representation obtained in this way is used to classify the image via a multi-class
SVM, which forms the ﬁnal layer of the network. We refer to this class of networks as PL-CNN.

We design a novel, principled algorithm to optimize the learning objective of a PL-CNN. Our
algorithm is a layerwise method, that is, it iteratively updates the parameters of one layer while
keeping the other layers ﬁxed. For this work, we use a simple schedule over the layers, namely,
repeated passes from the output layer to the input one. However, it may be possible to further improve
the accuracy and efﬁciency of our algorithm by designing more sophisticated scheduling strategies.
The key observation of our approach is that the parameter estimation of one layer of PL-CNN can be

1

formulated as a difference-of-convex (DC) program that can be viewed as a latent structured SVM
problem (Yu & Joachims, 2009). This allows us to solve the DC program using the concave-convex
procedure (CCCP) (Yuille & Rangarajan, 2002). Each iteration of CCCP requires us to solve a convex
structured SVM problem. To this end, we use the powerful block-coordinate Frank-Wolfe (BCFW)
algorithm (Lacoste-Julien et al., 2013), which solves the dual of the convex program iteratively
by computing the conditional gradients corresponding to a subset of training samples. In order to
further improve BCFW for PL-CNNs, we extend it in three important ways. First, we introduce a
trust-region term that allows us to initialize the BCFW algorithm using the current estimate of the
layer parameters. Second, we reduce the memory requirement of BCFW by an order of magnitude,
via an efﬁcient representation of the feature vectors corresponding to the dense layers. Third, we
show that, empirically, the number of constraints of the structural SVM problem can be reduced
substantially without any loss in accuracy, which allows us to signiﬁcantly reduce its time complexity.

Compared to backpropagation (Rumelhart et al., 1986) or its variants (Duchi et al., 2011; Zeiler, 2012;
Kingma & Ba, 2015), our algorithm offers three advantages. First, the CCCP algorithm provides
a monotonic decrease in the learning objective at each layer. Since layerwise optimization itself
can be viewed as a block-coordinate method, our algorithm guarantees a monotonic decrease of the
overall objective function after each layer’s parameters have been updated. Second, since the dual of
the SVM problem is a smooth convex quadratic program, each step of the BCFW algorithm (in the
inner iteration of the CCCP) provides a monotonic increase in its dual objective. Third, since the
only step-size required in our approach comes while solving the SVM dual, we can use the optimal
step-size that is computed analytically during each iteration of BCFW (Lacoste-Julien et al., 2013).
In other words, our algorithm has no learning rate, initial or not, that requires tuning.

Using standard network architectures and publicly available data sets, we show that our algorithm
provides a boost over the state of the art variants of backpropagation for learning PL-CNNs and we
demonstrate scalability of the method.

2 RELATED WORK

While some of the early successful approaches for the optimization of deep neural networks relied on
greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are
variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the
introduction.

At every iteration, backpropagation performs a forward pass and a backward pass on the network, and
updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the
choice of the learning rate critical for efﬁcient optimization. Duchi et al. (2011) have proposed the
Adagrad convex solver, which adapts the learning rate for every direction and takes into account past
updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been
observed frequently in past updates. When applied to the non-convex CNN optimization problem,
Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al.,
2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the
learning rate slower. It is worth noting that this ﬁx is empirical, and to the best of our knowledge,
provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning
rate, called Adam, which uses an online estimation of the ﬁrst and second moments of the gradients
to provide centered and normalized updates. However all these methods still require the tuning of the
initial learning rate to perform well.

Second-order and natural gradient optimization methods have also been a subject of attention. The
focus in this line of work has been to come up with appropriate approximations to make the updates
cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using ﬁnite
differences to approximate the Hessian and conjugate gradient to compute the update. Martens &
Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efﬁcient
method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on
natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of
the problem. Desjardins et al. (2015) demonstrate a scaled up natural gradient descent method by
training on the ImageNet data set (Russakovsky et al., 2015). Though providing more informative
updates and solid theoretical support than SGD-based approaches, these methods do not take into
account the structure of the problem offered by the commonly used non-linear operations.

2

Our work is also related to some of the recent developments in optimization for deep learning. For
example, Taylor et al. (2016) use ADMM for massive distribution of computation in a layer-wise
fashion, and in particular their method will yield closed-form updates for any PL-CNN. Lee et al.
(2015) propose to use targets instead of gradients to propagate information through the network,
which could help to extend our algorithm. Zhang et al. (2016) derive a convex relaxation for the
learning objective for a restricted class of CNNs, which also relies on solving an approximate convex
problem. In (Amos et al., 2016), the authors identify convex problems for the inference task, when
the neural network is a convex function of some of its inputs.

With a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU
nets with guarantees of time convergence and generalization error. Heinemann et al. (2016) show that
a subclass of neural networks can be modeled as an improper kernel, which then reduces the learning
problem to a simple SVM with the constructed kernel.

More generally, we believe that our hitherto unknown observation regarding the relationship between
PL-CNNs and latent SVMs can (i) allow the progress made in one ﬁeld to be transferred to the other
and (ii) help design a new generation of principled algorithms for deep learning optimization.

3 PIECEWISE LINEAR CONVOLUTIONAL NEURAL NETWORKS

A piecewise linear convolutional neural network (PL-CNN) consists of a series of convolutional
layers, followed by a series of dense layers, which provides a concise representation of an input image.
Each layer of the network performs two operations: a linear transformation (that is, a convolution
or a matrix multiplication), followed by a piecewise linear non-linear operation such as ReLU or
max-pool. The resulting representation of the image is used for classiﬁcation via an SVM. In the
remainder of this section, we provide a formal description of PL-CNN.

Piecewise Linear Functions. A piecewise linear (PL) function f (u) is a function of the following
form (Melzer, 1986):

f (u) = max
i∈[m]

{a(cid:62)

i u} − max
j∈[n]

{b(cid:62)

j u},

(1)

where [m] = {1, · · · , m}, and [n] = {1, · · · , n}. Each of the two maxima above is a convex function,
therefore such a function f is not generally convex, but it is rather a difference of two convex functions.
Importantly, many commonly used non-linear operations such as ReLU or max-pool are PL functions
of their input. For example, ReLU corresponds to the function R(v) = max{v, 0} where v is a
scalar. Similarly, max-pool for a D-dimensional vector u corresponds to M (u) = maxi∈[D]{e(cid:62)
i u},
where ei is a vector whose i-th element is 1 and all other elements are 0. Given a value of u,
we say that (i∗, j∗) is the activation of the PL function at u if i∗ = argmaxi∈[m]{a(cid:62)
i u} and
j∗ = argmaxj∈[n]{b(cid:62)

j u}.

PL-CNN Parameters. We denote the parameters of an L layer PL-CNN by W = {W l; l ∈ [L]}.
In other words, the parameters of the l-th layer is deﬁned as W l. The CNN deﬁnes a composite
function, that is, the output zl−1 of layer l − 1 is the input to the layer l. Given the input zl−1 to
layer l, the output is computed as zl = σl(W l · zl−1), where “·” is either a convolution or a matrix
multiplication, and σl is a PL non-linear function, such as ReLU or max-pool. The input to the ﬁrst
layer is an image x, that is, z0 = x. We denote the input to the ﬁnal layer by zL = Φ(x; W) ∈ RD.
In other words, given an image x, the convolutional and dense layers of a PL-CNN provide a D-
dimensional representation of x to the ﬁnal classiﬁcation layer. The ﬁnal layer of a PL-CNN is a C
class SVM W svm, which speciﬁes one parameter W svm

y ∈ RD for each class y ∈ Y.

Prediction. Given an image x, a PL-CNN predicts its class using the following rule:

y∗ = argmax

W svm

y Φ(x; W).

y∈Y

(2)

In other words, the dot product of the D-dimensional representation of x with the SVM parameter for
a class y provides the score for the class. The desired prediction is obtained by maximizing the score
over all possible classes.

3

Learning Objective. Given a training data set D = {(xi, yi), i ∈ [N ]}, where xi is the input
image and yi is its ground-truth class, we wish to estimate the parameters W ∪ W svm of the PL-CNN.
To this end, we minimize a regularized upper bound on the empirical risk. The risk of a prediction
y∗
i given the ground-truth yi is measured with a user-speciﬁed loss function ∆(y∗
i , yi). For example,
the standard 0 − 1 loss has a value of 0 for a correct prediction and 1 for an incorrect prediction.
Formally, the parameters of a PL-CNN are estimated using the following learning objective:

min
W,W svm

λ
2

(cid:88)

(cid:107)W l(cid:107)2

F +

l∈[L]∪{svm}

1
N

N
(cid:88)

i=1

(cid:16)

max
¯yi∈Y

∆(¯yi, yi) + (cid:0)W svm
¯yi

− W svm

yi

(cid:1)T

Φ(xi; W)

.

(3)

(cid:17)

The hyperparameter λ denotes the relative weight of the regularization compared to the upper bound of
the empirical risk. Note that, due to the presence of piecewise linear non-linearities, the representation
Φ(·; W) (and hence, the above objective) is highly non-convex in the PL-CNN parameters.

4 PARAMETER ESTIMATION FOR PL-CNN

In order to enable layerwise optimization of PL-CNNs, we show that parameter estimation of a layer
can be formulated as a difference-of-convex (DC) program (subsection 4.1). This allows us to use the
concave-convex procedure, which solves a series of convex optimization problems (subsection 4.2).
We show that each convex problem closely resembles a structured SVM objective, which can be
addressed by the powerful block-coordinate Frank-Wolfe (BCFW) algorithm. We extend BCFW to
improve its initialization, time complexity and memory requirements, thereby enabling its use in
learning PL-CNNs (subsection 4.3). For the sake of clarity, we only provide sketches of the proofs
for those propositions that are necessary for understanding the paper. The detailed proofs of the
remaining propositions are provided in the Appendix.

4.1 LAYERWISE OPTIMIZATION AS A DC PROGRAM

Given the values of the parameters for the convolutional and the dense layers (that is, W), the learning
objective (3) is the standard SVM problem in parameters W svm. In other words, it is a convex
optimization problem with several efﬁcient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009;
Shalev-Shwartz et al., 2009), including the BCFW algorithm (Lacoste-Julien et al., 2013). Hence, the
optimization of the ﬁnal layer is a computationally easy problem. In contrast, the optimization of the
parameters of a convolutional or a dense layer l does not result in a convex program. In general, this
problem can be arbitrarily hard to solve. However, in the case of PL-CNN, we show that the problem
can be formulated as a speciﬁc type of DC program, which enables efﬁcient optimization via the
iterative use of BCFW. The key property that enables our approach is the following proposition that
shows that the composition of PL functions is also a PL function.
Proposition 1. Consider PL functions g : Rm → R and gi : Rn → R, for all i ∈ [m]. Deﬁne
a function f : Rn → R as f (u) = g([g1(u), g2(u), · · · , gm(u)](cid:62)). Then f is also a PL function
(proof in Appendix A).

Using the above proposition, we can reformulate the problem of optimizing the parameters of one
layer of the network as a DC program. Speciﬁcally, the following proposition shows that the problem
can be formulated as a latent structured SVM objective (Yu & Joachims, 2009).
Proposition 2. The learning objective of a PL-CNN with respect to the parameters of the l-th layer
can be speciﬁed as follows:

min
W l

λ
2

(cid:107)W l(cid:107)2

F +

1
N

N
(cid:88)

i=1

max
hi∈H
¯yi∈Y

(cid:0)∆(¯yi, yi) + (W l)(cid:62)Ψ(xi, ¯yi, hi)(cid:1) − max
hi∈H

(cid:0)(W l)(cid:62)Ψ(xi, yi, hi)(cid:1) ,

(4)

for an appropriate choice of the latent space H and joint feature vectors Ψ(x, y, h) of the input
x, the output y and the latent variables h. In other words, parameter estimation for the l-th layer
corresponds to minimizing the sum of its Frobenius norm plus a PL function for each training sample.

Sketch of the Proof. For a given image x with the ground-truth class y, consider the input to the layer
l, which we denote by zl−1. Since all the layers except the l-th one are ﬁxed, the input zl−1 is a

4

constant vector, which only depends on the image x (that is, its value does not depend on the variables
W l). In other words, we can write zl−1 = ϕ(x).
Given the input zl−1, all the elements of the output of the l-th layer, denoted by zl, are a PL function
of W l since the layer performs a linear transformation of zl−1 according to the parameters W l,
followed by an application of PL operations such as ReLU or max-pool. The vector zl is then fed
to the (l + 1)-th layer. The output zl+1 of the (l + 1)-th layer is a vector whose elements are PL
functions of zl. Therefore, by proposition (1), the elements of zl+1 are a PL function of W l. By
applying the same argument until we reach the layer L, we can conclude that the representation
Φ(x; W) is a PL function of W l.

Next, consider the upper bound of the empirical risk, which is speciﬁed as follows:

(cid:16)

max
¯y∈Y

∆(¯y, y) + (cid:0)W svm

¯y − W svm

y

(cid:1)T

(cid:17)

Φ(x; W)

.

(5)

Once again, since W svm is ﬁxed, the above upper bound can be interpreted as a PL function of
Φ(x; W), and thus, by proposition (1), the upper bound is a PL function of W l. It only remains to
observe that the learning objective (3) also contains the Frobenius norm of W l. Thus, it follows that
the estimation of the parameters of layer l can be reformulated as minimizing the sum of its Frobenius
norm and the PL upper bound of the empirical risk over all training samples, as shown in problem (4).
Note that we have ignored the constants corresponding to the Frobenius norm of the parameters of all
the ﬁxed layers. This constitutes an existential proof of Proposition 2. In the next paragraph, we give
an intuition about the feature vectors Ψ(xi, ¯yi, hi) and the latent space H.

Feature Vectors & Latent Space. The exact form of the joint feature vectors depends on the
explicit DC decomposition of the objective function. In Appendix B, we detail the practical computa-
tions and give an example: we construct two interleaved neural networks whose outputs deﬁne the
convex and concave parts of the DC objective function. Given the explicit DC objective function, the
feature vectors are given by a subgradient and can therefore be obtained by automatic differentiation.

We now give an intuition of what the latent space H represents. Consider an input image x and
a corresponding latent variable h ∈ H. The latent variable can be viewed as a set of variables
hk, k ∈ {l + 1, · · · , L}. In other words, each subset hk of the latent variable corresponds to one of
the layers of the network that follow the layer l. Intuitively, hk represents the choice of activation at
layer k when going through the PL activation: for each neuron j of layer k, hk
j takes value i if and
only if the i-th piece of the piecewise linear activation is selected. For instance, i is the index of the
selected input in the case of a max-pooling unit.

Note that the latent space only depends on the layers that follow the current layer being optimized.
This is due to the fact that the input zl−1 to the l-th layer is a constant vector that does not depend on
the value of W l. However, the activations of all subsequent layers following the l-th one depend on
the value of the parameters W l. As a consequence, the greater the number of following layers, the
greater the size of the latent space, and this growth happens to be exponential. However, as will be
seen shortly, it is still possible to efﬁciently optimize problem (4) for all the layers of the network
despite this exponential increase.

4.2 CONCAVE-CONVEX PROCEDURE

The optimization problem (4) is a DC program in the parameters W l. This follows from the fact
that the upper bound of the empirical risk is a PL function, and can therefore be expressed as the
difference of two convex PL functions (Melzer, 1986). Furthermore, the Frobenius norm of W l
is also a convex function of W l. This observation allows us to obtain an approximate solution of
problem (4) using the iterative concave-convex procedure (CCCP) (Yuille & Rangarajan, 2002).

Algorithm 1 describes the main steps of CCCP. In step 3, we impute the best value of the latent variable
corresponding to the ground-truth class yi for each training sample. This imputation corresponds to
the linearization step of the CCCP. The selected latent variable corresponds to a choice of activations
at each non-linear layer of the network, and therefore deﬁnes a path of activations to the ground truth.
Next, in step 4, we update the parameters by solving a convex optimization problem. This convex
problem amounts to ﬁnding the path of activations which minimizes the maximum margin violations
given the path to the ground truth deﬁned in step 3.

5

The CCCP algorithm has the desirable property of providing a monotonic decrease in the objective
function at each iteration. In other words, the objective function value of problem (4) at W l
t is
greater than or equal to its value at W l
t+1. Since layerwise optimization itself can be viewed as a
block-coordinate algorithm for minimizing the learning objective (3), our overall algorithm provides
guarantees of monotonic decrease until convergence. This is one of the main advantages of our
approach compared to backpropagation and its variants, which fail to provide similar guarantees on
the value of the objective function from one iteration to the next.

Algorithm 1 CCCP for parameter estimation of the l-th layer of the PL-CNN.
Require: Data set D = {(xi, yi), i ∈ [N ]}, ﬁxed parameters {W ∪ W svm}\W l, initial estimate W l
0.

1: t = 0
2: repeat
3:

For each sample (xi, yi), ﬁnd the best latent variable value by solving the following problem:

h∗

i = argmax

(W l

t )(cid:62)Ψ(xi, yi, h).

h∈H

(6)

4:

Update the parameters by solving the following convex optimization problem:

W l

t+1 = argmin

(cid:107)W l(cid:107)2

F +

λ
2

W l

1
N

N
(cid:88)

i=1

max
¯yi∈Y
hi∈H

(cid:0)∆(¯yi, yi) + (W l)(cid:62)Ψ(xi, ¯yi, hi)(cid:1) −

(cid:0)(W l)(cid:62)Ψ(xi, yi, h∗

i )(cid:1) .

(7)

t = t+1

5:
6: until Objective function of problem (4) cannot be improved beyond a speciﬁed tolerance.

In order to solve the convex program (7), which corresponds to a structured SVM problem, we make
use of the powerful BCFW algorithm (Lacoste-Julien et al., 2013) that solves its dual via conditional
gradients. This has two main advantages: (i) as the dual is a smooth quadratic program, each iteration
of BCFW provides a monotonic increase in its objective; and (ii) the optimal step-size at each iteration
can be computed analytically. This is once again in stark contrast to backpropagation, where the
estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma
& Ba, 2015). As shown by Lacoste-Julien et al. (2013), given the current estimate of the parameters
W l, the conditional gradient of the dual of program (7) with respect to a training sample (xi, yi) can
be obtained by solving the following problem:

(ˆyi, ˆhi) = argmax
¯y∈Y,h∈H

(W l)(cid:62)Ψ(xi, ¯y, h) + ∆(¯y, yi).

(8)

We refer the interested reader to (Lacoste-Julien et al., 2013) for further details.

The overall efﬁciency of the CCCP algorithm relies on our ability to solve problems (6) and (8). At
ﬁrst glance, these problems may appear to be computationally intractable as the latent space H can
be very large, especially for layers close to the input (of the order of millions of dimensions for a
typical network). However, the following proposition shows that both the problems can be solved
efﬁciently using the forward and backward passes that are employed in backpropagation.
Proposition 3. Given the current estimate W l of the parameters for the l-th layer, as well as the
parameter values of all the other ﬁxed layers, problems (6) and (8) can be solved using a forward
pass on the network. Furthermore, the joint feature vectors Ψ(xi, ˆyi, ˆhi) and Ψ(xi, yi, h∗
i ) can be
computed using a backward pass on the network.

Sketch of the Proof. Recall that the latent space consists of the putative activations for each PL
operation in the layers following the current one. Thus, intuitively, the maximization over the latent
variables corresponds to ﬁnding the exact activations of all such PL operations. In other words,
we need to identify the indices of the linear pieces that are used to compute the value of the PL

6

function in the current state of the network. For a ReLU operation, this corresponds to estimating
max{0, v}, where the input to the ReLU is a scalar v. Similarly, for a max-pool operation, this
corresponds to estimating maxi{e(cid:62)
i u}, where u is the input vector to the max-pool. This is precisely
the computation that the forward pass of backpropagation performs. Given the activations, the joint
feature vector is the subgradient of the sample with respect to the current layer. Once again, this is
precisely what is computed during the backward pass of the backpropagation algorithm.

An example is constructed in Appendix B to illustrate how to compute the feature vectors in practice.

4.3

IMPROVING THE BCFW ALGORITHM

As the BCFW algorithm was originally designed to solve a structured SVM problem, it requires
further extensions to be suitable for training a PL-CNN. In what follows, we present three such
extensions that improve the initialization, memory requirements and time complexity of the BCFW
algorithm respectively.

Trust-Region for Initialization. The original BCFW algorithm starts with an initial parameter
W l = 0 (that is, all the parameters are set to 0). The reason for this initialization is that it is possible
to compute the dual variables that correspond to the 0 primal variable. However, since our algorithm
visits each layer of the network several times, it would be desirable to initialize its parameters using
its current value W t
l . To this end, we introduce a trust-region in the constraints of problem (7), or
equivalently, an (cid:96)2 norm based proximal term in its objective function (Parikh & Boyd, 2014). The
following proposition shows that this has the desired effect of initializing the BCFW algorithm close
to the current parameter values.
Proposition 4. By adding a proximal term µ
F to the objective function in (7), we
can compute a feasible dual solution whose corresponding primal solution is equal to µ
λ+µ W l
t .
Furthermore, the addition of the proximal term still allows us to efﬁciently compute the conditional
gradient using a forward-backward pass (proof in Appendix D).

2 (cid:107)W l − W l

t (cid:107)2

In practice, we always choose a value of µ = 10λ: this yields an initialization of (cid:39) 0.9W l
does not signiﬁcantly change the value of the objective function.

t which

Efﬁcient Representation of Joint Feature Vectors. The BCFW algorithm requires us to store a
linear combination of the feature vectors for each mini-batch. While this requirement is not too
stringent for convolutional and multi-class SVM layers, where the dimensionality of the feature
vectors is small, it becomes prohibitively expensive for dense layers. The following proposition
prevents a blow-up in the memory requirements of BCFW.
Proposition 5. When optimizing dense layer l, if W l ∈ Rp×q, we can store a representation of the
joint feature vectors Ψ(x, y, h) with vectors of size p in problems (6) and (7). This is in contrast to
the na¨ıve approach that requires them to be of size p × q.

Sketch of the Proof. By Proposition (3), the feature vectors are subgradients of the hinge loss function,
∂zl ·(cid:0)zl−1(cid:1)T
which we loosely denote by η for this proof. Then by the chain rule: ∂η
.
Noting that zl−1 ∈ Rq is a forward pass up until layer l (independent of W l), we can store only
∂zl ∈ Rp and still reconstruct the full feature vector ∂η
∂η

∂W l by a forward pass and an outer product.

∂zl
∂W l = ∂η

∂W l = ∂η

∂zl

Reducing the Number of Constraints.
In order to reduce the amount of time required for the
BCFW algorithm to converge, we use the structure of H to simplify problem (7) to a much simpler
problem. Speciﬁcally, since H represents the activations of the network for a given sample, it has a
natural decomposition over the layers: H = H1 × ... × HL. We use this structure in the following
observation.

Observation 1. Problem (7) can be approximately solved by optimizing the dual problem on increas-
ingly large search spaces. In other words, we start with constraints of Y, followed by Y × HL, then
Y × HL × HL−1 and so on. The algorithm converges when the primal-dual gap is below tolerance.

7

The latent variables which are not optimized over are set to be the same as the ones selected for the
ground truth. Experimentally, we observe that for convolutional layers (architectures in section 5),
restricting the search space to Y yields a dual gap low enough to consider the problem has converged.
This means that in practice for these layers, problem (7) can be solved by searching directions
over the search space Y instead of the much larger Y × H. The intuition is that the norm of the
difference-of-convex decomposition grows with the number of activations selected differently in the
convex and concave parts (see Appendix A for the decomposition of piecewise linear functions).
This compels the path of activations to be the same in the convex and the concave part to avoid large
margin violations, especially for convolutional layers which are followed by numerous non-linearities
at the max-pooling layers.

5 EXPERIMENTS

Our experiments are designed to assess the ability of LW-SVM (Layer-Wise SVM, our method) and
the SGD baselines to optimize problem (3). To compare LW-SVM with the state-of-the-art variants
of backpropagation, we look at the training and testing accuracies as well as the training objective
value. Unlike dropout, which effectively learns an ensemble model, we learn a single model using
each baseline optimization algorithm. All experiments are conducted on a GPU (Nvidia Titan X)
and use Theano (Bergstra et al., 2010; Bastien et al., 2012). We compare LW-SVM with Adagrad,
Adadelta and Adam. For all data sets, we start at a good solution provided by these solvers and
ﬁne-tune it with LW-SVM. We then check whether a longer run of the SGD solver reaches the same
level of performance.

The practical use of the LW-SVM algorithm needs choices at the three following levels: how to
select the layer to optimize (i), when to stop the CCCP on each layer (ii) and when to stop the
convex optimization at each inner iteration of the CCCP (iii). These choices are detailed in the next
paragraph.

The layer-wise schedule of LW-SVM is as follows: as long as the validation accuracy increases, we
perform passes from the end of the network (SVM) to the ﬁrst layer (i). At each pass, each layer is
optimized with one outer iteration of the CCCP (ii). The inner iterations are stopped when the dual
objective function does not increase by more than 1% over an epoch (iii). We point out that the dual
objective function is cheap to compute since we are maintaining its value at all time. By contrast, to
compute the exact primal objective function requires a forward pass over the data set without any
update.

5.1 MNIST DATA SET

Data set & Architecture The training data set consists in 60,000 gray scale images of size 28 × 28
with 10 classes, which we split into 50,000 samples for training and 10,000 for validating. The
images are normalized, and we do not use any data augmentation. The architecture used for this
experiment is shown in Figure 1.

Figure 1: Network architecture for the MNIST data set.

Method The number of epochs is set to 200, 100 and 100 for Adagrad, Adadelta and Adam -
Adagrad is given more epochs as we observed it took a longer time to converge. We then use LW-
SVM and compare the results on training objective, training accuracy and testing accuracy. We also
let the solvers run to up to 500 epochs to verify that we have not stopped the optimization prematurely.
The regularization hyperparameter λ and the initial learning rate are chosen by cross-validation. λ is
set to 0.001 for all solvers, and the initial learning rates can be found in Appendix C. For LW-SVM,
λ is set to the same value as the baseline, and the proximal term µ to µ = 10λ = 0.01.

8

Table 1: Results on MNIST: we compare the performance of LW-SVM with SGD algorithms on three
metrics: training objective, training accuracy and testing accuracy. LW-SVM outperforms Adadelta
and Adam on all three metrics, with marginal improvements since those ﬁnd already very good
solutions.

Solver (epochs)

Adagrad (200)
Adagrad (500)
Adagrad (200) + LW-SVM
Adadelta (100)
Adadelta (500)
Adadelta (100) + LW-SVM
Adam (100)
Adam (500)
Adam (100) + LW-SVM

Training
Training
Objective Accuracy
99.94%
99.96%
99.94%
99.56%
99.48%
99.85%
99.76%
99.72%
99.89%

0.027
0.024
0.025
0.049
0.048
0.033
0.038
0.038
0.029

Time (s)

707
1759
707+366
124
619
124+183
333
1661
333+353

Testing
Accuracy
99.22%
99.20%
99.21%
98.96%
99.05%
99.24%
99.19%
99.23%
99.23%

Figure 2: Results on MNIST of Adagrad, Adadelta and Adam followed by LW-SVM. We verify that
switching to LW-SVM leads to better solutions than running SGD longer (shaded continued plots).

Results As Table 1 shows, LW-SVM systematically improves on all training objective, training
accuracy and testing accuracy. In particular, it obtains the best testing accuracy when combined
with Adadelta. Because each convex sub-problem is run up to sufﬁcient convergence, the objective
function of LW-SVM features of monotonic decrease at each iteration of the CCCP (blue curves in
ﬁrst row of Figure 2).

5.2 CIFAR DATA SETS

Data sets & Architectures The CIFAR-10/100 data sets are comprised of 60,000 RGB natural
images of size 32 × 32 with 10/100 classes (Krizhevsky, 2009)). We split the training set into 45,000
training samples and 5,000 validation samples in both cases. The images are centered and normalized,
and we do not use any data augmentation. To obtain a strong enough baseline, we employ (i) a
pre-training with a softmax and cross-entropy loss and (ii) Batch-Normalization (BN) layers before
each non-linearity.

We have experimentally found out that pre-training with a softmax layer followed by a cross-entropy
loss led to better behavior and results than using an SVM loss alone. The baselines are trained with
batch normalization. Once they have converged, the estimated mean and standard deviation are ﬁxed
like they would be at test time. Then batch normalization becomes a linear transformation, which
can be handled by the LW-SVM algorithm. This allows us to compare LW-SVM with a baseline
beneﬁting from batch normalization. Speciﬁcally, we use the architecture shown in Figure 3:

9

Figure 3: Network architecture for the CIFAR data sets.

Method Again, the initial learning rates and regularization weight λ are obtained by cross-
validation, and a value of 0.001 is obtained for λ for all solvers on both datasets. As before, µ
is set to 10λ. The initial learning rates are reported in Appendix C. The layer schedule and conver-
gence criteria are as described at the beginning of the section. For each SGD optimizer, we train the
network for 10 epochs with a cross-entropy loss (preceded by a softmax layer). Then it is trained with
an SVM loss (without softmax) for respectively 1000, 100 and 100 epochs for Adagrad, Adadelta and
Adam. This amount is doubled to verify that the baselines are not harmed by a premature stopping.
Results are presented in Tables 2 and 3.

Table 2: Results on CIFAR-10: LW-SVM outperforms Adam and Adadelta on all three metrics. It
improves on Adagrad, but does not outperform it - however Adagrad takes a long time to converge
and does not obtain the best generalization.

Solver (epochs)

Adagrad (1000)
Adagrad (2000)
Adagrad (1000) + LW-SVM
Adadelta (100)
Adadelta (200)
Adadelta (100) + LW-SVM
Adam (100)
Adam (200)
Adam (100) + LW-SVM

Time (h)

10.58
21.14

Training
Training
Objective Accuracy
98.42%
100.00%
100.00% 10.58+1.66
97.96%
99.83%
100.00%
98.27%
99.76%
100.00%

0.83
1.66
0.83+0.68
0.83
1.65
0.83+1.07

0.059
0.009
0.012
0.113
0.054
0.038
0.113
0.055
0.034

Testing
Accuracy
83.15%
83.84%
83.43%
84.42%
85.02%
86.62%
84.18%
82.55%
85.52%

Table 3: Results on CIFAR-100: LW-SVM improves on all other solvers and obtains the best testing
accuracy.

Solver (epochs)

Adagrad (1000)
Adagrad (2000)
Adagrad (1000) + LW-SVM
Adadelta (100)
Adadelta (200)
Adadelta (100) + LW-SVM
Adam (100)
Adam (200)
Adam (100) + LW-SVM

Training
Training
Objective Accuracy
95.36%
99.98%
99.98%
95.68%
99.90%
99.98%
95.79%
99.87%
99.98%

0.201
0.044
0.062
0.204
0.088
0.052
0.221
0.088
0.059

Time (h)

10.68
21.20
10.68+3.40
0.84
1.67
0.84+1.48
0.84
1.66
0.84+1.69

Testing
Accuracy
54.00%
54.55%
53.97%
58.71%
58.03%
61.20%
58.32%
57.81%
60.17%

It can be seen from this set of results that LW-SVM always improves over the solution of
Results
the SGD algorithm, for example on CIFAR-100, decreasing the objective value of Adam from 0.22 to
0.06, or improving the test accuracy of Adadelta from 84.4% to 86.6% on CIFAR-10. The automatic
step-size allows for a precise ﬁne-tuning to optimize the training objective, while the regularization
of the proximal term helps for better generalization.

10

Figure 4: Results on CIFAR-10 of Adagrad, Adadelta and Adam followed by LW-SVM. The successive
drops of the training objective function with LW-SVM correspond to the passes over the layers.

Figure 5: Results on CIFAR-100 of Adagrad, Adadelta and Adam followed by LW-SVM. Although
Adagrad keeps improving the training objective function, it takes much longer to converge and the
improvement on the training and testing accuracies rapidly become marginal.

5.3

IMAGENET DATA SET

We show results on the classiﬁcation task of the ImageNet data set (Russakovsky et al., 2015).
The ImageNet data set contains 1.2 million images for training and 50,000 images for validation,
each of them mapped to one of the 1,000 classes. For this experiment we use a VGG-16 network
(conﬁguration D in (Simonyan & Zisserman, 2015)). We start with a pre-trained model as publicly
available online, and we tune each of the dense layers as well as the ﬁnal SVM layer with the
LW-SVM algorithm. This experiment is designed to test the scalability of LW-SVM to large data
sets and large networks, rather than comparing with the optimization baselines as before - indeed for
any baseline, obtaining proper convergence as in previous experiments would take a very long time.
We set the hyperparameters λ to 0.001 and µ to 10λ as previously. We budget ﬁve epochs per layer,
which in total takes two days of training on a single GPU (Nvidia Titan X). At training time we used
centered crops of size 224 × 224. The evaluation method is the same as the single test scale method
described in (Simonyan & Zisserman, 2015). We report the results on the validation set in Table 4,
for the Pre-Trained model (PT) and the same model further optimized by LW-SVM (PT+LW-SVM):

11

Table 4: Results on the 1,000-way classiﬁcation challenge of ImageNet on the validation set, for the
Pre-Trained model (PT) and the same model further optimized by LW-SVM (PT+LW-SVM).

Network
VGG-16 (PT)
VGG-16 (PT + LW-SVM)

Top-1 Accuracy Top-5 Accuracy
91.33%
91.61%

73.30%
73.81%

Since the objective function penalizes the top-1 error, it is logical to observe that the improvement
is most important on the top-1 accuracy. Importantly, having an efﬁcient representation of feature
vectors proves to be essential for such large networks: for instance, in the optimization of the ﬁrst
fully connected layer with a batch-size of 100, the use of our representation lowers the memory
requirements of the BCFW algorithm from 7,600GB to 20GB, which can then ﬁt in the memory of a
powerful computer.

6 DISCUSSION

We presented a novel layerwise optimization algorithm for a large and useful class of convolutional
neural networks, which we term PL-CNNs. Our key observation is that the optimization of the
parameters of one layer of a PL-CNN is equivalent to solving a latent structured SVM problem. As
the problem is a DC program, it naturally lends itself to the iterative CCCP approach, which optimizes
a convex structured SVM objective at each iteration. This allows us to leverage the advancements
made in structured SVM optimization over the past decade to design a computationally feasible
approach for learning PL-CNNs. Speciﬁcally, we use the BCFW algorithm and extend it to improve
its initialization, memory requirements and time complexity. In particular, this allows our method
to not require the tuning of any learning rate. Using the publicly available MNIST, CIFAR-10 and
CIFAR-100 data sets, we show that our approach provides a boost for learning PL-CNNs over the
state of the art backpropagation algorithms. Furthermore, we demonstrate scalability of the method
with results on the ImageNet data set with a large network.

When the mean and standard deviation estimations of batch normalization are not ﬁxed (unlike
in our experiments with LW-SVM), batch normalization is not a piecewise linear transformation,
and therefore cannot be used in conjunction with the BCFW algorithm for SVMs. However, it is
difference-of-convex as it is a C2 function (Horst & Thoai, 1999). Incorporating a normalization
scheme into our framework will be the object of future work. With our current methodology, LW-SVM
algorithm can already be used on most standard architectures like VGG, Inception and ResNet-type
architectures.

It is worth noting that other approaches for solving structured SVM problems, such as cutting-
plane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient
descent (Shalev-Shwartz et al., 2009), also rely on the efﬁciency of estimating the conditional
gradient of the dual. Hence, all these methods are equally applicable to our setting. Indeed, the
main strength of our approach is the establishment of a hitherto unknown connection between CNNs
and latent structured SVMs. We believe that our observation will allow researchers to transfer the
substantial existing knowledge of DC programs in general, and latent SVMs speciﬁcally, to produce
the next generation of principled optimization algorithms for deep learning. In fact, there are already
several such improvements that can be readily applied in our setting, which were not explored only
due to a lack of time. This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al.,
2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al., 2016).

ACKNOWLEDGMENTS

This work was supported by the EPSRC AIMS CDT grant EP/L015987/1, the EPSRC Programme
Grant Seebibyte EP/M013774/1 and Yougov. Many thanks to A. Desmaison, R. Bunel and D.
Bouchacourt for the helpful discussions.

12

REFERENCES

arXiv:1609.07152, 2016.

Brandon Amos, Lei Xu, and J. Zico Kolter.

Input convex neural networks. arXiv preprint

Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements,
2012.

Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training of

deep networks. Conference on Neural Information Processing Systems, 2007.

James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU
math expression compiler. Python for Scientiﬁc Computing Conference (SciPy), 2010.

Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. Conference

on Neural Information Processing Systems, 2015.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 2011.

Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in

polynomial time. arXiv preprint arXiv:1611.10258, 2016.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Uri Heinemann, Roi Livni, Elad Eban, Gal Elidan, and Amir Globerson. Improper deep kernels.

International Conference on Artiﬁcial Intelligence and Statistics, 2016.

Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief

nets. Neural computation, 2006.

and Applications, 1999.

Reiner Horst and Nguyen V. Thoai. DC programming: overview. Journal of Optimization Theory

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. International Conference on Machine Learning, 2015.

Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training of structural

SVMs. Machine Learning, 2009.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

International

Conference on Learning Representations, 2015.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University

of Toronto, 2009.

Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, and Patrick Pletscher. Block-coordinate Frank-
Wolfe optimization for structural SVMs. International Conference on Machine Learning, 2013.

Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2015.

James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate

curvature. International Conference on Machine Learning, 2015.

James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free optimiza-

tion. Neural Networks: Tricks of the Trade, 2012.

D. Melzer. On the expressibility of piecewise-linear continuous functions as the difference of two

piecewise-linear convex functions. Springer Berlin Heidelberg, 1986.

Pritish Mohapatra, Puneet Dokania, CV Jawahar, and M Pawan Kumar. Partial linearization based

optimization for multi-class SVM. European Conference on Computer Vision, 2016.

13

Yann Ollivier. Riemannian metrics for neural networks. Information and Inference: a Journal of the

IMA, 2013.

Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet Dokania, and Simon Lacoste-
Julien. Minding the gaps for block Frank-Wolfe optimization of structured SVMs. Inernational
Conference on Machine Learning, 2016.

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 2014.

David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-

propagating errors. Nature, 1986.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.

Neel Shah, Vladimir Kolmogorov, and Christoph H. Lampert. A multi-plane block-coordinate
Frank-Wolfe algorithm for training structural SVMs with a costly max-oracle. Conference on
Computer Vision and Pattern Recognition, 2015.

Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient

solver for SVM. International Conference on Machine Learning, 2009.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. International Conference on Learning Representations, 2015.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 2014.

Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable ADMM approach. International Conference on
Machine Learning, 2016.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector
machine learning for interdependent and structured output spaces. International Conference on
Machine Learning, 2004.

Chun-Nam John Yu and Thorsten Joachims. Learning structural SVMs with latent variables. Interna-

tional Conference on Machine Learning, 2009.

Alan L. Yuille and Anand Rangarajan. The concave-convex procedure (CCCP). Conference on

Neural Information Processing Systems, 2002.

Matthew Zeiler. ADADELTA: an adaptive learning rate method. CoRR, 2012.

Yuchen Zhang, Percy Liang, and Martin J. Wainwright. Convexiﬁed convolutional neural networks.

arXiv preprint arXiv:1609.01000, 2016.

14

A PIECEWISE LINEAR FUNCTIONS

Proof of Proposition (1) By the deﬁnition from (Melzer, 1986), we can write each function as the
difference of two point-wise maxima of linear functions:

g(v) = max
j∈[m+]

{a(cid:62)

And ∀i ∈ [n], gi(u) = g+

{b(cid:62)

j v}

i v} − max
k∈[m−]
i (u) − g−

i (u)

Where all the g+

i , g−

i are linear point-wise maxima of linear functions. Then:

f (u) = g([g1(u), · · · , gn(u)](cid:62))
{a(cid:62)

= max
j∈[m+]

j [g1(u), · · · , gn(u)](cid:62)} − max
k∈[m−]

{b(cid:62)

k [g1(u), · · · , gn(u)](cid:62)}

= max
j∈[m+]

aj,igi(u)

− max
k∈[m−]

(cid:41)

n
(cid:88)

i=1

(cid:40) n
(cid:88)

(cid:41)

bk,igi(u)

i=1

(cid:41)

− max
k∈[m−]

(cid:40) n
(cid:88)

i=1

aj,ig+

i (u) −

aj,ig−

i (u)

bk,ig+

i (u) −

bk,ig−

i (u)

(cid:41)

n
(cid:88)

i=1

aj,ig+

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u)

(cid:88)

n
(cid:88)

aj,ig−

i (u)

j(cid:48)∈[m+]\{j}

i=1

j(cid:48)∈[m+]

i=1

= max
j∈[m+]

= max
j∈[m+]

− max
k∈[m−]

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

+

(cid:88)

n
(cid:88)

bk,ig−

i (u)

k(cid:48)∈[m−]\{k}

i=1

k(cid:48)∈[m−]

i=1

= max
j∈[m+]



−

 max
k∈[m−]

n
(cid:88)



i=1

aj,ig+

i (u) +

aj,ig−

i (u)

+

bk,ig−

i (u)

(cid:88)

n
(cid:88)

j(cid:48)∈[m+]\{j}

i=1

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

(cid:88)

n
(cid:88)



aj,ig−

i (u)



k(cid:48)∈[m−]\{k}

i=1

j(cid:48)∈[m+]

i=1

n
(cid:88)

i=1

(cid:88)

k(cid:48)∈[m−]



+



= max
j∈[m+]

aj,ig+

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u)

j(cid:48)∈[m+]\{j}

i=1

k(cid:48)∈[m−]

i=1

− max
k∈[m−]

bk,ig+

i (u) +

(cid:88)

n
(cid:88)

bk,ig−

i (u) +

(cid:88)

n
(cid:88)

aj,ig−

i (u)

k(cid:48)∈[m−]\{k}

i=1

j(cid:48)∈[m+]

i=1











(cid:40) n
(cid:88)

i=1
(cid:40) n
(cid:88)

i=1

n
(cid:88)






i=1



n
(cid:88)



i=1






n
(cid:88)

i=1



n
(cid:88)






i=1



n
(cid:88)



i=1

−
















In each line of the last equality, we recognize a pointwise maximum of a linear combination of
pointwise maxima of linear functions. This constitutes a pointwise maximum of linear functions.

This derivation also extends equation (10) to the multi-dimensional case by showing an explicit

DC decomposition of the output.

B COMPUTING THE FEATURE VECTORS

We describe here how to compute the feature vectors in practice. To this end, we show how to
construct two (intertwined) neural networks that decompose the objective function into a convex
and a concave part. We call these Difference of Convex (DC) networks. Once the DC networks are
deﬁned, a standard forward and backward pass in the two networks yields the feature vectors for the
convex and concave contribution to the objective function. First, we derive how to perform a DC
decomposition in linear and non-linear layers, and then we construct an example of DC networks.

15

2 (|W | + W ) and W − = 1

DC Decomposition in a Linear Layer Let W be the weights of a ﬁxed linear layer. We introduce
W + = 1
2 (|W | − W ). We can note that W + and W − have exclusively
non-negative weights, and that W = W + − W −. Say we have an input u with the DC decomposition
(ucvx, uccv), that is: u = ucvx − uccv, where both ucvx and uccv are convex. Then we can decompose
the output of the layer as:

W · u = (W + · ucvx + W − · uccv)
(cid:125)

(cid:124)

(cid:123)(cid:122)
convex

− (W − · ucvx + W + · uccv)
(cid:123)(cid:122)
(cid:125)
convex

(cid:124)

(9)

DC Decomposition in a Piecewise Linear Activation Layer For simplicity purposes, we consider
that the non-linear layer is a point-wise maximum across [K] scalar inputs, that is, for an input
(uk)k∈[K] ∈ RK, the output is maxk∈[K] uk (the general multi-dimensional case can be found in
Appendix A). We suppose that we have a DC decomposition (ucvx
k ) for each input k. Then we
can write the following decomposition for the output of the layer:

k , uccv

max
k∈[K]

uk = max
k∈[K]

(ucvx

k − uccv
k )


ucvx

k +

(cid:88)

uccv
i



−

i∈[K],i(cid:54)=k
(cid:123)(cid:122)
convex



(cid:125)

(cid:88)

uccv
k

k∈[K]
(cid:123)(cid:122)
(cid:124)
convex

(cid:125)

= max
k∈[K]

(cid:124)

In particular, for a ReLU, we can write:

max(ucvx − uccv, 0) = max(ucvx, uccv)
(cid:125)

(cid:124)

− uccv
(cid:124)(cid:123)(cid:122)(cid:125)
convex

(cid:123)(cid:122)
convex

And for a Max-Pooling layer, one can easily verify that equation (10) is equivalent to:

M axP ool(ucvx − uccv) = M axP ool(ucvx − uccv) + SumP ool(uccv)
(cid:125)

(cid:124)

(cid:123)(cid:122)
convex

− SumP ool(uccv)
(cid:123)(cid:122)
(cid:125)
convex

(cid:124)

(12)

An Example of DC Networks We use the previous observations to obtain a DC decomposition in
any layer. We now take the example of the neural network used for the experiments on the MNIST
data set, and we show how to construct the two neural networks when optimizing W 1, the weights of
the ﬁrst convolutional layer. First let us recall the architecture without decomposition:

(10)

(11)

Figure 6: Detailed network architecture for the MNIST data set.

We want to optimize the ﬁrst convolutional layer, therefore we ﬁx all other parameters. Then we apply
all operations as described in the previous paragraphs, which yields the DC networks in Figure 7.
The network graph in Figure 7 illustrates Proposition 3 for the optimization of W 1: suppose we are
interested in f cvx(x, W 1), the convex part of the objective function for a given sample x, and we
wish to obtain the feature vector needed to perform an update of BCFW. With a forward pass, the
oracle for the latent and label variables (ˆh, ˆy) is efﬁciently computed; and with a backward pass, we
obtain the corresponding feature vector Ψ(x, ˆy, ˆh). Indeed, we recall from problem (8) that (ˆh, ˆy)
are the latent and label variables maximizing f cvx(x, W 1). Then given x, the forward pass in the DC
networks sequentially solves the nested maximization: it maximizes the activation of the ReLU and
MaxPooling units at each layer, thereby selecting the best latent variable ˆh at each non-linear layer,
and maximizes the output of the SVM layer, thereby selecting the best label ˆy. At the end of the
forward pass, f cvx(x, W 1) is therefore available as the output of the convex network, and the feature
vector Ψ(x, ˆy, ˆh) can be computed as a subgradient of f cvx(x, W 1) with respect to W 1.

16

Figure 7: Difference of Convex Networks for the optimization of Conv1 in the MNIST architecture.
The two leftmost columns represent the DC networks. For each layer, the right column indicates the
non-decomposed corresponding operation. Note that we represent the DC decomposition of the SVM
layer as unique blocks to keep the graph simple. Given the decomposition method for linear and
non-linear layers, one can write down the explicit operations without special difﬁculty.

Linearizing the concave part is equivalent to ﬁxing the activations of the DC networks, which can be
done by using a ﬁxed copy of W 1 at the linearization point (all other weights being ﬁxed anyway).
Then one can re-use the above reasoning to obtain the feature vectors for the linearized concave part.
Altogether, this methodology allows our algorithm to be implemented in any standard deep learning
library (our implementation is available at http://github.com/oval-group/pl-cnn).

17

C EXPERIMENTAL DETAILS

Hyper-parameters The hyper-parameters are obtained by cross-validation with a search on powers
of 10. In this section, η will denote the initial learning rate. We denote the Softmax + Cross-Entropy
loss by SCE, while SVM stands for the usual Support Vector Machines loss.

Table 5: Hyper-parameters for the SGD solvers

MNIST

η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

CIFAR-10
(SCE)
η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

CIFAR-10
(SVM)
η = 0.001
λ = 0.001
η = 0.1
λ = 0.001
η = 0.0001
λ = 0.001

CIFAR-100 CIFAR-100

(SCE)
η = 0.01
λ = 0.001
η = 1
λ = 0.001
η = 0.001
λ = 0.001

(SVM)
η = 0.001
λ = 0.001
η = 0.1
λ = 0.001
η = 0.0001
λ = 0.001

Adagrad

Adadelta

Adam

One may note that the hyper-parameters are the same for both CIFAR-10 and CIFAR-100 for each
combination of solver and loss. This makes sense since the initial learning rate mainly depends on
the architecture of the network (and not so much on which particular images are fed to this network),
which is very similar for the experiments on the CIFAR-10 and CIFAR-100 data sets.

D SVM FORMULATION & DUAL DERIVATION

Multi-Class SVM Suppose we are given a data set of N samples, for which every sample i has
a feature vector φi ∈ Rd and a ground truth label yi ∈ Y. For every possible label ¯yi ∈ Y, we
introduce the augmented feature vector ψi( ¯yi) ∈ R|Y|×d containing φi at index ¯yi, −φi at index yi,
and zeros everywhere else (then ψi(yi) is just a vector of zeros). We also deﬁne ∆( ¯yi, yi) as the loss
by choosing the output ¯yi instead of the ground truth yi in our task. For classiﬁcation, this is the
zero-one loss for example.

The SVM optimization problem is formulated as:

min
w,ξi

λ
2

(cid:107)w(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

Where λ is the regularization hyperparameter. We now add a proximal term to a given starting point
w0:

min
w,ξi

λ
2

µ
2

(cid:107)w(cid:107)2 +

(cid:107)w − w0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

Factorizing the second-order polynomial in w, we obtain the equivalent problem (changed by a
constant):

min
w,ξi

λ + µ
2

(cid:107)w −

µ
λ + µ

w0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

For simplicity, we introduce the ratio ρ =

µ
λ + µ

.

18

Dual Objective function The primal problem is:

min
w,ξi

λ + µ
2

(cid:107)w − ρw0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi

subject to:

∀i ∈ [N ], ∀ ¯yi ∈ Y, ξi ≥ wT ψi( ¯yi) + ∆(yi, ¯yi)

αi( ¯yi) (cid:0)∆(yi, ¯yi) + wT ψi( ¯yi) − ξi

(cid:1)

The dual problem can be written as:

max
α≥0

min
w,ξi

λ + µ
2

(cid:107)w − ρw0(cid:107)2 +

1
N

N
(cid:88)

i=1

ξi +

1
N

N
(cid:88)

(cid:88)

i=1

¯yi∈Y

Then we obtain the following KKT conditions:

∀i ∈ [N ],

= 0 −→

αi( ¯yi) = 1

(cid:88)

¯yi∈Y

∂·
∂ξi

∂·
∂w

= 0 −→ w = ρw0 −

αi( ¯yi)ψi( ¯yi)

1
N

1
λ + µ

(cid:124)

N
(cid:88)

(cid:88)

i=1

¯yi∈Y
(cid:123)(cid:122)
Aα

(cid:125)

We also introduce b = 1

N (∆(yi, ¯yi))i, ¯yi. We deﬁne Pn(Y) as the sample-wise probability simplex:

We inject back and simplify to:

Finally:

Where:

u ∈ Pn(Y) if: ∀i ∈ [N ], ∀ ¯yi ∈ Y, ui( ¯yi) ≥ 0

∀i ∈ [N ],

ui( ¯yi) = 1

(cid:88)

¯yi∈Y

max
α∈Pn(Y)

−(λ + µ)
2

(cid:107)Aα(cid:107)2 + µwT

0 (Aα) + αT b

min
α∈Pn(Y)

f (α)

f (α) (cid:44) λ + µ

2

(cid:107)Aα(cid:107)2 − µwT

0 (Aα) − αT b

BCFW derivation We write ∇(i)f the gradient of f w.r.t. the block (i) of variables in α, padded
with zeros on blocks (j) for j (cid:54)= i. Similarly, A(i) and b(i) contain the rows of A and the elements of
b for the block of coordinates (i) and zeros elsewhere. We can write:

∇(i)f (α) = (λ + µ)AT

(i)Aα − µA(i)w0 − b(i)

Then the search corner for the block of coordinates (i) is given by:

si = argmin

(cid:0)< s(cid:48)

i, ∇(i)f (α) >(cid:1)

s(cid:48)
i

(cid:16)

= argmin
s(cid:48)
i

(λ + µ)αT AT A(i)s(cid:48)

i − µwT

0 A(i)s(cid:48)

i − bT

(i)s(cid:48)
i

(cid:17)

We replace:

Aα = ρw0 − w
1
N

1
λ + µ

i =

A(i)s(cid:48)

(cid:88)

¯yi∈Y

s(cid:48)
i( ¯yi)ψi( ¯yi)

(i)s(cid:48)
bT

i =

s(cid:48)
i( ¯yi)∆( ¯yi, yi)

1
N

(cid:88)

¯yi∈Y

19

We then obtain:

−(w − ρw0)T (cid:88)

si = argmin

s(cid:48)
i

¯yi∈Y

i( ¯yi)ψi( ¯yi) − wT
s(cid:48)
0 ρ

s(cid:48)
i( ¯yi)ψi( ¯yi) −

s(cid:48)
i( ¯yi)∆( ¯yi, yi)



(cid:88)

¯yi∈Y



(cid:88)

¯yi∈Y



= argmax
s(cid:48)
i


wT (cid:88)

¯yi∈Y

s(cid:48)
i( ¯yi)ψi( ¯yi) +

s(cid:48)
i( ¯yi)∆( ¯yi, yi)



(cid:88)

¯yi∈Y

As expected, this maximum is obtained by setting si to one at y∗

and zeros elsewhere. We introduce the notation:

i = argmax

¯yi∈Y

(cid:0)wT ψi( ¯yi) + ∆( ¯yi, yi)(cid:1)

wi = −A(i)α(i)
li = bT
(i)α(i)
ws = −A(i)si
ls = bT

(i)si

Then we have:

ws = −

1
N

1
λ + µ

ψ(y∗

i ) = −

1
N

1
λ + µ

∂Hi(y∗
i )
∂w

ls =

∆(yi, y∗
i )

1
N

The optimal step size in the direction of the block of coordinates (i) is given by :
γ∗ = argmin

f (α + γ(si − αi))

γ

The optimal step-size is given by:

γ∗ =

< ∇(i)f (α), si − αi >
(λ + µ)(cid:107)A(si − αi)(cid:107)2

We introduce wd = −Aα = w − ρw0. Then we obtain:
(wi − ws)T (w − ρw0) + ρwT

γ∗ =

(cid:107)wi − ws(cid:107)2

0 (wi − ws) − 1

λ+µ (li − ls)

=

(wi − ws)T w − 1
(cid:107)wi − ws(cid:107)2

λ+µ (li − ls)

And the updates are the same as in standard BCFW:

Algorithm 2 BCFW with warm start

1: Let w(0) = w0,
2: Let l(0) = 0,
3: for k=0...K do
4:

∀i ∈ [N ], w(0)
l(0)
i = 0

∀i ∈ [N ],

i = 0

5:

6:

7:

8:

9:

10:

Pick i randomly in {1, .., n}

Get y∗

ls = 1

γ =

i = argmax
¯yi∈Y
N ∆(y∗
i , yi)
(wi − ws)T w − 1
(cid:107)wi − ws(cid:107)2

i + γws

w(k+1)
= (1 − γ)w(k)
i
= (1 − γ)l(k)
l(k+1)
i
w(k+1) = w(k) + w(k+1)
i
l(k+1) = l(k) + l(k+1)

i + γls

− l(k)
i

i

11:
12: end for

Hi(¯yi, w(k)) and ws = −

1
N

1
λ + µ

∂Hi(y∗

i , w(k))

∂w(k)

λ+µ (li − ls)

clipped to [0, 1]

− w(k)

i = w(k) + γ(w(k)

s − w(k)

i

)

20

In particular, we have proved Proposition (4) in this section: w is initialized to ρw0 (KKT conditions),

and the direction of the conditional gradient, ws, is given by

, which is independent of w0.

∂Hi(y∗
i )
∂w

Note that the derivation of the Lagrangian dual has introduced a dual variable αi( ¯yi) for each linear
constraint of the SVM problem (this can be replaced by αi(hi, ( ¯yi)) if we consider latent variables).
These dual variables indicate the complementary slackness not only for the output class ¯yi, but also
for each of the activation which deﬁnes a piece of the piecewise linear hinge loss. Therefore a choice
of α deﬁnes a path of activations.

E SENSITIVITY OF SGD ALGORITHMS

Here we discuss some weaknesses of the SGD-based algorithms that we have encountered in practice
for our learning objective function. These behaviors have been observed in the case of PL-CNNs,
and generally may not appear in different architectures (in particular the failure to learn with high
regularization goes away with the use of batch normalization layers).

E.1

INITIAL LEARNING RATE

As mentioned in the experiments section, the choice of the initial learning rate is critical for good
performance of all Adagrad, Adadelta and Adam. When the learning rate is too high, the network
does not learn anything and the training and validating accuracies are stuck at random level. When it
is too low, the network may take a considerably greater number of epochs to converge.

E.2 FAILURES TO LEARN

Regularization When the regularization hyper-parameter λ is set to a value of 0.01 or higher on
CIFAR-10, SGD solvers get trapped in a local minimum and fail to learn. The SGD solvers indeed
fall in the local minimum of shutting down all activations on ReLUs, which provide zero-valued
feature vector to the SVM loss layer (and a hinge loss of one). As a consequence, no information can
be back-propagated. We plot this behavior below:

Figure 8: Behavior of different algorithms for λ = 0.01. The x-axis has been rescaled to compare
the evolution of all algorithms (real training times vary between half an hour to a few hours for the
different runs).

In this situation, the network is at a bad saddle point (note that the training and validation accuracies
are stuck at random levels). Our algorithm does not fall into such bad situations, however it is not
able to get out of it either: each layer is at a pathological critical point of its own objective function,
which makes our algorithm unable to escape from it.

21

With a lower initial learning rate, the evolution is slower, but eventually the solver goes back to the
bad situation presented above.

Biases The same failing behavior as above has been observed when not using the biases in the
network. Again our algorithm is robust to this change.

22

