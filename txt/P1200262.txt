8
1
0
2
 
r
a

M
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
0
3
4
0
.
3
0
8
1
:
v
i
X
r
a

Learning unknown ODE models with Gaussian processes

Markus Heinonen1,2,∗

Cagatay Yildiz1,∗

Henrik Mannerstr¨om1

Jukka Intosalmi1

Harri L¨ahdesm¨aki1

1 Department of Computer Science, Aalto university
2 Helsinki Institute for Information Technology HIIT

Abstract

In conventional ODE modelling coeﬃcients of an equa-
tion driving the system state forward in time are es-
timated. However, for many complex systems it is
practically impossible to determine the equations or
interactions governing the underlying dynamics. In
these settings, parametric ODE model cannot be for-
mulated. Here, we overcome this issue by introducing
a novel paradigm of nonparametric ODE modeling
that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We
propose to learn non-linear, unknown diﬀerential func-
tions from state observations using Gaussian process
vector ﬁelds within the exact ODE formalism. We
demonstrate the model’s capabilities to infer dynamics
from sparse data and to simulate the system forward
into future.

1

Introduction

Dynamical systems modeling is a cornerstone of ex-
perimental sciences. In biology, as well as in physics
and chemistry, modelers attempt to capture the dy-
namical behavior of a given system or a phenomenon
in order to improve its understanding and make pre-
dictions about its future state. Systems of coupled
ordinary diﬀerential equations (ODEs) are undoubt-
edly the most widely used models in science. Even
simple ODE functions can describe complex dynam-
ical behaviours (Hirsch et al., 2004). Typically, the
dynamics are ﬁrmly grounded in physics with only a
few parameters to be estimated from data. However,
equally ubiquitous are the cases where the governing
dynamics are partially or completely unknown.

We consider the dynamics of a system governed by

∗Equal contribution

multivariate ordinary diﬀerential functions:

˙x(t) =

= f (x(t))

(1)

dx(t)
dt

where x(t) ∈ X = RD is the state vector of a D-
dimensional dynamical system at time t, and the
˙x(t) ∈ ˙X = RD is the ﬁrst order time derivative of
x(t) that drives the state x(t) forward, and where
f : RD → RD is the vector-valued derivative function.
The ODE solution is determined by

x(t) = x0 +

f (x(τ ))dτ,

(2)

(cid:90) t

0

where we integrate the system state from an initial
state x(0) = x0 for time t forward. We assume that
f (·) is completely unknown and we only observe one or
several multivariate time series Y = (y1, . . . , yN )T ∈
RN ×D obtained from an additive noisy observation
model at observation time points T = (t1, . . . , tN ) ∈
RN ,

y(t) = x(t) + εt,

(3)

where εt ∼ N (0, Ω) follows a stationary zero-mean
multivariate Gaussian distribution with diagonal noise
variances Ω = diag(ω2
1, . . . , ω2
D). The observation
time points do not need to be equally spaced. Our
task is to learn the diﬀerential function f (·) given
observations Y , with no prior knowledge of the ODE
system.

There is a vast literature on conventional ODEs
(Butcher, 2016) where a parametric form for function
f (x; θ, t) is assumed to be known, and its parameters
θ are subsequently optimised with least squares or
Bayesian approach, where the expensive forward solu-
tion xθ(ti) = (cid:82) ti
0 f (x(τ ); θ, t)dτ is required to evaluate
the system responses xθ(ti) from parameters θ against
observations y(ti). To overcome the computationally
intensive forward solution, a family of methods de-
noted as gradient matching (Varah, 1982; Ellner et al.,

1

2002; Ramsay et al., 2007) have proposed to replace
the forward solution by matching f (yi) ≈ ˙yi to em-
pirical gradients ˙yi of the data instead, which do not
require the costly integration step. Recently several
authors have proposed embedding a parametric diﬀer-
ential function within a Bayesian or Gaussian process
(GP) framework (Graepel, 2003; Calderhead et al.,
2008; Dondelinger et al., 2013; Wang and Barber,
2014; Macdonald, 2017) (see Macdonald et al. (2015)
for a review). GPs have been successfully applied
to model linear diﬀerential equations as they are an-
alytically tractable (Gao et al., 2008; Raissi et al.,
2017).

However, conventional ODE modeling can only pro-
ceed if a parametric form of the driving function f (·)
is known. Recently, initial work to handle unknown
or non-parametric ODE models have been proposed,
although with various limiting approximations. Early
works include spline-based smoothing and additive
functions (cid:80)D
j fj(xj) to infer gene regulatory networks
(De Hoon et al., 2002; Henderson and Michailidis,
2014). ¨Aij¨o and L¨ahdesm¨aki (2009) proposed estimat-
ing the unknown nonlinear function with GPs using
either ﬁnite time diﬀerences, or analytically solving
the derivative function as a function of only time,
˙x(t) = f (t) ( ¨Aij¨o et al., 2013). In a seminal techni-
cal report of Heinonen and d’Alche Buc (2014) a full
vector-valued kernel model f (x) was proposed, how-
ever using a gradient matching approximation. To our
knowledge, there exists no model that can learn non-
linear ODE functions ˙x(t) = f (x(t)) over the state x
against the true forward solutions x(ti).

In this work we propose npODE: the ﬁrst ODE
model for learning arbitrary, and a priori completely
unknown non-parametric, non-linear diﬀerential func-
tions f : X → ˙X from data in a Bayesian way. We
do not use gradient matching or other approxima-
tive models, but instead propose to directly optimise
the exact ODE system with the fully forward simu-
lated responses against data. We parameterise our
model as an augmented Gaussian process vector ﬁeld
with inducing points, while we propose sensitivity
equations to eﬃciently compute the gradients of the
system. Our model can forecast continuous-time sys-
tems arbitrary amounts to future, and we demonstrate
the state-of-the-art performance in human motion
datasets. The MATLAB implementation is publicly
available at github.com/cagatayyildiz/npode.

2 Nonparametric ODE model

The diﬀerential function f (x) to be learned deﬁnes a
vector ﬁeld 1 f , that is, an assignment of a gradient
vector f (x) ∈ RD to every state x ∈ RD. We model
the vector ﬁeld as a vector-valued Gaussian process
(GP) (Rasmussen and Williams, 2006)

f (x) ∼ GP(0, K(x, x(cid:48))),

which deﬁnes a priori distribution over function values
f (x) whose mean and covariances are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)),

(4)

(5)

(6)

and where the kernel K(x, x(cid:48)) ∈ RD×D is matrix-
valued. A GP prior deﬁnes that for any collection
of states X = (x1, . . . , xN )T ∈ RN ×D, the function
values F = (f (x1), . . . , f (xN ))T ∈ RN ×D follow a
matrix-valued normal distribution,

p(F ) = N (vec(F )|0, K(X, X)),

(7)

i,j=1 ∈ RN D×N D is a
where K(X, X) = (K(xi, xj))N
block matrix of matrix-valued kernels K(xi, xj). The
key property of Gaussian processes is that they encode
functions where similar states x, x(cid:48) induce similar
diﬀerentials f (x), f (x(cid:48)), and where the state similarity
is deﬁned by the kernel K(x, x(cid:48)).

In standard GP regression we would obtain poste-
rior of the vector ﬁeld by conditioning the GP prior
with the data (Rasmussen and Williams, 2006). In
ODE models the conditional f (x)|Y of a vector ﬁeld
is intractable due to the integral mapping (2) between
observed states y(ti) and diﬀerentials f (x). Instead,
we resort to augmenting the Gaussian process with a
set of M inducing points z ∈ X and u ∈ ˙X , such that
f (z) = u (Qui˜nonero-Candela and Rasmussen, 2005).
We choose to interpolate the diﬀerential function be-
tween the inducing points as (See Figure 1)

f (x) (cid:44) Kθ(x, Z)Kθ(Z, Z)−1vec(U ),

(8)

which supports the function f (x) with inducing lo-
cations Z = (z1, . . . , zM ), inducing vectors U =
(u1, . . . , uM ), and θ are the kernel parameters. The
function above corresponds to a vector-valued kernel
function (Alvarez et al., 2012), or to a multi-task Gaus-
sian process conditional mean without the variance
term (Rasmussen and Williams, 2006). This deﬁnition

1We use vector ﬁeld and diﬀerential function interchange-

ably.

2

kernel

Kθ(z, z(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(zj − z(cid:48)
(cid:96)2
j



 (9)

with diﬀerential variance σ2
f and dimension-speciﬁc
lengthscales (cid:96) = ((cid:96)1, . . . , (cid:96)D) is expanded into a di-
agonal matrix of size D × D. We collect the kernel
parameters as θ = (σf , (cid:96)).

We note that more complex kernels can also be
considered given prior information of the underlying
system characteristics. The divergence-free matrix-
valued kernel induces vector ﬁelds that have zero di-
vergence (Wahlstr¨om et al., 2013; Solin et al., 2015).
Intuitively, these vector ﬁelds do not have sinks or
sources, and every state always ﬁnally returns to itself
after suﬃcient amount of time. Similarly, curl-free
kernels induce curl-free vector ﬁelds that can contain
sources or sinks, that is, trajectories can accelerate or
decelerate. For theoretical treatment of vector ﬁeld
kernels, see (Narcowich and Ward, 1994; Bhatia et al.,
2013; Fuselier and Wright, 2017). Non-stationary
vector ﬁelds can be modeled with input-dependent
lengthscales (Heinonen et al., 2016), while spectral ker-
nels can represent stationary (Wilson et al., 2013) or
non-stationary (Remes et al., 2017) recurring patterns
in the diﬀerential function.

2.2 Joint model

We assume a Gaussian likelihood over the observations
yi and the corresponding simulated responses x(ti) of
Equation (2),

p(Y |x0, U, Z, ω) =

N (yi|x(ti), Ω),

(10)

N
(cid:89)

i=1

where x(ti) are forward simulated responses using the
integral equation (2) and diﬀerential equation (8), and
Ω = diag(ω2
D) collects the dimension-speciﬁc
noise variances.

1 . . . , ω2

The inducing vectors have a Gaussian process prior

p(U |Z, θ) = N (vec(U )|0, Kθ(Z, Z)).

(11)

The model posterior is then

p(U, x0, θ, ω|Y ) ∝ p(Y |x0, U, ω)p(U |θ) = L,

(12)

where we have for brevity omitted the dependency on
the locations of the inducing points Z and also the
parameter hyperpriors p(θ) and p(ω) since we assume

Figure 1: (a) Illustration of an ODE system vector
ﬁeld induced by the Gaussian process. The vector ﬁeld
f (x) (gray arrows) at arbitrary states x is interpolated
from the inducing points u, z (black arrows), with the
trajectory x(t) (red points) following the diﬀerential
system f (x) exactly. (b) The trajectory x(t) plotted
over time t.

is then compatible with the deterministic nature of
the ODE formalism. Due to universality of several
kernels and kernel functions (Shawe-Taylor and Cris-
tianini, 2004), we can represent arbitrary vector ﬁelds
with appropriate inducing point and kernel choices.

2.1 Operator-valued kernels

The vector-valued kernel function (8) uses operator-
valued kernels, which result in matrix-valued ker-
nels Kθ(z, z(cid:48)) ∈ RD×D for real valued states x, z,
while the kernel matrix over data points becomes
i,j=1 ∈ RM D×M D (See Alvarez et al.
Kθ = (K(zi, zj))M
(2012) for a review). Most straightforward operator-
valued kernel is the identity decomposable kernel
Kdec(z, z(cid:48)) = k(z, z(cid:48)) · ID, where the scalar Gaussian

3

them to be uniform, unless there is speciﬁc domain
knowledge of the priors.

The model parameters are the initial state x0

2, the
inducing vectors U , the noise standard deviations
ω = (ω1, . . . , ωD), and the kernel hyperparameters
θ = (σf , (cid:96)1, . . . , (cid:96)D).

2.3 Noncentral parameterisation

We apply a latent parameterisation using Cholesky
decomposition LθLT
θ = Kθ(Z, Z), which maps the
inducing vectors to whitened domain (Kuss and Ras-
mussen, 2005)

U = Lθ (cid:101)U ,

(cid:101)U = L−1

θ U.

(13)

The latent variables (cid:101)U are projected on the kernel
manifold Lθ to obtain the inducing vectors U . This
non-centered parameterisation (NCP) transforms the
hierarchical posterior L of Equation (12) into a repa-
rameterised form

p(x0, (cid:101)U , θ, ω|Y ) ∝ p(Y |x0, (cid:101)U , ω, θ)p( (cid:101)U ),

(14)

where all variables to be optimised are decoupled, with
the latent inducing vectors having a standard normal
prior (cid:101)U ∼ N (0, I). Optimizing (cid:101)U and θ is now more
eﬃcient since they have independent contributions to
the vector ﬁeld via U = Lθ (cid:101)U . The gradients of the
whitened posterior can be retrieved analytically as
(Heinonen et al., 2016)

∇

(cid:101)U log L = LT

θ ∇U log L.

(15)

Finally, we ﬁnd a MAP estimate for the initial state
x0, latent vector ﬁeld (cid:101)U , kernel parameters θ and
noises ω by gradient ascent,

x0,map, (cid:101)Umap, θmap, ωmap = arg max
x0, (cid:101)U ,θ,ω

log L,

(16)

while keeping the inducing locations Z ﬁxed on a
suﬃciently dense grid (See Figure 1). The partial
derivatives of the posterior with respect to noise pa-
rameters ω can be found analytically, while the deriva-
tives with respect to σf are approximated with ﬁnite
diﬀerences. We select the optimal lengthscales (cid:96) by
cross-validation.

2In case of multiple time-series, we will use one initial state

for each time-series.

3 Sensitivity equations

The key term to carry out the MAP gradient ascent
optimization is the likelihood

log p(Y |x0, (cid:101)U , ω)

that requires forward integration and computing the
partial derivatives with respect to the whitened induc-
ing vectors (cid:101)U . Given Equation (15) we only need to
compute the gradients with respect to the inducing
vectors u = vec(U ) ∈ RM D,

d log p(Y |x0, u, ω)
du

=

N
(cid:88)

s=1

d log N (ys|x(ts, u), Ω)
dx

dx(ts, u)
du

.

(17)

This requires computing the derivatives of the simu-
lated system response x(t, u) against the vector ﬁeld
parameters u,

dx(t, u)
du

≡ S(t) ∈ RD×M D,

(18)

∂uj

which we denote by Sij(t) = ∂x(t,u)i
, and expand
the notation to make the dependency of x on u ex-
plicit. Approximating these with ﬁnite diﬀerences is
possible in principle, but is highly ineﬃcient and has
been reported to cause unstability (Raue et al., 2013).
We instead turn to sensitivity equations for u and
x0 that provide computationally eﬃcient, analytical
gradients S(t) (Kokotovic and Heller, 1967; Fr¨ohlich
et al., 2017).

The solution for dx(t,u)

can be derived by diﬀer-
entiating the full nonparametric ODE system with
respect to u by

du

d
du

dx(t, u)
dt

d
du

=

f (x(t, u)).

(19)

The sensitivity equation for the given system can be
obtained by changing the order of diﬀerentiation on
the left hand side and carrying out the diﬀerentia-
tion on the right hand side. The resulting sensitivity
equation can then be expressed in the form

˙S(t)
(cid:125)(cid:124)

(cid:123)
dx(t, u)
du

(cid:122)
d
dt

J(t)
(cid:122)
(cid:123)
(cid:125)(cid:124)
∂f (x(t, u))
∂x

S(t)
(cid:122) (cid:125)(cid:124) (cid:123)
dx(t, u)
du

R(t)
(cid:123)
(cid:122)
(cid:125)(cid:124)
∂f (x(t, u))
,
∂u

+

=

(20)

where J(t) ∈ RD×D, R(t), ˙S(t) ∈ RD×M D (See Ap-
pendix for detailed speciﬁcation). For our nonpara-
metric ODE system the sensitivity equation is fully

4

determined by

J(t) =

∂K(x, Z)
∂x
R(t) = K(x, Z)K(Z, Z)−1.

K(Z, Z)−1u

(21)

(22)

The sensitivity equation provides us with an addi-
tional ODE system which describes the time evolution
of the derivatives with respect to the inducing vectors
S(t). The sensitivities are coupled with the actual
ODE system and, thus, both systems x(t) and S(t)
are concatenated as the new augmented state that is
solved jointly by Equation (2) driven by the diﬀer-
entials ˙x(t) and ˙S(t) (Leis and Kramer, 1988). The
initial sensitivities are computed as S(0) = dx0
du . In our
implementation, we merge x0 with u for sensitivity
analysis to obtain the partial derivatives with respect
to the initial state which is estimated along with the
other parameters. We use the cvodes solver from the
Sundials package (Hindmarsh et al., 2005) to solve
the nonparametric ODE models and the correspond-
ing gradients numerically. The sensitivity equation
based approach is superior to the ﬁnite diﬀerences
approximation because we have exact formulation for
the gradients of state over inducing points, which can
be solved up to the numerical accuracy of the ODE
solver.

4 Simple simulated dynamics

As ﬁrst illustration of the proposed nonparametric
ODE method we consider three simulated diﬀerential
systems: the Van der Pol (VDP), FitzHugh-Nagumo
(FHN) and Lotka-Volterra (LV) oscillators of form

vdp :

˙x1 = x2

˙x2 = (1 − x2

1)x2 − x1

fhn :

lv :

˙x1 = 3(x1 −

+ x2)

˙x2 =

x3
1
3

0.2 − 3x1 − 0.2x2
3

˙x1 = 1.5x1 − x1x2

˙x2 = −3x2 + x1x2.

In the conventional ODE case the coeﬃcients of these
equations can be inferred using standard statistical
techniques if suﬃcient amount of time series data is
available (Girolami, 2008; Raue et al., 2013). Our
main goal is to infer unknown dynamics, that is, when
these equations are unavailable and we instead repre-
sent the dynamics with a nonparametric vector ﬁeld
of Equation (8). We use these simulated models to
only illustrate our model behavior against the true
dynamics.

We employ 25 data points from one cycle of noisy
observation data from VDP and FHN models, and 25

data points from 1.7 cycles from the LV model with
noise variance of σ2
n = 0.12. We learn the npODE
model with these training data using M = 52 induc-
ing locations on a ﬁxed grid, and forecast between
4 and 8 future cycles starting from true initial state
x0 at time 0. Figure 2 (bottom) shows the training
datasets (grey regions), initial states, true trajectories
(black lines) and the forecasted trajectory likelihoods
(colored regions). The model accurately learns the
dynamics from less than two cycles of data and can
reproduce them reliably into future.

Figure 2 (top) shows the corresponding true vector
ﬁeld (black arrows) and the estimated vector ﬁeld
(grey arrows). The vector ﬁeld is a continuous func-
tion, which is plotted on a 8x8 grid for visualisation.
In general the most diﬃcult part of the system is learn-
ing the middle of the loop (as seen in the FHN model),
and learning the most outermost regions (bottom left
in the LV model). The model learns the underlying
diﬀerential f (x) accurately close to observed points,
while making only few errors in the border regions
with no data.

5 Unknown system estimation

Next, we illustrate how the model estimates real-
istic, unknown dynamics from noisy observations
y(t1), . . . , y(tN ). As in Section 4, we make no as-
sumptions on the structure or form of the underlying
system, and capture the underlying dynamics with
the nonparameteric system alone. We employ no sub-
jective priors, and assume no inputs, controls or other
sources of information. The task is to infer the under-
lying dynamics f (x), and interpolate or extrapolate
the state trajectory outside the observed data.

We use a benchmark dataset of human motion cap-
ture data from the Carnegie Mellon University motion
capture (CMU mocap) database. Our dataset con-
tains 50-dimensional pose measurements y(ti) from
humans walking, where each pose dimension records
a measurement in diﬀerent parts of the body during

Table 1: Means and standard deviations of RMSEs of
43 datasets in forecasting and ﬁlling experiments.

Model
npODE
GPDM
VGPLVM

Forecasting
4.52 ± 2.31
4.94 ± 3.3
8.74 ± 3.43

Imputation
3.94 ± 3.50
5.31 ± 3.39
3.91 ± 1.80

5

Figure 2: Estimated dynamics from Van der Pol, FitzHugh-Nagumo and Lotka-Volterra systems. The top
part (a-c) shows the learned vector ﬁeld (grey arrows) against the true vector ﬁeld (black arrows). The
bottom part (d-f ) shows the training data (grey region points) and forecasted future cycle likelihoods with
the learned model (shaded region) against the true trajectory (black line).

movement (Wang et al., 2008). We apply the prepro-
cessing of Wang et al. (2008) by downsampling the
datasets by a factor of four and centering the data.
This resulted in a total of 4303 datapoints spread
across 43 trajectories with on average 100 frames per
trajectory. In order to tackle the problem of dimen-
sionality, we project the original dataset with PCA to
a three dimensional latent space where the system is
speciﬁed, following Damianou et al. (2011) and Wang
et al. (2006). We place M = 53 inducing vectors on
a ﬁxed grid, and optimize our model from 100 initial
values, which we select by projecting empirical diﬀer-
ences y(ti) − y(ti−1) to the inducing vectors. We use
an LBFGS optimizer in Matlab. The whole inference
takes approximately few minutes per trajectory.

We evaluate the method with two types of experi-
ments: imputing missing values and forecasting future
cycles. For the forecasting the ﬁrst half of the trajec-
tory is for model training, and the second half is to be
forecasted. For imputation we remove roughly 20% of
the frames from the middle of the trajectory, which
are to be ﬁlled by the models. We perform model se-

lection for lengthscales (cid:96) with cross-validation split of
80/20. We record the root mean square error (RMSE)
over test points in the original feature space in both
cases, where we reconstruct the original dimensions
from the latent space trajectories.

Due to the current lack of ODE methods suitable for
this nonparametric inference task, we instead compare
our method to the state-of-the-art state-space models
where such problems have been previously considered
(Wang et al., 2008). In a state-space or dynamical
model a transition function x(tk+1) = g(x(tk)) moves
the system forward in discrete steps. With suﬃciently
high sampling rate, such models can estimate and fore-
cast ﬁnite approximations of smooth dynamics. In
Gaussian process dynamical model (Wang et al., 2006;
Frigola et al., 2014; Svensson et al., 2016) a GP transi-
tion function is inferred in a latent space, which can be
inferred with a standard GPLVM (Lawrence, 2004) or
with a dependent GPLVM (Zhao and Sun, 2016). In
dynamical systems the transition function is replaced
by a GP interpolation (Damianou et al., 2011). The
discrete time state-space models emphasize inference

6

Figure 3: Forecasting 50 future frames after 49 frames of training data of human motion dataset 35 12.amc.
(a) The estimated locations of the trajectory in a latent space (black points) and future forecast (colored
lines). (b) The original features reconstructed from the latent predictions with grey region showing the
training data.

of a low-dimensional manifold as an explanation of
the high-dimensional measurement trajectories.

We compare our method to the dynamical model
GPDM of Wang et al. (2006) and to the dynamical
system VGPLVM of Damianou et al. (2011), where we
directly apply the implementations provided by the
authors at inverseprobability.com/vargplvm and
dgp.toronto.edu/~jmwang/gpdm. Both methods op-
timize their latent spaces separately, and they are
thus not directly comparable.

5.1 Forecasting

In the forecasting task we train all models with the
ﬁrst half of the trajectory, while forecasting the sec-
ond half starting from the ﬁrst frame. The models
are trained and forecasted within a low-dimensional

space, and subsequently projected back into the orig-
inal space via inverting the PCA or with GPLVM
mean predictions. As all methods optimize their la-
tent spaces separately, they are not directly compa-
rable. Thus, the mean errors are computed in the
original high-dimensional space. Note that the low-
dimensional representation necessarily causes some
reconstruction errors.

Figure 3 illustrates the models on one of the trajec-
tories 35 12.amc. The top part (a) shows the training
data in the PCA space for npODE, and optimized
training data representation for GPDM and VGPLVM
(black points). The colored lines (npODE) and points
(GPDM, VGPLVM) indicate the future forecast. The
bottom part (b) shows the ﬁrst 9 reconstructed orig-
inal pose dimensions reconstructed from the latent
forecasted trajectories. The training data is shown

7

Figure 4: Imputation of 17 missing frames in the middle of a 94-length trajectory of human motion dataset
07 07.amc (subsampled every fourth frame). (a) The estimated locations of the missing points in the latent
space are colored. (b) The original features reconstructed from the latent trajectory.

in gray background, while test data is shown with
circles.

The VGPLVM has most trouble forecasting future
points, and reverts quickly after training data to a
value close to zero, failing to predict future points.
The GPDM model produces more realistic trajectories,
but fails to predict any of the poses accurately. Finally,
npODE can accurately predict ﬁve poses, and still
retains adequate performance on remaining poses,
except for pose 2.

Furthermore, Table 1 indicates that npODE is also
best performing method on average over the whole
dataset in the forecasting.

5.2 Imputation

In the imputation task we remove approximately 20%
of the training data from the middle of the trajectory.

The goal is to learn a model with the remaining data
and forecast the missing values. Figure 4 highlights
the performance of the three models on the trajectory
07 07.amc. The top part (a) shows the training data
(black points) in the PCA space (npODE) or optimized
training locations in the latent space (GPDM, VG-
PLVM). The middle part imputation is shown with
colored points or lines. Interestingly both npODE
and GPDM operate on cyclic representations, while
VGPLVM is not cyclic.

The bottom panel (b) shows the ﬁrst 9 recon-
structed pose dimensions from the three models. The
missing values are shown in circles, while training
points are shown with black dots. All models can
accurately reproduce the overall trends, while npODE
seems to ﬁt slightly worse than the other methods.
The PCA projection causes the seemingly perfect ﬁt of
the npODE prediction (at the top) to lead to slightly

8

warped reconstructions (at the bottom). All methods
mostly ﬁt the missing parts as well. Table 1 shows
that on average the npODE and VGPLVM have ap-
proximately equal top performance on the imputing
missing values task.

6 Discussion

We proposed the framework of nonparametric ODE
model that can accurately learn arbitrary, nonlinear
continuos-time dynamics from purely observational
data without making assumptions of the underlying
system dynamics. We demonstrated that the model
excels at learning dynamics that can be forecasted
into the future. We consider this work as the ﬁrst
in a line of studies of nonparametric ODE systems,
and foresee several aspects as future work. Currently
we do not handle non-stationary vector ﬁelds, that is
time-dependent diﬀerentials ft(x). Furthermore, an
interesting future avenue is the study of various vector
ﬁeld kernels, such as divergence-free, curl-free or spec-
tral kernels (Remes et al., 2017). Finally, including
inputs or controls to the system would allow precise
modelling in interactive settings, such as robotics.

The proposed nonparametric ODE model operates
along a continuous-time trajectory, while dynamic
models such as hidden Markov models or state-space
models are restricted to discrete time steps. These
models are unable to consider system state at arbitrary
times, for instance, between two successive timepoints.

Conventional ODE models have also been consid-
ered from the stochastic perspective with stochastic
diﬀerential equation (SDE) models that commonly
model the system drift and diﬀusion processes sepa-
rately leading to a distribution of trajectories p(x(t)).
As future work we will consider stochastic extensions
of our nonparametric ODE model, as well as MCMC
sampling of the inducing point posterior p(U |Y ), lead-
ing to trajectory distribution as well.

Acknowledgements. The data used in this project
was obtained from mocap.cs.cmu.edu. The database
was created with funding from NSF EIA-0196217.
This work has been supported by the Academy of
Finland Center of Excellence in Systems Immunol-
ogy and Physiology, the Academy of Finland grants
no. 260403, 299915, 275537, 311584.

9

References

Tarmo ¨Aij¨o, Kirsi Granberg, and Harri L¨ahdesm¨aki.
Sorad: a systems biology approach to predict
and modulate dynamic signaling pathway response
from phosphoproteome time-course measurements.
Bioinformatics, 29(10):1283–1291, 2013.

M. Alvarez, L. Rosasco, and N. Lawrence. Kernels
for vector-valued functions: A review. Foundations
and Trends in Machine Learning, 2012.

H. Bhatia, G. Norgard, V. Pascucci, and P. Bremer.
The helmholtz-hodge decomposition -— a survey.
IEEE Transactions on visualization and computer
graphics, 2013.

J. Butcher. Numerical methods for ordinary diﬀeren-

tial equations. John Wiley & Sons, 2016.

B. Calderhead, M. Girolami, and N. Lawrence. Accel-
erating bayesian inference over nonlinear diﬀerential
equations with gaussian processes. NIPS, 2008.

Andreas Damianou, Michalis K Titsias, and Neil D
Lawrence. Variational gaussian process dynami-
cal systems. In Advances in Neural Information
Processing Systems, pages 2510–2518, 2011.

Michiel JL De Hoon, Seiya Imoto, Kazuo Kobayashi,
Naotake Ogasawara, and Satoru Miyano. Inferring
gene regulatory networks from time-ordered gene
expression data of bacillus subtilis using diﬀeren-
tial equations. In Biocomputing 2003, pages 17–28.
World Scientiﬁc, 2002.

F. Dondelinger, M. Filippone, S Rogers, and D. Hus-
meier. Ode parameter inference using adaptive
gradient matching with gaussian processes. JMLR,
2013.

S. P. Ellner, Y. Seifu, and R. H. Smith. Fitting
population dynamic models to time-series data by
gradient matching. Ecology, 83(8):2256–2270, 2002.

Roger Frigola, Yutian Chen, and Carl Edward Ras-
mussen. Variational gaussian process state-space
models. In Advances in Neural Information Pro-
cessing Systems, pages 3680–3688, 2014.

Fabian Fr¨ohlich, Barbara Kaltenbacher, Fabian J.
Theis, and Jan Hasenauer. Scalable parameter esti-
mation for genome-scale biochemical reaction net-
works. PLOS Computational Biology, 13(1):1–18,
01 2017. doi: 10.1371/journal.pcbi.1005331.

E. Fuselier and G. Wright. A radial basis function
method for computing helmholtz–hodge decompo-
sitions. IMA Journal of Numerical Analysis, 2017.

Pei Gao, Antti Honkela, Magnus Rattray, and Neil D
Lawrence. Gaussian process modelling of latent
chemical species: applications to inferring tran-
scription factor activities. Bioinformatics, 24(16):
i70–i75, 2008.

Mark Girolami. Bayesian inference for diﬀerential
equations. Theor. Comput. Sci., 408(1):4–16, 2008.

T. Graepel. Solving noisy linear operator equations
by gaussian processes: Application to ordinary and
partial diﬀerential equations. ICML, 2003.

M. Heinonen and F. d’Alche Buc. Learning nonpara-
metric diﬀerential equations with operator-valued
kernels and gradient matching. arxiv, Telecom Paris-
Tech, 2014.

M. Heinonen, H. Mannerstr¨om, J. Rousu, S. Kaski,
and H. L¨ahdesm¨aki. Non-stationary Gaussian pro-
cess regression with Hamiltonian Monte Carlo. In
AISTATS, volume 51, pages 732–740, 2016.

J. Henderson and G. Michailidis. Network recon-
struction using nonparametric additive ode models.
PLOS ONE, 2014.

Alan C Hindmarsh, Peter N Brown, Keith E Grant,
Steven L Lee, Radu Serban, Dan E Shumaker, and
Carol S Woodward. SUNDIALS: Suite of nonlinear
and diﬀerential/algebraic equation solvers. ACM
Trans. Math. Softw., 31(3):363–396, 2005.

M. Hirsch, S. Smale, and Devaney. Diﬀerential Equa-
tions, Dynamical Systems, and an Introduction to
Chaos (Edition: 2). Elsevier Science & Technology
Books, 2004.

P Kokotovic and J Heller. Direct and adjoint sensi-
tivity equations for parameter optimization. IEEE
Transactions on Automatic Control, 12(5):609–610,
1967.

Malte Kuss and Carl Edward Rasmussen. Assessing
approximate inference for binary gaussian process
classiﬁcation. Journal of machine learning research,
6(Oct):1679–1704, 2005.

Jorge R. Leis and Mark A. Kramer. The simulta-
neous solution and sensitivity analysis of systems
described by ordinary diﬀerential equations. ACM
Trans. Math. Softw., 14(1), 1988.

Benn Macdonald. Statistical inference for ordinary
diﬀerential equations using gradient matching. PhD
thesis, University of Glasgow, 2017.

Benn Macdonald, Catherine Higham, and Dirk Hus-
meier. Controversy in mechanistic modelling with
gaussian processes. In International Conference on
Machine Learning, pages 1539–1547, 2015.

Francis J Narcowich and Joseph D Ward. Generalized
hermite interpolation via matrix-valued condition-
ally positive deﬁnite functions. Mathematics of
Computation, 63(208):661–687, 1994.

Joaquin Qui˜nonero-Candela and Carl Edward Ras-
mussen. A unifying view of sparse approximate
gaussian process regression. Journal of Machine
Learning Research, 6(Dec):1939–1959, 2005.

Maziar Raissi, Paris Perdikaris, and George Em Karni-
adakis. Inferring solutions of diﬀerential equations
using noisy multi-ﬁdelity data. Journal of Compu-
tational Physics, 335:736–746, 2017.

J. Ramsay, G. Hooker, D. Campbell, and J. Cao. Pa-
rameter estimation for diﬀerential equations: a gen-
eralized smoothing approach. Journal of the Royal
Statistical Society: Series B, 69:741–796, 2007.

C.E. Rasmussen and K.I. Williams. Gaussian pro-

cesses for machine learning. MIT Press, 2006.

Andreas Raue, Marcel Schilling, Julie Bachmann, An-
drew Matteson, Max Schelker, Daniel Kaschek,
Sabine Hug, Clemens Kreutz, Brian D. Harms,
Fabian J. Theis, Ursula Klingm¨uller, and Jens Tim-
mer. Lessons learned from quantitative dynamical
modeling in systems biology. PLOS ONE, 8(9):
1–17, 2013.

S. Remes, M. Heinonen, and S. Kaski. Non-stationary

spectral kernels. NIPS, 2017.

John Shawe-Taylor and Nello Cristianini. Kernel
methods for pattern analysis. Cambridge university
press, 2004.

Neil D Lawrence. Gaussian process latent variable
models for visualisation of high dimensional data. In
Advances in neural information processing systems,
pages 329–336, 2004.

A. Solin, M. Kok, N. Wahlstrom, T. Schon, and
S. S¨arkk¨a. Modeling and interpolation of the am-
bient magnetic ﬁeld by gaussian processes. arXiv,
2015. arXiv:1509.04634.

10

Andreas Svensson, Arno Solin, Simo S¨arkk¨a, and
Thomas Sch¨on. Computationally eﬃcient bayesian
learning of gaussian process state space models. In
Artiﬁcial Intelligence and Statistics, pages 213–221,
2016.

J. M. Varah. A spline least squares method for numer-
ical parameter estimation in diﬀerential equations.
SIAM J.sci. Stat. Comput., 3(1):28–46, 1982.

N. Wahlstr¨om, M. Kok, and T. Sch¨on. Modeling
magnetic ﬁelds using gaussian processes.
IEEE
conf on Acoustics, Speech and Signal Processing
(ICASSP), 2013.

Jack Wang, Aaron Hertzmann, and David M Blei.
Gaussian process dynamical models. In Advances in
neural information processing systems, pages 1441–
1448, 2006.

Jack M Wang, David J Fleet, and Aaron Hertzmann.
Gaussian process dynamical models for human mo-
tion. IEEE transactions on pattern analysis and
machine intelligence, 30(2):283–298, 2008.

Y. Wang and D. Barber. Gaussian processes for
bayesian estimation in ordinary diﬀerential equa-
tions. ICML, 2014.

A. Wilson, E. Gilboa, A. Nehorai, and J. Cunningham.
Fast multidimensional pattern extrapolation with
gaussian processes. AISTATS, 2013.

Jing Zhao and Shiliang Sun. Variational dependent
multi-output gaussian process dynamical systems.
The Journal of Machine Learning Research, 17(1):
4134–4169, 2016.

T ¨Aij¨o and H. L¨ahdesm¨aki. Learning gene regulatory
networks from gene expression measurements using
non-parametric molecular kinetics. Bioinformatics,
25:2937––2944, 2009.

11

Appendix of ‘Learning unknown ODE models with Gaussian pro-
cesses’

Sensitivity Equations

In the main text, the sensitivity equation is formulated using matrix notation

˙S(t) = J(t)S(t) + R(t).

Here, the time-dependent matrices are obtained by diﬀerentiating the vector valued functions with respect to
vectors i.e.



































S(t) =

J(t) =

R(t) =

dx1(t,U )
du1

dx2(t,U )
du1

· · ·
· · ·
dxD(t,U )
du1

dx1(t,U )
du2

dx2(t,U )
du2

· · ·
· · ·
dxD(t,U )
du2

· · ·

· · ·

· · ·
· · ·
· · ·

dx1(t,U )
duMD

dx2(t,U )
duMD

· · ·
· · ·
dxD(t,U )
duMD













∂f (x(t),U )1
∂x1

∂f (x(t),U )2
∂x1

· · ·
· · ·
∂f (x(t),U )D
∂x1

∂f (x(t),U )1
∂u1

∂f (x(t),U )2
∂u1

· · ·
· · ·
∂f (x(t),U )D
∂u1

∂f (x(t),U )1
∂x2

∂f (x(t),U )2
∂x2

· · ·
· · ·
∂f (x(t),U )D
∂x2

∂f (x(t),U )1
∂u2

∂f (x(t),U )2
∂u2

· · ·
· · ·
∂f (x(t),U )D
∂u2

· · ·

· · ·

· · ·
· · ·
· · ·

· · ·

· · ·

· · ·
· · ·
· · ·

D×M D
∂f (x(t),U )1
∂xD

∂f (x(t),U )2
∂xD

· · ·
· · ·
∂f (x(t),U )D
∂xD

∂f (x(t),U )1
∂uMD

∂f (x(t),U )2
∂uMD

· · ·
· · ·
∂f (x(t),U )D
∂uMD























D×D

D×M D

Optimization

Below is the explicit form of the log posterior. Note that we introduce u = vec(U ) and Ω = diag(ω2
for notational simplicity.

1, . . . , ω2

D)

log L = log p(U |θ) + log p(Y |x0, U, ω)

= log N (u|0, Kθ(Z, Z)) +

log N (yi|x(ti, U ), Ω)

N
(cid:88)

i=1

1
2

1
2

1
2

1
2

1
2

1
2

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

(yij − xj(ti, U, x0))2
ω2
j

−

N
(cid:88)

i=1

1
2

(yi,j − xj(ti, U, x0))2
ω2
j

D
(cid:88)

j=1

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

− N

log ωj

(30)

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

log |Ω|

(29)

Our goal is to compute the gradients with respect to the initial state x0, latent vector ﬁeld (cid:101)U , kernel
parameters θ and noise variables ω. As explained in the paper, we compute the gradient of the posterior with

12

(23)

(24)

(25)

(26)

(27)

(28)

respect to inducing vectors U and project them to the white domain thanks to noncentral parameterisation.
The analytical forms of the partial derivatives are as follows:

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂uk

− Kθ(Z, Z)−1u

∂ log L
∂uk

∂ log L
∂(x0)d

∂ log L
∂ωj

=

=

=

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

1
ω3
j

N
(cid:88)

i=1

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂(x0)d

(yi,j − xj(ti, U, x0))2 −

N
ωj

Seemingly hard to compute terms, ∂xj (ti,U,x0)

, are computed using sensitivities. The
lengthscale parameter (cid:96) is considered as a model complexity parameter and is chosen from a grid using
cross-validation. We furthermore need the gradient with respect to the other kernel variable, i.e., the signal
variance σ2
f . Because Kθ(Z, Z) and x(ti, U ) are the functions of kernel, computing the gradients with respect
to σ2

f is not trivial and we make use of ﬁnite diﬀerences:

and ∂xj (ti,U,x0)

∂(x0)d

∂uk

We use δ = 10−4 to compute the ﬁnite diﬀerences.

One problem of using gradient-based optimization techniques is that they do not ensure the positivity of
the parameters being optimized. Therefore, we perform the optimization of the noise standard deviations
ω = (ω1, . . . , ωD) and signal variance σf with respect to their logarithms:

∂ log L
∂σf

=

log L(σf + δ) − log L(σf )
δ

∂ log L
∂ log c

=

∂ log L
∂c

∂c
∂ log c

=

∂ log L
∂c

c

where c ∈ (σf , ω1, . . . , ωD).

Implementation details

We initialise the inducing vectors U = (u1, . . . uM ) by computing the empirical gradients ˙yi = yi − yi−1, and
conditioning as

U0 = K(Z, Y )K(Y, Y )−1c ˙y,

where we optimize the scale c against the posterior. The whitened inducing vector is obtained as (cid:101)U0 = L−1
θ U0.
This procedure produces initial vector ﬁelds that partially match the trajectory already. We then do 100
restarts of the optimization from random perturbations (cid:101)U = (cid:101)U0 + ε.

We use LBFGS gradient optimization routine in Matlab. We initialise the inducing vector locations Z on a
equidistant ﬁxed grid on a box containing the observed points. We select the lengthscales (cid:96)1, . . . , (cid:96)D using
cross-validation from values {0.5, 0.75, 1, 1.25, 1.5}. In general large lengthscales induce smoother models,
while lower lengthscales cause overﬁtting.

(31)

(32)

(33)

(34)

(35)

(36)

13

8
1
0
2
 
r
a

M
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
0
3
4
0
.
3
0
8
1
:
v
i
X
r
a

Learning unknown ODE models with Gaussian processes

Markus Heinonen1,2,∗

Cagatay Yildiz1,∗

Henrik Mannerstr¨om1

Jukka Intosalmi1

Harri L¨ahdesm¨aki1

1 Department of Computer Science, Aalto university
2 Helsinki Institute for Information Technology HIIT

Abstract

In conventional ODE modelling coeﬃcients of an equa-
tion driving the system state forward in time are es-
timated. However, for many complex systems it is
practically impossible to determine the equations or
interactions governing the underlying dynamics. In
these settings, parametric ODE model cannot be for-
mulated. Here, we overcome this issue by introducing
a novel paradigm of nonparametric ODE modeling
that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We
propose to learn non-linear, unknown diﬀerential func-
tions from state observations using Gaussian process
vector ﬁelds within the exact ODE formalism. We
demonstrate the model’s capabilities to infer dynamics
from sparse data and to simulate the system forward
into future.

1

Introduction

Dynamical systems modeling is a cornerstone of ex-
perimental sciences. In biology, as well as in physics
and chemistry, modelers attempt to capture the dy-
namical behavior of a given system or a phenomenon
in order to improve its understanding and make pre-
dictions about its future state. Systems of coupled
ordinary diﬀerential equations (ODEs) are undoubt-
edly the most widely used models in science. Even
simple ODE functions can describe complex dynam-
ical behaviours (Hirsch et al., 2004). Typically, the
dynamics are ﬁrmly grounded in physics with only a
few parameters to be estimated from data. However,
equally ubiquitous are the cases where the governing
dynamics are partially or completely unknown.

We consider the dynamics of a system governed by

∗Equal contribution

multivariate ordinary diﬀerential functions:

˙x(t) =

= f (x(t))

(1)

dx(t)
dt

where x(t) ∈ X = RD is the state vector of a D-
dimensional dynamical system at time t, and the
˙x(t) ∈ ˙X = RD is the ﬁrst order time derivative of
x(t) that drives the state x(t) forward, and where
f : RD → RD is the vector-valued derivative function.
The ODE solution is determined by

x(t) = x0 +

f (x(τ ))dτ,

(2)

(cid:90) t

0

where we integrate the system state from an initial
state x(0) = x0 for time t forward. We assume that
f (·) is completely unknown and we only observe one or
several multivariate time series Y = (y1, . . . , yN )T ∈
RN ×D obtained from an additive noisy observation
model at observation time points T = (t1, . . . , tN ) ∈
RN ,

y(t) = x(t) + εt,

(3)

where εt ∼ N (0, Ω) follows a stationary zero-mean
multivariate Gaussian distribution with diagonal noise
variances Ω = diag(ω2
1, . . . , ω2
D). The observation
time points do not need to be equally spaced. Our
task is to learn the diﬀerential function f (·) given
observations Y , with no prior knowledge of the ODE
system.

There is a vast literature on conventional ODEs
(Butcher, 2016) where a parametric form for function
f (x; θ, t) is assumed to be known, and its parameters
θ are subsequently optimised with least squares or
Bayesian approach, where the expensive forward solu-
tion xθ(ti) = (cid:82) ti
0 f (x(τ ); θ, t)dτ is required to evaluate
the system responses xθ(ti) from parameters θ against
observations y(ti). To overcome the computationally
intensive forward solution, a family of methods de-
noted as gradient matching (Varah, 1982; Ellner et al.,

1

2002; Ramsay et al., 2007) have proposed to replace
the forward solution by matching f (yi) ≈ ˙yi to em-
pirical gradients ˙yi of the data instead, which do not
require the costly integration step. Recently several
authors have proposed embedding a parametric diﬀer-
ential function within a Bayesian or Gaussian process
(GP) framework (Graepel, 2003; Calderhead et al.,
2008; Dondelinger et al., 2013; Wang and Barber,
2014; Macdonald, 2017) (see Macdonald et al. (2015)
for a review). GPs have been successfully applied
to model linear diﬀerential equations as they are an-
alytically tractable (Gao et al., 2008; Raissi et al.,
2017).

However, conventional ODE modeling can only pro-
ceed if a parametric form of the driving function f (·)
is known. Recently, initial work to handle unknown
or non-parametric ODE models have been proposed,
although with various limiting approximations. Early
works include spline-based smoothing and additive
functions (cid:80)D
j fj(xj) to infer gene regulatory networks
(De Hoon et al., 2002; Henderson and Michailidis,
2014). ¨Aij¨o and L¨ahdesm¨aki (2009) proposed estimat-
ing the unknown nonlinear function with GPs using
either ﬁnite time diﬀerences, or analytically solving
the derivative function as a function of only time,
˙x(t) = f (t) ( ¨Aij¨o et al., 2013). In a seminal techni-
cal report of Heinonen and d’Alche Buc (2014) a full
vector-valued kernel model f (x) was proposed, how-
ever using a gradient matching approximation. To our
knowledge, there exists no model that can learn non-
linear ODE functions ˙x(t) = f (x(t)) over the state x
against the true forward solutions x(ti).

In this work we propose npODE: the ﬁrst ODE
model for learning arbitrary, and a priori completely
unknown non-parametric, non-linear diﬀerential func-
tions f : X → ˙X from data in a Bayesian way. We
do not use gradient matching or other approxima-
tive models, but instead propose to directly optimise
the exact ODE system with the fully forward simu-
lated responses against data. We parameterise our
model as an augmented Gaussian process vector ﬁeld
with inducing points, while we propose sensitivity
equations to eﬃciently compute the gradients of the
system. Our model can forecast continuous-time sys-
tems arbitrary amounts to future, and we demonstrate
the state-of-the-art performance in human motion
datasets. The MATLAB implementation is publicly
available at github.com/cagatayyildiz/npode.

2 Nonparametric ODE model

The diﬀerential function f (x) to be learned deﬁnes a
vector ﬁeld 1 f , that is, an assignment of a gradient
vector f (x) ∈ RD to every state x ∈ RD. We model
the vector ﬁeld as a vector-valued Gaussian process
(GP) (Rasmussen and Williams, 2006)

f (x) ∼ GP(0, K(x, x(cid:48))),

which deﬁnes a priori distribution over function values
f (x) whose mean and covariances are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)),

(4)

(5)

(6)

and where the kernel K(x, x(cid:48)) ∈ RD×D is matrix-
valued. A GP prior deﬁnes that for any collection
of states X = (x1, . . . , xN )T ∈ RN ×D, the function
values F = (f (x1), . . . , f (xN ))T ∈ RN ×D follow a
matrix-valued normal distribution,

p(F ) = N (vec(F )|0, K(X, X)),

(7)

i,j=1 ∈ RN D×N D is a
where K(X, X) = (K(xi, xj))N
block matrix of matrix-valued kernels K(xi, xj). The
key property of Gaussian processes is that they encode
functions where similar states x, x(cid:48) induce similar
diﬀerentials f (x), f (x(cid:48)), and where the state similarity
is deﬁned by the kernel K(x, x(cid:48)).

In standard GP regression we would obtain poste-
rior of the vector ﬁeld by conditioning the GP prior
with the data (Rasmussen and Williams, 2006). In
ODE models the conditional f (x)|Y of a vector ﬁeld
is intractable due to the integral mapping (2) between
observed states y(ti) and diﬀerentials f (x). Instead,
we resort to augmenting the Gaussian process with a
set of M inducing points z ∈ X and u ∈ ˙X , such that
f (z) = u (Qui˜nonero-Candela and Rasmussen, 2005).
We choose to interpolate the diﬀerential function be-
tween the inducing points as (See Figure 1)

f (x) (cid:44) Kθ(x, Z)Kθ(Z, Z)−1vec(U ),

(8)

which supports the function f (x) with inducing lo-
cations Z = (z1, . . . , zM ), inducing vectors U =
(u1, . . . , uM ), and θ are the kernel parameters. The
function above corresponds to a vector-valued kernel
function (Alvarez et al., 2012), or to a multi-task Gaus-
sian process conditional mean without the variance
term (Rasmussen and Williams, 2006). This deﬁnition

1We use vector ﬁeld and diﬀerential function interchange-

ably.

2

kernel

Kθ(z, z(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(zj − z(cid:48)
(cid:96)2
j



 (9)

with diﬀerential variance σ2
f and dimension-speciﬁc
lengthscales (cid:96) = ((cid:96)1, . . . , (cid:96)D) is expanded into a di-
agonal matrix of size D × D. We collect the kernel
parameters as θ = (σf , (cid:96)).

We note that more complex kernels can also be
considered given prior information of the underlying
system characteristics. The divergence-free matrix-
valued kernel induces vector ﬁelds that have zero di-
vergence (Wahlstr¨om et al., 2013; Solin et al., 2015).
Intuitively, these vector ﬁelds do not have sinks or
sources, and every state always ﬁnally returns to itself
after suﬃcient amount of time. Similarly, curl-free
kernels induce curl-free vector ﬁelds that can contain
sources or sinks, that is, trajectories can accelerate or
decelerate. For theoretical treatment of vector ﬁeld
kernels, see (Narcowich and Ward, 1994; Bhatia et al.,
2013; Fuselier and Wright, 2017). Non-stationary
vector ﬁelds can be modeled with input-dependent
lengthscales (Heinonen et al., 2016), while spectral ker-
nels can represent stationary (Wilson et al., 2013) or
non-stationary (Remes et al., 2017) recurring patterns
in the diﬀerential function.

2.2 Joint model

We assume a Gaussian likelihood over the observations
yi and the corresponding simulated responses x(ti) of
Equation (2),

p(Y |x0, U, Z, ω) =

N (yi|x(ti), Ω),

(10)

N
(cid:89)

i=1

where x(ti) are forward simulated responses using the
integral equation (2) and diﬀerential equation (8), and
Ω = diag(ω2
D) collects the dimension-speciﬁc
noise variances.

1 . . . , ω2

The inducing vectors have a Gaussian process prior

p(U |Z, θ) = N (vec(U )|0, Kθ(Z, Z)).

(11)

The model posterior is then

p(U, x0, θ, ω|Y ) ∝ p(Y |x0, U, ω)p(U |θ) = L,

(12)

where we have for brevity omitted the dependency on
the locations of the inducing points Z and also the
parameter hyperpriors p(θ) and p(ω) since we assume

Figure 1: (a) Illustration of an ODE system vector
ﬁeld induced by the Gaussian process. The vector ﬁeld
f (x) (gray arrows) at arbitrary states x is interpolated
from the inducing points u, z (black arrows), with the
trajectory x(t) (red points) following the diﬀerential
system f (x) exactly. (b) The trajectory x(t) plotted
over time t.

is then compatible with the deterministic nature of
the ODE formalism. Due to universality of several
kernels and kernel functions (Shawe-Taylor and Cris-
tianini, 2004), we can represent arbitrary vector ﬁelds
with appropriate inducing point and kernel choices.

2.1 Operator-valued kernels

The vector-valued kernel function (8) uses operator-
valued kernels, which result in matrix-valued ker-
nels Kθ(z, z(cid:48)) ∈ RD×D for real valued states x, z,
while the kernel matrix over data points becomes
i,j=1 ∈ RM D×M D (See Alvarez et al.
Kθ = (K(zi, zj))M
(2012) for a review). Most straightforward operator-
valued kernel is the identity decomposable kernel
Kdec(z, z(cid:48)) = k(z, z(cid:48)) · ID, where the scalar Gaussian

3

them to be uniform, unless there is speciﬁc domain
knowledge of the priors.

The model parameters are the initial state x0

2, the
inducing vectors U , the noise standard deviations
ω = (ω1, . . . , ωD), and the kernel hyperparameters
θ = (σf , (cid:96)1, . . . , (cid:96)D).

2.3 Noncentral parameterisation

We apply a latent parameterisation using Cholesky
decomposition LθLT
θ = Kθ(Z, Z), which maps the
inducing vectors to whitened domain (Kuss and Ras-
mussen, 2005)

U = Lθ (cid:101)U ,

(cid:101)U = L−1

θ U.

(13)

The latent variables (cid:101)U are projected on the kernel
manifold Lθ to obtain the inducing vectors U . This
non-centered parameterisation (NCP) transforms the
hierarchical posterior L of Equation (12) into a repa-
rameterised form

p(x0, (cid:101)U , θ, ω|Y ) ∝ p(Y |x0, (cid:101)U , ω, θ)p( (cid:101)U ),

(14)

where all variables to be optimised are decoupled, with
the latent inducing vectors having a standard normal
prior (cid:101)U ∼ N (0, I). Optimizing (cid:101)U and θ is now more
eﬃcient since they have independent contributions to
the vector ﬁeld via U = Lθ (cid:101)U . The gradients of the
whitened posterior can be retrieved analytically as
(Heinonen et al., 2016)

∇

(cid:101)U log L = LT

θ ∇U log L.

(15)

Finally, we ﬁnd a MAP estimate for the initial state
x0, latent vector ﬁeld (cid:101)U , kernel parameters θ and
noises ω by gradient ascent,

x0,map, (cid:101)Umap, θmap, ωmap = arg max
x0, (cid:101)U ,θ,ω

log L,

(16)

while keeping the inducing locations Z ﬁxed on a
suﬃciently dense grid (See Figure 1). The partial
derivatives of the posterior with respect to noise pa-
rameters ω can be found analytically, while the deriva-
tives with respect to σf are approximated with ﬁnite
diﬀerences. We select the optimal lengthscales (cid:96) by
cross-validation.

2In case of multiple time-series, we will use one initial state

for each time-series.

3 Sensitivity equations

The key term to carry out the MAP gradient ascent
optimization is the likelihood

log p(Y |x0, (cid:101)U , ω)

that requires forward integration and computing the
partial derivatives with respect to the whitened induc-
ing vectors (cid:101)U . Given Equation (15) we only need to
compute the gradients with respect to the inducing
vectors u = vec(U ) ∈ RM D,

d log p(Y |x0, u, ω)
du

=

N
(cid:88)

s=1

d log N (ys|x(ts, u), Ω)
dx

dx(ts, u)
du

.

(17)

This requires computing the derivatives of the simu-
lated system response x(t, u) against the vector ﬁeld
parameters u,

dx(t, u)
du

≡ S(t) ∈ RD×M D,

(18)

∂uj

which we denote by Sij(t) = ∂x(t,u)i
, and expand
the notation to make the dependency of x on u ex-
plicit. Approximating these with ﬁnite diﬀerences is
possible in principle, but is highly ineﬃcient and has
been reported to cause unstability (Raue et al., 2013).
We instead turn to sensitivity equations for u and
x0 that provide computationally eﬃcient, analytical
gradients S(t) (Kokotovic and Heller, 1967; Fr¨ohlich
et al., 2017).

The solution for dx(t,u)

can be derived by diﬀer-
entiating the full nonparametric ODE system with
respect to u by

du

d
du

dx(t, u)
dt

d
du

=

f (x(t, u)).

(19)

The sensitivity equation for the given system can be
obtained by changing the order of diﬀerentiation on
the left hand side and carrying out the diﬀerentia-
tion on the right hand side. The resulting sensitivity
equation can then be expressed in the form

˙S(t)
(cid:125)(cid:124)

(cid:123)
dx(t, u)
du

(cid:122)
d
dt

J(t)
(cid:122)
(cid:123)
(cid:125)(cid:124)
∂f (x(t, u))
∂x

S(t)
(cid:122) (cid:125)(cid:124) (cid:123)
dx(t, u)
du

R(t)
(cid:123)
(cid:122)
(cid:125)(cid:124)
∂f (x(t, u))
,
∂u

+

=

(20)

where J(t) ∈ RD×D, R(t), ˙S(t) ∈ RD×M D (See Ap-
pendix for detailed speciﬁcation). For our nonpara-
metric ODE system the sensitivity equation is fully

4

determined by

J(t) =

∂K(x, Z)
∂x
R(t) = K(x, Z)K(Z, Z)−1.

K(Z, Z)−1u

(21)

(22)

The sensitivity equation provides us with an addi-
tional ODE system which describes the time evolution
of the derivatives with respect to the inducing vectors
S(t). The sensitivities are coupled with the actual
ODE system and, thus, both systems x(t) and S(t)
are concatenated as the new augmented state that is
solved jointly by Equation (2) driven by the diﬀer-
entials ˙x(t) and ˙S(t) (Leis and Kramer, 1988). The
initial sensitivities are computed as S(0) = dx0
du . In our
implementation, we merge x0 with u for sensitivity
analysis to obtain the partial derivatives with respect
to the initial state which is estimated along with the
other parameters. We use the cvodes solver from the
Sundials package (Hindmarsh et al., 2005) to solve
the nonparametric ODE models and the correspond-
ing gradients numerically. The sensitivity equation
based approach is superior to the ﬁnite diﬀerences
approximation because we have exact formulation for
the gradients of state over inducing points, which can
be solved up to the numerical accuracy of the ODE
solver.

4 Simple simulated dynamics

As ﬁrst illustration of the proposed nonparametric
ODE method we consider three simulated diﬀerential
systems: the Van der Pol (VDP), FitzHugh-Nagumo
(FHN) and Lotka-Volterra (LV) oscillators of form

vdp :

˙x1 = x2

˙x2 = (1 − x2

1)x2 − x1

fhn :

lv :

˙x1 = 3(x1 −

+ x2)

˙x2 =

x3
1
3

0.2 − 3x1 − 0.2x2
3

˙x1 = 1.5x1 − x1x2

˙x2 = −3x2 + x1x2.

In the conventional ODE case the coeﬃcients of these
equations can be inferred using standard statistical
techniques if suﬃcient amount of time series data is
available (Girolami, 2008; Raue et al., 2013). Our
main goal is to infer unknown dynamics, that is, when
these equations are unavailable and we instead repre-
sent the dynamics with a nonparametric vector ﬁeld
of Equation (8). We use these simulated models to
only illustrate our model behavior against the true
dynamics.

We employ 25 data points from one cycle of noisy
observation data from VDP and FHN models, and 25

data points from 1.7 cycles from the LV model with
noise variance of σ2
n = 0.12. We learn the npODE
model with these training data using M = 52 induc-
ing locations on a ﬁxed grid, and forecast between
4 and 8 future cycles starting from true initial state
x0 at time 0. Figure 2 (bottom) shows the training
datasets (grey regions), initial states, true trajectories
(black lines) and the forecasted trajectory likelihoods
(colored regions). The model accurately learns the
dynamics from less than two cycles of data and can
reproduce them reliably into future.

Figure 2 (top) shows the corresponding true vector
ﬁeld (black arrows) and the estimated vector ﬁeld
(grey arrows). The vector ﬁeld is a continuous func-
tion, which is plotted on a 8x8 grid for visualisation.
In general the most diﬃcult part of the system is learn-
ing the middle of the loop (as seen in the FHN model),
and learning the most outermost regions (bottom left
in the LV model). The model learns the underlying
diﬀerential f (x) accurately close to observed points,
while making only few errors in the border regions
with no data.

5 Unknown system estimation

Next, we illustrate how the model estimates real-
istic, unknown dynamics from noisy observations
y(t1), . . . , y(tN ). As in Section 4, we make no as-
sumptions on the structure or form of the underlying
system, and capture the underlying dynamics with
the nonparameteric system alone. We employ no sub-
jective priors, and assume no inputs, controls or other
sources of information. The task is to infer the under-
lying dynamics f (x), and interpolate or extrapolate
the state trajectory outside the observed data.

We use a benchmark dataset of human motion cap-
ture data from the Carnegie Mellon University motion
capture (CMU mocap) database. Our dataset con-
tains 50-dimensional pose measurements y(ti) from
humans walking, where each pose dimension records
a measurement in diﬀerent parts of the body during

Table 1: Means and standard deviations of RMSEs of
43 datasets in forecasting and ﬁlling experiments.

Model
npODE
GPDM
VGPLVM

Forecasting
4.52 ± 2.31
4.94 ± 3.3
8.74 ± 3.43

Imputation
3.94 ± 3.50
5.31 ± 3.39
3.91 ± 1.80

5

Figure 2: Estimated dynamics from Van der Pol, FitzHugh-Nagumo and Lotka-Volterra systems. The top
part (a-c) shows the learned vector ﬁeld (grey arrows) against the true vector ﬁeld (black arrows). The
bottom part (d-f ) shows the training data (grey region points) and forecasted future cycle likelihoods with
the learned model (shaded region) against the true trajectory (black line).

movement (Wang et al., 2008). We apply the prepro-
cessing of Wang et al. (2008) by downsampling the
datasets by a factor of four and centering the data.
This resulted in a total of 4303 datapoints spread
across 43 trajectories with on average 100 frames per
trajectory. In order to tackle the problem of dimen-
sionality, we project the original dataset with PCA to
a three dimensional latent space where the system is
speciﬁed, following Damianou et al. (2011) and Wang
et al. (2006). We place M = 53 inducing vectors on
a ﬁxed grid, and optimize our model from 100 initial
values, which we select by projecting empirical diﬀer-
ences y(ti) − y(ti−1) to the inducing vectors. We use
an LBFGS optimizer in Matlab. The whole inference
takes approximately few minutes per trajectory.

We evaluate the method with two types of experi-
ments: imputing missing values and forecasting future
cycles. For the forecasting the ﬁrst half of the trajec-
tory is for model training, and the second half is to be
forecasted. For imputation we remove roughly 20% of
the frames from the middle of the trajectory, which
are to be ﬁlled by the models. We perform model se-

lection for lengthscales (cid:96) with cross-validation split of
80/20. We record the root mean square error (RMSE)
over test points in the original feature space in both
cases, where we reconstruct the original dimensions
from the latent space trajectories.

Due to the current lack of ODE methods suitable for
this nonparametric inference task, we instead compare
our method to the state-of-the-art state-space models
where such problems have been previously considered
(Wang et al., 2008). In a state-space or dynamical
model a transition function x(tk+1) = g(x(tk)) moves
the system forward in discrete steps. With suﬃciently
high sampling rate, such models can estimate and fore-
cast ﬁnite approximations of smooth dynamics. In
Gaussian process dynamical model (Wang et al., 2006;
Frigola et al., 2014; Svensson et al., 2016) a GP transi-
tion function is inferred in a latent space, which can be
inferred with a standard GPLVM (Lawrence, 2004) or
with a dependent GPLVM (Zhao and Sun, 2016). In
dynamical systems the transition function is replaced
by a GP interpolation (Damianou et al., 2011). The
discrete time state-space models emphasize inference

6

Figure 3: Forecasting 50 future frames after 49 frames of training data of human motion dataset 35 12.amc.
(a) The estimated locations of the trajectory in a latent space (black points) and future forecast (colored
lines). (b) The original features reconstructed from the latent predictions with grey region showing the
training data.

of a low-dimensional manifold as an explanation of
the high-dimensional measurement trajectories.

We compare our method to the dynamical model
GPDM of Wang et al. (2006) and to the dynamical
system VGPLVM of Damianou et al. (2011), where we
directly apply the implementations provided by the
authors at inverseprobability.com/vargplvm and
dgp.toronto.edu/~jmwang/gpdm. Both methods op-
timize their latent spaces separately, and they are
thus not directly comparable.

5.1 Forecasting

In the forecasting task we train all models with the
ﬁrst half of the trajectory, while forecasting the sec-
ond half starting from the ﬁrst frame. The models
are trained and forecasted within a low-dimensional

space, and subsequently projected back into the orig-
inal space via inverting the PCA or with GPLVM
mean predictions. As all methods optimize their la-
tent spaces separately, they are not directly compa-
rable. Thus, the mean errors are computed in the
original high-dimensional space. Note that the low-
dimensional representation necessarily causes some
reconstruction errors.

Figure 3 illustrates the models on one of the trajec-
tories 35 12.amc. The top part (a) shows the training
data in the PCA space for npODE, and optimized
training data representation for GPDM and VGPLVM
(black points). The colored lines (npODE) and points
(GPDM, VGPLVM) indicate the future forecast. The
bottom part (b) shows the ﬁrst 9 reconstructed orig-
inal pose dimensions reconstructed from the latent
forecasted trajectories. The training data is shown

7

Figure 4: Imputation of 17 missing frames in the middle of a 94-length trajectory of human motion dataset
07 07.amc (subsampled every fourth frame). (a) The estimated locations of the missing points in the latent
space are colored. (b) The original features reconstructed from the latent trajectory.

in gray background, while test data is shown with
circles.

The VGPLVM has most trouble forecasting future
points, and reverts quickly after training data to a
value close to zero, failing to predict future points.
The GPDM model produces more realistic trajectories,
but fails to predict any of the poses accurately. Finally,
npODE can accurately predict ﬁve poses, and still
retains adequate performance on remaining poses,
except for pose 2.

Furthermore, Table 1 indicates that npODE is also
best performing method on average over the whole
dataset in the forecasting.

5.2 Imputation

In the imputation task we remove approximately 20%
of the training data from the middle of the trajectory.

The goal is to learn a model with the remaining data
and forecast the missing values. Figure 4 highlights
the performance of the three models on the trajectory
07 07.amc. The top part (a) shows the training data
(black points) in the PCA space (npODE) or optimized
training locations in the latent space (GPDM, VG-
PLVM). The middle part imputation is shown with
colored points or lines. Interestingly both npODE
and GPDM operate on cyclic representations, while
VGPLVM is not cyclic.

The bottom panel (b) shows the ﬁrst 9 recon-
structed pose dimensions from the three models. The
missing values are shown in circles, while training
points are shown with black dots. All models can
accurately reproduce the overall trends, while npODE
seems to ﬁt slightly worse than the other methods.
The PCA projection causes the seemingly perfect ﬁt of
the npODE prediction (at the top) to lead to slightly

8

warped reconstructions (at the bottom). All methods
mostly ﬁt the missing parts as well. Table 1 shows
that on average the npODE and VGPLVM have ap-
proximately equal top performance on the imputing
missing values task.

6 Discussion

We proposed the framework of nonparametric ODE
model that can accurately learn arbitrary, nonlinear
continuos-time dynamics from purely observational
data without making assumptions of the underlying
system dynamics. We demonstrated that the model
excels at learning dynamics that can be forecasted
into the future. We consider this work as the ﬁrst
in a line of studies of nonparametric ODE systems,
and foresee several aspects as future work. Currently
we do not handle non-stationary vector ﬁelds, that is
time-dependent diﬀerentials ft(x). Furthermore, an
interesting future avenue is the study of various vector
ﬁeld kernels, such as divergence-free, curl-free or spec-
tral kernels (Remes et al., 2017). Finally, including
inputs or controls to the system would allow precise
modelling in interactive settings, such as robotics.

The proposed nonparametric ODE model operates
along a continuous-time trajectory, while dynamic
models such as hidden Markov models or state-space
models are restricted to discrete time steps. These
models are unable to consider system state at arbitrary
times, for instance, between two successive timepoints.

Conventional ODE models have also been consid-
ered from the stochastic perspective with stochastic
diﬀerential equation (SDE) models that commonly
model the system drift and diﬀusion processes sepa-
rately leading to a distribution of trajectories p(x(t)).
As future work we will consider stochastic extensions
of our nonparametric ODE model, as well as MCMC
sampling of the inducing point posterior p(U |Y ), lead-
ing to trajectory distribution as well.

Acknowledgements. The data used in this project
was obtained from mocap.cs.cmu.edu. The database
was created with funding from NSF EIA-0196217.
This work has been supported by the Academy of
Finland Center of Excellence in Systems Immunol-
ogy and Physiology, the Academy of Finland grants
no. 260403, 299915, 275537, 311584.

9

References

Tarmo ¨Aij¨o, Kirsi Granberg, and Harri L¨ahdesm¨aki.
Sorad: a systems biology approach to predict
and modulate dynamic signaling pathway response
from phosphoproteome time-course measurements.
Bioinformatics, 29(10):1283–1291, 2013.

M. Alvarez, L. Rosasco, and N. Lawrence. Kernels
for vector-valued functions: A review. Foundations
and Trends in Machine Learning, 2012.

H. Bhatia, G. Norgard, V. Pascucci, and P. Bremer.
The helmholtz-hodge decomposition -— a survey.
IEEE Transactions on visualization and computer
graphics, 2013.

J. Butcher. Numerical methods for ordinary diﬀeren-

tial equations. John Wiley & Sons, 2016.

B. Calderhead, M. Girolami, and N. Lawrence. Accel-
erating bayesian inference over nonlinear diﬀerential
equations with gaussian processes. NIPS, 2008.

Andreas Damianou, Michalis K Titsias, and Neil D
Lawrence. Variational gaussian process dynami-
cal systems. In Advances in Neural Information
Processing Systems, pages 2510–2518, 2011.

Michiel JL De Hoon, Seiya Imoto, Kazuo Kobayashi,
Naotake Ogasawara, and Satoru Miyano. Inferring
gene regulatory networks from time-ordered gene
expression data of bacillus subtilis using diﬀeren-
tial equations. In Biocomputing 2003, pages 17–28.
World Scientiﬁc, 2002.

F. Dondelinger, M. Filippone, S Rogers, and D. Hus-
meier. Ode parameter inference using adaptive
gradient matching with gaussian processes. JMLR,
2013.

S. P. Ellner, Y. Seifu, and R. H. Smith. Fitting
population dynamic models to time-series data by
gradient matching. Ecology, 83(8):2256–2270, 2002.

Roger Frigola, Yutian Chen, and Carl Edward Ras-
mussen. Variational gaussian process state-space
models. In Advances in Neural Information Pro-
cessing Systems, pages 3680–3688, 2014.

Fabian Fr¨ohlich, Barbara Kaltenbacher, Fabian J.
Theis, and Jan Hasenauer. Scalable parameter esti-
mation for genome-scale biochemical reaction net-
works. PLOS Computational Biology, 13(1):1–18,
01 2017. doi: 10.1371/journal.pcbi.1005331.

E. Fuselier and G. Wright. A radial basis function
method for computing helmholtz–hodge decompo-
sitions. IMA Journal of Numerical Analysis, 2017.

Pei Gao, Antti Honkela, Magnus Rattray, and Neil D
Lawrence. Gaussian process modelling of latent
chemical species: applications to inferring tran-
scription factor activities. Bioinformatics, 24(16):
i70–i75, 2008.

Mark Girolami. Bayesian inference for diﬀerential
equations. Theor. Comput. Sci., 408(1):4–16, 2008.

T. Graepel. Solving noisy linear operator equations
by gaussian processes: Application to ordinary and
partial diﬀerential equations. ICML, 2003.

M. Heinonen and F. d’Alche Buc. Learning nonpara-
metric diﬀerential equations with operator-valued
kernels and gradient matching. arxiv, Telecom Paris-
Tech, 2014.

M. Heinonen, H. Mannerstr¨om, J. Rousu, S. Kaski,
and H. L¨ahdesm¨aki. Non-stationary Gaussian pro-
cess regression with Hamiltonian Monte Carlo. In
AISTATS, volume 51, pages 732–740, 2016.

J. Henderson and G. Michailidis. Network recon-
struction using nonparametric additive ode models.
PLOS ONE, 2014.

Alan C Hindmarsh, Peter N Brown, Keith E Grant,
Steven L Lee, Radu Serban, Dan E Shumaker, and
Carol S Woodward. SUNDIALS: Suite of nonlinear
and diﬀerential/algebraic equation solvers. ACM
Trans. Math. Softw., 31(3):363–396, 2005.

M. Hirsch, S. Smale, and Devaney. Diﬀerential Equa-
tions, Dynamical Systems, and an Introduction to
Chaos (Edition: 2). Elsevier Science & Technology
Books, 2004.

P Kokotovic and J Heller. Direct and adjoint sensi-
tivity equations for parameter optimization. IEEE
Transactions on Automatic Control, 12(5):609–610,
1967.

Malte Kuss and Carl Edward Rasmussen. Assessing
approximate inference for binary gaussian process
classiﬁcation. Journal of machine learning research,
6(Oct):1679–1704, 2005.

Jorge R. Leis and Mark A. Kramer. The simulta-
neous solution and sensitivity analysis of systems
described by ordinary diﬀerential equations. ACM
Trans. Math. Softw., 14(1), 1988.

Benn Macdonald. Statistical inference for ordinary
diﬀerential equations using gradient matching. PhD
thesis, University of Glasgow, 2017.

Benn Macdonald, Catherine Higham, and Dirk Hus-
meier. Controversy in mechanistic modelling with
gaussian processes. In International Conference on
Machine Learning, pages 1539–1547, 2015.

Francis J Narcowich and Joseph D Ward. Generalized
hermite interpolation via matrix-valued condition-
ally positive deﬁnite functions. Mathematics of
Computation, 63(208):661–687, 1994.

Joaquin Qui˜nonero-Candela and Carl Edward Ras-
mussen. A unifying view of sparse approximate
gaussian process regression. Journal of Machine
Learning Research, 6(Dec):1939–1959, 2005.

Maziar Raissi, Paris Perdikaris, and George Em Karni-
adakis. Inferring solutions of diﬀerential equations
using noisy multi-ﬁdelity data. Journal of Compu-
tational Physics, 335:736–746, 2017.

J. Ramsay, G. Hooker, D. Campbell, and J. Cao. Pa-
rameter estimation for diﬀerential equations: a gen-
eralized smoothing approach. Journal of the Royal
Statistical Society: Series B, 69:741–796, 2007.

C.E. Rasmussen and K.I. Williams. Gaussian pro-

cesses for machine learning. MIT Press, 2006.

Andreas Raue, Marcel Schilling, Julie Bachmann, An-
drew Matteson, Max Schelker, Daniel Kaschek,
Sabine Hug, Clemens Kreutz, Brian D. Harms,
Fabian J. Theis, Ursula Klingm¨uller, and Jens Tim-
mer. Lessons learned from quantitative dynamical
modeling in systems biology. PLOS ONE, 8(9):
1–17, 2013.

S. Remes, M. Heinonen, and S. Kaski. Non-stationary

spectral kernels. NIPS, 2017.

John Shawe-Taylor and Nello Cristianini. Kernel
methods for pattern analysis. Cambridge university
press, 2004.

Neil D Lawrence. Gaussian process latent variable
models for visualisation of high dimensional data. In
Advances in neural information processing systems,
pages 329–336, 2004.

A. Solin, M. Kok, N. Wahlstrom, T. Schon, and
S. S¨arkk¨a. Modeling and interpolation of the am-
bient magnetic ﬁeld by gaussian processes. arXiv,
2015. arXiv:1509.04634.

10

Andreas Svensson, Arno Solin, Simo S¨arkk¨a, and
Thomas Sch¨on. Computationally eﬃcient bayesian
learning of gaussian process state space models. In
Artiﬁcial Intelligence and Statistics, pages 213–221,
2016.

J. M. Varah. A spline least squares method for numer-
ical parameter estimation in diﬀerential equations.
SIAM J.sci. Stat. Comput., 3(1):28–46, 1982.

N. Wahlstr¨om, M. Kok, and T. Sch¨on. Modeling
magnetic ﬁelds using gaussian processes.
IEEE
conf on Acoustics, Speech and Signal Processing
(ICASSP), 2013.

Jack Wang, Aaron Hertzmann, and David M Blei.
Gaussian process dynamical models. In Advances in
neural information processing systems, pages 1441–
1448, 2006.

Jack M Wang, David J Fleet, and Aaron Hertzmann.
Gaussian process dynamical models for human mo-
tion. IEEE transactions on pattern analysis and
machine intelligence, 30(2):283–298, 2008.

Y. Wang and D. Barber. Gaussian processes for
bayesian estimation in ordinary diﬀerential equa-
tions. ICML, 2014.

A. Wilson, E. Gilboa, A. Nehorai, and J. Cunningham.
Fast multidimensional pattern extrapolation with
gaussian processes. AISTATS, 2013.

Jing Zhao and Shiliang Sun. Variational dependent
multi-output gaussian process dynamical systems.
The Journal of Machine Learning Research, 17(1):
4134–4169, 2016.

T ¨Aij¨o and H. L¨ahdesm¨aki. Learning gene regulatory
networks from gene expression measurements using
non-parametric molecular kinetics. Bioinformatics,
25:2937––2944, 2009.

11

Appendix of ‘Learning unknown ODE models with Gaussian pro-
cesses’

Sensitivity Equations

In the main text, the sensitivity equation is formulated using matrix notation

˙S(t) = J(t)S(t) + R(t).

Here, the time-dependent matrices are obtained by diﬀerentiating the vector valued functions with respect to
vectors i.e.



































S(t) =

J(t) =

R(t) =

dx1(t,U )
du1

dx2(t,U )
du1

· · ·
· · ·
dxD(t,U )
du1

dx1(t,U )
du2

dx2(t,U )
du2

· · ·
· · ·
dxD(t,U )
du2

· · ·

· · ·

· · ·
· · ·
· · ·

dx1(t,U )
duMD

dx2(t,U )
duMD

· · ·
· · ·
dxD(t,U )
duMD













∂f (x(t),U )1
∂x1

∂f (x(t),U )2
∂x1

· · ·
· · ·
∂f (x(t),U )D
∂x1

∂f (x(t),U )1
∂u1

∂f (x(t),U )2
∂u1

· · ·
· · ·
∂f (x(t),U )D
∂u1

∂f (x(t),U )1
∂x2

∂f (x(t),U )2
∂x2

· · ·
· · ·
∂f (x(t),U )D
∂x2

∂f (x(t),U )1
∂u2

∂f (x(t),U )2
∂u2

· · ·
· · ·
∂f (x(t),U )D
∂u2

· · ·

· · ·

· · ·
· · ·
· · ·

· · ·

· · ·

· · ·
· · ·
· · ·

D×M D
∂f (x(t),U )1
∂xD

∂f (x(t),U )2
∂xD

· · ·
· · ·
∂f (x(t),U )D
∂xD

∂f (x(t),U )1
∂uMD

∂f (x(t),U )2
∂uMD

· · ·
· · ·
∂f (x(t),U )D
∂uMD























D×D

D×M D

Optimization

Below is the explicit form of the log posterior. Note that we introduce u = vec(U ) and Ω = diag(ω2
for notational simplicity.

1, . . . , ω2

D)

log L = log p(U |θ) + log p(Y |x0, U, ω)

= log N (u|0, Kθ(Z, Z)) +

log N (yi|x(ti, U ), Ω)

N
(cid:88)

i=1

1
2

1
2

1
2

1
2

1
2

1
2

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

(yij − xj(ti, U, x0))2
ω2
j

−

N
(cid:88)

i=1

1
2

(yi,j − xj(ti, U, x0))2
ω2
j

D
(cid:88)

j=1

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

− N

log ωj

(30)

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

log |Ω|

(29)

Our goal is to compute the gradients with respect to the initial state x0, latent vector ﬁeld (cid:101)U , kernel
parameters θ and noise variables ω. As explained in the paper, we compute the gradient of the posterior with

12

(23)

(24)

(25)

(26)

(27)

(28)

respect to inducing vectors U and project them to the white domain thanks to noncentral parameterisation.
The analytical forms of the partial derivatives are as follows:

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂uk

− Kθ(Z, Z)−1u

∂ log L
∂uk

∂ log L
∂(x0)d

∂ log L
∂ωj

=

=

=

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

1
ω3
j

N
(cid:88)

i=1

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂(x0)d

(yi,j − xj(ti, U, x0))2 −

N
ωj

Seemingly hard to compute terms, ∂xj (ti,U,x0)

, are computed using sensitivities. The
lengthscale parameter (cid:96) is considered as a model complexity parameter and is chosen from a grid using
cross-validation. We furthermore need the gradient with respect to the other kernel variable, i.e., the signal
variance σ2
f . Because Kθ(Z, Z) and x(ti, U ) are the functions of kernel, computing the gradients with respect
to σ2

f is not trivial and we make use of ﬁnite diﬀerences:

and ∂xj (ti,U,x0)

∂(x0)d

∂uk

We use δ = 10−4 to compute the ﬁnite diﬀerences.

One problem of using gradient-based optimization techniques is that they do not ensure the positivity of
the parameters being optimized. Therefore, we perform the optimization of the noise standard deviations
ω = (ω1, . . . , ωD) and signal variance σf with respect to their logarithms:

∂ log L
∂σf

=

log L(σf + δ) − log L(σf )
δ

∂ log L
∂ log c

=

∂ log L
∂c

∂c
∂ log c

=

∂ log L
∂c

c

where c ∈ (σf , ω1, . . . , ωD).

Implementation details

We initialise the inducing vectors U = (u1, . . . uM ) by computing the empirical gradients ˙yi = yi − yi−1, and
conditioning as

U0 = K(Z, Y )K(Y, Y )−1c ˙y,

where we optimize the scale c against the posterior. The whitened inducing vector is obtained as (cid:101)U0 = L−1
θ U0.
This procedure produces initial vector ﬁelds that partially match the trajectory already. We then do 100
restarts of the optimization from random perturbations (cid:101)U = (cid:101)U0 + ε.

We use LBFGS gradient optimization routine in Matlab. We initialise the inducing vector locations Z on a
equidistant ﬁxed grid on a box containing the observed points. We select the lengthscales (cid:96)1, . . . , (cid:96)D using
cross-validation from values {0.5, 0.75, 1, 1.25, 1.5}. In general large lengthscales induce smoother models,
while lower lengthscales cause overﬁtting.

(31)

(32)

(33)

(34)

(35)

(36)

13

8
1
0
2
 
r
a

M
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
0
3
4
0
.
3
0
8
1
:
v
i
X
r
a

Learning unknown ODE models with Gaussian processes

Markus Heinonen1,2,∗

Cagatay Yildiz1,∗

Henrik Mannerstr¨om1

Jukka Intosalmi1

Harri L¨ahdesm¨aki1

1 Department of Computer Science, Aalto university
2 Helsinki Institute for Information Technology HIIT

Abstract

In conventional ODE modelling coeﬃcients of an equa-
tion driving the system state forward in time are es-
timated. However, for many complex systems it is
practically impossible to determine the equations or
interactions governing the underlying dynamics. In
these settings, parametric ODE model cannot be for-
mulated. Here, we overcome this issue by introducing
a novel paradigm of nonparametric ODE modeling
that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We
propose to learn non-linear, unknown diﬀerential func-
tions from state observations using Gaussian process
vector ﬁelds within the exact ODE formalism. We
demonstrate the model’s capabilities to infer dynamics
from sparse data and to simulate the system forward
into future.

1

Introduction

Dynamical systems modeling is a cornerstone of ex-
perimental sciences. In biology, as well as in physics
and chemistry, modelers attempt to capture the dy-
namical behavior of a given system or a phenomenon
in order to improve its understanding and make pre-
dictions about its future state. Systems of coupled
ordinary diﬀerential equations (ODEs) are undoubt-
edly the most widely used models in science. Even
simple ODE functions can describe complex dynam-
ical behaviours (Hirsch et al., 2004). Typically, the
dynamics are ﬁrmly grounded in physics with only a
few parameters to be estimated from data. However,
equally ubiquitous are the cases where the governing
dynamics are partially or completely unknown.

We consider the dynamics of a system governed by

∗Equal contribution

multivariate ordinary diﬀerential functions:

˙x(t) =

= f (x(t))

(1)

dx(t)
dt

where x(t) ∈ X = RD is the state vector of a D-
dimensional dynamical system at time t, and the
˙x(t) ∈ ˙X = RD is the ﬁrst order time derivative of
x(t) that drives the state x(t) forward, and where
f : RD → RD is the vector-valued derivative function.
The ODE solution is determined by

x(t) = x0 +

f (x(τ ))dτ,

(2)

(cid:90) t

0

where we integrate the system state from an initial
state x(0) = x0 for time t forward. We assume that
f (·) is completely unknown and we only observe one or
several multivariate time series Y = (y1, . . . , yN )T ∈
RN ×D obtained from an additive noisy observation
model at observation time points T = (t1, . . . , tN ) ∈
RN ,

y(t) = x(t) + εt,

(3)

where εt ∼ N (0, Ω) follows a stationary zero-mean
multivariate Gaussian distribution with diagonal noise
variances Ω = diag(ω2
1, . . . , ω2
D). The observation
time points do not need to be equally spaced. Our
task is to learn the diﬀerential function f (·) given
observations Y , with no prior knowledge of the ODE
system.

There is a vast literature on conventional ODEs
(Butcher, 2016) where a parametric form for function
f (x; θ, t) is assumed to be known, and its parameters
θ are subsequently optimised with least squares or
Bayesian approach, where the expensive forward solu-
tion xθ(ti) = (cid:82) ti
0 f (x(τ ); θ, t)dτ is required to evaluate
the system responses xθ(ti) from parameters θ against
observations y(ti). To overcome the computationally
intensive forward solution, a family of methods de-
noted as gradient matching (Varah, 1982; Ellner et al.,

1

2002; Ramsay et al., 2007) have proposed to replace
the forward solution by matching f (yi) ≈ ˙yi to em-
pirical gradients ˙yi of the data instead, which do not
require the costly integration step. Recently several
authors have proposed embedding a parametric diﬀer-
ential function within a Bayesian or Gaussian process
(GP) framework (Graepel, 2003; Calderhead et al.,
2008; Dondelinger et al., 2013; Wang and Barber,
2014; Macdonald, 2017) (see Macdonald et al. (2015)
for a review). GPs have been successfully applied
to model linear diﬀerential equations as they are an-
alytically tractable (Gao et al., 2008; Raissi et al.,
2017).

However, conventional ODE modeling can only pro-
ceed if a parametric form of the driving function f (·)
is known. Recently, initial work to handle unknown
or non-parametric ODE models have been proposed,
although with various limiting approximations. Early
works include spline-based smoothing and additive
functions (cid:80)D
j fj(xj) to infer gene regulatory networks
(De Hoon et al., 2002; Henderson and Michailidis,
2014). ¨Aij¨o and L¨ahdesm¨aki (2009) proposed estimat-
ing the unknown nonlinear function with GPs using
either ﬁnite time diﬀerences, or analytically solving
the derivative function as a function of only time,
˙x(t) = f (t) ( ¨Aij¨o et al., 2013). In a seminal techni-
cal report of Heinonen and d’Alche Buc (2014) a full
vector-valued kernel model f (x) was proposed, how-
ever using a gradient matching approximation. To our
knowledge, there exists no model that can learn non-
linear ODE functions ˙x(t) = f (x(t)) over the state x
against the true forward solutions x(ti).

In this work we propose npODE: the ﬁrst ODE
model for learning arbitrary, and a priori completely
unknown non-parametric, non-linear diﬀerential func-
tions f : X → ˙X from data in a Bayesian way. We
do not use gradient matching or other approxima-
tive models, but instead propose to directly optimise
the exact ODE system with the fully forward simu-
lated responses against data. We parameterise our
model as an augmented Gaussian process vector ﬁeld
with inducing points, while we propose sensitivity
equations to eﬃciently compute the gradients of the
system. Our model can forecast continuous-time sys-
tems arbitrary amounts to future, and we demonstrate
the state-of-the-art performance in human motion
datasets. The MATLAB implementation is publicly
available at github.com/cagatayyildiz/npode.

2 Nonparametric ODE model

The diﬀerential function f (x) to be learned deﬁnes a
vector ﬁeld 1 f , that is, an assignment of a gradient
vector f (x) ∈ RD to every state x ∈ RD. We model
the vector ﬁeld as a vector-valued Gaussian process
(GP) (Rasmussen and Williams, 2006)

f (x) ∼ GP(0, K(x, x(cid:48))),

which deﬁnes a priori distribution over function values
f (x) whose mean and covariances are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)),

(4)

(5)

(6)

and where the kernel K(x, x(cid:48)) ∈ RD×D is matrix-
valued. A GP prior deﬁnes that for any collection
of states X = (x1, . . . , xN )T ∈ RN ×D, the function
values F = (f (x1), . . . , f (xN ))T ∈ RN ×D follow a
matrix-valued normal distribution,

p(F ) = N (vec(F )|0, K(X, X)),

(7)

i,j=1 ∈ RN D×N D is a
where K(X, X) = (K(xi, xj))N
block matrix of matrix-valued kernels K(xi, xj). The
key property of Gaussian processes is that they encode
functions where similar states x, x(cid:48) induce similar
diﬀerentials f (x), f (x(cid:48)), and where the state similarity
is deﬁned by the kernel K(x, x(cid:48)).

In standard GP regression we would obtain poste-
rior of the vector ﬁeld by conditioning the GP prior
with the data (Rasmussen and Williams, 2006). In
ODE models the conditional f (x)|Y of a vector ﬁeld
is intractable due to the integral mapping (2) between
observed states y(ti) and diﬀerentials f (x). Instead,
we resort to augmenting the Gaussian process with a
set of M inducing points z ∈ X and u ∈ ˙X , such that
f (z) = u (Qui˜nonero-Candela and Rasmussen, 2005).
We choose to interpolate the diﬀerential function be-
tween the inducing points as (See Figure 1)

f (x) (cid:44) Kθ(x, Z)Kθ(Z, Z)−1vec(U ),

(8)

which supports the function f (x) with inducing lo-
cations Z = (z1, . . . , zM ), inducing vectors U =
(u1, . . . , uM ), and θ are the kernel parameters. The
function above corresponds to a vector-valued kernel
function (Alvarez et al., 2012), or to a multi-task Gaus-
sian process conditional mean without the variance
term (Rasmussen and Williams, 2006). This deﬁnition

1We use vector ﬁeld and diﬀerential function interchange-

ably.

2

kernel

Kθ(z, z(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(zj − z(cid:48)
(cid:96)2
j



 (9)

with diﬀerential variance σ2
f and dimension-speciﬁc
lengthscales (cid:96) = ((cid:96)1, . . . , (cid:96)D) is expanded into a di-
agonal matrix of size D × D. We collect the kernel
parameters as θ = (σf , (cid:96)).

We note that more complex kernels can also be
considered given prior information of the underlying
system characteristics. The divergence-free matrix-
valued kernel induces vector ﬁelds that have zero di-
vergence (Wahlstr¨om et al., 2013; Solin et al., 2015).
Intuitively, these vector ﬁelds do not have sinks or
sources, and every state always ﬁnally returns to itself
after suﬃcient amount of time. Similarly, curl-free
kernels induce curl-free vector ﬁelds that can contain
sources or sinks, that is, trajectories can accelerate or
decelerate. For theoretical treatment of vector ﬁeld
kernels, see (Narcowich and Ward, 1994; Bhatia et al.,
2013; Fuselier and Wright, 2017). Non-stationary
vector ﬁelds can be modeled with input-dependent
lengthscales (Heinonen et al., 2016), while spectral ker-
nels can represent stationary (Wilson et al., 2013) or
non-stationary (Remes et al., 2017) recurring patterns
in the diﬀerential function.

2.2 Joint model

We assume a Gaussian likelihood over the observations
yi and the corresponding simulated responses x(ti) of
Equation (2),

p(Y |x0, U, Z, ω) =

N (yi|x(ti), Ω),

(10)

N
(cid:89)

i=1

where x(ti) are forward simulated responses using the
integral equation (2) and diﬀerential equation (8), and
Ω = diag(ω2
D) collects the dimension-speciﬁc
noise variances.

1 . . . , ω2

The inducing vectors have a Gaussian process prior

p(U |Z, θ) = N (vec(U )|0, Kθ(Z, Z)).

(11)

The model posterior is then

p(U, x0, θ, ω|Y ) ∝ p(Y |x0, U, ω)p(U |θ) = L,

(12)

where we have for brevity omitted the dependency on
the locations of the inducing points Z and also the
parameter hyperpriors p(θ) and p(ω) since we assume

Figure 1: (a) Illustration of an ODE system vector
ﬁeld induced by the Gaussian process. The vector ﬁeld
f (x) (gray arrows) at arbitrary states x is interpolated
from the inducing points u, z (black arrows), with the
trajectory x(t) (red points) following the diﬀerential
system f (x) exactly. (b) The trajectory x(t) plotted
over time t.

is then compatible with the deterministic nature of
the ODE formalism. Due to universality of several
kernels and kernel functions (Shawe-Taylor and Cris-
tianini, 2004), we can represent arbitrary vector ﬁelds
with appropriate inducing point and kernel choices.

2.1 Operator-valued kernels

The vector-valued kernel function (8) uses operator-
valued kernels, which result in matrix-valued ker-
nels Kθ(z, z(cid:48)) ∈ RD×D for real valued states x, z,
while the kernel matrix over data points becomes
i,j=1 ∈ RM D×M D (See Alvarez et al.
Kθ = (K(zi, zj))M
(2012) for a review). Most straightforward operator-
valued kernel is the identity decomposable kernel
Kdec(z, z(cid:48)) = k(z, z(cid:48)) · ID, where the scalar Gaussian

3

them to be uniform, unless there is speciﬁc domain
knowledge of the priors.

The model parameters are the initial state x0

2, the
inducing vectors U , the noise standard deviations
ω = (ω1, . . . , ωD), and the kernel hyperparameters
θ = (σf , (cid:96)1, . . . , (cid:96)D).

2.3 Noncentral parameterisation

We apply a latent parameterisation using Cholesky
decomposition LθLT
θ = Kθ(Z, Z), which maps the
inducing vectors to whitened domain (Kuss and Ras-
mussen, 2005)

U = Lθ (cid:101)U ,

(cid:101)U = L−1

θ U.

(13)

The latent variables (cid:101)U are projected on the kernel
manifold Lθ to obtain the inducing vectors U . This
non-centered parameterisation (NCP) transforms the
hierarchical posterior L of Equation (12) into a repa-
rameterised form

p(x0, (cid:101)U , θ, ω|Y ) ∝ p(Y |x0, (cid:101)U , ω, θ)p( (cid:101)U ),

(14)

where all variables to be optimised are decoupled, with
the latent inducing vectors having a standard normal
prior (cid:101)U ∼ N (0, I). Optimizing (cid:101)U and θ is now more
eﬃcient since they have independent contributions to
the vector ﬁeld via U = Lθ (cid:101)U . The gradients of the
whitened posterior can be retrieved analytically as
(Heinonen et al., 2016)

∇

(cid:101)U log L = LT

θ ∇U log L.

(15)

Finally, we ﬁnd a MAP estimate for the initial state
x0, latent vector ﬁeld (cid:101)U , kernel parameters θ and
noises ω by gradient ascent,

x0,map, (cid:101)Umap, θmap, ωmap = arg max
x0, (cid:101)U ,θ,ω

log L,

(16)

while keeping the inducing locations Z ﬁxed on a
suﬃciently dense grid (See Figure 1). The partial
derivatives of the posterior with respect to noise pa-
rameters ω can be found analytically, while the deriva-
tives with respect to σf are approximated with ﬁnite
diﬀerences. We select the optimal lengthscales (cid:96) by
cross-validation.

2In case of multiple time-series, we will use one initial state

for each time-series.

3 Sensitivity equations

The key term to carry out the MAP gradient ascent
optimization is the likelihood

log p(Y |x0, (cid:101)U , ω)

that requires forward integration and computing the
partial derivatives with respect to the whitened induc-
ing vectors (cid:101)U . Given Equation (15) we only need to
compute the gradients with respect to the inducing
vectors u = vec(U ) ∈ RM D,

d log p(Y |x0, u, ω)
du

=

N
(cid:88)

s=1

d log N (ys|x(ts, u), Ω)
dx

dx(ts, u)
du

.

(17)

This requires computing the derivatives of the simu-
lated system response x(t, u) against the vector ﬁeld
parameters u,

dx(t, u)
du

≡ S(t) ∈ RD×M D,

(18)

∂uj

which we denote by Sij(t) = ∂x(t,u)i
, and expand
the notation to make the dependency of x on u ex-
plicit. Approximating these with ﬁnite diﬀerences is
possible in principle, but is highly ineﬃcient and has
been reported to cause unstability (Raue et al., 2013).
We instead turn to sensitivity equations for u and
x0 that provide computationally eﬃcient, analytical
gradients S(t) (Kokotovic and Heller, 1967; Fr¨ohlich
et al., 2017).

The solution for dx(t,u)

can be derived by diﬀer-
entiating the full nonparametric ODE system with
respect to u by

du

d
du

dx(t, u)
dt

d
du

=

f (x(t, u)).

(19)

The sensitivity equation for the given system can be
obtained by changing the order of diﬀerentiation on
the left hand side and carrying out the diﬀerentia-
tion on the right hand side. The resulting sensitivity
equation can then be expressed in the form

˙S(t)
(cid:125)(cid:124)

(cid:123)
dx(t, u)
du

(cid:122)
d
dt

J(t)
(cid:122)
(cid:123)
(cid:125)(cid:124)
∂f (x(t, u))
∂x

S(t)
(cid:122) (cid:125)(cid:124) (cid:123)
dx(t, u)
du

R(t)
(cid:123)
(cid:122)
(cid:125)(cid:124)
∂f (x(t, u))
,
∂u

+

=

(20)

where J(t) ∈ RD×D, R(t), ˙S(t) ∈ RD×M D (See Ap-
pendix for detailed speciﬁcation). For our nonpara-
metric ODE system the sensitivity equation is fully

4

determined by

J(t) =

∂K(x, Z)
∂x
R(t) = K(x, Z)K(Z, Z)−1.

K(Z, Z)−1u

(21)

(22)

The sensitivity equation provides us with an addi-
tional ODE system which describes the time evolution
of the derivatives with respect to the inducing vectors
S(t). The sensitivities are coupled with the actual
ODE system and, thus, both systems x(t) and S(t)
are concatenated as the new augmented state that is
solved jointly by Equation (2) driven by the diﬀer-
entials ˙x(t) and ˙S(t) (Leis and Kramer, 1988). The
initial sensitivities are computed as S(0) = dx0
du . In our
implementation, we merge x0 with u for sensitivity
analysis to obtain the partial derivatives with respect
to the initial state which is estimated along with the
other parameters. We use the cvodes solver from the
Sundials package (Hindmarsh et al., 2005) to solve
the nonparametric ODE models and the correspond-
ing gradients numerically. The sensitivity equation
based approach is superior to the ﬁnite diﬀerences
approximation because we have exact formulation for
the gradients of state over inducing points, which can
be solved up to the numerical accuracy of the ODE
solver.

4 Simple simulated dynamics

As ﬁrst illustration of the proposed nonparametric
ODE method we consider three simulated diﬀerential
systems: the Van der Pol (VDP), FitzHugh-Nagumo
(FHN) and Lotka-Volterra (LV) oscillators of form

vdp :

˙x1 = x2

˙x2 = (1 − x2

1)x2 − x1

fhn :

lv :

˙x1 = 3(x1 −

+ x2)

˙x2 =

x3
1
3

0.2 − 3x1 − 0.2x2
3

˙x1 = 1.5x1 − x1x2

˙x2 = −3x2 + x1x2.

In the conventional ODE case the coeﬃcients of these
equations can be inferred using standard statistical
techniques if suﬃcient amount of time series data is
available (Girolami, 2008; Raue et al., 2013). Our
main goal is to infer unknown dynamics, that is, when
these equations are unavailable and we instead repre-
sent the dynamics with a nonparametric vector ﬁeld
of Equation (8). We use these simulated models to
only illustrate our model behavior against the true
dynamics.

We employ 25 data points from one cycle of noisy
observation data from VDP and FHN models, and 25

data points from 1.7 cycles from the LV model with
noise variance of σ2
n = 0.12. We learn the npODE
model with these training data using M = 52 induc-
ing locations on a ﬁxed grid, and forecast between
4 and 8 future cycles starting from true initial state
x0 at time 0. Figure 2 (bottom) shows the training
datasets (grey regions), initial states, true trajectories
(black lines) and the forecasted trajectory likelihoods
(colored regions). The model accurately learns the
dynamics from less than two cycles of data and can
reproduce them reliably into future.

Figure 2 (top) shows the corresponding true vector
ﬁeld (black arrows) and the estimated vector ﬁeld
(grey arrows). The vector ﬁeld is a continuous func-
tion, which is plotted on a 8x8 grid for visualisation.
In general the most diﬃcult part of the system is learn-
ing the middle of the loop (as seen in the FHN model),
and learning the most outermost regions (bottom left
in the LV model). The model learns the underlying
diﬀerential f (x) accurately close to observed points,
while making only few errors in the border regions
with no data.

5 Unknown system estimation

Next, we illustrate how the model estimates real-
istic, unknown dynamics from noisy observations
y(t1), . . . , y(tN ). As in Section 4, we make no as-
sumptions on the structure or form of the underlying
system, and capture the underlying dynamics with
the nonparameteric system alone. We employ no sub-
jective priors, and assume no inputs, controls or other
sources of information. The task is to infer the under-
lying dynamics f (x), and interpolate or extrapolate
the state trajectory outside the observed data.

We use a benchmark dataset of human motion cap-
ture data from the Carnegie Mellon University motion
capture (CMU mocap) database. Our dataset con-
tains 50-dimensional pose measurements y(ti) from
humans walking, where each pose dimension records
a measurement in diﬀerent parts of the body during

Table 1: Means and standard deviations of RMSEs of
43 datasets in forecasting and ﬁlling experiments.

Model
npODE
GPDM
VGPLVM

Forecasting
4.52 ± 2.31
4.94 ± 3.3
8.74 ± 3.43

Imputation
3.94 ± 3.50
5.31 ± 3.39
3.91 ± 1.80

5

Figure 2: Estimated dynamics from Van der Pol, FitzHugh-Nagumo and Lotka-Volterra systems. The top
part (a-c) shows the learned vector ﬁeld (grey arrows) against the true vector ﬁeld (black arrows). The
bottom part (d-f ) shows the training data (grey region points) and forecasted future cycle likelihoods with
the learned model (shaded region) against the true trajectory (black line).

movement (Wang et al., 2008). We apply the prepro-
cessing of Wang et al. (2008) by downsampling the
datasets by a factor of four and centering the data.
This resulted in a total of 4303 datapoints spread
across 43 trajectories with on average 100 frames per
trajectory. In order to tackle the problem of dimen-
sionality, we project the original dataset with PCA to
a three dimensional latent space where the system is
speciﬁed, following Damianou et al. (2011) and Wang
et al. (2006). We place M = 53 inducing vectors on
a ﬁxed grid, and optimize our model from 100 initial
values, which we select by projecting empirical diﬀer-
ences y(ti) − y(ti−1) to the inducing vectors. We use
an LBFGS optimizer in Matlab. The whole inference
takes approximately few minutes per trajectory.

We evaluate the method with two types of experi-
ments: imputing missing values and forecasting future
cycles. For the forecasting the ﬁrst half of the trajec-
tory is for model training, and the second half is to be
forecasted. For imputation we remove roughly 20% of
the frames from the middle of the trajectory, which
are to be ﬁlled by the models. We perform model se-

lection for lengthscales (cid:96) with cross-validation split of
80/20. We record the root mean square error (RMSE)
over test points in the original feature space in both
cases, where we reconstruct the original dimensions
from the latent space trajectories.

Due to the current lack of ODE methods suitable for
this nonparametric inference task, we instead compare
our method to the state-of-the-art state-space models
where such problems have been previously considered
(Wang et al., 2008). In a state-space or dynamical
model a transition function x(tk+1) = g(x(tk)) moves
the system forward in discrete steps. With suﬃciently
high sampling rate, such models can estimate and fore-
cast ﬁnite approximations of smooth dynamics. In
Gaussian process dynamical model (Wang et al., 2006;
Frigola et al., 2014; Svensson et al., 2016) a GP transi-
tion function is inferred in a latent space, which can be
inferred with a standard GPLVM (Lawrence, 2004) or
with a dependent GPLVM (Zhao and Sun, 2016). In
dynamical systems the transition function is replaced
by a GP interpolation (Damianou et al., 2011). The
discrete time state-space models emphasize inference

6

Figure 3: Forecasting 50 future frames after 49 frames of training data of human motion dataset 35 12.amc.
(a) The estimated locations of the trajectory in a latent space (black points) and future forecast (colored
lines). (b) The original features reconstructed from the latent predictions with grey region showing the
training data.

of a low-dimensional manifold as an explanation of
the high-dimensional measurement trajectories.

We compare our method to the dynamical model
GPDM of Wang et al. (2006) and to the dynamical
system VGPLVM of Damianou et al. (2011), where we
directly apply the implementations provided by the
authors at inverseprobability.com/vargplvm and
dgp.toronto.edu/~jmwang/gpdm. Both methods op-
timize their latent spaces separately, and they are
thus not directly comparable.

5.1 Forecasting

In the forecasting task we train all models with the
ﬁrst half of the trajectory, while forecasting the sec-
ond half starting from the ﬁrst frame. The models
are trained and forecasted within a low-dimensional

space, and subsequently projected back into the orig-
inal space via inverting the PCA or with GPLVM
mean predictions. As all methods optimize their la-
tent spaces separately, they are not directly compa-
rable. Thus, the mean errors are computed in the
original high-dimensional space. Note that the low-
dimensional representation necessarily causes some
reconstruction errors.

Figure 3 illustrates the models on one of the trajec-
tories 35 12.amc. The top part (a) shows the training
data in the PCA space for npODE, and optimized
training data representation for GPDM and VGPLVM
(black points). The colored lines (npODE) and points
(GPDM, VGPLVM) indicate the future forecast. The
bottom part (b) shows the ﬁrst 9 reconstructed orig-
inal pose dimensions reconstructed from the latent
forecasted trajectories. The training data is shown

7

Figure 4: Imputation of 17 missing frames in the middle of a 94-length trajectory of human motion dataset
07 07.amc (subsampled every fourth frame). (a) The estimated locations of the missing points in the latent
space are colored. (b) The original features reconstructed from the latent trajectory.

in gray background, while test data is shown with
circles.

The VGPLVM has most trouble forecasting future
points, and reverts quickly after training data to a
value close to zero, failing to predict future points.
The GPDM model produces more realistic trajectories,
but fails to predict any of the poses accurately. Finally,
npODE can accurately predict ﬁve poses, and still
retains adequate performance on remaining poses,
except for pose 2.

Furthermore, Table 1 indicates that npODE is also
best performing method on average over the whole
dataset in the forecasting.

5.2 Imputation

In the imputation task we remove approximately 20%
of the training data from the middle of the trajectory.

The goal is to learn a model with the remaining data
and forecast the missing values. Figure 4 highlights
the performance of the three models on the trajectory
07 07.amc. The top part (a) shows the training data
(black points) in the PCA space (npODE) or optimized
training locations in the latent space (GPDM, VG-
PLVM). The middle part imputation is shown with
colored points or lines. Interestingly both npODE
and GPDM operate on cyclic representations, while
VGPLVM is not cyclic.

The bottom panel (b) shows the ﬁrst 9 recon-
structed pose dimensions from the three models. The
missing values are shown in circles, while training
points are shown with black dots. All models can
accurately reproduce the overall trends, while npODE
seems to ﬁt slightly worse than the other methods.
The PCA projection causes the seemingly perfect ﬁt of
the npODE prediction (at the top) to lead to slightly

8

warped reconstructions (at the bottom). All methods
mostly ﬁt the missing parts as well. Table 1 shows
that on average the npODE and VGPLVM have ap-
proximately equal top performance on the imputing
missing values task.

6 Discussion

We proposed the framework of nonparametric ODE
model that can accurately learn arbitrary, nonlinear
continuos-time dynamics from purely observational
data without making assumptions of the underlying
system dynamics. We demonstrated that the model
excels at learning dynamics that can be forecasted
into the future. We consider this work as the ﬁrst
in a line of studies of nonparametric ODE systems,
and foresee several aspects as future work. Currently
we do not handle non-stationary vector ﬁelds, that is
time-dependent diﬀerentials ft(x). Furthermore, an
interesting future avenue is the study of various vector
ﬁeld kernels, such as divergence-free, curl-free or spec-
tral kernels (Remes et al., 2017). Finally, including
inputs or controls to the system would allow precise
modelling in interactive settings, such as robotics.

The proposed nonparametric ODE model operates
along a continuous-time trajectory, while dynamic
models such as hidden Markov models or state-space
models are restricted to discrete time steps. These
models are unable to consider system state at arbitrary
times, for instance, between two successive timepoints.

Conventional ODE models have also been consid-
ered from the stochastic perspective with stochastic
diﬀerential equation (SDE) models that commonly
model the system drift and diﬀusion processes sepa-
rately leading to a distribution of trajectories p(x(t)).
As future work we will consider stochastic extensions
of our nonparametric ODE model, as well as MCMC
sampling of the inducing point posterior p(U |Y ), lead-
ing to trajectory distribution as well.

Acknowledgements. The data used in this project
was obtained from mocap.cs.cmu.edu. The database
was created with funding from NSF EIA-0196217.
This work has been supported by the Academy of
Finland Center of Excellence in Systems Immunol-
ogy and Physiology, the Academy of Finland grants
no. 260403, 299915, 275537, 311584.

9

References

Tarmo ¨Aij¨o, Kirsi Granberg, and Harri L¨ahdesm¨aki.
Sorad: a systems biology approach to predict
and modulate dynamic signaling pathway response
from phosphoproteome time-course measurements.
Bioinformatics, 29(10):1283–1291, 2013.

M. Alvarez, L. Rosasco, and N. Lawrence. Kernels
for vector-valued functions: A review. Foundations
and Trends in Machine Learning, 2012.

H. Bhatia, G. Norgard, V. Pascucci, and P. Bremer.
The helmholtz-hodge decomposition -— a survey.
IEEE Transactions on visualization and computer
graphics, 2013.

J. Butcher. Numerical methods for ordinary diﬀeren-

tial equations. John Wiley & Sons, 2016.

B. Calderhead, M. Girolami, and N. Lawrence. Accel-
erating bayesian inference over nonlinear diﬀerential
equations with gaussian processes. NIPS, 2008.

Andreas Damianou, Michalis K Titsias, and Neil D
Lawrence. Variational gaussian process dynami-
cal systems. In Advances in Neural Information
Processing Systems, pages 2510–2518, 2011.

Michiel JL De Hoon, Seiya Imoto, Kazuo Kobayashi,
Naotake Ogasawara, and Satoru Miyano. Inferring
gene regulatory networks from time-ordered gene
expression data of bacillus subtilis using diﬀeren-
tial equations. In Biocomputing 2003, pages 17–28.
World Scientiﬁc, 2002.

F. Dondelinger, M. Filippone, S Rogers, and D. Hus-
meier. Ode parameter inference using adaptive
gradient matching with gaussian processes. JMLR,
2013.

S. P. Ellner, Y. Seifu, and R. H. Smith. Fitting
population dynamic models to time-series data by
gradient matching. Ecology, 83(8):2256–2270, 2002.

Roger Frigola, Yutian Chen, and Carl Edward Ras-
mussen. Variational gaussian process state-space
models. In Advances in Neural Information Pro-
cessing Systems, pages 3680–3688, 2014.

Fabian Fr¨ohlich, Barbara Kaltenbacher, Fabian J.
Theis, and Jan Hasenauer. Scalable parameter esti-
mation for genome-scale biochemical reaction net-
works. PLOS Computational Biology, 13(1):1–18,
01 2017. doi: 10.1371/journal.pcbi.1005331.

E. Fuselier and G. Wright. A radial basis function
method for computing helmholtz–hodge decompo-
sitions. IMA Journal of Numerical Analysis, 2017.

Pei Gao, Antti Honkela, Magnus Rattray, and Neil D
Lawrence. Gaussian process modelling of latent
chemical species: applications to inferring tran-
scription factor activities. Bioinformatics, 24(16):
i70–i75, 2008.

Mark Girolami. Bayesian inference for diﬀerential
equations. Theor. Comput. Sci., 408(1):4–16, 2008.

T. Graepel. Solving noisy linear operator equations
by gaussian processes: Application to ordinary and
partial diﬀerential equations. ICML, 2003.

M. Heinonen and F. d’Alche Buc. Learning nonpara-
metric diﬀerential equations with operator-valued
kernels and gradient matching. arxiv, Telecom Paris-
Tech, 2014.

M. Heinonen, H. Mannerstr¨om, J. Rousu, S. Kaski,
and H. L¨ahdesm¨aki. Non-stationary Gaussian pro-
cess regression with Hamiltonian Monte Carlo. In
AISTATS, volume 51, pages 732–740, 2016.

J. Henderson and G. Michailidis. Network recon-
struction using nonparametric additive ode models.
PLOS ONE, 2014.

Alan C Hindmarsh, Peter N Brown, Keith E Grant,
Steven L Lee, Radu Serban, Dan E Shumaker, and
Carol S Woodward. SUNDIALS: Suite of nonlinear
and diﬀerential/algebraic equation solvers. ACM
Trans. Math. Softw., 31(3):363–396, 2005.

M. Hirsch, S. Smale, and Devaney. Diﬀerential Equa-
tions, Dynamical Systems, and an Introduction to
Chaos (Edition: 2). Elsevier Science & Technology
Books, 2004.

P Kokotovic and J Heller. Direct and adjoint sensi-
tivity equations for parameter optimization. IEEE
Transactions on Automatic Control, 12(5):609–610,
1967.

Malte Kuss and Carl Edward Rasmussen. Assessing
approximate inference for binary gaussian process
classiﬁcation. Journal of machine learning research,
6(Oct):1679–1704, 2005.

Jorge R. Leis and Mark A. Kramer. The simulta-
neous solution and sensitivity analysis of systems
described by ordinary diﬀerential equations. ACM
Trans. Math. Softw., 14(1), 1988.

Benn Macdonald. Statistical inference for ordinary
diﬀerential equations using gradient matching. PhD
thesis, University of Glasgow, 2017.

Benn Macdonald, Catherine Higham, and Dirk Hus-
meier. Controversy in mechanistic modelling with
gaussian processes. In International Conference on
Machine Learning, pages 1539–1547, 2015.

Francis J Narcowich and Joseph D Ward. Generalized
hermite interpolation via matrix-valued condition-
ally positive deﬁnite functions. Mathematics of
Computation, 63(208):661–687, 1994.

Joaquin Qui˜nonero-Candela and Carl Edward Ras-
mussen. A unifying view of sparse approximate
gaussian process regression. Journal of Machine
Learning Research, 6(Dec):1939–1959, 2005.

Maziar Raissi, Paris Perdikaris, and George Em Karni-
adakis. Inferring solutions of diﬀerential equations
using noisy multi-ﬁdelity data. Journal of Compu-
tational Physics, 335:736–746, 2017.

J. Ramsay, G. Hooker, D. Campbell, and J. Cao. Pa-
rameter estimation for diﬀerential equations: a gen-
eralized smoothing approach. Journal of the Royal
Statistical Society: Series B, 69:741–796, 2007.

C.E. Rasmussen and K.I. Williams. Gaussian pro-

cesses for machine learning. MIT Press, 2006.

Andreas Raue, Marcel Schilling, Julie Bachmann, An-
drew Matteson, Max Schelker, Daniel Kaschek,
Sabine Hug, Clemens Kreutz, Brian D. Harms,
Fabian J. Theis, Ursula Klingm¨uller, and Jens Tim-
mer. Lessons learned from quantitative dynamical
modeling in systems biology. PLOS ONE, 8(9):
1–17, 2013.

S. Remes, M. Heinonen, and S. Kaski. Non-stationary

spectral kernels. NIPS, 2017.

John Shawe-Taylor and Nello Cristianini. Kernel
methods for pattern analysis. Cambridge university
press, 2004.

Neil D Lawrence. Gaussian process latent variable
models for visualisation of high dimensional data. In
Advances in neural information processing systems,
pages 329–336, 2004.

A. Solin, M. Kok, N. Wahlstrom, T. Schon, and
S. S¨arkk¨a. Modeling and interpolation of the am-
bient magnetic ﬁeld by gaussian processes. arXiv,
2015. arXiv:1509.04634.

10

Andreas Svensson, Arno Solin, Simo S¨arkk¨a, and
Thomas Sch¨on. Computationally eﬃcient bayesian
learning of gaussian process state space models. In
Artiﬁcial Intelligence and Statistics, pages 213–221,
2016.

J. M. Varah. A spline least squares method for numer-
ical parameter estimation in diﬀerential equations.
SIAM J.sci. Stat. Comput., 3(1):28–46, 1982.

N. Wahlstr¨om, M. Kok, and T. Sch¨on. Modeling
magnetic ﬁelds using gaussian processes.
IEEE
conf on Acoustics, Speech and Signal Processing
(ICASSP), 2013.

Jack Wang, Aaron Hertzmann, and David M Blei.
Gaussian process dynamical models. In Advances in
neural information processing systems, pages 1441–
1448, 2006.

Jack M Wang, David J Fleet, and Aaron Hertzmann.
Gaussian process dynamical models for human mo-
tion. IEEE transactions on pattern analysis and
machine intelligence, 30(2):283–298, 2008.

Y. Wang and D. Barber. Gaussian processes for
bayesian estimation in ordinary diﬀerential equa-
tions. ICML, 2014.

A. Wilson, E. Gilboa, A. Nehorai, and J. Cunningham.
Fast multidimensional pattern extrapolation with
gaussian processes. AISTATS, 2013.

Jing Zhao and Shiliang Sun. Variational dependent
multi-output gaussian process dynamical systems.
The Journal of Machine Learning Research, 17(1):
4134–4169, 2016.

T ¨Aij¨o and H. L¨ahdesm¨aki. Learning gene regulatory
networks from gene expression measurements using
non-parametric molecular kinetics. Bioinformatics,
25:2937––2944, 2009.

11

Appendix of ‘Learning unknown ODE models with Gaussian pro-
cesses’

Sensitivity Equations

In the main text, the sensitivity equation is formulated using matrix notation

˙S(t) = J(t)S(t) + R(t).

Here, the time-dependent matrices are obtained by diﬀerentiating the vector valued functions with respect to
vectors i.e.



































S(t) =

J(t) =

R(t) =

dx1(t,U )
du1

dx2(t,U )
du1

· · ·
· · ·
dxD(t,U )
du1

dx1(t,U )
du2

dx2(t,U )
du2

· · ·
· · ·
dxD(t,U )
du2

· · ·

· · ·

· · ·
· · ·
· · ·

dx1(t,U )
duMD

dx2(t,U )
duMD

· · ·
· · ·
dxD(t,U )
duMD













∂f (x(t),U )1
∂x1

∂f (x(t),U )2
∂x1

· · ·
· · ·
∂f (x(t),U )D
∂x1

∂f (x(t),U )1
∂u1

∂f (x(t),U )2
∂u1

· · ·
· · ·
∂f (x(t),U )D
∂u1

∂f (x(t),U )1
∂x2

∂f (x(t),U )2
∂x2

· · ·
· · ·
∂f (x(t),U )D
∂x2

∂f (x(t),U )1
∂u2

∂f (x(t),U )2
∂u2

· · ·
· · ·
∂f (x(t),U )D
∂u2

· · ·

· · ·

· · ·
· · ·
· · ·

· · ·

· · ·

· · ·
· · ·
· · ·

D×M D
∂f (x(t),U )1
∂xD

∂f (x(t),U )2
∂xD

· · ·
· · ·
∂f (x(t),U )D
∂xD

∂f (x(t),U )1
∂uMD

∂f (x(t),U )2
∂uMD

· · ·
· · ·
∂f (x(t),U )D
∂uMD























D×D

D×M D

Optimization

Below is the explicit form of the log posterior. Note that we introduce u = vec(U ) and Ω = diag(ω2
for notational simplicity.

1, . . . , ω2

D)

log L = log p(U |θ) + log p(Y |x0, U, ω)

= log N (u|0, Kθ(Z, Z)) +

log N (yi|x(ti, U ), Ω)

N
(cid:88)

i=1

1
2

1
2

1
2

1
2

1
2

1
2

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

(yij − xj(ti, U, x0))2
ω2
j

−

N
(cid:88)

i=1

1
2

(yi,j − xj(ti, U, x0))2
ω2
j

D
(cid:88)

j=1

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

− N

log ωj

(30)

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

log |Ω|

(29)

Our goal is to compute the gradients with respect to the initial state x0, latent vector ﬁeld (cid:101)U , kernel
parameters θ and noise variables ω. As explained in the paper, we compute the gradient of the posterior with

12

(23)

(24)

(25)

(26)

(27)

(28)

respect to inducing vectors U and project them to the white domain thanks to noncentral parameterisation.
The analytical forms of the partial derivatives are as follows:

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂uk

− Kθ(Z, Z)−1u

∂ log L
∂uk

∂ log L
∂(x0)d

∂ log L
∂ωj

=

=

=

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

1
ω3
j

N
(cid:88)

i=1

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂(x0)d

(yi,j − xj(ti, U, x0))2 −

N
ωj

Seemingly hard to compute terms, ∂xj (ti,U,x0)

, are computed using sensitivities. The
lengthscale parameter (cid:96) is considered as a model complexity parameter and is chosen from a grid using
cross-validation. We furthermore need the gradient with respect to the other kernel variable, i.e., the signal
variance σ2
f . Because Kθ(Z, Z) and x(ti, U ) are the functions of kernel, computing the gradients with respect
to σ2

f is not trivial and we make use of ﬁnite diﬀerences:

and ∂xj (ti,U,x0)

∂(x0)d

∂uk

We use δ = 10−4 to compute the ﬁnite diﬀerences.

One problem of using gradient-based optimization techniques is that they do not ensure the positivity of
the parameters being optimized. Therefore, we perform the optimization of the noise standard deviations
ω = (ω1, . . . , ωD) and signal variance σf with respect to their logarithms:

∂ log L
∂σf

=

log L(σf + δ) − log L(σf )
δ

∂ log L
∂ log c

=

∂ log L
∂c

∂c
∂ log c

=

∂ log L
∂c

c

where c ∈ (σf , ω1, . . . , ωD).

Implementation details

We initialise the inducing vectors U = (u1, . . . uM ) by computing the empirical gradients ˙yi = yi − yi−1, and
conditioning as

U0 = K(Z, Y )K(Y, Y )−1c ˙y,

where we optimize the scale c against the posterior. The whitened inducing vector is obtained as (cid:101)U0 = L−1
θ U0.
This procedure produces initial vector ﬁelds that partially match the trajectory already. We then do 100
restarts of the optimization from random perturbations (cid:101)U = (cid:101)U0 + ε.

We use LBFGS gradient optimization routine in Matlab. We initialise the inducing vector locations Z on a
equidistant ﬁxed grid on a box containing the observed points. We select the lengthscales (cid:96)1, . . . , (cid:96)D using
cross-validation from values {0.5, 0.75, 1, 1.25, 1.5}. In general large lengthscales induce smoother models,
while lower lengthscales cause overﬁtting.

(31)

(32)

(33)

(34)

(35)

(36)

13

8
1
0
2
 
r
a

M
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
0
3
4
0
.
3
0
8
1
:
v
i
X
r
a

Learning unknown ODE models with Gaussian processes

Markus Heinonen1,2,∗

Cagatay Yildiz1,∗

Henrik Mannerstr¨om1

Jukka Intosalmi1

Harri L¨ahdesm¨aki1

1 Department of Computer Science, Aalto university
2 Helsinki Institute for Information Technology HIIT

Abstract

In conventional ODE modelling coeﬃcients of an equa-
tion driving the system state forward in time are es-
timated. However, for many complex systems it is
practically impossible to determine the equations or
interactions governing the underlying dynamics. In
these settings, parametric ODE model cannot be for-
mulated. Here, we overcome this issue by introducing
a novel paradigm of nonparametric ODE modeling
that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We
propose to learn non-linear, unknown diﬀerential func-
tions from state observations using Gaussian process
vector ﬁelds within the exact ODE formalism. We
demonstrate the model’s capabilities to infer dynamics
from sparse data and to simulate the system forward
into future.

1

Introduction

Dynamical systems modeling is a cornerstone of ex-
perimental sciences. In biology, as well as in physics
and chemistry, modelers attempt to capture the dy-
namical behavior of a given system or a phenomenon
in order to improve its understanding and make pre-
dictions about its future state. Systems of coupled
ordinary diﬀerential equations (ODEs) are undoubt-
edly the most widely used models in science. Even
simple ODE functions can describe complex dynam-
ical behaviours (Hirsch et al., 2004). Typically, the
dynamics are ﬁrmly grounded in physics with only a
few parameters to be estimated from data. However,
equally ubiquitous are the cases where the governing
dynamics are partially or completely unknown.

We consider the dynamics of a system governed by

∗Equal contribution

multivariate ordinary diﬀerential functions:

˙x(t) =

= f (x(t))

(1)

dx(t)
dt

where x(t) ∈ X = RD is the state vector of a D-
dimensional dynamical system at time t, and the
˙x(t) ∈ ˙X = RD is the ﬁrst order time derivative of
x(t) that drives the state x(t) forward, and where
f : RD → RD is the vector-valued derivative function.
The ODE solution is determined by

x(t) = x0 +

f (x(τ ))dτ,

(2)

(cid:90) t

0

where we integrate the system state from an initial
state x(0) = x0 for time t forward. We assume that
f (·) is completely unknown and we only observe one or
several multivariate time series Y = (y1, . . . , yN )T ∈
RN ×D obtained from an additive noisy observation
model at observation time points T = (t1, . . . , tN ) ∈
RN ,

y(t) = x(t) + εt,

(3)

where εt ∼ N (0, Ω) follows a stationary zero-mean
multivariate Gaussian distribution with diagonal noise
variances Ω = diag(ω2
1, . . . , ω2
D). The observation
time points do not need to be equally spaced. Our
task is to learn the diﬀerential function f (·) given
observations Y , with no prior knowledge of the ODE
system.

There is a vast literature on conventional ODEs
(Butcher, 2016) where a parametric form for function
f (x; θ, t) is assumed to be known, and its parameters
θ are subsequently optimised with least squares or
Bayesian approach, where the expensive forward solu-
tion xθ(ti) = (cid:82) ti
0 f (x(τ ); θ, t)dτ is required to evaluate
the system responses xθ(ti) from parameters θ against
observations y(ti). To overcome the computationally
intensive forward solution, a family of methods de-
noted as gradient matching (Varah, 1982; Ellner et al.,

1

2002; Ramsay et al., 2007) have proposed to replace
the forward solution by matching f (yi) ≈ ˙yi to em-
pirical gradients ˙yi of the data instead, which do not
require the costly integration step. Recently several
authors have proposed embedding a parametric diﬀer-
ential function within a Bayesian or Gaussian process
(GP) framework (Graepel, 2003; Calderhead et al.,
2008; Dondelinger et al., 2013; Wang and Barber,
2014; Macdonald, 2017) (see Macdonald et al. (2015)
for a review). GPs have been successfully applied
to model linear diﬀerential equations as they are an-
alytically tractable (Gao et al., 2008; Raissi et al.,
2017).

However, conventional ODE modeling can only pro-
ceed if a parametric form of the driving function f (·)
is known. Recently, initial work to handle unknown
or non-parametric ODE models have been proposed,
although with various limiting approximations. Early
works include spline-based smoothing and additive
functions (cid:80)D
j fj(xj) to infer gene regulatory networks
(De Hoon et al., 2002; Henderson and Michailidis,
2014). ¨Aij¨o and L¨ahdesm¨aki (2009) proposed estimat-
ing the unknown nonlinear function with GPs using
either ﬁnite time diﬀerences, or analytically solving
the derivative function as a function of only time,
˙x(t) = f (t) ( ¨Aij¨o et al., 2013). In a seminal techni-
cal report of Heinonen and d’Alche Buc (2014) a full
vector-valued kernel model f (x) was proposed, how-
ever using a gradient matching approximation. To our
knowledge, there exists no model that can learn non-
linear ODE functions ˙x(t) = f (x(t)) over the state x
against the true forward solutions x(ti).

In this work we propose npODE: the ﬁrst ODE
model for learning arbitrary, and a priori completely
unknown non-parametric, non-linear diﬀerential func-
tions f : X → ˙X from data in a Bayesian way. We
do not use gradient matching or other approxima-
tive models, but instead propose to directly optimise
the exact ODE system with the fully forward simu-
lated responses against data. We parameterise our
model as an augmented Gaussian process vector ﬁeld
with inducing points, while we propose sensitivity
equations to eﬃciently compute the gradients of the
system. Our model can forecast continuous-time sys-
tems arbitrary amounts to future, and we demonstrate
the state-of-the-art performance in human motion
datasets. The MATLAB implementation is publicly
available at github.com/cagatayyildiz/npode.

2 Nonparametric ODE model

The diﬀerential function f (x) to be learned deﬁnes a
vector ﬁeld 1 f , that is, an assignment of a gradient
vector f (x) ∈ RD to every state x ∈ RD. We model
the vector ﬁeld as a vector-valued Gaussian process
(GP) (Rasmussen and Williams, 2006)

f (x) ∼ GP(0, K(x, x(cid:48))),

which deﬁnes a priori distribution over function values
f (x) whose mean and covariances are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)),

(4)

(5)

(6)

and where the kernel K(x, x(cid:48)) ∈ RD×D is matrix-
valued. A GP prior deﬁnes that for any collection
of states X = (x1, . . . , xN )T ∈ RN ×D, the function
values F = (f (x1), . . . , f (xN ))T ∈ RN ×D follow a
matrix-valued normal distribution,

p(F ) = N (vec(F )|0, K(X, X)),

(7)

i,j=1 ∈ RN D×N D is a
where K(X, X) = (K(xi, xj))N
block matrix of matrix-valued kernels K(xi, xj). The
key property of Gaussian processes is that they encode
functions where similar states x, x(cid:48) induce similar
diﬀerentials f (x), f (x(cid:48)), and where the state similarity
is deﬁned by the kernel K(x, x(cid:48)).

In standard GP regression we would obtain poste-
rior of the vector ﬁeld by conditioning the GP prior
with the data (Rasmussen and Williams, 2006). In
ODE models the conditional f (x)|Y of a vector ﬁeld
is intractable due to the integral mapping (2) between
observed states y(ti) and diﬀerentials f (x). Instead,
we resort to augmenting the Gaussian process with a
set of M inducing points z ∈ X and u ∈ ˙X , such that
f (z) = u (Qui˜nonero-Candela and Rasmussen, 2005).
We choose to interpolate the diﬀerential function be-
tween the inducing points as (See Figure 1)

f (x) (cid:44) Kθ(x, Z)Kθ(Z, Z)−1vec(U ),

(8)

which supports the function f (x) with inducing lo-
cations Z = (z1, . . . , zM ), inducing vectors U =
(u1, . . . , uM ), and θ are the kernel parameters. The
function above corresponds to a vector-valued kernel
function (Alvarez et al., 2012), or to a multi-task Gaus-
sian process conditional mean without the variance
term (Rasmussen and Williams, 2006). This deﬁnition

1We use vector ﬁeld and diﬀerential function interchange-

ably.

2

kernel

Kθ(z, z(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(zj − z(cid:48)
(cid:96)2
j



 (9)

with diﬀerential variance σ2
f and dimension-speciﬁc
lengthscales (cid:96) = ((cid:96)1, . . . , (cid:96)D) is expanded into a di-
agonal matrix of size D × D. We collect the kernel
parameters as θ = (σf , (cid:96)).

We note that more complex kernels can also be
considered given prior information of the underlying
system characteristics. The divergence-free matrix-
valued kernel induces vector ﬁelds that have zero di-
vergence (Wahlstr¨om et al., 2013; Solin et al., 2015).
Intuitively, these vector ﬁelds do not have sinks or
sources, and every state always ﬁnally returns to itself
after suﬃcient amount of time. Similarly, curl-free
kernels induce curl-free vector ﬁelds that can contain
sources or sinks, that is, trajectories can accelerate or
decelerate. For theoretical treatment of vector ﬁeld
kernels, see (Narcowich and Ward, 1994; Bhatia et al.,
2013; Fuselier and Wright, 2017). Non-stationary
vector ﬁelds can be modeled with input-dependent
lengthscales (Heinonen et al., 2016), while spectral ker-
nels can represent stationary (Wilson et al., 2013) or
non-stationary (Remes et al., 2017) recurring patterns
in the diﬀerential function.

2.2 Joint model

We assume a Gaussian likelihood over the observations
yi and the corresponding simulated responses x(ti) of
Equation (2),

p(Y |x0, U, Z, ω) =

N (yi|x(ti), Ω),

(10)

N
(cid:89)

i=1

where x(ti) are forward simulated responses using the
integral equation (2) and diﬀerential equation (8), and
Ω = diag(ω2
D) collects the dimension-speciﬁc
noise variances.

1 . . . , ω2

The inducing vectors have a Gaussian process prior

p(U |Z, θ) = N (vec(U )|0, Kθ(Z, Z)).

(11)

The model posterior is then

p(U, x0, θ, ω|Y ) ∝ p(Y |x0, U, ω)p(U |θ) = L,

(12)

where we have for brevity omitted the dependency on
the locations of the inducing points Z and also the
parameter hyperpriors p(θ) and p(ω) since we assume

Figure 1: (a) Illustration of an ODE system vector
ﬁeld induced by the Gaussian process. The vector ﬁeld
f (x) (gray arrows) at arbitrary states x is interpolated
from the inducing points u, z (black arrows), with the
trajectory x(t) (red points) following the diﬀerential
system f (x) exactly. (b) The trajectory x(t) plotted
over time t.

is then compatible with the deterministic nature of
the ODE formalism. Due to universality of several
kernels and kernel functions (Shawe-Taylor and Cris-
tianini, 2004), we can represent arbitrary vector ﬁelds
with appropriate inducing point and kernel choices.

2.1 Operator-valued kernels

The vector-valued kernel function (8) uses operator-
valued kernels, which result in matrix-valued ker-
nels Kθ(z, z(cid:48)) ∈ RD×D for real valued states x, z,
while the kernel matrix over data points becomes
i,j=1 ∈ RM D×M D (See Alvarez et al.
Kθ = (K(zi, zj))M
(2012) for a review). Most straightforward operator-
valued kernel is the identity decomposable kernel
Kdec(z, z(cid:48)) = k(z, z(cid:48)) · ID, where the scalar Gaussian

3

them to be uniform, unless there is speciﬁc domain
knowledge of the priors.

The model parameters are the initial state x0

2, the
inducing vectors U , the noise standard deviations
ω = (ω1, . . . , ωD), and the kernel hyperparameters
θ = (σf , (cid:96)1, . . . , (cid:96)D).

2.3 Noncentral parameterisation

We apply a latent parameterisation using Cholesky
decomposition LθLT
θ = Kθ(Z, Z), which maps the
inducing vectors to whitened domain (Kuss and Ras-
mussen, 2005)

U = Lθ (cid:101)U ,

(cid:101)U = L−1

θ U.

(13)

The latent variables (cid:101)U are projected on the kernel
manifold Lθ to obtain the inducing vectors U . This
non-centered parameterisation (NCP) transforms the
hierarchical posterior L of Equation (12) into a repa-
rameterised form

p(x0, (cid:101)U , θ, ω|Y ) ∝ p(Y |x0, (cid:101)U , ω, θ)p( (cid:101)U ),

(14)

where all variables to be optimised are decoupled, with
the latent inducing vectors having a standard normal
prior (cid:101)U ∼ N (0, I). Optimizing (cid:101)U and θ is now more
eﬃcient since they have independent contributions to
the vector ﬁeld via U = Lθ (cid:101)U . The gradients of the
whitened posterior can be retrieved analytically as
(Heinonen et al., 2016)

∇

(cid:101)U log L = LT

θ ∇U log L.

(15)

Finally, we ﬁnd a MAP estimate for the initial state
x0, latent vector ﬁeld (cid:101)U , kernel parameters θ and
noises ω by gradient ascent,

x0,map, (cid:101)Umap, θmap, ωmap = arg max
x0, (cid:101)U ,θ,ω

log L,

(16)

while keeping the inducing locations Z ﬁxed on a
suﬃciently dense grid (See Figure 1). The partial
derivatives of the posterior with respect to noise pa-
rameters ω can be found analytically, while the deriva-
tives with respect to σf are approximated with ﬁnite
diﬀerences. We select the optimal lengthscales (cid:96) by
cross-validation.

2In case of multiple time-series, we will use one initial state

for each time-series.

3 Sensitivity equations

The key term to carry out the MAP gradient ascent
optimization is the likelihood

log p(Y |x0, (cid:101)U , ω)

that requires forward integration and computing the
partial derivatives with respect to the whitened induc-
ing vectors (cid:101)U . Given Equation (15) we only need to
compute the gradients with respect to the inducing
vectors u = vec(U ) ∈ RM D,

d log p(Y |x0, u, ω)
du

=

N
(cid:88)

s=1

d log N (ys|x(ts, u), Ω)
dx

dx(ts, u)
du

.

(17)

This requires computing the derivatives of the simu-
lated system response x(t, u) against the vector ﬁeld
parameters u,

dx(t, u)
du

≡ S(t) ∈ RD×M D,

(18)

∂uj

which we denote by Sij(t) = ∂x(t,u)i
, and expand
the notation to make the dependency of x on u ex-
plicit. Approximating these with ﬁnite diﬀerences is
possible in principle, but is highly ineﬃcient and has
been reported to cause unstability (Raue et al., 2013).
We instead turn to sensitivity equations for u and
x0 that provide computationally eﬃcient, analytical
gradients S(t) (Kokotovic and Heller, 1967; Fr¨ohlich
et al., 2017).

The solution for dx(t,u)

can be derived by diﬀer-
entiating the full nonparametric ODE system with
respect to u by

du

d
du

dx(t, u)
dt

d
du

=

f (x(t, u)).

(19)

The sensitivity equation for the given system can be
obtained by changing the order of diﬀerentiation on
the left hand side and carrying out the diﬀerentia-
tion on the right hand side. The resulting sensitivity
equation can then be expressed in the form

˙S(t)
(cid:125)(cid:124)

(cid:123)
dx(t, u)
du

(cid:122)
d
dt

J(t)
(cid:122)
(cid:123)
(cid:125)(cid:124)
∂f (x(t, u))
∂x

S(t)
(cid:122) (cid:125)(cid:124) (cid:123)
dx(t, u)
du

R(t)
(cid:123)
(cid:122)
(cid:125)(cid:124)
∂f (x(t, u))
,
∂u

+

=

(20)

where J(t) ∈ RD×D, R(t), ˙S(t) ∈ RD×M D (See Ap-
pendix for detailed speciﬁcation). For our nonpara-
metric ODE system the sensitivity equation is fully

4

determined by

J(t) =

∂K(x, Z)
∂x
R(t) = K(x, Z)K(Z, Z)−1.

K(Z, Z)−1u

(21)

(22)

The sensitivity equation provides us with an addi-
tional ODE system which describes the time evolution
of the derivatives with respect to the inducing vectors
S(t). The sensitivities are coupled with the actual
ODE system and, thus, both systems x(t) and S(t)
are concatenated as the new augmented state that is
solved jointly by Equation (2) driven by the diﬀer-
entials ˙x(t) and ˙S(t) (Leis and Kramer, 1988). The
initial sensitivities are computed as S(0) = dx0
du . In our
implementation, we merge x0 with u for sensitivity
analysis to obtain the partial derivatives with respect
to the initial state which is estimated along with the
other parameters. We use the cvodes solver from the
Sundials package (Hindmarsh et al., 2005) to solve
the nonparametric ODE models and the correspond-
ing gradients numerically. The sensitivity equation
based approach is superior to the ﬁnite diﬀerences
approximation because we have exact formulation for
the gradients of state over inducing points, which can
be solved up to the numerical accuracy of the ODE
solver.

4 Simple simulated dynamics

As ﬁrst illustration of the proposed nonparametric
ODE method we consider three simulated diﬀerential
systems: the Van der Pol (VDP), FitzHugh-Nagumo
(FHN) and Lotka-Volterra (LV) oscillators of form

vdp :

˙x1 = x2

˙x2 = (1 − x2

1)x2 − x1

fhn :

lv :

˙x1 = 3(x1 −

+ x2)

˙x2 =

x3
1
3

0.2 − 3x1 − 0.2x2
3

˙x1 = 1.5x1 − x1x2

˙x2 = −3x2 + x1x2.

In the conventional ODE case the coeﬃcients of these
equations can be inferred using standard statistical
techniques if suﬃcient amount of time series data is
available (Girolami, 2008; Raue et al., 2013). Our
main goal is to infer unknown dynamics, that is, when
these equations are unavailable and we instead repre-
sent the dynamics with a nonparametric vector ﬁeld
of Equation (8). We use these simulated models to
only illustrate our model behavior against the true
dynamics.

We employ 25 data points from one cycle of noisy
observation data from VDP and FHN models, and 25

data points from 1.7 cycles from the LV model with
noise variance of σ2
n = 0.12. We learn the npODE
model with these training data using M = 52 induc-
ing locations on a ﬁxed grid, and forecast between
4 and 8 future cycles starting from true initial state
x0 at time 0. Figure 2 (bottom) shows the training
datasets (grey regions), initial states, true trajectories
(black lines) and the forecasted trajectory likelihoods
(colored regions). The model accurately learns the
dynamics from less than two cycles of data and can
reproduce them reliably into future.

Figure 2 (top) shows the corresponding true vector
ﬁeld (black arrows) and the estimated vector ﬁeld
(grey arrows). The vector ﬁeld is a continuous func-
tion, which is plotted on a 8x8 grid for visualisation.
In general the most diﬃcult part of the system is learn-
ing the middle of the loop (as seen in the FHN model),
and learning the most outermost regions (bottom left
in the LV model). The model learns the underlying
diﬀerential f (x) accurately close to observed points,
while making only few errors in the border regions
with no data.

5 Unknown system estimation

Next, we illustrate how the model estimates real-
istic, unknown dynamics from noisy observations
y(t1), . . . , y(tN ). As in Section 4, we make no as-
sumptions on the structure or form of the underlying
system, and capture the underlying dynamics with
the nonparameteric system alone. We employ no sub-
jective priors, and assume no inputs, controls or other
sources of information. The task is to infer the under-
lying dynamics f (x), and interpolate or extrapolate
the state trajectory outside the observed data.

We use a benchmark dataset of human motion cap-
ture data from the Carnegie Mellon University motion
capture (CMU mocap) database. Our dataset con-
tains 50-dimensional pose measurements y(ti) from
humans walking, where each pose dimension records
a measurement in diﬀerent parts of the body during

Table 1: Means and standard deviations of RMSEs of
43 datasets in forecasting and ﬁlling experiments.

Model
npODE
GPDM
VGPLVM

Forecasting
4.52 ± 2.31
4.94 ± 3.3
8.74 ± 3.43

Imputation
3.94 ± 3.50
5.31 ± 3.39
3.91 ± 1.80

5

Figure 2: Estimated dynamics from Van der Pol, FitzHugh-Nagumo and Lotka-Volterra systems. The top
part (a-c) shows the learned vector ﬁeld (grey arrows) against the true vector ﬁeld (black arrows). The
bottom part (d-f ) shows the training data (grey region points) and forecasted future cycle likelihoods with
the learned model (shaded region) against the true trajectory (black line).

movement (Wang et al., 2008). We apply the prepro-
cessing of Wang et al. (2008) by downsampling the
datasets by a factor of four and centering the data.
This resulted in a total of 4303 datapoints spread
across 43 trajectories with on average 100 frames per
trajectory. In order to tackle the problem of dimen-
sionality, we project the original dataset with PCA to
a three dimensional latent space where the system is
speciﬁed, following Damianou et al. (2011) and Wang
et al. (2006). We place M = 53 inducing vectors on
a ﬁxed grid, and optimize our model from 100 initial
values, which we select by projecting empirical diﬀer-
ences y(ti) − y(ti−1) to the inducing vectors. We use
an LBFGS optimizer in Matlab. The whole inference
takes approximately few minutes per trajectory.

We evaluate the method with two types of experi-
ments: imputing missing values and forecasting future
cycles. For the forecasting the ﬁrst half of the trajec-
tory is for model training, and the second half is to be
forecasted. For imputation we remove roughly 20% of
the frames from the middle of the trajectory, which
are to be ﬁlled by the models. We perform model se-

lection for lengthscales (cid:96) with cross-validation split of
80/20. We record the root mean square error (RMSE)
over test points in the original feature space in both
cases, where we reconstruct the original dimensions
from the latent space trajectories.

Due to the current lack of ODE methods suitable for
this nonparametric inference task, we instead compare
our method to the state-of-the-art state-space models
where such problems have been previously considered
(Wang et al., 2008). In a state-space or dynamical
model a transition function x(tk+1) = g(x(tk)) moves
the system forward in discrete steps. With suﬃciently
high sampling rate, such models can estimate and fore-
cast ﬁnite approximations of smooth dynamics. In
Gaussian process dynamical model (Wang et al., 2006;
Frigola et al., 2014; Svensson et al., 2016) a GP transi-
tion function is inferred in a latent space, which can be
inferred with a standard GPLVM (Lawrence, 2004) or
with a dependent GPLVM (Zhao and Sun, 2016). In
dynamical systems the transition function is replaced
by a GP interpolation (Damianou et al., 2011). The
discrete time state-space models emphasize inference

6

Figure 3: Forecasting 50 future frames after 49 frames of training data of human motion dataset 35 12.amc.
(a) The estimated locations of the trajectory in a latent space (black points) and future forecast (colored
lines). (b) The original features reconstructed from the latent predictions with grey region showing the
training data.

of a low-dimensional manifold as an explanation of
the high-dimensional measurement trajectories.

We compare our method to the dynamical model
GPDM of Wang et al. (2006) and to the dynamical
system VGPLVM of Damianou et al. (2011), where we
directly apply the implementations provided by the
authors at inverseprobability.com/vargplvm and
dgp.toronto.edu/~jmwang/gpdm. Both methods op-
timize their latent spaces separately, and they are
thus not directly comparable.

5.1 Forecasting

In the forecasting task we train all models with the
ﬁrst half of the trajectory, while forecasting the sec-
ond half starting from the ﬁrst frame. The models
are trained and forecasted within a low-dimensional

space, and subsequently projected back into the orig-
inal space via inverting the PCA or with GPLVM
mean predictions. As all methods optimize their la-
tent spaces separately, they are not directly compa-
rable. Thus, the mean errors are computed in the
original high-dimensional space. Note that the low-
dimensional representation necessarily causes some
reconstruction errors.

Figure 3 illustrates the models on one of the trajec-
tories 35 12.amc. The top part (a) shows the training
data in the PCA space for npODE, and optimized
training data representation for GPDM and VGPLVM
(black points). The colored lines (npODE) and points
(GPDM, VGPLVM) indicate the future forecast. The
bottom part (b) shows the ﬁrst 9 reconstructed orig-
inal pose dimensions reconstructed from the latent
forecasted trajectories. The training data is shown

7

Figure 4: Imputation of 17 missing frames in the middle of a 94-length trajectory of human motion dataset
07 07.amc (subsampled every fourth frame). (a) The estimated locations of the missing points in the latent
space are colored. (b) The original features reconstructed from the latent trajectory.

in gray background, while test data is shown with
circles.

The VGPLVM has most trouble forecasting future
points, and reverts quickly after training data to a
value close to zero, failing to predict future points.
The GPDM model produces more realistic trajectories,
but fails to predict any of the poses accurately. Finally,
npODE can accurately predict ﬁve poses, and still
retains adequate performance on remaining poses,
except for pose 2.

Furthermore, Table 1 indicates that npODE is also
best performing method on average over the whole
dataset in the forecasting.

5.2 Imputation

In the imputation task we remove approximately 20%
of the training data from the middle of the trajectory.

The goal is to learn a model with the remaining data
and forecast the missing values. Figure 4 highlights
the performance of the three models on the trajectory
07 07.amc. The top part (a) shows the training data
(black points) in the PCA space (npODE) or optimized
training locations in the latent space (GPDM, VG-
PLVM). The middle part imputation is shown with
colored points or lines. Interestingly both npODE
and GPDM operate on cyclic representations, while
VGPLVM is not cyclic.

The bottom panel (b) shows the ﬁrst 9 recon-
structed pose dimensions from the three models. The
missing values are shown in circles, while training
points are shown with black dots. All models can
accurately reproduce the overall trends, while npODE
seems to ﬁt slightly worse than the other methods.
The PCA projection causes the seemingly perfect ﬁt of
the npODE prediction (at the top) to lead to slightly

8

warped reconstructions (at the bottom). All methods
mostly ﬁt the missing parts as well. Table 1 shows
that on average the npODE and VGPLVM have ap-
proximately equal top performance on the imputing
missing values task.

6 Discussion

We proposed the framework of nonparametric ODE
model that can accurately learn arbitrary, nonlinear
continuos-time dynamics from purely observational
data without making assumptions of the underlying
system dynamics. We demonstrated that the model
excels at learning dynamics that can be forecasted
into the future. We consider this work as the ﬁrst
in a line of studies of nonparametric ODE systems,
and foresee several aspects as future work. Currently
we do not handle non-stationary vector ﬁelds, that is
time-dependent diﬀerentials ft(x). Furthermore, an
interesting future avenue is the study of various vector
ﬁeld kernels, such as divergence-free, curl-free or spec-
tral kernels (Remes et al., 2017). Finally, including
inputs or controls to the system would allow precise
modelling in interactive settings, such as robotics.

The proposed nonparametric ODE model operates
along a continuous-time trajectory, while dynamic
models such as hidden Markov models or state-space
models are restricted to discrete time steps. These
models are unable to consider system state at arbitrary
times, for instance, between two successive timepoints.

Conventional ODE models have also been consid-
ered from the stochastic perspective with stochastic
diﬀerential equation (SDE) models that commonly
model the system drift and diﬀusion processes sepa-
rately leading to a distribution of trajectories p(x(t)).
As future work we will consider stochastic extensions
of our nonparametric ODE model, as well as MCMC
sampling of the inducing point posterior p(U |Y ), lead-
ing to trajectory distribution as well.

Acknowledgements. The data used in this project
was obtained from mocap.cs.cmu.edu. The database
was created with funding from NSF EIA-0196217.
This work has been supported by the Academy of
Finland Center of Excellence in Systems Immunol-
ogy and Physiology, the Academy of Finland grants
no. 260403, 299915, 275537, 311584.

9

References

Tarmo ¨Aij¨o, Kirsi Granberg, and Harri L¨ahdesm¨aki.
Sorad: a systems biology approach to predict
and modulate dynamic signaling pathway response
from phosphoproteome time-course measurements.
Bioinformatics, 29(10):1283–1291, 2013.

M. Alvarez, L. Rosasco, and N. Lawrence. Kernels
for vector-valued functions: A review. Foundations
and Trends in Machine Learning, 2012.

H. Bhatia, G. Norgard, V. Pascucci, and P. Bremer.
The helmholtz-hodge decomposition -— a survey.
IEEE Transactions on visualization and computer
graphics, 2013.

J. Butcher. Numerical methods for ordinary diﬀeren-

tial equations. John Wiley & Sons, 2016.

B. Calderhead, M. Girolami, and N. Lawrence. Accel-
erating bayesian inference over nonlinear diﬀerential
equations with gaussian processes. NIPS, 2008.

Andreas Damianou, Michalis K Titsias, and Neil D
Lawrence. Variational gaussian process dynami-
cal systems. In Advances in Neural Information
Processing Systems, pages 2510–2518, 2011.

Michiel JL De Hoon, Seiya Imoto, Kazuo Kobayashi,
Naotake Ogasawara, and Satoru Miyano. Inferring
gene regulatory networks from time-ordered gene
expression data of bacillus subtilis using diﬀeren-
tial equations. In Biocomputing 2003, pages 17–28.
World Scientiﬁc, 2002.

F. Dondelinger, M. Filippone, S Rogers, and D. Hus-
meier. Ode parameter inference using adaptive
gradient matching with gaussian processes. JMLR,
2013.

S. P. Ellner, Y. Seifu, and R. H. Smith. Fitting
population dynamic models to time-series data by
gradient matching. Ecology, 83(8):2256–2270, 2002.

Roger Frigola, Yutian Chen, and Carl Edward Ras-
mussen. Variational gaussian process state-space
models. In Advances in Neural Information Pro-
cessing Systems, pages 3680–3688, 2014.

Fabian Fr¨ohlich, Barbara Kaltenbacher, Fabian J.
Theis, and Jan Hasenauer. Scalable parameter esti-
mation for genome-scale biochemical reaction net-
works. PLOS Computational Biology, 13(1):1–18,
01 2017. doi: 10.1371/journal.pcbi.1005331.

E. Fuselier and G. Wright. A radial basis function
method for computing helmholtz–hodge decompo-
sitions. IMA Journal of Numerical Analysis, 2017.

Pei Gao, Antti Honkela, Magnus Rattray, and Neil D
Lawrence. Gaussian process modelling of latent
chemical species: applications to inferring tran-
scription factor activities. Bioinformatics, 24(16):
i70–i75, 2008.

Mark Girolami. Bayesian inference for diﬀerential
equations. Theor. Comput. Sci., 408(1):4–16, 2008.

T. Graepel. Solving noisy linear operator equations
by gaussian processes: Application to ordinary and
partial diﬀerential equations. ICML, 2003.

M. Heinonen and F. d’Alche Buc. Learning nonpara-
metric diﬀerential equations with operator-valued
kernels and gradient matching. arxiv, Telecom Paris-
Tech, 2014.

M. Heinonen, H. Mannerstr¨om, J. Rousu, S. Kaski,
and H. L¨ahdesm¨aki. Non-stationary Gaussian pro-
cess regression with Hamiltonian Monte Carlo. In
AISTATS, volume 51, pages 732–740, 2016.

J. Henderson and G. Michailidis. Network recon-
struction using nonparametric additive ode models.
PLOS ONE, 2014.

Alan C Hindmarsh, Peter N Brown, Keith E Grant,
Steven L Lee, Radu Serban, Dan E Shumaker, and
Carol S Woodward. SUNDIALS: Suite of nonlinear
and diﬀerential/algebraic equation solvers. ACM
Trans. Math. Softw., 31(3):363–396, 2005.

M. Hirsch, S. Smale, and Devaney. Diﬀerential Equa-
tions, Dynamical Systems, and an Introduction to
Chaos (Edition: 2). Elsevier Science & Technology
Books, 2004.

P Kokotovic and J Heller. Direct and adjoint sensi-
tivity equations for parameter optimization. IEEE
Transactions on Automatic Control, 12(5):609–610,
1967.

Malte Kuss and Carl Edward Rasmussen. Assessing
approximate inference for binary gaussian process
classiﬁcation. Journal of machine learning research,
6(Oct):1679–1704, 2005.

Jorge R. Leis and Mark A. Kramer. The simulta-
neous solution and sensitivity analysis of systems
described by ordinary diﬀerential equations. ACM
Trans. Math. Softw., 14(1), 1988.

Benn Macdonald. Statistical inference for ordinary
diﬀerential equations using gradient matching. PhD
thesis, University of Glasgow, 2017.

Benn Macdonald, Catherine Higham, and Dirk Hus-
meier. Controversy in mechanistic modelling with
gaussian processes. In International Conference on
Machine Learning, pages 1539–1547, 2015.

Francis J Narcowich and Joseph D Ward. Generalized
hermite interpolation via matrix-valued condition-
ally positive deﬁnite functions. Mathematics of
Computation, 63(208):661–687, 1994.

Joaquin Qui˜nonero-Candela and Carl Edward Ras-
mussen. A unifying view of sparse approximate
gaussian process regression. Journal of Machine
Learning Research, 6(Dec):1939–1959, 2005.

Maziar Raissi, Paris Perdikaris, and George Em Karni-
adakis. Inferring solutions of diﬀerential equations
using noisy multi-ﬁdelity data. Journal of Compu-
tational Physics, 335:736–746, 2017.

J. Ramsay, G. Hooker, D. Campbell, and J. Cao. Pa-
rameter estimation for diﬀerential equations: a gen-
eralized smoothing approach. Journal of the Royal
Statistical Society: Series B, 69:741–796, 2007.

C.E. Rasmussen and K.I. Williams. Gaussian pro-

cesses for machine learning. MIT Press, 2006.

Andreas Raue, Marcel Schilling, Julie Bachmann, An-
drew Matteson, Max Schelker, Daniel Kaschek,
Sabine Hug, Clemens Kreutz, Brian D. Harms,
Fabian J. Theis, Ursula Klingm¨uller, and Jens Tim-
mer. Lessons learned from quantitative dynamical
modeling in systems biology. PLOS ONE, 8(9):
1–17, 2013.

S. Remes, M. Heinonen, and S. Kaski. Non-stationary

spectral kernels. NIPS, 2017.

John Shawe-Taylor and Nello Cristianini. Kernel
methods for pattern analysis. Cambridge university
press, 2004.

Neil D Lawrence. Gaussian process latent variable
models for visualisation of high dimensional data. In
Advances in neural information processing systems,
pages 329–336, 2004.

A. Solin, M. Kok, N. Wahlstrom, T. Schon, and
S. S¨arkk¨a. Modeling and interpolation of the am-
bient magnetic ﬁeld by gaussian processes. arXiv,
2015. arXiv:1509.04634.

10

Andreas Svensson, Arno Solin, Simo S¨arkk¨a, and
Thomas Sch¨on. Computationally eﬃcient bayesian
learning of gaussian process state space models. In
Artiﬁcial Intelligence and Statistics, pages 213–221,
2016.

J. M. Varah. A spline least squares method for numer-
ical parameter estimation in diﬀerential equations.
SIAM J.sci. Stat. Comput., 3(1):28–46, 1982.

N. Wahlstr¨om, M. Kok, and T. Sch¨on. Modeling
magnetic ﬁelds using gaussian processes.
IEEE
conf on Acoustics, Speech and Signal Processing
(ICASSP), 2013.

Jack Wang, Aaron Hertzmann, and David M Blei.
Gaussian process dynamical models. In Advances in
neural information processing systems, pages 1441–
1448, 2006.

Jack M Wang, David J Fleet, and Aaron Hertzmann.
Gaussian process dynamical models for human mo-
tion. IEEE transactions on pattern analysis and
machine intelligence, 30(2):283–298, 2008.

Y. Wang and D. Barber. Gaussian processes for
bayesian estimation in ordinary diﬀerential equa-
tions. ICML, 2014.

A. Wilson, E. Gilboa, A. Nehorai, and J. Cunningham.
Fast multidimensional pattern extrapolation with
gaussian processes. AISTATS, 2013.

Jing Zhao and Shiliang Sun. Variational dependent
multi-output gaussian process dynamical systems.
The Journal of Machine Learning Research, 17(1):
4134–4169, 2016.

T ¨Aij¨o and H. L¨ahdesm¨aki. Learning gene regulatory
networks from gene expression measurements using
non-parametric molecular kinetics. Bioinformatics,
25:2937––2944, 2009.

11

Appendix of ‘Learning unknown ODE models with Gaussian pro-
cesses’

Sensitivity Equations

In the main text, the sensitivity equation is formulated using matrix notation

˙S(t) = J(t)S(t) + R(t).

Here, the time-dependent matrices are obtained by diﬀerentiating the vector valued functions with respect to
vectors i.e.



































S(t) =

J(t) =

R(t) =

dx1(t,U )
du1

dx2(t,U )
du1

· · ·
· · ·
dxD(t,U )
du1

dx1(t,U )
du2

dx2(t,U )
du2

· · ·
· · ·
dxD(t,U )
du2

· · ·

· · ·

· · ·
· · ·
· · ·

dx1(t,U )
duMD

dx2(t,U )
duMD

· · ·
· · ·
dxD(t,U )
duMD













∂f (x(t),U )1
∂x1

∂f (x(t),U )2
∂x1

· · ·
· · ·
∂f (x(t),U )D
∂x1

∂f (x(t),U )1
∂u1

∂f (x(t),U )2
∂u1

· · ·
· · ·
∂f (x(t),U )D
∂u1

∂f (x(t),U )1
∂x2

∂f (x(t),U )2
∂x2

· · ·
· · ·
∂f (x(t),U )D
∂x2

∂f (x(t),U )1
∂u2

∂f (x(t),U )2
∂u2

· · ·
· · ·
∂f (x(t),U )D
∂u2

· · ·

· · ·

· · ·
· · ·
· · ·

· · ·

· · ·

· · ·
· · ·
· · ·

D×M D
∂f (x(t),U )1
∂xD

∂f (x(t),U )2
∂xD

· · ·
· · ·
∂f (x(t),U )D
∂xD

∂f (x(t),U )1
∂uMD

∂f (x(t),U )2
∂uMD

· · ·
· · ·
∂f (x(t),U )D
∂uMD























D×D

D×M D

Optimization

Below is the explicit form of the log posterior. Note that we introduce u = vec(U ) and Ω = diag(ω2
for notational simplicity.

1, . . . , ω2

D)

log L = log p(U |θ) + log p(Y |x0, U, ω)

= log N (u|0, Kθ(Z, Z)) +

log N (yi|x(ti, U ), Ω)

N
(cid:88)

i=1

1
2

1
2

1
2

1
2

1
2

1
2

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

(yij − xj(ti, U, x0))2
ω2
j

−

N
(cid:88)

i=1

1
2

(yi,j − xj(ti, U, x0))2
ω2
j

D
(cid:88)

j=1

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

− N

log ωj

(30)

= −

uT Kθ(Z, Z)−1u −

log |Kθ(Z, Z)| −

log |Ω|

(29)

Our goal is to compute the gradients with respect to the initial state x0, latent vector ﬁeld (cid:101)U , kernel
parameters θ and noise variables ω. As explained in the paper, we compute the gradient of the posterior with

12

(23)

(24)

(25)

(26)

(27)

(28)

respect to inducing vectors U and project them to the white domain thanks to noncentral parameterisation.
The analytical forms of the partial derivatives are as follows:

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂uk

− Kθ(Z, Z)−1u

∂ log L
∂uk

∂ log L
∂(x0)d

∂ log L
∂ωj

=

=

=

N
(cid:88)

D
(cid:88)

i=1

j=1

N
(cid:88)

D
(cid:88)

i=1

j=1

1
ω3
j

N
(cid:88)

i=1

yi,j − xj(ti, U, x0)
ω2
j

∂xj(ti, U, x0)
∂(x0)d

(yi,j − xj(ti, U, x0))2 −

N
ωj

Seemingly hard to compute terms, ∂xj (ti,U,x0)

, are computed using sensitivities. The
lengthscale parameter (cid:96) is considered as a model complexity parameter and is chosen from a grid using
cross-validation. We furthermore need the gradient with respect to the other kernel variable, i.e., the signal
variance σ2
f . Because Kθ(Z, Z) and x(ti, U ) are the functions of kernel, computing the gradients with respect
to σ2

f is not trivial and we make use of ﬁnite diﬀerences:

and ∂xj (ti,U,x0)

∂(x0)d

∂uk

We use δ = 10−4 to compute the ﬁnite diﬀerences.

One problem of using gradient-based optimization techniques is that they do not ensure the positivity of
the parameters being optimized. Therefore, we perform the optimization of the noise standard deviations
ω = (ω1, . . . , ωD) and signal variance σf with respect to their logarithms:

∂ log L
∂σf

=

log L(σf + δ) − log L(σf )
δ

∂ log L
∂ log c

=

∂ log L
∂c

∂c
∂ log c

=

∂ log L
∂c

c

where c ∈ (σf , ω1, . . . , ωD).

Implementation details

We initialise the inducing vectors U = (u1, . . . uM ) by computing the empirical gradients ˙yi = yi − yi−1, and
conditioning as

U0 = K(Z, Y )K(Y, Y )−1c ˙y,

where we optimize the scale c against the posterior. The whitened inducing vector is obtained as (cid:101)U0 = L−1
θ U0.
This procedure produces initial vector ﬁelds that partially match the trajectory already. We then do 100
restarts of the optimization from random perturbations (cid:101)U = (cid:101)U0 + ε.

We use LBFGS gradient optimization routine in Matlab. We initialise the inducing vector locations Z on a
equidistant ﬁxed grid on a box containing the observed points. We select the lengthscales (cid:96)1, . . . , (cid:96)D using
cross-validation from values {0.5, 0.75, 1, 1.25, 1.5}. In general large lengthscales induce smoother models,
while lower lengthscales cause overﬁtting.

(31)

(32)

(33)

(34)

(35)

(36)

13

