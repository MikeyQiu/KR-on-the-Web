Spatial As Deep: Spatial CNN for Trafﬁc Scene Understanding

Xingang Pan1, Jianping Shi2, Ping Luo1, Xiaogang Wang1, and Xiaoou Tang1
1The Chinese University of Hong Kong 2SenseTime Group Limited
{px117, pluo, xtang}@ie.cuhk.edu.hk, shijianping@sensetime.com, xgwang@ee.cuhk.edu.hk

7
1
0
2
 
c
e
D
 
7
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
8
0
6
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

Convolutional neural networks (CNNs) are usually built by
stacking convolutional operations layer-by-layer. Although
CNN has shown strong capability to extract semantics from
raw pixels, its capacity to capture spatial relationships of pix-
els across rows and columns of an image is not fully ex-
plored. These relationships are important to learn semantic
objects with strong shape priors but weak appearance coher-
ences, such as trafﬁc lanes, which are often occluded or not
even painted on the road surface as shown in Fig. 1 (a). In
this paper, we propose Spatial CNN (SCNN), which general-
izes traditional deep layer-by-layer convolutions to slice-by-
slice convolutions within feature maps, thus enabling mes-
sage passings between pixels across rows and columns in a
layer. Such SCNN is particular suitable for long continuous
shape structure or large objects, with strong spatial relation-
ship but less appearance clues, such as trafﬁc lanes, poles, and
wall. We apply SCNN on a newly released very challenging
trafﬁc lane detection dataset and Cityscapse dataset1. The re-
sults show that SCNN could learn the spatial relationship for
structure output and signiﬁcantly improves the performance.
We show that SCNN outperforms the recurrent neural net-
work (RNN) based ReNet and MRF+CNN (MRFNet) in the
lane detection dataset by 8.7% and 4.6% respectively. More-
over, our SCNN won the 1st place on the TuSimple Bench-
mark Lane Detection Challenge, with an accuracy of 96.53%.

Introduction
In recent years, autonomous driving has received much at-
tention in both academy and industry. One of the most chal-
lenging task of autonomous driving is trafﬁc scene under-
standing, which comprises computer vision tasks like lane
detection and semantic segmentation. Lane detection helps
to guide vehicles and could be used in driving assistance
system (Urmson et al. 2008), while semantic segmentation
provides more detailed positions about surrounding objects
like vehicles or pedestrians. In real applications, however,
these tasks could be very challenging considering the many
harsh scenarios, including bad weather conditions, dim or
dazzle light, etc. Another challenge of trafﬁc scene under-
standing is that in many cases, especially in lane detection,
we need to tackle objects with strong structure prior but less

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1Code is available at https://github.com/XingangPan/SCNN

Figure 1: Comparison between CNN and SCNN in (a) lane
detection and (b) semantic segmentation. For each example,
from left to right are: input image, output of CNN, output
of SCNN. It can be seen that SCNN could better capture the
long continuous shape prior of lane markings and poles and
ﬁx the disconnected parts in CNN.

appearance clues like lane markings and poles, which have
long continuous shape and might be occluded. For instance,
in the ﬁrst example in Fig. 1 (a), the car at the right side fully
occludes the rightmost lane marking.

Although CNN based methods (Krizhevsky, Sutskever,
and Hinton 2012; Long, Shelhamer, and Darrell 2015) have
pushed scene understanding to a new level thanks to the
strong representation learning ability. It is still not perform-
ing well for objects having long structure region and could
be occluded, such as the lane markings and poles shown in
the red bounding boxes in Fig. 1. However, humans can eas-
ily infer their positions and ﬁll in the occluded part from the
context, i.e., the viewable part.

To address this issue, we propose Spatial CNN (SCNN),
a generalization of deep convolutional neural networks to
a rich spatial level. In a layer-by-layer CNN, a convolution
layer receives input from the former layer, applies convolu-
tion operation and nonlinear activation, and sends result to
the next layer. This process is done sequentially. Similarly,
SCNN views rows or columns of feature maps as layers and
applies convolution, nonlinear activation, and sum opera-
tions sequentially, which forms a deep neural network. In
this way information could be propagated between neurons
in the same layer. It is particularly useful for structured ob-
ject such as lanes, poles, or truck with occlusions, since the
spatial information can be reinforced via inter layer propa-

Figure 2: (a) Dataset examples for different scenarios. (b) Proportion of each scenario.

gation. As shown in Fig. 1, in cases where CNN is discon-
tinuous or is messy, SCNN could well preserve the smooth-
ness and continuity of lane markings and poles. In our ex-
periment, SCNN signiﬁcantly outperforms other RNN or
MRF/CRF based methods, and also gives better results than
the much deeper ResNet-101 (He et al. 2016).

Related Work. For lane detection, most existing algo-
rithms are based on hand-crafted low-level features (Aly
2008; Son et al. 2015; Jung, Youn, and Sull 2016), limiting
there capability to deal with harsh conditions. Only Huval
et al. (2015) gave a primacy attempt adopting deep learn-
ing in lane detection but without a large and general dataset.
While for semantic segmentation, CNN based methods have
become mainstream and achieved great success (Long, Shel-
hamer, and Darrell 2015; Chen et al. 2017).

There have been some other attempts to utilize spatial in-
formation in neural networks. Visin et al. (2015) and Bell
et al. (2016) used recurrent neural networks to pass infor-
mation along each row or column, thus in one RNN layer
each pixel position could only receive information from the
same row or column. Liang et al. (2016a; 2016b) proposed
variants of LSTM to exploit contextual information in se-
mantic object parsing, but such models are computation-
ally expensive. Researchers also attempted to combine CNN
with graphical models like MRF or CRF, in which message
pass is realized by convolution with large kernels (Liu et
al. 2015; Tompson et al. 2014; Chu et al. 2016). There are
three advantages of SCNN over these aforementioned meth-
ods: in SCNN, (1) the sequential message pass scheme is
much more computational efﬁciency than traditional dense
MRF/CRF, (2) the messages are propagated as residual,
making SCNN easy to train, and (3) SCNN is ﬂexible and
could be applied to any level of a deep neural network.

Spatial Convolutional Neural Network

Lane Detection Dataset
In this paper, we present a large scale challenging dataset
for trafﬁc lane detection. Despite the importance and dif-
ﬁculty of trafﬁc lane detection, existing datasets are either
too small or too simple, and a large public annotated bench-
mark is needed to compare different methods (Bar Hillel
et al. 2014). KITTI (Fritsch, Kuhnl, and Geiger 2013) and
CamVid (Brostow et al. 2008) contains pixel level anno-

tations for lane/lane markings, but have merely hundreds
of images, too small for deep learning methods. Caltech
Lanes Dataset (Aly 2008) and the recently released TuSim-
ple Benchmark Dataset (TuSimple 2017) consists of 1224
and 6408 images with annotated lane markings respectively,
while the trafﬁc is in a constrained scenario, which has
light trafﬁc and clear lane markings. Besides, none of these
datasets annotates the lane markings that are occluded or are
unseen because of abrasion, while such lane markings can be
inferred by human and is of high value in real applications.
To collect data, we mounted cameras on six different ve-
hicles driven by different drivers and recorded videos during
driving in Beijing on different days. More than 55 hours of
videos were collected and 133,235 frames were extracted,
which is more than 20 times of TuSimple Dataset. We have
divided the dataset into 88880 for training set, 9675 for vali-
dation set, and 34680 for test set. These images were undis-
torted using tools in (Scaramuzza, Martinelli, and Siegwart
2006) and have a resolution of 1640 × 590. Fig. 2 (a) shows
some examples, which comprises urban, rural, and highway
scenes. As one of the largest and most crowded cities in the
world, Beijing provides many challenging trafﬁc scenarios
for lane detection. We divided the test set into normal and
8 challenging categories, which correspond to the 9 exam-
ples in Fig. 2 (a). Fig. 2 (b) shows the proportion of each
scenario. It can be seen that the 8 challenging scenarios ac-
count for most (72.3%) of the dataset.

For each frame, we manually annotate the trafﬁc lanes
with cubic splines. As mentioned earlier, in many cases lane
markings are occluded by vehicles or are unseen. In real ap-
plications it is important that lane detection algorithms could
estimate lane positions from the context even in these chal-
lenging scenarios that occur frequently. Therefore, for these
cases we still annotate the lanes according to the context, as
shown in Fig. 2 (a) (2)(4). We also hope that our algorithm
could distinguish barriers on the road, like the one in Fig. 2
(a) (1). Thus the lanes on the other side of the barrier are not
annotated. In this paper we focus our attention on the detec-
tion of four lane markings, which are paid most attention to
in real applications. Other lane markings are not annotated.

Spatial CNN
Traditional methods to model spatial relationship are based
on Markov Random Fields (MRF) or Conditional Ran-

Figure 3: (a) MRF/CRF based method. (b) Our implementation of Spatial CNN. MRF/CRF are theoretically applied to unary
potentials whose channel number equals to the number of classes to be classiﬁed, while SCNN could be applied to the top
hidden layers with richer information.

dom Fields (CRF) (Kr¨ahenb¨uhl and Koltun 2011). Recent
works (Zheng et al. 2015; Liu et al. 2015; Chen et al. 2017)
to combine them with CNN all follow the pipeline of Fig. 3
(a), where the mean ﬁeld algorithm can be implemented with
neural networks. Speciﬁcally, the procedure is (1) Normal-
ize: the output of CNN is viewed as unary potentials and is
normalized by the Softmax operation, (2) Message Passing,
which could be realized by channel wise convolution with
large kernels (for dense CRF, the kernel size would cover
the whole image and the kernel weights are dependent on the
input image), (3) Compatibility Transform, which could be
implemented with a 1 × 1 convolution layer, and (4) Adding
unary potentials. This process is iterated for N times to give
the ﬁnal output.

It can be seen that in the message passing process of
traditional methods, each pixel receives information from
all other pixels, which is very computational expensive and
hard to be used in real time tasks as in autonomous driving.
For MRF, the large convolution kernel is hard to learn and
usually requires careful initialization (Tompson et al. 2014;
Liu et al. 2015). Moreover, these methods are applied to the
output of CNN, while the top hidden layer, which comprises
richer information, might be a better place to model spatial
relationship.

To address these issues, and to more efﬁciently learn the
spatial relationship and the smooth, continuous prior of lane
markings, or other structured object in the driving scenario,
we propose Spatial CNN. Note that the ’spatial’ here is
not the same with that in ’spatial convolution’, but denotes
propagating spatial information via specially designed CNN
structure.

As shown in the ’SCNN D’ module of Fig. 3 (b), consid-
ering a SCNN applied on a 3-D tensor of size C × H × W ,
where C, H, and W denote the number of channel, rows, and
columns respectively. The tensor would be splited into H
slices, and the ﬁrst slice is then sent into a convolution layer

with C kernels of size C ×w, where w is the kernel width. In
a traditional CNN the output of a convolution layer is then
fed into the next layer, while here the output is added to the
next slice to provide a new slice. The new slice is then sent to
the next convolution layer and this process would continue
until the last slice is updated.

Speciﬁcally, assume we have a 3-D kernel tensor K with
element Ki,j,k denoting the weight between an element in
channel i of the last slice and an element in channel j of
the current slice, with an offset of k columes between two
elements. Also denote the element of input 3-D tensor X as
Xi,j,k, where i, j, and k indicate indexes of channel, row,
and column respectively. Then the forward computation of
SCNN is:

X (cid:48)

i,j,k =






Xi,j,k,
Xi,j,k + f (cid:0) (cid:80)

(cid:80)
n

m

X (cid:48)
(cid:1),

×Km,i,n

j = 1

m, j − 1, k + n − 1

(1)

j = 2, 3, ..., H

where f is a nonlinear activation function as ReLU. The X
with superscript (cid:48) denotes the element that has been updated.
Note that the convolution kernel weights are shared across
all slices, thus SCNN is a kind of recurrent neural network.
Also note that SCNN has directions. In Fig. 3 (b), the four
’SCNN’ module with sufﬁx ’D’, ’U’, ’R’, ’L’ denotes SCNN
that is downward, upward, rightward, and leftward respec-
tively.

Analysis
There are three main advantages of Spatial CNN over tradi-
tional methods, which are concluded as follows.

(1) Computational efﬁciency. As show in Fig. 4, in dense
MRF/CRF each pixel receives messages from all other pix-
els directly, which could have much redundancy, while in
SCNN message passing is realized in a sequential propaga-
tion scheme. Speciﬁcally, assume a tensor with H rows and

Figure 4: Message passing directions in (a) dense MRF/CRF
and (b) Spatial CNN (rightward). For (a), only message
passing to the inner 4 pixels are shown for clearance.

W columns, then in dense MRF/CRF, there is message pass
between every two of the W H pixels. For niter iterations,
the number of message passing is niterW 2H 2. In SCNN,
each pixel only receive information from w pixels, thus the
number of message passing is ndirW Hw, where ndir and w
denotes the number of propagation directions in SCNN and
the kernel width of SCNN respectively. niter could range
from 10 to 100, while in this paper ndir is set to 4, cor-
responding to 4 directions, and w is usually no larger than
10 (in the example in Fig. 4 (b) w = 3). It can be seen
that for images with hundreds of rows and columns, SCNN
could save much computations, while each pixel still could
receive messages from all other pixels with message propa-
gation along 4 directions.

(2) Message as residual. In MRF/CRF, message passing
is achieved via weighted sum of all pixels, which, according
to the former paragraph, is computational expensive. And
recurrent neural network based methods might suffer from
gradient descent (Pascanu, Mikolov, and Bengio 2013), con-
sidering so many rows or columns. However, deep residual
learning (He et al. 2016) has shown its capability to easy
the training of very deep neural networks. Similarly, in our
deep SCNN messages are propagated as residual, which is
the output of ReLU in Eq.(1). Such residual could also be
viewed as a kind of modiﬁcation to the original neuron.
As our experiments will show, such message pass scheme
achieves better results than LSTM based methods.

(3) Flexibility Thanks to the computational efﬁciency of
SCNN, it could be easily incorporated into any part of a
CNN, rather than just output. Usually, the top hidden layer
contains information that is both rich and of high semantics,
thus is an ideal place to apply SCNN. Typically, Fig. 3 shows
our implementation of SCNN on the LargeFOV (Chen et al.
2017) model. SCNNs on four spatial directions are added
sequentially right after the top hidden layer (’fc7’ layer) to
introduce spatial message propagation.

Experiment
We evaluate SCNN on our lane detection dataset and
Cityscapes (Cordts et al. 2016). In both tasks, we train the
models using standard SGD with batch size 12, base learn-
ing rate 0.01, momentum 0.9, and weight decay 0.0001. The
learning rate policy is ”poly” with power and iteration num-
ber set to 0.9 and 60K respectively. Our models are modiﬁed
based on the LargeFOV model in (Chen et al. 2017). The ini-

Figure 5: (a) Training model, (b) Lane prediction process.
’Conv’,’HConv’, and ’FC’ denotes convolution layer, atrous
convolution layer (Chen et al. 2017), and fully connected
layer respectively. ’c’, ’w’, and ’h’ denotes number of output
channels, kernel width, and ’rate’ for atrous convolution.

tial weights of the ﬁrst 13 convolution layers are copied from
VGG16 (Simonyan and Zisserman 2015) trained on Ima-
geNet (Deng et al. 2009). All experiments are implemented
on the Torch7 (Collobert, Kavukcuoglu, and Farabet 2011)
framework.

Lane Detection
Lane detection model Unlike common object detection
task that only requires bounding boxes, lane detection re-
quires precise prediction of curves. A natural idea is that the
model should output probability maps (probmaps) of these
curves, thus we generate pixel level targets to train the net-
works, like in semantic segmentation tasks. Instead of view-
ing different lane markings as one class and do clustering
afterwards, we want the neural network to distinguish dif-
ferent lane markings on itself, which could be more robust.
Thus these four lanes are viewed as different classes. More-
over, the probmaps are then sent to a small network to give
prediction on the existence of lane markings.

During testing, we still need to go from probmaps to
curves. As shown in Fig.5 (b), for each lane marking whose
existence value is larger than 0.5, we search the correspond-
ing probmap every 20 rows for the position with the high-
est response. These positions are then connected by cubic
splines, which are the ﬁnal predictions.

As shown in Fig.5 (a), the detailed differences between
our baseline model and LargeFOV are: (1) the output chan-
nel number of the ’fc7’ layer is set to 128, (2) the ’rate’ for
the atrous convolution layer of ’fc6’ is set to 4, (3) batch nor-
malization (Ioffe and Szegedy 2015) is added before each
ReLU layer, (4) a small network is added to predict the ex-
istence of lane markings. During training, the line width of
the targets is set to 16 pixels, and the input and target images
are rescaled to 800 × 288. Considering the imbalanced label

Figure 6: Evaluation based on IoU. Green lines denote
ground truth, while blue and red lines denote TP and FP re-
spectively.

between background and lane markings, the loss of back-
ground is multiplied by 0.4.

Evaluation In order to judge whether a lane marking is
successfully detected, we view lane markings as lines with
widths equal to 30 pixel and calculate the intersection-
over-union (IoU) between the ground truth and the predic-
tion. Predictions whose IoUs are larger than certain thresh-
old are viewed as true positives (TP), as shown in Fig. 6.
Here we consider 0.3 and 0.5 thresholds corresponding to
loose and strict evaluations. Then we employ F-measure =
(1 + β2)
β2Precision+Recall as the ﬁnal evaluation index, where
Precision = T P
T P +F P and Recall = T P
T P +F N . Here β is set
to 1, corresponding to harmonic mean (F1-measure).

Precision Recall

Ablation Study
In section 2.2 we propose Spatial CNN to
enable spatial message propagation. To verify our method,
we will make detailed ablation studies in this subsection.
Our implementation of SCNN follows that shown in Fig. 3.
(1) Effectiveness of multidirectional SCNN. Firstly, we in-
vestigate the effects of directions in SCNN. We try SCNN
that has different direction implementations, the results are
shown in Table. 1. Here the kernel width w of SCNN is set to
5. It can be seen that the performance increases as more di-
rections are added. To prove that the improvement does not
result from more parameters but from the message passing
scheme brought about by SCNN, we add an extra convolu-
tion layer with 5×5 kernel width after the top hidden layer of
the baseline model and compare with our method. From the
results we can see that extra convolution layer could merely
bring about little improvement, which veriﬁes the effective-
ness of SCNN.

Table 1: Experimental results on SCNN with different di-
rectional settings. F1 denotes F1-measure, and the value in
the bracket denotes the IoU threshold. The sufﬁx ’D’, ’U’,
’R’, ’L’ denote downward, upward, rightward, and leftward
respectively.
Models

Baseline ExtraConv SCNN D SCNN DU SCNN DURL

F1 (0.3)
F1 (0.5)

77.7
63.2

77.6
64.0

79.5
68.6

79.9
69.4

80.2
70.4

(2) Effects of kernel width w. We further try SCNN with
different kernel width based on the ”SCNN DURL” model,
as shown in Table. 2. Here the kernel width denotes the num-
ber of pixels that a pixel could receive messages from, and
the w = 1 case is similar to the methods in (Visin et al. 2015;
Bell et al. 2016). The results show that larger w is beneﬁcial,

and w = 9 gives a satisfactory result, which surpasses the
baseline by a signiﬁcant margin 8.4% and 3.2% correspond-
ing to different IoU threshold.

Table 2: Experimental results on SCNN with different kernel
widths.

Kernel width w

1

F1 (0.3)
F1 (0.5)

3

79.5
68.9

5

80.2
70.4

7

80.5
71.2

9

80.9
71.6

11

80.6
71.7

78.5
66.3

(3) Spatial CNN on different positions. As mentioned ear-
lier, SCNN could be added to any place of a neural network.
Here we consider the SCNN DURL model applied on (1)
output and (2) the top hidden layer, which correspond to
Fig. 3. The results in Table. 3 indicate that the top hidden
layer, which comprises richer information than the output,
turns out to be a better position to apply SCNN.

Table 3: Experimental results on spatial CNN at different
positions, with w = 9.

Position Output

Top hidden layer

F1 (0.3)
F1 (0.5)

79.9
68.8

80.9
71.6

(4) Effectiveness of sequential propagation. In our SCNN,
information is propagated in a sequential way, i.e., a slice
does not pass information to the next slice until it has re-
ceived information from former slices. To verify the effec-
tiveness of this scheme, we compare it with parallel prop-
agation, i.e., each slice passes information to the next slice
simultaneously before being updated. For this parallel case,
the (cid:48) in the right part of Eq.(1) is removed. As Table. 4
shows, the sequential message passing scheme outperforms
the parallel scheme signiﬁcantly. This result indicates that
in SCNN, a pixel does not merely affected by nearby pixels,
but do receive information from further positions.

Table 4: Comparison between sequential and parallel mes-
sage passing scheme, for SCNN DULR with w = 9.
Sequential

Message passing scheme

Parallel

F1 (0.3)
F1 (0.5)

78.4
65.2

80.9
71.6

(5) Comparison with state-of-the-art methods. To fur-
ther verify the effectiveness of SCNN in lane detec-
tion, we compare it with several methods: the rnn based
ReNet (Visin et al. 2015), the MRF based MRFNet, the
DenseCRF (Kr¨ahenb¨uhl and Koltun 2011), and the very
deep residual network (He et al. 2016). For ReNet based
on LSTM, we replace the ”SCNN” layers in Fig. 3 with
two ReNet layers: one layer to pass horizontal information
and the other to pass vertical information. For DenseCRF,
we use dense CRF as post-processing and employ 10 mean
ﬁeld iterations as in (Chen et al. 2017). For MRFNet, we use

Table 5: Comparison with other methods, with IoU threshold=0.5. For crossroad, only FP are shown.

Category

Baseline

ReNet DenseCRF MRFNet ResNet-50 ResNet-101 Baseline+SCNN

Normal
Crowded
Night
No line
Shadow
Arrow
Dazzle light
Curve
Crossroad
Total

83.1
61.0
56.9
34.0
54.7
74.0
49.9
61.0
2060
63.2

83.3
60.5
56.3
34.5
55.0
74.1
48.2
59.9
2296
62.9

81.3
58.8
54.2
31.9
56.3
71.2
46.2
57.8
2253
61.0

86.3
65.2
61.3
37.2
59.3
76.9
53.7
62.3
1837
67.0

87.4
64.1
60.6
38.1
60.7
79.0
54.1
59.8
2505
66.7

90.2
68.2
65.9
41.7
64.6
84.0
59.8
65.5
2183
70.8

90.6
69.7
66.1
43.4
66.9
84.1
58.5
64.4
1990
71.6

Figure 7: Comparison between probmaps of baseline, ReNet, MRFNet, ResNet-101, and SCNN.

the implementation in Fig. 3 (a), with iteration times and
message passing kernel size set to 10 and 20 respectively.
The main difference of the MRF here with CRF is that the
weights of message passing kernels are learned during train-
ing rather than depending on the image. For ResNet, our
implementation is the same with (Chen et al. 2017) except
that we do not use the ASPP module. For SCNN, we add
SCNN DULR module to the baseline, and the kernel width
w is 9. The test results on different scenarios are shown in
Table 5, and visualizations are given in Fig. 7.

From the results, we can see that the performance of
ReNet is not even comparable with SCNN DULR with w =
1, indicating the effectiveness of our residual message pass-
ing scheme. Interestingly, DenseCRF leads to worse result
here, because lane markings usually have less appearance
clues so that dense CRF cannot distinguish lane markings
and background. In contrast, with kernel weights learned
from data, MRFNet could to some extent smooth the results
and improve performance, as Fig. 7 shows, but are still not
very satisfactory. Furthermore, our method even outperform
the much deeper ResNet-50 and ResNet-101. Despite the
over a hundred layers and the very large receptive ﬁeld of
ResNet-101, it still gives messy or discontinuous outputs in
challenging cases, while our method, with only 16 convolu-
tion layers plus 4 SCNN layers, could preserve the smooth-
ness and continuity of lane lines better. This demonstrates
the much stronger capability of SCNN to capture structure
prior of objects over traditional CNN.

(6) Computational efﬁciency over other methods. In the
Analysis section we give theoretical analysis on the com-
putational efﬁciency of SCNN over dense CRF. To verify

this, we compare their runtime experimentally. The results
are shown in Table. 6, where the runtime of the LSTM in
ReNet is also given. Here the runtime does not include run-
time of the backbone network. For SCNN, we test both the
practical case and the case with the same setting as dense
CRF. In the practical case, SCNN is applied on top hidden
layer, thus the input has more channels but less hight and
width. In the fair comparison case, the input size is modiﬁed
to be the same with that in dense CRF, and both methods are
tested on CPU. The results show that even in fair comparison
case, SCNN is over 4 times faster than dense CRF, despite
the efﬁcient implementation of dense CRF in (Kr¨ahenb¨uhl
and Koltun 2011). This is because SCNN signiﬁcantly re-
duces redundancy in message passing, as in Fig. 4. Also,
SCNN is more efﬁcient than LSTM, whose gate mechanism
requires more computation.

Table 6: Runtime of dense CRF, LSTM, and SCNN. The two
SCNNs correspond to the one used in practice and the one
whose input size is modiﬁed for fair comparison with dense
CRF respectively. The kernel width w of SCNN is 9.

Method

dense CRF

LSTM

SCNN DULR
(in practice)

SCNN DULR
(fair comparison)

5×288×800

128×36×100

128×36×100

5×288×800

Input size
(C × H × W )
Device
Runtime (ms)

CPU2
737

GPU3
115

GPU
42

CPU
176

2

3

Intel Core i7-4790K CPU
GeForce GTX TITAN Black

Table 7: Results on Cityscapes validation set.

trafﬁc
light

trafﬁc
sign

Method

road terrain building wall

car

pole

fence

sidewalk sky

rider person vegetation truck

bus

train motor bicycle mIoU

LargeFOV
LargeFOV+SCNN

97.0
97.0

ResNet-101
98.3
ResNet-101+SCNN 98.3

59.2
59.8

64.2
65.4

89.9
90.3

92.4
92.6

42.2
45.7

44.5
46.7

92.3 52.9
92.5 55.2

94.9 66.0
94.8 66.1

62.3
62.3

74.5
74.3

71.1
71.7

82.1
81.5

52.2
52.5

59.9
61.2

78.8
78.1

86.0
86.1

92.2
92.6

94.7
94.7

52.1
53.2

65.5
65.5

75.9
76.4

84.1
84.0

91.0
91.1

92.7
92.7

48.8
55.6

57.3
57.7

70.2 37.6
71.2 41.7

81.1 54.0
82.0 59.9

54.6
56.2

64.5
67.0

72.3
72.3

80.0
80.1

68.0
69.2

75.6
76.4

Figure 8: Visual improvements on Cityscapes validation set. For each example, from left to right are: input image, ground truth,
result of LargeFOV, result of LargeFOV+SCNN.

Semantic Segmentation on Cityscapes

To demonstrate the generality of our method, we also
evaluate Spatial CNN on Cityscapes (Cordts et al. 2016).
Cityscapes is a standard benchmark dataset for semantic
segmentation on urban trafﬁc scenes. It contains 5000 ﬁne
annotated images, including 2975 for training, 500 for val-
idation and 1525 for testing. 19 categories are deﬁned in-
cluding both stuff and objects. We use two classic mod-
els, the LargeFOV and ResNet-101 in DeepLab (Chen et al.
2017) as the baselines. Batch normalization layers (Ioffe and
Szegedy 2015) are added to LargeFOV to enable faster con-
vergence. For both models, the channel numbers of the top
hidden layers are modiﬁed to 128 to make them compacter.
We add SCNN to the baseline models in the same way as
in lane detection. The comparisons between baselines and
those combined with the SCNN DURL models with kernel
width w = 9 are shown in Table 7. It can be seen that SCNN
could also improve semantic segmentation results. With SC-
NNs added, the IoUs for all classes are at least comparable
to the baselines, while the ”wall”, ”pole”, ”truck”, ”bus”,
”train”, and ”motor” categories achieve signiﬁcant improve.
This is because for long shaped objects like train and pole,
SCNN could capture its continuous structure and connect
the disconnected part, as shown in Fig. 8. And for wall,
truck, and bus which could occupy large image area, the dif-
fusion effect of SCNN could correct the part that are mis-
classiﬁed according to the context. This shows that SCNN
is useful not only for long thin structure, but also for large
objects which require global information to be classiﬁed
correctly. There is another interesting phenomenon that the
head of the vehicle at the bottom of the images, whose label
is ignored during training, is in a mess in LargeFOV while
with SCNN added it is classiﬁed as road. This is also due to
the diffusion effects of SCNN, which passes the information
of road to the vehicle head area.

To compare our method with other MRF/CRF based

Table 8: Comparison between our SCNN and other
MRF/CRF based methods on Cityscapes test set.

Method

LargeFOV
(Chen et al. 2017)

DPN
(Liu et al. 2015)

mIoU

63.1

66.8

Ours

68.2

methods, we evaluate LargeFOV+SCNN on Cityscapes test
set, and compare with methods that also use VGG16 (Si-
monyan and Zisserman 2015) as the backbone network. The
results are shown in Table 8. Here LargeFOV, DPN, and our
method use dense CRF, dense MRF, and SCNN respectively,
and share nearly the same base CNN part. The results show
that our method achieves signiﬁcant better performance.

Conclusion
In this paper, we propose Spatial CNN, a CNN-like scheme
to achieve effective information propagation in the spatial
level. SCNN could be easily incorporated into deep neu-
ral networks and trained end-to-end. It is evaluated at two
tasks in trafﬁc scene understanding: lane detection and se-
mantic segmentation. The results show that SCNN could ef-
fectively preserve the continuity of long thin structure, while
in semantic segmentation its diffusion effects is also proved
to be beneﬁcial for large objects. Speciﬁcally, by introduc-
ing SCNN into the LargeFOV model, our 20-layer network
outperforms ReNet, MRF, and the very deep ResNet-101 in
lane detection. Last but not least, we believe that the large
challenging lane detection dataset we presented would push
forward researches on autonomous driving.

Acknowledgments
This work is supported by SenseTime Group Limited. We
would like to thank Xiaohang Zhan, Jun Li, and Xudong
Cao for helpful work in building the lane detection dataset.

Scaramuzza, D.; Martinelli, A.; and Siegwart, R. 2006. A ﬂexi-
ble technique for accurate omnidirectional camera calibration and
structure from motion. In Computer Vision Systems, 2006 ICVS’06.
IEEE International Conference on, 45–45. IEEE.
Simonyan, K., and Zisserman, A. 2015. Very deep convolutional
networks for large-scale image recognition. In ICLR.
Son, J.; Yoo, H.; Kim, S.; and Sohn, K. 2015. Real-time illumi-
nation invariant lane detection for lane departure warning system.
Expert Systems with Applications 42(4):1816–1824.
Tompson, J. J.; Jain, A.; LeCun, Y.; and Bregler, C. 2014. Joint
training of a convolutional network and a graphical model for hu-
man pose estimation. In NIPS.
TuSimple. 2017. Tusimple benchmark. http://benchmark.
tusimple.ai/#/.
Urmson, C.; Anhalt, J.; Bagnell, D.; Baker, C.; Bittner, R.; Clark,
M.; Dolan, J.; Duggins, D.; Galatali, T.; Geyer, C.; et al. 2008.
Autonomous driving in urban environments: Boss and the urban
challenge. Journal of Field Robotics 25(8):425–466.
Visin, F.; Kastner, K.; Cho, K.; Matteucci, M.; Courville, A.; and
Bengio, Y. 2015. Renet: A recurrent neural network based alterna-
tive to convolutional networks. arXiv preprint arXiv:1505.00393.
Zheng, S.; Jayasumana, S.; Romera-Paredes, B.; Vineet, V.; Su, Z.;
Du, D.; Huang, C.; and Torr, P. H. 2015. Conditional random ﬁelds
as recurrent neural networks. In ICCV.

Imagenet: A large-scale hierarchical image database.

References
Aly, M. 2008. Real time detection of lane markers in urban streets.
In Intelligent Vehicles Symposium, 2008 IEEE, 7–12. IEEE.
Bar Hillel, A.; Lerner, R.; Levi, D.; and Raz, G. 2014. Recent
progress in road and lane detection: a survey. Machine vision and
applications 1–19.
Bell, S.; Lawrence Zitnick, C.; Bala, K.; and Girshick, R. 2016.
Inside-outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR.
Brostow, G. J.; Shotton, J.; Fauqueur, J.; and Cipolla, R. 2008.
Segmentation and recognition using structure from motion point
clouds. In ECCV.
Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and Yuille,
A. L. 2017. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected crfs.
TPAMI.
Chu, X.; Ouyang, W.; Wang, X.; et al. 2016. Crf-cnn: Modeling
structured information in human pose estimation. In NIPS.
Collobert, R.; Kavukcuoglu, K.; and Farabet, C. 2011. Torch7: A
matlab-like environment for machine learning. In BigLearn, NIPS
Workshop, number EPFL-CONF-192376.
Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler, M.;
Benenson, R.; Franke, U.; Roth, S.; and Schiele, B. 2016. The
cityscapes dataset for semantic urban scene understanding.
In
CVPR.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L.
2009.
In
CVPR.
Fritsch, J.; Kuhnl, T.; and Geiger, A. 2013. A new performance
measure and evaluation benchmark for road detection algorithms.
In Intelligent Transportation Systems-(ITSC), 2013 16th Interna-
tional IEEE Conference on, 1693–1700. IEEE.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
ing for image recognition. In CVPR.
Huval, B.; Wang, T.; Tandon, S.; Kiske, J.; Song, W.; Pazhayam-
pallil, J.; Andriluka, M.; Rajpurkar, P.; Migimatsu, T.; Cheng-Yue,
R.; et al. 2015. An empirical evaluation of deep learning on high-
way driving. arXiv preprint arXiv:1504.01716.
Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerat-
ing deep network training by reducing internal covariate shift. In
ICML.
Jung, S.; Youn, J.; and Sull, S. 2016. Efﬁcient lane detection based
on spatiotemporal images. IEEE Transactions on Intelligent Trans-
portation Systems 17(1):289–295.
Kr¨ahenb¨uhl, P., and Koltun, V. 2011. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In NIPS.
Liang, X.; Shen, X.; Feng, J.; Lin, L.; and Yan, S. 2016a. Semantic
object parsing with graph lstm. In ECCV.
Liang, X.; Shen, X.; Xiang, D.; Feng, J.; Lin, L.; and Yan, S. 2016b.
Semantic object parsing with local-global long short-term memory.
In CVPR.
Liu, Z.; Li, X.; Luo, P.; Loy, C.-C.; and Tang, X. 2015. Semantic
image segmentation via deep parsing network. In ICCV.
Long, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convolutional
networks for semantic segmentation. In CVPR.
Pascanu, R.; Mikolov, T.; and Bengio, Y. 2013. On the difﬁculty
of training recurrent neural networks. In ICML.

Spatial As Deep: Spatial CNN for Trafﬁc Scene Understanding

Xingang Pan1, Jianping Shi2, Ping Luo1, Xiaogang Wang1, and Xiaoou Tang1
1The Chinese University of Hong Kong 2SenseTime Group Limited
{px117, pluo, xtang}@ie.cuhk.edu.hk, shijianping@sensetime.com, xgwang@ee.cuhk.edu.hk

7
1
0
2
 
c
e
D
 
7
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
8
0
6
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

Convolutional neural networks (CNNs) are usually built by
stacking convolutional operations layer-by-layer. Although
CNN has shown strong capability to extract semantics from
raw pixels, its capacity to capture spatial relationships of pix-
els across rows and columns of an image is not fully ex-
plored. These relationships are important to learn semantic
objects with strong shape priors but weak appearance coher-
ences, such as trafﬁc lanes, which are often occluded or not
even painted on the road surface as shown in Fig. 1 (a). In
this paper, we propose Spatial CNN (SCNN), which general-
izes traditional deep layer-by-layer convolutions to slice-by-
slice convolutions within feature maps, thus enabling mes-
sage passings between pixels across rows and columns in a
layer. Such SCNN is particular suitable for long continuous
shape structure or large objects, with strong spatial relation-
ship but less appearance clues, such as trafﬁc lanes, poles, and
wall. We apply SCNN on a newly released very challenging
trafﬁc lane detection dataset and Cityscapse dataset1. The re-
sults show that SCNN could learn the spatial relationship for
structure output and signiﬁcantly improves the performance.
We show that SCNN outperforms the recurrent neural net-
work (RNN) based ReNet and MRF+CNN (MRFNet) in the
lane detection dataset by 8.7% and 4.6% respectively. More-
over, our SCNN won the 1st place on the TuSimple Bench-
mark Lane Detection Challenge, with an accuracy of 96.53%.

Introduction
In recent years, autonomous driving has received much at-
tention in both academy and industry. One of the most chal-
lenging task of autonomous driving is trafﬁc scene under-
standing, which comprises computer vision tasks like lane
detection and semantic segmentation. Lane detection helps
to guide vehicles and could be used in driving assistance
system (Urmson et al. 2008), while semantic segmentation
provides more detailed positions about surrounding objects
like vehicles or pedestrians. In real applications, however,
these tasks could be very challenging considering the many
harsh scenarios, including bad weather conditions, dim or
dazzle light, etc. Another challenge of trafﬁc scene under-
standing is that in many cases, especially in lane detection,
we need to tackle objects with strong structure prior but less

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1Code is available at https://github.com/XingangPan/SCNN

Figure 1: Comparison between CNN and SCNN in (a) lane
detection and (b) semantic segmentation. For each example,
from left to right are: input image, output of CNN, output
of SCNN. It can be seen that SCNN could better capture the
long continuous shape prior of lane markings and poles and
ﬁx the disconnected parts in CNN.

appearance clues like lane markings and poles, which have
long continuous shape and might be occluded. For instance,
in the ﬁrst example in Fig. 1 (a), the car at the right side fully
occludes the rightmost lane marking.

Although CNN based methods (Krizhevsky, Sutskever,
and Hinton 2012; Long, Shelhamer, and Darrell 2015) have
pushed scene understanding to a new level thanks to the
strong representation learning ability. It is still not perform-
ing well for objects having long structure region and could
be occluded, such as the lane markings and poles shown in
the red bounding boxes in Fig. 1. However, humans can eas-
ily infer their positions and ﬁll in the occluded part from the
context, i.e., the viewable part.

To address this issue, we propose Spatial CNN (SCNN),
a generalization of deep convolutional neural networks to
a rich spatial level. In a layer-by-layer CNN, a convolution
layer receives input from the former layer, applies convolu-
tion operation and nonlinear activation, and sends result to
the next layer. This process is done sequentially. Similarly,
SCNN views rows or columns of feature maps as layers and
applies convolution, nonlinear activation, and sum opera-
tions sequentially, which forms a deep neural network. In
this way information could be propagated between neurons
in the same layer. It is particularly useful for structured ob-
ject such as lanes, poles, or truck with occlusions, since the
spatial information can be reinforced via inter layer propa-

Figure 2: (a) Dataset examples for different scenarios. (b) Proportion of each scenario.

gation. As shown in Fig. 1, in cases where CNN is discon-
tinuous or is messy, SCNN could well preserve the smooth-
ness and continuity of lane markings and poles. In our ex-
periment, SCNN signiﬁcantly outperforms other RNN or
MRF/CRF based methods, and also gives better results than
the much deeper ResNet-101 (He et al. 2016).

Related Work. For lane detection, most existing algo-
rithms are based on hand-crafted low-level features (Aly
2008; Son et al. 2015; Jung, Youn, and Sull 2016), limiting
there capability to deal with harsh conditions. Only Huval
et al. (2015) gave a primacy attempt adopting deep learn-
ing in lane detection but without a large and general dataset.
While for semantic segmentation, CNN based methods have
become mainstream and achieved great success (Long, Shel-
hamer, and Darrell 2015; Chen et al. 2017).

There have been some other attempts to utilize spatial in-
formation in neural networks. Visin et al. (2015) and Bell
et al. (2016) used recurrent neural networks to pass infor-
mation along each row or column, thus in one RNN layer
each pixel position could only receive information from the
same row or column. Liang et al. (2016a; 2016b) proposed
variants of LSTM to exploit contextual information in se-
mantic object parsing, but such models are computation-
ally expensive. Researchers also attempted to combine CNN
with graphical models like MRF or CRF, in which message
pass is realized by convolution with large kernels (Liu et
al. 2015; Tompson et al. 2014; Chu et al. 2016). There are
three advantages of SCNN over these aforementioned meth-
ods: in SCNN, (1) the sequential message pass scheme is
much more computational efﬁciency than traditional dense
MRF/CRF, (2) the messages are propagated as residual,
making SCNN easy to train, and (3) SCNN is ﬂexible and
could be applied to any level of a deep neural network.

Spatial Convolutional Neural Network

Lane Detection Dataset
In this paper, we present a large scale challenging dataset
for trafﬁc lane detection. Despite the importance and dif-
ﬁculty of trafﬁc lane detection, existing datasets are either
too small or too simple, and a large public annotated bench-
mark is needed to compare different methods (Bar Hillel
et al. 2014). KITTI (Fritsch, Kuhnl, and Geiger 2013) and
CamVid (Brostow et al. 2008) contains pixel level anno-

tations for lane/lane markings, but have merely hundreds
of images, too small for deep learning methods. Caltech
Lanes Dataset (Aly 2008) and the recently released TuSim-
ple Benchmark Dataset (TuSimple 2017) consists of 1224
and 6408 images with annotated lane markings respectively,
while the trafﬁc is in a constrained scenario, which has
light trafﬁc and clear lane markings. Besides, none of these
datasets annotates the lane markings that are occluded or are
unseen because of abrasion, while such lane markings can be
inferred by human and is of high value in real applications.
To collect data, we mounted cameras on six different ve-
hicles driven by different drivers and recorded videos during
driving in Beijing on different days. More than 55 hours of
videos were collected and 133,235 frames were extracted,
which is more than 20 times of TuSimple Dataset. We have
divided the dataset into 88880 for training set, 9675 for vali-
dation set, and 34680 for test set. These images were undis-
torted using tools in (Scaramuzza, Martinelli, and Siegwart
2006) and have a resolution of 1640 × 590. Fig. 2 (a) shows
some examples, which comprises urban, rural, and highway
scenes. As one of the largest and most crowded cities in the
world, Beijing provides many challenging trafﬁc scenarios
for lane detection. We divided the test set into normal and
8 challenging categories, which correspond to the 9 exam-
ples in Fig. 2 (a). Fig. 2 (b) shows the proportion of each
scenario. It can be seen that the 8 challenging scenarios ac-
count for most (72.3%) of the dataset.

For each frame, we manually annotate the trafﬁc lanes
with cubic splines. As mentioned earlier, in many cases lane
markings are occluded by vehicles or are unseen. In real ap-
plications it is important that lane detection algorithms could
estimate lane positions from the context even in these chal-
lenging scenarios that occur frequently. Therefore, for these
cases we still annotate the lanes according to the context, as
shown in Fig. 2 (a) (2)(4). We also hope that our algorithm
could distinguish barriers on the road, like the one in Fig. 2
(a) (1). Thus the lanes on the other side of the barrier are not
annotated. In this paper we focus our attention on the detec-
tion of four lane markings, which are paid most attention to
in real applications. Other lane markings are not annotated.

Spatial CNN
Traditional methods to model spatial relationship are based
on Markov Random Fields (MRF) or Conditional Ran-

Figure 3: (a) MRF/CRF based method. (b) Our implementation of Spatial CNN. MRF/CRF are theoretically applied to unary
potentials whose channel number equals to the number of classes to be classiﬁed, while SCNN could be applied to the top
hidden layers with richer information.

dom Fields (CRF) (Kr¨ahenb¨uhl and Koltun 2011). Recent
works (Zheng et al. 2015; Liu et al. 2015; Chen et al. 2017)
to combine them with CNN all follow the pipeline of Fig. 3
(a), where the mean ﬁeld algorithm can be implemented with
neural networks. Speciﬁcally, the procedure is (1) Normal-
ize: the output of CNN is viewed as unary potentials and is
normalized by the Softmax operation, (2) Message Passing,
which could be realized by channel wise convolution with
large kernels (for dense CRF, the kernel size would cover
the whole image and the kernel weights are dependent on the
input image), (3) Compatibility Transform, which could be
implemented with a 1 × 1 convolution layer, and (4) Adding
unary potentials. This process is iterated for N times to give
the ﬁnal output.

It can be seen that in the message passing process of
traditional methods, each pixel receives information from
all other pixels, which is very computational expensive and
hard to be used in real time tasks as in autonomous driving.
For MRF, the large convolution kernel is hard to learn and
usually requires careful initialization (Tompson et al. 2014;
Liu et al. 2015). Moreover, these methods are applied to the
output of CNN, while the top hidden layer, which comprises
richer information, might be a better place to model spatial
relationship.

To address these issues, and to more efﬁciently learn the
spatial relationship and the smooth, continuous prior of lane
markings, or other structured object in the driving scenario,
we propose Spatial CNN. Note that the ’spatial’ here is
not the same with that in ’spatial convolution’, but denotes
propagating spatial information via specially designed CNN
structure.

As shown in the ’SCNN D’ module of Fig. 3 (b), consid-
ering a SCNN applied on a 3-D tensor of size C × H × W ,
where C, H, and W denote the number of channel, rows, and
columns respectively. The tensor would be splited into H
slices, and the ﬁrst slice is then sent into a convolution layer

with C kernels of size C ×w, where w is the kernel width. In
a traditional CNN the output of a convolution layer is then
fed into the next layer, while here the output is added to the
next slice to provide a new slice. The new slice is then sent to
the next convolution layer and this process would continue
until the last slice is updated.

Speciﬁcally, assume we have a 3-D kernel tensor K with
element Ki,j,k denoting the weight between an element in
channel i of the last slice and an element in channel j of
the current slice, with an offset of k columes between two
elements. Also denote the element of input 3-D tensor X as
Xi,j,k, where i, j, and k indicate indexes of channel, row,
and column respectively. Then the forward computation of
SCNN is:

X (cid:48)

i,j,k =






Xi,j,k,
Xi,j,k + f (cid:0) (cid:80)

(cid:80)
n

m

X (cid:48)
(cid:1),

×Km,i,n

j = 1

m, j − 1, k + n − 1

(1)

j = 2, 3, ..., H

where f is a nonlinear activation function as ReLU. The X
with superscript (cid:48) denotes the element that has been updated.
Note that the convolution kernel weights are shared across
all slices, thus SCNN is a kind of recurrent neural network.
Also note that SCNN has directions. In Fig. 3 (b), the four
’SCNN’ module with sufﬁx ’D’, ’U’, ’R’, ’L’ denotes SCNN
that is downward, upward, rightward, and leftward respec-
tively.

Analysis
There are three main advantages of Spatial CNN over tradi-
tional methods, which are concluded as follows.

(1) Computational efﬁciency. As show in Fig. 4, in dense
MRF/CRF each pixel receives messages from all other pix-
els directly, which could have much redundancy, while in
SCNN message passing is realized in a sequential propaga-
tion scheme. Speciﬁcally, assume a tensor with H rows and

Figure 4: Message passing directions in (a) dense MRF/CRF
and (b) Spatial CNN (rightward). For (a), only message
passing to the inner 4 pixels are shown for clearance.

W columns, then in dense MRF/CRF, there is message pass
between every two of the W H pixels. For niter iterations,
the number of message passing is niterW 2H 2. In SCNN,
each pixel only receive information from w pixels, thus the
number of message passing is ndirW Hw, where ndir and w
denotes the number of propagation directions in SCNN and
the kernel width of SCNN respectively. niter could range
from 10 to 100, while in this paper ndir is set to 4, cor-
responding to 4 directions, and w is usually no larger than
10 (in the example in Fig. 4 (b) w = 3). It can be seen
that for images with hundreds of rows and columns, SCNN
could save much computations, while each pixel still could
receive messages from all other pixels with message propa-
gation along 4 directions.

(2) Message as residual. In MRF/CRF, message passing
is achieved via weighted sum of all pixels, which, according
to the former paragraph, is computational expensive. And
recurrent neural network based methods might suffer from
gradient descent (Pascanu, Mikolov, and Bengio 2013), con-
sidering so many rows or columns. However, deep residual
learning (He et al. 2016) has shown its capability to easy
the training of very deep neural networks. Similarly, in our
deep SCNN messages are propagated as residual, which is
the output of ReLU in Eq.(1). Such residual could also be
viewed as a kind of modiﬁcation to the original neuron.
As our experiments will show, such message pass scheme
achieves better results than LSTM based methods.

(3) Flexibility Thanks to the computational efﬁciency of
SCNN, it could be easily incorporated into any part of a
CNN, rather than just output. Usually, the top hidden layer
contains information that is both rich and of high semantics,
thus is an ideal place to apply SCNN. Typically, Fig. 3 shows
our implementation of SCNN on the LargeFOV (Chen et al.
2017) model. SCNNs on four spatial directions are added
sequentially right after the top hidden layer (’fc7’ layer) to
introduce spatial message propagation.

Experiment
We evaluate SCNN on our lane detection dataset and
Cityscapes (Cordts et al. 2016). In both tasks, we train the
models using standard SGD with batch size 12, base learn-
ing rate 0.01, momentum 0.9, and weight decay 0.0001. The
learning rate policy is ”poly” with power and iteration num-
ber set to 0.9 and 60K respectively. Our models are modiﬁed
based on the LargeFOV model in (Chen et al. 2017). The ini-

Figure 5: (a) Training model, (b) Lane prediction process.
’Conv’,’HConv’, and ’FC’ denotes convolution layer, atrous
convolution layer (Chen et al. 2017), and fully connected
layer respectively. ’c’, ’w’, and ’h’ denotes number of output
channels, kernel width, and ’rate’ for atrous convolution.

tial weights of the ﬁrst 13 convolution layers are copied from
VGG16 (Simonyan and Zisserman 2015) trained on Ima-
geNet (Deng et al. 2009). All experiments are implemented
on the Torch7 (Collobert, Kavukcuoglu, and Farabet 2011)
framework.

Lane Detection
Lane detection model Unlike common object detection
task that only requires bounding boxes, lane detection re-
quires precise prediction of curves. A natural idea is that the
model should output probability maps (probmaps) of these
curves, thus we generate pixel level targets to train the net-
works, like in semantic segmentation tasks. Instead of view-
ing different lane markings as one class and do clustering
afterwards, we want the neural network to distinguish dif-
ferent lane markings on itself, which could be more robust.
Thus these four lanes are viewed as different classes. More-
over, the probmaps are then sent to a small network to give
prediction on the existence of lane markings.

During testing, we still need to go from probmaps to
curves. As shown in Fig.5 (b), for each lane marking whose
existence value is larger than 0.5, we search the correspond-
ing probmap every 20 rows for the position with the high-
est response. These positions are then connected by cubic
splines, which are the ﬁnal predictions.

As shown in Fig.5 (a), the detailed differences between
our baseline model and LargeFOV are: (1) the output chan-
nel number of the ’fc7’ layer is set to 128, (2) the ’rate’ for
the atrous convolution layer of ’fc6’ is set to 4, (3) batch nor-
malization (Ioffe and Szegedy 2015) is added before each
ReLU layer, (4) a small network is added to predict the ex-
istence of lane markings. During training, the line width of
the targets is set to 16 pixels, and the input and target images
are rescaled to 800 × 288. Considering the imbalanced label

Figure 6: Evaluation based on IoU. Green lines denote
ground truth, while blue and red lines denote TP and FP re-
spectively.

between background and lane markings, the loss of back-
ground is multiplied by 0.4.

Evaluation In order to judge whether a lane marking is
successfully detected, we view lane markings as lines with
widths equal to 30 pixel and calculate the intersection-
over-union (IoU) between the ground truth and the predic-
tion. Predictions whose IoUs are larger than certain thresh-
old are viewed as true positives (TP), as shown in Fig. 6.
Here we consider 0.3 and 0.5 thresholds corresponding to
loose and strict evaluations. Then we employ F-measure =
(1 + β2)
β2Precision+Recall as the ﬁnal evaluation index, where
Precision = T P
T P +F P and Recall = T P
T P +F N . Here β is set
to 1, corresponding to harmonic mean (F1-measure).

Precision Recall

Ablation Study
In section 2.2 we propose Spatial CNN to
enable spatial message propagation. To verify our method,
we will make detailed ablation studies in this subsection.
Our implementation of SCNN follows that shown in Fig. 3.
(1) Effectiveness of multidirectional SCNN. Firstly, we in-
vestigate the effects of directions in SCNN. We try SCNN
that has different direction implementations, the results are
shown in Table. 1. Here the kernel width w of SCNN is set to
5. It can be seen that the performance increases as more di-
rections are added. To prove that the improvement does not
result from more parameters but from the message passing
scheme brought about by SCNN, we add an extra convolu-
tion layer with 5×5 kernel width after the top hidden layer of
the baseline model and compare with our method. From the
results we can see that extra convolution layer could merely
bring about little improvement, which veriﬁes the effective-
ness of SCNN.

Table 1: Experimental results on SCNN with different di-
rectional settings. F1 denotes F1-measure, and the value in
the bracket denotes the IoU threshold. The sufﬁx ’D’, ’U’,
’R’, ’L’ denote downward, upward, rightward, and leftward
respectively.
Models

Baseline ExtraConv SCNN D SCNN DU SCNN DURL

F1 (0.3)
F1 (0.5)

77.7
63.2

77.6
64.0

79.5
68.6

79.9
69.4

80.2
70.4

(2) Effects of kernel width w. We further try SCNN with
different kernel width based on the ”SCNN DURL” model,
as shown in Table. 2. Here the kernel width denotes the num-
ber of pixels that a pixel could receive messages from, and
the w = 1 case is similar to the methods in (Visin et al. 2015;
Bell et al. 2016). The results show that larger w is beneﬁcial,

and w = 9 gives a satisfactory result, which surpasses the
baseline by a signiﬁcant margin 8.4% and 3.2% correspond-
ing to different IoU threshold.

Table 2: Experimental results on SCNN with different kernel
widths.

Kernel width w

1

F1 (0.3)
F1 (0.5)

3

79.5
68.9

5

80.2
70.4

7

80.5
71.2

9

80.9
71.6

11

80.6
71.7

78.5
66.3

(3) Spatial CNN on different positions. As mentioned ear-
lier, SCNN could be added to any place of a neural network.
Here we consider the SCNN DURL model applied on (1)
output and (2) the top hidden layer, which correspond to
Fig. 3. The results in Table. 3 indicate that the top hidden
layer, which comprises richer information than the output,
turns out to be a better position to apply SCNN.

Table 3: Experimental results on spatial CNN at different
positions, with w = 9.

Position Output

Top hidden layer

F1 (0.3)
F1 (0.5)

79.9
68.8

80.9
71.6

(4) Effectiveness of sequential propagation. In our SCNN,
information is propagated in a sequential way, i.e., a slice
does not pass information to the next slice until it has re-
ceived information from former slices. To verify the effec-
tiveness of this scheme, we compare it with parallel prop-
agation, i.e., each slice passes information to the next slice
simultaneously before being updated. For this parallel case,
the (cid:48) in the right part of Eq.(1) is removed. As Table. 4
shows, the sequential message passing scheme outperforms
the parallel scheme signiﬁcantly. This result indicates that
in SCNN, a pixel does not merely affected by nearby pixels,
but do receive information from further positions.

Table 4: Comparison between sequential and parallel mes-
sage passing scheme, for SCNN DULR with w = 9.
Sequential

Message passing scheme

Parallel

F1 (0.3)
F1 (0.5)

78.4
65.2

80.9
71.6

(5) Comparison with state-of-the-art methods. To fur-
ther verify the effectiveness of SCNN in lane detec-
tion, we compare it with several methods: the rnn based
ReNet (Visin et al. 2015), the MRF based MRFNet, the
DenseCRF (Kr¨ahenb¨uhl and Koltun 2011), and the very
deep residual network (He et al. 2016). For ReNet based
on LSTM, we replace the ”SCNN” layers in Fig. 3 with
two ReNet layers: one layer to pass horizontal information
and the other to pass vertical information. For DenseCRF,
we use dense CRF as post-processing and employ 10 mean
ﬁeld iterations as in (Chen et al. 2017). For MRFNet, we use

Table 5: Comparison with other methods, with IoU threshold=0.5. For crossroad, only FP are shown.

Category

Baseline

ReNet DenseCRF MRFNet ResNet-50 ResNet-101 Baseline+SCNN

Normal
Crowded
Night
No line
Shadow
Arrow
Dazzle light
Curve
Crossroad
Total

83.1
61.0
56.9
34.0
54.7
74.0
49.9
61.0
2060
63.2

83.3
60.5
56.3
34.5
55.0
74.1
48.2
59.9
2296
62.9

81.3
58.8
54.2
31.9
56.3
71.2
46.2
57.8
2253
61.0

86.3
65.2
61.3
37.2
59.3
76.9
53.7
62.3
1837
67.0

87.4
64.1
60.6
38.1
60.7
79.0
54.1
59.8
2505
66.7

90.2
68.2
65.9
41.7
64.6
84.0
59.8
65.5
2183
70.8

90.6
69.7
66.1
43.4
66.9
84.1
58.5
64.4
1990
71.6

Figure 7: Comparison between probmaps of baseline, ReNet, MRFNet, ResNet-101, and SCNN.

the implementation in Fig. 3 (a), with iteration times and
message passing kernel size set to 10 and 20 respectively.
The main difference of the MRF here with CRF is that the
weights of message passing kernels are learned during train-
ing rather than depending on the image. For ResNet, our
implementation is the same with (Chen et al. 2017) except
that we do not use the ASPP module. For SCNN, we add
SCNN DULR module to the baseline, and the kernel width
w is 9. The test results on different scenarios are shown in
Table 5, and visualizations are given in Fig. 7.

From the results, we can see that the performance of
ReNet is not even comparable with SCNN DULR with w =
1, indicating the effectiveness of our residual message pass-
ing scheme. Interestingly, DenseCRF leads to worse result
here, because lane markings usually have less appearance
clues so that dense CRF cannot distinguish lane markings
and background. In contrast, with kernel weights learned
from data, MRFNet could to some extent smooth the results
and improve performance, as Fig. 7 shows, but are still not
very satisfactory. Furthermore, our method even outperform
the much deeper ResNet-50 and ResNet-101. Despite the
over a hundred layers and the very large receptive ﬁeld of
ResNet-101, it still gives messy or discontinuous outputs in
challenging cases, while our method, with only 16 convolu-
tion layers plus 4 SCNN layers, could preserve the smooth-
ness and continuity of lane lines better. This demonstrates
the much stronger capability of SCNN to capture structure
prior of objects over traditional CNN.

(6) Computational efﬁciency over other methods. In the
Analysis section we give theoretical analysis on the com-
putational efﬁciency of SCNN over dense CRF. To verify

this, we compare their runtime experimentally. The results
are shown in Table. 6, where the runtime of the LSTM in
ReNet is also given. Here the runtime does not include run-
time of the backbone network. For SCNN, we test both the
practical case and the case with the same setting as dense
CRF. In the practical case, SCNN is applied on top hidden
layer, thus the input has more channels but less hight and
width. In the fair comparison case, the input size is modiﬁed
to be the same with that in dense CRF, and both methods are
tested on CPU. The results show that even in fair comparison
case, SCNN is over 4 times faster than dense CRF, despite
the efﬁcient implementation of dense CRF in (Kr¨ahenb¨uhl
and Koltun 2011). This is because SCNN signiﬁcantly re-
duces redundancy in message passing, as in Fig. 4. Also,
SCNN is more efﬁcient than LSTM, whose gate mechanism
requires more computation.

Table 6: Runtime of dense CRF, LSTM, and SCNN. The two
SCNNs correspond to the one used in practice and the one
whose input size is modiﬁed for fair comparison with dense
CRF respectively. The kernel width w of SCNN is 9.

Method

dense CRF

LSTM

SCNN DULR
(in practice)

SCNN DULR
(fair comparison)

5×288×800

128×36×100

128×36×100

5×288×800

Input size
(C × H × W )
Device
Runtime (ms)

CPU2
737

GPU3
115

GPU
42

CPU
176

2

3

Intel Core i7-4790K CPU
GeForce GTX TITAN Black

Table 7: Results on Cityscapes validation set.

trafﬁc
light

trafﬁc
sign

Method

road terrain building wall

car

pole

fence

sidewalk sky

rider person vegetation truck

bus

train motor bicycle mIoU

LargeFOV
LargeFOV+SCNN

97.0
97.0

ResNet-101
98.3
ResNet-101+SCNN 98.3

59.2
59.8

64.2
65.4

89.9
90.3

92.4
92.6

42.2
45.7

44.5
46.7

92.3 52.9
92.5 55.2

94.9 66.0
94.8 66.1

62.3
62.3

74.5
74.3

71.1
71.7

82.1
81.5

52.2
52.5

59.9
61.2

78.8
78.1

86.0
86.1

92.2
92.6

94.7
94.7

52.1
53.2

65.5
65.5

75.9
76.4

84.1
84.0

91.0
91.1

92.7
92.7

48.8
55.6

57.3
57.7

70.2 37.6
71.2 41.7

81.1 54.0
82.0 59.9

54.6
56.2

64.5
67.0

72.3
72.3

80.0
80.1

68.0
69.2

75.6
76.4

Figure 8: Visual improvements on Cityscapes validation set. For each example, from left to right are: input image, ground truth,
result of LargeFOV, result of LargeFOV+SCNN.

Semantic Segmentation on Cityscapes

To demonstrate the generality of our method, we also
evaluate Spatial CNN on Cityscapes (Cordts et al. 2016).
Cityscapes is a standard benchmark dataset for semantic
segmentation on urban trafﬁc scenes. It contains 5000 ﬁne
annotated images, including 2975 for training, 500 for val-
idation and 1525 for testing. 19 categories are deﬁned in-
cluding both stuff and objects. We use two classic mod-
els, the LargeFOV and ResNet-101 in DeepLab (Chen et al.
2017) as the baselines. Batch normalization layers (Ioffe and
Szegedy 2015) are added to LargeFOV to enable faster con-
vergence. For both models, the channel numbers of the top
hidden layers are modiﬁed to 128 to make them compacter.
We add SCNN to the baseline models in the same way as
in lane detection. The comparisons between baselines and
those combined with the SCNN DURL models with kernel
width w = 9 are shown in Table 7. It can be seen that SCNN
could also improve semantic segmentation results. With SC-
NNs added, the IoUs for all classes are at least comparable
to the baselines, while the ”wall”, ”pole”, ”truck”, ”bus”,
”train”, and ”motor” categories achieve signiﬁcant improve.
This is because for long shaped objects like train and pole,
SCNN could capture its continuous structure and connect
the disconnected part, as shown in Fig. 8. And for wall,
truck, and bus which could occupy large image area, the dif-
fusion effect of SCNN could correct the part that are mis-
classiﬁed according to the context. This shows that SCNN
is useful not only for long thin structure, but also for large
objects which require global information to be classiﬁed
correctly. There is another interesting phenomenon that the
head of the vehicle at the bottom of the images, whose label
is ignored during training, is in a mess in LargeFOV while
with SCNN added it is classiﬁed as road. This is also due to
the diffusion effects of SCNN, which passes the information
of road to the vehicle head area.

To compare our method with other MRF/CRF based

Table 8: Comparison between our SCNN and other
MRF/CRF based methods on Cityscapes test set.

Method

LargeFOV
(Chen et al. 2017)

DPN
(Liu et al. 2015)

mIoU

63.1

66.8

Ours

68.2

methods, we evaluate LargeFOV+SCNN on Cityscapes test
set, and compare with methods that also use VGG16 (Si-
monyan and Zisserman 2015) as the backbone network. The
results are shown in Table 8. Here LargeFOV, DPN, and our
method use dense CRF, dense MRF, and SCNN respectively,
and share nearly the same base CNN part. The results show
that our method achieves signiﬁcant better performance.

Conclusion
In this paper, we propose Spatial CNN, a CNN-like scheme
to achieve effective information propagation in the spatial
level. SCNN could be easily incorporated into deep neu-
ral networks and trained end-to-end. It is evaluated at two
tasks in trafﬁc scene understanding: lane detection and se-
mantic segmentation. The results show that SCNN could ef-
fectively preserve the continuity of long thin structure, while
in semantic segmentation its diffusion effects is also proved
to be beneﬁcial for large objects. Speciﬁcally, by introduc-
ing SCNN into the LargeFOV model, our 20-layer network
outperforms ReNet, MRF, and the very deep ResNet-101 in
lane detection. Last but not least, we believe that the large
challenging lane detection dataset we presented would push
forward researches on autonomous driving.

Acknowledgments
This work is supported by SenseTime Group Limited. We
would like to thank Xiaohang Zhan, Jun Li, and Xudong
Cao for helpful work in building the lane detection dataset.

Scaramuzza, D.; Martinelli, A.; and Siegwart, R. 2006. A ﬂexi-
ble technique for accurate omnidirectional camera calibration and
structure from motion. In Computer Vision Systems, 2006 ICVS’06.
IEEE International Conference on, 45–45. IEEE.
Simonyan, K., and Zisserman, A. 2015. Very deep convolutional
networks for large-scale image recognition. In ICLR.
Son, J.; Yoo, H.; Kim, S.; and Sohn, K. 2015. Real-time illumi-
nation invariant lane detection for lane departure warning system.
Expert Systems with Applications 42(4):1816–1824.
Tompson, J. J.; Jain, A.; LeCun, Y.; and Bregler, C. 2014. Joint
training of a convolutional network and a graphical model for hu-
man pose estimation. In NIPS.
TuSimple. 2017. Tusimple benchmark. http://benchmark.
tusimple.ai/#/.
Urmson, C.; Anhalt, J.; Bagnell, D.; Baker, C.; Bittner, R.; Clark,
M.; Dolan, J.; Duggins, D.; Galatali, T.; Geyer, C.; et al. 2008.
Autonomous driving in urban environments: Boss and the urban
challenge. Journal of Field Robotics 25(8):425–466.
Visin, F.; Kastner, K.; Cho, K.; Matteucci, M.; Courville, A.; and
Bengio, Y. 2015. Renet: A recurrent neural network based alterna-
tive to convolutional networks. arXiv preprint arXiv:1505.00393.
Zheng, S.; Jayasumana, S.; Romera-Paredes, B.; Vineet, V.; Su, Z.;
Du, D.; Huang, C.; and Torr, P. H. 2015. Conditional random ﬁelds
as recurrent neural networks. In ICCV.

Imagenet: A large-scale hierarchical image database.

References
Aly, M. 2008. Real time detection of lane markers in urban streets.
In Intelligent Vehicles Symposium, 2008 IEEE, 7–12. IEEE.
Bar Hillel, A.; Lerner, R.; Levi, D.; and Raz, G. 2014. Recent
progress in road and lane detection: a survey. Machine vision and
applications 1–19.
Bell, S.; Lawrence Zitnick, C.; Bala, K.; and Girshick, R. 2016.
Inside-outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR.
Brostow, G. J.; Shotton, J.; Fauqueur, J.; and Cipolla, R. 2008.
Segmentation and recognition using structure from motion point
clouds. In ECCV.
Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and Yuille,
A. L. 2017. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected crfs.
TPAMI.
Chu, X.; Ouyang, W.; Wang, X.; et al. 2016. Crf-cnn: Modeling
structured information in human pose estimation. In NIPS.
Collobert, R.; Kavukcuoglu, K.; and Farabet, C. 2011. Torch7: A
matlab-like environment for machine learning. In BigLearn, NIPS
Workshop, number EPFL-CONF-192376.
Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler, M.;
Benenson, R.; Franke, U.; Roth, S.; and Schiele, B. 2016. The
cityscapes dataset for semantic urban scene understanding.
In
CVPR.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L.
2009.
In
CVPR.
Fritsch, J.; Kuhnl, T.; and Geiger, A. 2013. A new performance
measure and evaluation benchmark for road detection algorithms.
In Intelligent Transportation Systems-(ITSC), 2013 16th Interna-
tional IEEE Conference on, 1693–1700. IEEE.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
ing for image recognition. In CVPR.
Huval, B.; Wang, T.; Tandon, S.; Kiske, J.; Song, W.; Pazhayam-
pallil, J.; Andriluka, M.; Rajpurkar, P.; Migimatsu, T.; Cheng-Yue,
R.; et al. 2015. An empirical evaluation of deep learning on high-
way driving. arXiv preprint arXiv:1504.01716.
Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerat-
ing deep network training by reducing internal covariate shift. In
ICML.
Jung, S.; Youn, J.; and Sull, S. 2016. Efﬁcient lane detection based
on spatiotemporal images. IEEE Transactions on Intelligent Trans-
portation Systems 17(1):289–295.
Kr¨ahenb¨uhl, P., and Koltun, V. 2011. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In NIPS.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In NIPS.
Liang, X.; Shen, X.; Feng, J.; Lin, L.; and Yan, S. 2016a. Semantic
object parsing with graph lstm. In ECCV.
Liang, X.; Shen, X.; Xiang, D.; Feng, J.; Lin, L.; and Yan, S. 2016b.
Semantic object parsing with local-global long short-term memory.
In CVPR.
Liu, Z.; Li, X.; Luo, P.; Loy, C.-C.; and Tang, X. 2015. Semantic
image segmentation via deep parsing network. In ICCV.
Long, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convolutional
networks for semantic segmentation. In CVPR.
Pascanu, R.; Mikolov, T.; and Bengio, Y. 2013. On the difﬁculty
of training recurrent neural networks. In ICML.

