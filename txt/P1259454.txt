7
1
0
2
 
v
o
N
 
7
1
 
 
]

G
L
.
s
c
[
 
 
3
v
6
0
4
4
0
.
5
0
5
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2017) 1-67

Submitted 12/15; Revised 12/16; Published 10/17

Hinge-Loss Markov Random Fields
and Probabilistic Soft Logic

Stephen H. Bach
Computer Science Department
Stanford University
Stanford, CA 94305, USA

Matthias Broecheler
DataStax

Bert Huang
Computer Science Department
Virginia Tech
Blacksburg, VA 24061, USA

Lise Getoor
Computer Science Department
University of California, Santa Cruz
Santa Cruz, CA 95064, USA

Editor: Luc De Raedt

bach@cs.stanford.edu

matthias@datastax.com

bhuang@vt.edu

getoor@soe.ucsc.edu

Abstract

A fundamental challenge in developing high-impact machine learning technologies is bal-
ancing the need to model rich, structured domains with the ability to scale to big data.
Many important problem areas are both richly structured and large scale, from social and
biological networks, to knowledge graphs and the Web, to images, video, and natural lan-
guage. In this paper, we introduce two new formalisms for modeling structured data, and
show that they can both capture rich structure and scale to big data. The ﬁrst, hinge-
loss Markov random ﬁelds (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes diﬀerent approaches to convex inference. We unite three approaches from
the randomized algorithms, probabilistic graphical models, and fuzzy logic communities,
showing that all three lead to the same inference objective. We then deﬁne HL-MRFs
by generalizing this uniﬁed objective. The second new formalism, probabilistic soft logic
(PSL), is a probabilistic programming language that makes HL-MRFs easy to deﬁne using
a syntax based on ﬁrst-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than general-purpose
convex optimization methods, because it uses message passing to take advantage of sparse
dependency structures. We then show how to learn the parameters of HL-MRFs. The
learned HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at
scales not previously possible.

Keywords:
prediction

Probabilistic graphical models, statistical relational learning, structured

c(cid:13)2017 Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/15-631.html.

Bach, Broecheler, Huang, and Getoor

1. Introduction

In many problems in machine learning, the domains are rich and structured, with many
interdependent elements that are best modeled jointly. Examples include social networks,
biological networks, the Web, natural language, computer vision, sensor networks, and so on.
Machine learning subﬁelds such as statistical relational learning (Getoor and Taskar, 2007),
inductive logic programming (Muggleton and De Raedt, 1994), and structured prediction
(Bakir et al., 2007) all seek to represent dependencies in data induced by relational structure.
With the ever-increasing size of available data, there is a growing need for models that are
highly scalable while still able to capture rich structure.

In this paper, we introduce hinge-loss Markov random ﬁelds (HL-MRFs), a new class of
probabilistic graphical models designed to enable scalable modeling of rich, structured data.
HL-MRFs are analogous to discrete MRFs, which are undirected probabilistic graphical
models in which probability mass is log-proportional to a weighted sum of feature functions.
Unlike discrete MRFs, however, HL-MRFs are deﬁned over continuous variables in the [0, 1]
unit interval. To model dependencies among these continuous variables, we use linear and
quadratic hinge functions, so that probability density is lost according to a weighted sum of
hinge losses. As we will show, hinge-loss features capture many common modeling patterns
for structured data.

When designing classes of models, there is generally a trade oﬀ between scalability and
expressivity: the more complex the types and connectivity structure of the dependencies,
the more computationally challenging inference and learning become. HL-MRFs address
a crucial gap between the two extremes. By using hinge-loss functions to model the de-
pendencies among the variables, which admit highly scalable inference without restrictions
on their connectivity structure, HL-MRFs can capture a wide range of useful relationships.
One reason they are so expressive is that hinge-loss dependencies are at the core of a number
of scalable techniques for modeling both discrete and continuous structured data.

To motivate HL-MRFs, we unify three diﬀerent approaches for scalable inference in
structured models: (1) randomized algorithms for MAX SAT (Goemans and Williamson,
1994), (2) local consistency relaxation (Wainwright and Jordan, 2008) for discrete Markov
random ﬁelds deﬁned using Boolean logic, and (3) reasoning about continuous information
with fuzzy logic. We show that all three approaches lead to the same convex programming
objective. We then deﬁne HL-MRFs by generalizing this uniﬁed inference objective as a
weighted sum of hinge-loss features and using them as the weighted features of graphical
models. Since HL-MRFs generalize approaches that reason about relational data with
weighted logical knowledge bases, they retain the same high level of expressivity. As we
show in Section 6.4, they are eﬀective for modeling both discrete and continuous data.

We also introduce probabilistic soft logic (PSL), a new probabilistic programming lan-
guage that makes HL-MRFs easy to deﬁne and use for large, relational data sets.1 This
idea has been explored for other classes of models, such as Markov logic networks (Richard-
son and Domingos, 2006) for discrete MRFs, relational dependency networks (Neville and
Jensen, 2007) for dependency networks, and probabilistic relational models (Getoor et al.,
2002) for Bayesian networks. We build on these previous approaches, as well as the con-
nection between hinge-loss potentials and logical clauses, to deﬁne PSL. In addition to

1. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

2

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

probabilistic rules, PSL provides syntax that enables users to easily apply many common
modeling techniques, such as domain and range constraints, blocking and canopy functions,
and aggregate variables deﬁned over other random variables.

Our next contribution is to introduce a number of inference and learning algorithms.
First, we examine MAP inference, i.e., the problem of ﬁnding a most probable assignment
to the unobserved random variables. MAP inference in HL-MRFs is always a convex op-
timization. Although any oﬀ-the-shelf optimization toolkit could be used, such methods
typically do not leverage the sparse dependency structures common in graphical models.
We introduce a consensus-optimization approach to MAP inference for HL-MRFs, showing
how the problem can be decomposed using the alternating direction method of multipliers
(ADMM) and how the resulting subproblems can be solved analytically for hinge-loss poten-
tials. Our approach enables HL-MRFs to easily scale beyond the capabilities of oﬀ-the-shelf
optimization software or sampling-based inference in discrete MRFs. We then show how
to learn HL-MRFs from training data using a variety of methods: structured perceptron,
maximum pseudolikelihood, and large margin estimation. Since structured perceptron and
large margin estimation rely on inference as subroutines, and maximum pseudolikelihood
estimation is eﬃcient by design, all of these methods are highly scalable for HL-MRFs. We
evaluate them on core relational learning and structured prediction tasks, such as collec-
tive classiﬁcation and link prediction. We show that HL-MRFs oﬀer predictive accuracy
comparable to analogous discrete models while scaling much better to large data sets.

This paper brings together and expands work on scalable models for structured data that
can be either discrete, continuous, or a mixture of both (Broecheler et al., 2010a; Bach et al.,
2012, 2013, 2015b). The eﬀectiveness of HL-MRFs and PSL has been demonstrated on many
problems, including information extraction (Liu et al., 2016) and automatic knowledge base
construction (Pujara et al., 2013), extracting and evaluating natural-language arguments
on the Web (Samadi et al., 2016), high-level computer vision (London et al., 2013), drug
discovery (Fakhraei et al., 2014) and predicting drug-drug interactions (Sridhar et al., 2016),
natural language semantics (Beltagy et al., 2014; Sridhar et al., 2015; Deng and Wiebe,
2015; Ebrahimi et al., 2016), automobile-traﬃc modeling (Chen et al., 2014), recommender
systems (Kouki et al., 2015), information retrieval (Alshukaili et al., 2016), and predicting
attributes (Li et al., 2014) and trust (Huang et al., 2013; West et al., 2014) in social networks.
The ability to easily incorporate latent variables into HL-MRFs and PSL (Bach et al., 2015a)
has enabled further applications, including modeling latent topics in text (Foulds et al.,
2015), and predicting student outcomes in massive open online courses (MOOCs) (Ramesh
et al., 2014, 2015). Researchers have also studied how to make HL-MRFs and PSL even
more scalable by developing distributed implementations (Miao et al., 2013; Magliacane
et al., 2015). That they are already being widely applied indicates HL-MRFs and PSL
address an open need in the machine learning community.

The paper is organized as follows. In Section 2, we ﬁrst consider models for structured
prediction that are deﬁned using logical clauses. We unify three diﬀerent approaches to
scalable inference in such models, showing that they all optimize the same convex objec-
tive. We then generalize this objective in Section 3 to deﬁne HL-MRFs. In Section 4, we
introduce PSL, specifying the language and giving many examples of common usage. Next
we introduce a scalable message-passing algorithm for MAP inference in Section 5 and a

3

Bach, Broecheler, Huang, and Getoor

number of learning algorithms in Section 6, evaluating them on a range of tasks. Finally,
in Section 7, we discuss related work.

2. Unifying Convex Inference for Logic-Based Graphical Models

In many structured domains, propositional and ﬁrst-order logics are useful tools for describ-
ing the intricate dependencies that connect the unknown variables. However, these domains
are usually noisy; dependencies among the variables do not always hold. To address this,
logical semantics can be incorporated into probability distributions to create models that
capture both the structure and the uncertainty in machine learning tasks. One common
way to do this is to use logic to deﬁne feature functions in a probabilistic model. We focus
on Markov random ﬁelds (MRFs), a popular class of probabilistic graphical models. Infor-
mally, an MRF is a distribution that assigns probability mass using a scoring function that
is a weighted combination of feature functions called potentials. We will use logical clauses
to deﬁne these potentials. We ﬁrst deﬁne MRFs more formally to introduce necessary
notation:

Deﬁnition 1 Let x = (x1, . . . , xn) be a vector of random variables and let φ = (φ1, . . . , φm)
be a vector of potentials where each potential φj(x) assigns conﬁgurations of the variables
a real-valued score. Also, let w = (w1, . . . , wm) be a vector of real-valued weights. Then, a
Markov random ﬁeld is a probability distribution of the form

P (x) ∝ exp

w(cid:62)φ(x)

.

(cid:16)

(cid:17)

In an MRF, the potentials should capture how the domain behaves, assigning higher scores
If a modeler does not know how the
to more probable conﬁgurations of the variables.
domain behaves, the potentials should capture how it might behave, so that a learning
algorithm can ﬁnd weights that lead to accurate predictions. Logic provides an excellent
formalism for deﬁning such potentials in structured and relational domains.

We now introduce some notation to make this logic-based approach more formal. Con-
sider a set of logical clauses C = {C1, . . . , Cm}, i.e., a knowledge base, where each clause
Cj ∈ C is a disjunction of literals and each literal is a variable x or its negation ¬x drawn
from the variables x such that each variable xi ∈ x appears at most once in Cj. Let
j (resp. I −
I +
j ) ⊂ {1, . . . , n} be the set of indices of the variables that are not negated (resp.
negated) in Cj. Then Cj can be written as

Logical clauses of this form are expressive because they can be viewed equivalently as

implications from conditions to consequences:

This “if-then” reasoning is intuitive and can describe many dependencies in structured data.



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(cid:94)

i∈I −
j

xi =⇒

xi .

(cid:95)

i∈I +
j

4

(1)

(2)

(3)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Assuming we have a logical knowledge base C describing a structured domain, we can
embed it in an MRF by deﬁning each potential φj using a corresponding clause Cj. If an
assignment to the variables x satisﬁes Cj, then we let φj(x) equal 1, and we let it equal 0
otherwise. For our subsequent analysis we assume wj ≥ 0 (∀j = 1, . . . , m). The resulting
MRF preserves the structured dependencies described in C but enables much more ﬂexible
modeling. Clauses no longer must always hold, and the model can express uncertainty
over diﬀerent possible worlds. The weights express how strongly the model expects each
corresponding clause to hold; the higher the weight, the more probable that it is true
according to the model.

This notion of embedding weighted, logical knowledge bases in MRFs is an appealing
one. For example, Markov logic (Richardson and Domingos, 2006) is a popular formalism
that induces MRFs from weighted ﬁrst-order knowledge bases. Given a data set, the ﬁrst-
order clauses are grounded using the constants in the data to create the set of propositional
clauses C. Each propositional clause has the weight of the ﬁrst-order clause from which it
was grounded. In this way, a weighted, ﬁrst-order knowledge base can compactly specify
an entire family of MRFs for a structured machine-learning task.

Although we now have a method for easily deﬁning rich, structured models for a wide
range of problems, there is a new challenge: ﬁnding a most probable assignment to the
variables, i.e., MAP inference, is NP-hard (Shimony, 1994; Garey et al., 1976). This means
that (unless P=NP) our only hope for performing tractable inference is to perform it ap-
proximately. Observe that MAP inference for an MRF deﬁned by C is the integer linear
program

arg max
x∈{0,1}n

P (x) ≡ arg max
x∈{0,1}n

w(cid:62)φ(x)

≡ arg max
x∈{0,1}n

(cid:88)

Cj ∈C

wj min

xi +

(1 − xi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






(4)

While this program is intractable, it does admit convex programming relaxations.

In this section, we show how convex programming can be used to perform tractable
inference in MRFs deﬁned by weighted knowledge bases. We ﬁrst discuss in Section 2.1 an
approach developed by Goemans and Williamson (1994) that views MAP inference as an
instance of the classic MAX SAT problem and relaxes it to a convex program from that
perspective. This approach has the advantage of providing strong guarantees on the quality
of the discrete solutions it obtains. However, it has the disadvantage that general-purpose
convex programming toolkits do not scale well to relaxed MAP inference for large graphical
models (Yanover et al., 2006). In Section 2.2 we then discuss a seemingly distinct approach,
local consistency relaxation, with complementary advantages and disadvantages:
it oﬀers
highly scalable message-passing algorithms but comes with no quality guarantees. We then
unite these approaches by proving that they solve equivalent optimization problems with
identical solutions. Then, in Section 2.3, we show that the uniﬁed inference objective is also
equivalent to exact MAP inference if the knowledge base C is interpreted using (cid:32)Lukasiewicz
logic, an inﬁnite-valued logic for reasoning about naturally continuous quantities such as
similarity, vague or fuzzy concepts, and real-valued data.

5

Bach, Broecheler, Huang, and Getoor

That these three interpretations all lead to the same inference objective—whether rea-
soning about discrete or continuous information—is useful. To the best of our knowledge,
we are the ﬁrst to show their equivalence. This equivalence indicates that the same model-
ing formalism, inference algorithms, and learning algorithms can be used to reason scalably
and accurately about both discrete and continuous information in structured domains. We
generalize the uniﬁed inference objective in Section 3.1 to deﬁne hinge-loss MRFs, and in
the rest of the paper we develop a probabilistic programming language and algorithms that
realize the goal of a scalable and accurate framework for structured data, both discrete and
continuous.

2.1 MAX SAT Relaxation

One approach to approximating objective (4) is to use relaxation techniques developed in
the randomized algorithms community for the MAX SAT problem. Formally, the MAX
SAT problem is to ﬁnd a Boolean assignment to a set of variables that maximizes the total
weight of satisﬁed clauses in a knowledge base composed of disjunctive clauses annotated
with nonnegative weights.
In other words, objective (4) is an instance of MAX SAT.
Randomized approximation algorithms can be constructed for MAX SAT by independently
rounding each Boolean variable xi to true with probability pi. Then, the expected weighted
satisfaction ˆwj of a clause Cj is

also known as a (weighted) noisy-or function, and the expected total score ˆW is



ˆwj = wj


1 −

(1 − pi)

(cid:89)

i∈I +
j



pi


 ,

(cid:89)

i∈I −
j



ˆW =

(cid:88)

Cj ∈C

wj


1 −

(cid:89)

i∈I +
j

(1 − pi)



pi


 .

(cid:89)

i∈I −
j

(5)

(6)

Optimizing ˆW with respect to the rounding probabilities would give the exact MAX SAT so-
lution, so this randomized approach has not made the problem any easier yet, but Goemans
and Williamson (1994) showed how to bound ˆW below with a tractable linear program.

To approximately optimize ˆW , associate with each Boolean variable xi a corresponding
continuous variable ˆyi with domain [0, 1]. Then let ˆy(cid:63) be the optimum of the linear program

arg max
ˆy∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

ˆyi +

(1 − ˆyi), 1

.

(7)

Observe that objectives (4) and (7) are of the same form, except that the variables are
relaxed to the unit hypercube in objective (7). Goemans and Williamson (1994) proved
i for all i, then ˆW ≥ .632 Z(cid:63), where Z(cid:63) is the optimal total weight for
that if pi is set to ˆy(cid:63)
the MAX SAT problem. If each pi is set using any function in a special class, then this

6

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

lower bound improves to a .75 approximation. One simple example of such a function is
1
2

ˆy(cid:63)
i +

pi =

1
4

.

(8)

In this way, objective (7) leads to an expected .75 approximation of the MAX SAT solution.
The following method of conditional probabilities (Alon and Spencer, 2008) can ﬁnd a
single Boolean assignment that achieves at least the expected score from a set of rounding
probabilities, and therefore at least .75 of the MAX SAT solution when objective (7) and
function (8) are used to obtain them. Each variable xi is greedily set to the value that
maximizes the expected weight over the unassigned variables, conditioned on either possible
value of xi and the previously assigned variables. This greedy maximization can be applied
quickly because, in many models, variables only participate in a small fraction of the clauses,
making the change in expectation quick to compute for each variable. Speciﬁcally, referring
to the deﬁnition of ˆW (6), the assignment to xi only needs to maximize over the clauses Cj
in which xi participates, i.e., i ∈ I +

j , which is usually a small set.

j ∪ I −

This approximation is powerful because it is a tractable linear program that comes
with strong guarantees on solution quality. However, even though it is tractable, general-
purpose convex optimization toolkits do not scale well to large MAP problems.
In the
following subsection, we unify this approximation with a complementary one developed in
the probabilistic graphical models community.

2.2 Local Consistency Relaxation

Another approach to approximating objective (4) is to apply a relaxation developed for
Markov random ﬁelds called local consistency relaxation (Wainwright and Jordan, 2008).
This approach starts by viewing MAP inference as an equivalent optimization over marginal
probabilities.2 For each φj ∈ φ, let θj be a marginal distribution over joint assignments xj.
For example, θj(xj) is the probability that the subset of variables associated with potential
φj is in a particular joint state xj. Also, let xj(i) denote the setting of the variable with
index i in the state xj.

With this variational formulation, inference can be relaxed to an optimization over the
ﬁrst-order local polytope L. Let µ = (µ1, . . . , µn) be a vector of probability distributions,
where µi(k) is the marginal probability that xi is in state k. The ﬁrst-order local polytope
is




L (cid:44)

(θ, µ) ≥ 0

(cid:80)

(cid:80)

xj |xj (i)=k θj(xj) = µi(k) ∀i, j, k
∀j

θj(xj) = 1




,



xj
(cid:80)Ki−1
k=0 µi(k) = 1
which constrains each marginal distribution θj over joint states xj to be consistent only
with the marginal distributions µ over individual variables that participate in the potential
φj.



∀i

MAP inference can then be approximated with the ﬁrst-order local consistency relax-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ation:

(9)

(10)

arg max
(θ,µ)∈L

m
(cid:88)

j=1

(cid:88)

xj

wj

θj(xj) φj(xj),

2. This treatment is for discrete MRFs. We have omitted a discussion of continuous MRFs for conciseness.

7

Bach, Broecheler, Huang, and Getoor

which is an upper bound on the true MAP objective. Much work has focused on solving
the ﬁrst-order local consistency relaxation for large-scale MRFs, which we discuss further
in Section 7. These algorithms are appealing because they are well-suited to the sparse
dependency structures common in MRFs, so they can scale to large problems. However, in
general, the solutions can be fractional, and there are no guarantees on the approximation
quality of a tractable discretization of these fractional solutions.

We show that for MRFs with potentials deﬁned by C and nonnegative weights, local

consistency relaxation is equivalent to MAX SAT relaxation.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

We prove Theorem 2 in Appendix A. Our proof analyzes the local consistency relaxation to
derive an equivalent, more compact optimization over only the variable pseudomarginals µ
that is identical to the MAX SAT relaxation. Theorem 2 is signiﬁcant because it shows that
the rounding guarantees of MAX SAT relaxation also apply to local consistency relaxation,
and the scalable message-passing algorithms developed for local consistency relaxation also
apply to MAX SAT relaxation.

2.3 (cid:32)Lukasiewicz Logic

The previous two subsections showed that the same convex program can approximate MAP
inference in discrete, logic-based models, whether viewed from the perspective of randomized
algorithms or variational methods. In this subsection, we show that this convex program
can also be used to reason about naturally continuous information, such as similarity, vague
or fuzzy concepts, and real-valued data. Instead of interpreting the clauses C using Boolean
logic, we can interpret them using (cid:32)Lukasiewicz logic (Klir and Yuan, 1995), which extends
Boolean logic to inﬁnite-valued logic in which the propositions x can take truth values in the
continuous interval [0, 1]. Extending truth values to a continuous domain enables them to
represent concepts that are vague, in the sense that they are often neither completely true
nor completely false. For example, the propositions that a sensor value is high, two entities
are similar, or a protein is highly expressed can all be captured in a more nuanced manner
in (cid:32)Lukasiewicz logic. We can also use the now continuous valued x to represent quantities
that are naturally continuous (scaled to [0,1]), such as actual sensor values, similarity scores,
and protein expression levels. The ability to reason about continuous values is valuable, as
many important applications are not entirely discrete.

The extension to continuous values requires a corresponding extended interpretation of
the logical operators ∧ (conjunction), ∨ (disjunction), and ¬ (negation). The (cid:32)Lukasiewicz
t-norm and t-co-norm are ∧ and ∨ operators that correspond to the Boolean logic operators
for integer inputs (along with the negation operator ¬):

x1 ∧ x2 = max {x1 + x2 − 1, 0}
x1 ∨ x2 = min {x1 + x2, 1}

¬x = 1 − x .

8

(11)

(12)

(13)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The analogous MAX SAT problem for (cid:32)Lukasiewicz logic is therefore

arg max
x∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

xi +

(1 − xi), 1

,

(14)

which is identical in form to the relaxed MAX SAT objective (7). Therefore, if an MRF
is deﬁned over continuous variables with domain [0, 1]n and the logical knowledge base C
deﬁning the potentials is interpreted using (cid:32)Lukasiewicz logic, then exact MAP inference
is identical to ﬁnding the optimum using the uniﬁed, relaxed inference objective derived
for Boolean logic in the previous two subsections. This result shows the equivalence of all
three approaches: MAX SAT relaxation, local consistency relaxation, and MAX SAT using
(cid:32)Lukasiewicz logic.

3. Hinge-Loss Markov Random Fields

We have shown that a speciﬁc family of convex programs can be used to reason scalably and
accurately about both discrete and continuous information. In this section, we generalize
this family to deﬁne hinge-loss Markov random ﬁelds (HL-MRFs), a new kind of probabilis-
tic graphical model. HL-MRFs retain the convexity and expressivity of convex programs
discussed in Section 2, and additionally support an even richer space of dependencies.

To begin, we deﬁne HL-MRFs as density functions over continuous variables y =
(y1, . . . , yn) with joint domain [0, 1]n. These variables have diﬀerent possible interpreta-
tions depending on the application. Since we are generalizing the interpretations explored
in Section 2, HL-MRF MAP states can be viewed as rounding probabilities or pseudo-
marginals, or they can represent naturally continuous information. More generally, they
can be viewed simply as degrees of belief, conﬁdences, or rankings of possible states; and
they can describe discrete, continuous, or mixed domains. The application domain typi-
cally determines which interpretation is most appropriate. The formalisms and algorithms
described in the rest of this paper are general with respect to such interpretations.

3.1 Generalized Inference Objective

To deﬁne HL-MRFs, we will ﬁrst generalize the uniﬁed inference objective of Section 2 in
several ways, which we ﬁrst restate in terms of the HL-MRF variables y:

arg max
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

yi +

(1 − yi), 1

.

(15)

For now, we are still assuming that the objective terms are deﬁned using a weighted knowl-
edge base C, but we will quickly drop this requirement. To do so, we examine one term in
isolation. Observe that the maximum value of any unweighted term is 1, which is achieved
when a linear function of the variables is at least 1. We say that the term is satisﬁed when-
ever this occurs. When a term is unsatisﬁed, we can refer to its distance to satisfaction,
which is how far it is from achieving its maximum value. Also observe that we can rewrite

9

Bach, Broecheler, Huang, and Getoor

the optimization explicitly in terms of distances to satisfaction:

arg min
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj max

1 −

yi −

(1 − yi), 0

,

(16)

so that the objective is equivalently to minimize the total weighted distance to satisfaction.
Each unweighted objective term now measures how far the linear constraint

1 −

yi −

(1 − yi) ≤ 0

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

(17)

is from being satisﬁed.

3.1.1 Relaxed Linear Constraints

With this view of each term as a relaxed linear constraint, we can easily generalize them to
arbitrary linear constraints. We no longer require that the inference objective be deﬁned
using only logical clauses, and instead each term can be deﬁned using any function (cid:96)j(y)
that is linear in y. These functions can capture more general dependencies, such as beliefs
about the range of values a variable can take and arithmetic relationships among variables.

The new inference objective is

arg min
y∈[0,1]n

m
(cid:88)

j=1

wj max {(cid:96)j(y), 0} .

(18)

In this form, each term represents the distance to satisfaction of a linear constraint (cid:96)j(y) ≤ 0.
That constraint could be deﬁned using logical clauses as discussed above, or it could be
deﬁned using other knowledge about the domain. The weight wj indicates how important
it is to satisfy a constraint relative to others by scaling the distance to satisfaction. The
higher the weight, the more distance to satisfaction is penalized. Additionally, two relaxed
inequality constraints, (cid:96)j(y) ≤ 0 and −(cid:96)j(y) ≤ 0, can be combined to represent a relaxed
equality constraint (cid:96)j(y) = 0.

3.1.2 Hard Linear Constraints

Now that our inference objective admits arbitrary relaxed linear constraints, it is natural
to also allow hard constraints that must be satisﬁed at all times. Hard constraints are
important modeling tools. They enable groups of variables to represent mutually exclusive
possibilities, such as a multinomial or categorical variable, and functional or partial func-
tional relationships. Hard constraints can also represent background knowledge about the
domain, restricting the domain to regions that are feasible in the real world. Additionally,
they can encode more complex model components such as deﬁning a random variable as an
aggregate over other unobserved variables, which we discuss further in Section 4.3.5.

We can think of including hard constraints as allowing a weight wj to take an inﬁnite
value. Again, two inequality constraints can be combined to represent an equality con-
straint. However, when we introduce an inference algorithm for HL-MRFs in Section 5, it

10

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

will be useful to treat hard constraints separately from relaxed ones, and further, treat hard
inequality constraints separately from hard equality constraints. Therefore, in the deﬁnition
of HL-MRFs, we will deﬁne these three components separately.

3.1.3 Generalized Hinge-Loss Functions

The objective terms measuring each constraint’s distance to satisfaction are hinge losses.
There is a ﬂat region, on which the distance to satisfaction is 0, and an angled region, on
which the distance to satisfaction grows linearly away from the hyperplane (cid:96)j(y) = 0. This
loss function is useful—as we discuss in the previous section, it is a bound on the expected
loss in the discrete setting, among other things—but it is not appropriate for all modeling
situations.

A piecewise-linear loss function makes MAP inference “winner take all,” in the sense
that it is preferable to fully satisfy the most highly weighted objective terms completely
before reducing the distance to satisfaction of terms with lower weights. For example,
consider the following optimization problem:

arg min
y1∈[0,1]

w1 max {y1, 0} + w2 max {1 − y1, 0} .

(19)

If w1 > w2 ≥ 0, then the optimizer is y1 = 0 because the term that prefers y1 = 0 overrules
the term that prefers y1 = 1. The result does not indicate any ambiguity or uncertainty, but
if the two objective terms are potentials in a probabilistic model, it is sometimes preferable
that the result reﬂect the conﬂicting preferences. We can change the inference problem
so that it smoothly trades oﬀ satisfying conﬂicting objective terms by squaring the hinge
losses. Observe that in the modiﬁed problem

arg min
y1∈[0,1]

w1 (max {y1, 0})2 + w2 (max {1 − y1, 0})2

(20)

the optimizer is now y1 = w2

, reﬂecting the relative inﬂuence of the two loss functions.
Another advantage of squared hinge-loss functions is that they can behave more intu-

w1+w2

itively in the presence of hard constraints. Consider the problem

arg min
(y1,y2)∈[0,1]2

such that

max {0.9 − y1, 0} + max {0.6 − y2, 0}

y1 + y2 ≤ 1 .

(21)

The ﬁrst term prefers y1 ≥ 0.9, the second term prefers y2 ≥ 0.6, and the constraint requires
that y1 and y2 are mutually exclusive. Such problems are very common and arise when
conﬂicting evidence of diﬀerent strengths support two mutually exclusive possibilities. The
evidence values 0.9 and 0.6 could come from many sources, including base models trained to
make independent predictions on individual random variables, domain-specialized similarity
functions, or sensor readings. For this problem, any solution y1 ∈ [0.4, 0.9] and y2 = 1 − y1
is an optimizer. This solution set includes counterintuitive optimizers like y1 = 0.4 and
y2 = 0.6, even though the evidence supporting y1 is stronger. Again, squared hinge losses

11

Bach, Broecheler, Huang, and Getoor

ensure the optimizers better reﬂect the relative strength of evidence. For the problem

(max {0.9 − y1, 0})2 + (max {0.6 − y2, 0})2

arg min
(y1,y2)∈[0,1]2

such that

(22)

y1 + y2 ≤ 1 ,

the only optimizer is y1 = 0.65 and y2 = 0.35, which is a more informative solution.

We therefore complete our generalized inference objective by allowing either hinge-loss
or squared hinge-loss functions. Users of HL-MRFs have the choice of either one for each
potential, depending on which is appropriate for their task.

3.2 Deﬁnition

We can now formally state the full deﬁnition of HL-MRFs. They are deﬁned so that a MAP
state is a solution to the generalized inference objective proposed in the previous subsection.
We state the deﬁnition in a conditional form for later convenience, but this deﬁnition is fully
general since the vector of conditioning variables may be empty.

Deﬁnition 3 Let y = (y1, . . . , yn) be a vector of n variables and x = (x1, . . . , xn(cid:48)) a vector
of n(cid:48) variables with joint domain D = [0, 1]n+n(cid:48). Let φ = (φ1, . . . , φm) be a vector of m
continuous potentials of the form

φj(y, x) = (max {(cid:96)j(y, x), 0})pj

where (cid:96)j is a linear function of y and x and pj ∈ {1, 2}. Let c = (c1, . . . , cr) be a vector of
r linear constraint functions associated with index sets denoting equality constraints E and
inequality constraints I, which deﬁne the feasible set

(cid:26)

˜D =

(y, x) ∈ D

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I

(cid:27)

.

For (y, x) ∈ D, given a vector of m nonnegative free parameters, i.e., weights, w =
(w1, . . . , wm), a constrained hinge-loss energy function fw is deﬁned as

(23)

(24)

(25)

fw(y, x) =

wjφj(y, x) .

m
(cid:88)

j=1

We now deﬁne HL-MRFs by placing a probability density over the inputs to a con-
strained hinge-loss energy function. Note that we negate the hinge-loss energy function so
that states with lower energy are more probable, in contrast with Deﬁnition 1. This change
is made for later notational convenience.

Deﬁnition 4 A hinge-loss Markov random ﬁeld P over random variables y and con-
ditioned on random variables x is a probability density deﬁned as follows: if (y, x) /∈ ˜D,
then P (y|x) = 0; if (y, x) ∈ ˜D, then

P (y|x) =

exp (−fw(y, x))

(26)

1
Z(w, x)

12

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

where

(cid:90)

Z(w, x) =

y|(y,x)∈ ˜D

exp (−fw(y, x)) dy .

(27)

In the rest of this paper, we will explore how to use HL-MRFs to solve a wide range
of structured machine learning problems. We ﬁrst introduce a probabilistic programming
language that makes HL-MRFs easy to deﬁne for large, rich domains.

4. Probabilistic Soft Logic

In this section we introduce a general-purpose probabilistic programming language, prob-
abilistic soft logic (PSL). PSL allows HL-MRFs to be easily applied to a broad range of
structured machine learning problems by deﬁning templates for potentials and constraints.
In models for structured data, there are very often repeated patterns of probabilistic de-
pendencies. A few of the many examples include the strength of ties between similar people
in social networks, the preference for triadic closure when predicting transitive relation-
ships, and the “exactly one active” constraints on functional relationships. Often, to make
graphical models both easy to deﬁne and able to generalize across diﬀerent data sets, these
repeated dependencies are deﬁned using templates. Each template deﬁnes an abstract de-
pendency, such as the form of a potential function or constraint, along with any necessary
parameters, such as the weight of the potential, each of which has a single value across all
dependencies deﬁned by that template. Given input data, an undirected graphical model
is constructed from a set of templates by ﬁrst identifying the random variables in the data
and then “grounding out” each template by introducing a potential or constraint into the
graphical model for each subset of random variables to which the template applies.

A PSL program is written in a declarative, ﬁrst-order syntax and deﬁnes a class of
HL-MRFs that are parameterized by the input data. PSL provides a natural interface to
represent hinge-loss potential templates using two types of rules: logical rules and arithmetic
rules. Logical rules are based on the mapping from logical clauses to hinge-loss potentials
introduced in Section 2. Arithmetic rules provide additional syntax for deﬁning an even
wider range of hinge-loss potentials and hard constraints.

4.1 Deﬁnition

In this subsection we deﬁne PSL. Our deﬁnition covers the essential functionality that
should be supported by all implementations, but many extensions are possible. The PSL
syntax we describe can capture a wide range of HL-MRFs, but new settings and scenarios
could motivate the development of additional syntax to make the construction of diﬀerent
kinds of HL-MRFs more convenient.

4.1.1 Preliminaries

We begin with a high-level deﬁnition of PSL programs.

Deﬁnition 5 A PSL program is a set of rules, each of which is a template for hinge-loss
potentials or hard linear constraints. When grounded over a base of ground atoms, a PSL
program induces a HL-MRF conditioned on any speciﬁed observations.

13

Bach, Broecheler, Huang, and Getoor

In the PSL syntax, many components are named using identiﬁers, which are strings that
begin with a letter (from the set {A, . . . , Z, a, . . . , z}), followed by zero or more letters,
numeric digits, or underscores.

PSL programs are grounded out over data, so the universe over which to ground must

be deﬁned.

Deﬁnition 6 A constant is a string that denotes an element in the universe over which
a PSL program is grounded.

Constants are the elements in a universe of discourse. They can be entities or attributes.
For example, the constant "person1" can denote a person, the constant "Adam" can denote
In PSL programs,
a person’s name, and the constant "30" can denote a person’s age.
constants are written as strings in double or single quotes. Constants use backslashes as
escape characters, so they can be used to encode quotes within constants. It is assumed that
constants are unambiguous, i.e., diﬀerent constants refer to diﬀerent entities and attributes.3
Groups of constants can be represented using variables.

Deﬁnition 7 A variable is an identiﬁer for which constants can be substituted.

Variables and constants are the arguments to logical predicates. Together, they are generi-
cally referred to as terms.

Deﬁnition 8 A term is either a constant or a variable.

Terms are connected by relationships called predicates.

Deﬁnition 9 A predicate is a relation deﬁned by a unique identiﬁer and a positive integer
called its arity, which denotes the number of terms it accepts as arguments. Every predicate
in a PSL program must have a unique identiﬁer as its name.

We refer to a predicate using its identiﬁer and arity appended with a slash. For example,
the predicate Friends/2 is a binary predicate, i.e., taking two arguments, which represents
whether two constants are friends. As another example, the predicate Name/2 can relate
a person to the string that is that person’s name. As a third example, the predicate
EnrolledInClass/3 can relate two entities, a student and professor, with an additional
attribute, the subject of the class.

Predicates and terms are combined to create atoms.

Deﬁnition 10 An atom is a predicate combined with a sequence of terms of length equal
to the predicate’s arity. This sequence is called the atom’s arguments. An atom with only
constants for arguments is called a ground atom.

Ground atoms are the basic units of reasoning in PSL. Each represents an unknown or
observation of interest and can take any value in [0, 1]. For example, the ground atom
Friends("person1", "person2") represents whether "person1" and "person2" are friends.
Atoms that are not ground are placeholders for sets of ground atoms. For example, the
atom Friends(X, Y) stands for all ground atoms that can be obtained by substituting
constants for variables X and Y.

3. Note that ambiguous references to underlying entities can be modeled by using diﬀerent constants for
diﬀerent references and representing whether they refer to the same underlying entity as a predicate.

14

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

4.1.2 Inputs

As we have already stated, PSL deﬁnes templates for hinge-loss potentials and hard linear
constraints that are grounded out over a data set to induce a HL-MRF. We now describe how
that data set is represented and provided as the inputs to a PSL program. The ﬁrst inputs
are two sets of predicates: a set C of closed predicates, the atoms of which are completely
observed, and a set O of open predicates, the atoms of which may be unobserved. The third
input is the base A, which is the set of all ground atoms under consideration. All atoms in
A must have a predicate in either C or O. These are the atoms that can be substituted into
the rules and constraints of a PSL program, and each will later be associated with a HL-
MRF random variable with domain [0, 1]. The ﬁnal input is a function O : A → [0, 1] ∪ {∅}
that maps the ground atoms in the base to either an observed value in [0, 1] or a symbol ∅
indicating that it is unobserved. The function O is only valid if all atoms with a predicate in
C are mapped to a [0, 1] value. Note that this deﬁnition makes the sets C and O redundant
in a sense, since they can be derived from A and O, but it will be convenient later to have
C and O explicitly deﬁned.

Ultimately, the method for specifying PSL’s inputs is implementation-speciﬁc, since
In this paper,
diﬀerent choices make it more or less convenient for diﬀerent scenarios.
we will assume that C, O, A, and O exist, and we remain agnostic about how they were
speciﬁed. However, to make this aspect of using PSL more concrete, we will describe one
possible method for deﬁning them here.

Our example method for specifying PSL’s inputs is text-based. The ﬁrst section of the
text input is a deﬁnition of the constants in the universe, which are grouped into types. An
example universe deﬁnition follows.

Person = {"alexis", "bob", "claudia", "david"}
Professor = {"alexis", "bob"}
Student = {"claudia", "david"}
Subject = {"computer science", "statistics"}

This universe includes six constants, four with two types ("alexis", "bob", "claudia",
and "david") and two with one type ("computer science" and "statistics").

The next section of input is the deﬁnition of predicates. Each predicate includes the
types of constants it takes as arguments and whether it is closed. For example, we can
deﬁne predicates for an advisor-student relationship prediction task as follows:

Advises(Professor, Student)

Department(Person, Subject) (closed)

EnrolledInClass(Student, Subject, Professor) (closed)

In this case, there is one open predicate (Advises) and two closed predicates (Department
and EnrolledInClass).

15

Bach, Broecheler, Huang, and Getoor

The ﬁnal section of input is any associated observations. They can be speciﬁed in a list,

for example:

Advises("alexis", "david") = 1

Department("alexis", "computer science") = 1

Department("bob", "computer science") = 1

Department("claudia", "statistics") = 1

Department("david", "statistics") = 1

In addition, values for atoms with the EnrolledInClass predicate could also be speciﬁed.
If a ground atom does not have a speciﬁed value, it will have a default observed value of 0
if its predicate is closed or remain unobserved if its predicate is open.

We now describe how this text input is processed into the formal inputs C, O, A, and
O. First, each predicate is added to either C or O based on whether it is annotated with
the (closed) tag. Then, for each predicate in C or O, ground atoms of that predicate are
added to A with each sequence of constants as arguments that can be created by selecting
a constant of each of the predicate’s argument types. For example, assume that the input
ﬁle contains a single predicate deﬁnition

Category(Document, Cat Name)

where the universe is Document = {"d1", "d2"} and Cat Name = {"politics", "sports"}.
Then,

A =






Category("d1", "politics"),
Category("d1", "sports"),
Category("d2", "politics"),
Category("d2", "sports")






.

(28)

Finally, we deﬁne the function O. Any atom in the explicit list of observations is mapped
to the given value. Then, any remaining atoms in A with a predicate in C are mapped to
0, and any with a predicate in O are mapped to ∅.

Before moving on, we also note that PSL implementations can support predicates and
atoms that are deﬁned functionally. Such predicates can be thought of as a type of closed
predicate. Their observed values are deﬁned as a function of their arguments. One of the
most common examples is inequality, atoms of which can be represented with the shorthand
inﬁx operator !=. For example, the following atom has a value of 1 when two variables A
and B are replaced with diﬀerent constants and 0 when replaced with the same constant.

Such functionally deﬁned predicates can be implemented without requiring their values over
all arguments to be speciﬁed by the user.

4.1.3 Rules and Grounding

Before introducing the syntax and semantics of speciﬁc PSL rules, we deﬁne the grounding
procedure that induces HL-MRFs in general. Given the inputs C, O, A, and O, PSL induces

A != B

16

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

a HL-MRF P (y|x) as follows. First, each ground atom a ∈ A is associated with a random
variable with domain [0, 1]. If O(a) = ∅, then the variable is included in the free variables
y, and otherwise it is included in the observations x with a value of O(a).

With the variables in the distribution deﬁned, each rule in the PSL program is applied
to the inputs and produces hinge-loss potentials or hard linear constraints, which are added
to the HL-MRF. In the rest of this subsection, we describe two kinds of PSL rules: logical
rules and arithmetic rules.

4.1.4 Logical Rules

The ﬁrst kind of PSL rule is a logical rule, which is made up of literals.

Deﬁnition 11 A literal is an atom or a negated atom.

In PSL, the preﬁx operator ! or ~ is used for negation. A negated atom has a value of one
minus the value of the unmodiﬁed atom. For example, if Friends("person1", "person2")
has a value of 0.7, then !Friends("person1", "person2") has a value of 0.3.

Deﬁnition 12 A logical rule is a disjunctive clause of literals. Logical rules are either
weighted or unweighted. If a logical rule is weighted, it is annotated with a nonnegative
weight and optionally a power of two.

Logical rules express logical dependencies in the model. As in Boolean logic, the negation,
disjunction (written as || or |), and conjunction (written as && or &) operators obey De
Morgan’s Laws. Also, an implication (written as -> or <-) can be rewritten as the negation
of the body disjuncted with the head. For example

P1(A, B) && P2(A, B) -> P3(A, B) || P4(A, B)
≡ !(P1(A, B) && P2(A, B)) || P3(A, B) || P4(A, B)
≡ !P1(A, B) || !P2(A, B) || P3(A, B) || P4(A, B)

Therefore, any formula written as an implication with (1) a literal or conjunction of literals
in the body and (2) a literal or disjunction of literals in the head is also a valid logical rule,
because it is equivalent to a disjunctive clause.

There are two kinds of logical rules: weighted or unweighted. A weighted logical rule is a
template for a hinge-loss potential that penalizes how far the rule is from being satisﬁed. A
weighted logical rule begins with a nonnegative weight and optionally ends with an exponent
of two (^2). For example, the weighted logical rule

1 : Advisor(Prof, S) && Department(Prof, Sub) -> Department(S, Sub)

has a weight of 1 and induces potentials propagating department membership from advisors
to advisees. An unweighted logical rule is a template for a hard linear constraint that
requires that the rule always be satisﬁed. For example, the unweighted logical rule

Friends(X, Y) && Friends(Y, Z) -> Friends(X, Z) .

induces hard linear constraints enforcing the transitivity of the Friends/2 predicate. Note
the period (.) that is used to emphasize that this rule is always enforced and disambiguate
it from weighted rules.

17

Bach, Broecheler, Huang, and Getoor

A logical rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. This procedure produces
a set of ground rules, which are rules containing only ground atoms. Each ground rule will
then be interpreted as either a potential or hard constraint in the induced HL-MRF. For
notational convenience, we assume without loss of generality that all the random variables
are unobserved, i.e., O(a) = ∅, ∀a ∈ A. If the input data contain any observations, the
following description still applies, except that some free variables will be replaced with
observations from x. The ﬁrst step in interpreting a ground rule is to map its disjunctive
clause to a linear constraint. This mapping is based on the uniﬁed inference objective
derived in Section 2. Any ground PSL rule is a disjunction of literals, some of which are
negated. Let I + be the set of indices of the variables that correspond to atoms that are not
negated in the ground rule, when expressed as a disjunctive clause, and, likewise, let I − be
the indices of the variables corresponding to atoms that are negated. Then, the clause is
mapped to the inequality

(cid:88)

1 −

(cid:88)

yi −

(1 − yi) ≤ 0 .

i∈I +

i∈I −

If the logical rule that templated the ground rule is weighted with a weight of w and is not
annotated with ^2, then the potential

is added to the HL-MRF with a parameter of w. If the rule is weighted with a weight w
and annotated with ^2, then the potential











φ(y, x) = max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −



φ(y, x) =

max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −

c(y, x) = 1 −

(cid:88)

(cid:88)

yi −

(1 − yi)

i∈I +

i∈I −








2








is added to the HL-MRF with a parameter of w. If the rule is unweighted, then the function

is added to the set of constraint functions and its index is included in the set I to deﬁne a
hard inequality constraint c(y, x) ≤ 0.

As an example of the grounding process, consider the following logical rule. As part of
a program for link prediction, it is often helpful to model the transitivity of a relationship.

3 : Friends(A, B) && Friends(B, C) -> Friends(C, A) ^2

Imagine that the input data are C = {}, O = {Friends/2},

(29)

(30)

(31)

(32)

(33)

A =






Friends("p1", "p2"),
Friends("p1", "p3"),
Friends("p2", "p1"),
Friends("p2", "p3"),
Friends("p3", "p1"),
Friends("p3", "p2")






,

18

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and O(a) = ∅, ∀a ∈ A. Then, the rule will induce six ground rules. One such ground rule is

3 : Friends("p1", "p2") && Friends("p2", "p3") -> Friends("p3", "p1") ^2

which is equivalent to the following.

3 : !Friends("p1", "p2") || !Friends("p2", "p3") || Friends("p3", "p1") ^2

If the atoms Friends("p1", "p2"), Friends("p2", "p3"), and Friends("p3", "p1")
correspond to the random variables y1, y2, and y3, respectively, then this ground rule is
interpreted as the weighted hinge-loss potential

3 (max{y1 + y2 − y3 − 1, 0})2 .

(34)

Since the grounding process uses the mapping from Section 2, logical rules can be used
to reason accurately and eﬃciently about both discrete and continuous information. They
are a convenient method for constructing HL-MRFs with the uniﬁed inference objective
for weighted logical knowledge bases as their MAP inference objective. They also allow
the user to seamlessly incorporate some of the additional features of HL-MRFs, such as
squared potentials and hard constraints. Next, we introduce an even more ﬂexible class of
PSL rules.

4.1.5 Arithmetic Rules

Arithmetic rules in PSL are more general templates for hinge-loss potentials and hard
linear constraints. Like logical rules, they come in weighted and unweighted variants, but
instead of using logical operators they use arithmetic operators. In general, an arithmetic
rule relates two linear combinations of atoms with an inequality or an equality. A simple
example enforces the mutual exclusivity of liberal and conservative ideologies.

Liberal(P) + Conservative(P) = 1 .

Just like logical rules, arithmetic rules are grounded out by performing all possible substi-
tutions of constants for variables to make ground atoms in the base A. In this example,
each substitution for Liberal(P) and Conservative(P) is constrained to sum to 1. Since
the rule is unweighted and arithmetic, it deﬁnes a hard constraint c(y, x) and its index will
be included in E because it is an equality constraint.

To make arithmetic rules more ﬂexible and easy to use, we deﬁne some additional syntax.
The ﬁrst is a generalized deﬁnition of atoms that can be substituted with sums of ground
atoms, rather than just a single atom.

Deﬁnition 13 A summation atom is an atom that takes terms and/or sum variables
as arguments. A summation atom represents the summations of ground atoms that can be
obtained by substituting individual constants for variables and summing over all possible
constants for sum variables.

A sum variable is represented by prepending a plus symbol (+) to a variable. For example,
the summation atom

Friends(P, +F)

19

Bach, Broecheler, Huang, and Getoor

is a placeholder for the sum of all ground atoms with predicate Friends/2 in A that share
a ﬁrst argument. Note that sum variables can be used at most once in a rule, i.e., each
sum variable in a rule must have a unique identiﬁer. Summation atoms are useful because
they can describe dependencies without needing to specify the number of atoms that can
participate. For example, the arithmetic rule

Label(X, +L) = 1 .

says that labels for each constant substituted for X should sum to one, without needing to
specify how many possible labels there are.

The substitutions for sum variables can be restricted using logical clauses as ﬁlters.

Deﬁnition 14 A ﬁlter clause is a logical clause deﬁned for a sum variable in an arithmetic
rule. The logical clause only contains atoms (1) with predicates that appear in C and (2)
that only take as arguments (a) constants, (b) variables that appear in the arithmetic rule,
and (c) the sum variable for which it is deﬁned.

Filter clauses restrict the substitutions for a sum variable in the corresponding arithmetic
rule by only including substitutions for which the clause evaluates to true. The ﬁlters are
evaluated using Boolean logic. Each ground atom a is treated as having a value of 0 if and
only if O(a) = 0. Otherwise, it is treated as having a value of 1. For example, imagine that
we want to restrict the summation in the following arithmetic rule to only constants that
satisfy a property Property/1.

Then, we can add the following ﬁlter clause.

Link(X, +Y) <= 1 .

{Y: Property(Y)}

Then, the hard linear constraints templated by the arithmetic rule will only sum over
constants substituted for Y such that Property(Y) is non-zero.

In arithmetic rules, atoms can also be modiﬁed with coeﬃcients. These coeﬃcients can

be hard-coded. As a simple example, in the rule

Susceptible(X) >= 0.5 Biomarker1(X) + 0.5 Biomarker2(X) .

the property Susceptible/1, which represents the degree to which a patient is susceptible
to a particular disease, must be at least the average value of two biomarkers.

PSL also supports two forms of coeﬃcient-deﬁning syntax. The ﬁrst form of coeﬃcient
syntax is a cardinality function that counts the number of terms substituted for a sum
variable. Cardinality functions enable rules that depend on the number of substitutions in
order to be scaled correctly, such as when averaging. Cardinality is denoted by enclosing a
sum variable, without the +, in pipes. For example, the rule

1 / |Y| Friends(X, +Y) = Friendliness(X) .

20

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

deﬁnes the Friendliness/1 property of a person X in a social network as the average
strength of their outgoing friendship links. In cases in which Friends/2 is not symmetric,
we can extend this rule to sum over both outgoing and incoming links as follows.

1 / |Y1| |Y2| Friends(X, +Y1) + 1 / |Y1| |Y2| Friends(+Y2, X)

= Friendliness(X) .

The second form of coeﬃcient syntax is built-in coeﬃcient functions. The exact set of
supported functions is implementation speciﬁc, but standard functions like maximum and
minimum should be included. Coeﬃcient functions are prepended with @ and use square
brackets instead of parentheses to distinguish them from predicates. Coeﬃcient functions
can take either scalars or cardinality functions as arguments. For example, the following
rule for matching two sets of constants requires that the sum of the Matched/2 atoms be
the minimum of the sizes of the two sets.

Matched(+X, +Y) = @Min[|X|, |Y|] .

Note that PSL’s coeﬃcient syntax can also be used to deﬁne constants, as in this example.
So far we have focused on using arithmetic rules to deﬁne templates for linear constraints,
but they can also be used to deﬁne hinge-loss potentials. For example, the following arith-
metic rule prefers that the degree to which a person X is extroverted (represented with
Extroverted/1) does not exceed the average extroversion of their friends:

2 : Extroverted(X) <= 1 / |Y| Extroverted(+Y) ^2
{Y: Friends(X, Y) || Friends(Y, X)}

This rule is a template for weighted hinge-loss potentials of the form

(cid:32)

(cid:40)

2

max

yi(cid:48) −

(cid:41)(cid:33)2

yi, 0

,

1
|F|

(cid:88)

i∈F

(35)

where yi(cid:48) is the variable corresponding to a grounding of the atom Extroverted(X) and
F is the set of the indices of the variables corresponding to Extroverted(Y) atoms of the
friends Y that satisfy the rule’s ﬁlter clause. Note that the weight of 2 is distinct from
the coeﬃcients in the linear constraint (cid:96)(y, x) ≤ 0 deﬁning the hinge-loss potential.
If
the arithmetic rule were an equality instead of an inequality, each grounding would be
two hinge-loss potentials, one using (cid:96)(y, x) ≤ 0 and one using −(cid:96)(y, x) ≤ 0. In this way,
arithmetic rules can deﬁne general hinge-loss potentials.

For completeness, we state the full, formal deﬁnition of an arithmetic rule and deﬁne its

grounding procedure.

Deﬁnition 15 An arithmetic rule is an inequality or equality relating two linear combi-
nations of summation atoms. Each sum variable in an arithmetic rule can be used once.
An arithmetic rule can be annotated with ﬁlter clauses for a subset of its sum variables that
restrict its groundings. Arithmetic rules are either weighted or unweighted. If an arithmetic
rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.

21

Bach, Broecheler, Huang, and Getoor

An arithmetic rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. In addition, summation
atoms are replaced by the appropriate summations over ground atoms (possibly restricted
by corresponding ﬁlter clauses) and the coeﬃcient is distributed across the summands. This
leads to a set of ground rules for each arithmetic rule given a set of inputs. If the arithmetic
rule is an unweighted inequality, each ground rule can be algebraically manipulated to be
of the form c(y, x) ≤ 0. Then c(y, x) is added to the set of constraint functions and its
index is added to I. If instead the arithmetic rule is an unweighted equality, each ground
rule is manipulated to c(y, x) = 0, c(y, x) is added to the set of constraint functions, and
its index is added to E. If the arithmetic rule is a weighted inequality with weight w, each
ground rule is manipulated to (cid:96)(y, x) ≤ 0 and included as a potential of the form

φ(y, x) = max {(cid:96)(y, x), 0}

(36)

with a weight of w. If the arithmetic rule is a weighted equality with weight w, each ground
rule is again manipulated to (cid:96)(y, x) ≤ 0 and two potentials are included,

φ1(y, x) = max {(cid:96)(y, x), 0}, φ2(y, x) = max {−(cid:96)(y, x), 0} ,

(37)

each with a weight of w. In either case, if the weighted arithmetic rule is annotated with
^2, then the induced potentials are squared.

4.2 Expressivity

An important question is the expressivity of PSL, which uses disjunctive clauses with pos-
itive weights for its logical rules. Other logic-based languages support diﬀerent types of
clauses, such as Markov logic networks (Richardson and Domingos, 2006), which support
clauses with conjunctions and clauses with negative weights. As we discuss in this section,
PSL’s logical rules capture a general class of structural dependencies, capable of model-
ing arbitrary probabilistic relationships among Boolean variables, such as those deﬁned by
Markov logic networks. The advantage of PSL is that it deﬁnes HL-MRFs, which are much
more scalable than discrete MRFs and often just as accurate, as we show in Section 6.4.

The expressivity of PSL is tied to the expressivity of the MAX SAT problem, since
they both use the same class of weighted clauses. There are two conditions on the clauses:
(1) they have nonnegative weights, and (2) they are disjunctive. We ﬁrst consider the
nonnegativity requirement and show that can actually be viewed as a restriction on the
structure of a clause. To illustrate, consider a weighted disjunctive clause of the form

−w :



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(38)

If this clause were part of a generalized MAX SAT problem, in which there were no restric-
tions on weight sign or clause structure, but the goal were still to maximize the sum of the
weights of the satisﬁed clauses, then this clause could be replaced with an equivalent one

22

(39)

(40)

(41)

(42)

(43)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

without changing the optimizer:

w :






¬xi






(cid:94)

i∈I +
j

(cid:94)

(cid:94)








xi


 .

i∈I −
j

Note that the clause has been changed in three ways: (1) the sign of the weight has been
changed, (2) the disjunctions have been replaced with conjunctions, and (3) the literals
have all been negated. Due to this equivalence, the restriction on the sign of the weights
is subsumed by the restriction on the structure of the clauses. In other words, any set of
clauses can be converted to a set with nonnegative weights that has the same optimizer,
but it might require including conjunctions in the clauses. It is also easy to verify that if
Equation (38) is used to deﬁne a potential in a discrete MRF, replacing it with a potential
deﬁned by (39) leaves the distribution unchanged, due to the normalizing partition function.
We now consider the requirement that clauses be disjunctive and illustrate how con-
junctive clauses can be replaced by an equivalent set of disjunctive clauses. The idea is to
construct a set of disjunctive clauses such that all assignments to the variables are mapped
to the same score, up to a constant. A simple example is replacing a conjunction

with disjunctions

w : x1 ∧ x2

w : x1 ∨ x2
w : ¬x1 ∨ x2
w : x1 ∨ ¬x2 .

Observe that the total score for all assignments to the variables remains the same, up to a
constant.

This example generalizes to a procedure for encoding any Boolean MRF into a set of
disjunctive clauses with nonnegative weights. Park (2002) showed that the MAP problem
for any discrete Bayesian network can be represented as an instance of MAX SAT. For
distributions of bounded factor size, the MAX SAT problem has size polynomial in the
number of variables and factors of the distribution. We describe how any Boolean MRF
can be represented with disjunctive clauses and nonnegative weights. Given a Boolean MRF
with arbitrary potentials deﬁned by mappings from joint states of subsets of the variables
to scores, a new MRF is created as follows. For each potential in the original MRF, a new
set of potentials deﬁned by disjunctive clauses is created. A conjunctive clause is created
corresponding to each entry in the potential’s mapping with a weight equal to the score
assigned by the weighted potential in the original MRF. Then, these clauses are converted to
equivalent disjunctive clauses as in the example of Equations (38) and (39) by also ﬂipping
the sign of their weights and negating the literals. Once this is done for all entries of all
potentials, what remains is an MRF deﬁned by disjunctive clauses, some of which might
have negative weights. We make all weights positive by adding a suﬃciently large constant
to all weights of all clauses, which leaves the distribution unchanged due to the normalizing
partition function.

23

Bach, Broecheler, Huang, and Getoor

It is important to note two caveats when converting arbitrary Boolean MRFs to MRFs
deﬁned using only disjunctive clauses with nonnegative weights. First, the number of clauses
required to represent a potential in the original MRF is exponential in the degree of the
potential. In practice, this is rarely a signiﬁcant limitation, since MRFs often contain low-
degree potentials. The other important point is that the step of adding a constant to all
the weights increases the total score of the MAP state. Since the bound of Goemans and
Williamson (1994) is relative to this score, the bound is loosened for the original problem the
larger the constant added to the weights is. This is to be expected, since even approximating
MAP is NP-hard in general (Abdelbar and Hedetniemi, 1998).

We have described how general structural dependencies can be modeled with the logical
rules of PSL. It is possible to represent arbitrary logical relationships with them. The
process for converting general rules to PSL’s logical rules can be done automatically and
made transparent to the user. We have elected in this section to deﬁne PSL’s logical rules
without making this conversion automatic to make clear the underlying formalism.

4.3 Modeling Patterns

PSL is a ﬂexible language, and there are some patterns of usage that come up in many
applications. We illustrate some of them in this subsection with a number of examples.

4.3.1 Domain and Range Rules

In many problems, the number of relations that can be predicted among some constants
is known. For binary predicates, this background knowledge can be viewed as constraints
on the domain (ﬁrst argument) or range (second argument) of the predicate. For example,
it might be background knowledge that each entity, such as a document, has exactly one
label. An arithmetic rule to express this follows.

Label(Document, +LabelName) = 1 .

The predicate Label is said to be functional.

Alternatively, sometimes it is the ﬁrst argument that should be summed over. For ex-
ample, imagine the task of predicting relationships among students and professors. Perhaps
it is known that each student has exactly one advisor. This constraint can be written as
follows.

Advisor(+Professor, Student) = 1 .

The predicate Advisor is said to be inverse functional.

Finally, imagine a scenario in which two social networks are being aligned. The goal is
to predict whether each pair of people, one from each network, is the same person, which is
represented with atoms of the Same predicate. Each person aligns with at most one person
in the other network, but might not align with anyone. This can be expressed with the
following two arithmetic rules.

The predicate Same is said to be both partial functional and partial inverse functional.

Same(Person1, +Person2) <= 1 .

Same(+Person1, Person2) <= 1 .

24

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Many variations on these examples are possible. For example, they can be generalized
to predicates with more than two arguments. Additional arguments can either be ﬁxed or
summed over in each rule. As another example, domain and range rules can incorporate
multiple predicates, so that an entity can participate in a ﬁxed number of relations counted
among multiple predicates.

4.3.2 Similarity

Many problems require explicitly reasoning about similarity, rather than simply whether
entities are the same or diﬀerent. For example, reasoning with similarity has been explored
using kernel methods, such as kFoil (Landwehr et al., 2010) that bases similarity computa-
tion on the relational structure of the data. The continuous variables of HL-MRFs make
modeling similarity straightforward, and PSL’s support for functionally deﬁned predicates
makes it even easier. For example, in an entity resolution task, the degree to which two
entities are believed to be the same might depend on how similar their names are. A rule
expressing this dependency is

1.0 : Name(P1, N1) && Name(P2, N2) && Similar(N1, N2) -> Same(P1, P2)

This rule uses the Similar predicate to measure similarity. Since it is a functionally deﬁned
predicate, it can be implemented as one of many diﬀerent, possibly domain specialized,
string similarity functions. Any similarity function that can output values in the range
[0, 1] can be used.

4.3.3 Priors

If no potentials are deﬁned over a particular atom, then it is equally probable that it has any
value between zero and one. Often, however, it should be more probable that an atom has
a value of zero, unless there is evidence that it has a nonzero value. Since atoms typically
represent the existence of some entity, attribute, or relation, this bias promotes sparsity
among the things inferred to exist. Further, if there is a potential that prefers that an
atom should have a value that is at least some numeric constant, such as when reasoning
with similarities as discussed in Section 4.3.2, it often should also be more probable that an
atom is no higher in value than is necessary to satisfy that potential. To accomplish both
these goals, simple priors can be used to state that atoms should have low values in the
absence of evidence to overrules those priors. A prior in PSL can be a rule consisting of
just a negative literal with a small weight. For example, in a link prediction task, imagine
that this preference should apply to atoms of the Link predicate. A prior is then

0.1 : !Link(A, B)

which acts as a regularizer on Link atoms.

4.3.4 Blocks and Canopies

In many tasks, the number of unknowns can quickly grow large, even for modest amounts of
data. For example, in a link prediction task the goal is to predict relations among entities.
The number of possible links grows quadratically with the number of entities (for binary

25

Bach, Broecheler, Huang, and Getoor

relations). If handled naively, this growth could make scaling to large data sets diﬃcult,
but this problem is often handled by constructing blocks (e.g., Newcombe and Kennedy,
1962) or canopies (McCallum et al., 2000) over the entities, so that a limited subset of all
possible links are actually considered. Blocking partitions the entities so that only links
among entities in the same partition element, i.e., block, are considered. Alternatively, for
a ﬁner grained pruning, a canopy is deﬁned for each entity, which is the set of other entities
to which it could possibly link. Blocks and canopies can be computed using specialized,
domain-speciﬁc functions, and PSL can incorporate them by including them as atoms in
the bodies of rules. Since blocks can be seen as a special case of canopies, we let the atom
InCanopy(A, B) be 1 if B is in the canopy or block of A, and 0 if it is not.
Including
InCanopy(A, B) atoms as additional conditions in the bodies of logical rules will ensure
that the dependencies only exist between the desired entities.

4.3.5 Aggregates

Another powerful feature of PSL is its ability to easily deﬁne aggregates, which are rules
that deﬁne random variables to be deterministic functions of sets of other random variables.
The advantage of aggregates is that they can be used to deﬁne dependencies that do not
scale in magnitude with the number of groundings in the data. For example, consider a
model for predicting interests in a social network. A fragment of a PSL program for this
task follows.

1.0 : Interest(P1, I) && Friends(P1, P2) -> Interest(P2, I)

1.0 : Age(P, "20-29") && Lives(P, "California") -> Interest(P, "Surfing")

These two rules express the belief that interests are correlated along friendship links in
the social network, and also that certain demographic information is predictive of speciﬁc
interests. The question any domain expert or learning algorithm faces is how strongly each
rule should be weighted relative to each other. The challenge of answering this question
when using templates is that the number of groundings of the ﬁrst rule varies from person to
person based on the number of friends, while the groundings of the second remain constant
(one per person). This inconsistent scaling of the two types of dependencies makes it diﬃcult
to ﬁnd weights that accurately reﬂect the relative inﬂuence each type of dependency should
have across people with diﬀerent numbers of friends.

Using an aggregate can solve this problem of inconsistent scaling. Instead of using a
separate ground rule to relate the interest of each friend, we can deﬁne a rule that is only
grounded once for each person, relating an average interest across all friends to each person’s
own interests. A PSL fragment for this approach is

1.0 : AverageFriendInterest(P, I) -> Interest(P, I)

AverageFriendInterest(P, I) = 1 / |F| Interest(+F, I) .
{F: Friends(P, F)}

/* Demographic dependencies are also included.

*/

where the predicate AverageFriendInterest/2 is an aggregate that is constrained to be
the average amount of interest each friend of a person P has in an interest I. The weight

26

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

of the logical rule can now be scaled more appropriately relative to other types of features
because there is only one grounding per person.

For a more complex example, consider the problem of determining whether two refer-
ences in the data refer to the same underlying person. One useful feature to use is whether
they have similar sets of friends in the social network. Again, a rule could be deﬁned that
is grounded out for each friendship pair, but this would suﬀer from the same scaling issues
as the previous example. Instead, we can use an aggregate to directly express how similar
the two references’ sets of friends are. A function that measures the similarity of two sets
A and B is Jaccard similarity:

J(A, B) =

|A ∩ B|
|A ∪ B|

.

Jaccard similarity is a nonlinear function, meaning that it cannot be used directly without
breaking the log-concavity of HL-MRFs, but we can approximate it with a linear function.
We deﬁne SameFriends/2 as an aggregate that approximates Jaccard similarity (where
SamePerson/2 is functional and inverse functional).

SameFriends(A, B) = 1 / @Max[|FA|, |FB|] SamePerson(+FA, +FB) .
{FA : Friends(A, FA)}
{FB : Friends(B, FB)}

SamePerson(+P1, P2) = 1 .

SamePerson(P1, +P2) = 1 .

The aggregate SameFriends/2 uses the sum of the SamePerson/2 atoms as the intersection
of the two sets, and the maximum of the sizes of the two sets of friends as a lower bound
on the size of their union.

5. MAP Inference

Having deﬁned HL-MRFs and a language for creating them, PSL, we turn to algorithms
for inference and learning. The ﬁrst task we consider is maximum a posteriori (MAP)
inference, the problem of ﬁnding a most probable assignment to the free variables y given
observations x. In HL-MRFs, the normalizing function Z(w, x) is constant over y and the
exponential is maximized by minimizing its negated argument, so the MAP problem is

arg max
y

P (y|x) ≡ arg min
y|y,x∈ ˜D

fw(y, x)

≡ arg min
y∈[0,1]n

w(cid:62)φ(y, x)

such that

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I .

(44)

MAP is a fundamental problem because (1) it is the method we will use to make predictions,
and (2) weight learning often requires performing MAP inference many times with diﬀerent
weights (as we discuss in Section 6). Here, HL-MRFs have a distinct advantage over general

27

Bach, Broecheler, Huang, and Getoor

discrete models, since minimizing fw is a convex optimization rather than a combinatorial
one. There are many oﬀ-the-shelf solutions for convex optimization, the most popular
of which are interior-point methods, which have worst-case polynomial time complexity
in the number of variables, potentials, and constraints (Nesterov and Nemirovskii, 1994).
Although in practice they perform better than their worst-case bounds (Wright, 2005), they
do not scale well to large structured prediction problems (Yanover et al., 2006). We therefore
introduce a new algorithm for exact MAP inference designed to scale to large HL-MRFs by
leveraging the sparse connectivity structure of the potentials and hard constraints that are
typical of models for real-world tasks.

5.1 Consensus Optimization Formulation

Our algorithm uses consensus optimization, a technique that divides an optimization prob-
lem into independent subproblems and then iterates to reach a consensus on the optimum
(Boyd et al., 2011). Given a HL-MRF P (y|x), we ﬁrst construct an equivalent MAP prob-
lem in which each potential and hard constraint is a function of diﬀerent variables. The
variables are then constrained to make the new and original MAP problems equivalent. We
let y(L,j) be a local copy of the variables in y that are used in the potential function φj,
j = 1, . . . , m and y(L,k+m) be a copy of those used in the constraint function ck, k = 1, . . . , r.
We refer to the concatenation of all of these vectors as yL. We also introduce a characteristic
= 0 if the constraint is
function χk for each constraint function where χk
satisﬁed and inﬁnity if it is not. Likewise, let χ[0,1] be a characteristic function that is 0 if the
input is in the interval [0, 1] and inﬁnity if it is not. We drop the constraints on the domain
of y, letting them range in principle over Rn and instead use these characteristic functions
to enforce the domain constraints. This formulation will make computation easier when
be the variables in y that correspond
the problem is later decomposed. Finally, let y

(cid:104)
ck(y(L,k+m), x)

(cid:105)

(C,ˆi)

(L,ˆi)

, ˆi = 1, . . . , m + r. Operators between y

to y
are deﬁned element-wise,
pairing the corresponding copied variables. Consensus optimization solves the reformulated
MAP problem

and y

(C,ˆi)

(L,ˆi)

arg min
(yL,y)

m
(cid:88)

j=1

such that

wjφj

(cid:16)

(cid:17)
y(L,j), x

+

(cid:104)

(cid:16)

χk

ck

y(L,k+m), x

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

(45)

y

(L,ˆi)

= y

(C,ˆi)

∀ˆi = 1, . . . , m + r .

Inspection shows that problems (44) and (45) are equivalent.

This reformulation enables us to relax the equality constraints y

in order
to divide problem (45) into independent subproblems that are easier to solve, using the
alternating direction method of multipliers (ADMM) (Glowinski and Marrocco, 1975; Gabay
and Mercier, 1976; Boyd et al., 2011). The ﬁrst step is to form the augmented Lagrangian
function for the problem. Let α = (α1, . . . , αm+r) be a concatenation of vectors of Lagrange

= y

(C,ˆi)

(L,ˆi)

28

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

multipliers. Then the augmented Lagrangian is

L(yL, α, y) =

wjφj

y(L,j), x

+

(cid:16)

(cid:17)

(cid:16)

(cid:104)
ck

χk

y(L,k+m), x

m
(cid:88)

j=1

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

+

m+r
(cid:88)

ˆi=1

(cid:16)

y

α(cid:62)
ˆi

(L,ˆi)

− y

(C,ˆi)

(cid:17)

+

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)y

(L,ˆi)

− y

(C,ˆi)

(46)

(cid:13)
2
(cid:13)
(cid:13)
2

using a step-size parameter ρ > 0. ADMM ﬁnds a saddle point of L(yL, α, y) by updating
the three blocks of variables at each iteration t:

∀ˆi = 1, . . . , m + r

αt
ˆi

← αt−1
ˆi

+ ρ

yt

L ← arg min

(cid:17)

(cid:16)

yt−1
(L,ˆi)

− yt−1
(C,ˆi)
L (cid:0)yL, αt, yt−1(cid:1)

yt ← arg min

L (cid:0)yt

L, αt, y(cid:1)

yL

y

The ADMM updates ensure that y converges to the global optimum y(cid:63), the MAP state
of P (y|x), assuming that there exists a feasible assignment to y. We check convergence
using the criteria suggested by Boyd et al. (2011), measuring the primal and dual residuals
at the end of iteration t, deﬁned as

(cid:107)¯rt(cid:107)2 (cid:44)

(cid:107)yt

(L,ˆi)

− yt

(C,ˆi)

(cid:107)2
2



(cid:107)¯st(cid:107)2 (cid:44) ρ

Ki(yt

i − yt−1
i

)2

(50)



1
2

(cid:33) 1
2

(cid:32) n
(cid:88)

i=1





m+r
(cid:88)

ˆi=1

where Ki is the number of copies made of the variable yi, i.e., the number of diﬀerent
potentials and constraints in which the variable participates. The updates are terminated
when both of the following conditions are satisﬁed










m+r
(cid:88)

ˆi=1

(cid:107)yt

(L,ˆi)

(cid:107)2
2



,

Ki(yt

i)2



1
2

(cid:32) n
(cid:88)

i=1

(cid:33) 1
2






(cid:107)¯rt(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel max

(cid:107)¯st(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel



1
2

(cid:107)2
(cid:107)αt
2
ˆi







m+r
(cid:88)

ˆi=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

using convergence parameters (cid:15)abs and (cid:15)rel.

5.2 Block Updates

We now describe how to implement the ADMM block updates (47), (48), and (49). Updating
the Lagrange multipliers α is a simple step in the gradient direction (47). Updating the
local copies yL (48) decomposes over each potential and constraint in the HL-MRF. For the

29

(47)

(48)

(49)

(51)

(52)

Bach, Broecheler, Huang, and Getoor

variables y(L,j) for each potential φj, this requires independently optimizing the weighted
potential plus a squared norm:

(cid:16)

(cid:110)

wj

max

(cid:96)j(y(L,j), x), 0

(cid:111)(cid:17)pj

arg min
y(L,j)

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(53)

There are three cases for y(cid:63)

Although this optimization problem is convex, the presence of the hinge function complicates
it. It could be solved in principle with an iterative method, such as an interior-point method,
but such methods would become very expensive over many ADMM updates. Fortunately,
we can reduce the problem to checking several cases and ﬁnd solutions much more quickly.
(L,j), the optimizer of problem (53), which correspond to
the three regions in which the solution could lie: (1) the region (cid:96)(y(L,j), x) < 0, (2) the
region (cid:96)(y(L,j), x) > 0, and (3) the region (cid:96)(y(L,j), x) = 0. We check each case by replacing
the potential with its value on the corresponding region, optimizing, and checking if the
optimizer is in the correct region. We check the ﬁrst case by replacing the potential φj
with zero. Then, the optimizer of the modiﬁed problem is y(C,j) − αj/ρ.
If (cid:96)j(y(C,j) −
αj/ρ, x) ≤ 0, then y(cid:63)
(L,j) = y(C,j) − αj/ρ, because it optimizes both the potential and the
squared norm independently. If instead (cid:96)j(y(C,j) − αj/ρ, x) > 0, then we can conclude that
(cid:96)j(y(cid:63)

(L,j), x) ≥ 0, leading to one of the next two cases.
In the second case, we replace the maximum term with the inner linear function. Then
the optimizer of the modiﬁed problem is found by taking the gradient of the objective with
respect to y(L,j), setting the gradient equal to the zero vector, and solving for y(L,j). In
other words, the optimizer is the solution for y(L,j) to the equation

(cid:34)

(cid:16)

∇y(L,j)

wj

(cid:96)j(y(L,j), x)

(cid:17)pj

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

(cid:35)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0 .

(54)

This condition deﬁnes a simple system of linear equations. If pj = 1, then the coeﬃcient
matrix is diagonal and trivial to solve. If pj = 2, then the coeﬃcient matrix is symmetric
and positive deﬁnite, and the system can be solved via Cholesky decomposition. (Since
the potentials of an HL-MRF often have shared structures, perhaps templated by a PSL
program, the Cholesky decompositions can be cached and shared among potentials for
improved performance.) Let y(cid:48)
(L,j) be the optimizer of the modiﬁed problem, i.e., the
solution to equation (54). If (cid:96)j(y(cid:48)
(L,j) = y(cid:48)
(L,j), x) ≥ 0, then y(cid:63)
(L,j) because we know the
solution lies in the region (cid:96)j(y(L,j), x) ≥ 0 and the objective of problem (53) and the
modiﬁed objective are equal on that region.
(L,j), x) ≥ 0
whenever (cid:96)j(y(C,j) − αj/ρ, x) ≥ 0, because the modiﬁed term is symmetric about the line
(cid:96)j(y(L,j), x) = 0. We therefore will only reach the following third case when pj = 1. If
(cid:96)j(y(C,j) − αj/ρ, x) > 0 and (cid:96)j(y(cid:48)
(L,j) is the
projection of y(C,j) − αj/ρ onto the hyperplane ck(y(L,j), x) = 0. This constraint must be
active because it is violated by the optimizers of both modiﬁed objectives (Martins et al.,
2015, Lemma 17). Since the potential has a value of zero whenever the constraint is active,
solving problem (53) reduces to the projection operation.

(L,j), x) < 0, then we can conclude that y(cid:63)

In fact, if pj = 2, then (cid:96)j(y(cid:48)

30

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

For the local copies y(L,k+m) for each constraint ck, the subproblem is easier:

(cid:104)

(cid:105)
ck(y(L,k+m), x)

+

χk

(cid:13)
(cid:13)
y(L,k+m) − y(C,k+m) +
(cid:13)
(cid:13)

ρ
2

1
ρ

arg min
y(L,k+m)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

αk+m

.

(55)

Whether ck is an equality or inequality constraint, the solution is the projection of y(C,k+m)−
αk+m/ρ to the feasible set deﬁned by the constraint. If ck is an equality constraint, i.e., k ∈
E, then the optimizer y(cid:63)
(L,k+m) is the projection of y(C,k+m)−αk+m/ρ onto ck(y(L,k+m), x) =
0. If, on the other hand, ck is an inequality constraint, i.e., k ∈ I, then there are two cases.
First, if ck(y(C,k+m) − αk+m/ρ, x) ≤ 0, then the solution is simply y(C,k+m) − αk+m/ρ.
Otherwise, it is again the projection onto ck(y(L,k+m), x) = 0.
To update the variables y (49), we solve the optimization

arg min
y

n
(cid:88)

i=1

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

χ[0,1] [yi] +

y

(L,ˆi)

− y

(C,ˆi)

+

(56)

1
ρ

(cid:13)
2
(cid:13)
αˆi
(cid:13)
(cid:13)
2

.

The optimizer is the state in which yi is set to the average of its corresponding local copies
added with their corresponding Lagrange multipliers divided by the step size ρ, and then
clipped to the [0, 1] interval. More formally, let copies(yi) be the set of local copies yc of
yi, each with a corresponding Lagrange multiplier αc. Then, we update each yi using

yi ←

1
|copies(yi)|

(cid:88)

yc∈copies(yi)

(cid:18)

yc +

(cid:19)

αc
ρ

(57)

and clip the result to [0, 1]. Speciﬁcally, if, after update (57), yi > 1, then we set yi to 1
and likewise set it to 0 if yi < 0.

Algorithm 1 shows the complete pseudocode for MAP inference. The method starts
by initializing local copies of the variables that appear in each potential and constraint,
along with a corresponding Lagrange multiplier for each copy. Then, until convergence, it
iteratively performs the updates (47), (48), and (49). In the pseudocode, we have interleaved
updates (47) and (48), updating both the Lagrange multipliers αˆi and the local copies y
(L,ˆi)
together for each subproblem, because they are local operations that do not depend on other
variables once y is updated in the previous iteration. This independence reveals another
advantage of our inference algorithm:
it is very easy to parallelize. The updates (47)
and (48) can be performed in parallel, the results gathered, update (49) performed, and the
updated y broadcast back to the subproblems. Parallelization makes our MAP inference
algorithm even faster and more scalable.

5.3 Lazy MAP Inference

One interesting and useful property of HL-MRFs is that it is not always necessary to
completely materialize the distribution in order to ﬁnd a MAP state. Consider a subset ˆφ
of the index set {1, . . . , m} of the potentials φ. Observe that if a feasible assignment to y
minimizes

(58)

wjφj(y, x)

(cid:88)

j∈ ˆφ

31

Bach, Broecheler, Huang, and Getoor

Algorithm 1 MAP Inference for HL-MRFs

Input: HL-MRF P (y|x), ρ > 0
Initialize y(L,j) as local copies of variables y(C,j) that are in φj, j = 1, . . . , m
Initialize y(L,k+m) as local copies of variables y(C,k+m) that are in ck, k = 1, . . . , r
, ˆi = 1, . . . , m + r
Initialize Lagrange multipliers αˆi corresponding to copies y

(L,ˆi)

(cid:96)j(y(L,j), x)

(cid:17)pj

+ ρ
2

(cid:13)
(cid:13)y(L,j) − y(C,j) + 1
(cid:13)

ρ αj

(cid:13)
2
(cid:13)
(cid:13)
2

while not converged do

for j = 1, . . . , m do

ρ αj

αj ← αj + ρ(y(L,j) − y(C,j))
y(L,j) ← y(C,j) − 1
if (cid:96)j(y(L,j), x) > 0 then
y(L,j) ← arg miny(L,j)
wj
if (cid:96)j(y(L,j), x) < 0 then

(cid:16)

y(L,j) ← Proj(cid:96)j =0(y(C,j) − 1

ρ αj)

end if

end if
end for

for k = 1, . . . , r do

αk+m ← αk+m + ρ(y(L,k+m) − y(C,k+m))
y(L,k+m) ← Projck

(y(C,k+m) − 1

ρ αk+m)

end for

for i = 1, . . . , n do

1
yi ←
|copies(yi)|
Clip yi to [0,1]

(cid:80)

end for

end while

yc∈copies(yi)

(cid:16)

yc + αc
ρ

(cid:17)

32

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and φj(y, x) = 0, ∀j /∈ ˆφ, then that assignment must be a MAP state because 0 is the
global minimum for any potential. Therefore, if we can identify a set of potentials that
is small, such that all the other potentials are 0 in a MAP state, then we can perform
MAP inference in a reduced amount of time. Of course, identifying this set is as hard as
MAP inference itself, but we can iteratively grow the set by starting with an initial set,
performing inference over the current set, adding any potentials that have nonzero values,
and repeating.

Since the lazy inference procedure requires that the assignment be feasible, there are
two ways to handle any constraints in the HL-MRF. One is to include all constraints in
the inference problem from the beginning. This strategy ensures feasibility, but the idea of
lazy grounding can also be extended to constraints to improve performance further. Just
as we check if potentials are unsatisﬁed, i.e., nonzero, we can also check if constraints are
unsatisﬁed, i.e., violated. So the algorithm now iteratively grows the set of active potentials
and active constraints, adding any that are unsatisﬁed until the MAP state of the HL-MRF
deﬁned by the active potentials and constraints is also a feasible MAP state of the true
HL-MRF.

The eﬃciency of lazy MAP inference can be improved heuristically by not adding all
unsatisﬁed potentials and constraints, but instead only adding those that are unsatisﬁed by
some threshold. This heuristic can decrease computational cost signiﬁcantly, although the
results are no longer guaranteed to be correct. Bounding the resulting error when possible
is an important direction for future work.

5.4 Evaluation of MAP Inference

In this section we evaluate the empirical performance of our MAP inference algorithm.4 We
compare its running times against those of MOSEK,5 a commercial convex optimization
toolkit that uses interior-point methods (IPMs). We conﬁrm the results of Yanover et al.
(2006) that IPMs do not scale well to large structured-prediction problems, and we show
that our MAP inference algorithm scales much better. In fact, we observe that our method
scales linearly in practice with the number of potentials and constraints in the HL-MRF.

We evaluate scalability by generating social networks of varying sizes, constructing HL-
MRFs over them, and measuring the running time required to ﬁnd a MAP state. We
compare our algorithm to MOSEK’s IPM. The social networks we generate are designed to
be representative of common social-network analysis tasks. We generate networks of users
that are connected by diﬀerent types of relationships, such as friendship and marriage, and
our goal is to predict the political preferences, e.g., liberal or conservative, of each user. We
also assume that we have local information about each user, representing features such as
demographic information.

We generate the social networks using power-law distributions according to a procedure
described by Broecheler et al. (2010b). For a target number of users N , in-degrees and out-
degrees d for each edge type are sampled from the power-law distribution D(k) ≡ αk−γ.
Incoming and outgoing edges of the same type are then matched randomly to create edges
until no more matches are possible. The number of users is initially the target number

4. Code is available at https://github.com/stephenbach/bach-jmlr17-code.
5. http://www.mosek.com

33

Bach, Broecheler, Huang, and Getoor

plus the expected number of users with zero edges, and then users without any edges are
removed. We use six edge types with various parameters to represent relationships in social
networks with diﬀerent combinations of abundance and exclusivity, choosing γ between 2
and 3, and α between 0 and 1, as suggested by Broecheler et al. We then annotate each
vertex with a value in [−1, 1] uniformly at random to represent local features indicating one
political preference or the other.

We generate social networks with between 22k and 66k vertices, which induce HL-
MRFs with between 130k and 397k total potentials and constraints. In all the HL-MRFs,
roughly 85% of those totals are potentials. For each social network, we create both a (log)
piecewise-linear HL-MRF (pj = 1, ∀j = 1, . . . , m in Deﬁnition 3) and a piecewise-quadratic
one (pj = 2, ∀j = 1, . . . , m). We weight local features with a parameter of 0.5 and choose
parameters in [0, 1] for the relationship potentials representing a mix of more and less
inﬂuential relationships.

We implement ADMM in Java and compare with the IPM in MOSEK (version 6) by
encoding the entire MPE problem as a linear program or a second-order cone program as
appropriate and passing the encoded problem via the Java native interface wrapper. All
experiments are performed on a single machine with a 4-core 3.4 GHz Intel Core i7-3770
processor with 32GB of RAM. Each optimizer used a single thread, and all results are
averaged over 3 runs.

We ﬁrst evaluate the scalability of ADMM when solving piecewise-linear MAP problems
and compare with MOSEK’s interior-point method. Figures 1a (normal scale) and 1c (log
scale) show the results. The running time of the IPM quickly explodes as the problem
size increases. The IPM’s average running time on the largest problem is about 2,200
seconds (37 minutes). This result demonstrates the limited scalability of the interior-point
method. In contrast, ADMM displays excellent scalability. The average running time on
the largest problem is about 70 seconds. Further, the running time appears to grow linearly
in the number of potential functions and constraints in the HL-MRF, i.e., the number of
subproblems that must be solved at each iteration. The line of best ﬁt for all runs on all sizes
has a coeﬃcient of determination R2 = 0.9972. Combined with Figure 1a, this shows that
ADMM scales linearly with increasing problem size in this experiment. We emphasize that
the implementation of ADMM is research code written in Java and the IPM is a commercial
package compiled to native machine code.

We then evaluate the scalability of ADMM when solving piecewise-quadratic MAP prob-
lem and again compare with MOSEK. Figures 1b (normal scale) and 1d (log scale) show
the results. Again, the running time of the interior-point method quickly explodes. We
can only test it on the three smallest problems, the largest of which took an average of
about 21k seconds to solve (over 6 hours). ADMM again scales linearly to the problem
(R2 = 0.9854). It is just as fast for quadratic problems as linear ones, taking average of
about 70 seconds on the largest problem.

One of the advantages of IPMs is great numerical stability and accuracy. Consensus
optimization, which treats both objective terms and constraints as subproblems, often re-
turns solutions that are only optimal and feasible to moderate precision for non-trivially
constrained problems (Boyd et al., 2011). Although this is often acceptable, we quantify
the mix of infeasibility and suboptimality by repairing the infeasibility and measuring the
resulting total suboptimality. We ﬁrst project the solutions returned by consensus opti-

34

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

(a) Linear MAP problems

(b) Quadratic MAP problems

(c) Linear MAP problems (log scale)

(d) Quadratic MAP problems (log scale)

Figure 1: Average running times to ﬁnd a MAP state for HL-MRFs.

35

Bach, Broecheler, Huang, and Getoor

mization onto the feasible region, which took a negligible amount of computational time.
Let pADMM be the value of the objective in Problem (45) at such a point and let pIPM be
the value of the objective at the solution returned by the IPM. Then the relative error on
that problem is (pADMM − pIPM)/pIPM. The relative error was consistently small; it varied
between 0.2% and 0.4%, and did not trend upward as the problem size increased. This
shows that ADMM was accurate, in addition to being much more scalable.

6. Weight Learning

In this section we present three weight learning methods for HL-MRFs, each with a diﬀerent
objective function. The ﬁrst method approximately maximizes the likelihood of the training
data. The second method maximizes the pseudolikelihood. The third method ﬁnds a large-
margin solution, preferring weights that discriminate the ground truth from other nearby
states. Since weights are often shared among many potentials deﬁned by a template, such
as all the groundings of a PSL rule, we describe these learning algorithms in terms of
templated HL-MRFs. We introduce some necessary notation for HL-MRF templates. Let
T = (t1, . . . , ts) denote a vector of templates with associated weights W = (W1, . . . , Ws).
We partition the potentials by their associated templates and let tq also denote the set of
indices of the potentials deﬁned by that template. So, j ∈ tq is a shorthand for saying
that the potential φj(y, x) was deﬁned by template tq. Then, we refer to the sum of the
potentials deﬁned by a template as

Φq(y, x) =

φj(y, x) .

(cid:88)

j∈tq

In the deﬁned HL-MRF, the weight of the j-th hinge-loss potential is set to the weight of
the template from which it was derived, i.e., wj = Wq, for each j ∈ tq. Equivalently, we can
rewrite the hinge-loss energy function as

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)).

fw(y, x) = W (cid:62)Φ(y, x) ,

6.1 Structured Perceptron and Approximate Maximum Likelihood Estimation

The canonical approach for learning parameters W is to maximize the log-likelihood of
training data. The partial derivative of the log-likelihood with respect to a parameter Wq
is

∂ log P (y|x)
∂Wq

= EW [Φq(y, x)] − Φq(y, x),

(61)

where EW is the expectation under the distribution deﬁned by W . For a smoother ascent,
it is often helpful to divide the q-th component of the gradient by the number of groundings
|tq| of the q-th template (Lowd and Domingos, 2007), which we do in our experiments.
Computing the expectation is intractable, so we use a common approximation (e.g., Collins,
2002; Singla and Domingos, 2005; Poon and Domingos, 2011): the values of the potentials
at the most probable setting of y with the current parameters, i.e., a MAP state. Using a
MAP state makes this learning approach a structured variant of voted perceptron (Collins,

36

(59)

(60)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

2002), and we expect it to do best when the space of explored distributions has relatively
low entropy. Following voted perceptron, we take steps of ﬁxed length in the direction of
the gradient, then average the points after all steps. Any step that is outside the feasible
region is projected back before continuing.

6.2 Maximum Pseudolikelihood Estimation

An alternative to structured perceptron is maximum-pseudolikelihood estimation (MPLE)
(Besag, 1975), which maximizes the likelihood of each variable conditioned on all other
variables, i.e.,

P ∗(y|x) =

P ∗(yi|MB(yi), x)

=

1
Zi(W , y, x)

exp (cid:2)−f i

w(yi, y, x)(cid:3) ;

Zi(W , y, x) =

exp (cid:2)−f i

f i
w(yi, y, x) =

wjφj

w(yi, y, x)(cid:3) ;
(cid:16)

{yi ∪ y\i}, x

(cid:17)

.

n
(cid:89)

i=1
n
(cid:89)

i=1
(cid:90)

yi
(cid:88)

j:i∈φj

(62)

(63)

(64)

(65)

Here, i ∈ φj means that yi is involved in φj, and MB(yi) denotes the Markov blanket of
yi—that is, the set of variables that co-occur with yi in any potential function. The partial
derivative of the log-pseudolikelihood with respect to Wq is

∂ log P ∗(y|x)
∂Wq

=





Eyi|MB

n
(cid:88)

i=1

(cid:88)

j∈tq:i∈φj



φj(y, x)

 − Φq(y, x) .

(66)

Computing the pseudolikelihood gradient does not require joint inference and takes time
linear in the size of y. However, the integral in the above expectation does not readily admit
a closed-form antiderivative, so we approximate the expectation. When a variable is uncon-
strained, the domain of integration is a one-dimensional interval on the real number line,
so Monte Carlo integration quickly converges to an accurate estimate of the expectation.

We can also apply MPLE when the constraints are not too interdependent. For example,
for linear equality constraints over disjoint groups of variables (e.g., variable sets that must
sum to 1.0), we can block-sample the constrained variables by sampling uniformly from a
simplex. These types of constraints are often used to represent categorical labels. We can
compute accurate estimates quickly because these blocks are typically low-dimensional.

6.3 Large-Margin Estimation

A diﬀerent approach to learning drops the probabilistic interpretation of the model and
views HL-MRF inference as a prediction function. Large-margin estimation (LME) shifts
the goal of learning from producing accurate probabilistic models to instead producing
accurate MAP predictions. The learning task is then to ﬁnd weights W that separate
the ground truth from other nearby states by a large margin. We describe in this section

37

Bach, Broecheler, Huang, and Getoor

a large-margin method based on the cutting-plane approach for structural support vector
machines (Joachims et al., 2009).

The intuition behind large-margin structured prediction is that the ground-truth state
should have energy lower than any alternate state by a large margin. In our setting, the
output space is continuous, so we parameterize this margin criterion with a continuous loss
function. For any valid output state ˜y, a large-margin solution should satisfy

fw(y, x) ≤ fw(˜y, x) − L(y, ˜y), ∀˜y,

(67)

where the loss function L(y, ˜y) measures the disagreement between a state ˜y and the train-
ing label state y. A common assumption is that the loss function decomposes over the
prediction components, i.e., L(y, ˜y) = (cid:80)
i L(yi, ˜yi). In this work, we use the (cid:96)1 distance
as the loss function, so L(y, ˜y) = (cid:80)
i (cid:107)yi − ˜yi(cid:107)1. Since we do not expect all problems to
be perfectly separable, we relax the large-margin constraint with a penalized slack ξ. We
obtain a convex learning objective for a large-margin solution

min
W ≥0

1
2

||W ||2 + Cξ

s.t. W (cid:62)(Φ(y, x) − Φ(˜y, x)) ≤ −L(y, ˜y) + ξ, ∀˜y,

(68)

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)) and C > 0 is a user-speciﬁed parameter. This for-
mulation is analogous to the margin-rescaling approach by Joachims et al. (2009). Though
such a structured objective is natural and intuitive, its number of constraints is the car-
dinality of the output space, which here is inﬁnite. Following their approach, we optimize
subject to the inﬁnite constraint set using a cutting-plane algorithm: we greedily grow a set
K of constraints by iteratively adding the worst-violated constraint given by a separation
oracle, then updating W subject to the current constraints. The goal of the cutting-plane
approach is to eﬃciently ﬁnd the set of active constraints at the solution for the full ob-
jective, without having to enumerate the inﬁnite inactive constraints. The worst-violated
constraint is

arg min
˜y

W (cid:62)Φ(˜y, x) − L(y, ˜y).

(69)

The separation oracle performs loss-augmented inference by adding additional potentials to
the HL-MRF. For ground truth in {0, 1}, these loss-augmenting potentials are also examples
of hinge-losses, and thus adding them simply creates an augmented HL-MRF. The worst-
violated constraint is then computed as standard inference on the loss-augmented HL-
MRF. However, ground truth values in the interior (0, 1) cause any distance-based loss to
be concave, which require the separation oracle to solve a non-convex objective. In this
case, we use the diﬀerence of convex functions algorithm (An and Tao, 2005) to ﬁnd a local
optimum. Since the concave portion of the loss-augmented inference objective pivots around
the ground truth value, the subgradients are 1 or −1, depending on whether the current
value is greater than the ground truth. We simply choose an initial direction for interior
labels by rounding, and ﬂip the direction of the subgradients for variables whose solution
states are not in the interval corresponding to the subgradient direction until convergence.

38

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Given a set K of constraints, we solve the SVM objective as in the primal form

||W ||2 + Cξ

min
W ≥0

1
2

s.t. K.

(70)

We then iteratively invoke the separation oracle to ﬁnd the worst-violated constraint. If
this new constraint is not violated, or its violation is within numerical tolerance, we have
found the max-margin solution. Otherwise, we add the new constraint to K, and repeat.

One fact of note is that the large-margin criterion always requires some slack for HL-
MRFs with squared potentials. Since the squared hinge potential is quadratic and the loss
is linear, there always exists a small enough distance from the ground truth such that an
absolute (i.e., linear) distance is greater than the squared distance. In these cases, the slack
parameter trades oﬀ between the peakedness of the learned quadratic energy function and
the margin criterion.

6.4 Evaluation of Learning

To demonstrate the ﬂexibility and eﬀectiveness of learning with HL-MRFs, we test them
on four diverse tasks: node labeling, link labeling, link prediction, and image completion.6
Each of these experiments represents a problem domain that is best solved with structured-
prediction approaches because their dependencies are highly structural. The experiments
show that HL-MRFs perform as well as or better than canonical approaches.

For these diverse tasks, we compare against a number of competing methods. For node
and link labeling, we compare HL-MRFs to discrete Markov random ﬁelds (MRFs). We
construct them with Markov logic networks (MLNs) (Richardson and Domingos, 2006),
which template discrete MRFs using logical rules similarly to PSL. We perform inference in
discrete MRFs using Gibbs sampling, and we ﬁnd approximate MAP states during learn-
ing using the search algorithm MaxWalkSat (Richardson and Domingos, 2006). For link
prediction for preference prediction, a task that is inherently continuous and nontrivial to
encode in discrete logic, we compare against Bayesian probabilistic matrix factorization
(BPMF) (Salakhutdinov and Mnih, 2008). Finally, for image completion, we run the same
experimental setup as Poon and Domingos (2011) and compare against the results they
report, which include tests using sum product networks, deep belief networks (Hinton and
Salakhutdinov, 2006), and deep Boltzmann machines (Salakhutdinov and Hinton, 2009).

We train HL-MRFs and discrete MRFs with all three learning methods: structured per-
ceptron (SP), maximum pseudolikelihood estimation(MPLE), and large-margin estimation
(LME). When appropriate, we evaluate statistical signiﬁcance using a paired t-test with re-
jection threshold 0.01. We describe the HL-MRFs used for our experiments using the PSL
rules that deﬁne them. To investigate the diﬀerences between linear and squared potentials
we use both in our experiments. HL-MRF-L refers to a model with all linear potentials
and HL-MRF-Q to one with all squared potentials. When training with SP and MPLE, we
use 100 gradient steps and a step size of 1.0 (unless otherwise noted), and we average the
iterates as in voted perceptron. For LME, we set C = 0.1. We experimented with various
settings, but the scores of HL-MRFs and discrete MRFs were not sensitive to changes.

6. Code is available at https://github.com/stephenbach/bach-jmlr17-code.

39

Bach, Broecheler, Huang, and Getoor

6.4.1 Node Labeling

When classifying documents, links between those documents—such as hyperlinks, citations,
or shared authorship—provide extra signal beyond the local features of individual docu-
ments. Collectively predicting document classes with these links tends to improve accuracy
(Sen et al., 2008). We classify documents in citation networks using data from the Cora
and Citeseer scientiﬁc paper repositories. The Cora data set contains 2,708 papers in seven
categories, and 5,429 directed citation links. The Citeseer data set contains 3,312 papers
in six categories, and 4,591 directed citation links. Let the predicate Category/2 represent
the category of each document and Cites/2 represent a citation from one document to
another.

The prediction task is, given a set of seed documents whose labels are observed, to infer
the remaining document classes by propagating the seed information through the network.
For each of 20 runs, we split the data sets 50/50 into training and testing partitions, and
seed half of each set. To predict discrete categories with HL-MRFs we predict the category
with the highest predicted value.

We compare HL-MRFs to discrete MRFs on this task. For prediction, we performed
2500 rounds of Gibbs sampling, 500 of which were discarded as burn-in. We construct both
using the same logical rules, which simply encode the tendency for a class to propagate
across citations. For each category "C i", we have the following two rules, one for each
direction of citation.

Category(A, "C i") && Cites(A, B) -> Category(B, "C i")

Category(A, "C i") && Cites(B, A) -> Category(B, "C i")

We also constrain the atoms of the Category/2 predicate to sum to 1.0 for a given document
as follows.

Category(D, +C) = 1.0 .

Table 1 lists the results of this experiment. HL-MRFs are the most accurate predictors on
both data sets. Both variants of HL-MRFs are also much faster than discrete MRFs. See
Table 3 for average inference times over ﬁve folds.

6.4.2 Link Labeling

An emerging problem in the analysis of online social networks is the task of inferring the
level of trust between individuals. Predicting the strength of trust relationships can provide
useful information for viral marketing, recommendation engines, and internet security. HL-
MRFs with linear potentials have been applied by Huang et al. (2013) to this task, showing
superior results with models based on sociological theory. We reproduce their experimen-
tal setup using their sample of the signed Epinions trust network, orginally collected by
Richardson et al. (2003), in which users indicate whether they trust or distrust other users.
We perform eight-fold cross-validation. In each fold, the prediction algorithm observes the
entire unsigned social network and all but 1/8 of the trust ratings. We measure predic-
tion accuracy on the held-out 1/8. The sampled network contains 2,000 users, with 8,675
signed links. Of these links, 7,974 are positive and only 701 are negative, making it a sparse
prediction task.

40

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Table 1: Average accuracy of classiﬁcation by HL-MRFs and discrete MRFs. Scores statis-
tically equivalent to the best scoring method are typed in bold.

Table 2: Average area under ROC and precision-recall curves of social-trust prediction by
HL-MRFs and discrete MRFs. Scores statistically equivalent to the best scoring method
by metric are typed in bold.

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

Citeseer Cora

0.729
0.729
0.683

0.724
0.729
0.695

0.686
0.715
0.687

0.816
0.818
0.789

0.802
0.808
0.789

0.756
0.797
0.783

ROC P-R (+) P-R (-)

0.822
HL-MRF-Q (SP)
HL-MRF-Q (MPLE) 0.832
0.814
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

0.765
0.757
0.783

0.655
0.725
0.795

0.978
0.979
0.976

0.965
0.963
0.967

0.942
0.963
0.973

0.452
0.482
0.462

0.357
0.333
0.453

0.270
0.298
0.441

41

Table 3: Average inference times (reported in seconds) of single-threaded HL-MRFs and
discrete MRFs.

Bach, Broecheler, Huang, and Getoor

Citeseer

Cora Epinions

HL-MRF-Q
HL-MRF-L
MRF

0.42
0.46
110.96

0.70
0.50
184.32

0.32
0.28
212.36

We use a model based on the social theory of structural balance, which suggests that so-
cial structures are governed by a system that prefers triangles that are considered balanced.
Balanced triangles have an odd number of positive trust relationships; thus, considering all
possible directions of links that form a triad of users, there are sixteen logical implications
of the following form.

Trusts(A,B) && Trusts(B,C) -> Trusts(A,C)

Huang et al. (2013) list all sixteen of these rules, a reciprocity rule, and a prior in their
Balance-Recip model, which we omit to save space.

Since we expect these structural implications to vary in accuracy, learning weights for
these rules provides better models. Again, we use these rules to deﬁne HL-MRFs and
discrete MRFs, and we train them using various learning algorithms. For inference with
discrete MRFs, we perform 5000 rounds of Gibbs sampling, of which the ﬁrst 500 are
burn-in. We compute three metrics: the area under the receiver operating characteristic
(ROC) curve, and the areas under the precision-recall curves for positive trust and negative
trust. On all three metrics, HL-MRFs with squared potentials score signiﬁcantly higher.
The diﬀerences among the learning methods for squared HL-MRFs are insigniﬁcant, but
the diﬀerences among the models is statistically signiﬁcant for the ROC metric. For area
under the precision-recall curve for positive trust, discrete MRFs trained with LME are
statistically tied with the best score, and both HL-MRF-L and discrete MRFs trained with
LME are statistically tied with the best area under the precision-recall curve for negative
trust. The results are listed in Table 2.

Though the random fold splits are not the same, using the same experimental setup,
Huang et al. (2013) also scored the precision-recall area for negative trust of standard trust
prediction algorithms EigenTrust (Kamvar et al., 2003) and TidalTrust (Golbeck, 2005),
which scored 0.131 and 0.130, respectively. The logical models based on structural balance
that we run here are signiﬁcantly more accurate, and HL-MRFs more than discrete MRFs.
In addition to comparing favorably with regard to predictive accuracy, inference in HL-
MRFs is also much faster than in discrete MRFs. Table 3 lists average inference times
on ﬁve folds of three prediction tasks: Cora, Citeseer, and Epinions. This illustrates an
important diﬀerence between performing structured prediction via convex inference versus
sampling in a discrete prediction space: convex inference can be much faster.

42

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

6.4.3 Link Prediction

Preference prediction is the task of inferring user attitudes (often quantiﬁed by ratings)
toward a set of items. This problem is naturally structured, since a user’s preferences
are often interdependent, as are an item’s ratings. Collaborative ﬁltering is the task of
predicting unknown ratings using only a subset of observed ratings. Methods for this
task range from simple nearest-neighbor classiﬁers to complex latent factor models. More
generally, this problem is an instance of link prediction, since the goal is to predict links
indicating preference between users and content. Since preferences are ordered rather than
Boolean, it is natural to represent them with the continuous variables of HL-MRFs, with
higher values indicating greater preference. To illustrate the versatility of HL-MRFs, we
design a simple, interpretable collaborative ﬁltering model for predicting humor preferences.
We test this model on the Jester dataset, a repository of ratings from 24,983 users on a set
of 100 jokes (Goldberg et al., 2001). Each joke is rated on a scale of [−10, +10], which we
normalize to [0, 1]. We sample a random 2,000 users from the set of those who rated all 100
jokes, which we then split into 1,000 train and 1,000 test users. From each train and test
matrix, we sample a random 50% to use as the observed features x; the remaining ratings
are treated as the variables y.

Our HL-MRF model uses an item-item similarity rule:

SimRating(J1, J2) && Likes(U, J1) -> Likes(U, J2)

where J1 and J2 are jokes and U is a user; the predicate Likes/2 indicates the degree
of preference (i.e., rating value); and SimRating/2 is a closed predicate that measures
the mean-adjusted cosine similarity between the observed ratings of two jokes. We also
include the following rules to enforce that Likes(U,J) concentrates around the observed
average rating of user U (represented with the predicate AvgUserRating/1) and item J
(represented with the predicate AvgJokeRating/1), and the global average (represented
with the predicate AvgRating/1).

AvgUserRating(U) -> Likes(U, J)

Likes(U, J) -> AvgUserRating(U)

AvgJokeRating(J) -> Likes(U, J)

Likes(U, J) -> AvgJokeRating(J)

AvgRating("constant") -> Likes(U, J)

Likes(U, J) -> AvgRating("constant")

The atom AvgRating("constant") takes a placeholder constant as an argument, since there
is only one grounding of it for the entire HL-MRF. Again, all three of these predicates are
closed and computed using averages of observed ratings. In all cases, the observed ratings
are taken only from the training data for learning (to avoid leaking information about the
test data) and only from the test data during testing.

We compare our HL-MRF model to a canonical latent factor model, Bayesian proba-
bilistic matrix factorization (BPMF) (Salakhutdinov and Mnih, 2008). BPMF is a fully
Bayesian treatment and is therefore considered “parameter-free;” the only parameter that
must be speciﬁed is the rank of the decomposition. Based on settings used by Xiong et al.

43

Table 4: Normalized mean squared/absolute errors (NMSE/NMAE) for preference predic-
tion using the Jester dataset. The lowest errors are typed in bold.

Bach, Broecheler, Huang, and Getoor

NMSE NMAE

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

0.0554
0.0549
0.0738

0.0578
0.0535
0.0544

0.1974
0.1953
0.2297

0.2021
0.1885
0.1875

BPMF

0.0501 0.1832

Table 5: Mean squared errors per pixel for image completion. HL-MRFs produce the
most accurate completions on the Caltech101 and the left-half Olivetti faces, and only sum-
product networks produce better completions on Olivetti bottom-half faces. Scores for other
methods are reported in Poon and Domingos (2011).

HL-MRF-Q (SP) SPN DBM DBN PCA NN

Caltech-Left
Caltech-Bottom
Olivetti-Left
Olivetti-Bottom

1741
1910
927
1226

1815
1924
942
918

2998
2656
1866
2401

4960
3447
2386
1931

2851
1944
1076
1265

2327
2575
1527
1793

(2010), we set the rank of the decomposition to 30 and use 100 iterations of burn in and 100
iterations of sampling. For our experiments, we use the code of Xiong et al. (2010). Since
BPMF does not train a model, we allow BPMF to use all of the training matrix during the
prediction phase.

Table 4 lists the normalized mean squared error (NMSE) and normalized mean absolute
error (NMAE), averaged over 10 random splits. Though BPMF produces the best scores,
the improvement over HL-MRF-L (LME) is not signiﬁcant in NMAE.

6.4.4 Image Completion

Digital image completion requires models that understand how pixels relate to each other,
such that when some pixels are unobserved, the model can infer their values from parts of the
image that are observed. We construct pixel-grid HL-MRFs for image completion. We test
these models using the experimental setup of Poon and Domingos (2011): we reconstruct
images from the Olivetti face data set and the Caltech101 face category. The Olivetti data
set contains 400 images, 64 pixels wide and tall, and the Caltech101 face category contains
435 examples of faces, which we crop to the center 64 by 64 patch, as was done by Poon

44

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Figure 2: Example results on image completion of Caltech101 (left) and Olivetti (right)
faces. From left to right in each column: (1) true face, left side predictions by (2) HL-
MRFs and (3) SPNs, and bottom half predictions by (4) HL-MRFs and (5) SPNs. SPN
completions are downloaded from Poon and Domingos (2011).

and Domingos (2011). Following their experimental setup, we hold out the last ﬁfty images
and predict either the left half of the image or the bottom half.

The HL-MRFs in this experiment are much more complex than the ones in our other
experiments because we allow each pixel to have its own weight for the following rules,
which encode agreement or disagreement between neighboring pixels:

Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

where Bright("P ij", I) is the normalized brightness of pixel "P ij" in image I, and
North("P ij", Q) indicates that Q is the north neighbor of "P ij". We similarly include
analogous rules for the south, east, and west neighbors, as well as the pixels mirrored across
the horizontal and vertical axes. This setup results in up to 24 rules per pixel, (boundary
pixels may not have north, south, east, or west neighbors) which, in a 64 by 64 image,
produces 80,896 PSL rules.

We train these HL-MRFs using SP with a 5.0 step size on the ﬁrst 200 images of each data
set and test on the last ﬁfty. For training, we maximize the data log-likelihood of uniformly
random held-out pixels for each training image, allowing for generalization throughout the
image. Table 5 lists our results and others reported by Poon and Domingos (2011) for sum-
product networks (SPN), deep Boltzmann machines (DBM), deep belief networks (DBN),
principal component analysis (PCA), and nearest neighbor (NN). HL-MRFs produce the
best mean squared error on the left- and bottom-half settings for the Caltech101 set and
the left-half setting in the Olivetti set. Only sum product networks produce lower error

45

Bach, Broecheler, Huang, and Getoor

on the Olivetti bottom-half faces. Some reconstructed faces are displayed in Figure 2,
where the shallow, pixel-based HL-MRFs produce comparably convincing images to sum-
product networks, especially in the left-half setting, where HL-MRFs can learn which pixels
are likely to mimic their horizontal mirror. While neither method is particularly good at
reconstructing the bottom half of faces, the qualitative diﬀerence between the deep SPN
and the shallow HL-MRF completions is that SPNs seem to hallucinate diﬀerent faces, often
with some artifacts, while HL-MRFs predict blurry shapes roughly the same pixel intensity
as the observed, top half of the face. The tendency to better match pixel intensity helps
HL-MRFs score better quantitatively on the Caltech101 faces, where the lighting conditions
are more varied than in Olivetti faces.

Training and predicting with these HL-MRFs takes little time.

In our experiments,
training each model takes about 45 minutes on a 12-core machine, while predicting takes
under a second per image. While Poon and Domingos (2011) report faster training with
SPNs, both HL-MRFs and SPNs clearly belong to a class of faster models when compared
to DBNs and DBMs, which can take days to train on modern hardware.

7. Related Work

Researchers in artiﬁcial intelligence and machine learning have long been interested in pre-
dicting interdependent unknowns using structural dependencies. Some of the earliest work
in this area is inductive logic programming (ILP) (Muggleton and De Raedt, 1994), in
which structural dependencies are described with ﬁrst-order logic. Using ﬁrst-order logic
has several advantages. First, it can capture many types of dependencies among variables,
such as correlations, anti-correlations, and implications. Second, it can compactly specify
dependencies that hold across many diﬀerent sets of propositions by using variables as wild-
cards that match entities in the data. These features enable the construction of intuitive,
general-purpose models that are easily applicable or adapted to diﬀerent domains. Inference
for ILP ﬁnds the propositions that satisfy a query, consistent with a relational knowledge
base. However, ILP is limited by its diﬃculty in coping with uncertainty. Standard ILP
approaches only model dependencies which hold universally, and such dependencies are rare
in real-world data.

Another broad area of research, probabilistic methods, directly models uncertainty over
unknowns. Probabilistic graphical models (PGMs) (Koller and Friedman, 2009) are a fam-
ily of formalisms for specifying joint distributions over interdependent unknowns through
graphical structures. The graphical structure of a PGM generally represents conditional
independence relationships among random variables. Explicitly representing conditional
independence relationships allows a distribution to be more compactly parametrized. For
example, in the worst case, a discrete distribution could be represented by an exponen-
tially large table over joint assignments to the random variables. However, describing the
distribution in smaller, conditionally independent pieces can be much more compact. Sim-
ilar beneﬁts apply to continuous distributions. Algorithms for probabilistic inference and
learning can also operate over the conditionally independent pieces described by the graph
structure. They are therefore straightforward to apply to a wide variety of distributions.
Categories of PGMs include Markov random ﬁelds (MRFs), Bayesian networks (BNs), and

46

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

dependency networks (DNs). Constructing PGMs often requires careful design, and models
are usually constructed for single tasks and data sets.

More recently, researchers have sought to combine the advantages of relational and
probabilistic approaches, creating the ﬁeld of statistical relational learning (SRL) (Getoor
and Taskar, 2007). SRL techniques build probabilistic models of relational data, i.e., data
composed of entities and relationships connecting them. Relational data is most often
described using a relational calculus, but SRL techniques are also equally applicable to
similar categories of data that go by other names, such as graph data or network data.
Modeling relational data is inherently complicated by the large number of interconnected
and overlapping structural dependencies that are typically present. This complication has
motivated two directions of work. The ﬁrst direction is algorithmic, seeking inference and
learning methods that scale up to high dimensional models. The other direction is both
user-oriented and—as a growing body of evidence shows—supported by learning theory,
seeking formalisms for compactly specifying entire groups of dependencies in the model
that share both form and parameters. Specifying these grouped dependencies, often in the
form of templates via a domain-speciﬁc language, is convenient for users. Most often in
relational data the structural dependencies hold without regard to the identities of entities,
instead being induced by an entity’s class (or classes) and the structure of its relationships
with other entities. Therefore, many SRL models and languages give users the ability to
specify dependencies in this abstract form and ground out models over speciﬁc data sets
based on these deﬁnitions. In addition to convenience, recent work in learning theory says
that repeated dependencies with tied parameters can be the key to generalizing from a
few—or even one—large, structured training example(s) (London et al., 2016).

A related ﬁeld to SRL is structured prediction (SP) (Bakir et al., 2007; Nowozin et al.,
2016), which generalizes the tasks of classiﬁcation and regression to the task of predict-
ing structured objects. The loss function used during learning is generalized to a task-
appropriate loss function that scores disagreement between predictions and the true struc-
tures. Often, models for structured prediction take the form of energy functions that are
linear in their parameters. Therefore, prediction with such models is equivalent to MAP
inference for MRFs. A distinct branch of SP is learn-to-search methods, in which the prob-
lem is decomposed into a series of one-dimension prediction problems. The challenge is to
learn a good order in which to predict the components of the structure, so that each one-
dimension prediction problem can be conditioned on the most useful information. Examples
of learn-to-search methods include incremental structured perceptron (Collins and Roark,
2004), SEARN (Daum´e III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross
and Bagnell, 2014).

In this paper we focus on SP methods that perform joint prediction directly. Better
understanding the diﬀerences and relative advantages of joint-prediction methods and learn-
to-search methods is an important direction for future work.
In the rest of this section
we survey models and domain-speciﬁc languages for SP and SRL (Section 7.1), inference
methods (Section 7.2), and learning methods (Section 7.3).

47

Bach, Broecheler, Huang, and Getoor

7.1 Models and Languages

SP and SRL encompass many approaches. One broad area of work—of which PSL is a
part—uses ﬁrst-order logic and other relational formalisms to specify templates for PGMs.
Probabilistic relational models (Friedman et al., 1999) deﬁne templates for BNs in terms of a
database schema, and they can be grounded out over instances of that schema to create BNs.
Relational dependency networks (Neville and Jensen, 2007) template RNs using structured
query language (SQL) queries over a relational schema. Markov logic networks (MLNs)
(Richardson and Domingos, 2006) use ﬁrst-order logic to deﬁne Boolean MRFs. Each logical
clause in a ﬁrst-order knowledge base is a template for a set of potentials when the MLN
is grounded out over a set of propositions. Whether each proposition is true is a Boolean
random variable, and the potential has a value of one when the corresponding ground clause
is satisﬁed by the propositions and zero when it is not. (MLNs are formulated such that
higher values of the energy function are more probable.) Clauses can either be weighted,
in which case the potential has the weight of the clause that templated it, or unweighted,
in which case in must hold universally, as in ILP. In these ways, MLNs are similar to
PSL. Whereas MLNs are deﬁned over Boolean variables, PSL is a templating language
for HL-MRFs, which are deﬁned over continuous variables. However, these continuous
variables can be used to model discrete quantities. See Section 2 for more information
on the relationships between HL-MRFs and discrete MRFs, and Section 6.4 for empirical
comparisons between the two. As we show, HL-MRFs and PSL scale much better while
In addition,
retaining the rich expressivity and accuracy of their discrete counterparts.
HL-MRFs and PSL can reason directly about continuous data.

PSL is part of a broad family of probabilistic programming languages (Gordon et al.,
2014). The goals of probabilistic programming and SRL often overlap. Probabilistic pro-
gramming seeks to make constructing probabilistic models easy for the end user, and sep-
arate model speciﬁcation from the development of inference and learning algorithms.
If
algorithms can be developed for the entire space of models covered by a language, then it
is easy for users to experiment with including and excluding diﬀerent model components.
It also makes it easy for existing models to beneﬁt from improved algorithms. Separation
of model speciﬁcation and algorithms is also useful in SRL for the same reasons. In this
paper we emphasize designing algorithms that are ﬂexible enough to support the full class of
HL-MRFs. Examples of probabilistic programming languages include IBAL (Pfeﬀer, 2001),
BLOG (Milch et al., 2005), Markov logic (Richardson and Domingos, 2006), ProbLog (De
Raedt et al., 2007), Church (Goodman et al., 2008), Figaro (Pfeﬀer, 2009), FACTORIE
(McCallum et al., 2009), Anglican (Wood et al., 2014), and Edward (Tran et al., 2016).

Other formalisms have also been proposed for probabilistic reasoning over continuous
domains and other domains equipped with semirings. Hybrid Markov logic networks (Wang
and Domingos, 2008) mix discrete and continuous variables. In addition to the dependencies
over discrete variables supported by MLNs, they support soft equality constraints between
two variables of the same form as those deﬁned by squared arithmetic rules in PSL, as well
as linear potentials of the form y1 − y2 for a soft inequality constraint y1 > y2. Inference in
hybrid MLNs is intractable. Wang and Domingos (2008) propose a random walk algorithm
for approximate MAP inference. Another related formalism is aProbLog (Kimmig et al.,
2011), which generalizes ProbLog to allow clauses to be annotated with elements from a

48

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

semiring, generalizing ProbLog’s support for clauses annotated with probabilities. Many
common inference tasks can be generalized from this perspective as algebraic model counting
(Kimmig et al., 2016). The PITA system (Riguzzi and Swift, 2011) for probabilistic logic
programming can also be viewe as implementing inference over various semirings.

7.2 Inference

Whether viewed as MAP inference for an MRF or SP without probabilistic semantics,
searching over a structured space to ﬁnd the optimal prediction is an important but diﬃcult
task. It is NP-hard in general (Shimony, 1994), so much work has focused on approximations
and identifying classes of problems for which it is tractable. A well-studied approximation
technique is local consistency relaxation (LCR) (Wainwright and Jordan, 2008). Inference
is ﬁrst viewed as an equivalent optimization over the realizable expected values of the
potentials, called the marginal polytope. When the variables are discrete and each potential
is an indicator that a subset of variables is in a certain state, this optimization becomes a
linear program. Each variable in the program is the marginal probability that a variable
is a particular state or the variables associated with a potential are in a particular joint
state. The marginal polytope is then the set of marginal probabilities that are globally
consistent. The number of linear constraints required to deﬁne the marginal polytope is
exponential in the size of the problem, however, so the linear program has to be relaxed in
order to be tractable. In a local consistency relaxation, the marginal polytope is relaxed
to the local polytope, in which the marginals over variables and potential states are only
locally consistent in the sense that each marginal over potential states sums to the marginal
distributions over the associated variables.

A large body of work has focused on solving the LCR objective quickly. Typically,
oﬀ-the-shelf convex optimization methods do not scale well for large graphical models and
structured predictors (Yanover et al., 2006), so a large branch of research has investigated
highly scalable message-passing algorithms. One approach is dual decomposition (DD)
(Sontag et al., 2011), which solves a problem dual to the LCR objective. Many DD algo-
rithms use coordinate descent, such as TRW-S (Kolmogorov, 2006), MSD (Werner, 2007),
MPLP (Globerson and Jaakkola, 2007), and ADLP (Meshi and Globerson, 2011). Other
DD algorithms use subgradient-based approaches (e.g., Jojic et al., 2010; Komodakis et al.,
2011; Schwing et al., 2012).

Another approach to solving the LCR objective uses message-passing algorithms to
solve the problem directly in its primal form. One well-known algorithm is that of Raviku-
mar et al. (2010a), which uses proximal optimization, a general approach that iteratively
improves the solution by searching for nearby improvements. The authors also provide
rounding guarantees for when the relaxed solution is integral, i.e., the relaxation is tight,
allowing the algorithm to converge faster. Another message-passing algorithm that solves
the primal objective is AD3 (Martins et al., 2015), which uses the alternating direction
method of multipliers (ADMM). AD3 optimizes objective (10) for binary, pairwise MRFs
and supports the addition of certain deterministic constraints on the variables. A third ex-
ample of a primal message-passing algorithm is APLP (Meshi and Globerson, 2011), which
is the primal analog of ADLP. Like AD3, it uses ADMM to optimize the objective.

49

Bach, Broecheler, Huang, and Getoor

Other approaches to approximate inference include tighter linear programming relax-
ations (Sontag et al., 2008, 2012). These tighter relaxations enforce local consistency on
variable subsets that are larger than individual variables, which makes them higher-order
local consistency relaxations. Mezuman et al. (2013) developed techniques for special cases
of higher-order relaxations, such as when the MRF contains cardinality potentials, in which
the probability of a conﬁguration depends on the number of variables in a particular state.
Researchers have also explored nonlinear convex programming relaxations, e.g., Ravikumar
and Laﬀerty (2006) and Kumar et al. (2006).

Previous analyses have identiﬁed particular subclasses whose local consistency relax-
ations are tight, i.e., the maximum of the relaxed program is exactly the maximum of the
original problem. These special classes include graphical models with tree-structured depen-
dencies, models with submodular potential functions, models encoding bipartite matching
problems, and those with nand potentials and perfect graph structures (Wainwright and
Jordan, 2008; Schrijver, 2003; Jebara, 2009; Foulds et al., 2011). Researchers have also
studied performance guarantees of other subclasses of the ﬁrst-order local consistency re-
laxation. Kleinberg and Tardos (2002) and Chekuri et al. (2005) considered the metric
labeling problem. Feldman et al. (2005) used the local consistency relaxation to decode
binary linear codes.

In this paper we examine the classic problem of MAX SAT—ﬁnding a joint Boolean
assignment to a set of propositions that maximizes the sum of a set of weighted clauses
that are satisﬁed—as an instance of SP. Researchers have also considered approaches to
solving MAX SAT other than the one one we study, the randomized algorithm of Goemans
and Williamson (1994). One line of work focusing on convex programming relaxations has
obtained stronger rounding guarantees than Goemans and Williamson (1994) by using non-
linear programming, e.g., Asano and Williamson (2002) and references therein. Other work
does not use the probabilistic method but instead searches for discrete solutions directly,
e.g., Mills and Tsang (2000), Larrosa et al. (2008), and Choi et al. (2009). We note that
one such approach, that of Wah and Shang (1997), is essentially a type of DD formulated
for MAX SAT. A more recent approach blends convex programming and discrete search via
mixed integer programming (Davies and Bacchus, 2013). Additionally, Huynh and Mooney
(2009) introduced a linear programming relaxation for MLNs inspired by MAX SAT re-
laxations, but the relaxation of general Markov logic provides no known guarantees on the
quality of solutions.

Finally, lifted inference takes advantage of symmetries in probability distributions to re-
duce the amount of work required for inference. Some of the earliest approaches identiﬁed
repeated dependency structures in PGMs to avoid repeated computations (Koller and Pfef-
fer, 1997; Pfeﬀer et al., 1999). Lifted inference has been widely applied in SRL because the
templates that are commonly used to deﬁne PGMs often induce symmetries. Various infer-
ence techniques for discrete MRFs have been extended to a lifted approach, including belief
propagation (Jaimovich et al., 2007; Singla and Domingos, 2008; Kersting et al., 2009) and
Gibbs sampling (Venugopal and Gogate, 2012). Approaches to lifted convex optimization
(Mladenov et al., 2012) might be extended to HL-MRFs. See de Salvo Braz et al. (2007),
Kersting (2012), and Kimmig et al. (2015) for more information on lifted inference.

50

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

7.3 Learning

Taskar et al. (2004) connected SP and PGMs by showing how to train MRFs with large-
margin estimation, a generalization of the large-margin objective for binary classiﬁcation
used to train support vector machines (Vapnik, 2000). Large-margin learning is a well-
studied approach to train structured predictors because it directly incorporates the struc-
tured loss function into a convex upper bound on the true objective: the regularized ex-
pected risk. The learning objective is to ﬁnd the parameters with smallest norm such that
a linear combination of feature functions assign a better score to the training data than
all other possible predictions. The amount by which the score of the correct prediction
must exceed the score of other predictions is scaled using the structured loss function.
The objective is therefore encoded as a norm minimization problem subject to many linear
constraints, one for each possible prediction in the structured space.

Structured SVMs (Tsochantaridis et al., 2005) extend large-margin estimation to a broad
class of structured predictors and admit a tractable cutting-plane learning algorithm. This
algorithm will terminate in a number of iterations linear in the size of the problem, and so
the computational challenge of large-margin learning for structured prediction comes down
to the task of ﬁnding the most violated constraint in the learning objective. This can be
accomplished by optimizing the energy function plus the loss function. In other words, the
task is to ﬁnd the structure that is the best combination of being favored by the energy
function but unfavored by the loss function. Often, the loss function decomposes over the
components of the prediction space, so the combined energy function and loss function can
often be viewed as simply the energy function of another structured predictor that is equally
challenging or easy to optimize, such as when the space of structures is a set of discrete
vectors and the loss function is the Hamming distance.

It is common during large-margin estimation that no setting of the parameters can
predict all the training data without error. In this case, the training data is said to not
be separable, again generalizing the notion of linear separability in the feature space from
binary classiﬁcation. The solution to this problem is to add slack variables to the constraints
that require the training data to be assigned the best score. The magnitude of the slack
variables are penalized in the learning objective, so estimation must trade oﬀ between the
norm of the parameters and violating the constraints. Joachims et al. (2009) extend this
formulation to a “one slack” formulation, in which a single slack variable is used for all the
constraints across all training examples, which is more eﬃcient. We use this framework for
large-margin estimation for HL-MRFs in Section 6.3.

The repeated inferences required for large-margin learning, one to ﬁnd the most-violated
constraint at each iteration, can become computationally expensive. Therefore researchers
have explored speeding up learning by interleaving the inference problem with the learning
problem. In the cutting-plane formulation discussed above, the objective is equivalently a
saddle-point problem, with the solution at the minimum with respect to the parameters and
the maximum with respect to the inference variables. Taskar et al. (2005) proposed dualizing
the inner inference problem to form a joint minimization. For SP problems with a tight
duality gap, i.e., the dual problem has the same optimal value as the primal problem, this
approach leads to an equivalent, convex optimization that can be solved for all variables
In other words, the learning and most-violated constraint problems are
simultaneously.

51

Bach, Broecheler, Huang, and Getoor

solved simultaneously, greatly reducing training time. For problems with non-tight duality
gaps, e.g., MAP inference in general, discrete MRFs, Meshi et al. (2010) showed that
the same principle can be applied by using approximate inference algorithms like dual
decomposition to bound the primal objective.

A related problem to parameter learning is structure learning, i.e., identifying an ac-
curate dependency structure for a model. A common SRL approach is searching over the
space of templates for PGMs. For probabilistic relational models, Friedman et al. (1999)
learned structures described in the vocabulary of relational schemas. For models that are
templated with ﬁrst-order-logic-like languages, such as PSL and MLNs, these approaches
take the form of rule learning. Based on rule-learning techniques from inductive logic pro-
gramming (e.g., Richards and Mooney, 1992; De Raedt and Dehaspe, 1996) a series of
approaches have sought to learn MLN rules from relational data. Initially, Kok and Domin-
gos (2005) learned rules by generating candidates and performing a beam search to identify
rules that improved a weighted pseudolikelihood objective. Then, Mihalkova and Mooney
(2007) observed that the previous approach generated candidate rules without regard to
the data, so they introduced an approach that used the data to guide the proposal of rules
via relational pathﬁnding. Kok and Domingos (2010) improved on that by ﬁrst perform-
ing graph clustering to ﬁnd common motifs, which are common subgraphs, to guide rule
proposal. They observed that modifying a rule set one clause at a time often got stuck
in poor local optima, and by using the motifs as reﬁnement operators instead, they were
able to converge to better optima. Other approaches to structure learning search directly
over grounded PGMs, including (cid:96)1-regularized pseudolikelihood maximization (Ravikumar
et al., 2010b) and grafting (Perkins et al., 2003; Zhu et al., 2010). These methods can all
be extended to HL-MRFs and PSL.

8. Conclusion

In this paper we introduced HL-MRFs, a new class of probabilistic graphical models that
unite and generalize several approaches to modeling relational and structured data: Boolean
logic, probabilistic graphical models, and fuzzy logic. HL-MRFs can capture relaxed, prob-
abilistic inference with Boolean logic and exact, probabilistic inference with fuzzy logic,
making them useful models for both discrete and continuous data. HL-MRFs also general-
ize these inference techniques with additional expressivity, allowing for even more ﬂexibility.
HL-MRFs are a signiﬁcant addition to the the library of machine learning tools because
they embody a useful point in the spectrum of models that trade oﬀ between scalability
and expressivity. As we showed, they can be easily applied to a wide range of structured
problems in machine learning and achieve high-quality predictive performance, competitive
with or surpassing the performance of canonical approaches. However, these other models
either do not scale as well, like discrete MRFs, or are not as versatile in their ability to
capture a wide range of problems, like Bayesian probabilistic matrix factorization.

We also introduced PSL, a probabilistic programming language for HL-MRFs. PSL
makes HL-MRFs easy to design, allowing users to encode their ideas for structural depen-
dencies using an intuitive syntax based on ﬁrst-order logic. PSL also helps accelerate a
time-consuming aspect of the modeling process: reﬁning a model. In contrast with other
types of models that require specialized inference and learning algorithms depending on

52

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

which structural dependencies are included, HL-MRFs can encode many types of depen-
dencies and scale well with the same inference and learning algorithms. PSL makes it easy
to quickly add, remove, and modify dependencies in the model and rerun inference and
learning, allowing users to quickly improve the quality of their models. Finally, because
PSL uses a ﬁrst-order syntax, each PSL program actually speciﬁes an entire class of HL-
MRFs, parameterized by the particular data set over which it is grounded. Therefore, a
model or components of a model reﬁned for one data set can easily be applied to others.

Next, we introduced inference and learning algorithms that scale to large problems. The
MAP inference algorithm is far more scalable than standard tools for convex optimization
because it leverages the sparsity that is so common to the dependencies in structured
prediction. The supervised learning algorithms extend standard learning objectives to HL-
MRFs. Together, this combination of an expressive formalism, a user-friendly probabilistic
programming language, and highly scalable algorithms enables researchers and practitioners
to easily build large-scale, accurate models of relational and structured data.7

This paper also lays the foundation for many lines of future work. Our analysis of local
consistency relaxation (LCR) as a hierarchical optimization is a general proof technique,
and it could be used to derive compact forms for other LCR objectives. As in the case of
MRFs deﬁned using logical clauses, such compact forms can simplify analysis and could
lead to a greater understanding of LCR for other classes of MRFs. Another important line
of work is understanding what guarantees apply to the MAP states of HL-MRFs. Can
anything be said about their ability to approximate MAP inference in discrete models that
go beyond the models already covered by the known rounding guarantees? Future directions
also include developing new algorithms for HL-MRFs. One important direction is marginal
inference for HL-MRFs and algorithms for sampling from them. Unlike marginal inference
for discrete distributions, which computes the marginal probability that a variable is in a
particular state, marginal inference for HL-MRFs requires ﬁnding the marginal probability
that a variable is in a particular range. One option for doing so, as well as generating samples
from HL-MRFs, is to extend the hit-and-run sampling scheme of Broecheler and Getoor
(2010). This method was developed for continuous constrained MRFs with piecewise-linear
potentials. There are also many new domains to which HL-MRFs and PSL can be applied.
With these modeling tools, researchers can design and apply new solutions to structured
prediction problems.

Acknowledgments

We acknowledge the many people who have contributed to the development of HL-MRFs
and PSL. Contributors include Eriq Augustine, Shobeir Fakhraei, James Foulds, Angelika
Kimmig, Stanley Kok, Ben London, Hui Miao, Lilyana Mihalkova, Dianne P. O’Leary, Jay
Pujara, Arti Ramesh, Theodoros Rekatsinas, and V.S. Subrahmanian. This work was sup-
ported by NSF grants CCF0937094 and IIS1218488, and IARPA via DoI/NBC contract
number D12PC00337. The U.S. Government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those of the authors and should

7. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

53

Bach, Broecheler, Huang, and Getoor

not be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.

Appendix A. Proof of Theorem 2

In this appendix, we prove the equivalence of objectives (7) and (10). Our proof analyzes
the local consistency relaxation to derive an equivalent, more compact optimization over
only the variable pseudomarginals µ that is identical to the MAX SAT relaxation. Since the
variables are Boolean, we refer to each pseudomarginal µi(1) as simply µi. Let xF
j denote
the unique setting such that φj(xF
j ) = 0. (I.e., xF
is the setting in which each literal in the
j
clause Cj is false.)

We begin by reformulating the local consistency relaxation as a hierarchical optimiza-
tion, ﬁrst over the variable pseudomarginals µ and then over the factor pseudomarginals θ.
Due to the structure of local polytope L, the pseudomarginals µ parameterize inner linear
programs that decompose over the structure of the MRF, such that—given ﬁxed µ—there is
an independent linear program ˆφj(µ) over θj for each clause Cj. We rewrite objective (10)
as

arg max
µ∈[0,1]n

(cid:88)

ˆφj(µ),

Cj ∈C

where

ˆφj(µ) = max
θj

wj

(cid:88)

θj(xj)

xj |xj (cid:54)=xF
j

(cid:88)

such that

θj(xj) = µi

θj(xj) = 1 − µi

xj |xj (i)=1
(cid:88)

xj |xj (i)=0
(cid:88)

θj(xj) = 1

xj
θj(xj) ≥ 0

∀i ∈ I +
j

∀i ∈ I −
j

∀xj .

(71)

(72)

(73)

(74)

(75)

(76)

It is straightforward to verify that objectives (10) and (71) are equivalent for MRFs with
disjunctive clauses for potentials. All constraints deﬁning L can be derived from the con-
straint µ ∈ [0, 1]n and the constraints in the deﬁnition of ˆφj(µ). We have omitted redundant
constraints to simplify analysis.

To make this optimization more compact, we replace each inner linear program ˆφj(µ)
with an expression that gives its optimal value for any setting of µ. Deriving this expression
j of ˆφj(µ), which is guaranteed to exist because
requires reasoning about any maximizer θ(cid:63)
problem (72) is bounded and feasible8 for any parameters µ ∈ [0, 1]n and wj.

We ﬁrst derive a suﬃcient condition for the linear program to not be fully satisﬁable, in
the sense that it cannot achieve a value of wj, the maximum value of the weighted potential

8. Setting θj(xj) to the probability deﬁned by µ under the assumption that the elements of xj are inde-

pendent, i.e., the product of the pseudomarginals, is always feasible.

54

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

wjφj(x). Observe that, by the objective (72) and the simplex constraint (75), showing that
ˆφj(µ) is not fully satisﬁable is equivalent to showing that θ(cid:63)

j (xF

j ) > 0.

Lemma 16 If

µi +

(1 − µi) < 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

then θ(cid:63)

j (xF

j ) > 0.

Proof By the simplex constraint (75),

Also, by summing all the constraints (73) and (74),

µi +

(1 − µi) <

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) .

(cid:88)

xj

(cid:88)

θ(cid:63)
j (xj) ≤

xj |xj (cid:54)=xF
j

µi +

(1 − µi) ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

because all the components of θ(cid:63) are nonnegative, and—except for θ(cid:63)
at least once in constraints (73) and (74). These bounds imply

j (xF

j )—they all appear

(cid:88)

xj |xj (cid:54)=xF
j

θ(cid:63)
j (xj) <

θ(cid:63)
j (xj) ,

(cid:88)

xj

which means θ(cid:63)

j (xF

j ) > 0, completing the proof.

We next show that if ˆφj(µ) is parameterized such that it is not fully satisﬁable, as in

Lemma 16, then its optimum always takes a particular value deﬁned by µ.

Lemma 17 If wj > 0 and θ(cid:63)

j (xF

j ) > 0, then

(cid:88)

θ(cid:63)
j (xj) =

xj |xj (cid:54)=xF
j

µi +

(1 − µi) .

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

Proof We prove the lemma via the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939;
Kuhn and Tucker, 1951). Since problem (72) is a maximization of a linear function subject
to linear constraints, the KKT conditions are necessary and suﬃcient for any optimum θ(cid:63)
j .
Before writing the relevant KKT conditions, we introduce some necessary notation. For
a state xj, we need to reason about the variables that disagree with the unsatisﬁed state
xF

j . Let

(cid:110)

d(xj) (cid:44)

i ∈ I +

j ∪ I −

j |xj(i) (cid:54)= xF

j (i)

(cid:111)

be the set of indices for the variables that do not have the same value in the two states xj
and xF
j .

We now write the relevant KKT conditions for θ(cid:63)

j . Let λ, α be real-valued vectors where
j | + 1 and |α| = |θj|. Let each λi correspond to a constraint (73) or (74)

j | + |I −

|λ| = |I +

55

Bach, Broecheler, Huang, and Getoor

for i ∈ I +
correspond to a constraint (76) for each xj. Then, the following KKT conditions hold:

j , and let λ∆ correspond to the simplex constraint (75). Also, let each αxj

j ∪ I −

αxj ≥ 0
αxj θ(cid:63)
λ∆ + αxF
j
(cid:88)

j (xj) = 0
= 0

wj +

i∈d(xj )

λi + λ∆ + αxj = 0

∀xj (cid:54)= xF
j

.

∀xj
∀xj

(77)

(78)

(79)

(80)

Since θ(cid:63)

j (xF

j ) > 0, by condition (78), αxF

= 0. By condition (79), then λ∆ = 0. From
here we can bound the other elements of λ. Observe that for every i ∈ I +
j , there
exists a state xj such that d(xj) = {i}. Then, it follows from condition (80) that there
exists xj such that, for every i ∈ I +

j ∪ I −

j

j ∪ I −
j ,

wj + λi + λ∆ + αxj = 0 .

Since αxj ≥ 0 by condition (77) and λ∆ = 0, it follows that λi ≤ −wj. With these bounds,
we show that, for any state xj, if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0. Assume that for some state
xj, |d(xj)| ≥ 2. By condition (80) and the derived constraints on λ,

αxj ≥ (|d(xj)| − 1)wj > 0 .
j (xj) = 0. Next, observe that for all i ∈ I +
With condition (78), θ(cid:63)
j ) and for
j
any state xj, if d(xj) = {i}, then xj(i) = 1 (resp. xj(i) = 0), and for any other state x(cid:48)
j
such that x(cid:48)
j) ≥ 2. By constraint (73) (resp. constraint (74)),
θ(cid:63)(xj) = µi (resp. θ(cid:63)(xj) = 1 − µi).
We have shown that if θ(cid:63)
j ), then θ(cid:63)

j ) > 0, then for all states xj, if d(xj) = {i} and i ∈ I +
j
j (xj) = 1 − µi), and if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0.

j (xF
j (xj) = µi (resp. θ(cid:63)

j(i) = 1 (resp. x(cid:48)

j(i) = 0), d(x(cid:48)

(resp. i ∈ I −

(resp. i ∈ I −
This completes the proof.

Lemma 16 says if (cid:80)

(1 − µi) < 1, then ˆφj(µ) is not fully satisﬁable,
and Lemma 17 provides its optimal value. We now reason about the other case, when
(cid:80)
(1 − µi) ≥ 1, and we show that this condition is suﬃcient to ensure that

µi + (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

i∈I +
j

i∈I −
j

ˆφj(µ) is fully satisﬁable.

Lemma 18 If wj > 0 and

j (xF

j ) = 0.

then θ(cid:63)
Proof We prove the lemma by contradiction. Assume that wj > 0, (cid:80)
µi) ≥ 1, and that the lemma is false, θ(cid:63)
j ) > 0. Then, by Lemma 17,

µi + (cid:80)

(1 −

i∈I −
j

i∈I +
j

µi +

(1 − µi) ≥ 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) ≥ 1 .

j (xF
(cid:88)

xj |xj (cid:54)=xF
j

56

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The assumption that θ(cid:63)

j (xF

j ) > 0 implies

θ(cid:63)
j (xj) > 1,

(cid:88)

xj

j ) < 0 is excluded by the nonnegativity constraints (76).

which is a contradiction, since it violates the simplex constraint (75). The possibility that
θ(cid:63)
j (xF
For completeness and later convenience, we also state the value of ˆφj(µ) when it is fully
satisﬁable.

Lemma 19 If θ(cid:63)

j (xF

j ) = 0, then

(cid:88)

θ(cid:63)
j (xj) = 1 .

xj |xj (cid:54)=xF
j

Proof The lemma follows from the simplex constraint (75).

We can now combine the previous lemmas into a single expression for the value of ˆφj(µ).

Lemma 20 For any feasible setting of µ,

ˆφj(µ) = wj min

µi +

(1 − µi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






Proof The lemma is trivially true if wj = 0 since any assignment will yield zero value. If
wj > 0, then we consider two cases. In the ﬁrst case, if (cid:80)
(1 − µi) < 1,
then, by Lemmas 16 and 17,

µi + (cid:80)

i∈I −
j

i∈I +
j

In the second case, if (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

(1 − µi) ≥ 1, then, by Lemmas 18 and 19,

ˆφj(µ) = wj






(cid:88)

i∈I +
j



µi +

(1 − µi)


 .

(cid:88)

i∈I −
j

ˆφj(µ) = wj .

By factoring out wj, we can rewrite this piecewise deﬁnition of ˆφj(µ) as wj multiplied by
the minimum of (cid:80)

(1 − µi) and 1, completing the proof.

µi + (cid:80)

i∈I +
j

i∈I −
j

This leads to our ﬁnal equivalence result.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

57

Bach, Broecheler, Huang, and Getoor

Proof Substituting the solution of the inner optimization from Lemma 20 into the local
consistency relaxation objective (71) gives a projected optimization over only µ which is
identical to the MAX SAT relaxation objective (7).

References

2008.

A. Abdelbar and S. Hedetniemi. Approximating MAPs for belief networks is NP-hard and

other theorems. Artiﬁcial Intelligence, 102(1):21–38, 1998.

N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, third edition,

D. Alshukaili, A. A. A. Fernandes, and N. W. Paton. Structuring linked data search results
using probabilistic soft logic. In International Semantic Web Conference (ISWC), 2016.

L. An and P. Tao. The DC (diﬀerence of convex functions) programming and DCA revisited
with DC models of real world nonconvex optimization problems. Annals of Operations
Research, 133:23–46, 2005.

T. Asano and D. P. Williamson. Improved approximation algorithms for MAX SAT. J.

Algorithms, 42(1):173–202, 2002.

S. H. Bach, M. Broecheler, L. Getoor, and D. P. O’Leary. Scaling MPE inference for con-
strained continuous Markov random ﬁelds. In Advances in Neural Information Processing
Systems (NIPS), 2012.

S. H. Bach, B. Huang, B. London, and L. Getoor. Hinge-loss Markov random ﬁelds: Convex
inference for structured prediction. In Uncertainty in Artiﬁcial Intelligence (UAI), 2013.

S. H. Bach, B. Huang, J. Boyd-Graber, and L. Getoor. Paired-dual learning for fast training
of latent variable hinge-loss MRFs. In International Conference on Machine Learning
(ICML), 2015a.

S. H. Bach, B. Huang, and L. Getoor. Unifying local consistency and MAX SAT relaxations
for scalable inference with rounding guarantees. In Artiﬁcial Intelligence and Statistics
(AISTATS), 2015b.

G. Bakir, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan,

editors. Predicting Structured Data. MIT Press, 2007.

I. Beltagy, K. Erk, and R. J. Mooney. Probabilistic soft logic for semantic textual similarity.

In Annual Meeting of the Association for Computational Linguistics (ACL), 2014.

J. Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society,

24(3):179–195, 1975.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and
Statistical Learning Via the Alternating Direction Method of Multipliers. Now Publishers,
2011.

58

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

M. Broecheler and L. Getoor. Computing marginal distributions over continuous Markov
networks for statistical relational learning. In Advances in Neural Information Processing
Systems (NIPS), 2010.

M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2010a.

M. Broecheler, P. Shakarian, and V. S. Subrahmanian. A scalable framework for modeling

competitive diﬀusion in social networks. In Social Computing (SocialCom), 2010b.

C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and
approximation algorithms for the metric labeling problem. SIAM J. Discrete Math., 18
(3):608–625, 2005.

P. Chen, F. Chen, and Z. Qian. Road traﬃc congestion monitoring in social media with
In IEEE International Conference on Data Mining

hinge-loss Markov random ﬁelds.
(ICDM), 2014.

A. Choi, T. Standley, and A. Darwiche. Approximating weighted Max-SAT problems by
compensating for relaxations. In International Conference on Principles and Practice of
Constraint Programming, 2009.

M. Collins. Discriminative training methods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Empirical Methods in Natural Language Processing
(EMNLP), 2002.

M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2004.

H. Daum´e III, J. Langford, and D. Marcu. Search-based structured prediction. Machine

Learning, 75(3):297–325, 2009.

J. Davies and F. Bacchus. Exploiting the power of MIP solvers in MAXSAT. In M. J¨arvisalo
and A. Van Gelder, editors, Theory and Applications of Satisﬁability Testing – SAT 2013,
Lecture Notes in Computer Science, pages 166–181. Springer Berlin Heidelberg, 2013.

L. De Raedt and L. Dehaspe. Clausal discovery. Machine Learning, 26:1058–1063, 1996.

L. De Raedt, A. Kimmig, and H. Toivonen. ProbLog: A probabilistic Prolog and its
application in link discovery. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

R. de Salvo Braz, E. Amir, and D. Roth. Lifted ﬁrst-order probabilistic inference.

In
L. Getoor and B. Taskar, editors, Introduction to statistical relational learning, pages
433–451. MIT Press, 2007.

L. Deng and J. Wiebe. Joint prediction for entity/event-level sentiment analysis using
probabilistic soft logic models. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2015.

59

Bach, Broecheler, Huang, and Getoor

J. Ebrahimi, D. Dou, and D. Lowd. Weakly supervised tweet stance classiﬁcation by rela-
tional bootstrapping. In Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), 2016.

S. Fakhraei, B. Huang, L. Raschid, and L. Getoor. Network-based drug-target interac-
tion prediction with probabilistic soft logic. IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 2014.

J. Feldman, M. J. Wainwright, and D. R. Karger. Using linear programming to decode

binary linear codes. Information Theory, IEEE Trans. on, 51(3):954–972, 2005.

J. Foulds, N. Navaroli, P. Smyth, and A. Ihler. Revisiting MAP estimation, message passing

and perfect graphs. In AI & Statistics, 2011.

J. Foulds, S. Kumar, and L. Getoor. Latent topic networks: A versatile probabilistic pro-
gramming framework for topic models. In International Conference on Machine Learning
(ICML), 2015.

N. Friedman, L. Getoor, D. Koller, and A. Pfeﬀer. Learning probabilistic relational models.

In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1999.

D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems
via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1):17–
40, 1976.

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simpliﬁed NP-complete graph

problems. Theoretical Computer Science, 1(3):237–267, 1976.

L. Getoor and B. Taskar, editors. Introduction to statistical relational learning. MIT press,

2007.

L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning probabilistic models of link

structure. Journal of Machine Learning Research (JMLR), 3:679–707, 2002.

A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms
for MAP LP-relaxations. In Advances in Neural Information Processing Systems (NIPS),
2007.

R. Glowinski and A. Marrocco. Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et la
r´esolution, par p´enalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires.
Revue fran¸caise d’automatique, informatique, recherche op´erationnelle, 9(2):41–76, 1975.

M. X. Goemans and D. P. Williamson. New 3/4-approximation algorithms for the maximum

satisﬁability problem. SIAM J. Discrete Math., 7(4):656–666, 1994.

J. Golbeck. Computing and Applying Trust in Web-based Social Networks. PhD thesis,

University of Maryland, 2005.

K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collabo-

rative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.

60

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church:
A language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.

A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. Probabilistic programming.

In International Conference on Software Engineering (ICSE, FOSE track), 2014.

G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science, 313(5786):504–507, 2006.

B. Huang, A. Kimmig, L. Getoor, and J. Golbeck. A ﬂexible framework for probabilistic
models of social trust. In Conference on Social Computing, Behavioral-Cultural Modeling,
& Prediction (SBP), 2013.

T. Huynh and R. Mooney. Max-margin weight learning for Markov logic networks.

In

European Conference on Machine Learning (ECML), 2009.

A. Jaimovich, O. Meshi, and N. Friedman. Template based inference in symmetric relational

Markov random ﬁelds. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.

T. Jebara. MAP estimation, message passing, and perfect graphs. In Uncertainty in Arti-

ﬁcial Intelligence (UAI), 2009.

Learning, 77(1):27–59, 2009.

T. Joachims, T. Finley, and C. Yu. Cutting-plane training of structural SVMs. Machine

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In

International Conference on Machine Learning (ICML), 2010.

S. Kamvar, M. Schlosser, and H. Garcia-Molina. The eigentrust algorithm for reputation
In International Conference on the World Wide Web

management in P2P networks.
(WWW), 2003.

W. Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints.

Master’s thesis, University of Chicago, 1939.

K. Kersting. Lifted probabilistic inference. In European Conference on Artiﬁcial Intelligence

(ECAI), 2012.

K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Uncertainty in

Artiﬁcial Intelligence (UAI), 2009.

A. Kimmig, G. Van den Broeck, and L. De Raedt. An algebraic Prolog for reasoning about

possible worlds. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2011.

A. Kimmig, L. Mihalkova, and L. Getoor. Lifted graphical models: A survey. Machine

Learning, 99:1–45, 2015.

Applied Logic, 2016.

A. Kimmig, G. Van den Broeck, and L. De Raedt. Algebraic model counting. Journal of

61

Bach, Broecheler, Huang, and Getoor

J. Kleinberg and ´E. Tardos. Approximation algorithms for classiﬁcation problems with
pairwise relationships: Metric labeling and Markov random ﬁelds. J. ACM, 49(5):616–
639, 2002.

G. J. Klir and B. Yuan. Fuzzy Sets and Fuzzy Logic: Theory and Applications. Prentice

Hall, 1995.

S. Kok and P. Domingos. Learning the structure of Markov logic networks. In International

Conference on Machine Learning (ICML), 2005.

S. Kok and P. Domingos. Learning Markov logic networks using structural motifs.

In

International Conference on Machine Learning (ICML), 2010.

D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.

MIT Press, 2009.

Intelligence (UAI), 1997.

D. Koller and A. Pfeﬀer. Object-oriented Bayesian networks. In Uncertainty in Artiﬁcial

V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. Pat-

tern Analysis and Machine Intelligence, IEEE Trans. on, 28(10):1568–1583, 2006.

N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via
dual decomposition. Pattern Analysis and Machine Intelligence, IEEE Trans. on, 33(3):
531–552, 2011.

P. Kouki, S. Fakhraei, J. Foulds, M. Eirinaki, and L. Getoor. HyPER: A ﬂexible and
extensible probabilistic framework for hybrid recommender systems. In ACM Conference
on Recommender Systems (RecSys), 2015.

H. W. Kuhn and A. W. Tucker. Nonlinear programming.

In Berkeley Symp. on Math.

Statist. and Prob., 1951.

M. P. Kumar, P. H. S. Torr, and A. Zisserman. Solving Markov random ﬁelds using sec-
ond order cone programming relaxations. In Computer Vision and Pattern Recognition
(CVPR), 2006.

N. Landwehr, A. Passerini, L. De Raedt, and P. Frasconi. Fast learning of relational kernels.

Machine Learning, 78(3):305–342, 2010.

J. Larrosa, F. Heras, and S. de Givry. A logical approach to eﬃcient Max-SAT solving.

Artiﬁcial Intelligence, 172(2-3):204–233, 2008.

J. Li, A. Ritter, and D. Jurafsky. Inferring user preferences by probabilistic logical reasoning

over social networks. arXiv preprint arXiv:1411.2679, 2014.

S. Liu, K. Liu, S. He, and J. Zhao. A probabilistic soft logic based approach to exploiting
latent and global information in event classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2016.

62

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

B. London, S. Khamis, S. H. Bach, B. Huang, L. Getoor, and L. Davis. Collective activity
In CVPR Workshop on Structured

detection using hinge-loss Markov random ﬁelds.
Prediction: Tractability, Learning and Inference, 2013.

B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

D. Lowd and P. Domingos. Eﬃcient weight learning for Markov logic networks. In Principles

and Practice of Knowledge Discovery in Databases (PKDD), 2007.

S. Magliacane, P. Stutz, P. Groth, and A. Bernstein. FoxPSL: An extended and scalable
In AAAI Spring Symposium on Knowledge Representation and

PSL implementation.
Reasoning: Integrating Symbolic and Neural Approaches, 2015.

A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing.
AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Mod-
els. Journal of Machine Learning Research (JMLR), 16(Mar):495–545, 2015.

A. McCallum, K. Nigam, and L. H. Ungar. Eﬃcient clustering of high-dimensional data
sets with application to reference matching. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2000.

A. McCallum, K. Schultz, and S. Singh. FACTORIE: Probabilistic programming via im-
peratively deﬁned factor graphs. In Advances in Neural Information Processing Systems
(NIPS), 2009.

O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation.

In European Conference on Machine learning (ECML), 2011.

O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning eﬃciently with approximate
In International Conference on Machine Learning (ICML),

inference via dual losses.
2010.

E. Mezuman, D. Tarlow, A. Globerson, and Y. Weiss. Tighter linear program relaxations
for high order graphical models. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2013.

H. Miao, X. Liu, B. Huang, and L. Getoor. A hypergraph-partitioned vertex programming
approach for large-scale consensus optimization. In IEEE International Conference on
Big Data, 2013.

L. Mihalkova and R. J. Mooney. Bottom-up learning of Markov logic network structure. In

International Conference on Machine Learning (ICML), 2007.

B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic
models with unknown objects. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2005.

P. Mills and E. Tsang. Guided local search for solving SAT and weighted MAX-SAT

problems. J. Automated Reasoning, 24(1-2):205–223, 2000.

63

Bach, Broecheler, Huang, and Getoor

M. Mladenov, B. Ahmadi, and K. Kersting. Lifted linear programming. In Artiﬁcial Intel-

ligence & Statistics (AISTATS), 2012.

S. Muggleton and L. De Raedt. Inductive logic programming: Theory and methods. The

Journal of Logic Programming, 19:629–679, 1994.

Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Program-

ming. Society for Industrial and Applied Mathematics, 1994.

J. Neville and D. Jensen. Relational dependency networks. Journal of Machine Learning

Research (JMLR), 8:653–692, 2007.

H. B. Newcombe and J. M. Kennedy. Record linkage: Making maximum use of the discrim-
inating power of identifying information. Communications of the ACM, 5(11):563–566,
1962.

S. Nowozin, P. V. Gehler, J. Jancsary, and C. H. Lampert, editors. Advanced Structured

Prediction. Neural Information Processing. MIT press, 2016.

J. D. Park. Using weighted MAX-SAT engines to solve MPE.

In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2002.

S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by
gradient descent in function space. Journal of Machine Learning Research (JMLR), 3:
1333–1356, 2003.

A. Pfeﬀer. IBAL: A probabilistic rational programming language. In International Joint

Conference on Artiﬁcial Intelligence (IJCAI), 2001.

A. Pfeﬀer. Figaro: An object-oriented probabilistic programming language. Technical

report, Charles River Analytics, 2009.

A. Pfeﬀer, D. Koller, B. Milch, and K. T. Takusagawa. SPOOK: A system for probabilistic
object-oriented knowledge representation. In Uncertainty in Artiﬁcial Intelligence (UAI),
1999.

H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2011.

J. Pujara, H. Miao, L. Getoor, and W. Cohen. Knowledge graph identiﬁcation. In Inter-

national Semantic Web Conference (ISWC), 2013.

A. Ramesh, D. Goldwasser, B. Huang, H. Daum´e III, and L. Getoor. Learning latent
In AAAI Conference on Artiﬁcial

engagement patterns of students in online courses.
Intelligence (AAAI), 2014.

A. Ramesh, S. Kumar, J. Foulds, and L. Getoor. Weakly supervised models of aspect-
sentiment for online course discussion forums. In Annual Meeting of the Association for
Computational Linguistics (ACL), 2015.

64

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

P. Ravikumar and J. Laﬀerty. Quadratic programming relaxations for metric labeling and
Markov random ﬁeld MAP estimation. In International Conference on Machine Learning
(ICML), 2006.

P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured
linear programs: Proximal methods and rounding schemes. Journal of Machine Learning
Research (JMLR), 11:1043–1080, 2010a.

P. Ravikumar, M. J. Wainwright, and J. D. Laﬀerty. High-dimensional Ising model selection
using (cid:96)1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010b.

B. L. Richards and R. J. Mooney. Learning relations by pathﬁnding. In AAAI Conference

on Artiﬁcial Intelligence (AAAI), 1992.

M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2):107–

136, 2006.

M. Richardson, R. Agrawal, and P. Domingos. Trust management for the semantic web.
In D. Fensel, K. Sycara, and J. Mylopoulos, editors, The Semantic Web - ISWC 2003,
volume 2870 of Lecture Notes in Computer Science, pages 351–368. Springer Berlin /
Heidelberg, 2003.

F. Riguzzi and T. Swift. The PITA system: Tabling and answer subsumption for reasoning
under uncertainty. In International Conference on Logic Programming (ICLP), 2011.

S. Ross and J. A. Bagnell. Reinforcement and Imitation Learning via Interactive No-Regret

Learning, 2014.

S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Artiﬁcial Intelligence & Statistics (AISTATS),
2011.

R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Artiﬁcial Intelligence &

Statistics (AISTATS), 2009.

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In International Conference on Machine Learning (ICML), 2008.

M. Samadi, P. Talukdar, M. Veloso, and M. Blum. ClaimEval: Integrated and ﬂexible
In AAAI Conference on

framework for claim evaluation using credibility of sources.
Artiﬁcial Intelligence (AAAI), 2016.

A. Schrijver. Combinatorial Optimization: Polyhedra and Eﬃciency. Springer-Verlag, 2003.

A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally convergent dual MAP
LP relaxation solvers using Fenchel-Young margins. In Advances in Neural Information
Processing Systems (NIPS), 2012.

P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective

classiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.

65

Bach, Broecheler, Huang, and Getoor

S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artiﬁcial Intelligence, 68(2):

399–410, 1994.

P. Singla and P. Domingos. Discriminative training of Markov logic networks. In AAAI

Conference on Artiﬁcial Intelligence (AAAI), 2005.

P. Singla and P. Domingos. Lifted ﬁrst-order belief propagation. In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2008.

D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations
for MAP using message passing. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2008.

D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference.
In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, pages
219–254. MIT Press, 2011.

D. Sontag, D. K. Choe, and Y. Li. Eﬃciently searching for frustrated cycles in MAP

inference. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2012.

D. Sridhar, J. Foulds, M. Walker, B. Huang, and L. Getoor. Joint models of disagreement
and stance in online debate. In Annual Meeting of the Association for Computational
Linguistics (ACL), 2015.

D. Sridhar, S. Fakhraei, and L. Getoor. A probabilistic approach for collective similarity-

based drug-drug interaction prediction. Bioinformatics, 32(20):3175–3182, 2016.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Neural Information

Processing Systems (NIPS), 2004.

B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction
In International Conference on Machine Learning

models: A large margin approach.
(ICML), 2005.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Ed-
inference, and criticism. arXiv preprint

ward: A library for probabilistic modeling,
arXiv:1610.09787, 2016.

I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research
(JMLR), 6:1453–1484, 2005.

V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2000.

D. Venugopal and V. Gogate. On lifting the Gibbs sampling algorithm. In Neural Infor-

mation Processing Systems (NIPS), 2012.

B. W. Wah and Y. Shang. Discrete Lagrangian-based search for solving MAX-SAT prob-

lems. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1997.

M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Varia-

tional Inference. Now Publishers, 2008.

66

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

J. Wang and P. Domingos. Hybrid Markov logic networks. In AAAI Conference on Artiﬁcial

Intelligence (AAAI), 2008.

T. Werner. A linear programming approach to max-sum problem: A review. Pattern

Analysis and Machine Intelligence, IEEE Trans. on, 29(7):1165–1179, 2007.

R. West, H. S. Paskov, J. Leskovec, and C. Potts. Exploiting social network structure for
person-to-person sentiment analysis. Transactions of the Association for Computational
Linguistics (TACL), 2:297–310, 2014.

F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic

programming inference. In Artiﬁcial Intelligence & Statistics (AISTATS), 2014.

M. Wright. The interior-point revolution in optimization: History, recent developments,
and lasting consequences. Bulletin of the American Mathematical Society, 42(1):39–56,
2005.

L. Xiong, X. Chen, T. Huang, J. Schneider, and J. Carbonell. Temporal collaborative ﬁlter-
ing with Bayesian probabilistic tensor factorization. In SIAM International Conference
on Data Mining, 2010.

C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propaga-
tion – An empirical study. Journal of Machine Learning Research (JMLR), 7:1887–1907,
2006.

J. Zhu, N. Lao, and E. P. Xing. Grafting-Light: Fast, Incremental Feature Selection and
Structure Learning of Markov Random Fields. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2010.

67

7
1
0
2
 
v
o
N
 
7
1
 
 
]

G
L
.
s
c
[
 
 
3
v
6
0
4
4
0
.
5
0
5
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2017) 1-67

Submitted 12/15; Revised 12/16; Published 10/17

Hinge-Loss Markov Random Fields
and Probabilistic Soft Logic

Stephen H. Bach
Computer Science Department
Stanford University
Stanford, CA 94305, USA

Matthias Broecheler
DataStax

Bert Huang
Computer Science Department
Virginia Tech
Blacksburg, VA 24061, USA

Lise Getoor
Computer Science Department
University of California, Santa Cruz
Santa Cruz, CA 95064, USA

Editor: Luc De Raedt

bach@cs.stanford.edu

matthias@datastax.com

bhuang@vt.edu

getoor@soe.ucsc.edu

Abstract

A fundamental challenge in developing high-impact machine learning technologies is bal-
ancing the need to model rich, structured domains with the ability to scale to big data.
Many important problem areas are both richly structured and large scale, from social and
biological networks, to knowledge graphs and the Web, to images, video, and natural lan-
guage. In this paper, we introduce two new formalisms for modeling structured data, and
show that they can both capture rich structure and scale to big data. The ﬁrst, hinge-
loss Markov random ﬁelds (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes diﬀerent approaches to convex inference. We unite three approaches from
the randomized algorithms, probabilistic graphical models, and fuzzy logic communities,
showing that all three lead to the same inference objective. We then deﬁne HL-MRFs
by generalizing this uniﬁed objective. The second new formalism, probabilistic soft logic
(PSL), is a probabilistic programming language that makes HL-MRFs easy to deﬁne using
a syntax based on ﬁrst-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than general-purpose
convex optimization methods, because it uses message passing to take advantage of sparse
dependency structures. We then show how to learn the parameters of HL-MRFs. The
learned HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at
scales not previously possible.

Keywords:
prediction

Probabilistic graphical models, statistical relational learning, structured

c(cid:13)2017 Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/15-631.html.

Bach, Broecheler, Huang, and Getoor

1. Introduction

In many problems in machine learning, the domains are rich and structured, with many
interdependent elements that are best modeled jointly. Examples include social networks,
biological networks, the Web, natural language, computer vision, sensor networks, and so on.
Machine learning subﬁelds such as statistical relational learning (Getoor and Taskar, 2007),
inductive logic programming (Muggleton and De Raedt, 1994), and structured prediction
(Bakir et al., 2007) all seek to represent dependencies in data induced by relational structure.
With the ever-increasing size of available data, there is a growing need for models that are
highly scalable while still able to capture rich structure.

In this paper, we introduce hinge-loss Markov random ﬁelds (HL-MRFs), a new class of
probabilistic graphical models designed to enable scalable modeling of rich, structured data.
HL-MRFs are analogous to discrete MRFs, which are undirected probabilistic graphical
models in which probability mass is log-proportional to a weighted sum of feature functions.
Unlike discrete MRFs, however, HL-MRFs are deﬁned over continuous variables in the [0, 1]
unit interval. To model dependencies among these continuous variables, we use linear and
quadratic hinge functions, so that probability density is lost according to a weighted sum of
hinge losses. As we will show, hinge-loss features capture many common modeling patterns
for structured data.

When designing classes of models, there is generally a trade oﬀ between scalability and
expressivity: the more complex the types and connectivity structure of the dependencies,
the more computationally challenging inference and learning become. HL-MRFs address
a crucial gap between the two extremes. By using hinge-loss functions to model the de-
pendencies among the variables, which admit highly scalable inference without restrictions
on their connectivity structure, HL-MRFs can capture a wide range of useful relationships.
One reason they are so expressive is that hinge-loss dependencies are at the core of a number
of scalable techniques for modeling both discrete and continuous structured data.

To motivate HL-MRFs, we unify three diﬀerent approaches for scalable inference in
structured models: (1) randomized algorithms for MAX SAT (Goemans and Williamson,
1994), (2) local consistency relaxation (Wainwright and Jordan, 2008) for discrete Markov
random ﬁelds deﬁned using Boolean logic, and (3) reasoning about continuous information
with fuzzy logic. We show that all three approaches lead to the same convex programming
objective. We then deﬁne HL-MRFs by generalizing this uniﬁed inference objective as a
weighted sum of hinge-loss features and using them as the weighted features of graphical
models. Since HL-MRFs generalize approaches that reason about relational data with
weighted logical knowledge bases, they retain the same high level of expressivity. As we
show in Section 6.4, they are eﬀective for modeling both discrete and continuous data.

We also introduce probabilistic soft logic (PSL), a new probabilistic programming lan-
guage that makes HL-MRFs easy to deﬁne and use for large, relational data sets.1 This
idea has been explored for other classes of models, such as Markov logic networks (Richard-
son and Domingos, 2006) for discrete MRFs, relational dependency networks (Neville and
Jensen, 2007) for dependency networks, and probabilistic relational models (Getoor et al.,
2002) for Bayesian networks. We build on these previous approaches, as well as the con-
nection between hinge-loss potentials and logical clauses, to deﬁne PSL. In addition to

1. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

2

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

probabilistic rules, PSL provides syntax that enables users to easily apply many common
modeling techniques, such as domain and range constraints, blocking and canopy functions,
and aggregate variables deﬁned over other random variables.

Our next contribution is to introduce a number of inference and learning algorithms.
First, we examine MAP inference, i.e., the problem of ﬁnding a most probable assignment
to the unobserved random variables. MAP inference in HL-MRFs is always a convex op-
timization. Although any oﬀ-the-shelf optimization toolkit could be used, such methods
typically do not leverage the sparse dependency structures common in graphical models.
We introduce a consensus-optimization approach to MAP inference for HL-MRFs, showing
how the problem can be decomposed using the alternating direction method of multipliers
(ADMM) and how the resulting subproblems can be solved analytically for hinge-loss poten-
tials. Our approach enables HL-MRFs to easily scale beyond the capabilities of oﬀ-the-shelf
optimization software or sampling-based inference in discrete MRFs. We then show how
to learn HL-MRFs from training data using a variety of methods: structured perceptron,
maximum pseudolikelihood, and large margin estimation. Since structured perceptron and
large margin estimation rely on inference as subroutines, and maximum pseudolikelihood
estimation is eﬃcient by design, all of these methods are highly scalable for HL-MRFs. We
evaluate them on core relational learning and structured prediction tasks, such as collec-
tive classiﬁcation and link prediction. We show that HL-MRFs oﬀer predictive accuracy
comparable to analogous discrete models while scaling much better to large data sets.

This paper brings together and expands work on scalable models for structured data that
can be either discrete, continuous, or a mixture of both (Broecheler et al., 2010a; Bach et al.,
2012, 2013, 2015b). The eﬀectiveness of HL-MRFs and PSL has been demonstrated on many
problems, including information extraction (Liu et al., 2016) and automatic knowledge base
construction (Pujara et al., 2013), extracting and evaluating natural-language arguments
on the Web (Samadi et al., 2016), high-level computer vision (London et al., 2013), drug
discovery (Fakhraei et al., 2014) and predicting drug-drug interactions (Sridhar et al., 2016),
natural language semantics (Beltagy et al., 2014; Sridhar et al., 2015; Deng and Wiebe,
2015; Ebrahimi et al., 2016), automobile-traﬃc modeling (Chen et al., 2014), recommender
systems (Kouki et al., 2015), information retrieval (Alshukaili et al., 2016), and predicting
attributes (Li et al., 2014) and trust (Huang et al., 2013; West et al., 2014) in social networks.
The ability to easily incorporate latent variables into HL-MRFs and PSL (Bach et al., 2015a)
has enabled further applications, including modeling latent topics in text (Foulds et al.,
2015), and predicting student outcomes in massive open online courses (MOOCs) (Ramesh
et al., 2014, 2015). Researchers have also studied how to make HL-MRFs and PSL even
more scalable by developing distributed implementations (Miao et al., 2013; Magliacane
et al., 2015). That they are already being widely applied indicates HL-MRFs and PSL
address an open need in the machine learning community.

The paper is organized as follows. In Section 2, we ﬁrst consider models for structured
prediction that are deﬁned using logical clauses. We unify three diﬀerent approaches to
scalable inference in such models, showing that they all optimize the same convex objec-
tive. We then generalize this objective in Section 3 to deﬁne HL-MRFs. In Section 4, we
introduce PSL, specifying the language and giving many examples of common usage. Next
we introduce a scalable message-passing algorithm for MAP inference in Section 5 and a

3

Bach, Broecheler, Huang, and Getoor

number of learning algorithms in Section 6, evaluating them on a range of tasks. Finally,
in Section 7, we discuss related work.

2. Unifying Convex Inference for Logic-Based Graphical Models

In many structured domains, propositional and ﬁrst-order logics are useful tools for describ-
ing the intricate dependencies that connect the unknown variables. However, these domains
are usually noisy; dependencies among the variables do not always hold. To address this,
logical semantics can be incorporated into probability distributions to create models that
capture both the structure and the uncertainty in machine learning tasks. One common
way to do this is to use logic to deﬁne feature functions in a probabilistic model. We focus
on Markov random ﬁelds (MRFs), a popular class of probabilistic graphical models. Infor-
mally, an MRF is a distribution that assigns probability mass using a scoring function that
is a weighted combination of feature functions called potentials. We will use logical clauses
to deﬁne these potentials. We ﬁrst deﬁne MRFs more formally to introduce necessary
notation:

Deﬁnition 1 Let x = (x1, . . . , xn) be a vector of random variables and let φ = (φ1, . . . , φm)
be a vector of potentials where each potential φj(x) assigns conﬁgurations of the variables
a real-valued score. Also, let w = (w1, . . . , wm) be a vector of real-valued weights. Then, a
Markov random ﬁeld is a probability distribution of the form

P (x) ∝ exp

w(cid:62)φ(x)

.

(cid:16)

(cid:17)

In an MRF, the potentials should capture how the domain behaves, assigning higher scores
If a modeler does not know how the
to more probable conﬁgurations of the variables.
domain behaves, the potentials should capture how it might behave, so that a learning
algorithm can ﬁnd weights that lead to accurate predictions. Logic provides an excellent
formalism for deﬁning such potentials in structured and relational domains.

We now introduce some notation to make this logic-based approach more formal. Con-
sider a set of logical clauses C = {C1, . . . , Cm}, i.e., a knowledge base, where each clause
Cj ∈ C is a disjunction of literals and each literal is a variable x or its negation ¬x drawn
from the variables x such that each variable xi ∈ x appears at most once in Cj. Let
j (resp. I −
I +
j ) ⊂ {1, . . . , n} be the set of indices of the variables that are not negated (resp.
negated) in Cj. Then Cj can be written as

Logical clauses of this form are expressive because they can be viewed equivalently as

implications from conditions to consequences:

This “if-then” reasoning is intuitive and can describe many dependencies in structured data.



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(cid:94)

i∈I −
j

xi =⇒

xi .

(cid:95)

i∈I +
j

4

(1)

(2)

(3)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Assuming we have a logical knowledge base C describing a structured domain, we can
embed it in an MRF by deﬁning each potential φj using a corresponding clause Cj. If an
assignment to the variables x satisﬁes Cj, then we let φj(x) equal 1, and we let it equal 0
otherwise. For our subsequent analysis we assume wj ≥ 0 (∀j = 1, . . . , m). The resulting
MRF preserves the structured dependencies described in C but enables much more ﬂexible
modeling. Clauses no longer must always hold, and the model can express uncertainty
over diﬀerent possible worlds. The weights express how strongly the model expects each
corresponding clause to hold; the higher the weight, the more probable that it is true
according to the model.

This notion of embedding weighted, logical knowledge bases in MRFs is an appealing
one. For example, Markov logic (Richardson and Domingos, 2006) is a popular formalism
that induces MRFs from weighted ﬁrst-order knowledge bases. Given a data set, the ﬁrst-
order clauses are grounded using the constants in the data to create the set of propositional
clauses C. Each propositional clause has the weight of the ﬁrst-order clause from which it
was grounded. In this way, a weighted, ﬁrst-order knowledge base can compactly specify
an entire family of MRFs for a structured machine-learning task.

Although we now have a method for easily deﬁning rich, structured models for a wide
range of problems, there is a new challenge: ﬁnding a most probable assignment to the
variables, i.e., MAP inference, is NP-hard (Shimony, 1994; Garey et al., 1976). This means
that (unless P=NP) our only hope for performing tractable inference is to perform it ap-
proximately. Observe that MAP inference for an MRF deﬁned by C is the integer linear
program

arg max
x∈{0,1}n

P (x) ≡ arg max
x∈{0,1}n

w(cid:62)φ(x)

≡ arg max
x∈{0,1}n

(cid:88)

Cj ∈C

wj min

xi +

(1 − xi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






(4)

While this program is intractable, it does admit convex programming relaxations.

In this section, we show how convex programming can be used to perform tractable
inference in MRFs deﬁned by weighted knowledge bases. We ﬁrst discuss in Section 2.1 an
approach developed by Goemans and Williamson (1994) that views MAP inference as an
instance of the classic MAX SAT problem and relaxes it to a convex program from that
perspective. This approach has the advantage of providing strong guarantees on the quality
of the discrete solutions it obtains. However, it has the disadvantage that general-purpose
convex programming toolkits do not scale well to relaxed MAP inference for large graphical
models (Yanover et al., 2006). In Section 2.2 we then discuss a seemingly distinct approach,
local consistency relaxation, with complementary advantages and disadvantages:
it oﬀers
highly scalable message-passing algorithms but comes with no quality guarantees. We then
unite these approaches by proving that they solve equivalent optimization problems with
identical solutions. Then, in Section 2.3, we show that the uniﬁed inference objective is also
equivalent to exact MAP inference if the knowledge base C is interpreted using (cid:32)Lukasiewicz
logic, an inﬁnite-valued logic for reasoning about naturally continuous quantities such as
similarity, vague or fuzzy concepts, and real-valued data.

5

Bach, Broecheler, Huang, and Getoor

That these three interpretations all lead to the same inference objective—whether rea-
soning about discrete or continuous information—is useful. To the best of our knowledge,
we are the ﬁrst to show their equivalence. This equivalence indicates that the same model-
ing formalism, inference algorithms, and learning algorithms can be used to reason scalably
and accurately about both discrete and continuous information in structured domains. We
generalize the uniﬁed inference objective in Section 3.1 to deﬁne hinge-loss MRFs, and in
the rest of the paper we develop a probabilistic programming language and algorithms that
realize the goal of a scalable and accurate framework for structured data, both discrete and
continuous.

2.1 MAX SAT Relaxation

One approach to approximating objective (4) is to use relaxation techniques developed in
the randomized algorithms community for the MAX SAT problem. Formally, the MAX
SAT problem is to ﬁnd a Boolean assignment to a set of variables that maximizes the total
weight of satisﬁed clauses in a knowledge base composed of disjunctive clauses annotated
with nonnegative weights.
In other words, objective (4) is an instance of MAX SAT.
Randomized approximation algorithms can be constructed for MAX SAT by independently
rounding each Boolean variable xi to true with probability pi. Then, the expected weighted
satisfaction ˆwj of a clause Cj is

also known as a (weighted) noisy-or function, and the expected total score ˆW is



ˆwj = wj


1 −

(1 − pi)

(cid:89)

i∈I +
j



pi


 ,

(cid:89)

i∈I −
j



ˆW =

(cid:88)

Cj ∈C

wj


1 −

(cid:89)

i∈I +
j

(1 − pi)



pi


 .

(cid:89)

i∈I −
j

(5)

(6)

Optimizing ˆW with respect to the rounding probabilities would give the exact MAX SAT so-
lution, so this randomized approach has not made the problem any easier yet, but Goemans
and Williamson (1994) showed how to bound ˆW below with a tractable linear program.

To approximately optimize ˆW , associate with each Boolean variable xi a corresponding
continuous variable ˆyi with domain [0, 1]. Then let ˆy(cid:63) be the optimum of the linear program

arg max
ˆy∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

ˆyi +

(1 − ˆyi), 1

.

(7)

Observe that objectives (4) and (7) are of the same form, except that the variables are
relaxed to the unit hypercube in objective (7). Goemans and Williamson (1994) proved
i for all i, then ˆW ≥ .632 Z(cid:63), where Z(cid:63) is the optimal total weight for
that if pi is set to ˆy(cid:63)
the MAX SAT problem. If each pi is set using any function in a special class, then this

6

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

lower bound improves to a .75 approximation. One simple example of such a function is
1
2

ˆy(cid:63)
i +

pi =

1
4

.

(8)

In this way, objective (7) leads to an expected .75 approximation of the MAX SAT solution.
The following method of conditional probabilities (Alon and Spencer, 2008) can ﬁnd a
single Boolean assignment that achieves at least the expected score from a set of rounding
probabilities, and therefore at least .75 of the MAX SAT solution when objective (7) and
function (8) are used to obtain them. Each variable xi is greedily set to the value that
maximizes the expected weight over the unassigned variables, conditioned on either possible
value of xi and the previously assigned variables. This greedy maximization can be applied
quickly because, in many models, variables only participate in a small fraction of the clauses,
making the change in expectation quick to compute for each variable. Speciﬁcally, referring
to the deﬁnition of ˆW (6), the assignment to xi only needs to maximize over the clauses Cj
in which xi participates, i.e., i ∈ I +

j , which is usually a small set.

j ∪ I −

This approximation is powerful because it is a tractable linear program that comes
with strong guarantees on solution quality. However, even though it is tractable, general-
purpose convex optimization toolkits do not scale well to large MAP problems.
In the
following subsection, we unify this approximation with a complementary one developed in
the probabilistic graphical models community.

2.2 Local Consistency Relaxation

Another approach to approximating objective (4) is to apply a relaxation developed for
Markov random ﬁelds called local consistency relaxation (Wainwright and Jordan, 2008).
This approach starts by viewing MAP inference as an equivalent optimization over marginal
probabilities.2 For each φj ∈ φ, let θj be a marginal distribution over joint assignments xj.
For example, θj(xj) is the probability that the subset of variables associated with potential
φj is in a particular joint state xj. Also, let xj(i) denote the setting of the variable with
index i in the state xj.

With this variational formulation, inference can be relaxed to an optimization over the
ﬁrst-order local polytope L. Let µ = (µ1, . . . , µn) be a vector of probability distributions,
where µi(k) is the marginal probability that xi is in state k. The ﬁrst-order local polytope
is




L (cid:44)

(θ, µ) ≥ 0

(cid:80)

(cid:80)

xj |xj (i)=k θj(xj) = µi(k) ∀i, j, k
∀j

θj(xj) = 1




,



xj
(cid:80)Ki−1
k=0 µi(k) = 1
which constrains each marginal distribution θj over joint states xj to be consistent only
with the marginal distributions µ over individual variables that participate in the potential
φj.



∀i

MAP inference can then be approximated with the ﬁrst-order local consistency relax-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ation:

(9)

(10)

arg max
(θ,µ)∈L

m
(cid:88)

j=1

(cid:88)

xj

wj

θj(xj) φj(xj),

2. This treatment is for discrete MRFs. We have omitted a discussion of continuous MRFs for conciseness.

7

Bach, Broecheler, Huang, and Getoor

which is an upper bound on the true MAP objective. Much work has focused on solving
the ﬁrst-order local consistency relaxation for large-scale MRFs, which we discuss further
in Section 7. These algorithms are appealing because they are well-suited to the sparse
dependency structures common in MRFs, so they can scale to large problems. However, in
general, the solutions can be fractional, and there are no guarantees on the approximation
quality of a tractable discretization of these fractional solutions.

We show that for MRFs with potentials deﬁned by C and nonnegative weights, local

consistency relaxation is equivalent to MAX SAT relaxation.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

We prove Theorem 2 in Appendix A. Our proof analyzes the local consistency relaxation to
derive an equivalent, more compact optimization over only the variable pseudomarginals µ
that is identical to the MAX SAT relaxation. Theorem 2 is signiﬁcant because it shows that
the rounding guarantees of MAX SAT relaxation also apply to local consistency relaxation,
and the scalable message-passing algorithms developed for local consistency relaxation also
apply to MAX SAT relaxation.

2.3 (cid:32)Lukasiewicz Logic

The previous two subsections showed that the same convex program can approximate MAP
inference in discrete, logic-based models, whether viewed from the perspective of randomized
algorithms or variational methods. In this subsection, we show that this convex program
can also be used to reason about naturally continuous information, such as similarity, vague
or fuzzy concepts, and real-valued data. Instead of interpreting the clauses C using Boolean
logic, we can interpret them using (cid:32)Lukasiewicz logic (Klir and Yuan, 1995), which extends
Boolean logic to inﬁnite-valued logic in which the propositions x can take truth values in the
continuous interval [0, 1]. Extending truth values to a continuous domain enables them to
represent concepts that are vague, in the sense that they are often neither completely true
nor completely false. For example, the propositions that a sensor value is high, two entities
are similar, or a protein is highly expressed can all be captured in a more nuanced manner
in (cid:32)Lukasiewicz logic. We can also use the now continuous valued x to represent quantities
that are naturally continuous (scaled to [0,1]), such as actual sensor values, similarity scores,
and protein expression levels. The ability to reason about continuous values is valuable, as
many important applications are not entirely discrete.

The extension to continuous values requires a corresponding extended interpretation of
the logical operators ∧ (conjunction), ∨ (disjunction), and ¬ (negation). The (cid:32)Lukasiewicz
t-norm and t-co-norm are ∧ and ∨ operators that correspond to the Boolean logic operators
for integer inputs (along with the negation operator ¬):

x1 ∧ x2 = max {x1 + x2 − 1, 0}
x1 ∨ x2 = min {x1 + x2, 1}

¬x = 1 − x .

8

(11)

(12)

(13)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The analogous MAX SAT problem for (cid:32)Lukasiewicz logic is therefore

arg max
x∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

xi +

(1 − xi), 1

,

(14)

which is identical in form to the relaxed MAX SAT objective (7). Therefore, if an MRF
is deﬁned over continuous variables with domain [0, 1]n and the logical knowledge base C
deﬁning the potentials is interpreted using (cid:32)Lukasiewicz logic, then exact MAP inference
is identical to ﬁnding the optimum using the uniﬁed, relaxed inference objective derived
for Boolean logic in the previous two subsections. This result shows the equivalence of all
three approaches: MAX SAT relaxation, local consistency relaxation, and MAX SAT using
(cid:32)Lukasiewicz logic.

3. Hinge-Loss Markov Random Fields

We have shown that a speciﬁc family of convex programs can be used to reason scalably and
accurately about both discrete and continuous information. In this section, we generalize
this family to deﬁne hinge-loss Markov random ﬁelds (HL-MRFs), a new kind of probabilis-
tic graphical model. HL-MRFs retain the convexity and expressivity of convex programs
discussed in Section 2, and additionally support an even richer space of dependencies.

To begin, we deﬁne HL-MRFs as density functions over continuous variables y =
(y1, . . . , yn) with joint domain [0, 1]n. These variables have diﬀerent possible interpreta-
tions depending on the application. Since we are generalizing the interpretations explored
in Section 2, HL-MRF MAP states can be viewed as rounding probabilities or pseudo-
marginals, or they can represent naturally continuous information. More generally, they
can be viewed simply as degrees of belief, conﬁdences, or rankings of possible states; and
they can describe discrete, continuous, or mixed domains. The application domain typi-
cally determines which interpretation is most appropriate. The formalisms and algorithms
described in the rest of this paper are general with respect to such interpretations.

3.1 Generalized Inference Objective

To deﬁne HL-MRFs, we will ﬁrst generalize the uniﬁed inference objective of Section 2 in
several ways, which we ﬁrst restate in terms of the HL-MRF variables y:

arg max
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

yi +

(1 − yi), 1

.

(15)

For now, we are still assuming that the objective terms are deﬁned using a weighted knowl-
edge base C, but we will quickly drop this requirement. To do so, we examine one term in
isolation. Observe that the maximum value of any unweighted term is 1, which is achieved
when a linear function of the variables is at least 1. We say that the term is satisﬁed when-
ever this occurs. When a term is unsatisﬁed, we can refer to its distance to satisfaction,
which is how far it is from achieving its maximum value. Also observe that we can rewrite

9

Bach, Broecheler, Huang, and Getoor

the optimization explicitly in terms of distances to satisfaction:

arg min
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj max

1 −

yi −

(1 − yi), 0

,

(16)

so that the objective is equivalently to minimize the total weighted distance to satisfaction.
Each unweighted objective term now measures how far the linear constraint

1 −

yi −

(1 − yi) ≤ 0

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

(17)

is from being satisﬁed.

3.1.1 Relaxed Linear Constraints

With this view of each term as a relaxed linear constraint, we can easily generalize them to
arbitrary linear constraints. We no longer require that the inference objective be deﬁned
using only logical clauses, and instead each term can be deﬁned using any function (cid:96)j(y)
that is linear in y. These functions can capture more general dependencies, such as beliefs
about the range of values a variable can take and arithmetic relationships among variables.

The new inference objective is

arg min
y∈[0,1]n

m
(cid:88)

j=1

wj max {(cid:96)j(y), 0} .

(18)

In this form, each term represents the distance to satisfaction of a linear constraint (cid:96)j(y) ≤ 0.
That constraint could be deﬁned using logical clauses as discussed above, or it could be
deﬁned using other knowledge about the domain. The weight wj indicates how important
it is to satisfy a constraint relative to others by scaling the distance to satisfaction. The
higher the weight, the more distance to satisfaction is penalized. Additionally, two relaxed
inequality constraints, (cid:96)j(y) ≤ 0 and −(cid:96)j(y) ≤ 0, can be combined to represent a relaxed
equality constraint (cid:96)j(y) = 0.

3.1.2 Hard Linear Constraints

Now that our inference objective admits arbitrary relaxed linear constraints, it is natural
to also allow hard constraints that must be satisﬁed at all times. Hard constraints are
important modeling tools. They enable groups of variables to represent mutually exclusive
possibilities, such as a multinomial or categorical variable, and functional or partial func-
tional relationships. Hard constraints can also represent background knowledge about the
domain, restricting the domain to regions that are feasible in the real world. Additionally,
they can encode more complex model components such as deﬁning a random variable as an
aggregate over other unobserved variables, which we discuss further in Section 4.3.5.

We can think of including hard constraints as allowing a weight wj to take an inﬁnite
value. Again, two inequality constraints can be combined to represent an equality con-
straint. However, when we introduce an inference algorithm for HL-MRFs in Section 5, it

10

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

will be useful to treat hard constraints separately from relaxed ones, and further, treat hard
inequality constraints separately from hard equality constraints. Therefore, in the deﬁnition
of HL-MRFs, we will deﬁne these three components separately.

3.1.3 Generalized Hinge-Loss Functions

The objective terms measuring each constraint’s distance to satisfaction are hinge losses.
There is a ﬂat region, on which the distance to satisfaction is 0, and an angled region, on
which the distance to satisfaction grows linearly away from the hyperplane (cid:96)j(y) = 0. This
loss function is useful—as we discuss in the previous section, it is a bound on the expected
loss in the discrete setting, among other things—but it is not appropriate for all modeling
situations.

A piecewise-linear loss function makes MAP inference “winner take all,” in the sense
that it is preferable to fully satisfy the most highly weighted objective terms completely
before reducing the distance to satisfaction of terms with lower weights. For example,
consider the following optimization problem:

arg min
y1∈[0,1]

w1 max {y1, 0} + w2 max {1 − y1, 0} .

(19)

If w1 > w2 ≥ 0, then the optimizer is y1 = 0 because the term that prefers y1 = 0 overrules
the term that prefers y1 = 1. The result does not indicate any ambiguity or uncertainty, but
if the two objective terms are potentials in a probabilistic model, it is sometimes preferable
that the result reﬂect the conﬂicting preferences. We can change the inference problem
so that it smoothly trades oﬀ satisfying conﬂicting objective terms by squaring the hinge
losses. Observe that in the modiﬁed problem

arg min
y1∈[0,1]

w1 (max {y1, 0})2 + w2 (max {1 − y1, 0})2

(20)

the optimizer is now y1 = w2

, reﬂecting the relative inﬂuence of the two loss functions.
Another advantage of squared hinge-loss functions is that they can behave more intu-

w1+w2

itively in the presence of hard constraints. Consider the problem

arg min
(y1,y2)∈[0,1]2

such that

max {0.9 − y1, 0} + max {0.6 − y2, 0}

y1 + y2 ≤ 1 .

(21)

The ﬁrst term prefers y1 ≥ 0.9, the second term prefers y2 ≥ 0.6, and the constraint requires
that y1 and y2 are mutually exclusive. Such problems are very common and arise when
conﬂicting evidence of diﬀerent strengths support two mutually exclusive possibilities. The
evidence values 0.9 and 0.6 could come from many sources, including base models trained to
make independent predictions on individual random variables, domain-specialized similarity
functions, or sensor readings. For this problem, any solution y1 ∈ [0.4, 0.9] and y2 = 1 − y1
is an optimizer. This solution set includes counterintuitive optimizers like y1 = 0.4 and
y2 = 0.6, even though the evidence supporting y1 is stronger. Again, squared hinge losses

11

Bach, Broecheler, Huang, and Getoor

ensure the optimizers better reﬂect the relative strength of evidence. For the problem

(max {0.9 − y1, 0})2 + (max {0.6 − y2, 0})2

arg min
(y1,y2)∈[0,1]2

such that

(22)

y1 + y2 ≤ 1 ,

the only optimizer is y1 = 0.65 and y2 = 0.35, which is a more informative solution.

We therefore complete our generalized inference objective by allowing either hinge-loss
or squared hinge-loss functions. Users of HL-MRFs have the choice of either one for each
potential, depending on which is appropriate for their task.

3.2 Deﬁnition

We can now formally state the full deﬁnition of HL-MRFs. They are deﬁned so that a MAP
state is a solution to the generalized inference objective proposed in the previous subsection.
We state the deﬁnition in a conditional form for later convenience, but this deﬁnition is fully
general since the vector of conditioning variables may be empty.

Deﬁnition 3 Let y = (y1, . . . , yn) be a vector of n variables and x = (x1, . . . , xn(cid:48)) a vector
of n(cid:48) variables with joint domain D = [0, 1]n+n(cid:48). Let φ = (φ1, . . . , φm) be a vector of m
continuous potentials of the form

φj(y, x) = (max {(cid:96)j(y, x), 0})pj

where (cid:96)j is a linear function of y and x and pj ∈ {1, 2}. Let c = (c1, . . . , cr) be a vector of
r linear constraint functions associated with index sets denoting equality constraints E and
inequality constraints I, which deﬁne the feasible set

(cid:26)

˜D =

(y, x) ∈ D

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I

(cid:27)

.

For (y, x) ∈ D, given a vector of m nonnegative free parameters, i.e., weights, w =
(w1, . . . , wm), a constrained hinge-loss energy function fw is deﬁned as

(23)

(24)

(25)

fw(y, x) =

wjφj(y, x) .

m
(cid:88)

j=1

We now deﬁne HL-MRFs by placing a probability density over the inputs to a con-
strained hinge-loss energy function. Note that we negate the hinge-loss energy function so
that states with lower energy are more probable, in contrast with Deﬁnition 1. This change
is made for later notational convenience.

Deﬁnition 4 A hinge-loss Markov random ﬁeld P over random variables y and con-
ditioned on random variables x is a probability density deﬁned as follows: if (y, x) /∈ ˜D,
then P (y|x) = 0; if (y, x) ∈ ˜D, then

P (y|x) =

exp (−fw(y, x))

(26)

1
Z(w, x)

12

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

where

(cid:90)

Z(w, x) =

y|(y,x)∈ ˜D

exp (−fw(y, x)) dy .

(27)

In the rest of this paper, we will explore how to use HL-MRFs to solve a wide range
of structured machine learning problems. We ﬁrst introduce a probabilistic programming
language that makes HL-MRFs easy to deﬁne for large, rich domains.

4. Probabilistic Soft Logic

In this section we introduce a general-purpose probabilistic programming language, prob-
abilistic soft logic (PSL). PSL allows HL-MRFs to be easily applied to a broad range of
structured machine learning problems by deﬁning templates for potentials and constraints.
In models for structured data, there are very often repeated patterns of probabilistic de-
pendencies. A few of the many examples include the strength of ties between similar people
in social networks, the preference for triadic closure when predicting transitive relation-
ships, and the “exactly one active” constraints on functional relationships. Often, to make
graphical models both easy to deﬁne and able to generalize across diﬀerent data sets, these
repeated dependencies are deﬁned using templates. Each template deﬁnes an abstract de-
pendency, such as the form of a potential function or constraint, along with any necessary
parameters, such as the weight of the potential, each of which has a single value across all
dependencies deﬁned by that template. Given input data, an undirected graphical model
is constructed from a set of templates by ﬁrst identifying the random variables in the data
and then “grounding out” each template by introducing a potential or constraint into the
graphical model for each subset of random variables to which the template applies.

A PSL program is written in a declarative, ﬁrst-order syntax and deﬁnes a class of
HL-MRFs that are parameterized by the input data. PSL provides a natural interface to
represent hinge-loss potential templates using two types of rules: logical rules and arithmetic
rules. Logical rules are based on the mapping from logical clauses to hinge-loss potentials
introduced in Section 2. Arithmetic rules provide additional syntax for deﬁning an even
wider range of hinge-loss potentials and hard constraints.

4.1 Deﬁnition

In this subsection we deﬁne PSL. Our deﬁnition covers the essential functionality that
should be supported by all implementations, but many extensions are possible. The PSL
syntax we describe can capture a wide range of HL-MRFs, but new settings and scenarios
could motivate the development of additional syntax to make the construction of diﬀerent
kinds of HL-MRFs more convenient.

4.1.1 Preliminaries

We begin with a high-level deﬁnition of PSL programs.

Deﬁnition 5 A PSL program is a set of rules, each of which is a template for hinge-loss
potentials or hard linear constraints. When grounded over a base of ground atoms, a PSL
program induces a HL-MRF conditioned on any speciﬁed observations.

13

Bach, Broecheler, Huang, and Getoor

In the PSL syntax, many components are named using identiﬁers, which are strings that
begin with a letter (from the set {A, . . . , Z, a, . . . , z}), followed by zero or more letters,
numeric digits, or underscores.

PSL programs are grounded out over data, so the universe over which to ground must

be deﬁned.

Deﬁnition 6 A constant is a string that denotes an element in the universe over which
a PSL program is grounded.

Constants are the elements in a universe of discourse. They can be entities or attributes.
For example, the constant "person1" can denote a person, the constant "Adam" can denote
In PSL programs,
a person’s name, and the constant "30" can denote a person’s age.
constants are written as strings in double or single quotes. Constants use backslashes as
escape characters, so they can be used to encode quotes within constants. It is assumed that
constants are unambiguous, i.e., diﬀerent constants refer to diﬀerent entities and attributes.3
Groups of constants can be represented using variables.

Deﬁnition 7 A variable is an identiﬁer for which constants can be substituted.

Variables and constants are the arguments to logical predicates. Together, they are generi-
cally referred to as terms.

Deﬁnition 8 A term is either a constant or a variable.

Terms are connected by relationships called predicates.

Deﬁnition 9 A predicate is a relation deﬁned by a unique identiﬁer and a positive integer
called its arity, which denotes the number of terms it accepts as arguments. Every predicate
in a PSL program must have a unique identiﬁer as its name.

We refer to a predicate using its identiﬁer and arity appended with a slash. For example,
the predicate Friends/2 is a binary predicate, i.e., taking two arguments, which represents
whether two constants are friends. As another example, the predicate Name/2 can relate
a person to the string that is that person’s name. As a third example, the predicate
EnrolledInClass/3 can relate two entities, a student and professor, with an additional
attribute, the subject of the class.

Predicates and terms are combined to create atoms.

Deﬁnition 10 An atom is a predicate combined with a sequence of terms of length equal
to the predicate’s arity. This sequence is called the atom’s arguments. An atom with only
constants for arguments is called a ground atom.

Ground atoms are the basic units of reasoning in PSL. Each represents an unknown or
observation of interest and can take any value in [0, 1]. For example, the ground atom
Friends("person1", "person2") represents whether "person1" and "person2" are friends.
Atoms that are not ground are placeholders for sets of ground atoms. For example, the
atom Friends(X, Y) stands for all ground atoms that can be obtained by substituting
constants for variables X and Y.

3. Note that ambiguous references to underlying entities can be modeled by using diﬀerent constants for
diﬀerent references and representing whether they refer to the same underlying entity as a predicate.

14

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

4.1.2 Inputs

As we have already stated, PSL deﬁnes templates for hinge-loss potentials and hard linear
constraints that are grounded out over a data set to induce a HL-MRF. We now describe how
that data set is represented and provided as the inputs to a PSL program. The ﬁrst inputs
are two sets of predicates: a set C of closed predicates, the atoms of which are completely
observed, and a set O of open predicates, the atoms of which may be unobserved. The third
input is the base A, which is the set of all ground atoms under consideration. All atoms in
A must have a predicate in either C or O. These are the atoms that can be substituted into
the rules and constraints of a PSL program, and each will later be associated with a HL-
MRF random variable with domain [0, 1]. The ﬁnal input is a function O : A → [0, 1] ∪ {∅}
that maps the ground atoms in the base to either an observed value in [0, 1] or a symbol ∅
indicating that it is unobserved. The function O is only valid if all atoms with a predicate in
C are mapped to a [0, 1] value. Note that this deﬁnition makes the sets C and O redundant
in a sense, since they can be derived from A and O, but it will be convenient later to have
C and O explicitly deﬁned.

Ultimately, the method for specifying PSL’s inputs is implementation-speciﬁc, since
In this paper,
diﬀerent choices make it more or less convenient for diﬀerent scenarios.
we will assume that C, O, A, and O exist, and we remain agnostic about how they were
speciﬁed. However, to make this aspect of using PSL more concrete, we will describe one
possible method for deﬁning them here.

Our example method for specifying PSL’s inputs is text-based. The ﬁrst section of the
text input is a deﬁnition of the constants in the universe, which are grouped into types. An
example universe deﬁnition follows.

Person = {"alexis", "bob", "claudia", "david"}
Professor = {"alexis", "bob"}
Student = {"claudia", "david"}
Subject = {"computer science", "statistics"}

This universe includes six constants, four with two types ("alexis", "bob", "claudia",
and "david") and two with one type ("computer science" and "statistics").

The next section of input is the deﬁnition of predicates. Each predicate includes the
types of constants it takes as arguments and whether it is closed. For example, we can
deﬁne predicates for an advisor-student relationship prediction task as follows:

Advises(Professor, Student)

Department(Person, Subject) (closed)

EnrolledInClass(Student, Subject, Professor) (closed)

In this case, there is one open predicate (Advises) and two closed predicates (Department
and EnrolledInClass).

15

Bach, Broecheler, Huang, and Getoor

The ﬁnal section of input is any associated observations. They can be speciﬁed in a list,

for example:

Advises("alexis", "david") = 1

Department("alexis", "computer science") = 1

Department("bob", "computer science") = 1

Department("claudia", "statistics") = 1

Department("david", "statistics") = 1

In addition, values for atoms with the EnrolledInClass predicate could also be speciﬁed.
If a ground atom does not have a speciﬁed value, it will have a default observed value of 0
if its predicate is closed or remain unobserved if its predicate is open.

We now describe how this text input is processed into the formal inputs C, O, A, and
O. First, each predicate is added to either C or O based on whether it is annotated with
the (closed) tag. Then, for each predicate in C or O, ground atoms of that predicate are
added to A with each sequence of constants as arguments that can be created by selecting
a constant of each of the predicate’s argument types. For example, assume that the input
ﬁle contains a single predicate deﬁnition

Category(Document, Cat Name)

where the universe is Document = {"d1", "d2"} and Cat Name = {"politics", "sports"}.
Then,

A =






Category("d1", "politics"),
Category("d1", "sports"),
Category("d2", "politics"),
Category("d2", "sports")






.

(28)

Finally, we deﬁne the function O. Any atom in the explicit list of observations is mapped
to the given value. Then, any remaining atoms in A with a predicate in C are mapped to
0, and any with a predicate in O are mapped to ∅.

Before moving on, we also note that PSL implementations can support predicates and
atoms that are deﬁned functionally. Such predicates can be thought of as a type of closed
predicate. Their observed values are deﬁned as a function of their arguments. One of the
most common examples is inequality, atoms of which can be represented with the shorthand
inﬁx operator !=. For example, the following atom has a value of 1 when two variables A
and B are replaced with diﬀerent constants and 0 when replaced with the same constant.

Such functionally deﬁned predicates can be implemented without requiring their values over
all arguments to be speciﬁed by the user.

4.1.3 Rules and Grounding

Before introducing the syntax and semantics of speciﬁc PSL rules, we deﬁne the grounding
procedure that induces HL-MRFs in general. Given the inputs C, O, A, and O, PSL induces

A != B

16

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

a HL-MRF P (y|x) as follows. First, each ground atom a ∈ A is associated with a random
variable with domain [0, 1]. If O(a) = ∅, then the variable is included in the free variables
y, and otherwise it is included in the observations x with a value of O(a).

With the variables in the distribution deﬁned, each rule in the PSL program is applied
to the inputs and produces hinge-loss potentials or hard linear constraints, which are added
to the HL-MRF. In the rest of this subsection, we describe two kinds of PSL rules: logical
rules and arithmetic rules.

4.1.4 Logical Rules

The ﬁrst kind of PSL rule is a logical rule, which is made up of literals.

Deﬁnition 11 A literal is an atom or a negated atom.

In PSL, the preﬁx operator ! or ~ is used for negation. A negated atom has a value of one
minus the value of the unmodiﬁed atom. For example, if Friends("person1", "person2")
has a value of 0.7, then !Friends("person1", "person2") has a value of 0.3.

Deﬁnition 12 A logical rule is a disjunctive clause of literals. Logical rules are either
weighted or unweighted. If a logical rule is weighted, it is annotated with a nonnegative
weight and optionally a power of two.

Logical rules express logical dependencies in the model. As in Boolean logic, the negation,
disjunction (written as || or |), and conjunction (written as && or &) operators obey De
Morgan’s Laws. Also, an implication (written as -> or <-) can be rewritten as the negation
of the body disjuncted with the head. For example

P1(A, B) && P2(A, B) -> P3(A, B) || P4(A, B)
≡ !(P1(A, B) && P2(A, B)) || P3(A, B) || P4(A, B)
≡ !P1(A, B) || !P2(A, B) || P3(A, B) || P4(A, B)

Therefore, any formula written as an implication with (1) a literal or conjunction of literals
in the body and (2) a literal or disjunction of literals in the head is also a valid logical rule,
because it is equivalent to a disjunctive clause.

There are two kinds of logical rules: weighted or unweighted. A weighted logical rule is a
template for a hinge-loss potential that penalizes how far the rule is from being satisﬁed. A
weighted logical rule begins with a nonnegative weight and optionally ends with an exponent
of two (^2). For example, the weighted logical rule

1 : Advisor(Prof, S) && Department(Prof, Sub) -> Department(S, Sub)

has a weight of 1 and induces potentials propagating department membership from advisors
to advisees. An unweighted logical rule is a template for a hard linear constraint that
requires that the rule always be satisﬁed. For example, the unweighted logical rule

Friends(X, Y) && Friends(Y, Z) -> Friends(X, Z) .

induces hard linear constraints enforcing the transitivity of the Friends/2 predicate. Note
the period (.) that is used to emphasize that this rule is always enforced and disambiguate
it from weighted rules.

17

Bach, Broecheler, Huang, and Getoor

A logical rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. This procedure produces
a set of ground rules, which are rules containing only ground atoms. Each ground rule will
then be interpreted as either a potential or hard constraint in the induced HL-MRF. For
notational convenience, we assume without loss of generality that all the random variables
are unobserved, i.e., O(a) = ∅, ∀a ∈ A. If the input data contain any observations, the
following description still applies, except that some free variables will be replaced with
observations from x. The ﬁrst step in interpreting a ground rule is to map its disjunctive
clause to a linear constraint. This mapping is based on the uniﬁed inference objective
derived in Section 2. Any ground PSL rule is a disjunction of literals, some of which are
negated. Let I + be the set of indices of the variables that correspond to atoms that are not
negated in the ground rule, when expressed as a disjunctive clause, and, likewise, let I − be
the indices of the variables corresponding to atoms that are negated. Then, the clause is
mapped to the inequality

(cid:88)

1 −

(cid:88)

yi −

(1 − yi) ≤ 0 .

i∈I +

i∈I −

If the logical rule that templated the ground rule is weighted with a weight of w and is not
annotated with ^2, then the potential

is added to the HL-MRF with a parameter of w. If the rule is weighted with a weight w
and annotated with ^2, then the potential











φ(y, x) = max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −



φ(y, x) =

max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −

c(y, x) = 1 −

(cid:88)

(cid:88)

yi −

(1 − yi)

i∈I +

i∈I −








2








is added to the HL-MRF with a parameter of w. If the rule is unweighted, then the function

is added to the set of constraint functions and its index is included in the set I to deﬁne a
hard inequality constraint c(y, x) ≤ 0.

As an example of the grounding process, consider the following logical rule. As part of
a program for link prediction, it is often helpful to model the transitivity of a relationship.

3 : Friends(A, B) && Friends(B, C) -> Friends(C, A) ^2

Imagine that the input data are C = {}, O = {Friends/2},

(29)

(30)

(31)

(32)

(33)

A =






Friends("p1", "p2"),
Friends("p1", "p3"),
Friends("p2", "p1"),
Friends("p2", "p3"),
Friends("p3", "p1"),
Friends("p3", "p2")






,

18

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and O(a) = ∅, ∀a ∈ A. Then, the rule will induce six ground rules. One such ground rule is

3 : Friends("p1", "p2") && Friends("p2", "p3") -> Friends("p3", "p1") ^2

which is equivalent to the following.

3 : !Friends("p1", "p2") || !Friends("p2", "p3") || Friends("p3", "p1") ^2

If the atoms Friends("p1", "p2"), Friends("p2", "p3"), and Friends("p3", "p1")
correspond to the random variables y1, y2, and y3, respectively, then this ground rule is
interpreted as the weighted hinge-loss potential

3 (max{y1 + y2 − y3 − 1, 0})2 .

(34)

Since the grounding process uses the mapping from Section 2, logical rules can be used
to reason accurately and eﬃciently about both discrete and continuous information. They
are a convenient method for constructing HL-MRFs with the uniﬁed inference objective
for weighted logical knowledge bases as their MAP inference objective. They also allow
the user to seamlessly incorporate some of the additional features of HL-MRFs, such as
squared potentials and hard constraints. Next, we introduce an even more ﬂexible class of
PSL rules.

4.1.5 Arithmetic Rules

Arithmetic rules in PSL are more general templates for hinge-loss potentials and hard
linear constraints. Like logical rules, they come in weighted and unweighted variants, but
instead of using logical operators they use arithmetic operators. In general, an arithmetic
rule relates two linear combinations of atoms with an inequality or an equality. A simple
example enforces the mutual exclusivity of liberal and conservative ideologies.

Liberal(P) + Conservative(P) = 1 .

Just like logical rules, arithmetic rules are grounded out by performing all possible substi-
tutions of constants for variables to make ground atoms in the base A. In this example,
each substitution for Liberal(P) and Conservative(P) is constrained to sum to 1. Since
the rule is unweighted and arithmetic, it deﬁnes a hard constraint c(y, x) and its index will
be included in E because it is an equality constraint.

To make arithmetic rules more ﬂexible and easy to use, we deﬁne some additional syntax.
The ﬁrst is a generalized deﬁnition of atoms that can be substituted with sums of ground
atoms, rather than just a single atom.

Deﬁnition 13 A summation atom is an atom that takes terms and/or sum variables
as arguments. A summation atom represents the summations of ground atoms that can be
obtained by substituting individual constants for variables and summing over all possible
constants for sum variables.

A sum variable is represented by prepending a plus symbol (+) to a variable. For example,
the summation atom

Friends(P, +F)

19

Bach, Broecheler, Huang, and Getoor

is a placeholder for the sum of all ground atoms with predicate Friends/2 in A that share
a ﬁrst argument. Note that sum variables can be used at most once in a rule, i.e., each
sum variable in a rule must have a unique identiﬁer. Summation atoms are useful because
they can describe dependencies without needing to specify the number of atoms that can
participate. For example, the arithmetic rule

Label(X, +L) = 1 .

says that labels for each constant substituted for X should sum to one, without needing to
specify how many possible labels there are.

The substitutions for sum variables can be restricted using logical clauses as ﬁlters.

Deﬁnition 14 A ﬁlter clause is a logical clause deﬁned for a sum variable in an arithmetic
rule. The logical clause only contains atoms (1) with predicates that appear in C and (2)
that only take as arguments (a) constants, (b) variables that appear in the arithmetic rule,
and (c) the sum variable for which it is deﬁned.

Filter clauses restrict the substitutions for a sum variable in the corresponding arithmetic
rule by only including substitutions for which the clause evaluates to true. The ﬁlters are
evaluated using Boolean logic. Each ground atom a is treated as having a value of 0 if and
only if O(a) = 0. Otherwise, it is treated as having a value of 1. For example, imagine that
we want to restrict the summation in the following arithmetic rule to only constants that
satisfy a property Property/1.

Then, we can add the following ﬁlter clause.

Link(X, +Y) <= 1 .

{Y: Property(Y)}

Then, the hard linear constraints templated by the arithmetic rule will only sum over
constants substituted for Y such that Property(Y) is non-zero.

In arithmetic rules, atoms can also be modiﬁed with coeﬃcients. These coeﬃcients can

be hard-coded. As a simple example, in the rule

Susceptible(X) >= 0.5 Biomarker1(X) + 0.5 Biomarker2(X) .

the property Susceptible/1, which represents the degree to which a patient is susceptible
to a particular disease, must be at least the average value of two biomarkers.

PSL also supports two forms of coeﬃcient-deﬁning syntax. The ﬁrst form of coeﬃcient
syntax is a cardinality function that counts the number of terms substituted for a sum
variable. Cardinality functions enable rules that depend on the number of substitutions in
order to be scaled correctly, such as when averaging. Cardinality is denoted by enclosing a
sum variable, without the +, in pipes. For example, the rule

1 / |Y| Friends(X, +Y) = Friendliness(X) .

20

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

deﬁnes the Friendliness/1 property of a person X in a social network as the average
strength of their outgoing friendship links. In cases in which Friends/2 is not symmetric,
we can extend this rule to sum over both outgoing and incoming links as follows.

1 / |Y1| |Y2| Friends(X, +Y1) + 1 / |Y1| |Y2| Friends(+Y2, X)

= Friendliness(X) .

The second form of coeﬃcient syntax is built-in coeﬃcient functions. The exact set of
supported functions is implementation speciﬁc, but standard functions like maximum and
minimum should be included. Coeﬃcient functions are prepended with @ and use square
brackets instead of parentheses to distinguish them from predicates. Coeﬃcient functions
can take either scalars or cardinality functions as arguments. For example, the following
rule for matching two sets of constants requires that the sum of the Matched/2 atoms be
the minimum of the sizes of the two sets.

Matched(+X, +Y) = @Min[|X|, |Y|] .

Note that PSL’s coeﬃcient syntax can also be used to deﬁne constants, as in this example.
So far we have focused on using arithmetic rules to deﬁne templates for linear constraints,
but they can also be used to deﬁne hinge-loss potentials. For example, the following arith-
metic rule prefers that the degree to which a person X is extroverted (represented with
Extroverted/1) does not exceed the average extroversion of their friends:

2 : Extroverted(X) <= 1 / |Y| Extroverted(+Y) ^2
{Y: Friends(X, Y) || Friends(Y, X)}

This rule is a template for weighted hinge-loss potentials of the form

(cid:32)

(cid:40)

2

max

yi(cid:48) −

(cid:41)(cid:33)2

yi, 0

,

1
|F|

(cid:88)

i∈F

(35)

where yi(cid:48) is the variable corresponding to a grounding of the atom Extroverted(X) and
F is the set of the indices of the variables corresponding to Extroverted(Y) atoms of the
friends Y that satisfy the rule’s ﬁlter clause. Note that the weight of 2 is distinct from
the coeﬃcients in the linear constraint (cid:96)(y, x) ≤ 0 deﬁning the hinge-loss potential.
If
the arithmetic rule were an equality instead of an inequality, each grounding would be
two hinge-loss potentials, one using (cid:96)(y, x) ≤ 0 and one using −(cid:96)(y, x) ≤ 0. In this way,
arithmetic rules can deﬁne general hinge-loss potentials.

For completeness, we state the full, formal deﬁnition of an arithmetic rule and deﬁne its

grounding procedure.

Deﬁnition 15 An arithmetic rule is an inequality or equality relating two linear combi-
nations of summation atoms. Each sum variable in an arithmetic rule can be used once.
An arithmetic rule can be annotated with ﬁlter clauses for a subset of its sum variables that
restrict its groundings. Arithmetic rules are either weighted or unweighted. If an arithmetic
rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.

21

Bach, Broecheler, Huang, and Getoor

An arithmetic rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. In addition, summation
atoms are replaced by the appropriate summations over ground atoms (possibly restricted
by corresponding ﬁlter clauses) and the coeﬃcient is distributed across the summands. This
leads to a set of ground rules for each arithmetic rule given a set of inputs. If the arithmetic
rule is an unweighted inequality, each ground rule can be algebraically manipulated to be
of the form c(y, x) ≤ 0. Then c(y, x) is added to the set of constraint functions and its
index is added to I. If instead the arithmetic rule is an unweighted equality, each ground
rule is manipulated to c(y, x) = 0, c(y, x) is added to the set of constraint functions, and
its index is added to E. If the arithmetic rule is a weighted inequality with weight w, each
ground rule is manipulated to (cid:96)(y, x) ≤ 0 and included as a potential of the form

φ(y, x) = max {(cid:96)(y, x), 0}

(36)

with a weight of w. If the arithmetic rule is a weighted equality with weight w, each ground
rule is again manipulated to (cid:96)(y, x) ≤ 0 and two potentials are included,

φ1(y, x) = max {(cid:96)(y, x), 0}, φ2(y, x) = max {−(cid:96)(y, x), 0} ,

(37)

each with a weight of w. In either case, if the weighted arithmetic rule is annotated with
^2, then the induced potentials are squared.

4.2 Expressivity

An important question is the expressivity of PSL, which uses disjunctive clauses with pos-
itive weights for its logical rules. Other logic-based languages support diﬀerent types of
clauses, such as Markov logic networks (Richardson and Domingos, 2006), which support
clauses with conjunctions and clauses with negative weights. As we discuss in this section,
PSL’s logical rules capture a general class of structural dependencies, capable of model-
ing arbitrary probabilistic relationships among Boolean variables, such as those deﬁned by
Markov logic networks. The advantage of PSL is that it deﬁnes HL-MRFs, which are much
more scalable than discrete MRFs and often just as accurate, as we show in Section 6.4.

The expressivity of PSL is tied to the expressivity of the MAX SAT problem, since
they both use the same class of weighted clauses. There are two conditions on the clauses:
(1) they have nonnegative weights, and (2) they are disjunctive. We ﬁrst consider the
nonnegativity requirement and show that can actually be viewed as a restriction on the
structure of a clause. To illustrate, consider a weighted disjunctive clause of the form

−w :



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(38)

If this clause were part of a generalized MAX SAT problem, in which there were no restric-
tions on weight sign or clause structure, but the goal were still to maximize the sum of the
weights of the satisﬁed clauses, then this clause could be replaced with an equivalent one

22

(39)

(40)

(41)

(42)

(43)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

without changing the optimizer:

w :






¬xi






(cid:94)

i∈I +
j

(cid:94)

(cid:94)








xi


 .

i∈I −
j

Note that the clause has been changed in three ways: (1) the sign of the weight has been
changed, (2) the disjunctions have been replaced with conjunctions, and (3) the literals
have all been negated. Due to this equivalence, the restriction on the sign of the weights
is subsumed by the restriction on the structure of the clauses. In other words, any set of
clauses can be converted to a set with nonnegative weights that has the same optimizer,
but it might require including conjunctions in the clauses. It is also easy to verify that if
Equation (38) is used to deﬁne a potential in a discrete MRF, replacing it with a potential
deﬁned by (39) leaves the distribution unchanged, due to the normalizing partition function.
We now consider the requirement that clauses be disjunctive and illustrate how con-
junctive clauses can be replaced by an equivalent set of disjunctive clauses. The idea is to
construct a set of disjunctive clauses such that all assignments to the variables are mapped
to the same score, up to a constant. A simple example is replacing a conjunction

with disjunctions

w : x1 ∧ x2

w : x1 ∨ x2
w : ¬x1 ∨ x2
w : x1 ∨ ¬x2 .

Observe that the total score for all assignments to the variables remains the same, up to a
constant.

This example generalizes to a procedure for encoding any Boolean MRF into a set of
disjunctive clauses with nonnegative weights. Park (2002) showed that the MAP problem
for any discrete Bayesian network can be represented as an instance of MAX SAT. For
distributions of bounded factor size, the MAX SAT problem has size polynomial in the
number of variables and factors of the distribution. We describe how any Boolean MRF
can be represented with disjunctive clauses and nonnegative weights. Given a Boolean MRF
with arbitrary potentials deﬁned by mappings from joint states of subsets of the variables
to scores, a new MRF is created as follows. For each potential in the original MRF, a new
set of potentials deﬁned by disjunctive clauses is created. A conjunctive clause is created
corresponding to each entry in the potential’s mapping with a weight equal to the score
assigned by the weighted potential in the original MRF. Then, these clauses are converted to
equivalent disjunctive clauses as in the example of Equations (38) and (39) by also ﬂipping
the sign of their weights and negating the literals. Once this is done for all entries of all
potentials, what remains is an MRF deﬁned by disjunctive clauses, some of which might
have negative weights. We make all weights positive by adding a suﬃciently large constant
to all weights of all clauses, which leaves the distribution unchanged due to the normalizing
partition function.

23

Bach, Broecheler, Huang, and Getoor

It is important to note two caveats when converting arbitrary Boolean MRFs to MRFs
deﬁned using only disjunctive clauses with nonnegative weights. First, the number of clauses
required to represent a potential in the original MRF is exponential in the degree of the
potential. In practice, this is rarely a signiﬁcant limitation, since MRFs often contain low-
degree potentials. The other important point is that the step of adding a constant to all
the weights increases the total score of the MAP state. Since the bound of Goemans and
Williamson (1994) is relative to this score, the bound is loosened for the original problem the
larger the constant added to the weights is. This is to be expected, since even approximating
MAP is NP-hard in general (Abdelbar and Hedetniemi, 1998).

We have described how general structural dependencies can be modeled with the logical
rules of PSL. It is possible to represent arbitrary logical relationships with them. The
process for converting general rules to PSL’s logical rules can be done automatically and
made transparent to the user. We have elected in this section to deﬁne PSL’s logical rules
without making this conversion automatic to make clear the underlying formalism.

4.3 Modeling Patterns

PSL is a ﬂexible language, and there are some patterns of usage that come up in many
applications. We illustrate some of them in this subsection with a number of examples.

4.3.1 Domain and Range Rules

In many problems, the number of relations that can be predicted among some constants
is known. For binary predicates, this background knowledge can be viewed as constraints
on the domain (ﬁrst argument) or range (second argument) of the predicate. For example,
it might be background knowledge that each entity, such as a document, has exactly one
label. An arithmetic rule to express this follows.

Label(Document, +LabelName) = 1 .

The predicate Label is said to be functional.

Alternatively, sometimes it is the ﬁrst argument that should be summed over. For ex-
ample, imagine the task of predicting relationships among students and professors. Perhaps
it is known that each student has exactly one advisor. This constraint can be written as
follows.

Advisor(+Professor, Student) = 1 .

The predicate Advisor is said to be inverse functional.

Finally, imagine a scenario in which two social networks are being aligned. The goal is
to predict whether each pair of people, one from each network, is the same person, which is
represented with atoms of the Same predicate. Each person aligns with at most one person
in the other network, but might not align with anyone. This can be expressed with the
following two arithmetic rules.

The predicate Same is said to be both partial functional and partial inverse functional.

Same(Person1, +Person2) <= 1 .

Same(+Person1, Person2) <= 1 .

24

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Many variations on these examples are possible. For example, they can be generalized
to predicates with more than two arguments. Additional arguments can either be ﬁxed or
summed over in each rule. As another example, domain and range rules can incorporate
multiple predicates, so that an entity can participate in a ﬁxed number of relations counted
among multiple predicates.

4.3.2 Similarity

Many problems require explicitly reasoning about similarity, rather than simply whether
entities are the same or diﬀerent. For example, reasoning with similarity has been explored
using kernel methods, such as kFoil (Landwehr et al., 2010) that bases similarity computa-
tion on the relational structure of the data. The continuous variables of HL-MRFs make
modeling similarity straightforward, and PSL’s support for functionally deﬁned predicates
makes it even easier. For example, in an entity resolution task, the degree to which two
entities are believed to be the same might depend on how similar their names are. A rule
expressing this dependency is

1.0 : Name(P1, N1) && Name(P2, N2) && Similar(N1, N2) -> Same(P1, P2)

This rule uses the Similar predicate to measure similarity. Since it is a functionally deﬁned
predicate, it can be implemented as one of many diﬀerent, possibly domain specialized,
string similarity functions. Any similarity function that can output values in the range
[0, 1] can be used.

4.3.3 Priors

If no potentials are deﬁned over a particular atom, then it is equally probable that it has any
value between zero and one. Often, however, it should be more probable that an atom has
a value of zero, unless there is evidence that it has a nonzero value. Since atoms typically
represent the existence of some entity, attribute, or relation, this bias promotes sparsity
among the things inferred to exist. Further, if there is a potential that prefers that an
atom should have a value that is at least some numeric constant, such as when reasoning
with similarities as discussed in Section 4.3.2, it often should also be more probable that an
atom is no higher in value than is necessary to satisfy that potential. To accomplish both
these goals, simple priors can be used to state that atoms should have low values in the
absence of evidence to overrules those priors. A prior in PSL can be a rule consisting of
just a negative literal with a small weight. For example, in a link prediction task, imagine
that this preference should apply to atoms of the Link predicate. A prior is then

0.1 : !Link(A, B)

which acts as a regularizer on Link atoms.

4.3.4 Blocks and Canopies

In many tasks, the number of unknowns can quickly grow large, even for modest amounts of
data. For example, in a link prediction task the goal is to predict relations among entities.
The number of possible links grows quadratically with the number of entities (for binary

25

Bach, Broecheler, Huang, and Getoor

relations). If handled naively, this growth could make scaling to large data sets diﬃcult,
but this problem is often handled by constructing blocks (e.g., Newcombe and Kennedy,
1962) or canopies (McCallum et al., 2000) over the entities, so that a limited subset of all
possible links are actually considered. Blocking partitions the entities so that only links
among entities in the same partition element, i.e., block, are considered. Alternatively, for
a ﬁner grained pruning, a canopy is deﬁned for each entity, which is the set of other entities
to which it could possibly link. Blocks and canopies can be computed using specialized,
domain-speciﬁc functions, and PSL can incorporate them by including them as atoms in
the bodies of rules. Since blocks can be seen as a special case of canopies, we let the atom
InCanopy(A, B) be 1 if B is in the canopy or block of A, and 0 if it is not.
Including
InCanopy(A, B) atoms as additional conditions in the bodies of logical rules will ensure
that the dependencies only exist between the desired entities.

4.3.5 Aggregates

Another powerful feature of PSL is its ability to easily deﬁne aggregates, which are rules
that deﬁne random variables to be deterministic functions of sets of other random variables.
The advantage of aggregates is that they can be used to deﬁne dependencies that do not
scale in magnitude with the number of groundings in the data. For example, consider a
model for predicting interests in a social network. A fragment of a PSL program for this
task follows.

1.0 : Interest(P1, I) && Friends(P1, P2) -> Interest(P2, I)

1.0 : Age(P, "20-29") && Lives(P, "California") -> Interest(P, "Surfing")

These two rules express the belief that interests are correlated along friendship links in
the social network, and also that certain demographic information is predictive of speciﬁc
interests. The question any domain expert or learning algorithm faces is how strongly each
rule should be weighted relative to each other. The challenge of answering this question
when using templates is that the number of groundings of the ﬁrst rule varies from person to
person based on the number of friends, while the groundings of the second remain constant
(one per person). This inconsistent scaling of the two types of dependencies makes it diﬃcult
to ﬁnd weights that accurately reﬂect the relative inﬂuence each type of dependency should
have across people with diﬀerent numbers of friends.

Using an aggregate can solve this problem of inconsistent scaling. Instead of using a
separate ground rule to relate the interest of each friend, we can deﬁne a rule that is only
grounded once for each person, relating an average interest across all friends to each person’s
own interests. A PSL fragment for this approach is

1.0 : AverageFriendInterest(P, I) -> Interest(P, I)

AverageFriendInterest(P, I) = 1 / |F| Interest(+F, I) .
{F: Friends(P, F)}

/* Demographic dependencies are also included.

*/

where the predicate AverageFriendInterest/2 is an aggregate that is constrained to be
the average amount of interest each friend of a person P has in an interest I. The weight

26

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

of the logical rule can now be scaled more appropriately relative to other types of features
because there is only one grounding per person.

For a more complex example, consider the problem of determining whether two refer-
ences in the data refer to the same underlying person. One useful feature to use is whether
they have similar sets of friends in the social network. Again, a rule could be deﬁned that
is grounded out for each friendship pair, but this would suﬀer from the same scaling issues
as the previous example. Instead, we can use an aggregate to directly express how similar
the two references’ sets of friends are. A function that measures the similarity of two sets
A and B is Jaccard similarity:

J(A, B) =

|A ∩ B|
|A ∪ B|

.

Jaccard similarity is a nonlinear function, meaning that it cannot be used directly without
breaking the log-concavity of HL-MRFs, but we can approximate it with a linear function.
We deﬁne SameFriends/2 as an aggregate that approximates Jaccard similarity (where
SamePerson/2 is functional and inverse functional).

SameFriends(A, B) = 1 / @Max[|FA|, |FB|] SamePerson(+FA, +FB) .
{FA : Friends(A, FA)}
{FB : Friends(B, FB)}

SamePerson(+P1, P2) = 1 .

SamePerson(P1, +P2) = 1 .

The aggregate SameFriends/2 uses the sum of the SamePerson/2 atoms as the intersection
of the two sets, and the maximum of the sizes of the two sets of friends as a lower bound
on the size of their union.

5. MAP Inference

Having deﬁned HL-MRFs and a language for creating them, PSL, we turn to algorithms
for inference and learning. The ﬁrst task we consider is maximum a posteriori (MAP)
inference, the problem of ﬁnding a most probable assignment to the free variables y given
observations x. In HL-MRFs, the normalizing function Z(w, x) is constant over y and the
exponential is maximized by minimizing its negated argument, so the MAP problem is

arg max
y

P (y|x) ≡ arg min
y|y,x∈ ˜D

fw(y, x)

≡ arg min
y∈[0,1]n

w(cid:62)φ(y, x)

such that

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I .

(44)

MAP is a fundamental problem because (1) it is the method we will use to make predictions,
and (2) weight learning often requires performing MAP inference many times with diﬀerent
weights (as we discuss in Section 6). Here, HL-MRFs have a distinct advantage over general

27

Bach, Broecheler, Huang, and Getoor

discrete models, since minimizing fw is a convex optimization rather than a combinatorial
one. There are many oﬀ-the-shelf solutions for convex optimization, the most popular
of which are interior-point methods, which have worst-case polynomial time complexity
in the number of variables, potentials, and constraints (Nesterov and Nemirovskii, 1994).
Although in practice they perform better than their worst-case bounds (Wright, 2005), they
do not scale well to large structured prediction problems (Yanover et al., 2006). We therefore
introduce a new algorithm for exact MAP inference designed to scale to large HL-MRFs by
leveraging the sparse connectivity structure of the potentials and hard constraints that are
typical of models for real-world tasks.

5.1 Consensus Optimization Formulation

Our algorithm uses consensus optimization, a technique that divides an optimization prob-
lem into independent subproblems and then iterates to reach a consensus on the optimum
(Boyd et al., 2011). Given a HL-MRF P (y|x), we ﬁrst construct an equivalent MAP prob-
lem in which each potential and hard constraint is a function of diﬀerent variables. The
variables are then constrained to make the new and original MAP problems equivalent. We
let y(L,j) be a local copy of the variables in y that are used in the potential function φj,
j = 1, . . . , m and y(L,k+m) be a copy of those used in the constraint function ck, k = 1, . . . , r.
We refer to the concatenation of all of these vectors as yL. We also introduce a characteristic
= 0 if the constraint is
function χk for each constraint function where χk
satisﬁed and inﬁnity if it is not. Likewise, let χ[0,1] be a characteristic function that is 0 if the
input is in the interval [0, 1] and inﬁnity if it is not. We drop the constraints on the domain
of y, letting them range in principle over Rn and instead use these characteristic functions
to enforce the domain constraints. This formulation will make computation easier when
be the variables in y that correspond
the problem is later decomposed. Finally, let y

(cid:104)
ck(y(L,k+m), x)

(cid:105)

(C,ˆi)

(L,ˆi)

, ˆi = 1, . . . , m + r. Operators between y

to y
are deﬁned element-wise,
pairing the corresponding copied variables. Consensus optimization solves the reformulated
MAP problem

and y

(C,ˆi)

(L,ˆi)

arg min
(yL,y)

m
(cid:88)

j=1

such that

wjφj

(cid:16)

(cid:17)
y(L,j), x

+

(cid:104)

(cid:16)

χk

ck

y(L,k+m), x

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

(45)

y

(L,ˆi)

= y

(C,ˆi)

∀ˆi = 1, . . . , m + r .

Inspection shows that problems (44) and (45) are equivalent.

This reformulation enables us to relax the equality constraints y

in order
to divide problem (45) into independent subproblems that are easier to solve, using the
alternating direction method of multipliers (ADMM) (Glowinski and Marrocco, 1975; Gabay
and Mercier, 1976; Boyd et al., 2011). The ﬁrst step is to form the augmented Lagrangian
function for the problem. Let α = (α1, . . . , αm+r) be a concatenation of vectors of Lagrange

= y

(C,ˆi)

(L,ˆi)

28

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

multipliers. Then the augmented Lagrangian is

L(yL, α, y) =

wjφj

y(L,j), x

+

(cid:16)

(cid:17)

(cid:16)

(cid:104)
ck

χk

y(L,k+m), x

m
(cid:88)

j=1

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

+

m+r
(cid:88)

ˆi=1

(cid:16)

y

α(cid:62)
ˆi

(L,ˆi)

− y

(C,ˆi)

(cid:17)

+

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)y

(L,ˆi)

− y

(C,ˆi)

(46)

(cid:13)
2
(cid:13)
(cid:13)
2

using a step-size parameter ρ > 0. ADMM ﬁnds a saddle point of L(yL, α, y) by updating
the three blocks of variables at each iteration t:

∀ˆi = 1, . . . , m + r

αt
ˆi

← αt−1
ˆi

+ ρ

yt

L ← arg min

(cid:17)

(cid:16)

yt−1
(L,ˆi)

− yt−1
(C,ˆi)
L (cid:0)yL, αt, yt−1(cid:1)

yt ← arg min

L (cid:0)yt

L, αt, y(cid:1)

yL

y

The ADMM updates ensure that y converges to the global optimum y(cid:63), the MAP state
of P (y|x), assuming that there exists a feasible assignment to y. We check convergence
using the criteria suggested by Boyd et al. (2011), measuring the primal and dual residuals
at the end of iteration t, deﬁned as

(cid:107)¯rt(cid:107)2 (cid:44)

(cid:107)yt

(L,ˆi)

− yt

(C,ˆi)

(cid:107)2
2



(cid:107)¯st(cid:107)2 (cid:44) ρ

Ki(yt

i − yt−1
i

)2

(50)



1
2

(cid:33) 1
2

(cid:32) n
(cid:88)

i=1





m+r
(cid:88)

ˆi=1

where Ki is the number of copies made of the variable yi, i.e., the number of diﬀerent
potentials and constraints in which the variable participates. The updates are terminated
when both of the following conditions are satisﬁed










m+r
(cid:88)

ˆi=1

(cid:107)yt

(L,ˆi)

(cid:107)2
2



,

Ki(yt

i)2



1
2

(cid:32) n
(cid:88)

i=1

(cid:33) 1
2






(cid:107)¯rt(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel max

(cid:107)¯st(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel



1
2

(cid:107)2
(cid:107)αt
2
ˆi







m+r
(cid:88)

ˆi=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

using convergence parameters (cid:15)abs and (cid:15)rel.

5.2 Block Updates

We now describe how to implement the ADMM block updates (47), (48), and (49). Updating
the Lagrange multipliers α is a simple step in the gradient direction (47). Updating the
local copies yL (48) decomposes over each potential and constraint in the HL-MRF. For the

29

(47)

(48)

(49)

(51)

(52)

Bach, Broecheler, Huang, and Getoor

variables y(L,j) for each potential φj, this requires independently optimizing the weighted
potential plus a squared norm:

(cid:16)

(cid:110)

wj

max

(cid:96)j(y(L,j), x), 0

(cid:111)(cid:17)pj

arg min
y(L,j)

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(53)

There are three cases for y(cid:63)

Although this optimization problem is convex, the presence of the hinge function complicates
it. It could be solved in principle with an iterative method, such as an interior-point method,
but such methods would become very expensive over many ADMM updates. Fortunately,
we can reduce the problem to checking several cases and ﬁnd solutions much more quickly.
(L,j), the optimizer of problem (53), which correspond to
the three regions in which the solution could lie: (1) the region (cid:96)(y(L,j), x) < 0, (2) the
region (cid:96)(y(L,j), x) > 0, and (3) the region (cid:96)(y(L,j), x) = 0. We check each case by replacing
the potential with its value on the corresponding region, optimizing, and checking if the
optimizer is in the correct region. We check the ﬁrst case by replacing the potential φj
with zero. Then, the optimizer of the modiﬁed problem is y(C,j) − αj/ρ.
If (cid:96)j(y(C,j) −
αj/ρ, x) ≤ 0, then y(cid:63)
(L,j) = y(C,j) − αj/ρ, because it optimizes both the potential and the
squared norm independently. If instead (cid:96)j(y(C,j) − αj/ρ, x) > 0, then we can conclude that
(cid:96)j(y(cid:63)

(L,j), x) ≥ 0, leading to one of the next two cases.
In the second case, we replace the maximum term with the inner linear function. Then
the optimizer of the modiﬁed problem is found by taking the gradient of the objective with
respect to y(L,j), setting the gradient equal to the zero vector, and solving for y(L,j). In
other words, the optimizer is the solution for y(L,j) to the equation

(cid:34)

(cid:16)

∇y(L,j)

wj

(cid:96)j(y(L,j), x)

(cid:17)pj

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

(cid:35)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0 .

(54)

This condition deﬁnes a simple system of linear equations. If pj = 1, then the coeﬃcient
matrix is diagonal and trivial to solve. If pj = 2, then the coeﬃcient matrix is symmetric
and positive deﬁnite, and the system can be solved via Cholesky decomposition. (Since
the potentials of an HL-MRF often have shared structures, perhaps templated by a PSL
program, the Cholesky decompositions can be cached and shared among potentials for
improved performance.) Let y(cid:48)
(L,j) be the optimizer of the modiﬁed problem, i.e., the
solution to equation (54). If (cid:96)j(y(cid:48)
(L,j) = y(cid:48)
(L,j), x) ≥ 0, then y(cid:63)
(L,j) because we know the
solution lies in the region (cid:96)j(y(L,j), x) ≥ 0 and the objective of problem (53) and the
modiﬁed objective are equal on that region.
(L,j), x) ≥ 0
whenever (cid:96)j(y(C,j) − αj/ρ, x) ≥ 0, because the modiﬁed term is symmetric about the line
(cid:96)j(y(L,j), x) = 0. We therefore will only reach the following third case when pj = 1. If
(cid:96)j(y(C,j) − αj/ρ, x) > 0 and (cid:96)j(y(cid:48)
(L,j) is the
projection of y(C,j) − αj/ρ onto the hyperplane ck(y(L,j), x) = 0. This constraint must be
active because it is violated by the optimizers of both modiﬁed objectives (Martins et al.,
2015, Lemma 17). Since the potential has a value of zero whenever the constraint is active,
solving problem (53) reduces to the projection operation.

(L,j), x) < 0, then we can conclude that y(cid:63)

In fact, if pj = 2, then (cid:96)j(y(cid:48)

30

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

For the local copies y(L,k+m) for each constraint ck, the subproblem is easier:

(cid:104)

(cid:105)
ck(y(L,k+m), x)

+

χk

(cid:13)
(cid:13)
y(L,k+m) − y(C,k+m) +
(cid:13)
(cid:13)

ρ
2

1
ρ

arg min
y(L,k+m)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

αk+m

.

(55)

Whether ck is an equality or inequality constraint, the solution is the projection of y(C,k+m)−
αk+m/ρ to the feasible set deﬁned by the constraint. If ck is an equality constraint, i.e., k ∈
E, then the optimizer y(cid:63)
(L,k+m) is the projection of y(C,k+m)−αk+m/ρ onto ck(y(L,k+m), x) =
0. If, on the other hand, ck is an inequality constraint, i.e., k ∈ I, then there are two cases.
First, if ck(y(C,k+m) − αk+m/ρ, x) ≤ 0, then the solution is simply y(C,k+m) − αk+m/ρ.
Otherwise, it is again the projection onto ck(y(L,k+m), x) = 0.
To update the variables y (49), we solve the optimization

arg min
y

n
(cid:88)

i=1

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

χ[0,1] [yi] +

y

(L,ˆi)

− y

(C,ˆi)

+

(56)

1
ρ

(cid:13)
2
(cid:13)
αˆi
(cid:13)
(cid:13)
2

.

The optimizer is the state in which yi is set to the average of its corresponding local copies
added with their corresponding Lagrange multipliers divided by the step size ρ, and then
clipped to the [0, 1] interval. More formally, let copies(yi) be the set of local copies yc of
yi, each with a corresponding Lagrange multiplier αc. Then, we update each yi using

yi ←

1
|copies(yi)|

(cid:88)

yc∈copies(yi)

(cid:18)

yc +

(cid:19)

αc
ρ

(57)

and clip the result to [0, 1]. Speciﬁcally, if, after update (57), yi > 1, then we set yi to 1
and likewise set it to 0 if yi < 0.

Algorithm 1 shows the complete pseudocode for MAP inference. The method starts
by initializing local copies of the variables that appear in each potential and constraint,
along with a corresponding Lagrange multiplier for each copy. Then, until convergence, it
iteratively performs the updates (47), (48), and (49). In the pseudocode, we have interleaved
updates (47) and (48), updating both the Lagrange multipliers αˆi and the local copies y
(L,ˆi)
together for each subproblem, because they are local operations that do not depend on other
variables once y is updated in the previous iteration. This independence reveals another
advantage of our inference algorithm:
it is very easy to parallelize. The updates (47)
and (48) can be performed in parallel, the results gathered, update (49) performed, and the
updated y broadcast back to the subproblems. Parallelization makes our MAP inference
algorithm even faster and more scalable.

5.3 Lazy MAP Inference

One interesting and useful property of HL-MRFs is that it is not always necessary to
completely materialize the distribution in order to ﬁnd a MAP state. Consider a subset ˆφ
of the index set {1, . . . , m} of the potentials φ. Observe that if a feasible assignment to y
minimizes

(58)

wjφj(y, x)

(cid:88)

j∈ ˆφ

31

Bach, Broecheler, Huang, and Getoor

Algorithm 1 MAP Inference for HL-MRFs

Input: HL-MRF P (y|x), ρ > 0
Initialize y(L,j) as local copies of variables y(C,j) that are in φj, j = 1, . . . , m
Initialize y(L,k+m) as local copies of variables y(C,k+m) that are in ck, k = 1, . . . , r
, ˆi = 1, . . . , m + r
Initialize Lagrange multipliers αˆi corresponding to copies y

(L,ˆi)

(cid:96)j(y(L,j), x)

(cid:17)pj

+ ρ
2

(cid:13)
(cid:13)y(L,j) − y(C,j) + 1
(cid:13)

ρ αj

(cid:13)
2
(cid:13)
(cid:13)
2

while not converged do

for j = 1, . . . , m do

ρ αj

αj ← αj + ρ(y(L,j) − y(C,j))
y(L,j) ← y(C,j) − 1
if (cid:96)j(y(L,j), x) > 0 then
y(L,j) ← arg miny(L,j)
wj
if (cid:96)j(y(L,j), x) < 0 then

(cid:16)

y(L,j) ← Proj(cid:96)j =0(y(C,j) − 1

ρ αj)

end if

end if
end for

for k = 1, . . . , r do

αk+m ← αk+m + ρ(y(L,k+m) − y(C,k+m))
y(L,k+m) ← Projck

(y(C,k+m) − 1

ρ αk+m)

end for

for i = 1, . . . , n do

1
yi ←
|copies(yi)|
Clip yi to [0,1]

(cid:80)

end for

end while

yc∈copies(yi)

(cid:16)

yc + αc
ρ

(cid:17)

32

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and φj(y, x) = 0, ∀j /∈ ˆφ, then that assignment must be a MAP state because 0 is the
global minimum for any potential. Therefore, if we can identify a set of potentials that
is small, such that all the other potentials are 0 in a MAP state, then we can perform
MAP inference in a reduced amount of time. Of course, identifying this set is as hard as
MAP inference itself, but we can iteratively grow the set by starting with an initial set,
performing inference over the current set, adding any potentials that have nonzero values,
and repeating.

Since the lazy inference procedure requires that the assignment be feasible, there are
two ways to handle any constraints in the HL-MRF. One is to include all constraints in
the inference problem from the beginning. This strategy ensures feasibility, but the idea of
lazy grounding can also be extended to constraints to improve performance further. Just
as we check if potentials are unsatisﬁed, i.e., nonzero, we can also check if constraints are
unsatisﬁed, i.e., violated. So the algorithm now iteratively grows the set of active potentials
and active constraints, adding any that are unsatisﬁed until the MAP state of the HL-MRF
deﬁned by the active potentials and constraints is also a feasible MAP state of the true
HL-MRF.

The eﬃciency of lazy MAP inference can be improved heuristically by not adding all
unsatisﬁed potentials and constraints, but instead only adding those that are unsatisﬁed by
some threshold. This heuristic can decrease computational cost signiﬁcantly, although the
results are no longer guaranteed to be correct. Bounding the resulting error when possible
is an important direction for future work.

5.4 Evaluation of MAP Inference

In this section we evaluate the empirical performance of our MAP inference algorithm.4 We
compare its running times against those of MOSEK,5 a commercial convex optimization
toolkit that uses interior-point methods (IPMs). We conﬁrm the results of Yanover et al.
(2006) that IPMs do not scale well to large structured-prediction problems, and we show
that our MAP inference algorithm scales much better. In fact, we observe that our method
scales linearly in practice with the number of potentials and constraints in the HL-MRF.

We evaluate scalability by generating social networks of varying sizes, constructing HL-
MRFs over them, and measuring the running time required to ﬁnd a MAP state. We
compare our algorithm to MOSEK’s IPM. The social networks we generate are designed to
be representative of common social-network analysis tasks. We generate networks of users
that are connected by diﬀerent types of relationships, such as friendship and marriage, and
our goal is to predict the political preferences, e.g., liberal or conservative, of each user. We
also assume that we have local information about each user, representing features such as
demographic information.

We generate the social networks using power-law distributions according to a procedure
described by Broecheler et al. (2010b). For a target number of users N , in-degrees and out-
degrees d for each edge type are sampled from the power-law distribution D(k) ≡ αk−γ.
Incoming and outgoing edges of the same type are then matched randomly to create edges
until no more matches are possible. The number of users is initially the target number

4. Code is available at https://github.com/stephenbach/bach-jmlr17-code.
5. http://www.mosek.com

33

Bach, Broecheler, Huang, and Getoor

plus the expected number of users with zero edges, and then users without any edges are
removed. We use six edge types with various parameters to represent relationships in social
networks with diﬀerent combinations of abundance and exclusivity, choosing γ between 2
and 3, and α between 0 and 1, as suggested by Broecheler et al. We then annotate each
vertex with a value in [−1, 1] uniformly at random to represent local features indicating one
political preference or the other.

We generate social networks with between 22k and 66k vertices, which induce HL-
MRFs with between 130k and 397k total potentials and constraints. In all the HL-MRFs,
roughly 85% of those totals are potentials. For each social network, we create both a (log)
piecewise-linear HL-MRF (pj = 1, ∀j = 1, . . . , m in Deﬁnition 3) and a piecewise-quadratic
one (pj = 2, ∀j = 1, . . . , m). We weight local features with a parameter of 0.5 and choose
parameters in [0, 1] for the relationship potentials representing a mix of more and less
inﬂuential relationships.

We implement ADMM in Java and compare with the IPM in MOSEK (version 6) by
encoding the entire MPE problem as a linear program or a second-order cone program as
appropriate and passing the encoded problem via the Java native interface wrapper. All
experiments are performed on a single machine with a 4-core 3.4 GHz Intel Core i7-3770
processor with 32GB of RAM. Each optimizer used a single thread, and all results are
averaged over 3 runs.

We ﬁrst evaluate the scalability of ADMM when solving piecewise-linear MAP problems
and compare with MOSEK’s interior-point method. Figures 1a (normal scale) and 1c (log
scale) show the results. The running time of the IPM quickly explodes as the problem
size increases. The IPM’s average running time on the largest problem is about 2,200
seconds (37 minutes). This result demonstrates the limited scalability of the interior-point
method. In contrast, ADMM displays excellent scalability. The average running time on
the largest problem is about 70 seconds. Further, the running time appears to grow linearly
in the number of potential functions and constraints in the HL-MRF, i.e., the number of
subproblems that must be solved at each iteration. The line of best ﬁt for all runs on all sizes
has a coeﬃcient of determination R2 = 0.9972. Combined with Figure 1a, this shows that
ADMM scales linearly with increasing problem size in this experiment. We emphasize that
the implementation of ADMM is research code written in Java and the IPM is a commercial
package compiled to native machine code.

We then evaluate the scalability of ADMM when solving piecewise-quadratic MAP prob-
lem and again compare with MOSEK. Figures 1b (normal scale) and 1d (log scale) show
the results. Again, the running time of the interior-point method quickly explodes. We
can only test it on the three smallest problems, the largest of which took an average of
about 21k seconds to solve (over 6 hours). ADMM again scales linearly to the problem
(R2 = 0.9854). It is just as fast for quadratic problems as linear ones, taking average of
about 70 seconds on the largest problem.

One of the advantages of IPMs is great numerical stability and accuracy. Consensus
optimization, which treats both objective terms and constraints as subproblems, often re-
turns solutions that are only optimal and feasible to moderate precision for non-trivially
constrained problems (Boyd et al., 2011). Although this is often acceptable, we quantify
the mix of infeasibility and suboptimality by repairing the infeasibility and measuring the
resulting total suboptimality. We ﬁrst project the solutions returned by consensus opti-

34

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

(a) Linear MAP problems

(b) Quadratic MAP problems

(c) Linear MAP problems (log scale)

(d) Quadratic MAP problems (log scale)

Figure 1: Average running times to ﬁnd a MAP state for HL-MRFs.

35

Bach, Broecheler, Huang, and Getoor

mization onto the feasible region, which took a negligible amount of computational time.
Let pADMM be the value of the objective in Problem (45) at such a point and let pIPM be
the value of the objective at the solution returned by the IPM. Then the relative error on
that problem is (pADMM − pIPM)/pIPM. The relative error was consistently small; it varied
between 0.2% and 0.4%, and did not trend upward as the problem size increased. This
shows that ADMM was accurate, in addition to being much more scalable.

6. Weight Learning

In this section we present three weight learning methods for HL-MRFs, each with a diﬀerent
objective function. The ﬁrst method approximately maximizes the likelihood of the training
data. The second method maximizes the pseudolikelihood. The third method ﬁnds a large-
margin solution, preferring weights that discriminate the ground truth from other nearby
states. Since weights are often shared among many potentials deﬁned by a template, such
as all the groundings of a PSL rule, we describe these learning algorithms in terms of
templated HL-MRFs. We introduce some necessary notation for HL-MRF templates. Let
T = (t1, . . . , ts) denote a vector of templates with associated weights W = (W1, . . . , Ws).
We partition the potentials by their associated templates and let tq also denote the set of
indices of the potentials deﬁned by that template. So, j ∈ tq is a shorthand for saying
that the potential φj(y, x) was deﬁned by template tq. Then, we refer to the sum of the
potentials deﬁned by a template as

Φq(y, x) =

φj(y, x) .

(cid:88)

j∈tq

In the deﬁned HL-MRF, the weight of the j-th hinge-loss potential is set to the weight of
the template from which it was derived, i.e., wj = Wq, for each j ∈ tq. Equivalently, we can
rewrite the hinge-loss energy function as

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)).

fw(y, x) = W (cid:62)Φ(y, x) ,

6.1 Structured Perceptron and Approximate Maximum Likelihood Estimation

The canonical approach for learning parameters W is to maximize the log-likelihood of
training data. The partial derivative of the log-likelihood with respect to a parameter Wq
is

∂ log P (y|x)
∂Wq

= EW [Φq(y, x)] − Φq(y, x),

(61)

where EW is the expectation under the distribution deﬁned by W . For a smoother ascent,
it is often helpful to divide the q-th component of the gradient by the number of groundings
|tq| of the q-th template (Lowd and Domingos, 2007), which we do in our experiments.
Computing the expectation is intractable, so we use a common approximation (e.g., Collins,
2002; Singla and Domingos, 2005; Poon and Domingos, 2011): the values of the potentials
at the most probable setting of y with the current parameters, i.e., a MAP state. Using a
MAP state makes this learning approach a structured variant of voted perceptron (Collins,

36

(59)

(60)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

2002), and we expect it to do best when the space of explored distributions has relatively
low entropy. Following voted perceptron, we take steps of ﬁxed length in the direction of
the gradient, then average the points after all steps. Any step that is outside the feasible
region is projected back before continuing.

6.2 Maximum Pseudolikelihood Estimation

An alternative to structured perceptron is maximum-pseudolikelihood estimation (MPLE)
(Besag, 1975), which maximizes the likelihood of each variable conditioned on all other
variables, i.e.,

P ∗(y|x) =

P ∗(yi|MB(yi), x)

=

1
Zi(W , y, x)

exp (cid:2)−f i

w(yi, y, x)(cid:3) ;

Zi(W , y, x) =

exp (cid:2)−f i

f i
w(yi, y, x) =

wjφj

w(yi, y, x)(cid:3) ;
(cid:16)

{yi ∪ y\i}, x

(cid:17)

.

n
(cid:89)

i=1
n
(cid:89)

i=1
(cid:90)

yi
(cid:88)

j:i∈φj

(62)

(63)

(64)

(65)

Here, i ∈ φj means that yi is involved in φj, and MB(yi) denotes the Markov blanket of
yi—that is, the set of variables that co-occur with yi in any potential function. The partial
derivative of the log-pseudolikelihood with respect to Wq is

∂ log P ∗(y|x)
∂Wq

=





Eyi|MB

n
(cid:88)

i=1

(cid:88)

j∈tq:i∈φj



φj(y, x)

 − Φq(y, x) .

(66)

Computing the pseudolikelihood gradient does not require joint inference and takes time
linear in the size of y. However, the integral in the above expectation does not readily admit
a closed-form antiderivative, so we approximate the expectation. When a variable is uncon-
strained, the domain of integration is a one-dimensional interval on the real number line,
so Monte Carlo integration quickly converges to an accurate estimate of the expectation.

We can also apply MPLE when the constraints are not too interdependent. For example,
for linear equality constraints over disjoint groups of variables (e.g., variable sets that must
sum to 1.0), we can block-sample the constrained variables by sampling uniformly from a
simplex. These types of constraints are often used to represent categorical labels. We can
compute accurate estimates quickly because these blocks are typically low-dimensional.

6.3 Large-Margin Estimation

A diﬀerent approach to learning drops the probabilistic interpretation of the model and
views HL-MRF inference as a prediction function. Large-margin estimation (LME) shifts
the goal of learning from producing accurate probabilistic models to instead producing
accurate MAP predictions. The learning task is then to ﬁnd weights W that separate
the ground truth from other nearby states by a large margin. We describe in this section

37

Bach, Broecheler, Huang, and Getoor

a large-margin method based on the cutting-plane approach for structural support vector
machines (Joachims et al., 2009).

The intuition behind large-margin structured prediction is that the ground-truth state
should have energy lower than any alternate state by a large margin. In our setting, the
output space is continuous, so we parameterize this margin criterion with a continuous loss
function. For any valid output state ˜y, a large-margin solution should satisfy

fw(y, x) ≤ fw(˜y, x) − L(y, ˜y), ∀˜y,

(67)

where the loss function L(y, ˜y) measures the disagreement between a state ˜y and the train-
ing label state y. A common assumption is that the loss function decomposes over the
prediction components, i.e., L(y, ˜y) = (cid:80)
i L(yi, ˜yi). In this work, we use the (cid:96)1 distance
as the loss function, so L(y, ˜y) = (cid:80)
i (cid:107)yi − ˜yi(cid:107)1. Since we do not expect all problems to
be perfectly separable, we relax the large-margin constraint with a penalized slack ξ. We
obtain a convex learning objective for a large-margin solution

min
W ≥0

1
2

||W ||2 + Cξ

s.t. W (cid:62)(Φ(y, x) − Φ(˜y, x)) ≤ −L(y, ˜y) + ξ, ∀˜y,

(68)

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)) and C > 0 is a user-speciﬁed parameter. This for-
mulation is analogous to the margin-rescaling approach by Joachims et al. (2009). Though
such a structured objective is natural and intuitive, its number of constraints is the car-
dinality of the output space, which here is inﬁnite. Following their approach, we optimize
subject to the inﬁnite constraint set using a cutting-plane algorithm: we greedily grow a set
K of constraints by iteratively adding the worst-violated constraint given by a separation
oracle, then updating W subject to the current constraints. The goal of the cutting-plane
approach is to eﬃciently ﬁnd the set of active constraints at the solution for the full ob-
jective, without having to enumerate the inﬁnite inactive constraints. The worst-violated
constraint is

arg min
˜y

W (cid:62)Φ(˜y, x) − L(y, ˜y).

(69)

The separation oracle performs loss-augmented inference by adding additional potentials to
the HL-MRF. For ground truth in {0, 1}, these loss-augmenting potentials are also examples
of hinge-losses, and thus adding them simply creates an augmented HL-MRF. The worst-
violated constraint is then computed as standard inference on the loss-augmented HL-
MRF. However, ground truth values in the interior (0, 1) cause any distance-based loss to
be concave, which require the separation oracle to solve a non-convex objective. In this
case, we use the diﬀerence of convex functions algorithm (An and Tao, 2005) to ﬁnd a local
optimum. Since the concave portion of the loss-augmented inference objective pivots around
the ground truth value, the subgradients are 1 or −1, depending on whether the current
value is greater than the ground truth. We simply choose an initial direction for interior
labels by rounding, and ﬂip the direction of the subgradients for variables whose solution
states are not in the interval corresponding to the subgradient direction until convergence.

38

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Given a set K of constraints, we solve the SVM objective as in the primal form

||W ||2 + Cξ

min
W ≥0

1
2

s.t. K.

(70)

We then iteratively invoke the separation oracle to ﬁnd the worst-violated constraint. If
this new constraint is not violated, or its violation is within numerical tolerance, we have
found the max-margin solution. Otherwise, we add the new constraint to K, and repeat.

One fact of note is that the large-margin criterion always requires some slack for HL-
MRFs with squared potentials. Since the squared hinge potential is quadratic and the loss
is linear, there always exists a small enough distance from the ground truth such that an
absolute (i.e., linear) distance is greater than the squared distance. In these cases, the slack
parameter trades oﬀ between the peakedness of the learned quadratic energy function and
the margin criterion.

6.4 Evaluation of Learning

To demonstrate the ﬂexibility and eﬀectiveness of learning with HL-MRFs, we test them
on four diverse tasks: node labeling, link labeling, link prediction, and image completion.6
Each of these experiments represents a problem domain that is best solved with structured-
prediction approaches because their dependencies are highly structural. The experiments
show that HL-MRFs perform as well as or better than canonical approaches.

For these diverse tasks, we compare against a number of competing methods. For node
and link labeling, we compare HL-MRFs to discrete Markov random ﬁelds (MRFs). We
construct them with Markov logic networks (MLNs) (Richardson and Domingos, 2006),
which template discrete MRFs using logical rules similarly to PSL. We perform inference in
discrete MRFs using Gibbs sampling, and we ﬁnd approximate MAP states during learn-
ing using the search algorithm MaxWalkSat (Richardson and Domingos, 2006). For link
prediction for preference prediction, a task that is inherently continuous and nontrivial to
encode in discrete logic, we compare against Bayesian probabilistic matrix factorization
(BPMF) (Salakhutdinov and Mnih, 2008). Finally, for image completion, we run the same
experimental setup as Poon and Domingos (2011) and compare against the results they
report, which include tests using sum product networks, deep belief networks (Hinton and
Salakhutdinov, 2006), and deep Boltzmann machines (Salakhutdinov and Hinton, 2009).

We train HL-MRFs and discrete MRFs with all three learning methods: structured per-
ceptron (SP), maximum pseudolikelihood estimation(MPLE), and large-margin estimation
(LME). When appropriate, we evaluate statistical signiﬁcance using a paired t-test with re-
jection threshold 0.01. We describe the HL-MRFs used for our experiments using the PSL
rules that deﬁne them. To investigate the diﬀerences between linear and squared potentials
we use both in our experiments. HL-MRF-L refers to a model with all linear potentials
and HL-MRF-Q to one with all squared potentials. When training with SP and MPLE, we
use 100 gradient steps and a step size of 1.0 (unless otherwise noted), and we average the
iterates as in voted perceptron. For LME, we set C = 0.1. We experimented with various
settings, but the scores of HL-MRFs and discrete MRFs were not sensitive to changes.

6. Code is available at https://github.com/stephenbach/bach-jmlr17-code.

39

Bach, Broecheler, Huang, and Getoor

6.4.1 Node Labeling

When classifying documents, links between those documents—such as hyperlinks, citations,
or shared authorship—provide extra signal beyond the local features of individual docu-
ments. Collectively predicting document classes with these links tends to improve accuracy
(Sen et al., 2008). We classify documents in citation networks using data from the Cora
and Citeseer scientiﬁc paper repositories. The Cora data set contains 2,708 papers in seven
categories, and 5,429 directed citation links. The Citeseer data set contains 3,312 papers
in six categories, and 4,591 directed citation links. Let the predicate Category/2 represent
the category of each document and Cites/2 represent a citation from one document to
another.

The prediction task is, given a set of seed documents whose labels are observed, to infer
the remaining document classes by propagating the seed information through the network.
For each of 20 runs, we split the data sets 50/50 into training and testing partitions, and
seed half of each set. To predict discrete categories with HL-MRFs we predict the category
with the highest predicted value.

We compare HL-MRFs to discrete MRFs on this task. For prediction, we performed
2500 rounds of Gibbs sampling, 500 of which were discarded as burn-in. We construct both
using the same logical rules, which simply encode the tendency for a class to propagate
across citations. For each category "C i", we have the following two rules, one for each
direction of citation.

Category(A, "C i") && Cites(A, B) -> Category(B, "C i")

Category(A, "C i") && Cites(B, A) -> Category(B, "C i")

We also constrain the atoms of the Category/2 predicate to sum to 1.0 for a given document
as follows.

Category(D, +C) = 1.0 .

Table 1 lists the results of this experiment. HL-MRFs are the most accurate predictors on
both data sets. Both variants of HL-MRFs are also much faster than discrete MRFs. See
Table 3 for average inference times over ﬁve folds.

6.4.2 Link Labeling

An emerging problem in the analysis of online social networks is the task of inferring the
level of trust between individuals. Predicting the strength of trust relationships can provide
useful information for viral marketing, recommendation engines, and internet security. HL-
MRFs with linear potentials have been applied by Huang et al. (2013) to this task, showing
superior results with models based on sociological theory. We reproduce their experimen-
tal setup using their sample of the signed Epinions trust network, orginally collected by
Richardson et al. (2003), in which users indicate whether they trust or distrust other users.
We perform eight-fold cross-validation. In each fold, the prediction algorithm observes the
entire unsigned social network and all but 1/8 of the trust ratings. We measure predic-
tion accuracy on the held-out 1/8. The sampled network contains 2,000 users, with 8,675
signed links. Of these links, 7,974 are positive and only 701 are negative, making it a sparse
prediction task.

40

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Table 1: Average accuracy of classiﬁcation by HL-MRFs and discrete MRFs. Scores statis-
tically equivalent to the best scoring method are typed in bold.

Table 2: Average area under ROC and precision-recall curves of social-trust prediction by
HL-MRFs and discrete MRFs. Scores statistically equivalent to the best scoring method
by metric are typed in bold.

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

Citeseer Cora

0.729
0.729
0.683

0.724
0.729
0.695

0.686
0.715
0.687

0.816
0.818
0.789

0.802
0.808
0.789

0.756
0.797
0.783

ROC P-R (+) P-R (-)

0.822
HL-MRF-Q (SP)
HL-MRF-Q (MPLE) 0.832
0.814
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

0.765
0.757
0.783

0.655
0.725
0.795

0.978
0.979
0.976

0.965
0.963
0.967

0.942
0.963
0.973

0.452
0.482
0.462

0.357
0.333
0.453

0.270
0.298
0.441

41

Table 3: Average inference times (reported in seconds) of single-threaded HL-MRFs and
discrete MRFs.

Bach, Broecheler, Huang, and Getoor

Citeseer

Cora Epinions

HL-MRF-Q
HL-MRF-L
MRF

0.42
0.46
110.96

0.70
0.50
184.32

0.32
0.28
212.36

We use a model based on the social theory of structural balance, which suggests that so-
cial structures are governed by a system that prefers triangles that are considered balanced.
Balanced triangles have an odd number of positive trust relationships; thus, considering all
possible directions of links that form a triad of users, there are sixteen logical implications
of the following form.

Trusts(A,B) && Trusts(B,C) -> Trusts(A,C)

Huang et al. (2013) list all sixteen of these rules, a reciprocity rule, and a prior in their
Balance-Recip model, which we omit to save space.

Since we expect these structural implications to vary in accuracy, learning weights for
these rules provides better models. Again, we use these rules to deﬁne HL-MRFs and
discrete MRFs, and we train them using various learning algorithms. For inference with
discrete MRFs, we perform 5000 rounds of Gibbs sampling, of which the ﬁrst 500 are
burn-in. We compute three metrics: the area under the receiver operating characteristic
(ROC) curve, and the areas under the precision-recall curves for positive trust and negative
trust. On all three metrics, HL-MRFs with squared potentials score signiﬁcantly higher.
The diﬀerences among the learning methods for squared HL-MRFs are insigniﬁcant, but
the diﬀerences among the models is statistically signiﬁcant for the ROC metric. For area
under the precision-recall curve for positive trust, discrete MRFs trained with LME are
statistically tied with the best score, and both HL-MRF-L and discrete MRFs trained with
LME are statistically tied with the best area under the precision-recall curve for negative
trust. The results are listed in Table 2.

Though the random fold splits are not the same, using the same experimental setup,
Huang et al. (2013) also scored the precision-recall area for negative trust of standard trust
prediction algorithms EigenTrust (Kamvar et al., 2003) and TidalTrust (Golbeck, 2005),
which scored 0.131 and 0.130, respectively. The logical models based on structural balance
that we run here are signiﬁcantly more accurate, and HL-MRFs more than discrete MRFs.
In addition to comparing favorably with regard to predictive accuracy, inference in HL-
MRFs is also much faster than in discrete MRFs. Table 3 lists average inference times
on ﬁve folds of three prediction tasks: Cora, Citeseer, and Epinions. This illustrates an
important diﬀerence between performing structured prediction via convex inference versus
sampling in a discrete prediction space: convex inference can be much faster.

42

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

6.4.3 Link Prediction

Preference prediction is the task of inferring user attitudes (often quantiﬁed by ratings)
toward a set of items. This problem is naturally structured, since a user’s preferences
are often interdependent, as are an item’s ratings. Collaborative ﬁltering is the task of
predicting unknown ratings using only a subset of observed ratings. Methods for this
task range from simple nearest-neighbor classiﬁers to complex latent factor models. More
generally, this problem is an instance of link prediction, since the goal is to predict links
indicating preference between users and content. Since preferences are ordered rather than
Boolean, it is natural to represent them with the continuous variables of HL-MRFs, with
higher values indicating greater preference. To illustrate the versatility of HL-MRFs, we
design a simple, interpretable collaborative ﬁltering model for predicting humor preferences.
We test this model on the Jester dataset, a repository of ratings from 24,983 users on a set
of 100 jokes (Goldberg et al., 2001). Each joke is rated on a scale of [−10, +10], which we
normalize to [0, 1]. We sample a random 2,000 users from the set of those who rated all 100
jokes, which we then split into 1,000 train and 1,000 test users. From each train and test
matrix, we sample a random 50% to use as the observed features x; the remaining ratings
are treated as the variables y.

Our HL-MRF model uses an item-item similarity rule:

SimRating(J1, J2) && Likes(U, J1) -> Likes(U, J2)

where J1 and J2 are jokes and U is a user; the predicate Likes/2 indicates the degree
of preference (i.e., rating value); and SimRating/2 is a closed predicate that measures
the mean-adjusted cosine similarity between the observed ratings of two jokes. We also
include the following rules to enforce that Likes(U,J) concentrates around the observed
average rating of user U (represented with the predicate AvgUserRating/1) and item J
(represented with the predicate AvgJokeRating/1), and the global average (represented
with the predicate AvgRating/1).

AvgUserRating(U) -> Likes(U, J)

Likes(U, J) -> AvgUserRating(U)

AvgJokeRating(J) -> Likes(U, J)

Likes(U, J) -> AvgJokeRating(J)

AvgRating("constant") -> Likes(U, J)

Likes(U, J) -> AvgRating("constant")

The atom AvgRating("constant") takes a placeholder constant as an argument, since there
is only one grounding of it for the entire HL-MRF. Again, all three of these predicates are
closed and computed using averages of observed ratings. In all cases, the observed ratings
are taken only from the training data for learning (to avoid leaking information about the
test data) and only from the test data during testing.

We compare our HL-MRF model to a canonical latent factor model, Bayesian proba-
bilistic matrix factorization (BPMF) (Salakhutdinov and Mnih, 2008). BPMF is a fully
Bayesian treatment and is therefore considered “parameter-free;” the only parameter that
must be speciﬁed is the rank of the decomposition. Based on settings used by Xiong et al.

43

Table 4: Normalized mean squared/absolute errors (NMSE/NMAE) for preference predic-
tion using the Jester dataset. The lowest errors are typed in bold.

Bach, Broecheler, Huang, and Getoor

NMSE NMAE

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

0.0554
0.0549
0.0738

0.0578
0.0535
0.0544

0.1974
0.1953
0.2297

0.2021
0.1885
0.1875

BPMF

0.0501 0.1832

Table 5: Mean squared errors per pixel for image completion. HL-MRFs produce the
most accurate completions on the Caltech101 and the left-half Olivetti faces, and only sum-
product networks produce better completions on Olivetti bottom-half faces. Scores for other
methods are reported in Poon and Domingos (2011).

HL-MRF-Q (SP) SPN DBM DBN PCA NN

Caltech-Left
Caltech-Bottom
Olivetti-Left
Olivetti-Bottom

1741
1910
927
1226

1815
1924
942
918

2998
2656
1866
2401

4960
3447
2386
1931

2851
1944
1076
1265

2327
2575
1527
1793

(2010), we set the rank of the decomposition to 30 and use 100 iterations of burn in and 100
iterations of sampling. For our experiments, we use the code of Xiong et al. (2010). Since
BPMF does not train a model, we allow BPMF to use all of the training matrix during the
prediction phase.

Table 4 lists the normalized mean squared error (NMSE) and normalized mean absolute
error (NMAE), averaged over 10 random splits. Though BPMF produces the best scores,
the improvement over HL-MRF-L (LME) is not signiﬁcant in NMAE.

6.4.4 Image Completion

Digital image completion requires models that understand how pixels relate to each other,
such that when some pixels are unobserved, the model can infer their values from parts of the
image that are observed. We construct pixel-grid HL-MRFs for image completion. We test
these models using the experimental setup of Poon and Domingos (2011): we reconstruct
images from the Olivetti face data set and the Caltech101 face category. The Olivetti data
set contains 400 images, 64 pixels wide and tall, and the Caltech101 face category contains
435 examples of faces, which we crop to the center 64 by 64 patch, as was done by Poon

44

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Figure 2: Example results on image completion of Caltech101 (left) and Olivetti (right)
faces. From left to right in each column: (1) true face, left side predictions by (2) HL-
MRFs and (3) SPNs, and bottom half predictions by (4) HL-MRFs and (5) SPNs. SPN
completions are downloaded from Poon and Domingos (2011).

and Domingos (2011). Following their experimental setup, we hold out the last ﬁfty images
and predict either the left half of the image or the bottom half.

The HL-MRFs in this experiment are much more complex than the ones in our other
experiments because we allow each pixel to have its own weight for the following rules,
which encode agreement or disagreement between neighboring pixels:

Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

where Bright("P ij", I) is the normalized brightness of pixel "P ij" in image I, and
North("P ij", Q) indicates that Q is the north neighbor of "P ij". We similarly include
analogous rules for the south, east, and west neighbors, as well as the pixels mirrored across
the horizontal and vertical axes. This setup results in up to 24 rules per pixel, (boundary
pixels may not have north, south, east, or west neighbors) which, in a 64 by 64 image,
produces 80,896 PSL rules.

We train these HL-MRFs using SP with a 5.0 step size on the ﬁrst 200 images of each data
set and test on the last ﬁfty. For training, we maximize the data log-likelihood of uniformly
random held-out pixels for each training image, allowing for generalization throughout the
image. Table 5 lists our results and others reported by Poon and Domingos (2011) for sum-
product networks (SPN), deep Boltzmann machines (DBM), deep belief networks (DBN),
principal component analysis (PCA), and nearest neighbor (NN). HL-MRFs produce the
best mean squared error on the left- and bottom-half settings for the Caltech101 set and
the left-half setting in the Olivetti set. Only sum product networks produce lower error

45

Bach, Broecheler, Huang, and Getoor

on the Olivetti bottom-half faces. Some reconstructed faces are displayed in Figure 2,
where the shallow, pixel-based HL-MRFs produce comparably convincing images to sum-
product networks, especially in the left-half setting, where HL-MRFs can learn which pixels
are likely to mimic their horizontal mirror. While neither method is particularly good at
reconstructing the bottom half of faces, the qualitative diﬀerence between the deep SPN
and the shallow HL-MRF completions is that SPNs seem to hallucinate diﬀerent faces, often
with some artifacts, while HL-MRFs predict blurry shapes roughly the same pixel intensity
as the observed, top half of the face. The tendency to better match pixel intensity helps
HL-MRFs score better quantitatively on the Caltech101 faces, where the lighting conditions
are more varied than in Olivetti faces.

Training and predicting with these HL-MRFs takes little time.

In our experiments,
training each model takes about 45 minutes on a 12-core machine, while predicting takes
under a second per image. While Poon and Domingos (2011) report faster training with
SPNs, both HL-MRFs and SPNs clearly belong to a class of faster models when compared
to DBNs and DBMs, which can take days to train on modern hardware.

7. Related Work

Researchers in artiﬁcial intelligence and machine learning have long been interested in pre-
dicting interdependent unknowns using structural dependencies. Some of the earliest work
in this area is inductive logic programming (ILP) (Muggleton and De Raedt, 1994), in
which structural dependencies are described with ﬁrst-order logic. Using ﬁrst-order logic
has several advantages. First, it can capture many types of dependencies among variables,
such as correlations, anti-correlations, and implications. Second, it can compactly specify
dependencies that hold across many diﬀerent sets of propositions by using variables as wild-
cards that match entities in the data. These features enable the construction of intuitive,
general-purpose models that are easily applicable or adapted to diﬀerent domains. Inference
for ILP ﬁnds the propositions that satisfy a query, consistent with a relational knowledge
base. However, ILP is limited by its diﬃculty in coping with uncertainty. Standard ILP
approaches only model dependencies which hold universally, and such dependencies are rare
in real-world data.

Another broad area of research, probabilistic methods, directly models uncertainty over
unknowns. Probabilistic graphical models (PGMs) (Koller and Friedman, 2009) are a fam-
ily of formalisms for specifying joint distributions over interdependent unknowns through
graphical structures. The graphical structure of a PGM generally represents conditional
independence relationships among random variables. Explicitly representing conditional
independence relationships allows a distribution to be more compactly parametrized. For
example, in the worst case, a discrete distribution could be represented by an exponen-
tially large table over joint assignments to the random variables. However, describing the
distribution in smaller, conditionally independent pieces can be much more compact. Sim-
ilar beneﬁts apply to continuous distributions. Algorithms for probabilistic inference and
learning can also operate over the conditionally independent pieces described by the graph
structure. They are therefore straightforward to apply to a wide variety of distributions.
Categories of PGMs include Markov random ﬁelds (MRFs), Bayesian networks (BNs), and

46

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

dependency networks (DNs). Constructing PGMs often requires careful design, and models
are usually constructed for single tasks and data sets.

More recently, researchers have sought to combine the advantages of relational and
probabilistic approaches, creating the ﬁeld of statistical relational learning (SRL) (Getoor
and Taskar, 2007). SRL techniques build probabilistic models of relational data, i.e., data
composed of entities and relationships connecting them. Relational data is most often
described using a relational calculus, but SRL techniques are also equally applicable to
similar categories of data that go by other names, such as graph data or network data.
Modeling relational data is inherently complicated by the large number of interconnected
and overlapping structural dependencies that are typically present. This complication has
motivated two directions of work. The ﬁrst direction is algorithmic, seeking inference and
learning methods that scale up to high dimensional models. The other direction is both
user-oriented and—as a growing body of evidence shows—supported by learning theory,
seeking formalisms for compactly specifying entire groups of dependencies in the model
that share both form and parameters. Specifying these grouped dependencies, often in the
form of templates via a domain-speciﬁc language, is convenient for users. Most often in
relational data the structural dependencies hold without regard to the identities of entities,
instead being induced by an entity’s class (or classes) and the structure of its relationships
with other entities. Therefore, many SRL models and languages give users the ability to
specify dependencies in this abstract form and ground out models over speciﬁc data sets
based on these deﬁnitions. In addition to convenience, recent work in learning theory says
that repeated dependencies with tied parameters can be the key to generalizing from a
few—or even one—large, structured training example(s) (London et al., 2016).

A related ﬁeld to SRL is structured prediction (SP) (Bakir et al., 2007; Nowozin et al.,
2016), which generalizes the tasks of classiﬁcation and regression to the task of predict-
ing structured objects. The loss function used during learning is generalized to a task-
appropriate loss function that scores disagreement between predictions and the true struc-
tures. Often, models for structured prediction take the form of energy functions that are
linear in their parameters. Therefore, prediction with such models is equivalent to MAP
inference for MRFs. A distinct branch of SP is learn-to-search methods, in which the prob-
lem is decomposed into a series of one-dimension prediction problems. The challenge is to
learn a good order in which to predict the components of the structure, so that each one-
dimension prediction problem can be conditioned on the most useful information. Examples
of learn-to-search methods include incremental structured perceptron (Collins and Roark,
2004), SEARN (Daum´e III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross
and Bagnell, 2014).

In this paper we focus on SP methods that perform joint prediction directly. Better
understanding the diﬀerences and relative advantages of joint-prediction methods and learn-
to-search methods is an important direction for future work.
In the rest of this section
we survey models and domain-speciﬁc languages for SP and SRL (Section 7.1), inference
methods (Section 7.2), and learning methods (Section 7.3).

47

Bach, Broecheler, Huang, and Getoor

7.1 Models and Languages

SP and SRL encompass many approaches. One broad area of work—of which PSL is a
part—uses ﬁrst-order logic and other relational formalisms to specify templates for PGMs.
Probabilistic relational models (Friedman et al., 1999) deﬁne templates for BNs in terms of a
database schema, and they can be grounded out over instances of that schema to create BNs.
Relational dependency networks (Neville and Jensen, 2007) template RNs using structured
query language (SQL) queries over a relational schema. Markov logic networks (MLNs)
(Richardson and Domingos, 2006) use ﬁrst-order logic to deﬁne Boolean MRFs. Each logical
clause in a ﬁrst-order knowledge base is a template for a set of potentials when the MLN
is grounded out over a set of propositions. Whether each proposition is true is a Boolean
random variable, and the potential has a value of one when the corresponding ground clause
is satisﬁed by the propositions and zero when it is not. (MLNs are formulated such that
higher values of the energy function are more probable.) Clauses can either be weighted,
in which case the potential has the weight of the clause that templated it, or unweighted,
in which case in must hold universally, as in ILP. In these ways, MLNs are similar to
PSL. Whereas MLNs are deﬁned over Boolean variables, PSL is a templating language
for HL-MRFs, which are deﬁned over continuous variables. However, these continuous
variables can be used to model discrete quantities. See Section 2 for more information
on the relationships between HL-MRFs and discrete MRFs, and Section 6.4 for empirical
comparisons between the two. As we show, HL-MRFs and PSL scale much better while
In addition,
retaining the rich expressivity and accuracy of their discrete counterparts.
HL-MRFs and PSL can reason directly about continuous data.

PSL is part of a broad family of probabilistic programming languages (Gordon et al.,
2014). The goals of probabilistic programming and SRL often overlap. Probabilistic pro-
gramming seeks to make constructing probabilistic models easy for the end user, and sep-
arate model speciﬁcation from the development of inference and learning algorithms.
If
algorithms can be developed for the entire space of models covered by a language, then it
is easy for users to experiment with including and excluding diﬀerent model components.
It also makes it easy for existing models to beneﬁt from improved algorithms. Separation
of model speciﬁcation and algorithms is also useful in SRL for the same reasons. In this
paper we emphasize designing algorithms that are ﬂexible enough to support the full class of
HL-MRFs. Examples of probabilistic programming languages include IBAL (Pfeﬀer, 2001),
BLOG (Milch et al., 2005), Markov logic (Richardson and Domingos, 2006), ProbLog (De
Raedt et al., 2007), Church (Goodman et al., 2008), Figaro (Pfeﬀer, 2009), FACTORIE
(McCallum et al., 2009), Anglican (Wood et al., 2014), and Edward (Tran et al., 2016).

Other formalisms have also been proposed for probabilistic reasoning over continuous
domains and other domains equipped with semirings. Hybrid Markov logic networks (Wang
and Domingos, 2008) mix discrete and continuous variables. In addition to the dependencies
over discrete variables supported by MLNs, they support soft equality constraints between
two variables of the same form as those deﬁned by squared arithmetic rules in PSL, as well
as linear potentials of the form y1 − y2 for a soft inequality constraint y1 > y2. Inference in
hybrid MLNs is intractable. Wang and Domingos (2008) propose a random walk algorithm
for approximate MAP inference. Another related formalism is aProbLog (Kimmig et al.,
2011), which generalizes ProbLog to allow clauses to be annotated with elements from a

48

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

semiring, generalizing ProbLog’s support for clauses annotated with probabilities. Many
common inference tasks can be generalized from this perspective as algebraic model counting
(Kimmig et al., 2016). The PITA system (Riguzzi and Swift, 2011) for probabilistic logic
programming can also be viewe as implementing inference over various semirings.

7.2 Inference

Whether viewed as MAP inference for an MRF or SP without probabilistic semantics,
searching over a structured space to ﬁnd the optimal prediction is an important but diﬃcult
task. It is NP-hard in general (Shimony, 1994), so much work has focused on approximations
and identifying classes of problems for which it is tractable. A well-studied approximation
technique is local consistency relaxation (LCR) (Wainwright and Jordan, 2008). Inference
is ﬁrst viewed as an equivalent optimization over the realizable expected values of the
potentials, called the marginal polytope. When the variables are discrete and each potential
is an indicator that a subset of variables is in a certain state, this optimization becomes a
linear program. Each variable in the program is the marginal probability that a variable
is a particular state or the variables associated with a potential are in a particular joint
state. The marginal polytope is then the set of marginal probabilities that are globally
consistent. The number of linear constraints required to deﬁne the marginal polytope is
exponential in the size of the problem, however, so the linear program has to be relaxed in
order to be tractable. In a local consistency relaxation, the marginal polytope is relaxed
to the local polytope, in which the marginals over variables and potential states are only
locally consistent in the sense that each marginal over potential states sums to the marginal
distributions over the associated variables.

A large body of work has focused on solving the LCR objective quickly. Typically,
oﬀ-the-shelf convex optimization methods do not scale well for large graphical models and
structured predictors (Yanover et al., 2006), so a large branch of research has investigated
highly scalable message-passing algorithms. One approach is dual decomposition (DD)
(Sontag et al., 2011), which solves a problem dual to the LCR objective. Many DD algo-
rithms use coordinate descent, such as TRW-S (Kolmogorov, 2006), MSD (Werner, 2007),
MPLP (Globerson and Jaakkola, 2007), and ADLP (Meshi and Globerson, 2011). Other
DD algorithms use subgradient-based approaches (e.g., Jojic et al., 2010; Komodakis et al.,
2011; Schwing et al., 2012).

Another approach to solving the LCR objective uses message-passing algorithms to
solve the problem directly in its primal form. One well-known algorithm is that of Raviku-
mar et al. (2010a), which uses proximal optimization, a general approach that iteratively
improves the solution by searching for nearby improvements. The authors also provide
rounding guarantees for when the relaxed solution is integral, i.e., the relaxation is tight,
allowing the algorithm to converge faster. Another message-passing algorithm that solves
the primal objective is AD3 (Martins et al., 2015), which uses the alternating direction
method of multipliers (ADMM). AD3 optimizes objective (10) for binary, pairwise MRFs
and supports the addition of certain deterministic constraints on the variables. A third ex-
ample of a primal message-passing algorithm is APLP (Meshi and Globerson, 2011), which
is the primal analog of ADLP. Like AD3, it uses ADMM to optimize the objective.

49

Bach, Broecheler, Huang, and Getoor

Other approaches to approximate inference include tighter linear programming relax-
ations (Sontag et al., 2008, 2012). These tighter relaxations enforce local consistency on
variable subsets that are larger than individual variables, which makes them higher-order
local consistency relaxations. Mezuman et al. (2013) developed techniques for special cases
of higher-order relaxations, such as when the MRF contains cardinality potentials, in which
the probability of a conﬁguration depends on the number of variables in a particular state.
Researchers have also explored nonlinear convex programming relaxations, e.g., Ravikumar
and Laﬀerty (2006) and Kumar et al. (2006).

Previous analyses have identiﬁed particular subclasses whose local consistency relax-
ations are tight, i.e., the maximum of the relaxed program is exactly the maximum of the
original problem. These special classes include graphical models with tree-structured depen-
dencies, models with submodular potential functions, models encoding bipartite matching
problems, and those with nand potentials and perfect graph structures (Wainwright and
Jordan, 2008; Schrijver, 2003; Jebara, 2009; Foulds et al., 2011). Researchers have also
studied performance guarantees of other subclasses of the ﬁrst-order local consistency re-
laxation. Kleinberg and Tardos (2002) and Chekuri et al. (2005) considered the metric
labeling problem. Feldman et al. (2005) used the local consistency relaxation to decode
binary linear codes.

In this paper we examine the classic problem of MAX SAT—ﬁnding a joint Boolean
assignment to a set of propositions that maximizes the sum of a set of weighted clauses
that are satisﬁed—as an instance of SP. Researchers have also considered approaches to
solving MAX SAT other than the one one we study, the randomized algorithm of Goemans
and Williamson (1994). One line of work focusing on convex programming relaxations has
obtained stronger rounding guarantees than Goemans and Williamson (1994) by using non-
linear programming, e.g., Asano and Williamson (2002) and references therein. Other work
does not use the probabilistic method but instead searches for discrete solutions directly,
e.g., Mills and Tsang (2000), Larrosa et al. (2008), and Choi et al. (2009). We note that
one such approach, that of Wah and Shang (1997), is essentially a type of DD formulated
for MAX SAT. A more recent approach blends convex programming and discrete search via
mixed integer programming (Davies and Bacchus, 2013). Additionally, Huynh and Mooney
(2009) introduced a linear programming relaxation for MLNs inspired by MAX SAT re-
laxations, but the relaxation of general Markov logic provides no known guarantees on the
quality of solutions.

Finally, lifted inference takes advantage of symmetries in probability distributions to re-
duce the amount of work required for inference. Some of the earliest approaches identiﬁed
repeated dependency structures in PGMs to avoid repeated computations (Koller and Pfef-
fer, 1997; Pfeﬀer et al., 1999). Lifted inference has been widely applied in SRL because the
templates that are commonly used to deﬁne PGMs often induce symmetries. Various infer-
ence techniques for discrete MRFs have been extended to a lifted approach, including belief
propagation (Jaimovich et al., 2007; Singla and Domingos, 2008; Kersting et al., 2009) and
Gibbs sampling (Venugopal and Gogate, 2012). Approaches to lifted convex optimization
(Mladenov et al., 2012) might be extended to HL-MRFs. See de Salvo Braz et al. (2007),
Kersting (2012), and Kimmig et al. (2015) for more information on lifted inference.

50

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

7.3 Learning

Taskar et al. (2004) connected SP and PGMs by showing how to train MRFs with large-
margin estimation, a generalization of the large-margin objective for binary classiﬁcation
used to train support vector machines (Vapnik, 2000). Large-margin learning is a well-
studied approach to train structured predictors because it directly incorporates the struc-
tured loss function into a convex upper bound on the true objective: the regularized ex-
pected risk. The learning objective is to ﬁnd the parameters with smallest norm such that
a linear combination of feature functions assign a better score to the training data than
all other possible predictions. The amount by which the score of the correct prediction
must exceed the score of other predictions is scaled using the structured loss function.
The objective is therefore encoded as a norm minimization problem subject to many linear
constraints, one for each possible prediction in the structured space.

Structured SVMs (Tsochantaridis et al., 2005) extend large-margin estimation to a broad
class of structured predictors and admit a tractable cutting-plane learning algorithm. This
algorithm will terminate in a number of iterations linear in the size of the problem, and so
the computational challenge of large-margin learning for structured prediction comes down
to the task of ﬁnding the most violated constraint in the learning objective. This can be
accomplished by optimizing the energy function plus the loss function. In other words, the
task is to ﬁnd the structure that is the best combination of being favored by the energy
function but unfavored by the loss function. Often, the loss function decomposes over the
components of the prediction space, so the combined energy function and loss function can
often be viewed as simply the energy function of another structured predictor that is equally
challenging or easy to optimize, such as when the space of structures is a set of discrete
vectors and the loss function is the Hamming distance.

It is common during large-margin estimation that no setting of the parameters can
predict all the training data without error. In this case, the training data is said to not
be separable, again generalizing the notion of linear separability in the feature space from
binary classiﬁcation. The solution to this problem is to add slack variables to the constraints
that require the training data to be assigned the best score. The magnitude of the slack
variables are penalized in the learning objective, so estimation must trade oﬀ between the
norm of the parameters and violating the constraints. Joachims et al. (2009) extend this
formulation to a “one slack” formulation, in which a single slack variable is used for all the
constraints across all training examples, which is more eﬃcient. We use this framework for
large-margin estimation for HL-MRFs in Section 6.3.

The repeated inferences required for large-margin learning, one to ﬁnd the most-violated
constraint at each iteration, can become computationally expensive. Therefore researchers
have explored speeding up learning by interleaving the inference problem with the learning
problem. In the cutting-plane formulation discussed above, the objective is equivalently a
saddle-point problem, with the solution at the minimum with respect to the parameters and
the maximum with respect to the inference variables. Taskar et al. (2005) proposed dualizing
the inner inference problem to form a joint minimization. For SP problems with a tight
duality gap, i.e., the dual problem has the same optimal value as the primal problem, this
approach leads to an equivalent, convex optimization that can be solved for all variables
In other words, the learning and most-violated constraint problems are
simultaneously.

51

Bach, Broecheler, Huang, and Getoor

solved simultaneously, greatly reducing training time. For problems with non-tight duality
gaps, e.g., MAP inference in general, discrete MRFs, Meshi et al. (2010) showed that
the same principle can be applied by using approximate inference algorithms like dual
decomposition to bound the primal objective.

A related problem to parameter learning is structure learning, i.e., identifying an ac-
curate dependency structure for a model. A common SRL approach is searching over the
space of templates for PGMs. For probabilistic relational models, Friedman et al. (1999)
learned structures described in the vocabulary of relational schemas. For models that are
templated with ﬁrst-order-logic-like languages, such as PSL and MLNs, these approaches
take the form of rule learning. Based on rule-learning techniques from inductive logic pro-
gramming (e.g., Richards and Mooney, 1992; De Raedt and Dehaspe, 1996) a series of
approaches have sought to learn MLN rules from relational data. Initially, Kok and Domin-
gos (2005) learned rules by generating candidates and performing a beam search to identify
rules that improved a weighted pseudolikelihood objective. Then, Mihalkova and Mooney
(2007) observed that the previous approach generated candidate rules without regard to
the data, so they introduced an approach that used the data to guide the proposal of rules
via relational pathﬁnding. Kok and Domingos (2010) improved on that by ﬁrst perform-
ing graph clustering to ﬁnd common motifs, which are common subgraphs, to guide rule
proposal. They observed that modifying a rule set one clause at a time often got stuck
in poor local optima, and by using the motifs as reﬁnement operators instead, they were
able to converge to better optima. Other approaches to structure learning search directly
over grounded PGMs, including (cid:96)1-regularized pseudolikelihood maximization (Ravikumar
et al., 2010b) and grafting (Perkins et al., 2003; Zhu et al., 2010). These methods can all
be extended to HL-MRFs and PSL.

8. Conclusion

In this paper we introduced HL-MRFs, a new class of probabilistic graphical models that
unite and generalize several approaches to modeling relational and structured data: Boolean
logic, probabilistic graphical models, and fuzzy logic. HL-MRFs can capture relaxed, prob-
abilistic inference with Boolean logic and exact, probabilistic inference with fuzzy logic,
making them useful models for both discrete and continuous data. HL-MRFs also general-
ize these inference techniques with additional expressivity, allowing for even more ﬂexibility.
HL-MRFs are a signiﬁcant addition to the the library of machine learning tools because
they embody a useful point in the spectrum of models that trade oﬀ between scalability
and expressivity. As we showed, they can be easily applied to a wide range of structured
problems in machine learning and achieve high-quality predictive performance, competitive
with or surpassing the performance of canonical approaches. However, these other models
either do not scale as well, like discrete MRFs, or are not as versatile in their ability to
capture a wide range of problems, like Bayesian probabilistic matrix factorization.

We also introduced PSL, a probabilistic programming language for HL-MRFs. PSL
makes HL-MRFs easy to design, allowing users to encode their ideas for structural depen-
dencies using an intuitive syntax based on ﬁrst-order logic. PSL also helps accelerate a
time-consuming aspect of the modeling process: reﬁning a model. In contrast with other
types of models that require specialized inference and learning algorithms depending on

52

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

which structural dependencies are included, HL-MRFs can encode many types of depen-
dencies and scale well with the same inference and learning algorithms. PSL makes it easy
to quickly add, remove, and modify dependencies in the model and rerun inference and
learning, allowing users to quickly improve the quality of their models. Finally, because
PSL uses a ﬁrst-order syntax, each PSL program actually speciﬁes an entire class of HL-
MRFs, parameterized by the particular data set over which it is grounded. Therefore, a
model or components of a model reﬁned for one data set can easily be applied to others.

Next, we introduced inference and learning algorithms that scale to large problems. The
MAP inference algorithm is far more scalable than standard tools for convex optimization
because it leverages the sparsity that is so common to the dependencies in structured
prediction. The supervised learning algorithms extend standard learning objectives to HL-
MRFs. Together, this combination of an expressive formalism, a user-friendly probabilistic
programming language, and highly scalable algorithms enables researchers and practitioners
to easily build large-scale, accurate models of relational and structured data.7

This paper also lays the foundation for many lines of future work. Our analysis of local
consistency relaxation (LCR) as a hierarchical optimization is a general proof technique,
and it could be used to derive compact forms for other LCR objectives. As in the case of
MRFs deﬁned using logical clauses, such compact forms can simplify analysis and could
lead to a greater understanding of LCR for other classes of MRFs. Another important line
of work is understanding what guarantees apply to the MAP states of HL-MRFs. Can
anything be said about their ability to approximate MAP inference in discrete models that
go beyond the models already covered by the known rounding guarantees? Future directions
also include developing new algorithms for HL-MRFs. One important direction is marginal
inference for HL-MRFs and algorithms for sampling from them. Unlike marginal inference
for discrete distributions, which computes the marginal probability that a variable is in a
particular state, marginal inference for HL-MRFs requires ﬁnding the marginal probability
that a variable is in a particular range. One option for doing so, as well as generating samples
from HL-MRFs, is to extend the hit-and-run sampling scheme of Broecheler and Getoor
(2010). This method was developed for continuous constrained MRFs with piecewise-linear
potentials. There are also many new domains to which HL-MRFs and PSL can be applied.
With these modeling tools, researchers can design and apply new solutions to structured
prediction problems.

Acknowledgments

We acknowledge the many people who have contributed to the development of HL-MRFs
and PSL. Contributors include Eriq Augustine, Shobeir Fakhraei, James Foulds, Angelika
Kimmig, Stanley Kok, Ben London, Hui Miao, Lilyana Mihalkova, Dianne P. O’Leary, Jay
Pujara, Arti Ramesh, Theodoros Rekatsinas, and V.S. Subrahmanian. This work was sup-
ported by NSF grants CCF0937094 and IIS1218488, and IARPA via DoI/NBC contract
number D12PC00337. The U.S. Government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those of the authors and should

7. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

53

Bach, Broecheler, Huang, and Getoor

not be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.

Appendix A. Proof of Theorem 2

In this appendix, we prove the equivalence of objectives (7) and (10). Our proof analyzes
the local consistency relaxation to derive an equivalent, more compact optimization over
only the variable pseudomarginals µ that is identical to the MAX SAT relaxation. Since the
variables are Boolean, we refer to each pseudomarginal µi(1) as simply µi. Let xF
j denote
the unique setting such that φj(xF
j ) = 0. (I.e., xF
is the setting in which each literal in the
j
clause Cj is false.)

We begin by reformulating the local consistency relaxation as a hierarchical optimiza-
tion, ﬁrst over the variable pseudomarginals µ and then over the factor pseudomarginals θ.
Due to the structure of local polytope L, the pseudomarginals µ parameterize inner linear
programs that decompose over the structure of the MRF, such that—given ﬁxed µ—there is
an independent linear program ˆφj(µ) over θj for each clause Cj. We rewrite objective (10)
as

arg max
µ∈[0,1]n

(cid:88)

ˆφj(µ),

Cj ∈C

where

ˆφj(µ) = max
θj

wj

(cid:88)

θj(xj)

xj |xj (cid:54)=xF
j

(cid:88)

such that

θj(xj) = µi

θj(xj) = 1 − µi

xj |xj (i)=1
(cid:88)

xj |xj (i)=0
(cid:88)

θj(xj) = 1

xj
θj(xj) ≥ 0

∀i ∈ I +
j

∀i ∈ I −
j

∀xj .

(71)

(72)

(73)

(74)

(75)

(76)

It is straightforward to verify that objectives (10) and (71) are equivalent for MRFs with
disjunctive clauses for potentials. All constraints deﬁning L can be derived from the con-
straint µ ∈ [0, 1]n and the constraints in the deﬁnition of ˆφj(µ). We have omitted redundant
constraints to simplify analysis.

To make this optimization more compact, we replace each inner linear program ˆφj(µ)
with an expression that gives its optimal value for any setting of µ. Deriving this expression
j of ˆφj(µ), which is guaranteed to exist because
requires reasoning about any maximizer θ(cid:63)
problem (72) is bounded and feasible8 for any parameters µ ∈ [0, 1]n and wj.

We ﬁrst derive a suﬃcient condition for the linear program to not be fully satisﬁable, in
the sense that it cannot achieve a value of wj, the maximum value of the weighted potential

8. Setting θj(xj) to the probability deﬁned by µ under the assumption that the elements of xj are inde-

pendent, i.e., the product of the pseudomarginals, is always feasible.

54

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

wjφj(x). Observe that, by the objective (72) and the simplex constraint (75), showing that
ˆφj(µ) is not fully satisﬁable is equivalent to showing that θ(cid:63)

j (xF

j ) > 0.

Lemma 16 If

µi +

(1 − µi) < 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

then θ(cid:63)

j (xF

j ) > 0.

Proof By the simplex constraint (75),

Also, by summing all the constraints (73) and (74),

µi +

(1 − µi) <

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) .

(cid:88)

xj

(cid:88)

θ(cid:63)
j (xj) ≤

xj |xj (cid:54)=xF
j

µi +

(1 − µi) ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

because all the components of θ(cid:63) are nonnegative, and—except for θ(cid:63)
at least once in constraints (73) and (74). These bounds imply

j (xF

j )—they all appear

(cid:88)

xj |xj (cid:54)=xF
j

θ(cid:63)
j (xj) <

θ(cid:63)
j (xj) ,

(cid:88)

xj

which means θ(cid:63)

j (xF

j ) > 0, completing the proof.

We next show that if ˆφj(µ) is parameterized such that it is not fully satisﬁable, as in

Lemma 16, then its optimum always takes a particular value deﬁned by µ.

Lemma 17 If wj > 0 and θ(cid:63)

j (xF

j ) > 0, then

(cid:88)

θ(cid:63)
j (xj) =

xj |xj (cid:54)=xF
j

µi +

(1 − µi) .

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

Proof We prove the lemma via the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939;
Kuhn and Tucker, 1951). Since problem (72) is a maximization of a linear function subject
to linear constraints, the KKT conditions are necessary and suﬃcient for any optimum θ(cid:63)
j .
Before writing the relevant KKT conditions, we introduce some necessary notation. For
a state xj, we need to reason about the variables that disagree with the unsatisﬁed state
xF

j . Let

(cid:110)

d(xj) (cid:44)

i ∈ I +

j ∪ I −

j |xj(i) (cid:54)= xF

j (i)

(cid:111)

be the set of indices for the variables that do not have the same value in the two states xj
and xF
j .

We now write the relevant KKT conditions for θ(cid:63)

j . Let λ, α be real-valued vectors where
j | + 1 and |α| = |θj|. Let each λi correspond to a constraint (73) or (74)

j | + |I −

|λ| = |I +

55

Bach, Broecheler, Huang, and Getoor

for i ∈ I +
correspond to a constraint (76) for each xj. Then, the following KKT conditions hold:

j , and let λ∆ correspond to the simplex constraint (75). Also, let each αxj

j ∪ I −

αxj ≥ 0
αxj θ(cid:63)
λ∆ + αxF
j
(cid:88)

j (xj) = 0
= 0

wj +

i∈d(xj )

λi + λ∆ + αxj = 0

∀xj (cid:54)= xF
j

.

∀xj
∀xj

(77)

(78)

(79)

(80)

Since θ(cid:63)

j (xF

j ) > 0, by condition (78), αxF

= 0. By condition (79), then λ∆ = 0. From
here we can bound the other elements of λ. Observe that for every i ∈ I +
j , there
exists a state xj such that d(xj) = {i}. Then, it follows from condition (80) that there
exists xj such that, for every i ∈ I +

j ∪ I −

j

j ∪ I −
j ,

wj + λi + λ∆ + αxj = 0 .

Since αxj ≥ 0 by condition (77) and λ∆ = 0, it follows that λi ≤ −wj. With these bounds,
we show that, for any state xj, if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0. Assume that for some state
xj, |d(xj)| ≥ 2. By condition (80) and the derived constraints on λ,

αxj ≥ (|d(xj)| − 1)wj > 0 .
j (xj) = 0. Next, observe that for all i ∈ I +
With condition (78), θ(cid:63)
j ) and for
j
any state xj, if d(xj) = {i}, then xj(i) = 1 (resp. xj(i) = 0), and for any other state x(cid:48)
j
such that x(cid:48)
j) ≥ 2. By constraint (73) (resp. constraint (74)),
θ(cid:63)(xj) = µi (resp. θ(cid:63)(xj) = 1 − µi).
We have shown that if θ(cid:63)
j ), then θ(cid:63)

j ) > 0, then for all states xj, if d(xj) = {i} and i ∈ I +
j
j (xj) = 1 − µi), and if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0.

j (xF
j (xj) = µi (resp. θ(cid:63)

j(i) = 1 (resp. x(cid:48)

j(i) = 0), d(x(cid:48)

(resp. i ∈ I −

(resp. i ∈ I −
This completes the proof.

Lemma 16 says if (cid:80)

(1 − µi) < 1, then ˆφj(µ) is not fully satisﬁable,
and Lemma 17 provides its optimal value. We now reason about the other case, when
(cid:80)
(1 − µi) ≥ 1, and we show that this condition is suﬃcient to ensure that

µi + (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

i∈I +
j

i∈I −
j

ˆφj(µ) is fully satisﬁable.

Lemma 18 If wj > 0 and

j (xF

j ) = 0.

then θ(cid:63)
Proof We prove the lemma by contradiction. Assume that wj > 0, (cid:80)
µi) ≥ 1, and that the lemma is false, θ(cid:63)
j ) > 0. Then, by Lemma 17,

µi + (cid:80)

(1 −

i∈I −
j

i∈I +
j

µi +

(1 − µi) ≥ 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) ≥ 1 .

j (xF
(cid:88)

xj |xj (cid:54)=xF
j

56

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The assumption that θ(cid:63)

j (xF

j ) > 0 implies

θ(cid:63)
j (xj) > 1,

(cid:88)

xj

j ) < 0 is excluded by the nonnegativity constraints (76).

which is a contradiction, since it violates the simplex constraint (75). The possibility that
θ(cid:63)
j (xF
For completeness and later convenience, we also state the value of ˆφj(µ) when it is fully
satisﬁable.

Lemma 19 If θ(cid:63)

j (xF

j ) = 0, then

(cid:88)

θ(cid:63)
j (xj) = 1 .

xj |xj (cid:54)=xF
j

Proof The lemma follows from the simplex constraint (75).

We can now combine the previous lemmas into a single expression for the value of ˆφj(µ).

Lemma 20 For any feasible setting of µ,

ˆφj(µ) = wj min

µi +

(1 − µi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






Proof The lemma is trivially true if wj = 0 since any assignment will yield zero value. If
wj > 0, then we consider two cases. In the ﬁrst case, if (cid:80)
(1 − µi) < 1,
then, by Lemmas 16 and 17,

µi + (cid:80)

i∈I −
j

i∈I +
j

In the second case, if (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

(1 − µi) ≥ 1, then, by Lemmas 18 and 19,

ˆφj(µ) = wj






(cid:88)

i∈I +
j



µi +

(1 − µi)


 .

(cid:88)

i∈I −
j

ˆφj(µ) = wj .

By factoring out wj, we can rewrite this piecewise deﬁnition of ˆφj(µ) as wj multiplied by
the minimum of (cid:80)

(1 − µi) and 1, completing the proof.

µi + (cid:80)

i∈I +
j

i∈I −
j

This leads to our ﬁnal equivalence result.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

57

Bach, Broecheler, Huang, and Getoor

Proof Substituting the solution of the inner optimization from Lemma 20 into the local
consistency relaxation objective (71) gives a projected optimization over only µ which is
identical to the MAX SAT relaxation objective (7).

References

2008.

A. Abdelbar and S. Hedetniemi. Approximating MAPs for belief networks is NP-hard and

other theorems. Artiﬁcial Intelligence, 102(1):21–38, 1998.

N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, third edition,

D. Alshukaili, A. A. A. Fernandes, and N. W. Paton. Structuring linked data search results
using probabilistic soft logic. In International Semantic Web Conference (ISWC), 2016.

L. An and P. Tao. The DC (diﬀerence of convex functions) programming and DCA revisited
with DC models of real world nonconvex optimization problems. Annals of Operations
Research, 133:23–46, 2005.

T. Asano and D. P. Williamson. Improved approximation algorithms for MAX SAT. J.

Algorithms, 42(1):173–202, 2002.

S. H. Bach, M. Broecheler, L. Getoor, and D. P. O’Leary. Scaling MPE inference for con-
strained continuous Markov random ﬁelds. In Advances in Neural Information Processing
Systems (NIPS), 2012.

S. H. Bach, B. Huang, B. London, and L. Getoor. Hinge-loss Markov random ﬁelds: Convex
inference for structured prediction. In Uncertainty in Artiﬁcial Intelligence (UAI), 2013.

S. H. Bach, B. Huang, J. Boyd-Graber, and L. Getoor. Paired-dual learning for fast training
of latent variable hinge-loss MRFs. In International Conference on Machine Learning
(ICML), 2015a.

S. H. Bach, B. Huang, and L. Getoor. Unifying local consistency and MAX SAT relaxations
for scalable inference with rounding guarantees. In Artiﬁcial Intelligence and Statistics
(AISTATS), 2015b.

G. Bakir, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan,

editors. Predicting Structured Data. MIT Press, 2007.

I. Beltagy, K. Erk, and R. J. Mooney. Probabilistic soft logic for semantic textual similarity.

In Annual Meeting of the Association for Computational Linguistics (ACL), 2014.

J. Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society,

24(3):179–195, 1975.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and
Statistical Learning Via the Alternating Direction Method of Multipliers. Now Publishers,
2011.

58

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

M. Broecheler and L. Getoor. Computing marginal distributions over continuous Markov
networks for statistical relational learning. In Advances in Neural Information Processing
Systems (NIPS), 2010.

M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2010a.

M. Broecheler, P. Shakarian, and V. S. Subrahmanian. A scalable framework for modeling

competitive diﬀusion in social networks. In Social Computing (SocialCom), 2010b.

C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and
approximation algorithms for the metric labeling problem. SIAM J. Discrete Math., 18
(3):608–625, 2005.

P. Chen, F. Chen, and Z. Qian. Road traﬃc congestion monitoring in social media with
In IEEE International Conference on Data Mining

hinge-loss Markov random ﬁelds.
(ICDM), 2014.

A. Choi, T. Standley, and A. Darwiche. Approximating weighted Max-SAT problems by
compensating for relaxations. In International Conference on Principles and Practice of
Constraint Programming, 2009.

M. Collins. Discriminative training methods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Empirical Methods in Natural Language Processing
(EMNLP), 2002.

M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2004.

H. Daum´e III, J. Langford, and D. Marcu. Search-based structured prediction. Machine

Learning, 75(3):297–325, 2009.

J. Davies and F. Bacchus. Exploiting the power of MIP solvers in MAXSAT. In M. J¨arvisalo
and A. Van Gelder, editors, Theory and Applications of Satisﬁability Testing – SAT 2013,
Lecture Notes in Computer Science, pages 166–181. Springer Berlin Heidelberg, 2013.

L. De Raedt and L. Dehaspe. Clausal discovery. Machine Learning, 26:1058–1063, 1996.

L. De Raedt, A. Kimmig, and H. Toivonen. ProbLog: A probabilistic Prolog and its
application in link discovery. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

R. de Salvo Braz, E. Amir, and D. Roth. Lifted ﬁrst-order probabilistic inference.

In
L. Getoor and B. Taskar, editors, Introduction to statistical relational learning, pages
433–451. MIT Press, 2007.

L. Deng and J. Wiebe. Joint prediction for entity/event-level sentiment analysis using
probabilistic soft logic models. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2015.

59

Bach, Broecheler, Huang, and Getoor

J. Ebrahimi, D. Dou, and D. Lowd. Weakly supervised tweet stance classiﬁcation by rela-
tional bootstrapping. In Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), 2016.

S. Fakhraei, B. Huang, L. Raschid, and L. Getoor. Network-based drug-target interac-
tion prediction with probabilistic soft logic. IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 2014.

J. Feldman, M. J. Wainwright, and D. R. Karger. Using linear programming to decode

binary linear codes. Information Theory, IEEE Trans. on, 51(3):954–972, 2005.

J. Foulds, N. Navaroli, P. Smyth, and A. Ihler. Revisiting MAP estimation, message passing

and perfect graphs. In AI & Statistics, 2011.

J. Foulds, S. Kumar, and L. Getoor. Latent topic networks: A versatile probabilistic pro-
gramming framework for topic models. In International Conference on Machine Learning
(ICML), 2015.

N. Friedman, L. Getoor, D. Koller, and A. Pfeﬀer. Learning probabilistic relational models.

In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1999.

D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems
via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1):17–
40, 1976.

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simpliﬁed NP-complete graph

problems. Theoretical Computer Science, 1(3):237–267, 1976.

L. Getoor and B. Taskar, editors. Introduction to statistical relational learning. MIT press,

2007.

L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning probabilistic models of link

structure. Journal of Machine Learning Research (JMLR), 3:679–707, 2002.

A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms
for MAP LP-relaxations. In Advances in Neural Information Processing Systems (NIPS),
2007.

R. Glowinski and A. Marrocco. Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et la
r´esolution, par p´enalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires.
Revue fran¸caise d’automatique, informatique, recherche op´erationnelle, 9(2):41–76, 1975.

M. X. Goemans and D. P. Williamson. New 3/4-approximation algorithms for the maximum

satisﬁability problem. SIAM J. Discrete Math., 7(4):656–666, 1994.

J. Golbeck. Computing and Applying Trust in Web-based Social Networks. PhD thesis,

University of Maryland, 2005.

K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collabo-

rative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.

60

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church:
A language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.

A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. Probabilistic programming.

In International Conference on Software Engineering (ICSE, FOSE track), 2014.

G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science, 313(5786):504–507, 2006.

B. Huang, A. Kimmig, L. Getoor, and J. Golbeck. A ﬂexible framework for probabilistic
models of social trust. In Conference on Social Computing, Behavioral-Cultural Modeling,
& Prediction (SBP), 2013.

T. Huynh and R. Mooney. Max-margin weight learning for Markov logic networks.

In

European Conference on Machine Learning (ECML), 2009.

A. Jaimovich, O. Meshi, and N. Friedman. Template based inference in symmetric relational

Markov random ﬁelds. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.

T. Jebara. MAP estimation, message passing, and perfect graphs. In Uncertainty in Arti-

ﬁcial Intelligence (UAI), 2009.

Learning, 77(1):27–59, 2009.

T. Joachims, T. Finley, and C. Yu. Cutting-plane training of structural SVMs. Machine

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In

International Conference on Machine Learning (ICML), 2010.

S. Kamvar, M. Schlosser, and H. Garcia-Molina. The eigentrust algorithm for reputation
In International Conference on the World Wide Web

management in P2P networks.
(WWW), 2003.

W. Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints.

Master’s thesis, University of Chicago, 1939.

K. Kersting. Lifted probabilistic inference. In European Conference on Artiﬁcial Intelligence

(ECAI), 2012.

K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Uncertainty in

Artiﬁcial Intelligence (UAI), 2009.

A. Kimmig, G. Van den Broeck, and L. De Raedt. An algebraic Prolog for reasoning about

possible worlds. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2011.

A. Kimmig, L. Mihalkova, and L. Getoor. Lifted graphical models: A survey. Machine

Learning, 99:1–45, 2015.

Applied Logic, 2016.

A. Kimmig, G. Van den Broeck, and L. De Raedt. Algebraic model counting. Journal of

61

Bach, Broecheler, Huang, and Getoor

J. Kleinberg and ´E. Tardos. Approximation algorithms for classiﬁcation problems with
pairwise relationships: Metric labeling and Markov random ﬁelds. J. ACM, 49(5):616–
639, 2002.

G. J. Klir and B. Yuan. Fuzzy Sets and Fuzzy Logic: Theory and Applications. Prentice

Hall, 1995.

S. Kok and P. Domingos. Learning the structure of Markov logic networks. In International

Conference on Machine Learning (ICML), 2005.

S. Kok and P. Domingos. Learning Markov logic networks using structural motifs.

In

International Conference on Machine Learning (ICML), 2010.

D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.

MIT Press, 2009.

Intelligence (UAI), 1997.

D. Koller and A. Pfeﬀer. Object-oriented Bayesian networks. In Uncertainty in Artiﬁcial

V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. Pat-

tern Analysis and Machine Intelligence, IEEE Trans. on, 28(10):1568–1583, 2006.

N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via
dual decomposition. Pattern Analysis and Machine Intelligence, IEEE Trans. on, 33(3):
531–552, 2011.

P. Kouki, S. Fakhraei, J. Foulds, M. Eirinaki, and L. Getoor. HyPER: A ﬂexible and
extensible probabilistic framework for hybrid recommender systems. In ACM Conference
on Recommender Systems (RecSys), 2015.

H. W. Kuhn and A. W. Tucker. Nonlinear programming.

In Berkeley Symp. on Math.

Statist. and Prob., 1951.

M. P. Kumar, P. H. S. Torr, and A. Zisserman. Solving Markov random ﬁelds using sec-
ond order cone programming relaxations. In Computer Vision and Pattern Recognition
(CVPR), 2006.

N. Landwehr, A. Passerini, L. De Raedt, and P. Frasconi. Fast learning of relational kernels.

Machine Learning, 78(3):305–342, 2010.

J. Larrosa, F. Heras, and S. de Givry. A logical approach to eﬃcient Max-SAT solving.

Artiﬁcial Intelligence, 172(2-3):204–233, 2008.

J. Li, A. Ritter, and D. Jurafsky. Inferring user preferences by probabilistic logical reasoning

over social networks. arXiv preprint arXiv:1411.2679, 2014.

S. Liu, K. Liu, S. He, and J. Zhao. A probabilistic soft logic based approach to exploiting
latent and global information in event classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2016.

62

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

B. London, S. Khamis, S. H. Bach, B. Huang, L. Getoor, and L. Davis. Collective activity
In CVPR Workshop on Structured

detection using hinge-loss Markov random ﬁelds.
Prediction: Tractability, Learning and Inference, 2013.

B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

D. Lowd and P. Domingos. Eﬃcient weight learning for Markov logic networks. In Principles

and Practice of Knowledge Discovery in Databases (PKDD), 2007.

S. Magliacane, P. Stutz, P. Groth, and A. Bernstein. FoxPSL: An extended and scalable
In AAAI Spring Symposium on Knowledge Representation and

PSL implementation.
Reasoning: Integrating Symbolic and Neural Approaches, 2015.

A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing.
AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Mod-
els. Journal of Machine Learning Research (JMLR), 16(Mar):495–545, 2015.

A. McCallum, K. Nigam, and L. H. Ungar. Eﬃcient clustering of high-dimensional data
sets with application to reference matching. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2000.

A. McCallum, K. Schultz, and S. Singh. FACTORIE: Probabilistic programming via im-
peratively deﬁned factor graphs. In Advances in Neural Information Processing Systems
(NIPS), 2009.

O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation.

In European Conference on Machine learning (ECML), 2011.

O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning eﬃciently with approximate
In International Conference on Machine Learning (ICML),

inference via dual losses.
2010.

E. Mezuman, D. Tarlow, A. Globerson, and Y. Weiss. Tighter linear program relaxations
for high order graphical models. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2013.

H. Miao, X. Liu, B. Huang, and L. Getoor. A hypergraph-partitioned vertex programming
approach for large-scale consensus optimization. In IEEE International Conference on
Big Data, 2013.

L. Mihalkova and R. J. Mooney. Bottom-up learning of Markov logic network structure. In

International Conference on Machine Learning (ICML), 2007.

B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic
models with unknown objects. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2005.

P. Mills and E. Tsang. Guided local search for solving SAT and weighted MAX-SAT

problems. J. Automated Reasoning, 24(1-2):205–223, 2000.

63

Bach, Broecheler, Huang, and Getoor

M. Mladenov, B. Ahmadi, and K. Kersting. Lifted linear programming. In Artiﬁcial Intel-

ligence & Statistics (AISTATS), 2012.

S. Muggleton and L. De Raedt. Inductive logic programming: Theory and methods. The

Journal of Logic Programming, 19:629–679, 1994.

Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Program-

ming. Society for Industrial and Applied Mathematics, 1994.

J. Neville and D. Jensen. Relational dependency networks. Journal of Machine Learning

Research (JMLR), 8:653–692, 2007.

H. B. Newcombe and J. M. Kennedy. Record linkage: Making maximum use of the discrim-
inating power of identifying information. Communications of the ACM, 5(11):563–566,
1962.

S. Nowozin, P. V. Gehler, J. Jancsary, and C. H. Lampert, editors. Advanced Structured

Prediction. Neural Information Processing. MIT press, 2016.

J. D. Park. Using weighted MAX-SAT engines to solve MPE.

In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2002.

S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by
gradient descent in function space. Journal of Machine Learning Research (JMLR), 3:
1333–1356, 2003.

A. Pfeﬀer. IBAL: A probabilistic rational programming language. In International Joint

Conference on Artiﬁcial Intelligence (IJCAI), 2001.

A. Pfeﬀer. Figaro: An object-oriented probabilistic programming language. Technical

report, Charles River Analytics, 2009.

A. Pfeﬀer, D. Koller, B. Milch, and K. T. Takusagawa. SPOOK: A system for probabilistic
object-oriented knowledge representation. In Uncertainty in Artiﬁcial Intelligence (UAI),
1999.

H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2011.

J. Pujara, H. Miao, L. Getoor, and W. Cohen. Knowledge graph identiﬁcation. In Inter-

national Semantic Web Conference (ISWC), 2013.

A. Ramesh, D. Goldwasser, B. Huang, H. Daum´e III, and L. Getoor. Learning latent
In AAAI Conference on Artiﬁcial

engagement patterns of students in online courses.
Intelligence (AAAI), 2014.

A. Ramesh, S. Kumar, J. Foulds, and L. Getoor. Weakly supervised models of aspect-
sentiment for online course discussion forums. In Annual Meeting of the Association for
Computational Linguistics (ACL), 2015.

64

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

P. Ravikumar and J. Laﬀerty. Quadratic programming relaxations for metric labeling and
Markov random ﬁeld MAP estimation. In International Conference on Machine Learning
(ICML), 2006.

P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured
linear programs: Proximal methods and rounding schemes. Journal of Machine Learning
Research (JMLR), 11:1043–1080, 2010a.

P. Ravikumar, M. J. Wainwright, and J. D. Laﬀerty. High-dimensional Ising model selection
using (cid:96)1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010b.

B. L. Richards and R. J. Mooney. Learning relations by pathﬁnding. In AAAI Conference

on Artiﬁcial Intelligence (AAAI), 1992.

M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2):107–

136, 2006.

M. Richardson, R. Agrawal, and P. Domingos. Trust management for the semantic web.
In D. Fensel, K. Sycara, and J. Mylopoulos, editors, The Semantic Web - ISWC 2003,
volume 2870 of Lecture Notes in Computer Science, pages 351–368. Springer Berlin /
Heidelberg, 2003.

F. Riguzzi and T. Swift. The PITA system: Tabling and answer subsumption for reasoning
under uncertainty. In International Conference on Logic Programming (ICLP), 2011.

S. Ross and J. A. Bagnell. Reinforcement and Imitation Learning via Interactive No-Regret

Learning, 2014.

S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Artiﬁcial Intelligence & Statistics (AISTATS),
2011.

R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Artiﬁcial Intelligence &

Statistics (AISTATS), 2009.

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In International Conference on Machine Learning (ICML), 2008.

M. Samadi, P. Talukdar, M. Veloso, and M. Blum. ClaimEval: Integrated and ﬂexible
In AAAI Conference on

framework for claim evaluation using credibility of sources.
Artiﬁcial Intelligence (AAAI), 2016.

A. Schrijver. Combinatorial Optimization: Polyhedra and Eﬃciency. Springer-Verlag, 2003.

A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally convergent dual MAP
LP relaxation solvers using Fenchel-Young margins. In Advances in Neural Information
Processing Systems (NIPS), 2012.

P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective

classiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.

65

Bach, Broecheler, Huang, and Getoor

S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artiﬁcial Intelligence, 68(2):

399–410, 1994.

P. Singla and P. Domingos. Discriminative training of Markov logic networks. In AAAI

Conference on Artiﬁcial Intelligence (AAAI), 2005.

P. Singla and P. Domingos. Lifted ﬁrst-order belief propagation. In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2008.

D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations
for MAP using message passing. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2008.

D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference.
In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, pages
219–254. MIT Press, 2011.

D. Sontag, D. K. Choe, and Y. Li. Eﬃciently searching for frustrated cycles in MAP

inference. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2012.

D. Sridhar, J. Foulds, M. Walker, B. Huang, and L. Getoor. Joint models of disagreement
and stance in online debate. In Annual Meeting of the Association for Computational
Linguistics (ACL), 2015.

D. Sridhar, S. Fakhraei, and L. Getoor. A probabilistic approach for collective similarity-

based drug-drug interaction prediction. Bioinformatics, 32(20):3175–3182, 2016.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Neural Information

Processing Systems (NIPS), 2004.

B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction
In International Conference on Machine Learning

models: A large margin approach.
(ICML), 2005.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Ed-
inference, and criticism. arXiv preprint

ward: A library for probabilistic modeling,
arXiv:1610.09787, 2016.

I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research
(JMLR), 6:1453–1484, 2005.

V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2000.

D. Venugopal and V. Gogate. On lifting the Gibbs sampling algorithm. In Neural Infor-

mation Processing Systems (NIPS), 2012.

B. W. Wah and Y. Shang. Discrete Lagrangian-based search for solving MAX-SAT prob-

lems. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1997.

M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Varia-

tional Inference. Now Publishers, 2008.

66

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

J. Wang and P. Domingos. Hybrid Markov logic networks. In AAAI Conference on Artiﬁcial

Intelligence (AAAI), 2008.

T. Werner. A linear programming approach to max-sum problem: A review. Pattern

Analysis and Machine Intelligence, IEEE Trans. on, 29(7):1165–1179, 2007.

R. West, H. S. Paskov, J. Leskovec, and C. Potts. Exploiting social network structure for
person-to-person sentiment analysis. Transactions of the Association for Computational
Linguistics (TACL), 2:297–310, 2014.

F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic

programming inference. In Artiﬁcial Intelligence & Statistics (AISTATS), 2014.

M. Wright. The interior-point revolution in optimization: History, recent developments,
and lasting consequences. Bulletin of the American Mathematical Society, 42(1):39–56,
2005.

L. Xiong, X. Chen, T. Huang, J. Schneider, and J. Carbonell. Temporal collaborative ﬁlter-
ing with Bayesian probabilistic tensor factorization. In SIAM International Conference
on Data Mining, 2010.

C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propaga-
tion – An empirical study. Journal of Machine Learning Research (JMLR), 7:1887–1907,
2006.

J. Zhu, N. Lao, and E. P. Xing. Grafting-Light: Fast, Incremental Feature Selection and
Structure Learning of Markov Random Fields. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2010.

67

7
1
0
2
 
v
o
N
 
7
1
 
 
]

G
L
.
s
c
[
 
 
3
v
6
0
4
4
0
.
5
0
5
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2017) 1-67

Submitted 12/15; Revised 12/16; Published 10/17

Hinge-Loss Markov Random Fields
and Probabilistic Soft Logic

Stephen H. Bach
Computer Science Department
Stanford University
Stanford, CA 94305, USA

Matthias Broecheler
DataStax

Bert Huang
Computer Science Department
Virginia Tech
Blacksburg, VA 24061, USA

Lise Getoor
Computer Science Department
University of California, Santa Cruz
Santa Cruz, CA 95064, USA

Editor: Luc De Raedt

bach@cs.stanford.edu

matthias@datastax.com

bhuang@vt.edu

getoor@soe.ucsc.edu

Abstract

A fundamental challenge in developing high-impact machine learning technologies is bal-
ancing the need to model rich, structured domains with the ability to scale to big data.
Many important problem areas are both richly structured and large scale, from social and
biological networks, to knowledge graphs and the Web, to images, video, and natural lan-
guage. In this paper, we introduce two new formalisms for modeling structured data, and
show that they can both capture rich structure and scale to big data. The ﬁrst, hinge-
loss Markov random ﬁelds (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes diﬀerent approaches to convex inference. We unite three approaches from
the randomized algorithms, probabilistic graphical models, and fuzzy logic communities,
showing that all three lead to the same inference objective. We then deﬁne HL-MRFs
by generalizing this uniﬁed objective. The second new formalism, probabilistic soft logic
(PSL), is a probabilistic programming language that makes HL-MRFs easy to deﬁne using
a syntax based on ﬁrst-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than general-purpose
convex optimization methods, because it uses message passing to take advantage of sparse
dependency structures. We then show how to learn the parameters of HL-MRFs. The
learned HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at
scales not previously possible.

Keywords:
prediction

Probabilistic graphical models, statistical relational learning, structured

c(cid:13)2017 Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/15-631.html.

Bach, Broecheler, Huang, and Getoor

1. Introduction

In many problems in machine learning, the domains are rich and structured, with many
interdependent elements that are best modeled jointly. Examples include social networks,
biological networks, the Web, natural language, computer vision, sensor networks, and so on.
Machine learning subﬁelds such as statistical relational learning (Getoor and Taskar, 2007),
inductive logic programming (Muggleton and De Raedt, 1994), and structured prediction
(Bakir et al., 2007) all seek to represent dependencies in data induced by relational structure.
With the ever-increasing size of available data, there is a growing need for models that are
highly scalable while still able to capture rich structure.

In this paper, we introduce hinge-loss Markov random ﬁelds (HL-MRFs), a new class of
probabilistic graphical models designed to enable scalable modeling of rich, structured data.
HL-MRFs are analogous to discrete MRFs, which are undirected probabilistic graphical
models in which probability mass is log-proportional to a weighted sum of feature functions.
Unlike discrete MRFs, however, HL-MRFs are deﬁned over continuous variables in the [0, 1]
unit interval. To model dependencies among these continuous variables, we use linear and
quadratic hinge functions, so that probability density is lost according to a weighted sum of
hinge losses. As we will show, hinge-loss features capture many common modeling patterns
for structured data.

When designing classes of models, there is generally a trade oﬀ between scalability and
expressivity: the more complex the types and connectivity structure of the dependencies,
the more computationally challenging inference and learning become. HL-MRFs address
a crucial gap between the two extremes. By using hinge-loss functions to model the de-
pendencies among the variables, which admit highly scalable inference without restrictions
on their connectivity structure, HL-MRFs can capture a wide range of useful relationships.
One reason they are so expressive is that hinge-loss dependencies are at the core of a number
of scalable techniques for modeling both discrete and continuous structured data.

To motivate HL-MRFs, we unify three diﬀerent approaches for scalable inference in
structured models: (1) randomized algorithms for MAX SAT (Goemans and Williamson,
1994), (2) local consistency relaxation (Wainwright and Jordan, 2008) for discrete Markov
random ﬁelds deﬁned using Boolean logic, and (3) reasoning about continuous information
with fuzzy logic. We show that all three approaches lead to the same convex programming
objective. We then deﬁne HL-MRFs by generalizing this uniﬁed inference objective as a
weighted sum of hinge-loss features and using them as the weighted features of graphical
models. Since HL-MRFs generalize approaches that reason about relational data with
weighted logical knowledge bases, they retain the same high level of expressivity. As we
show in Section 6.4, they are eﬀective for modeling both discrete and continuous data.

We also introduce probabilistic soft logic (PSL), a new probabilistic programming lan-
guage that makes HL-MRFs easy to deﬁne and use for large, relational data sets.1 This
idea has been explored for other classes of models, such as Markov logic networks (Richard-
son and Domingos, 2006) for discrete MRFs, relational dependency networks (Neville and
Jensen, 2007) for dependency networks, and probabilistic relational models (Getoor et al.,
2002) for Bayesian networks. We build on these previous approaches, as well as the con-
nection between hinge-loss potentials and logical clauses, to deﬁne PSL. In addition to

1. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

2

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

probabilistic rules, PSL provides syntax that enables users to easily apply many common
modeling techniques, such as domain and range constraints, blocking and canopy functions,
and aggregate variables deﬁned over other random variables.

Our next contribution is to introduce a number of inference and learning algorithms.
First, we examine MAP inference, i.e., the problem of ﬁnding a most probable assignment
to the unobserved random variables. MAP inference in HL-MRFs is always a convex op-
timization. Although any oﬀ-the-shelf optimization toolkit could be used, such methods
typically do not leverage the sparse dependency structures common in graphical models.
We introduce a consensus-optimization approach to MAP inference for HL-MRFs, showing
how the problem can be decomposed using the alternating direction method of multipliers
(ADMM) and how the resulting subproblems can be solved analytically for hinge-loss poten-
tials. Our approach enables HL-MRFs to easily scale beyond the capabilities of oﬀ-the-shelf
optimization software or sampling-based inference in discrete MRFs. We then show how
to learn HL-MRFs from training data using a variety of methods: structured perceptron,
maximum pseudolikelihood, and large margin estimation. Since structured perceptron and
large margin estimation rely on inference as subroutines, and maximum pseudolikelihood
estimation is eﬃcient by design, all of these methods are highly scalable for HL-MRFs. We
evaluate them on core relational learning and structured prediction tasks, such as collec-
tive classiﬁcation and link prediction. We show that HL-MRFs oﬀer predictive accuracy
comparable to analogous discrete models while scaling much better to large data sets.

This paper brings together and expands work on scalable models for structured data that
can be either discrete, continuous, or a mixture of both (Broecheler et al., 2010a; Bach et al.,
2012, 2013, 2015b). The eﬀectiveness of HL-MRFs and PSL has been demonstrated on many
problems, including information extraction (Liu et al., 2016) and automatic knowledge base
construction (Pujara et al., 2013), extracting and evaluating natural-language arguments
on the Web (Samadi et al., 2016), high-level computer vision (London et al., 2013), drug
discovery (Fakhraei et al., 2014) and predicting drug-drug interactions (Sridhar et al., 2016),
natural language semantics (Beltagy et al., 2014; Sridhar et al., 2015; Deng and Wiebe,
2015; Ebrahimi et al., 2016), automobile-traﬃc modeling (Chen et al., 2014), recommender
systems (Kouki et al., 2015), information retrieval (Alshukaili et al., 2016), and predicting
attributes (Li et al., 2014) and trust (Huang et al., 2013; West et al., 2014) in social networks.
The ability to easily incorporate latent variables into HL-MRFs and PSL (Bach et al., 2015a)
has enabled further applications, including modeling latent topics in text (Foulds et al.,
2015), and predicting student outcomes in massive open online courses (MOOCs) (Ramesh
et al., 2014, 2015). Researchers have also studied how to make HL-MRFs and PSL even
more scalable by developing distributed implementations (Miao et al., 2013; Magliacane
et al., 2015). That they are already being widely applied indicates HL-MRFs and PSL
address an open need in the machine learning community.

The paper is organized as follows. In Section 2, we ﬁrst consider models for structured
prediction that are deﬁned using logical clauses. We unify three diﬀerent approaches to
scalable inference in such models, showing that they all optimize the same convex objec-
tive. We then generalize this objective in Section 3 to deﬁne HL-MRFs. In Section 4, we
introduce PSL, specifying the language and giving many examples of common usage. Next
we introduce a scalable message-passing algorithm for MAP inference in Section 5 and a

3

Bach, Broecheler, Huang, and Getoor

number of learning algorithms in Section 6, evaluating them on a range of tasks. Finally,
in Section 7, we discuss related work.

2. Unifying Convex Inference for Logic-Based Graphical Models

In many structured domains, propositional and ﬁrst-order logics are useful tools for describ-
ing the intricate dependencies that connect the unknown variables. However, these domains
are usually noisy; dependencies among the variables do not always hold. To address this,
logical semantics can be incorporated into probability distributions to create models that
capture both the structure and the uncertainty in machine learning tasks. One common
way to do this is to use logic to deﬁne feature functions in a probabilistic model. We focus
on Markov random ﬁelds (MRFs), a popular class of probabilistic graphical models. Infor-
mally, an MRF is a distribution that assigns probability mass using a scoring function that
is a weighted combination of feature functions called potentials. We will use logical clauses
to deﬁne these potentials. We ﬁrst deﬁne MRFs more formally to introduce necessary
notation:

Deﬁnition 1 Let x = (x1, . . . , xn) be a vector of random variables and let φ = (φ1, . . . , φm)
be a vector of potentials where each potential φj(x) assigns conﬁgurations of the variables
a real-valued score. Also, let w = (w1, . . . , wm) be a vector of real-valued weights. Then, a
Markov random ﬁeld is a probability distribution of the form

P (x) ∝ exp

w(cid:62)φ(x)

.

(cid:16)

(cid:17)

In an MRF, the potentials should capture how the domain behaves, assigning higher scores
If a modeler does not know how the
to more probable conﬁgurations of the variables.
domain behaves, the potentials should capture how it might behave, so that a learning
algorithm can ﬁnd weights that lead to accurate predictions. Logic provides an excellent
formalism for deﬁning such potentials in structured and relational domains.

We now introduce some notation to make this logic-based approach more formal. Con-
sider a set of logical clauses C = {C1, . . . , Cm}, i.e., a knowledge base, where each clause
Cj ∈ C is a disjunction of literals and each literal is a variable x or its negation ¬x drawn
from the variables x such that each variable xi ∈ x appears at most once in Cj. Let
j (resp. I −
I +
j ) ⊂ {1, . . . , n} be the set of indices of the variables that are not negated (resp.
negated) in Cj. Then Cj can be written as

Logical clauses of this form are expressive because they can be viewed equivalently as

implications from conditions to consequences:

This “if-then” reasoning is intuitive and can describe many dependencies in structured data.



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(cid:94)

i∈I −
j

xi =⇒

xi .

(cid:95)

i∈I +
j

4

(1)

(2)

(3)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Assuming we have a logical knowledge base C describing a structured domain, we can
embed it in an MRF by deﬁning each potential φj using a corresponding clause Cj. If an
assignment to the variables x satisﬁes Cj, then we let φj(x) equal 1, and we let it equal 0
otherwise. For our subsequent analysis we assume wj ≥ 0 (∀j = 1, . . . , m). The resulting
MRF preserves the structured dependencies described in C but enables much more ﬂexible
modeling. Clauses no longer must always hold, and the model can express uncertainty
over diﬀerent possible worlds. The weights express how strongly the model expects each
corresponding clause to hold; the higher the weight, the more probable that it is true
according to the model.

This notion of embedding weighted, logical knowledge bases in MRFs is an appealing
one. For example, Markov logic (Richardson and Domingos, 2006) is a popular formalism
that induces MRFs from weighted ﬁrst-order knowledge bases. Given a data set, the ﬁrst-
order clauses are grounded using the constants in the data to create the set of propositional
clauses C. Each propositional clause has the weight of the ﬁrst-order clause from which it
was grounded. In this way, a weighted, ﬁrst-order knowledge base can compactly specify
an entire family of MRFs for a structured machine-learning task.

Although we now have a method for easily deﬁning rich, structured models for a wide
range of problems, there is a new challenge: ﬁnding a most probable assignment to the
variables, i.e., MAP inference, is NP-hard (Shimony, 1994; Garey et al., 1976). This means
that (unless P=NP) our only hope for performing tractable inference is to perform it ap-
proximately. Observe that MAP inference for an MRF deﬁned by C is the integer linear
program

arg max
x∈{0,1}n

P (x) ≡ arg max
x∈{0,1}n

w(cid:62)φ(x)

≡ arg max
x∈{0,1}n

(cid:88)

Cj ∈C

wj min

xi +

(1 − xi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






(4)

While this program is intractable, it does admit convex programming relaxations.

In this section, we show how convex programming can be used to perform tractable
inference in MRFs deﬁned by weighted knowledge bases. We ﬁrst discuss in Section 2.1 an
approach developed by Goemans and Williamson (1994) that views MAP inference as an
instance of the classic MAX SAT problem and relaxes it to a convex program from that
perspective. This approach has the advantage of providing strong guarantees on the quality
of the discrete solutions it obtains. However, it has the disadvantage that general-purpose
convex programming toolkits do not scale well to relaxed MAP inference for large graphical
models (Yanover et al., 2006). In Section 2.2 we then discuss a seemingly distinct approach,
local consistency relaxation, with complementary advantages and disadvantages:
it oﬀers
highly scalable message-passing algorithms but comes with no quality guarantees. We then
unite these approaches by proving that they solve equivalent optimization problems with
identical solutions. Then, in Section 2.3, we show that the uniﬁed inference objective is also
equivalent to exact MAP inference if the knowledge base C is interpreted using (cid:32)Lukasiewicz
logic, an inﬁnite-valued logic for reasoning about naturally continuous quantities such as
similarity, vague or fuzzy concepts, and real-valued data.

5

Bach, Broecheler, Huang, and Getoor

That these three interpretations all lead to the same inference objective—whether rea-
soning about discrete or continuous information—is useful. To the best of our knowledge,
we are the ﬁrst to show their equivalence. This equivalence indicates that the same model-
ing formalism, inference algorithms, and learning algorithms can be used to reason scalably
and accurately about both discrete and continuous information in structured domains. We
generalize the uniﬁed inference objective in Section 3.1 to deﬁne hinge-loss MRFs, and in
the rest of the paper we develop a probabilistic programming language and algorithms that
realize the goal of a scalable and accurate framework for structured data, both discrete and
continuous.

2.1 MAX SAT Relaxation

One approach to approximating objective (4) is to use relaxation techniques developed in
the randomized algorithms community for the MAX SAT problem. Formally, the MAX
SAT problem is to ﬁnd a Boolean assignment to a set of variables that maximizes the total
weight of satisﬁed clauses in a knowledge base composed of disjunctive clauses annotated
with nonnegative weights.
In other words, objective (4) is an instance of MAX SAT.
Randomized approximation algorithms can be constructed for MAX SAT by independently
rounding each Boolean variable xi to true with probability pi. Then, the expected weighted
satisfaction ˆwj of a clause Cj is

also known as a (weighted) noisy-or function, and the expected total score ˆW is



ˆwj = wj


1 −

(1 − pi)

(cid:89)

i∈I +
j



pi


 ,

(cid:89)

i∈I −
j



ˆW =

(cid:88)

Cj ∈C

wj


1 −

(cid:89)

i∈I +
j

(1 − pi)



pi


 .

(cid:89)

i∈I −
j

(5)

(6)

Optimizing ˆW with respect to the rounding probabilities would give the exact MAX SAT so-
lution, so this randomized approach has not made the problem any easier yet, but Goemans
and Williamson (1994) showed how to bound ˆW below with a tractable linear program.

To approximately optimize ˆW , associate with each Boolean variable xi a corresponding
continuous variable ˆyi with domain [0, 1]. Then let ˆy(cid:63) be the optimum of the linear program

arg max
ˆy∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

ˆyi +

(1 − ˆyi), 1

.

(7)

Observe that objectives (4) and (7) are of the same form, except that the variables are
relaxed to the unit hypercube in objective (7). Goemans and Williamson (1994) proved
i for all i, then ˆW ≥ .632 Z(cid:63), where Z(cid:63) is the optimal total weight for
that if pi is set to ˆy(cid:63)
the MAX SAT problem. If each pi is set using any function in a special class, then this

6

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

lower bound improves to a .75 approximation. One simple example of such a function is
1
2

ˆy(cid:63)
i +

pi =

1
4

.

(8)

In this way, objective (7) leads to an expected .75 approximation of the MAX SAT solution.
The following method of conditional probabilities (Alon and Spencer, 2008) can ﬁnd a
single Boolean assignment that achieves at least the expected score from a set of rounding
probabilities, and therefore at least .75 of the MAX SAT solution when objective (7) and
function (8) are used to obtain them. Each variable xi is greedily set to the value that
maximizes the expected weight over the unassigned variables, conditioned on either possible
value of xi and the previously assigned variables. This greedy maximization can be applied
quickly because, in many models, variables only participate in a small fraction of the clauses,
making the change in expectation quick to compute for each variable. Speciﬁcally, referring
to the deﬁnition of ˆW (6), the assignment to xi only needs to maximize over the clauses Cj
in which xi participates, i.e., i ∈ I +

j , which is usually a small set.

j ∪ I −

This approximation is powerful because it is a tractable linear program that comes
with strong guarantees on solution quality. However, even though it is tractable, general-
purpose convex optimization toolkits do not scale well to large MAP problems.
In the
following subsection, we unify this approximation with a complementary one developed in
the probabilistic graphical models community.

2.2 Local Consistency Relaxation

Another approach to approximating objective (4) is to apply a relaxation developed for
Markov random ﬁelds called local consistency relaxation (Wainwright and Jordan, 2008).
This approach starts by viewing MAP inference as an equivalent optimization over marginal
probabilities.2 For each φj ∈ φ, let θj be a marginal distribution over joint assignments xj.
For example, θj(xj) is the probability that the subset of variables associated with potential
φj is in a particular joint state xj. Also, let xj(i) denote the setting of the variable with
index i in the state xj.

With this variational formulation, inference can be relaxed to an optimization over the
ﬁrst-order local polytope L. Let µ = (µ1, . . . , µn) be a vector of probability distributions,
where µi(k) is the marginal probability that xi is in state k. The ﬁrst-order local polytope
is




L (cid:44)

(θ, µ) ≥ 0

(cid:80)

(cid:80)

xj |xj (i)=k θj(xj) = µi(k) ∀i, j, k
∀j

θj(xj) = 1




,



xj
(cid:80)Ki−1
k=0 µi(k) = 1
which constrains each marginal distribution θj over joint states xj to be consistent only
with the marginal distributions µ over individual variables that participate in the potential
φj.



∀i

MAP inference can then be approximated with the ﬁrst-order local consistency relax-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ation:

(9)

(10)

arg max
(θ,µ)∈L

m
(cid:88)

j=1

(cid:88)

xj

wj

θj(xj) φj(xj),

2. This treatment is for discrete MRFs. We have omitted a discussion of continuous MRFs for conciseness.

7

Bach, Broecheler, Huang, and Getoor

which is an upper bound on the true MAP objective. Much work has focused on solving
the ﬁrst-order local consistency relaxation for large-scale MRFs, which we discuss further
in Section 7. These algorithms are appealing because they are well-suited to the sparse
dependency structures common in MRFs, so they can scale to large problems. However, in
general, the solutions can be fractional, and there are no guarantees on the approximation
quality of a tractable discretization of these fractional solutions.

We show that for MRFs with potentials deﬁned by C and nonnegative weights, local

consistency relaxation is equivalent to MAX SAT relaxation.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

We prove Theorem 2 in Appendix A. Our proof analyzes the local consistency relaxation to
derive an equivalent, more compact optimization over only the variable pseudomarginals µ
that is identical to the MAX SAT relaxation. Theorem 2 is signiﬁcant because it shows that
the rounding guarantees of MAX SAT relaxation also apply to local consistency relaxation,
and the scalable message-passing algorithms developed for local consistency relaxation also
apply to MAX SAT relaxation.

2.3 (cid:32)Lukasiewicz Logic

The previous two subsections showed that the same convex program can approximate MAP
inference in discrete, logic-based models, whether viewed from the perspective of randomized
algorithms or variational methods. In this subsection, we show that this convex program
can also be used to reason about naturally continuous information, such as similarity, vague
or fuzzy concepts, and real-valued data. Instead of interpreting the clauses C using Boolean
logic, we can interpret them using (cid:32)Lukasiewicz logic (Klir and Yuan, 1995), which extends
Boolean logic to inﬁnite-valued logic in which the propositions x can take truth values in the
continuous interval [0, 1]. Extending truth values to a continuous domain enables them to
represent concepts that are vague, in the sense that they are often neither completely true
nor completely false. For example, the propositions that a sensor value is high, two entities
are similar, or a protein is highly expressed can all be captured in a more nuanced manner
in (cid:32)Lukasiewicz logic. We can also use the now continuous valued x to represent quantities
that are naturally continuous (scaled to [0,1]), such as actual sensor values, similarity scores,
and protein expression levels. The ability to reason about continuous values is valuable, as
many important applications are not entirely discrete.

The extension to continuous values requires a corresponding extended interpretation of
the logical operators ∧ (conjunction), ∨ (disjunction), and ¬ (negation). The (cid:32)Lukasiewicz
t-norm and t-co-norm are ∧ and ∨ operators that correspond to the Boolean logic operators
for integer inputs (along with the negation operator ¬):

x1 ∧ x2 = max {x1 + x2 − 1, 0}
x1 ∨ x2 = min {x1 + x2, 1}

¬x = 1 − x .

8

(11)

(12)

(13)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The analogous MAX SAT problem for (cid:32)Lukasiewicz logic is therefore

arg max
x∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

xi +

(1 − xi), 1

,

(14)

which is identical in form to the relaxed MAX SAT objective (7). Therefore, if an MRF
is deﬁned over continuous variables with domain [0, 1]n and the logical knowledge base C
deﬁning the potentials is interpreted using (cid:32)Lukasiewicz logic, then exact MAP inference
is identical to ﬁnding the optimum using the uniﬁed, relaxed inference objective derived
for Boolean logic in the previous two subsections. This result shows the equivalence of all
three approaches: MAX SAT relaxation, local consistency relaxation, and MAX SAT using
(cid:32)Lukasiewicz logic.

3. Hinge-Loss Markov Random Fields

We have shown that a speciﬁc family of convex programs can be used to reason scalably and
accurately about both discrete and continuous information. In this section, we generalize
this family to deﬁne hinge-loss Markov random ﬁelds (HL-MRFs), a new kind of probabilis-
tic graphical model. HL-MRFs retain the convexity and expressivity of convex programs
discussed in Section 2, and additionally support an even richer space of dependencies.

To begin, we deﬁne HL-MRFs as density functions over continuous variables y =
(y1, . . . , yn) with joint domain [0, 1]n. These variables have diﬀerent possible interpreta-
tions depending on the application. Since we are generalizing the interpretations explored
in Section 2, HL-MRF MAP states can be viewed as rounding probabilities or pseudo-
marginals, or they can represent naturally continuous information. More generally, they
can be viewed simply as degrees of belief, conﬁdences, or rankings of possible states; and
they can describe discrete, continuous, or mixed domains. The application domain typi-
cally determines which interpretation is most appropriate. The formalisms and algorithms
described in the rest of this paper are general with respect to such interpretations.

3.1 Generalized Inference Objective

To deﬁne HL-MRFs, we will ﬁrst generalize the uniﬁed inference objective of Section 2 in
several ways, which we ﬁrst restate in terms of the HL-MRF variables y:

arg max
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

yi +

(1 − yi), 1

.

(15)

For now, we are still assuming that the objective terms are deﬁned using a weighted knowl-
edge base C, but we will quickly drop this requirement. To do so, we examine one term in
isolation. Observe that the maximum value of any unweighted term is 1, which is achieved
when a linear function of the variables is at least 1. We say that the term is satisﬁed when-
ever this occurs. When a term is unsatisﬁed, we can refer to its distance to satisfaction,
which is how far it is from achieving its maximum value. Also observe that we can rewrite

9

Bach, Broecheler, Huang, and Getoor

the optimization explicitly in terms of distances to satisfaction:

arg min
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj max

1 −

yi −

(1 − yi), 0

,

(16)

so that the objective is equivalently to minimize the total weighted distance to satisfaction.
Each unweighted objective term now measures how far the linear constraint

1 −

yi −

(1 − yi) ≤ 0

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

(17)

is from being satisﬁed.

3.1.1 Relaxed Linear Constraints

With this view of each term as a relaxed linear constraint, we can easily generalize them to
arbitrary linear constraints. We no longer require that the inference objective be deﬁned
using only logical clauses, and instead each term can be deﬁned using any function (cid:96)j(y)
that is linear in y. These functions can capture more general dependencies, such as beliefs
about the range of values a variable can take and arithmetic relationships among variables.

The new inference objective is

arg min
y∈[0,1]n

m
(cid:88)

j=1

wj max {(cid:96)j(y), 0} .

(18)

In this form, each term represents the distance to satisfaction of a linear constraint (cid:96)j(y) ≤ 0.
That constraint could be deﬁned using logical clauses as discussed above, or it could be
deﬁned using other knowledge about the domain. The weight wj indicates how important
it is to satisfy a constraint relative to others by scaling the distance to satisfaction. The
higher the weight, the more distance to satisfaction is penalized. Additionally, two relaxed
inequality constraints, (cid:96)j(y) ≤ 0 and −(cid:96)j(y) ≤ 0, can be combined to represent a relaxed
equality constraint (cid:96)j(y) = 0.

3.1.2 Hard Linear Constraints

Now that our inference objective admits arbitrary relaxed linear constraints, it is natural
to also allow hard constraints that must be satisﬁed at all times. Hard constraints are
important modeling tools. They enable groups of variables to represent mutually exclusive
possibilities, such as a multinomial or categorical variable, and functional or partial func-
tional relationships. Hard constraints can also represent background knowledge about the
domain, restricting the domain to regions that are feasible in the real world. Additionally,
they can encode more complex model components such as deﬁning a random variable as an
aggregate over other unobserved variables, which we discuss further in Section 4.3.5.

We can think of including hard constraints as allowing a weight wj to take an inﬁnite
value. Again, two inequality constraints can be combined to represent an equality con-
straint. However, when we introduce an inference algorithm for HL-MRFs in Section 5, it

10

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

will be useful to treat hard constraints separately from relaxed ones, and further, treat hard
inequality constraints separately from hard equality constraints. Therefore, in the deﬁnition
of HL-MRFs, we will deﬁne these three components separately.

3.1.3 Generalized Hinge-Loss Functions

The objective terms measuring each constraint’s distance to satisfaction are hinge losses.
There is a ﬂat region, on which the distance to satisfaction is 0, and an angled region, on
which the distance to satisfaction grows linearly away from the hyperplane (cid:96)j(y) = 0. This
loss function is useful—as we discuss in the previous section, it is a bound on the expected
loss in the discrete setting, among other things—but it is not appropriate for all modeling
situations.

A piecewise-linear loss function makes MAP inference “winner take all,” in the sense
that it is preferable to fully satisfy the most highly weighted objective terms completely
before reducing the distance to satisfaction of terms with lower weights. For example,
consider the following optimization problem:

arg min
y1∈[0,1]

w1 max {y1, 0} + w2 max {1 − y1, 0} .

(19)

If w1 > w2 ≥ 0, then the optimizer is y1 = 0 because the term that prefers y1 = 0 overrules
the term that prefers y1 = 1. The result does not indicate any ambiguity or uncertainty, but
if the two objective terms are potentials in a probabilistic model, it is sometimes preferable
that the result reﬂect the conﬂicting preferences. We can change the inference problem
so that it smoothly trades oﬀ satisfying conﬂicting objective terms by squaring the hinge
losses. Observe that in the modiﬁed problem

arg min
y1∈[0,1]

w1 (max {y1, 0})2 + w2 (max {1 − y1, 0})2

(20)

the optimizer is now y1 = w2

, reﬂecting the relative inﬂuence of the two loss functions.
Another advantage of squared hinge-loss functions is that they can behave more intu-

w1+w2

itively in the presence of hard constraints. Consider the problem

arg min
(y1,y2)∈[0,1]2

such that

max {0.9 − y1, 0} + max {0.6 − y2, 0}

y1 + y2 ≤ 1 .

(21)

The ﬁrst term prefers y1 ≥ 0.9, the second term prefers y2 ≥ 0.6, and the constraint requires
that y1 and y2 are mutually exclusive. Such problems are very common and arise when
conﬂicting evidence of diﬀerent strengths support two mutually exclusive possibilities. The
evidence values 0.9 and 0.6 could come from many sources, including base models trained to
make independent predictions on individual random variables, domain-specialized similarity
functions, or sensor readings. For this problem, any solution y1 ∈ [0.4, 0.9] and y2 = 1 − y1
is an optimizer. This solution set includes counterintuitive optimizers like y1 = 0.4 and
y2 = 0.6, even though the evidence supporting y1 is stronger. Again, squared hinge losses

11

Bach, Broecheler, Huang, and Getoor

ensure the optimizers better reﬂect the relative strength of evidence. For the problem

(max {0.9 − y1, 0})2 + (max {0.6 − y2, 0})2

arg min
(y1,y2)∈[0,1]2

such that

(22)

y1 + y2 ≤ 1 ,

the only optimizer is y1 = 0.65 and y2 = 0.35, which is a more informative solution.

We therefore complete our generalized inference objective by allowing either hinge-loss
or squared hinge-loss functions. Users of HL-MRFs have the choice of either one for each
potential, depending on which is appropriate for their task.

3.2 Deﬁnition

We can now formally state the full deﬁnition of HL-MRFs. They are deﬁned so that a MAP
state is a solution to the generalized inference objective proposed in the previous subsection.
We state the deﬁnition in a conditional form for later convenience, but this deﬁnition is fully
general since the vector of conditioning variables may be empty.

Deﬁnition 3 Let y = (y1, . . . , yn) be a vector of n variables and x = (x1, . . . , xn(cid:48)) a vector
of n(cid:48) variables with joint domain D = [0, 1]n+n(cid:48). Let φ = (φ1, . . . , φm) be a vector of m
continuous potentials of the form

φj(y, x) = (max {(cid:96)j(y, x), 0})pj

where (cid:96)j is a linear function of y and x and pj ∈ {1, 2}. Let c = (c1, . . . , cr) be a vector of
r linear constraint functions associated with index sets denoting equality constraints E and
inequality constraints I, which deﬁne the feasible set

(cid:26)

˜D =

(y, x) ∈ D

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I

(cid:27)

.

For (y, x) ∈ D, given a vector of m nonnegative free parameters, i.e., weights, w =
(w1, . . . , wm), a constrained hinge-loss energy function fw is deﬁned as

(23)

(24)

(25)

fw(y, x) =

wjφj(y, x) .

m
(cid:88)

j=1

We now deﬁne HL-MRFs by placing a probability density over the inputs to a con-
strained hinge-loss energy function. Note that we negate the hinge-loss energy function so
that states with lower energy are more probable, in contrast with Deﬁnition 1. This change
is made for later notational convenience.

Deﬁnition 4 A hinge-loss Markov random ﬁeld P over random variables y and con-
ditioned on random variables x is a probability density deﬁned as follows: if (y, x) /∈ ˜D,
then P (y|x) = 0; if (y, x) ∈ ˜D, then

P (y|x) =

exp (−fw(y, x))

(26)

1
Z(w, x)

12

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

where

(cid:90)

Z(w, x) =

y|(y,x)∈ ˜D

exp (−fw(y, x)) dy .

(27)

In the rest of this paper, we will explore how to use HL-MRFs to solve a wide range
of structured machine learning problems. We ﬁrst introduce a probabilistic programming
language that makes HL-MRFs easy to deﬁne for large, rich domains.

4. Probabilistic Soft Logic

In this section we introduce a general-purpose probabilistic programming language, prob-
abilistic soft logic (PSL). PSL allows HL-MRFs to be easily applied to a broad range of
structured machine learning problems by deﬁning templates for potentials and constraints.
In models for structured data, there are very often repeated patterns of probabilistic de-
pendencies. A few of the many examples include the strength of ties between similar people
in social networks, the preference for triadic closure when predicting transitive relation-
ships, and the “exactly one active” constraints on functional relationships. Often, to make
graphical models both easy to deﬁne and able to generalize across diﬀerent data sets, these
repeated dependencies are deﬁned using templates. Each template deﬁnes an abstract de-
pendency, such as the form of a potential function or constraint, along with any necessary
parameters, such as the weight of the potential, each of which has a single value across all
dependencies deﬁned by that template. Given input data, an undirected graphical model
is constructed from a set of templates by ﬁrst identifying the random variables in the data
and then “grounding out” each template by introducing a potential or constraint into the
graphical model for each subset of random variables to which the template applies.

A PSL program is written in a declarative, ﬁrst-order syntax and deﬁnes a class of
HL-MRFs that are parameterized by the input data. PSL provides a natural interface to
represent hinge-loss potential templates using two types of rules: logical rules and arithmetic
rules. Logical rules are based on the mapping from logical clauses to hinge-loss potentials
introduced in Section 2. Arithmetic rules provide additional syntax for deﬁning an even
wider range of hinge-loss potentials and hard constraints.

4.1 Deﬁnition

In this subsection we deﬁne PSL. Our deﬁnition covers the essential functionality that
should be supported by all implementations, but many extensions are possible. The PSL
syntax we describe can capture a wide range of HL-MRFs, but new settings and scenarios
could motivate the development of additional syntax to make the construction of diﬀerent
kinds of HL-MRFs more convenient.

4.1.1 Preliminaries

We begin with a high-level deﬁnition of PSL programs.

Deﬁnition 5 A PSL program is a set of rules, each of which is a template for hinge-loss
potentials or hard linear constraints. When grounded over a base of ground atoms, a PSL
program induces a HL-MRF conditioned on any speciﬁed observations.

13

Bach, Broecheler, Huang, and Getoor

In the PSL syntax, many components are named using identiﬁers, which are strings that
begin with a letter (from the set {A, . . . , Z, a, . . . , z}), followed by zero or more letters,
numeric digits, or underscores.

PSL programs are grounded out over data, so the universe over which to ground must

be deﬁned.

Deﬁnition 6 A constant is a string that denotes an element in the universe over which
a PSL program is grounded.

Constants are the elements in a universe of discourse. They can be entities or attributes.
For example, the constant "person1" can denote a person, the constant "Adam" can denote
In PSL programs,
a person’s name, and the constant "30" can denote a person’s age.
constants are written as strings in double or single quotes. Constants use backslashes as
escape characters, so they can be used to encode quotes within constants. It is assumed that
constants are unambiguous, i.e., diﬀerent constants refer to diﬀerent entities and attributes.3
Groups of constants can be represented using variables.

Deﬁnition 7 A variable is an identiﬁer for which constants can be substituted.

Variables and constants are the arguments to logical predicates. Together, they are generi-
cally referred to as terms.

Deﬁnition 8 A term is either a constant or a variable.

Terms are connected by relationships called predicates.

Deﬁnition 9 A predicate is a relation deﬁned by a unique identiﬁer and a positive integer
called its arity, which denotes the number of terms it accepts as arguments. Every predicate
in a PSL program must have a unique identiﬁer as its name.

We refer to a predicate using its identiﬁer and arity appended with a slash. For example,
the predicate Friends/2 is a binary predicate, i.e., taking two arguments, which represents
whether two constants are friends. As another example, the predicate Name/2 can relate
a person to the string that is that person’s name. As a third example, the predicate
EnrolledInClass/3 can relate two entities, a student and professor, with an additional
attribute, the subject of the class.

Predicates and terms are combined to create atoms.

Deﬁnition 10 An atom is a predicate combined with a sequence of terms of length equal
to the predicate’s arity. This sequence is called the atom’s arguments. An atom with only
constants for arguments is called a ground atom.

Ground atoms are the basic units of reasoning in PSL. Each represents an unknown or
observation of interest and can take any value in [0, 1]. For example, the ground atom
Friends("person1", "person2") represents whether "person1" and "person2" are friends.
Atoms that are not ground are placeholders for sets of ground atoms. For example, the
atom Friends(X, Y) stands for all ground atoms that can be obtained by substituting
constants for variables X and Y.

3. Note that ambiguous references to underlying entities can be modeled by using diﬀerent constants for
diﬀerent references and representing whether they refer to the same underlying entity as a predicate.

14

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

4.1.2 Inputs

As we have already stated, PSL deﬁnes templates for hinge-loss potentials and hard linear
constraints that are grounded out over a data set to induce a HL-MRF. We now describe how
that data set is represented and provided as the inputs to a PSL program. The ﬁrst inputs
are two sets of predicates: a set C of closed predicates, the atoms of which are completely
observed, and a set O of open predicates, the atoms of which may be unobserved. The third
input is the base A, which is the set of all ground atoms under consideration. All atoms in
A must have a predicate in either C or O. These are the atoms that can be substituted into
the rules and constraints of a PSL program, and each will later be associated with a HL-
MRF random variable with domain [0, 1]. The ﬁnal input is a function O : A → [0, 1] ∪ {∅}
that maps the ground atoms in the base to either an observed value in [0, 1] or a symbol ∅
indicating that it is unobserved. The function O is only valid if all atoms with a predicate in
C are mapped to a [0, 1] value. Note that this deﬁnition makes the sets C and O redundant
in a sense, since they can be derived from A and O, but it will be convenient later to have
C and O explicitly deﬁned.

Ultimately, the method for specifying PSL’s inputs is implementation-speciﬁc, since
In this paper,
diﬀerent choices make it more or less convenient for diﬀerent scenarios.
we will assume that C, O, A, and O exist, and we remain agnostic about how they were
speciﬁed. However, to make this aspect of using PSL more concrete, we will describe one
possible method for deﬁning them here.

Our example method for specifying PSL’s inputs is text-based. The ﬁrst section of the
text input is a deﬁnition of the constants in the universe, which are grouped into types. An
example universe deﬁnition follows.

Person = {"alexis", "bob", "claudia", "david"}
Professor = {"alexis", "bob"}
Student = {"claudia", "david"}
Subject = {"computer science", "statistics"}

This universe includes six constants, four with two types ("alexis", "bob", "claudia",
and "david") and two with one type ("computer science" and "statistics").

The next section of input is the deﬁnition of predicates. Each predicate includes the
types of constants it takes as arguments and whether it is closed. For example, we can
deﬁne predicates for an advisor-student relationship prediction task as follows:

Advises(Professor, Student)

Department(Person, Subject) (closed)

EnrolledInClass(Student, Subject, Professor) (closed)

In this case, there is one open predicate (Advises) and two closed predicates (Department
and EnrolledInClass).

15

Bach, Broecheler, Huang, and Getoor

The ﬁnal section of input is any associated observations. They can be speciﬁed in a list,

for example:

Advises("alexis", "david") = 1

Department("alexis", "computer science") = 1

Department("bob", "computer science") = 1

Department("claudia", "statistics") = 1

Department("david", "statistics") = 1

In addition, values for atoms with the EnrolledInClass predicate could also be speciﬁed.
If a ground atom does not have a speciﬁed value, it will have a default observed value of 0
if its predicate is closed or remain unobserved if its predicate is open.

We now describe how this text input is processed into the formal inputs C, O, A, and
O. First, each predicate is added to either C or O based on whether it is annotated with
the (closed) tag. Then, for each predicate in C or O, ground atoms of that predicate are
added to A with each sequence of constants as arguments that can be created by selecting
a constant of each of the predicate’s argument types. For example, assume that the input
ﬁle contains a single predicate deﬁnition

Category(Document, Cat Name)

where the universe is Document = {"d1", "d2"} and Cat Name = {"politics", "sports"}.
Then,

A =






Category("d1", "politics"),
Category("d1", "sports"),
Category("d2", "politics"),
Category("d2", "sports")






.

(28)

Finally, we deﬁne the function O. Any atom in the explicit list of observations is mapped
to the given value. Then, any remaining atoms in A with a predicate in C are mapped to
0, and any with a predicate in O are mapped to ∅.

Before moving on, we also note that PSL implementations can support predicates and
atoms that are deﬁned functionally. Such predicates can be thought of as a type of closed
predicate. Their observed values are deﬁned as a function of their arguments. One of the
most common examples is inequality, atoms of which can be represented with the shorthand
inﬁx operator !=. For example, the following atom has a value of 1 when two variables A
and B are replaced with diﬀerent constants and 0 when replaced with the same constant.

Such functionally deﬁned predicates can be implemented without requiring their values over
all arguments to be speciﬁed by the user.

4.1.3 Rules and Grounding

Before introducing the syntax and semantics of speciﬁc PSL rules, we deﬁne the grounding
procedure that induces HL-MRFs in general. Given the inputs C, O, A, and O, PSL induces

A != B

16

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

a HL-MRF P (y|x) as follows. First, each ground atom a ∈ A is associated with a random
variable with domain [0, 1]. If O(a) = ∅, then the variable is included in the free variables
y, and otherwise it is included in the observations x with a value of O(a).

With the variables in the distribution deﬁned, each rule in the PSL program is applied
to the inputs and produces hinge-loss potentials or hard linear constraints, which are added
to the HL-MRF. In the rest of this subsection, we describe two kinds of PSL rules: logical
rules and arithmetic rules.

4.1.4 Logical Rules

The ﬁrst kind of PSL rule is a logical rule, which is made up of literals.

Deﬁnition 11 A literal is an atom or a negated atom.

In PSL, the preﬁx operator ! or ~ is used for negation. A negated atom has a value of one
minus the value of the unmodiﬁed atom. For example, if Friends("person1", "person2")
has a value of 0.7, then !Friends("person1", "person2") has a value of 0.3.

Deﬁnition 12 A logical rule is a disjunctive clause of literals. Logical rules are either
weighted or unweighted. If a logical rule is weighted, it is annotated with a nonnegative
weight and optionally a power of two.

Logical rules express logical dependencies in the model. As in Boolean logic, the negation,
disjunction (written as || or |), and conjunction (written as && or &) operators obey De
Morgan’s Laws. Also, an implication (written as -> or <-) can be rewritten as the negation
of the body disjuncted with the head. For example

P1(A, B) && P2(A, B) -> P3(A, B) || P4(A, B)
≡ !(P1(A, B) && P2(A, B)) || P3(A, B) || P4(A, B)
≡ !P1(A, B) || !P2(A, B) || P3(A, B) || P4(A, B)

Therefore, any formula written as an implication with (1) a literal or conjunction of literals
in the body and (2) a literal or disjunction of literals in the head is also a valid logical rule,
because it is equivalent to a disjunctive clause.

There are two kinds of logical rules: weighted or unweighted. A weighted logical rule is a
template for a hinge-loss potential that penalizes how far the rule is from being satisﬁed. A
weighted logical rule begins with a nonnegative weight and optionally ends with an exponent
of two (^2). For example, the weighted logical rule

1 : Advisor(Prof, S) && Department(Prof, Sub) -> Department(S, Sub)

has a weight of 1 and induces potentials propagating department membership from advisors
to advisees. An unweighted logical rule is a template for a hard linear constraint that
requires that the rule always be satisﬁed. For example, the unweighted logical rule

Friends(X, Y) && Friends(Y, Z) -> Friends(X, Z) .

induces hard linear constraints enforcing the transitivity of the Friends/2 predicate. Note
the period (.) that is used to emphasize that this rule is always enforced and disambiguate
it from weighted rules.

17

Bach, Broecheler, Huang, and Getoor

A logical rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. This procedure produces
a set of ground rules, which are rules containing only ground atoms. Each ground rule will
then be interpreted as either a potential or hard constraint in the induced HL-MRF. For
notational convenience, we assume without loss of generality that all the random variables
are unobserved, i.e., O(a) = ∅, ∀a ∈ A. If the input data contain any observations, the
following description still applies, except that some free variables will be replaced with
observations from x. The ﬁrst step in interpreting a ground rule is to map its disjunctive
clause to a linear constraint. This mapping is based on the uniﬁed inference objective
derived in Section 2. Any ground PSL rule is a disjunction of literals, some of which are
negated. Let I + be the set of indices of the variables that correspond to atoms that are not
negated in the ground rule, when expressed as a disjunctive clause, and, likewise, let I − be
the indices of the variables corresponding to atoms that are negated. Then, the clause is
mapped to the inequality

(cid:88)

1 −

(cid:88)

yi −

(1 − yi) ≤ 0 .

i∈I +

i∈I −

If the logical rule that templated the ground rule is weighted with a weight of w and is not
annotated with ^2, then the potential

is added to the HL-MRF with a parameter of w. If the rule is weighted with a weight w
and annotated with ^2, then the potential











φ(y, x) = max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −



φ(y, x) =

max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −

c(y, x) = 1 −

(cid:88)

(cid:88)

yi −

(1 − yi)

i∈I +

i∈I −








2








is added to the HL-MRF with a parameter of w. If the rule is unweighted, then the function

is added to the set of constraint functions and its index is included in the set I to deﬁne a
hard inequality constraint c(y, x) ≤ 0.

As an example of the grounding process, consider the following logical rule. As part of
a program for link prediction, it is often helpful to model the transitivity of a relationship.

3 : Friends(A, B) && Friends(B, C) -> Friends(C, A) ^2

Imagine that the input data are C = {}, O = {Friends/2},

(29)

(30)

(31)

(32)

(33)

A =






Friends("p1", "p2"),
Friends("p1", "p3"),
Friends("p2", "p1"),
Friends("p2", "p3"),
Friends("p3", "p1"),
Friends("p3", "p2")






,

18

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and O(a) = ∅, ∀a ∈ A. Then, the rule will induce six ground rules. One such ground rule is

3 : Friends("p1", "p2") && Friends("p2", "p3") -> Friends("p3", "p1") ^2

which is equivalent to the following.

3 : !Friends("p1", "p2") || !Friends("p2", "p3") || Friends("p3", "p1") ^2

If the atoms Friends("p1", "p2"), Friends("p2", "p3"), and Friends("p3", "p1")
correspond to the random variables y1, y2, and y3, respectively, then this ground rule is
interpreted as the weighted hinge-loss potential

3 (max{y1 + y2 − y3 − 1, 0})2 .

(34)

Since the grounding process uses the mapping from Section 2, logical rules can be used
to reason accurately and eﬃciently about both discrete and continuous information. They
are a convenient method for constructing HL-MRFs with the uniﬁed inference objective
for weighted logical knowledge bases as their MAP inference objective. They also allow
the user to seamlessly incorporate some of the additional features of HL-MRFs, such as
squared potentials and hard constraints. Next, we introduce an even more ﬂexible class of
PSL rules.

4.1.5 Arithmetic Rules

Arithmetic rules in PSL are more general templates for hinge-loss potentials and hard
linear constraints. Like logical rules, they come in weighted and unweighted variants, but
instead of using logical operators they use arithmetic operators. In general, an arithmetic
rule relates two linear combinations of atoms with an inequality or an equality. A simple
example enforces the mutual exclusivity of liberal and conservative ideologies.

Liberal(P) + Conservative(P) = 1 .

Just like logical rules, arithmetic rules are grounded out by performing all possible substi-
tutions of constants for variables to make ground atoms in the base A. In this example,
each substitution for Liberal(P) and Conservative(P) is constrained to sum to 1. Since
the rule is unweighted and arithmetic, it deﬁnes a hard constraint c(y, x) and its index will
be included in E because it is an equality constraint.

To make arithmetic rules more ﬂexible and easy to use, we deﬁne some additional syntax.
The ﬁrst is a generalized deﬁnition of atoms that can be substituted with sums of ground
atoms, rather than just a single atom.

Deﬁnition 13 A summation atom is an atom that takes terms and/or sum variables
as arguments. A summation atom represents the summations of ground atoms that can be
obtained by substituting individual constants for variables and summing over all possible
constants for sum variables.

A sum variable is represented by prepending a plus symbol (+) to a variable. For example,
the summation atom

Friends(P, +F)

19

Bach, Broecheler, Huang, and Getoor

is a placeholder for the sum of all ground atoms with predicate Friends/2 in A that share
a ﬁrst argument. Note that sum variables can be used at most once in a rule, i.e., each
sum variable in a rule must have a unique identiﬁer. Summation atoms are useful because
they can describe dependencies without needing to specify the number of atoms that can
participate. For example, the arithmetic rule

Label(X, +L) = 1 .

says that labels for each constant substituted for X should sum to one, without needing to
specify how many possible labels there are.

The substitutions for sum variables can be restricted using logical clauses as ﬁlters.

Deﬁnition 14 A ﬁlter clause is a logical clause deﬁned for a sum variable in an arithmetic
rule. The logical clause only contains atoms (1) with predicates that appear in C and (2)
that only take as arguments (a) constants, (b) variables that appear in the arithmetic rule,
and (c) the sum variable for which it is deﬁned.

Filter clauses restrict the substitutions for a sum variable in the corresponding arithmetic
rule by only including substitutions for which the clause evaluates to true. The ﬁlters are
evaluated using Boolean logic. Each ground atom a is treated as having a value of 0 if and
only if O(a) = 0. Otherwise, it is treated as having a value of 1. For example, imagine that
we want to restrict the summation in the following arithmetic rule to only constants that
satisfy a property Property/1.

Then, we can add the following ﬁlter clause.

Link(X, +Y) <= 1 .

{Y: Property(Y)}

Then, the hard linear constraints templated by the arithmetic rule will only sum over
constants substituted for Y such that Property(Y) is non-zero.

In arithmetic rules, atoms can also be modiﬁed with coeﬃcients. These coeﬃcients can

be hard-coded. As a simple example, in the rule

Susceptible(X) >= 0.5 Biomarker1(X) + 0.5 Biomarker2(X) .

the property Susceptible/1, which represents the degree to which a patient is susceptible
to a particular disease, must be at least the average value of two biomarkers.

PSL also supports two forms of coeﬃcient-deﬁning syntax. The ﬁrst form of coeﬃcient
syntax is a cardinality function that counts the number of terms substituted for a sum
variable. Cardinality functions enable rules that depend on the number of substitutions in
order to be scaled correctly, such as when averaging. Cardinality is denoted by enclosing a
sum variable, without the +, in pipes. For example, the rule

1 / |Y| Friends(X, +Y) = Friendliness(X) .

20

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

deﬁnes the Friendliness/1 property of a person X in a social network as the average
strength of their outgoing friendship links. In cases in which Friends/2 is not symmetric,
we can extend this rule to sum over both outgoing and incoming links as follows.

1 / |Y1| |Y2| Friends(X, +Y1) + 1 / |Y1| |Y2| Friends(+Y2, X)

= Friendliness(X) .

The second form of coeﬃcient syntax is built-in coeﬃcient functions. The exact set of
supported functions is implementation speciﬁc, but standard functions like maximum and
minimum should be included. Coeﬃcient functions are prepended with @ and use square
brackets instead of parentheses to distinguish them from predicates. Coeﬃcient functions
can take either scalars or cardinality functions as arguments. For example, the following
rule for matching two sets of constants requires that the sum of the Matched/2 atoms be
the minimum of the sizes of the two sets.

Matched(+X, +Y) = @Min[|X|, |Y|] .

Note that PSL’s coeﬃcient syntax can also be used to deﬁne constants, as in this example.
So far we have focused on using arithmetic rules to deﬁne templates for linear constraints,
but they can also be used to deﬁne hinge-loss potentials. For example, the following arith-
metic rule prefers that the degree to which a person X is extroverted (represented with
Extroverted/1) does not exceed the average extroversion of their friends:

2 : Extroverted(X) <= 1 / |Y| Extroverted(+Y) ^2
{Y: Friends(X, Y) || Friends(Y, X)}

This rule is a template for weighted hinge-loss potentials of the form

(cid:32)

(cid:40)

2

max

yi(cid:48) −

(cid:41)(cid:33)2

yi, 0

,

1
|F|

(cid:88)

i∈F

(35)

where yi(cid:48) is the variable corresponding to a grounding of the atom Extroverted(X) and
F is the set of the indices of the variables corresponding to Extroverted(Y) atoms of the
friends Y that satisfy the rule’s ﬁlter clause. Note that the weight of 2 is distinct from
the coeﬃcients in the linear constraint (cid:96)(y, x) ≤ 0 deﬁning the hinge-loss potential.
If
the arithmetic rule were an equality instead of an inequality, each grounding would be
two hinge-loss potentials, one using (cid:96)(y, x) ≤ 0 and one using −(cid:96)(y, x) ≤ 0. In this way,
arithmetic rules can deﬁne general hinge-loss potentials.

For completeness, we state the full, formal deﬁnition of an arithmetic rule and deﬁne its

grounding procedure.

Deﬁnition 15 An arithmetic rule is an inequality or equality relating two linear combi-
nations of summation atoms. Each sum variable in an arithmetic rule can be used once.
An arithmetic rule can be annotated with ﬁlter clauses for a subset of its sum variables that
restrict its groundings. Arithmetic rules are either weighted or unweighted. If an arithmetic
rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.

21

Bach, Broecheler, Huang, and Getoor

An arithmetic rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. In addition, summation
atoms are replaced by the appropriate summations over ground atoms (possibly restricted
by corresponding ﬁlter clauses) and the coeﬃcient is distributed across the summands. This
leads to a set of ground rules for each arithmetic rule given a set of inputs. If the arithmetic
rule is an unweighted inequality, each ground rule can be algebraically manipulated to be
of the form c(y, x) ≤ 0. Then c(y, x) is added to the set of constraint functions and its
index is added to I. If instead the arithmetic rule is an unweighted equality, each ground
rule is manipulated to c(y, x) = 0, c(y, x) is added to the set of constraint functions, and
its index is added to E. If the arithmetic rule is a weighted inequality with weight w, each
ground rule is manipulated to (cid:96)(y, x) ≤ 0 and included as a potential of the form

φ(y, x) = max {(cid:96)(y, x), 0}

(36)

with a weight of w. If the arithmetic rule is a weighted equality with weight w, each ground
rule is again manipulated to (cid:96)(y, x) ≤ 0 and two potentials are included,

φ1(y, x) = max {(cid:96)(y, x), 0}, φ2(y, x) = max {−(cid:96)(y, x), 0} ,

(37)

each with a weight of w. In either case, if the weighted arithmetic rule is annotated with
^2, then the induced potentials are squared.

4.2 Expressivity

An important question is the expressivity of PSL, which uses disjunctive clauses with pos-
itive weights for its logical rules. Other logic-based languages support diﬀerent types of
clauses, such as Markov logic networks (Richardson and Domingos, 2006), which support
clauses with conjunctions and clauses with negative weights. As we discuss in this section,
PSL’s logical rules capture a general class of structural dependencies, capable of model-
ing arbitrary probabilistic relationships among Boolean variables, such as those deﬁned by
Markov logic networks. The advantage of PSL is that it deﬁnes HL-MRFs, which are much
more scalable than discrete MRFs and often just as accurate, as we show in Section 6.4.

The expressivity of PSL is tied to the expressivity of the MAX SAT problem, since
they both use the same class of weighted clauses. There are two conditions on the clauses:
(1) they have nonnegative weights, and (2) they are disjunctive. We ﬁrst consider the
nonnegativity requirement and show that can actually be viewed as a restriction on the
structure of a clause. To illustrate, consider a weighted disjunctive clause of the form

−w :



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(38)

If this clause were part of a generalized MAX SAT problem, in which there were no restric-
tions on weight sign or clause structure, but the goal were still to maximize the sum of the
weights of the satisﬁed clauses, then this clause could be replaced with an equivalent one

22

(39)

(40)

(41)

(42)

(43)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

without changing the optimizer:

w :






¬xi






(cid:94)

i∈I +
j

(cid:94)

(cid:94)








xi


 .

i∈I −
j

Note that the clause has been changed in three ways: (1) the sign of the weight has been
changed, (2) the disjunctions have been replaced with conjunctions, and (3) the literals
have all been negated. Due to this equivalence, the restriction on the sign of the weights
is subsumed by the restriction on the structure of the clauses. In other words, any set of
clauses can be converted to a set with nonnegative weights that has the same optimizer,
but it might require including conjunctions in the clauses. It is also easy to verify that if
Equation (38) is used to deﬁne a potential in a discrete MRF, replacing it with a potential
deﬁned by (39) leaves the distribution unchanged, due to the normalizing partition function.
We now consider the requirement that clauses be disjunctive and illustrate how con-
junctive clauses can be replaced by an equivalent set of disjunctive clauses. The idea is to
construct a set of disjunctive clauses such that all assignments to the variables are mapped
to the same score, up to a constant. A simple example is replacing a conjunction

with disjunctions

w : x1 ∧ x2

w : x1 ∨ x2
w : ¬x1 ∨ x2
w : x1 ∨ ¬x2 .

Observe that the total score for all assignments to the variables remains the same, up to a
constant.

This example generalizes to a procedure for encoding any Boolean MRF into a set of
disjunctive clauses with nonnegative weights. Park (2002) showed that the MAP problem
for any discrete Bayesian network can be represented as an instance of MAX SAT. For
distributions of bounded factor size, the MAX SAT problem has size polynomial in the
number of variables and factors of the distribution. We describe how any Boolean MRF
can be represented with disjunctive clauses and nonnegative weights. Given a Boolean MRF
with arbitrary potentials deﬁned by mappings from joint states of subsets of the variables
to scores, a new MRF is created as follows. For each potential in the original MRF, a new
set of potentials deﬁned by disjunctive clauses is created. A conjunctive clause is created
corresponding to each entry in the potential’s mapping with a weight equal to the score
assigned by the weighted potential in the original MRF. Then, these clauses are converted to
equivalent disjunctive clauses as in the example of Equations (38) and (39) by also ﬂipping
the sign of their weights and negating the literals. Once this is done for all entries of all
potentials, what remains is an MRF deﬁned by disjunctive clauses, some of which might
have negative weights. We make all weights positive by adding a suﬃciently large constant
to all weights of all clauses, which leaves the distribution unchanged due to the normalizing
partition function.

23

Bach, Broecheler, Huang, and Getoor

It is important to note two caveats when converting arbitrary Boolean MRFs to MRFs
deﬁned using only disjunctive clauses with nonnegative weights. First, the number of clauses
required to represent a potential in the original MRF is exponential in the degree of the
potential. In practice, this is rarely a signiﬁcant limitation, since MRFs often contain low-
degree potentials. The other important point is that the step of adding a constant to all
the weights increases the total score of the MAP state. Since the bound of Goemans and
Williamson (1994) is relative to this score, the bound is loosened for the original problem the
larger the constant added to the weights is. This is to be expected, since even approximating
MAP is NP-hard in general (Abdelbar and Hedetniemi, 1998).

We have described how general structural dependencies can be modeled with the logical
rules of PSL. It is possible to represent arbitrary logical relationships with them. The
process for converting general rules to PSL’s logical rules can be done automatically and
made transparent to the user. We have elected in this section to deﬁne PSL’s logical rules
without making this conversion automatic to make clear the underlying formalism.

4.3 Modeling Patterns

PSL is a ﬂexible language, and there are some patterns of usage that come up in many
applications. We illustrate some of them in this subsection with a number of examples.

4.3.1 Domain and Range Rules

In many problems, the number of relations that can be predicted among some constants
is known. For binary predicates, this background knowledge can be viewed as constraints
on the domain (ﬁrst argument) or range (second argument) of the predicate. For example,
it might be background knowledge that each entity, such as a document, has exactly one
label. An arithmetic rule to express this follows.

Label(Document, +LabelName) = 1 .

The predicate Label is said to be functional.

Alternatively, sometimes it is the ﬁrst argument that should be summed over. For ex-
ample, imagine the task of predicting relationships among students and professors. Perhaps
it is known that each student has exactly one advisor. This constraint can be written as
follows.

Advisor(+Professor, Student) = 1 .

The predicate Advisor is said to be inverse functional.

Finally, imagine a scenario in which two social networks are being aligned. The goal is
to predict whether each pair of people, one from each network, is the same person, which is
represented with atoms of the Same predicate. Each person aligns with at most one person
in the other network, but might not align with anyone. This can be expressed with the
following two arithmetic rules.

The predicate Same is said to be both partial functional and partial inverse functional.

Same(Person1, +Person2) <= 1 .

Same(+Person1, Person2) <= 1 .

24

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Many variations on these examples are possible. For example, they can be generalized
to predicates with more than two arguments. Additional arguments can either be ﬁxed or
summed over in each rule. As another example, domain and range rules can incorporate
multiple predicates, so that an entity can participate in a ﬁxed number of relations counted
among multiple predicates.

4.3.2 Similarity

Many problems require explicitly reasoning about similarity, rather than simply whether
entities are the same or diﬀerent. For example, reasoning with similarity has been explored
using kernel methods, such as kFoil (Landwehr et al., 2010) that bases similarity computa-
tion on the relational structure of the data. The continuous variables of HL-MRFs make
modeling similarity straightforward, and PSL’s support for functionally deﬁned predicates
makes it even easier. For example, in an entity resolution task, the degree to which two
entities are believed to be the same might depend on how similar their names are. A rule
expressing this dependency is

1.0 : Name(P1, N1) && Name(P2, N2) && Similar(N1, N2) -> Same(P1, P2)

This rule uses the Similar predicate to measure similarity. Since it is a functionally deﬁned
predicate, it can be implemented as one of many diﬀerent, possibly domain specialized,
string similarity functions. Any similarity function that can output values in the range
[0, 1] can be used.

4.3.3 Priors

If no potentials are deﬁned over a particular atom, then it is equally probable that it has any
value between zero and one. Often, however, it should be more probable that an atom has
a value of zero, unless there is evidence that it has a nonzero value. Since atoms typically
represent the existence of some entity, attribute, or relation, this bias promotes sparsity
among the things inferred to exist. Further, if there is a potential that prefers that an
atom should have a value that is at least some numeric constant, such as when reasoning
with similarities as discussed in Section 4.3.2, it often should also be more probable that an
atom is no higher in value than is necessary to satisfy that potential. To accomplish both
these goals, simple priors can be used to state that atoms should have low values in the
absence of evidence to overrules those priors. A prior in PSL can be a rule consisting of
just a negative literal with a small weight. For example, in a link prediction task, imagine
that this preference should apply to atoms of the Link predicate. A prior is then

0.1 : !Link(A, B)

which acts as a regularizer on Link atoms.

4.3.4 Blocks and Canopies

In many tasks, the number of unknowns can quickly grow large, even for modest amounts of
data. For example, in a link prediction task the goal is to predict relations among entities.
The number of possible links grows quadratically with the number of entities (for binary

25

Bach, Broecheler, Huang, and Getoor

relations). If handled naively, this growth could make scaling to large data sets diﬃcult,
but this problem is often handled by constructing blocks (e.g., Newcombe and Kennedy,
1962) or canopies (McCallum et al., 2000) over the entities, so that a limited subset of all
possible links are actually considered. Blocking partitions the entities so that only links
among entities in the same partition element, i.e., block, are considered. Alternatively, for
a ﬁner grained pruning, a canopy is deﬁned for each entity, which is the set of other entities
to which it could possibly link. Blocks and canopies can be computed using specialized,
domain-speciﬁc functions, and PSL can incorporate them by including them as atoms in
the bodies of rules. Since blocks can be seen as a special case of canopies, we let the atom
InCanopy(A, B) be 1 if B is in the canopy or block of A, and 0 if it is not.
Including
InCanopy(A, B) atoms as additional conditions in the bodies of logical rules will ensure
that the dependencies only exist between the desired entities.

4.3.5 Aggregates

Another powerful feature of PSL is its ability to easily deﬁne aggregates, which are rules
that deﬁne random variables to be deterministic functions of sets of other random variables.
The advantage of aggregates is that they can be used to deﬁne dependencies that do not
scale in magnitude with the number of groundings in the data. For example, consider a
model for predicting interests in a social network. A fragment of a PSL program for this
task follows.

1.0 : Interest(P1, I) && Friends(P1, P2) -> Interest(P2, I)

1.0 : Age(P, "20-29") && Lives(P, "California") -> Interest(P, "Surfing")

These two rules express the belief that interests are correlated along friendship links in
the social network, and also that certain demographic information is predictive of speciﬁc
interests. The question any domain expert or learning algorithm faces is how strongly each
rule should be weighted relative to each other. The challenge of answering this question
when using templates is that the number of groundings of the ﬁrst rule varies from person to
person based on the number of friends, while the groundings of the second remain constant
(one per person). This inconsistent scaling of the two types of dependencies makes it diﬃcult
to ﬁnd weights that accurately reﬂect the relative inﬂuence each type of dependency should
have across people with diﬀerent numbers of friends.

Using an aggregate can solve this problem of inconsistent scaling. Instead of using a
separate ground rule to relate the interest of each friend, we can deﬁne a rule that is only
grounded once for each person, relating an average interest across all friends to each person’s
own interests. A PSL fragment for this approach is

1.0 : AverageFriendInterest(P, I) -> Interest(P, I)

AverageFriendInterest(P, I) = 1 / |F| Interest(+F, I) .
{F: Friends(P, F)}

/* Demographic dependencies are also included.

*/

where the predicate AverageFriendInterest/2 is an aggregate that is constrained to be
the average amount of interest each friend of a person P has in an interest I. The weight

26

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

of the logical rule can now be scaled more appropriately relative to other types of features
because there is only one grounding per person.

For a more complex example, consider the problem of determining whether two refer-
ences in the data refer to the same underlying person. One useful feature to use is whether
they have similar sets of friends in the social network. Again, a rule could be deﬁned that
is grounded out for each friendship pair, but this would suﬀer from the same scaling issues
as the previous example. Instead, we can use an aggregate to directly express how similar
the two references’ sets of friends are. A function that measures the similarity of two sets
A and B is Jaccard similarity:

J(A, B) =

|A ∩ B|
|A ∪ B|

.

Jaccard similarity is a nonlinear function, meaning that it cannot be used directly without
breaking the log-concavity of HL-MRFs, but we can approximate it with a linear function.
We deﬁne SameFriends/2 as an aggregate that approximates Jaccard similarity (where
SamePerson/2 is functional and inverse functional).

SameFriends(A, B) = 1 / @Max[|FA|, |FB|] SamePerson(+FA, +FB) .
{FA : Friends(A, FA)}
{FB : Friends(B, FB)}

SamePerson(+P1, P2) = 1 .

SamePerson(P1, +P2) = 1 .

The aggregate SameFriends/2 uses the sum of the SamePerson/2 atoms as the intersection
of the two sets, and the maximum of the sizes of the two sets of friends as a lower bound
on the size of their union.

5. MAP Inference

Having deﬁned HL-MRFs and a language for creating them, PSL, we turn to algorithms
for inference and learning. The ﬁrst task we consider is maximum a posteriori (MAP)
inference, the problem of ﬁnding a most probable assignment to the free variables y given
observations x. In HL-MRFs, the normalizing function Z(w, x) is constant over y and the
exponential is maximized by minimizing its negated argument, so the MAP problem is

arg max
y

P (y|x) ≡ arg min
y|y,x∈ ˜D

fw(y, x)

≡ arg min
y∈[0,1]n

w(cid:62)φ(y, x)

such that

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I .

(44)

MAP is a fundamental problem because (1) it is the method we will use to make predictions,
and (2) weight learning often requires performing MAP inference many times with diﬀerent
weights (as we discuss in Section 6). Here, HL-MRFs have a distinct advantage over general

27

Bach, Broecheler, Huang, and Getoor

discrete models, since minimizing fw is a convex optimization rather than a combinatorial
one. There are many oﬀ-the-shelf solutions for convex optimization, the most popular
of which are interior-point methods, which have worst-case polynomial time complexity
in the number of variables, potentials, and constraints (Nesterov and Nemirovskii, 1994).
Although in practice they perform better than their worst-case bounds (Wright, 2005), they
do not scale well to large structured prediction problems (Yanover et al., 2006). We therefore
introduce a new algorithm for exact MAP inference designed to scale to large HL-MRFs by
leveraging the sparse connectivity structure of the potentials and hard constraints that are
typical of models for real-world tasks.

5.1 Consensus Optimization Formulation

Our algorithm uses consensus optimization, a technique that divides an optimization prob-
lem into independent subproblems and then iterates to reach a consensus on the optimum
(Boyd et al., 2011). Given a HL-MRF P (y|x), we ﬁrst construct an equivalent MAP prob-
lem in which each potential and hard constraint is a function of diﬀerent variables. The
variables are then constrained to make the new and original MAP problems equivalent. We
let y(L,j) be a local copy of the variables in y that are used in the potential function φj,
j = 1, . . . , m and y(L,k+m) be a copy of those used in the constraint function ck, k = 1, . . . , r.
We refer to the concatenation of all of these vectors as yL. We also introduce a characteristic
= 0 if the constraint is
function χk for each constraint function where χk
satisﬁed and inﬁnity if it is not. Likewise, let χ[0,1] be a characteristic function that is 0 if the
input is in the interval [0, 1] and inﬁnity if it is not. We drop the constraints on the domain
of y, letting them range in principle over Rn and instead use these characteristic functions
to enforce the domain constraints. This formulation will make computation easier when
be the variables in y that correspond
the problem is later decomposed. Finally, let y

(cid:104)
ck(y(L,k+m), x)

(cid:105)

(C,ˆi)

(L,ˆi)

, ˆi = 1, . . . , m + r. Operators between y

to y
are deﬁned element-wise,
pairing the corresponding copied variables. Consensus optimization solves the reformulated
MAP problem

and y

(C,ˆi)

(L,ˆi)

arg min
(yL,y)

m
(cid:88)

j=1

such that

wjφj

(cid:16)

(cid:17)
y(L,j), x

+

(cid:104)

(cid:16)

χk

ck

y(L,k+m), x

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

(45)

y

(L,ˆi)

= y

(C,ˆi)

∀ˆi = 1, . . . , m + r .

Inspection shows that problems (44) and (45) are equivalent.

This reformulation enables us to relax the equality constraints y

in order
to divide problem (45) into independent subproblems that are easier to solve, using the
alternating direction method of multipliers (ADMM) (Glowinski and Marrocco, 1975; Gabay
and Mercier, 1976; Boyd et al., 2011). The ﬁrst step is to form the augmented Lagrangian
function for the problem. Let α = (α1, . . . , αm+r) be a concatenation of vectors of Lagrange

= y

(C,ˆi)

(L,ˆi)

28

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

multipliers. Then the augmented Lagrangian is

L(yL, α, y) =

wjφj

y(L,j), x

+

(cid:16)

(cid:17)

(cid:16)

(cid:104)
ck

χk

y(L,k+m), x

m
(cid:88)

j=1

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

+

m+r
(cid:88)

ˆi=1

(cid:16)

y

α(cid:62)
ˆi

(L,ˆi)

− y

(C,ˆi)

(cid:17)

+

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)y

(L,ˆi)

− y

(C,ˆi)

(46)

(cid:13)
2
(cid:13)
(cid:13)
2

using a step-size parameter ρ > 0. ADMM ﬁnds a saddle point of L(yL, α, y) by updating
the three blocks of variables at each iteration t:

∀ˆi = 1, . . . , m + r

αt
ˆi

← αt−1
ˆi

+ ρ

yt

L ← arg min

(cid:17)

(cid:16)

yt−1
(L,ˆi)

− yt−1
(C,ˆi)
L (cid:0)yL, αt, yt−1(cid:1)

yt ← arg min

L (cid:0)yt

L, αt, y(cid:1)

yL

y

The ADMM updates ensure that y converges to the global optimum y(cid:63), the MAP state
of P (y|x), assuming that there exists a feasible assignment to y. We check convergence
using the criteria suggested by Boyd et al. (2011), measuring the primal and dual residuals
at the end of iteration t, deﬁned as

(cid:107)¯rt(cid:107)2 (cid:44)

(cid:107)yt

(L,ˆi)

− yt

(C,ˆi)

(cid:107)2
2



(cid:107)¯st(cid:107)2 (cid:44) ρ

Ki(yt

i − yt−1
i

)2

(50)



1
2

(cid:33) 1
2

(cid:32) n
(cid:88)

i=1





m+r
(cid:88)

ˆi=1

where Ki is the number of copies made of the variable yi, i.e., the number of diﬀerent
potentials and constraints in which the variable participates. The updates are terminated
when both of the following conditions are satisﬁed










m+r
(cid:88)

ˆi=1

(cid:107)yt

(L,ˆi)

(cid:107)2
2



,

Ki(yt

i)2



1
2

(cid:32) n
(cid:88)

i=1

(cid:33) 1
2






(cid:107)¯rt(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel max

(cid:107)¯st(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel



1
2

(cid:107)2
(cid:107)αt
2
ˆi







m+r
(cid:88)

ˆi=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

using convergence parameters (cid:15)abs and (cid:15)rel.

5.2 Block Updates

We now describe how to implement the ADMM block updates (47), (48), and (49). Updating
the Lagrange multipliers α is a simple step in the gradient direction (47). Updating the
local copies yL (48) decomposes over each potential and constraint in the HL-MRF. For the

29

(47)

(48)

(49)

(51)

(52)

Bach, Broecheler, Huang, and Getoor

variables y(L,j) for each potential φj, this requires independently optimizing the weighted
potential plus a squared norm:

(cid:16)

(cid:110)

wj

max

(cid:96)j(y(L,j), x), 0

(cid:111)(cid:17)pj

arg min
y(L,j)

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(53)

There are three cases for y(cid:63)

Although this optimization problem is convex, the presence of the hinge function complicates
it. It could be solved in principle with an iterative method, such as an interior-point method,
but such methods would become very expensive over many ADMM updates. Fortunately,
we can reduce the problem to checking several cases and ﬁnd solutions much more quickly.
(L,j), the optimizer of problem (53), which correspond to
the three regions in which the solution could lie: (1) the region (cid:96)(y(L,j), x) < 0, (2) the
region (cid:96)(y(L,j), x) > 0, and (3) the region (cid:96)(y(L,j), x) = 0. We check each case by replacing
the potential with its value on the corresponding region, optimizing, and checking if the
optimizer is in the correct region. We check the ﬁrst case by replacing the potential φj
with zero. Then, the optimizer of the modiﬁed problem is y(C,j) − αj/ρ.
If (cid:96)j(y(C,j) −
αj/ρ, x) ≤ 0, then y(cid:63)
(L,j) = y(C,j) − αj/ρ, because it optimizes both the potential and the
squared norm independently. If instead (cid:96)j(y(C,j) − αj/ρ, x) > 0, then we can conclude that
(cid:96)j(y(cid:63)

(L,j), x) ≥ 0, leading to one of the next two cases.
In the second case, we replace the maximum term with the inner linear function. Then
the optimizer of the modiﬁed problem is found by taking the gradient of the objective with
respect to y(L,j), setting the gradient equal to the zero vector, and solving for y(L,j). In
other words, the optimizer is the solution for y(L,j) to the equation

(cid:34)

(cid:16)

∇y(L,j)

wj

(cid:96)j(y(L,j), x)

(cid:17)pj

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

(cid:35)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0 .

(54)

This condition deﬁnes a simple system of linear equations. If pj = 1, then the coeﬃcient
matrix is diagonal and trivial to solve. If pj = 2, then the coeﬃcient matrix is symmetric
and positive deﬁnite, and the system can be solved via Cholesky decomposition. (Since
the potentials of an HL-MRF often have shared structures, perhaps templated by a PSL
program, the Cholesky decompositions can be cached and shared among potentials for
improved performance.) Let y(cid:48)
(L,j) be the optimizer of the modiﬁed problem, i.e., the
solution to equation (54). If (cid:96)j(y(cid:48)
(L,j) = y(cid:48)
(L,j), x) ≥ 0, then y(cid:63)
(L,j) because we know the
solution lies in the region (cid:96)j(y(L,j), x) ≥ 0 and the objective of problem (53) and the
modiﬁed objective are equal on that region.
(L,j), x) ≥ 0
whenever (cid:96)j(y(C,j) − αj/ρ, x) ≥ 0, because the modiﬁed term is symmetric about the line
(cid:96)j(y(L,j), x) = 0. We therefore will only reach the following third case when pj = 1. If
(cid:96)j(y(C,j) − αj/ρ, x) > 0 and (cid:96)j(y(cid:48)
(L,j) is the
projection of y(C,j) − αj/ρ onto the hyperplane ck(y(L,j), x) = 0. This constraint must be
active because it is violated by the optimizers of both modiﬁed objectives (Martins et al.,
2015, Lemma 17). Since the potential has a value of zero whenever the constraint is active,
solving problem (53) reduces to the projection operation.

(L,j), x) < 0, then we can conclude that y(cid:63)

In fact, if pj = 2, then (cid:96)j(y(cid:48)

30

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

For the local copies y(L,k+m) for each constraint ck, the subproblem is easier:

(cid:104)

(cid:105)
ck(y(L,k+m), x)

+

χk

(cid:13)
(cid:13)
y(L,k+m) − y(C,k+m) +
(cid:13)
(cid:13)

ρ
2

1
ρ

arg min
y(L,k+m)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

αk+m

.

(55)

Whether ck is an equality or inequality constraint, the solution is the projection of y(C,k+m)−
αk+m/ρ to the feasible set deﬁned by the constraint. If ck is an equality constraint, i.e., k ∈
E, then the optimizer y(cid:63)
(L,k+m) is the projection of y(C,k+m)−αk+m/ρ onto ck(y(L,k+m), x) =
0. If, on the other hand, ck is an inequality constraint, i.e., k ∈ I, then there are two cases.
First, if ck(y(C,k+m) − αk+m/ρ, x) ≤ 0, then the solution is simply y(C,k+m) − αk+m/ρ.
Otherwise, it is again the projection onto ck(y(L,k+m), x) = 0.
To update the variables y (49), we solve the optimization

arg min
y

n
(cid:88)

i=1

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

χ[0,1] [yi] +

y

(L,ˆi)

− y

(C,ˆi)

+

(56)

1
ρ

(cid:13)
2
(cid:13)
αˆi
(cid:13)
(cid:13)
2

.

The optimizer is the state in which yi is set to the average of its corresponding local copies
added with their corresponding Lagrange multipliers divided by the step size ρ, and then
clipped to the [0, 1] interval. More formally, let copies(yi) be the set of local copies yc of
yi, each with a corresponding Lagrange multiplier αc. Then, we update each yi using

yi ←

1
|copies(yi)|

(cid:88)

yc∈copies(yi)

(cid:18)

yc +

(cid:19)

αc
ρ

(57)

and clip the result to [0, 1]. Speciﬁcally, if, after update (57), yi > 1, then we set yi to 1
and likewise set it to 0 if yi < 0.

Algorithm 1 shows the complete pseudocode for MAP inference. The method starts
by initializing local copies of the variables that appear in each potential and constraint,
along with a corresponding Lagrange multiplier for each copy. Then, until convergence, it
iteratively performs the updates (47), (48), and (49). In the pseudocode, we have interleaved
updates (47) and (48), updating both the Lagrange multipliers αˆi and the local copies y
(L,ˆi)
together for each subproblem, because they are local operations that do not depend on other
variables once y is updated in the previous iteration. This independence reveals another
advantage of our inference algorithm:
it is very easy to parallelize. The updates (47)
and (48) can be performed in parallel, the results gathered, update (49) performed, and the
updated y broadcast back to the subproblems. Parallelization makes our MAP inference
algorithm even faster and more scalable.

5.3 Lazy MAP Inference

One interesting and useful property of HL-MRFs is that it is not always necessary to
completely materialize the distribution in order to ﬁnd a MAP state. Consider a subset ˆφ
of the index set {1, . . . , m} of the potentials φ. Observe that if a feasible assignment to y
minimizes

(58)

wjφj(y, x)

(cid:88)

j∈ ˆφ

31

Bach, Broecheler, Huang, and Getoor

Algorithm 1 MAP Inference for HL-MRFs

Input: HL-MRF P (y|x), ρ > 0
Initialize y(L,j) as local copies of variables y(C,j) that are in φj, j = 1, . . . , m
Initialize y(L,k+m) as local copies of variables y(C,k+m) that are in ck, k = 1, . . . , r
, ˆi = 1, . . . , m + r
Initialize Lagrange multipliers αˆi corresponding to copies y

(L,ˆi)

(cid:96)j(y(L,j), x)

(cid:17)pj

+ ρ
2

(cid:13)
(cid:13)y(L,j) − y(C,j) + 1
(cid:13)

ρ αj

(cid:13)
2
(cid:13)
(cid:13)
2

while not converged do

for j = 1, . . . , m do

ρ αj

αj ← αj + ρ(y(L,j) − y(C,j))
y(L,j) ← y(C,j) − 1
if (cid:96)j(y(L,j), x) > 0 then
y(L,j) ← arg miny(L,j)
wj
if (cid:96)j(y(L,j), x) < 0 then

(cid:16)

y(L,j) ← Proj(cid:96)j =0(y(C,j) − 1

ρ αj)

end if

end if
end for

for k = 1, . . . , r do

αk+m ← αk+m + ρ(y(L,k+m) − y(C,k+m))
y(L,k+m) ← Projck

(y(C,k+m) − 1

ρ αk+m)

end for

for i = 1, . . . , n do

1
yi ←
|copies(yi)|
Clip yi to [0,1]

(cid:80)

end for

end while

yc∈copies(yi)

(cid:16)

yc + αc
ρ

(cid:17)

32

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and φj(y, x) = 0, ∀j /∈ ˆφ, then that assignment must be a MAP state because 0 is the
global minimum for any potential. Therefore, if we can identify a set of potentials that
is small, such that all the other potentials are 0 in a MAP state, then we can perform
MAP inference in a reduced amount of time. Of course, identifying this set is as hard as
MAP inference itself, but we can iteratively grow the set by starting with an initial set,
performing inference over the current set, adding any potentials that have nonzero values,
and repeating.

Since the lazy inference procedure requires that the assignment be feasible, there are
two ways to handle any constraints in the HL-MRF. One is to include all constraints in
the inference problem from the beginning. This strategy ensures feasibility, but the idea of
lazy grounding can also be extended to constraints to improve performance further. Just
as we check if potentials are unsatisﬁed, i.e., nonzero, we can also check if constraints are
unsatisﬁed, i.e., violated. So the algorithm now iteratively grows the set of active potentials
and active constraints, adding any that are unsatisﬁed until the MAP state of the HL-MRF
deﬁned by the active potentials and constraints is also a feasible MAP state of the true
HL-MRF.

The eﬃciency of lazy MAP inference can be improved heuristically by not adding all
unsatisﬁed potentials and constraints, but instead only adding those that are unsatisﬁed by
some threshold. This heuristic can decrease computational cost signiﬁcantly, although the
results are no longer guaranteed to be correct. Bounding the resulting error when possible
is an important direction for future work.

5.4 Evaluation of MAP Inference

In this section we evaluate the empirical performance of our MAP inference algorithm.4 We
compare its running times against those of MOSEK,5 a commercial convex optimization
toolkit that uses interior-point methods (IPMs). We conﬁrm the results of Yanover et al.
(2006) that IPMs do not scale well to large structured-prediction problems, and we show
that our MAP inference algorithm scales much better. In fact, we observe that our method
scales linearly in practice with the number of potentials and constraints in the HL-MRF.

We evaluate scalability by generating social networks of varying sizes, constructing HL-
MRFs over them, and measuring the running time required to ﬁnd a MAP state. We
compare our algorithm to MOSEK’s IPM. The social networks we generate are designed to
be representative of common social-network analysis tasks. We generate networks of users
that are connected by diﬀerent types of relationships, such as friendship and marriage, and
our goal is to predict the political preferences, e.g., liberal or conservative, of each user. We
also assume that we have local information about each user, representing features such as
demographic information.

We generate the social networks using power-law distributions according to a procedure
described by Broecheler et al. (2010b). For a target number of users N , in-degrees and out-
degrees d for each edge type are sampled from the power-law distribution D(k) ≡ αk−γ.
Incoming and outgoing edges of the same type are then matched randomly to create edges
until no more matches are possible. The number of users is initially the target number

4. Code is available at https://github.com/stephenbach/bach-jmlr17-code.
5. http://www.mosek.com

33

Bach, Broecheler, Huang, and Getoor

plus the expected number of users with zero edges, and then users without any edges are
removed. We use six edge types with various parameters to represent relationships in social
networks with diﬀerent combinations of abundance and exclusivity, choosing γ between 2
and 3, and α between 0 and 1, as suggested by Broecheler et al. We then annotate each
vertex with a value in [−1, 1] uniformly at random to represent local features indicating one
political preference or the other.

We generate social networks with between 22k and 66k vertices, which induce HL-
MRFs with between 130k and 397k total potentials and constraints. In all the HL-MRFs,
roughly 85% of those totals are potentials. For each social network, we create both a (log)
piecewise-linear HL-MRF (pj = 1, ∀j = 1, . . . , m in Deﬁnition 3) and a piecewise-quadratic
one (pj = 2, ∀j = 1, . . . , m). We weight local features with a parameter of 0.5 and choose
parameters in [0, 1] for the relationship potentials representing a mix of more and less
inﬂuential relationships.

We implement ADMM in Java and compare with the IPM in MOSEK (version 6) by
encoding the entire MPE problem as a linear program or a second-order cone program as
appropriate and passing the encoded problem via the Java native interface wrapper. All
experiments are performed on a single machine with a 4-core 3.4 GHz Intel Core i7-3770
processor with 32GB of RAM. Each optimizer used a single thread, and all results are
averaged over 3 runs.

We ﬁrst evaluate the scalability of ADMM when solving piecewise-linear MAP problems
and compare with MOSEK’s interior-point method. Figures 1a (normal scale) and 1c (log
scale) show the results. The running time of the IPM quickly explodes as the problem
size increases. The IPM’s average running time on the largest problem is about 2,200
seconds (37 minutes). This result demonstrates the limited scalability of the interior-point
method. In contrast, ADMM displays excellent scalability. The average running time on
the largest problem is about 70 seconds. Further, the running time appears to grow linearly
in the number of potential functions and constraints in the HL-MRF, i.e., the number of
subproblems that must be solved at each iteration. The line of best ﬁt for all runs on all sizes
has a coeﬃcient of determination R2 = 0.9972. Combined with Figure 1a, this shows that
ADMM scales linearly with increasing problem size in this experiment. We emphasize that
the implementation of ADMM is research code written in Java and the IPM is a commercial
package compiled to native machine code.

We then evaluate the scalability of ADMM when solving piecewise-quadratic MAP prob-
lem and again compare with MOSEK. Figures 1b (normal scale) and 1d (log scale) show
the results. Again, the running time of the interior-point method quickly explodes. We
can only test it on the three smallest problems, the largest of which took an average of
about 21k seconds to solve (over 6 hours). ADMM again scales linearly to the problem
(R2 = 0.9854). It is just as fast for quadratic problems as linear ones, taking average of
about 70 seconds on the largest problem.

One of the advantages of IPMs is great numerical stability and accuracy. Consensus
optimization, which treats both objective terms and constraints as subproblems, often re-
turns solutions that are only optimal and feasible to moderate precision for non-trivially
constrained problems (Boyd et al., 2011). Although this is often acceptable, we quantify
the mix of infeasibility and suboptimality by repairing the infeasibility and measuring the
resulting total suboptimality. We ﬁrst project the solutions returned by consensus opti-

34

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

(a) Linear MAP problems

(b) Quadratic MAP problems

(c) Linear MAP problems (log scale)

(d) Quadratic MAP problems (log scale)

Figure 1: Average running times to ﬁnd a MAP state for HL-MRFs.

35

Bach, Broecheler, Huang, and Getoor

mization onto the feasible region, which took a negligible amount of computational time.
Let pADMM be the value of the objective in Problem (45) at such a point and let pIPM be
the value of the objective at the solution returned by the IPM. Then the relative error on
that problem is (pADMM − pIPM)/pIPM. The relative error was consistently small; it varied
between 0.2% and 0.4%, and did not trend upward as the problem size increased. This
shows that ADMM was accurate, in addition to being much more scalable.

6. Weight Learning

In this section we present three weight learning methods for HL-MRFs, each with a diﬀerent
objective function. The ﬁrst method approximately maximizes the likelihood of the training
data. The second method maximizes the pseudolikelihood. The third method ﬁnds a large-
margin solution, preferring weights that discriminate the ground truth from other nearby
states. Since weights are often shared among many potentials deﬁned by a template, such
as all the groundings of a PSL rule, we describe these learning algorithms in terms of
templated HL-MRFs. We introduce some necessary notation for HL-MRF templates. Let
T = (t1, . . . , ts) denote a vector of templates with associated weights W = (W1, . . . , Ws).
We partition the potentials by their associated templates and let tq also denote the set of
indices of the potentials deﬁned by that template. So, j ∈ tq is a shorthand for saying
that the potential φj(y, x) was deﬁned by template tq. Then, we refer to the sum of the
potentials deﬁned by a template as

Φq(y, x) =

φj(y, x) .

(cid:88)

j∈tq

In the deﬁned HL-MRF, the weight of the j-th hinge-loss potential is set to the weight of
the template from which it was derived, i.e., wj = Wq, for each j ∈ tq. Equivalently, we can
rewrite the hinge-loss energy function as

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)).

fw(y, x) = W (cid:62)Φ(y, x) ,

6.1 Structured Perceptron and Approximate Maximum Likelihood Estimation

The canonical approach for learning parameters W is to maximize the log-likelihood of
training data. The partial derivative of the log-likelihood with respect to a parameter Wq
is

∂ log P (y|x)
∂Wq

= EW [Φq(y, x)] − Φq(y, x),

(61)

where EW is the expectation under the distribution deﬁned by W . For a smoother ascent,
it is often helpful to divide the q-th component of the gradient by the number of groundings
|tq| of the q-th template (Lowd and Domingos, 2007), which we do in our experiments.
Computing the expectation is intractable, so we use a common approximation (e.g., Collins,
2002; Singla and Domingos, 2005; Poon and Domingos, 2011): the values of the potentials
at the most probable setting of y with the current parameters, i.e., a MAP state. Using a
MAP state makes this learning approach a structured variant of voted perceptron (Collins,

36

(59)

(60)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

2002), and we expect it to do best when the space of explored distributions has relatively
low entropy. Following voted perceptron, we take steps of ﬁxed length in the direction of
the gradient, then average the points after all steps. Any step that is outside the feasible
region is projected back before continuing.

6.2 Maximum Pseudolikelihood Estimation

An alternative to structured perceptron is maximum-pseudolikelihood estimation (MPLE)
(Besag, 1975), which maximizes the likelihood of each variable conditioned on all other
variables, i.e.,

P ∗(y|x) =

P ∗(yi|MB(yi), x)

=

1
Zi(W , y, x)

exp (cid:2)−f i

w(yi, y, x)(cid:3) ;

Zi(W , y, x) =

exp (cid:2)−f i

f i
w(yi, y, x) =

wjφj

w(yi, y, x)(cid:3) ;
(cid:16)

{yi ∪ y\i}, x

(cid:17)

.

n
(cid:89)

i=1
n
(cid:89)

i=1
(cid:90)

yi
(cid:88)

j:i∈φj

(62)

(63)

(64)

(65)

Here, i ∈ φj means that yi is involved in φj, and MB(yi) denotes the Markov blanket of
yi—that is, the set of variables that co-occur with yi in any potential function. The partial
derivative of the log-pseudolikelihood with respect to Wq is

∂ log P ∗(y|x)
∂Wq

=





Eyi|MB

n
(cid:88)

i=1

(cid:88)

j∈tq:i∈φj



φj(y, x)

 − Φq(y, x) .

(66)

Computing the pseudolikelihood gradient does not require joint inference and takes time
linear in the size of y. However, the integral in the above expectation does not readily admit
a closed-form antiderivative, so we approximate the expectation. When a variable is uncon-
strained, the domain of integration is a one-dimensional interval on the real number line,
so Monte Carlo integration quickly converges to an accurate estimate of the expectation.

We can also apply MPLE when the constraints are not too interdependent. For example,
for linear equality constraints over disjoint groups of variables (e.g., variable sets that must
sum to 1.0), we can block-sample the constrained variables by sampling uniformly from a
simplex. These types of constraints are often used to represent categorical labels. We can
compute accurate estimates quickly because these blocks are typically low-dimensional.

6.3 Large-Margin Estimation

A diﬀerent approach to learning drops the probabilistic interpretation of the model and
views HL-MRF inference as a prediction function. Large-margin estimation (LME) shifts
the goal of learning from producing accurate probabilistic models to instead producing
accurate MAP predictions. The learning task is then to ﬁnd weights W that separate
the ground truth from other nearby states by a large margin. We describe in this section

37

Bach, Broecheler, Huang, and Getoor

a large-margin method based on the cutting-plane approach for structural support vector
machines (Joachims et al., 2009).

The intuition behind large-margin structured prediction is that the ground-truth state
should have energy lower than any alternate state by a large margin. In our setting, the
output space is continuous, so we parameterize this margin criterion with a continuous loss
function. For any valid output state ˜y, a large-margin solution should satisfy

fw(y, x) ≤ fw(˜y, x) − L(y, ˜y), ∀˜y,

(67)

where the loss function L(y, ˜y) measures the disagreement between a state ˜y and the train-
ing label state y. A common assumption is that the loss function decomposes over the
prediction components, i.e., L(y, ˜y) = (cid:80)
i L(yi, ˜yi). In this work, we use the (cid:96)1 distance
as the loss function, so L(y, ˜y) = (cid:80)
i (cid:107)yi − ˜yi(cid:107)1. Since we do not expect all problems to
be perfectly separable, we relax the large-margin constraint with a penalized slack ξ. We
obtain a convex learning objective for a large-margin solution

min
W ≥0

1
2

||W ||2 + Cξ

s.t. W (cid:62)(Φ(y, x) − Φ(˜y, x)) ≤ −L(y, ˜y) + ξ, ∀˜y,

(68)

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)) and C > 0 is a user-speciﬁed parameter. This for-
mulation is analogous to the margin-rescaling approach by Joachims et al. (2009). Though
such a structured objective is natural and intuitive, its number of constraints is the car-
dinality of the output space, which here is inﬁnite. Following their approach, we optimize
subject to the inﬁnite constraint set using a cutting-plane algorithm: we greedily grow a set
K of constraints by iteratively adding the worst-violated constraint given by a separation
oracle, then updating W subject to the current constraints. The goal of the cutting-plane
approach is to eﬃciently ﬁnd the set of active constraints at the solution for the full ob-
jective, without having to enumerate the inﬁnite inactive constraints. The worst-violated
constraint is

arg min
˜y

W (cid:62)Φ(˜y, x) − L(y, ˜y).

(69)

The separation oracle performs loss-augmented inference by adding additional potentials to
the HL-MRF. For ground truth in {0, 1}, these loss-augmenting potentials are also examples
of hinge-losses, and thus adding them simply creates an augmented HL-MRF. The worst-
violated constraint is then computed as standard inference on the loss-augmented HL-
MRF. However, ground truth values in the interior (0, 1) cause any distance-based loss to
be concave, which require the separation oracle to solve a non-convex objective. In this
case, we use the diﬀerence of convex functions algorithm (An and Tao, 2005) to ﬁnd a local
optimum. Since the concave portion of the loss-augmented inference objective pivots around
the ground truth value, the subgradients are 1 or −1, depending on whether the current
value is greater than the ground truth. We simply choose an initial direction for interior
labels by rounding, and ﬂip the direction of the subgradients for variables whose solution
states are not in the interval corresponding to the subgradient direction until convergence.

38

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Given a set K of constraints, we solve the SVM objective as in the primal form

||W ||2 + Cξ

min
W ≥0

1
2

s.t. K.

(70)

We then iteratively invoke the separation oracle to ﬁnd the worst-violated constraint. If
this new constraint is not violated, or its violation is within numerical tolerance, we have
found the max-margin solution. Otherwise, we add the new constraint to K, and repeat.

One fact of note is that the large-margin criterion always requires some slack for HL-
MRFs with squared potentials. Since the squared hinge potential is quadratic and the loss
is linear, there always exists a small enough distance from the ground truth such that an
absolute (i.e., linear) distance is greater than the squared distance. In these cases, the slack
parameter trades oﬀ between the peakedness of the learned quadratic energy function and
the margin criterion.

6.4 Evaluation of Learning

To demonstrate the ﬂexibility and eﬀectiveness of learning with HL-MRFs, we test them
on four diverse tasks: node labeling, link labeling, link prediction, and image completion.6
Each of these experiments represents a problem domain that is best solved with structured-
prediction approaches because their dependencies are highly structural. The experiments
show that HL-MRFs perform as well as or better than canonical approaches.

For these diverse tasks, we compare against a number of competing methods. For node
and link labeling, we compare HL-MRFs to discrete Markov random ﬁelds (MRFs). We
construct them with Markov logic networks (MLNs) (Richardson and Domingos, 2006),
which template discrete MRFs using logical rules similarly to PSL. We perform inference in
discrete MRFs using Gibbs sampling, and we ﬁnd approximate MAP states during learn-
ing using the search algorithm MaxWalkSat (Richardson and Domingos, 2006). For link
prediction for preference prediction, a task that is inherently continuous and nontrivial to
encode in discrete logic, we compare against Bayesian probabilistic matrix factorization
(BPMF) (Salakhutdinov and Mnih, 2008). Finally, for image completion, we run the same
experimental setup as Poon and Domingos (2011) and compare against the results they
report, which include tests using sum product networks, deep belief networks (Hinton and
Salakhutdinov, 2006), and deep Boltzmann machines (Salakhutdinov and Hinton, 2009).

We train HL-MRFs and discrete MRFs with all three learning methods: structured per-
ceptron (SP), maximum pseudolikelihood estimation(MPLE), and large-margin estimation
(LME). When appropriate, we evaluate statistical signiﬁcance using a paired t-test with re-
jection threshold 0.01. We describe the HL-MRFs used for our experiments using the PSL
rules that deﬁne them. To investigate the diﬀerences between linear and squared potentials
we use both in our experiments. HL-MRF-L refers to a model with all linear potentials
and HL-MRF-Q to one with all squared potentials. When training with SP and MPLE, we
use 100 gradient steps and a step size of 1.0 (unless otherwise noted), and we average the
iterates as in voted perceptron. For LME, we set C = 0.1. We experimented with various
settings, but the scores of HL-MRFs and discrete MRFs were not sensitive to changes.

6. Code is available at https://github.com/stephenbach/bach-jmlr17-code.

39

Bach, Broecheler, Huang, and Getoor

6.4.1 Node Labeling

When classifying documents, links between those documents—such as hyperlinks, citations,
or shared authorship—provide extra signal beyond the local features of individual docu-
ments. Collectively predicting document classes with these links tends to improve accuracy
(Sen et al., 2008). We classify documents in citation networks using data from the Cora
and Citeseer scientiﬁc paper repositories. The Cora data set contains 2,708 papers in seven
categories, and 5,429 directed citation links. The Citeseer data set contains 3,312 papers
in six categories, and 4,591 directed citation links. Let the predicate Category/2 represent
the category of each document and Cites/2 represent a citation from one document to
another.

The prediction task is, given a set of seed documents whose labels are observed, to infer
the remaining document classes by propagating the seed information through the network.
For each of 20 runs, we split the data sets 50/50 into training and testing partitions, and
seed half of each set. To predict discrete categories with HL-MRFs we predict the category
with the highest predicted value.

We compare HL-MRFs to discrete MRFs on this task. For prediction, we performed
2500 rounds of Gibbs sampling, 500 of which were discarded as burn-in. We construct both
using the same logical rules, which simply encode the tendency for a class to propagate
across citations. For each category "C i", we have the following two rules, one for each
direction of citation.

Category(A, "C i") && Cites(A, B) -> Category(B, "C i")

Category(A, "C i") && Cites(B, A) -> Category(B, "C i")

We also constrain the atoms of the Category/2 predicate to sum to 1.0 for a given document
as follows.

Category(D, +C) = 1.0 .

Table 1 lists the results of this experiment. HL-MRFs are the most accurate predictors on
both data sets. Both variants of HL-MRFs are also much faster than discrete MRFs. See
Table 3 for average inference times over ﬁve folds.

6.4.2 Link Labeling

An emerging problem in the analysis of online social networks is the task of inferring the
level of trust between individuals. Predicting the strength of trust relationships can provide
useful information for viral marketing, recommendation engines, and internet security. HL-
MRFs with linear potentials have been applied by Huang et al. (2013) to this task, showing
superior results with models based on sociological theory. We reproduce their experimen-
tal setup using their sample of the signed Epinions trust network, orginally collected by
Richardson et al. (2003), in which users indicate whether they trust or distrust other users.
We perform eight-fold cross-validation. In each fold, the prediction algorithm observes the
entire unsigned social network and all but 1/8 of the trust ratings. We measure predic-
tion accuracy on the held-out 1/8. The sampled network contains 2,000 users, with 8,675
signed links. Of these links, 7,974 are positive and only 701 are negative, making it a sparse
prediction task.

40

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Table 1: Average accuracy of classiﬁcation by HL-MRFs and discrete MRFs. Scores statis-
tically equivalent to the best scoring method are typed in bold.

Table 2: Average area under ROC and precision-recall curves of social-trust prediction by
HL-MRFs and discrete MRFs. Scores statistically equivalent to the best scoring method
by metric are typed in bold.

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

Citeseer Cora

0.729
0.729
0.683

0.724
0.729
0.695

0.686
0.715
0.687

0.816
0.818
0.789

0.802
0.808
0.789

0.756
0.797
0.783

ROC P-R (+) P-R (-)

0.822
HL-MRF-Q (SP)
HL-MRF-Q (MPLE) 0.832
0.814
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

0.765
0.757
0.783

0.655
0.725
0.795

0.978
0.979
0.976

0.965
0.963
0.967

0.942
0.963
0.973

0.452
0.482
0.462

0.357
0.333
0.453

0.270
0.298
0.441

41

Table 3: Average inference times (reported in seconds) of single-threaded HL-MRFs and
discrete MRFs.

Bach, Broecheler, Huang, and Getoor

Citeseer

Cora Epinions

HL-MRF-Q
HL-MRF-L
MRF

0.42
0.46
110.96

0.70
0.50
184.32

0.32
0.28
212.36

We use a model based on the social theory of structural balance, which suggests that so-
cial structures are governed by a system that prefers triangles that are considered balanced.
Balanced triangles have an odd number of positive trust relationships; thus, considering all
possible directions of links that form a triad of users, there are sixteen logical implications
of the following form.

Trusts(A,B) && Trusts(B,C) -> Trusts(A,C)

Huang et al. (2013) list all sixteen of these rules, a reciprocity rule, and a prior in their
Balance-Recip model, which we omit to save space.

Since we expect these structural implications to vary in accuracy, learning weights for
these rules provides better models. Again, we use these rules to deﬁne HL-MRFs and
discrete MRFs, and we train them using various learning algorithms. For inference with
discrete MRFs, we perform 5000 rounds of Gibbs sampling, of which the ﬁrst 500 are
burn-in. We compute three metrics: the area under the receiver operating characteristic
(ROC) curve, and the areas under the precision-recall curves for positive trust and negative
trust. On all three metrics, HL-MRFs with squared potentials score signiﬁcantly higher.
The diﬀerences among the learning methods for squared HL-MRFs are insigniﬁcant, but
the diﬀerences among the models is statistically signiﬁcant for the ROC metric. For area
under the precision-recall curve for positive trust, discrete MRFs trained with LME are
statistically tied with the best score, and both HL-MRF-L and discrete MRFs trained with
LME are statistically tied with the best area under the precision-recall curve for negative
trust. The results are listed in Table 2.

Though the random fold splits are not the same, using the same experimental setup,
Huang et al. (2013) also scored the precision-recall area for negative trust of standard trust
prediction algorithms EigenTrust (Kamvar et al., 2003) and TidalTrust (Golbeck, 2005),
which scored 0.131 and 0.130, respectively. The logical models based on structural balance
that we run here are signiﬁcantly more accurate, and HL-MRFs more than discrete MRFs.
In addition to comparing favorably with regard to predictive accuracy, inference in HL-
MRFs is also much faster than in discrete MRFs. Table 3 lists average inference times
on ﬁve folds of three prediction tasks: Cora, Citeseer, and Epinions. This illustrates an
important diﬀerence between performing structured prediction via convex inference versus
sampling in a discrete prediction space: convex inference can be much faster.

42

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

6.4.3 Link Prediction

Preference prediction is the task of inferring user attitudes (often quantiﬁed by ratings)
toward a set of items. This problem is naturally structured, since a user’s preferences
are often interdependent, as are an item’s ratings. Collaborative ﬁltering is the task of
predicting unknown ratings using only a subset of observed ratings. Methods for this
task range from simple nearest-neighbor classiﬁers to complex latent factor models. More
generally, this problem is an instance of link prediction, since the goal is to predict links
indicating preference between users and content. Since preferences are ordered rather than
Boolean, it is natural to represent them with the continuous variables of HL-MRFs, with
higher values indicating greater preference. To illustrate the versatility of HL-MRFs, we
design a simple, interpretable collaborative ﬁltering model for predicting humor preferences.
We test this model on the Jester dataset, a repository of ratings from 24,983 users on a set
of 100 jokes (Goldberg et al., 2001). Each joke is rated on a scale of [−10, +10], which we
normalize to [0, 1]. We sample a random 2,000 users from the set of those who rated all 100
jokes, which we then split into 1,000 train and 1,000 test users. From each train and test
matrix, we sample a random 50% to use as the observed features x; the remaining ratings
are treated as the variables y.

Our HL-MRF model uses an item-item similarity rule:

SimRating(J1, J2) && Likes(U, J1) -> Likes(U, J2)

where J1 and J2 are jokes and U is a user; the predicate Likes/2 indicates the degree
of preference (i.e., rating value); and SimRating/2 is a closed predicate that measures
the mean-adjusted cosine similarity between the observed ratings of two jokes. We also
include the following rules to enforce that Likes(U,J) concentrates around the observed
average rating of user U (represented with the predicate AvgUserRating/1) and item J
(represented with the predicate AvgJokeRating/1), and the global average (represented
with the predicate AvgRating/1).

AvgUserRating(U) -> Likes(U, J)

Likes(U, J) -> AvgUserRating(U)

AvgJokeRating(J) -> Likes(U, J)

Likes(U, J) -> AvgJokeRating(J)

AvgRating("constant") -> Likes(U, J)

Likes(U, J) -> AvgRating("constant")

The atom AvgRating("constant") takes a placeholder constant as an argument, since there
is only one grounding of it for the entire HL-MRF. Again, all three of these predicates are
closed and computed using averages of observed ratings. In all cases, the observed ratings
are taken only from the training data for learning (to avoid leaking information about the
test data) and only from the test data during testing.

We compare our HL-MRF model to a canonical latent factor model, Bayesian proba-
bilistic matrix factorization (BPMF) (Salakhutdinov and Mnih, 2008). BPMF is a fully
Bayesian treatment and is therefore considered “parameter-free;” the only parameter that
must be speciﬁed is the rank of the decomposition. Based on settings used by Xiong et al.

43

Table 4: Normalized mean squared/absolute errors (NMSE/NMAE) for preference predic-
tion using the Jester dataset. The lowest errors are typed in bold.

Bach, Broecheler, Huang, and Getoor

NMSE NMAE

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

0.0554
0.0549
0.0738

0.0578
0.0535
0.0544

0.1974
0.1953
0.2297

0.2021
0.1885
0.1875

BPMF

0.0501 0.1832

Table 5: Mean squared errors per pixel for image completion. HL-MRFs produce the
most accurate completions on the Caltech101 and the left-half Olivetti faces, and only sum-
product networks produce better completions on Olivetti bottom-half faces. Scores for other
methods are reported in Poon and Domingos (2011).

HL-MRF-Q (SP) SPN DBM DBN PCA NN

Caltech-Left
Caltech-Bottom
Olivetti-Left
Olivetti-Bottom

1741
1910
927
1226

1815
1924
942
918

2998
2656
1866
2401

4960
3447
2386
1931

2851
1944
1076
1265

2327
2575
1527
1793

(2010), we set the rank of the decomposition to 30 and use 100 iterations of burn in and 100
iterations of sampling. For our experiments, we use the code of Xiong et al. (2010). Since
BPMF does not train a model, we allow BPMF to use all of the training matrix during the
prediction phase.

Table 4 lists the normalized mean squared error (NMSE) and normalized mean absolute
error (NMAE), averaged over 10 random splits. Though BPMF produces the best scores,
the improvement over HL-MRF-L (LME) is not signiﬁcant in NMAE.

6.4.4 Image Completion

Digital image completion requires models that understand how pixels relate to each other,
such that when some pixels are unobserved, the model can infer their values from parts of the
image that are observed. We construct pixel-grid HL-MRFs for image completion. We test
these models using the experimental setup of Poon and Domingos (2011): we reconstruct
images from the Olivetti face data set and the Caltech101 face category. The Olivetti data
set contains 400 images, 64 pixels wide and tall, and the Caltech101 face category contains
435 examples of faces, which we crop to the center 64 by 64 patch, as was done by Poon

44

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Figure 2: Example results on image completion of Caltech101 (left) and Olivetti (right)
faces. From left to right in each column: (1) true face, left side predictions by (2) HL-
MRFs and (3) SPNs, and bottom half predictions by (4) HL-MRFs and (5) SPNs. SPN
completions are downloaded from Poon and Domingos (2011).

and Domingos (2011). Following their experimental setup, we hold out the last ﬁfty images
and predict either the left half of the image or the bottom half.

The HL-MRFs in this experiment are much more complex than the ones in our other
experiments because we allow each pixel to have its own weight for the following rules,
which encode agreement or disagreement between neighboring pixels:

Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

where Bright("P ij", I) is the normalized brightness of pixel "P ij" in image I, and
North("P ij", Q) indicates that Q is the north neighbor of "P ij". We similarly include
analogous rules for the south, east, and west neighbors, as well as the pixels mirrored across
the horizontal and vertical axes. This setup results in up to 24 rules per pixel, (boundary
pixels may not have north, south, east, or west neighbors) which, in a 64 by 64 image,
produces 80,896 PSL rules.

We train these HL-MRFs using SP with a 5.0 step size on the ﬁrst 200 images of each data
set and test on the last ﬁfty. For training, we maximize the data log-likelihood of uniformly
random held-out pixels for each training image, allowing for generalization throughout the
image. Table 5 lists our results and others reported by Poon and Domingos (2011) for sum-
product networks (SPN), deep Boltzmann machines (DBM), deep belief networks (DBN),
principal component analysis (PCA), and nearest neighbor (NN). HL-MRFs produce the
best mean squared error on the left- and bottom-half settings for the Caltech101 set and
the left-half setting in the Olivetti set. Only sum product networks produce lower error

45

Bach, Broecheler, Huang, and Getoor

on the Olivetti bottom-half faces. Some reconstructed faces are displayed in Figure 2,
where the shallow, pixel-based HL-MRFs produce comparably convincing images to sum-
product networks, especially in the left-half setting, where HL-MRFs can learn which pixels
are likely to mimic their horizontal mirror. While neither method is particularly good at
reconstructing the bottom half of faces, the qualitative diﬀerence between the deep SPN
and the shallow HL-MRF completions is that SPNs seem to hallucinate diﬀerent faces, often
with some artifacts, while HL-MRFs predict blurry shapes roughly the same pixel intensity
as the observed, top half of the face. The tendency to better match pixel intensity helps
HL-MRFs score better quantitatively on the Caltech101 faces, where the lighting conditions
are more varied than in Olivetti faces.

Training and predicting with these HL-MRFs takes little time.

In our experiments,
training each model takes about 45 minutes on a 12-core machine, while predicting takes
under a second per image. While Poon and Domingos (2011) report faster training with
SPNs, both HL-MRFs and SPNs clearly belong to a class of faster models when compared
to DBNs and DBMs, which can take days to train on modern hardware.

7. Related Work

Researchers in artiﬁcial intelligence and machine learning have long been interested in pre-
dicting interdependent unknowns using structural dependencies. Some of the earliest work
in this area is inductive logic programming (ILP) (Muggleton and De Raedt, 1994), in
which structural dependencies are described with ﬁrst-order logic. Using ﬁrst-order logic
has several advantages. First, it can capture many types of dependencies among variables,
such as correlations, anti-correlations, and implications. Second, it can compactly specify
dependencies that hold across many diﬀerent sets of propositions by using variables as wild-
cards that match entities in the data. These features enable the construction of intuitive,
general-purpose models that are easily applicable or adapted to diﬀerent domains. Inference
for ILP ﬁnds the propositions that satisfy a query, consistent with a relational knowledge
base. However, ILP is limited by its diﬃculty in coping with uncertainty. Standard ILP
approaches only model dependencies which hold universally, and such dependencies are rare
in real-world data.

Another broad area of research, probabilistic methods, directly models uncertainty over
unknowns. Probabilistic graphical models (PGMs) (Koller and Friedman, 2009) are a fam-
ily of formalisms for specifying joint distributions over interdependent unknowns through
graphical structures. The graphical structure of a PGM generally represents conditional
independence relationships among random variables. Explicitly representing conditional
independence relationships allows a distribution to be more compactly parametrized. For
example, in the worst case, a discrete distribution could be represented by an exponen-
tially large table over joint assignments to the random variables. However, describing the
distribution in smaller, conditionally independent pieces can be much more compact. Sim-
ilar beneﬁts apply to continuous distributions. Algorithms for probabilistic inference and
learning can also operate over the conditionally independent pieces described by the graph
structure. They are therefore straightforward to apply to a wide variety of distributions.
Categories of PGMs include Markov random ﬁelds (MRFs), Bayesian networks (BNs), and

46

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

dependency networks (DNs). Constructing PGMs often requires careful design, and models
are usually constructed for single tasks and data sets.

More recently, researchers have sought to combine the advantages of relational and
probabilistic approaches, creating the ﬁeld of statistical relational learning (SRL) (Getoor
and Taskar, 2007). SRL techniques build probabilistic models of relational data, i.e., data
composed of entities and relationships connecting them. Relational data is most often
described using a relational calculus, but SRL techniques are also equally applicable to
similar categories of data that go by other names, such as graph data or network data.
Modeling relational data is inherently complicated by the large number of interconnected
and overlapping structural dependencies that are typically present. This complication has
motivated two directions of work. The ﬁrst direction is algorithmic, seeking inference and
learning methods that scale up to high dimensional models. The other direction is both
user-oriented and—as a growing body of evidence shows—supported by learning theory,
seeking formalisms for compactly specifying entire groups of dependencies in the model
that share both form and parameters. Specifying these grouped dependencies, often in the
form of templates via a domain-speciﬁc language, is convenient for users. Most often in
relational data the structural dependencies hold without regard to the identities of entities,
instead being induced by an entity’s class (or classes) and the structure of its relationships
with other entities. Therefore, many SRL models and languages give users the ability to
specify dependencies in this abstract form and ground out models over speciﬁc data sets
based on these deﬁnitions. In addition to convenience, recent work in learning theory says
that repeated dependencies with tied parameters can be the key to generalizing from a
few—or even one—large, structured training example(s) (London et al., 2016).

A related ﬁeld to SRL is structured prediction (SP) (Bakir et al., 2007; Nowozin et al.,
2016), which generalizes the tasks of classiﬁcation and regression to the task of predict-
ing structured objects. The loss function used during learning is generalized to a task-
appropriate loss function that scores disagreement between predictions and the true struc-
tures. Often, models for structured prediction take the form of energy functions that are
linear in their parameters. Therefore, prediction with such models is equivalent to MAP
inference for MRFs. A distinct branch of SP is learn-to-search methods, in which the prob-
lem is decomposed into a series of one-dimension prediction problems. The challenge is to
learn a good order in which to predict the components of the structure, so that each one-
dimension prediction problem can be conditioned on the most useful information. Examples
of learn-to-search methods include incremental structured perceptron (Collins and Roark,
2004), SEARN (Daum´e III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross
and Bagnell, 2014).

In this paper we focus on SP methods that perform joint prediction directly. Better
understanding the diﬀerences and relative advantages of joint-prediction methods and learn-
to-search methods is an important direction for future work.
In the rest of this section
we survey models and domain-speciﬁc languages for SP and SRL (Section 7.1), inference
methods (Section 7.2), and learning methods (Section 7.3).

47

Bach, Broecheler, Huang, and Getoor

7.1 Models and Languages

SP and SRL encompass many approaches. One broad area of work—of which PSL is a
part—uses ﬁrst-order logic and other relational formalisms to specify templates for PGMs.
Probabilistic relational models (Friedman et al., 1999) deﬁne templates for BNs in terms of a
database schema, and they can be grounded out over instances of that schema to create BNs.
Relational dependency networks (Neville and Jensen, 2007) template RNs using structured
query language (SQL) queries over a relational schema. Markov logic networks (MLNs)
(Richardson and Domingos, 2006) use ﬁrst-order logic to deﬁne Boolean MRFs. Each logical
clause in a ﬁrst-order knowledge base is a template for a set of potentials when the MLN
is grounded out over a set of propositions. Whether each proposition is true is a Boolean
random variable, and the potential has a value of one when the corresponding ground clause
is satisﬁed by the propositions and zero when it is not. (MLNs are formulated such that
higher values of the energy function are more probable.) Clauses can either be weighted,
in which case the potential has the weight of the clause that templated it, or unweighted,
in which case in must hold universally, as in ILP. In these ways, MLNs are similar to
PSL. Whereas MLNs are deﬁned over Boolean variables, PSL is a templating language
for HL-MRFs, which are deﬁned over continuous variables. However, these continuous
variables can be used to model discrete quantities. See Section 2 for more information
on the relationships between HL-MRFs and discrete MRFs, and Section 6.4 for empirical
comparisons between the two. As we show, HL-MRFs and PSL scale much better while
In addition,
retaining the rich expressivity and accuracy of their discrete counterparts.
HL-MRFs and PSL can reason directly about continuous data.

PSL is part of a broad family of probabilistic programming languages (Gordon et al.,
2014). The goals of probabilistic programming and SRL often overlap. Probabilistic pro-
gramming seeks to make constructing probabilistic models easy for the end user, and sep-
arate model speciﬁcation from the development of inference and learning algorithms.
If
algorithms can be developed for the entire space of models covered by a language, then it
is easy for users to experiment with including and excluding diﬀerent model components.
It also makes it easy for existing models to beneﬁt from improved algorithms. Separation
of model speciﬁcation and algorithms is also useful in SRL for the same reasons. In this
paper we emphasize designing algorithms that are ﬂexible enough to support the full class of
HL-MRFs. Examples of probabilistic programming languages include IBAL (Pfeﬀer, 2001),
BLOG (Milch et al., 2005), Markov logic (Richardson and Domingos, 2006), ProbLog (De
Raedt et al., 2007), Church (Goodman et al., 2008), Figaro (Pfeﬀer, 2009), FACTORIE
(McCallum et al., 2009), Anglican (Wood et al., 2014), and Edward (Tran et al., 2016).

Other formalisms have also been proposed for probabilistic reasoning over continuous
domains and other domains equipped with semirings. Hybrid Markov logic networks (Wang
and Domingos, 2008) mix discrete and continuous variables. In addition to the dependencies
over discrete variables supported by MLNs, they support soft equality constraints between
two variables of the same form as those deﬁned by squared arithmetic rules in PSL, as well
as linear potentials of the form y1 − y2 for a soft inequality constraint y1 > y2. Inference in
hybrid MLNs is intractable. Wang and Domingos (2008) propose a random walk algorithm
for approximate MAP inference. Another related formalism is aProbLog (Kimmig et al.,
2011), which generalizes ProbLog to allow clauses to be annotated with elements from a

48

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

semiring, generalizing ProbLog’s support for clauses annotated with probabilities. Many
common inference tasks can be generalized from this perspective as algebraic model counting
(Kimmig et al., 2016). The PITA system (Riguzzi and Swift, 2011) for probabilistic logic
programming can also be viewe as implementing inference over various semirings.

7.2 Inference

Whether viewed as MAP inference for an MRF or SP without probabilistic semantics,
searching over a structured space to ﬁnd the optimal prediction is an important but diﬃcult
task. It is NP-hard in general (Shimony, 1994), so much work has focused on approximations
and identifying classes of problems for which it is tractable. A well-studied approximation
technique is local consistency relaxation (LCR) (Wainwright and Jordan, 2008). Inference
is ﬁrst viewed as an equivalent optimization over the realizable expected values of the
potentials, called the marginal polytope. When the variables are discrete and each potential
is an indicator that a subset of variables is in a certain state, this optimization becomes a
linear program. Each variable in the program is the marginal probability that a variable
is a particular state or the variables associated with a potential are in a particular joint
state. The marginal polytope is then the set of marginal probabilities that are globally
consistent. The number of linear constraints required to deﬁne the marginal polytope is
exponential in the size of the problem, however, so the linear program has to be relaxed in
order to be tractable. In a local consistency relaxation, the marginal polytope is relaxed
to the local polytope, in which the marginals over variables and potential states are only
locally consistent in the sense that each marginal over potential states sums to the marginal
distributions over the associated variables.

A large body of work has focused on solving the LCR objective quickly. Typically,
oﬀ-the-shelf convex optimization methods do not scale well for large graphical models and
structured predictors (Yanover et al., 2006), so a large branch of research has investigated
highly scalable message-passing algorithms. One approach is dual decomposition (DD)
(Sontag et al., 2011), which solves a problem dual to the LCR objective. Many DD algo-
rithms use coordinate descent, such as TRW-S (Kolmogorov, 2006), MSD (Werner, 2007),
MPLP (Globerson and Jaakkola, 2007), and ADLP (Meshi and Globerson, 2011). Other
DD algorithms use subgradient-based approaches (e.g., Jojic et al., 2010; Komodakis et al.,
2011; Schwing et al., 2012).

Another approach to solving the LCR objective uses message-passing algorithms to
solve the problem directly in its primal form. One well-known algorithm is that of Raviku-
mar et al. (2010a), which uses proximal optimization, a general approach that iteratively
improves the solution by searching for nearby improvements. The authors also provide
rounding guarantees for when the relaxed solution is integral, i.e., the relaxation is tight,
allowing the algorithm to converge faster. Another message-passing algorithm that solves
the primal objective is AD3 (Martins et al., 2015), which uses the alternating direction
method of multipliers (ADMM). AD3 optimizes objective (10) for binary, pairwise MRFs
and supports the addition of certain deterministic constraints on the variables. A third ex-
ample of a primal message-passing algorithm is APLP (Meshi and Globerson, 2011), which
is the primal analog of ADLP. Like AD3, it uses ADMM to optimize the objective.

49

Bach, Broecheler, Huang, and Getoor

Other approaches to approximate inference include tighter linear programming relax-
ations (Sontag et al., 2008, 2012). These tighter relaxations enforce local consistency on
variable subsets that are larger than individual variables, which makes them higher-order
local consistency relaxations. Mezuman et al. (2013) developed techniques for special cases
of higher-order relaxations, such as when the MRF contains cardinality potentials, in which
the probability of a conﬁguration depends on the number of variables in a particular state.
Researchers have also explored nonlinear convex programming relaxations, e.g., Ravikumar
and Laﬀerty (2006) and Kumar et al. (2006).

Previous analyses have identiﬁed particular subclasses whose local consistency relax-
ations are tight, i.e., the maximum of the relaxed program is exactly the maximum of the
original problem. These special classes include graphical models with tree-structured depen-
dencies, models with submodular potential functions, models encoding bipartite matching
problems, and those with nand potentials and perfect graph structures (Wainwright and
Jordan, 2008; Schrijver, 2003; Jebara, 2009; Foulds et al., 2011). Researchers have also
studied performance guarantees of other subclasses of the ﬁrst-order local consistency re-
laxation. Kleinberg and Tardos (2002) and Chekuri et al. (2005) considered the metric
labeling problem. Feldman et al. (2005) used the local consistency relaxation to decode
binary linear codes.

In this paper we examine the classic problem of MAX SAT—ﬁnding a joint Boolean
assignment to a set of propositions that maximizes the sum of a set of weighted clauses
that are satisﬁed—as an instance of SP. Researchers have also considered approaches to
solving MAX SAT other than the one one we study, the randomized algorithm of Goemans
and Williamson (1994). One line of work focusing on convex programming relaxations has
obtained stronger rounding guarantees than Goemans and Williamson (1994) by using non-
linear programming, e.g., Asano and Williamson (2002) and references therein. Other work
does not use the probabilistic method but instead searches for discrete solutions directly,
e.g., Mills and Tsang (2000), Larrosa et al. (2008), and Choi et al. (2009). We note that
one such approach, that of Wah and Shang (1997), is essentially a type of DD formulated
for MAX SAT. A more recent approach blends convex programming and discrete search via
mixed integer programming (Davies and Bacchus, 2013). Additionally, Huynh and Mooney
(2009) introduced a linear programming relaxation for MLNs inspired by MAX SAT re-
laxations, but the relaxation of general Markov logic provides no known guarantees on the
quality of solutions.

Finally, lifted inference takes advantage of symmetries in probability distributions to re-
duce the amount of work required for inference. Some of the earliest approaches identiﬁed
repeated dependency structures in PGMs to avoid repeated computations (Koller and Pfef-
fer, 1997; Pfeﬀer et al., 1999). Lifted inference has been widely applied in SRL because the
templates that are commonly used to deﬁne PGMs often induce symmetries. Various infer-
ence techniques for discrete MRFs have been extended to a lifted approach, including belief
propagation (Jaimovich et al., 2007; Singla and Domingos, 2008; Kersting et al., 2009) and
Gibbs sampling (Venugopal and Gogate, 2012). Approaches to lifted convex optimization
(Mladenov et al., 2012) might be extended to HL-MRFs. See de Salvo Braz et al. (2007),
Kersting (2012), and Kimmig et al. (2015) for more information on lifted inference.

50

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

7.3 Learning

Taskar et al. (2004) connected SP and PGMs by showing how to train MRFs with large-
margin estimation, a generalization of the large-margin objective for binary classiﬁcation
used to train support vector machines (Vapnik, 2000). Large-margin learning is a well-
studied approach to train structured predictors because it directly incorporates the struc-
tured loss function into a convex upper bound on the true objective: the regularized ex-
pected risk. The learning objective is to ﬁnd the parameters with smallest norm such that
a linear combination of feature functions assign a better score to the training data than
all other possible predictions. The amount by which the score of the correct prediction
must exceed the score of other predictions is scaled using the structured loss function.
The objective is therefore encoded as a norm minimization problem subject to many linear
constraints, one for each possible prediction in the structured space.

Structured SVMs (Tsochantaridis et al., 2005) extend large-margin estimation to a broad
class of structured predictors and admit a tractable cutting-plane learning algorithm. This
algorithm will terminate in a number of iterations linear in the size of the problem, and so
the computational challenge of large-margin learning for structured prediction comes down
to the task of ﬁnding the most violated constraint in the learning objective. This can be
accomplished by optimizing the energy function plus the loss function. In other words, the
task is to ﬁnd the structure that is the best combination of being favored by the energy
function but unfavored by the loss function. Often, the loss function decomposes over the
components of the prediction space, so the combined energy function and loss function can
often be viewed as simply the energy function of another structured predictor that is equally
challenging or easy to optimize, such as when the space of structures is a set of discrete
vectors and the loss function is the Hamming distance.

It is common during large-margin estimation that no setting of the parameters can
predict all the training data without error. In this case, the training data is said to not
be separable, again generalizing the notion of linear separability in the feature space from
binary classiﬁcation. The solution to this problem is to add slack variables to the constraints
that require the training data to be assigned the best score. The magnitude of the slack
variables are penalized in the learning objective, so estimation must trade oﬀ between the
norm of the parameters and violating the constraints. Joachims et al. (2009) extend this
formulation to a “one slack” formulation, in which a single slack variable is used for all the
constraints across all training examples, which is more eﬃcient. We use this framework for
large-margin estimation for HL-MRFs in Section 6.3.

The repeated inferences required for large-margin learning, one to ﬁnd the most-violated
constraint at each iteration, can become computationally expensive. Therefore researchers
have explored speeding up learning by interleaving the inference problem with the learning
problem. In the cutting-plane formulation discussed above, the objective is equivalently a
saddle-point problem, with the solution at the minimum with respect to the parameters and
the maximum with respect to the inference variables. Taskar et al. (2005) proposed dualizing
the inner inference problem to form a joint minimization. For SP problems with a tight
duality gap, i.e., the dual problem has the same optimal value as the primal problem, this
approach leads to an equivalent, convex optimization that can be solved for all variables
In other words, the learning and most-violated constraint problems are
simultaneously.

51

Bach, Broecheler, Huang, and Getoor

solved simultaneously, greatly reducing training time. For problems with non-tight duality
gaps, e.g., MAP inference in general, discrete MRFs, Meshi et al. (2010) showed that
the same principle can be applied by using approximate inference algorithms like dual
decomposition to bound the primal objective.

A related problem to parameter learning is structure learning, i.e., identifying an ac-
curate dependency structure for a model. A common SRL approach is searching over the
space of templates for PGMs. For probabilistic relational models, Friedman et al. (1999)
learned structures described in the vocabulary of relational schemas. For models that are
templated with ﬁrst-order-logic-like languages, such as PSL and MLNs, these approaches
take the form of rule learning. Based on rule-learning techniques from inductive logic pro-
gramming (e.g., Richards and Mooney, 1992; De Raedt and Dehaspe, 1996) a series of
approaches have sought to learn MLN rules from relational data. Initially, Kok and Domin-
gos (2005) learned rules by generating candidates and performing a beam search to identify
rules that improved a weighted pseudolikelihood objective. Then, Mihalkova and Mooney
(2007) observed that the previous approach generated candidate rules without regard to
the data, so they introduced an approach that used the data to guide the proposal of rules
via relational pathﬁnding. Kok and Domingos (2010) improved on that by ﬁrst perform-
ing graph clustering to ﬁnd common motifs, which are common subgraphs, to guide rule
proposal. They observed that modifying a rule set one clause at a time often got stuck
in poor local optima, and by using the motifs as reﬁnement operators instead, they were
able to converge to better optima. Other approaches to structure learning search directly
over grounded PGMs, including (cid:96)1-regularized pseudolikelihood maximization (Ravikumar
et al., 2010b) and grafting (Perkins et al., 2003; Zhu et al., 2010). These methods can all
be extended to HL-MRFs and PSL.

8. Conclusion

In this paper we introduced HL-MRFs, a new class of probabilistic graphical models that
unite and generalize several approaches to modeling relational and structured data: Boolean
logic, probabilistic graphical models, and fuzzy logic. HL-MRFs can capture relaxed, prob-
abilistic inference with Boolean logic and exact, probabilistic inference with fuzzy logic,
making them useful models for both discrete and continuous data. HL-MRFs also general-
ize these inference techniques with additional expressivity, allowing for even more ﬂexibility.
HL-MRFs are a signiﬁcant addition to the the library of machine learning tools because
they embody a useful point in the spectrum of models that trade oﬀ between scalability
and expressivity. As we showed, they can be easily applied to a wide range of structured
problems in machine learning and achieve high-quality predictive performance, competitive
with or surpassing the performance of canonical approaches. However, these other models
either do not scale as well, like discrete MRFs, or are not as versatile in their ability to
capture a wide range of problems, like Bayesian probabilistic matrix factorization.

We also introduced PSL, a probabilistic programming language for HL-MRFs. PSL
makes HL-MRFs easy to design, allowing users to encode their ideas for structural depen-
dencies using an intuitive syntax based on ﬁrst-order logic. PSL also helps accelerate a
time-consuming aspect of the modeling process: reﬁning a model. In contrast with other
types of models that require specialized inference and learning algorithms depending on

52

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

which structural dependencies are included, HL-MRFs can encode many types of depen-
dencies and scale well with the same inference and learning algorithms. PSL makes it easy
to quickly add, remove, and modify dependencies in the model and rerun inference and
learning, allowing users to quickly improve the quality of their models. Finally, because
PSL uses a ﬁrst-order syntax, each PSL program actually speciﬁes an entire class of HL-
MRFs, parameterized by the particular data set over which it is grounded. Therefore, a
model or components of a model reﬁned for one data set can easily be applied to others.

Next, we introduced inference and learning algorithms that scale to large problems. The
MAP inference algorithm is far more scalable than standard tools for convex optimization
because it leverages the sparsity that is so common to the dependencies in structured
prediction. The supervised learning algorithms extend standard learning objectives to HL-
MRFs. Together, this combination of an expressive formalism, a user-friendly probabilistic
programming language, and highly scalable algorithms enables researchers and practitioners
to easily build large-scale, accurate models of relational and structured data.7

This paper also lays the foundation for many lines of future work. Our analysis of local
consistency relaxation (LCR) as a hierarchical optimization is a general proof technique,
and it could be used to derive compact forms for other LCR objectives. As in the case of
MRFs deﬁned using logical clauses, such compact forms can simplify analysis and could
lead to a greater understanding of LCR for other classes of MRFs. Another important line
of work is understanding what guarantees apply to the MAP states of HL-MRFs. Can
anything be said about their ability to approximate MAP inference in discrete models that
go beyond the models already covered by the known rounding guarantees? Future directions
also include developing new algorithms for HL-MRFs. One important direction is marginal
inference for HL-MRFs and algorithms for sampling from them. Unlike marginal inference
for discrete distributions, which computes the marginal probability that a variable is in a
particular state, marginal inference for HL-MRFs requires ﬁnding the marginal probability
that a variable is in a particular range. One option for doing so, as well as generating samples
from HL-MRFs, is to extend the hit-and-run sampling scheme of Broecheler and Getoor
(2010). This method was developed for continuous constrained MRFs with piecewise-linear
potentials. There are also many new domains to which HL-MRFs and PSL can be applied.
With these modeling tools, researchers can design and apply new solutions to structured
prediction problems.

Acknowledgments

We acknowledge the many people who have contributed to the development of HL-MRFs
and PSL. Contributors include Eriq Augustine, Shobeir Fakhraei, James Foulds, Angelika
Kimmig, Stanley Kok, Ben London, Hui Miao, Lilyana Mihalkova, Dianne P. O’Leary, Jay
Pujara, Arti Ramesh, Theodoros Rekatsinas, and V.S. Subrahmanian. This work was sup-
ported by NSF grants CCF0937094 and IIS1218488, and IARPA via DoI/NBC contract
number D12PC00337. The U.S. Government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those of the authors and should

7. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

53

Bach, Broecheler, Huang, and Getoor

not be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.

Appendix A. Proof of Theorem 2

In this appendix, we prove the equivalence of objectives (7) and (10). Our proof analyzes
the local consistency relaxation to derive an equivalent, more compact optimization over
only the variable pseudomarginals µ that is identical to the MAX SAT relaxation. Since the
variables are Boolean, we refer to each pseudomarginal µi(1) as simply µi. Let xF
j denote
the unique setting such that φj(xF
j ) = 0. (I.e., xF
is the setting in which each literal in the
j
clause Cj is false.)

We begin by reformulating the local consistency relaxation as a hierarchical optimiza-
tion, ﬁrst over the variable pseudomarginals µ and then over the factor pseudomarginals θ.
Due to the structure of local polytope L, the pseudomarginals µ parameterize inner linear
programs that decompose over the structure of the MRF, such that—given ﬁxed µ—there is
an independent linear program ˆφj(µ) over θj for each clause Cj. We rewrite objective (10)
as

arg max
µ∈[0,1]n

(cid:88)

ˆφj(µ),

Cj ∈C

where

ˆφj(µ) = max
θj

wj

(cid:88)

θj(xj)

xj |xj (cid:54)=xF
j

(cid:88)

such that

θj(xj) = µi

θj(xj) = 1 − µi

xj |xj (i)=1
(cid:88)

xj |xj (i)=0
(cid:88)

θj(xj) = 1

xj
θj(xj) ≥ 0

∀i ∈ I +
j

∀i ∈ I −
j

∀xj .

(71)

(72)

(73)

(74)

(75)

(76)

It is straightforward to verify that objectives (10) and (71) are equivalent for MRFs with
disjunctive clauses for potentials. All constraints deﬁning L can be derived from the con-
straint µ ∈ [0, 1]n and the constraints in the deﬁnition of ˆφj(µ). We have omitted redundant
constraints to simplify analysis.

To make this optimization more compact, we replace each inner linear program ˆφj(µ)
with an expression that gives its optimal value for any setting of µ. Deriving this expression
j of ˆφj(µ), which is guaranteed to exist because
requires reasoning about any maximizer θ(cid:63)
problem (72) is bounded and feasible8 for any parameters µ ∈ [0, 1]n and wj.

We ﬁrst derive a suﬃcient condition for the linear program to not be fully satisﬁable, in
the sense that it cannot achieve a value of wj, the maximum value of the weighted potential

8. Setting θj(xj) to the probability deﬁned by µ under the assumption that the elements of xj are inde-

pendent, i.e., the product of the pseudomarginals, is always feasible.

54

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

wjφj(x). Observe that, by the objective (72) and the simplex constraint (75), showing that
ˆφj(µ) is not fully satisﬁable is equivalent to showing that θ(cid:63)

j (xF

j ) > 0.

Lemma 16 If

µi +

(1 − µi) < 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

then θ(cid:63)

j (xF

j ) > 0.

Proof By the simplex constraint (75),

Also, by summing all the constraints (73) and (74),

µi +

(1 − µi) <

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) .

(cid:88)

xj

(cid:88)

θ(cid:63)
j (xj) ≤

xj |xj (cid:54)=xF
j

µi +

(1 − µi) ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

because all the components of θ(cid:63) are nonnegative, and—except for θ(cid:63)
at least once in constraints (73) and (74). These bounds imply

j (xF

j )—they all appear

(cid:88)

xj |xj (cid:54)=xF
j

θ(cid:63)
j (xj) <

θ(cid:63)
j (xj) ,

(cid:88)

xj

which means θ(cid:63)

j (xF

j ) > 0, completing the proof.

We next show that if ˆφj(µ) is parameterized such that it is not fully satisﬁable, as in

Lemma 16, then its optimum always takes a particular value deﬁned by µ.

Lemma 17 If wj > 0 and θ(cid:63)

j (xF

j ) > 0, then

(cid:88)

θ(cid:63)
j (xj) =

xj |xj (cid:54)=xF
j

µi +

(1 − µi) .

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

Proof We prove the lemma via the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939;
Kuhn and Tucker, 1951). Since problem (72) is a maximization of a linear function subject
to linear constraints, the KKT conditions are necessary and suﬃcient for any optimum θ(cid:63)
j .
Before writing the relevant KKT conditions, we introduce some necessary notation. For
a state xj, we need to reason about the variables that disagree with the unsatisﬁed state
xF

j . Let

(cid:110)

d(xj) (cid:44)

i ∈ I +

j ∪ I −

j |xj(i) (cid:54)= xF

j (i)

(cid:111)

be the set of indices for the variables that do not have the same value in the two states xj
and xF
j .

We now write the relevant KKT conditions for θ(cid:63)

j . Let λ, α be real-valued vectors where
j | + 1 and |α| = |θj|. Let each λi correspond to a constraint (73) or (74)

j | + |I −

|λ| = |I +

55

Bach, Broecheler, Huang, and Getoor

for i ∈ I +
correspond to a constraint (76) for each xj. Then, the following KKT conditions hold:

j , and let λ∆ correspond to the simplex constraint (75). Also, let each αxj

j ∪ I −

αxj ≥ 0
αxj θ(cid:63)
λ∆ + αxF
j
(cid:88)

j (xj) = 0
= 0

wj +

i∈d(xj )

λi + λ∆ + αxj = 0

∀xj (cid:54)= xF
j

.

∀xj
∀xj

(77)

(78)

(79)

(80)

Since θ(cid:63)

j (xF

j ) > 0, by condition (78), αxF

= 0. By condition (79), then λ∆ = 0. From
here we can bound the other elements of λ. Observe that for every i ∈ I +
j , there
exists a state xj such that d(xj) = {i}. Then, it follows from condition (80) that there
exists xj such that, for every i ∈ I +

j ∪ I −

j

j ∪ I −
j ,

wj + λi + λ∆ + αxj = 0 .

Since αxj ≥ 0 by condition (77) and λ∆ = 0, it follows that λi ≤ −wj. With these bounds,
we show that, for any state xj, if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0. Assume that for some state
xj, |d(xj)| ≥ 2. By condition (80) and the derived constraints on λ,

αxj ≥ (|d(xj)| − 1)wj > 0 .
j (xj) = 0. Next, observe that for all i ∈ I +
With condition (78), θ(cid:63)
j ) and for
j
any state xj, if d(xj) = {i}, then xj(i) = 1 (resp. xj(i) = 0), and for any other state x(cid:48)
j
such that x(cid:48)
j) ≥ 2. By constraint (73) (resp. constraint (74)),
θ(cid:63)(xj) = µi (resp. θ(cid:63)(xj) = 1 − µi).
We have shown that if θ(cid:63)
j ), then θ(cid:63)

j ) > 0, then for all states xj, if d(xj) = {i} and i ∈ I +
j
j (xj) = 1 − µi), and if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0.

j (xF
j (xj) = µi (resp. θ(cid:63)

j(i) = 1 (resp. x(cid:48)

j(i) = 0), d(x(cid:48)

(resp. i ∈ I −

(resp. i ∈ I −
This completes the proof.

Lemma 16 says if (cid:80)

(1 − µi) < 1, then ˆφj(µ) is not fully satisﬁable,
and Lemma 17 provides its optimal value. We now reason about the other case, when
(cid:80)
(1 − µi) ≥ 1, and we show that this condition is suﬃcient to ensure that

µi + (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

i∈I +
j

i∈I −
j

ˆφj(µ) is fully satisﬁable.

Lemma 18 If wj > 0 and

j (xF

j ) = 0.

then θ(cid:63)
Proof We prove the lemma by contradiction. Assume that wj > 0, (cid:80)
µi) ≥ 1, and that the lemma is false, θ(cid:63)
j ) > 0. Then, by Lemma 17,

µi + (cid:80)

(1 −

i∈I −
j

i∈I +
j

µi +

(1 − µi) ≥ 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) ≥ 1 .

j (xF
(cid:88)

xj |xj (cid:54)=xF
j

56

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The assumption that θ(cid:63)

j (xF

j ) > 0 implies

θ(cid:63)
j (xj) > 1,

(cid:88)

xj

j ) < 0 is excluded by the nonnegativity constraints (76).

which is a contradiction, since it violates the simplex constraint (75). The possibility that
θ(cid:63)
j (xF
For completeness and later convenience, we also state the value of ˆφj(µ) when it is fully
satisﬁable.

Lemma 19 If θ(cid:63)

j (xF

j ) = 0, then

(cid:88)

θ(cid:63)
j (xj) = 1 .

xj |xj (cid:54)=xF
j

Proof The lemma follows from the simplex constraint (75).

We can now combine the previous lemmas into a single expression for the value of ˆφj(µ).

Lemma 20 For any feasible setting of µ,

ˆφj(µ) = wj min

µi +

(1 − µi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






Proof The lemma is trivially true if wj = 0 since any assignment will yield zero value. If
wj > 0, then we consider two cases. In the ﬁrst case, if (cid:80)
(1 − µi) < 1,
then, by Lemmas 16 and 17,

µi + (cid:80)

i∈I −
j

i∈I +
j

In the second case, if (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

(1 − µi) ≥ 1, then, by Lemmas 18 and 19,

ˆφj(µ) = wj






(cid:88)

i∈I +
j



µi +

(1 − µi)


 .

(cid:88)

i∈I −
j

ˆφj(µ) = wj .

By factoring out wj, we can rewrite this piecewise deﬁnition of ˆφj(µ) as wj multiplied by
the minimum of (cid:80)

(1 − µi) and 1, completing the proof.

µi + (cid:80)

i∈I +
j

i∈I −
j

This leads to our ﬁnal equivalence result.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

57

Bach, Broecheler, Huang, and Getoor

Proof Substituting the solution of the inner optimization from Lemma 20 into the local
consistency relaxation objective (71) gives a projected optimization over only µ which is
identical to the MAX SAT relaxation objective (7).

References

2008.

A. Abdelbar and S. Hedetniemi. Approximating MAPs for belief networks is NP-hard and

other theorems. Artiﬁcial Intelligence, 102(1):21–38, 1998.

N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, third edition,

D. Alshukaili, A. A. A. Fernandes, and N. W. Paton. Structuring linked data search results
using probabilistic soft logic. In International Semantic Web Conference (ISWC), 2016.

L. An and P. Tao. The DC (diﬀerence of convex functions) programming and DCA revisited
with DC models of real world nonconvex optimization problems. Annals of Operations
Research, 133:23–46, 2005.

T. Asano and D. P. Williamson. Improved approximation algorithms for MAX SAT. J.

Algorithms, 42(1):173–202, 2002.

S. H. Bach, M. Broecheler, L. Getoor, and D. P. O’Leary. Scaling MPE inference for con-
strained continuous Markov random ﬁelds. In Advances in Neural Information Processing
Systems (NIPS), 2012.

S. H. Bach, B. Huang, B. London, and L. Getoor. Hinge-loss Markov random ﬁelds: Convex
inference for structured prediction. In Uncertainty in Artiﬁcial Intelligence (UAI), 2013.

S. H. Bach, B. Huang, J. Boyd-Graber, and L. Getoor. Paired-dual learning for fast training
of latent variable hinge-loss MRFs. In International Conference on Machine Learning
(ICML), 2015a.

S. H. Bach, B. Huang, and L. Getoor. Unifying local consistency and MAX SAT relaxations
for scalable inference with rounding guarantees. In Artiﬁcial Intelligence and Statistics
(AISTATS), 2015b.

G. Bakir, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan,

editors. Predicting Structured Data. MIT Press, 2007.

I. Beltagy, K. Erk, and R. J. Mooney. Probabilistic soft logic for semantic textual similarity.

In Annual Meeting of the Association for Computational Linguistics (ACL), 2014.

J. Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society,

24(3):179–195, 1975.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and
Statistical Learning Via the Alternating Direction Method of Multipliers. Now Publishers,
2011.

58

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

M. Broecheler and L. Getoor. Computing marginal distributions over continuous Markov
networks for statistical relational learning. In Advances in Neural Information Processing
Systems (NIPS), 2010.

M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2010a.

M. Broecheler, P. Shakarian, and V. S. Subrahmanian. A scalable framework for modeling

competitive diﬀusion in social networks. In Social Computing (SocialCom), 2010b.

C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and
approximation algorithms for the metric labeling problem. SIAM J. Discrete Math., 18
(3):608–625, 2005.

P. Chen, F. Chen, and Z. Qian. Road traﬃc congestion monitoring in social media with
In IEEE International Conference on Data Mining

hinge-loss Markov random ﬁelds.
(ICDM), 2014.

A. Choi, T. Standley, and A. Darwiche. Approximating weighted Max-SAT problems by
compensating for relaxations. In International Conference on Principles and Practice of
Constraint Programming, 2009.

M. Collins. Discriminative training methods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Empirical Methods in Natural Language Processing
(EMNLP), 2002.

M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2004.

H. Daum´e III, J. Langford, and D. Marcu. Search-based structured prediction. Machine

Learning, 75(3):297–325, 2009.

J. Davies and F. Bacchus. Exploiting the power of MIP solvers in MAXSAT. In M. J¨arvisalo
and A. Van Gelder, editors, Theory and Applications of Satisﬁability Testing – SAT 2013,
Lecture Notes in Computer Science, pages 166–181. Springer Berlin Heidelberg, 2013.

L. De Raedt and L. Dehaspe. Clausal discovery. Machine Learning, 26:1058–1063, 1996.

L. De Raedt, A. Kimmig, and H. Toivonen. ProbLog: A probabilistic Prolog and its
application in link discovery. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

R. de Salvo Braz, E. Amir, and D. Roth. Lifted ﬁrst-order probabilistic inference.

In
L. Getoor and B. Taskar, editors, Introduction to statistical relational learning, pages
433–451. MIT Press, 2007.

L. Deng and J. Wiebe. Joint prediction for entity/event-level sentiment analysis using
probabilistic soft logic models. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2015.

59

Bach, Broecheler, Huang, and Getoor

J. Ebrahimi, D. Dou, and D. Lowd. Weakly supervised tweet stance classiﬁcation by rela-
tional bootstrapping. In Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), 2016.

S. Fakhraei, B. Huang, L. Raschid, and L. Getoor. Network-based drug-target interac-
tion prediction with probabilistic soft logic. IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 2014.

J. Feldman, M. J. Wainwright, and D. R. Karger. Using linear programming to decode

binary linear codes. Information Theory, IEEE Trans. on, 51(3):954–972, 2005.

J. Foulds, N. Navaroli, P. Smyth, and A. Ihler. Revisiting MAP estimation, message passing

and perfect graphs. In AI & Statistics, 2011.

J. Foulds, S. Kumar, and L. Getoor. Latent topic networks: A versatile probabilistic pro-
gramming framework for topic models. In International Conference on Machine Learning
(ICML), 2015.

N. Friedman, L. Getoor, D. Koller, and A. Pfeﬀer. Learning probabilistic relational models.

In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1999.

D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems
via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1):17–
40, 1976.

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simpliﬁed NP-complete graph

problems. Theoretical Computer Science, 1(3):237–267, 1976.

L. Getoor and B. Taskar, editors. Introduction to statistical relational learning. MIT press,

2007.

L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning probabilistic models of link

structure. Journal of Machine Learning Research (JMLR), 3:679–707, 2002.

A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms
for MAP LP-relaxations. In Advances in Neural Information Processing Systems (NIPS),
2007.

R. Glowinski and A. Marrocco. Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et la
r´esolution, par p´enalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires.
Revue fran¸caise d’automatique, informatique, recherche op´erationnelle, 9(2):41–76, 1975.

M. X. Goemans and D. P. Williamson. New 3/4-approximation algorithms for the maximum

satisﬁability problem. SIAM J. Discrete Math., 7(4):656–666, 1994.

J. Golbeck. Computing and Applying Trust in Web-based Social Networks. PhD thesis,

University of Maryland, 2005.

K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collabo-

rative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.

60

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church:
A language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.

A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. Probabilistic programming.

In International Conference on Software Engineering (ICSE, FOSE track), 2014.

G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science, 313(5786):504–507, 2006.

B. Huang, A. Kimmig, L. Getoor, and J. Golbeck. A ﬂexible framework for probabilistic
models of social trust. In Conference on Social Computing, Behavioral-Cultural Modeling,
& Prediction (SBP), 2013.

T. Huynh and R. Mooney. Max-margin weight learning for Markov logic networks.

In

European Conference on Machine Learning (ECML), 2009.

A. Jaimovich, O. Meshi, and N. Friedman. Template based inference in symmetric relational

Markov random ﬁelds. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.

T. Jebara. MAP estimation, message passing, and perfect graphs. In Uncertainty in Arti-

ﬁcial Intelligence (UAI), 2009.

Learning, 77(1):27–59, 2009.

T. Joachims, T. Finley, and C. Yu. Cutting-plane training of structural SVMs. Machine

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In

International Conference on Machine Learning (ICML), 2010.

S. Kamvar, M. Schlosser, and H. Garcia-Molina. The eigentrust algorithm for reputation
In International Conference on the World Wide Web

management in P2P networks.
(WWW), 2003.

W. Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints.

Master’s thesis, University of Chicago, 1939.

K. Kersting. Lifted probabilistic inference. In European Conference on Artiﬁcial Intelligence

(ECAI), 2012.

K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Uncertainty in

Artiﬁcial Intelligence (UAI), 2009.

A. Kimmig, G. Van den Broeck, and L. De Raedt. An algebraic Prolog for reasoning about

possible worlds. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2011.

A. Kimmig, L. Mihalkova, and L. Getoor. Lifted graphical models: A survey. Machine

Learning, 99:1–45, 2015.

Applied Logic, 2016.

A. Kimmig, G. Van den Broeck, and L. De Raedt. Algebraic model counting. Journal of

61

Bach, Broecheler, Huang, and Getoor

J. Kleinberg and ´E. Tardos. Approximation algorithms for classiﬁcation problems with
pairwise relationships: Metric labeling and Markov random ﬁelds. J. ACM, 49(5):616–
639, 2002.

G. J. Klir and B. Yuan. Fuzzy Sets and Fuzzy Logic: Theory and Applications. Prentice

Hall, 1995.

S. Kok and P. Domingos. Learning the structure of Markov logic networks. In International

Conference on Machine Learning (ICML), 2005.

S. Kok and P. Domingos. Learning Markov logic networks using structural motifs.

In

International Conference on Machine Learning (ICML), 2010.

D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.

MIT Press, 2009.

Intelligence (UAI), 1997.

D. Koller and A. Pfeﬀer. Object-oriented Bayesian networks. In Uncertainty in Artiﬁcial

V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. Pat-

tern Analysis and Machine Intelligence, IEEE Trans. on, 28(10):1568–1583, 2006.

N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via
dual decomposition. Pattern Analysis and Machine Intelligence, IEEE Trans. on, 33(3):
531–552, 2011.

P. Kouki, S. Fakhraei, J. Foulds, M. Eirinaki, and L. Getoor. HyPER: A ﬂexible and
extensible probabilistic framework for hybrid recommender systems. In ACM Conference
on Recommender Systems (RecSys), 2015.

H. W. Kuhn and A. W. Tucker. Nonlinear programming.

In Berkeley Symp. on Math.

Statist. and Prob., 1951.

M. P. Kumar, P. H. S. Torr, and A. Zisserman. Solving Markov random ﬁelds using sec-
ond order cone programming relaxations. In Computer Vision and Pattern Recognition
(CVPR), 2006.

N. Landwehr, A. Passerini, L. De Raedt, and P. Frasconi. Fast learning of relational kernels.

Machine Learning, 78(3):305–342, 2010.

J. Larrosa, F. Heras, and S. de Givry. A logical approach to eﬃcient Max-SAT solving.

Artiﬁcial Intelligence, 172(2-3):204–233, 2008.

J. Li, A. Ritter, and D. Jurafsky. Inferring user preferences by probabilistic logical reasoning

over social networks. arXiv preprint arXiv:1411.2679, 2014.

S. Liu, K. Liu, S. He, and J. Zhao. A probabilistic soft logic based approach to exploiting
latent and global information in event classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2016.

62

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

B. London, S. Khamis, S. H. Bach, B. Huang, L. Getoor, and L. Davis. Collective activity
In CVPR Workshop on Structured

detection using hinge-loss Markov random ﬁelds.
Prediction: Tractability, Learning and Inference, 2013.

B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

D. Lowd and P. Domingos. Eﬃcient weight learning for Markov logic networks. In Principles

and Practice of Knowledge Discovery in Databases (PKDD), 2007.

S. Magliacane, P. Stutz, P. Groth, and A. Bernstein. FoxPSL: An extended and scalable
In AAAI Spring Symposium on Knowledge Representation and

PSL implementation.
Reasoning: Integrating Symbolic and Neural Approaches, 2015.

A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing.
AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Mod-
els. Journal of Machine Learning Research (JMLR), 16(Mar):495–545, 2015.

A. McCallum, K. Nigam, and L. H. Ungar. Eﬃcient clustering of high-dimensional data
sets with application to reference matching. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2000.

A. McCallum, K. Schultz, and S. Singh. FACTORIE: Probabilistic programming via im-
peratively deﬁned factor graphs. In Advances in Neural Information Processing Systems
(NIPS), 2009.

O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation.

In European Conference on Machine learning (ECML), 2011.

O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning eﬃciently with approximate
In International Conference on Machine Learning (ICML),

inference via dual losses.
2010.

E. Mezuman, D. Tarlow, A. Globerson, and Y. Weiss. Tighter linear program relaxations
for high order graphical models. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2013.

H. Miao, X. Liu, B. Huang, and L. Getoor. A hypergraph-partitioned vertex programming
approach for large-scale consensus optimization. In IEEE International Conference on
Big Data, 2013.

L. Mihalkova and R. J. Mooney. Bottom-up learning of Markov logic network structure. In

International Conference on Machine Learning (ICML), 2007.

B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic
models with unknown objects. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2005.

P. Mills and E. Tsang. Guided local search for solving SAT and weighted MAX-SAT

problems. J. Automated Reasoning, 24(1-2):205–223, 2000.

63

Bach, Broecheler, Huang, and Getoor

M. Mladenov, B. Ahmadi, and K. Kersting. Lifted linear programming. In Artiﬁcial Intel-

ligence & Statistics (AISTATS), 2012.

S. Muggleton and L. De Raedt. Inductive logic programming: Theory and methods. The

Journal of Logic Programming, 19:629–679, 1994.

Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Program-

ming. Society for Industrial and Applied Mathematics, 1994.

J. Neville and D. Jensen. Relational dependency networks. Journal of Machine Learning

Research (JMLR), 8:653–692, 2007.

H. B. Newcombe and J. M. Kennedy. Record linkage: Making maximum use of the discrim-
inating power of identifying information. Communications of the ACM, 5(11):563–566,
1962.

S. Nowozin, P. V. Gehler, J. Jancsary, and C. H. Lampert, editors. Advanced Structured

Prediction. Neural Information Processing. MIT press, 2016.

J. D. Park. Using weighted MAX-SAT engines to solve MPE.

In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2002.

S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by
gradient descent in function space. Journal of Machine Learning Research (JMLR), 3:
1333–1356, 2003.

A. Pfeﬀer. IBAL: A probabilistic rational programming language. In International Joint

Conference on Artiﬁcial Intelligence (IJCAI), 2001.

A. Pfeﬀer. Figaro: An object-oriented probabilistic programming language. Technical

report, Charles River Analytics, 2009.

A. Pfeﬀer, D. Koller, B. Milch, and K. T. Takusagawa. SPOOK: A system for probabilistic
object-oriented knowledge representation. In Uncertainty in Artiﬁcial Intelligence (UAI),
1999.

H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2011.

J. Pujara, H. Miao, L. Getoor, and W. Cohen. Knowledge graph identiﬁcation. In Inter-

national Semantic Web Conference (ISWC), 2013.

A. Ramesh, D. Goldwasser, B. Huang, H. Daum´e III, and L. Getoor. Learning latent
In AAAI Conference on Artiﬁcial

engagement patterns of students in online courses.
Intelligence (AAAI), 2014.

A. Ramesh, S. Kumar, J. Foulds, and L. Getoor. Weakly supervised models of aspect-
sentiment for online course discussion forums. In Annual Meeting of the Association for
Computational Linguistics (ACL), 2015.

64

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

P. Ravikumar and J. Laﬀerty. Quadratic programming relaxations for metric labeling and
Markov random ﬁeld MAP estimation. In International Conference on Machine Learning
(ICML), 2006.

P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured
linear programs: Proximal methods and rounding schemes. Journal of Machine Learning
Research (JMLR), 11:1043–1080, 2010a.

P. Ravikumar, M. J. Wainwright, and J. D. Laﬀerty. High-dimensional Ising model selection
using (cid:96)1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010b.

B. L. Richards and R. J. Mooney. Learning relations by pathﬁnding. In AAAI Conference

on Artiﬁcial Intelligence (AAAI), 1992.

M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2):107–

136, 2006.

M. Richardson, R. Agrawal, and P. Domingos. Trust management for the semantic web.
In D. Fensel, K. Sycara, and J. Mylopoulos, editors, The Semantic Web - ISWC 2003,
volume 2870 of Lecture Notes in Computer Science, pages 351–368. Springer Berlin /
Heidelberg, 2003.

F. Riguzzi and T. Swift. The PITA system: Tabling and answer subsumption for reasoning
under uncertainty. In International Conference on Logic Programming (ICLP), 2011.

S. Ross and J. A. Bagnell. Reinforcement and Imitation Learning via Interactive No-Regret

Learning, 2014.

S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Artiﬁcial Intelligence & Statistics (AISTATS),
2011.

R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Artiﬁcial Intelligence &

Statistics (AISTATS), 2009.

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In International Conference on Machine Learning (ICML), 2008.

M. Samadi, P. Talukdar, M. Veloso, and M. Blum. ClaimEval: Integrated and ﬂexible
In AAAI Conference on

framework for claim evaluation using credibility of sources.
Artiﬁcial Intelligence (AAAI), 2016.

A. Schrijver. Combinatorial Optimization: Polyhedra and Eﬃciency. Springer-Verlag, 2003.

A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally convergent dual MAP
LP relaxation solvers using Fenchel-Young margins. In Advances in Neural Information
Processing Systems (NIPS), 2012.

P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective

classiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.

65

Bach, Broecheler, Huang, and Getoor

S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artiﬁcial Intelligence, 68(2):

399–410, 1994.

P. Singla and P. Domingos. Discriminative training of Markov logic networks. In AAAI

Conference on Artiﬁcial Intelligence (AAAI), 2005.

P. Singla and P. Domingos. Lifted ﬁrst-order belief propagation. In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2008.

D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations
for MAP using message passing. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2008.

D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference.
In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, pages
219–254. MIT Press, 2011.

D. Sontag, D. K. Choe, and Y. Li. Eﬃciently searching for frustrated cycles in MAP

inference. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2012.

D. Sridhar, J. Foulds, M. Walker, B. Huang, and L. Getoor. Joint models of disagreement
and stance in online debate. In Annual Meeting of the Association for Computational
Linguistics (ACL), 2015.

D. Sridhar, S. Fakhraei, and L. Getoor. A probabilistic approach for collective similarity-

based drug-drug interaction prediction. Bioinformatics, 32(20):3175–3182, 2016.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Neural Information

Processing Systems (NIPS), 2004.

B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction
In International Conference on Machine Learning

models: A large margin approach.
(ICML), 2005.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Ed-
inference, and criticism. arXiv preprint

ward: A library for probabilistic modeling,
arXiv:1610.09787, 2016.

I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research
(JMLR), 6:1453–1484, 2005.

V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2000.

D. Venugopal and V. Gogate. On lifting the Gibbs sampling algorithm. In Neural Infor-

mation Processing Systems (NIPS), 2012.

B. W. Wah and Y. Shang. Discrete Lagrangian-based search for solving MAX-SAT prob-

lems. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1997.

M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Varia-

tional Inference. Now Publishers, 2008.

66

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

J. Wang and P. Domingos. Hybrid Markov logic networks. In AAAI Conference on Artiﬁcial

Intelligence (AAAI), 2008.

T. Werner. A linear programming approach to max-sum problem: A review. Pattern

Analysis and Machine Intelligence, IEEE Trans. on, 29(7):1165–1179, 2007.

R. West, H. S. Paskov, J. Leskovec, and C. Potts. Exploiting social network structure for
person-to-person sentiment analysis. Transactions of the Association for Computational
Linguistics (TACL), 2:297–310, 2014.

F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic

programming inference. In Artiﬁcial Intelligence & Statistics (AISTATS), 2014.

M. Wright. The interior-point revolution in optimization: History, recent developments,
and lasting consequences. Bulletin of the American Mathematical Society, 42(1):39–56,
2005.

L. Xiong, X. Chen, T. Huang, J. Schneider, and J. Carbonell. Temporal collaborative ﬁlter-
ing with Bayesian probabilistic tensor factorization. In SIAM International Conference
on Data Mining, 2010.

C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propaga-
tion – An empirical study. Journal of Machine Learning Research (JMLR), 7:1887–1907,
2006.

J. Zhu, N. Lao, and E. P. Xing. Grafting-Light: Fast, Incremental Feature Selection and
Structure Learning of Markov Random Fields. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2010.

67

7
1
0
2
 
v
o
N
 
7
1
 
 
]

G
L
.
s
c
[
 
 
3
v
6
0
4
4
0
.
5
0
5
1
:
v
i
X
r
a

Journal of Machine Learning Research 18 (2017) 1-67

Submitted 12/15; Revised 12/16; Published 10/17

Hinge-Loss Markov Random Fields
and Probabilistic Soft Logic

Stephen H. Bach
Computer Science Department
Stanford University
Stanford, CA 94305, USA

Matthias Broecheler
DataStax

Bert Huang
Computer Science Department
Virginia Tech
Blacksburg, VA 24061, USA

Lise Getoor
Computer Science Department
University of California, Santa Cruz
Santa Cruz, CA 95064, USA

Editor: Luc De Raedt

bach@cs.stanford.edu

matthias@datastax.com

bhuang@vt.edu

getoor@soe.ucsc.edu

Abstract

A fundamental challenge in developing high-impact machine learning technologies is bal-
ancing the need to model rich, structured domains with the ability to scale to big data.
Many important problem areas are both richly structured and large scale, from social and
biological networks, to knowledge graphs and the Web, to images, video, and natural lan-
guage. In this paper, we introduce two new formalisms for modeling structured data, and
show that they can both capture rich structure and scale to big data. The ﬁrst, hinge-
loss Markov random ﬁelds (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes diﬀerent approaches to convex inference. We unite three approaches from
the randomized algorithms, probabilistic graphical models, and fuzzy logic communities,
showing that all three lead to the same inference objective. We then deﬁne HL-MRFs
by generalizing this uniﬁed objective. The second new formalism, probabilistic soft logic
(PSL), is a probabilistic programming language that makes HL-MRFs easy to deﬁne using
a syntax based on ﬁrst-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than general-purpose
convex optimization methods, because it uses message passing to take advantage of sparse
dependency structures. We then show how to learn the parameters of HL-MRFs. The
learned HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at
scales not previously possible.

Keywords:
prediction

Probabilistic graphical models, statistical relational learning, structured

c(cid:13)2017 Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/15-631.html.

Bach, Broecheler, Huang, and Getoor

1. Introduction

In many problems in machine learning, the domains are rich and structured, with many
interdependent elements that are best modeled jointly. Examples include social networks,
biological networks, the Web, natural language, computer vision, sensor networks, and so on.
Machine learning subﬁelds such as statistical relational learning (Getoor and Taskar, 2007),
inductive logic programming (Muggleton and De Raedt, 1994), and structured prediction
(Bakir et al., 2007) all seek to represent dependencies in data induced by relational structure.
With the ever-increasing size of available data, there is a growing need for models that are
highly scalable while still able to capture rich structure.

In this paper, we introduce hinge-loss Markov random ﬁelds (HL-MRFs), a new class of
probabilistic graphical models designed to enable scalable modeling of rich, structured data.
HL-MRFs are analogous to discrete MRFs, which are undirected probabilistic graphical
models in which probability mass is log-proportional to a weighted sum of feature functions.
Unlike discrete MRFs, however, HL-MRFs are deﬁned over continuous variables in the [0, 1]
unit interval. To model dependencies among these continuous variables, we use linear and
quadratic hinge functions, so that probability density is lost according to a weighted sum of
hinge losses. As we will show, hinge-loss features capture many common modeling patterns
for structured data.

When designing classes of models, there is generally a trade oﬀ between scalability and
expressivity: the more complex the types and connectivity structure of the dependencies,
the more computationally challenging inference and learning become. HL-MRFs address
a crucial gap between the two extremes. By using hinge-loss functions to model the de-
pendencies among the variables, which admit highly scalable inference without restrictions
on their connectivity structure, HL-MRFs can capture a wide range of useful relationships.
One reason they are so expressive is that hinge-loss dependencies are at the core of a number
of scalable techniques for modeling both discrete and continuous structured data.

To motivate HL-MRFs, we unify three diﬀerent approaches for scalable inference in
structured models: (1) randomized algorithms for MAX SAT (Goemans and Williamson,
1994), (2) local consistency relaxation (Wainwright and Jordan, 2008) for discrete Markov
random ﬁelds deﬁned using Boolean logic, and (3) reasoning about continuous information
with fuzzy logic. We show that all three approaches lead to the same convex programming
objective. We then deﬁne HL-MRFs by generalizing this uniﬁed inference objective as a
weighted sum of hinge-loss features and using them as the weighted features of graphical
models. Since HL-MRFs generalize approaches that reason about relational data with
weighted logical knowledge bases, they retain the same high level of expressivity. As we
show in Section 6.4, they are eﬀective for modeling both discrete and continuous data.

We also introduce probabilistic soft logic (PSL), a new probabilistic programming lan-
guage that makes HL-MRFs easy to deﬁne and use for large, relational data sets.1 This
idea has been explored for other classes of models, such as Markov logic networks (Richard-
son and Domingos, 2006) for discrete MRFs, relational dependency networks (Neville and
Jensen, 2007) for dependency networks, and probabilistic relational models (Getoor et al.,
2002) for Bayesian networks. We build on these previous approaches, as well as the con-
nection between hinge-loss potentials and logical clauses, to deﬁne PSL. In addition to

1. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

2

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

probabilistic rules, PSL provides syntax that enables users to easily apply many common
modeling techniques, such as domain and range constraints, blocking and canopy functions,
and aggregate variables deﬁned over other random variables.

Our next contribution is to introduce a number of inference and learning algorithms.
First, we examine MAP inference, i.e., the problem of ﬁnding a most probable assignment
to the unobserved random variables. MAP inference in HL-MRFs is always a convex op-
timization. Although any oﬀ-the-shelf optimization toolkit could be used, such methods
typically do not leverage the sparse dependency structures common in graphical models.
We introduce a consensus-optimization approach to MAP inference for HL-MRFs, showing
how the problem can be decomposed using the alternating direction method of multipliers
(ADMM) and how the resulting subproblems can be solved analytically for hinge-loss poten-
tials. Our approach enables HL-MRFs to easily scale beyond the capabilities of oﬀ-the-shelf
optimization software or sampling-based inference in discrete MRFs. We then show how
to learn HL-MRFs from training data using a variety of methods: structured perceptron,
maximum pseudolikelihood, and large margin estimation. Since structured perceptron and
large margin estimation rely on inference as subroutines, and maximum pseudolikelihood
estimation is eﬃcient by design, all of these methods are highly scalable for HL-MRFs. We
evaluate them on core relational learning and structured prediction tasks, such as collec-
tive classiﬁcation and link prediction. We show that HL-MRFs oﬀer predictive accuracy
comparable to analogous discrete models while scaling much better to large data sets.

This paper brings together and expands work on scalable models for structured data that
can be either discrete, continuous, or a mixture of both (Broecheler et al., 2010a; Bach et al.,
2012, 2013, 2015b). The eﬀectiveness of HL-MRFs and PSL has been demonstrated on many
problems, including information extraction (Liu et al., 2016) and automatic knowledge base
construction (Pujara et al., 2013), extracting and evaluating natural-language arguments
on the Web (Samadi et al., 2016), high-level computer vision (London et al., 2013), drug
discovery (Fakhraei et al., 2014) and predicting drug-drug interactions (Sridhar et al., 2016),
natural language semantics (Beltagy et al., 2014; Sridhar et al., 2015; Deng and Wiebe,
2015; Ebrahimi et al., 2016), automobile-traﬃc modeling (Chen et al., 2014), recommender
systems (Kouki et al., 2015), information retrieval (Alshukaili et al., 2016), and predicting
attributes (Li et al., 2014) and trust (Huang et al., 2013; West et al., 2014) in social networks.
The ability to easily incorporate latent variables into HL-MRFs and PSL (Bach et al., 2015a)
has enabled further applications, including modeling latent topics in text (Foulds et al.,
2015), and predicting student outcomes in massive open online courses (MOOCs) (Ramesh
et al., 2014, 2015). Researchers have also studied how to make HL-MRFs and PSL even
more scalable by developing distributed implementations (Miao et al., 2013; Magliacane
et al., 2015). That they are already being widely applied indicates HL-MRFs and PSL
address an open need in the machine learning community.

The paper is organized as follows. In Section 2, we ﬁrst consider models for structured
prediction that are deﬁned using logical clauses. We unify three diﬀerent approaches to
scalable inference in such models, showing that they all optimize the same convex objec-
tive. We then generalize this objective in Section 3 to deﬁne HL-MRFs. In Section 4, we
introduce PSL, specifying the language and giving many examples of common usage. Next
we introduce a scalable message-passing algorithm for MAP inference in Section 5 and a

3

Bach, Broecheler, Huang, and Getoor

number of learning algorithms in Section 6, evaluating them on a range of tasks. Finally,
in Section 7, we discuss related work.

2. Unifying Convex Inference for Logic-Based Graphical Models

In many structured domains, propositional and ﬁrst-order logics are useful tools for describ-
ing the intricate dependencies that connect the unknown variables. However, these domains
are usually noisy; dependencies among the variables do not always hold. To address this,
logical semantics can be incorporated into probability distributions to create models that
capture both the structure and the uncertainty in machine learning tasks. One common
way to do this is to use logic to deﬁne feature functions in a probabilistic model. We focus
on Markov random ﬁelds (MRFs), a popular class of probabilistic graphical models. Infor-
mally, an MRF is a distribution that assigns probability mass using a scoring function that
is a weighted combination of feature functions called potentials. We will use logical clauses
to deﬁne these potentials. We ﬁrst deﬁne MRFs more formally to introduce necessary
notation:

Deﬁnition 1 Let x = (x1, . . . , xn) be a vector of random variables and let φ = (φ1, . . . , φm)
be a vector of potentials where each potential φj(x) assigns conﬁgurations of the variables
a real-valued score. Also, let w = (w1, . . . , wm) be a vector of real-valued weights. Then, a
Markov random ﬁeld is a probability distribution of the form

P (x) ∝ exp

w(cid:62)φ(x)

.

(cid:16)

(cid:17)

In an MRF, the potentials should capture how the domain behaves, assigning higher scores
If a modeler does not know how the
to more probable conﬁgurations of the variables.
domain behaves, the potentials should capture how it might behave, so that a learning
algorithm can ﬁnd weights that lead to accurate predictions. Logic provides an excellent
formalism for deﬁning such potentials in structured and relational domains.

We now introduce some notation to make this logic-based approach more formal. Con-
sider a set of logical clauses C = {C1, . . . , Cm}, i.e., a knowledge base, where each clause
Cj ∈ C is a disjunction of literals and each literal is a variable x or its negation ¬x drawn
from the variables x such that each variable xi ∈ x appears at most once in Cj. Let
j (resp. I −
I +
j ) ⊂ {1, . . . , n} be the set of indices of the variables that are not negated (resp.
negated) in Cj. Then Cj can be written as

Logical clauses of this form are expressive because they can be viewed equivalently as

implications from conditions to consequences:

This “if-then” reasoning is intuitive and can describe many dependencies in structured data.



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(cid:94)

i∈I −
j

xi =⇒

xi .

(cid:95)

i∈I +
j

4

(1)

(2)

(3)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Assuming we have a logical knowledge base C describing a structured domain, we can
embed it in an MRF by deﬁning each potential φj using a corresponding clause Cj. If an
assignment to the variables x satisﬁes Cj, then we let φj(x) equal 1, and we let it equal 0
otherwise. For our subsequent analysis we assume wj ≥ 0 (∀j = 1, . . . , m). The resulting
MRF preserves the structured dependencies described in C but enables much more ﬂexible
modeling. Clauses no longer must always hold, and the model can express uncertainty
over diﬀerent possible worlds. The weights express how strongly the model expects each
corresponding clause to hold; the higher the weight, the more probable that it is true
according to the model.

This notion of embedding weighted, logical knowledge bases in MRFs is an appealing
one. For example, Markov logic (Richardson and Domingos, 2006) is a popular formalism
that induces MRFs from weighted ﬁrst-order knowledge bases. Given a data set, the ﬁrst-
order clauses are grounded using the constants in the data to create the set of propositional
clauses C. Each propositional clause has the weight of the ﬁrst-order clause from which it
was grounded. In this way, a weighted, ﬁrst-order knowledge base can compactly specify
an entire family of MRFs for a structured machine-learning task.

Although we now have a method for easily deﬁning rich, structured models for a wide
range of problems, there is a new challenge: ﬁnding a most probable assignment to the
variables, i.e., MAP inference, is NP-hard (Shimony, 1994; Garey et al., 1976). This means
that (unless P=NP) our only hope for performing tractable inference is to perform it ap-
proximately. Observe that MAP inference for an MRF deﬁned by C is the integer linear
program

arg max
x∈{0,1}n

P (x) ≡ arg max
x∈{0,1}n

w(cid:62)φ(x)

≡ arg max
x∈{0,1}n

(cid:88)

Cj ∈C

wj min

xi +

(1 − xi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






(4)

While this program is intractable, it does admit convex programming relaxations.

In this section, we show how convex programming can be used to perform tractable
inference in MRFs deﬁned by weighted knowledge bases. We ﬁrst discuss in Section 2.1 an
approach developed by Goemans and Williamson (1994) that views MAP inference as an
instance of the classic MAX SAT problem and relaxes it to a convex program from that
perspective. This approach has the advantage of providing strong guarantees on the quality
of the discrete solutions it obtains. However, it has the disadvantage that general-purpose
convex programming toolkits do not scale well to relaxed MAP inference for large graphical
models (Yanover et al., 2006). In Section 2.2 we then discuss a seemingly distinct approach,
local consistency relaxation, with complementary advantages and disadvantages:
it oﬀers
highly scalable message-passing algorithms but comes with no quality guarantees. We then
unite these approaches by proving that they solve equivalent optimization problems with
identical solutions. Then, in Section 2.3, we show that the uniﬁed inference objective is also
equivalent to exact MAP inference if the knowledge base C is interpreted using (cid:32)Lukasiewicz
logic, an inﬁnite-valued logic for reasoning about naturally continuous quantities such as
similarity, vague or fuzzy concepts, and real-valued data.

5

Bach, Broecheler, Huang, and Getoor

That these three interpretations all lead to the same inference objective—whether rea-
soning about discrete or continuous information—is useful. To the best of our knowledge,
we are the ﬁrst to show their equivalence. This equivalence indicates that the same model-
ing formalism, inference algorithms, and learning algorithms can be used to reason scalably
and accurately about both discrete and continuous information in structured domains. We
generalize the uniﬁed inference objective in Section 3.1 to deﬁne hinge-loss MRFs, and in
the rest of the paper we develop a probabilistic programming language and algorithms that
realize the goal of a scalable and accurate framework for structured data, both discrete and
continuous.

2.1 MAX SAT Relaxation

One approach to approximating objective (4) is to use relaxation techniques developed in
the randomized algorithms community for the MAX SAT problem. Formally, the MAX
SAT problem is to ﬁnd a Boolean assignment to a set of variables that maximizes the total
weight of satisﬁed clauses in a knowledge base composed of disjunctive clauses annotated
with nonnegative weights.
In other words, objective (4) is an instance of MAX SAT.
Randomized approximation algorithms can be constructed for MAX SAT by independently
rounding each Boolean variable xi to true with probability pi. Then, the expected weighted
satisfaction ˆwj of a clause Cj is

also known as a (weighted) noisy-or function, and the expected total score ˆW is



ˆwj = wj


1 −

(1 − pi)

(cid:89)

i∈I +
j



pi


 ,

(cid:89)

i∈I −
j



ˆW =

(cid:88)

Cj ∈C

wj


1 −

(cid:89)

i∈I +
j

(1 − pi)



pi


 .

(cid:89)

i∈I −
j

(5)

(6)

Optimizing ˆW with respect to the rounding probabilities would give the exact MAX SAT so-
lution, so this randomized approach has not made the problem any easier yet, but Goemans
and Williamson (1994) showed how to bound ˆW below with a tractable linear program.

To approximately optimize ˆW , associate with each Boolean variable xi a corresponding
continuous variable ˆyi with domain [0, 1]. Then let ˆy(cid:63) be the optimum of the linear program

arg max
ˆy∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

ˆyi +

(1 − ˆyi), 1

.

(7)

Observe that objectives (4) and (7) are of the same form, except that the variables are
relaxed to the unit hypercube in objective (7). Goemans and Williamson (1994) proved
i for all i, then ˆW ≥ .632 Z(cid:63), where Z(cid:63) is the optimal total weight for
that if pi is set to ˆy(cid:63)
the MAX SAT problem. If each pi is set using any function in a special class, then this

6

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

lower bound improves to a .75 approximation. One simple example of such a function is
1
2

ˆy(cid:63)
i +

pi =

1
4

.

(8)

In this way, objective (7) leads to an expected .75 approximation of the MAX SAT solution.
The following method of conditional probabilities (Alon and Spencer, 2008) can ﬁnd a
single Boolean assignment that achieves at least the expected score from a set of rounding
probabilities, and therefore at least .75 of the MAX SAT solution when objective (7) and
function (8) are used to obtain them. Each variable xi is greedily set to the value that
maximizes the expected weight over the unassigned variables, conditioned on either possible
value of xi and the previously assigned variables. This greedy maximization can be applied
quickly because, in many models, variables only participate in a small fraction of the clauses,
making the change in expectation quick to compute for each variable. Speciﬁcally, referring
to the deﬁnition of ˆW (6), the assignment to xi only needs to maximize over the clauses Cj
in which xi participates, i.e., i ∈ I +

j , which is usually a small set.

j ∪ I −

This approximation is powerful because it is a tractable linear program that comes
with strong guarantees on solution quality. However, even though it is tractable, general-
purpose convex optimization toolkits do not scale well to large MAP problems.
In the
following subsection, we unify this approximation with a complementary one developed in
the probabilistic graphical models community.

2.2 Local Consistency Relaxation

Another approach to approximating objective (4) is to apply a relaxation developed for
Markov random ﬁelds called local consistency relaxation (Wainwright and Jordan, 2008).
This approach starts by viewing MAP inference as an equivalent optimization over marginal
probabilities.2 For each φj ∈ φ, let θj be a marginal distribution over joint assignments xj.
For example, θj(xj) is the probability that the subset of variables associated with potential
φj is in a particular joint state xj. Also, let xj(i) denote the setting of the variable with
index i in the state xj.

With this variational formulation, inference can be relaxed to an optimization over the
ﬁrst-order local polytope L. Let µ = (µ1, . . . , µn) be a vector of probability distributions,
where µi(k) is the marginal probability that xi is in state k. The ﬁrst-order local polytope
is




L (cid:44)

(θ, µ) ≥ 0

(cid:80)

(cid:80)

xj |xj (i)=k θj(xj) = µi(k) ∀i, j, k
∀j

θj(xj) = 1




,



xj
(cid:80)Ki−1
k=0 µi(k) = 1
which constrains each marginal distribution θj over joint states xj to be consistent only
with the marginal distributions µ over individual variables that participate in the potential
φj.



∀i

MAP inference can then be approximated with the ﬁrst-order local consistency relax-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ation:

(9)

(10)

arg max
(θ,µ)∈L

m
(cid:88)

j=1

(cid:88)

xj

wj

θj(xj) φj(xj),

2. This treatment is for discrete MRFs. We have omitted a discussion of continuous MRFs for conciseness.

7

Bach, Broecheler, Huang, and Getoor

which is an upper bound on the true MAP objective. Much work has focused on solving
the ﬁrst-order local consistency relaxation for large-scale MRFs, which we discuss further
in Section 7. These algorithms are appealing because they are well-suited to the sparse
dependency structures common in MRFs, so they can scale to large problems. However, in
general, the solutions can be fractional, and there are no guarantees on the approximation
quality of a tractable discretization of these fractional solutions.

We show that for MRFs with potentials deﬁned by C and nonnegative weights, local

consistency relaxation is equivalent to MAX SAT relaxation.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

We prove Theorem 2 in Appendix A. Our proof analyzes the local consistency relaxation to
derive an equivalent, more compact optimization over only the variable pseudomarginals µ
that is identical to the MAX SAT relaxation. Theorem 2 is signiﬁcant because it shows that
the rounding guarantees of MAX SAT relaxation also apply to local consistency relaxation,
and the scalable message-passing algorithms developed for local consistency relaxation also
apply to MAX SAT relaxation.

2.3 (cid:32)Lukasiewicz Logic

The previous two subsections showed that the same convex program can approximate MAP
inference in discrete, logic-based models, whether viewed from the perspective of randomized
algorithms or variational methods. In this subsection, we show that this convex program
can also be used to reason about naturally continuous information, such as similarity, vague
or fuzzy concepts, and real-valued data. Instead of interpreting the clauses C using Boolean
logic, we can interpret them using (cid:32)Lukasiewicz logic (Klir and Yuan, 1995), which extends
Boolean logic to inﬁnite-valued logic in which the propositions x can take truth values in the
continuous interval [0, 1]. Extending truth values to a continuous domain enables them to
represent concepts that are vague, in the sense that they are often neither completely true
nor completely false. For example, the propositions that a sensor value is high, two entities
are similar, or a protein is highly expressed can all be captured in a more nuanced manner
in (cid:32)Lukasiewicz logic. We can also use the now continuous valued x to represent quantities
that are naturally continuous (scaled to [0,1]), such as actual sensor values, similarity scores,
and protein expression levels. The ability to reason about continuous values is valuable, as
many important applications are not entirely discrete.

The extension to continuous values requires a corresponding extended interpretation of
the logical operators ∧ (conjunction), ∨ (disjunction), and ¬ (negation). The (cid:32)Lukasiewicz
t-norm and t-co-norm are ∧ and ∨ operators that correspond to the Boolean logic operators
for integer inputs (along with the negation operator ¬):

x1 ∧ x2 = max {x1 + x2 − 1, 0}
x1 ∨ x2 = min {x1 + x2, 1}

¬x = 1 − x .

8

(11)

(12)

(13)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The analogous MAX SAT problem for (cid:32)Lukasiewicz logic is therefore

arg max
x∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

xi +

(1 − xi), 1

,

(14)

which is identical in form to the relaxed MAX SAT objective (7). Therefore, if an MRF
is deﬁned over continuous variables with domain [0, 1]n and the logical knowledge base C
deﬁning the potentials is interpreted using (cid:32)Lukasiewicz logic, then exact MAP inference
is identical to ﬁnding the optimum using the uniﬁed, relaxed inference objective derived
for Boolean logic in the previous two subsections. This result shows the equivalence of all
three approaches: MAX SAT relaxation, local consistency relaxation, and MAX SAT using
(cid:32)Lukasiewicz logic.

3. Hinge-Loss Markov Random Fields

We have shown that a speciﬁc family of convex programs can be used to reason scalably and
accurately about both discrete and continuous information. In this section, we generalize
this family to deﬁne hinge-loss Markov random ﬁelds (HL-MRFs), a new kind of probabilis-
tic graphical model. HL-MRFs retain the convexity and expressivity of convex programs
discussed in Section 2, and additionally support an even richer space of dependencies.

To begin, we deﬁne HL-MRFs as density functions over continuous variables y =
(y1, . . . , yn) with joint domain [0, 1]n. These variables have diﬀerent possible interpreta-
tions depending on the application. Since we are generalizing the interpretations explored
in Section 2, HL-MRF MAP states can be viewed as rounding probabilities or pseudo-
marginals, or they can represent naturally continuous information. More generally, they
can be viewed simply as degrees of belief, conﬁdences, or rankings of possible states; and
they can describe discrete, continuous, or mixed domains. The application domain typi-
cally determines which interpretation is most appropriate. The formalisms and algorithms
described in the rest of this paper are general with respect to such interpretations.

3.1 Generalized Inference Objective

To deﬁne HL-MRFs, we will ﬁrst generalize the uniﬁed inference objective of Section 2 in
several ways, which we ﬁrst restate in terms of the HL-MRF variables y:

arg max
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj min

yi +

(1 − yi), 1

.

(15)

For now, we are still assuming that the objective terms are deﬁned using a weighted knowl-
edge base C, but we will quickly drop this requirement. To do so, we examine one term in
isolation. Observe that the maximum value of any unweighted term is 1, which is achieved
when a linear function of the variables is at least 1. We say that the term is satisﬁed when-
ever this occurs. When a term is unsatisﬁed, we can refer to its distance to satisfaction,
which is how far it is from achieving its maximum value. Also observe that we can rewrite

9

Bach, Broecheler, Huang, and Getoor

the optimization explicitly in terms of distances to satisfaction:

arg min
y∈[0,1]n

(cid:88)

Cj ∈C






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






wj max

1 −

yi −

(1 − yi), 0

,

(16)

so that the objective is equivalently to minimize the total weighted distance to satisfaction.
Each unweighted objective term now measures how far the linear constraint

1 −

yi −

(1 − yi) ≤ 0

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

(17)

is from being satisﬁed.

3.1.1 Relaxed Linear Constraints

With this view of each term as a relaxed linear constraint, we can easily generalize them to
arbitrary linear constraints. We no longer require that the inference objective be deﬁned
using only logical clauses, and instead each term can be deﬁned using any function (cid:96)j(y)
that is linear in y. These functions can capture more general dependencies, such as beliefs
about the range of values a variable can take and arithmetic relationships among variables.

The new inference objective is

arg min
y∈[0,1]n

m
(cid:88)

j=1

wj max {(cid:96)j(y), 0} .

(18)

In this form, each term represents the distance to satisfaction of a linear constraint (cid:96)j(y) ≤ 0.
That constraint could be deﬁned using logical clauses as discussed above, or it could be
deﬁned using other knowledge about the domain. The weight wj indicates how important
it is to satisfy a constraint relative to others by scaling the distance to satisfaction. The
higher the weight, the more distance to satisfaction is penalized. Additionally, two relaxed
inequality constraints, (cid:96)j(y) ≤ 0 and −(cid:96)j(y) ≤ 0, can be combined to represent a relaxed
equality constraint (cid:96)j(y) = 0.

3.1.2 Hard Linear Constraints

Now that our inference objective admits arbitrary relaxed linear constraints, it is natural
to also allow hard constraints that must be satisﬁed at all times. Hard constraints are
important modeling tools. They enable groups of variables to represent mutually exclusive
possibilities, such as a multinomial or categorical variable, and functional or partial func-
tional relationships. Hard constraints can also represent background knowledge about the
domain, restricting the domain to regions that are feasible in the real world. Additionally,
they can encode more complex model components such as deﬁning a random variable as an
aggregate over other unobserved variables, which we discuss further in Section 4.3.5.

We can think of including hard constraints as allowing a weight wj to take an inﬁnite
value. Again, two inequality constraints can be combined to represent an equality con-
straint. However, when we introduce an inference algorithm for HL-MRFs in Section 5, it

10

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

will be useful to treat hard constraints separately from relaxed ones, and further, treat hard
inequality constraints separately from hard equality constraints. Therefore, in the deﬁnition
of HL-MRFs, we will deﬁne these three components separately.

3.1.3 Generalized Hinge-Loss Functions

The objective terms measuring each constraint’s distance to satisfaction are hinge losses.
There is a ﬂat region, on which the distance to satisfaction is 0, and an angled region, on
which the distance to satisfaction grows linearly away from the hyperplane (cid:96)j(y) = 0. This
loss function is useful—as we discuss in the previous section, it is a bound on the expected
loss in the discrete setting, among other things—but it is not appropriate for all modeling
situations.

A piecewise-linear loss function makes MAP inference “winner take all,” in the sense
that it is preferable to fully satisfy the most highly weighted objective terms completely
before reducing the distance to satisfaction of terms with lower weights. For example,
consider the following optimization problem:

arg min
y1∈[0,1]

w1 max {y1, 0} + w2 max {1 − y1, 0} .

(19)

If w1 > w2 ≥ 0, then the optimizer is y1 = 0 because the term that prefers y1 = 0 overrules
the term that prefers y1 = 1. The result does not indicate any ambiguity or uncertainty, but
if the two objective terms are potentials in a probabilistic model, it is sometimes preferable
that the result reﬂect the conﬂicting preferences. We can change the inference problem
so that it smoothly trades oﬀ satisfying conﬂicting objective terms by squaring the hinge
losses. Observe that in the modiﬁed problem

arg min
y1∈[0,1]

w1 (max {y1, 0})2 + w2 (max {1 − y1, 0})2

(20)

the optimizer is now y1 = w2

, reﬂecting the relative inﬂuence of the two loss functions.
Another advantage of squared hinge-loss functions is that they can behave more intu-

w1+w2

itively in the presence of hard constraints. Consider the problem

arg min
(y1,y2)∈[0,1]2

such that

max {0.9 − y1, 0} + max {0.6 − y2, 0}

y1 + y2 ≤ 1 .

(21)

The ﬁrst term prefers y1 ≥ 0.9, the second term prefers y2 ≥ 0.6, and the constraint requires
that y1 and y2 are mutually exclusive. Such problems are very common and arise when
conﬂicting evidence of diﬀerent strengths support two mutually exclusive possibilities. The
evidence values 0.9 and 0.6 could come from many sources, including base models trained to
make independent predictions on individual random variables, domain-specialized similarity
functions, or sensor readings. For this problem, any solution y1 ∈ [0.4, 0.9] and y2 = 1 − y1
is an optimizer. This solution set includes counterintuitive optimizers like y1 = 0.4 and
y2 = 0.6, even though the evidence supporting y1 is stronger. Again, squared hinge losses

11

Bach, Broecheler, Huang, and Getoor

ensure the optimizers better reﬂect the relative strength of evidence. For the problem

(max {0.9 − y1, 0})2 + (max {0.6 − y2, 0})2

arg min
(y1,y2)∈[0,1]2

such that

(22)

y1 + y2 ≤ 1 ,

the only optimizer is y1 = 0.65 and y2 = 0.35, which is a more informative solution.

We therefore complete our generalized inference objective by allowing either hinge-loss
or squared hinge-loss functions. Users of HL-MRFs have the choice of either one for each
potential, depending on which is appropriate for their task.

3.2 Deﬁnition

We can now formally state the full deﬁnition of HL-MRFs. They are deﬁned so that a MAP
state is a solution to the generalized inference objective proposed in the previous subsection.
We state the deﬁnition in a conditional form for later convenience, but this deﬁnition is fully
general since the vector of conditioning variables may be empty.

Deﬁnition 3 Let y = (y1, . . . , yn) be a vector of n variables and x = (x1, . . . , xn(cid:48)) a vector
of n(cid:48) variables with joint domain D = [0, 1]n+n(cid:48). Let φ = (φ1, . . . , φm) be a vector of m
continuous potentials of the form

φj(y, x) = (max {(cid:96)j(y, x), 0})pj

where (cid:96)j is a linear function of y and x and pj ∈ {1, 2}. Let c = (c1, . . . , cr) be a vector of
r linear constraint functions associated with index sets denoting equality constraints E and
inequality constraints I, which deﬁne the feasible set

(cid:26)

˜D =

(y, x) ∈ D

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I

(cid:27)

.

For (y, x) ∈ D, given a vector of m nonnegative free parameters, i.e., weights, w =
(w1, . . . , wm), a constrained hinge-loss energy function fw is deﬁned as

(23)

(24)

(25)

fw(y, x) =

wjφj(y, x) .

m
(cid:88)

j=1

We now deﬁne HL-MRFs by placing a probability density over the inputs to a con-
strained hinge-loss energy function. Note that we negate the hinge-loss energy function so
that states with lower energy are more probable, in contrast with Deﬁnition 1. This change
is made for later notational convenience.

Deﬁnition 4 A hinge-loss Markov random ﬁeld P over random variables y and con-
ditioned on random variables x is a probability density deﬁned as follows: if (y, x) /∈ ˜D,
then P (y|x) = 0; if (y, x) ∈ ˜D, then

P (y|x) =

exp (−fw(y, x))

(26)

1
Z(w, x)

12

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

where

(cid:90)

Z(w, x) =

y|(y,x)∈ ˜D

exp (−fw(y, x)) dy .

(27)

In the rest of this paper, we will explore how to use HL-MRFs to solve a wide range
of structured machine learning problems. We ﬁrst introduce a probabilistic programming
language that makes HL-MRFs easy to deﬁne for large, rich domains.

4. Probabilistic Soft Logic

In this section we introduce a general-purpose probabilistic programming language, prob-
abilistic soft logic (PSL). PSL allows HL-MRFs to be easily applied to a broad range of
structured machine learning problems by deﬁning templates for potentials and constraints.
In models for structured data, there are very often repeated patterns of probabilistic de-
pendencies. A few of the many examples include the strength of ties between similar people
in social networks, the preference for triadic closure when predicting transitive relation-
ships, and the “exactly one active” constraints on functional relationships. Often, to make
graphical models both easy to deﬁne and able to generalize across diﬀerent data sets, these
repeated dependencies are deﬁned using templates. Each template deﬁnes an abstract de-
pendency, such as the form of a potential function or constraint, along with any necessary
parameters, such as the weight of the potential, each of which has a single value across all
dependencies deﬁned by that template. Given input data, an undirected graphical model
is constructed from a set of templates by ﬁrst identifying the random variables in the data
and then “grounding out” each template by introducing a potential or constraint into the
graphical model for each subset of random variables to which the template applies.

A PSL program is written in a declarative, ﬁrst-order syntax and deﬁnes a class of
HL-MRFs that are parameterized by the input data. PSL provides a natural interface to
represent hinge-loss potential templates using two types of rules: logical rules and arithmetic
rules. Logical rules are based on the mapping from logical clauses to hinge-loss potentials
introduced in Section 2. Arithmetic rules provide additional syntax for deﬁning an even
wider range of hinge-loss potentials and hard constraints.

4.1 Deﬁnition

In this subsection we deﬁne PSL. Our deﬁnition covers the essential functionality that
should be supported by all implementations, but many extensions are possible. The PSL
syntax we describe can capture a wide range of HL-MRFs, but new settings and scenarios
could motivate the development of additional syntax to make the construction of diﬀerent
kinds of HL-MRFs more convenient.

4.1.1 Preliminaries

We begin with a high-level deﬁnition of PSL programs.

Deﬁnition 5 A PSL program is a set of rules, each of which is a template for hinge-loss
potentials or hard linear constraints. When grounded over a base of ground atoms, a PSL
program induces a HL-MRF conditioned on any speciﬁed observations.

13

Bach, Broecheler, Huang, and Getoor

In the PSL syntax, many components are named using identiﬁers, which are strings that
begin with a letter (from the set {A, . . . , Z, a, . . . , z}), followed by zero or more letters,
numeric digits, or underscores.

PSL programs are grounded out over data, so the universe over which to ground must

be deﬁned.

Deﬁnition 6 A constant is a string that denotes an element in the universe over which
a PSL program is grounded.

Constants are the elements in a universe of discourse. They can be entities or attributes.
For example, the constant "person1" can denote a person, the constant "Adam" can denote
In PSL programs,
a person’s name, and the constant "30" can denote a person’s age.
constants are written as strings in double or single quotes. Constants use backslashes as
escape characters, so they can be used to encode quotes within constants. It is assumed that
constants are unambiguous, i.e., diﬀerent constants refer to diﬀerent entities and attributes.3
Groups of constants can be represented using variables.

Deﬁnition 7 A variable is an identiﬁer for which constants can be substituted.

Variables and constants are the arguments to logical predicates. Together, they are generi-
cally referred to as terms.

Deﬁnition 8 A term is either a constant or a variable.

Terms are connected by relationships called predicates.

Deﬁnition 9 A predicate is a relation deﬁned by a unique identiﬁer and a positive integer
called its arity, which denotes the number of terms it accepts as arguments. Every predicate
in a PSL program must have a unique identiﬁer as its name.

We refer to a predicate using its identiﬁer and arity appended with a slash. For example,
the predicate Friends/2 is a binary predicate, i.e., taking two arguments, which represents
whether two constants are friends. As another example, the predicate Name/2 can relate
a person to the string that is that person’s name. As a third example, the predicate
EnrolledInClass/3 can relate two entities, a student and professor, with an additional
attribute, the subject of the class.

Predicates and terms are combined to create atoms.

Deﬁnition 10 An atom is a predicate combined with a sequence of terms of length equal
to the predicate’s arity. This sequence is called the atom’s arguments. An atom with only
constants for arguments is called a ground atom.

Ground atoms are the basic units of reasoning in PSL. Each represents an unknown or
observation of interest and can take any value in [0, 1]. For example, the ground atom
Friends("person1", "person2") represents whether "person1" and "person2" are friends.
Atoms that are not ground are placeholders for sets of ground atoms. For example, the
atom Friends(X, Y) stands for all ground atoms that can be obtained by substituting
constants for variables X and Y.

3. Note that ambiguous references to underlying entities can be modeled by using diﬀerent constants for
diﬀerent references and representing whether they refer to the same underlying entity as a predicate.

14

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

4.1.2 Inputs

As we have already stated, PSL deﬁnes templates for hinge-loss potentials and hard linear
constraints that are grounded out over a data set to induce a HL-MRF. We now describe how
that data set is represented and provided as the inputs to a PSL program. The ﬁrst inputs
are two sets of predicates: a set C of closed predicates, the atoms of which are completely
observed, and a set O of open predicates, the atoms of which may be unobserved. The third
input is the base A, which is the set of all ground atoms under consideration. All atoms in
A must have a predicate in either C or O. These are the atoms that can be substituted into
the rules and constraints of a PSL program, and each will later be associated with a HL-
MRF random variable with domain [0, 1]. The ﬁnal input is a function O : A → [0, 1] ∪ {∅}
that maps the ground atoms in the base to either an observed value in [0, 1] or a symbol ∅
indicating that it is unobserved. The function O is only valid if all atoms with a predicate in
C are mapped to a [0, 1] value. Note that this deﬁnition makes the sets C and O redundant
in a sense, since they can be derived from A and O, but it will be convenient later to have
C and O explicitly deﬁned.

Ultimately, the method for specifying PSL’s inputs is implementation-speciﬁc, since
In this paper,
diﬀerent choices make it more or less convenient for diﬀerent scenarios.
we will assume that C, O, A, and O exist, and we remain agnostic about how they were
speciﬁed. However, to make this aspect of using PSL more concrete, we will describe one
possible method for deﬁning them here.

Our example method for specifying PSL’s inputs is text-based. The ﬁrst section of the
text input is a deﬁnition of the constants in the universe, which are grouped into types. An
example universe deﬁnition follows.

Person = {"alexis", "bob", "claudia", "david"}
Professor = {"alexis", "bob"}
Student = {"claudia", "david"}
Subject = {"computer science", "statistics"}

This universe includes six constants, four with two types ("alexis", "bob", "claudia",
and "david") and two with one type ("computer science" and "statistics").

The next section of input is the deﬁnition of predicates. Each predicate includes the
types of constants it takes as arguments and whether it is closed. For example, we can
deﬁne predicates for an advisor-student relationship prediction task as follows:

Advises(Professor, Student)

Department(Person, Subject) (closed)

EnrolledInClass(Student, Subject, Professor) (closed)

In this case, there is one open predicate (Advises) and two closed predicates (Department
and EnrolledInClass).

15

Bach, Broecheler, Huang, and Getoor

The ﬁnal section of input is any associated observations. They can be speciﬁed in a list,

for example:

Advises("alexis", "david") = 1

Department("alexis", "computer science") = 1

Department("bob", "computer science") = 1

Department("claudia", "statistics") = 1

Department("david", "statistics") = 1

In addition, values for atoms with the EnrolledInClass predicate could also be speciﬁed.
If a ground atom does not have a speciﬁed value, it will have a default observed value of 0
if its predicate is closed or remain unobserved if its predicate is open.

We now describe how this text input is processed into the formal inputs C, O, A, and
O. First, each predicate is added to either C or O based on whether it is annotated with
the (closed) tag. Then, for each predicate in C or O, ground atoms of that predicate are
added to A with each sequence of constants as arguments that can be created by selecting
a constant of each of the predicate’s argument types. For example, assume that the input
ﬁle contains a single predicate deﬁnition

Category(Document, Cat Name)

where the universe is Document = {"d1", "d2"} and Cat Name = {"politics", "sports"}.
Then,

A =






Category("d1", "politics"),
Category("d1", "sports"),
Category("d2", "politics"),
Category("d2", "sports")






.

(28)

Finally, we deﬁne the function O. Any atom in the explicit list of observations is mapped
to the given value. Then, any remaining atoms in A with a predicate in C are mapped to
0, and any with a predicate in O are mapped to ∅.

Before moving on, we also note that PSL implementations can support predicates and
atoms that are deﬁned functionally. Such predicates can be thought of as a type of closed
predicate. Their observed values are deﬁned as a function of their arguments. One of the
most common examples is inequality, atoms of which can be represented with the shorthand
inﬁx operator !=. For example, the following atom has a value of 1 when two variables A
and B are replaced with diﬀerent constants and 0 when replaced with the same constant.

Such functionally deﬁned predicates can be implemented without requiring their values over
all arguments to be speciﬁed by the user.

4.1.3 Rules and Grounding

Before introducing the syntax and semantics of speciﬁc PSL rules, we deﬁne the grounding
procedure that induces HL-MRFs in general. Given the inputs C, O, A, and O, PSL induces

A != B

16

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

a HL-MRF P (y|x) as follows. First, each ground atom a ∈ A is associated with a random
variable with domain [0, 1]. If O(a) = ∅, then the variable is included in the free variables
y, and otherwise it is included in the observations x with a value of O(a).

With the variables in the distribution deﬁned, each rule in the PSL program is applied
to the inputs and produces hinge-loss potentials or hard linear constraints, which are added
to the HL-MRF. In the rest of this subsection, we describe two kinds of PSL rules: logical
rules and arithmetic rules.

4.1.4 Logical Rules

The ﬁrst kind of PSL rule is a logical rule, which is made up of literals.

Deﬁnition 11 A literal is an atom or a negated atom.

In PSL, the preﬁx operator ! or ~ is used for negation. A negated atom has a value of one
minus the value of the unmodiﬁed atom. For example, if Friends("person1", "person2")
has a value of 0.7, then !Friends("person1", "person2") has a value of 0.3.

Deﬁnition 12 A logical rule is a disjunctive clause of literals. Logical rules are either
weighted or unweighted. If a logical rule is weighted, it is annotated with a nonnegative
weight and optionally a power of two.

Logical rules express logical dependencies in the model. As in Boolean logic, the negation,
disjunction (written as || or |), and conjunction (written as && or &) operators obey De
Morgan’s Laws. Also, an implication (written as -> or <-) can be rewritten as the negation
of the body disjuncted with the head. For example

P1(A, B) && P2(A, B) -> P3(A, B) || P4(A, B)
≡ !(P1(A, B) && P2(A, B)) || P3(A, B) || P4(A, B)
≡ !P1(A, B) || !P2(A, B) || P3(A, B) || P4(A, B)

Therefore, any formula written as an implication with (1) a literal or conjunction of literals
in the body and (2) a literal or disjunction of literals in the head is also a valid logical rule,
because it is equivalent to a disjunctive clause.

There are two kinds of logical rules: weighted or unweighted. A weighted logical rule is a
template for a hinge-loss potential that penalizes how far the rule is from being satisﬁed. A
weighted logical rule begins with a nonnegative weight and optionally ends with an exponent
of two (^2). For example, the weighted logical rule

1 : Advisor(Prof, S) && Department(Prof, Sub) -> Department(S, Sub)

has a weight of 1 and induces potentials propagating department membership from advisors
to advisees. An unweighted logical rule is a template for a hard linear constraint that
requires that the rule always be satisﬁed. For example, the unweighted logical rule

Friends(X, Y) && Friends(Y, Z) -> Friends(X, Z) .

induces hard linear constraints enforcing the transitivity of the Friends/2 predicate. Note
the period (.) that is used to emphasize that this rule is always enforced and disambiguate
it from weighted rules.

17

Bach, Broecheler, Huang, and Getoor

A logical rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. This procedure produces
a set of ground rules, which are rules containing only ground atoms. Each ground rule will
then be interpreted as either a potential or hard constraint in the induced HL-MRF. For
notational convenience, we assume without loss of generality that all the random variables
are unobserved, i.e., O(a) = ∅, ∀a ∈ A. If the input data contain any observations, the
following description still applies, except that some free variables will be replaced with
observations from x. The ﬁrst step in interpreting a ground rule is to map its disjunctive
clause to a linear constraint. This mapping is based on the uniﬁed inference objective
derived in Section 2. Any ground PSL rule is a disjunction of literals, some of which are
negated. Let I + be the set of indices of the variables that correspond to atoms that are not
negated in the ground rule, when expressed as a disjunctive clause, and, likewise, let I − be
the indices of the variables corresponding to atoms that are negated. Then, the clause is
mapped to the inequality

(cid:88)

1 −

(cid:88)

yi −

(1 − yi) ≤ 0 .

i∈I +

i∈I −

If the logical rule that templated the ground rule is weighted with a weight of w and is not
annotated with ^2, then the potential

is added to the HL-MRF with a parameter of w. If the rule is weighted with a weight w
and annotated with ^2, then the potential











φ(y, x) = max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −



φ(y, x) =

max

1 −

(cid:88)

(cid:88)

yi −

(1 − yi), 0

i∈I +

i∈I −

c(y, x) = 1 −

(cid:88)

(cid:88)

yi −

(1 − yi)

i∈I +

i∈I −








2








is added to the HL-MRF with a parameter of w. If the rule is unweighted, then the function

is added to the set of constraint functions and its index is included in the set I to deﬁne a
hard inequality constraint c(y, x) ≤ 0.

As an example of the grounding process, consider the following logical rule. As part of
a program for link prediction, it is often helpful to model the transitivity of a relationship.

3 : Friends(A, B) && Friends(B, C) -> Friends(C, A) ^2

Imagine that the input data are C = {}, O = {Friends/2},

(29)

(30)

(31)

(32)

(33)

A =






Friends("p1", "p2"),
Friends("p1", "p3"),
Friends("p2", "p1"),
Friends("p2", "p3"),
Friends("p3", "p1"),
Friends("p3", "p2")






,

18

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and O(a) = ∅, ∀a ∈ A. Then, the rule will induce six ground rules. One such ground rule is

3 : Friends("p1", "p2") && Friends("p2", "p3") -> Friends("p3", "p1") ^2

which is equivalent to the following.

3 : !Friends("p1", "p2") || !Friends("p2", "p3") || Friends("p3", "p1") ^2

If the atoms Friends("p1", "p2"), Friends("p2", "p3"), and Friends("p3", "p1")
correspond to the random variables y1, y2, and y3, respectively, then this ground rule is
interpreted as the weighted hinge-loss potential

3 (max{y1 + y2 − y3 − 1, 0})2 .

(34)

Since the grounding process uses the mapping from Section 2, logical rules can be used
to reason accurately and eﬃciently about both discrete and continuous information. They
are a convenient method for constructing HL-MRFs with the uniﬁed inference objective
for weighted logical knowledge bases as their MAP inference objective. They also allow
the user to seamlessly incorporate some of the additional features of HL-MRFs, such as
squared potentials and hard constraints. Next, we introduce an even more ﬂexible class of
PSL rules.

4.1.5 Arithmetic Rules

Arithmetic rules in PSL are more general templates for hinge-loss potentials and hard
linear constraints. Like logical rules, they come in weighted and unweighted variants, but
instead of using logical operators they use arithmetic operators. In general, an arithmetic
rule relates two linear combinations of atoms with an inequality or an equality. A simple
example enforces the mutual exclusivity of liberal and conservative ideologies.

Liberal(P) + Conservative(P) = 1 .

Just like logical rules, arithmetic rules are grounded out by performing all possible substi-
tutions of constants for variables to make ground atoms in the base A. In this example,
each substitution for Liberal(P) and Conservative(P) is constrained to sum to 1. Since
the rule is unweighted and arithmetic, it deﬁnes a hard constraint c(y, x) and its index will
be included in E because it is an equality constraint.

To make arithmetic rules more ﬂexible and easy to use, we deﬁne some additional syntax.
The ﬁrst is a generalized deﬁnition of atoms that can be substituted with sums of ground
atoms, rather than just a single atom.

Deﬁnition 13 A summation atom is an atom that takes terms and/or sum variables
as arguments. A summation atom represents the summations of ground atoms that can be
obtained by substituting individual constants for variables and summing over all possible
constants for sum variables.

A sum variable is represented by prepending a plus symbol (+) to a variable. For example,
the summation atom

Friends(P, +F)

19

Bach, Broecheler, Huang, and Getoor

is a placeholder for the sum of all ground atoms with predicate Friends/2 in A that share
a ﬁrst argument. Note that sum variables can be used at most once in a rule, i.e., each
sum variable in a rule must have a unique identiﬁer. Summation atoms are useful because
they can describe dependencies without needing to specify the number of atoms that can
participate. For example, the arithmetic rule

Label(X, +L) = 1 .

says that labels for each constant substituted for X should sum to one, without needing to
specify how many possible labels there are.

The substitutions for sum variables can be restricted using logical clauses as ﬁlters.

Deﬁnition 14 A ﬁlter clause is a logical clause deﬁned for a sum variable in an arithmetic
rule. The logical clause only contains atoms (1) with predicates that appear in C and (2)
that only take as arguments (a) constants, (b) variables that appear in the arithmetic rule,
and (c) the sum variable for which it is deﬁned.

Filter clauses restrict the substitutions for a sum variable in the corresponding arithmetic
rule by only including substitutions for which the clause evaluates to true. The ﬁlters are
evaluated using Boolean logic. Each ground atom a is treated as having a value of 0 if and
only if O(a) = 0. Otherwise, it is treated as having a value of 1. For example, imagine that
we want to restrict the summation in the following arithmetic rule to only constants that
satisfy a property Property/1.

Then, we can add the following ﬁlter clause.

Link(X, +Y) <= 1 .

{Y: Property(Y)}

Then, the hard linear constraints templated by the arithmetic rule will only sum over
constants substituted for Y such that Property(Y) is non-zero.

In arithmetic rules, atoms can also be modiﬁed with coeﬃcients. These coeﬃcients can

be hard-coded. As a simple example, in the rule

Susceptible(X) >= 0.5 Biomarker1(X) + 0.5 Biomarker2(X) .

the property Susceptible/1, which represents the degree to which a patient is susceptible
to a particular disease, must be at least the average value of two biomarkers.

PSL also supports two forms of coeﬃcient-deﬁning syntax. The ﬁrst form of coeﬃcient
syntax is a cardinality function that counts the number of terms substituted for a sum
variable. Cardinality functions enable rules that depend on the number of substitutions in
order to be scaled correctly, such as when averaging. Cardinality is denoted by enclosing a
sum variable, without the +, in pipes. For example, the rule

1 / |Y| Friends(X, +Y) = Friendliness(X) .

20

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

deﬁnes the Friendliness/1 property of a person X in a social network as the average
strength of their outgoing friendship links. In cases in which Friends/2 is not symmetric,
we can extend this rule to sum over both outgoing and incoming links as follows.

1 / |Y1| |Y2| Friends(X, +Y1) + 1 / |Y1| |Y2| Friends(+Y2, X)

= Friendliness(X) .

The second form of coeﬃcient syntax is built-in coeﬃcient functions. The exact set of
supported functions is implementation speciﬁc, but standard functions like maximum and
minimum should be included. Coeﬃcient functions are prepended with @ and use square
brackets instead of parentheses to distinguish them from predicates. Coeﬃcient functions
can take either scalars or cardinality functions as arguments. For example, the following
rule for matching two sets of constants requires that the sum of the Matched/2 atoms be
the minimum of the sizes of the two sets.

Matched(+X, +Y) = @Min[|X|, |Y|] .

Note that PSL’s coeﬃcient syntax can also be used to deﬁne constants, as in this example.
So far we have focused on using arithmetic rules to deﬁne templates for linear constraints,
but they can also be used to deﬁne hinge-loss potentials. For example, the following arith-
metic rule prefers that the degree to which a person X is extroverted (represented with
Extroverted/1) does not exceed the average extroversion of their friends:

2 : Extroverted(X) <= 1 / |Y| Extroverted(+Y) ^2
{Y: Friends(X, Y) || Friends(Y, X)}

This rule is a template for weighted hinge-loss potentials of the form

(cid:32)

(cid:40)

2

max

yi(cid:48) −

(cid:41)(cid:33)2

yi, 0

,

1
|F|

(cid:88)

i∈F

(35)

where yi(cid:48) is the variable corresponding to a grounding of the atom Extroverted(X) and
F is the set of the indices of the variables corresponding to Extroverted(Y) atoms of the
friends Y that satisfy the rule’s ﬁlter clause. Note that the weight of 2 is distinct from
the coeﬃcients in the linear constraint (cid:96)(y, x) ≤ 0 deﬁning the hinge-loss potential.
If
the arithmetic rule were an equality instead of an inequality, each grounding would be
two hinge-loss potentials, one using (cid:96)(y, x) ≤ 0 and one using −(cid:96)(y, x) ≤ 0. In this way,
arithmetic rules can deﬁne general hinge-loss potentials.

For completeness, we state the full, formal deﬁnition of an arithmetic rule and deﬁne its

grounding procedure.

Deﬁnition 15 An arithmetic rule is an inequality or equality relating two linear combi-
nations of summation atoms. Each sum variable in an arithmetic rule can be used once.
An arithmetic rule can be annotated with ﬁlter clauses for a subset of its sum variables that
restrict its groundings. Arithmetic rules are either weighted or unweighted. If an arithmetic
rule is weighted, it is annotated with a nonnegative weight and optionally a power of two.

21

Bach, Broecheler, Huang, and Getoor

An arithmetic rule is grounded out by performing all distinct substitutions from variables to
constants such that the resulting ground atoms are in the base A. In addition, summation
atoms are replaced by the appropriate summations over ground atoms (possibly restricted
by corresponding ﬁlter clauses) and the coeﬃcient is distributed across the summands. This
leads to a set of ground rules for each arithmetic rule given a set of inputs. If the arithmetic
rule is an unweighted inequality, each ground rule can be algebraically manipulated to be
of the form c(y, x) ≤ 0. Then c(y, x) is added to the set of constraint functions and its
index is added to I. If instead the arithmetic rule is an unweighted equality, each ground
rule is manipulated to c(y, x) = 0, c(y, x) is added to the set of constraint functions, and
its index is added to E. If the arithmetic rule is a weighted inequality with weight w, each
ground rule is manipulated to (cid:96)(y, x) ≤ 0 and included as a potential of the form

φ(y, x) = max {(cid:96)(y, x), 0}

(36)

with a weight of w. If the arithmetic rule is a weighted equality with weight w, each ground
rule is again manipulated to (cid:96)(y, x) ≤ 0 and two potentials are included,

φ1(y, x) = max {(cid:96)(y, x), 0}, φ2(y, x) = max {−(cid:96)(y, x), 0} ,

(37)

each with a weight of w. In either case, if the weighted arithmetic rule is annotated with
^2, then the induced potentials are squared.

4.2 Expressivity

An important question is the expressivity of PSL, which uses disjunctive clauses with pos-
itive weights for its logical rules. Other logic-based languages support diﬀerent types of
clauses, such as Markov logic networks (Richardson and Domingos, 2006), which support
clauses with conjunctions and clauses with negative weights. As we discuss in this section,
PSL’s logical rules capture a general class of structural dependencies, capable of model-
ing arbitrary probabilistic relationships among Boolean variables, such as those deﬁned by
Markov logic networks. The advantage of PSL is that it deﬁnes HL-MRFs, which are much
more scalable than discrete MRFs and often just as accurate, as we show in Section 6.4.

The expressivity of PSL is tied to the expressivity of the MAX SAT problem, since
they both use the same class of weighted clauses. There are two conditions on the clauses:
(1) they have nonnegative weights, and (2) they are disjunctive. We ﬁrst consider the
nonnegativity requirement and show that can actually be viewed as a restriction on the
structure of a clause. To illustrate, consider a weighted disjunctive clause of the form

−w :



xi









(cid:95)

i∈I +
j

(cid:95)

(cid:95)








¬xi


 .

i∈I −
j

(38)

If this clause were part of a generalized MAX SAT problem, in which there were no restric-
tions on weight sign or clause structure, but the goal were still to maximize the sum of the
weights of the satisﬁed clauses, then this clause could be replaced with an equivalent one

22

(39)

(40)

(41)

(42)

(43)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

without changing the optimizer:

w :






¬xi






(cid:94)

i∈I +
j

(cid:94)

(cid:94)








xi


 .

i∈I −
j

Note that the clause has been changed in three ways: (1) the sign of the weight has been
changed, (2) the disjunctions have been replaced with conjunctions, and (3) the literals
have all been negated. Due to this equivalence, the restriction on the sign of the weights
is subsumed by the restriction on the structure of the clauses. In other words, any set of
clauses can be converted to a set with nonnegative weights that has the same optimizer,
but it might require including conjunctions in the clauses. It is also easy to verify that if
Equation (38) is used to deﬁne a potential in a discrete MRF, replacing it with a potential
deﬁned by (39) leaves the distribution unchanged, due to the normalizing partition function.
We now consider the requirement that clauses be disjunctive and illustrate how con-
junctive clauses can be replaced by an equivalent set of disjunctive clauses. The idea is to
construct a set of disjunctive clauses such that all assignments to the variables are mapped
to the same score, up to a constant. A simple example is replacing a conjunction

with disjunctions

w : x1 ∧ x2

w : x1 ∨ x2
w : ¬x1 ∨ x2
w : x1 ∨ ¬x2 .

Observe that the total score for all assignments to the variables remains the same, up to a
constant.

This example generalizes to a procedure for encoding any Boolean MRF into a set of
disjunctive clauses with nonnegative weights. Park (2002) showed that the MAP problem
for any discrete Bayesian network can be represented as an instance of MAX SAT. For
distributions of bounded factor size, the MAX SAT problem has size polynomial in the
number of variables and factors of the distribution. We describe how any Boolean MRF
can be represented with disjunctive clauses and nonnegative weights. Given a Boolean MRF
with arbitrary potentials deﬁned by mappings from joint states of subsets of the variables
to scores, a new MRF is created as follows. For each potential in the original MRF, a new
set of potentials deﬁned by disjunctive clauses is created. A conjunctive clause is created
corresponding to each entry in the potential’s mapping with a weight equal to the score
assigned by the weighted potential in the original MRF. Then, these clauses are converted to
equivalent disjunctive clauses as in the example of Equations (38) and (39) by also ﬂipping
the sign of their weights and negating the literals. Once this is done for all entries of all
potentials, what remains is an MRF deﬁned by disjunctive clauses, some of which might
have negative weights. We make all weights positive by adding a suﬃciently large constant
to all weights of all clauses, which leaves the distribution unchanged due to the normalizing
partition function.

23

Bach, Broecheler, Huang, and Getoor

It is important to note two caveats when converting arbitrary Boolean MRFs to MRFs
deﬁned using only disjunctive clauses with nonnegative weights. First, the number of clauses
required to represent a potential in the original MRF is exponential in the degree of the
potential. In practice, this is rarely a signiﬁcant limitation, since MRFs often contain low-
degree potentials. The other important point is that the step of adding a constant to all
the weights increases the total score of the MAP state. Since the bound of Goemans and
Williamson (1994) is relative to this score, the bound is loosened for the original problem the
larger the constant added to the weights is. This is to be expected, since even approximating
MAP is NP-hard in general (Abdelbar and Hedetniemi, 1998).

We have described how general structural dependencies can be modeled with the logical
rules of PSL. It is possible to represent arbitrary logical relationships with them. The
process for converting general rules to PSL’s logical rules can be done automatically and
made transparent to the user. We have elected in this section to deﬁne PSL’s logical rules
without making this conversion automatic to make clear the underlying formalism.

4.3 Modeling Patterns

PSL is a ﬂexible language, and there are some patterns of usage that come up in many
applications. We illustrate some of them in this subsection with a number of examples.

4.3.1 Domain and Range Rules

In many problems, the number of relations that can be predicted among some constants
is known. For binary predicates, this background knowledge can be viewed as constraints
on the domain (ﬁrst argument) or range (second argument) of the predicate. For example,
it might be background knowledge that each entity, such as a document, has exactly one
label. An arithmetic rule to express this follows.

Label(Document, +LabelName) = 1 .

The predicate Label is said to be functional.

Alternatively, sometimes it is the ﬁrst argument that should be summed over. For ex-
ample, imagine the task of predicting relationships among students and professors. Perhaps
it is known that each student has exactly one advisor. This constraint can be written as
follows.

Advisor(+Professor, Student) = 1 .

The predicate Advisor is said to be inverse functional.

Finally, imagine a scenario in which two social networks are being aligned. The goal is
to predict whether each pair of people, one from each network, is the same person, which is
represented with atoms of the Same predicate. Each person aligns with at most one person
in the other network, but might not align with anyone. This can be expressed with the
following two arithmetic rules.

The predicate Same is said to be both partial functional and partial inverse functional.

Same(Person1, +Person2) <= 1 .

Same(+Person1, Person2) <= 1 .

24

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Many variations on these examples are possible. For example, they can be generalized
to predicates with more than two arguments. Additional arguments can either be ﬁxed or
summed over in each rule. As another example, domain and range rules can incorporate
multiple predicates, so that an entity can participate in a ﬁxed number of relations counted
among multiple predicates.

4.3.2 Similarity

Many problems require explicitly reasoning about similarity, rather than simply whether
entities are the same or diﬀerent. For example, reasoning with similarity has been explored
using kernel methods, such as kFoil (Landwehr et al., 2010) that bases similarity computa-
tion on the relational structure of the data. The continuous variables of HL-MRFs make
modeling similarity straightforward, and PSL’s support for functionally deﬁned predicates
makes it even easier. For example, in an entity resolution task, the degree to which two
entities are believed to be the same might depend on how similar their names are. A rule
expressing this dependency is

1.0 : Name(P1, N1) && Name(P2, N2) && Similar(N1, N2) -> Same(P1, P2)

This rule uses the Similar predicate to measure similarity. Since it is a functionally deﬁned
predicate, it can be implemented as one of many diﬀerent, possibly domain specialized,
string similarity functions. Any similarity function that can output values in the range
[0, 1] can be used.

4.3.3 Priors

If no potentials are deﬁned over a particular atom, then it is equally probable that it has any
value between zero and one. Often, however, it should be more probable that an atom has
a value of zero, unless there is evidence that it has a nonzero value. Since atoms typically
represent the existence of some entity, attribute, or relation, this bias promotes sparsity
among the things inferred to exist. Further, if there is a potential that prefers that an
atom should have a value that is at least some numeric constant, such as when reasoning
with similarities as discussed in Section 4.3.2, it often should also be more probable that an
atom is no higher in value than is necessary to satisfy that potential. To accomplish both
these goals, simple priors can be used to state that atoms should have low values in the
absence of evidence to overrules those priors. A prior in PSL can be a rule consisting of
just a negative literal with a small weight. For example, in a link prediction task, imagine
that this preference should apply to atoms of the Link predicate. A prior is then

0.1 : !Link(A, B)

which acts as a regularizer on Link atoms.

4.3.4 Blocks and Canopies

In many tasks, the number of unknowns can quickly grow large, even for modest amounts of
data. For example, in a link prediction task the goal is to predict relations among entities.
The number of possible links grows quadratically with the number of entities (for binary

25

Bach, Broecheler, Huang, and Getoor

relations). If handled naively, this growth could make scaling to large data sets diﬃcult,
but this problem is often handled by constructing blocks (e.g., Newcombe and Kennedy,
1962) or canopies (McCallum et al., 2000) over the entities, so that a limited subset of all
possible links are actually considered. Blocking partitions the entities so that only links
among entities in the same partition element, i.e., block, are considered. Alternatively, for
a ﬁner grained pruning, a canopy is deﬁned for each entity, which is the set of other entities
to which it could possibly link. Blocks and canopies can be computed using specialized,
domain-speciﬁc functions, and PSL can incorporate them by including them as atoms in
the bodies of rules. Since blocks can be seen as a special case of canopies, we let the atom
InCanopy(A, B) be 1 if B is in the canopy or block of A, and 0 if it is not.
Including
InCanopy(A, B) atoms as additional conditions in the bodies of logical rules will ensure
that the dependencies only exist between the desired entities.

4.3.5 Aggregates

Another powerful feature of PSL is its ability to easily deﬁne aggregates, which are rules
that deﬁne random variables to be deterministic functions of sets of other random variables.
The advantage of aggregates is that they can be used to deﬁne dependencies that do not
scale in magnitude with the number of groundings in the data. For example, consider a
model for predicting interests in a social network. A fragment of a PSL program for this
task follows.

1.0 : Interest(P1, I) && Friends(P1, P2) -> Interest(P2, I)

1.0 : Age(P, "20-29") && Lives(P, "California") -> Interest(P, "Surfing")

These two rules express the belief that interests are correlated along friendship links in
the social network, and also that certain demographic information is predictive of speciﬁc
interests. The question any domain expert or learning algorithm faces is how strongly each
rule should be weighted relative to each other. The challenge of answering this question
when using templates is that the number of groundings of the ﬁrst rule varies from person to
person based on the number of friends, while the groundings of the second remain constant
(one per person). This inconsistent scaling of the two types of dependencies makes it diﬃcult
to ﬁnd weights that accurately reﬂect the relative inﬂuence each type of dependency should
have across people with diﬀerent numbers of friends.

Using an aggregate can solve this problem of inconsistent scaling. Instead of using a
separate ground rule to relate the interest of each friend, we can deﬁne a rule that is only
grounded once for each person, relating an average interest across all friends to each person’s
own interests. A PSL fragment for this approach is

1.0 : AverageFriendInterest(P, I) -> Interest(P, I)

AverageFriendInterest(P, I) = 1 / |F| Interest(+F, I) .
{F: Friends(P, F)}

/* Demographic dependencies are also included.

*/

where the predicate AverageFriendInterest/2 is an aggregate that is constrained to be
the average amount of interest each friend of a person P has in an interest I. The weight

26

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

of the logical rule can now be scaled more appropriately relative to other types of features
because there is only one grounding per person.

For a more complex example, consider the problem of determining whether two refer-
ences in the data refer to the same underlying person. One useful feature to use is whether
they have similar sets of friends in the social network. Again, a rule could be deﬁned that
is grounded out for each friendship pair, but this would suﬀer from the same scaling issues
as the previous example. Instead, we can use an aggregate to directly express how similar
the two references’ sets of friends are. A function that measures the similarity of two sets
A and B is Jaccard similarity:

J(A, B) =

|A ∩ B|
|A ∪ B|

.

Jaccard similarity is a nonlinear function, meaning that it cannot be used directly without
breaking the log-concavity of HL-MRFs, but we can approximate it with a linear function.
We deﬁne SameFriends/2 as an aggregate that approximates Jaccard similarity (where
SamePerson/2 is functional and inverse functional).

SameFriends(A, B) = 1 / @Max[|FA|, |FB|] SamePerson(+FA, +FB) .
{FA : Friends(A, FA)}
{FB : Friends(B, FB)}

SamePerson(+P1, P2) = 1 .

SamePerson(P1, +P2) = 1 .

The aggregate SameFriends/2 uses the sum of the SamePerson/2 atoms as the intersection
of the two sets, and the maximum of the sizes of the two sets of friends as a lower bound
on the size of their union.

5. MAP Inference

Having deﬁned HL-MRFs and a language for creating them, PSL, we turn to algorithms
for inference and learning. The ﬁrst task we consider is maximum a posteriori (MAP)
inference, the problem of ﬁnding a most probable assignment to the free variables y given
observations x. In HL-MRFs, the normalizing function Z(w, x) is constant over y and the
exponential is maximized by minimizing its negated argument, so the MAP problem is

arg max
y

P (y|x) ≡ arg min
y|y,x∈ ˜D

fw(y, x)

≡ arg min
y∈[0,1]n

w(cid:62)φ(y, x)

such that

ck(y, x) = 0, ∀k ∈ E
ck(y, x) ≤ 0, ∀k ∈ I .

(44)

MAP is a fundamental problem because (1) it is the method we will use to make predictions,
and (2) weight learning often requires performing MAP inference many times with diﬀerent
weights (as we discuss in Section 6). Here, HL-MRFs have a distinct advantage over general

27

Bach, Broecheler, Huang, and Getoor

discrete models, since minimizing fw is a convex optimization rather than a combinatorial
one. There are many oﬀ-the-shelf solutions for convex optimization, the most popular
of which are interior-point methods, which have worst-case polynomial time complexity
in the number of variables, potentials, and constraints (Nesterov and Nemirovskii, 1994).
Although in practice they perform better than their worst-case bounds (Wright, 2005), they
do not scale well to large structured prediction problems (Yanover et al., 2006). We therefore
introduce a new algorithm for exact MAP inference designed to scale to large HL-MRFs by
leveraging the sparse connectivity structure of the potentials and hard constraints that are
typical of models for real-world tasks.

5.1 Consensus Optimization Formulation

Our algorithm uses consensus optimization, a technique that divides an optimization prob-
lem into independent subproblems and then iterates to reach a consensus on the optimum
(Boyd et al., 2011). Given a HL-MRF P (y|x), we ﬁrst construct an equivalent MAP prob-
lem in which each potential and hard constraint is a function of diﬀerent variables. The
variables are then constrained to make the new and original MAP problems equivalent. We
let y(L,j) be a local copy of the variables in y that are used in the potential function φj,
j = 1, . . . , m and y(L,k+m) be a copy of those used in the constraint function ck, k = 1, . . . , r.
We refer to the concatenation of all of these vectors as yL. We also introduce a characteristic
= 0 if the constraint is
function χk for each constraint function where χk
satisﬁed and inﬁnity if it is not. Likewise, let χ[0,1] be a characteristic function that is 0 if the
input is in the interval [0, 1] and inﬁnity if it is not. We drop the constraints on the domain
of y, letting them range in principle over Rn and instead use these characteristic functions
to enforce the domain constraints. This formulation will make computation easier when
be the variables in y that correspond
the problem is later decomposed. Finally, let y

(cid:104)
ck(y(L,k+m), x)

(cid:105)

(C,ˆi)

(L,ˆi)

, ˆi = 1, . . . , m + r. Operators between y

to y
are deﬁned element-wise,
pairing the corresponding copied variables. Consensus optimization solves the reformulated
MAP problem

and y

(C,ˆi)

(L,ˆi)

arg min
(yL,y)

m
(cid:88)

j=1

such that

wjφj

(cid:16)

(cid:17)
y(L,j), x

+

(cid:104)

(cid:16)

χk

ck

y(L,k+m), x

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

(45)

y

(L,ˆi)

= y

(C,ˆi)

∀ˆi = 1, . . . , m + r .

Inspection shows that problems (44) and (45) are equivalent.

This reformulation enables us to relax the equality constraints y

in order
to divide problem (45) into independent subproblems that are easier to solve, using the
alternating direction method of multipliers (ADMM) (Glowinski and Marrocco, 1975; Gabay
and Mercier, 1976; Boyd et al., 2011). The ﬁrst step is to form the augmented Lagrangian
function for the problem. Let α = (α1, . . . , αm+r) be a concatenation of vectors of Lagrange

= y

(C,ˆi)

(L,ˆi)

28

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

multipliers. Then the augmented Lagrangian is

L(yL, α, y) =

wjφj

y(L,j), x

+

(cid:16)

(cid:17)

(cid:16)

(cid:104)
ck

χk

y(L,k+m), x

m
(cid:88)

j=1

r
(cid:88)

k=1

(cid:17)(cid:105)

+

n
(cid:88)

i=1

χ[0,1] [yi]

+

m+r
(cid:88)

ˆi=1

(cid:16)

y

α(cid:62)
ˆi

(L,ˆi)

− y

(C,ˆi)

(cid:17)

+

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)y

(L,ˆi)

− y

(C,ˆi)

(46)

(cid:13)
2
(cid:13)
(cid:13)
2

using a step-size parameter ρ > 0. ADMM ﬁnds a saddle point of L(yL, α, y) by updating
the three blocks of variables at each iteration t:

∀ˆi = 1, . . . , m + r

αt
ˆi

← αt−1
ˆi

+ ρ

yt

L ← arg min

(cid:17)

(cid:16)

yt−1
(L,ˆi)

− yt−1
(C,ˆi)
L (cid:0)yL, αt, yt−1(cid:1)

yt ← arg min

L (cid:0)yt

L, αt, y(cid:1)

yL

y

The ADMM updates ensure that y converges to the global optimum y(cid:63), the MAP state
of P (y|x), assuming that there exists a feasible assignment to y. We check convergence
using the criteria suggested by Boyd et al. (2011), measuring the primal and dual residuals
at the end of iteration t, deﬁned as

(cid:107)¯rt(cid:107)2 (cid:44)

(cid:107)yt

(L,ˆi)

− yt

(C,ˆi)

(cid:107)2
2



(cid:107)¯st(cid:107)2 (cid:44) ρ

Ki(yt

i − yt−1
i

)2

(50)



1
2

(cid:33) 1
2

(cid:32) n
(cid:88)

i=1





m+r
(cid:88)

ˆi=1

where Ki is the number of copies made of the variable yi, i.e., the number of diﬀerent
potentials and constraints in which the variable participates. The updates are terminated
when both of the following conditions are satisﬁed










m+r
(cid:88)

ˆi=1

(cid:107)yt

(L,ˆi)

(cid:107)2
2



,

Ki(yt

i)2



1
2

(cid:32) n
(cid:88)

i=1

(cid:33) 1
2






(cid:107)¯rt(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel max

(cid:107)¯st(cid:107)2 ≤ (cid:15)abs

Ki + (cid:15)rel



1
2

(cid:107)2
(cid:107)αt
2
ˆi







m+r
(cid:88)

ˆi=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

using convergence parameters (cid:15)abs and (cid:15)rel.

5.2 Block Updates

We now describe how to implement the ADMM block updates (47), (48), and (49). Updating
the Lagrange multipliers α is a simple step in the gradient direction (47). Updating the
local copies yL (48) decomposes over each potential and constraint in the HL-MRF. For the

29

(47)

(48)

(49)

(51)

(52)

Bach, Broecheler, Huang, and Getoor

variables y(L,j) for each potential φj, this requires independently optimizing the weighted
potential plus a squared norm:

(cid:16)

(cid:110)

wj

max

(cid:96)j(y(L,j), x), 0

(cid:111)(cid:17)pj

arg min
y(L,j)

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

.

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

(53)

There are three cases for y(cid:63)

Although this optimization problem is convex, the presence of the hinge function complicates
it. It could be solved in principle with an iterative method, such as an interior-point method,
but such methods would become very expensive over many ADMM updates. Fortunately,
we can reduce the problem to checking several cases and ﬁnd solutions much more quickly.
(L,j), the optimizer of problem (53), which correspond to
the three regions in which the solution could lie: (1) the region (cid:96)(y(L,j), x) < 0, (2) the
region (cid:96)(y(L,j), x) > 0, and (3) the region (cid:96)(y(L,j), x) = 0. We check each case by replacing
the potential with its value on the corresponding region, optimizing, and checking if the
optimizer is in the correct region. We check the ﬁrst case by replacing the potential φj
with zero. Then, the optimizer of the modiﬁed problem is y(C,j) − αj/ρ.
If (cid:96)j(y(C,j) −
αj/ρ, x) ≤ 0, then y(cid:63)
(L,j) = y(C,j) − αj/ρ, because it optimizes both the potential and the
squared norm independently. If instead (cid:96)j(y(C,j) − αj/ρ, x) > 0, then we can conclude that
(cid:96)j(y(cid:63)

(L,j), x) ≥ 0, leading to one of the next two cases.
In the second case, we replace the maximum term with the inner linear function. Then
the optimizer of the modiﬁed problem is found by taking the gradient of the objective with
respect to y(L,j), setting the gradient equal to the zero vector, and solving for y(L,j). In
other words, the optimizer is the solution for y(L,j) to the equation

(cid:34)

(cid:16)

∇y(L,j)

wj

(cid:96)j(y(L,j), x)

(cid:17)pj

+

(cid:13)
(cid:13)
y(L,j) − y(C,j) +
(cid:13)
(cid:13)

ρ
2

1
ρ

αj

(cid:35)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= 0 .

(54)

This condition deﬁnes a simple system of linear equations. If pj = 1, then the coeﬃcient
matrix is diagonal and trivial to solve. If pj = 2, then the coeﬃcient matrix is symmetric
and positive deﬁnite, and the system can be solved via Cholesky decomposition. (Since
the potentials of an HL-MRF often have shared structures, perhaps templated by a PSL
program, the Cholesky decompositions can be cached and shared among potentials for
improved performance.) Let y(cid:48)
(L,j) be the optimizer of the modiﬁed problem, i.e., the
solution to equation (54). If (cid:96)j(y(cid:48)
(L,j) = y(cid:48)
(L,j), x) ≥ 0, then y(cid:63)
(L,j) because we know the
solution lies in the region (cid:96)j(y(L,j), x) ≥ 0 and the objective of problem (53) and the
modiﬁed objective are equal on that region.
(L,j), x) ≥ 0
whenever (cid:96)j(y(C,j) − αj/ρ, x) ≥ 0, because the modiﬁed term is symmetric about the line
(cid:96)j(y(L,j), x) = 0. We therefore will only reach the following third case when pj = 1. If
(cid:96)j(y(C,j) − αj/ρ, x) > 0 and (cid:96)j(y(cid:48)
(L,j) is the
projection of y(C,j) − αj/ρ onto the hyperplane ck(y(L,j), x) = 0. This constraint must be
active because it is violated by the optimizers of both modiﬁed objectives (Martins et al.,
2015, Lemma 17). Since the potential has a value of zero whenever the constraint is active,
solving problem (53) reduces to the projection operation.

(L,j), x) < 0, then we can conclude that y(cid:63)

In fact, if pj = 2, then (cid:96)j(y(cid:48)

30

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

For the local copies y(L,k+m) for each constraint ck, the subproblem is easier:

(cid:104)

(cid:105)
ck(y(L,k+m), x)

+

χk

(cid:13)
(cid:13)
y(L,k+m) − y(C,k+m) +
(cid:13)
(cid:13)

ρ
2

1
ρ

arg min
y(L,k+m)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

αk+m

.

(55)

Whether ck is an equality or inequality constraint, the solution is the projection of y(C,k+m)−
αk+m/ρ to the feasible set deﬁned by the constraint. If ck is an equality constraint, i.e., k ∈
E, then the optimizer y(cid:63)
(L,k+m) is the projection of y(C,k+m)−αk+m/ρ onto ck(y(L,k+m), x) =
0. If, on the other hand, ck is an inequality constraint, i.e., k ∈ I, then there are two cases.
First, if ck(y(C,k+m) − αk+m/ρ, x) ≤ 0, then the solution is simply y(C,k+m) − αk+m/ρ.
Otherwise, it is again the projection onto ck(y(L,k+m), x) = 0.
To update the variables y (49), we solve the optimization

arg min
y

n
(cid:88)

i=1

ρ
2

m+r
(cid:88)

ˆi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

χ[0,1] [yi] +

y

(L,ˆi)

− y

(C,ˆi)

+

(56)

1
ρ

(cid:13)
2
(cid:13)
αˆi
(cid:13)
(cid:13)
2

.

The optimizer is the state in which yi is set to the average of its corresponding local copies
added with their corresponding Lagrange multipliers divided by the step size ρ, and then
clipped to the [0, 1] interval. More formally, let copies(yi) be the set of local copies yc of
yi, each with a corresponding Lagrange multiplier αc. Then, we update each yi using

yi ←

1
|copies(yi)|

(cid:88)

yc∈copies(yi)

(cid:18)

yc +

(cid:19)

αc
ρ

(57)

and clip the result to [0, 1]. Speciﬁcally, if, after update (57), yi > 1, then we set yi to 1
and likewise set it to 0 if yi < 0.

Algorithm 1 shows the complete pseudocode for MAP inference. The method starts
by initializing local copies of the variables that appear in each potential and constraint,
along with a corresponding Lagrange multiplier for each copy. Then, until convergence, it
iteratively performs the updates (47), (48), and (49). In the pseudocode, we have interleaved
updates (47) and (48), updating both the Lagrange multipliers αˆi and the local copies y
(L,ˆi)
together for each subproblem, because they are local operations that do not depend on other
variables once y is updated in the previous iteration. This independence reveals another
advantage of our inference algorithm:
it is very easy to parallelize. The updates (47)
and (48) can be performed in parallel, the results gathered, update (49) performed, and the
updated y broadcast back to the subproblems. Parallelization makes our MAP inference
algorithm even faster and more scalable.

5.3 Lazy MAP Inference

One interesting and useful property of HL-MRFs is that it is not always necessary to
completely materialize the distribution in order to ﬁnd a MAP state. Consider a subset ˆφ
of the index set {1, . . . , m} of the potentials φ. Observe that if a feasible assignment to y
minimizes

(58)

wjφj(y, x)

(cid:88)

j∈ ˆφ

31

Bach, Broecheler, Huang, and Getoor

Algorithm 1 MAP Inference for HL-MRFs

Input: HL-MRF P (y|x), ρ > 0
Initialize y(L,j) as local copies of variables y(C,j) that are in φj, j = 1, . . . , m
Initialize y(L,k+m) as local copies of variables y(C,k+m) that are in ck, k = 1, . . . , r
, ˆi = 1, . . . , m + r
Initialize Lagrange multipliers αˆi corresponding to copies y

(L,ˆi)

(cid:96)j(y(L,j), x)

(cid:17)pj

+ ρ
2

(cid:13)
(cid:13)y(L,j) − y(C,j) + 1
(cid:13)

ρ αj

(cid:13)
2
(cid:13)
(cid:13)
2

while not converged do

for j = 1, . . . , m do

ρ αj

αj ← αj + ρ(y(L,j) − y(C,j))
y(L,j) ← y(C,j) − 1
if (cid:96)j(y(L,j), x) > 0 then
y(L,j) ← arg miny(L,j)
wj
if (cid:96)j(y(L,j), x) < 0 then

(cid:16)

y(L,j) ← Proj(cid:96)j =0(y(C,j) − 1

ρ αj)

end if

end if
end for

for k = 1, . . . , r do

αk+m ← αk+m + ρ(y(L,k+m) − y(C,k+m))
y(L,k+m) ← Projck

(y(C,k+m) − 1

ρ αk+m)

end for

for i = 1, . . . , n do

1
yi ←
|copies(yi)|
Clip yi to [0,1]

(cid:80)

end for

end while

yc∈copies(yi)

(cid:16)

yc + αc
ρ

(cid:17)

32

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

and φj(y, x) = 0, ∀j /∈ ˆφ, then that assignment must be a MAP state because 0 is the
global minimum for any potential. Therefore, if we can identify a set of potentials that
is small, such that all the other potentials are 0 in a MAP state, then we can perform
MAP inference in a reduced amount of time. Of course, identifying this set is as hard as
MAP inference itself, but we can iteratively grow the set by starting with an initial set,
performing inference over the current set, adding any potentials that have nonzero values,
and repeating.

Since the lazy inference procedure requires that the assignment be feasible, there are
two ways to handle any constraints in the HL-MRF. One is to include all constraints in
the inference problem from the beginning. This strategy ensures feasibility, but the idea of
lazy grounding can also be extended to constraints to improve performance further. Just
as we check if potentials are unsatisﬁed, i.e., nonzero, we can also check if constraints are
unsatisﬁed, i.e., violated. So the algorithm now iteratively grows the set of active potentials
and active constraints, adding any that are unsatisﬁed until the MAP state of the HL-MRF
deﬁned by the active potentials and constraints is also a feasible MAP state of the true
HL-MRF.

The eﬃciency of lazy MAP inference can be improved heuristically by not adding all
unsatisﬁed potentials and constraints, but instead only adding those that are unsatisﬁed by
some threshold. This heuristic can decrease computational cost signiﬁcantly, although the
results are no longer guaranteed to be correct. Bounding the resulting error when possible
is an important direction for future work.

5.4 Evaluation of MAP Inference

In this section we evaluate the empirical performance of our MAP inference algorithm.4 We
compare its running times against those of MOSEK,5 a commercial convex optimization
toolkit that uses interior-point methods (IPMs). We conﬁrm the results of Yanover et al.
(2006) that IPMs do not scale well to large structured-prediction problems, and we show
that our MAP inference algorithm scales much better. In fact, we observe that our method
scales linearly in practice with the number of potentials and constraints in the HL-MRF.

We evaluate scalability by generating social networks of varying sizes, constructing HL-
MRFs over them, and measuring the running time required to ﬁnd a MAP state. We
compare our algorithm to MOSEK’s IPM. The social networks we generate are designed to
be representative of common social-network analysis tasks. We generate networks of users
that are connected by diﬀerent types of relationships, such as friendship and marriage, and
our goal is to predict the political preferences, e.g., liberal or conservative, of each user. We
also assume that we have local information about each user, representing features such as
demographic information.

We generate the social networks using power-law distributions according to a procedure
described by Broecheler et al. (2010b). For a target number of users N , in-degrees and out-
degrees d for each edge type are sampled from the power-law distribution D(k) ≡ αk−γ.
Incoming and outgoing edges of the same type are then matched randomly to create edges
until no more matches are possible. The number of users is initially the target number

4. Code is available at https://github.com/stephenbach/bach-jmlr17-code.
5. http://www.mosek.com

33

Bach, Broecheler, Huang, and Getoor

plus the expected number of users with zero edges, and then users without any edges are
removed. We use six edge types with various parameters to represent relationships in social
networks with diﬀerent combinations of abundance and exclusivity, choosing γ between 2
and 3, and α between 0 and 1, as suggested by Broecheler et al. We then annotate each
vertex with a value in [−1, 1] uniformly at random to represent local features indicating one
political preference or the other.

We generate social networks with between 22k and 66k vertices, which induce HL-
MRFs with between 130k and 397k total potentials and constraints. In all the HL-MRFs,
roughly 85% of those totals are potentials. For each social network, we create both a (log)
piecewise-linear HL-MRF (pj = 1, ∀j = 1, . . . , m in Deﬁnition 3) and a piecewise-quadratic
one (pj = 2, ∀j = 1, . . . , m). We weight local features with a parameter of 0.5 and choose
parameters in [0, 1] for the relationship potentials representing a mix of more and less
inﬂuential relationships.

We implement ADMM in Java and compare with the IPM in MOSEK (version 6) by
encoding the entire MPE problem as a linear program or a second-order cone program as
appropriate and passing the encoded problem via the Java native interface wrapper. All
experiments are performed on a single machine with a 4-core 3.4 GHz Intel Core i7-3770
processor with 32GB of RAM. Each optimizer used a single thread, and all results are
averaged over 3 runs.

We ﬁrst evaluate the scalability of ADMM when solving piecewise-linear MAP problems
and compare with MOSEK’s interior-point method. Figures 1a (normal scale) and 1c (log
scale) show the results. The running time of the IPM quickly explodes as the problem
size increases. The IPM’s average running time on the largest problem is about 2,200
seconds (37 minutes). This result demonstrates the limited scalability of the interior-point
method. In contrast, ADMM displays excellent scalability. The average running time on
the largest problem is about 70 seconds. Further, the running time appears to grow linearly
in the number of potential functions and constraints in the HL-MRF, i.e., the number of
subproblems that must be solved at each iteration. The line of best ﬁt for all runs on all sizes
has a coeﬃcient of determination R2 = 0.9972. Combined with Figure 1a, this shows that
ADMM scales linearly with increasing problem size in this experiment. We emphasize that
the implementation of ADMM is research code written in Java and the IPM is a commercial
package compiled to native machine code.

We then evaluate the scalability of ADMM when solving piecewise-quadratic MAP prob-
lem and again compare with MOSEK. Figures 1b (normal scale) and 1d (log scale) show
the results. Again, the running time of the interior-point method quickly explodes. We
can only test it on the three smallest problems, the largest of which took an average of
about 21k seconds to solve (over 6 hours). ADMM again scales linearly to the problem
(R2 = 0.9854). It is just as fast for quadratic problems as linear ones, taking average of
about 70 seconds on the largest problem.

One of the advantages of IPMs is great numerical stability and accuracy. Consensus
optimization, which treats both objective terms and constraints as subproblems, often re-
turns solutions that are only optimal and feasible to moderate precision for non-trivially
constrained problems (Boyd et al., 2011). Although this is often acceptable, we quantify
the mix of infeasibility and suboptimality by repairing the infeasibility and measuring the
resulting total suboptimality. We ﬁrst project the solutions returned by consensus opti-

34

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

(a) Linear MAP problems

(b) Quadratic MAP problems

(c) Linear MAP problems (log scale)

(d) Quadratic MAP problems (log scale)

Figure 1: Average running times to ﬁnd a MAP state for HL-MRFs.

35

Bach, Broecheler, Huang, and Getoor

mization onto the feasible region, which took a negligible amount of computational time.
Let pADMM be the value of the objective in Problem (45) at such a point and let pIPM be
the value of the objective at the solution returned by the IPM. Then the relative error on
that problem is (pADMM − pIPM)/pIPM. The relative error was consistently small; it varied
between 0.2% and 0.4%, and did not trend upward as the problem size increased. This
shows that ADMM was accurate, in addition to being much more scalable.

6. Weight Learning

In this section we present three weight learning methods for HL-MRFs, each with a diﬀerent
objective function. The ﬁrst method approximately maximizes the likelihood of the training
data. The second method maximizes the pseudolikelihood. The third method ﬁnds a large-
margin solution, preferring weights that discriminate the ground truth from other nearby
states. Since weights are often shared among many potentials deﬁned by a template, such
as all the groundings of a PSL rule, we describe these learning algorithms in terms of
templated HL-MRFs. We introduce some necessary notation for HL-MRF templates. Let
T = (t1, . . . , ts) denote a vector of templates with associated weights W = (W1, . . . , Ws).
We partition the potentials by their associated templates and let tq also denote the set of
indices of the potentials deﬁned by that template. So, j ∈ tq is a shorthand for saying
that the potential φj(y, x) was deﬁned by template tq. Then, we refer to the sum of the
potentials deﬁned by a template as

Φq(y, x) =

φj(y, x) .

(cid:88)

j∈tq

In the deﬁned HL-MRF, the weight of the j-th hinge-loss potential is set to the weight of
the template from which it was derived, i.e., wj = Wq, for each j ∈ tq. Equivalently, we can
rewrite the hinge-loss energy function as

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)).

fw(y, x) = W (cid:62)Φ(y, x) ,

6.1 Structured Perceptron and Approximate Maximum Likelihood Estimation

The canonical approach for learning parameters W is to maximize the log-likelihood of
training data. The partial derivative of the log-likelihood with respect to a parameter Wq
is

∂ log P (y|x)
∂Wq

= EW [Φq(y, x)] − Φq(y, x),

(61)

where EW is the expectation under the distribution deﬁned by W . For a smoother ascent,
it is often helpful to divide the q-th component of the gradient by the number of groundings
|tq| of the q-th template (Lowd and Domingos, 2007), which we do in our experiments.
Computing the expectation is intractable, so we use a common approximation (e.g., Collins,
2002; Singla and Domingos, 2005; Poon and Domingos, 2011): the values of the potentials
at the most probable setting of y with the current parameters, i.e., a MAP state. Using a
MAP state makes this learning approach a structured variant of voted perceptron (Collins,

36

(59)

(60)

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

2002), and we expect it to do best when the space of explored distributions has relatively
low entropy. Following voted perceptron, we take steps of ﬁxed length in the direction of
the gradient, then average the points after all steps. Any step that is outside the feasible
region is projected back before continuing.

6.2 Maximum Pseudolikelihood Estimation

An alternative to structured perceptron is maximum-pseudolikelihood estimation (MPLE)
(Besag, 1975), which maximizes the likelihood of each variable conditioned on all other
variables, i.e.,

P ∗(y|x) =

P ∗(yi|MB(yi), x)

=

1
Zi(W , y, x)

exp (cid:2)−f i

w(yi, y, x)(cid:3) ;

Zi(W , y, x) =

exp (cid:2)−f i

f i
w(yi, y, x) =

wjφj

w(yi, y, x)(cid:3) ;
(cid:16)

{yi ∪ y\i}, x

(cid:17)

.

n
(cid:89)

i=1
n
(cid:89)

i=1
(cid:90)

yi
(cid:88)

j:i∈φj

(62)

(63)

(64)

(65)

Here, i ∈ φj means that yi is involved in φj, and MB(yi) denotes the Markov blanket of
yi—that is, the set of variables that co-occur with yi in any potential function. The partial
derivative of the log-pseudolikelihood with respect to Wq is

∂ log P ∗(y|x)
∂Wq

=





Eyi|MB

n
(cid:88)

i=1

(cid:88)

j∈tq:i∈φj



φj(y, x)

 − Φq(y, x) .

(66)

Computing the pseudolikelihood gradient does not require joint inference and takes time
linear in the size of y. However, the integral in the above expectation does not readily admit
a closed-form antiderivative, so we approximate the expectation. When a variable is uncon-
strained, the domain of integration is a one-dimensional interval on the real number line,
so Monte Carlo integration quickly converges to an accurate estimate of the expectation.

We can also apply MPLE when the constraints are not too interdependent. For example,
for linear equality constraints over disjoint groups of variables (e.g., variable sets that must
sum to 1.0), we can block-sample the constrained variables by sampling uniformly from a
simplex. These types of constraints are often used to represent categorical labels. We can
compute accurate estimates quickly because these blocks are typically low-dimensional.

6.3 Large-Margin Estimation

A diﬀerent approach to learning drops the probabilistic interpretation of the model and
views HL-MRF inference as a prediction function. Large-margin estimation (LME) shifts
the goal of learning from producing accurate probabilistic models to instead producing
accurate MAP predictions. The learning task is then to ﬁnd weights W that separate
the ground truth from other nearby states by a large margin. We describe in this section

37

Bach, Broecheler, Huang, and Getoor

a large-margin method based on the cutting-plane approach for structural support vector
machines (Joachims et al., 2009).

The intuition behind large-margin structured prediction is that the ground-truth state
should have energy lower than any alternate state by a large margin. In our setting, the
output space is continuous, so we parameterize this margin criterion with a continuous loss
function. For any valid output state ˜y, a large-margin solution should satisfy

fw(y, x) ≤ fw(˜y, x) − L(y, ˜y), ∀˜y,

(67)

where the loss function L(y, ˜y) measures the disagreement between a state ˜y and the train-
ing label state y. A common assumption is that the loss function decomposes over the
prediction components, i.e., L(y, ˜y) = (cid:80)
i L(yi, ˜yi). In this work, we use the (cid:96)1 distance
as the loss function, so L(y, ˜y) = (cid:80)
i (cid:107)yi − ˜yi(cid:107)1. Since we do not expect all problems to
be perfectly separable, we relax the large-margin constraint with a penalized slack ξ. We
obtain a convex learning objective for a large-margin solution

min
W ≥0

1
2

||W ||2 + Cξ

s.t. W (cid:62)(Φ(y, x) − Φ(˜y, x)) ≤ −L(y, ˜y) + ξ, ∀˜y,

(68)

where Φ(y, x) = (Φ1(y, x), . . . , Φs(y, x)) and C > 0 is a user-speciﬁed parameter. This for-
mulation is analogous to the margin-rescaling approach by Joachims et al. (2009). Though
such a structured objective is natural and intuitive, its number of constraints is the car-
dinality of the output space, which here is inﬁnite. Following their approach, we optimize
subject to the inﬁnite constraint set using a cutting-plane algorithm: we greedily grow a set
K of constraints by iteratively adding the worst-violated constraint given by a separation
oracle, then updating W subject to the current constraints. The goal of the cutting-plane
approach is to eﬃciently ﬁnd the set of active constraints at the solution for the full ob-
jective, without having to enumerate the inﬁnite inactive constraints. The worst-violated
constraint is

arg min
˜y

W (cid:62)Φ(˜y, x) − L(y, ˜y).

(69)

The separation oracle performs loss-augmented inference by adding additional potentials to
the HL-MRF. For ground truth in {0, 1}, these loss-augmenting potentials are also examples
of hinge-losses, and thus adding them simply creates an augmented HL-MRF. The worst-
violated constraint is then computed as standard inference on the loss-augmented HL-
MRF. However, ground truth values in the interior (0, 1) cause any distance-based loss to
be concave, which require the separation oracle to solve a non-convex objective. In this
case, we use the diﬀerence of convex functions algorithm (An and Tao, 2005) to ﬁnd a local
optimum. Since the concave portion of the loss-augmented inference objective pivots around
the ground truth value, the subgradients are 1 or −1, depending on whether the current
value is greater than the ground truth. We simply choose an initial direction for interior
labels by rounding, and ﬂip the direction of the subgradients for variables whose solution
states are not in the interval corresponding to the subgradient direction until convergence.

38

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Given a set K of constraints, we solve the SVM objective as in the primal form

||W ||2 + Cξ

min
W ≥0

1
2

s.t. K.

(70)

We then iteratively invoke the separation oracle to ﬁnd the worst-violated constraint. If
this new constraint is not violated, or its violation is within numerical tolerance, we have
found the max-margin solution. Otherwise, we add the new constraint to K, and repeat.

One fact of note is that the large-margin criterion always requires some slack for HL-
MRFs with squared potentials. Since the squared hinge potential is quadratic and the loss
is linear, there always exists a small enough distance from the ground truth such that an
absolute (i.e., linear) distance is greater than the squared distance. In these cases, the slack
parameter trades oﬀ between the peakedness of the learned quadratic energy function and
the margin criterion.

6.4 Evaluation of Learning

To demonstrate the ﬂexibility and eﬀectiveness of learning with HL-MRFs, we test them
on four diverse tasks: node labeling, link labeling, link prediction, and image completion.6
Each of these experiments represents a problem domain that is best solved with structured-
prediction approaches because their dependencies are highly structural. The experiments
show that HL-MRFs perform as well as or better than canonical approaches.

For these diverse tasks, we compare against a number of competing methods. For node
and link labeling, we compare HL-MRFs to discrete Markov random ﬁelds (MRFs). We
construct them with Markov logic networks (MLNs) (Richardson and Domingos, 2006),
which template discrete MRFs using logical rules similarly to PSL. We perform inference in
discrete MRFs using Gibbs sampling, and we ﬁnd approximate MAP states during learn-
ing using the search algorithm MaxWalkSat (Richardson and Domingos, 2006). For link
prediction for preference prediction, a task that is inherently continuous and nontrivial to
encode in discrete logic, we compare against Bayesian probabilistic matrix factorization
(BPMF) (Salakhutdinov and Mnih, 2008). Finally, for image completion, we run the same
experimental setup as Poon and Domingos (2011) and compare against the results they
report, which include tests using sum product networks, deep belief networks (Hinton and
Salakhutdinov, 2006), and deep Boltzmann machines (Salakhutdinov and Hinton, 2009).

We train HL-MRFs and discrete MRFs with all three learning methods: structured per-
ceptron (SP), maximum pseudolikelihood estimation(MPLE), and large-margin estimation
(LME). When appropriate, we evaluate statistical signiﬁcance using a paired t-test with re-
jection threshold 0.01. We describe the HL-MRFs used for our experiments using the PSL
rules that deﬁne them. To investigate the diﬀerences between linear and squared potentials
we use both in our experiments. HL-MRF-L refers to a model with all linear potentials
and HL-MRF-Q to one with all squared potentials. When training with SP and MPLE, we
use 100 gradient steps and a step size of 1.0 (unless otherwise noted), and we average the
iterates as in voted perceptron. For LME, we set C = 0.1. We experimented with various
settings, but the scores of HL-MRFs and discrete MRFs were not sensitive to changes.

6. Code is available at https://github.com/stephenbach/bach-jmlr17-code.

39

Bach, Broecheler, Huang, and Getoor

6.4.1 Node Labeling

When classifying documents, links between those documents—such as hyperlinks, citations,
or shared authorship—provide extra signal beyond the local features of individual docu-
ments. Collectively predicting document classes with these links tends to improve accuracy
(Sen et al., 2008). We classify documents in citation networks using data from the Cora
and Citeseer scientiﬁc paper repositories. The Cora data set contains 2,708 papers in seven
categories, and 5,429 directed citation links. The Citeseer data set contains 3,312 papers
in six categories, and 4,591 directed citation links. Let the predicate Category/2 represent
the category of each document and Cites/2 represent a citation from one document to
another.

The prediction task is, given a set of seed documents whose labels are observed, to infer
the remaining document classes by propagating the seed information through the network.
For each of 20 runs, we split the data sets 50/50 into training and testing partitions, and
seed half of each set. To predict discrete categories with HL-MRFs we predict the category
with the highest predicted value.

We compare HL-MRFs to discrete MRFs on this task. For prediction, we performed
2500 rounds of Gibbs sampling, 500 of which were discarded as burn-in. We construct both
using the same logical rules, which simply encode the tendency for a class to propagate
across citations. For each category "C i", we have the following two rules, one for each
direction of citation.

Category(A, "C i") && Cites(A, B) -> Category(B, "C i")

Category(A, "C i") && Cites(B, A) -> Category(B, "C i")

We also constrain the atoms of the Category/2 predicate to sum to 1.0 for a given document
as follows.

Category(D, +C) = 1.0 .

Table 1 lists the results of this experiment. HL-MRFs are the most accurate predictors on
both data sets. Both variants of HL-MRFs are also much faster than discrete MRFs. See
Table 3 for average inference times over ﬁve folds.

6.4.2 Link Labeling

An emerging problem in the analysis of online social networks is the task of inferring the
level of trust between individuals. Predicting the strength of trust relationships can provide
useful information for viral marketing, recommendation engines, and internet security. HL-
MRFs with linear potentials have been applied by Huang et al. (2013) to this task, showing
superior results with models based on sociological theory. We reproduce their experimen-
tal setup using their sample of the signed Epinions trust network, orginally collected by
Richardson et al. (2003), in which users indicate whether they trust or distrust other users.
We perform eight-fold cross-validation. In each fold, the prediction algorithm observes the
entire unsigned social network and all but 1/8 of the trust ratings. We measure predic-
tion accuracy on the held-out 1/8. The sampled network contains 2,000 users, with 8,675
signed links. Of these links, 7,974 are positive and only 701 are negative, making it a sparse
prediction task.

40

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Table 1: Average accuracy of classiﬁcation by HL-MRFs and discrete MRFs. Scores statis-
tically equivalent to the best scoring method are typed in bold.

Table 2: Average area under ROC and precision-recall curves of social-trust prediction by
HL-MRFs and discrete MRFs. Scores statistically equivalent to the best scoring method
by metric are typed in bold.

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

Citeseer Cora

0.729
0.729
0.683

0.724
0.729
0.695

0.686
0.715
0.687

0.816
0.818
0.789

0.802
0.808
0.789

0.756
0.797
0.783

ROC P-R (+) P-R (-)

0.822
HL-MRF-Q (SP)
HL-MRF-Q (MPLE) 0.832
0.814
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

MRF (SP)
MRF (MPLE)
MRF (LME)

0.765
0.757
0.783

0.655
0.725
0.795

0.978
0.979
0.976

0.965
0.963
0.967

0.942
0.963
0.973

0.452
0.482
0.462

0.357
0.333
0.453

0.270
0.298
0.441

41

Table 3: Average inference times (reported in seconds) of single-threaded HL-MRFs and
discrete MRFs.

Bach, Broecheler, Huang, and Getoor

Citeseer

Cora Epinions

HL-MRF-Q
HL-MRF-L
MRF

0.42
0.46
110.96

0.70
0.50
184.32

0.32
0.28
212.36

We use a model based on the social theory of structural balance, which suggests that so-
cial structures are governed by a system that prefers triangles that are considered balanced.
Balanced triangles have an odd number of positive trust relationships; thus, considering all
possible directions of links that form a triad of users, there are sixteen logical implications
of the following form.

Trusts(A,B) && Trusts(B,C) -> Trusts(A,C)

Huang et al. (2013) list all sixteen of these rules, a reciprocity rule, and a prior in their
Balance-Recip model, which we omit to save space.

Since we expect these structural implications to vary in accuracy, learning weights for
these rules provides better models. Again, we use these rules to deﬁne HL-MRFs and
discrete MRFs, and we train them using various learning algorithms. For inference with
discrete MRFs, we perform 5000 rounds of Gibbs sampling, of which the ﬁrst 500 are
burn-in. We compute three metrics: the area under the receiver operating characteristic
(ROC) curve, and the areas under the precision-recall curves for positive trust and negative
trust. On all three metrics, HL-MRFs with squared potentials score signiﬁcantly higher.
The diﬀerences among the learning methods for squared HL-MRFs are insigniﬁcant, but
the diﬀerences among the models is statistically signiﬁcant for the ROC metric. For area
under the precision-recall curve for positive trust, discrete MRFs trained with LME are
statistically tied with the best score, and both HL-MRF-L and discrete MRFs trained with
LME are statistically tied with the best area under the precision-recall curve for negative
trust. The results are listed in Table 2.

Though the random fold splits are not the same, using the same experimental setup,
Huang et al. (2013) also scored the precision-recall area for negative trust of standard trust
prediction algorithms EigenTrust (Kamvar et al., 2003) and TidalTrust (Golbeck, 2005),
which scored 0.131 and 0.130, respectively. The logical models based on structural balance
that we run here are signiﬁcantly more accurate, and HL-MRFs more than discrete MRFs.
In addition to comparing favorably with regard to predictive accuracy, inference in HL-
MRFs is also much faster than in discrete MRFs. Table 3 lists average inference times
on ﬁve folds of three prediction tasks: Cora, Citeseer, and Epinions. This illustrates an
important diﬀerence between performing structured prediction via convex inference versus
sampling in a discrete prediction space: convex inference can be much faster.

42

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

6.4.3 Link Prediction

Preference prediction is the task of inferring user attitudes (often quantiﬁed by ratings)
toward a set of items. This problem is naturally structured, since a user’s preferences
are often interdependent, as are an item’s ratings. Collaborative ﬁltering is the task of
predicting unknown ratings using only a subset of observed ratings. Methods for this
task range from simple nearest-neighbor classiﬁers to complex latent factor models. More
generally, this problem is an instance of link prediction, since the goal is to predict links
indicating preference between users and content. Since preferences are ordered rather than
Boolean, it is natural to represent them with the continuous variables of HL-MRFs, with
higher values indicating greater preference. To illustrate the versatility of HL-MRFs, we
design a simple, interpretable collaborative ﬁltering model for predicting humor preferences.
We test this model on the Jester dataset, a repository of ratings from 24,983 users on a set
of 100 jokes (Goldberg et al., 2001). Each joke is rated on a scale of [−10, +10], which we
normalize to [0, 1]. We sample a random 2,000 users from the set of those who rated all 100
jokes, which we then split into 1,000 train and 1,000 test users. From each train and test
matrix, we sample a random 50% to use as the observed features x; the remaining ratings
are treated as the variables y.

Our HL-MRF model uses an item-item similarity rule:

SimRating(J1, J2) && Likes(U, J1) -> Likes(U, J2)

where J1 and J2 are jokes and U is a user; the predicate Likes/2 indicates the degree
of preference (i.e., rating value); and SimRating/2 is a closed predicate that measures
the mean-adjusted cosine similarity between the observed ratings of two jokes. We also
include the following rules to enforce that Likes(U,J) concentrates around the observed
average rating of user U (represented with the predicate AvgUserRating/1) and item J
(represented with the predicate AvgJokeRating/1), and the global average (represented
with the predicate AvgRating/1).

AvgUserRating(U) -> Likes(U, J)

Likes(U, J) -> AvgUserRating(U)

AvgJokeRating(J) -> Likes(U, J)

Likes(U, J) -> AvgJokeRating(J)

AvgRating("constant") -> Likes(U, J)

Likes(U, J) -> AvgRating("constant")

The atom AvgRating("constant") takes a placeholder constant as an argument, since there
is only one grounding of it for the entire HL-MRF. Again, all three of these predicates are
closed and computed using averages of observed ratings. In all cases, the observed ratings
are taken only from the training data for learning (to avoid leaking information about the
test data) and only from the test data during testing.

We compare our HL-MRF model to a canonical latent factor model, Bayesian proba-
bilistic matrix factorization (BPMF) (Salakhutdinov and Mnih, 2008). BPMF is a fully
Bayesian treatment and is therefore considered “parameter-free;” the only parameter that
must be speciﬁed is the rank of the decomposition. Based on settings used by Xiong et al.

43

Table 4: Normalized mean squared/absolute errors (NMSE/NMAE) for preference predic-
tion using the Jester dataset. The lowest errors are typed in bold.

Bach, Broecheler, Huang, and Getoor

NMSE NMAE

HL-MRF-Q (SP)
HL-MRF-Q (MPLE)
HL-MRF-Q (LME)

HL-MRF-L (SP)
HL-MRF-L (MPLE)
HL-MRF-L (LME)

0.0554
0.0549
0.0738

0.0578
0.0535
0.0544

0.1974
0.1953
0.2297

0.2021
0.1885
0.1875

BPMF

0.0501 0.1832

Table 5: Mean squared errors per pixel for image completion. HL-MRFs produce the
most accurate completions on the Caltech101 and the left-half Olivetti faces, and only sum-
product networks produce better completions on Olivetti bottom-half faces. Scores for other
methods are reported in Poon and Domingos (2011).

HL-MRF-Q (SP) SPN DBM DBN PCA NN

Caltech-Left
Caltech-Bottom
Olivetti-Left
Olivetti-Bottom

1741
1910
927
1226

1815
1924
942
918

2998
2656
1866
2401

4960
3447
2386
1931

2851
1944
1076
1265

2327
2575
1527
1793

(2010), we set the rank of the decomposition to 30 and use 100 iterations of burn in and 100
iterations of sampling. For our experiments, we use the code of Xiong et al. (2010). Since
BPMF does not train a model, we allow BPMF to use all of the training matrix during the
prediction phase.

Table 4 lists the normalized mean squared error (NMSE) and normalized mean absolute
error (NMAE), averaged over 10 random splits. Though BPMF produces the best scores,
the improvement over HL-MRF-L (LME) is not signiﬁcant in NMAE.

6.4.4 Image Completion

Digital image completion requires models that understand how pixels relate to each other,
such that when some pixels are unobserved, the model can infer their values from parts of the
image that are observed. We construct pixel-grid HL-MRFs for image completion. We test
these models using the experimental setup of Poon and Domingos (2011): we reconstruct
images from the Olivetti face data set and the Caltech101 face category. The Olivetti data
set contains 400 images, 64 pixels wide and tall, and the Caltech101 face category contains
435 examples of faces, which we crop to the center 64 by 64 patch, as was done by Poon

44

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

Figure 2: Example results on image completion of Caltech101 (left) and Olivetti (right)
faces. From left to right in each column: (1) true face, left side predictions by (2) HL-
MRFs and (3) SPNs, and bottom half predictions by (4) HL-MRFs and (5) SPNs. SPN
completions are downloaded from Poon and Domingos (2011).

and Domingos (2011). Following their experimental setup, we hold out the last ﬁfty images
and predict either the left half of the image or the bottom half.

The HL-MRFs in this experiment are much more complex than the ones in our other
experiments because we allow each pixel to have its own weight for the following rules,
which encode agreement or disagreement between neighboring pixels:

Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> Bright(Q, I)

!Bright("P ij", I) && North("P ij", Q) -> !Bright(Q, I)

where Bright("P ij", I) is the normalized brightness of pixel "P ij" in image I, and
North("P ij", Q) indicates that Q is the north neighbor of "P ij". We similarly include
analogous rules for the south, east, and west neighbors, as well as the pixels mirrored across
the horizontal and vertical axes. This setup results in up to 24 rules per pixel, (boundary
pixels may not have north, south, east, or west neighbors) which, in a 64 by 64 image,
produces 80,896 PSL rules.

We train these HL-MRFs using SP with a 5.0 step size on the ﬁrst 200 images of each data
set and test on the last ﬁfty. For training, we maximize the data log-likelihood of uniformly
random held-out pixels for each training image, allowing for generalization throughout the
image. Table 5 lists our results and others reported by Poon and Domingos (2011) for sum-
product networks (SPN), deep Boltzmann machines (DBM), deep belief networks (DBN),
principal component analysis (PCA), and nearest neighbor (NN). HL-MRFs produce the
best mean squared error on the left- and bottom-half settings for the Caltech101 set and
the left-half setting in the Olivetti set. Only sum product networks produce lower error

45

Bach, Broecheler, Huang, and Getoor

on the Olivetti bottom-half faces. Some reconstructed faces are displayed in Figure 2,
where the shallow, pixel-based HL-MRFs produce comparably convincing images to sum-
product networks, especially in the left-half setting, where HL-MRFs can learn which pixels
are likely to mimic their horizontal mirror. While neither method is particularly good at
reconstructing the bottom half of faces, the qualitative diﬀerence between the deep SPN
and the shallow HL-MRF completions is that SPNs seem to hallucinate diﬀerent faces, often
with some artifacts, while HL-MRFs predict blurry shapes roughly the same pixel intensity
as the observed, top half of the face. The tendency to better match pixel intensity helps
HL-MRFs score better quantitatively on the Caltech101 faces, where the lighting conditions
are more varied than in Olivetti faces.

Training and predicting with these HL-MRFs takes little time.

In our experiments,
training each model takes about 45 minutes on a 12-core machine, while predicting takes
under a second per image. While Poon and Domingos (2011) report faster training with
SPNs, both HL-MRFs and SPNs clearly belong to a class of faster models when compared
to DBNs and DBMs, which can take days to train on modern hardware.

7. Related Work

Researchers in artiﬁcial intelligence and machine learning have long been interested in pre-
dicting interdependent unknowns using structural dependencies. Some of the earliest work
in this area is inductive logic programming (ILP) (Muggleton and De Raedt, 1994), in
which structural dependencies are described with ﬁrst-order logic. Using ﬁrst-order logic
has several advantages. First, it can capture many types of dependencies among variables,
such as correlations, anti-correlations, and implications. Second, it can compactly specify
dependencies that hold across many diﬀerent sets of propositions by using variables as wild-
cards that match entities in the data. These features enable the construction of intuitive,
general-purpose models that are easily applicable or adapted to diﬀerent domains. Inference
for ILP ﬁnds the propositions that satisfy a query, consistent with a relational knowledge
base. However, ILP is limited by its diﬃculty in coping with uncertainty. Standard ILP
approaches only model dependencies which hold universally, and such dependencies are rare
in real-world data.

Another broad area of research, probabilistic methods, directly models uncertainty over
unknowns. Probabilistic graphical models (PGMs) (Koller and Friedman, 2009) are a fam-
ily of formalisms for specifying joint distributions over interdependent unknowns through
graphical structures. The graphical structure of a PGM generally represents conditional
independence relationships among random variables. Explicitly representing conditional
independence relationships allows a distribution to be more compactly parametrized. For
example, in the worst case, a discrete distribution could be represented by an exponen-
tially large table over joint assignments to the random variables. However, describing the
distribution in smaller, conditionally independent pieces can be much more compact. Sim-
ilar beneﬁts apply to continuous distributions. Algorithms for probabilistic inference and
learning can also operate over the conditionally independent pieces described by the graph
structure. They are therefore straightforward to apply to a wide variety of distributions.
Categories of PGMs include Markov random ﬁelds (MRFs), Bayesian networks (BNs), and

46

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

dependency networks (DNs). Constructing PGMs often requires careful design, and models
are usually constructed for single tasks and data sets.

More recently, researchers have sought to combine the advantages of relational and
probabilistic approaches, creating the ﬁeld of statistical relational learning (SRL) (Getoor
and Taskar, 2007). SRL techniques build probabilistic models of relational data, i.e., data
composed of entities and relationships connecting them. Relational data is most often
described using a relational calculus, but SRL techniques are also equally applicable to
similar categories of data that go by other names, such as graph data or network data.
Modeling relational data is inherently complicated by the large number of interconnected
and overlapping structural dependencies that are typically present. This complication has
motivated two directions of work. The ﬁrst direction is algorithmic, seeking inference and
learning methods that scale up to high dimensional models. The other direction is both
user-oriented and—as a growing body of evidence shows—supported by learning theory,
seeking formalisms for compactly specifying entire groups of dependencies in the model
that share both form and parameters. Specifying these grouped dependencies, often in the
form of templates via a domain-speciﬁc language, is convenient for users. Most often in
relational data the structural dependencies hold without regard to the identities of entities,
instead being induced by an entity’s class (or classes) and the structure of its relationships
with other entities. Therefore, many SRL models and languages give users the ability to
specify dependencies in this abstract form and ground out models over speciﬁc data sets
based on these deﬁnitions. In addition to convenience, recent work in learning theory says
that repeated dependencies with tied parameters can be the key to generalizing from a
few—or even one—large, structured training example(s) (London et al., 2016).

A related ﬁeld to SRL is structured prediction (SP) (Bakir et al., 2007; Nowozin et al.,
2016), which generalizes the tasks of classiﬁcation and regression to the task of predict-
ing structured objects. The loss function used during learning is generalized to a task-
appropriate loss function that scores disagreement between predictions and the true struc-
tures. Often, models for structured prediction take the form of energy functions that are
linear in their parameters. Therefore, prediction with such models is equivalent to MAP
inference for MRFs. A distinct branch of SP is learn-to-search methods, in which the prob-
lem is decomposed into a series of one-dimension prediction problems. The challenge is to
learn a good order in which to predict the components of the structure, so that each one-
dimension prediction problem can be conditioned on the most useful information. Examples
of learn-to-search methods include incremental structured perceptron (Collins and Roark,
2004), SEARN (Daum´e III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross
and Bagnell, 2014).

In this paper we focus on SP methods that perform joint prediction directly. Better
understanding the diﬀerences and relative advantages of joint-prediction methods and learn-
to-search methods is an important direction for future work.
In the rest of this section
we survey models and domain-speciﬁc languages for SP and SRL (Section 7.1), inference
methods (Section 7.2), and learning methods (Section 7.3).

47

Bach, Broecheler, Huang, and Getoor

7.1 Models and Languages

SP and SRL encompass many approaches. One broad area of work—of which PSL is a
part—uses ﬁrst-order logic and other relational formalisms to specify templates for PGMs.
Probabilistic relational models (Friedman et al., 1999) deﬁne templates for BNs in terms of a
database schema, and they can be grounded out over instances of that schema to create BNs.
Relational dependency networks (Neville and Jensen, 2007) template RNs using structured
query language (SQL) queries over a relational schema. Markov logic networks (MLNs)
(Richardson and Domingos, 2006) use ﬁrst-order logic to deﬁne Boolean MRFs. Each logical
clause in a ﬁrst-order knowledge base is a template for a set of potentials when the MLN
is grounded out over a set of propositions. Whether each proposition is true is a Boolean
random variable, and the potential has a value of one when the corresponding ground clause
is satisﬁed by the propositions and zero when it is not. (MLNs are formulated such that
higher values of the energy function are more probable.) Clauses can either be weighted,
in which case the potential has the weight of the clause that templated it, or unweighted,
in which case in must hold universally, as in ILP. In these ways, MLNs are similar to
PSL. Whereas MLNs are deﬁned over Boolean variables, PSL is a templating language
for HL-MRFs, which are deﬁned over continuous variables. However, these continuous
variables can be used to model discrete quantities. See Section 2 for more information
on the relationships between HL-MRFs and discrete MRFs, and Section 6.4 for empirical
comparisons between the two. As we show, HL-MRFs and PSL scale much better while
In addition,
retaining the rich expressivity and accuracy of their discrete counterparts.
HL-MRFs and PSL can reason directly about continuous data.

PSL is part of a broad family of probabilistic programming languages (Gordon et al.,
2014). The goals of probabilistic programming and SRL often overlap. Probabilistic pro-
gramming seeks to make constructing probabilistic models easy for the end user, and sep-
arate model speciﬁcation from the development of inference and learning algorithms.
If
algorithms can be developed for the entire space of models covered by a language, then it
is easy for users to experiment with including and excluding diﬀerent model components.
It also makes it easy for existing models to beneﬁt from improved algorithms. Separation
of model speciﬁcation and algorithms is also useful in SRL for the same reasons. In this
paper we emphasize designing algorithms that are ﬂexible enough to support the full class of
HL-MRFs. Examples of probabilistic programming languages include IBAL (Pfeﬀer, 2001),
BLOG (Milch et al., 2005), Markov logic (Richardson and Domingos, 2006), ProbLog (De
Raedt et al., 2007), Church (Goodman et al., 2008), Figaro (Pfeﬀer, 2009), FACTORIE
(McCallum et al., 2009), Anglican (Wood et al., 2014), and Edward (Tran et al., 2016).

Other formalisms have also been proposed for probabilistic reasoning over continuous
domains and other domains equipped with semirings. Hybrid Markov logic networks (Wang
and Domingos, 2008) mix discrete and continuous variables. In addition to the dependencies
over discrete variables supported by MLNs, they support soft equality constraints between
two variables of the same form as those deﬁned by squared arithmetic rules in PSL, as well
as linear potentials of the form y1 − y2 for a soft inequality constraint y1 > y2. Inference in
hybrid MLNs is intractable. Wang and Domingos (2008) propose a random walk algorithm
for approximate MAP inference. Another related formalism is aProbLog (Kimmig et al.,
2011), which generalizes ProbLog to allow clauses to be annotated with elements from a

48

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

semiring, generalizing ProbLog’s support for clauses annotated with probabilities. Many
common inference tasks can be generalized from this perspective as algebraic model counting
(Kimmig et al., 2016). The PITA system (Riguzzi and Swift, 2011) for probabilistic logic
programming can also be viewe as implementing inference over various semirings.

7.2 Inference

Whether viewed as MAP inference for an MRF or SP without probabilistic semantics,
searching over a structured space to ﬁnd the optimal prediction is an important but diﬃcult
task. It is NP-hard in general (Shimony, 1994), so much work has focused on approximations
and identifying classes of problems for which it is tractable. A well-studied approximation
technique is local consistency relaxation (LCR) (Wainwright and Jordan, 2008). Inference
is ﬁrst viewed as an equivalent optimization over the realizable expected values of the
potentials, called the marginal polytope. When the variables are discrete and each potential
is an indicator that a subset of variables is in a certain state, this optimization becomes a
linear program. Each variable in the program is the marginal probability that a variable
is a particular state or the variables associated with a potential are in a particular joint
state. The marginal polytope is then the set of marginal probabilities that are globally
consistent. The number of linear constraints required to deﬁne the marginal polytope is
exponential in the size of the problem, however, so the linear program has to be relaxed in
order to be tractable. In a local consistency relaxation, the marginal polytope is relaxed
to the local polytope, in which the marginals over variables and potential states are only
locally consistent in the sense that each marginal over potential states sums to the marginal
distributions over the associated variables.

A large body of work has focused on solving the LCR objective quickly. Typically,
oﬀ-the-shelf convex optimization methods do not scale well for large graphical models and
structured predictors (Yanover et al., 2006), so a large branch of research has investigated
highly scalable message-passing algorithms. One approach is dual decomposition (DD)
(Sontag et al., 2011), which solves a problem dual to the LCR objective. Many DD algo-
rithms use coordinate descent, such as TRW-S (Kolmogorov, 2006), MSD (Werner, 2007),
MPLP (Globerson and Jaakkola, 2007), and ADLP (Meshi and Globerson, 2011). Other
DD algorithms use subgradient-based approaches (e.g., Jojic et al., 2010; Komodakis et al.,
2011; Schwing et al., 2012).

Another approach to solving the LCR objective uses message-passing algorithms to
solve the problem directly in its primal form. One well-known algorithm is that of Raviku-
mar et al. (2010a), which uses proximal optimization, a general approach that iteratively
improves the solution by searching for nearby improvements. The authors also provide
rounding guarantees for when the relaxed solution is integral, i.e., the relaxation is tight,
allowing the algorithm to converge faster. Another message-passing algorithm that solves
the primal objective is AD3 (Martins et al., 2015), which uses the alternating direction
method of multipliers (ADMM). AD3 optimizes objective (10) for binary, pairwise MRFs
and supports the addition of certain deterministic constraints on the variables. A third ex-
ample of a primal message-passing algorithm is APLP (Meshi and Globerson, 2011), which
is the primal analog of ADLP. Like AD3, it uses ADMM to optimize the objective.

49

Bach, Broecheler, Huang, and Getoor

Other approaches to approximate inference include tighter linear programming relax-
ations (Sontag et al., 2008, 2012). These tighter relaxations enforce local consistency on
variable subsets that are larger than individual variables, which makes them higher-order
local consistency relaxations. Mezuman et al. (2013) developed techniques for special cases
of higher-order relaxations, such as when the MRF contains cardinality potentials, in which
the probability of a conﬁguration depends on the number of variables in a particular state.
Researchers have also explored nonlinear convex programming relaxations, e.g., Ravikumar
and Laﬀerty (2006) and Kumar et al. (2006).

Previous analyses have identiﬁed particular subclasses whose local consistency relax-
ations are tight, i.e., the maximum of the relaxed program is exactly the maximum of the
original problem. These special classes include graphical models with tree-structured depen-
dencies, models with submodular potential functions, models encoding bipartite matching
problems, and those with nand potentials and perfect graph structures (Wainwright and
Jordan, 2008; Schrijver, 2003; Jebara, 2009; Foulds et al., 2011). Researchers have also
studied performance guarantees of other subclasses of the ﬁrst-order local consistency re-
laxation. Kleinberg and Tardos (2002) and Chekuri et al. (2005) considered the metric
labeling problem. Feldman et al. (2005) used the local consistency relaxation to decode
binary linear codes.

In this paper we examine the classic problem of MAX SAT—ﬁnding a joint Boolean
assignment to a set of propositions that maximizes the sum of a set of weighted clauses
that are satisﬁed—as an instance of SP. Researchers have also considered approaches to
solving MAX SAT other than the one one we study, the randomized algorithm of Goemans
and Williamson (1994). One line of work focusing on convex programming relaxations has
obtained stronger rounding guarantees than Goemans and Williamson (1994) by using non-
linear programming, e.g., Asano and Williamson (2002) and references therein. Other work
does not use the probabilistic method but instead searches for discrete solutions directly,
e.g., Mills and Tsang (2000), Larrosa et al. (2008), and Choi et al. (2009). We note that
one such approach, that of Wah and Shang (1997), is essentially a type of DD formulated
for MAX SAT. A more recent approach blends convex programming and discrete search via
mixed integer programming (Davies and Bacchus, 2013). Additionally, Huynh and Mooney
(2009) introduced a linear programming relaxation for MLNs inspired by MAX SAT re-
laxations, but the relaxation of general Markov logic provides no known guarantees on the
quality of solutions.

Finally, lifted inference takes advantage of symmetries in probability distributions to re-
duce the amount of work required for inference. Some of the earliest approaches identiﬁed
repeated dependency structures in PGMs to avoid repeated computations (Koller and Pfef-
fer, 1997; Pfeﬀer et al., 1999). Lifted inference has been widely applied in SRL because the
templates that are commonly used to deﬁne PGMs often induce symmetries. Various infer-
ence techniques for discrete MRFs have been extended to a lifted approach, including belief
propagation (Jaimovich et al., 2007; Singla and Domingos, 2008; Kersting et al., 2009) and
Gibbs sampling (Venugopal and Gogate, 2012). Approaches to lifted convex optimization
(Mladenov et al., 2012) might be extended to HL-MRFs. See de Salvo Braz et al. (2007),
Kersting (2012), and Kimmig et al. (2015) for more information on lifted inference.

50

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

7.3 Learning

Taskar et al. (2004) connected SP and PGMs by showing how to train MRFs with large-
margin estimation, a generalization of the large-margin objective for binary classiﬁcation
used to train support vector machines (Vapnik, 2000). Large-margin learning is a well-
studied approach to train structured predictors because it directly incorporates the struc-
tured loss function into a convex upper bound on the true objective: the regularized ex-
pected risk. The learning objective is to ﬁnd the parameters with smallest norm such that
a linear combination of feature functions assign a better score to the training data than
all other possible predictions. The amount by which the score of the correct prediction
must exceed the score of other predictions is scaled using the structured loss function.
The objective is therefore encoded as a norm minimization problem subject to many linear
constraints, one for each possible prediction in the structured space.

Structured SVMs (Tsochantaridis et al., 2005) extend large-margin estimation to a broad
class of structured predictors and admit a tractable cutting-plane learning algorithm. This
algorithm will terminate in a number of iterations linear in the size of the problem, and so
the computational challenge of large-margin learning for structured prediction comes down
to the task of ﬁnding the most violated constraint in the learning objective. This can be
accomplished by optimizing the energy function plus the loss function. In other words, the
task is to ﬁnd the structure that is the best combination of being favored by the energy
function but unfavored by the loss function. Often, the loss function decomposes over the
components of the prediction space, so the combined energy function and loss function can
often be viewed as simply the energy function of another structured predictor that is equally
challenging or easy to optimize, such as when the space of structures is a set of discrete
vectors and the loss function is the Hamming distance.

It is common during large-margin estimation that no setting of the parameters can
predict all the training data without error. In this case, the training data is said to not
be separable, again generalizing the notion of linear separability in the feature space from
binary classiﬁcation. The solution to this problem is to add slack variables to the constraints
that require the training data to be assigned the best score. The magnitude of the slack
variables are penalized in the learning objective, so estimation must trade oﬀ between the
norm of the parameters and violating the constraints. Joachims et al. (2009) extend this
formulation to a “one slack” formulation, in which a single slack variable is used for all the
constraints across all training examples, which is more eﬃcient. We use this framework for
large-margin estimation for HL-MRFs in Section 6.3.

The repeated inferences required for large-margin learning, one to ﬁnd the most-violated
constraint at each iteration, can become computationally expensive. Therefore researchers
have explored speeding up learning by interleaving the inference problem with the learning
problem. In the cutting-plane formulation discussed above, the objective is equivalently a
saddle-point problem, with the solution at the minimum with respect to the parameters and
the maximum with respect to the inference variables. Taskar et al. (2005) proposed dualizing
the inner inference problem to form a joint minimization. For SP problems with a tight
duality gap, i.e., the dual problem has the same optimal value as the primal problem, this
approach leads to an equivalent, convex optimization that can be solved for all variables
In other words, the learning and most-violated constraint problems are
simultaneously.

51

Bach, Broecheler, Huang, and Getoor

solved simultaneously, greatly reducing training time. For problems with non-tight duality
gaps, e.g., MAP inference in general, discrete MRFs, Meshi et al. (2010) showed that
the same principle can be applied by using approximate inference algorithms like dual
decomposition to bound the primal objective.

A related problem to parameter learning is structure learning, i.e., identifying an ac-
curate dependency structure for a model. A common SRL approach is searching over the
space of templates for PGMs. For probabilistic relational models, Friedman et al. (1999)
learned structures described in the vocabulary of relational schemas. For models that are
templated with ﬁrst-order-logic-like languages, such as PSL and MLNs, these approaches
take the form of rule learning. Based on rule-learning techniques from inductive logic pro-
gramming (e.g., Richards and Mooney, 1992; De Raedt and Dehaspe, 1996) a series of
approaches have sought to learn MLN rules from relational data. Initially, Kok and Domin-
gos (2005) learned rules by generating candidates and performing a beam search to identify
rules that improved a weighted pseudolikelihood objective. Then, Mihalkova and Mooney
(2007) observed that the previous approach generated candidate rules without regard to
the data, so they introduced an approach that used the data to guide the proposal of rules
via relational pathﬁnding. Kok and Domingos (2010) improved on that by ﬁrst perform-
ing graph clustering to ﬁnd common motifs, which are common subgraphs, to guide rule
proposal. They observed that modifying a rule set one clause at a time often got stuck
in poor local optima, and by using the motifs as reﬁnement operators instead, they were
able to converge to better optima. Other approaches to structure learning search directly
over grounded PGMs, including (cid:96)1-regularized pseudolikelihood maximization (Ravikumar
et al., 2010b) and grafting (Perkins et al., 2003; Zhu et al., 2010). These methods can all
be extended to HL-MRFs and PSL.

8. Conclusion

In this paper we introduced HL-MRFs, a new class of probabilistic graphical models that
unite and generalize several approaches to modeling relational and structured data: Boolean
logic, probabilistic graphical models, and fuzzy logic. HL-MRFs can capture relaxed, prob-
abilistic inference with Boolean logic and exact, probabilistic inference with fuzzy logic,
making them useful models for both discrete and continuous data. HL-MRFs also general-
ize these inference techniques with additional expressivity, allowing for even more ﬂexibility.
HL-MRFs are a signiﬁcant addition to the the library of machine learning tools because
they embody a useful point in the spectrum of models that trade oﬀ between scalability
and expressivity. As we showed, they can be easily applied to a wide range of structured
problems in machine learning and achieve high-quality predictive performance, competitive
with or surpassing the performance of canonical approaches. However, these other models
either do not scale as well, like discrete MRFs, or are not as versatile in their ability to
capture a wide range of problems, like Bayesian probabilistic matrix factorization.

We also introduced PSL, a probabilistic programming language for HL-MRFs. PSL
makes HL-MRFs easy to design, allowing users to encode their ideas for structural depen-
dencies using an intuitive syntax based on ﬁrst-order logic. PSL also helps accelerate a
time-consuming aspect of the modeling process: reﬁning a model. In contrast with other
types of models that require specialized inference and learning algorithms depending on

52

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

which structural dependencies are included, HL-MRFs can encode many types of depen-
dencies and scale well with the same inference and learning algorithms. PSL makes it easy
to quickly add, remove, and modify dependencies in the model and rerun inference and
learning, allowing users to quickly improve the quality of their models. Finally, because
PSL uses a ﬁrst-order syntax, each PSL program actually speciﬁes an entire class of HL-
MRFs, parameterized by the particular data set over which it is grounded. Therefore, a
model or components of a model reﬁned for one data set can easily be applied to others.

Next, we introduced inference and learning algorithms that scale to large problems. The
MAP inference algorithm is far more scalable than standard tools for convex optimization
because it leverages the sparsity that is so common to the dependencies in structured
prediction. The supervised learning algorithms extend standard learning objectives to HL-
MRFs. Together, this combination of an expressive formalism, a user-friendly probabilistic
programming language, and highly scalable algorithms enables researchers and practitioners
to easily build large-scale, accurate models of relational and structured data.7

This paper also lays the foundation for many lines of future work. Our analysis of local
consistency relaxation (LCR) as a hierarchical optimization is a general proof technique,
and it could be used to derive compact forms for other LCR objectives. As in the case of
MRFs deﬁned using logical clauses, such compact forms can simplify analysis and could
lead to a greater understanding of LCR for other classes of MRFs. Another important line
of work is understanding what guarantees apply to the MAP states of HL-MRFs. Can
anything be said about their ability to approximate MAP inference in discrete models that
go beyond the models already covered by the known rounding guarantees? Future directions
also include developing new algorithms for HL-MRFs. One important direction is marginal
inference for HL-MRFs and algorithms for sampling from them. Unlike marginal inference
for discrete distributions, which computes the marginal probability that a variable is in a
particular state, marginal inference for HL-MRFs requires ﬁnding the marginal probability
that a variable is in a particular range. One option for doing so, as well as generating samples
from HL-MRFs, is to extend the hit-and-run sampling scheme of Broecheler and Getoor
(2010). This method was developed for continuous constrained MRFs with piecewise-linear
potentials. There are also many new domains to which HL-MRFs and PSL can be applied.
With these modeling tools, researchers can design and apply new solutions to structured
prediction problems.

Acknowledgments

We acknowledge the many people who have contributed to the development of HL-MRFs
and PSL. Contributors include Eriq Augustine, Shobeir Fakhraei, James Foulds, Angelika
Kimmig, Stanley Kok, Ben London, Hui Miao, Lilyana Mihalkova, Dianne P. O’Leary, Jay
Pujara, Arti Ramesh, Theodoros Rekatsinas, and V.S. Subrahmanian. This work was sup-
ported by NSF grants CCF0937094 and IIS1218488, and IARPA via DoI/NBC contract
number D12PC00337. The U.S. Government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those of the authors and should

7. An open source implementation, tutorials, and data sets are available at http://psl.linqs.org.

53

Bach, Broecheler, Huang, and Getoor

not be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.

Appendix A. Proof of Theorem 2

In this appendix, we prove the equivalence of objectives (7) and (10). Our proof analyzes
the local consistency relaxation to derive an equivalent, more compact optimization over
only the variable pseudomarginals µ that is identical to the MAX SAT relaxation. Since the
variables are Boolean, we refer to each pseudomarginal µi(1) as simply µi. Let xF
j denote
the unique setting such that φj(xF
j ) = 0. (I.e., xF
is the setting in which each literal in the
j
clause Cj is false.)

We begin by reformulating the local consistency relaxation as a hierarchical optimiza-
tion, ﬁrst over the variable pseudomarginals µ and then over the factor pseudomarginals θ.
Due to the structure of local polytope L, the pseudomarginals µ parameterize inner linear
programs that decompose over the structure of the MRF, such that—given ﬁxed µ—there is
an independent linear program ˆφj(µ) over θj for each clause Cj. We rewrite objective (10)
as

arg max
µ∈[0,1]n

(cid:88)

ˆφj(µ),

Cj ∈C

where

ˆφj(µ) = max
θj

wj

(cid:88)

θj(xj)

xj |xj (cid:54)=xF
j

(cid:88)

such that

θj(xj) = µi

θj(xj) = 1 − µi

xj |xj (i)=1
(cid:88)

xj |xj (i)=0
(cid:88)

θj(xj) = 1

xj
θj(xj) ≥ 0

∀i ∈ I +
j

∀i ∈ I −
j

∀xj .

(71)

(72)

(73)

(74)

(75)

(76)

It is straightforward to verify that objectives (10) and (71) are equivalent for MRFs with
disjunctive clauses for potentials. All constraints deﬁning L can be derived from the con-
straint µ ∈ [0, 1]n and the constraints in the deﬁnition of ˆφj(µ). We have omitted redundant
constraints to simplify analysis.

To make this optimization more compact, we replace each inner linear program ˆφj(µ)
with an expression that gives its optimal value for any setting of µ. Deriving this expression
j of ˆφj(µ), which is guaranteed to exist because
requires reasoning about any maximizer θ(cid:63)
problem (72) is bounded and feasible8 for any parameters µ ∈ [0, 1]n and wj.

We ﬁrst derive a suﬃcient condition for the linear program to not be fully satisﬁable, in
the sense that it cannot achieve a value of wj, the maximum value of the weighted potential

8. Setting θj(xj) to the probability deﬁned by µ under the assumption that the elements of xj are inde-

pendent, i.e., the product of the pseudomarginals, is always feasible.

54

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

wjφj(x). Observe that, by the objective (72) and the simplex constraint (75), showing that
ˆφj(µ) is not fully satisﬁable is equivalent to showing that θ(cid:63)

j (xF

j ) > 0.

Lemma 16 If

µi +

(1 − µi) < 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

then θ(cid:63)

j (xF

j ) > 0.

Proof By the simplex constraint (75),

Also, by summing all the constraints (73) and (74),

µi +

(1 − µi) <

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) .

(cid:88)

xj

(cid:88)

θ(cid:63)
j (xj) ≤

xj |xj (cid:54)=xF
j

µi +

(1 − µi) ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

because all the components of θ(cid:63) are nonnegative, and—except for θ(cid:63)
at least once in constraints (73) and (74). These bounds imply

j (xF

j )—they all appear

(cid:88)

xj |xj (cid:54)=xF
j

θ(cid:63)
j (xj) <

θ(cid:63)
j (xj) ,

(cid:88)

xj

which means θ(cid:63)

j (xF

j ) > 0, completing the proof.

We next show that if ˆφj(µ) is parameterized such that it is not fully satisﬁable, as in

Lemma 16, then its optimum always takes a particular value deﬁned by µ.

Lemma 17 If wj > 0 and θ(cid:63)

j (xF

j ) > 0, then

(cid:88)

θ(cid:63)
j (xj) =

xj |xj (cid:54)=xF
j

µi +

(1 − µi) .

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

Proof We prove the lemma via the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939;
Kuhn and Tucker, 1951). Since problem (72) is a maximization of a linear function subject
to linear constraints, the KKT conditions are necessary and suﬃcient for any optimum θ(cid:63)
j .
Before writing the relevant KKT conditions, we introduce some necessary notation. For
a state xj, we need to reason about the variables that disagree with the unsatisﬁed state
xF

j . Let

(cid:110)

d(xj) (cid:44)

i ∈ I +

j ∪ I −

j |xj(i) (cid:54)= xF

j (i)

(cid:111)

be the set of indices for the variables that do not have the same value in the two states xj
and xF
j .

We now write the relevant KKT conditions for θ(cid:63)

j . Let λ, α be real-valued vectors where
j | + 1 and |α| = |θj|. Let each λi correspond to a constraint (73) or (74)

j | + |I −

|λ| = |I +

55

Bach, Broecheler, Huang, and Getoor

for i ∈ I +
correspond to a constraint (76) for each xj. Then, the following KKT conditions hold:

j , and let λ∆ correspond to the simplex constraint (75). Also, let each αxj

j ∪ I −

αxj ≥ 0
αxj θ(cid:63)
λ∆ + αxF
j
(cid:88)

j (xj) = 0
= 0

wj +

i∈d(xj )

λi + λ∆ + αxj = 0

∀xj (cid:54)= xF
j

.

∀xj
∀xj

(77)

(78)

(79)

(80)

Since θ(cid:63)

j (xF

j ) > 0, by condition (78), αxF

= 0. By condition (79), then λ∆ = 0. From
here we can bound the other elements of λ. Observe that for every i ∈ I +
j , there
exists a state xj such that d(xj) = {i}. Then, it follows from condition (80) that there
exists xj such that, for every i ∈ I +

j ∪ I −

j

j ∪ I −
j ,

wj + λi + λ∆ + αxj = 0 .

Since αxj ≥ 0 by condition (77) and λ∆ = 0, it follows that λi ≤ −wj. With these bounds,
we show that, for any state xj, if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0. Assume that for some state
xj, |d(xj)| ≥ 2. By condition (80) and the derived constraints on λ,

αxj ≥ (|d(xj)| − 1)wj > 0 .
j (xj) = 0. Next, observe that for all i ∈ I +
With condition (78), θ(cid:63)
j ) and for
j
any state xj, if d(xj) = {i}, then xj(i) = 1 (resp. xj(i) = 0), and for any other state x(cid:48)
j
such that x(cid:48)
j) ≥ 2. By constraint (73) (resp. constraint (74)),
θ(cid:63)(xj) = µi (resp. θ(cid:63)(xj) = 1 − µi).
We have shown that if θ(cid:63)
j ), then θ(cid:63)

j ) > 0, then for all states xj, if d(xj) = {i} and i ∈ I +
j
j (xj) = 1 − µi), and if |d(xj)| ≥ 2, then θ(cid:63)
j (xj) = 0.

j (xF
j (xj) = µi (resp. θ(cid:63)

j(i) = 1 (resp. x(cid:48)

j(i) = 0), d(x(cid:48)

(resp. i ∈ I −

(resp. i ∈ I −
This completes the proof.

Lemma 16 says if (cid:80)

(1 − µi) < 1, then ˆφj(µ) is not fully satisﬁable,
and Lemma 17 provides its optimal value. We now reason about the other case, when
(cid:80)
(1 − µi) ≥ 1, and we show that this condition is suﬃcient to ensure that

µi + (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

i∈I +
j

i∈I −
j

ˆφj(µ) is fully satisﬁable.

Lemma 18 If wj > 0 and

j (xF

j ) = 0.

then θ(cid:63)
Proof We prove the lemma by contradiction. Assume that wj > 0, (cid:80)
µi) ≥ 1, and that the lemma is false, θ(cid:63)
j ) > 0. Then, by Lemma 17,

µi + (cid:80)

(1 −

i∈I −
j

i∈I +
j

µi +

(1 − µi) ≥ 1 ,

(cid:88)

i∈I +
j

(cid:88)

i∈I −
j

θ(cid:63)
j (xj) ≥ 1 .

j (xF
(cid:88)

xj |xj (cid:54)=xF
j

56

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

The assumption that θ(cid:63)

j (xF

j ) > 0 implies

θ(cid:63)
j (xj) > 1,

(cid:88)

xj

j ) < 0 is excluded by the nonnegativity constraints (76).

which is a contradiction, since it violates the simplex constraint (75). The possibility that
θ(cid:63)
j (xF
For completeness and later convenience, we also state the value of ˆφj(µ) when it is fully
satisﬁable.

Lemma 19 If θ(cid:63)

j (xF

j ) = 0, then

(cid:88)

θ(cid:63)
j (xj) = 1 .

xj |xj (cid:54)=xF
j

Proof The lemma follows from the simplex constraint (75).

We can now combine the previous lemmas into a single expression for the value of ˆφj(µ).

Lemma 20 For any feasible setting of µ,

ˆφj(µ) = wj min

µi +

(1 − µi), 1

.






(cid:88)

i∈I +
j

(cid:88)

i∈I −
j






Proof The lemma is trivially true if wj = 0 since any assignment will yield zero value. If
wj > 0, then we consider two cases. In the ﬁrst case, if (cid:80)
(1 − µi) < 1,
then, by Lemmas 16 and 17,

µi + (cid:80)

i∈I −
j

i∈I +
j

In the second case, if (cid:80)

µi + (cid:80)

i∈I −
j

i∈I +
j

(1 − µi) ≥ 1, then, by Lemmas 18 and 19,

ˆφj(µ) = wj






(cid:88)

i∈I +
j



µi +

(1 − µi)


 .

(cid:88)

i∈I −
j

ˆφj(µ) = wj .

By factoring out wj, we can rewrite this piecewise deﬁnition of ˆφj(µ) as wj multiplied by
the minimum of (cid:80)

(1 − µi) and 1, completing the proof.

µi + (cid:80)

i∈I +
j

i∈I −
j

This leads to our ﬁnal equivalence result.

Theorem 2 For an MRF with potentials corresponding to disjunctive logical clauses and
associated nonnegative weights, the ﬁrst-order local consistency relaxation of MAP inference
is equivalent to the MAX SAT relaxation of Goemans and Williamson (1994). Speciﬁcally,
any partial optimum µ(cid:63) of objective (10) is an optimum ˆy(cid:63) of objective (7), and vice versa.

57

Bach, Broecheler, Huang, and Getoor

Proof Substituting the solution of the inner optimization from Lemma 20 into the local
consistency relaxation objective (71) gives a projected optimization over only µ which is
identical to the MAX SAT relaxation objective (7).

References

2008.

A. Abdelbar and S. Hedetniemi. Approximating MAPs for belief networks is NP-hard and

other theorems. Artiﬁcial Intelligence, 102(1):21–38, 1998.

N. Alon and J. H. Spencer. The Probabilistic Method. Wiley-Interscience, third edition,

D. Alshukaili, A. A. A. Fernandes, and N. W. Paton. Structuring linked data search results
using probabilistic soft logic. In International Semantic Web Conference (ISWC), 2016.

L. An and P. Tao. The DC (diﬀerence of convex functions) programming and DCA revisited
with DC models of real world nonconvex optimization problems. Annals of Operations
Research, 133:23–46, 2005.

T. Asano and D. P. Williamson. Improved approximation algorithms for MAX SAT. J.

Algorithms, 42(1):173–202, 2002.

S. H. Bach, M. Broecheler, L. Getoor, and D. P. O’Leary. Scaling MPE inference for con-
strained continuous Markov random ﬁelds. In Advances in Neural Information Processing
Systems (NIPS), 2012.

S. H. Bach, B. Huang, B. London, and L. Getoor. Hinge-loss Markov random ﬁelds: Convex
inference for structured prediction. In Uncertainty in Artiﬁcial Intelligence (UAI), 2013.

S. H. Bach, B. Huang, J. Boyd-Graber, and L. Getoor. Paired-dual learning for fast training
of latent variable hinge-loss MRFs. In International Conference on Machine Learning
(ICML), 2015a.

S. H. Bach, B. Huang, and L. Getoor. Unifying local consistency and MAX SAT relaxations
for scalable inference with rounding guarantees. In Artiﬁcial Intelligence and Statistics
(AISTATS), 2015b.

G. Bakir, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan,

editors. Predicting Structured Data. MIT Press, 2007.

I. Beltagy, K. Erk, and R. J. Mooney. Probabilistic soft logic for semantic textual similarity.

In Annual Meeting of the Association for Computational Linguistics (ACL), 2014.

J. Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society,

24(3):179–195, 1975.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and
Statistical Learning Via the Alternating Direction Method of Multipliers. Now Publishers,
2011.

58

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

M. Broecheler and L. Getoor. Computing marginal distributions over continuous Markov
networks for statistical relational learning. In Advances in Neural Information Processing
Systems (NIPS), 2010.

M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2010a.

M. Broecheler, P. Shakarian, and V. S. Subrahmanian. A scalable framework for modeling

competitive diﬀusion in social networks. In Social Computing (SocialCom), 2010b.

C. Chekuri, S. Khanna, J. Naor, and L. Zosin. A linear programming formulation and
approximation algorithms for the metric labeling problem. SIAM J. Discrete Math., 18
(3):608–625, 2005.

P. Chen, F. Chen, and Z. Qian. Road traﬃc congestion monitoring in social media with
In IEEE International Conference on Data Mining

hinge-loss Markov random ﬁelds.
(ICDM), 2014.

A. Choi, T. Standley, and A. Darwiche. Approximating weighted Max-SAT problems by
compensating for relaxations. In International Conference on Principles and Practice of
Constraint Programming, 2009.

M. Collins. Discriminative training methods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Empirical Methods in Natural Language Processing
(EMNLP), 2002.

M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2004.

H. Daum´e III, J. Langford, and D. Marcu. Search-based structured prediction. Machine

Learning, 75(3):297–325, 2009.

J. Davies and F. Bacchus. Exploiting the power of MIP solvers in MAXSAT. In M. J¨arvisalo
and A. Van Gelder, editors, Theory and Applications of Satisﬁability Testing – SAT 2013,
Lecture Notes in Computer Science, pages 166–181. Springer Berlin Heidelberg, 2013.

L. De Raedt and L. Dehaspe. Clausal discovery. Machine Learning, 26:1058–1063, 1996.

L. De Raedt, A. Kimmig, and H. Toivonen. ProbLog: A probabilistic Prolog and its
application in link discovery. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

R. de Salvo Braz, E. Amir, and D. Roth. Lifted ﬁrst-order probabilistic inference.

In
L. Getoor and B. Taskar, editors, Introduction to statistical relational learning, pages
433–451. MIT Press, 2007.

L. Deng and J. Wiebe. Joint prediction for entity/event-level sentiment analysis using
probabilistic soft logic models. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2015.

59

Bach, Broecheler, Huang, and Getoor

J. Ebrahimi, D. Dou, and D. Lowd. Weakly supervised tweet stance classiﬁcation by rela-
tional bootstrapping. In Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), 2016.

S. Fakhraei, B. Huang, L. Raschid, and L. Getoor. Network-based drug-target interac-
tion prediction with probabilistic soft logic. IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 2014.

J. Feldman, M. J. Wainwright, and D. R. Karger. Using linear programming to decode

binary linear codes. Information Theory, IEEE Trans. on, 51(3):954–972, 2005.

J. Foulds, N. Navaroli, P. Smyth, and A. Ihler. Revisiting MAP estimation, message passing

and perfect graphs. In AI & Statistics, 2011.

J. Foulds, S. Kumar, and L. Getoor. Latent topic networks: A versatile probabilistic pro-
gramming framework for topic models. In International Conference on Machine Learning
(ICML), 2015.

N. Friedman, L. Getoor, D. Koller, and A. Pfeﬀer. Learning probabilistic relational models.

In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1999.

D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems
via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1):17–
40, 1976.

M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simpliﬁed NP-complete graph

problems. Theoretical Computer Science, 1(3):237–267, 1976.

L. Getoor and B. Taskar, editors. Introduction to statistical relational learning. MIT press,

2007.

L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning probabilistic models of link

structure. Journal of Machine Learning Research (JMLR), 3:679–707, 2002.

A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms
for MAP LP-relaxations. In Advances in Neural Information Processing Systems (NIPS),
2007.

R. Glowinski and A. Marrocco. Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et la
r´esolution, par p´enalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires.
Revue fran¸caise d’automatique, informatique, recherche op´erationnelle, 9(2):41–76, 1975.

M. X. Goemans and D. P. Williamson. New 3/4-approximation algorithms for the maximum

satisﬁability problem. SIAM J. Discrete Math., 7(4):656–666, 1994.

J. Golbeck. Computing and Applying Trust in Web-based Social Networks. PhD thesis,

University of Maryland, 2005.

K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collabo-

rative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.

60

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church:
A language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.

A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. Probabilistic programming.

In International Conference on Software Engineering (ICSE, FOSE track), 2014.

G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science, 313(5786):504–507, 2006.

B. Huang, A. Kimmig, L. Getoor, and J. Golbeck. A ﬂexible framework for probabilistic
models of social trust. In Conference on Social Computing, Behavioral-Cultural Modeling,
& Prediction (SBP), 2013.

T. Huynh and R. Mooney. Max-margin weight learning for Markov logic networks.

In

European Conference on Machine Learning (ECML), 2009.

A. Jaimovich, O. Meshi, and N. Friedman. Template based inference in symmetric relational

Markov random ﬁelds. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.

T. Jebara. MAP estimation, message passing, and perfect graphs. In Uncertainty in Arti-

ﬁcial Intelligence (UAI), 2009.

Learning, 77(1):27–59, 2009.

T. Joachims, T. Finley, and C. Yu. Cutting-plane training of structural SVMs. Machine

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In

International Conference on Machine Learning (ICML), 2010.

S. Kamvar, M. Schlosser, and H. Garcia-Molina. The eigentrust algorithm for reputation
In International Conference on the World Wide Web

management in P2P networks.
(WWW), 2003.

W. Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints.

Master’s thesis, University of Chicago, 1939.

K. Kersting. Lifted probabilistic inference. In European Conference on Artiﬁcial Intelligence

(ECAI), 2012.

K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Uncertainty in

Artiﬁcial Intelligence (UAI), 2009.

A. Kimmig, G. Van den Broeck, and L. De Raedt. An algebraic Prolog for reasoning about

possible worlds. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2011.

A. Kimmig, L. Mihalkova, and L. Getoor. Lifted graphical models: A survey. Machine

Learning, 99:1–45, 2015.

Applied Logic, 2016.

A. Kimmig, G. Van den Broeck, and L. De Raedt. Algebraic model counting. Journal of

61

Bach, Broecheler, Huang, and Getoor

J. Kleinberg and ´E. Tardos. Approximation algorithms for classiﬁcation problems with
pairwise relationships: Metric labeling and Markov random ﬁelds. J. ACM, 49(5):616–
639, 2002.

G. J. Klir and B. Yuan. Fuzzy Sets and Fuzzy Logic: Theory and Applications. Prentice

Hall, 1995.

S. Kok and P. Domingos. Learning the structure of Markov logic networks. In International

Conference on Machine Learning (ICML), 2005.

S. Kok and P. Domingos. Learning Markov logic networks using structural motifs.

In

International Conference on Machine Learning (ICML), 2010.

D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.

MIT Press, 2009.

Intelligence (UAI), 1997.

D. Koller and A. Pfeﬀer. Object-oriented Bayesian networks. In Uncertainty in Artiﬁcial

V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. Pat-

tern Analysis and Machine Intelligence, IEEE Trans. on, 28(10):1568–1583, 2006.

N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via
dual decomposition. Pattern Analysis and Machine Intelligence, IEEE Trans. on, 33(3):
531–552, 2011.

P. Kouki, S. Fakhraei, J. Foulds, M. Eirinaki, and L. Getoor. HyPER: A ﬂexible and
extensible probabilistic framework for hybrid recommender systems. In ACM Conference
on Recommender Systems (RecSys), 2015.

H. W. Kuhn and A. W. Tucker. Nonlinear programming.

In Berkeley Symp. on Math.

Statist. and Prob., 1951.

M. P. Kumar, P. H. S. Torr, and A. Zisserman. Solving Markov random ﬁelds using sec-
ond order cone programming relaxations. In Computer Vision and Pattern Recognition
(CVPR), 2006.

N. Landwehr, A. Passerini, L. De Raedt, and P. Frasconi. Fast learning of relational kernels.

Machine Learning, 78(3):305–342, 2010.

J. Larrosa, F. Heras, and S. de Givry. A logical approach to eﬃcient Max-SAT solving.

Artiﬁcial Intelligence, 172(2-3):204–233, 2008.

J. Li, A. Ritter, and D. Jurafsky. Inferring user preferences by probabilistic logical reasoning

over social networks. arXiv preprint arXiv:1411.2679, 2014.

S. Liu, K. Liu, S. He, and J. Zhao. A probabilistic soft logic based approach to exploiting
latent and global information in event classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2016.

62

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

B. London, S. Khamis, S. H. Bach, B. Huang, L. Getoor, and L. Davis. Collective activity
In CVPR Workshop on Structured

detection using hinge-loss Markov random ﬁelds.
Prediction: Tractability, Learning and Inference, 2013.

B. London, B. Huang, and L. Getoor. Stability and generalization in structured prediction.

Journal of Machine Learning Research (JMLR), 17(222):1–52, 2016.

D. Lowd and P. Domingos. Eﬃcient weight learning for Markov logic networks. In Principles

and Practice of Knowledge Discovery in Databases (PKDD), 2007.

S. Magliacane, P. Stutz, P. Groth, and A. Bernstein. FoxPSL: An extended and scalable
In AAAI Spring Symposium on Knowledge Representation and

PSL implementation.
Reasoning: Integrating Symbolic and Neural Approaches, 2015.

A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing.
AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Mod-
els. Journal of Machine Learning Research (JMLR), 16(Mar):495–545, 2015.

A. McCallum, K. Nigam, and L. H. Ungar. Eﬃcient clustering of high-dimensional data
sets with application to reference matching. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2000.

A. McCallum, K. Schultz, and S. Singh. FACTORIE: Probabilistic programming via im-
peratively deﬁned factor graphs. In Advances in Neural Information Processing Systems
(NIPS), 2009.

O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation.

In European Conference on Machine learning (ECML), 2011.

O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning eﬃciently with approximate
In International Conference on Machine Learning (ICML),

inference via dual losses.
2010.

E. Mezuman, D. Tarlow, A. Globerson, and Y. Weiss. Tighter linear program relaxations
for high order graphical models. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2013.

H. Miao, X. Liu, B. Huang, and L. Getoor. A hypergraph-partitioned vertex programming
approach for large-scale consensus optimization. In IEEE International Conference on
Big Data, 2013.

L. Mihalkova and R. J. Mooney. Bottom-up learning of Markov logic network structure. In

International Conference on Machine Learning (ICML), 2007.

B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. BLOG: Probabilistic
models with unknown objects. In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2005.

P. Mills and E. Tsang. Guided local search for solving SAT and weighted MAX-SAT

problems. J. Automated Reasoning, 24(1-2):205–223, 2000.

63

Bach, Broecheler, Huang, and Getoor

M. Mladenov, B. Ahmadi, and K. Kersting. Lifted linear programming. In Artiﬁcial Intel-

ligence & Statistics (AISTATS), 2012.

S. Muggleton and L. De Raedt. Inductive logic programming: Theory and methods. The

Journal of Logic Programming, 19:629–679, 1994.

Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Program-

ming. Society for Industrial and Applied Mathematics, 1994.

J. Neville and D. Jensen. Relational dependency networks. Journal of Machine Learning

Research (JMLR), 8:653–692, 2007.

H. B. Newcombe and J. M. Kennedy. Record linkage: Making maximum use of the discrim-
inating power of identifying information. Communications of the ACM, 5(11):563–566,
1962.

S. Nowozin, P. V. Gehler, J. Jancsary, and C. H. Lampert, editors. Advanced Structured

Prediction. Neural Information Processing. MIT press, 2016.

J. D. Park. Using weighted MAX-SAT engines to solve MPE.

In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2002.

S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by
gradient descent in function space. Journal of Machine Learning Research (JMLR), 3:
1333–1356, 2003.

A. Pfeﬀer. IBAL: A probabilistic rational programming language. In International Joint

Conference on Artiﬁcial Intelligence (IJCAI), 2001.

A. Pfeﬀer. Figaro: An object-oriented probabilistic programming language. Technical

report, Charles River Analytics, 2009.

A. Pfeﬀer, D. Koller, B. Milch, and K. T. Takusagawa. SPOOK: A system for probabilistic
object-oriented knowledge representation. In Uncertainty in Artiﬁcial Intelligence (UAI),
1999.

H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Uncertainty

in Artiﬁcial Intelligence (UAI), 2011.

J. Pujara, H. Miao, L. Getoor, and W. Cohen. Knowledge graph identiﬁcation. In Inter-

national Semantic Web Conference (ISWC), 2013.

A. Ramesh, D. Goldwasser, B. Huang, H. Daum´e III, and L. Getoor. Learning latent
In AAAI Conference on Artiﬁcial

engagement patterns of students in online courses.
Intelligence (AAAI), 2014.

A. Ramesh, S. Kumar, J. Foulds, and L. Getoor. Weakly supervised models of aspect-
sentiment for online course discussion forums. In Annual Meeting of the Association for
Computational Linguistics (ACL), 2015.

64

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

P. Ravikumar and J. Laﬀerty. Quadratic programming relaxations for metric labeling and
Markov random ﬁeld MAP estimation. In International Conference on Machine Learning
(ICML), 2006.

P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured
linear programs: Proximal methods and rounding schemes. Journal of Machine Learning
Research (JMLR), 11:1043–1080, 2010a.

P. Ravikumar, M. J. Wainwright, and J. D. Laﬀerty. High-dimensional Ising model selection
using (cid:96)1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010b.

B. L. Richards and R. J. Mooney. Learning relations by pathﬁnding. In AAAI Conference

on Artiﬁcial Intelligence (AAAI), 1992.

M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2):107–

136, 2006.

M. Richardson, R. Agrawal, and P. Domingos. Trust management for the semantic web.
In D. Fensel, K. Sycara, and J. Mylopoulos, editors, The Semantic Web - ISWC 2003,
volume 2870 of Lecture Notes in Computer Science, pages 351–368. Springer Berlin /
Heidelberg, 2003.

F. Riguzzi and T. Swift. The PITA system: Tabling and answer subsumption for reasoning
under uncertainty. In International Conference on Logic Programming (ICLP), 2011.

S. Ross and J. A. Bagnell. Reinforcement and Imitation Learning via Interactive No-Regret

Learning, 2014.

S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Artiﬁcial Intelligence & Statistics (AISTATS),
2011.

R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Artiﬁcial Intelligence &

Statistics (AISTATS), 2009.

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In International Conference on Machine Learning (ICML), 2008.

M. Samadi, P. Talukdar, M. Veloso, and M. Blum. ClaimEval: Integrated and ﬂexible
In AAAI Conference on

framework for claim evaluation using credibility of sources.
Artiﬁcial Intelligence (AAAI), 2016.

A. Schrijver. Combinatorial Optimization: Polyhedra and Eﬃciency. Springer-Verlag, 2003.

A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally convergent dual MAP
LP relaxation solvers using Fenchel-Young margins. In Advances in Neural Information
Processing Systems (NIPS), 2012.

P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective

classiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.

65

Bach, Broecheler, Huang, and Getoor

S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artiﬁcial Intelligence, 68(2):

399–410, 1994.

P. Singla and P. Domingos. Discriminative training of Markov logic networks. In AAAI

Conference on Artiﬁcial Intelligence (AAAI), 2005.

P. Singla and P. Domingos. Lifted ﬁrst-order belief propagation. In AAAI Conference on

Artiﬁcial Intelligence (AAAI), 2008.

D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations
for MAP using message passing. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2008.

D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference.
In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning, pages
219–254. MIT Press, 2011.

D. Sontag, D. K. Choe, and Y. Li. Eﬃciently searching for frustrated cycles in MAP

inference. In Uncertainty in Aritiﬁcial Intelligence (UAI), 2012.

D. Sridhar, J. Foulds, M. Walker, B. Huang, and L. Getoor. Joint models of disagreement
and stance in online debate. In Annual Meeting of the Association for Computational
Linguistics (ACL), 2015.

D. Sridhar, S. Fakhraei, and L. Getoor. A probabilistic approach for collective similarity-

based drug-drug interaction prediction. Bioinformatics, 32(20):3175–3182, 2016.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Neural Information

Processing Systems (NIPS), 2004.

B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction
In International Conference on Machine Learning

models: A large margin approach.
(ICML), 2005.

D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Ed-
inference, and criticism. arXiv preprint

ward: A library for probabilistic modeling,
arXiv:1610.09787, 2016.

I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research
(JMLR), 6:1453–1484, 2005.

V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2000.

D. Venugopal and V. Gogate. On lifting the Gibbs sampling algorithm. In Neural Infor-

mation Processing Systems (NIPS), 2012.

B. W. Wah and Y. Shang. Discrete Lagrangian-based search for solving MAX-SAT prob-

lems. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1997.

M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Varia-

tional Inference. Now Publishers, 2008.

66

Hinge-Loss Markov Random Fields and Probabilistic Soft Logic

J. Wang and P. Domingos. Hybrid Markov logic networks. In AAAI Conference on Artiﬁcial

Intelligence (AAAI), 2008.

T. Werner. A linear programming approach to max-sum problem: A review. Pattern

Analysis and Machine Intelligence, IEEE Trans. on, 29(7):1165–1179, 2007.

R. West, H. S. Paskov, J. Leskovec, and C. Potts. Exploiting social network structure for
person-to-person sentiment analysis. Transactions of the Association for Computational
Linguistics (TACL), 2:297–310, 2014.

F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic

programming inference. In Artiﬁcial Intelligence & Statistics (AISTATS), 2014.

M. Wright. The interior-point revolution in optimization: History, recent developments,
and lasting consequences. Bulletin of the American Mathematical Society, 42(1):39–56,
2005.

L. Xiong, X. Chen, T. Huang, J. Schneider, and J. Carbonell. Temporal collaborative ﬁlter-
ing with Bayesian probabilistic tensor factorization. In SIAM International Conference
on Data Mining, 2010.

C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propaga-
tion – An empirical study. Journal of Machine Learning Research (JMLR), 7:1887–1907,
2006.

J. Zhu, N. Lao, and E. P. Xing. Grafting-Light: Fast, Incremental Feature Selection and
Structure Learning of Markov Random Fields. In International Conference on Knowledge
Discovery and Data Mining (KDD), 2010.

67

