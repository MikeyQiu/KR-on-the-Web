6
1
0
2
 
t
c
O
 
4
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
9
7
5
4
0
.
0
1
6
1
:
v
i
X
r
a

Automatically tracking neurons in a moving and

deforming brain

Jeﬀrey P. Nguyen1,2, Ashley N. Linder3, George S. Plummer2, #, Joshua

W. Shaevitz1,2 and Andrew M. Leifer1,3,*

1Department of Physics, Princeton University, Princeton, New Jersey, United

States of America

2Lewis-Sigler Institute for Integrative Genomics, Princeton University,

Princeton, New Jersey, United States of America

#Current Address: Tufts University School of Medicine, Boston,

Massachusetts, United States of America

3Princeton Neuroscience Institute, Princeton University, Princeton, New

Jersey, United States of America

*Corresponding author, E-mail: leifer@princeton.edu (AML)

October 17, 2016

Abstract

Advances in optical neuroimaging techniques now allow neural activity to be

recorded with cellular resolution in awake and behaving animals. Brain motion in

these recordings pose a unique challenge. The location of individual neurons must

be tracked in 3D over time to accurately extract single neuron activity traces.

Recordings from small invertebrates like C. elegans are especially challenging

because they undergo very large brain motion and deformation during animal

movement. Here we present an automated computer vision pipeline to reliably

track populations of neurons with single neuron resolution in the brain of a freely

moving C. elegans undergoing large motion and deformation. 3D volumetric

ﬂuorescent images of the animal’s brain are straightened, aligned and registered,

and the locations of neurons in the images are found via segmentation. Each

neuron is then assigned an identity using a new time-independent machine-learning

approach we call Neuron Registration Vector Encoding. In this approach, non-rigid

point-set registration is used to match each segmented neuron in each volume with

a set of reference volumes taken from throughout the recording. The way each

neuron matches with the references deﬁnes a feature vector which is clustered to

assign an identity to each neuron in each volume. Finally, thin-plate spline

interpolation is used to correct errors in segmentation and check consistency of

assigned identities. The Neuron Registration Vector Encoding approach proposed

here is uniquely well suited for tracking neurons in brains undergoing large

deformations. When applied to whole-brain calcium imaging recordings in freely

moving C. elegans, this analysis pipeline located 150 neurons for the duration of an

8 minute recording and consistently found more neurons more quickly than manual

or semi-automated approaches.

2

Author Summary

Computer algorithms for identifying and tracking neurons in images of a brain have

struggled to keep pace with rapid advances in neuroimaging. In small transparent

organism like the nematode C. elgeans, it is now possible to record neural activity

from all of the neurons in the animal’s head with single-cell resolution as it crawls.

A critical challenge is to identify and track each individual neuron as the brain

moves and bends. Previous methods required large amounts of manual human

annotation. In this work, we present a fully automated algorithm for neuron

segmentation and tracking in freely behaving C. elegans. Our approach uses

non-rigid point-set registration to construct feature vectors describing the location

of each neuron relative to other neurons and other volumes in the recording. Then

we cluster feature vectors in a time-independent fashion to track neurons through

time. This new approach works very well when compared to a human.

Introduction

Optical neural imaging has ushered in a new frontier in neuroscience that seeks to

understand how neural activity generates animal behavior by recording from large

populations of neurons at cellular resolution in awake and behaving animals.

Population recordings have now been used to elucidate mechanisms behind zebra

ﬁnch song production [1], spatial encoding in mice [2], and limb movement in

primates [3]. When applied to small transparent organisms, like Caenorhabditis

elegans [4], Drosophila [5], and zebraﬁsh [6], nearly every neuron in the brain can

be recorded, permitting the study of whole brain neural dynamics at cellular

resolution.

1

Methods for segmenting and tracking neurons have struggled to keep up as new

imaging technologies now record from more neurons over longer times in

environments with greater motion. Accounting for brain motion in particular has

become a major challenge, especially in recordings of unrestrained animals. Brains

in motion undergo translations and deformations in 3D that make robust tracking

of individual neurons very diﬃcult. The problem is compounded in invertebrates

like C. elegans where the head of the animal is ﬂexible and deforms greatly. If left

unaccounted for, brain motion not only prevents tracking of neurons, but it can

also introduce artifacts that mask the true neural signal. In this work we propose

an automated approach to segment and track neurons in the presence of dramatic

brain motion and deformation. Our approach is optimized for calcium imaging in

unrestrained C. elegans.

Neural activity can be imaged optically with the use of genetically encoded calcium

sensitive ﬂuorescent indicators, such as GCaMP6s used in this work [7].

Historically calcium imaging was often conducted in head-ﬁxed or anesthetized

animals to avoid challenges involved with imaging moving samples [4, 8, 9].

Recently, however, whole-brain imaging was demonstrated in freely behaving C.

elegans [10, 11]. C. elegans are a small transparent nematode, approximately 1mm

in length, with a compact nervous system of only 302 neurons. About half of the

neurons are located in the animal’s head, which we refer to as its brain.

Analyzing ﬂuorescent images of moving and deforming brains requires algorithms

to detect neurons across time and extract ﬂuorescent signals in 3D. Several

strategies exist for tracking neurons in volumetric recordings. One approach is to

ﬁnd correspondences between neuron positions in consecutive time points, for

example, by applying a distance minimization, and then stitching these

2

correspondences together through time [12]. This type of time-dependent tracking

requires that neuron displacements for each time step are less than the distance

between neighboring neurons, and that the neurons remain identiﬁable at all times.

If these requirements break down, even for only a few time points, errors can

quickly accumulate. Other common methods, like independent component analysis

(ICA) [13] are also exquisitely sensitive to motion and as a result they have not

been successfully applied to recordings with large brain deformations.

Large inter-volume motion arises when the recorded image volume acquisition rate

is too low compared to animal motion. Unfortunately, large inter-volume brain

motion is likely to be a prominent feature of whole-brain recordings of moving

brains for the foreseeable future. In all modern imaging approaches there is a

fundamental tradeoﬀ between the following attributes: acquisition rate (temporal

resolution), spatial resolution, signal to noise, and the spatial extent of the

recording. As recordings seek to capture larger brain regions at single cell

resolution, they necessarily compromise on temporal resolution. For example,

whole brain imaging in freely moving C. elegans has only been demonstrated at

slow acquisition rates because of the requirements to scan the entire brain volume

and expose each slice for suﬃciently long time. At these rates, a signiﬁcant amount

of motion is present between image planes within a single brain volume. Similarly,

large brain motions also remain between sequential volumes. Neurons can move the

entire width of the worm’s head between sequential volumes when recording at 6

brain-volumes per second, as in [10]. In addition to motion, the brain also bends

and deform as it moves. Such changes to the brain’s conformation greatly alter the

pattern of neuron positions making constellations of neurons diﬃcult to compare

across time.

3

To account for this motion, previous work that measured neural activity in freely

moving C. elegans required either large amounts of manual annotation as reference

data for comparison [11] or required a human user to supervise and correct

semi-automated algorithms for each and every neuron-time point [10]. This level of

manual annotation becomes impractical as the length of recordings and the number

of neurons increases. For example, 10 minutes of recorded neural activity from [10],

had over 360,000 neuron time points and required over 200 person-hours of manual

annotation. Here, we introduce a new time-independent algorithm that uses

machine learning to automatically segment and track all neurons in the head of a

freely moving animal. We call this technique Neuron Registration Vector Encoding,

and we use it to extract neural signals in unrestrained C. elegans expressing the

calcium indicator GCaMP6s and the ﬂuorescent label RFP.

Results

Overview of neuron tracking analysis

We introduce a method to track over 100 neurons in the brain of a freely moving

C. elegans. The analysis pipeline is made of ﬁve modules and an overview is shown

in Figure 1. The ﬁrst three modules, “Centerline Detection,” “Straightening” and

“Segmentation,” collectively assemble the individually recorded planes into a

sequence of 3D volumes and identify each neuron’s location in each volume. The

next two modules, “Registration Vector Construction” and “Clustering,” form the

core of the method and represent a signiﬁcant advance over previous approaches.

Collectively, these two modules are called “Neuron Registration Vector Encoding".

The “Registration Vector Construction” module leverages information from across

4

Figure 1:
Schematic of analysis pipeline to segment and track neurons through
time and extract their neural activity in a deforming brain. Neurons are labeled
with calcium insensitive red ﬂuorescent proteins, RFP, and calcium sensitive green
ﬂuorescent proteins, GCaMP. Videos of the animal’s behavior and volumetric ﬂuo-
rescent images of the animal’s brain serve as input to the pipeline. The algorithm
detects all neurons in the head and produces tracks of the neural activity across time
as the animal moves.

5

the entire recording in a time-independent way to generate feature vectors that

characterize every neuron at every time point in relation to a repertoire of brain

conﬁrmations. The “Clustering” module then clusters these feature vectors to

assign a consistent identity to each neuron across the entire recording. A ﬁnal

module corrects for errors the original segmentation. The implementation and

results of this approach are described below.

Recording of whole-brain calcium activity and body posture in

moving animal

Worms expressing the calcium indicator GCaMP6s and a calcium-insensitive

ﬂuorescent protein RFP in the nuclei of all neurons were imaged during

unrestrained behavior in a custom 3D tracking microscope, as described in [10].

Two recordings are presented in this work: a new 8 minute recording of an animal

of of strain AML32 and a previously reported 4 minute recording of strain ﬁrst

described in [10].

The signal of interest in both recordings is the green ﬂuorescence intensity from

GCaMP6s in each neuron. Red ﬂuorescence from the RFP protein serves as a

reference for locating and tracking the neurons. The microscope provides four raw

image streams that serve as inputs for our neural tracking pipeline, seen in Fig 2A.

They are: (1) low-magniﬁcation dark-ﬁeld images of the animal’s body posture (2)

low-magniﬁcation ﬂuorescent images of the animal’s brain (3) high-magniﬁcation

green ﬂuorescent images of single optical slices of the brain showing GCaMP6s

activity and (4) high-magniﬁcation red ﬂuorescent images of single optical slices of

the brain showing the location of RFP. The animal’s brain is kept centered in the

ﬁeld of view by realtime feedback loops that adjust a motorized stage to

6

Figure 2: (A) Example images from all four video feeds from our imaging system.
Both scale bars are 100µm (B)A schematic illustrating the timings from all the
devices that run in open loop in our imaging setup. The camera that collects high
magniﬁcation images captures at 200Hz. The two low magniﬁcation images capture
at 60Hz, and the focal plane moves up and down in a 3 Hz triangle wave. The
cameras are synchronized post-hoc using a camera ﬂash and each image is assigned
a timestamp on a common timeline for the purposes of analysis.

compensate for the animal’s crawling. To acquire volumetric information, the high

magniﬁcation imaging plane scans back and forth along the axial dimension, z, at 3

Hz as shown in Fig 2B, acquiring roughly 33 optical slices per volume, sequentially,

for 6 brain-volumes per second. The animal’s continuous motion causes each

volume to be arbitrarily sheared in z. Although the image streams operate at

diﬀerent volume acquisition rates and on diﬀerent clocks, they are later

synchronized by simultaneous light ﬂashes and given a timestamp on a common

timeline. Each of the four imaging streams are aligned to each other in software

using aﬃne transformations found by imaging ﬂuorescent beads.

7

Centerline detection and gross brain alignment

The animal’s posture contains information about the brain’s orientation and about

any deformations arising from the animal’s side-to-side head swings. The ﬁrst step

of the pipeline is to extract the centerline that describes the animal’s posture.

Centerline detection in C. elegans is an active ﬁeld of research. Most algorithms

use intensity thresholds to detect the worm’s body and then use binary image

operations to extract a centerline [14, 15, 16]. Here we use an open active contour

approach [17, 18] to extract the centerline from dark ﬁeld images with

modiﬁcations to account for cases when the worm’s body crosses over itself as

occurs during so-called “Omega Turns.” In principle any method, automated or

otherwise, that detects the centerlines should be suﬃcient. At rare times where the

worm is coiled and the head position and orientation cannot be determined

automatically, the head and the tail of the worm are manually identiﬁed.

The animal’s centerline allows us to correct for gross changes in the worm’s

position, orientation, and conformation (Fig 3a). We use the centerlines

determined by the low magniﬁcation behavior images to straighten the high

magniﬁcation images of the worm’s brain. An aﬃne transform must be applied to

the centerline coordinates to transform them from the dark ﬁeld coordinate system

into the coordinate system of the high magniﬁcation images. Each image slice of

the worm brain is straightened independently to account for motion within a single

volume. The behavior images are taken at a lower acquisition rate than the high

magniﬁcation brain images, so a linear interpolation is to used obtain a centerline

for each slice of the brain volume. In each slice, we ﬁnd the tangent and normal

vectors at every point of the centerline (Fig 3b). The points are interpolated with a

single pixel spacing along the centerline to preserve the resolution of the image.

8

(A) Centerlines are detected from the low magniﬁcation dark ﬁeld images.
Figure 3:
The centerline is shown in green and the tip of the worm’s head is indicated by a
blue dot. (B) The centerline found from the low magniﬁcation image is overlaid on
the high magniﬁcation RFP images. The lines normal to the centerline, shown in
blue, are used to straighten the image. All scale bars are 100 µm.(C) A maximum
intensity projection of the straightened volume is shown. Individual neuronal nuclei
are shown (D) before and (E) after segmentation.

The image intensities along each of the normal directions are interpolated and the

slices are stacked to produce a straightened image in each slice (Fig 3c). In the new

coordinate system, the orientation of the animal is ﬁxed and the worm’s bending is

greatly suppressed. We further reduce shearing between slices using standard video

stabilization techniques [19]. Speciﬁcally, bright-intensity peaks in the image are

tracked between neighboring image slices and aﬃne transformations are calculated

between each slice of the volume. All slices are registered to the middle slice by

applying these transformations sequentially throughout the volume. Each slice

would undergo transformations for every slice in between it and the middle slice to

correct shear throughout the volume. A ﬁnal rigid translation is required to align

each volume to the initial volume. The translations are found by ﬁnding an oﬀset

that maximizes the cross-correlation between each volume and the initial volume.

9

Segmentation

Before neuron identities can be matched across time, we must ﬁrst segment the

individual neurons within a volume to recovers each neuron’s size, location, and

brightness (Fig 3d,e). Many algorithms have been developed to segment neurons in

a dense region [20, 21]. We segment the neurons by ﬁnding volumes of curvature in

ﬂuorescence intensity. We compute the 3D Hessian matrix at each point in space

and threshold for points where all of the three eigenvalues of the Hessian matrix

are negative. In order to further divide regions into objects that are more likely to

represent neurons, we use a watershed separation on the distance transform of the

thresholded image. The distance transform is found by replacing each thresholded

pixel with the Euclidean distance between it and the closest zero pixel in the

thresholded image. Image blurring from animal motion poses a challenge for

segmentation. We allow for some noise and error in the segmentation because we

will have the opportunity to automatically correct many of these errors later in the

pipeline.

Neuron registration vector construction

Extracting neural signals requires the ability to match neurons found at diﬀerent

time points. Even after gross alignment and straightening, neurons in our images

are still subject to local nonlinear deformations and there is signiﬁcant movement

of neurons between volumes. Rather than tracking through time, the neurons in

each volume are characterized based on how they match to neurons in a set of

reference volumes. Our algorithm compares constellations of neurons in one volume

to unannotated reference volumes and assigns correspondences or “matches”

10

between the neurons in the sample and each reference volume. We modiﬁed a point

set registration algorithm developed by Jian and Vemuri [22] to do this (Fig 4a).

The registration algorithm represents two point sets, a sample point-set denoted by

X = {xi} and a reference point-set indicated by R = {ri}, as Gaussian mixtures

and then attempts to register them by deforming space to minimize the distance

between the two mixtures. Here, each neuron is modeled by a 3D Gaussian with

uniform covariance. Since we are matching images of neurons rather than just

points, we can use the additional information from the size and brightness of each

neuron. We add this information to the representation of each neuron by adjusting

the amplitude and standard deviation of the Gaussians. The Gaussian mixture

representation of an image is given by,

f (ξ, X) =

Ai exp

−

(cid:88)

i

(cid:18)

(cid:107)ξ − xi(cid:107)2
2(λσi)2

(cid:19)
,

(1)

where Ai, xi, and σi are the amplitude, mean, and standard deviation of the i-th

Gaussian. These parameters are derived from the brightness, centroid, and size of

the segmented neuron, while ξ is the 3D spatial coordinate. A scale factor λ is

added to the standard deviation to scale the size of each Gaussian. This will be

used later during gradient descent. The sample constellation of neurons is then

represented by the Gaussian mixture f (ξ, X). Similarly, the reference

constellation’s own neurons is represented as a f (ξ, R).

To match a sample constellation of neurons X with a reference constellation of

neurons R, we use the non rigid transformation u : IR3 (cid:55)→ IR3. The transformation

u maps X to u[X] such that the L2 distance between f (ξ, u[X]) and f (ξ, R) is

minimized with some constraint on the amount of deformation. This can be

written as an energy minimization problem, with the energy of the transformation,

11

E(u), written as

(cid:90)

E(u) =

(cid:2)f (ξ, u[X]) − f (ξ, R)(cid:3)2dξ + EDeformation(u).

(2)

Note that the point-sets X and R are allowed to have diﬀerent numbers of points.

We model the deformations as a thin-plate spline (TPS). The transformation

equations and resulting form of Edeformation(u) is shown in the methods. The

minimization of E is found by gradient descent. Since the energy landscape has

many local minima, we initially chose a large scale factor, λ, to increase the size of

each Gaussian and smooth over smaller features. During gradient descent, λ is

decreased to better represent the original image. After the transformation, sample

points are matched to reference points by minimizing distances between assigned

pairs [12]. The matching is not greedy, and neurons in the sample that are far from

any neurons in the reference are not matched. A neuron at xi is assigned a match

vi to indicate which neuron in the set R it was matched to. For example if xi

matched with rj when X is registered to R, then vi = j. If xi has no match in R,

then vi = ∅.

The modiﬁed non-rigid point set registration algorithm described above allows us

to compare one constellation of neurons to another. In principle, neuron tracking

could be achieved by registering the constellation of neurons at each time-volume

to a single common reference. That approach is susceptible to failures in non-rigid

point set registration. Non-rigid point-set registration works well when the

conformation of the animal in the sample and the reference are similar, but it is

unreliable when there are large deformations between the sample and the reference,

as happens with some regularity in our recordings. In addition, this approach is

especially sensitive to any errors in segmentation, especially in the reference. An

12

Figure 4:
Schematic illustration of neuron registration vector encoding. (A) The
registration between a sample volume and a single reference volume is done in several
steps. I. The image is segmented into regions corresponding to each of the neurons.
II. The image is represented as a Gaussian mixture, with a single Gaussian for each
segmented region. The amplitude and the standard deviation of the Gaussians are
derived from the brightness and the size of the segmented regions. III. Non-rigid
point-set registration is then used to deform the sample points to best overlap the
reference point-set. IV. Neurons from the sample and the reference point-sets are
paired by minimizing distances between neurons. (B) Neuron registration vectors
are constructed by assigning a feature vector vi,t to each neuron xi,t in a sample
volume xt by performing the registration between the sample volume and a set of
300 reference volumes, each denoted by rk. Each registration of the neuron results in
a neuron match, vk
i , and the set of matches becomes the feature vector vi,t. (C) The
vectors from all neuron-times, vi,t, are hierarchically clustered. The same neuron
found at diﬀerent times will have a similar set of features and therefore will contain
the same neuron found at diﬀerent times. Real matches occur in a high dimensional
space. Only two dimensions are illustrated here for clarity. Each of the feature
vector is assigned a cluster, and the cluster labels are given by S. (D) The clustering
of the feature vectors shown in (C) assigns an identity to each of the neurons in
every volume. This allows us to track the neurons across diﬀerent volumes of the
recording.

13

alternative approach would be to sequentially register neurons in each time volume

to the next time-volume. This approach, however, accumulates even small errors

and quickly becomes unreliable. Instead of either of those approaches, we use

registration to compare the constellation of neurons at each time volume to a set of

reference time-volumes that span a representative space of brain conformations

(Fig 4b), as described below.

The constellation of neurons at a particular time in our recording is given by Xt,

and the position of the i-th neuron at time t is denoted by xi,t. We select a set of

K reference constellations, each from a diﬀerent time volume Xt in our recording,

so as to achieve a representative sampling of the many diﬀerent possible brain

conformations the animal can attain. These K reference volumes are denoted by

{R1, R2, R3, ..., RK}. For simplicity, we use 300 volumes spaced evenly through

time as our reference constellations. Each Xt is separately matched with each of

the references, and each neuron in the sample, xi,t, gets a set of matches

vi,t = {v1

i,t, v2

i,t, v3

i,t, ..vK

i,t}, one match for each of the K references. This set of

matches is a feature vector which we call a Neuron Registration Vector. It

describes the neuron’s location in relation to its neighbors when compared with the

set of references. This vector can be used to identify neurons across diﬀerent times.

Clustering registration vectors

The neuron registration vector provides information about that neuron’s position

relative to its neighbors, and how that relative position compares with many other

reference volumes. A neuron with a particular identity will match similarly to the

set of reference volumes and thus that neuron will have similar neuron registration

vectors over time. Clustering similar registration vectors allows for the

14

identiﬁcation of that particular neuron across time (Fig 4c,d).

To illustrate the motivation for clustering, consider a neuron with identity s that is

found at diﬀerent times in two sample constellations X 1 and X 2. When X 1 and

X 2 have similar deformations, the neuron s from both constellations will be

assigned the same set of matches when registered to the set of reference

constellations, and as a result the corresponding neuron registration vectors v1 and

v2 will be identical. This is true even if the registration algorithm itself fails to

correctly match neuron s in the sample to its true neuron s in the reference. As the

deformations separating X 1 and X 2 become larger, the distance between the

feature vectors v1 and v2 also becomes larger. This is because the two samples will

be matched to diﬀerent neurons in some of the reference volumes as each sample is

more likely to register poorly with references that are far from it in the space of

deformations.

Crucially, the reference volumes consist of instances of the animal in many diﬀerent

deformation states. So while errors in registering some samples will exist for

certain reference, they do not persist across all references, and thus do not eﬀect

the entire feature vector. For the biologically relevant deformations that we

observe, the distance between v1 and v2 will be smaller if both are derived from

neuron s than compared to the distance between v1 and v2 if they were derived

from s and another neuron. We can therefore cluster the feature vectors to produce

groups that consist of the same neuron found at many diﬀerent time points.

The list of neuron registration vectors from all neuron at all times, {vi,t}, is

hierarchically clustered. Each match in the vectors, vk

i,t, is represented as a binary

vector of 0s with a 1 at the vk−th

i

position. The size of the vector is equal to the

number of neurons in Rk. The feature vector {vi,t} is the concatenation of all of

15

Figure 5: Example of consensus voting to correct a misidentiﬁed neuron. In volume
735, neuron #111 is found successfully and is indicated in green. In volume 736,
however, the neuron is misidentiﬁed, shown in red. During the correction phase, all
other time points vote for what the position of neuron #111 should be assuming a
thin-plate spline deformation, indicated by the cloud of blue points. Since the initial
estimate of the position is far from the cloud of blue points, a corrected position is
selected as the centroid of the votes weighted by image intensity. This process is
repeated to correct errors for every neuron at every time.

the binary vectors from all matches to the K reference constellations. The

correlation distance, 1 − corr(vm, vn), was used as the pair wise distance metric for

clustering. Clusters were checked to ensure that at most one neuron was

represented from each time. Clusters containing neurons from less than 40% of the

volumes were removed. Each cluster is assigned a label {S1, S2, S3, ...} which

uniquely identiﬁes a single neuron over time, and each neuron at each time xi,t is

given an identiﬁer si,t which corresponds to which cluster that neuron-time belongs

to. Neurons that are not assigned to one of these clusters are removed because they

are likely artifactual or represent a neuron that is segmented too poorly for

inclusion.

Correcting errors in tracking and segmentation

Neuron Registration Vector Encoding successfully identiﬁes segmented neurons

consistently across time. A transient segmentation error, however, would

16

necessarily lead to missing or misidentiﬁed neurons. To identify and correct for

missing and misidentiﬁed neurons, we check each neuron’s locations and ﬁll in

missing neurons using a consensus comparison and interpolation in a thin-plate

spline (TPS) deformed space. For each neuron identiﬁer s and time t(cid:63), we use all

other point-sets, {Xt} to guess what that neuron’s location might be. This is done

by ﬁnding the TPS transformation, ut→t(cid:63) : Xt (cid:55)→ Xt(cid:63), that maps the identiﬁed

points from Xt to the corresponding points in Xt(cid:63) excluding the point s. The

position estimate is then given by ut→t(cid:63)[xi,t] with i selected such that si,t = s. This

results in a set of points representing the predicted location of the neuron at time

t(cid:63) from all other times. When a neuron identiﬁer is missing for a given time, the

position of that neuron s is inferred by consensus. Namely, correct location is

deemed to be the centroid of the set of inferred locations weighted by the

underlying image intensity. This weighted centroid is also used if the current

identiﬁed location of the neuron s has a distance greater than 3 standard

deviations away from the centroid of the set of points, implying that an error may

have occurred in the assignment. This is shown in Fig 5, where neuron 111 is

identiﬁed in volume 735, but the the label for neuron 111 is incorrectly located in

volume 736. In that case the weighted centroid from consensus voting was used.

Comparison with manually annotated data

To asses the accuracy of the Neuron Registration Vector Encoding pipeline, we

applied our automated tracking system to a 4 minute recording of whole brain

activity in a moving C. elegans that had previously been hand annotated [10]. A

custom Matlab GUI was used for manually identifying and tracking neurons. Nine

researchers collectively annotated 70 neurons from each of the 1519 volumes in the

17

Figure 6:
Comparison of the automated Neuron Registration Vector Encoding
algorithm with manual human annotation over a 4 minute recording on brain activity
(strain AML14). (A) Spheres show position of neurons that were detected by the
automated algorithm. Grey indicates a neuron detected by both the algorithm and
the human (70 neurons). Red indicates neurons that were missed by the human
and detected only by the algorithm (49 neurons). (B) Histogram showing number of
neurons that were mismatched for a given fraction of time-volumes when comparing
automated and manual approaches. Only those neurons that were consistently found
by both algorithm and human were considered. An automatically identiﬁed neuron
was deemed correctly matched for a given time-volume if it was paired with the
correct corresponding manual neuron.

18

4 minute video. This is much less than the 181 neurons predicted to be found in

the head [23]. The discrepancy is likely caused by a combination of imaging

conditions and human nature. The short exposure time of our recordings makes it

hard to resolve dim neurons, and the relatively long recordings tend to cause

photobleaching which make the neurons even dimmer. Additionally, human

researchers naturally tend to select only those neurons that are brightest and are

most unambiguous for annotation, and tend to skip dim neurons or those neurons

that are most densely clustered.

We compared human annotations to our automated analysis in this same dataset.

We performed the entire pipeline including detecting centerlines, performing worm

straightening, segmentation, and neuron registration vector encoding and

clustering, and correction. Automated tracking detected 119 neurons from the

video compared to 70 from the human. In each volume, we paired the

automatically tracked neurons with those found by manual detection by ﬁnding the

closest matches in the unstraightened coordinate system. A neuron was perfectly

tracked if it matched with the same manual neuron at all times. Tracking errors

were labelled when a neuron matched with a manual neuron that was diﬀerent

than the one it matched with most often. The locations of the detected neurons are

shown in Fig 6A. Only one neuron was incorrectly identiﬁed in more than 5% of

the time volumes (Fig 6B). The locations of neurons and the corresponding error

rates is shown in Fig 6B. Neurons that were detected by the algorithm but not

annotated manually are shown in gray. Upon further inspection, it was noted that

some of the mismatches between our method and the manual annotation were due

to human errors in the manual annotation, meaning the algorithm is able to correct

humans on some occasions.

19

Figure 7: A trace of neural activity from 144 neurons in the brain a C. elegans as it
freely moves on an agarose plate over 8 minutes (strain AML32). The neural activity
is expressed as a fold change over baseline of the ratio of GCaMP6s to RFP for each
neuron. The behavior is indicated in the ethogram. On the right is the locations of
all of the detected neurons (the head of the worm is towards the top of the page).
The neurons that have signiﬁcant correlation with reverse locomotion are indicated
in red. White gaps indicate instances where neurons failed to segment.

Neural activity from tracked neurons

Fluorescent intensity is ultimately the measurement of interest and this can be

easily extracted from the tracks of the neuron locations across time. The pixels

within a 2 µm radius sphere around each point are used to calculate the average

ﬂuorescent intensity of a neuron in both the red RFP and green GCaMP6s

channels. We used the RFP as a reference ﬂuorophore and measure neural activity

as a fold change over baseline of the ratio of GcAMP6s to RFP intensity,

Activity =

∆R
R0

=

R − R0
R0

,

R =

IGCaM P 6s
IRF P

.

(3)

20

The baseline for each neuron, R0, is deﬁned as the 20th percentile value of the ratio

R for that neuron. Fig 7 shows calcium imaging traces extracted from new

whole-brain recordings using the registration vector pipeline. 144 neurons were

tracked for approximately 8 minutes as the worm moves. Many neurons show clear

correlation with reversal behaviors in the worm.

Discussion

The Neuron Registration Vector Encoding method presented here is able to process

longer recordings and locate more neurons with less human input compared to

previous examples of whole-brain imaging in freely moving C. elegans [10]. Fully

automated image processing means that we are no longer limited by the human

labor required for manual annotation. In new recordings presented here, we are

able to observe 144 neurons of the expected 181 neurons, much larger than the 80

observed in previous work from our lab and others [10, 11]. By automating

tracking and segmentation, this relieves one of the major bottlenecks to analyzing

longer recordings.

The neuron registration vector encoding algorithm primarily relies on the local

coherence of the motion of the neurons. It permits large deformations of the

worm’s centerline so long as deformations around the centerline remain modest.

Crucially, the algorithm’s time-independent approach allows it to tolerate large

motion between consecutive time-volumes. These properties make it well suited for

our neural recordings of C. elegans and we suspect that the our approach would be

applicable to tracking neurons in moving and deforming brains from other

organisms as well.

21

Certain classes of recordings, however, would not be well suited for Neuron

Registration Vector Encoding and Clustering. The approach will fail when the

local coherence of neuron motion breaks down. For example, if one neuron were to

completely swap locations with another neuron relative to its surroundings,

registration would not detect the switch and our method would fail. In this case a

time-dependent tracking approach may perform better.

In addition, proper clustering of the feature vectors requires the animal’s brain to

explore a contiguous region of deformation space. For example, if a hypothetical

brain were only ever to occupy two distinct conformations that are diﬀerent enough

that registration is not reliable between these two conformation states, the

algorithm would fail to cluster feature vectors from the same neuron across the two

states. To eﬀectively identify the neurons in these two conformations, the animal’s

brain must sample many conformations in between those two states. This way,

discrepancies in registration arise gradually and the resulting feature vectors

occupy a continuous region in the space of possible feature vectors. Note that a

similar requirement would necessarily apply to any time-dependent tracking

algorithm as well.

We suspect that brain recordings from most species of interest meet these two

requirements: namely neuron motion will have local coherence and the brain will

explore a contiguous region of deformation space. Where these conditions are

satisﬁed, we expect registration vector encoding to work well. Tracking in C.

elegans is especially challenging because the entire brain undergoes large

deformations as the animal bends. In most other organisms like zebraﬁsh and

Drosophila, brains are contained within a skull or exoskeleton and relative motion

of the neurons is small. In those organisms, ﬂuctuations in neuron positions take

22

the form of rigid global transformations as the animal moves, or local non-linear

deformations due to motion of blood vessels. We expect that this approach will be

applicable there as well.

Methods

Strains

twice.

Imaging C. elegans

Transgenic worms were cultivated on nematode growth medium (NGM) plates with

OP50 bacteria. Strain AML32 (wtfIs5[Prab-3 ::NLS::GCaMP6s;

Prab-3 ::NLS::tagRFP]) was generated by UV irradiating animals of strain AML14

(wtfEx4[Prab-3 ::NLS::GCaMP6s; Prab-3 ::NLS::tagRFP]) [10] and outcrossing

Imaging is performed as described in Nguyen et al [10]. The worm is placed

between an agarose slab and a large glass coverslip. The coverslip is held up by a

0.006” plastic shims in order to reduce the amount of pressure on the worm from

the glass, and mineral oil is spread over the worm to better match refractive indices

in the space between the coverglass and the worm. The dark ﬁeld image is used to

extract the animal’s centerline while the ﬂuorescent image is used for tracking the

worm’s brain. Only the head of the worm is illuminated by the ﬂuorescent

excitation light and can be observed in the low magniﬁcation ﬂuorescent image.

23

Thin Plate Spline deformations

Point-set registration was done as described by Jian and Vemuri [22], using TPS

deformations. Given a set of n initial control points X = {xi}, and the set of

transformed points, u[X], the transformation u can be written as

u[X] = WU(X) + AX + t. The aﬃne portion of the transformation is AX + t,

while WU(X) is the non-linear part of the transformation from TPS. U(X) is an

n × 1 vector with Ui(x) = U(x, xi) = U((cid:107)x − xi(cid:107)) = 1

(cid:107)x−xi(cid:107) and W is a 3 × n

matrix. The elements of W, A and t can be ﬁt given the set of control points X

and the location of the transformed points u(X). The energy of bending,

EBending(u), depends on how the control points are deformed. We use the same

energy as in [22], with EBending(u) = trace(WKWT) where Kij = U (xi, xj). Since

the integral in Eqn. 2 is easily computed analytically, the energy can be quickly

calculated and the parameters for W, A, and t for the minimization can be found

using gradient descent.

Algorithm implementation

The analysis steps shown in Fig 1 were performed on Princeton University’s

high-performance scientiﬁc computing cluster, “Della.” Jobs were run on up to 200

cores. Straightening, segmentation, and feature extraction are parallelized over

each volume, with each volume being processed on a single core. Error correction is

parallelized over each neuron. Centerline extraction in each image relies on the

previous centerline and must be computed linearly. The computation methods are

summarized in Table 1. An 8 minute recording of a moving animal has about 3000

volumes and 250 GB of raw imaging data and can be processes from start to ﬁnish

24

Approximate
Percentage of time
4

Analysis Step

Computation

Centerline Detection Linear
Worm Straightening
& Segmentation
Registration Vector
Encoding
Clustering
Error Correction

Parallel over volumes

10

Parallel over volumes

80

Linear
Parallel over neurons

2
4

Table 1: Breakdown of computation time for Neuron Registration Vector Encoding
pipeline.

in less than 40 hours. Data can be found via a “requester-pays” Amazon Web

Services S3 “bucket” at s3://leiferlabnguyentracking and code can be found at

https://github.com/leiferlab/NeRVEclustering. Centerline extraction code is

available at https://github.com/leiferlab/CenterlineTracking.

Acknowledgments

We thank the following for their help manually annotating recordings: F Shipley,

M Liu, S Setru, K Mizes, D Mazumder, J Chinchilla, and L Novak. This work was

supported by Simons Foundation Grant SCGB 324285 (to AML) and Princeton

University’s Inaugural Dean for Research Innovation Fund for New Ideas in the

Natural Sciences (to JWS and AML). JPN is supported by a grant from the Swartz

Foundation. ANL is supported by a National Institutes of Health institutional

training grant through the Princeton Neuroscience Institute. This work is also

supported by a grant from the Glenn Foundation for Medical Research. The

analysis presented in this article was performed on computational resources

supported by the Princeton Institute for Computational Science and Engineering

(PICSciE) and the Oﬃce of Information Technology’s High Performance

25

Computing Center and Visualization Laboratory at Princeton University.

Author contributions

Conceptualization: AML JWS JPN

Methodology: JPN ANL

Software: JPN ANL

Validation: JPN

Formal Analysis: JPN

Investigation: JPN ANL

Resources: JPN ANL GSP

Writing - Original Draft Preparation: JPN

Writing - Review & Editing: AML JWS

Supervision: AML

Funding Acquisition: AML

26

References

[1] Michel A Picardo, Josh Merel, Kalman A Katlowitz, Daniela Vallentin,

Daniel E Okobi, Sam E Benezra, Rachel C Clary, Eftychios A Pnevmatikakis,

Liam Paninski, and Michael A Long. Population-level representation of a

temporal sequence underlying song production in the zebra ﬁnch. Neuron,

90(4):866–876, 2016.

[2] John P Rickgauer, Karl Deisseroth, and David W Tank. Simultaneous

cellular-resolution optical perturbation and imaging of place cell ﬁring ﬁelds.

Nature neuroscience, 17(12):1816–24, 2014.

[3] EM Maynard, NG Hatsopoulos, CL Ojakangas, BD Acuna, JN Sanes,

RA Normann, and JP Donoghue. Neuronal interactions improve cortical

population coding of movement direction. The journal of Neuroscience,

19(18):8083–8093, 1999.

[4] Saul Kato, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H.

Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. Global Brain

Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans.

Cell, 163(3):656–669, 2015.

27

[5] Wenze Li, Venkatakaushik Voleti, Evan Schaﬀer, Rebecca Vaadia, Wesley B.

Grueber, Richard S. Mann, and Elizabeth M. Hillman. Scape microscopy for

high speed, 3d whole-brain imaging in drosophila melanogaster. In Biomedical

Optics 2016, page BTu4D.3. Optical Society of America, 2016.

[6] Robert Prevedel, Young-Gyu Yoon, Maximilian Hoﬀmann, Nikita Pak,

Gordon Wetzstein, Saul Kato, Tina Schrödel, Ramesh Raskar, Manuel

Zimmer, Edward S. Boyden, and Alipasha Vaziri. Simultaneous whole-animal

3D imaging of neuronal activity using light-ﬁeld microscopy. Nat Meth,

11(7):727–730, July 2014.

[7] Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pulver, Sabine L

Renninger, Amy Baohan, Eric R Schreiter, Rex a Kerr, Michael B Orger,

Vivek Jayaraman, Loren L Looger, Karel Svoboda, and Douglas S Kim.

SUPPLEMENTAL - Ultrasensitive ﬂuorescent proteins for imaging neuronal

activity. Nature, 499(7458):295–300, 2013.

[8] Christopher D Harvey, Forrest Collman, Daniel A Dombeck, and David W

Tank. Intracellular dynamics of hippocampal place cells during virtual

navigation. Nature, 461(7266):941–946, 2009.

[9] Misha B Ahrens, Jennifer M Li, Michael B Orger, Drew N Robson,

Alexander F Schier, Florian Engert, and Ruben Portugues. Brain-wide

neuronal dynamics during motor adaptation in zebraﬁsh. Nature,

485(7399):471–477, May 2012. PMID: 22622571.

[10] Jeﬀrey P. Nguyen, Frederick B. Shipley, Ashley N. Linder, George S.

Plummer, Mochi Liu, Sagar U. Setru, Joshua W. Shaevitz, and Andrew M.

Leifer. Whole-brain calcium imaging with cellular resolution in freely behaving

28

caenorhabditis elegans. Proceedings of the National Academy of Sciences,

113(8):E1074–E1081, 2016.

[11] Vivek Venkatachalam, Ni Ji, Xian Wang, Christopher Clark, James Kameron

Mitchell, Mason Klein, Christopher J Tabone, Jeremy Florman, Hongfei Ji,

Joel Greenwood, Andrew D Chisholm, Jagan Srinivasan, Mark Alkema, Mei

Zhen, and Aravinthan D T Samuel. Pan-neuronal imaging in roaming

Caenorhabditis elegans. 113(8):E1082–8, 2016.

[12] J Crocker and D Grier. Methods of Digital Video Microscopy for Colloidal

Studies. Journal of Colloid and Interface Science, 179(1):298–310, 1996.

[13] Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer. Automated

Analysis of Cellular Signals from Large-Scale Calcium Imaging Data. Neuron,

63(6):747–760, 2009.

[14] Greg J. Stephens, Bethany Johnson-Kerner, William Bialek, and William S.

Ryu. Dimensionality and dynamics in the behavior of C. elegans. PLoS

Computational Biology, 4(4), 2008.

[15] Hanchuan Peng, Fuhui Long, Xiao Liu, Stuart K. Kim, and Eugene W. Myers.

Straightening Caenorhabditis elegans images. Bioinformatics, 24(2):234–242,

2008.

[16] Onno D Broekmans, Jarlath B Rodgers, William S Ryu, and Greg J Stephens.

Resolving coiled shapes reveals new reorientation behaviors in C. elegans.

eLife, 5:e17227, sep 2016.

[17] Yi Deng, Philip Coen, Mingzhai Sun, and Joshua W Shaevitz. Eﬃcient

multiple object tracking using mutually repulsive active membranes. PloS one,

8(6):e65769, 2013.

29

[18] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active

contour models. International Journal of Computer Vision, 1(4):321–331,

1988.

2002.

[19] Ben Tordoﬀ and David W Murray. Guided sampling and consensus for motion

estimation. In European conference on computer vision, pages 82–96. Springer,

[20] Gang Lin, Umesh Adiga, Kathy Olson, John F Guzowski, Carol a Barnes, and

Badrinath Roysam. A hybrid 3D watershed algorithm incorporating gradient

cues and object models for automatic segmentation of nuclei in confocal image

stacks. Cytometry. Part A : the journal of the International Society for

Analytical Cytology, 56(1):23–36, 2003.

[21] Yu Toyoshima, Terumasa Tokunaga, Osamu Hirose, Manami Kanamori,

Takayuki Teramoto, Moon Sun Jang, Sayuri Kuge, Takeshi Ishihara, Ryo

Yoshida, and Yuichi Iino. Accurate Automatic Detection of Densely

Distributed Cell Nuclei in 3D Space. PLOS Computational Biology,

12(6):e1004970, 2016.

[22] Bing Jian and Baba C Vemuri. A Robust Algorithm for Point Set Registration

Using Mixture of Gaussians. Proceedings / IEEE International Conference on

Computer Vision. IEEE International Conference on Computer Vision,

2:1246–1251, oct 2005.

[23] John G White, Eileen Southgate, J Nichol Thomson, and Sydney Brenner.

The structure of the nervous system of the nematode caenorhabditis elegans.

Philos Trans R Soc Lond B Biol Sci, 314(1165):1–340, 1986.

30

6
1
0
2
 
t
c
O
 
4
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
9
7
5
4
0
.
0
1
6
1
:
v
i
X
r
a

Automatically tracking neurons in a moving and

deforming brain

Jeﬀrey P. Nguyen1,2, Ashley N. Linder3, George S. Plummer2, #, Joshua

W. Shaevitz1,2 and Andrew M. Leifer1,3,*

1Department of Physics, Princeton University, Princeton, New Jersey, United

States of America

2Lewis-Sigler Institute for Integrative Genomics, Princeton University,

Princeton, New Jersey, United States of America

#Current Address: Tufts University School of Medicine, Boston,

Massachusetts, United States of America

3Princeton Neuroscience Institute, Princeton University, Princeton, New

Jersey, United States of America

*Corresponding author, E-mail: leifer@princeton.edu (AML)

October 17, 2016

Abstract

Advances in optical neuroimaging techniques now allow neural activity to be

recorded with cellular resolution in awake and behaving animals. Brain motion in

these recordings pose a unique challenge. The location of individual neurons must

be tracked in 3D over time to accurately extract single neuron activity traces.

Recordings from small invertebrates like C. elegans are especially challenging

because they undergo very large brain motion and deformation during animal

movement. Here we present an automated computer vision pipeline to reliably

track populations of neurons with single neuron resolution in the brain of a freely

moving C. elegans undergoing large motion and deformation. 3D volumetric

ﬂuorescent images of the animal’s brain are straightened, aligned and registered,

and the locations of neurons in the images are found via segmentation. Each

neuron is then assigned an identity using a new time-independent machine-learning

approach we call Neuron Registration Vector Encoding. In this approach, non-rigid

point-set registration is used to match each segmented neuron in each volume with

a set of reference volumes taken from throughout the recording. The way each

neuron matches with the references deﬁnes a feature vector which is clustered to

assign an identity to each neuron in each volume. Finally, thin-plate spline

interpolation is used to correct errors in segmentation and check consistency of

assigned identities. The Neuron Registration Vector Encoding approach proposed

here is uniquely well suited for tracking neurons in brains undergoing large

deformations. When applied to whole-brain calcium imaging recordings in freely

moving C. elegans, this analysis pipeline located 150 neurons for the duration of an

8 minute recording and consistently found more neurons more quickly than manual

or semi-automated approaches.

2

Author Summary

Computer algorithms for identifying and tracking neurons in images of a brain have

struggled to keep pace with rapid advances in neuroimaging. In small transparent

organism like the nematode C. elgeans, it is now possible to record neural activity

from all of the neurons in the animal’s head with single-cell resolution as it crawls.

A critical challenge is to identify and track each individual neuron as the brain

moves and bends. Previous methods required large amounts of manual human

annotation. In this work, we present a fully automated algorithm for neuron

segmentation and tracking in freely behaving C. elegans. Our approach uses

non-rigid point-set registration to construct feature vectors describing the location

of each neuron relative to other neurons and other volumes in the recording. Then

we cluster feature vectors in a time-independent fashion to track neurons through

time. This new approach works very well when compared to a human.

Introduction

Optical neural imaging has ushered in a new frontier in neuroscience that seeks to

understand how neural activity generates animal behavior by recording from large

populations of neurons at cellular resolution in awake and behaving animals.

Population recordings have now been used to elucidate mechanisms behind zebra

ﬁnch song production [1], spatial encoding in mice [2], and limb movement in

primates [3]. When applied to small transparent organisms, like Caenorhabditis

elegans [4], Drosophila [5], and zebraﬁsh [6], nearly every neuron in the brain can

be recorded, permitting the study of whole brain neural dynamics at cellular

resolution.

1

Methods for segmenting and tracking neurons have struggled to keep up as new

imaging technologies now record from more neurons over longer times in

environments with greater motion. Accounting for brain motion in particular has

become a major challenge, especially in recordings of unrestrained animals. Brains

in motion undergo translations and deformations in 3D that make robust tracking

of individual neurons very diﬃcult. The problem is compounded in invertebrates

like C. elegans where the head of the animal is ﬂexible and deforms greatly. If left

unaccounted for, brain motion not only prevents tracking of neurons, but it can

also introduce artifacts that mask the true neural signal. In this work we propose

an automated approach to segment and track neurons in the presence of dramatic

brain motion and deformation. Our approach is optimized for calcium imaging in

unrestrained C. elegans.

Neural activity can be imaged optically with the use of genetically encoded calcium

sensitive ﬂuorescent indicators, such as GCaMP6s used in this work [7].

Historically calcium imaging was often conducted in head-ﬁxed or anesthetized

animals to avoid challenges involved with imaging moving samples [4, 8, 9].

Recently, however, whole-brain imaging was demonstrated in freely behaving C.

elegans [10, 11]. C. elegans are a small transparent nematode, approximately 1mm

in length, with a compact nervous system of only 302 neurons. About half of the

neurons are located in the animal’s head, which we refer to as its brain.

Analyzing ﬂuorescent images of moving and deforming brains requires algorithms

to detect neurons across time and extract ﬂuorescent signals in 3D. Several

strategies exist for tracking neurons in volumetric recordings. One approach is to

ﬁnd correspondences between neuron positions in consecutive time points, for

example, by applying a distance minimization, and then stitching these

2

correspondences together through time [12]. This type of time-dependent tracking

requires that neuron displacements for each time step are less than the distance

between neighboring neurons, and that the neurons remain identiﬁable at all times.

If these requirements break down, even for only a few time points, errors can

quickly accumulate. Other common methods, like independent component analysis

(ICA) [13] are also exquisitely sensitive to motion and as a result they have not

been successfully applied to recordings with large brain deformations.

Large inter-volume motion arises when the recorded image volume acquisition rate

is too low compared to animal motion. Unfortunately, large inter-volume brain

motion is likely to be a prominent feature of whole-brain recordings of moving

brains for the foreseeable future. In all modern imaging approaches there is a

fundamental tradeoﬀ between the following attributes: acquisition rate (temporal

resolution), spatial resolution, signal to noise, and the spatial extent of the

recording. As recordings seek to capture larger brain regions at single cell

resolution, they necessarily compromise on temporal resolution. For example,

whole brain imaging in freely moving C. elegans has only been demonstrated at

slow acquisition rates because of the requirements to scan the entire brain volume

and expose each slice for suﬃciently long time. At these rates, a signiﬁcant amount

of motion is present between image planes within a single brain volume. Similarly,

large brain motions also remain between sequential volumes. Neurons can move the

entire width of the worm’s head between sequential volumes when recording at 6

brain-volumes per second, as in [10]. In addition to motion, the brain also bends

and deform as it moves. Such changes to the brain’s conformation greatly alter the

pattern of neuron positions making constellations of neurons diﬃcult to compare

across time.

3

To account for this motion, previous work that measured neural activity in freely

moving C. elegans required either large amounts of manual annotation as reference

data for comparison [11] or required a human user to supervise and correct

semi-automated algorithms for each and every neuron-time point [10]. This level of

manual annotation becomes impractical as the length of recordings and the number

of neurons increases. For example, 10 minutes of recorded neural activity from [10],

had over 360,000 neuron time points and required over 200 person-hours of manual

annotation. Here, we introduce a new time-independent algorithm that uses

machine learning to automatically segment and track all neurons in the head of a

freely moving animal. We call this technique Neuron Registration Vector Encoding,

and we use it to extract neural signals in unrestrained C. elegans expressing the

calcium indicator GCaMP6s and the ﬂuorescent label RFP.

Results

Overview of neuron tracking analysis

We introduce a method to track over 100 neurons in the brain of a freely moving

C. elegans. The analysis pipeline is made of ﬁve modules and an overview is shown

in Figure 1. The ﬁrst three modules, “Centerline Detection,” “Straightening” and

“Segmentation,” collectively assemble the individually recorded planes into a

sequence of 3D volumes and identify each neuron’s location in each volume. The

next two modules, “Registration Vector Construction” and “Clustering,” form the

core of the method and represent a signiﬁcant advance over previous approaches.

Collectively, these two modules are called “Neuron Registration Vector Encoding".

The “Registration Vector Construction” module leverages information from across

4

Figure 1:
Schematic of analysis pipeline to segment and track neurons through
time and extract their neural activity in a deforming brain. Neurons are labeled
with calcium insensitive red ﬂuorescent proteins, RFP, and calcium sensitive green
ﬂuorescent proteins, GCaMP. Videos of the animal’s behavior and volumetric ﬂuo-
rescent images of the animal’s brain serve as input to the pipeline. The algorithm
detects all neurons in the head and produces tracks of the neural activity across time
as the animal moves.

5

the entire recording in a time-independent way to generate feature vectors that

characterize every neuron at every time point in relation to a repertoire of brain

conﬁrmations. The “Clustering” module then clusters these feature vectors to

assign a consistent identity to each neuron across the entire recording. A ﬁnal

module corrects for errors the original segmentation. The implementation and

results of this approach are described below.

Recording of whole-brain calcium activity and body posture in

moving animal

Worms expressing the calcium indicator GCaMP6s and a calcium-insensitive

ﬂuorescent protein RFP in the nuclei of all neurons were imaged during

unrestrained behavior in a custom 3D tracking microscope, as described in [10].

Two recordings are presented in this work: a new 8 minute recording of an animal

of of strain AML32 and a previously reported 4 minute recording of strain ﬁrst

described in [10].

The signal of interest in both recordings is the green ﬂuorescence intensity from

GCaMP6s in each neuron. Red ﬂuorescence from the RFP protein serves as a

reference for locating and tracking the neurons. The microscope provides four raw

image streams that serve as inputs for our neural tracking pipeline, seen in Fig 2A.

They are: (1) low-magniﬁcation dark-ﬁeld images of the animal’s body posture (2)

low-magniﬁcation ﬂuorescent images of the animal’s brain (3) high-magniﬁcation

green ﬂuorescent images of single optical slices of the brain showing GCaMP6s

activity and (4) high-magniﬁcation red ﬂuorescent images of single optical slices of

the brain showing the location of RFP. The animal’s brain is kept centered in the

ﬁeld of view by realtime feedback loops that adjust a motorized stage to

6

Figure 2: (A) Example images from all four video feeds from our imaging system.
Both scale bars are 100µm (B)A schematic illustrating the timings from all the
devices that run in open loop in our imaging setup. The camera that collects high
magniﬁcation images captures at 200Hz. The two low magniﬁcation images capture
at 60Hz, and the focal plane moves up and down in a 3 Hz triangle wave. The
cameras are synchronized post-hoc using a camera ﬂash and each image is assigned
a timestamp on a common timeline for the purposes of analysis.

compensate for the animal’s crawling. To acquire volumetric information, the high

magniﬁcation imaging plane scans back and forth along the axial dimension, z, at 3

Hz as shown in Fig 2B, acquiring roughly 33 optical slices per volume, sequentially,

for 6 brain-volumes per second. The animal’s continuous motion causes each

volume to be arbitrarily sheared in z. Although the image streams operate at

diﬀerent volume acquisition rates and on diﬀerent clocks, they are later

synchronized by simultaneous light ﬂashes and given a timestamp on a common

timeline. Each of the four imaging streams are aligned to each other in software

using aﬃne transformations found by imaging ﬂuorescent beads.

7

Centerline detection and gross brain alignment

The animal’s posture contains information about the brain’s orientation and about

any deformations arising from the animal’s side-to-side head swings. The ﬁrst step

of the pipeline is to extract the centerline that describes the animal’s posture.

Centerline detection in C. elegans is an active ﬁeld of research. Most algorithms

use intensity thresholds to detect the worm’s body and then use binary image

operations to extract a centerline [14, 15, 16]. Here we use an open active contour

approach [17, 18] to extract the centerline from dark ﬁeld images with

modiﬁcations to account for cases when the worm’s body crosses over itself as

occurs during so-called “Omega Turns.” In principle any method, automated or

otherwise, that detects the centerlines should be suﬃcient. At rare times where the

worm is coiled and the head position and orientation cannot be determined

automatically, the head and the tail of the worm are manually identiﬁed.

The animal’s centerline allows us to correct for gross changes in the worm’s

position, orientation, and conformation (Fig 3a). We use the centerlines

determined by the low magniﬁcation behavior images to straighten the high

magniﬁcation images of the worm’s brain. An aﬃne transform must be applied to

the centerline coordinates to transform them from the dark ﬁeld coordinate system

into the coordinate system of the high magniﬁcation images. Each image slice of

the worm brain is straightened independently to account for motion within a single

volume. The behavior images are taken at a lower acquisition rate than the high

magniﬁcation brain images, so a linear interpolation is to used obtain a centerline

for each slice of the brain volume. In each slice, we ﬁnd the tangent and normal

vectors at every point of the centerline (Fig 3b). The points are interpolated with a

single pixel spacing along the centerline to preserve the resolution of the image.

8

(A) Centerlines are detected from the low magniﬁcation dark ﬁeld images.
Figure 3:
The centerline is shown in green and the tip of the worm’s head is indicated by a
blue dot. (B) The centerline found from the low magniﬁcation image is overlaid on
the high magniﬁcation RFP images. The lines normal to the centerline, shown in
blue, are used to straighten the image. All scale bars are 100 µm.(C) A maximum
intensity projection of the straightened volume is shown. Individual neuronal nuclei
are shown (D) before and (E) after segmentation.

The image intensities along each of the normal directions are interpolated and the

slices are stacked to produce a straightened image in each slice (Fig 3c). In the new

coordinate system, the orientation of the animal is ﬁxed and the worm’s bending is

greatly suppressed. We further reduce shearing between slices using standard video

stabilization techniques [19]. Speciﬁcally, bright-intensity peaks in the image are

tracked between neighboring image slices and aﬃne transformations are calculated

between each slice of the volume. All slices are registered to the middle slice by

applying these transformations sequentially throughout the volume. Each slice

would undergo transformations for every slice in between it and the middle slice to

correct shear throughout the volume. A ﬁnal rigid translation is required to align

each volume to the initial volume. The translations are found by ﬁnding an oﬀset

that maximizes the cross-correlation between each volume and the initial volume.

9

Segmentation

Before neuron identities can be matched across time, we must ﬁrst segment the

individual neurons within a volume to recovers each neuron’s size, location, and

brightness (Fig 3d,e). Many algorithms have been developed to segment neurons in

a dense region [20, 21]. We segment the neurons by ﬁnding volumes of curvature in

ﬂuorescence intensity. We compute the 3D Hessian matrix at each point in space

and threshold for points where all of the three eigenvalues of the Hessian matrix

are negative. In order to further divide regions into objects that are more likely to

represent neurons, we use a watershed separation on the distance transform of the

thresholded image. The distance transform is found by replacing each thresholded

pixel with the Euclidean distance between it and the closest zero pixel in the

thresholded image. Image blurring from animal motion poses a challenge for

segmentation. We allow for some noise and error in the segmentation because we

will have the opportunity to automatically correct many of these errors later in the

pipeline.

Neuron registration vector construction

Extracting neural signals requires the ability to match neurons found at diﬀerent

time points. Even after gross alignment and straightening, neurons in our images

are still subject to local nonlinear deformations and there is signiﬁcant movement

of neurons between volumes. Rather than tracking through time, the neurons in

each volume are characterized based on how they match to neurons in a set of

reference volumes. Our algorithm compares constellations of neurons in one volume

to unannotated reference volumes and assigns correspondences or “matches”

10

between the neurons in the sample and each reference volume. We modiﬁed a point

set registration algorithm developed by Jian and Vemuri [22] to do this (Fig 4a).

The registration algorithm represents two point sets, a sample point-set denoted by

X = {xi} and a reference point-set indicated by R = {ri}, as Gaussian mixtures

and then attempts to register them by deforming space to minimize the distance

between the two mixtures. Here, each neuron is modeled by a 3D Gaussian with

uniform covariance. Since we are matching images of neurons rather than just

points, we can use the additional information from the size and brightness of each

neuron. We add this information to the representation of each neuron by adjusting

the amplitude and standard deviation of the Gaussians. The Gaussian mixture

representation of an image is given by,

f (ξ, X) =

Ai exp

−

(cid:88)

i

(cid:18)

(cid:107)ξ − xi(cid:107)2
2(λσi)2

(cid:19)
,

(1)

where Ai, xi, and σi are the amplitude, mean, and standard deviation of the i-th

Gaussian. These parameters are derived from the brightness, centroid, and size of

the segmented neuron, while ξ is the 3D spatial coordinate. A scale factor λ is

added to the standard deviation to scale the size of each Gaussian. This will be

used later during gradient descent. The sample constellation of neurons is then

represented by the Gaussian mixture f (ξ, X). Similarly, the reference

constellation’s own neurons is represented as a f (ξ, R).

To match a sample constellation of neurons X with a reference constellation of

neurons R, we use the non rigid transformation u : IR3 (cid:55)→ IR3. The transformation

u maps X to u[X] such that the L2 distance between f (ξ, u[X]) and f (ξ, R) is

minimized with some constraint on the amount of deformation. This can be

written as an energy minimization problem, with the energy of the transformation,

11

E(u), written as

(cid:90)

E(u) =

(cid:2)f (ξ, u[X]) − f (ξ, R)(cid:3)2dξ + EDeformation(u).

(2)

Note that the point-sets X and R are allowed to have diﬀerent numbers of points.

We model the deformations as a thin-plate spline (TPS). The transformation

equations and resulting form of Edeformation(u) is shown in the methods. The

minimization of E is found by gradient descent. Since the energy landscape has

many local minima, we initially chose a large scale factor, λ, to increase the size of

each Gaussian and smooth over smaller features. During gradient descent, λ is

decreased to better represent the original image. After the transformation, sample

points are matched to reference points by minimizing distances between assigned

pairs [12]. The matching is not greedy, and neurons in the sample that are far from

any neurons in the reference are not matched. A neuron at xi is assigned a match

vi to indicate which neuron in the set R it was matched to. For example if xi

matched with rj when X is registered to R, then vi = j. If xi has no match in R,

then vi = ∅.

The modiﬁed non-rigid point set registration algorithm described above allows us

to compare one constellation of neurons to another. In principle, neuron tracking

could be achieved by registering the constellation of neurons at each time-volume

to a single common reference. That approach is susceptible to failures in non-rigid

point set registration. Non-rigid point-set registration works well when the

conformation of the animal in the sample and the reference are similar, but it is

unreliable when there are large deformations between the sample and the reference,

as happens with some regularity in our recordings. In addition, this approach is

especially sensitive to any errors in segmentation, especially in the reference. An

12

Figure 4:
Schematic illustration of neuron registration vector encoding. (A) The
registration between a sample volume and a single reference volume is done in several
steps. I. The image is segmented into regions corresponding to each of the neurons.
II. The image is represented as a Gaussian mixture, with a single Gaussian for each
segmented region. The amplitude and the standard deviation of the Gaussians are
derived from the brightness and the size of the segmented regions. III. Non-rigid
point-set registration is then used to deform the sample points to best overlap the
reference point-set. IV. Neurons from the sample and the reference point-sets are
paired by minimizing distances between neurons. (B) Neuron registration vectors
are constructed by assigning a feature vector vi,t to each neuron xi,t in a sample
volume xt by performing the registration between the sample volume and a set of
300 reference volumes, each denoted by rk. Each registration of the neuron results in
a neuron match, vk
i , and the set of matches becomes the feature vector vi,t. (C) The
vectors from all neuron-times, vi,t, are hierarchically clustered. The same neuron
found at diﬀerent times will have a similar set of features and therefore will contain
the same neuron found at diﬀerent times. Real matches occur in a high dimensional
space. Only two dimensions are illustrated here for clarity. Each of the feature
vector is assigned a cluster, and the cluster labels are given by S. (D) The clustering
of the feature vectors shown in (C) assigns an identity to each of the neurons in
every volume. This allows us to track the neurons across diﬀerent volumes of the
recording.

13

alternative approach would be to sequentially register neurons in each time volume

to the next time-volume. This approach, however, accumulates even small errors

and quickly becomes unreliable. Instead of either of those approaches, we use

registration to compare the constellation of neurons at each time volume to a set of

reference time-volumes that span a representative space of brain conformations

(Fig 4b), as described below.

The constellation of neurons at a particular time in our recording is given by Xt,

and the position of the i-th neuron at time t is denoted by xi,t. We select a set of

K reference constellations, each from a diﬀerent time volume Xt in our recording,

so as to achieve a representative sampling of the many diﬀerent possible brain

conformations the animal can attain. These K reference volumes are denoted by

{R1, R2, R3, ..., RK}. For simplicity, we use 300 volumes spaced evenly through

time as our reference constellations. Each Xt is separately matched with each of

the references, and each neuron in the sample, xi,t, gets a set of matches

vi,t = {v1

i,t, v2

i,t, v3

i,t, ..vK

i,t}, one match for each of the K references. This set of

matches is a feature vector which we call a Neuron Registration Vector. It

describes the neuron’s location in relation to its neighbors when compared with the

set of references. This vector can be used to identify neurons across diﬀerent times.

Clustering registration vectors

The neuron registration vector provides information about that neuron’s position

relative to its neighbors, and how that relative position compares with many other

reference volumes. A neuron with a particular identity will match similarly to the

set of reference volumes and thus that neuron will have similar neuron registration

vectors over time. Clustering similar registration vectors allows for the

14

identiﬁcation of that particular neuron across time (Fig 4c,d).

To illustrate the motivation for clustering, consider a neuron with identity s that is

found at diﬀerent times in two sample constellations X 1 and X 2. When X 1 and

X 2 have similar deformations, the neuron s from both constellations will be

assigned the same set of matches when registered to the set of reference

constellations, and as a result the corresponding neuron registration vectors v1 and

v2 will be identical. This is true even if the registration algorithm itself fails to

correctly match neuron s in the sample to its true neuron s in the reference. As the

deformations separating X 1 and X 2 become larger, the distance between the

feature vectors v1 and v2 also becomes larger. This is because the two samples will

be matched to diﬀerent neurons in some of the reference volumes as each sample is

more likely to register poorly with references that are far from it in the space of

deformations.

Crucially, the reference volumes consist of instances of the animal in many diﬀerent

deformation states. So while errors in registering some samples will exist for

certain reference, they do not persist across all references, and thus do not eﬀect

the entire feature vector. For the biologically relevant deformations that we

observe, the distance between v1 and v2 will be smaller if both are derived from

neuron s than compared to the distance between v1 and v2 if they were derived

from s and another neuron. We can therefore cluster the feature vectors to produce

groups that consist of the same neuron found at many diﬀerent time points.

The list of neuron registration vectors from all neuron at all times, {vi,t}, is

hierarchically clustered. Each match in the vectors, vk

i,t, is represented as a binary

vector of 0s with a 1 at the vk−th

i

position. The size of the vector is equal to the

number of neurons in Rk. The feature vector {vi,t} is the concatenation of all of

15

Figure 5: Example of consensus voting to correct a misidentiﬁed neuron. In volume
735, neuron #111 is found successfully and is indicated in green. In volume 736,
however, the neuron is misidentiﬁed, shown in red. During the correction phase, all
other time points vote for what the position of neuron #111 should be assuming a
thin-plate spline deformation, indicated by the cloud of blue points. Since the initial
estimate of the position is far from the cloud of blue points, a corrected position is
selected as the centroid of the votes weighted by image intensity. This process is
repeated to correct errors for every neuron at every time.

the binary vectors from all matches to the K reference constellations. The

correlation distance, 1 − corr(vm, vn), was used as the pair wise distance metric for

clustering. Clusters were checked to ensure that at most one neuron was

represented from each time. Clusters containing neurons from less than 40% of the

volumes were removed. Each cluster is assigned a label {S1, S2, S3, ...} which

uniquely identiﬁes a single neuron over time, and each neuron at each time xi,t is

given an identiﬁer si,t which corresponds to which cluster that neuron-time belongs

to. Neurons that are not assigned to one of these clusters are removed because they

are likely artifactual or represent a neuron that is segmented too poorly for

inclusion.

Correcting errors in tracking and segmentation

Neuron Registration Vector Encoding successfully identiﬁes segmented neurons

consistently across time. A transient segmentation error, however, would

16

necessarily lead to missing or misidentiﬁed neurons. To identify and correct for

missing and misidentiﬁed neurons, we check each neuron’s locations and ﬁll in

missing neurons using a consensus comparison and interpolation in a thin-plate

spline (TPS) deformed space. For each neuron identiﬁer s and time t(cid:63), we use all

other point-sets, {Xt} to guess what that neuron’s location might be. This is done

by ﬁnding the TPS transformation, ut→t(cid:63) : Xt (cid:55)→ Xt(cid:63), that maps the identiﬁed

points from Xt to the corresponding points in Xt(cid:63) excluding the point s. The

position estimate is then given by ut→t(cid:63)[xi,t] with i selected such that si,t = s. This

results in a set of points representing the predicted location of the neuron at time

t(cid:63) from all other times. When a neuron identiﬁer is missing for a given time, the

position of that neuron s is inferred by consensus. Namely, correct location is

deemed to be the centroid of the set of inferred locations weighted by the

underlying image intensity. This weighted centroid is also used if the current

identiﬁed location of the neuron s has a distance greater than 3 standard

deviations away from the centroid of the set of points, implying that an error may

have occurred in the assignment. This is shown in Fig 5, where neuron 111 is

identiﬁed in volume 735, but the the label for neuron 111 is incorrectly located in

volume 736. In that case the weighted centroid from consensus voting was used.

Comparison with manually annotated data

To asses the accuracy of the Neuron Registration Vector Encoding pipeline, we

applied our automated tracking system to a 4 minute recording of whole brain

activity in a moving C. elegans that had previously been hand annotated [10]. A

custom Matlab GUI was used for manually identifying and tracking neurons. Nine

researchers collectively annotated 70 neurons from each of the 1519 volumes in the

17

Figure 6:
Comparison of the automated Neuron Registration Vector Encoding
algorithm with manual human annotation over a 4 minute recording on brain activity
(strain AML14). (A) Spheres show position of neurons that were detected by the
automated algorithm. Grey indicates a neuron detected by both the algorithm and
the human (70 neurons). Red indicates neurons that were missed by the human
and detected only by the algorithm (49 neurons). (B) Histogram showing number of
neurons that were mismatched for a given fraction of time-volumes when comparing
automated and manual approaches. Only those neurons that were consistently found
by both algorithm and human were considered. An automatically identiﬁed neuron
was deemed correctly matched for a given time-volume if it was paired with the
correct corresponding manual neuron.

18

4 minute video. This is much less than the 181 neurons predicted to be found in

the head [23]. The discrepancy is likely caused by a combination of imaging

conditions and human nature. The short exposure time of our recordings makes it

hard to resolve dim neurons, and the relatively long recordings tend to cause

photobleaching which make the neurons even dimmer. Additionally, human

researchers naturally tend to select only those neurons that are brightest and are

most unambiguous for annotation, and tend to skip dim neurons or those neurons

that are most densely clustered.

We compared human annotations to our automated analysis in this same dataset.

We performed the entire pipeline including detecting centerlines, performing worm

straightening, segmentation, and neuron registration vector encoding and

clustering, and correction. Automated tracking detected 119 neurons from the

video compared to 70 from the human. In each volume, we paired the

automatically tracked neurons with those found by manual detection by ﬁnding the

closest matches in the unstraightened coordinate system. A neuron was perfectly

tracked if it matched with the same manual neuron at all times. Tracking errors

were labelled when a neuron matched with a manual neuron that was diﬀerent

than the one it matched with most often. The locations of the detected neurons are

shown in Fig 6A. Only one neuron was incorrectly identiﬁed in more than 5% of

the time volumes (Fig 6B). The locations of neurons and the corresponding error

rates is shown in Fig 6B. Neurons that were detected by the algorithm but not

annotated manually are shown in gray. Upon further inspection, it was noted that

some of the mismatches between our method and the manual annotation were due

to human errors in the manual annotation, meaning the algorithm is able to correct

humans on some occasions.

19

Figure 7: A trace of neural activity from 144 neurons in the brain a C. elegans as it
freely moves on an agarose plate over 8 minutes (strain AML32). The neural activity
is expressed as a fold change over baseline of the ratio of GCaMP6s to RFP for each
neuron. The behavior is indicated in the ethogram. On the right is the locations of
all of the detected neurons (the head of the worm is towards the top of the page).
The neurons that have signiﬁcant correlation with reverse locomotion are indicated
in red. White gaps indicate instances where neurons failed to segment.

Neural activity from tracked neurons

Fluorescent intensity is ultimately the measurement of interest and this can be

easily extracted from the tracks of the neuron locations across time. The pixels

within a 2 µm radius sphere around each point are used to calculate the average

ﬂuorescent intensity of a neuron in both the red RFP and green GCaMP6s

channels. We used the RFP as a reference ﬂuorophore and measure neural activity

as a fold change over baseline of the ratio of GcAMP6s to RFP intensity,

Activity =

∆R
R0

=

R − R0
R0

,

R =

IGCaM P 6s
IRF P

.

(3)

20

The baseline for each neuron, R0, is deﬁned as the 20th percentile value of the ratio

R for that neuron. Fig 7 shows calcium imaging traces extracted from new

whole-brain recordings using the registration vector pipeline. 144 neurons were

tracked for approximately 8 minutes as the worm moves. Many neurons show clear

correlation with reversal behaviors in the worm.

Discussion

The Neuron Registration Vector Encoding method presented here is able to process

longer recordings and locate more neurons with less human input compared to

previous examples of whole-brain imaging in freely moving C. elegans [10]. Fully

automated image processing means that we are no longer limited by the human

labor required for manual annotation. In new recordings presented here, we are

able to observe 144 neurons of the expected 181 neurons, much larger than the 80

observed in previous work from our lab and others [10, 11]. By automating

tracking and segmentation, this relieves one of the major bottlenecks to analyzing

longer recordings.

The neuron registration vector encoding algorithm primarily relies on the local

coherence of the motion of the neurons. It permits large deformations of the

worm’s centerline so long as deformations around the centerline remain modest.

Crucially, the algorithm’s time-independent approach allows it to tolerate large

motion between consecutive time-volumes. These properties make it well suited for

our neural recordings of C. elegans and we suspect that the our approach would be

applicable to tracking neurons in moving and deforming brains from other

organisms as well.

21

Certain classes of recordings, however, would not be well suited for Neuron

Registration Vector Encoding and Clustering. The approach will fail when the

local coherence of neuron motion breaks down. For example, if one neuron were to

completely swap locations with another neuron relative to its surroundings,

registration would not detect the switch and our method would fail. In this case a

time-dependent tracking approach may perform better.

In addition, proper clustering of the feature vectors requires the animal’s brain to

explore a contiguous region of deformation space. For example, if a hypothetical

brain were only ever to occupy two distinct conformations that are diﬀerent enough

that registration is not reliable between these two conformation states, the

algorithm would fail to cluster feature vectors from the same neuron across the two

states. To eﬀectively identify the neurons in these two conformations, the animal’s

brain must sample many conformations in between those two states. This way,

discrepancies in registration arise gradually and the resulting feature vectors

occupy a continuous region in the space of possible feature vectors. Note that a

similar requirement would necessarily apply to any time-dependent tracking

algorithm as well.

We suspect that brain recordings from most species of interest meet these two

requirements: namely neuron motion will have local coherence and the brain will

explore a contiguous region of deformation space. Where these conditions are

satisﬁed, we expect registration vector encoding to work well. Tracking in C.

elegans is especially challenging because the entire brain undergoes large

deformations as the animal bends. In most other organisms like zebraﬁsh and

Drosophila, brains are contained within a skull or exoskeleton and relative motion

of the neurons is small. In those organisms, ﬂuctuations in neuron positions take

22

the form of rigid global transformations as the animal moves, or local non-linear

deformations due to motion of blood vessels. We expect that this approach will be

applicable there as well.

Methods

Strains

twice.

Imaging C. elegans

Transgenic worms were cultivated on nematode growth medium (NGM) plates with

OP50 bacteria. Strain AML32 (wtfIs5[Prab-3 ::NLS::GCaMP6s;

Prab-3 ::NLS::tagRFP]) was generated by UV irradiating animals of strain AML14

(wtfEx4[Prab-3 ::NLS::GCaMP6s; Prab-3 ::NLS::tagRFP]) [10] and outcrossing

Imaging is performed as described in Nguyen et al [10]. The worm is placed

between an agarose slab and a large glass coverslip. The coverslip is held up by a

0.006” plastic shims in order to reduce the amount of pressure on the worm from

the glass, and mineral oil is spread over the worm to better match refractive indices

in the space between the coverglass and the worm. The dark ﬁeld image is used to

extract the animal’s centerline while the ﬂuorescent image is used for tracking the

worm’s brain. Only the head of the worm is illuminated by the ﬂuorescent

excitation light and can be observed in the low magniﬁcation ﬂuorescent image.

23

Thin Plate Spline deformations

Point-set registration was done as described by Jian and Vemuri [22], using TPS

deformations. Given a set of n initial control points X = {xi}, and the set of

transformed points, u[X], the transformation u can be written as

u[X] = WU(X) + AX + t. The aﬃne portion of the transformation is AX + t,

while WU(X) is the non-linear part of the transformation from TPS. U(X) is an

n × 1 vector with Ui(x) = U(x, xi) = U((cid:107)x − xi(cid:107)) = 1

(cid:107)x−xi(cid:107) and W is a 3 × n

matrix. The elements of W, A and t can be ﬁt given the set of control points X

and the location of the transformed points u(X). The energy of bending,

EBending(u), depends on how the control points are deformed. We use the same

energy as in [22], with EBending(u) = trace(WKWT) where Kij = U (xi, xj). Since

the integral in Eqn. 2 is easily computed analytically, the energy can be quickly

calculated and the parameters for W, A, and t for the minimization can be found

using gradient descent.

Algorithm implementation

The analysis steps shown in Fig 1 were performed on Princeton University’s

high-performance scientiﬁc computing cluster, “Della.” Jobs were run on up to 200

cores. Straightening, segmentation, and feature extraction are parallelized over

each volume, with each volume being processed on a single core. Error correction is

parallelized over each neuron. Centerline extraction in each image relies on the

previous centerline and must be computed linearly. The computation methods are

summarized in Table 1. An 8 minute recording of a moving animal has about 3000

volumes and 250 GB of raw imaging data and can be processes from start to ﬁnish

24

Approximate
Percentage of time
4

Analysis Step

Computation

Centerline Detection Linear
Worm Straightening
& Segmentation
Registration Vector
Encoding
Clustering
Error Correction

Parallel over volumes

10

Parallel over volumes

80

Linear
Parallel over neurons

2
4

Table 1: Breakdown of computation time for Neuron Registration Vector Encoding
pipeline.

in less than 40 hours. Data can be found via a “requester-pays” Amazon Web

Services S3 “bucket” at s3://leiferlabnguyentracking and code can be found at

https://github.com/leiferlab/NeRVEclustering. Centerline extraction code is

available at https://github.com/leiferlab/CenterlineTracking.

Acknowledgments

We thank the following for their help manually annotating recordings: F Shipley,

M Liu, S Setru, K Mizes, D Mazumder, J Chinchilla, and L Novak. This work was

supported by Simons Foundation Grant SCGB 324285 (to AML) and Princeton

University’s Inaugural Dean for Research Innovation Fund for New Ideas in the

Natural Sciences (to JWS and AML). JPN is supported by a grant from the Swartz

Foundation. ANL is supported by a National Institutes of Health institutional

training grant through the Princeton Neuroscience Institute. This work is also

supported by a grant from the Glenn Foundation for Medical Research. The

analysis presented in this article was performed on computational resources

supported by the Princeton Institute for Computational Science and Engineering

(PICSciE) and the Oﬃce of Information Technology’s High Performance

25

Computing Center and Visualization Laboratory at Princeton University.

Author contributions

Conceptualization: AML JWS JPN

Methodology: JPN ANL

Software: JPN ANL

Validation: JPN

Formal Analysis: JPN

Investigation: JPN ANL

Resources: JPN ANL GSP

Writing - Original Draft Preparation: JPN

Writing - Review & Editing: AML JWS

Supervision: AML

Funding Acquisition: AML

26

References

[1] Michel A Picardo, Josh Merel, Kalman A Katlowitz, Daniela Vallentin,

Daniel E Okobi, Sam E Benezra, Rachel C Clary, Eftychios A Pnevmatikakis,

Liam Paninski, and Michael A Long. Population-level representation of a

temporal sequence underlying song production in the zebra ﬁnch. Neuron,

90(4):866–876, 2016.

[2] John P Rickgauer, Karl Deisseroth, and David W Tank. Simultaneous

cellular-resolution optical perturbation and imaging of place cell ﬁring ﬁelds.

Nature neuroscience, 17(12):1816–24, 2014.

[3] EM Maynard, NG Hatsopoulos, CL Ojakangas, BD Acuna, JN Sanes,

RA Normann, and JP Donoghue. Neuronal interactions improve cortical

population coding of movement direction. The journal of Neuroscience,

19(18):8083–8093, 1999.

[4] Saul Kato, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H.

Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. Global Brain

Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans.

Cell, 163(3):656–669, 2015.

27

[5] Wenze Li, Venkatakaushik Voleti, Evan Schaﬀer, Rebecca Vaadia, Wesley B.

Grueber, Richard S. Mann, and Elizabeth M. Hillman. Scape microscopy for

high speed, 3d whole-brain imaging in drosophila melanogaster. In Biomedical

Optics 2016, page BTu4D.3. Optical Society of America, 2016.

[6] Robert Prevedel, Young-Gyu Yoon, Maximilian Hoﬀmann, Nikita Pak,

Gordon Wetzstein, Saul Kato, Tina Schrödel, Ramesh Raskar, Manuel

Zimmer, Edward S. Boyden, and Alipasha Vaziri. Simultaneous whole-animal

3D imaging of neuronal activity using light-ﬁeld microscopy. Nat Meth,

11(7):727–730, July 2014.

[7] Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pulver, Sabine L

Renninger, Amy Baohan, Eric R Schreiter, Rex a Kerr, Michael B Orger,

Vivek Jayaraman, Loren L Looger, Karel Svoboda, and Douglas S Kim.

SUPPLEMENTAL - Ultrasensitive ﬂuorescent proteins for imaging neuronal

activity. Nature, 499(7458):295–300, 2013.

[8] Christopher D Harvey, Forrest Collman, Daniel A Dombeck, and David W

Tank. Intracellular dynamics of hippocampal place cells during virtual

navigation. Nature, 461(7266):941–946, 2009.

[9] Misha B Ahrens, Jennifer M Li, Michael B Orger, Drew N Robson,

Alexander F Schier, Florian Engert, and Ruben Portugues. Brain-wide

neuronal dynamics during motor adaptation in zebraﬁsh. Nature,

485(7399):471–477, May 2012. PMID: 22622571.

[10] Jeﬀrey P. Nguyen, Frederick B. Shipley, Ashley N. Linder, George S.

Plummer, Mochi Liu, Sagar U. Setru, Joshua W. Shaevitz, and Andrew M.

Leifer. Whole-brain calcium imaging with cellular resolution in freely behaving

28

caenorhabditis elegans. Proceedings of the National Academy of Sciences,

113(8):E1074–E1081, 2016.

[11] Vivek Venkatachalam, Ni Ji, Xian Wang, Christopher Clark, James Kameron

Mitchell, Mason Klein, Christopher J Tabone, Jeremy Florman, Hongfei Ji,

Joel Greenwood, Andrew D Chisholm, Jagan Srinivasan, Mark Alkema, Mei

Zhen, and Aravinthan D T Samuel. Pan-neuronal imaging in roaming

Caenorhabditis elegans. 113(8):E1082–8, 2016.

[12] J Crocker and D Grier. Methods of Digital Video Microscopy for Colloidal

Studies. Journal of Colloid and Interface Science, 179(1):298–310, 1996.

[13] Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer. Automated

Analysis of Cellular Signals from Large-Scale Calcium Imaging Data. Neuron,

63(6):747–760, 2009.

[14] Greg J. Stephens, Bethany Johnson-Kerner, William Bialek, and William S.

Ryu. Dimensionality and dynamics in the behavior of C. elegans. PLoS

Computational Biology, 4(4), 2008.

[15] Hanchuan Peng, Fuhui Long, Xiao Liu, Stuart K. Kim, and Eugene W. Myers.

Straightening Caenorhabditis elegans images. Bioinformatics, 24(2):234–242,

2008.

[16] Onno D Broekmans, Jarlath B Rodgers, William S Ryu, and Greg J Stephens.

Resolving coiled shapes reveals new reorientation behaviors in C. elegans.

eLife, 5:e17227, sep 2016.

[17] Yi Deng, Philip Coen, Mingzhai Sun, and Joshua W Shaevitz. Eﬃcient

multiple object tracking using mutually repulsive active membranes. PloS one,

8(6):e65769, 2013.

29

[18] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active

contour models. International Journal of Computer Vision, 1(4):321–331,

1988.

2002.

[19] Ben Tordoﬀ and David W Murray. Guided sampling and consensus for motion

estimation. In European conference on computer vision, pages 82–96. Springer,

[20] Gang Lin, Umesh Adiga, Kathy Olson, John F Guzowski, Carol a Barnes, and

Badrinath Roysam. A hybrid 3D watershed algorithm incorporating gradient

cues and object models for automatic segmentation of nuclei in confocal image

stacks. Cytometry. Part A : the journal of the International Society for

Analytical Cytology, 56(1):23–36, 2003.

[21] Yu Toyoshima, Terumasa Tokunaga, Osamu Hirose, Manami Kanamori,

Takayuki Teramoto, Moon Sun Jang, Sayuri Kuge, Takeshi Ishihara, Ryo

Yoshida, and Yuichi Iino. Accurate Automatic Detection of Densely

Distributed Cell Nuclei in 3D Space. PLOS Computational Biology,

12(6):e1004970, 2016.

[22] Bing Jian and Baba C Vemuri. A Robust Algorithm for Point Set Registration

Using Mixture of Gaussians. Proceedings / IEEE International Conference on

Computer Vision. IEEE International Conference on Computer Vision,

2:1246–1251, oct 2005.

[23] John G White, Eileen Southgate, J Nichol Thomson, and Sydney Brenner.

The structure of the nervous system of the nematode caenorhabditis elegans.

Philos Trans R Soc Lond B Biol Sci, 314(1165):1–340, 1986.

30

6
1
0
2
 
t
c
O
 
4
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
9
7
5
4
0
.
0
1
6
1
:
v
i
X
r
a

Automatically tracking neurons in a moving and

deforming brain

Jeﬀrey P. Nguyen1,2, Ashley N. Linder3, George S. Plummer2, #, Joshua

W. Shaevitz1,2 and Andrew M. Leifer1,3,*

1Department of Physics, Princeton University, Princeton, New Jersey, United

States of America

2Lewis-Sigler Institute for Integrative Genomics, Princeton University,

Princeton, New Jersey, United States of America

#Current Address: Tufts University School of Medicine, Boston,

Massachusetts, United States of America

3Princeton Neuroscience Institute, Princeton University, Princeton, New

Jersey, United States of America

*Corresponding author, E-mail: leifer@princeton.edu (AML)

October 17, 2016

Abstract

Advances in optical neuroimaging techniques now allow neural activity to be

recorded with cellular resolution in awake and behaving animals. Brain motion in

these recordings pose a unique challenge. The location of individual neurons must

be tracked in 3D over time to accurately extract single neuron activity traces.

Recordings from small invertebrates like C. elegans are especially challenging

because they undergo very large brain motion and deformation during animal

movement. Here we present an automated computer vision pipeline to reliably

track populations of neurons with single neuron resolution in the brain of a freely

moving C. elegans undergoing large motion and deformation. 3D volumetric

ﬂuorescent images of the animal’s brain are straightened, aligned and registered,

and the locations of neurons in the images are found via segmentation. Each

neuron is then assigned an identity using a new time-independent machine-learning

approach we call Neuron Registration Vector Encoding. In this approach, non-rigid

point-set registration is used to match each segmented neuron in each volume with

a set of reference volumes taken from throughout the recording. The way each

neuron matches with the references deﬁnes a feature vector which is clustered to

assign an identity to each neuron in each volume. Finally, thin-plate spline

interpolation is used to correct errors in segmentation and check consistency of

assigned identities. The Neuron Registration Vector Encoding approach proposed

here is uniquely well suited for tracking neurons in brains undergoing large

deformations. When applied to whole-brain calcium imaging recordings in freely

moving C. elegans, this analysis pipeline located 150 neurons for the duration of an

8 minute recording and consistently found more neurons more quickly than manual

or semi-automated approaches.

2

Author Summary

Computer algorithms for identifying and tracking neurons in images of a brain have

struggled to keep pace with rapid advances in neuroimaging. In small transparent

organism like the nematode C. elgeans, it is now possible to record neural activity

from all of the neurons in the animal’s head with single-cell resolution as it crawls.

A critical challenge is to identify and track each individual neuron as the brain

moves and bends. Previous methods required large amounts of manual human

annotation. In this work, we present a fully automated algorithm for neuron

segmentation and tracking in freely behaving C. elegans. Our approach uses

non-rigid point-set registration to construct feature vectors describing the location

of each neuron relative to other neurons and other volumes in the recording. Then

we cluster feature vectors in a time-independent fashion to track neurons through

time. This new approach works very well when compared to a human.

Introduction

Optical neural imaging has ushered in a new frontier in neuroscience that seeks to

understand how neural activity generates animal behavior by recording from large

populations of neurons at cellular resolution in awake and behaving animals.

Population recordings have now been used to elucidate mechanisms behind zebra

ﬁnch song production [1], spatial encoding in mice [2], and limb movement in

primates [3]. When applied to small transparent organisms, like Caenorhabditis

elegans [4], Drosophila [5], and zebraﬁsh [6], nearly every neuron in the brain can

be recorded, permitting the study of whole brain neural dynamics at cellular

resolution.

1

Methods for segmenting and tracking neurons have struggled to keep up as new

imaging technologies now record from more neurons over longer times in

environments with greater motion. Accounting for brain motion in particular has

become a major challenge, especially in recordings of unrestrained animals. Brains

in motion undergo translations and deformations in 3D that make robust tracking

of individual neurons very diﬃcult. The problem is compounded in invertebrates

like C. elegans where the head of the animal is ﬂexible and deforms greatly. If left

unaccounted for, brain motion not only prevents tracking of neurons, but it can

also introduce artifacts that mask the true neural signal. In this work we propose

an automated approach to segment and track neurons in the presence of dramatic

brain motion and deformation. Our approach is optimized for calcium imaging in

unrestrained C. elegans.

Neural activity can be imaged optically with the use of genetically encoded calcium

sensitive ﬂuorescent indicators, such as GCaMP6s used in this work [7].

Historically calcium imaging was often conducted in head-ﬁxed or anesthetized

animals to avoid challenges involved with imaging moving samples [4, 8, 9].

Recently, however, whole-brain imaging was demonstrated in freely behaving C.

elegans [10, 11]. C. elegans are a small transparent nematode, approximately 1mm

in length, with a compact nervous system of only 302 neurons. About half of the

neurons are located in the animal’s head, which we refer to as its brain.

Analyzing ﬂuorescent images of moving and deforming brains requires algorithms

to detect neurons across time and extract ﬂuorescent signals in 3D. Several

strategies exist for tracking neurons in volumetric recordings. One approach is to

ﬁnd correspondences between neuron positions in consecutive time points, for

example, by applying a distance minimization, and then stitching these

2

correspondences together through time [12]. This type of time-dependent tracking

requires that neuron displacements for each time step are less than the distance

between neighboring neurons, and that the neurons remain identiﬁable at all times.

If these requirements break down, even for only a few time points, errors can

quickly accumulate. Other common methods, like independent component analysis

(ICA) [13] are also exquisitely sensitive to motion and as a result they have not

been successfully applied to recordings with large brain deformations.

Large inter-volume motion arises when the recorded image volume acquisition rate

is too low compared to animal motion. Unfortunately, large inter-volume brain

motion is likely to be a prominent feature of whole-brain recordings of moving

brains for the foreseeable future. In all modern imaging approaches there is a

fundamental tradeoﬀ between the following attributes: acquisition rate (temporal

resolution), spatial resolution, signal to noise, and the spatial extent of the

recording. As recordings seek to capture larger brain regions at single cell

resolution, they necessarily compromise on temporal resolution. For example,

whole brain imaging in freely moving C. elegans has only been demonstrated at

slow acquisition rates because of the requirements to scan the entire brain volume

and expose each slice for suﬃciently long time. At these rates, a signiﬁcant amount

of motion is present between image planes within a single brain volume. Similarly,

large brain motions also remain between sequential volumes. Neurons can move the

entire width of the worm’s head between sequential volumes when recording at 6

brain-volumes per second, as in [10]. In addition to motion, the brain also bends

and deform as it moves. Such changes to the brain’s conformation greatly alter the

pattern of neuron positions making constellations of neurons diﬃcult to compare

across time.

3

To account for this motion, previous work that measured neural activity in freely

moving C. elegans required either large amounts of manual annotation as reference

data for comparison [11] or required a human user to supervise and correct

semi-automated algorithms for each and every neuron-time point [10]. This level of

manual annotation becomes impractical as the length of recordings and the number

of neurons increases. For example, 10 minutes of recorded neural activity from [10],

had over 360,000 neuron time points and required over 200 person-hours of manual

annotation. Here, we introduce a new time-independent algorithm that uses

machine learning to automatically segment and track all neurons in the head of a

freely moving animal. We call this technique Neuron Registration Vector Encoding,

and we use it to extract neural signals in unrestrained C. elegans expressing the

calcium indicator GCaMP6s and the ﬂuorescent label RFP.

Results

Overview of neuron tracking analysis

We introduce a method to track over 100 neurons in the brain of a freely moving

C. elegans. The analysis pipeline is made of ﬁve modules and an overview is shown

in Figure 1. The ﬁrst three modules, “Centerline Detection,” “Straightening” and

“Segmentation,” collectively assemble the individually recorded planes into a

sequence of 3D volumes and identify each neuron’s location in each volume. The

next two modules, “Registration Vector Construction” and “Clustering,” form the

core of the method and represent a signiﬁcant advance over previous approaches.

Collectively, these two modules are called “Neuron Registration Vector Encoding".

The “Registration Vector Construction” module leverages information from across

4

Figure 1:
Schematic of analysis pipeline to segment and track neurons through
time and extract their neural activity in a deforming brain. Neurons are labeled
with calcium insensitive red ﬂuorescent proteins, RFP, and calcium sensitive green
ﬂuorescent proteins, GCaMP. Videos of the animal’s behavior and volumetric ﬂuo-
rescent images of the animal’s brain serve as input to the pipeline. The algorithm
detects all neurons in the head and produces tracks of the neural activity across time
as the animal moves.

5

the entire recording in a time-independent way to generate feature vectors that

characterize every neuron at every time point in relation to a repertoire of brain

conﬁrmations. The “Clustering” module then clusters these feature vectors to

assign a consistent identity to each neuron across the entire recording. A ﬁnal

module corrects for errors the original segmentation. The implementation and

results of this approach are described below.

Recording of whole-brain calcium activity and body posture in

moving animal

Worms expressing the calcium indicator GCaMP6s and a calcium-insensitive

ﬂuorescent protein RFP in the nuclei of all neurons were imaged during

unrestrained behavior in a custom 3D tracking microscope, as described in [10].

Two recordings are presented in this work: a new 8 minute recording of an animal

of of strain AML32 and a previously reported 4 minute recording of strain ﬁrst

described in [10].

The signal of interest in both recordings is the green ﬂuorescence intensity from

GCaMP6s in each neuron. Red ﬂuorescence from the RFP protein serves as a

reference for locating and tracking the neurons. The microscope provides four raw

image streams that serve as inputs for our neural tracking pipeline, seen in Fig 2A.

They are: (1) low-magniﬁcation dark-ﬁeld images of the animal’s body posture (2)

low-magniﬁcation ﬂuorescent images of the animal’s brain (3) high-magniﬁcation

green ﬂuorescent images of single optical slices of the brain showing GCaMP6s

activity and (4) high-magniﬁcation red ﬂuorescent images of single optical slices of

the brain showing the location of RFP. The animal’s brain is kept centered in the

ﬁeld of view by realtime feedback loops that adjust a motorized stage to

6

Figure 2: (A) Example images from all four video feeds from our imaging system.
Both scale bars are 100µm (B)A schematic illustrating the timings from all the
devices that run in open loop in our imaging setup. The camera that collects high
magniﬁcation images captures at 200Hz. The two low magniﬁcation images capture
at 60Hz, and the focal plane moves up and down in a 3 Hz triangle wave. The
cameras are synchronized post-hoc using a camera ﬂash and each image is assigned
a timestamp on a common timeline for the purposes of analysis.

compensate for the animal’s crawling. To acquire volumetric information, the high

magniﬁcation imaging plane scans back and forth along the axial dimension, z, at 3

Hz as shown in Fig 2B, acquiring roughly 33 optical slices per volume, sequentially,

for 6 brain-volumes per second. The animal’s continuous motion causes each

volume to be arbitrarily sheared in z. Although the image streams operate at

diﬀerent volume acquisition rates and on diﬀerent clocks, they are later

synchronized by simultaneous light ﬂashes and given a timestamp on a common

timeline. Each of the four imaging streams are aligned to each other in software

using aﬃne transformations found by imaging ﬂuorescent beads.

7

Centerline detection and gross brain alignment

The animal’s posture contains information about the brain’s orientation and about

any deformations arising from the animal’s side-to-side head swings. The ﬁrst step

of the pipeline is to extract the centerline that describes the animal’s posture.

Centerline detection in C. elegans is an active ﬁeld of research. Most algorithms

use intensity thresholds to detect the worm’s body and then use binary image

operations to extract a centerline [14, 15, 16]. Here we use an open active contour

approach [17, 18] to extract the centerline from dark ﬁeld images with

modiﬁcations to account for cases when the worm’s body crosses over itself as

occurs during so-called “Omega Turns.” In principle any method, automated or

otherwise, that detects the centerlines should be suﬃcient. At rare times where the

worm is coiled and the head position and orientation cannot be determined

automatically, the head and the tail of the worm are manually identiﬁed.

The animal’s centerline allows us to correct for gross changes in the worm’s

position, orientation, and conformation (Fig 3a). We use the centerlines

determined by the low magniﬁcation behavior images to straighten the high

magniﬁcation images of the worm’s brain. An aﬃne transform must be applied to

the centerline coordinates to transform them from the dark ﬁeld coordinate system

into the coordinate system of the high magniﬁcation images. Each image slice of

the worm brain is straightened independently to account for motion within a single

volume. The behavior images are taken at a lower acquisition rate than the high

magniﬁcation brain images, so a linear interpolation is to used obtain a centerline

for each slice of the brain volume. In each slice, we ﬁnd the tangent and normal

vectors at every point of the centerline (Fig 3b). The points are interpolated with a

single pixel spacing along the centerline to preserve the resolution of the image.

8

(A) Centerlines are detected from the low magniﬁcation dark ﬁeld images.
Figure 3:
The centerline is shown in green and the tip of the worm’s head is indicated by a
blue dot. (B) The centerline found from the low magniﬁcation image is overlaid on
the high magniﬁcation RFP images. The lines normal to the centerline, shown in
blue, are used to straighten the image. All scale bars are 100 µm.(C) A maximum
intensity projection of the straightened volume is shown. Individual neuronal nuclei
are shown (D) before and (E) after segmentation.

The image intensities along each of the normal directions are interpolated and the

slices are stacked to produce a straightened image in each slice (Fig 3c). In the new

coordinate system, the orientation of the animal is ﬁxed and the worm’s bending is

greatly suppressed. We further reduce shearing between slices using standard video

stabilization techniques [19]. Speciﬁcally, bright-intensity peaks in the image are

tracked between neighboring image slices and aﬃne transformations are calculated

between each slice of the volume. All slices are registered to the middle slice by

applying these transformations sequentially throughout the volume. Each slice

would undergo transformations for every slice in between it and the middle slice to

correct shear throughout the volume. A ﬁnal rigid translation is required to align

each volume to the initial volume. The translations are found by ﬁnding an oﬀset

that maximizes the cross-correlation between each volume and the initial volume.

9

Segmentation

Before neuron identities can be matched across time, we must ﬁrst segment the

individual neurons within a volume to recovers each neuron’s size, location, and

brightness (Fig 3d,e). Many algorithms have been developed to segment neurons in

a dense region [20, 21]. We segment the neurons by ﬁnding volumes of curvature in

ﬂuorescence intensity. We compute the 3D Hessian matrix at each point in space

and threshold for points where all of the three eigenvalues of the Hessian matrix

are negative. In order to further divide regions into objects that are more likely to

represent neurons, we use a watershed separation on the distance transform of the

thresholded image. The distance transform is found by replacing each thresholded

pixel with the Euclidean distance between it and the closest zero pixel in the

thresholded image. Image blurring from animal motion poses a challenge for

segmentation. We allow for some noise and error in the segmentation because we

will have the opportunity to automatically correct many of these errors later in the

pipeline.

Neuron registration vector construction

Extracting neural signals requires the ability to match neurons found at diﬀerent

time points. Even after gross alignment and straightening, neurons in our images

are still subject to local nonlinear deformations and there is signiﬁcant movement

of neurons between volumes. Rather than tracking through time, the neurons in

each volume are characterized based on how they match to neurons in a set of

reference volumes. Our algorithm compares constellations of neurons in one volume

to unannotated reference volumes and assigns correspondences or “matches”

10

between the neurons in the sample and each reference volume. We modiﬁed a point

set registration algorithm developed by Jian and Vemuri [22] to do this (Fig 4a).

The registration algorithm represents two point sets, a sample point-set denoted by

X = {xi} and a reference point-set indicated by R = {ri}, as Gaussian mixtures

and then attempts to register them by deforming space to minimize the distance

between the two mixtures. Here, each neuron is modeled by a 3D Gaussian with

uniform covariance. Since we are matching images of neurons rather than just

points, we can use the additional information from the size and brightness of each

neuron. We add this information to the representation of each neuron by adjusting

the amplitude and standard deviation of the Gaussians. The Gaussian mixture

representation of an image is given by,

f (ξ, X) =

Ai exp

−

(cid:88)

i

(cid:18)

(cid:107)ξ − xi(cid:107)2
2(λσi)2

(cid:19)
,

(1)

where Ai, xi, and σi are the amplitude, mean, and standard deviation of the i-th

Gaussian. These parameters are derived from the brightness, centroid, and size of

the segmented neuron, while ξ is the 3D spatial coordinate. A scale factor λ is

added to the standard deviation to scale the size of each Gaussian. This will be

used later during gradient descent. The sample constellation of neurons is then

represented by the Gaussian mixture f (ξ, X). Similarly, the reference

constellation’s own neurons is represented as a f (ξ, R).

To match a sample constellation of neurons X with a reference constellation of

neurons R, we use the non rigid transformation u : IR3 (cid:55)→ IR3. The transformation

u maps X to u[X] such that the L2 distance between f (ξ, u[X]) and f (ξ, R) is

minimized with some constraint on the amount of deformation. This can be

written as an energy minimization problem, with the energy of the transformation,

11

E(u), written as

(cid:90)

E(u) =

(cid:2)f (ξ, u[X]) − f (ξ, R)(cid:3)2dξ + EDeformation(u).

(2)

Note that the point-sets X and R are allowed to have diﬀerent numbers of points.

We model the deformations as a thin-plate spline (TPS). The transformation

equations and resulting form of Edeformation(u) is shown in the methods. The

minimization of E is found by gradient descent. Since the energy landscape has

many local minima, we initially chose a large scale factor, λ, to increase the size of

each Gaussian and smooth over smaller features. During gradient descent, λ is

decreased to better represent the original image. After the transformation, sample

points are matched to reference points by minimizing distances between assigned

pairs [12]. The matching is not greedy, and neurons in the sample that are far from

any neurons in the reference are not matched. A neuron at xi is assigned a match

vi to indicate which neuron in the set R it was matched to. For example if xi

matched with rj when X is registered to R, then vi = j. If xi has no match in R,

then vi = ∅.

The modiﬁed non-rigid point set registration algorithm described above allows us

to compare one constellation of neurons to another. In principle, neuron tracking

could be achieved by registering the constellation of neurons at each time-volume

to a single common reference. That approach is susceptible to failures in non-rigid

point set registration. Non-rigid point-set registration works well when the

conformation of the animal in the sample and the reference are similar, but it is

unreliable when there are large deformations between the sample and the reference,

as happens with some regularity in our recordings. In addition, this approach is

especially sensitive to any errors in segmentation, especially in the reference. An

12

Figure 4:
Schematic illustration of neuron registration vector encoding. (A) The
registration between a sample volume and a single reference volume is done in several
steps. I. The image is segmented into regions corresponding to each of the neurons.
II. The image is represented as a Gaussian mixture, with a single Gaussian for each
segmented region. The amplitude and the standard deviation of the Gaussians are
derived from the brightness and the size of the segmented regions. III. Non-rigid
point-set registration is then used to deform the sample points to best overlap the
reference point-set. IV. Neurons from the sample and the reference point-sets are
paired by minimizing distances between neurons. (B) Neuron registration vectors
are constructed by assigning a feature vector vi,t to each neuron xi,t in a sample
volume xt by performing the registration between the sample volume and a set of
300 reference volumes, each denoted by rk. Each registration of the neuron results in
a neuron match, vk
i , and the set of matches becomes the feature vector vi,t. (C) The
vectors from all neuron-times, vi,t, are hierarchically clustered. The same neuron
found at diﬀerent times will have a similar set of features and therefore will contain
the same neuron found at diﬀerent times. Real matches occur in a high dimensional
space. Only two dimensions are illustrated here for clarity. Each of the feature
vector is assigned a cluster, and the cluster labels are given by S. (D) The clustering
of the feature vectors shown in (C) assigns an identity to each of the neurons in
every volume. This allows us to track the neurons across diﬀerent volumes of the
recording.

13

alternative approach would be to sequentially register neurons in each time volume

to the next time-volume. This approach, however, accumulates even small errors

and quickly becomes unreliable. Instead of either of those approaches, we use

registration to compare the constellation of neurons at each time volume to a set of

reference time-volumes that span a representative space of brain conformations

(Fig 4b), as described below.

The constellation of neurons at a particular time in our recording is given by Xt,

and the position of the i-th neuron at time t is denoted by xi,t. We select a set of

K reference constellations, each from a diﬀerent time volume Xt in our recording,

so as to achieve a representative sampling of the many diﬀerent possible brain

conformations the animal can attain. These K reference volumes are denoted by

{R1, R2, R3, ..., RK}. For simplicity, we use 300 volumes spaced evenly through

time as our reference constellations. Each Xt is separately matched with each of

the references, and each neuron in the sample, xi,t, gets a set of matches

vi,t = {v1

i,t, v2

i,t, v3

i,t, ..vK

i,t}, one match for each of the K references. This set of

matches is a feature vector which we call a Neuron Registration Vector. It

describes the neuron’s location in relation to its neighbors when compared with the

set of references. This vector can be used to identify neurons across diﬀerent times.

Clustering registration vectors

The neuron registration vector provides information about that neuron’s position

relative to its neighbors, and how that relative position compares with many other

reference volumes. A neuron with a particular identity will match similarly to the

set of reference volumes and thus that neuron will have similar neuron registration

vectors over time. Clustering similar registration vectors allows for the

14

identiﬁcation of that particular neuron across time (Fig 4c,d).

To illustrate the motivation for clustering, consider a neuron with identity s that is

found at diﬀerent times in two sample constellations X 1 and X 2. When X 1 and

X 2 have similar deformations, the neuron s from both constellations will be

assigned the same set of matches when registered to the set of reference

constellations, and as a result the corresponding neuron registration vectors v1 and

v2 will be identical. This is true even if the registration algorithm itself fails to

correctly match neuron s in the sample to its true neuron s in the reference. As the

deformations separating X 1 and X 2 become larger, the distance between the

feature vectors v1 and v2 also becomes larger. This is because the two samples will

be matched to diﬀerent neurons in some of the reference volumes as each sample is

more likely to register poorly with references that are far from it in the space of

deformations.

Crucially, the reference volumes consist of instances of the animal in many diﬀerent

deformation states. So while errors in registering some samples will exist for

certain reference, they do not persist across all references, and thus do not eﬀect

the entire feature vector. For the biologically relevant deformations that we

observe, the distance between v1 and v2 will be smaller if both are derived from

neuron s than compared to the distance between v1 and v2 if they were derived

from s and another neuron. We can therefore cluster the feature vectors to produce

groups that consist of the same neuron found at many diﬀerent time points.

The list of neuron registration vectors from all neuron at all times, {vi,t}, is

hierarchically clustered. Each match in the vectors, vk

i,t, is represented as a binary

vector of 0s with a 1 at the vk−th

i

position. The size of the vector is equal to the

number of neurons in Rk. The feature vector {vi,t} is the concatenation of all of

15

Figure 5: Example of consensus voting to correct a misidentiﬁed neuron. In volume
735, neuron #111 is found successfully and is indicated in green. In volume 736,
however, the neuron is misidentiﬁed, shown in red. During the correction phase, all
other time points vote for what the position of neuron #111 should be assuming a
thin-plate spline deformation, indicated by the cloud of blue points. Since the initial
estimate of the position is far from the cloud of blue points, a corrected position is
selected as the centroid of the votes weighted by image intensity. This process is
repeated to correct errors for every neuron at every time.

the binary vectors from all matches to the K reference constellations. The

correlation distance, 1 − corr(vm, vn), was used as the pair wise distance metric for

clustering. Clusters were checked to ensure that at most one neuron was

represented from each time. Clusters containing neurons from less than 40% of the

volumes were removed. Each cluster is assigned a label {S1, S2, S3, ...} which

uniquely identiﬁes a single neuron over time, and each neuron at each time xi,t is

given an identiﬁer si,t which corresponds to which cluster that neuron-time belongs

to. Neurons that are not assigned to one of these clusters are removed because they

are likely artifactual or represent a neuron that is segmented too poorly for

inclusion.

Correcting errors in tracking and segmentation

Neuron Registration Vector Encoding successfully identiﬁes segmented neurons

consistently across time. A transient segmentation error, however, would

16

necessarily lead to missing or misidentiﬁed neurons. To identify and correct for

missing and misidentiﬁed neurons, we check each neuron’s locations and ﬁll in

missing neurons using a consensus comparison and interpolation in a thin-plate

spline (TPS) deformed space. For each neuron identiﬁer s and time t(cid:63), we use all

other point-sets, {Xt} to guess what that neuron’s location might be. This is done

by ﬁnding the TPS transformation, ut→t(cid:63) : Xt (cid:55)→ Xt(cid:63), that maps the identiﬁed

points from Xt to the corresponding points in Xt(cid:63) excluding the point s. The

position estimate is then given by ut→t(cid:63)[xi,t] with i selected such that si,t = s. This

results in a set of points representing the predicted location of the neuron at time

t(cid:63) from all other times. When a neuron identiﬁer is missing for a given time, the

position of that neuron s is inferred by consensus. Namely, correct location is

deemed to be the centroid of the set of inferred locations weighted by the

underlying image intensity. This weighted centroid is also used if the current

identiﬁed location of the neuron s has a distance greater than 3 standard

deviations away from the centroid of the set of points, implying that an error may

have occurred in the assignment. This is shown in Fig 5, where neuron 111 is

identiﬁed in volume 735, but the the label for neuron 111 is incorrectly located in

volume 736. In that case the weighted centroid from consensus voting was used.

Comparison with manually annotated data

To asses the accuracy of the Neuron Registration Vector Encoding pipeline, we

applied our automated tracking system to a 4 minute recording of whole brain

activity in a moving C. elegans that had previously been hand annotated [10]. A

custom Matlab GUI was used for manually identifying and tracking neurons. Nine

researchers collectively annotated 70 neurons from each of the 1519 volumes in the

17

Figure 6:
Comparison of the automated Neuron Registration Vector Encoding
algorithm with manual human annotation over a 4 minute recording on brain activity
(strain AML14). (A) Spheres show position of neurons that were detected by the
automated algorithm. Grey indicates a neuron detected by both the algorithm and
the human (70 neurons). Red indicates neurons that were missed by the human
and detected only by the algorithm (49 neurons). (B) Histogram showing number of
neurons that were mismatched for a given fraction of time-volumes when comparing
automated and manual approaches. Only those neurons that were consistently found
by both algorithm and human were considered. An automatically identiﬁed neuron
was deemed correctly matched for a given time-volume if it was paired with the
correct corresponding manual neuron.

18

4 minute video. This is much less than the 181 neurons predicted to be found in

the head [23]. The discrepancy is likely caused by a combination of imaging

conditions and human nature. The short exposure time of our recordings makes it

hard to resolve dim neurons, and the relatively long recordings tend to cause

photobleaching which make the neurons even dimmer. Additionally, human

researchers naturally tend to select only those neurons that are brightest and are

most unambiguous for annotation, and tend to skip dim neurons or those neurons

that are most densely clustered.

We compared human annotations to our automated analysis in this same dataset.

We performed the entire pipeline including detecting centerlines, performing worm

straightening, segmentation, and neuron registration vector encoding and

clustering, and correction. Automated tracking detected 119 neurons from the

video compared to 70 from the human. In each volume, we paired the

automatically tracked neurons with those found by manual detection by ﬁnding the

closest matches in the unstraightened coordinate system. A neuron was perfectly

tracked if it matched with the same manual neuron at all times. Tracking errors

were labelled when a neuron matched with a manual neuron that was diﬀerent

than the one it matched with most often. The locations of the detected neurons are

shown in Fig 6A. Only one neuron was incorrectly identiﬁed in more than 5% of

the time volumes (Fig 6B). The locations of neurons and the corresponding error

rates is shown in Fig 6B. Neurons that were detected by the algorithm but not

annotated manually are shown in gray. Upon further inspection, it was noted that

some of the mismatches between our method and the manual annotation were due

to human errors in the manual annotation, meaning the algorithm is able to correct

humans on some occasions.

19

Figure 7: A trace of neural activity from 144 neurons in the brain a C. elegans as it
freely moves on an agarose plate over 8 minutes (strain AML32). The neural activity
is expressed as a fold change over baseline of the ratio of GCaMP6s to RFP for each
neuron. The behavior is indicated in the ethogram. On the right is the locations of
all of the detected neurons (the head of the worm is towards the top of the page).
The neurons that have signiﬁcant correlation with reverse locomotion are indicated
in red. White gaps indicate instances where neurons failed to segment.

Neural activity from tracked neurons

Fluorescent intensity is ultimately the measurement of interest and this can be

easily extracted from the tracks of the neuron locations across time. The pixels

within a 2 µm radius sphere around each point are used to calculate the average

ﬂuorescent intensity of a neuron in both the red RFP and green GCaMP6s

channels. We used the RFP as a reference ﬂuorophore and measure neural activity

as a fold change over baseline of the ratio of GcAMP6s to RFP intensity,

Activity =

∆R
R0

=

R − R0
R0

,

R =

IGCaM P 6s
IRF P

.

(3)

20

The baseline for each neuron, R0, is deﬁned as the 20th percentile value of the ratio

R for that neuron. Fig 7 shows calcium imaging traces extracted from new

whole-brain recordings using the registration vector pipeline. 144 neurons were

tracked for approximately 8 minutes as the worm moves. Many neurons show clear

correlation with reversal behaviors in the worm.

Discussion

The Neuron Registration Vector Encoding method presented here is able to process

longer recordings and locate more neurons with less human input compared to

previous examples of whole-brain imaging in freely moving C. elegans [10]. Fully

automated image processing means that we are no longer limited by the human

labor required for manual annotation. In new recordings presented here, we are

able to observe 144 neurons of the expected 181 neurons, much larger than the 80

observed in previous work from our lab and others [10, 11]. By automating

tracking and segmentation, this relieves one of the major bottlenecks to analyzing

longer recordings.

The neuron registration vector encoding algorithm primarily relies on the local

coherence of the motion of the neurons. It permits large deformations of the

worm’s centerline so long as deformations around the centerline remain modest.

Crucially, the algorithm’s time-independent approach allows it to tolerate large

motion between consecutive time-volumes. These properties make it well suited for

our neural recordings of C. elegans and we suspect that the our approach would be

applicable to tracking neurons in moving and deforming brains from other

organisms as well.

21

Certain classes of recordings, however, would not be well suited for Neuron

Registration Vector Encoding and Clustering. The approach will fail when the

local coherence of neuron motion breaks down. For example, if one neuron were to

completely swap locations with another neuron relative to its surroundings,

registration would not detect the switch and our method would fail. In this case a

time-dependent tracking approach may perform better.

In addition, proper clustering of the feature vectors requires the animal’s brain to

explore a contiguous region of deformation space. For example, if a hypothetical

brain were only ever to occupy two distinct conformations that are diﬀerent enough

that registration is not reliable between these two conformation states, the

algorithm would fail to cluster feature vectors from the same neuron across the two

states. To eﬀectively identify the neurons in these two conformations, the animal’s

brain must sample many conformations in between those two states. This way,

discrepancies in registration arise gradually and the resulting feature vectors

occupy a continuous region in the space of possible feature vectors. Note that a

similar requirement would necessarily apply to any time-dependent tracking

algorithm as well.

We suspect that brain recordings from most species of interest meet these two

requirements: namely neuron motion will have local coherence and the brain will

explore a contiguous region of deformation space. Where these conditions are

satisﬁed, we expect registration vector encoding to work well. Tracking in C.

elegans is especially challenging because the entire brain undergoes large

deformations as the animal bends. In most other organisms like zebraﬁsh and

Drosophila, brains are contained within a skull or exoskeleton and relative motion

of the neurons is small. In those organisms, ﬂuctuations in neuron positions take

22

the form of rigid global transformations as the animal moves, or local non-linear

deformations due to motion of blood vessels. We expect that this approach will be

applicable there as well.

Methods

Strains

twice.

Imaging C. elegans

Transgenic worms were cultivated on nematode growth medium (NGM) plates with

OP50 bacteria. Strain AML32 (wtfIs5[Prab-3 ::NLS::GCaMP6s;

Prab-3 ::NLS::tagRFP]) was generated by UV irradiating animals of strain AML14

(wtfEx4[Prab-3 ::NLS::GCaMP6s; Prab-3 ::NLS::tagRFP]) [10] and outcrossing

Imaging is performed as described in Nguyen et al [10]. The worm is placed

between an agarose slab and a large glass coverslip. The coverslip is held up by a

0.006” plastic shims in order to reduce the amount of pressure on the worm from

the glass, and mineral oil is spread over the worm to better match refractive indices

in the space between the coverglass and the worm. The dark ﬁeld image is used to

extract the animal’s centerline while the ﬂuorescent image is used for tracking the

worm’s brain. Only the head of the worm is illuminated by the ﬂuorescent

excitation light and can be observed in the low magniﬁcation ﬂuorescent image.

23

Thin Plate Spline deformations

Point-set registration was done as described by Jian and Vemuri [22], using TPS

deformations. Given a set of n initial control points X = {xi}, and the set of

transformed points, u[X], the transformation u can be written as

u[X] = WU(X) + AX + t. The aﬃne portion of the transformation is AX + t,

while WU(X) is the non-linear part of the transformation from TPS. U(X) is an

n × 1 vector with Ui(x) = U(x, xi) = U((cid:107)x − xi(cid:107)) = 1

(cid:107)x−xi(cid:107) and W is a 3 × n

matrix. The elements of W, A and t can be ﬁt given the set of control points X

and the location of the transformed points u(X). The energy of bending,

EBending(u), depends on how the control points are deformed. We use the same

energy as in [22], with EBending(u) = trace(WKWT) where Kij = U (xi, xj). Since

the integral in Eqn. 2 is easily computed analytically, the energy can be quickly

calculated and the parameters for W, A, and t for the minimization can be found

using gradient descent.

Algorithm implementation

The analysis steps shown in Fig 1 were performed on Princeton University’s

high-performance scientiﬁc computing cluster, “Della.” Jobs were run on up to 200

cores. Straightening, segmentation, and feature extraction are parallelized over

each volume, with each volume being processed on a single core. Error correction is

parallelized over each neuron. Centerline extraction in each image relies on the

previous centerline and must be computed linearly. The computation methods are

summarized in Table 1. An 8 minute recording of a moving animal has about 3000

volumes and 250 GB of raw imaging data and can be processes from start to ﬁnish

24

Approximate
Percentage of time
4

Analysis Step

Computation

Centerline Detection Linear
Worm Straightening
& Segmentation
Registration Vector
Encoding
Clustering
Error Correction

Parallel over volumes

10

Parallel over volumes

80

Linear
Parallel over neurons

2
4

Table 1: Breakdown of computation time for Neuron Registration Vector Encoding
pipeline.

in less than 40 hours. Data can be found via a “requester-pays” Amazon Web

Services S3 “bucket” at s3://leiferlabnguyentracking and code can be found at

https://github.com/leiferlab/NeRVEclustering. Centerline extraction code is

available at https://github.com/leiferlab/CenterlineTracking.

Acknowledgments

We thank the following for their help manually annotating recordings: F Shipley,

M Liu, S Setru, K Mizes, D Mazumder, J Chinchilla, and L Novak. This work was

supported by Simons Foundation Grant SCGB 324285 (to AML) and Princeton

University’s Inaugural Dean for Research Innovation Fund for New Ideas in the

Natural Sciences (to JWS and AML). JPN is supported by a grant from the Swartz

Foundation. ANL is supported by a National Institutes of Health institutional

training grant through the Princeton Neuroscience Institute. This work is also

supported by a grant from the Glenn Foundation for Medical Research. The

analysis presented in this article was performed on computational resources

supported by the Princeton Institute for Computational Science and Engineering

(PICSciE) and the Oﬃce of Information Technology’s High Performance

25

Computing Center and Visualization Laboratory at Princeton University.

Author contributions

Conceptualization: AML JWS JPN

Methodology: JPN ANL

Software: JPN ANL

Validation: JPN

Formal Analysis: JPN

Investigation: JPN ANL

Resources: JPN ANL GSP

Writing - Original Draft Preparation: JPN

Writing - Review & Editing: AML JWS

Supervision: AML

Funding Acquisition: AML

26

References

[1] Michel A Picardo, Josh Merel, Kalman A Katlowitz, Daniela Vallentin,

Daniel E Okobi, Sam E Benezra, Rachel C Clary, Eftychios A Pnevmatikakis,

Liam Paninski, and Michael A Long. Population-level representation of a

temporal sequence underlying song production in the zebra ﬁnch. Neuron,

90(4):866–876, 2016.

[2] John P Rickgauer, Karl Deisseroth, and David W Tank. Simultaneous

cellular-resolution optical perturbation and imaging of place cell ﬁring ﬁelds.

Nature neuroscience, 17(12):1816–24, 2014.

[3] EM Maynard, NG Hatsopoulos, CL Ojakangas, BD Acuna, JN Sanes,

RA Normann, and JP Donoghue. Neuronal interactions improve cortical

population coding of movement direction. The journal of Neuroscience,

19(18):8083–8093, 1999.

[4] Saul Kato, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H.

Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. Global Brain

Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans.

Cell, 163(3):656–669, 2015.

27

[5] Wenze Li, Venkatakaushik Voleti, Evan Schaﬀer, Rebecca Vaadia, Wesley B.

Grueber, Richard S. Mann, and Elizabeth M. Hillman. Scape microscopy for

high speed, 3d whole-brain imaging in drosophila melanogaster. In Biomedical

Optics 2016, page BTu4D.3. Optical Society of America, 2016.

[6] Robert Prevedel, Young-Gyu Yoon, Maximilian Hoﬀmann, Nikita Pak,

Gordon Wetzstein, Saul Kato, Tina Schrödel, Ramesh Raskar, Manuel

Zimmer, Edward S. Boyden, and Alipasha Vaziri. Simultaneous whole-animal

3D imaging of neuronal activity using light-ﬁeld microscopy. Nat Meth,

11(7):727–730, July 2014.

[7] Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pulver, Sabine L

Renninger, Amy Baohan, Eric R Schreiter, Rex a Kerr, Michael B Orger,

Vivek Jayaraman, Loren L Looger, Karel Svoboda, and Douglas S Kim.

SUPPLEMENTAL - Ultrasensitive ﬂuorescent proteins for imaging neuronal

activity. Nature, 499(7458):295–300, 2013.

[8] Christopher D Harvey, Forrest Collman, Daniel A Dombeck, and David W

Tank. Intracellular dynamics of hippocampal place cells during virtual

navigation. Nature, 461(7266):941–946, 2009.

[9] Misha B Ahrens, Jennifer M Li, Michael B Orger, Drew N Robson,

Alexander F Schier, Florian Engert, and Ruben Portugues. Brain-wide

neuronal dynamics during motor adaptation in zebraﬁsh. Nature,

485(7399):471–477, May 2012. PMID: 22622571.

[10] Jeﬀrey P. Nguyen, Frederick B. Shipley, Ashley N. Linder, George S.

Plummer, Mochi Liu, Sagar U. Setru, Joshua W. Shaevitz, and Andrew M.

Leifer. Whole-brain calcium imaging with cellular resolution in freely behaving

28

caenorhabditis elegans. Proceedings of the National Academy of Sciences,

113(8):E1074–E1081, 2016.

[11] Vivek Venkatachalam, Ni Ji, Xian Wang, Christopher Clark, James Kameron

Mitchell, Mason Klein, Christopher J Tabone, Jeremy Florman, Hongfei Ji,

Joel Greenwood, Andrew D Chisholm, Jagan Srinivasan, Mark Alkema, Mei

Zhen, and Aravinthan D T Samuel. Pan-neuronal imaging in roaming

Caenorhabditis elegans. 113(8):E1082–8, 2016.

[12] J Crocker and D Grier. Methods of Digital Video Microscopy for Colloidal

Studies. Journal of Colloid and Interface Science, 179(1):298–310, 1996.

[13] Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer. Automated

Analysis of Cellular Signals from Large-Scale Calcium Imaging Data. Neuron,

63(6):747–760, 2009.

[14] Greg J. Stephens, Bethany Johnson-Kerner, William Bialek, and William S.

Ryu. Dimensionality and dynamics in the behavior of C. elegans. PLoS

Computational Biology, 4(4), 2008.

[15] Hanchuan Peng, Fuhui Long, Xiao Liu, Stuart K. Kim, and Eugene W. Myers.

Straightening Caenorhabditis elegans images. Bioinformatics, 24(2):234–242,

2008.

[16] Onno D Broekmans, Jarlath B Rodgers, William S Ryu, and Greg J Stephens.

Resolving coiled shapes reveals new reorientation behaviors in C. elegans.

eLife, 5:e17227, sep 2016.

[17] Yi Deng, Philip Coen, Mingzhai Sun, and Joshua W Shaevitz. Eﬃcient

multiple object tracking using mutually repulsive active membranes. PloS one,

8(6):e65769, 2013.

29

[18] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active

contour models. International Journal of Computer Vision, 1(4):321–331,

1988.

2002.

[19] Ben Tordoﬀ and David W Murray. Guided sampling and consensus for motion

estimation. In European conference on computer vision, pages 82–96. Springer,

[20] Gang Lin, Umesh Adiga, Kathy Olson, John F Guzowski, Carol a Barnes, and

Badrinath Roysam. A hybrid 3D watershed algorithm incorporating gradient

cues and object models for automatic segmentation of nuclei in confocal image

stacks. Cytometry. Part A : the journal of the International Society for

Analytical Cytology, 56(1):23–36, 2003.

[21] Yu Toyoshima, Terumasa Tokunaga, Osamu Hirose, Manami Kanamori,

Takayuki Teramoto, Moon Sun Jang, Sayuri Kuge, Takeshi Ishihara, Ryo

Yoshida, and Yuichi Iino. Accurate Automatic Detection of Densely

Distributed Cell Nuclei in 3D Space. PLOS Computational Biology,

12(6):e1004970, 2016.

[22] Bing Jian and Baba C Vemuri. A Robust Algorithm for Point Set Registration

Using Mixture of Gaussians. Proceedings / IEEE International Conference on

Computer Vision. IEEE International Conference on Computer Vision,

2:1246–1251, oct 2005.

[23] John G White, Eileen Southgate, J Nichol Thomson, and Sydney Brenner.

The structure of the nervous system of the nematode caenorhabditis elegans.

Philos Trans R Soc Lond B Biol Sci, 314(1165):1–340, 1986.

30

6
1
0
2
 
t
c
O
 
4
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
9
7
5
4
0
.
0
1
6
1
:
v
i
X
r
a

Automatically tracking neurons in a moving and

deforming brain

Jeﬀrey P. Nguyen1,2, Ashley N. Linder3, George S. Plummer2, #, Joshua

W. Shaevitz1,2 and Andrew M. Leifer1,3,*

1Department of Physics, Princeton University, Princeton, New Jersey, United

States of America

2Lewis-Sigler Institute for Integrative Genomics, Princeton University,

Princeton, New Jersey, United States of America

#Current Address: Tufts University School of Medicine, Boston,

Massachusetts, United States of America

3Princeton Neuroscience Institute, Princeton University, Princeton, New

Jersey, United States of America

*Corresponding author, E-mail: leifer@princeton.edu (AML)

October 17, 2016

Abstract

Advances in optical neuroimaging techniques now allow neural activity to be

recorded with cellular resolution in awake and behaving animals. Brain motion in

these recordings pose a unique challenge. The location of individual neurons must

be tracked in 3D over time to accurately extract single neuron activity traces.

Recordings from small invertebrates like C. elegans are especially challenging

because they undergo very large brain motion and deformation during animal

movement. Here we present an automated computer vision pipeline to reliably

track populations of neurons with single neuron resolution in the brain of a freely

moving C. elegans undergoing large motion and deformation. 3D volumetric

ﬂuorescent images of the animal’s brain are straightened, aligned and registered,

and the locations of neurons in the images are found via segmentation. Each

neuron is then assigned an identity using a new time-independent machine-learning

approach we call Neuron Registration Vector Encoding. In this approach, non-rigid

point-set registration is used to match each segmented neuron in each volume with

a set of reference volumes taken from throughout the recording. The way each

neuron matches with the references deﬁnes a feature vector which is clustered to

assign an identity to each neuron in each volume. Finally, thin-plate spline

interpolation is used to correct errors in segmentation and check consistency of

assigned identities. The Neuron Registration Vector Encoding approach proposed

here is uniquely well suited for tracking neurons in brains undergoing large

deformations. When applied to whole-brain calcium imaging recordings in freely

moving C. elegans, this analysis pipeline located 150 neurons for the duration of an

8 minute recording and consistently found more neurons more quickly than manual

or semi-automated approaches.

2

Author Summary

Computer algorithms for identifying and tracking neurons in images of a brain have

struggled to keep pace with rapid advances in neuroimaging. In small transparent

organism like the nematode C. elgeans, it is now possible to record neural activity

from all of the neurons in the animal’s head with single-cell resolution as it crawls.

A critical challenge is to identify and track each individual neuron as the brain

moves and bends. Previous methods required large amounts of manual human

annotation. In this work, we present a fully automated algorithm for neuron

segmentation and tracking in freely behaving C. elegans. Our approach uses

non-rigid point-set registration to construct feature vectors describing the location

of each neuron relative to other neurons and other volumes in the recording. Then

we cluster feature vectors in a time-independent fashion to track neurons through

time. This new approach works very well when compared to a human.

Introduction

Optical neural imaging has ushered in a new frontier in neuroscience that seeks to

understand how neural activity generates animal behavior by recording from large

populations of neurons at cellular resolution in awake and behaving animals.

Population recordings have now been used to elucidate mechanisms behind zebra

ﬁnch song production [1], spatial encoding in mice [2], and limb movement in

primates [3]. When applied to small transparent organisms, like Caenorhabditis

elegans [4], Drosophila [5], and zebraﬁsh [6], nearly every neuron in the brain can

be recorded, permitting the study of whole brain neural dynamics at cellular

resolution.

1

Methods for segmenting and tracking neurons have struggled to keep up as new

imaging technologies now record from more neurons over longer times in

environments with greater motion. Accounting for brain motion in particular has

become a major challenge, especially in recordings of unrestrained animals. Brains

in motion undergo translations and deformations in 3D that make robust tracking

of individual neurons very diﬃcult. The problem is compounded in invertebrates

like C. elegans where the head of the animal is ﬂexible and deforms greatly. If left

unaccounted for, brain motion not only prevents tracking of neurons, but it can

also introduce artifacts that mask the true neural signal. In this work we propose

an automated approach to segment and track neurons in the presence of dramatic

brain motion and deformation. Our approach is optimized for calcium imaging in

unrestrained C. elegans.

Neural activity can be imaged optically with the use of genetically encoded calcium

sensitive ﬂuorescent indicators, such as GCaMP6s used in this work [7].

Historically calcium imaging was often conducted in head-ﬁxed or anesthetized

animals to avoid challenges involved with imaging moving samples [4, 8, 9].

Recently, however, whole-brain imaging was demonstrated in freely behaving C.

elegans [10, 11]. C. elegans are a small transparent nematode, approximately 1mm

in length, with a compact nervous system of only 302 neurons. About half of the

neurons are located in the animal’s head, which we refer to as its brain.

Analyzing ﬂuorescent images of moving and deforming brains requires algorithms

to detect neurons across time and extract ﬂuorescent signals in 3D. Several

strategies exist for tracking neurons in volumetric recordings. One approach is to

ﬁnd correspondences between neuron positions in consecutive time points, for

example, by applying a distance minimization, and then stitching these

2

correspondences together through time [12]. This type of time-dependent tracking

requires that neuron displacements for each time step are less than the distance

between neighboring neurons, and that the neurons remain identiﬁable at all times.

If these requirements break down, even for only a few time points, errors can

quickly accumulate. Other common methods, like independent component analysis

(ICA) [13] are also exquisitely sensitive to motion and as a result they have not

been successfully applied to recordings with large brain deformations.

Large inter-volume motion arises when the recorded image volume acquisition rate

is too low compared to animal motion. Unfortunately, large inter-volume brain

motion is likely to be a prominent feature of whole-brain recordings of moving

brains for the foreseeable future. In all modern imaging approaches there is a

fundamental tradeoﬀ between the following attributes: acquisition rate (temporal

resolution), spatial resolution, signal to noise, and the spatial extent of the

recording. As recordings seek to capture larger brain regions at single cell

resolution, they necessarily compromise on temporal resolution. For example,

whole brain imaging in freely moving C. elegans has only been demonstrated at

slow acquisition rates because of the requirements to scan the entire brain volume

and expose each slice for suﬃciently long time. At these rates, a signiﬁcant amount

of motion is present between image planes within a single brain volume. Similarly,

large brain motions also remain between sequential volumes. Neurons can move the

entire width of the worm’s head between sequential volumes when recording at 6

brain-volumes per second, as in [10]. In addition to motion, the brain also bends

and deform as it moves. Such changes to the brain’s conformation greatly alter the

pattern of neuron positions making constellations of neurons diﬃcult to compare

across time.

3

To account for this motion, previous work that measured neural activity in freely

moving C. elegans required either large amounts of manual annotation as reference

data for comparison [11] or required a human user to supervise and correct

semi-automated algorithms for each and every neuron-time point [10]. This level of

manual annotation becomes impractical as the length of recordings and the number

of neurons increases. For example, 10 minutes of recorded neural activity from [10],

had over 360,000 neuron time points and required over 200 person-hours of manual

annotation. Here, we introduce a new time-independent algorithm that uses

machine learning to automatically segment and track all neurons in the head of a

freely moving animal. We call this technique Neuron Registration Vector Encoding,

and we use it to extract neural signals in unrestrained C. elegans expressing the

calcium indicator GCaMP6s and the ﬂuorescent label RFP.

Results

Overview of neuron tracking analysis

We introduce a method to track over 100 neurons in the brain of a freely moving

C. elegans. The analysis pipeline is made of ﬁve modules and an overview is shown

in Figure 1. The ﬁrst three modules, “Centerline Detection,” “Straightening” and

“Segmentation,” collectively assemble the individually recorded planes into a

sequence of 3D volumes and identify each neuron’s location in each volume. The

next two modules, “Registration Vector Construction” and “Clustering,” form the

core of the method and represent a signiﬁcant advance over previous approaches.

Collectively, these two modules are called “Neuron Registration Vector Encoding".

The “Registration Vector Construction” module leverages information from across

4

Figure 1:
Schematic of analysis pipeline to segment and track neurons through
time and extract their neural activity in a deforming brain. Neurons are labeled
with calcium insensitive red ﬂuorescent proteins, RFP, and calcium sensitive green
ﬂuorescent proteins, GCaMP. Videos of the animal’s behavior and volumetric ﬂuo-
rescent images of the animal’s brain serve as input to the pipeline. The algorithm
detects all neurons in the head and produces tracks of the neural activity across time
as the animal moves.

5

the entire recording in a time-independent way to generate feature vectors that

characterize every neuron at every time point in relation to a repertoire of brain

conﬁrmations. The “Clustering” module then clusters these feature vectors to

assign a consistent identity to each neuron across the entire recording. A ﬁnal

module corrects for errors the original segmentation. The implementation and

results of this approach are described below.

Recording of whole-brain calcium activity and body posture in

moving animal

Worms expressing the calcium indicator GCaMP6s and a calcium-insensitive

ﬂuorescent protein RFP in the nuclei of all neurons were imaged during

unrestrained behavior in a custom 3D tracking microscope, as described in [10].

Two recordings are presented in this work: a new 8 minute recording of an animal

of of strain AML32 and a previously reported 4 minute recording of strain ﬁrst

described in [10].

The signal of interest in both recordings is the green ﬂuorescence intensity from

GCaMP6s in each neuron. Red ﬂuorescence from the RFP protein serves as a

reference for locating and tracking the neurons. The microscope provides four raw

image streams that serve as inputs for our neural tracking pipeline, seen in Fig 2A.

They are: (1) low-magniﬁcation dark-ﬁeld images of the animal’s body posture (2)

low-magniﬁcation ﬂuorescent images of the animal’s brain (3) high-magniﬁcation

green ﬂuorescent images of single optical slices of the brain showing GCaMP6s

activity and (4) high-magniﬁcation red ﬂuorescent images of single optical slices of

the brain showing the location of RFP. The animal’s brain is kept centered in the

ﬁeld of view by realtime feedback loops that adjust a motorized stage to

6

Figure 2: (A) Example images from all four video feeds from our imaging system.
Both scale bars are 100µm (B)A schematic illustrating the timings from all the
devices that run in open loop in our imaging setup. The camera that collects high
magniﬁcation images captures at 200Hz. The two low magniﬁcation images capture
at 60Hz, and the focal plane moves up and down in a 3 Hz triangle wave. The
cameras are synchronized post-hoc using a camera ﬂash and each image is assigned
a timestamp on a common timeline for the purposes of analysis.

compensate for the animal’s crawling. To acquire volumetric information, the high

magniﬁcation imaging plane scans back and forth along the axial dimension, z, at 3

Hz as shown in Fig 2B, acquiring roughly 33 optical slices per volume, sequentially,

for 6 brain-volumes per second. The animal’s continuous motion causes each

volume to be arbitrarily sheared in z. Although the image streams operate at

diﬀerent volume acquisition rates and on diﬀerent clocks, they are later

synchronized by simultaneous light ﬂashes and given a timestamp on a common

timeline. Each of the four imaging streams are aligned to each other in software

using aﬃne transformations found by imaging ﬂuorescent beads.

7

Centerline detection and gross brain alignment

The animal’s posture contains information about the brain’s orientation and about

any deformations arising from the animal’s side-to-side head swings. The ﬁrst step

of the pipeline is to extract the centerline that describes the animal’s posture.

Centerline detection in C. elegans is an active ﬁeld of research. Most algorithms

use intensity thresholds to detect the worm’s body and then use binary image

operations to extract a centerline [14, 15, 16]. Here we use an open active contour

approach [17, 18] to extract the centerline from dark ﬁeld images with

modiﬁcations to account for cases when the worm’s body crosses over itself as

occurs during so-called “Omega Turns.” In principle any method, automated or

otherwise, that detects the centerlines should be suﬃcient. At rare times where the

worm is coiled and the head position and orientation cannot be determined

automatically, the head and the tail of the worm are manually identiﬁed.

The animal’s centerline allows us to correct for gross changes in the worm’s

position, orientation, and conformation (Fig 3a). We use the centerlines

determined by the low magniﬁcation behavior images to straighten the high

magniﬁcation images of the worm’s brain. An aﬃne transform must be applied to

the centerline coordinates to transform them from the dark ﬁeld coordinate system

into the coordinate system of the high magniﬁcation images. Each image slice of

the worm brain is straightened independently to account for motion within a single

volume. The behavior images are taken at a lower acquisition rate than the high

magniﬁcation brain images, so a linear interpolation is to used obtain a centerline

for each slice of the brain volume. In each slice, we ﬁnd the tangent and normal

vectors at every point of the centerline (Fig 3b). The points are interpolated with a

single pixel spacing along the centerline to preserve the resolution of the image.

8

(A) Centerlines are detected from the low magniﬁcation dark ﬁeld images.
Figure 3:
The centerline is shown in green and the tip of the worm’s head is indicated by a
blue dot. (B) The centerline found from the low magniﬁcation image is overlaid on
the high magniﬁcation RFP images. The lines normal to the centerline, shown in
blue, are used to straighten the image. All scale bars are 100 µm.(C) A maximum
intensity projection of the straightened volume is shown. Individual neuronal nuclei
are shown (D) before and (E) after segmentation.

The image intensities along each of the normal directions are interpolated and the

slices are stacked to produce a straightened image in each slice (Fig 3c). In the new

coordinate system, the orientation of the animal is ﬁxed and the worm’s bending is

greatly suppressed. We further reduce shearing between slices using standard video

stabilization techniques [19]. Speciﬁcally, bright-intensity peaks in the image are

tracked between neighboring image slices and aﬃne transformations are calculated

between each slice of the volume. All slices are registered to the middle slice by

applying these transformations sequentially throughout the volume. Each slice

would undergo transformations for every slice in between it and the middle slice to

correct shear throughout the volume. A ﬁnal rigid translation is required to align

each volume to the initial volume. The translations are found by ﬁnding an oﬀset

that maximizes the cross-correlation between each volume and the initial volume.

9

Segmentation

Before neuron identities can be matched across time, we must ﬁrst segment the

individual neurons within a volume to recovers each neuron’s size, location, and

brightness (Fig 3d,e). Many algorithms have been developed to segment neurons in

a dense region [20, 21]. We segment the neurons by ﬁnding volumes of curvature in

ﬂuorescence intensity. We compute the 3D Hessian matrix at each point in space

and threshold for points where all of the three eigenvalues of the Hessian matrix

are negative. In order to further divide regions into objects that are more likely to

represent neurons, we use a watershed separation on the distance transform of the

thresholded image. The distance transform is found by replacing each thresholded

pixel with the Euclidean distance between it and the closest zero pixel in the

thresholded image. Image blurring from animal motion poses a challenge for

segmentation. We allow for some noise and error in the segmentation because we

will have the opportunity to automatically correct many of these errors later in the

pipeline.

Neuron registration vector construction

Extracting neural signals requires the ability to match neurons found at diﬀerent

time points. Even after gross alignment and straightening, neurons in our images

are still subject to local nonlinear deformations and there is signiﬁcant movement

of neurons between volumes. Rather than tracking through time, the neurons in

each volume are characterized based on how they match to neurons in a set of

reference volumes. Our algorithm compares constellations of neurons in one volume

to unannotated reference volumes and assigns correspondences or “matches”

10

between the neurons in the sample and each reference volume. We modiﬁed a point

set registration algorithm developed by Jian and Vemuri [22] to do this (Fig 4a).

The registration algorithm represents two point sets, a sample point-set denoted by

X = {xi} and a reference point-set indicated by R = {ri}, as Gaussian mixtures

and then attempts to register them by deforming space to minimize the distance

between the two mixtures. Here, each neuron is modeled by a 3D Gaussian with

uniform covariance. Since we are matching images of neurons rather than just

points, we can use the additional information from the size and brightness of each

neuron. We add this information to the representation of each neuron by adjusting

the amplitude and standard deviation of the Gaussians. The Gaussian mixture

representation of an image is given by,

f (ξ, X) =

Ai exp

−

(cid:88)

i

(cid:18)

(cid:107)ξ − xi(cid:107)2
2(λσi)2

(cid:19)
,

(1)

where Ai, xi, and σi are the amplitude, mean, and standard deviation of the i-th

Gaussian. These parameters are derived from the brightness, centroid, and size of

the segmented neuron, while ξ is the 3D spatial coordinate. A scale factor λ is

added to the standard deviation to scale the size of each Gaussian. This will be

used later during gradient descent. The sample constellation of neurons is then

represented by the Gaussian mixture f (ξ, X). Similarly, the reference

constellation’s own neurons is represented as a f (ξ, R).

To match a sample constellation of neurons X with a reference constellation of

neurons R, we use the non rigid transformation u : IR3 (cid:55)→ IR3. The transformation

u maps X to u[X] such that the L2 distance between f (ξ, u[X]) and f (ξ, R) is

minimized with some constraint on the amount of deformation. This can be

written as an energy minimization problem, with the energy of the transformation,

11

E(u), written as

(cid:90)

E(u) =

(cid:2)f (ξ, u[X]) − f (ξ, R)(cid:3)2dξ + EDeformation(u).

(2)

Note that the point-sets X and R are allowed to have diﬀerent numbers of points.

We model the deformations as a thin-plate spline (TPS). The transformation

equations and resulting form of Edeformation(u) is shown in the methods. The

minimization of E is found by gradient descent. Since the energy landscape has

many local minima, we initially chose a large scale factor, λ, to increase the size of

each Gaussian and smooth over smaller features. During gradient descent, λ is

decreased to better represent the original image. After the transformation, sample

points are matched to reference points by minimizing distances between assigned

pairs [12]. The matching is not greedy, and neurons in the sample that are far from

any neurons in the reference are not matched. A neuron at xi is assigned a match

vi to indicate which neuron in the set R it was matched to. For example if xi

matched with rj when X is registered to R, then vi = j. If xi has no match in R,

then vi = ∅.

The modiﬁed non-rigid point set registration algorithm described above allows us

to compare one constellation of neurons to another. In principle, neuron tracking

could be achieved by registering the constellation of neurons at each time-volume

to a single common reference. That approach is susceptible to failures in non-rigid

point set registration. Non-rigid point-set registration works well when the

conformation of the animal in the sample and the reference are similar, but it is

unreliable when there are large deformations between the sample and the reference,

as happens with some regularity in our recordings. In addition, this approach is

especially sensitive to any errors in segmentation, especially in the reference. An

12

Figure 4:
Schematic illustration of neuron registration vector encoding. (A) The
registration between a sample volume and a single reference volume is done in several
steps. I. The image is segmented into regions corresponding to each of the neurons.
II. The image is represented as a Gaussian mixture, with a single Gaussian for each
segmented region. The amplitude and the standard deviation of the Gaussians are
derived from the brightness and the size of the segmented regions. III. Non-rigid
point-set registration is then used to deform the sample points to best overlap the
reference point-set. IV. Neurons from the sample and the reference point-sets are
paired by minimizing distances between neurons. (B) Neuron registration vectors
are constructed by assigning a feature vector vi,t to each neuron xi,t in a sample
volume xt by performing the registration between the sample volume and a set of
300 reference volumes, each denoted by rk. Each registration of the neuron results in
a neuron match, vk
i , and the set of matches becomes the feature vector vi,t. (C) The
vectors from all neuron-times, vi,t, are hierarchically clustered. The same neuron
found at diﬀerent times will have a similar set of features and therefore will contain
the same neuron found at diﬀerent times. Real matches occur in a high dimensional
space. Only two dimensions are illustrated here for clarity. Each of the feature
vector is assigned a cluster, and the cluster labels are given by S. (D) The clustering
of the feature vectors shown in (C) assigns an identity to each of the neurons in
every volume. This allows us to track the neurons across diﬀerent volumes of the
recording.

13

alternative approach would be to sequentially register neurons in each time volume

to the next time-volume. This approach, however, accumulates even small errors

and quickly becomes unreliable. Instead of either of those approaches, we use

registration to compare the constellation of neurons at each time volume to a set of

reference time-volumes that span a representative space of brain conformations

(Fig 4b), as described below.

The constellation of neurons at a particular time in our recording is given by Xt,

and the position of the i-th neuron at time t is denoted by xi,t. We select a set of

K reference constellations, each from a diﬀerent time volume Xt in our recording,

so as to achieve a representative sampling of the many diﬀerent possible brain

conformations the animal can attain. These K reference volumes are denoted by

{R1, R2, R3, ..., RK}. For simplicity, we use 300 volumes spaced evenly through

time as our reference constellations. Each Xt is separately matched with each of

the references, and each neuron in the sample, xi,t, gets a set of matches

vi,t = {v1

i,t, v2

i,t, v3

i,t, ..vK

i,t}, one match for each of the K references. This set of

matches is a feature vector which we call a Neuron Registration Vector. It

describes the neuron’s location in relation to its neighbors when compared with the

set of references. This vector can be used to identify neurons across diﬀerent times.

Clustering registration vectors

The neuron registration vector provides information about that neuron’s position

relative to its neighbors, and how that relative position compares with many other

reference volumes. A neuron with a particular identity will match similarly to the

set of reference volumes and thus that neuron will have similar neuron registration

vectors over time. Clustering similar registration vectors allows for the

14

identiﬁcation of that particular neuron across time (Fig 4c,d).

To illustrate the motivation for clustering, consider a neuron with identity s that is

found at diﬀerent times in two sample constellations X 1 and X 2. When X 1 and

X 2 have similar deformations, the neuron s from both constellations will be

assigned the same set of matches when registered to the set of reference

constellations, and as a result the corresponding neuron registration vectors v1 and

v2 will be identical. This is true even if the registration algorithm itself fails to

correctly match neuron s in the sample to its true neuron s in the reference. As the

deformations separating X 1 and X 2 become larger, the distance between the

feature vectors v1 and v2 also becomes larger. This is because the two samples will

be matched to diﬀerent neurons in some of the reference volumes as each sample is

more likely to register poorly with references that are far from it in the space of

deformations.

Crucially, the reference volumes consist of instances of the animal in many diﬀerent

deformation states. So while errors in registering some samples will exist for

certain reference, they do not persist across all references, and thus do not eﬀect

the entire feature vector. For the biologically relevant deformations that we

observe, the distance between v1 and v2 will be smaller if both are derived from

neuron s than compared to the distance between v1 and v2 if they were derived

from s and another neuron. We can therefore cluster the feature vectors to produce

groups that consist of the same neuron found at many diﬀerent time points.

The list of neuron registration vectors from all neuron at all times, {vi,t}, is

hierarchically clustered. Each match in the vectors, vk

i,t, is represented as a binary

vector of 0s with a 1 at the vk−th

i

position. The size of the vector is equal to the

number of neurons in Rk. The feature vector {vi,t} is the concatenation of all of

15

Figure 5: Example of consensus voting to correct a misidentiﬁed neuron. In volume
735, neuron #111 is found successfully and is indicated in green. In volume 736,
however, the neuron is misidentiﬁed, shown in red. During the correction phase, all
other time points vote for what the position of neuron #111 should be assuming a
thin-plate spline deformation, indicated by the cloud of blue points. Since the initial
estimate of the position is far from the cloud of blue points, a corrected position is
selected as the centroid of the votes weighted by image intensity. This process is
repeated to correct errors for every neuron at every time.

the binary vectors from all matches to the K reference constellations. The

correlation distance, 1 − corr(vm, vn), was used as the pair wise distance metric for

clustering. Clusters were checked to ensure that at most one neuron was

represented from each time. Clusters containing neurons from less than 40% of the

volumes were removed. Each cluster is assigned a label {S1, S2, S3, ...} which

uniquely identiﬁes a single neuron over time, and each neuron at each time xi,t is

given an identiﬁer si,t which corresponds to which cluster that neuron-time belongs

to. Neurons that are not assigned to one of these clusters are removed because they

are likely artifactual or represent a neuron that is segmented too poorly for

inclusion.

Correcting errors in tracking and segmentation

Neuron Registration Vector Encoding successfully identiﬁes segmented neurons

consistently across time. A transient segmentation error, however, would

16

necessarily lead to missing or misidentiﬁed neurons. To identify and correct for

missing and misidentiﬁed neurons, we check each neuron’s locations and ﬁll in

missing neurons using a consensus comparison and interpolation in a thin-plate

spline (TPS) deformed space. For each neuron identiﬁer s and time t(cid:63), we use all

other point-sets, {Xt} to guess what that neuron’s location might be. This is done

by ﬁnding the TPS transformation, ut→t(cid:63) : Xt (cid:55)→ Xt(cid:63), that maps the identiﬁed

points from Xt to the corresponding points in Xt(cid:63) excluding the point s. The

position estimate is then given by ut→t(cid:63)[xi,t] with i selected such that si,t = s. This

results in a set of points representing the predicted location of the neuron at time

t(cid:63) from all other times. When a neuron identiﬁer is missing for a given time, the

position of that neuron s is inferred by consensus. Namely, correct location is

deemed to be the centroid of the set of inferred locations weighted by the

underlying image intensity. This weighted centroid is also used if the current

identiﬁed location of the neuron s has a distance greater than 3 standard

deviations away from the centroid of the set of points, implying that an error may

have occurred in the assignment. This is shown in Fig 5, where neuron 111 is

identiﬁed in volume 735, but the the label for neuron 111 is incorrectly located in

volume 736. In that case the weighted centroid from consensus voting was used.

Comparison with manually annotated data

To asses the accuracy of the Neuron Registration Vector Encoding pipeline, we

applied our automated tracking system to a 4 minute recording of whole brain

activity in a moving C. elegans that had previously been hand annotated [10]. A

custom Matlab GUI was used for manually identifying and tracking neurons. Nine

researchers collectively annotated 70 neurons from each of the 1519 volumes in the

17

Figure 6:
Comparison of the automated Neuron Registration Vector Encoding
algorithm with manual human annotation over a 4 minute recording on brain activity
(strain AML14). (A) Spheres show position of neurons that were detected by the
automated algorithm. Grey indicates a neuron detected by both the algorithm and
the human (70 neurons). Red indicates neurons that were missed by the human
and detected only by the algorithm (49 neurons). (B) Histogram showing number of
neurons that were mismatched for a given fraction of time-volumes when comparing
automated and manual approaches. Only those neurons that were consistently found
by both algorithm and human were considered. An automatically identiﬁed neuron
was deemed correctly matched for a given time-volume if it was paired with the
correct corresponding manual neuron.

18

4 minute video. This is much less than the 181 neurons predicted to be found in

the head [23]. The discrepancy is likely caused by a combination of imaging

conditions and human nature. The short exposure time of our recordings makes it

hard to resolve dim neurons, and the relatively long recordings tend to cause

photobleaching which make the neurons even dimmer. Additionally, human

researchers naturally tend to select only those neurons that are brightest and are

most unambiguous for annotation, and tend to skip dim neurons or those neurons

that are most densely clustered.

We compared human annotations to our automated analysis in this same dataset.

We performed the entire pipeline including detecting centerlines, performing worm

straightening, segmentation, and neuron registration vector encoding and

clustering, and correction. Automated tracking detected 119 neurons from the

video compared to 70 from the human. In each volume, we paired the

automatically tracked neurons with those found by manual detection by ﬁnding the

closest matches in the unstraightened coordinate system. A neuron was perfectly

tracked if it matched with the same manual neuron at all times. Tracking errors

were labelled when a neuron matched with a manual neuron that was diﬀerent

than the one it matched with most often. The locations of the detected neurons are

shown in Fig 6A. Only one neuron was incorrectly identiﬁed in more than 5% of

the time volumes (Fig 6B). The locations of neurons and the corresponding error

rates is shown in Fig 6B. Neurons that were detected by the algorithm but not

annotated manually are shown in gray. Upon further inspection, it was noted that

some of the mismatches between our method and the manual annotation were due

to human errors in the manual annotation, meaning the algorithm is able to correct

humans on some occasions.

19

Figure 7: A trace of neural activity from 144 neurons in the brain a C. elegans as it
freely moves on an agarose plate over 8 minutes (strain AML32). The neural activity
is expressed as a fold change over baseline of the ratio of GCaMP6s to RFP for each
neuron. The behavior is indicated in the ethogram. On the right is the locations of
all of the detected neurons (the head of the worm is towards the top of the page).
The neurons that have signiﬁcant correlation with reverse locomotion are indicated
in red. White gaps indicate instances where neurons failed to segment.

Neural activity from tracked neurons

Fluorescent intensity is ultimately the measurement of interest and this can be

easily extracted from the tracks of the neuron locations across time. The pixels

within a 2 µm radius sphere around each point are used to calculate the average

ﬂuorescent intensity of a neuron in both the red RFP and green GCaMP6s

channels. We used the RFP as a reference ﬂuorophore and measure neural activity

as a fold change over baseline of the ratio of GcAMP6s to RFP intensity,

Activity =

∆R
R0

=

R − R0
R0

,

R =

IGCaM P 6s
IRF P

.

(3)

20

The baseline for each neuron, R0, is deﬁned as the 20th percentile value of the ratio

R for that neuron. Fig 7 shows calcium imaging traces extracted from new

whole-brain recordings using the registration vector pipeline. 144 neurons were

tracked for approximately 8 minutes as the worm moves. Many neurons show clear

correlation with reversal behaviors in the worm.

Discussion

The Neuron Registration Vector Encoding method presented here is able to process

longer recordings and locate more neurons with less human input compared to

previous examples of whole-brain imaging in freely moving C. elegans [10]. Fully

automated image processing means that we are no longer limited by the human

labor required for manual annotation. In new recordings presented here, we are

able to observe 144 neurons of the expected 181 neurons, much larger than the 80

observed in previous work from our lab and others [10, 11]. By automating

tracking and segmentation, this relieves one of the major bottlenecks to analyzing

longer recordings.

The neuron registration vector encoding algorithm primarily relies on the local

coherence of the motion of the neurons. It permits large deformations of the

worm’s centerline so long as deformations around the centerline remain modest.

Crucially, the algorithm’s time-independent approach allows it to tolerate large

motion between consecutive time-volumes. These properties make it well suited for

our neural recordings of C. elegans and we suspect that the our approach would be

applicable to tracking neurons in moving and deforming brains from other

organisms as well.

21

Certain classes of recordings, however, would not be well suited for Neuron

Registration Vector Encoding and Clustering. The approach will fail when the

local coherence of neuron motion breaks down. For example, if one neuron were to

completely swap locations with another neuron relative to its surroundings,

registration would not detect the switch and our method would fail. In this case a

time-dependent tracking approach may perform better.

In addition, proper clustering of the feature vectors requires the animal’s brain to

explore a contiguous region of deformation space. For example, if a hypothetical

brain were only ever to occupy two distinct conformations that are diﬀerent enough

that registration is not reliable between these two conformation states, the

algorithm would fail to cluster feature vectors from the same neuron across the two

states. To eﬀectively identify the neurons in these two conformations, the animal’s

brain must sample many conformations in between those two states. This way,

discrepancies in registration arise gradually and the resulting feature vectors

occupy a continuous region in the space of possible feature vectors. Note that a

similar requirement would necessarily apply to any time-dependent tracking

algorithm as well.

We suspect that brain recordings from most species of interest meet these two

requirements: namely neuron motion will have local coherence and the brain will

explore a contiguous region of deformation space. Where these conditions are

satisﬁed, we expect registration vector encoding to work well. Tracking in C.

elegans is especially challenging because the entire brain undergoes large

deformations as the animal bends. In most other organisms like zebraﬁsh and

Drosophila, brains are contained within a skull or exoskeleton and relative motion

of the neurons is small. In those organisms, ﬂuctuations in neuron positions take

22

the form of rigid global transformations as the animal moves, or local non-linear

deformations due to motion of blood vessels. We expect that this approach will be

applicable there as well.

Methods

Strains

twice.

Imaging C. elegans

Transgenic worms were cultivated on nematode growth medium (NGM) plates with

OP50 bacteria. Strain AML32 (wtfIs5[Prab-3 ::NLS::GCaMP6s;

Prab-3 ::NLS::tagRFP]) was generated by UV irradiating animals of strain AML14

(wtfEx4[Prab-3 ::NLS::GCaMP6s; Prab-3 ::NLS::tagRFP]) [10] and outcrossing

Imaging is performed as described in Nguyen et al [10]. The worm is placed

between an agarose slab and a large glass coverslip. The coverslip is held up by a

0.006” plastic shims in order to reduce the amount of pressure on the worm from

the glass, and mineral oil is spread over the worm to better match refractive indices

in the space between the coverglass and the worm. The dark ﬁeld image is used to

extract the animal’s centerline while the ﬂuorescent image is used for tracking the

worm’s brain. Only the head of the worm is illuminated by the ﬂuorescent

excitation light and can be observed in the low magniﬁcation ﬂuorescent image.

23

Thin Plate Spline deformations

Point-set registration was done as described by Jian and Vemuri [22], using TPS

deformations. Given a set of n initial control points X = {xi}, and the set of

transformed points, u[X], the transformation u can be written as

u[X] = WU(X) + AX + t. The aﬃne portion of the transformation is AX + t,

while WU(X) is the non-linear part of the transformation from TPS. U(X) is an

n × 1 vector with Ui(x) = U(x, xi) = U((cid:107)x − xi(cid:107)) = 1

(cid:107)x−xi(cid:107) and W is a 3 × n

matrix. The elements of W, A and t can be ﬁt given the set of control points X

and the location of the transformed points u(X). The energy of bending,

EBending(u), depends on how the control points are deformed. We use the same

energy as in [22], with EBending(u) = trace(WKWT) where Kij = U (xi, xj). Since

the integral in Eqn. 2 is easily computed analytically, the energy can be quickly

calculated and the parameters for W, A, and t for the minimization can be found

using gradient descent.

Algorithm implementation

The analysis steps shown in Fig 1 were performed on Princeton University’s

high-performance scientiﬁc computing cluster, “Della.” Jobs were run on up to 200

cores. Straightening, segmentation, and feature extraction are parallelized over

each volume, with each volume being processed on a single core. Error correction is

parallelized over each neuron. Centerline extraction in each image relies on the

previous centerline and must be computed linearly. The computation methods are

summarized in Table 1. An 8 minute recording of a moving animal has about 3000

volumes and 250 GB of raw imaging data and can be processes from start to ﬁnish

24

Approximate
Percentage of time
4

Analysis Step

Computation

Centerline Detection Linear
Worm Straightening
& Segmentation
Registration Vector
Encoding
Clustering
Error Correction

Parallel over volumes

10

Parallel over volumes

80

Linear
Parallel over neurons

2
4

Table 1: Breakdown of computation time for Neuron Registration Vector Encoding
pipeline.

in less than 40 hours. Data can be found via a “requester-pays” Amazon Web

Services S3 “bucket” at s3://leiferlabnguyentracking and code can be found at

https://github.com/leiferlab/NeRVEclustering. Centerline extraction code is

available at https://github.com/leiferlab/CenterlineTracking.

Acknowledgments

We thank the following for their help manually annotating recordings: F Shipley,

M Liu, S Setru, K Mizes, D Mazumder, J Chinchilla, and L Novak. This work was

supported by Simons Foundation Grant SCGB 324285 (to AML) and Princeton

University’s Inaugural Dean for Research Innovation Fund for New Ideas in the

Natural Sciences (to JWS and AML). JPN is supported by a grant from the Swartz

Foundation. ANL is supported by a National Institutes of Health institutional

training grant through the Princeton Neuroscience Institute. This work is also

supported by a grant from the Glenn Foundation for Medical Research. The

analysis presented in this article was performed on computational resources

supported by the Princeton Institute for Computational Science and Engineering

(PICSciE) and the Oﬃce of Information Technology’s High Performance

25

Computing Center and Visualization Laboratory at Princeton University.

Author contributions

Conceptualization: AML JWS JPN

Methodology: JPN ANL

Software: JPN ANL

Validation: JPN

Formal Analysis: JPN

Investigation: JPN ANL

Resources: JPN ANL GSP

Writing - Original Draft Preparation: JPN

Writing - Review & Editing: AML JWS

Supervision: AML

Funding Acquisition: AML

26

References

[1] Michel A Picardo, Josh Merel, Kalman A Katlowitz, Daniela Vallentin,

Daniel E Okobi, Sam E Benezra, Rachel C Clary, Eftychios A Pnevmatikakis,

Liam Paninski, and Michael A Long. Population-level representation of a

temporal sequence underlying song production in the zebra ﬁnch. Neuron,

90(4):866–876, 2016.

[2] John P Rickgauer, Karl Deisseroth, and David W Tank. Simultaneous

cellular-resolution optical perturbation and imaging of place cell ﬁring ﬁelds.

Nature neuroscience, 17(12):1816–24, 2014.

[3] EM Maynard, NG Hatsopoulos, CL Ojakangas, BD Acuna, JN Sanes,

RA Normann, and JP Donoghue. Neuronal interactions improve cortical

population coding of movement direction. The journal of Neuroscience,

19(18):8083–8093, 1999.

[4] Saul Kato, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H.

Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. Global Brain

Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans.

Cell, 163(3):656–669, 2015.

27

[5] Wenze Li, Venkatakaushik Voleti, Evan Schaﬀer, Rebecca Vaadia, Wesley B.

Grueber, Richard S. Mann, and Elizabeth M. Hillman. Scape microscopy for

high speed, 3d whole-brain imaging in drosophila melanogaster. In Biomedical

Optics 2016, page BTu4D.3. Optical Society of America, 2016.

[6] Robert Prevedel, Young-Gyu Yoon, Maximilian Hoﬀmann, Nikita Pak,

Gordon Wetzstein, Saul Kato, Tina Schrödel, Ramesh Raskar, Manuel

Zimmer, Edward S. Boyden, and Alipasha Vaziri. Simultaneous whole-animal

3D imaging of neuronal activity using light-ﬁeld microscopy. Nat Meth,

11(7):727–730, July 2014.

[7] Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pulver, Sabine L

Renninger, Amy Baohan, Eric R Schreiter, Rex a Kerr, Michael B Orger,

Vivek Jayaraman, Loren L Looger, Karel Svoboda, and Douglas S Kim.

SUPPLEMENTAL - Ultrasensitive ﬂuorescent proteins for imaging neuronal

activity. Nature, 499(7458):295–300, 2013.

[8] Christopher D Harvey, Forrest Collman, Daniel A Dombeck, and David W

Tank. Intracellular dynamics of hippocampal place cells during virtual

navigation. Nature, 461(7266):941–946, 2009.

[9] Misha B Ahrens, Jennifer M Li, Michael B Orger, Drew N Robson,

Alexander F Schier, Florian Engert, and Ruben Portugues. Brain-wide

neuronal dynamics during motor adaptation in zebraﬁsh. Nature,

485(7399):471–477, May 2012. PMID: 22622571.

[10] Jeﬀrey P. Nguyen, Frederick B. Shipley, Ashley N. Linder, George S.

Plummer, Mochi Liu, Sagar U. Setru, Joshua W. Shaevitz, and Andrew M.

Leifer. Whole-brain calcium imaging with cellular resolution in freely behaving

28

caenorhabditis elegans. Proceedings of the National Academy of Sciences,

113(8):E1074–E1081, 2016.

[11] Vivek Venkatachalam, Ni Ji, Xian Wang, Christopher Clark, James Kameron

Mitchell, Mason Klein, Christopher J Tabone, Jeremy Florman, Hongfei Ji,

Joel Greenwood, Andrew D Chisholm, Jagan Srinivasan, Mark Alkema, Mei

Zhen, and Aravinthan D T Samuel. Pan-neuronal imaging in roaming

Caenorhabditis elegans. 113(8):E1082–8, 2016.

[12] J Crocker and D Grier. Methods of Digital Video Microscopy for Colloidal

Studies. Journal of Colloid and Interface Science, 179(1):298–310, 1996.

[13] Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer. Automated

Analysis of Cellular Signals from Large-Scale Calcium Imaging Data. Neuron,

63(6):747–760, 2009.

[14] Greg J. Stephens, Bethany Johnson-Kerner, William Bialek, and William S.

Ryu. Dimensionality and dynamics in the behavior of C. elegans. PLoS

Computational Biology, 4(4), 2008.

[15] Hanchuan Peng, Fuhui Long, Xiao Liu, Stuart K. Kim, and Eugene W. Myers.

Straightening Caenorhabditis elegans images. Bioinformatics, 24(2):234–242,

2008.

[16] Onno D Broekmans, Jarlath B Rodgers, William S Ryu, and Greg J Stephens.

Resolving coiled shapes reveals new reorientation behaviors in C. elegans.

eLife, 5:e17227, sep 2016.

[17] Yi Deng, Philip Coen, Mingzhai Sun, and Joshua W Shaevitz. Eﬃcient

multiple object tracking using mutually repulsive active membranes. PloS one,

8(6):e65769, 2013.

29

[18] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active

contour models. International Journal of Computer Vision, 1(4):321–331,

1988.

2002.

[19] Ben Tordoﬀ and David W Murray. Guided sampling and consensus for motion

estimation. In European conference on computer vision, pages 82–96. Springer,

[20] Gang Lin, Umesh Adiga, Kathy Olson, John F Guzowski, Carol a Barnes, and

Badrinath Roysam. A hybrid 3D watershed algorithm incorporating gradient

cues and object models for automatic segmentation of nuclei in confocal image

stacks. Cytometry. Part A : the journal of the International Society for

Analytical Cytology, 56(1):23–36, 2003.

[21] Yu Toyoshima, Terumasa Tokunaga, Osamu Hirose, Manami Kanamori,

Takayuki Teramoto, Moon Sun Jang, Sayuri Kuge, Takeshi Ishihara, Ryo

Yoshida, and Yuichi Iino. Accurate Automatic Detection of Densely

Distributed Cell Nuclei in 3D Space. PLOS Computational Biology,

12(6):e1004970, 2016.

[22] Bing Jian and Baba C Vemuri. A Robust Algorithm for Point Set Registration

Using Mixture of Gaussians. Proceedings / IEEE International Conference on

Computer Vision. IEEE International Conference on Computer Vision,

2:1246–1251, oct 2005.

[23] John G White, Eileen Southgate, J Nichol Thomson, and Sydney Brenner.

The structure of the nervous system of the nematode caenorhabditis elegans.

Philos Trans R Soc Lond B Biol Sci, 314(1165):1–340, 1986.

30

