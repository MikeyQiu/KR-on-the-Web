MEMEN: Multi-layer Embedding with Memory Networks for Machine
Comprehension

Boyuan Pan∗

(cid:52), Hao Li∗

(cid:52), Zhou Zhao†, Bin Cao‡, Deng Cai∗, Xiaofei He∗

∗State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, China
†College of Computer Science, Zhejiang University, Hangzhou, China
‡Eigen Technologies, Hangzhou, China
{panby, haolics, zhaozhou}@zju.edu.cn, bincao@aidigger.com, {dengcai, xiaofeihe}@gmail.com,

7
1
0
2
 
l
u
J
 
8
2
 
 
]
I

A
.
s
c
[
 
 
1
v
8
9
0
9
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Machine comprehension(MC) style question answering
is a representative problem in natural language process-
ing. Previous methods rarely spend time on the im-
provement of encoding layer, especially the embedding
of syntactic information and name entity of the words,
which are very crucial to the quality of encoding. More-
over, existing attention methods represent each query
word as a vector or use a single vector to represent
the whole query sentence, neither of them can handle
the proper weight of the key words in query sentence.
In this paper, we introduce a novel neural network ar-
chitecture called Multi-layer Embedding with Memory
Network(MEMEN) for machine reading task. In the en-
coding layer, we employ classic skip-gram model to the
syntactic and semantic information of the words to train
a new kind of embedding layer. We also propose a mem-
ory network of full-orientation matching of the query
and passage to catch more pivotal information. Exper-
iments show that our model has competitive results
both from the perspectives of precision and efﬁciency in
Stanford Question Answering Dataset(SQuAD) among
all published results and achieves the state-of-the-art re-
sults on TriviaQA dataset.

Introduction
Machine comprehension(MC) has gained signiﬁcant popu-
larity over the past few years and it is a coveted goal in
the ﬁeld of natural language processing and artiﬁcial intel-
ligence (Shen et al., 2016; Seo et al., 2016). Its task is to
teach machine to understand the content of a given passage
and then answer the question related to it. Figure 1 shows a
simple example from the popular dataset SQuAD (Rajpurkar
et al., 2016).

Many signiﬁcant works are based on this task, and most of
them focus on the improvement of a sequence model that is
augmented with an attention mechanism. However, the en-
coding of the words is also crucial and a better encoding
layer can lead to substantial difference to the ﬁnal perfor-
mance. Many powerful methods (Hu et al., 2017; Wang et
al., 2017; Seo et al., 2016) only represent their words in
two ways, word-level embeddings and character-level em-
beddings. They use pre-train vectors, like GloVe(Pennington

(cid:52)The majority of the work was done while the authors were in-
terning at the Eigen Technologies .

Figure 1: An example from the SQuAD dataset. The answer
is a segment text of context.

et al., 2014), to do the word-level embeddings, which ignore
syntactic information and name entity of the words. Liu et
al. (2017) construct a sequence of syntactic nodes for the
words and encodes the sequence into a vector representa-
tion. However, they neglected the optimization of the initial
embedding and didn’t take the semantic information of the
words into account, which are very important parts in the
vector representations of the words. For example, the word
“Apple” is a ﬁxed vector in GloVe and noun in syntactics
whatever it represents the fruit or the company, but name
entity tags can help recognize.

Moreover, the attention mechanism can be divided into
two categories: one dimensional attention (Chen et al., 2016;
Dhingra et al., 2016; Kadlec et al., 2016) and two dimen-
sional attention (Cui et al., 2016; Wang et al., 2016; Seo et
al., 2016). In one dimensional attention, the whole query is
represented by one embedding vector, which is usually the
last hidden state in the neural network. However, using only
one vector to represent the whole query will attenuate the
attention of key words. On the contrary, every word in the
query has its own embedding vector in the situation of two
dimensional attention, but many words in the question sen-
tence are useless even if disturbing, such as the stopwords.

In this paper, we introduce the Multi-layer Embedding
with Memory Networks(MEMEN), an end-to-end neural
network for machine comprehension task. Our model con-
sists of three parts: 1) the encoding of context and query, in

Figure 2: MEMEN structure overview. In the ﬁgure above, there are two hops in the memory network.

which we add useful syntactic and semantic information in
the embedding of every word, 2) the high-efﬁciency multi-
layer memory network of full-orientation matching to match
the question and context, 3) the pointer-network based an-
swer boundary prediction layer to get the location of the an-
swer in the passage. The contributions of this paper can be
summarized as follows.
• First, we propose a novel multi-layer embedding of the
words in the passage and query. We use skip-gram model
to train the part-of-speech(POS) tags and name-entity
recognition(NER) tags embedding that represent the syn-
tactic and semantic information of the words respectively.
The analogy inference provided by skip-gram model can
make the similar attributes close in their embedding space
such that more adept at helping ﬁnd the answer.
• Second, we introduce a memory networks of

full-
orientation matching.To combines the advantages of one
dimensional attention and two dimensional attention, our
novel hierarchical attention vectors contain both of them.
Because key words in query often appear at ends of
the sentence, one-dimensional attention, in which the bi-
directional last hidden states are regarded as representa-
tion, is able to capture more useful information compared
to only applying two dimensional attention. In order to
deepen the memory and better understand the passage ac-
cording to the query, we employ the structure of multi-
hops to repeatedly read the passage. Moreover, we add a
gate to the end of each memory to improve the speed of
convergence.

• Finally, the proposed method yields competitive results on
the large machine comprehension bench marks SQuAD
and the state-of-the-art results on TriviaQA dataset. On

SQuAD, our model achieves 75.37% exact match and
82.66% F1 score. Moreover, our model avoids the high
computation complexity self-matching mechanism which
is popular in many previous works, thus we spend much
less time and memory when training the model.

Model Structure
As Figure 2 shows, our machine reading model consists of
three parts. First, we concatenate several layers of embed-
ding of questions and contexts and pass them into a bi-
directional RNN(Mikolov et al., 2010). Then we obtain the
relationship between query and context through a novel full-
orientation matching and apply memory networks in order to
deeply understand. In the end, the output layer helps locate
the answer in the passage.

Encoding of Context and Query
In the encoding layer, we represent all tokens in the context
and question as a sequence of embeddings and pass them as
the input to a recurrent neural network.

Word-level embeddings and character-level embed-
dings are ﬁrst applied.We use pre-trained word vectors
GloVe(Pennington et al., 2014) to obtain the ﬁxed word em-
bedding of each word.The character-level embeddings are
generated by using Convolutional Neural Networks(CNN)
which is applied to the characters of each word(Kim, et al.,
2014). This layer maps each token to a high dimensional
vector space and is proved to be helpful in handling out-of-
vocab(OOV) words.

We also use skip-gram model to train the embeddings
of part-of-speech(POS) tags and named-entity recogni-
tion(NER) tags. We ﬁrst transform all of the given train-

Figure 3: The passage and its according transformed “passages”. The ﬁrst row(green) is the original sentence from the passage,
the second row(red) is the name-entity recognition(NER) tags, and the last row(blue) is the part-of-speech(POS) tags.

ing set into their part-of-speech(POS) tags and named-
entity recognition(NER) tags, which can be showed in Fig-
ure 3. Then we employ skip-sram model, which is one of
the core algorithms in the popular off-the-shelf embedding
word2vec(Mikolov et al., 2013), to the transformed “pas-
sage” just like it works in word2vec for the normal passage.
Given a sequence of training words in the transformed pas-
sage: w1, w2, ..., wN , the objective of the skip-gram model
is to maximize the average log probability:

N
(cid:88)

1
N

(cid:88)

n=1

−c≤i≤c,p(cid:54)=0

log p(wn+i|wn)

where c is the size of the context which can be set manu-
ally, a large c means more accurate results and more training
time. The p(wn+i|wn) is deﬁned by:
exp(v(cid:48)T
wO
w=1 exp(v(cid:48)T

vwI )
wvwI )

p(wO|wI ) =

(cid:80)V

where vw and v(cid:48)
V is the vocabulary size.

w are the input and output vector of w, and

We ﬁnally get the ﬁxed length embedding of each tag. Al-
though the number of tags limits the effect of word analogy
inference, it still be very helpful compared to simple one hot
embedding since similar tags have similar surroundings.

t }n

t=1 and {rQ

In the end, we use a BiLSTM to encode both the con-
text and query embeddings and obtain their representations
t }m
{rP
t=1 and the last hidden state of both di-
rections of query representation uQ.
t ; sP
t = BiLSTM([wP
rP
t ; sQ
t = BiLSTM([wQ
rQ

t ; cP
t ; cQ
where w, c, s represent word-level embedding, character-
level embedding and tags embedding respectively. uQ is the
concatenation of both directions’ last hidden state.

t ]), t ∈ [1, ..., m]

t ]), t ∈ [1, ..., n]

Memory Network of Full-Orientation Matching
Attention mechanism is a common way to link and blend
the content between the context and query. Unlike previous
methods that are either two dimensional matching or one
dimensional matching, we propose a full-orientation match-
ing layer that synthesizes both of them and thus combine
the advantages of both side and hedge the weakness. After
concatenating all the attention vectors, we will pass them
into a bi-directional LSTM. We start by describing our
model in the single layer case, which implements a single

memory hop operation. We then show it can be stacked to
give multiple hops in memory.

t }n

t=1, {rQ

Integral Query Matching The input of this step is the
t=1 and uQ. At ﬁrst we ob-
t }m
representations {rP
tain the importance of each word in passage according to the
integral query by means of computing the match between
uQ and each representation rP
t by taking the inner product
followed by a softmax:

ct = softmax((cid:104)uQ, rP

t (cid:105))

Subsequently the ﬁrst matching module is the sum of the

inputs {rP

t }n

t=1 weighted by attention ct:

m1 =

(cid:88)

ctrP
t

t

1 [rP

i ; rQ

Query-Based Similarity Matching We then obtain an
alignment matrix A ∈ Rn×m between the query and con-
i ◦ rQ
text by Aij = wT
j ], w1 is the weight param-
eter, ◦ is elementwise multiplication. Like Seo et al. (2017),
we use this alignment matrix to compute whether the query
words are relevant to each context word. For each context
word, there is an attention weight that represents how much
it is relevant to every query word:

j ; rP

B = softmaxrow(A) ∈ Rn×m

softmaxrow(A) means the softmax function is per-
formed across the row vector, and each attention vector is
M 2
t , which is based on the query embedding.
Hence the second matching module is M 2, where each M 2
t
is the column of M 2.

t = B · rQ

Context-Based Similarity Matching When we consider
the relevance between context and query, the most repre-
sentative word in the query sentence can be chosen by e =
maxrow(A) ∈ Rn, and the attention is d = softmax(e).
Then we obtain the last matching module

(cid:88)

m3 =

rP
t

· dt

t

which is based on the context embedding. We put all of
the memories in a linear function to get the integrated hier-
archical matching module:

M = f (M 1, M 2, M 3)

where f is an simple linear function, M 1 and M 3 are

matrixes that are tiled n times by m1 and m3.

probability ak to the Gated Recurrent Unit(GRU)(Chung et
al., 2014):

Moreover, Wang et al. (2017) add an additional gate to

the input of RNN:

gt = sigmoid(WgM )

M ∗ = gt (cid:12) M
The gate is based on the integration of hierarchical atten-
tion vectors, and it effectively ﬁltrates the part of tokens that
are helpful in understanding the relation between passage
and query. Additionally, we add a bias to improve the esti-
mation:

gt = sigmoid(WgM + b)

Experiments prove that this gate can also accelerate the
speed of convergence. Finally, the integrated memory M is
passed into a bi-directional LSTM, and the output will cap-
tures the interaction among the context words and the query
words:

Ot = BiLSTM(Ot−1, M )

In multiple layers, the integrated hierarchical matching
module M can be regarded as the input {rP
t=1 of next
layer after a dimensionality reduction processing. We call
this memory networks of full-orientation matching.

t }n

Output layer
In this layer, we follow Wang and Jiang (2016) to use the
boundary model of pointer networks (Vinyals et al., 2015)
to locate the answer in the passage. Moreover, we follow
Wang et al. (2017) to initialize the hidden state of the pointer
network by a query-aware representation:

zj = sT tanh(W QrQ

j + bQ)

ai =

(cid:80)m

exp(zi)
j=1 exp(zj)
m
(cid:88)

airQ
i

l0 =

i=1
where sT ,W Q and bQ are parameters, l0 is the initial hidden
state of the pointer network. Then we use the passage repre-
sentation along with the initialized hidden state to predict the
indices that represent the answer’s location in the passage:

j = cT tanh(W P Oj + W hl0)
zk

ak
i =

(cid:80)n

exp(zk
i )
j=1 exp(zk
j )
1, ..., ak
n)

pk = argmax(ak

where W h is parameter, k = 1, 2 that respectively repre-
sent the start point and the end point of the answer, Oj is the
vector that represents j-th word in the passage of the ﬁnal
output of the memory networks. To get the next layer of hid-
den state, we need to pass O weighted by current predicted

n
(cid:88)

vk =

ak
i Oi

i=1
t = GRU(l1
l1

t−1, vk)

For the loss function, we minimize the sum of the negative
probabilities of the true start and end indices by the predicted
distributions.

Experiment

Implementation Settings
The tokenizers we use in the step of preprocessing data are
from Stanford CoreNLP (Manning et al., 2014). We also use
part-of-speech tagger and named-entity recognition tagger
in Stanford CoreNLP utilities to transform the passage and
question. For the skip-gram model, our model refers to the
word2vec module in open source software library, Tensor-
ﬂow, the skip window is set as 2. The dataset we use to train
the embedding of POS tags and NER tags are the training set
given by SQuAD, in which all the sentences are tokenized
and regrouped as a list. To improve the reliability and sta-
bllity, we screen out the sentences whose length are shorter
than 9. We use 100 one dimensional ﬁlters for CNN in the
character level embedding, with width of 5 for each one. We
set the hidden size as 100 for all the LSTM and GRU layers
and apply dropout(Srivastava et al., 2014) between layers
with a dropout ratio as 0.2. We use the AdaDelta (Zeiler,
2012) optimizer with a initial learning rate as 0.001. For the
memory networks, we set the number of layer as 3.

TriviaQA Results
We ﬁrst evaluate our model on a large scale reading com-
prehension dataset TriviaQA version1.0(Joshi et al., 2017).
TriviaQA contains over 650K question-answer-evidence
triples,
that are derived from Web search results and
Wikipedia pages with highly differing levels of information
redundancy. TriviaQA is the ﬁrst dataset where questions are
authored by trivia enthusiasts, independently of the evidence
documents.

Figure 4: The performance of our MEMEN and baselines on
TriviaQA dataset.

Figure 5: The performance of our MEMEN and competing approaches on SQuAD dataset as we submitted our model (May,
22, 2017). * indicates ensemble models.

There are two different metrics to evaluate model accu-
racy: Exact Match(EM) and F1 Score, which measures the
weighted average of the precision and recall rate at character
level. Because the evidence is gathered by an automated pro-
cess, the documents are not guaranteed to contain all facts
needed to answer the question. In addition to distant super-
vision evaluation, we also evaluate models on a veriﬁed sub-
sets. Because the test set is not released, we train our model
on training set and evaluate our model on dev set. As we
can see in Figure 4, our model outperforms all other base-
lines and achieves the state-of-the-art result on all subsets on
TriviaQA.

SQuAD Results

We also use the Stanford Question Answering Dataset
(SQuAD) v1.1 to conduct our experiments. Passages in
the dataset are retrieved from English Wikipedia by means
of Project Nayuki’s Wikipedia’s internal PageRanks. They
sampled 536 articles uniformly at random with a wide range
of topics, from musical celebrities to abstract concepts.The
dataset is partitioned randomly into a training set(80%), a
development set(10%), and a hidden test set(10%). The host
of SQuAD didn’t release the test set to the public, so every-
body has to submit their model and the host will run it on
the test set for them.

Figure 5 shows the performance of our model and com-
peting approaches on the SQuAD. The results of this dataset
are all exhibited on a leaderboard, and top methods are
almost all ensemble models, our model achieves an exact
match score of 75.37% and an F1 score of 82.66%, which is
competitive to state-of-the-art method.

Ensemble Details
The main current ensemble methods in the machine compre-
hension is simply choosing the answer with the highest sum
of conﬁdence scores among several single models which are
exactly identical except the random initial seed. However,
the performance of ensemble model can obviously be bet-
ter if there is some diversity among single models. In our
SQuAD experiment, we get the value of learning rate and
dropout ratio of each model by a gaussian distribution, in
which the mean value are 0.001 and 0.2 respectively.

learning rate ∼ N (0.001, 0.0001)

dropout ∼ N (0.2, 0.05)

To keep the diversity in a reasonable scope, we set the vari-
ance of gaussian distribution as 0.0001 and 0.05 respec-
tively. Finally, we build an ensemble model which consists
of 14 single models with different parameters.

Speed and Efﬁciency
Compared to (Wang et al., 2017), which achieves state-of-
the-art result on the SQuAD test set, our model doesn’t con-
tain the self-matching attention layer which is stuck with
high computational complexity. Our MEMEN was trained
with NVIDIA Titan X GPU, and the training process of
the 3-hops model took roughly 5 hours on a single GPU.
However, an one-hop model took 22 hours when we added
self-matching layer in attention memory. Although the ac-
curacy is improved a little compared to one-hop MEMEN
model, it declined sharply as the number of hops increased,
not to speak of the disadvantage of running time. The reason
might be that multi-hops model with self-matching layer is

too complex to efﬁciently learn the features for this dataset.
As a result, our model is competitive both in accuracy and
efﬁciency.

Hops and Ablations
Figure 6 shows the performance of our single model on
SQuAD dev set with different number of hops in the mem-
ory network. As we can see, both the EM and F1 score in-
crease as the number of hops enlarges until it arrives 3. Af-
ter the model achieves the best performance with 3 hops, the
performance gets worse as the number of hops gets large,
which might result in overﬁtting.

We also run the ablations of our single model on SQuAD
dev set to evaluate the individual contribution. As Figure
7 shows, both syntactic embeddings and semantic embed-
dings contribute towards the model’s performance and the
POS tags seem to be more important. The reason may be
that the number of POS tags is larger than that of NER tags
so the embedding is easier to train. For the full-orientation
matching, we remove each kind of attention vector respec-
tively and the linear function can handle any two of the rest
hierarchical attention vectors. For ablating integral query
matching, the result drops about 2% on both metrics and it
shows that the integral information of query for each word
in passage is crucial. The query-based similarity matching
accounts for about 10% performance degradation, which
proves the effectiveness of alignment context words against
query. For context-based similarity matching, we simply
took out the M 3 from the linear function and it is proved to
be contributory to the performance of full-orientation match-
ing.

Figure 6: Performance comparison among different number
of hops on the SQuAD dev set.

Figure 7: Ablation results on the SQuAD dev set.

Related Work

Machine Reading Comprehension Dataset.
Several benchmark datasets play an important role in
progress of machine comprehension task and question an-
swering research in recent years. MCTest(Richardson et al.,
2013) is one of the famous and high quality datasets. There
are 660 ﬁctional stories and 4 multiple choice questions per
story contained in it, and the labels are all made by hu-
mans. Researchers also released cloze-style datasets(Hill et
al., 2015; Hermann et al., 2015; Onishi et al., 2016; Paperno
et al., 2016). However, these datasets are either not large
enough to support deep neural network models or too easy
to challenge natural language.

Recently, Rajpurkar et al. (2016) released the Stanford
Question Answering dataset (SQuAD), which is almost two
orders of magnitude larger than all previous hand-annotated
datasets. Moreover, this dataset consists 100,000+ questions
posed by crowdworkers on a set of Wikipedia articles, where
the answer to each question is a segment of text from the
corresponding passage, rather than a limited set of multi-
ple choices or entities. TriviaQA (Joshi et al., 2017) is also
a large and high quality dataset, and the crucial difference
between TriviaQA and SQuAD is that TriviaQA questions
have not been crowdsourced from pre-selected passages.

Attention Based Models for Machine Reading
Many works are based on the task of machine reading com-
prehension, and attention mechanism have been particularly
successful(Xiong et al., 2016; Cui et al., 2016; Wang et al.,
2016; Seo et al., 2016; Hu et al., 2017; Shen et al., 2016;
Wang et al., 2017). Xiong et al. (2016) present a coattention
encoder and dynamic decoder to locate the answer. Cui et al.
(2016) propose a two side attention mechanism to compute
the matching between the passage and query. Wang et al.
(2016) match the passage and query from several perspec-
tives and predict the answer by globally normalizing proba-
bility distributions. Seo et al. (2016) propose a bi-directional
attention ﬂow to achieve a query-aware context representa-
tion. Hu et al. (2017) propose self-aware representation and
multi-hop query-sensitive pointer to predict the answer span.
Shen et al. (2016) propose iterarively inferring the answer
with a dynamic number of steps trained with reinforcement
learning. Wang et al. (2017) employ gated self-matching
attention to obtain the relation between the question and
passage. Our MEMEN construct a hierarchical orientation
attention mechanism to get a wider match while applying
memory network(Sukhbaatar et al., 2015) for deeper under-
stand.

Conclusion
In this paper, we introduce MEMEN for Machine compre-
hension style question answering. We propose the multi-
layer embedding to encode the document and the memory
network of full-orientation matching to obtain the interac-
tion of the context and query. The experimental evaluation
shows that our model achieves the state-of-the-art result on
TriviaQA dataset and competitive result in SQuAD. More-
over, the ablations and hops analysis demonstrate the im-

portance of every part of the hierarchical attention vectors
and the beneﬁt of multi-hops in memory network. For fu-
ture work, we will focus on question generative method and
sentence ranking in machine reading tasks.

References
Danqi Chen, Jason Bolton, and Christopher D Manning. A
thorough examination of the cnn/daily mail reading com-
prehension task. arXiv preprint arXiv:1606.02858, 2016.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting
Liu, and Guoping Hu. Attention-over-attention neural
arXiv preprint
networks for reading comprehension.
arXiv:1607.04423, 2016.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W
Gated-attention
arXiv preprint

Cohen, and Ruslan Salakhutdinov.
readers
arXiv:1606.01549, 2016.

text comprehension.

for

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and
Phil Blunsom. Teaching machines to read and compre-
hend. In Advances in Neural Information Processing Sys-
tems, pages 1693–1701, 2015.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason We-
ston. The goldilocks principle: Reading children’s books
arXiv preprint
with explicit memory representations.
arXiv:1511.02301, 2015.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. Mnemonic
arXiv preprint

reader for machine comprehension.
arXiv:1705.02798, 2017.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettle-
moyer. Triviaqa: A large scale distantly supervised chal-
lenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551, 2017.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. Text understanding with the attention sum
reader network. arXiv preprint arXiv:1603.01547, 2016.

Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Nyberg.
Structural embedding of syntactic trees for machine com-
prehension. arXiv preprint arXiv:1703.00572, 2017.

Tomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-
nock`y, and Sanjeev Khudanpur. Recurrent neural network
based language model. In Interspeech, volume 2, page 3,
2010.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. Distributed representations of words and
phrases and their compositionality. In Advances in neural
information processing systems, pages 3111–3119, 2013.

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. Who did what: A large-
arXiv preprint
scale person-centered cloze dataset.
arXiv:1608.05457, 2016.

Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou,
Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fern´andez.

The lambada dataset: Word prediction requiring a broad
arXiv preprint arXiv:1606.06031,
discourse context.
2016.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. Glove: Global vectors for word representation.
In Empirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543, 2014.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. Squad: 100,000+ questions for machine
comprehension of text. arXiv preprint arXiv:1606.05250,
2016.

Matthew Richardson, Christopher JC Burges, and Erin Ren-
shaw. Mctest: A challenge dataset for the open-domain
In EMNLP, volume 3,
machine comprehension of text.
page 4, 2013.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Han-
naneh Hajishirzi. Bidirectional attention ﬂow for machine
comprehension. arXiv preprint arXiv:1611.01603, 2016.
Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu
Chen. Reasonet: Learning to stop reading in machine
comprehension. arXiv preprint arXiv:1609.05284, 2016.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research, 15(1):1929–1958, 2014.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
End-to-end memory networks. In Advances in neural in-
formation processing systems, pages 2440–2448, 2015.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer
networks. In Advances in Neural Information Processing
Systems, pages 2692–2700, 2015.

Shuohang Wang and Jing Jiang. Machine comprehension
arXiv preprint

using match-lstm and answer pointer.
arXiv:1608.07905, 2016.

Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Florian.
Multi-perspective context matching for machine compre-
hension. arXiv preprint arXiv:1612.04211, 2016.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and
Ming Zhou. Gated self-matching networks for reading
comprehension and question answering. In Proceedings
of the 55th Annual Meeting of the Association for Com-
putational Linguistics, 2017.

Caiming Xiong, Victor Zhong, and Richard Socher. Dy-
namic coattention networks for question answering. arXiv
preprint arXiv:1611.01604, 2016.

Matthew D Zeiler. Adadelta: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

