0
2
0
2
 
r
p
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
0
5
6
0
.
4
0
0
2
:
v
i
X
r
a

Unsupervised Multimodal Video-to-Video
Translation via Self-Supervised Learning

Kangning Liu1,2(cid:63), Shuhang Gu2(cid:63), Andr´es Romero2, and Radu Timofte2

1 Center for Data Science, New York University, USA
2 Computer Vision Lab, ETH Z¨urich, Switzerland

Abstract. Existing unsupervised video-to-video translation methods fail
to produce translated videos which are frame-wise realistic, semantic in-
formation preserving and video-level consistent. In this work, we pro-
pose UVIT, a novel unsupervised video-to-video translation model. Our
model decomposes the style and the content, uses the specialized encoder-
decoder structure and propagates the inter-frame information through
bidirectional recurrent neural network (RNN) units. The style-content
decomposition mechanism enables us to achieve style consistent video
translation results as well as provides us with a good interface for modal-
ity ﬂexible translation. In addition, by changing the input frames and
style codes incorporated in our translation, we propose a video interpo-
lation loss, which captures temporal information within the sequence to
train our building blocks in a self-supervised manner. Our model can
produce photo-realistic, spatio-temporal consistent translated videos in
a multimodal way. Subjective and objective experimental results validate
the superiority of our model over existing methods. More details can be
found on our project website: https://uvit.netlify.com/.

1

Introduction

Recent image-to-image translation (I2I) methods have achieved astonishing re-
sults by employing Generative Adversarial Networks (GANs) [18].

While there is an explosion of papers on I2I, its video counterpart is much less
explored. Nevertheless, the ability to synthesize dynamic visual representations
is important to a wide range of tasks such as video colorization [44], medical
imaging [32], model-based reinforcement learning [5,21], computer graphics ren-
dering [27], etc.

Compared with the I2I task, the video-to-video translation (V2V) is more
challenging. Besides the frame-wise realistic and semantic preserving require-
ments, which are also required in the I2I task, V2V methods additionally need to
consider the temporal consistency for generating sequence-wise realistic videos.
Consequently, directly applying I2I methods on each frame of the video is not
an optimal solution because I2I cross-domain mapping does not hold temporal
consistency within the sequence.

(cid:63) Equal contributions

2

Liu et al.

Fig. 1. A consistent video should be 1) style inconsistent 2) content consis-
tent First row: label inputs; Second row: ReCycleGAN[6] outputs; Third row: UVIT
(ours) outputs. To overcome the style shift (e.g. sunset frame gradually changes to rain
frame), we utilize style-conditioned translation. To reduce artifacts across frames, our
translator incorporate multi-frame information. We use systematic sampling to get the
results from a 64-frame sequence. The full video is provided in supplementary material

Recent methods [6,7,12] have included diﬀerent constraints to model the tem-
poral consistency based on the Cycle-GAN approach [45] for unpaired datasets.
They either use a 3D spatio-temporal translator [7] or add a temporal loss on
traditional image-level translator [6,12]. However, 3DCycleGAN [7] heavily sac-
riﬁces image-level quality, and RecycleGAN [6] suﬀers from style shift and inter-
frame random noise.

In this paper we propose Unsupervised Multimodal Video-to-Video Trans-
lation via Self-Supervised Learning (UVIT), a novel framework for video-to-
video cross-domain mapping. To this end, a temporally consistent video sequence
translation should simultaneously guarantee: (1) Style consistency, and (2) Con-
tent consistency, see Figure 1 for a visual example. Style consistency requires the
whole video sequence to have the same style, thus ensuring the video frames to
be overall realistic. Meanwhile, content consistency refers to the appearance con-
tinuity of contents in adjacent video frames, which ensures the video sequence
to be dynamically vivid.

In UVIT, by assuming that all domains share the same underlying structure,
namely content space, we exploit the style-conditioned translation. To simultane-
ously impose style and content consistency, we adopt an Encoder-RNN-Decoder
architecture as the video translator, see Figure 2 for an illustration of the pro-
posed framework. There are two key ingredients in our framework:

Conditional video translation: By applying the same style code to decode
the content feature for a speciﬁc translated video, the translated video is style
consistent. Besides, by changing the style code across videos, we achieve sub-
domain3 and modality ﬂexible video translation, see Figure 3 for an illustration
of subdomains (columns) and modalities (rows). This overcomes the limitations
of existing CycleGAN-based video translation techniques, i.e. performing deter-
ministic translations (generator as an injective function).

3 Hereafter we call it subdomain and not domain because a subdomain must belong
to a subset of a domain (for instance, subdomains of day, night, snow, etc. belong
to the scene video domain)

UVIT

3

Fig. 2. Overview of our proposed UVIT model: given an input video sequence, we ﬁrst
decompose it to the content by a Content Encoder and the style by a Style Encoder.
Then the content is processed by special RNN units, namely TrajGRUs [37] in or-
der to get the content used for translation and interpolation in a recurrent manner.
Finally, the translation content and the interpolation content are decoded to the trans-
lated video and the interpolated video together with the style latent variable. We also
show the video adversarial loss (orange), the cycle consistency loss (violet), the video
interpolation loss (green) and the style encoder loss (blue)

Fig. 3. Our proposed UVIT model can produce photo-realistic, spatio-temporal con-
sistent translated videos in a multimodal way for multiple subdomains

Consistent video translation: Building inter-frame dependency is essen-
tial for generating dynamically vivid videos. Existing video translators utilize
optical ﬂow or 3D Convolutional Neural Networks (CNNs), which are incapable
to fully capture the complex relationships between multiple video frames. We
adopts a RNN-based translator to incorporate inter-frame and current frame in-
formation in the high-dimensional hidden space from more frames. As validated
by Bojanowski et al. on image generation task [8], integrating features in hidden
space is beneﬁcial to produce semantically meaningful, smooth nonlinear inter-
polation in image space. Lacking paired supervision, another crucial aspect in
unsupervised video translation lies in training criterion. Besides GAN [18] loss
and spatial cycle consistency [45] loss, we propose video interpolation loss as
the temporal constraint to strengthen semantic preserving. Speciﬁcally, we use
translator building blocks to interpolate the current frame according to inter-
frame information produced during translation. The current frame is then used
as self-supervised target to tailor the inter-frame information. Meanwhile, it is
validated that introducing self-supervision task is beneﬁcial for cross-domain
unsupervised tasks [38,10,34]. Such self-supervision is applied to all building
blocks of the translator, which stabilizes the challenging unpaired video adver-
sarial learning. Eﬀectiveness of this loss is validated through ablation study.

4

Liu et al.

The main contributions of our paper are summarized as follows:

1. We introduce an Encoder-RNN-Decoder video translator, which decomposes
the temporal consistency into independent style and content consistencies for
more stable consistent video translation.

2. Our style-conditioned decoder ensures style consistency as well as facilitates

multimodal and multi-subdomain V2V translation.

3. We use self-supervised learning to incorporate an innovative video interpo-
lation loss which preserves inter-frame information according to the current
frame. The combined translation frame is more semantically preserved w.r.t
the corresponding input frame. Therefore, our RNN-based translator can
recurrently generate dynamically vivid and photo-realistic video frames.

2 Related Work

Image-to-Image Translation. Most of the GAN-based I2I methods mainly
focus on the case where paired data exists [25,46,40]. However, with the cycle-
consistency loss introduced in CycleGAN [45], promising performance has been
achieved also for the unsupervised I2I [24,2,29,31,36,17,14,42,13,41,1]. The con-
ditional distribution of the translated pictures on the input pictures is quite
likely to be multimodal (e.g. from a semantic label to diﬀerent images in a ﬁxed
weather condition). However, traditional I2I problem often lacks this character-
istic and produces an unimodal outcome. Zhu et al . [46] proposed Bicycle-GAN
that can output diverse translations in a supervised manner. There are also some
extensions [24,2,28] of CycleGAN to decompose the style and content so that the
output can be multimodal in the unsupervised scenario. Our work goes in this
direction, and under the assumption that close frames within the same domain
share the same style, we adopt the style control strategy in the image domain
proposed by Almahairi et al . [2] to the video domain.

Video-to-Video Translation In the seminal work, Wang et al . [39] (vid2vid)
combined the optical ﬂow and video-speciﬁc constraints and proposed a general
solution for V2V in a supervised way, which achieves long-term high-resolution
video sequences. However, vid2vid relies heavily on labeled data which makes it
diﬃcult to scale in unsupervised real-world scenarios. As our approach exploits
the unsupervised V2V representation, it is the focus of our document.

Based on the I2I CycleGAN approach, recent methods [7,6,12] on unsuper-
vised V2V proposed to design spatio-temporal loss to achieve more temporally
consistent results while preserving semantic information. Bashkirova et al . [7]
proposed a 3DCycleGAN method which adopts 3D convolutions in the genera-
tor and discriminator of the CycleGAN framework to capture temporal informa-
tion. However, since the small 3D convolution operator (with a small temporal
dimension 3) only captures dependency between adjacent frames. 3DCycleGAN
therefore can not exploit temporal information for generating longer style con-
sistent video sequences. Furthermore, the 3D discriminator is also limited in
capturing complex temporal relationships between video frames. As a result,

UVIT

5

when the gap between input and target domain is large, 3DCycleGAN tends to
sacriﬁce the image-level quality and generates blurry and gray translations.

Additionally, Bansal et al . [6] designed a recycle loss (ReCycleGAN) for
jointly modeling the spatio-temporal relationship between video frames and thus
solving the semantic preserving problem. They trained a temporal predictor to
predict the next frame based on two past frames, and plugged the temporal
predictor in the cycle-loss to impose the spatio-temporal constraint on the tra-
ditional image-level translator. Although ReCycleGAN succeeds in V2V trans-
lation scenarios such as face-to-face or ﬂower-to-ﬂower, similar to CycleGAN, it
lacks domain generalization as the translation fails to be consistent in domains
with a large gap with respect to the input. We argue that there are two major
reasons that aﬀect ReCycleGAN performance in complex scenarios. First, the
translator is a traditional image-level translator without the ability to record the
inter-frame information within videos. It processes input frames independently,
which has limited capacity in exploiting temporal information, being not con-
tent consistent enough. Second, ReCycleGAN temporal predictor only imposes
the temporal constraint between a few adjacent frames, the generated video
content still might shift abnormally: a sunny scene could change to a snowy
scene in the following frames. Note that Chen et al . [12] incorporate optical ﬂow
to add motion cycle consistency and motion translation constraints. However,
their Motion-guided CycleGAN still suﬀers from the same two limitations as in
ReCycleGAN.

In summary, previous methods fail to produce style consistent and mul-
timodal video sequences. Besides, they lack the ability to achieve translation
which is both content consistent enough and frame-wise realistic. In this paper,
we propose UVIT, a novel method for Unsupervised Multimodal Video-to-Video
Translation via Self-Supervised Learning, which produces high-quality semantic
preserving frames with consistency within the video sequence. Besides, to the
best of our knowledge, our method is the ﬁrst method that jointly addresses
multiple-subdomains and multimodality in V2V cross-domain translations.

3 Unsupervised Multimodal VIdeo-to-video Translation

via Self-Supervised Learning (UVIT)

3.1 Problem setting

Let A be the video domain A, a1:T = {a1, a2, ..., aT } be a sequence of video
frames in A, let B be the video domain B, b1:T = {b1, b2, ..., bT } be a sequence of
video frames in B. For example, they can be sequences of semantic segmentation
labels or scene images. Our general goal of unsupervised video-to-video transla-
tion is to train a generator to convert videos between domain A and domain B
with many-to-many mappings. Either domain A or domain B can have multiple
subdomains (sunny, snow, rain for the case of weather conditions). More con-
cretely, to generate the style consistent video sequence, we assume each video
frame has a shared style latent variable z. Let za ∈ ZA and zb ∈ ZB be the style
latent variables in domain A and B, respectively.

6

Liu et al.

We aim to achieve two conditional video translation mappings: Gtrans

AB : A ×
ZB (cid:55)→ Btrans and Gtrans
BA : B × ZA (cid:55)→ Atrans. As we propose to use the video
interpolation loss to train the translator components in a self-supervised manner,
we also deﬁne the video interpolation mappings: Ginterp
: A × ZA (cid:55)→ Ainterp and
Ginterp
: B × ZB (cid:55)→ Binterp. Interpolation and translation mappings use exactly
B
the same building blocks.

A

3.2 Translation and Interpolation pipeline

In this work, inspired by UNIT [29], we assume a shared content space such that
corresponding frames in two domains are mapped to the same latent content
representation. We show the translation and interpolation processes in Figure 4.
To achieve the goal of unsupervised video-to-video translation, we propose an
Encoder-RNN-Decoder translator which contains the following components:

– Two content encoders (CEA and CEB), which extract the frame-wise content

information from each domain to the common spatial content space.

– Two style encoders (SEA and SEB), which encode video frames to the re-

spective style domains.

– Two Trajectory Gated Recurrent Units (TrajGRUs)

[37] to form a Bi-
TrajGRU (T), which propagates the inter-frame content information bidi-
rectionally. TrajGRU [37] is one variant of Convolutional RNN (Recurrent
Neural Network) [43], which can actively learn the location-variant structure
in the video data. More details in supplementary material.

– One merge module (M), which adaptively combines the inter-frame content

from two directions.

– Two conditional content decoders (CDA and CDB), which take the spatio-
temporal content information and the style code to generate the output
frame. If needed, it also takes the conditional subdomain information as an
one hot vector encoding.

Video translation: Given an input frame sequence (..., at−1, at, at+1, ...), we
extract the posterior style (zpost
) from the ﬁrst frame (a1) with a style encoder
(SEA). Additionally, we extract each content representation (..., ct−1, ct, ct+1, ...)
with the content encoder (CEA).

a

Translation is conducted in a recurrent way. To get the translation result
btrans
for time t , we process the independent content representation: (1) propa-
t
gate content for the surrounding frames (..., ct−1, ct+1, ...) through Bi-TrajGRU
(T) to obtain the inter-frame content information. (2) update this informa-
tion with the current frame content (ct) (see Figure 4 left, Merge Module M)
to get the spatio-temporal content (ctrans
) for translation. At last, using the
t
same style-conditioned strategy as Augment CycleGAN [2,16,33], the content
decoder (CDB) takes the prior style information (zprior
) as the condition and
) = btrans
utilizes ctrans
).
t
This process is repeated until we get the whole translated sequence (...,btrans
t−1 ,
btrans
t

to generate the translation result (CDB(ctrans

t+1 ,...).

, zprior
b

,btrans

b

t

t

UVIT

7

Fig. 4. Video translation (left) and video interpolation (right): two processes share
modules organically. The input latent content is processed by the Merge Module to
merge information from TrajGRUs in both the forward and the backward direction.
) is obtained by updating interpolation content (cinterp
The translation content (ctrans
)
with the content (ct) from the current frame (at)

t

t

Style code is induced as the condition of (AdaIN-based [23]) content decoder.
If a domain (e.g. scene images) is presorted, we have prior information on which
subset (rain, night, etc.) a video belongs to, so we can take such prior information
as a subdomain (subset) label to achieve deterministic control for the style.
Within each subset, there are still diﬀerent modalities (e.g. overcast day, sunny
day in day subdomain), yet we do not have prior access to it. This modality
information is therefore learned by style encoder. Subdomain label (taken as
one-hot vector if available) and modality information together constitute 21-
dimensional style code. Style consistency is ensured by sharing style code among
a speciﬁc video sequence. Multimodal translation is realized by inducing diﬀerent
style codes across videos. When subdomain information is unavailable, simply
using style encoder to learn subdomain styles as modalities, we can still generate
multimodal style consistent results in a stochastic way.

Video interpolation: In video translation process in Figure 4, when translating
a speciﬁc frame (at), the translation content (ctrans
) is integrated by the current
frame content (ct) and inter-frame information (cinterp
) from the surrounding
t
frames (..., at−1, at+1, ...). The inter-frame content information helps to build up
the dependency between each frame and its surrounding frames, ensuring content
consistency across frames. However, if cinterp
and ct are not aligned well, image-
level quality can be aﬀected. The translated frame (btrans
) will incline to over
smooth image-level details and sacriﬁce high-level semantic correspondence with
at. Tailoring inter-frame information is thus of pivotal importance.

t

t

t

t

Thanks to the ﬂexible Encoder-Decoder structure, our decoder can generate
interpolated frame (ainterp
) from cinterp
. Video interpolation loss is proposed
t
to compute the L1-Norm distance between interpolated frame(ainterp
) and the
current frame(at), which adds supervision to the inter-frame content(cinterp
).
cinterp
can thus be combined with ct more organically. Therefore, the transla-
t
tion task directly beneﬁts from the interpolation task, producing more semantic
preserving and photo-realistic outcomes.

t

t

8

Liu et al.

Meanwhile, such self-supervised training would be beneﬁcial to make the
network more stable in the challenging unpaired video adversarial learning [38].
GANs are powerful methods to learn a data probability distribution with no
supervision, yet training GANs is well known for being delicate, unstable [3,30,4]
and easy to suﬀer from mode collapse[6]. Besides cycle loss acting as spatial
constraint, we introduce the video interpolation loss as a temporal constraint
for GAN training in a self-supervised way. It have been validated that bringing
self-supervision is beneﬁcial for cross-domain unsupervised tasks (e.g., natural
image synthesis) [38,10,34].

Furthermore, our framework aims to learn latent representation for style and
content, while it has been empirically observed [20,11] that it is non-trivial to
use latent variables when coupled with a strong autoregressive decoder (e.g.,
RNN). Goyal et al . [19] found that auxiliary cost could ease training of the
latent variables in RNN-based generative latent variable models. Therefore, the
video interpolation task provides the latent variables with a auxiliary objective
that enhances the performance of the overall model.

Note that the proposed temporal loss highly diﬀers from the previous ReCy-
cleGAN loss [6] as: (1) we use a RNN-based architecture that captures temporal
information better in a high-level feature space, (2) interpolation is conducted
within the translator building blocks rather than using diﬀerent modules, train-
ing the translator with direct self-supervision, (3) the translator directly utilizes
tailored inter-frame information for better semantic preserving translations.

3.3 Loss functions

We use the Relativistic GAN (RGAN) [26] and the least square [30] version for
the adversarial loss. RGAN estimates the probability that the given real data is
more realistic than a randomly sampled fake data.

We use image-level discriminators (Dimg

x ) discrimina-
tors to ensure that output frames resemble a real video clip in both video-level
and image-level. Moreover, we also add style discriminators (DZx ) to adopt an
adversarial approach for training style encoders.

) and video-level (Dvid

x

Video adversarial loss. The translated video frames aim to be realistic com-
pared to the real samples in the target domain for both an image-level and a
video-level basis.

Ladv

B =

[Dimg

B (btrans

i

)−Dimg

B (bi)−1]2+[Dvid

B (btrans
1:T

)−Dvid

B (b1:T )−1]2, (1)

1
T

i=T
(cid:88)

i=1

1:T

where, btrans
discriminator for domain B , Dvid
Adversarial loss for domain A (Ladv

are the translated frames from time 1 to T . Dimg

B is the image-level
B is the video-level discriminator for domain B.

A ) is deﬁned similarly.

Video interpolation loss. The interpolated video frames should be close to
the ground truth frames (pixel-wise loss). Additionally, they aim to be realistic

UVIT

9

compared to other real frames within the domain (adversarial loss).

Linterp
A

=

1
(T −2)

(λinterp(cid:107) a2:T −1−ainterp

2:T −1 (cid:107)1 +

[Dimg

A (ainterp

i

)−Dimg

A (ai)−1]2).

i=T −1
(cid:88)

i=2

Since we are using bidirectional TrajGRUs, we use frames from time 2 to T − 1
to compute the video interpolation loss. ainterp
2:t−1 are the interpolated frames. The
ﬁrst part of the loss is the supervised pixel-wise L1 loss, and the later part is
the GAN loss computed on the image-level discriminator Dimg
A . λinterp is used
to control the weight between two loss elements.
Cycle consistency loss. In order to ensure semantic consistency in an unpaired
setting, we use a cycle-consistency loss:

Lcycle

A =

λcycle
T

(cid:107) a1:T − arec

1:T (cid:107)1,

(2)

where arec
1:T are the reconstructed frames of domain A from time 1 to T , i.e.
1:T = Gtrans
BA (btrans
arec
is the posterior style variable produced
by using the style encoder to encode a1. λcycle is the cycle consistency loss weight.
Style encoder loss. To train the style encoder, the style reconstruction loss
and style adversarial loss are deﬁne in a similar way as Augment CycleGAN [2]:

). Where zpost

, zpost
a

1:T

a

Lstyle
ZA

= λrec (cid:107) zrec

a − zprior

a

(cid:107)1 +[DZA(zpost

a

) − DZA(zprior

a

) − 1]2.

(3)

Here, zprior
a
distribution. zrec
the style encoder to encode atrans

is the prior style latent variable of domain A drawn from the prior
is the reconstructed style latent variable of domain A by using
. λrec is the style reconstruction loss weight.

a

1

Therefore, the objective for the generator is:

Ltotal

G = Ladv

A + Ladv

B + Linterp

A

+ Linterp
B

+ Lcycle

A + Lcycle

B + Lstyle
ZA

+ Lstyle
ZB

(4)

Detailed λ values and loss functions for discriminators are attached in the supple-
mentary material. Detailed training algorithm for RGANs can be found in [26].

4 Experiments

We validate our method using two common yet challenging datasets: Viper [35],
and Cityscapes [15] datasets. In order to feed more frames within limited single
GPU resource, we use the image with 128 × 128 and 10 frames per batch for the
main experiments. During inference, we use video sequences of 30 frames. These
30 frames are divided into 4 smaller sub-sequences of 10 frames with overlap.
They all share the same style code to be style consistent. Note that our model
can be easily extended to process longer style-consistent video sequences by
keeping sharing the same style code for the sub-sequences. The video example
of longer style consistent video is provided in supplementary material, where
detailed description of the dataset and implementation are also attached.

10

Liu et al.

Table 1. Image-to-Label (Semantic segmentation) quantitative evaluation.
We validate UVIT without video interpolation loss (wo/vi) under Mean Intersection
over Union (mIoU), Average Class Accuracy (AC) and Pixel Accuracy (PA) scores

Criterion Model

Day Sunset Rain Snow Night

mIoU ↑

UVIT wo/vi 10.14 10.70 11.06 10.30 9.06
13.71 13.89 14.34 13.23 10.10

UVIT

AC ↑

PA ↑

UVIT wo/vi 15.07 15.78 15.46 15.01 13.06
18.74 19.13 18.98 17.81 13.99

UVIT

UVIT wo/vi 56.33 57.16 58.76 55.45 55.19
68.06 66.35 67.21 65.49 58.97

UVIT

Table 2. Label-to-image quantitative evaluation. We validate our system without
video interpolation loss (wo/vi) under the Fr´echet Inception Distance (FID) score

Criterion Model

Day Sunset Rain Snow Night

FID ↓

UVIT wo/vi 26.95 23.04 30.48 34.62 47.50
17.32 16.79 19.52 18.91 19.93

UVIT

4.1 Ablation Study

In order to demonstrate the contribution of our method, we ﬁrst conduct ablation
study experiments. We provide quantitative and qualitative experimental results
that evidence the proposed video interpolation loss for a better V2V translation.
Besides, we study how the number of frames inﬂuence the semantic preserving
performance. We also provide multimodal consistent results of our model trained
without using subdomain label in the supplementary material.

Video interpolation loss. We provide ablation experiments to show the ef-
fectiveness of the proposed video interpolation loss. We conduct experiments on
both the image-to-label and the label-to-image tasks. We denote UVIT trained
without video interpolation loss as ”UVIT wo/vi”.

We follow the experimental setting of ReCycleGAN [6] and use semantic
segmentation metrics to quantitatively evaluate the image-to-label results. The
Mean Intersection over Union (mIoU), Average Class Accuracy (AC) and Pixel
Accuracy (PA) scores for ablation experiments are reported in Table 1. Our
model with video interpolation loss achieves the best performance across subdo-
mains, which conﬁrms that the video interpolation helps to preserve the semantic
information between the translated frame and the corresponding input frame.

For the label-to-image task, we use the Fr´echet Inception Distance (FID) [22]
to evaluate the feature distribution distance between translated videos and ground-
truth videos. Similar to vid2vid [39], we use the pre-trained network (I3D [9]) to
extract features from videos. We extract the semantic labels from the respective
sub-domains to generate videos and evaluate the FID score on all the subdomains
of the Viper dataset. Table 2 shows the FID score for UVIT and the correspond-
ing ablation experiment. On both the image-to-label and label-to-image tasks,

Table 3. Quantitative results of UVIT with diﬀerent number of frames per
batch in training on the image-to-label (Semantic segmentation) task. With
the increase of input frames number in the sub-sequence, our RNN-based translator
can utilize the temporal information better, resulting in better semantic preserving

UVIT

11

Criterion Frame number Day Sunset Rain Snow Night All

mIoU↑

4
6
8
10

11.84 11.91 12.35 11.37 8.49 11.19
12.29 12.66 13.03 11.77 9.79 11.94
13.05 13.21 14.23 13.07 11.00 12.87
13.71 13.89 14.34 13.23 10.10 13.07

the proposed video interpolation loss plays a crucial role for UVIT to achieve
good translation results.
Diﬀerent number of input frame. Our RNN-based translator incorporates
temporal information from multiple frames. We also investigate the inﬂuence of
frame number on the performance of our model. As shown in Table 3 UVIT can
achieve better semantic preserving with more frames feeding during training as
the RNNs are better trained to leverage the temporal information. Speciﬁcally,
for the image-to-label translation, with the increase of the number from 4 to 10,
the overall mIoU increase from 11.19 to 13.07. Complete table and analysis are
attached in supplementary material.

4.2 Comparison of UVIT with State-of-the-Art Methods

Image-to-label mapping. To further ensure reproducibility, we use the same
setting as our ablation study to compare UVIT with ReCycleGAN [6] in the
image-to-label mapping task. We report the mIoU, AC and PA metrics by the
proposed approach and competing methods in Table 4. The results clearly val-
idate the advantage of our method over the competing approaches in terms of
preserving semantic information. Our model can eﬀectively leverage the inter-
frame information from more frames in a direct way, which utilizes the temporal
information better than the indirect way in ReCycleGAN [6].
Label-to-image mapping. In this setting, we compare the quality of the trans-
lated video sequence by diﬀerent methods. We ﬁrst report the FID score [22] on
all the sub-domains of the Viper dataset in the same setting as our ablation ex-
periments. As the original ReCycleGAN output video sequences can not ensure
style consistency, just as shown in Figure 1, we also report the results achieved
by our improved version of the ReCycleGAN for a fair comparison. Concretely,
we develop a conditional version which formally controls the style of generated
video sequences in a similar way as our UVIT model, and denote the condi-
tional version as improved ReCycleGAN. The FID results by diﬀerent methods
are shown in Table 5. The proposed UVIT achieves better FID on all the 5
sub-domains, which validates the eﬀectiveness of our model in achieving better
visual quality and temporal consistency. Combining Table 2 and Table 5, there
is another observation – the UVIT w/o vi-loss could not dominate the Improved

12

Liu et al.

Table 4. Quantitative comparison between UVIT and baseline approaches
on the image-to-label (Semantic segmentation) task. Our translator eﬀectively
leverage the temporal information directly, thus producing more semantic persevering
translation outcomes

Criterion

Model

Day Sunset Rain Snow Night All

mIoU↑

Cycle-GAN

3.05
ReCycleGAN (Reproduced)1 10.31 11.18 11.26 9.81
ReCycleGAN (Reported)2
13.20 10.10 9.60
UVIT (Ours)

7.76
4.10
7.74 10.11
8.50
8.90
3.10
13.71 13.89 14.34 13.23 10.10 13.07

3.82

3.39

3.02

Cycle-GAN

7.53 11.12 8.55
ReCycleGAN (Reproduced)1 15.78 15.80 15.95 15.56 11.46 14.84
ReCycleGAN (Reported)2
12.60 13.20 10.10 13.30 5.90 12.40
18.74 19.13 18.98 17.81 13.99 17.59
UVIT (Ours)

7.91

8.56

7.83

Cycle-GAN

15.46 16.34 12.83 13.20 49.03 19.59
ReCycleGAN (Reproduced)1 54.68 55.91 57.72 50.84 49.10 53.65
ReCycleGAN (Reported)2
48.70 70.00 60.10 58.90 33.70 53.70
68.06 66.35 67.21 65.49 58.97 65.20
UVIT

AC↑

PA↑

Table 5. Quantitative comparison between UVIT and baseline approaches
on the label-to-image task. Better FID indicates that our translation has better
visual quality and temporal consistency

Criterion

Model

Day Sunset Rain Snow Night

FID↓

ReCycleGAN [6]

23.60 24.45 28.54 31.58 35.74
Improved ReCycleGAN 20.39 21.32 25.67 21.44 21.45
17.32 16.79 19.52 18.91 19.93

UVIT (ours)

ReCycleGAN in terms of FID. This shows that the video interpolation loss is
crucial for the superiority of our spatio-temporal translator.

To thoroughly evaluate the visual quality of the video translation results,
we conduct a subjective evaluation on the Amazon Mechanical Turk (AMT)
platform. The detailed information of conducting this subjective test is provided
in the supplementary material. We compare the proposed UVIT with 3DCy-
cleGAN and ReCycleGAN. The video-level and image-level human preference
scores (HPS) are reported in Table 6. For reference, we also compare the video-
level quality between UVIT and the supervised vid2vid model [39]. Meanwhile,
image-level quality comparison between UVIT and CycleGAN (the image trans-
lation baseline) is also included. Table 6 clearly demonstrates the eﬀectiveness
of our proposed UVIT model. In the video-level comparison, our unsupervised
UVIT model outperforms the competing unsupervised ReCycleGAN and 3DCy-
cleGAN by a large margin, and achieves comparable results with the supervised
benchmark. In the image-level comparison, UVIT achieves better HPS than both

1 The result is reproduced by us. The output would be in a resolution of 256 × 256,

we then downscale it to 128 × 128 to compute the statistics.

2 This is the result reported in the original paper [6] with a resolution of 256 × 256.

Table 6. Label-to-image Human Preference Score. Our method outperforms all
the competing unsupervised methods in both video-level and image-level reality. Note
that we achieve comparable performance with vid2vid although it is supervised

UVIT

13

Human Preference Score

Video level Image level

UVIT (ours) / Improved ReCycleGAN 0.67 / 0.33 0.66 / 0.34
0.75 / 0.25 0.70 / 0.30
0.49 / 0.51
–

UVIT (ours) / 3DCycleGAN [7]
UVIT (ours) / vid2vid [39]
UVIT (ours) / CycleGAN [45]

–
0.61 / 0.39

the V2V competing approaches and the image-to-image baseline. Qualitative
examples in Figure 5 also show that UVIT model produces a more content con-
sistent video sequence. It could not be achieved by simply introducing the style
control without the specialized network structure to record the inter-frame in-
formation. For a better comparison, we include several examples of generated
videos in the supplementary material.

Fig. 5. Label-to-image qualitative comparison. Top left: label inputs; Top right:
improved ReCycleGAN outputs; Bottom left: UVIT outputs. Bottom right: ground
truth. Video examples can be found in supplementary material

4.3 More experimental results

Fig. 6. Viper Sunset-and-Day Top left: input Sunset video; Top right: input Day
video; Bottom left: translated Day video; Bottom right: translated Sunset video

High resolution results. To get a higher resolution and show more details
within the existing GPU constraint, we also train our model using images of
256 × 256 and 4 frames per batch, then test with longer sequence, which is
divided into subsequences of 4 frames with overlap. A visual example is shown
in Figure 1. More results and videos are provided in supplementary material.
Translation on other datasets. Besides translating video sequences between
image and semantic label domains, we also train models to translate video se-
quences between diﬀerent scene image subdomains and diﬀerent video datasets.

14

Liu et al.

In Figure 6 and Figure 7, we provide visual examples of video translation of
Sunset-and-Day and Rain-and-Snow scenes in the Viper dataset. Visual exam-
ples of translation between Viper and Cityscapes [15] datasets is organized in
ﬁgure 8. They show the ability of our approach to learn the association between
synthetic videos and real-world videos. More examples and the corresponding
videos are attached in supplementary material.

Fig. 7. Viper Rain-and-Snow Top left: input Rain video; Top right: input Snow
video; Bottom left: translated Snow video; Bottom right: translated Rain video

Fig. 8. Cityscapes to Viper translation. Top left: input Cityscapes video; Top
right: translated Viper video in the night scenario; Bottom left: translated Viper video
in the snow scenario; Bottom right: translated Viper video in the sunset scenario

5 Conclusion

In this paper, we have proposed UVIT, a novel method for unsupervised video-to-
video translation. A specialized Encoder-RNN-Decoder spatio-temporal trans-
lator has been proposed to decompose style and content in the video for tem-
porally consistent and modality ﬂexible video-to-video translation. In addition,
we have designed a video interpolation loss within the translator which utilizes
highly structured video data to train our translators in a self-supervised manner.
This enables the eﬀective application of RNN-based network in the challenging
V2V task. Extensive experiments have been conducted to show the eﬀective-
ness of the proposed UVIT model. Without using any paired training data, the
proposed UVIT model is capable of producing excellent multimodal video trans-
lation results, which are image-level realistic, semantic information preserving
and video-level consistent.

Acknowledgments. This work was partly supported by the ETH Z¨urich Fund
(OK), and by Huawei, Amazon AWS and Nvidia grants.

UVIT

15

References

1. Alharbi, Y., Smith, N., Wonka, P.: Latent ﬁlter scaling for multimodal unsupervised
image-to-image translation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 1458–1466 (2019)

2. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented
cyclegan: Learning many-to-many mappings from unpaired data. In: ICML. pp.
195–204 (2018)

3. Arjovsky, M., Bottou, L.: Towards principled methods for training generative ad-

versarial networks. In: ICLR (2017)

4. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. In: ICML. pp. 214–223

(2017)

5. Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A.: A brief survey

of deep reinforcement learning. arXiv preprint arXiv:1708.05866 (2017)

6. Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: Unsupervised video

retargeting. In: ECCV. pp. 119–135 (2018)

7. Bashkirova, D., Usman, B., Saenko, K.: Unsupervised video-to-video translation.

arXiv preprint arXiv:1806.03698 (2018)

8. Bojanowski, P., et al.: Optimizing the latent space of generative networks. In:

ICML. pp. 600–609 (2018)

9. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the

kinetics dataset. In: CVPR. pp. 6299–6308 (2017)

10. Chen, T., Zhai, X., Ritter, M., Lucic, M., Houlsby, N.: Self-supervised gans via
auxiliary rotation loss. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 12154–12163 (2019)

11. Chen, X., Kingma, D.P., Salimans, T., Duan, Y., Dhariwal, P., Schulman,
lossy autoencoder. arXiv preprint

J., Sutskever, I., Abbeel, P.: Variational
arXiv:1611.02731 (2016)

12. Chen, Y., Pan, Y., Yao, T., Tian, X., Mei, T.: Mocycle-gan: Unpaired video-to-
video translation. In: ACM International Conference on Multimedia. pp. 647–655.
ACM (2019)

13. Cho, W., Choi, S., Park, D.K., Shin, I., Choo, J.: Image-to-image translation
via group-wise deep whitening-and-coloring transformation. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. pp. 10639–10647
(2019)

14. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Uniﬁed gener-
ative adversarial networks for multi-domain image-to-image translation. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 8789–8797 (2018)

15. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: CVPR. pp. 3213–3223 (2016)

16. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.

In: ICLR (2017)

17. Gong, R., Li, W., Chen, Y., Gool, L.V.: Dlow: Domain ﬂow for adaptation and

generalization. In: CVPR. pp. 2477–2486 (2019)

18. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. pp. 2672–2680
(2014)

16

Liu et al.

19. Goyal, A.G.A.P., Sordoni, A., Cˆot´e, M.A., Ke, N.R., Bengio, Y.: Z-forcing: Train-
ing stochastic recurrent networks. In: Advances in neural information processing
systems. pp. 6713–6723 (2017)

20. Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A.A., Visin, F., Vazquez, D.,
Courville, A.: Pixelvae: A latent variable model for natural images. arXiv preprint
arXiv:1611.05013 (2016)

21. Ha, D., Schmidhuber, J.: World models. arXiv preprint arXiv:1803.10122 (2018)
22. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In: NIPS.
pp. 6626–6637 (2017)

23. Huang, X., Belongie, S.J.: Arbitrary style transfer in real-time with adaptive in-

stance normalization. In: ICCV. pp. 1501–1510 (2017)

24. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-to-

image translation. In: ECCV. pp. 172–189 (2018)

25. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-

tional adversarial networks. In: CVPR. pp. 1125–1134 (2017)

26. Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from

27. Kajiya, J.T.: The rendering equation. In: ACM SIGGRAPH computer graphics.

standard gan. In: ICLR (2019)

vol. 20, pp. 143–150. ACM (1986)

28. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative

adversarial networks. In: CVPR. pp. 4401–4410 (2019)

29. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-

works. In: NIPS. pp. 700–708 (2017)

30. Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Paul Smolley, S.: Least squares

generative adversarial networks. In: ICCV. pp. 2794–2802 (2017)

31. Mo, S., Cho, M., Shin, J.: Instagan: Instance-aware image-to-image translation. In:

ICLR (2019)

32. Nie, D., Cao, X., Gao, Y., Wang, L., Shen, D.: Estimating ct image from mri data
using 3d fully convolutional networks. In: Deep Learning and Data Labeling for
Medical Applications, pp. 170–178. Springer (2016)

33. Perez, E., Strub, F., De Vries, H., Dumoulin, V., Courville, A.: Film: Visual reason-
ing with a general conditioning layer. In: AAAI Conference on Artiﬁcial Intelligence
(2018)

34. Ren, Z., Jae Lee, Y.: Cross-domain self-supervised multi-task feature learning using
synthetic imagery. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 762–771 (2018)

35. Richter, S.R., Hayder, Z., Koltun, V.: Playing for benchmarks. In: ICCV. pp. 2213–

2222 (2017)

36. Romero, A., Arbel´aez, P., Van Gool, L., Timofte, R.: Smit: Stochastic multi-label

image-to-image translation. In: ICCV Workshops. pp. 0–0 (2019)

37. Shi, X., Gao, Z., Lausen, L., Wang, H., Yeung, D.Y., Wong, W.k., Woo, W.c.: Deep
learning for precipitation nowcasting: A benchmark and a new model. In: NIPS.
pp. 5617–5627 (2017)

38. Sun, Y., et al.: Unsupervised domain adaptation through self-supervision. arXiv

preprint arXiv:1909.11825 (2019)

39. Wang, C., Huang, H., Han, X., Wang, J.: Video inpainting by jointly learning tem-
poral structure and spatial details. In: AAAI Conference on Artiﬁcial Intelligence.
vol. 33, pp. 5232–5239 (2019)

UVIT

17

40. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
CVPR. pp. 8798–8807 (2018)

41. Wu, P.W., Lin, Y.J., Chang, C.H., Chang, E.Y., Liao, S.W.: Relgan: Multi-domain
image-to-image translation via relative attributes. In: Proceedings of the IEEE
International Conference on Computer Vision. pp. 5914–5922 (2019)

42. Wu, W., Cao, K., Li, C., Qian, C., Loy, C.C.: Transgaga: Geometry-aware unsu-
pervised image-to-image translation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 8012–8021 (2019)

43. Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convo-
lutional lstm network: A machine learning approach for precipitation nowcasting.
In: Advances in neural information processing systems. pp. 802–810 (2015)

44. Zhang, B., He, M., Liao, J., Sander, P.V., Yuan, L., Bermak, A., Chen, D.: Deep
exemplar-based video colorization. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 8052–8061 (2019)

45. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: ICCV. pp. 2223–2232 (2017)
46. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,
E.: Toward multimodal image-to-image translation. In: NIPS. pp. 465–476 (2017)

18

Liu et al.

Supplementary Material

In this supplementary material, we provide more details and results of our

method as follows:

1. Additional loss details and implementation details;
2. More experimental results with the resolution of 128 × 128;
3. More experimental results with the resolution of 256 × 256;

– Translation between image and label: comparison with the baseline, mul-

timodal results and a 1680-frame style-consistent sequence;

– Translation on other datasets: Rain and Snow, Sunset and Day, Viper

and Cityscapes;

4. Additional ablation experiments details.

All the videos can be found in our project website:
Project Website https://uvit.netlify.com/

1 Additional Loss Details and Implementation Details

1.1 Loss functions for the discriminator

In this section, we provide more details of our image-level (Dimg), video-level
(Dvid), and style latent (DZ) discriminator losses. For the purpose of simplicity,
we only present the loss functions for domain A, and the loss functions for domain
B are deﬁned following the same set of equations. Our adversarial loss is based
on Relativistic GAN (RGAN) [26], which tries to predict the probability that a
real sample is relatively more realistic than a fake one.

Image level discriminator loss The loss term Dimg

is deﬁned as follows:

A

LGAN
Dimg
A

=

1
2(T − 2)

[Dimg

A (ai) − Dimg

A (ainterp

i

) − 1]2

[Dimg

A (ai) − Dimg

A (atrans

i

) − 1]2.

i=T −1
(cid:88)

i=2

i=T
(cid:88)

i=1

+

1
2T

Video level discriminator loss Dvid

A for domain A is deﬁned as follows:

LGAN
Dvid
A

= [Dvid

A (a1:T ) − Dvid

A (atrans
1:T

) − 1]2.

Style latent variable discriminator loss This loss term (DZA ) for the style domain
A is deﬁned as follows:

LGAN
DZA

= [DZA(zprior

a

) − DZA(zpost

a

) − 1]2.

(5)

(6)

(7)

UVIT

19

1.2 Network structure

Style Encoder, Content Encoder and Content Decoder Our style encoder is sim-
ilar to the one used in Augment CycleGAN [2]. Under the shared content space
assumption [29], we decompose the style-conditioned Resnet-Generator used in
Augment CycleGAN [2] into a Content Encoder and a Content Decoder. More-
over, when the sub-domain information is available, we assign part of the style
latent variable to record such prior information. Concretely, we use one-hot vec-
tor to encode the sub-domain information.

RNN - Trajectory Gated Recurrent Units (TrajGRUs) Traditional RNN (Recur-
rent Neural Network) is based on the fully connected layer, which has limited
capacity of proﬁting from the underlying spatio-temporal information in video
sequence. In order to take full advantage of the spatial and temporal correlations,
UVIT utilizes a convolutional RNN architecture in the generator. TrajGRU [37]
is one variant of Convolutional RNN (Recurrent Neural Network) [43], which
can actively learn the location-variant structure in the video data. It uses the
input and hidden state to generate the local neighborhood set for each location
at each time, thus warping the previous state to compensate for the motion in-
formation. We take two TrajGRUs to propagate the inter-frame information in
both directions in the shared content space.

Discriminators (Dimg, Dvid, DZ) For the image-level discriminators Dimg, the
architecture is based on the PatchGANs [25] approach. Likewise, Video-level
discriminators Dvid are similar to PatchGANs, yet we employ 3D convolutional
ﬁlters. For the style latent variable discriminators DZ, we use the same archi-
tecture as in Augmented CycleGAN [2].

1.3 Datasets

We validate our method using two common yet challenging datasets: Viper [35],
and Cityscapes [15] datasets.

Viper has semantic label videos and scene image videos. There are 5 subdo-
mains for the scene videos: day, sunset, rain, snow and night. The large diversity
of scene scenarios makes this dataset a very challenging testing bed for the un-
supervised V2V task. We quantitatively evaluate translation performance by
diﬀerent methods on the image-to-label and the label-to-image mapping tasks.
We further conduct the translation between diﬀerent subdomains of the scene
videos for qualitative analysis.

Cityscapes has real-world street scene videos. As there is not subdomain in-
formation for Cityscapes, we conduct experiments without subdomain label for
Cityscapes. We conduct qualitative analysis on the translation between scene
videos of Cityscapes and Viper dataset. Note that there is no ground truth
semantic labels for the continuous Cityscapes video sequences. The semantic la-
bels are only available to a limited portion of none-continuous individual images.
Therefore, we could not use it for our evaluation of image-to-label (semantic seg-
mentation) performance.

20

Liu et al.

1.4 Implementation Details

We train our model using images of 128×128 and 10 frames per batch in a single
NVIDIA P100 GPU for the main experiments to capture temporal information
with more frames. Setting the batch size to one, it takes about one week to train.
Note that it takes roughly 4 days to train using 6 frames per batch.

During inference, we use video sequences of 30 frames. These 30 frames are
divided into 4 smaller sequences of 10 frames with overlap. They all share the
same style code to be style consistent. To get a higher resolution and show more
details within the existing GPU resource constraint, we also train our model
using images of 256 × 256 and 4 frames per batch.

The λ parameters. Video interpolation loss weight λinterp is set to 10. Cycle
consistency loss weight λcycle is set to 10. Style reconstruction loss weight λrec
is set to 0.025.

1.5 Human Preference Score

We have conducted human subjective experiments to evaluate the visual quality
of synthesized videos using the Amazon Mechanical Turk (AMT) platform.

For the video-level evaluation, we show two videos (synthesized by two diﬀer-
ent models) to AMT participants, and ask them to select which one looks more
realistic regarding a video-consistency and video quality criteria.

– UVIT (ours) / 3DCycleGAN: Since 3DcycleGAN [7] generates consis-
tent output with 8 frames in the original paper setting, UVIT results are
organized to 8 frames for a fair comparison.

– UVIT (ours) / Improved ReCycleGAN: When comparing with im-

proved RecycleGAN [6], we take each video clip with 30 frames.

– UVIT (ours) / vid2vid: When comparing with vid2vid [39], we take each

video clip with 28 frames, following the setting in vid2vid [39].

For the image-level evaluation, we show to AMT participants two generated
frames synthesized by two diﬀerent algorithms, and ask them which one looks
more real in visual quality.

These evaluations have been conducted for 100 videos and frame samples
to assess the image-level and video-level qualities, respectively. We gathered
answers from 10 diﬀerent workers for each sample.

2 Additional examples of the label-to-image qualitative

comparison (128 × 128)

More results on the label-to-image mapping comparison of UVIT (ours) and
Improved ReCycleGAN are depicted in Figure 9 and the attached video Com-
pare.mp4. For the 1 LRCompare.mp4, we give a short description to guide the
comparison. From left to right, there are outputs for six diﬀerent input samples
to compare:

UVIT

21

– 1: Please see the trajectory of the car and the surrounding road.
– 2: Please see the boundary between two cars.
– 3: Please see the translation of the road to check the complete translation

and consistency across frames.

– 4: Please see the walls and the pillar across frames.
– 5: Please see the consistency of the road.
– 6: Please see the consistency of the wall.

Fig. 9. Video screen cut of the label-to-image qualitative comparison.
First row: semantic label inputs; Second row: improved ReCycleGAN outputs; Third
row: UVIT outputs. Fourth row: ground truth. A full video ﬁle can be found in
1 LRCompare.mp4.

3 Higher resolution results (256 × 256)

To get a higher resolution and show more details within the existing GPU re-
source constraint, we also train our model using images of 256×256 and 4 frames
per batch. During the test time, we divide a longer sequence into sub-sequences
of 4 frames with overlap. All the results of this section are trained using images
of 256 × 256 and 4 frames per batch.

22

Liu et al.

Table 7. Quantitative comparison between UVIT and baseline approaches
on the image-to-label (Semantic segmentation) task.(256 × 256 with 4 frames
per batch during training) .Our translator eﬀectively leverage the temporal infor-
mation directly, thus producing more semantic persevering translation outcomes

Criterion

Model

Day Sunset Rain Snow Night All

mIoU↑

ReCycleGAN (Reproduced) 10.32 11.19 11.25 9.83

7.73 10.12
12.05 12.23 13.37 11.54 10.49 11.93

UVIT (Ours) (frame 4)

AC↑

PA↑

ReCycleGAN (Reproduced) 15.80 15.79 15.93 15.57 11.47 14.85
17.21 17.41 18.16 17.37 14.30 16.50

UVIT (Ours) (frame 4)

ReCycleGAN (Reproduced) 54.70 55.92 57.71 50.85 49.11 53.66
63.44 61.98 64.72 60.83 62.05 62.35

UVIT (Ours) (frame 4)

3.1 Additional examples of the label-to-image qualitative

comparison

In Figure 10 (corresponding video 2 HRcompare.mp4) and Figure 10 (corre-
sponding video 3 HRcomapare2.mp4), we provide the visual examples of how
our UVIT method compares with respect to RecycleGAN [6]. The RecycleGAN
outputs are generated by the original code provided by the author of Recycle-
GAN in 256 × 256. Besides the video-level quality comparison from videos, we
encourage the reader to also check the frame-level quality from images since
.mp4 format may fail to preserve some image-level quality.

Fig. 10. Video screenshot of the video corresponding to Fig. 1 in the main
paper. The video is attached as 2 HRcomapare.mp4

3.2 Quantitative comparison of the label-to-image and

image-to-label

In Table 7 and 8, we show quantitative results for our proposed method trained
with a resolution of 256 × 256 and 4 frames per batch.

UVIT

23

Fig. 11. Video screenshot of the comparison with RecycleGAN [6]: We aim
to compare the content consistency and image-level quality. Here the RecycleGAN
results are produced by the original RecycleGAN code in a resolution of 256 × 256.
Since there is no guarantee of style consistency for RecycleGAN, we select some Re-
cycleGAN visual results in a small sequence length of 30 frames where style is almost
consistency to compare with UVIT (ours). The corresponding video is attached as
3 HRcomapare2.mp4

24

Liu et al.

Table 8. Quantitative comparison between UVIT and baseline approaches
on the label-to-image task (256 × 256 with 4 frames per batch during train-
ing). Better FID indicates that our translation has better visual quality and temporal
consistency. We use the pre-trained network (I3D [9]) to extract features from 30-frame
sequences just as the experiments in the main paper.

Criterion

Model

Day Sunset Rain Snow Night

FID↓

ReCycleGAN [6]

23.60 24.45 28.54 31.58 35.74
UVIT (ours) (frame 4) 18.68 16.70 20.20 18.27 19.29

3.3 Label-to-image multi-subdomain and multimodality results

Video results of UVIT on label sequences to image sequences with multi-subdomain
and multimodality are shown in Figure 12 and the enclosed video 4 Multimodality.mp4.
The videos are all with a length of 220 frames.

Fig. 12. Video screenshot of the label-to-image multi-subdomain and mul-
timodality results. Better depicted in 4 Multimodality.mp4.

3.4 Long video example (1680 frames)

In Figure 13 and attached video 5 long consistency.mp4, we provide a video
sequence example with more than 1680 frames to give a qualitative example
of how our UVIT model performs in terms of style consistency. Note that the
semantic labels in Viper [35] are automatic generated, however, we observe that
there may still exist a little small ﬂips in the input semantic label sequence
occasionally.

UVIT

25

Fig. 13. Screenshot of a long style consistent translation video visual ex-
ample (1680 frames). Left: input semantic labels; Right: UVIT translated video in
sunset scenario. All frames within the video share the same style code to keep style
consistency. The video is attached as 5 long consistency.mp4

26

Liu et al.

3.5 Translation on other datasets

In Figure 14 and in the attached video 6 Rainandsnow.mp4, we provide visual
examples of UVIT video translation between Rain and Snow scenes in the Viper
dataset. In Figure 15 and in the attached video 7 Sunsetanday.mp4, we provide
visual examples of UVIT video translation between Sunset and Day scenes in the
Viper dataset. In Figure 16 and in the attached video 8 Cityscapesandviper.mp4,
we provide visual examples of UVIT video translation between Cityscapes dataset
and Viper dataset. Besides the video-level quality evaluation from videos, we en-
courage the reader to also check the frame-level quality from images since .mp4
format may fail to preserve some image-level quality.

UVIT

27

Fig. 14. Screenshot of Viper Rain-and-Snow translation. First row: real rain
inputs; Second row: translated snow videos; Third row: real snow inputs; Fourth row:
translated rain videos. Video is attached as 6 Rainandsnow.mp4

28

Liu et al.

Fig. 15. Screenshot of Viper Sunset-and-Day translation. First row: real sunset
inputs; Second row: translated day videos; Third row: real day inputs; Fourth row:
translated sunset videos. Video is attached as 7 Sunsetandday.mp4

UVIT

29

Fig. 16. Screenshot of Cityscapes-and-Viper translation. Top left:
real
Cityscapes input; Top right: translated Viper videos with diﬀerent style codes; Bot-
tom left: real Viper input; Bottom right: translated Cityscapes videos with diﬀerent
style codes. Since the general distribution between Cityscapes and Viper may be dif-
ferent (e.g. there are more buildings in Cityscapes), the translated Viper video may
diﬀer from input Cityscapes video in class distribution to fool the discriminator, so
as to be close to the class distribution in the target domain. Video is attached as
8 Cityscapesandviper.mp4

30

Liu et al.

4 Additional Results for Ablation Study

Here we provide the supplementary results for the ablation part. We ﬁrst give a
complete table for the ablation experiment on how the performance depends on
the input frame number. After that, we give the qualitative results of multimodal
consistent videos when UVIT is trained and tested without the sub-domain label.

4.1 Diﬀerent input frame number

Our RNN-based translator incorporates temporal information from multiple
frames. In Table 9, we provide the complete table corresponding to the ”Dif-
ferent input frame number” ablation study section in the main paper.

4.2 UVIT without the subdomain label

To check how our UVIT performs in terms of style consistency without the
subdomain label information, we run this ablation experiment. The results are
attached in Figure 17 and 9 N o subdomain.mp4. By randomly sampling the
style code from prior distribution, we can get multimodal consistent video results
in a stochastic way.

Fig. 17. Ablation study: when no sub-domain label is used during train-
ing and testing. First video is the input semantic label sequence, the rest videos
are the translated scene videos with style codes randomly sampled from prior distri-
bution. There are 220 frames for each video. The corresponding video is attached as
9 N o subdomain.mp4

UVIT

31

Table 9. Quantitative results of UVIT with diﬀerent number of frames per
batch in training on the image-to-label (Semantic segmentation) task. With
the increase of input frames number, our RNN-based translator can utilize the temporal
information better, resulting in better semantic preserving

Criterion Frame number Day Sunset Rain Snow Night All

mIoU↑

AC↑

PA↑

4
6
8
10

4
6
8
10

4
6
8
10

11.19
11.84 11.91 12.35 11.37
12.29 12.66 13.03 11.77
11.94
13.05 13.21 14.23 13.07 11.00 12.87
13.71 13.89 14.34 13.23 10.10 13.07

8.49
9.79

16.78 16.75 16.57 16.32 12.21
15.7
17.50 17.46 17.66 16.73 14.23 16.62
18.42 18.28 19.19 17.80 15.18 17.68
18.74 19.13 18.98 17.81 13.99 17.59

62.84 60.34 61.97 58.77 51.68 59.04
62.85 61.21 62.21 59.77 56.84 60.51
65.56 64.11 66.26 64.18 62.02 64.25
68.06 66.35 67.21 65.49 58.97 65.20

