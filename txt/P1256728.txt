6
1
0
2
 
n
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
6
4
6
0
.
2
0
6
1
:
v
i
X
r
a

FLASH: Fast Bayesian Optimization for
Data Analytic Pipelines

Yuyu Zhang Mohammad Taha Bahadori Hang Su

Jimeng Sun

Georgia Institute of Technology
{yuyu,bahadori,hangsu}@gatech.edu,jsun@cc.gatech.edu

Abstract

Modern data science relies on data analytic pipelines to organize interdependent compu-
tational steps. Such analytic pipelines often involve diﬀerent algorithms across multiple steps,
each with its own hyperparameters. To achieve the best performance, it is often critical to select
optimal algorithms and to set appropriate hyperparameters, which requires large computational
eﬀorts. Bayesian optimization provides a principled way for searching optimal hyperparame-
ters for a single algorithm. However, many challenges remain in solving pipeline optimization
problems with high-dimensional and highly conditional search space. In this work, we propose
Fast LineAr SearcH (FLASH), an eﬃcient method for tuning analytic pipelines. FLASH is
a two-layer Bayesian optimization framework, which ﬁrstly uses a parametric model to select
promising algorithms, then computes a nonparametric model to ﬁne-tune hyperparameters of
the promising algorithms. FLASH also includes an eﬀective caching algorithm which can fur-
ther accelerate the search process. Extensive experiments on a number of benchmark datasets
have demonstrated that FLASH signiﬁcantly outperforms previous state-of-the-art methods in
both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%
improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art
performance on a real-world application for healthcare predictive modeling.

1 Introduction

Modern data science often requires many computational steps such as data preprocessing, fea-
ture extraction, model building, and model evaluation, all connected in a data analytic pipeline.
Pipelines provide a natural way to represent, organize and standardize data analytic tasks, which
are considered to be an essential element in the data science ﬁeld [11] due to their key role in large-
scale data science projects. Many machine learning toolboxes such as scikit-learn [36], RapidMiner
[31], SPSS [10], Apache Spark [30] provide mechanisms for conﬁguring analytic pipelines.

An analytic pipeline skeleton is shown in Figure 1. Each step, such as feature preprocessing
and classiﬁcation, includes many algorithms to choose from. These algorithms usually require
users to set hyperparameters, ranging from optimization hyperparameters such as learning rate and
regularization coeﬃcients, to model design hyperparameters such as the number of trees in random
forest and the number of hidden layers in neural networks. There are an exponential number of
choices for the combination of algorithms and hyperparameters in a given analytic pipeline skeleton.
Because of the interdependency between all the algorithms and their hyperparameters, the choices
can have huge impact on the performance of the best model.

1

Figure 1: A typical data analytic pipeline.

Tuning hyperparameters of a single algorithm can be viewed as an optimization problem of a
black-box objective function, which is noisy and often expensive to evaluate. Here the input of
black-box are the hyperparameters, and the objective function is the output performance such as
accuracy, precision and recall. To tackle this problem, simple methods have been applied such as
grid or random search [5, 3]. While on diﬃcult problems where these simple approaches are not
eﬃcient, a more promising model-based approach is Bayesian optimization [32, 27, 7, 39]. The high-
level idea of Bayesian optimization is to deﬁne a relatively cheap surrogate function and use that
to search the hyperparameter space. Indeed, there exist other global optimization methods, such
as evolutionary algorithms [2] and optimistic optimization [33]. We choose Bayesian optimization
framework due to its great performance in practice. Recently, Bayesian optimization methods have
been shown to outperform other methods on various tasks, and in some cases even beat human
domain experts to achieve better performance via tuning hyperparameters [42, 4].

Despite its success, applying Bayesian optimization for tuning analytic pipelines faces several
signiﬁcant challenges: Existing Bayesian optimization methods are usually based on nonparametric
models, such as Gaussian process and random forest. A major drawback of these methods is that
they require a large number of observations to ﬁnd reasonable solutions in high-dimensional space.
When tuning a single algorithm with several hyperparameters, Bayesian optimization works well
with just a few observations. However, when it comes to pipeline tuning, thousands of possible
combinations of algorithms plus their hyperparameters jointly create a large hierarchical high-
dimensional space to search over, whereas existing methods tend to become ineﬃcient. Wang et
al. [46] tackled the high-dimensional problem by making a low eﬀective dimensional assumption.
However, it is still a ﬂat Bayesian optimization method and not able to handle the exploding
dimensionality problem caused by hierarchically structured hyperparameters in analytic pipeline
tuning.
Motivating example: We build an analytic pipeline for classiﬁcation task (details in Section 4). If
we give 10 trials for each hyperparameter over 1,456 unique pipeline paths and 102 hyperparameters,
we have more than 2 million conﬁgurations, which can take years to complete with a brute-force
search. Even with the state-of-the-art Bayesian optimization algorithm such as Sequential Model-
based Algorithm Conﬁguration (SMAC) [22], the process can still be slow as shown in Figure 2.
If we know the optimal algorithms ahead time (Oracle) with just hyperparameter tuning of the
optimal algorithms, we can obtain signiﬁcant time saving, which is however not possible. Finally,
our proposed method FLASH can converge towards the oracle performance much more quickly
than SMAC.

In this paper, we propose a two-layer Bayesian optimization algorithm called Fast LineAr SearcH
(FLASH): the ﬁrst layer for selecting algorithms, and the second layer for tuning the hyperparame-
ters of selected algorithms. FLASH is able to outperform the state-of-the-art Bayesian optimization
algorithms by a large margin, as shown in Figure 2. By designing FLASH, we make three main
contributions:

• We propose a linear model for propagation of error (or other quantitative metrics) in analytic

2

Figure 2: Performance comparison of a data analytic pipeline on MRBI dataset, including the
previous state of the art SMAC, proposed method FLASH, and Oracle (i.e., pretending the optimal
algorithm conﬁguration is given, and only performing hyperparameter tuning with SMAC on those
algorithms). We show the median percent error rate on the test set along with standard error bars
(generated by 10 independent runs) over time. FLASH outperforms SMAC by a big margin and
converges toward Oracle performance quickly.

pipelines. We also propose a Bayesian optimization algorithm for minimizing the aggregated error
using our linear error model. Our proposed mechanism can be considered as a hybrid model: a
parametric linear model for fast exploration and pruning of the algorithm space, followed by a
nonparametric hyperparameter ﬁne-tuning algorithm.

• We propose to initialize the hyperparameter tuning algorithm using the optimal design strategy
[1, 15, 38] which is more robust than the random initialization. We also propose a fast greedy
algorithm to eﬃciently solve the optimal design problem for any given analytic pipeline.

• Finally, we introduce a caching algorithm that can signiﬁcantly accelerate the tuning process. In
particular, we model the (time) cost of each algorithm, and incorporate that in the optimization
process. This ensures the eﬃciency of fast search.

We demonstrate the eﬀectiveness of FLASH with extensive experiments on a number of diﬃcult
problems. On the benchmark datasets for pipeline conﬁgurations tuning, FLASH substantially
improves the previous state of the art by 7% to 25% in test error rate within the same time budget.
We also experiment with large-scale real-world datasets on healthcare data analytic tasks where
FLASH also exhibits superior results.

2 Background and Related Work

2.1 Data Analytic Pipelines

The data analytic pipeline refers to a framework consisting of a sequence of computational transfor-
mations on the data to produce the ﬁnal predictions (or outputs) [26]. Pipelines help users better

3

understand and organize the analysis task, as well as increase the reusability of algorithm imple-
mentations in each step. Several existing widely adopted machine learning toolboxes provide the
functionality to run analytic pipelines. Scikit-learn [36] and Spark ML [30] provide programmatic
ways to instantiate a pipeline. SPSS [10] and RapidMiner [31] provide a visual way to assemble
an analytic pipeline instance together and run. Microsoft Azure Machine Learning1 provides a
similar capability in a cloud setting. There are also specialized pipelines, such as PARAMO [35] in
healthcare data analysis.

However, a major diﬃculty in using these systems is that none of the above described tools is
able to eﬃciently help users decide which algorithms to use in each step. Some of the tools such as
scikit-learn, Spark ML, and PARAMO allow searching all possible pipeline paths and tuning the
hyperparameters of each step using an expensive grid search approach. While the search process
can be sped up by running in parallel, the search space is still too large for the exhaustive search
algorithms.

2.2 Bayesian Optimization

Bayesian optimization is a well-established technique for global and black-box optimization prob-
lems. In a nutshell, it comprises two main components: a probabilistic model and an acquisition
function. For the probabilistic model, there are several popular choices: Gaussian process [41, 42],
random forest such as Sequential Model-based Algorithm Conﬁguration (SMAC) [22], and density
estimation models such as Tree-structured Parzen Estimator (TPE) [5]. Given any of these models,
the posterior mean and variance of a new input can be computed, and used for computation of
the acquisition function. The acquisition function deﬁnes the criterion to determine future input
candidates for evaluation. Compared to the objective function, the acquisition function is chosen
to be relatively cheap to evaluate, so that the most promising next input for querying can be found
quickly. Various forms of acquisition functions have been proposed [43, 20, 45, 21]. One of the
most prominent acquisition function is the Expected Improvement (EI) function [32], which has
been widely used in Bayesian optimization. In this work, we use EI as our acquisition function,
which is formally described in Section 3.

Bayesian optimization is known to be successful in tuning hyperparameters for various learning
algorithms on diﬀerent types of tasks [42, 14, 4, 41, 46]. Recently, for the problem of pipeline con-
ﬁgurations tuning, several Bayesian optimization based systems have been proposed: Auto-WEKA
[44] which applies SMAC [22] to WEKA [17], auto-sklearn [13] which applies SMAC to scikit-learn
[36], and hyperopt-sklearn [24] which applies TPE [5] to scikit-learn. The basic idea of applying
Bayesian optimization to pipeline tuning is to expand the hyperparameters of all algorithms and
create large search space to perform optimization as we will show in the experiments. However,
for practical pipelines the space becomes too large which hinders convergence of the optimization
process. Auto-sklearn [13] uses a meta-learning algorithm that leverages performance history of
algorithms on existing datasets to reduce the search space. However, in real-world applications,
we often have unique datasets and tasks such that ﬁnding similar datasets and problems for the
meta-learning algorithm will be diﬃcult.

1https://studio.azureml.net

4

Figure 3: A toy example of data analytic pipeline. One possible pipeline path, ﬂowing from the
input Vin to the output Vout, is highlighted in shaded area.

3 Methodology

A data analytic pipeline G = (V, E) can be represented as a multi-step Directed Acyclic Graph
(DAG), where V is the set of algorithms, and E is the set of directed edges indicating dependency
between algorithms. Algorithms are distributed among multiple steps. Let V (k)
denote the ith
i
algorithm in the kth step. Each directed edge (V (k)
) ∈ E represents the connection from
algorithm V (k)
. Note that there is no edge between algorithms in the same step. We also
have an input data vertex Vin which points to all algorithms in the ﬁrst step, and an output vertex
Vout which is pointed by all algorithms in the last step.

to V (k+1)
j

, V (k+1)
j

i

i

A pipeline path is any path from the input Vin to the output Vout in pipeline graph G . To denote
a pipeline path of K steps, we use K one-hot vectors p(k) (1 ≤ k ≤ K), each denoting the algorithm
selected in the k-th step. Thus, the concatenation of one-hot vectors p = (cid:2)p(1), . . . , p(K)(cid:3) ∈ {0, 1}N
denotes a pipeline path, where N is the total number of algorithms in the pipeline G. Figure 3
shows a small data analytic pipeline with two steps. The ﬁrst step contains two algorithms, and the
second step contains three. One possible pipeline path is highlighted in the shaded area. On this
pipeline path, V (1)
are selected in the ﬁrst and second step, so that we have p(1) = [0, 1]
and p(2) = [0, 0, 1]. Thereby, the highlighted pipeline path is given by p = (cid:2)p(1), p(2)(cid:3) = [0, 1, 0, 0, 1].
For any pipeline path p, we concatenate all of its hyperparameters in a vector λp. The pair of path
and hyperparameters, i.e. (p, λp), forms a pipeline conﬁguration to be run. For ease of reference,
we list the notations in Table 1.

and V (2)

2

3

The problem of tuning data analytic pipelines can be formalized as an optimization problem:

Problem 1. Given a data analytic pipeline G with input data D, resource budget T , evaluation
metric function m(G, D; p, λp), resource cost of running pipeline τ (G, D; p, λp), how to ﬁnd the
pipeline path p and its hyperparameters λp with best performance m(cid:63)?

The performance of the best pipeline path is denoted by m(cid:63) = minp,λp m(G, D; p, λp) subject

5

Table 1: Mathematical notations used in this paper.

Symbol Description

G
V
E
K
N
D
Vin
Vout
V (k)
i
p(k)
λ(k)
p
p
λp
m(·)
τ (·)
Tinit
Tprune
Ttotal

data analytic pipeline
set of algorithms in G
set of dependency between algorithms
total number of steps in G
total number of algorithms in G
input data of pipeline
input vertex of G
output vertex of G
ith algorithm in kth step
one-hot vector for kth step
hyperparameters for kth step
pipeline path
all hyperparameters of p
evaluation metric function
time cost of running pipeline
budget for Phase 1
budget for Phase 2
budget for Phase 3

to budget T . The objective is to approach the optimal performance within the budget T via
(cid:98)p) ≤ m(cid:63) + (cid:15) for
optimizing over p, λp; i.e., we would like our solution (cid:98)p, (cid:98)λ
small values of (cid:15).

(cid:98)p to satisfy m(G, D; (cid:98)p, (cid:98)λ

To eﬃciently tackle this problem, we propose a two-layer Bayesian optimization approach named
Fast LineAr SearcH (FLASH). We generally introduce the idea of linear model and describe the
algorithm in Section 3.1. An immediate advantage of using linear model is that we can use more
principled initialization instead of random initialization, as discussed in Section 3.2. We use cost-
sensitive modeling to prune the pipeline, as described in Section 3.3. Finally, we accelerate the
entire optimization procedure via pipeline caching, which we describe in Section 3.4.

3.1 Two-layer Bayesian Optimization

Inspired by the performance of linear regression under model misspeciﬁcation [48, 16, 28] and supe-
rior sample complexity compared to more ﬂexible nonparametric techniques [47], we seek parametric
models for propagation of error (or other quantitative metrics) in analytic pipelines. The high level
idea of FLASH is as follows: we propose a linear model for estimating the propagation of error (or
any other metric) in a given analytic pipeline. The linear model assumes that the performance of
algorithms in diﬀerent steps are independent, and the ﬁnal performance is additive from all algo-
rithms. That is, we can imagine that each algorithm is associated with a performance metric, and
the total performance of a pipeline path is the sum of the metrics for all algorithms in the path.
This linear model will replace the Gaussian process or random forest in the initial stages of the
pipeline tuning process. In the rest of this section, we provide the details of Bayesian optimization

6

with our linear model.

We apply the linear model only to the pipeline selection vector p and assume that the variations
due to hyperparameters of the algorithms are captured in the noise term. That is, we assume that
the error of any pipeline path p can be written as

m = β(cid:62)p + ε

where β ∈ RN denotes the parameters of the linear model. Given a set of observations of the
algorithm selection and the corresponding evaluation metric for the selected pipeline path in the
form of (pi, mi), i = 1, . . . , n, we can ﬁt this model and infer its mean µ(p) and variance σ2(p)
of the performance estimation for any new pipeline path represented by p. In particular, let the
design matrix P ∈ Rn×N denote the stacked version of the pipeline paths, i.e., P = [p1, . . . , pn](cid:62),
and m ∈ Rn be the corresponding response values of the evaluation metrics, m = [m1, . . . , mn]. We
use the following L2 regularized linear regression to obtain the robust estimate for β from history
observations:

(cid:98)β(P , m) = argmin

β

(cid:26) 1
n

(cid:107)P β − m(cid:107)2

2 + λ(cid:107)β(cid:107)2
2

(cid:27)

(1)

(cid:113)(cid:0)(cid:80)n

(cid:1). The predictive
where for any vector x ∈ Rn the L2 norm is deﬁned as (cid:107)x(cid:107)2 =
distribution for the linear model is Gaussian with mean (cid:98)µp = (cid:98)β(cid:62)p and variance (cid:98)σp = σ2
ε (1 +
p(cid:62)(P (cid:62)P + λI)−1p) where σ2
ε is the variance of noise in the model. We estimate σε as follows: the
residual in the ith observation is computed as (cid:98)(cid:15)i = (cid:98)µi − mi where (cid:98)µi = (cid:98)β(cid:62)pi is the estimate of mi
by our model. Thus, the variance of the residual can be found as (cid:98)σ2
(cid:15) = var((cid:98)µi − mi) where var(·)
denotes the variance operator.

i=1 x2
i

To perform Bayesian optimization with linear model, we use the popular Expected Improvement
(EI) criteria, which recommends to select the next sample pt+1 such that the following acquisition
function is maximized. The acquisition function represents the expected improvement over the best
observed result m+ at a new pipeline path p [44]:

EI(p) = E[Im+(p)] = σp[uΦ(u) + φ(u)]

(2)

u = m+−ξ−µp

σp

where
and ξ is a parameter to balance the trade-oﬀ between exploitation and
exploration. EI function is maximized for paths with small values of mp and large values of σp,
reﬂecting the exploitation and exploration trade-oﬀs, respectively. To be more speciﬁc, larger ξ
encourages more exploration in selecting the next sample. The functions Φ(·) and φ(·) represent
CDF and PDF of standard normal distribution, respectively. The idea of Bayesian optimization
with EI is that at each step, we compute the EI with the predictive distribution of the existing linear
model and ﬁnd the pipeline path that maximizes EI. We choose that path and run the pipeline
with it to obtain a new (pi, mi) pair. We use this pair to reﬁt and update our linear model and
repeat the process. Later on we also present an enhanced version of EI via normalizing it by cost
called Expected Improvement Per Second (EIPS).

We provide the full details of FLASH in Algorithm 1. While the main idea of FLASH is
performing Bayesian optimization using linear model and EI, it has several additional ideas to
make it practical. Speciﬁcally, FLASH has three phases:

• Phase 1, we initialize the algorithm using ideas from optimal design, see Section 3.2. The

budget is bounded by Tinit.

7

Algorithm 1: Fast Linear Search (FLASH)

input : Data analytic pipeline G; input data D; total budget for entire optimization Ttotal;

budget for initialization Tinit; budget for pipeline pruning Tprune; number of top
pipeline paths r

(cid:98)p

output: Optimized pipeline conﬁguration (cid:98)p and (cid:98)λ
/* Phase 1: Initialization (Section 3.2)
1 while budget Tinit not exhausted do
2

p ← new pipeline path from Algorithm 2
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:99)βτ (P , τ ) using Eq. (1)

5

4

3

6

/* Phase 2: Pipeline pruning (Section 3.3)
7 while budget Tprune not exhausted do
8

9

10

11

p ← argmaxp EIP S(p, P , β, βτ ) using Eq. (4)
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:98)βτ (P , τ ) using Eq. (1)

using Eq. (4)
/* Phase 3: Pipeline tuning

14 S ← history observations within G(cid:48)
15 Initialize model M given S
16 while budget Ttotal not exhausted do
p, λp ← next candidate from M
17
m ← RunPipeline(G(cid:48), D; p, λp) with Algorithm 3
S ← S ∪ {(p, λp, m)}
Update M given S
(cid:98)p ← Best conﬁguration so far found for G(cid:48)

21 (cid:98)p, (cid:98)λ

19

18

20

12
13 G(cid:48) ← construct subgraph of G with top r pipeline paths with largest EIP S(p, P , β, βτ )

*/

*/

*/

• Phase 2, we leverage Bayesian optimization to ﬁnd the top r best pipeline paths and prune

the pipeline G to obtain simpler one G(cid:48). The budget is bounded by Tprune

2.

• Phase 3, we use general model-based Bayesian optimization methods to ﬁne-tune the pruned

pipeline together with their hyperparameters.

In Phase 3, we use state-of-the-art Bayesian optimization algorithm, either SMAC or TPE. These
algorithms are iterative: they use a model M such as Gaussian process or random forest and use
EI to pick up a promising pipeline path with hyperparameters for running, and then update the
model with the new observation just obtained, and again pick up the next one for running. The

2In practice, it is better to use the time normalized EI (that is EIPS) during Phase 2; this idea is described in

Section 3.3.

8

i=1; number of desired pipeline paths ninit

Algorithm 2: Initialization with Optimal Design

/* Batch version
input : B initial candidates {pi}B
output: Optimal set of pipeline paths Q
1 p1 ← random pipeline path for initialization
2 H ← p1p(cid:62)
1
3 Q ← {p1}
4 for (cid:96) = 2, . . . , ninit do
5

j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)
H ← H + pj(cid:63)p(cid:62)
j(cid:63)
Q ← Q ∪ {pj(cid:63)}

6

7

j ) for j = 1, . . . , B.

/* Online version
input : B candidates {pi}B
output: Next pipeline path pj(cid:63), j(cid:63) ∈ {1, . . . , B}

i=1; current Gram matrix H

8 j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)

j ) for j = 1, . . . , B

*/

*/

budget is bounded by Ttotal. Note that our algorithm is currently described for a sequential setting
but can be easily extended to support parallel runs of multiple pipeline paths as well.

3.2 Optimal Design for Initialization

Most Bayesian optimization algorithms rely on random initialization which can be ineﬃcient; for
example, it may select duplicate pipeline paths for initialization. Intuitively, the pipeline paths
used for initialization should cover the pipeline graph well, such that all algorithms are included
enough times in the initialization phase. The ultimate goal is to select a set of pipeline paths
for initialization such that the error in estimation of β is minimized. Given our proposed linear
model, we can ﬁnd the optimal strategy for initialization to make sure the pipeline graph is well
covered and the tuning process is robust. In this section, we describe diﬀerent optimality criteria
studied in statistical experiment design [1, 15] and active learning [38], and design an algorithm for
initialization step of FLASH.

Given a set of pipeline paths with size n, there are several diﬀerent optimality criteria in terms
i=1 pip(cid:62)

i as follows [38]:

of the eigenvalues of the Gram matrix H = (cid:80)n
A-optimality: maximize (cid:80)n
D-optimality: maximize (cid:81)n

(cid:96)=1 λ(cid:96)(H).

(cid:96)=1 λ(cid:96)(H).

E-optimality: maximize λn(H), the nth largest eigenvalue.

It is easy to see that any arbitrary set of pipeline path designs satisﬁes the A-optimality criterion.

Proposition 1. Any arbitrary set of pipeline paths with size n is a size-n A-optimal design.

Proof. For any arbitrary set of pipeline paths with size n, we have:

n
(cid:88)

(cid:96)=1

λ(cid:96)(H) = tr(H) = tr

(cid:33)

pip(cid:62)
i

=

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:16)

tr

pip(cid:62)
i

(cid:17)

= nK.

9

The last step is due to particular pattern of p in our problem. Thus, we show that (cid:80)n
constant, independent of the design of pipeline paths.

(cid:96)=1 λ(cid:96)(H) is

Proposition 1 rules out use of A-optimality in pipeline initialization. Given the computational
complexity of E-optimality and the fact that it intends for optimality in the extreme cases, we
choose D-optimality criterion. The D-optimality criterion for design of optimal linear regression
can be stated as follows: suppose we are allowed to evaluate ninit samples pi, i = 1, . . . , ninit,
these samples should be designed such that the determinant of the Gram matrix H is maximized.
While we can formulate an optimization problem that directly ﬁnds pi values, we found that an
alternative approach can be computationally more eﬃcient. In this approach, we ﬁrst generate B
candidate pipeline paths for an integer B larger than the number of algorithms in the pipeline N .
This set may include all possible pipeline paths if the total number of paths is small. Then, our
goal becomes selecting a subset of size ninit from them. We can formulate the optimal design as
follows

a(cid:63) = argmax

det

a

(cid:41)

aipip(cid:62)
i

(cid:40) B
(cid:88)

i=1

s.t.

a ∈ {0, 1}K,

1(cid:62)a = ninit.

(3)

The last constraint 1(cid:62)a = ninit indicates that only ninit pipeline paths should be selected. The
objective function is concave in terms of continuous valued a [6, Chapter 3.1.5]. Thus, a traditional
approach is to solve it by convex programming after relaxation of the integrality constraint on a.
The matrix in the argument of the determinant is only N -dimensional which means calculation of
the determinant should be fast. Nestrov’s accelerated gradient descent [34] or Frank-Wolfe’s [23]
algorithms can be used for eﬃciently solving such problems.

An even faster solution can be found by using greedy forward selection ideas which are fast
and popular for optimal experiment design, for example see [37, 18, 25] and the references therein.
To apply greedy technique to our problem, we initialize the solution by picking one of the pipeline
i . Then, at (cid:96)th step, we add the path that maximizes j(cid:63) = argmaxj D(cid:96)(H + pjp(cid:62)
path H = pip(cid:62)
j )
where D(cid:96)(H) = (cid:81)min((cid:96),p)
λi(H) denotes the product of top min((cid:96), p) eigenvalues of its argument.
The algorithm is described in Algorithm 2. The optimization problem in Eq. (3) appears in other
ﬁelds such as optimal facility location and sensor planning where greedy algorithm is known to
have a 1 − 1

e approximation guarantee [8, 40].

One further desirable property of the greedy algorithm is that it is easy to run it under a
time budget constraint. We call this version the online version in Algorithm 2, where instead of a
ﬁxed number of iteration ninit, we run it until the exhaustion of our time budget. See Line 2 in
Algorithm 1 and the online version of Algorithm 2.

i=1

3.3 Cost-sensitive Modeling

The Expected Improvement aims at approaching the true optimal (doing well) within a small number
of function evaluations (doing fast). However, the time cost of each function evaluation may vary
a lot due to diﬀerent settings of hyperparameters. This problem is particularly highlighted in
pipeline conﬁgurations tuning, since the choice of algorithms can make a huge diﬀerence in running
time. Therefore, fewer pipeline runs are not always “faster” in terms of wall-clock time. Also, in
practice, what we care about is the performance we can get within limited resource budget, rather

10

than within certain evaluation times. That is why we need cost-sensitive modeling for the pipeline
tuning problem.

Expected Improvement Per Second (EIPS) [41] proposes another acquisition function for tuning
of a single learning algorithm by dividing the EI of each hyperparameter by its runtime. To apply
EIPS in pipeline tuning problem, we use a separate linear model to model the total runtime of
pipeline paths. Similar to the linear model for error propagation, the linear model for time assumes
that on a pipeline path each algorithm partly contributes to the total time cost and the runtimes are
additive. To apply the linear model, we replace the performance metric m with the cost metric τ .
The linear cost model parametrized by βτ can be eﬃciently updated using Eq. (1). As described in
Algorithm 1, βτ will be updated together with β at the end of Phase 2. We note that, in practice,
the budget T and the cost τ (·) can be any quantitative costs of budgeted resources (e.g., money,
CPU time), which is a natural generalization of our idea.

With the cost model above, we get the cost-sensitive acquisition function over the best observed

result m+ at a new pipeline path p:

E[Im+(p)]
E[log τ (p)]

=

σp[uΦ(u) + φ(u)]
E[log τ (p)]

(4)

EIP S(p, P , β, βτ ) =

where u =

m+ − ξ − µp
σp

.

Here the dependency in β and βτ is captured during computation of µp, σp, and τ (p). We take
logarithm of cost τ (·) to compensate the large variations in the runtime of diﬀerent algorithms.
This acquisition function balances “doing well” and “doing fast” in selecting the next candidate
path to run. During the optimization, it will help avoid those costly paths with poor expected
improvement. More importantly, at the end of Phase 2 in Algorithm 1, EIPS is responsible to
determine the most promising paths, which perform better but cost less, to construct a subgraph
for the last phase ﬁne-tuning. For this purpose, we set the exploration parameter ξ to 0 to only
select (Line 13 in Algorithm 1).

3.4 Pipeline Caching

During the experiments, we note that many pipeline runs have overlapped algorithms in their paths.
Sometimes these algorithms have exactly the same pipeline path and the same hyperparameter
settings along the path. This means that we are wasting time on generating the same intermediate
output again and again. For example, consider the min-max normalization algorithm in the ﬁrst
pipeline step: this algorithm will be executed many times, especially when it performs well so that
Bayesian optimization methods prefer to choose it.

To reduce this overhead, we propose a pipeline caching algorithm, as described in Algorithm 3.
When running a pipeline, we check the cache before we run each algorithm. If it turns out to be a
cache hit, the result will be immediately returned from cache. Otherwise, we run the algorithm and
cache the result. There is a caching pool (e.g., disk space, memory usage) for this algorithm. We
use the Least Recently Used (LRU) strategy to clean up the caching pool when budget becomes
exhausted.

Caching can signiﬁcantly reduce the cost of pipeline runs, and accelerates all three phases of
FLASH. Algorithms closer to the pipeline input vertex, usually the data preprocessing steps, have
higher chance to hit the cache. In fact, when we deal with large datasets on real-world problems,
the preprocessing step can be quite time-consuming such that caching can be very eﬃcient.

11

Algorithm 3: Pipeline Caching

input : Data analytic pipeline G; input data D; pipeline conﬁguration p and λp to be run;

available caching budget Tcache; current cache pool C

1 D(1) ← D

/* Run pipeline with cache pool

2 for k ← 1, . . . , K do

3

4

5

6

7

8

h ← Hash(p(1) . . . p(k), λ(1)
if h ∈ C then

p . . . λ(k)
p )

D(k+1) ← cached result from C

else

D(k+1) ← RunAlgorithm(G, D(k), p(k), λ(k)
p )
C ← C ∪ {(cid:104)h, D(k+1)(cid:105)}

/* Clean up cache pool when necessary

9 if Tcache exhausted then
10

Discard least recently used (LRU) items in C

4 Benchmark Experiments

*/

*/

In this section, we perform extensive experiments on a number of benchmark datasets to evaluate
our algorithm compared to the existing approaches. Then we study the impact of diﬀerent algorithm
choices in each component of FLASH.

Benchmark Datasets We conduct experiments on a group of public benchmark datasets on
classiﬁcation task, including Madelon, MNIST, MRBI and Convex3. These prominent datasets
have been widely used to evaluate the eﬀectiveness of Bayesian optimization methods [44, 13, 5].
We follow the original train/test split of all the datasets. Test data will never be used during
the optimization: the once and only usage of test data is for oﬄine evaluations to determine the
In all benchmark experiments, we use
performance of optimized pipelines on unseen test set.
percent error rate as the evaluation metric.

Baseline Methods As discussed in Section 2, SMAC [22] and TPE [5] are the state-of-the-art
algorithms for Bayesian optimization [44, 13, 24], which are used as baselines. Note that Spearmint
[41], a Bayesian optimization algorithm based on Gaussian process is not applicable since it does
not provide a mechanism to handle the hierarchical space [12]. Besides SMAC and TPE, we also
choose random search as a simple baseline for sanity check. Thus, we compare both versions of
our method FLASH (with SMAC in Phase 3) and FLASH(cid:63) (with TPE in Phase 3) against three
baselines in the experiments.
Implementation: To avoid possible mistakes in implementing other methods, we choose a gen-
eral platform for hyperparameter optimization called HPOlib [12], which provides the original
In order to fairly compare our method
implementations of SMAC, TPE, and random search.

3The benchmark datasets are publicly available at http://www.cs.ubc.ca/labs/beta/Projects/autoweka/

datasets.

12

with others, we also implement our algorithm on top of HPOlib, and evaluate all the compared
methods on this platform. We make the source code of FLASH publicly available at https:
//github.com/yuyuz/FLASH.

Experimental Settings We build a general data analytic pipeline based on scikit-learn [36], a
popular used machine learning toolbox in Python. We follow the pipeline design of auto-sklearn
[13]. There are four computational steps in our pipeline: 1) feature rescaling, 2) sample balancing,
3) feature preprocessing, and 4) classiﬁcation model. Each step has various algorithms, and each
algorithm has its own hyperparameters. Adjacent steps are fully connected.
In total, our data
analytic pipeline contains 33 algorithms distributed in four steps, creating 1,456 possible pipeline
paths with 102 hyperparameters (30 categorical and 72 continuous), which creates complex high-
dimensional and highly conditional search space. Details and statistics of this pipeline are listed in
Table 5 in Appendix A.

In all experiments, we set a wall-clock time limit of 10 hours for the entire optimization, 15
minutes time limit and 10GB RAM limit for each pipeline run. We perform 10 independent
optimization runs with each baseline on each benchmark dataset. All experiments were run on
Linux machines with Intel Xeon E5-2630 v3 eight-core processors at 2.40GHz with 256GB RAM.
Since we ran experiments in parallel, to prevent potential competence in CPU resource, we use the
numactl utility to bound each independent run in single CPU core.

For our algorithm FLASH, we set both Tinit and Tprune as 30 iterations (equal to the number of
algorithms in the pipeline), which can be naturally generalized to other budgeted resources such as
wall-clock time or money. We set ξ to 100 in the EIPS function. Note that the performance are not
sensitive to the choices of those parameters. Finally, we set the number of pipeline paths r to 10
, which works well in generating a reasonable-size pruned pipeline G(cid:48). In benchmark experiments,
we compare the performance of FLASH without caching to other methods because the pipelines do
not have complex data preprocessing data like many real-world datasets have. We will use caching
for real-world experiments later in Section 5.

4.1 Results and Discussions

Table 2 reports the experimental results on benchmark datasets. For each dataset, we report the
performance achieved within three diﬀerent time budgets. As shown in the table, our methods
FLASH and FLASH(cid:63) perform signiﬁcantly better than other baselines consistently in all settings,
in terms of both lower error rate and faster convergence. For example, on the Madelon dataset,
our methods reach around 12% test error in only 3 hours, while other baselines are still far from
that even after 10 hours.

Performing statistical signiﬁcance test via bootstrapping, we ﬁnd that often FLASH and FLASH(cid:63)
tie with each other on these benchmark datasets. For all the methods, the test error is quite con-
sistent with the validation error, showing that the potential overﬁtting problem is well prevented
by using cross validation.

Figure 4 plots the convergence curves of median test error rate along with time for all baseline
methods. As shown in the ﬁgure, after running about 4 hours, FLASH and FLASH(cid:63) start to lead
others with steep drop of error rate, and then quickly converge on a superior performance.

13

Table 2: Performance on both 3-fold cross-validation and test data of benchmark datasets. For
each method, we perform 10 independent runs of 10 hours each. Results are reported as the
median percent error across the 10 runs within diﬀerent time budgets. Test data is never seen by
any optimization method, which is only used for oﬄine evaluations to compute test error rates.
Boldface indicates the best result within a block of comparable methods. We underline those results
not statistically signiﬁcantly diﬀerent from the best according to a 10,000 times bootstrap test with
p = 0.05.

Dataset

Budget
(hours)

Cross-validation Performance (%)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Madelon

MNIST

MRBI

Convex

3
5
10

3
5
10

3
5
10

3
5
10

25.16
23.60
20.77

7.68
6.58
6.58

61.80
58.67
57.20

28.14
25.25
24.51

18.90
18.82
17.28

6.78
5.94
5.39

59.83
58.61
53.92

24.70
23.61
22.21

20.25
19.12
17.34

6.05
5.83
5.64

62.89
58.14
54.60

24.69
23.30
23.30

14.84
14.31
13.87

4.93
4.26
4.03

57.43
45.11
41.15

22.63
21.34
20.49

14.04
14.04
13.76

5.05
4.87
4.46

57.08
54.25
41.90

23.31
22.02
20.62

19.17
18.21
15.58

7.75
7.10
6.64

60.58
56.42
54.43

25.04
23.18
22.18

16.15
15.26
14.49

5.41
5.41
5.03

59.83
58.61
52.01

21.42
21.37
20.31

16.03
15.38
13.97

6.11
5.40
5.23

60.58
55.81
52.30

21.97
20.82
20.82

12.18
12.18
11.49

4.62
3.94
3.78

54.72
43.19
39.13

20.65
19.56
18.94

11.73
11.60
11.47

4.84
4.57
4.37

54.28
51.65
39.89

21.04
19.71
19.01

Figure 4: Performance of our methods (FLASH and FLASH(cid:63)) and other compared methods on
MRBI dataset. We show the median percent error rate on test set along with standard error bars
(generated by 10 independent runs) over time.

14

(a) The impact of optimal design
on MRBI dataset

(b) The impact of pipeline pruning
on MRBI dataset

(c) The impact of pipeline caching
on real-world dataset

Figure 5: Component analysis experiments. (a) Optimal design makes the initialization phase more
robust. (b) Pipeline pruning in the second phase of FLASH is the key to its superior performance.
(c) Performance of FLASH without caching and the original FLASH with caching on real-world
dataset. In all ﬁgures, we show the median error rate on test set along with standard error bars
(generated by 10 independent runs). Note that (a) and (b) are plotted with diﬀerent x-axes; (c) is
on a diﬀerent dataset as (a) and (b).

4.2 Detailed Study of FLASH Components

FLASH has three main components: optimal design for initialization, cost-sensitive model for
pipeline pruning, and pipeline caching. To study their individual contributions to the performance
gain, we drop out each of the component and compare the performance with original FLASH. Since
caching will be used for real-world experiments on large dataset, we describe the analysis of caching
component in Section 5. Here we use MRBI dataset for these experiments.

Figure 5(a) shows the diﬀerence between using random initialization and optimal design by
plotting the performance on initial 30 pipeline runs. The desirable property of optimal design en-
sures to run reasonable pipeline paths, giving FLASH a head start at the beginning of optimization.
While random initialization is not robust enough, especially when the number of pipeline runs is
very limited and some algorithms will have no chance to run due to the randomness. Figure 5(b)
shows the impact of pipeline pruning in the second phase of FLASH. Dropping out the pruning
phase with EIPS, and using SMAC immediately after Phase 1, we see a major degradation of the
performance. The ﬁgure clearly shows that in Phase 2 of FLASH, the linear model with EIPS
acquisition function is able to eﬃciently shrink the search space signiﬁcantly such that SMAC can
focus on those algorithms which perform well with little cost. This ﬁgure conﬁrms the main idea
of this paper that a simple linear model can be more eﬀective in searching high-dimensional and
highly conditional hyperparameter space.

5 Real-world Experiments

In this section, to demonstrate a real-world use case, we apply FLASH on a large de-identiﬁed
medical dataset for classifying drug non-responders. We show how our method can quickly ﬁnd
good classiﬁer for diﬀerentiating non-responders vs. responders.

15

Table 3: Statistics of the medical dataset.

#Patient #Event #Med #Class

train case
train control
test case
test control
total

18,581
18,582
4,646
4,646
46,455

982,025
622,777
245,776
153,303
2,003,881

434,171
286,198
108,702
70,395
899,466

547,854
336,579
137,074
82,908
1,104,415

Budget
(hours)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

3
5
10

30.32
16.66
11.75

27.03
19.09
4.86

35.40
33.22
21.03

21.02
14.40
2.51

23.28
19.86
3.44

Table 4: Performance of real-world dataset. Results are reported using the same settings as Table 2.

Experimental Settings With a collaboration with a pharmaceutical company, we created a
balanced cohort of 46,455 patients from a large claim dataset. Patients who have at least 4 times of
treatment failure are regarded as drug non-responders (case group). Other patients are responders
of the drug (control group). The prediction target is whether a patient belongs to case group or
control group. Each patient is associated with a sequence of events, where each event is a tuple of
format (patient-id, event-id, timestamp, value). Table 3 summarizes the statistics of this
clinical dataset, including the count of patient, event, medication, and medication class.

Unlike benchmark experiments, the input to the real-world pipeline is not directly as feature
vectors. Given a cohort of patients with their event sequences, like [9] the pipeline for non-responder
classiﬁcation has two more additional steps than the pipeline described in previous benchmark
experiments: 1) Feature construction to convert patient event sequence data into numerical feature
vectors. This step can be quite time-consuming as advanced feature construction techniques like
sequential mining [29] and tensor factorization [19] can be expensive to compute. On this medical
i) frequency threshold to remove rare
dataset, we consider two kinds of parameters in this step:
events (frequency ranging from 2 to 5) ii) various aggregation functions (including binary, count,
sum and average) to aggregate multiple occurrence of events into features. The output of this step
will be a feature matrix and corresponding classiﬁcation targets; 2) Densify the feature matrix
from above feature construction step if necessary. Features of this real-world dataset can be quite
sparse. We by default use sparse representation to save space and accelerate computation in some
algorithms. Unfortunately, not all algorithm implementations in scikit-learn accept sparse features.
A decision has to be made here: either sparse matrix for faster result or dense matrix for broader
algorithm choices in later steps.

We run the experiments on same machine, use same parameter setting and same budget as
benchmark experiments. We compare our method with the same baselines as benchmark experi-
ments and we continue using error rate as metric. Our algorithm has built-in caching mechanism

16

and we will use that. For this real-world dataset, we ﬁrst compare with baselines with cache
enabled. Then we analyze the contribution of caching.

5.1 Results and Discussions

Table 4 shows the performance of our methods compared to baselines when caching is enabled. Due
to lack of space we only report the test performance. All cases FLASH and FLASH(cid:63) signiﬁcantly
outperform all the baselines.

Figure 5(c) shows the performance of FLASH without caching and original FLASH with caching
on the real-world medical dataset. With caching, more pipeline paths can be evaluated within
given period of time and our EIPS-based path selection leverages caching to select paths with high
performance that run fast. As a result, we can see FLASH with caching converges much faster.
For example, with caching we can get low test error within 6 hours.

6 Conclusions

In this work, we propose a two-layer Bayesian optimization algorithm named FLASH, which enables
highly eﬃcient optimization of complex data analytic pipelines. We showed that all components
of FLASH complement each other: 1) our optimal design strategy ensures better initialization,
giving a head start to the optimization procedure; 2) the cost-sensitive model takes advantage of
this head start, and signiﬁcantly improves the performance by pruning ineﬃcient pipeline paths;
3) the pipeline caching reduces the cost during the entire optimization, which provides a global
acceleration of our algorithm. We demonstrate that our method signiﬁcantly outperforms previous
state-of-the-art approaches in both benchmark and real-world experiments.

7 Acknowledgments

This work was supported by the National Science Foundation, award IIS- #1418511 and CCF-
#1533768, research partnership between Children’s Healthcare of Atlanta and the Georgia Institute
of Technology, CDC I-SMILE project, Google Faculty Award, Sutter health and UCB.

17

References

2012.

[1] A. Atkinson and A. Donev. Optimum experimental designs. 1992.

[2] T. B¨ack. Evolutionary algorithms in theory and practice. 1996.

[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 13(1),

[4] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures. In ICML, 2013.

[5] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl. Algorithms for hyper-parameter opti-

mization. In NIPS, 2011.

[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[7] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.

[8] G. Calinescu, C. Chekuri, M. P´al, and J. Vondr´ak. Maximizing a monotone submodular

function subject to a matroid constraint. SIAM Journal on Computing, 40(6), 2011.

[9] R. Chen, H. Su, Y. Zhen, M. Khalilia, D. Hirsch, M. Thompson, T. Davis, Y. Peng, S. Lin,
J. Tejedor-Sojo, E. Searles, and J. Sun. Cloud-based predictive modeling system and its
application to asthma readmission prediction. In AMIA. AMIA, 2015.

[10] S. J. Coakes and L. Steed. SPSS: Analysis without anguish using SPSS version 14.0 for

Windows. John Wiley & Sons, Inc., 2009.

[11] D. Donoho. 50 years of Data Science. Technical report, University of California Berkeley, 2015.

[12] K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown.
Towards an empirical foundation for assessing bayesian optimization of hyperparameters. In
NIPS workshop on Bayesian Optimization in Theory and Practice, 2013.

[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Eﬃcient and

robust automated machine learning. In NIPS, 2015.

[14] M. Feurer, T. Springenberg, and F. Hutter. Initializing bayesian hyperparameter optimization

via meta-learning. In AAAI, 2015.

[15] P. Flaherty, A. Arkin, and M. I. Jordan. Robust design of biological experiments. In NIPS,

2005.

[16] C. J. Flynn, C. M. Hurvich, and J. S. Simonoﬀ. Eﬃciency for regularization parameter selection

in penalized likelihood estimation of misspeciﬁed models. JASA, 108(503), 2013.

[17] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data

mining software: an update. ACM SIGKDD explorations newsletter, 11(1), 2009.

18

[18] X. He. Laplacian regularized d-optimal design for active learning and its application to image

retrieval. Image Processing, IEEE Transactions on, 19(1), 2010.

[19] J. C. Ho, J. Ghosh, and J. Sun. Marble: high-throughput phenotyping from electronic health

records via sparse nonnegative tensor factorization. In KDD, 2014.

[20] M. D. Hoﬀman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization.

In UAI. Citeseer, 2011.

[21] M. D. Hoﬀman, B. Shahriari, and N. de Freitas. On correlation and budget constraints in
In AIS-

model-based bandit optimization with application to automatic machine learning.
TATS, 2014.

[22] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general

algorithm conﬁguration. In Learning and Intelligent Optimization. Springer, 2011.

[23] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, 2013.

[24] B. Komer, J. Bergstra, and C. Eliasmith. Hyperoptsklearn: Automatic hyperparameter con-

ﬁguration for scikitlearn. In ICML workshop on AutoML, 2014.

[25] A. Krause and C. Guestrin. Submodularity and its applications in optimized information

gathering. TIST, 2(4), 2011.

[26] A. Kumar, R. McCann, J. Naughton, and J. M. Patel. Model selection management systems:

The next frontier of advanced analytics. ACM SIGMOD Record, 2015.

[27] D. J. Lizotte. Practical bayesian optimization. University of Alberta, 2008.

[28] J. Lv and J. S. Liu. Model selection principles in misspeciﬁed models. JRSS-B, 76(1), 2014.

[29] K. Malhotra, T. Hobson, S. Valkova, L. Pullum, and A. Ramanathan. Sequential pattern min-
ing of electronic healthcare reimbursement claims: Experiences and challenges in uncovering
how patients are treated by physicians. In Big Data, Oct 2015.

[30] X. Meng, J. Bradley, E. Sparks, and S. Venkataraman. Ml pipelines: a new high-level api for

mllib, 2015.

[31] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. Yale: Rapid prototyping for

complex data mining tasks. In KDD, 2006.

[32] J. Mockus, V. Tiesis, and A. Zilinskas. The application of bayesian methods for seeking the

extremum. Towards Global Optimization, 2(117-129), 1978.

[33] R. Munos. Optimistic optimization of deterministic functions without the knowledge of its

smoothness. In Advances in neural information processing systems, 2011.

[34] Y. Nesterov. Gradient methods for minimizing composite objective function, 2007.

[35] K. Ng, A. Ghoting, S. R. Steinhubl, W. F. Stewart, B. Malin, and J. Sun. Paramo: A parallel
predictive modeling platform for healthcare analytic research using electronic health records.
Journal of biomedical informatics, 48, 2014.

19

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. JMLR,
12, 2011.

[37] T. Robertazzi and S. Schwartz. An accelerated sequential algorithm for producing d-optimal

designs. SIAM Journal on Scientiﬁc and Statistical Computing, 10(2), 1989.

[38] B. Settles. Active Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning,

6(1), jun 2012.

[39] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of
the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.

[40] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor selection: Leveraging submodularity.

In CDC, 2010.

algorithms. In NIPS, 2012.

[41] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning

[42] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Ali,
R. P. Adams, et al. Scalable bayesian optimization using deep neural networks. arXiv preprint
arXiv:1502.05700, 2015.

[43] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.

[44] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka: Combined selection

and hyperparameter optimization of classiﬁcation algorithms. In KDD, 2013.

[45] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global opti-
mization of expensive-to-evaluate functions. Journal of Global Optimization, 44(4), 2009.

[46] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high

dimensions via random embeddings. In IJCAI. Citeseer, 2013.

[47] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

[48] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 1982.

20

Table 5: Detailed information of the data analytic pipeline constructed for benchmark experiments.

Algorithm

#categorical #continuous

Total
#hyper-
parameters

A Appendix

Pipeline
step

Feature
rescaling

Sample
balancing

Feature
preprocessing

Classiﬁcation

0
0
0
0

0
0

2
3
2
1
0
0
0
1
1
1
0
1
1

1
1
2
0
0
2
1
0
2
1
1
0
2
4

Min-max scaler
None
Normalization
Standardization

Class weighting
None

Extremely randomized trees
Fast ICA
Feature agglomeration
Kernel PCA
Random kitchen sinks
Linear SVM
None
Nystroem sampler
PCA
Polynomial combinations
Random trees embedding
Percentile feature selection
Univariate feature selection

AdaBoost
Decision tree
Extremely randomized trees
Gaussian naive Bayes
Gradient boosting
K-nearest neighbors
LDA
Linear SVM
Kernel SVM
Multinomial naive Bayes
Passive aggressive
QDA
Random forest
Stochastic gradient descent

21

0
0
0
0

0
0

3
1
1
6
2
2
0
8
1
2
4
1
2

3
3
3
0
6
1
3
2
5
1
2
1
3
6

0
0
0
0

0
0

5
4
3
7
2
2
0
9
2
3
4
2
3

4
4
5
0
6
3
4
2
7
2
3
1
5
10

Total#

33

30

72

102

6
1
0
2
 
n
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
6
4
6
0
.
2
0
6
1
:
v
i
X
r
a

FLASH: Fast Bayesian Optimization for
Data Analytic Pipelines

Yuyu Zhang Mohammad Taha Bahadori Hang Su

Jimeng Sun

Georgia Institute of Technology
{yuyu,bahadori,hangsu}@gatech.edu,jsun@cc.gatech.edu

Abstract

Modern data science relies on data analytic pipelines to organize interdependent compu-
tational steps. Such analytic pipelines often involve diﬀerent algorithms across multiple steps,
each with its own hyperparameters. To achieve the best performance, it is often critical to select
optimal algorithms and to set appropriate hyperparameters, which requires large computational
eﬀorts. Bayesian optimization provides a principled way for searching optimal hyperparame-
ters for a single algorithm. However, many challenges remain in solving pipeline optimization
problems with high-dimensional and highly conditional search space. In this work, we propose
Fast LineAr SearcH (FLASH), an eﬃcient method for tuning analytic pipelines. FLASH is
a two-layer Bayesian optimization framework, which ﬁrstly uses a parametric model to select
promising algorithms, then computes a nonparametric model to ﬁne-tune hyperparameters of
the promising algorithms. FLASH also includes an eﬀective caching algorithm which can fur-
ther accelerate the search process. Extensive experiments on a number of benchmark datasets
have demonstrated that FLASH signiﬁcantly outperforms previous state-of-the-art methods in
both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%
improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art
performance on a real-world application for healthcare predictive modeling.

1 Introduction

Modern data science often requires many computational steps such as data preprocessing, fea-
ture extraction, model building, and model evaluation, all connected in a data analytic pipeline.
Pipelines provide a natural way to represent, organize and standardize data analytic tasks, which
are considered to be an essential element in the data science ﬁeld [11] due to their key role in large-
scale data science projects. Many machine learning toolboxes such as scikit-learn [36], RapidMiner
[31], SPSS [10], Apache Spark [30] provide mechanisms for conﬁguring analytic pipelines.

An analytic pipeline skeleton is shown in Figure 1. Each step, such as feature preprocessing
and classiﬁcation, includes many algorithms to choose from. These algorithms usually require
users to set hyperparameters, ranging from optimization hyperparameters such as learning rate and
regularization coeﬃcients, to model design hyperparameters such as the number of trees in random
forest and the number of hidden layers in neural networks. There are an exponential number of
choices for the combination of algorithms and hyperparameters in a given analytic pipeline skeleton.
Because of the interdependency between all the algorithms and their hyperparameters, the choices
can have huge impact on the performance of the best model.

1

Figure 1: A typical data analytic pipeline.

Tuning hyperparameters of a single algorithm can be viewed as an optimization problem of a
black-box objective function, which is noisy and often expensive to evaluate. Here the input of
black-box are the hyperparameters, and the objective function is the output performance such as
accuracy, precision and recall. To tackle this problem, simple methods have been applied such as
grid or random search [5, 3]. While on diﬃcult problems where these simple approaches are not
eﬃcient, a more promising model-based approach is Bayesian optimization [32, 27, 7, 39]. The high-
level idea of Bayesian optimization is to deﬁne a relatively cheap surrogate function and use that
to search the hyperparameter space. Indeed, there exist other global optimization methods, such
as evolutionary algorithms [2] and optimistic optimization [33]. We choose Bayesian optimization
framework due to its great performance in practice. Recently, Bayesian optimization methods have
been shown to outperform other methods on various tasks, and in some cases even beat human
domain experts to achieve better performance via tuning hyperparameters [42, 4].

Despite its success, applying Bayesian optimization for tuning analytic pipelines faces several
signiﬁcant challenges: Existing Bayesian optimization methods are usually based on nonparametric
models, such as Gaussian process and random forest. A major drawback of these methods is that
they require a large number of observations to ﬁnd reasonable solutions in high-dimensional space.
When tuning a single algorithm with several hyperparameters, Bayesian optimization works well
with just a few observations. However, when it comes to pipeline tuning, thousands of possible
combinations of algorithms plus their hyperparameters jointly create a large hierarchical high-
dimensional space to search over, whereas existing methods tend to become ineﬃcient. Wang et
al. [46] tackled the high-dimensional problem by making a low eﬀective dimensional assumption.
However, it is still a ﬂat Bayesian optimization method and not able to handle the exploding
dimensionality problem caused by hierarchically structured hyperparameters in analytic pipeline
tuning.
Motivating example: We build an analytic pipeline for classiﬁcation task (details in Section 4). If
we give 10 trials for each hyperparameter over 1,456 unique pipeline paths and 102 hyperparameters,
we have more than 2 million conﬁgurations, which can take years to complete with a brute-force
search. Even with the state-of-the-art Bayesian optimization algorithm such as Sequential Model-
based Algorithm Conﬁguration (SMAC) [22], the process can still be slow as shown in Figure 2.
If we know the optimal algorithms ahead time (Oracle) with just hyperparameter tuning of the
optimal algorithms, we can obtain signiﬁcant time saving, which is however not possible. Finally,
our proposed method FLASH can converge towards the oracle performance much more quickly
than SMAC.

In this paper, we propose a two-layer Bayesian optimization algorithm called Fast LineAr SearcH
(FLASH): the ﬁrst layer for selecting algorithms, and the second layer for tuning the hyperparame-
ters of selected algorithms. FLASH is able to outperform the state-of-the-art Bayesian optimization
algorithms by a large margin, as shown in Figure 2. By designing FLASH, we make three main
contributions:

• We propose a linear model for propagation of error (or other quantitative metrics) in analytic

2

Figure 2: Performance comparison of a data analytic pipeline on MRBI dataset, including the
previous state of the art SMAC, proposed method FLASH, and Oracle (i.e., pretending the optimal
algorithm conﬁguration is given, and only performing hyperparameter tuning with SMAC on those
algorithms). We show the median percent error rate on the test set along with standard error bars
(generated by 10 independent runs) over time. FLASH outperforms SMAC by a big margin and
converges toward Oracle performance quickly.

pipelines. We also propose a Bayesian optimization algorithm for minimizing the aggregated error
using our linear error model. Our proposed mechanism can be considered as a hybrid model: a
parametric linear model for fast exploration and pruning of the algorithm space, followed by a
nonparametric hyperparameter ﬁne-tuning algorithm.

• We propose to initialize the hyperparameter tuning algorithm using the optimal design strategy
[1, 15, 38] which is more robust than the random initialization. We also propose a fast greedy
algorithm to eﬃciently solve the optimal design problem for any given analytic pipeline.

• Finally, we introduce a caching algorithm that can signiﬁcantly accelerate the tuning process. In
particular, we model the (time) cost of each algorithm, and incorporate that in the optimization
process. This ensures the eﬃciency of fast search.

We demonstrate the eﬀectiveness of FLASH with extensive experiments on a number of diﬃcult
problems. On the benchmark datasets for pipeline conﬁgurations tuning, FLASH substantially
improves the previous state of the art by 7% to 25% in test error rate within the same time budget.
We also experiment with large-scale real-world datasets on healthcare data analytic tasks where
FLASH also exhibits superior results.

2 Background and Related Work

2.1 Data Analytic Pipelines

The data analytic pipeline refers to a framework consisting of a sequence of computational transfor-
mations on the data to produce the ﬁnal predictions (or outputs) [26]. Pipelines help users better

3

understand and organize the analysis task, as well as increase the reusability of algorithm imple-
mentations in each step. Several existing widely adopted machine learning toolboxes provide the
functionality to run analytic pipelines. Scikit-learn [36] and Spark ML [30] provide programmatic
ways to instantiate a pipeline. SPSS [10] and RapidMiner [31] provide a visual way to assemble
an analytic pipeline instance together and run. Microsoft Azure Machine Learning1 provides a
similar capability in a cloud setting. There are also specialized pipelines, such as PARAMO [35] in
healthcare data analysis.

However, a major diﬃculty in using these systems is that none of the above described tools is
able to eﬃciently help users decide which algorithms to use in each step. Some of the tools such as
scikit-learn, Spark ML, and PARAMO allow searching all possible pipeline paths and tuning the
hyperparameters of each step using an expensive grid search approach. While the search process
can be sped up by running in parallel, the search space is still too large for the exhaustive search
algorithms.

2.2 Bayesian Optimization

Bayesian optimization is a well-established technique for global and black-box optimization prob-
lems. In a nutshell, it comprises two main components: a probabilistic model and an acquisition
function. For the probabilistic model, there are several popular choices: Gaussian process [41, 42],
random forest such as Sequential Model-based Algorithm Conﬁguration (SMAC) [22], and density
estimation models such as Tree-structured Parzen Estimator (TPE) [5]. Given any of these models,
the posterior mean and variance of a new input can be computed, and used for computation of
the acquisition function. The acquisition function deﬁnes the criterion to determine future input
candidates for evaluation. Compared to the objective function, the acquisition function is chosen
to be relatively cheap to evaluate, so that the most promising next input for querying can be found
quickly. Various forms of acquisition functions have been proposed [43, 20, 45, 21]. One of the
most prominent acquisition function is the Expected Improvement (EI) function [32], which has
been widely used in Bayesian optimization. In this work, we use EI as our acquisition function,
which is formally described in Section 3.

Bayesian optimization is known to be successful in tuning hyperparameters for various learning
algorithms on diﬀerent types of tasks [42, 14, 4, 41, 46]. Recently, for the problem of pipeline con-
ﬁgurations tuning, several Bayesian optimization based systems have been proposed: Auto-WEKA
[44] which applies SMAC [22] to WEKA [17], auto-sklearn [13] which applies SMAC to scikit-learn
[36], and hyperopt-sklearn [24] which applies TPE [5] to scikit-learn. The basic idea of applying
Bayesian optimization to pipeline tuning is to expand the hyperparameters of all algorithms and
create large search space to perform optimization as we will show in the experiments. However,
for practical pipelines the space becomes too large which hinders convergence of the optimization
process. Auto-sklearn [13] uses a meta-learning algorithm that leverages performance history of
algorithms on existing datasets to reduce the search space. However, in real-world applications,
we often have unique datasets and tasks such that ﬁnding similar datasets and problems for the
meta-learning algorithm will be diﬃcult.

1https://studio.azureml.net

4

Figure 3: A toy example of data analytic pipeline. One possible pipeline path, ﬂowing from the
input Vin to the output Vout, is highlighted in shaded area.

3 Methodology

A data analytic pipeline G = (V, E) can be represented as a multi-step Directed Acyclic Graph
(DAG), where V is the set of algorithms, and E is the set of directed edges indicating dependency
between algorithms. Algorithms are distributed among multiple steps. Let V (k)
denote the ith
i
algorithm in the kth step. Each directed edge (V (k)
) ∈ E represents the connection from
algorithm V (k)
. Note that there is no edge between algorithms in the same step. We also
have an input data vertex Vin which points to all algorithms in the ﬁrst step, and an output vertex
Vout which is pointed by all algorithms in the last step.

to V (k+1)
j

, V (k+1)
j

i

i

A pipeline path is any path from the input Vin to the output Vout in pipeline graph G . To denote
a pipeline path of K steps, we use K one-hot vectors p(k) (1 ≤ k ≤ K), each denoting the algorithm
selected in the k-th step. Thus, the concatenation of one-hot vectors p = (cid:2)p(1), . . . , p(K)(cid:3) ∈ {0, 1}N
denotes a pipeline path, where N is the total number of algorithms in the pipeline G. Figure 3
shows a small data analytic pipeline with two steps. The ﬁrst step contains two algorithms, and the
second step contains three. One possible pipeline path is highlighted in the shaded area. On this
pipeline path, V (1)
are selected in the ﬁrst and second step, so that we have p(1) = [0, 1]
and p(2) = [0, 0, 1]. Thereby, the highlighted pipeline path is given by p = (cid:2)p(1), p(2)(cid:3) = [0, 1, 0, 0, 1].
For any pipeline path p, we concatenate all of its hyperparameters in a vector λp. The pair of path
and hyperparameters, i.e. (p, λp), forms a pipeline conﬁguration to be run. For ease of reference,
we list the notations in Table 1.

and V (2)

3

2

The problem of tuning data analytic pipelines can be formalized as an optimization problem:

Problem 1. Given a data analytic pipeline G with input data D, resource budget T , evaluation
metric function m(G, D; p, λp), resource cost of running pipeline τ (G, D; p, λp), how to ﬁnd the
pipeline path p and its hyperparameters λp with best performance m(cid:63)?

The performance of the best pipeline path is denoted by m(cid:63) = minp,λp m(G, D; p, λp) subject

5

Table 1: Mathematical notations used in this paper.

Symbol Description

G
V
E
K
N
D
Vin
Vout
V (k)
i
p(k)
λ(k)
p
p
λp
m(·)
τ (·)
Tinit
Tprune
Ttotal

data analytic pipeline
set of algorithms in G
set of dependency between algorithms
total number of steps in G
total number of algorithms in G
input data of pipeline
input vertex of G
output vertex of G
ith algorithm in kth step
one-hot vector for kth step
hyperparameters for kth step
pipeline path
all hyperparameters of p
evaluation metric function
time cost of running pipeline
budget for Phase 1
budget for Phase 2
budget for Phase 3

to budget T . The objective is to approach the optimal performance within the budget T via
(cid:98)p) ≤ m(cid:63) + (cid:15) for
optimizing over p, λp; i.e., we would like our solution (cid:98)p, (cid:98)λ
small values of (cid:15).

(cid:98)p to satisfy m(G, D; (cid:98)p, (cid:98)λ

To eﬃciently tackle this problem, we propose a two-layer Bayesian optimization approach named
Fast LineAr SearcH (FLASH). We generally introduce the idea of linear model and describe the
algorithm in Section 3.1. An immediate advantage of using linear model is that we can use more
principled initialization instead of random initialization, as discussed in Section 3.2. We use cost-
sensitive modeling to prune the pipeline, as described in Section 3.3. Finally, we accelerate the
entire optimization procedure via pipeline caching, which we describe in Section 3.4.

3.1 Two-layer Bayesian Optimization

Inspired by the performance of linear regression under model misspeciﬁcation [48, 16, 28] and supe-
rior sample complexity compared to more ﬂexible nonparametric techniques [47], we seek parametric
models for propagation of error (or other quantitative metrics) in analytic pipelines. The high level
idea of FLASH is as follows: we propose a linear model for estimating the propagation of error (or
any other metric) in a given analytic pipeline. The linear model assumes that the performance of
algorithms in diﬀerent steps are independent, and the ﬁnal performance is additive from all algo-
rithms. That is, we can imagine that each algorithm is associated with a performance metric, and
the total performance of a pipeline path is the sum of the metrics for all algorithms in the path.
This linear model will replace the Gaussian process or random forest in the initial stages of the
pipeline tuning process. In the rest of this section, we provide the details of Bayesian optimization

6

with our linear model.

We apply the linear model only to the pipeline selection vector p and assume that the variations
due to hyperparameters of the algorithms are captured in the noise term. That is, we assume that
the error of any pipeline path p can be written as

m = β(cid:62)p + ε

where β ∈ RN denotes the parameters of the linear model. Given a set of observations of the
algorithm selection and the corresponding evaluation metric for the selected pipeline path in the
form of (pi, mi), i = 1, . . . , n, we can ﬁt this model and infer its mean µ(p) and variance σ2(p)
of the performance estimation for any new pipeline path represented by p. In particular, let the
design matrix P ∈ Rn×N denote the stacked version of the pipeline paths, i.e., P = [p1, . . . , pn](cid:62),
and m ∈ Rn be the corresponding response values of the evaluation metrics, m = [m1, . . . , mn]. We
use the following L2 regularized linear regression to obtain the robust estimate for β from history
observations:

(cid:98)β(P , m) = argmin

β

(cid:26) 1
n

(cid:107)P β − m(cid:107)2

2 + λ(cid:107)β(cid:107)2
2

(cid:27)

(1)

(cid:113)(cid:0)(cid:80)n

(cid:1). The predictive
where for any vector x ∈ Rn the L2 norm is deﬁned as (cid:107)x(cid:107)2 =
distribution for the linear model is Gaussian with mean (cid:98)µp = (cid:98)β(cid:62)p and variance (cid:98)σp = σ2
ε (1 +
p(cid:62)(P (cid:62)P + λI)−1p) where σ2
ε is the variance of noise in the model. We estimate σε as follows: the
residual in the ith observation is computed as (cid:98)(cid:15)i = (cid:98)µi − mi where (cid:98)µi = (cid:98)β(cid:62)pi is the estimate of mi
by our model. Thus, the variance of the residual can be found as (cid:98)σ2
(cid:15) = var((cid:98)µi − mi) where var(·)
denotes the variance operator.

i=1 x2
i

To perform Bayesian optimization with linear model, we use the popular Expected Improvement
(EI) criteria, which recommends to select the next sample pt+1 such that the following acquisition
function is maximized. The acquisition function represents the expected improvement over the best
observed result m+ at a new pipeline path p [44]:

EI(p) = E[Im+(p)] = σp[uΦ(u) + φ(u)]

(2)

u = m+−ξ−µp

σp

where
and ξ is a parameter to balance the trade-oﬀ between exploitation and
exploration. EI function is maximized for paths with small values of mp and large values of σp,
reﬂecting the exploitation and exploration trade-oﬀs, respectively. To be more speciﬁc, larger ξ
encourages more exploration in selecting the next sample. The functions Φ(·) and φ(·) represent
CDF and PDF of standard normal distribution, respectively. The idea of Bayesian optimization
with EI is that at each step, we compute the EI with the predictive distribution of the existing linear
model and ﬁnd the pipeline path that maximizes EI. We choose that path and run the pipeline
with it to obtain a new (pi, mi) pair. We use this pair to reﬁt and update our linear model and
repeat the process. Later on we also present an enhanced version of EI via normalizing it by cost
called Expected Improvement Per Second (EIPS).

We provide the full details of FLASH in Algorithm 1. While the main idea of FLASH is
performing Bayesian optimization using linear model and EI, it has several additional ideas to
make it practical. Speciﬁcally, FLASH has three phases:

• Phase 1, we initialize the algorithm using ideas from optimal design, see Section 3.2. The

budget is bounded by Tinit.

7

Algorithm 1: Fast Linear Search (FLASH)

input : Data analytic pipeline G; input data D; total budget for entire optimization Ttotal;

budget for initialization Tinit; budget for pipeline pruning Tprune; number of top
pipeline paths r

(cid:98)p

output: Optimized pipeline conﬁguration (cid:98)p and (cid:98)λ
/* Phase 1: Initialization (Section 3.2)
1 while budget Tinit not exhausted do
2

p ← new pipeline path from Algorithm 2
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:99)βτ (P , τ ) using Eq. (1)

5

3

4

6

/* Phase 2: Pipeline pruning (Section 3.3)
7 while budget Tprune not exhausted do
8

9

10

11

p ← argmaxp EIP S(p, P , β, βτ ) using Eq. (4)
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:98)βτ (P , τ ) using Eq. (1)

using Eq. (4)
/* Phase 3: Pipeline tuning

14 S ← history observations within G(cid:48)
15 Initialize model M given S
16 while budget Ttotal not exhausted do
p, λp ← next candidate from M
17
m ← RunPipeline(G(cid:48), D; p, λp) with Algorithm 3
S ← S ∪ {(p, λp, m)}
Update M given S
(cid:98)p ← Best conﬁguration so far found for G(cid:48)

21 (cid:98)p, (cid:98)λ

19

18

20

12
13 G(cid:48) ← construct subgraph of G with top r pipeline paths with largest EIP S(p, P , β, βτ )

*/

*/

*/

• Phase 2, we leverage Bayesian optimization to ﬁnd the top r best pipeline paths and prune

the pipeline G to obtain simpler one G(cid:48). The budget is bounded by Tprune

2.

• Phase 3, we use general model-based Bayesian optimization methods to ﬁne-tune the pruned

pipeline together with their hyperparameters.

In Phase 3, we use state-of-the-art Bayesian optimization algorithm, either SMAC or TPE. These
algorithms are iterative: they use a model M such as Gaussian process or random forest and use
EI to pick up a promising pipeline path with hyperparameters for running, and then update the
model with the new observation just obtained, and again pick up the next one for running. The

2In practice, it is better to use the time normalized EI (that is EIPS) during Phase 2; this idea is described in

Section 3.3.

8

i=1; number of desired pipeline paths ninit

Algorithm 2: Initialization with Optimal Design

/* Batch version
input : B initial candidates {pi}B
output: Optimal set of pipeline paths Q
1 p1 ← random pipeline path for initialization
2 H ← p1p(cid:62)
1
3 Q ← {p1}
4 for (cid:96) = 2, . . . , ninit do
5

j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)
H ← H + pj(cid:63)p(cid:62)
j(cid:63)
Q ← Q ∪ {pj(cid:63)}

6

7

j ) for j = 1, . . . , B.

/* Online version
input : B candidates {pi}B
output: Next pipeline path pj(cid:63), j(cid:63) ∈ {1, . . . , B}

i=1; current Gram matrix H

8 j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)

j ) for j = 1, . . . , B

*/

*/

budget is bounded by Ttotal. Note that our algorithm is currently described for a sequential setting
but can be easily extended to support parallel runs of multiple pipeline paths as well.

3.2 Optimal Design for Initialization

Most Bayesian optimization algorithms rely on random initialization which can be ineﬃcient; for
example, it may select duplicate pipeline paths for initialization. Intuitively, the pipeline paths
used for initialization should cover the pipeline graph well, such that all algorithms are included
enough times in the initialization phase. The ultimate goal is to select a set of pipeline paths
for initialization such that the error in estimation of β is minimized. Given our proposed linear
model, we can ﬁnd the optimal strategy for initialization to make sure the pipeline graph is well
covered and the tuning process is robust. In this section, we describe diﬀerent optimality criteria
studied in statistical experiment design [1, 15] and active learning [38], and design an algorithm for
initialization step of FLASH.

Given a set of pipeline paths with size n, there are several diﬀerent optimality criteria in terms
i=1 pip(cid:62)

i as follows [38]:

of the eigenvalues of the Gram matrix H = (cid:80)n
A-optimality: maximize (cid:80)n
D-optimality: maximize (cid:81)n

(cid:96)=1 λ(cid:96)(H).

(cid:96)=1 λ(cid:96)(H).

E-optimality: maximize λn(H), the nth largest eigenvalue.

It is easy to see that any arbitrary set of pipeline path designs satisﬁes the A-optimality criterion.

Proposition 1. Any arbitrary set of pipeline paths with size n is a size-n A-optimal design.

Proof. For any arbitrary set of pipeline paths with size n, we have:

n
(cid:88)

(cid:96)=1

λ(cid:96)(H) = tr(H) = tr

(cid:33)

pip(cid:62)
i

=

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:16)

tr

pip(cid:62)
i

(cid:17)

= nK.

9

The last step is due to particular pattern of p in our problem. Thus, we show that (cid:80)n
constant, independent of the design of pipeline paths.

(cid:96)=1 λ(cid:96)(H) is

Proposition 1 rules out use of A-optimality in pipeline initialization. Given the computational
complexity of E-optimality and the fact that it intends for optimality in the extreme cases, we
choose D-optimality criterion. The D-optimality criterion for design of optimal linear regression
can be stated as follows: suppose we are allowed to evaluate ninit samples pi, i = 1, . . . , ninit,
these samples should be designed such that the determinant of the Gram matrix H is maximized.
While we can formulate an optimization problem that directly ﬁnds pi values, we found that an
alternative approach can be computationally more eﬃcient. In this approach, we ﬁrst generate B
candidate pipeline paths for an integer B larger than the number of algorithms in the pipeline N .
This set may include all possible pipeline paths if the total number of paths is small. Then, our
goal becomes selecting a subset of size ninit from them. We can formulate the optimal design as
follows

a(cid:63) = argmax

det

a

(cid:41)

aipip(cid:62)
i

(cid:40) B
(cid:88)

i=1

s.t.

a ∈ {0, 1}K,

1(cid:62)a = ninit.

(3)

The last constraint 1(cid:62)a = ninit indicates that only ninit pipeline paths should be selected. The
objective function is concave in terms of continuous valued a [6, Chapter 3.1.5]. Thus, a traditional
approach is to solve it by convex programming after relaxation of the integrality constraint on a.
The matrix in the argument of the determinant is only N -dimensional which means calculation of
the determinant should be fast. Nestrov’s accelerated gradient descent [34] or Frank-Wolfe’s [23]
algorithms can be used for eﬃciently solving such problems.

An even faster solution can be found by using greedy forward selection ideas which are fast
and popular for optimal experiment design, for example see [37, 18, 25] and the references therein.
To apply greedy technique to our problem, we initialize the solution by picking one of the pipeline
i . Then, at (cid:96)th step, we add the path that maximizes j(cid:63) = argmaxj D(cid:96)(H + pjp(cid:62)
path H = pip(cid:62)
j )
where D(cid:96)(H) = (cid:81)min((cid:96),p)
λi(H) denotes the product of top min((cid:96), p) eigenvalues of its argument.
The algorithm is described in Algorithm 2. The optimization problem in Eq. (3) appears in other
ﬁelds such as optimal facility location and sensor planning where greedy algorithm is known to
have a 1 − 1

e approximation guarantee [8, 40].

One further desirable property of the greedy algorithm is that it is easy to run it under a
time budget constraint. We call this version the online version in Algorithm 2, where instead of a
ﬁxed number of iteration ninit, we run it until the exhaustion of our time budget. See Line 2 in
Algorithm 1 and the online version of Algorithm 2.

i=1

3.3 Cost-sensitive Modeling

The Expected Improvement aims at approaching the true optimal (doing well) within a small number
of function evaluations (doing fast). However, the time cost of each function evaluation may vary
a lot due to diﬀerent settings of hyperparameters. This problem is particularly highlighted in
pipeline conﬁgurations tuning, since the choice of algorithms can make a huge diﬀerence in running
time. Therefore, fewer pipeline runs are not always “faster” in terms of wall-clock time. Also, in
practice, what we care about is the performance we can get within limited resource budget, rather

10

than within certain evaluation times. That is why we need cost-sensitive modeling for the pipeline
tuning problem.

Expected Improvement Per Second (EIPS) [41] proposes another acquisition function for tuning
of a single learning algorithm by dividing the EI of each hyperparameter by its runtime. To apply
EIPS in pipeline tuning problem, we use a separate linear model to model the total runtime of
pipeline paths. Similar to the linear model for error propagation, the linear model for time assumes
that on a pipeline path each algorithm partly contributes to the total time cost and the runtimes are
additive. To apply the linear model, we replace the performance metric m with the cost metric τ .
The linear cost model parametrized by βτ can be eﬃciently updated using Eq. (1). As described in
Algorithm 1, βτ will be updated together with β at the end of Phase 2. We note that, in practice,
the budget T and the cost τ (·) can be any quantitative costs of budgeted resources (e.g., money,
CPU time), which is a natural generalization of our idea.

With the cost model above, we get the cost-sensitive acquisition function over the best observed

result m+ at a new pipeline path p:

E[Im+(p)]
E[log τ (p)]

=

σp[uΦ(u) + φ(u)]
E[log τ (p)]

(4)

EIP S(p, P , β, βτ ) =

where u =

m+ − ξ − µp
σp

.

Here the dependency in β and βτ is captured during computation of µp, σp, and τ (p). We take
logarithm of cost τ (·) to compensate the large variations in the runtime of diﬀerent algorithms.
This acquisition function balances “doing well” and “doing fast” in selecting the next candidate
path to run. During the optimization, it will help avoid those costly paths with poor expected
improvement. More importantly, at the end of Phase 2 in Algorithm 1, EIPS is responsible to
determine the most promising paths, which perform better but cost less, to construct a subgraph
for the last phase ﬁne-tuning. For this purpose, we set the exploration parameter ξ to 0 to only
select (Line 13 in Algorithm 1).

3.4 Pipeline Caching

During the experiments, we note that many pipeline runs have overlapped algorithms in their paths.
Sometimes these algorithms have exactly the same pipeline path and the same hyperparameter
settings along the path. This means that we are wasting time on generating the same intermediate
output again and again. For example, consider the min-max normalization algorithm in the ﬁrst
pipeline step: this algorithm will be executed many times, especially when it performs well so that
Bayesian optimization methods prefer to choose it.

To reduce this overhead, we propose a pipeline caching algorithm, as described in Algorithm 3.
When running a pipeline, we check the cache before we run each algorithm. If it turns out to be a
cache hit, the result will be immediately returned from cache. Otherwise, we run the algorithm and
cache the result. There is a caching pool (e.g., disk space, memory usage) for this algorithm. We
use the Least Recently Used (LRU) strategy to clean up the caching pool when budget becomes
exhausted.

Caching can signiﬁcantly reduce the cost of pipeline runs, and accelerates all three phases of
FLASH. Algorithms closer to the pipeline input vertex, usually the data preprocessing steps, have
higher chance to hit the cache. In fact, when we deal with large datasets on real-world problems,
the preprocessing step can be quite time-consuming such that caching can be very eﬃcient.

11

Algorithm 3: Pipeline Caching

input : Data analytic pipeline G; input data D; pipeline conﬁguration p and λp to be run;

available caching budget Tcache; current cache pool C

1 D(1) ← D

/* Run pipeline with cache pool

2 for k ← 1, . . . , K do

3

4

5

6

7

8

h ← Hash(p(1) . . . p(k), λ(1)
if h ∈ C then

p . . . λ(k)
p )

D(k+1) ← cached result from C

else

D(k+1) ← RunAlgorithm(G, D(k), p(k), λ(k)
p )
C ← C ∪ {(cid:104)h, D(k+1)(cid:105)}

/* Clean up cache pool when necessary

9 if Tcache exhausted then
10

Discard least recently used (LRU) items in C

4 Benchmark Experiments

*/

*/

In this section, we perform extensive experiments on a number of benchmark datasets to evaluate
our algorithm compared to the existing approaches. Then we study the impact of diﬀerent algorithm
choices in each component of FLASH.

Benchmark Datasets We conduct experiments on a group of public benchmark datasets on
classiﬁcation task, including Madelon, MNIST, MRBI and Convex3. These prominent datasets
have been widely used to evaluate the eﬀectiveness of Bayesian optimization methods [44, 13, 5].
We follow the original train/test split of all the datasets. Test data will never be used during
the optimization: the once and only usage of test data is for oﬄine evaluations to determine the
In all benchmark experiments, we use
performance of optimized pipelines on unseen test set.
percent error rate as the evaluation metric.

Baseline Methods As discussed in Section 2, SMAC [22] and TPE [5] are the state-of-the-art
algorithms for Bayesian optimization [44, 13, 24], which are used as baselines. Note that Spearmint
[41], a Bayesian optimization algorithm based on Gaussian process is not applicable since it does
not provide a mechanism to handle the hierarchical space [12]. Besides SMAC and TPE, we also
choose random search as a simple baseline for sanity check. Thus, we compare both versions of
our method FLASH (with SMAC in Phase 3) and FLASH(cid:63) (with TPE in Phase 3) against three
baselines in the experiments.
Implementation: To avoid possible mistakes in implementing other methods, we choose a gen-
eral platform for hyperparameter optimization called HPOlib [12], which provides the original
In order to fairly compare our method
implementations of SMAC, TPE, and random search.

3The benchmark datasets are publicly available at http://www.cs.ubc.ca/labs/beta/Projects/autoweka/

datasets.

12

with others, we also implement our algorithm on top of HPOlib, and evaluate all the compared
methods on this platform. We make the source code of FLASH publicly available at https:
//github.com/yuyuz/FLASH.

Experimental Settings We build a general data analytic pipeline based on scikit-learn [36], a
popular used machine learning toolbox in Python. We follow the pipeline design of auto-sklearn
[13]. There are four computational steps in our pipeline: 1) feature rescaling, 2) sample balancing,
3) feature preprocessing, and 4) classiﬁcation model. Each step has various algorithms, and each
algorithm has its own hyperparameters. Adjacent steps are fully connected.
In total, our data
analytic pipeline contains 33 algorithms distributed in four steps, creating 1,456 possible pipeline
paths with 102 hyperparameters (30 categorical and 72 continuous), which creates complex high-
dimensional and highly conditional search space. Details and statistics of this pipeline are listed in
Table 5 in Appendix A.

In all experiments, we set a wall-clock time limit of 10 hours for the entire optimization, 15
minutes time limit and 10GB RAM limit for each pipeline run. We perform 10 independent
optimization runs with each baseline on each benchmark dataset. All experiments were run on
Linux machines with Intel Xeon E5-2630 v3 eight-core processors at 2.40GHz with 256GB RAM.
Since we ran experiments in parallel, to prevent potential competence in CPU resource, we use the
numactl utility to bound each independent run in single CPU core.

For our algorithm FLASH, we set both Tinit and Tprune as 30 iterations (equal to the number of
algorithms in the pipeline), which can be naturally generalized to other budgeted resources such as
wall-clock time or money. We set ξ to 100 in the EIPS function. Note that the performance are not
sensitive to the choices of those parameters. Finally, we set the number of pipeline paths r to 10
, which works well in generating a reasonable-size pruned pipeline G(cid:48). In benchmark experiments,
we compare the performance of FLASH without caching to other methods because the pipelines do
not have complex data preprocessing data like many real-world datasets have. We will use caching
for real-world experiments later in Section 5.

4.1 Results and Discussions

Table 2 reports the experimental results on benchmark datasets. For each dataset, we report the
performance achieved within three diﬀerent time budgets. As shown in the table, our methods
FLASH and FLASH(cid:63) perform signiﬁcantly better than other baselines consistently in all settings,
in terms of both lower error rate and faster convergence. For example, on the Madelon dataset,
our methods reach around 12% test error in only 3 hours, while other baselines are still far from
that even after 10 hours.

Performing statistical signiﬁcance test via bootstrapping, we ﬁnd that often FLASH and FLASH(cid:63)
tie with each other on these benchmark datasets. For all the methods, the test error is quite con-
sistent with the validation error, showing that the potential overﬁtting problem is well prevented
by using cross validation.

Figure 4 plots the convergence curves of median test error rate along with time for all baseline
methods. As shown in the ﬁgure, after running about 4 hours, FLASH and FLASH(cid:63) start to lead
others with steep drop of error rate, and then quickly converge on a superior performance.

13

Table 2: Performance on both 3-fold cross-validation and test data of benchmark datasets. For
each method, we perform 10 independent runs of 10 hours each. Results are reported as the
median percent error across the 10 runs within diﬀerent time budgets. Test data is never seen by
any optimization method, which is only used for oﬄine evaluations to compute test error rates.
Boldface indicates the best result within a block of comparable methods. We underline those results
not statistically signiﬁcantly diﬀerent from the best according to a 10,000 times bootstrap test with
p = 0.05.

Dataset

Budget
(hours)

Cross-validation Performance (%)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Madelon

MNIST

MRBI

Convex

3
5
10

3
5
10

3
5
10

3
5
10

25.16
23.60
20.77

7.68
6.58
6.58

61.80
58.67
57.20

28.14
25.25
24.51

18.90
18.82
17.28

6.78
5.94
5.39

59.83
58.61
53.92

24.70
23.61
22.21

20.25
19.12
17.34

6.05
5.83
5.64

62.89
58.14
54.60

24.69
23.30
23.30

14.84
14.31
13.87

4.93
4.26
4.03

57.43
45.11
41.15

22.63
21.34
20.49

14.04
14.04
13.76

5.05
4.87
4.46

57.08
54.25
41.90

23.31
22.02
20.62

19.17
18.21
15.58

7.75
7.10
6.64

60.58
56.42
54.43

25.04
23.18
22.18

16.15
15.26
14.49

5.41
5.41
5.03

59.83
58.61
52.01

21.42
21.37
20.31

16.03
15.38
13.97

6.11
5.40
5.23

60.58
55.81
52.30

21.97
20.82
20.82

12.18
12.18
11.49

4.62
3.94
3.78

54.72
43.19
39.13

20.65
19.56
18.94

11.73
11.60
11.47

4.84
4.57
4.37

54.28
51.65
39.89

21.04
19.71
19.01

Figure 4: Performance of our methods (FLASH and FLASH(cid:63)) and other compared methods on
MRBI dataset. We show the median percent error rate on test set along with standard error bars
(generated by 10 independent runs) over time.

14

(a) The impact of optimal design
on MRBI dataset

(b) The impact of pipeline pruning
on MRBI dataset

(c) The impact of pipeline caching
on real-world dataset

Figure 5: Component analysis experiments. (a) Optimal design makes the initialization phase more
robust. (b) Pipeline pruning in the second phase of FLASH is the key to its superior performance.
(c) Performance of FLASH without caching and the original FLASH with caching on real-world
dataset. In all ﬁgures, we show the median error rate on test set along with standard error bars
(generated by 10 independent runs). Note that (a) and (b) are plotted with diﬀerent x-axes; (c) is
on a diﬀerent dataset as (a) and (b).

4.2 Detailed Study of FLASH Components

FLASH has three main components: optimal design for initialization, cost-sensitive model for
pipeline pruning, and pipeline caching. To study their individual contributions to the performance
gain, we drop out each of the component and compare the performance with original FLASH. Since
caching will be used for real-world experiments on large dataset, we describe the analysis of caching
component in Section 5. Here we use MRBI dataset for these experiments.

Figure 5(a) shows the diﬀerence between using random initialization and optimal design by
plotting the performance on initial 30 pipeline runs. The desirable property of optimal design en-
sures to run reasonable pipeline paths, giving FLASH a head start at the beginning of optimization.
While random initialization is not robust enough, especially when the number of pipeline runs is
very limited and some algorithms will have no chance to run due to the randomness. Figure 5(b)
shows the impact of pipeline pruning in the second phase of FLASH. Dropping out the pruning
phase with EIPS, and using SMAC immediately after Phase 1, we see a major degradation of the
performance. The ﬁgure clearly shows that in Phase 2 of FLASH, the linear model with EIPS
acquisition function is able to eﬃciently shrink the search space signiﬁcantly such that SMAC can
focus on those algorithms which perform well with little cost. This ﬁgure conﬁrms the main idea
of this paper that a simple linear model can be more eﬀective in searching high-dimensional and
highly conditional hyperparameter space.

5 Real-world Experiments

In this section, to demonstrate a real-world use case, we apply FLASH on a large de-identiﬁed
medical dataset for classifying drug non-responders. We show how our method can quickly ﬁnd
good classiﬁer for diﬀerentiating non-responders vs. responders.

15

Table 3: Statistics of the medical dataset.

#Patient #Event #Med #Class

train case
train control
test case
test control
total

18,581
18,582
4,646
4,646
46,455

982,025
622,777
245,776
153,303
2,003,881

434,171
286,198
108,702
70,395
899,466

547,854
336,579
137,074
82,908
1,104,415

Budget
(hours)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

3
5
10

30.32
16.66
11.75

27.03
19.09
4.86

35.40
33.22
21.03

21.02
14.40
2.51

23.28
19.86
3.44

Table 4: Performance of real-world dataset. Results are reported using the same settings as Table 2.

Experimental Settings With a collaboration with a pharmaceutical company, we created a
balanced cohort of 46,455 patients from a large claim dataset. Patients who have at least 4 times of
treatment failure are regarded as drug non-responders (case group). Other patients are responders
of the drug (control group). The prediction target is whether a patient belongs to case group or
control group. Each patient is associated with a sequence of events, where each event is a tuple of
format (patient-id, event-id, timestamp, value). Table 3 summarizes the statistics of this
clinical dataset, including the count of patient, event, medication, and medication class.

Unlike benchmark experiments, the input to the real-world pipeline is not directly as feature
vectors. Given a cohort of patients with their event sequences, like [9] the pipeline for non-responder
classiﬁcation has two more additional steps than the pipeline described in previous benchmark
experiments: 1) Feature construction to convert patient event sequence data into numerical feature
vectors. This step can be quite time-consuming as advanced feature construction techniques like
sequential mining [29] and tensor factorization [19] can be expensive to compute. On this medical
i) frequency threshold to remove rare
dataset, we consider two kinds of parameters in this step:
events (frequency ranging from 2 to 5) ii) various aggregation functions (including binary, count,
sum and average) to aggregate multiple occurrence of events into features. The output of this step
will be a feature matrix and corresponding classiﬁcation targets; 2) Densify the feature matrix
from above feature construction step if necessary. Features of this real-world dataset can be quite
sparse. We by default use sparse representation to save space and accelerate computation in some
algorithms. Unfortunately, not all algorithm implementations in scikit-learn accept sparse features.
A decision has to be made here: either sparse matrix for faster result or dense matrix for broader
algorithm choices in later steps.

We run the experiments on same machine, use same parameter setting and same budget as
benchmark experiments. We compare our method with the same baselines as benchmark experi-
ments and we continue using error rate as metric. Our algorithm has built-in caching mechanism

16

and we will use that. For this real-world dataset, we ﬁrst compare with baselines with cache
enabled. Then we analyze the contribution of caching.

5.1 Results and Discussions

Table 4 shows the performance of our methods compared to baselines when caching is enabled. Due
to lack of space we only report the test performance. All cases FLASH and FLASH(cid:63) signiﬁcantly
outperform all the baselines.

Figure 5(c) shows the performance of FLASH without caching and original FLASH with caching
on the real-world medical dataset. With caching, more pipeline paths can be evaluated within
given period of time and our EIPS-based path selection leverages caching to select paths with high
performance that run fast. As a result, we can see FLASH with caching converges much faster.
For example, with caching we can get low test error within 6 hours.

6 Conclusions

In this work, we propose a two-layer Bayesian optimization algorithm named FLASH, which enables
highly eﬃcient optimization of complex data analytic pipelines. We showed that all components
of FLASH complement each other: 1) our optimal design strategy ensures better initialization,
giving a head start to the optimization procedure; 2) the cost-sensitive model takes advantage of
this head start, and signiﬁcantly improves the performance by pruning ineﬃcient pipeline paths;
3) the pipeline caching reduces the cost during the entire optimization, which provides a global
acceleration of our algorithm. We demonstrate that our method signiﬁcantly outperforms previous
state-of-the-art approaches in both benchmark and real-world experiments.

7 Acknowledgments

This work was supported by the National Science Foundation, award IIS- #1418511 and CCF-
#1533768, research partnership between Children’s Healthcare of Atlanta and the Georgia Institute
of Technology, CDC I-SMILE project, Google Faculty Award, Sutter health and UCB.

17

References

2012.

[1] A. Atkinson and A. Donev. Optimum experimental designs. 1992.

[2] T. B¨ack. Evolutionary algorithms in theory and practice. 1996.

[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 13(1),

[4] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures. In ICML, 2013.

[5] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl. Algorithms for hyper-parameter opti-

mization. In NIPS, 2011.

[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[7] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.

[8] G. Calinescu, C. Chekuri, M. P´al, and J. Vondr´ak. Maximizing a monotone submodular

function subject to a matroid constraint. SIAM Journal on Computing, 40(6), 2011.

[9] R. Chen, H. Su, Y. Zhen, M. Khalilia, D. Hirsch, M. Thompson, T. Davis, Y. Peng, S. Lin,
J. Tejedor-Sojo, E. Searles, and J. Sun. Cloud-based predictive modeling system and its
application to asthma readmission prediction. In AMIA. AMIA, 2015.

[10] S. J. Coakes and L. Steed. SPSS: Analysis without anguish using SPSS version 14.0 for

Windows. John Wiley & Sons, Inc., 2009.

[11] D. Donoho. 50 years of Data Science. Technical report, University of California Berkeley, 2015.

[12] K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown.
Towards an empirical foundation for assessing bayesian optimization of hyperparameters. In
NIPS workshop on Bayesian Optimization in Theory and Practice, 2013.

[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Eﬃcient and

robust automated machine learning. In NIPS, 2015.

[14] M. Feurer, T. Springenberg, and F. Hutter. Initializing bayesian hyperparameter optimization

via meta-learning. In AAAI, 2015.

[15] P. Flaherty, A. Arkin, and M. I. Jordan. Robust design of biological experiments. In NIPS,

2005.

[16] C. J. Flynn, C. M. Hurvich, and J. S. Simonoﬀ. Eﬃciency for regularization parameter selection

in penalized likelihood estimation of misspeciﬁed models. JASA, 108(503), 2013.

[17] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data

mining software: an update. ACM SIGKDD explorations newsletter, 11(1), 2009.

18

[18] X. He. Laplacian regularized d-optimal design for active learning and its application to image

retrieval. Image Processing, IEEE Transactions on, 19(1), 2010.

[19] J. C. Ho, J. Ghosh, and J. Sun. Marble: high-throughput phenotyping from electronic health

records via sparse nonnegative tensor factorization. In KDD, 2014.

[20] M. D. Hoﬀman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization.

In UAI. Citeseer, 2011.

[21] M. D. Hoﬀman, B. Shahriari, and N. de Freitas. On correlation and budget constraints in
In AIS-

model-based bandit optimization with application to automatic machine learning.
TATS, 2014.

[22] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general

algorithm conﬁguration. In Learning and Intelligent Optimization. Springer, 2011.

[23] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, 2013.

[24] B. Komer, J. Bergstra, and C. Eliasmith. Hyperoptsklearn: Automatic hyperparameter con-

ﬁguration for scikitlearn. In ICML workshop on AutoML, 2014.

[25] A. Krause and C. Guestrin. Submodularity and its applications in optimized information

gathering. TIST, 2(4), 2011.

[26] A. Kumar, R. McCann, J. Naughton, and J. M. Patel. Model selection management systems:

The next frontier of advanced analytics. ACM SIGMOD Record, 2015.

[27] D. J. Lizotte. Practical bayesian optimization. University of Alberta, 2008.

[28] J. Lv and J. S. Liu. Model selection principles in misspeciﬁed models. JRSS-B, 76(1), 2014.

[29] K. Malhotra, T. Hobson, S. Valkova, L. Pullum, and A. Ramanathan. Sequential pattern min-
ing of electronic healthcare reimbursement claims: Experiences and challenges in uncovering
how patients are treated by physicians. In Big Data, Oct 2015.

[30] X. Meng, J. Bradley, E. Sparks, and S. Venkataraman. Ml pipelines: a new high-level api for

mllib, 2015.

[31] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. Yale: Rapid prototyping for

complex data mining tasks. In KDD, 2006.

[32] J. Mockus, V. Tiesis, and A. Zilinskas. The application of bayesian methods for seeking the

extremum. Towards Global Optimization, 2(117-129), 1978.

[33] R. Munos. Optimistic optimization of deterministic functions without the knowledge of its

smoothness. In Advances in neural information processing systems, 2011.

[34] Y. Nesterov. Gradient methods for minimizing composite objective function, 2007.

[35] K. Ng, A. Ghoting, S. R. Steinhubl, W. F. Stewart, B. Malin, and J. Sun. Paramo: A parallel
predictive modeling platform for healthcare analytic research using electronic health records.
Journal of biomedical informatics, 48, 2014.

19

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. JMLR,
12, 2011.

[37] T. Robertazzi and S. Schwartz. An accelerated sequential algorithm for producing d-optimal

designs. SIAM Journal on Scientiﬁc and Statistical Computing, 10(2), 1989.

[38] B. Settles. Active Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning,

6(1), jun 2012.

[39] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of
the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.

[40] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor selection: Leveraging submodularity.

In CDC, 2010.

algorithms. In NIPS, 2012.

[41] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning

[42] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Ali,
R. P. Adams, et al. Scalable bayesian optimization using deep neural networks. arXiv preprint
arXiv:1502.05700, 2015.

[43] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.

[44] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka: Combined selection

and hyperparameter optimization of classiﬁcation algorithms. In KDD, 2013.

[45] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global opti-
mization of expensive-to-evaluate functions. Journal of Global Optimization, 44(4), 2009.

[46] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high

dimensions via random embeddings. In IJCAI. Citeseer, 2013.

[47] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

[48] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 1982.

20

Table 5: Detailed information of the data analytic pipeline constructed for benchmark experiments.

Algorithm

#categorical #continuous

Total
#hyper-
parameters

A Appendix

Pipeline
step

Feature
rescaling

Sample
balancing

Feature
preprocessing

Classiﬁcation

0
0
0
0

0
0

2
3
2
1
0
0
0
1
1
1
0
1
1

1
1
2
0
0
2
1
0
2
1
1
0
2
4

Min-max scaler
None
Normalization
Standardization

Class weighting
None

Extremely randomized trees
Fast ICA
Feature agglomeration
Kernel PCA
Random kitchen sinks
Linear SVM
None
Nystroem sampler
PCA
Polynomial combinations
Random trees embedding
Percentile feature selection
Univariate feature selection

AdaBoost
Decision tree
Extremely randomized trees
Gaussian naive Bayes
Gradient boosting
K-nearest neighbors
LDA
Linear SVM
Kernel SVM
Multinomial naive Bayes
Passive aggressive
QDA
Random forest
Stochastic gradient descent

21

0
0
0
0

0
0

3
1
1
6
2
2
0
8
1
2
4
1
2

3
3
3
0
6
1
3
2
5
1
2
1
3
6

0
0
0
0

0
0

5
4
3
7
2
2
0
9
2
3
4
2
3

4
4
5
0
6
3
4
2
7
2
3
1
5
10

Total#

33

30

72

102

6
1
0
2
 
n
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
6
4
6
0
.
2
0
6
1
:
v
i
X
r
a

FLASH: Fast Bayesian Optimization for
Data Analytic Pipelines

Yuyu Zhang Mohammad Taha Bahadori Hang Su

Jimeng Sun

Georgia Institute of Technology
{yuyu,bahadori,hangsu}@gatech.edu,jsun@cc.gatech.edu

Abstract

Modern data science relies on data analytic pipelines to organize interdependent compu-
tational steps. Such analytic pipelines often involve diﬀerent algorithms across multiple steps,
each with its own hyperparameters. To achieve the best performance, it is often critical to select
optimal algorithms and to set appropriate hyperparameters, which requires large computational
eﬀorts. Bayesian optimization provides a principled way for searching optimal hyperparame-
ters for a single algorithm. However, many challenges remain in solving pipeline optimization
problems with high-dimensional and highly conditional search space. In this work, we propose
Fast LineAr SearcH (FLASH), an eﬃcient method for tuning analytic pipelines. FLASH is
a two-layer Bayesian optimization framework, which ﬁrstly uses a parametric model to select
promising algorithms, then computes a nonparametric model to ﬁne-tune hyperparameters of
the promising algorithms. FLASH also includes an eﬀective caching algorithm which can fur-
ther accelerate the search process. Extensive experiments on a number of benchmark datasets
have demonstrated that FLASH signiﬁcantly outperforms previous state-of-the-art methods in
both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%
improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art
performance on a real-world application for healthcare predictive modeling.

1 Introduction

Modern data science often requires many computational steps such as data preprocessing, fea-
ture extraction, model building, and model evaluation, all connected in a data analytic pipeline.
Pipelines provide a natural way to represent, organize and standardize data analytic tasks, which
are considered to be an essential element in the data science ﬁeld [11] due to their key role in large-
scale data science projects. Many machine learning toolboxes such as scikit-learn [36], RapidMiner
[31], SPSS [10], Apache Spark [30] provide mechanisms for conﬁguring analytic pipelines.

An analytic pipeline skeleton is shown in Figure 1. Each step, such as feature preprocessing
and classiﬁcation, includes many algorithms to choose from. These algorithms usually require
users to set hyperparameters, ranging from optimization hyperparameters such as learning rate and
regularization coeﬃcients, to model design hyperparameters such as the number of trees in random
forest and the number of hidden layers in neural networks. There are an exponential number of
choices for the combination of algorithms and hyperparameters in a given analytic pipeline skeleton.
Because of the interdependency between all the algorithms and their hyperparameters, the choices
can have huge impact on the performance of the best model.

1

Figure 1: A typical data analytic pipeline.

Tuning hyperparameters of a single algorithm can be viewed as an optimization problem of a
black-box objective function, which is noisy and often expensive to evaluate. Here the input of
black-box are the hyperparameters, and the objective function is the output performance such as
accuracy, precision and recall. To tackle this problem, simple methods have been applied such as
grid or random search [5, 3]. While on diﬃcult problems where these simple approaches are not
eﬃcient, a more promising model-based approach is Bayesian optimization [32, 27, 7, 39]. The high-
level idea of Bayesian optimization is to deﬁne a relatively cheap surrogate function and use that
to search the hyperparameter space. Indeed, there exist other global optimization methods, such
as evolutionary algorithms [2] and optimistic optimization [33]. We choose Bayesian optimization
framework due to its great performance in practice. Recently, Bayesian optimization methods have
been shown to outperform other methods on various tasks, and in some cases even beat human
domain experts to achieve better performance via tuning hyperparameters [42, 4].

Despite its success, applying Bayesian optimization for tuning analytic pipelines faces several
signiﬁcant challenges: Existing Bayesian optimization methods are usually based on nonparametric
models, such as Gaussian process and random forest. A major drawback of these methods is that
they require a large number of observations to ﬁnd reasonable solutions in high-dimensional space.
When tuning a single algorithm with several hyperparameters, Bayesian optimization works well
with just a few observations. However, when it comes to pipeline tuning, thousands of possible
combinations of algorithms plus their hyperparameters jointly create a large hierarchical high-
dimensional space to search over, whereas existing methods tend to become ineﬃcient. Wang et
al. [46] tackled the high-dimensional problem by making a low eﬀective dimensional assumption.
However, it is still a ﬂat Bayesian optimization method and not able to handle the exploding
dimensionality problem caused by hierarchically structured hyperparameters in analytic pipeline
tuning.
Motivating example: We build an analytic pipeline for classiﬁcation task (details in Section 4). If
we give 10 trials for each hyperparameter over 1,456 unique pipeline paths and 102 hyperparameters,
we have more than 2 million conﬁgurations, which can take years to complete with a brute-force
search. Even with the state-of-the-art Bayesian optimization algorithm such as Sequential Model-
based Algorithm Conﬁguration (SMAC) [22], the process can still be slow as shown in Figure 2.
If we know the optimal algorithms ahead time (Oracle) with just hyperparameter tuning of the
optimal algorithms, we can obtain signiﬁcant time saving, which is however not possible. Finally,
our proposed method FLASH can converge towards the oracle performance much more quickly
than SMAC.

In this paper, we propose a two-layer Bayesian optimization algorithm called Fast LineAr SearcH
(FLASH): the ﬁrst layer for selecting algorithms, and the second layer for tuning the hyperparame-
ters of selected algorithms. FLASH is able to outperform the state-of-the-art Bayesian optimization
algorithms by a large margin, as shown in Figure 2. By designing FLASH, we make three main
contributions:

• We propose a linear model for propagation of error (or other quantitative metrics) in analytic

2

Figure 2: Performance comparison of a data analytic pipeline on MRBI dataset, including the
previous state of the art SMAC, proposed method FLASH, and Oracle (i.e., pretending the optimal
algorithm conﬁguration is given, and only performing hyperparameter tuning with SMAC on those
algorithms). We show the median percent error rate on the test set along with standard error bars
(generated by 10 independent runs) over time. FLASH outperforms SMAC by a big margin and
converges toward Oracle performance quickly.

pipelines. We also propose a Bayesian optimization algorithm for minimizing the aggregated error
using our linear error model. Our proposed mechanism can be considered as a hybrid model: a
parametric linear model for fast exploration and pruning of the algorithm space, followed by a
nonparametric hyperparameter ﬁne-tuning algorithm.

• We propose to initialize the hyperparameter tuning algorithm using the optimal design strategy
[1, 15, 38] which is more robust than the random initialization. We also propose a fast greedy
algorithm to eﬃciently solve the optimal design problem for any given analytic pipeline.

• Finally, we introduce a caching algorithm that can signiﬁcantly accelerate the tuning process. In
particular, we model the (time) cost of each algorithm, and incorporate that in the optimization
process. This ensures the eﬃciency of fast search.

We demonstrate the eﬀectiveness of FLASH with extensive experiments on a number of diﬃcult
problems. On the benchmark datasets for pipeline conﬁgurations tuning, FLASH substantially
improves the previous state of the art by 7% to 25% in test error rate within the same time budget.
We also experiment with large-scale real-world datasets on healthcare data analytic tasks where
FLASH also exhibits superior results.

2 Background and Related Work

2.1 Data Analytic Pipelines

The data analytic pipeline refers to a framework consisting of a sequence of computational transfor-
mations on the data to produce the ﬁnal predictions (or outputs) [26]. Pipelines help users better

3

understand and organize the analysis task, as well as increase the reusability of algorithm imple-
mentations in each step. Several existing widely adopted machine learning toolboxes provide the
functionality to run analytic pipelines. Scikit-learn [36] and Spark ML [30] provide programmatic
ways to instantiate a pipeline. SPSS [10] and RapidMiner [31] provide a visual way to assemble
an analytic pipeline instance together and run. Microsoft Azure Machine Learning1 provides a
similar capability in a cloud setting. There are also specialized pipelines, such as PARAMO [35] in
healthcare data analysis.

However, a major diﬃculty in using these systems is that none of the above described tools is
able to eﬃciently help users decide which algorithms to use in each step. Some of the tools such as
scikit-learn, Spark ML, and PARAMO allow searching all possible pipeline paths and tuning the
hyperparameters of each step using an expensive grid search approach. While the search process
can be sped up by running in parallel, the search space is still too large for the exhaustive search
algorithms.

2.2 Bayesian Optimization

Bayesian optimization is a well-established technique for global and black-box optimization prob-
lems. In a nutshell, it comprises two main components: a probabilistic model and an acquisition
function. For the probabilistic model, there are several popular choices: Gaussian process [41, 42],
random forest such as Sequential Model-based Algorithm Conﬁguration (SMAC) [22], and density
estimation models such as Tree-structured Parzen Estimator (TPE) [5]. Given any of these models,
the posterior mean and variance of a new input can be computed, and used for computation of
the acquisition function. The acquisition function deﬁnes the criterion to determine future input
candidates for evaluation. Compared to the objective function, the acquisition function is chosen
to be relatively cheap to evaluate, so that the most promising next input for querying can be found
quickly. Various forms of acquisition functions have been proposed [43, 20, 45, 21]. One of the
most prominent acquisition function is the Expected Improvement (EI) function [32], which has
been widely used in Bayesian optimization. In this work, we use EI as our acquisition function,
which is formally described in Section 3.

Bayesian optimization is known to be successful in tuning hyperparameters for various learning
algorithms on diﬀerent types of tasks [42, 14, 4, 41, 46]. Recently, for the problem of pipeline con-
ﬁgurations tuning, several Bayesian optimization based systems have been proposed: Auto-WEKA
[44] which applies SMAC [22] to WEKA [17], auto-sklearn [13] which applies SMAC to scikit-learn
[36], and hyperopt-sklearn [24] which applies TPE [5] to scikit-learn. The basic idea of applying
Bayesian optimization to pipeline tuning is to expand the hyperparameters of all algorithms and
create large search space to perform optimization as we will show in the experiments. However,
for practical pipelines the space becomes too large which hinders convergence of the optimization
process. Auto-sklearn [13] uses a meta-learning algorithm that leverages performance history of
algorithms on existing datasets to reduce the search space. However, in real-world applications,
we often have unique datasets and tasks such that ﬁnding similar datasets and problems for the
meta-learning algorithm will be diﬃcult.

1https://studio.azureml.net

4

Figure 3: A toy example of data analytic pipeline. One possible pipeline path, ﬂowing from the
input Vin to the output Vout, is highlighted in shaded area.

3 Methodology

A data analytic pipeline G = (V, E) can be represented as a multi-step Directed Acyclic Graph
(DAG), where V is the set of algorithms, and E is the set of directed edges indicating dependency
between algorithms. Algorithms are distributed among multiple steps. Let V (k)
denote the ith
i
algorithm in the kth step. Each directed edge (V (k)
) ∈ E represents the connection from
algorithm V (k)
. Note that there is no edge between algorithms in the same step. We also
have an input data vertex Vin which points to all algorithms in the ﬁrst step, and an output vertex
Vout which is pointed by all algorithms in the last step.

to V (k+1)
j

, V (k+1)
j

i

i

A pipeline path is any path from the input Vin to the output Vout in pipeline graph G . To denote
a pipeline path of K steps, we use K one-hot vectors p(k) (1 ≤ k ≤ K), each denoting the algorithm
selected in the k-th step. Thus, the concatenation of one-hot vectors p = (cid:2)p(1), . . . , p(K)(cid:3) ∈ {0, 1}N
denotes a pipeline path, where N is the total number of algorithms in the pipeline G. Figure 3
shows a small data analytic pipeline with two steps. The ﬁrst step contains two algorithms, and the
second step contains three. One possible pipeline path is highlighted in the shaded area. On this
pipeline path, V (1)
are selected in the ﬁrst and second step, so that we have p(1) = [0, 1]
and p(2) = [0, 0, 1]. Thereby, the highlighted pipeline path is given by p = (cid:2)p(1), p(2)(cid:3) = [0, 1, 0, 0, 1].
For any pipeline path p, we concatenate all of its hyperparameters in a vector λp. The pair of path
and hyperparameters, i.e. (p, λp), forms a pipeline conﬁguration to be run. For ease of reference,
we list the notations in Table 1.

and V (2)

3

2

The problem of tuning data analytic pipelines can be formalized as an optimization problem:

Problem 1. Given a data analytic pipeline G with input data D, resource budget T , evaluation
metric function m(G, D; p, λp), resource cost of running pipeline τ (G, D; p, λp), how to ﬁnd the
pipeline path p and its hyperparameters λp with best performance m(cid:63)?

The performance of the best pipeline path is denoted by m(cid:63) = minp,λp m(G, D; p, λp) subject

5

Table 1: Mathematical notations used in this paper.

Symbol Description

G
V
E
K
N
D
Vin
Vout
V (k)
i
p(k)
λ(k)
p
p
λp
m(·)
τ (·)
Tinit
Tprune
Ttotal

data analytic pipeline
set of algorithms in G
set of dependency between algorithms
total number of steps in G
total number of algorithms in G
input data of pipeline
input vertex of G
output vertex of G
ith algorithm in kth step
one-hot vector for kth step
hyperparameters for kth step
pipeline path
all hyperparameters of p
evaluation metric function
time cost of running pipeline
budget for Phase 1
budget for Phase 2
budget for Phase 3

to budget T . The objective is to approach the optimal performance within the budget T via
(cid:98)p) ≤ m(cid:63) + (cid:15) for
optimizing over p, λp; i.e., we would like our solution (cid:98)p, (cid:98)λ
small values of (cid:15).

(cid:98)p to satisfy m(G, D; (cid:98)p, (cid:98)λ

To eﬃciently tackle this problem, we propose a two-layer Bayesian optimization approach named
Fast LineAr SearcH (FLASH). We generally introduce the idea of linear model and describe the
algorithm in Section 3.1. An immediate advantage of using linear model is that we can use more
principled initialization instead of random initialization, as discussed in Section 3.2. We use cost-
sensitive modeling to prune the pipeline, as described in Section 3.3. Finally, we accelerate the
entire optimization procedure via pipeline caching, which we describe in Section 3.4.

3.1 Two-layer Bayesian Optimization

Inspired by the performance of linear regression under model misspeciﬁcation [48, 16, 28] and supe-
rior sample complexity compared to more ﬂexible nonparametric techniques [47], we seek parametric
models for propagation of error (or other quantitative metrics) in analytic pipelines. The high level
idea of FLASH is as follows: we propose a linear model for estimating the propagation of error (or
any other metric) in a given analytic pipeline. The linear model assumes that the performance of
algorithms in diﬀerent steps are independent, and the ﬁnal performance is additive from all algo-
rithms. That is, we can imagine that each algorithm is associated with a performance metric, and
the total performance of a pipeline path is the sum of the metrics for all algorithms in the path.
This linear model will replace the Gaussian process or random forest in the initial stages of the
pipeline tuning process. In the rest of this section, we provide the details of Bayesian optimization

6

with our linear model.

We apply the linear model only to the pipeline selection vector p and assume that the variations
due to hyperparameters of the algorithms are captured in the noise term. That is, we assume that
the error of any pipeline path p can be written as

m = β(cid:62)p + ε

where β ∈ RN denotes the parameters of the linear model. Given a set of observations of the
algorithm selection and the corresponding evaluation metric for the selected pipeline path in the
form of (pi, mi), i = 1, . . . , n, we can ﬁt this model and infer its mean µ(p) and variance σ2(p)
of the performance estimation for any new pipeline path represented by p. In particular, let the
design matrix P ∈ Rn×N denote the stacked version of the pipeline paths, i.e., P = [p1, . . . , pn](cid:62),
and m ∈ Rn be the corresponding response values of the evaluation metrics, m = [m1, . . . , mn]. We
use the following L2 regularized linear regression to obtain the robust estimate for β from history
observations:

(cid:98)β(P , m) = argmin

β

(cid:26) 1
n

(cid:107)P β − m(cid:107)2

2 + λ(cid:107)β(cid:107)2
2

(cid:27)

(1)

(cid:113)(cid:0)(cid:80)n

(cid:1). The predictive
where for any vector x ∈ Rn the L2 norm is deﬁned as (cid:107)x(cid:107)2 =
distribution for the linear model is Gaussian with mean (cid:98)µp = (cid:98)β(cid:62)p and variance (cid:98)σp = σ2
ε (1 +
p(cid:62)(P (cid:62)P + λI)−1p) where σ2
ε is the variance of noise in the model. We estimate σε as follows: the
residual in the ith observation is computed as (cid:98)(cid:15)i = (cid:98)µi − mi where (cid:98)µi = (cid:98)β(cid:62)pi is the estimate of mi
by our model. Thus, the variance of the residual can be found as (cid:98)σ2
(cid:15) = var((cid:98)µi − mi) where var(·)
denotes the variance operator.

i=1 x2
i

To perform Bayesian optimization with linear model, we use the popular Expected Improvement
(EI) criteria, which recommends to select the next sample pt+1 such that the following acquisition
function is maximized. The acquisition function represents the expected improvement over the best
observed result m+ at a new pipeline path p [44]:

EI(p) = E[Im+(p)] = σp[uΦ(u) + φ(u)]

(2)

u = m+−ξ−µp

σp

where
and ξ is a parameter to balance the trade-oﬀ between exploitation and
exploration. EI function is maximized for paths with small values of mp and large values of σp,
reﬂecting the exploitation and exploration trade-oﬀs, respectively. To be more speciﬁc, larger ξ
encourages more exploration in selecting the next sample. The functions Φ(·) and φ(·) represent
CDF and PDF of standard normal distribution, respectively. The idea of Bayesian optimization
with EI is that at each step, we compute the EI with the predictive distribution of the existing linear
model and ﬁnd the pipeline path that maximizes EI. We choose that path and run the pipeline
with it to obtain a new (pi, mi) pair. We use this pair to reﬁt and update our linear model and
repeat the process. Later on we also present an enhanced version of EI via normalizing it by cost
called Expected Improvement Per Second (EIPS).

We provide the full details of FLASH in Algorithm 1. While the main idea of FLASH is
performing Bayesian optimization using linear model and EI, it has several additional ideas to
make it practical. Speciﬁcally, FLASH has three phases:

• Phase 1, we initialize the algorithm using ideas from optimal design, see Section 3.2. The

budget is bounded by Tinit.

7

Algorithm 1: Fast Linear Search (FLASH)

input : Data analytic pipeline G; input data D; total budget for entire optimization Ttotal;

budget for initialization Tinit; budget for pipeline pruning Tprune; number of top
pipeline paths r

(cid:98)p

output: Optimized pipeline conﬁguration (cid:98)p and (cid:98)λ
/* Phase 1: Initialization (Section 3.2)
1 while budget Tinit not exhausted do
2

p ← new pipeline path from Algorithm 2
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:99)βτ (P , τ ) using Eq. (1)

5

4

6

3

/* Phase 2: Pipeline pruning (Section 3.3)
7 while budget Tprune not exhausted do
8

9

10

11

p ← argmaxp EIP S(p, P , β, βτ ) using Eq. (4)
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:98)βτ (P , τ ) using Eq. (1)

using Eq. (4)
/* Phase 3: Pipeline tuning

14 S ← history observations within G(cid:48)
15 Initialize model M given S
16 while budget Ttotal not exhausted do
p, λp ← next candidate from M
17
m ← RunPipeline(G(cid:48), D; p, λp) with Algorithm 3
S ← S ∪ {(p, λp, m)}
Update M given S
(cid:98)p ← Best conﬁguration so far found for G(cid:48)

21 (cid:98)p, (cid:98)λ

19

18

20

12
13 G(cid:48) ← construct subgraph of G with top r pipeline paths with largest EIP S(p, P , β, βτ )

*/

*/

*/

• Phase 2, we leverage Bayesian optimization to ﬁnd the top r best pipeline paths and prune

the pipeline G to obtain simpler one G(cid:48). The budget is bounded by Tprune

2.

• Phase 3, we use general model-based Bayesian optimization methods to ﬁne-tune the pruned

pipeline together with their hyperparameters.

In Phase 3, we use state-of-the-art Bayesian optimization algorithm, either SMAC or TPE. These
algorithms are iterative: they use a model M such as Gaussian process or random forest and use
EI to pick up a promising pipeline path with hyperparameters for running, and then update the
model with the new observation just obtained, and again pick up the next one for running. The

2In practice, it is better to use the time normalized EI (that is EIPS) during Phase 2; this idea is described in

Section 3.3.

8

i=1; number of desired pipeline paths ninit

Algorithm 2: Initialization with Optimal Design

/* Batch version
input : B initial candidates {pi}B
output: Optimal set of pipeline paths Q
1 p1 ← random pipeline path for initialization
2 H ← p1p(cid:62)
1
3 Q ← {p1}
4 for (cid:96) = 2, . . . , ninit do
5

j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)
H ← H + pj(cid:63)p(cid:62)
j(cid:63)
Q ← Q ∪ {pj(cid:63)}

6

7

j ) for j = 1, . . . , B.

/* Online version
input : B candidates {pi}B
output: Next pipeline path pj(cid:63), j(cid:63) ∈ {1, . . . , B}

i=1; current Gram matrix H

8 j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)

j ) for j = 1, . . . , B

*/

*/

budget is bounded by Ttotal. Note that our algorithm is currently described for a sequential setting
but can be easily extended to support parallel runs of multiple pipeline paths as well.

3.2 Optimal Design for Initialization

Most Bayesian optimization algorithms rely on random initialization which can be ineﬃcient; for
example, it may select duplicate pipeline paths for initialization. Intuitively, the pipeline paths
used for initialization should cover the pipeline graph well, such that all algorithms are included
enough times in the initialization phase. The ultimate goal is to select a set of pipeline paths
for initialization such that the error in estimation of β is minimized. Given our proposed linear
model, we can ﬁnd the optimal strategy for initialization to make sure the pipeline graph is well
covered and the tuning process is robust. In this section, we describe diﬀerent optimality criteria
studied in statistical experiment design [1, 15] and active learning [38], and design an algorithm for
initialization step of FLASH.

Given a set of pipeline paths with size n, there are several diﬀerent optimality criteria in terms
i=1 pip(cid:62)

i as follows [38]:

of the eigenvalues of the Gram matrix H = (cid:80)n
A-optimality: maximize (cid:80)n
D-optimality: maximize (cid:81)n

(cid:96)=1 λ(cid:96)(H).

(cid:96)=1 λ(cid:96)(H).

E-optimality: maximize λn(H), the nth largest eigenvalue.

It is easy to see that any arbitrary set of pipeline path designs satisﬁes the A-optimality criterion.

Proposition 1. Any arbitrary set of pipeline paths with size n is a size-n A-optimal design.

Proof. For any arbitrary set of pipeline paths with size n, we have:

n
(cid:88)

(cid:96)=1

λ(cid:96)(H) = tr(H) = tr

(cid:33)

pip(cid:62)
i

=

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:16)

tr

pip(cid:62)
i

(cid:17)

= nK.

9

The last step is due to particular pattern of p in our problem. Thus, we show that (cid:80)n
constant, independent of the design of pipeline paths.

(cid:96)=1 λ(cid:96)(H) is

Proposition 1 rules out use of A-optimality in pipeline initialization. Given the computational
complexity of E-optimality and the fact that it intends for optimality in the extreme cases, we
choose D-optimality criterion. The D-optimality criterion for design of optimal linear regression
can be stated as follows: suppose we are allowed to evaluate ninit samples pi, i = 1, . . . , ninit,
these samples should be designed such that the determinant of the Gram matrix H is maximized.
While we can formulate an optimization problem that directly ﬁnds pi values, we found that an
alternative approach can be computationally more eﬃcient. In this approach, we ﬁrst generate B
candidate pipeline paths for an integer B larger than the number of algorithms in the pipeline N .
This set may include all possible pipeline paths if the total number of paths is small. Then, our
goal becomes selecting a subset of size ninit from them. We can formulate the optimal design as
follows

a(cid:63) = argmax

det

a

(cid:41)

aipip(cid:62)
i

(cid:40) B
(cid:88)

i=1

s.t.

a ∈ {0, 1}K,

1(cid:62)a = ninit.

(3)

The last constraint 1(cid:62)a = ninit indicates that only ninit pipeline paths should be selected. The
objective function is concave in terms of continuous valued a [6, Chapter 3.1.5]. Thus, a traditional
approach is to solve it by convex programming after relaxation of the integrality constraint on a.
The matrix in the argument of the determinant is only N -dimensional which means calculation of
the determinant should be fast. Nestrov’s accelerated gradient descent [34] or Frank-Wolfe’s [23]
algorithms can be used for eﬃciently solving such problems.

An even faster solution can be found by using greedy forward selection ideas which are fast
and popular for optimal experiment design, for example see [37, 18, 25] and the references therein.
To apply greedy technique to our problem, we initialize the solution by picking one of the pipeline
i . Then, at (cid:96)th step, we add the path that maximizes j(cid:63) = argmaxj D(cid:96)(H + pjp(cid:62)
path H = pip(cid:62)
j )
where D(cid:96)(H) = (cid:81)min((cid:96),p)
λi(H) denotes the product of top min((cid:96), p) eigenvalues of its argument.
The algorithm is described in Algorithm 2. The optimization problem in Eq. (3) appears in other
ﬁelds such as optimal facility location and sensor planning where greedy algorithm is known to
have a 1 − 1

e approximation guarantee [8, 40].

One further desirable property of the greedy algorithm is that it is easy to run it under a
time budget constraint. We call this version the online version in Algorithm 2, where instead of a
ﬁxed number of iteration ninit, we run it until the exhaustion of our time budget. See Line 2 in
Algorithm 1 and the online version of Algorithm 2.

i=1

3.3 Cost-sensitive Modeling

The Expected Improvement aims at approaching the true optimal (doing well) within a small number
of function evaluations (doing fast). However, the time cost of each function evaluation may vary
a lot due to diﬀerent settings of hyperparameters. This problem is particularly highlighted in
pipeline conﬁgurations tuning, since the choice of algorithms can make a huge diﬀerence in running
time. Therefore, fewer pipeline runs are not always “faster” in terms of wall-clock time. Also, in
practice, what we care about is the performance we can get within limited resource budget, rather

10

than within certain evaluation times. That is why we need cost-sensitive modeling for the pipeline
tuning problem.

Expected Improvement Per Second (EIPS) [41] proposes another acquisition function for tuning
of a single learning algorithm by dividing the EI of each hyperparameter by its runtime. To apply
EIPS in pipeline tuning problem, we use a separate linear model to model the total runtime of
pipeline paths. Similar to the linear model for error propagation, the linear model for time assumes
that on a pipeline path each algorithm partly contributes to the total time cost and the runtimes are
additive. To apply the linear model, we replace the performance metric m with the cost metric τ .
The linear cost model parametrized by βτ can be eﬃciently updated using Eq. (1). As described in
Algorithm 1, βτ will be updated together with β at the end of Phase 2. We note that, in practice,
the budget T and the cost τ (·) can be any quantitative costs of budgeted resources (e.g., money,
CPU time), which is a natural generalization of our idea.

With the cost model above, we get the cost-sensitive acquisition function over the best observed

result m+ at a new pipeline path p:

E[Im+(p)]
E[log τ (p)]

=

σp[uΦ(u) + φ(u)]
E[log τ (p)]

(4)

EIP S(p, P , β, βτ ) =

where u =

m+ − ξ − µp
σp

.

Here the dependency in β and βτ is captured during computation of µp, σp, and τ (p). We take
logarithm of cost τ (·) to compensate the large variations in the runtime of diﬀerent algorithms.
This acquisition function balances “doing well” and “doing fast” in selecting the next candidate
path to run. During the optimization, it will help avoid those costly paths with poor expected
improvement. More importantly, at the end of Phase 2 in Algorithm 1, EIPS is responsible to
determine the most promising paths, which perform better but cost less, to construct a subgraph
for the last phase ﬁne-tuning. For this purpose, we set the exploration parameter ξ to 0 to only
select (Line 13 in Algorithm 1).

3.4 Pipeline Caching

During the experiments, we note that many pipeline runs have overlapped algorithms in their paths.
Sometimes these algorithms have exactly the same pipeline path and the same hyperparameter
settings along the path. This means that we are wasting time on generating the same intermediate
output again and again. For example, consider the min-max normalization algorithm in the ﬁrst
pipeline step: this algorithm will be executed many times, especially when it performs well so that
Bayesian optimization methods prefer to choose it.

To reduce this overhead, we propose a pipeline caching algorithm, as described in Algorithm 3.
When running a pipeline, we check the cache before we run each algorithm. If it turns out to be a
cache hit, the result will be immediately returned from cache. Otherwise, we run the algorithm and
cache the result. There is a caching pool (e.g., disk space, memory usage) for this algorithm. We
use the Least Recently Used (LRU) strategy to clean up the caching pool when budget becomes
exhausted.

Caching can signiﬁcantly reduce the cost of pipeline runs, and accelerates all three phases of
FLASH. Algorithms closer to the pipeline input vertex, usually the data preprocessing steps, have
higher chance to hit the cache. In fact, when we deal with large datasets on real-world problems,
the preprocessing step can be quite time-consuming such that caching can be very eﬃcient.

11

Algorithm 3: Pipeline Caching

input : Data analytic pipeline G; input data D; pipeline conﬁguration p and λp to be run;

available caching budget Tcache; current cache pool C

1 D(1) ← D

/* Run pipeline with cache pool

2 for k ← 1, . . . , K do

3

4

5

6

7

8

h ← Hash(p(1) . . . p(k), λ(1)
if h ∈ C then

p . . . λ(k)
p )

D(k+1) ← cached result from C

else

D(k+1) ← RunAlgorithm(G, D(k), p(k), λ(k)
p )
C ← C ∪ {(cid:104)h, D(k+1)(cid:105)}

/* Clean up cache pool when necessary

9 if Tcache exhausted then
10

Discard least recently used (LRU) items in C

4 Benchmark Experiments

*/

*/

In this section, we perform extensive experiments on a number of benchmark datasets to evaluate
our algorithm compared to the existing approaches. Then we study the impact of diﬀerent algorithm
choices in each component of FLASH.

Benchmark Datasets We conduct experiments on a group of public benchmark datasets on
classiﬁcation task, including Madelon, MNIST, MRBI and Convex3. These prominent datasets
have been widely used to evaluate the eﬀectiveness of Bayesian optimization methods [44, 13, 5].
We follow the original train/test split of all the datasets. Test data will never be used during
the optimization: the once and only usage of test data is for oﬄine evaluations to determine the
In all benchmark experiments, we use
performance of optimized pipelines on unseen test set.
percent error rate as the evaluation metric.

Baseline Methods As discussed in Section 2, SMAC [22] and TPE [5] are the state-of-the-art
algorithms for Bayesian optimization [44, 13, 24], which are used as baselines. Note that Spearmint
[41], a Bayesian optimization algorithm based on Gaussian process is not applicable since it does
not provide a mechanism to handle the hierarchical space [12]. Besides SMAC and TPE, we also
choose random search as a simple baseline for sanity check. Thus, we compare both versions of
our method FLASH (with SMAC in Phase 3) and FLASH(cid:63) (with TPE in Phase 3) against three
baselines in the experiments.
Implementation: To avoid possible mistakes in implementing other methods, we choose a gen-
eral platform for hyperparameter optimization called HPOlib [12], which provides the original
In order to fairly compare our method
implementations of SMAC, TPE, and random search.

3The benchmark datasets are publicly available at http://www.cs.ubc.ca/labs/beta/Projects/autoweka/

datasets.

12

with others, we also implement our algorithm on top of HPOlib, and evaluate all the compared
methods on this platform. We make the source code of FLASH publicly available at https:
//github.com/yuyuz/FLASH.

Experimental Settings We build a general data analytic pipeline based on scikit-learn [36], a
popular used machine learning toolbox in Python. We follow the pipeline design of auto-sklearn
[13]. There are four computational steps in our pipeline: 1) feature rescaling, 2) sample balancing,
3) feature preprocessing, and 4) classiﬁcation model. Each step has various algorithms, and each
algorithm has its own hyperparameters. Adjacent steps are fully connected.
In total, our data
analytic pipeline contains 33 algorithms distributed in four steps, creating 1,456 possible pipeline
paths with 102 hyperparameters (30 categorical and 72 continuous), which creates complex high-
dimensional and highly conditional search space. Details and statistics of this pipeline are listed in
Table 5 in Appendix A.

In all experiments, we set a wall-clock time limit of 10 hours for the entire optimization, 15
minutes time limit and 10GB RAM limit for each pipeline run. We perform 10 independent
optimization runs with each baseline on each benchmark dataset. All experiments were run on
Linux machines with Intel Xeon E5-2630 v3 eight-core processors at 2.40GHz with 256GB RAM.
Since we ran experiments in parallel, to prevent potential competence in CPU resource, we use the
numactl utility to bound each independent run in single CPU core.

For our algorithm FLASH, we set both Tinit and Tprune as 30 iterations (equal to the number of
algorithms in the pipeline), which can be naturally generalized to other budgeted resources such as
wall-clock time or money. We set ξ to 100 in the EIPS function. Note that the performance are not
sensitive to the choices of those parameters. Finally, we set the number of pipeline paths r to 10
, which works well in generating a reasonable-size pruned pipeline G(cid:48). In benchmark experiments,
we compare the performance of FLASH without caching to other methods because the pipelines do
not have complex data preprocessing data like many real-world datasets have. We will use caching
for real-world experiments later in Section 5.

4.1 Results and Discussions

Table 2 reports the experimental results on benchmark datasets. For each dataset, we report the
performance achieved within three diﬀerent time budgets. As shown in the table, our methods
FLASH and FLASH(cid:63) perform signiﬁcantly better than other baselines consistently in all settings,
in terms of both lower error rate and faster convergence. For example, on the Madelon dataset,
our methods reach around 12% test error in only 3 hours, while other baselines are still far from
that even after 10 hours.

Performing statistical signiﬁcance test via bootstrapping, we ﬁnd that often FLASH and FLASH(cid:63)
tie with each other on these benchmark datasets. For all the methods, the test error is quite con-
sistent with the validation error, showing that the potential overﬁtting problem is well prevented
by using cross validation.

Figure 4 plots the convergence curves of median test error rate along with time for all baseline
methods. As shown in the ﬁgure, after running about 4 hours, FLASH and FLASH(cid:63) start to lead
others with steep drop of error rate, and then quickly converge on a superior performance.

13

Table 2: Performance on both 3-fold cross-validation and test data of benchmark datasets. For
each method, we perform 10 independent runs of 10 hours each. Results are reported as the
median percent error across the 10 runs within diﬀerent time budgets. Test data is never seen by
any optimization method, which is only used for oﬄine evaluations to compute test error rates.
Boldface indicates the best result within a block of comparable methods. We underline those results
not statistically signiﬁcantly diﬀerent from the best according to a 10,000 times bootstrap test with
p = 0.05.

Dataset

Budget
(hours)

Cross-validation Performance (%)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Madelon

MNIST

MRBI

Convex

3
5
10

3
5
10

3
5
10

3
5
10

25.16
23.60
20.77

7.68
6.58
6.58

61.80
58.67
57.20

28.14
25.25
24.51

18.90
18.82
17.28

6.78
5.94
5.39

59.83
58.61
53.92

24.70
23.61
22.21

20.25
19.12
17.34

6.05
5.83
5.64

62.89
58.14
54.60

24.69
23.30
23.30

14.84
14.31
13.87

4.93
4.26
4.03

57.43
45.11
41.15

22.63
21.34
20.49

14.04
14.04
13.76

5.05
4.87
4.46

57.08
54.25
41.90

23.31
22.02
20.62

19.17
18.21
15.58

7.75
7.10
6.64

60.58
56.42
54.43

25.04
23.18
22.18

16.15
15.26
14.49

5.41
5.41
5.03

59.83
58.61
52.01

21.42
21.37
20.31

16.03
15.38
13.97

6.11
5.40
5.23

60.58
55.81
52.30

21.97
20.82
20.82

12.18
12.18
11.49

4.62
3.94
3.78

54.72
43.19
39.13

20.65
19.56
18.94

11.73
11.60
11.47

4.84
4.57
4.37

54.28
51.65
39.89

21.04
19.71
19.01

Figure 4: Performance of our methods (FLASH and FLASH(cid:63)) and other compared methods on
MRBI dataset. We show the median percent error rate on test set along with standard error bars
(generated by 10 independent runs) over time.

14

(a) The impact of optimal design
on MRBI dataset

(b) The impact of pipeline pruning
on MRBI dataset

(c) The impact of pipeline caching
on real-world dataset

Figure 5: Component analysis experiments. (a) Optimal design makes the initialization phase more
robust. (b) Pipeline pruning in the second phase of FLASH is the key to its superior performance.
(c) Performance of FLASH without caching and the original FLASH with caching on real-world
dataset. In all ﬁgures, we show the median error rate on test set along with standard error bars
(generated by 10 independent runs). Note that (a) and (b) are plotted with diﬀerent x-axes; (c) is
on a diﬀerent dataset as (a) and (b).

4.2 Detailed Study of FLASH Components

FLASH has three main components: optimal design for initialization, cost-sensitive model for
pipeline pruning, and pipeline caching. To study their individual contributions to the performance
gain, we drop out each of the component and compare the performance with original FLASH. Since
caching will be used for real-world experiments on large dataset, we describe the analysis of caching
component in Section 5. Here we use MRBI dataset for these experiments.

Figure 5(a) shows the diﬀerence between using random initialization and optimal design by
plotting the performance on initial 30 pipeline runs. The desirable property of optimal design en-
sures to run reasonable pipeline paths, giving FLASH a head start at the beginning of optimization.
While random initialization is not robust enough, especially when the number of pipeline runs is
very limited and some algorithms will have no chance to run due to the randomness. Figure 5(b)
shows the impact of pipeline pruning in the second phase of FLASH. Dropping out the pruning
phase with EIPS, and using SMAC immediately after Phase 1, we see a major degradation of the
performance. The ﬁgure clearly shows that in Phase 2 of FLASH, the linear model with EIPS
acquisition function is able to eﬃciently shrink the search space signiﬁcantly such that SMAC can
focus on those algorithms which perform well with little cost. This ﬁgure conﬁrms the main idea
of this paper that a simple linear model can be more eﬀective in searching high-dimensional and
highly conditional hyperparameter space.

5 Real-world Experiments

In this section, to demonstrate a real-world use case, we apply FLASH on a large de-identiﬁed
medical dataset for classifying drug non-responders. We show how our method can quickly ﬁnd
good classiﬁer for diﬀerentiating non-responders vs. responders.

15

Table 3: Statistics of the medical dataset.

#Patient #Event #Med #Class

train case
train control
test case
test control
total

18,581
18,582
4,646
4,646
46,455

982,025
622,777
245,776
153,303
2,003,881

434,171
286,198
108,702
70,395
899,466

547,854
336,579
137,074
82,908
1,104,415

Budget
(hours)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

3
5
10

30.32
16.66
11.75

27.03
19.09
4.86

35.40
33.22
21.03

21.02
14.40
2.51

23.28
19.86
3.44

Table 4: Performance of real-world dataset. Results are reported using the same settings as Table 2.

Experimental Settings With a collaboration with a pharmaceutical company, we created a
balanced cohort of 46,455 patients from a large claim dataset. Patients who have at least 4 times of
treatment failure are regarded as drug non-responders (case group). Other patients are responders
of the drug (control group). The prediction target is whether a patient belongs to case group or
control group. Each patient is associated with a sequence of events, where each event is a tuple of
format (patient-id, event-id, timestamp, value). Table 3 summarizes the statistics of this
clinical dataset, including the count of patient, event, medication, and medication class.

Unlike benchmark experiments, the input to the real-world pipeline is not directly as feature
vectors. Given a cohort of patients with their event sequences, like [9] the pipeline for non-responder
classiﬁcation has two more additional steps than the pipeline described in previous benchmark
experiments: 1) Feature construction to convert patient event sequence data into numerical feature
vectors. This step can be quite time-consuming as advanced feature construction techniques like
sequential mining [29] and tensor factorization [19] can be expensive to compute. On this medical
i) frequency threshold to remove rare
dataset, we consider two kinds of parameters in this step:
events (frequency ranging from 2 to 5) ii) various aggregation functions (including binary, count,
sum and average) to aggregate multiple occurrence of events into features. The output of this step
will be a feature matrix and corresponding classiﬁcation targets; 2) Densify the feature matrix
from above feature construction step if necessary. Features of this real-world dataset can be quite
sparse. We by default use sparse representation to save space and accelerate computation in some
algorithms. Unfortunately, not all algorithm implementations in scikit-learn accept sparse features.
A decision has to be made here: either sparse matrix for faster result or dense matrix for broader
algorithm choices in later steps.

We run the experiments on same machine, use same parameter setting and same budget as
benchmark experiments. We compare our method with the same baselines as benchmark experi-
ments and we continue using error rate as metric. Our algorithm has built-in caching mechanism

16

and we will use that. For this real-world dataset, we ﬁrst compare with baselines with cache
enabled. Then we analyze the contribution of caching.

5.1 Results and Discussions

Table 4 shows the performance of our methods compared to baselines when caching is enabled. Due
to lack of space we only report the test performance. All cases FLASH and FLASH(cid:63) signiﬁcantly
outperform all the baselines.

Figure 5(c) shows the performance of FLASH without caching and original FLASH with caching
on the real-world medical dataset. With caching, more pipeline paths can be evaluated within
given period of time and our EIPS-based path selection leverages caching to select paths with high
performance that run fast. As a result, we can see FLASH with caching converges much faster.
For example, with caching we can get low test error within 6 hours.

6 Conclusions

In this work, we propose a two-layer Bayesian optimization algorithm named FLASH, which enables
highly eﬃcient optimization of complex data analytic pipelines. We showed that all components
of FLASH complement each other: 1) our optimal design strategy ensures better initialization,
giving a head start to the optimization procedure; 2) the cost-sensitive model takes advantage of
this head start, and signiﬁcantly improves the performance by pruning ineﬃcient pipeline paths;
3) the pipeline caching reduces the cost during the entire optimization, which provides a global
acceleration of our algorithm. We demonstrate that our method signiﬁcantly outperforms previous
state-of-the-art approaches in both benchmark and real-world experiments.

7 Acknowledgments

This work was supported by the National Science Foundation, award IIS- #1418511 and CCF-
#1533768, research partnership between Children’s Healthcare of Atlanta and the Georgia Institute
of Technology, CDC I-SMILE project, Google Faculty Award, Sutter health and UCB.

17

References

2012.

[1] A. Atkinson and A. Donev. Optimum experimental designs. 1992.

[2] T. B¨ack. Evolutionary algorithms in theory and practice. 1996.

[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 13(1),

[4] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures. In ICML, 2013.

[5] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl. Algorithms for hyper-parameter opti-

mization. In NIPS, 2011.

[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[7] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.

[8] G. Calinescu, C. Chekuri, M. P´al, and J. Vondr´ak. Maximizing a monotone submodular

function subject to a matroid constraint. SIAM Journal on Computing, 40(6), 2011.

[9] R. Chen, H. Su, Y. Zhen, M. Khalilia, D. Hirsch, M. Thompson, T. Davis, Y. Peng, S. Lin,
J. Tejedor-Sojo, E. Searles, and J. Sun. Cloud-based predictive modeling system and its
application to asthma readmission prediction. In AMIA. AMIA, 2015.

[10] S. J. Coakes and L. Steed. SPSS: Analysis without anguish using SPSS version 14.0 for

Windows. John Wiley & Sons, Inc., 2009.

[11] D. Donoho. 50 years of Data Science. Technical report, University of California Berkeley, 2015.

[12] K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown.
Towards an empirical foundation for assessing bayesian optimization of hyperparameters. In
NIPS workshop on Bayesian Optimization in Theory and Practice, 2013.

[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Eﬃcient and

robust automated machine learning. In NIPS, 2015.

[14] M. Feurer, T. Springenberg, and F. Hutter. Initializing bayesian hyperparameter optimization

via meta-learning. In AAAI, 2015.

[15] P. Flaherty, A. Arkin, and M. I. Jordan. Robust design of biological experiments. In NIPS,

2005.

[16] C. J. Flynn, C. M. Hurvich, and J. S. Simonoﬀ. Eﬃciency for regularization parameter selection

in penalized likelihood estimation of misspeciﬁed models. JASA, 108(503), 2013.

[17] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data

mining software: an update. ACM SIGKDD explorations newsletter, 11(1), 2009.

18

[18] X. He. Laplacian regularized d-optimal design for active learning and its application to image

retrieval. Image Processing, IEEE Transactions on, 19(1), 2010.

[19] J. C. Ho, J. Ghosh, and J. Sun. Marble: high-throughput phenotyping from electronic health

records via sparse nonnegative tensor factorization. In KDD, 2014.

[20] M. D. Hoﬀman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization.

In UAI. Citeseer, 2011.

[21] M. D. Hoﬀman, B. Shahriari, and N. de Freitas. On correlation and budget constraints in
In AIS-

model-based bandit optimization with application to automatic machine learning.
TATS, 2014.

[22] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general

algorithm conﬁguration. In Learning and Intelligent Optimization. Springer, 2011.

[23] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, 2013.

[24] B. Komer, J. Bergstra, and C. Eliasmith. Hyperoptsklearn: Automatic hyperparameter con-

ﬁguration for scikitlearn. In ICML workshop on AutoML, 2014.

[25] A. Krause and C. Guestrin. Submodularity and its applications in optimized information

gathering. TIST, 2(4), 2011.

[26] A. Kumar, R. McCann, J. Naughton, and J. M. Patel. Model selection management systems:

The next frontier of advanced analytics. ACM SIGMOD Record, 2015.

[27] D. J. Lizotte. Practical bayesian optimization. University of Alberta, 2008.

[28] J. Lv and J. S. Liu. Model selection principles in misspeciﬁed models. JRSS-B, 76(1), 2014.

[29] K. Malhotra, T. Hobson, S. Valkova, L. Pullum, and A. Ramanathan. Sequential pattern min-
ing of electronic healthcare reimbursement claims: Experiences and challenges in uncovering
how patients are treated by physicians. In Big Data, Oct 2015.

[30] X. Meng, J. Bradley, E. Sparks, and S. Venkataraman. Ml pipelines: a new high-level api for

mllib, 2015.

[31] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. Yale: Rapid prototyping for

complex data mining tasks. In KDD, 2006.

[32] J. Mockus, V. Tiesis, and A. Zilinskas. The application of bayesian methods for seeking the

extremum. Towards Global Optimization, 2(117-129), 1978.

[33] R. Munos. Optimistic optimization of deterministic functions without the knowledge of its

smoothness. In Advances in neural information processing systems, 2011.

[34] Y. Nesterov. Gradient methods for minimizing composite objective function, 2007.

[35] K. Ng, A. Ghoting, S. R. Steinhubl, W. F. Stewart, B. Malin, and J. Sun. Paramo: A parallel
predictive modeling platform for healthcare analytic research using electronic health records.
Journal of biomedical informatics, 48, 2014.

19

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. JMLR,
12, 2011.

[37] T. Robertazzi and S. Schwartz. An accelerated sequential algorithm for producing d-optimal

designs. SIAM Journal on Scientiﬁc and Statistical Computing, 10(2), 1989.

[38] B. Settles. Active Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning,

6(1), jun 2012.

[39] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of
the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.

[40] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor selection: Leveraging submodularity.

In CDC, 2010.

algorithms. In NIPS, 2012.

[41] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning

[42] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Ali,
R. P. Adams, et al. Scalable bayesian optimization using deep neural networks. arXiv preprint
arXiv:1502.05700, 2015.

[43] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.

[44] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka: Combined selection

and hyperparameter optimization of classiﬁcation algorithms. In KDD, 2013.

[45] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global opti-
mization of expensive-to-evaluate functions. Journal of Global Optimization, 44(4), 2009.

[46] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high

dimensions via random embeddings. In IJCAI. Citeseer, 2013.

[47] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

[48] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 1982.

20

Table 5: Detailed information of the data analytic pipeline constructed for benchmark experiments.

Algorithm

#categorical #continuous

Total
#hyper-
parameters

A Appendix

Pipeline
step

Feature
rescaling

Sample
balancing

Feature
preprocessing

Classiﬁcation

0
0
0
0

0
0

2
3
2
1
0
0
0
1
1
1
0
1
1

1
1
2
0
0
2
1
0
2
1
1
0
2
4

Min-max scaler
None
Normalization
Standardization

Class weighting
None

Extremely randomized trees
Fast ICA
Feature agglomeration
Kernel PCA
Random kitchen sinks
Linear SVM
None
Nystroem sampler
PCA
Polynomial combinations
Random trees embedding
Percentile feature selection
Univariate feature selection

AdaBoost
Decision tree
Extremely randomized trees
Gaussian naive Bayes
Gradient boosting
K-nearest neighbors
LDA
Linear SVM
Kernel SVM
Multinomial naive Bayes
Passive aggressive
QDA
Random forest
Stochastic gradient descent

21

0
0
0
0

0
0

3
1
1
6
2
2
0
8
1
2
4
1
2

3
3
3
0
6
1
3
2
5
1
2
1
3
6

0
0
0
0

0
0

5
4
3
7
2
2
0
9
2
3
4
2
3

4
4
5
0
6
3
4
2
7
2
3
1
5
10

Total#

33

30

72

102

6
1
0
2
 
n
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
3
v
8
6
4
6
0
.
2
0
6
1
:
v
i
X
r
a

FLASH: Fast Bayesian Optimization for
Data Analytic Pipelines

Yuyu Zhang Mohammad Taha Bahadori Hang Su

Jimeng Sun

Georgia Institute of Technology
{yuyu,bahadori,hangsu}@gatech.edu,jsun@cc.gatech.edu

Abstract

Modern data science relies on data analytic pipelines to organize interdependent compu-
tational steps. Such analytic pipelines often involve diﬀerent algorithms across multiple steps,
each with its own hyperparameters. To achieve the best performance, it is often critical to select
optimal algorithms and to set appropriate hyperparameters, which requires large computational
eﬀorts. Bayesian optimization provides a principled way for searching optimal hyperparame-
ters for a single algorithm. However, many challenges remain in solving pipeline optimization
problems with high-dimensional and highly conditional search space. In this work, we propose
Fast LineAr SearcH (FLASH), an eﬃcient method for tuning analytic pipelines. FLASH is
a two-layer Bayesian optimization framework, which ﬁrstly uses a parametric model to select
promising algorithms, then computes a nonparametric model to ﬁne-tune hyperparameters of
the promising algorithms. FLASH also includes an eﬀective caching algorithm which can fur-
ther accelerate the search process. Extensive experiments on a number of benchmark datasets
have demonstrated that FLASH signiﬁcantly outperforms previous state-of-the-art methods in
both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%
improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art
performance on a real-world application for healthcare predictive modeling.

1 Introduction

Modern data science often requires many computational steps such as data preprocessing, fea-
ture extraction, model building, and model evaluation, all connected in a data analytic pipeline.
Pipelines provide a natural way to represent, organize and standardize data analytic tasks, which
are considered to be an essential element in the data science ﬁeld [11] due to their key role in large-
scale data science projects. Many machine learning toolboxes such as scikit-learn [36], RapidMiner
[31], SPSS [10], Apache Spark [30] provide mechanisms for conﬁguring analytic pipelines.

An analytic pipeline skeleton is shown in Figure 1. Each step, such as feature preprocessing
and classiﬁcation, includes many algorithms to choose from. These algorithms usually require
users to set hyperparameters, ranging from optimization hyperparameters such as learning rate and
regularization coeﬃcients, to model design hyperparameters such as the number of trees in random
forest and the number of hidden layers in neural networks. There are an exponential number of
choices for the combination of algorithms and hyperparameters in a given analytic pipeline skeleton.
Because of the interdependency between all the algorithms and their hyperparameters, the choices
can have huge impact on the performance of the best model.

1

Figure 1: A typical data analytic pipeline.

Tuning hyperparameters of a single algorithm can be viewed as an optimization problem of a
black-box objective function, which is noisy and often expensive to evaluate. Here the input of
black-box are the hyperparameters, and the objective function is the output performance such as
accuracy, precision and recall. To tackle this problem, simple methods have been applied such as
grid or random search [5, 3]. While on diﬃcult problems where these simple approaches are not
eﬃcient, a more promising model-based approach is Bayesian optimization [32, 27, 7, 39]. The high-
level idea of Bayesian optimization is to deﬁne a relatively cheap surrogate function and use that
to search the hyperparameter space. Indeed, there exist other global optimization methods, such
as evolutionary algorithms [2] and optimistic optimization [33]. We choose Bayesian optimization
framework due to its great performance in practice. Recently, Bayesian optimization methods have
been shown to outperform other methods on various tasks, and in some cases even beat human
domain experts to achieve better performance via tuning hyperparameters [42, 4].

Despite its success, applying Bayesian optimization for tuning analytic pipelines faces several
signiﬁcant challenges: Existing Bayesian optimization methods are usually based on nonparametric
models, such as Gaussian process and random forest. A major drawback of these methods is that
they require a large number of observations to ﬁnd reasonable solutions in high-dimensional space.
When tuning a single algorithm with several hyperparameters, Bayesian optimization works well
with just a few observations. However, when it comes to pipeline tuning, thousands of possible
combinations of algorithms plus their hyperparameters jointly create a large hierarchical high-
dimensional space to search over, whereas existing methods tend to become ineﬃcient. Wang et
al. [46] tackled the high-dimensional problem by making a low eﬀective dimensional assumption.
However, it is still a ﬂat Bayesian optimization method and not able to handle the exploding
dimensionality problem caused by hierarchically structured hyperparameters in analytic pipeline
tuning.
Motivating example: We build an analytic pipeline for classiﬁcation task (details in Section 4). If
we give 10 trials for each hyperparameter over 1,456 unique pipeline paths and 102 hyperparameters,
we have more than 2 million conﬁgurations, which can take years to complete with a brute-force
search. Even with the state-of-the-art Bayesian optimization algorithm such as Sequential Model-
based Algorithm Conﬁguration (SMAC) [22], the process can still be slow as shown in Figure 2.
If we know the optimal algorithms ahead time (Oracle) with just hyperparameter tuning of the
optimal algorithms, we can obtain signiﬁcant time saving, which is however not possible. Finally,
our proposed method FLASH can converge towards the oracle performance much more quickly
than SMAC.

In this paper, we propose a two-layer Bayesian optimization algorithm called Fast LineAr SearcH
(FLASH): the ﬁrst layer for selecting algorithms, and the second layer for tuning the hyperparame-
ters of selected algorithms. FLASH is able to outperform the state-of-the-art Bayesian optimization
algorithms by a large margin, as shown in Figure 2. By designing FLASH, we make three main
contributions:

• We propose a linear model for propagation of error (or other quantitative metrics) in analytic

2

Figure 2: Performance comparison of a data analytic pipeline on MRBI dataset, including the
previous state of the art SMAC, proposed method FLASH, and Oracle (i.e., pretending the optimal
algorithm conﬁguration is given, and only performing hyperparameter tuning with SMAC on those
algorithms). We show the median percent error rate on the test set along with standard error bars
(generated by 10 independent runs) over time. FLASH outperforms SMAC by a big margin and
converges toward Oracle performance quickly.

pipelines. We also propose a Bayesian optimization algorithm for minimizing the aggregated error
using our linear error model. Our proposed mechanism can be considered as a hybrid model: a
parametric linear model for fast exploration and pruning of the algorithm space, followed by a
nonparametric hyperparameter ﬁne-tuning algorithm.

• We propose to initialize the hyperparameter tuning algorithm using the optimal design strategy
[1, 15, 38] which is more robust than the random initialization. We also propose a fast greedy
algorithm to eﬃciently solve the optimal design problem for any given analytic pipeline.

• Finally, we introduce a caching algorithm that can signiﬁcantly accelerate the tuning process. In
particular, we model the (time) cost of each algorithm, and incorporate that in the optimization
process. This ensures the eﬃciency of fast search.

We demonstrate the eﬀectiveness of FLASH with extensive experiments on a number of diﬃcult
problems. On the benchmark datasets for pipeline conﬁgurations tuning, FLASH substantially
improves the previous state of the art by 7% to 25% in test error rate within the same time budget.
We also experiment with large-scale real-world datasets on healthcare data analytic tasks where
FLASH also exhibits superior results.

2 Background and Related Work

2.1 Data Analytic Pipelines

The data analytic pipeline refers to a framework consisting of a sequence of computational transfor-
mations on the data to produce the ﬁnal predictions (or outputs) [26]. Pipelines help users better

3

understand and organize the analysis task, as well as increase the reusability of algorithm imple-
mentations in each step. Several existing widely adopted machine learning toolboxes provide the
functionality to run analytic pipelines. Scikit-learn [36] and Spark ML [30] provide programmatic
ways to instantiate a pipeline. SPSS [10] and RapidMiner [31] provide a visual way to assemble
an analytic pipeline instance together and run. Microsoft Azure Machine Learning1 provides a
similar capability in a cloud setting. There are also specialized pipelines, such as PARAMO [35] in
healthcare data analysis.

However, a major diﬃculty in using these systems is that none of the above described tools is
able to eﬃciently help users decide which algorithms to use in each step. Some of the tools such as
scikit-learn, Spark ML, and PARAMO allow searching all possible pipeline paths and tuning the
hyperparameters of each step using an expensive grid search approach. While the search process
can be sped up by running in parallel, the search space is still too large for the exhaustive search
algorithms.

2.2 Bayesian Optimization

Bayesian optimization is a well-established technique for global and black-box optimization prob-
lems. In a nutshell, it comprises two main components: a probabilistic model and an acquisition
function. For the probabilistic model, there are several popular choices: Gaussian process [41, 42],
random forest such as Sequential Model-based Algorithm Conﬁguration (SMAC) [22], and density
estimation models such as Tree-structured Parzen Estimator (TPE) [5]. Given any of these models,
the posterior mean and variance of a new input can be computed, and used for computation of
the acquisition function. The acquisition function deﬁnes the criterion to determine future input
candidates for evaluation. Compared to the objective function, the acquisition function is chosen
to be relatively cheap to evaluate, so that the most promising next input for querying can be found
quickly. Various forms of acquisition functions have been proposed [43, 20, 45, 21]. One of the
most prominent acquisition function is the Expected Improvement (EI) function [32], which has
been widely used in Bayesian optimization. In this work, we use EI as our acquisition function,
which is formally described in Section 3.

Bayesian optimization is known to be successful in tuning hyperparameters for various learning
algorithms on diﬀerent types of tasks [42, 14, 4, 41, 46]. Recently, for the problem of pipeline con-
ﬁgurations tuning, several Bayesian optimization based systems have been proposed: Auto-WEKA
[44] which applies SMAC [22] to WEKA [17], auto-sklearn [13] which applies SMAC to scikit-learn
[36], and hyperopt-sklearn [24] which applies TPE [5] to scikit-learn. The basic idea of applying
Bayesian optimization to pipeline tuning is to expand the hyperparameters of all algorithms and
create large search space to perform optimization as we will show in the experiments. However,
for practical pipelines the space becomes too large which hinders convergence of the optimization
process. Auto-sklearn [13] uses a meta-learning algorithm that leverages performance history of
algorithms on existing datasets to reduce the search space. However, in real-world applications,
we often have unique datasets and tasks such that ﬁnding similar datasets and problems for the
meta-learning algorithm will be diﬃcult.

1https://studio.azureml.net

4

Figure 3: A toy example of data analytic pipeline. One possible pipeline path, ﬂowing from the
input Vin to the output Vout, is highlighted in shaded area.

3 Methodology

A data analytic pipeline G = (V, E) can be represented as a multi-step Directed Acyclic Graph
(DAG), where V is the set of algorithms, and E is the set of directed edges indicating dependency
between algorithms. Algorithms are distributed among multiple steps. Let V (k)
denote the ith
i
algorithm in the kth step. Each directed edge (V (k)
) ∈ E represents the connection from
algorithm V (k)
. Note that there is no edge between algorithms in the same step. We also
have an input data vertex Vin which points to all algorithms in the ﬁrst step, and an output vertex
Vout which is pointed by all algorithms in the last step.

to V (k+1)
j

, V (k+1)
j

i

i

A pipeline path is any path from the input Vin to the output Vout in pipeline graph G . To denote
a pipeline path of K steps, we use K one-hot vectors p(k) (1 ≤ k ≤ K), each denoting the algorithm
selected in the k-th step. Thus, the concatenation of one-hot vectors p = (cid:2)p(1), . . . , p(K)(cid:3) ∈ {0, 1}N
denotes a pipeline path, where N is the total number of algorithms in the pipeline G. Figure 3
shows a small data analytic pipeline with two steps. The ﬁrst step contains two algorithms, and the
second step contains three. One possible pipeline path is highlighted in the shaded area. On this
pipeline path, V (1)
are selected in the ﬁrst and second step, so that we have p(1) = [0, 1]
and p(2) = [0, 0, 1]. Thereby, the highlighted pipeline path is given by p = (cid:2)p(1), p(2)(cid:3) = [0, 1, 0, 0, 1].
For any pipeline path p, we concatenate all of its hyperparameters in a vector λp. The pair of path
and hyperparameters, i.e. (p, λp), forms a pipeline conﬁguration to be run. For ease of reference,
we list the notations in Table 1.

and V (2)

2

3

The problem of tuning data analytic pipelines can be formalized as an optimization problem:

Problem 1. Given a data analytic pipeline G with input data D, resource budget T , evaluation
metric function m(G, D; p, λp), resource cost of running pipeline τ (G, D; p, λp), how to ﬁnd the
pipeline path p and its hyperparameters λp with best performance m(cid:63)?

The performance of the best pipeline path is denoted by m(cid:63) = minp,λp m(G, D; p, λp) subject

5

Table 1: Mathematical notations used in this paper.

Symbol Description

G
V
E
K
N
D
Vin
Vout
V (k)
i
p(k)
λ(k)
p
p
λp
m(·)
τ (·)
Tinit
Tprune
Ttotal

data analytic pipeline
set of algorithms in G
set of dependency between algorithms
total number of steps in G
total number of algorithms in G
input data of pipeline
input vertex of G
output vertex of G
ith algorithm in kth step
one-hot vector for kth step
hyperparameters for kth step
pipeline path
all hyperparameters of p
evaluation metric function
time cost of running pipeline
budget for Phase 1
budget for Phase 2
budget for Phase 3

to budget T . The objective is to approach the optimal performance within the budget T via
(cid:98)p) ≤ m(cid:63) + (cid:15) for
optimizing over p, λp; i.e., we would like our solution (cid:98)p, (cid:98)λ
small values of (cid:15).

(cid:98)p to satisfy m(G, D; (cid:98)p, (cid:98)λ

To eﬃciently tackle this problem, we propose a two-layer Bayesian optimization approach named
Fast LineAr SearcH (FLASH). We generally introduce the idea of linear model and describe the
algorithm in Section 3.1. An immediate advantage of using linear model is that we can use more
principled initialization instead of random initialization, as discussed in Section 3.2. We use cost-
sensitive modeling to prune the pipeline, as described in Section 3.3. Finally, we accelerate the
entire optimization procedure via pipeline caching, which we describe in Section 3.4.

3.1 Two-layer Bayesian Optimization

Inspired by the performance of linear regression under model misspeciﬁcation [48, 16, 28] and supe-
rior sample complexity compared to more ﬂexible nonparametric techniques [47], we seek parametric
models for propagation of error (or other quantitative metrics) in analytic pipelines. The high level
idea of FLASH is as follows: we propose a linear model for estimating the propagation of error (or
any other metric) in a given analytic pipeline. The linear model assumes that the performance of
algorithms in diﬀerent steps are independent, and the ﬁnal performance is additive from all algo-
rithms. That is, we can imagine that each algorithm is associated with a performance metric, and
the total performance of a pipeline path is the sum of the metrics for all algorithms in the path.
This linear model will replace the Gaussian process or random forest in the initial stages of the
pipeline tuning process. In the rest of this section, we provide the details of Bayesian optimization

6

with our linear model.

We apply the linear model only to the pipeline selection vector p and assume that the variations
due to hyperparameters of the algorithms are captured in the noise term. That is, we assume that
the error of any pipeline path p can be written as

m = β(cid:62)p + ε

where β ∈ RN denotes the parameters of the linear model. Given a set of observations of the
algorithm selection and the corresponding evaluation metric for the selected pipeline path in the
form of (pi, mi), i = 1, . . . , n, we can ﬁt this model and infer its mean µ(p) and variance σ2(p)
of the performance estimation for any new pipeline path represented by p. In particular, let the
design matrix P ∈ Rn×N denote the stacked version of the pipeline paths, i.e., P = [p1, . . . , pn](cid:62),
and m ∈ Rn be the corresponding response values of the evaluation metrics, m = [m1, . . . , mn]. We
use the following L2 regularized linear regression to obtain the robust estimate for β from history
observations:

(cid:98)β(P , m) = argmin

β

(cid:26) 1
n

(cid:107)P β − m(cid:107)2

2 + λ(cid:107)β(cid:107)2
2

(cid:27)

(1)

(cid:113)(cid:0)(cid:80)n

(cid:1). The predictive
where for any vector x ∈ Rn the L2 norm is deﬁned as (cid:107)x(cid:107)2 =
distribution for the linear model is Gaussian with mean (cid:98)µp = (cid:98)β(cid:62)p and variance (cid:98)σp = σ2
ε (1 +
p(cid:62)(P (cid:62)P + λI)−1p) where σ2
ε is the variance of noise in the model. We estimate σε as follows: the
residual in the ith observation is computed as (cid:98)(cid:15)i = (cid:98)µi − mi where (cid:98)µi = (cid:98)β(cid:62)pi is the estimate of mi
by our model. Thus, the variance of the residual can be found as (cid:98)σ2
(cid:15) = var((cid:98)µi − mi) where var(·)
denotes the variance operator.

i=1 x2
i

To perform Bayesian optimization with linear model, we use the popular Expected Improvement
(EI) criteria, which recommends to select the next sample pt+1 such that the following acquisition
function is maximized. The acquisition function represents the expected improvement over the best
observed result m+ at a new pipeline path p [44]:

EI(p) = E[Im+(p)] = σp[uΦ(u) + φ(u)]

(2)

u = m+−ξ−µp

σp

where
and ξ is a parameter to balance the trade-oﬀ between exploitation and
exploration. EI function is maximized for paths with small values of mp and large values of σp,
reﬂecting the exploitation and exploration trade-oﬀs, respectively. To be more speciﬁc, larger ξ
encourages more exploration in selecting the next sample. The functions Φ(·) and φ(·) represent
CDF and PDF of standard normal distribution, respectively. The idea of Bayesian optimization
with EI is that at each step, we compute the EI with the predictive distribution of the existing linear
model and ﬁnd the pipeline path that maximizes EI. We choose that path and run the pipeline
with it to obtain a new (pi, mi) pair. We use this pair to reﬁt and update our linear model and
repeat the process. Later on we also present an enhanced version of EI via normalizing it by cost
called Expected Improvement Per Second (EIPS).

We provide the full details of FLASH in Algorithm 1. While the main idea of FLASH is
performing Bayesian optimization using linear model and EI, it has several additional ideas to
make it practical. Speciﬁcally, FLASH has three phases:

• Phase 1, we initialize the algorithm using ideas from optimal design, see Section 3.2. The

budget is bounded by Tinit.

7

Algorithm 1: Fast Linear Search (FLASH)

input : Data analytic pipeline G; input data D; total budget for entire optimization Ttotal;

budget for initialization Tinit; budget for pipeline pruning Tprune; number of top
pipeline paths r

(cid:98)p

output: Optimized pipeline conﬁguration (cid:98)p and (cid:98)λ
/* Phase 1: Initialization (Section 3.2)
1 while budget Tinit not exhausted do
2

p ← new pipeline path from Algorithm 2
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:99)βτ (P , τ ) using Eq. (1)

3

5

4

6

/* Phase 2: Pipeline pruning (Section 3.3)
7 while budget Tprune not exhausted do
8

9

10

11

p ← argmaxp EIP S(p, P , β, βτ ) using Eq. (4)
λp ← random hyperparameters for p
m, τ ← RunPipeline(G, D; p, λp) with Algorithm 3
P ← [P ; p(cid:62)], m ← [m, m], τ ← [τ , τ ]
β ← (cid:98)β(P , m), βτ ← (cid:98)βτ (P , τ ) using Eq. (1)

using Eq. (4)
/* Phase 3: Pipeline tuning

14 S ← history observations within G(cid:48)
15 Initialize model M given S
16 while budget Ttotal not exhausted do
p, λp ← next candidate from M
17
m ← RunPipeline(G(cid:48), D; p, λp) with Algorithm 3
S ← S ∪ {(p, λp, m)}
Update M given S
(cid:98)p ← Best conﬁguration so far found for G(cid:48)

21 (cid:98)p, (cid:98)λ

20

18

19

12
13 G(cid:48) ← construct subgraph of G with top r pipeline paths with largest EIP S(p, P , β, βτ )

*/

*/

*/

• Phase 2, we leverage Bayesian optimization to ﬁnd the top r best pipeline paths and prune

the pipeline G to obtain simpler one G(cid:48). The budget is bounded by Tprune

2.

• Phase 3, we use general model-based Bayesian optimization methods to ﬁne-tune the pruned

pipeline together with their hyperparameters.

In Phase 3, we use state-of-the-art Bayesian optimization algorithm, either SMAC or TPE. These
algorithms are iterative: they use a model M such as Gaussian process or random forest and use
EI to pick up a promising pipeline path with hyperparameters for running, and then update the
model with the new observation just obtained, and again pick up the next one for running. The

2In practice, it is better to use the time normalized EI (that is EIPS) during Phase 2; this idea is described in

Section 3.3.

8

i=1; number of desired pipeline paths ninit

Algorithm 2: Initialization with Optimal Design

/* Batch version
input : B initial candidates {pi}B
output: Optimal set of pipeline paths Q
1 p1 ← random pipeline path for initialization
2 H ← p1p(cid:62)
1
3 Q ← {p1}
4 for (cid:96) = 2, . . . , ninit do
5

j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)
H ← H + pj(cid:63)p(cid:62)
j(cid:63)
Q ← Q ∪ {pj(cid:63)}

6

7

j ) for j = 1, . . . , B.

/* Online version
input : B candidates {pi}B
output: Next pipeline path pj(cid:63), j(cid:63) ∈ {1, . . . , B}

i=1; current Gram matrix H

8 j(cid:63) ← argmaxj D(cid:96)(H + pjp(cid:62)

j ) for j = 1, . . . , B

*/

*/

budget is bounded by Ttotal. Note that our algorithm is currently described for a sequential setting
but can be easily extended to support parallel runs of multiple pipeline paths as well.

3.2 Optimal Design for Initialization

Most Bayesian optimization algorithms rely on random initialization which can be ineﬃcient; for
example, it may select duplicate pipeline paths for initialization. Intuitively, the pipeline paths
used for initialization should cover the pipeline graph well, such that all algorithms are included
enough times in the initialization phase. The ultimate goal is to select a set of pipeline paths
for initialization such that the error in estimation of β is minimized. Given our proposed linear
model, we can ﬁnd the optimal strategy for initialization to make sure the pipeline graph is well
covered and the tuning process is robust. In this section, we describe diﬀerent optimality criteria
studied in statistical experiment design [1, 15] and active learning [38], and design an algorithm for
initialization step of FLASH.

Given a set of pipeline paths with size n, there are several diﬀerent optimality criteria in terms
i=1 pip(cid:62)

i as follows [38]:

of the eigenvalues of the Gram matrix H = (cid:80)n
A-optimality: maximize (cid:80)n
D-optimality: maximize (cid:81)n

(cid:96)=1 λ(cid:96)(H).

(cid:96)=1 λ(cid:96)(H).

E-optimality: maximize λn(H), the nth largest eigenvalue.

It is easy to see that any arbitrary set of pipeline path designs satisﬁes the A-optimality criterion.

Proposition 1. Any arbitrary set of pipeline paths with size n is a size-n A-optimal design.

Proof. For any arbitrary set of pipeline paths with size n, we have:

n
(cid:88)

(cid:96)=1

λ(cid:96)(H) = tr(H) = tr

(cid:33)

pip(cid:62)
i

=

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:16)

tr

pip(cid:62)
i

(cid:17)

= nK.

9

The last step is due to particular pattern of p in our problem. Thus, we show that (cid:80)n
constant, independent of the design of pipeline paths.

(cid:96)=1 λ(cid:96)(H) is

Proposition 1 rules out use of A-optimality in pipeline initialization. Given the computational
complexity of E-optimality and the fact that it intends for optimality in the extreme cases, we
choose D-optimality criterion. The D-optimality criterion for design of optimal linear regression
can be stated as follows: suppose we are allowed to evaluate ninit samples pi, i = 1, . . . , ninit,
these samples should be designed such that the determinant of the Gram matrix H is maximized.
While we can formulate an optimization problem that directly ﬁnds pi values, we found that an
alternative approach can be computationally more eﬃcient. In this approach, we ﬁrst generate B
candidate pipeline paths for an integer B larger than the number of algorithms in the pipeline N .
This set may include all possible pipeline paths if the total number of paths is small. Then, our
goal becomes selecting a subset of size ninit from them. We can formulate the optimal design as
follows

a(cid:63) = argmax

det

a

(cid:41)

aipip(cid:62)
i

(cid:40) B
(cid:88)

i=1

s.t.

a ∈ {0, 1}K,

1(cid:62)a = ninit.

(3)

The last constraint 1(cid:62)a = ninit indicates that only ninit pipeline paths should be selected. The
objective function is concave in terms of continuous valued a [6, Chapter 3.1.5]. Thus, a traditional
approach is to solve it by convex programming after relaxation of the integrality constraint on a.
The matrix in the argument of the determinant is only N -dimensional which means calculation of
the determinant should be fast. Nestrov’s accelerated gradient descent [34] or Frank-Wolfe’s [23]
algorithms can be used for eﬃciently solving such problems.

An even faster solution can be found by using greedy forward selection ideas which are fast
and popular for optimal experiment design, for example see [37, 18, 25] and the references therein.
To apply greedy technique to our problem, we initialize the solution by picking one of the pipeline
i . Then, at (cid:96)th step, we add the path that maximizes j(cid:63) = argmaxj D(cid:96)(H + pjp(cid:62)
path H = pip(cid:62)
j )
where D(cid:96)(H) = (cid:81)min((cid:96),p)
λi(H) denotes the product of top min((cid:96), p) eigenvalues of its argument.
The algorithm is described in Algorithm 2. The optimization problem in Eq. (3) appears in other
ﬁelds such as optimal facility location and sensor planning where greedy algorithm is known to
have a 1 − 1

e approximation guarantee [8, 40].

One further desirable property of the greedy algorithm is that it is easy to run it under a
time budget constraint. We call this version the online version in Algorithm 2, where instead of a
ﬁxed number of iteration ninit, we run it until the exhaustion of our time budget. See Line 2 in
Algorithm 1 and the online version of Algorithm 2.

i=1

3.3 Cost-sensitive Modeling

The Expected Improvement aims at approaching the true optimal (doing well) within a small number
of function evaluations (doing fast). However, the time cost of each function evaluation may vary
a lot due to diﬀerent settings of hyperparameters. This problem is particularly highlighted in
pipeline conﬁgurations tuning, since the choice of algorithms can make a huge diﬀerence in running
time. Therefore, fewer pipeline runs are not always “faster” in terms of wall-clock time. Also, in
practice, what we care about is the performance we can get within limited resource budget, rather

10

than within certain evaluation times. That is why we need cost-sensitive modeling for the pipeline
tuning problem.

Expected Improvement Per Second (EIPS) [41] proposes another acquisition function for tuning
of a single learning algorithm by dividing the EI of each hyperparameter by its runtime. To apply
EIPS in pipeline tuning problem, we use a separate linear model to model the total runtime of
pipeline paths. Similar to the linear model for error propagation, the linear model for time assumes
that on a pipeline path each algorithm partly contributes to the total time cost and the runtimes are
additive. To apply the linear model, we replace the performance metric m with the cost metric τ .
The linear cost model parametrized by βτ can be eﬃciently updated using Eq. (1). As described in
Algorithm 1, βτ will be updated together with β at the end of Phase 2. We note that, in practice,
the budget T and the cost τ (·) can be any quantitative costs of budgeted resources (e.g., money,
CPU time), which is a natural generalization of our idea.

With the cost model above, we get the cost-sensitive acquisition function over the best observed

result m+ at a new pipeline path p:

E[Im+(p)]
E[log τ (p)]

=

σp[uΦ(u) + φ(u)]
E[log τ (p)]

(4)

EIP S(p, P , β, βτ ) =

where u =

m+ − ξ − µp
σp

.

Here the dependency in β and βτ is captured during computation of µp, σp, and τ (p). We take
logarithm of cost τ (·) to compensate the large variations in the runtime of diﬀerent algorithms.
This acquisition function balances “doing well” and “doing fast” in selecting the next candidate
path to run. During the optimization, it will help avoid those costly paths with poor expected
improvement. More importantly, at the end of Phase 2 in Algorithm 1, EIPS is responsible to
determine the most promising paths, which perform better but cost less, to construct a subgraph
for the last phase ﬁne-tuning. For this purpose, we set the exploration parameter ξ to 0 to only
select (Line 13 in Algorithm 1).

3.4 Pipeline Caching

During the experiments, we note that many pipeline runs have overlapped algorithms in their paths.
Sometimes these algorithms have exactly the same pipeline path and the same hyperparameter
settings along the path. This means that we are wasting time on generating the same intermediate
output again and again. For example, consider the min-max normalization algorithm in the ﬁrst
pipeline step: this algorithm will be executed many times, especially when it performs well so that
Bayesian optimization methods prefer to choose it.

To reduce this overhead, we propose a pipeline caching algorithm, as described in Algorithm 3.
When running a pipeline, we check the cache before we run each algorithm. If it turns out to be a
cache hit, the result will be immediately returned from cache. Otherwise, we run the algorithm and
cache the result. There is a caching pool (e.g., disk space, memory usage) for this algorithm. We
use the Least Recently Used (LRU) strategy to clean up the caching pool when budget becomes
exhausted.

Caching can signiﬁcantly reduce the cost of pipeline runs, and accelerates all three phases of
FLASH. Algorithms closer to the pipeline input vertex, usually the data preprocessing steps, have
higher chance to hit the cache. In fact, when we deal with large datasets on real-world problems,
the preprocessing step can be quite time-consuming such that caching can be very eﬃcient.

11

Algorithm 3: Pipeline Caching

input : Data analytic pipeline G; input data D; pipeline conﬁguration p and λp to be run;

available caching budget Tcache; current cache pool C

1 D(1) ← D

/* Run pipeline with cache pool

2 for k ← 1, . . . , K do

3

4

5

6

7

8

h ← Hash(p(1) . . . p(k), λ(1)
if h ∈ C then

p . . . λ(k)
p )

D(k+1) ← cached result from C

else

D(k+1) ← RunAlgorithm(G, D(k), p(k), λ(k)
p )
C ← C ∪ {(cid:104)h, D(k+1)(cid:105)}

/* Clean up cache pool when necessary

9 if Tcache exhausted then
10

Discard least recently used (LRU) items in C

4 Benchmark Experiments

*/

*/

In this section, we perform extensive experiments on a number of benchmark datasets to evaluate
our algorithm compared to the existing approaches. Then we study the impact of diﬀerent algorithm
choices in each component of FLASH.

Benchmark Datasets We conduct experiments on a group of public benchmark datasets on
classiﬁcation task, including Madelon, MNIST, MRBI and Convex3. These prominent datasets
have been widely used to evaluate the eﬀectiveness of Bayesian optimization methods [44, 13, 5].
We follow the original train/test split of all the datasets. Test data will never be used during
the optimization: the once and only usage of test data is for oﬄine evaluations to determine the
In all benchmark experiments, we use
performance of optimized pipelines on unseen test set.
percent error rate as the evaluation metric.

Baseline Methods As discussed in Section 2, SMAC [22] and TPE [5] are the state-of-the-art
algorithms for Bayesian optimization [44, 13, 24], which are used as baselines. Note that Spearmint
[41], a Bayesian optimization algorithm based on Gaussian process is not applicable since it does
not provide a mechanism to handle the hierarchical space [12]. Besides SMAC and TPE, we also
choose random search as a simple baseline for sanity check. Thus, we compare both versions of
our method FLASH (with SMAC in Phase 3) and FLASH(cid:63) (with TPE in Phase 3) against three
baselines in the experiments.
Implementation: To avoid possible mistakes in implementing other methods, we choose a gen-
eral platform for hyperparameter optimization called HPOlib [12], which provides the original
In order to fairly compare our method
implementations of SMAC, TPE, and random search.

3The benchmark datasets are publicly available at http://www.cs.ubc.ca/labs/beta/Projects/autoweka/

datasets.

12

with others, we also implement our algorithm on top of HPOlib, and evaluate all the compared
methods on this platform. We make the source code of FLASH publicly available at https:
//github.com/yuyuz/FLASH.

Experimental Settings We build a general data analytic pipeline based on scikit-learn [36], a
popular used machine learning toolbox in Python. We follow the pipeline design of auto-sklearn
[13]. There are four computational steps in our pipeline: 1) feature rescaling, 2) sample balancing,
3) feature preprocessing, and 4) classiﬁcation model. Each step has various algorithms, and each
algorithm has its own hyperparameters. Adjacent steps are fully connected.
In total, our data
analytic pipeline contains 33 algorithms distributed in four steps, creating 1,456 possible pipeline
paths with 102 hyperparameters (30 categorical and 72 continuous), which creates complex high-
dimensional and highly conditional search space. Details and statistics of this pipeline are listed in
Table 5 in Appendix A.

In all experiments, we set a wall-clock time limit of 10 hours for the entire optimization, 15
minutes time limit and 10GB RAM limit for each pipeline run. We perform 10 independent
optimization runs with each baseline on each benchmark dataset. All experiments were run on
Linux machines with Intel Xeon E5-2630 v3 eight-core processors at 2.40GHz with 256GB RAM.
Since we ran experiments in parallel, to prevent potential competence in CPU resource, we use the
numactl utility to bound each independent run in single CPU core.

For our algorithm FLASH, we set both Tinit and Tprune as 30 iterations (equal to the number of
algorithms in the pipeline), which can be naturally generalized to other budgeted resources such as
wall-clock time or money. We set ξ to 100 in the EIPS function. Note that the performance are not
sensitive to the choices of those parameters. Finally, we set the number of pipeline paths r to 10
, which works well in generating a reasonable-size pruned pipeline G(cid:48). In benchmark experiments,
we compare the performance of FLASH without caching to other methods because the pipelines do
not have complex data preprocessing data like many real-world datasets have. We will use caching
for real-world experiments later in Section 5.

4.1 Results and Discussions

Table 2 reports the experimental results on benchmark datasets. For each dataset, we report the
performance achieved within three diﬀerent time budgets. As shown in the table, our methods
FLASH and FLASH(cid:63) perform signiﬁcantly better than other baselines consistently in all settings,
in terms of both lower error rate and faster convergence. For example, on the Madelon dataset,
our methods reach around 12% test error in only 3 hours, while other baselines are still far from
that even after 10 hours.

Performing statistical signiﬁcance test via bootstrapping, we ﬁnd that often FLASH and FLASH(cid:63)
tie with each other on these benchmark datasets. For all the methods, the test error is quite con-
sistent with the validation error, showing that the potential overﬁtting problem is well prevented
by using cross validation.

Figure 4 plots the convergence curves of median test error rate along with time for all baseline
methods. As shown in the ﬁgure, after running about 4 hours, FLASH and FLASH(cid:63) start to lead
others with steep drop of error rate, and then quickly converge on a superior performance.

13

Table 2: Performance on both 3-fold cross-validation and test data of benchmark datasets. For
each method, we perform 10 independent runs of 10 hours each. Results are reported as the
median percent error across the 10 runs within diﬀerent time budgets. Test data is never seen by
any optimization method, which is only used for oﬄine evaluations to compute test error rates.
Boldface indicates the best result within a block of comparable methods. We underline those results
not statistically signiﬁcantly diﬀerent from the best according to a 10,000 times bootstrap test with
p = 0.05.

Dataset

Budget
(hours)

Cross-validation Performance (%)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

Madelon

MNIST

MRBI

Convex

3
5
10

3
5
10

3
5
10

3
5
10

25.16
23.60
20.77

7.68
6.58
6.58

61.80
58.67
57.20

28.14
25.25
24.51

18.90
18.82
17.28

6.78
5.94
5.39

59.83
58.61
53.92

24.70
23.61
22.21

20.25
19.12
17.34

6.05
5.83
5.64

62.89
58.14
54.60

24.69
23.30
23.30

14.84
14.31
13.87

4.93
4.26
4.03

57.43
45.11
41.15

22.63
21.34
20.49

14.04
14.04
13.76

5.05
4.87
4.46

57.08
54.25
41.90

23.31
22.02
20.62

19.17
18.21
15.58

7.75
7.10
6.64

60.58
56.42
54.43

25.04
23.18
22.18

16.15
15.26
14.49

5.41
5.41
5.03

59.83
58.61
52.01

21.42
21.37
20.31

16.03
15.38
13.97

6.11
5.40
5.23

60.58
55.81
52.30

21.97
20.82
20.82

12.18
12.18
11.49

4.62
3.94
3.78

54.72
43.19
39.13

20.65
19.56
18.94

11.73
11.60
11.47

4.84
4.57
4.37

54.28
51.65
39.89

21.04
19.71
19.01

Figure 4: Performance of our methods (FLASH and FLASH(cid:63)) and other compared methods on
MRBI dataset. We show the median percent error rate on test set along with standard error bars
(generated by 10 independent runs) over time.

14

(a) The impact of optimal design
on MRBI dataset

(b) The impact of pipeline pruning
on MRBI dataset

(c) The impact of pipeline caching
on real-world dataset

Figure 5: Component analysis experiments. (a) Optimal design makes the initialization phase more
robust. (b) Pipeline pruning in the second phase of FLASH is the key to its superior performance.
(c) Performance of FLASH without caching and the original FLASH with caching on real-world
dataset. In all ﬁgures, we show the median error rate on test set along with standard error bars
(generated by 10 independent runs). Note that (a) and (b) are plotted with diﬀerent x-axes; (c) is
on a diﬀerent dataset as (a) and (b).

4.2 Detailed Study of FLASH Components

FLASH has three main components: optimal design for initialization, cost-sensitive model for
pipeline pruning, and pipeline caching. To study their individual contributions to the performance
gain, we drop out each of the component and compare the performance with original FLASH. Since
caching will be used for real-world experiments on large dataset, we describe the analysis of caching
component in Section 5. Here we use MRBI dataset for these experiments.

Figure 5(a) shows the diﬀerence between using random initialization and optimal design by
plotting the performance on initial 30 pipeline runs. The desirable property of optimal design en-
sures to run reasonable pipeline paths, giving FLASH a head start at the beginning of optimization.
While random initialization is not robust enough, especially when the number of pipeline runs is
very limited and some algorithms will have no chance to run due to the randomness. Figure 5(b)
shows the impact of pipeline pruning in the second phase of FLASH. Dropping out the pruning
phase with EIPS, and using SMAC immediately after Phase 1, we see a major degradation of the
performance. The ﬁgure clearly shows that in Phase 2 of FLASH, the linear model with EIPS
acquisition function is able to eﬃciently shrink the search space signiﬁcantly such that SMAC can
focus on those algorithms which perform well with little cost. This ﬁgure conﬁrms the main idea
of this paper that a simple linear model can be more eﬀective in searching high-dimensional and
highly conditional hyperparameter space.

5 Real-world Experiments

In this section, to demonstrate a real-world use case, we apply FLASH on a large de-identiﬁed
medical dataset for classifying drug non-responders. We show how our method can quickly ﬁnd
good classiﬁer for diﬀerentiating non-responders vs. responders.

15

Table 3: Statistics of the medical dataset.

#Patient #Event #Med #Class

train case
train control
test case
test control
total

18,581
18,582
4,646
4,646
46,455

982,025
622,777
245,776
153,303
2,003,881

434,171
286,198
108,702
70,395
899,466

547,854
336,579
137,074
82,908
1,104,415

Budget
(hours)

Test Performance (%)

Rand.
Search TPE SMAC FLASH FLASH(cid:63)

3
5
10

30.32
16.66
11.75

27.03
19.09
4.86

35.40
33.22
21.03

21.02
14.40
2.51

23.28
19.86
3.44

Table 4: Performance of real-world dataset. Results are reported using the same settings as Table 2.

Experimental Settings With a collaboration with a pharmaceutical company, we created a
balanced cohort of 46,455 patients from a large claim dataset. Patients who have at least 4 times of
treatment failure are regarded as drug non-responders (case group). Other patients are responders
of the drug (control group). The prediction target is whether a patient belongs to case group or
control group. Each patient is associated with a sequence of events, where each event is a tuple of
format (patient-id, event-id, timestamp, value). Table 3 summarizes the statistics of this
clinical dataset, including the count of patient, event, medication, and medication class.

Unlike benchmark experiments, the input to the real-world pipeline is not directly as feature
vectors. Given a cohort of patients with their event sequences, like [9] the pipeline for non-responder
classiﬁcation has two more additional steps than the pipeline described in previous benchmark
experiments: 1) Feature construction to convert patient event sequence data into numerical feature
vectors. This step can be quite time-consuming as advanced feature construction techniques like
sequential mining [29] and tensor factorization [19] can be expensive to compute. On this medical
i) frequency threshold to remove rare
dataset, we consider two kinds of parameters in this step:
events (frequency ranging from 2 to 5) ii) various aggregation functions (including binary, count,
sum and average) to aggregate multiple occurrence of events into features. The output of this step
will be a feature matrix and corresponding classiﬁcation targets; 2) Densify the feature matrix
from above feature construction step if necessary. Features of this real-world dataset can be quite
sparse. We by default use sparse representation to save space and accelerate computation in some
algorithms. Unfortunately, not all algorithm implementations in scikit-learn accept sparse features.
A decision has to be made here: either sparse matrix for faster result or dense matrix for broader
algorithm choices in later steps.

We run the experiments on same machine, use same parameter setting and same budget as
benchmark experiments. We compare our method with the same baselines as benchmark experi-
ments and we continue using error rate as metric. Our algorithm has built-in caching mechanism

16

and we will use that. For this real-world dataset, we ﬁrst compare with baselines with cache
enabled. Then we analyze the contribution of caching.

5.1 Results and Discussions

Table 4 shows the performance of our methods compared to baselines when caching is enabled. Due
to lack of space we only report the test performance. All cases FLASH and FLASH(cid:63) signiﬁcantly
outperform all the baselines.

Figure 5(c) shows the performance of FLASH without caching and original FLASH with caching
on the real-world medical dataset. With caching, more pipeline paths can be evaluated within
given period of time and our EIPS-based path selection leverages caching to select paths with high
performance that run fast. As a result, we can see FLASH with caching converges much faster.
For example, with caching we can get low test error within 6 hours.

6 Conclusions

In this work, we propose a two-layer Bayesian optimization algorithm named FLASH, which enables
highly eﬃcient optimization of complex data analytic pipelines. We showed that all components
of FLASH complement each other: 1) our optimal design strategy ensures better initialization,
giving a head start to the optimization procedure; 2) the cost-sensitive model takes advantage of
this head start, and signiﬁcantly improves the performance by pruning ineﬃcient pipeline paths;
3) the pipeline caching reduces the cost during the entire optimization, which provides a global
acceleration of our algorithm. We demonstrate that our method signiﬁcantly outperforms previous
state-of-the-art approaches in both benchmark and real-world experiments.

7 Acknowledgments

This work was supported by the National Science Foundation, award IIS- #1418511 and CCF-
#1533768, research partnership between Children’s Healthcare of Atlanta and the Georgia Institute
of Technology, CDC I-SMILE project, Google Faculty Award, Sutter health and UCB.

17

References

2012.

[1] A. Atkinson and A. Donev. Optimum experimental designs. 1992.

[2] T. B¨ack. Evolutionary algorithms in theory and practice. 1996.

[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 13(1),

[4] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures. In ICML, 2013.

[5] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl. Algorithms for hyper-parameter opti-

mization. In NIPS, 2011.

[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[7] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.

[8] G. Calinescu, C. Chekuri, M. P´al, and J. Vondr´ak. Maximizing a monotone submodular

function subject to a matroid constraint. SIAM Journal on Computing, 40(6), 2011.

[9] R. Chen, H. Su, Y. Zhen, M. Khalilia, D. Hirsch, M. Thompson, T. Davis, Y. Peng, S. Lin,
J. Tejedor-Sojo, E. Searles, and J. Sun. Cloud-based predictive modeling system and its
application to asthma readmission prediction. In AMIA. AMIA, 2015.

[10] S. J. Coakes and L. Steed. SPSS: Analysis without anguish using SPSS version 14.0 for

Windows. John Wiley & Sons, Inc., 2009.

[11] D. Donoho. 50 years of Data Science. Technical report, University of California Berkeley, 2015.

[12] K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown.
Towards an empirical foundation for assessing bayesian optimization of hyperparameters. In
NIPS workshop on Bayesian Optimization in Theory and Practice, 2013.

[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Eﬃcient and

robust automated machine learning. In NIPS, 2015.

[14] M. Feurer, T. Springenberg, and F. Hutter. Initializing bayesian hyperparameter optimization

via meta-learning. In AAAI, 2015.

[15] P. Flaherty, A. Arkin, and M. I. Jordan. Robust design of biological experiments. In NIPS,

2005.

[16] C. J. Flynn, C. M. Hurvich, and J. S. Simonoﬀ. Eﬃciency for regularization parameter selection

in penalized likelihood estimation of misspeciﬁed models. JASA, 108(503), 2013.

[17] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data

mining software: an update. ACM SIGKDD explorations newsletter, 11(1), 2009.

18

[18] X. He. Laplacian regularized d-optimal design for active learning and its application to image

retrieval. Image Processing, IEEE Transactions on, 19(1), 2010.

[19] J. C. Ho, J. Ghosh, and J. Sun. Marble: high-throughput phenotyping from electronic health

records via sparse nonnegative tensor factorization. In KDD, 2014.

[20] M. D. Hoﬀman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization.

In UAI. Citeseer, 2011.

[21] M. D. Hoﬀman, B. Shahriari, and N. de Freitas. On correlation and budget constraints in
In AIS-

model-based bandit optimization with application to automatic machine learning.
TATS, 2014.

[22] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general

algorithm conﬁguration. In Learning and Intelligent Optimization. Springer, 2011.

[23] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, 2013.

[24] B. Komer, J. Bergstra, and C. Eliasmith. Hyperoptsklearn: Automatic hyperparameter con-

ﬁguration for scikitlearn. In ICML workshop on AutoML, 2014.

[25] A. Krause and C. Guestrin. Submodularity and its applications in optimized information

gathering. TIST, 2(4), 2011.

[26] A. Kumar, R. McCann, J. Naughton, and J. M. Patel. Model selection management systems:

The next frontier of advanced analytics. ACM SIGMOD Record, 2015.

[27] D. J. Lizotte. Practical bayesian optimization. University of Alberta, 2008.

[28] J. Lv and J. S. Liu. Model selection principles in misspeciﬁed models. JRSS-B, 76(1), 2014.

[29] K. Malhotra, T. Hobson, S. Valkova, L. Pullum, and A. Ramanathan. Sequential pattern min-
ing of electronic healthcare reimbursement claims: Experiences and challenges in uncovering
how patients are treated by physicians. In Big Data, Oct 2015.

[30] X. Meng, J. Bradley, E. Sparks, and S. Venkataraman. Ml pipelines: a new high-level api for

mllib, 2015.

[31] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. Yale: Rapid prototyping for

complex data mining tasks. In KDD, 2006.

[32] J. Mockus, V. Tiesis, and A. Zilinskas. The application of bayesian methods for seeking the

extremum. Towards Global Optimization, 2(117-129), 1978.

[33] R. Munos. Optimistic optimization of deterministic functions without the knowledge of its

smoothness. In Advances in neural information processing systems, 2011.

[34] Y. Nesterov. Gradient methods for minimizing composite objective function, 2007.

[35] K. Ng, A. Ghoting, S. R. Steinhubl, W. F. Stewart, B. Malin, and J. Sun. Paramo: A parallel
predictive modeling platform for healthcare analytic research using electronic health records.
Journal of biomedical informatics, 48, 2014.

19

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. JMLR,
12, 2011.

[37] T. Robertazzi and S. Schwartz. An accelerated sequential algorithm for producing d-optimal

designs. SIAM Journal on Scientiﬁc and Statistical Computing, 10(2), 1989.

[38] B. Settles. Active Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning,

6(1), jun 2012.

[39] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of
the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.

[40] M. Shamaiah, S. Banerjee, and H. Vikalo. Greedy sensor selection: Leveraging submodularity.

In CDC, 2010.

algorithms. In NIPS, 2012.

[41] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning

[42] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Ali,
R. P. Adams, et al. Scalable bayesian optimization using deep neural networks. arXiv preprint
arXiv:1502.05700, 2015.

[43] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.

[44] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka: Combined selection

and hyperparameter optimization of classiﬁcation algorithms. In KDD, 2013.

[45] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global opti-
mization of expensive-to-evaluate functions. Journal of Global Optimization, 44(4), 2009.

[46] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high

dimensions via random embeddings. In IJCAI. Citeseer, 2013.

[47] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

[48] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 1982.

20

Table 5: Detailed information of the data analytic pipeline constructed for benchmark experiments.

Algorithm

#categorical #continuous

Total
#hyper-
parameters

A Appendix

Pipeline
step

Feature
rescaling

Sample
balancing

Feature
preprocessing

Classiﬁcation

0
0
0
0

0
0

2
3
2
1
0
0
0
1
1
1
0
1
1

1
1
2
0
0
2
1
0
2
1
1
0
2
4

Min-max scaler
None
Normalization
Standardization

Class weighting
None

Extremely randomized trees
Fast ICA
Feature agglomeration
Kernel PCA
Random kitchen sinks
Linear SVM
None
Nystroem sampler
PCA
Polynomial combinations
Random trees embedding
Percentile feature selection
Univariate feature selection

AdaBoost
Decision tree
Extremely randomized trees
Gaussian naive Bayes
Gradient boosting
K-nearest neighbors
LDA
Linear SVM
Kernel SVM
Multinomial naive Bayes
Passive aggressive
QDA
Random forest
Stochastic gradient descent

21

0
0
0
0

0
0

3
1
1
6
2
2
0
8
1
2
4
1
2

3
3
3
0
6
1
3
2
5
1
2
1
3
6

0
0
0
0

0
0

5
4
3
7
2
2
0
9
2
3
4
2
3

4
4
5
0
6
3
4
2
7
2
3
1
5
10

Total#

33

30

72

102

