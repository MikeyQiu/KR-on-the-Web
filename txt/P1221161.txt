How Images Inspire Poems: Generating Classical Chinese Poetry from Images
with Memory Networks

Linli Xu†, Liang Jiang†, Chuan Qin†, Zhe Wang‡, Dongfang Du†
†Anhui Province Key Laboratory of Big Data Analysis and Application,
School of Computer Science and Technology, University of Science and Technology of China
‡AI Department, Ant Financial Services Group
linlixu@ustc.edu.cn, jal@mail.ustc.edu.cn, chuanqin0426@gmail.com
wz143459@antﬁn.com, dfdu@mail.ustc.edu.cn

8
1
0
2
 
r
a

M
 
8
 
 
]
L
C
.
s
c
[
 
 
1
v
4
9
9
2
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

With the recent advances of neural models and natural lan-
guage processing, automatic generation of classical Chinese
poetry has drawn signiﬁcant attention due to its artistic and
cultural value. Previous works mainly focus on generating
poetry given keywords or other text information, while visual
inspirations for poetry have been rarely explored. Generating
poetry from images is much more challenging than gener-
ating poetry from text, since images contain very rich visual
information which cannot be described completely using sev-
eral keywords, and a good poem should convey the image
accurately. In this paper, we propose a memory based neu-
ral model which exploits images to generate poems. Specif-
ically, an Encoder-Decoder model with a topic memory net-
work is proposed to generate classical Chinese poetry from
images. To the best of our knowledge, this is the ﬁrst work
attempting to generate classical Chinese poetry from images
with neural networks. A comprehensive experimental inves-
tigation with both human evaluation and quantitative analy-
sis demonstrates that the proposed model can generate poems
which convey images accurately.

Introduction
Classical Chinese poetry is a priceless and important her-
itage in Chinese culture. During the history of more than
2000 years in China, millions of classical Chinese po-
ems have been written to praise heroic characters, beautiful
scenery, love, etc. Classical Chinese poetry is still fascinat-
ing us today with its concise structure, rhythmic beauty and
rich emotions. There are distinct genres of classical Chinese
poetry, including Tang poetry, Song iambics and Qing po-
etry, etc., each of which has different structures and rules.
Among them, quatrain is the most popular one, which con-
sists of four lines, with ﬁve or seven characters in each line.
The lines of a quatrain follow speciﬁc rules including the
regulated rhythmical pattern, where the last characters in the
ﬁrst (optional), second and fourth line must belong to the
same rhythm category. In addition, each Chinese character is
associated with one tone which is either Ping (the level tone)
or Ze (the downward tone), and quatrains are required to fol-
low a pre-deﬁned tonal pattern which regulates the tones of
characters at various positions (Wang 2002). An example of

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Table 1: An example of a 7-char quatrain. The tone of each
character is shown at the end of each line, where “P” repre-
sents Ping (the level tone), “Z” represents Ze (the downward
tone), and “*” indicates the tone can be either. Rhyming
characters are underlined.

a quatrain written by a very famous classical Chinese poet
Li Bai is shown in Table 1.

The stringent rhyme and tone regulations of classical Chi-
nese poetry pose a major challenge for generating Chi-
nese poems automatically. In recent years, various attempts
have been made on automatic generation of classical Chi-
nese poems from text information such as keywords. Among
them, rule-based approaches (Tosa, Obara, and Minoh 2008;
Wu, Tosa, and Nakatsu 2009; Netzer et al. 2009; Oliveira
2009; 2012), genetic algorithms (Manurung 2004; Zhou,
You, and Ding 2010; Manurung, Ritchie, and Thompson
2012) and statistical machine translation methods (Jiang and
Zhou 2008; He, Zhou, and Jiang 2012) have been developed.
More recently, with the signiﬁcant advances in deep neural
networks, a number of poetry generation algorithms based
on neural networks have been proposed with the paradigm of
sequence-to-sequence learning, where poems are generated
line by line and each line is generated by taking the previous
lines as input (Zhang and Lapata 2014; Yi, Li, and Sun 2016;
Wang et al. 2016a; 2016b; Zhang et al. 2017). However, re-
strictions exist in previous works, including topic drift and
semantic inconsistency which are caused by only consider-
ing the writing intent of a user in the ﬁrst line. In addition,
a limited number of keywords with a proper order is usually
required (Wang et al. 2016b), which limits the ﬂexibility of

the process.

On the other hand, visual inspirations are more natural
and intuitive than text for poem writing. People can write
poems to express their aesthetic appreciation or sentimental
reﬂections at the sight of absorbing scenery, such as mag-
niﬁcent mountains and fast rivers. As a consequence, there
usually exists a correspondence between a poem and an im-
age explicitly or implicitly, where a poem either describes a
scene, or leaves visual impressions on the readers. For ex-
ample, the poem shown in Table 1 represents an image with
a magniﬁcent waterfall falling down from a very high moun-
tain. Because of this inherent correlation, generating classi-
cal Chinese poems from images becomes an interesting re-
search topic.

As far as we know, visual inspirations from images have
been rarely explored in automatic generation of classical
Chinese poetry. The task of generating poetry from images
is much more challenging than generating poetry from key-
words in general, since very rich visual information is con-
tained in an image, which requires sophisticated representa-
tion as a bridge to convey the essential visual features and
semantic concepts of the image to the poem generator. In
addition, to generate a poem that is consistent with the im-
age and coherent itself, the topic ﬂow should be carefully
manipulated in the generated sequence of characters.

In this paper, we propose an Encoder-Decoder framework
with topic memory to generate classical Chinese poetry from
images, which integrates direct visual information with se-
mantic topic information in the form of keywords extracted
from images. We consider poetry generation as a sequence-
to-sequence learning problem, where a poem is generated
line by line, and each line is generated by taking all previ-
ous lines into account. Moreover, we introduce a memory
network to support unlimited number of keywords extracted
from images and to determine a latent topic for each charac-
ter in the generated poem dynamically. This addresses the is-
sues of topic drift and semantic inconsistency, while resolv-
ing the restriction of requiring a limited number of keywords
with a proper order in previous works. Meanwhile, to lever-
age the visual information of images not contained in the se-
mantic concepts, we integrate direct visual information into
the proposed Encoder-Decoder framework to ensure the cor-
respondence between images and poems. The experimental
results demonstrate that our model has great advantages in
exploiting the visual and topic information in images, and it
can generate poetry with high quality which conveys images
accurately and consistently.
The main contributions of this paper are:

1. We consider a new research topic of generating classical
Chinese poetry from images, which is not only for enter-
tainment or education, but also an exploration integrating
natural language processing and computer vision.

2. We employ memory networks to tackle the problems of
topic drift and semantic inconsistency, while resolving the
restriction of requiring a limited number of keywords with
a proper order in previous works.

3. We integrate keywords summarizing semantic topics and
direct visual information to capture the information con-

veyed in images when generating poems.

Related Work
Poetry generation has been a challenging task over the past
decades. A variety of approaches have been proposed, most
of which focus on generating poetry from text. Among
them, phrase search based approaches are proposed in (Tosa,
Obara, and Minoh 2008; Wu, Tosa, and Nakatsu 2009) for
Japanese poetry generation. Semantic and grammar tem-
plates are used in (Oliveira 2009), while genetic algorithms
are employed in (Manurung 2004; Manurung, Ritchie, and
Thompson 2012; Zhou, You, and Ding 2010). In the work
of
(Jiang and Zhou 2008; Zhou, You, and Ding 2010;
He, Zhou, and Jiang 2012), poetry generation is treated as a
statistical machine translation problem, where the next line
is generated by translating the previous line. Another ap-
proach in (Yan et al. 2013) generates poetry by summarizing
users’ queries.

More recently, deep neural networks have been applied
in automatic poetry generation. An RNN-based framework
is proposed in (Zhang and Lapata 2014) where each poem
line is generated by taking the previously generated lines as
input. In the work of (Yi, Li, and Sun 2016), the attention
mechanism is introduced into poetry generation, where an
attention-based Encoder-Decoder model is proposed to gen-
erate poem lines sequentially. A different genre of classical
Chinese poetry is generated in (Wang et al. 2016a), which is
the ﬁrst work to generate Chinese Song iambics, with each
line of variable length. However, the neural methods intro-
duced above share the limitation that the writing intent of a
user is taken into consideration only in the ﬁrst line, which
will cause topic drift in the following lines. To address that,
a modiﬁed attention-based Encoder-Decoder model is pre-
sented in (Wang et al. 2016b) which assigns a sub-topic key-
word for each poem line. As a consequence, the number of
keywords is ﬁxed to the number of poem lines, and the key-
words have to be sorted in a proper order manually, which
limits the ﬂexibility of the approach.

In this work, we address the issues in the previous works
by introducing a memory network with unlimited capac-
ity of keywords which can dynamically determine a topic
for each character. Memory networks are a class of neu-
ral networks proposed in (Weston, Chopra, and Bordes
2014) to augment recurrent neural networks with a mem-
ory component to model long term memory. In the work
of (Sukhbaatar et al. 2015), memory networks are extended
with an end-to-end training mechanism, which makes the
model more generally applicable. Zhang et al. (2017) intro-
duces memory networks into automatic poetry generation
for the ﬁrst time to leverage the prior knowledges in the ex-
isting poems while writing new poems. In the testing stage,
by using a memory network, some existing poems are repre-
sented as external memory which provides prior knowledge
for new poems. In contrast to (Zhang et al. 2017), we em-
ploy memory networks in a completely different way, where
we represent keywords as memory entities, such that we can
handle keywords without restrictions and dynamically deter-
mine the topic for each character.

Figure 1: An illustration of the pipeline to generate a poem from an image using Memory-Based Image to Poem Generator.

Memory-Based Image to Poem Generator

Images convey rich information from visual signals to se-
mantic topics that can inspire good poems. To build a natural
correspondence between an image and a poem, we propose a
framework which integrates the semantic keywords summa-
rizing the important topics of an image, along with the direct
visual information to generate a poem. Speciﬁcally, given an
image, keywords containing semantic topics are extracted as
the outline when generating the poem, while visual informa-
tion is exploited to embody the information not conveyed by
the keywords.

Framework

Given an image I, we are generating a poem P which con-
sists of L poem lines {l1, l2, .., lL}. In the framework as
illustrated in Figure 1, we ﬁrst extract a set of keywords
K = {k1, k2, ..., kN } and a set of visual feature vectors
V = {v1, v2, ..., vB} from I with keyword extractor and
a convolutional neural network (CNN) based visual feature
extractor respectively. The poem is then generated line by
line. Speciﬁcally, when generating the i-th line li, the pre-
viously generated lines denoted by l1:i−1, which is the con-
catenation from l1 to li−1, the keywords K and visual fea-
ture vectors V are jointly exploited in the Memory-based
Image to Poem Generator (MIPG) model, which is the key
component in the framework.

As illustrated in Figure 2, the MIPG model is essentially
an Encoder-Decoder model consisting of two modules: an
Image-based Encoder (I-Enc) and a Memory-based Decoder
(M-Dec). In I-Enc, visual features V are extracted from the
image I with a CNN, while a bi-directional Gated Recurrent
Unit (Bi-GRU) model (Cho et al. 2014) is employed to build
semantic features H from previously generated lines l1:i−1.
In M-Dec, the i-th poem line, denoted by a sequence of char-
acters {y1, ..., yG}, is generated based on the keywords K,
the visual feature vectors V , as well as the semantic fea-
tures H from the previous lines. To generate each character
yt ∈ li, we ﬁrst transform V and H into dynamic represen-
tations for yt with the attention mechanism, followed by a
Topic Memory Network designed to dynamically determine
a topic for yt. Finally yt is predicted using a topic-bias prob-
ability which enhances the consistency between the image
and poem.

Image-based Encoder (I-Enc)
In I-Enc, as illustrated in the lower half of Figure 2, we
ﬁrst encode the image I into B local visual features vectors
V = {v1, v2, ..., vB}, each of which is a Dv-dimensional
representation corresponding to a different part of the im-
age. We use a CNN model as the visual feature extractor.
Speciﬁcally, V is produced by fetching the output of a cer-
tain convolutional layer of the CNN feature extractor which
takes I as input

V = CNN(I).

Meanwhile, we use a Bi-GRU to encode the preceding lines
of the generated poem l1:i−1, which takes the form of a se-
quence of the character embeddings {x1, x2, ..., xC} into
the corresponding hidden vectors H = [h1, h2, ..., hC],
where C denotes the length of l1:i−1 and hj is the concatena-
−→
h j and backward hidden
tion of the forward hidden vector
vector

←−
h j at the j-th step in the Bi-GRU. That is,

−→
h j = GRU(
←−
h j = GRU(
−→
h j;

hj = [

−→
h j−1, xj),
←−
h j+1, xj),

←−
h j].

The visual features V and semantic features H extracted in
I-Enc are then exploited in M-Dec to dynamically determine
which parts of the image and what content in the preceding
lines it should focus on when generating each character.

Memory-based Decoder (M-Dec)
In M-Dec, as illustrated in the upper half of Figure 2, each
line li = {y1, ..., yG} is generated character by character.
Speciﬁcally, at the t-th step, we use another GRU which
maintains an internal state st to predict yt. st is updated
based on st−1, yt−1, H and V recurrently, and can be for-
mulated as

st = f (st−1, yt−1, ˆht, ˆvt),
ˆht = attH (H),
ˆvt = attV (V ),
where f denotes the function to update the internal state in
GRU, attH and attV denote the functions of attention which
transform H and V into dynamic representations ˆht and ˆvt
respectively, indicating which parts of the image and what

Figure 2: An illustration of the Memory-based Image to Poem Generator (MIPG).

C
(cid:88)

j=1

B
(cid:88)

j=1

content in the preceding lines the model should focus on
when generating the next character. More formally,

ˆht = attH (H) =

αtjhj,

hj ∈ H,

where αtj is the weight of the j-th character computed by
the attention model:

αtj =

exp(atj)
C
(cid:80)
n=1

exp(atn)

,

atn = uT

a tanh(Wast−1 + Uahn),

in which atj is the attention score on hj at the t-th step. ua,
Wa and Ua are the parameters to learn.

Similarly, ˆvt is computed using attV taking V as input:

ˆvt = attV (V ) =

βtjvj,

vj ∈ V,

βtj =

exp(btj)
B
(cid:80)
n=1

exp(btn)

,

btn = uT

b tanh(Wbst−1 + Ubvn).

Instead of predicting the t-th character yt by using st di-
rectly, we introduce a Topic Memory Network to dynam-
ically determine a proper topic for yt by taking st as input
and output a topic-aware state vector ot which contains not
only information in the image and the preceding lines, but
also the latent topic to generate yt. A multi-layer perceptron
is then used to predict yt from ot.

Topic Memory Network. Considering the rich informa-
tion conveyed by images, it is essentially impossible to fully
describe an image with too few keywords. In the meantime,
the problem of topic drift would weaken the consistency be-
tween a pair of image and poem. To address these issues,

as illustrated in Figure 3, we use a Topic Memory Network,
in which each memory entity is a keyword extracted from
an image, to dynamically determine a latent topic for each
character by taking all keywords extracted from the image
into consideration.

To achieve that, we use the general-v1.3 model provided
by Clarifai1 to extract a set of keywords K = {k1, ..., kN },
and encode each keyword kj in K into two memory vectors:
an input memory vector qj which is used to calculate the
importance of kj on yt, and an output memory vector mj
which contains the semantic information of kj.

Speciﬁcally, for each keyword kj ∈ K with Cj char-
acters, we encode kj into a semantic vector. To take the
order of Cj characters into account, we use a Bi-GRU to
encode kj into a sequence of forward hidden state vectors
[−→q1 , ..., −→qCj ] and a sequence of backward hidden state vec-
tors [←−q1 , ..., ←−qCj ]. Then the input memory vector qj is com-
puted by concatenating the last forward state −→qCj and the
ﬁrst backward state ←−q1 , that is, qj = [−→qCj ; ←−q1 ].

The output memory representation mj of each keyword
kj is computed by the mean value of the embedding vectors
of all characters in kj,

mj =

ejn,

1
Cj

Cj
(cid:88)

n=1

where ejn represents the word embedding of the n-th char-
acter in kj that is learned during training.

With the hidden state vector at the t-th step st, we com-
pute the importance zj of each keyword kj ∈ K based on
the similarity between st and the input memory representa-
tion qj of the keyword. This can be formulated as follows:

zj = softmax(sT

t qj),

1 ≤ j ≤ N.

Given the weights of the keywords [z1, ..., zN ], a latent topic
vector dt for yt is calculated by the weighted sum of the

1https://clarifai.com

Experiments

Dataset

In this paper, we are interested in the task of generating qua-
trains given images. To investigate the performance of our
proposed framework on this task, a dataset of image-poem
pairs needs to be constructed where images and poems are
matched. For the convenience of data preparation, we focus
on generating quatrains with 4 lines and 7 characters in each
line. The framework proposed in this paper can be easily
generalized to generate other types of poetry.

To construct the dataset of image-poem pairs, we col-
lect 68,715 images from the Internet and use the poem
dataset provided in (Zhang and Lapata 2014), which con-
tains 65,559 7-character quatrains. Given the large numbers
of images and poems, it is impractical to match them manu-
ally. Instead, we exploit the key concepts of the images and
poems and match them automatically. Speciﬁcally, for each
image, we obtain several keywords (e.g., water, tree) with
the general-v1.3 model provided by Clarifai; similarly we
extract key concepts from each line of poems. Images and
poem lines with common concepts can then be matched. In
this way, a dataset with 2,311,359 samples is constructed,
each of which consists of an image, the preceding poem lines
and the next poem line. We randomly select 50,000 samples
for validation, 1,000 for testing and the rest for training.

Training Details

We use 6,000 most frequently used characters as the vocab-
ulary EG. The number of recurrent hidden units is set to 512
for both encoder and decoder. The dimensions of the input
and output memory in the topic memory network are also
both set to 512. The hyperparameter λ to balance PT and
PG is set to 0.5 which is tuned on the validation set from
λ = 0.0, 0.1, ..., 1.0. All parameters are randomly initialized
from a uniform distribution with support from [-0.08, 0.08].
The model is trained with the AdaDelta algorithm (Zeiler
2012) with the batch size set to 128, and the ﬁnal model is
selected according to the cross entropy loss on the valida-
tion set. During training, we invert each line to be generated
following (Yi, Li, and Sun 2016) to make it easier for the
model to generate poems obeying rhythm rules. For the vi-
sual feature extractor, we choose a pre-trained VGG-19 (Si-
monyan and Zisserman 2014) model and use the output of
the conv5 4 layer, which includes 196 vectors of 512 dimen-
sions, as the local visual features of an image. For the ma-
jority of images, 10-20 keywords are extracted.

Evaluation Metrics

For the general task of text generation in natural language
processing, there exist various metrics for evaluation includ-
ing BLEU and ROUGE. However, it has been shown that the
overlap-based automatic evaluation metrics have little cor-
relation with human evaluation (Liu et al. 2016). Thus, for
automatic evaluation, we only calculate the recall rate of the
key concepts in an image which are described in the gener-
ated poem to evaluate whether our model can generate con-
sistent poems given images. Considering the uniqueness of

Figure 3: An illustration of the Topic Memory Network.

output memory representations of keywords [m1, ..., mN ]

dt =

zjmj.

N
(cid:88)

j=1

Based on dt and st, we compute a topic-aware state vector
ot = dt + st to integrate the latent topics, visual features
and semantic information from the previously generated
characters when predicting yt.

Finally, the t-th character yt is predicted based on ot,
ˆvt, ˆht and yt−1. Moreover,
to enhance the topic con-
sistency between poems and images, we encourage the
decoder to generate characters that appear in the keywords
extracted from the images by adding a bias probability
to the characters in K. Speciﬁcally, we deﬁne a generic
vocabulary EG which contains all possible characters, and
a topic vocabulary ET which contains all characters in
K, satisfying ET ⊆ EG. To predict yt, we calculate the
topic character probability pT (yt) in addition to the generic
character probability pG(yt) by

pG(yt = w) = gG(ot, ˆvt, ˆht),
(cid:40)

w ∈ EG,
gT (ot, ˆvt, ˆht), w ∈ ET ,
0,

w ∈ EG \ ET ,

pT (yt = w) =

where gT and gG represent functions corresponding to mul-
tilayer perceptrons followed by softmax, to compute pT and
pG respectively. pT and pG are summed to compute the char-
acter probability p, and the character with the maximum
probability is picked as the next character:

p(yt = w) = λpT (yt = w) + pG(yt = w),
p(yt = w), w ∈ EG,

yt = arg max

w

where λ is a hyperparameter to balance the generic proba-
bility pG and the topic bias probability pT .

Table 2: Human evaluation of all models. Bold values indicate the best performance.

Models
SMT
RNNPG-A
RNNPG-H
PPG-R
PPG-H
MIPG (full)
MIPG (w/o keywords)
MIPG (w/o visual)

Poeticness
6.97
7.27
7.36
6.47
6.52
8.30
7.21
7.05

Fluency Coherence Meaning Consistency Average
5.22
5.91
5.67
4.82
5.09
7.16
6.15
5.21

5.18
5.95
5.51
4.91
5.24
7.07
6.13
4.76

5.15
5.09
5.41
4.96
5.48
6.70
4.10
3.98

5.67
6.17
6.03
5.29
5.58
7.38
6.08
5.16

5.85
6.62
6.20
5.27
5.57
7.69
6.78
4.81

the poem generation task in terms of text structure and liter-
ary creation, we evaluate the quality of the generated poems
with a human study.

Following (Wang et al. 2016b), we use the metrics listed

below to evaluate the quality of a generated poem:

• Poeticness. Does the poem follow the rhyme and tone

regulations?

• Fluency. Does the poem read smoothly and ﬂuently?

• Coherence. Is the poem coherent across lines?

• Meaning. Does the poem have a reasonable meaning and

artistic conception?

In addition to these metrics, for our task of generating
Chinese classical poems given images, we need to evaluate
how well the generated poem conveys the input image. Here
we introduce a metric Consistency to measure whether the
topics of the generated poem and the given image match.

Model Variants
In addition to the proposed framework of MIPG, we evaluate
two variants of the model to examine the inﬂuence of the
visual and semantic topic information on the quality of the
poems generated:

• MIPG (full). The proposed model, which integrates di-
rect visual information and semantic topic information.

• MIPG (w/o keywords). Based on MIPG, the semantic
topic information is removed by setting the input and out-
put memory vectors of keywords to (cid:126)0, such that the model
only leverages visual information in poetry generation.

• MIPG (w/o visual). Based on MIPG, the visual informa-
tion is removed by setting the visual feature vectors of
image to (cid:126)0, such that the model only leverages semantic
topic information in poetry generation.

Baselines
As far as we know, there is no prior work on generating Chi-
nese classical poetry from images. Therefore, for the base-
lines we implement several previously proposed keyword-
based methods listed below:

SMT. A statistical machine translation model (He, Zhou,
and Jiang 2012), which translates the preceding lines to gen-
erate the next line, and the ﬁrst line is generated from input
keywords with a template-based method.

Models

Table 3: Recall rate of the key concepts in an image de-
scribed in the poems generated by all the models.
Recall
Recall Models
RNNPG-A 12.85% PPG-R
33.5%
RNNPG-H 11.82% PPG-H 33.7%
58.8%

19.79% MIPG

SMT

RNNPG. A poetry generation model based on recurrent
neural networks (Zhang and Lapata 2014), where the ﬁrst
line is generated with a template-based method taking key-
words as input and the other three lines are generated se-
quentially. In our implementation, two schemes are used to
select the keywords for the ﬁrst line, including using all the
keywords (RNNPG-A) and using the important keywords
selected by human (RNNPG-H) respectively. Speciﬁcally,
for RNNPG-A, we use all the keywords extracted from an
image (10-20 keywords), while for RNNPG-H, we invite 3
volunteers to vote for the most important keywords in the
image (3-6 keywords).

PPG. An attention-based encoder-decoder framework
with a sub-topic keyword assigned to each line (Wang et al.
2016b). Since PPG requires that the number of keywords to
be equal to the number of lines (4 for quatrains), we consider
two schemes to select 4 keywords from the keyword set ex-
tracted from the image, including random selection (PPG-R)
and human selection (PPG-H). Speciﬁcally, for PPG-R, we
randomly select 4 keywords from the keyword set and ran-
domly sort them. For PPG-H, we invite 3 volunteers to vote
for the 4 most important keywords in the image and sort
them in the order of relevance.

Human Evaluation
We invite eighteen volunteers, who are knowledgeable in
classical Chinese poetry from reading to writing, to evaluate
the results of various methods. 45 images are randomly sam-
pled as our testing set. Volunteers rate every generated poem
with a score from 1 to 10 from 5 aspects: Poeticness, Flu-
ency, Coherence, Meaning and Consistency. Table 2 sum-
marizes the results.

Overall Performance. The results in Table 2 indicate that
the proposed model MIPG outperforms the baselines with
all metrics. It is worth mentioning that in terms of “Con-
sistency” which measures how well the generated poem

Table 4: Two sample poems generated from the corresponding image by MIPG.

can describe the given image, MIPG achieves the best per-
formance, demonstrating the effectiveness of the proposed
model at capturing the visual information and semantic topic
information in the generated poems. From the comparison
of RNNPG-A and RNNPG-H, one can notice that, by pick-
ing important keywords manually, the poems generated by
RNNPG-H are more consistent with images, which implies
the importance of keyword selection in poetry generation.
Similarly, with important keywords selected manually, PPG-
H outperforms PPG-R from all aspects, especially in “Co-
herence” and “Consistency”. As a comparison, in the pro-
posed MIPG framework, the topic memory network makes
it possible to dynamically determine a topic while gener-
ating each character, in the meantime, the encoder-decoder
framework ensures the generation of ﬂuent poems follow-
ing the regulations. As a whole, the visual information and
the topic memory network work together to generate poems
consistent with given images.

Analysis of Model Variants. The results in the bottom
rows of Table 2 correspond to the model variants includ-
ing MIPG (full), MIPG (w/o keywords) and MIPG (w/o vi-
sual). One can observe that ignoring semantic keywords in
MIPG (w/o keywords) or visual information in MIPG (w/o
visual) degrades the performance of the proposed model sig-
niﬁcantly, especially in terms of “Consistency”. This pro-
vides clear evidence justifying that the visual information
and semantic topic information work together to generate
poems consistent with images.

Automatic Evaluation of Image-Poem Consistency
Different from the keyword-based poetry generation models,
for the task of poetry generation from images, the image-
poem consistency is a new and very important metric while
evaluating the quality of the generated poems. Therefore, we
conduct an automatic evaluation in terms of “Consistency”
in addition to human evaluation, which is achieved by com-

puting the recall rate of the key concepts in an image that are
described in the generated poem.

The results are shown in Table 3, where one can ob-
serve that the proposed MIPG framework outperforms all
the other baselines with a large margin, which indicates
that the poems generated by MIPG can better describe the
given images. One should also notice the difference between
the subjective and quantitative evaluation in “Consistency”
from Table 2 and Table 3. For instance, PPG-R and PPG-H
achieve almost equal recall rates of keywords, but the “Con-
sistency” score of PPG-R is signiﬁcantly lower than PPG-H
in Table 2. This is reasonable considering that both PPG-R
and PPG-H select 4 keywords extracted from the given im-
age to generate poems, which yield equal recall rates. In the
meantime, the keywords selected by human describe the im-
age better than randomly picked keywords, therefore PPG-H
can generate poems more consistent with images than PPG-
R from the human perspective. In addition, the reason that
the keyword recall rates of RNNPG-A and RNNPG-H are
relatively low is probably due to the fact that RNNPG often
generates the ﬁrst poem line which is semantically relevant
with given keywords while not containing any of them. As a
consequence, RNNPG-A and RNNPG-H may generate po-
ems with low keyword recall rates but high “Consistency”
scores.

Examples

To further illustrate the quality of the poems generated by the
proposed MIPG framework, we include two examples of the
poems generated with the corresponding images in Table 4 .
As shown in these examples, the poems generated by MIPG
can capture the visual information and semantic concepts in
the given images. More importantly, the poems nicely de-
scribe the the images in a poetic manner, while following
the strict regulations of classical Chinese poetry.

Oliveira, H. G. 2012. Poetryme: a versatile platform for
poetry generation. Computational Creativity, Concept In-
vention, and General Intelligence 1:21.
Simonyan, K., and Zisserman, A. 2014. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556.
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-
end memory networks. In Advances in neural information
processing systems, 2440–2448.
Tosa, N.; Obara, H.; and Minoh, M. 2008. Hitch haiku: An
interactive supporting system for composing haiku poem.
In International Conference on Entertainment Computing,
209–216. Springer.
Wang, Q.; Luo, T.; Wang, D.; and Xing, C. 2016a. Chinese
song iambics generation with neural attention-based model.
arXiv preprint arXiv:1604.06274.
Wang, Z.; He, W.; Wu, H.; Wu, H.; Li, W.; Wang, H.; and
Chen, E. 2016b. Chinese poetry generation with planning
based neural network. arXiv preprint arXiv:1610.09889.
Wang, L. 2002. A summary of rhyming constraints of chi-
nese poems.
Weston, J.; Chopra, S.; and Bordes, A. 2014. Memory net-
works. arXiv preprint arXiv:1410.3916.
2009. New hitch
Wu, X.; Tosa, N.; and Nakatsu, R.
haiku: An interactive renku poem composition supporting
tool applied for sightseeing navigation system. In Interna-
tional Conference on Entertainment Computing, 191–196.
Springer.
Yan, R.; Jiang, H.; Lapata, M.; Lin, S.-D.; Lv, X.; and Li,
X. 2013.
i, poet: Automatic chinese poetry composition
through a generative summarization framework under con-
strained optimization. In IJCAI.
Yi, X.; Li, R.; and Sun, M. 2016. Generating chinese
classical poems with rnn encoder-decoder. arXiv preprint
arXiv:1604.01537.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701.
Zhang, X., and Lapata, M. 2014. Chinese poetry generation
with recurrent neural networks. In EMNLP, 670–680.
Zhang, J.; Feng, Y.; Wang, D.; Wang, Y.; Abel, A.; Zhang,
S.; and Zhang, A. 2017. Flexible and creative chinese
arXiv preprint
poetry generation using neural memory.
arXiv:1705.03773.
Zhou, C.-L.; You, W.; and Ding, X. 2010. Genetic algorithm
and its implementation of automatic generation of chinese
songci. Journal of Software 21(3):427–437.

Conclusion
In this paper, we propose a memory based neural network
model for classical Chinese poetry generation from images
(MIPG) where visual features as well as semantic topics of
images are exploited when generating poems. Given an im-
age, semantic keywords are extracted as the skeleton of a
poem, where a topic memory network is proposed that can
take as many keywords as possible and dynamically select
the most relevant ones to use during poem generation. On
this basis, visual features are integrated to embody the in-
formation missing in the keywords. Numerical and human
evaluation regarding the quality of the poems from different
perspectives justiﬁes that the proposed model can generate
poems that describe the given images accurately in a poetic
manner, while following the strict regulations of classical
Chinese poetry.

Acknowledgements
This research was supported by the National Natural Sci-
ence Foundation of China (No. 61375060, No. 61673364,
No. 61727809 and No. 61325010), and the Fundamental Re-
search Funds for the Central Universities (WK2150110008).
We also gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Titan X GPU used for
this work.

References
Cho, K.; Van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning
phrase representations using rnn encoder-decoder for statis-
tical machine translation. arXiv preprint arXiv:1406.1078.
He, J.; Zhou, M.; and Jiang, L. 2012. Generating chinese
classical poems with statistical machine translation models.
In AAAI.
Jiang, L., and Zhou, M. 2008. Generating chinese couplets
using a statistical mt approach. In Proceedings of the 22nd
International Conference on Computational Linguistics-
Volume 1, 377–384. Association for Computational Linguis-
tics.
Liu, C.-W.; Lowe, R.; Serban, I. V.; Noseworthy, M.; Char-
lin, L.; and Pineau, J. 2016. How not to evaluate your di-
alogue system: An empirical study of unsupervised evalua-
tion metrics for dialogue response generation. arXiv preprint
arXiv:1603.08023.
Manurung, R.; Ritchie, G.; and Thompson, H. 2012. Using
genetic algorithms to create meaningful poetic text. Jour-
nal of Experimental & Theoretical Artiﬁcial Intelligence
24(1):43–64.
Manurung, H. 2004. An evolutionary algorithm approach to
poetry generation.
Netzer, Y.; Gabay, D.; Goldberg, Y.; and Elhadad, M. 2009.
Gaiku: Generating haiku with word associations norms. In
Proceedings of the Workshop on Computational Approaches
to Linguistic Creativity, 32–39. Association for Computa-
tional Linguistics.
Oliveira, H. 2009. Automatic generation of poetry: an
overview. Universidade de Coimbra.

