Structure Discovery in Nonparametric Regression through
Compositional Kernel Search

3
1
0
2
 
y
a
M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
2
9
4
.
2
0
3
1
:
v
i
X
r
a

David Duvenaud∗†
James Robert Lloyd∗†
Roger Grosse‡
Joshua B. Tenenbaum‡
Zoubin Ghahramani†

Abstract
Despite its importance, choosing the struc-
tural form of the kernel
in nonparametric
regression remains a black art. We deﬁne
a space of kernel structures which are built
compositionally by adding and multiplying a
small number of base kernels. We present a
method for searching over this space of struc-
tures which mirrors the scientiﬁc discovery
process. The learned structures can often
decompose functions into interpretable com-
ponents and enable long-range extrapolation
on time-series datasets. Our structure search
method outperforms many widely used ker-
nels and kernel combination methods on a
variety of prediction tasks.

1. Introduction

Kernel-based nonparametric models, such as support
vector machines and Gaussian processes (gps), have
been one of the dominant paradigms for supervised
machine learning over the last 20 years. These meth-
ods depend on deﬁning a kernel function, k(x, x(cid:48)),
which speciﬁes how similar or correlated outputs y and
y(cid:48) are expected to be at two inputs x and x(cid:48). By deﬁn-
ing the measure of similarity between inputs, the ker-
nel determines the pattern of inductive generalization.

Most existing techniques pose kernel
learning as
a (possibly high-dimensional) parameter estimation
problem. Examples include learning hyperparameters
(Rasmussen & Williams, 2006), linear combinations of
ﬁxed kernels (Bach, 2009), and mappings from the in-
put space to an embedding space (Salakhutdinov &
Hinton, 2008).

‡MIT. Proceedings
†U. Cambridge.
∗Equal contribution.
of the 30 th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28.

dkd23@cam.ac.uk
jrl44@cam.ac.uk
rgrosse@mit.edu
jbt@mit.edu
zoubin@eng.cam.ac.uk

However, to apply existing kernel learning algorithms,
the user must specify the parametric form of the ker-
nel, and this can require considerable expertise, as well
as trial and error.

To make kernel learning more generally applicable, we
reframe the kernel learning problem as one of structure
discovery, and automate the choice of kernel form. In
particular, we formulate a space of kernel structures
deﬁned compositionally in terms of sums and prod-
ucts of a small number of base kernel structures. This
provides an expressive modeling language which con-
cisely captures many widely used techniques for con-
structing kernels. We focus on Gaussian process re-
gression, where the kernel speciﬁes a covariance func-
tion, because the Bayesian framework is a convenient
way to formalize structure discovery. Borrowing dis-
crete search techniques which have proved successful in
equation discovery (Todorovski & Dzeroski, 1997) and
unsupervised learning (Grosse et al., 2012), we auto-
matically search over this space of kernel structures
using marginal likelihood as the search criterion.

We found that our structure discovery algorithm is
able to automatically recover known structures from
synthetic data as well as plausible structures for a va-
riety of real-world datasets. On a variety of time series
datasets, the learned kernels yield decompositions of
the unknown function into interpretable components
that enable accurate extrapolation beyond the range
of the observations. Furthermore, the automatically
discovered kernels outperform a variety of widely used
kernel classes and kernel combination methods on su-
pervised prediction tasks.

While we focus on Gaussian process regression, we be-
lieve our kernel search method can be extended to
other supervised learning frameworks such as classi-
ﬁcation or ordinal regression, or to other kinds of ker-
nel architectures such as kernel SVMs. We hope that
the algorithm developed in this paper will help replace

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

the current and often opaque art of kernel engineering
with a more transparent science of automated kernel
construction.

scripts, e.g. SE2 represents an SE kernel over the
second dimension of x.

2. Expressing structure through kernels

Gaussian process models use a kernel to deﬁne
the covariance between any two function values:
Cov(y, y(cid:48)) = k(x, x(cid:48)). The kernel speciﬁes which struc-
tures are likely under the gp prior, which in turn de-
termines the generalization properties of the model. In
this section, we review the ways in which kernel fam-
ilies1can be composed to express diverse priors over
functions.

There has been signiﬁcant work on constructing gp
kernels and analyzing their properties, summarized in
Chapter 4 of (Rasmussen & Williams, 2006). Com-
monly used kernels families include the squared expo-
nential (SE), periodic (Per), linear (Lin), and ratio-
nal quadratic (RQ) (see Figure 1 and the appendix).

Lin × Lin

quadratic
functions

SE × Per

locally
periodic

Lin + Per

periodic
with trend

SE + Per

periodic
with noise

Lin × SE

increasing
variation

Lin × Per

growing
amplitude

Squared-
exp (SE)

local
variation

Periodic
(Per)

repeating
structure

SE1 + SE2

f1(x1)
+f2(x2)

SE1 × SE2

f (x1, x2)

Linear
(Lin)

linear
functions

Rational-
quadratic(RQ)

multi-scale
variation

Figure 1. Left and third columns: base kernels k(·, 0). Sec-
ond and fourth columns: draws from a gp with each repec-
tive kernel. The x-axis has the same range on all plots.

Composing Kernels Positive semideﬁnite kernels
(i.e. those which deﬁne valid covariance functions) are
closed under addition and multiplication. This allows
one to create richly structured and interpretable ker-
nels from well understood base components.

All of the base kernels we use are one-dimensional; ker-
nels over multidimensional inputs are constructed by
adding and multiplying kernels over individual dimen-
sions. These dimensions are represented using sub-

1When unclear from context, we use ‘kernel family’ to
refer to the parametric forms of the functions given in the
appendix. A kernel is a kernel family with all of the pa-
rameters speciﬁed.

Figure 2. Examples of structures expressible by composite
kernels. Left column and third columns: composite kernels
k(·, 0). Plots have same meaning as in Figure 1.

Summation By summing kernels, we can model
the data as a superposition of
independent func-
tions, possibly representing diﬀerent structures. Sup-
pose functions f1, f2 are draw from independent gp
priors, f1 ∼ GP(µ1, k1), f2 ∼ GP(µ2, k2). Then
f := f1 + f2 ∼ GP(µ1 + µ2, k1 + k2).

In time series models, sums of kernels can express su-
perposition of diﬀerent processes, possibly operating
at diﬀerent scales. In multiple dimensions, summing
kernels gives additive structure over diﬀerent dimen-
sions, similar to generalized additive models (Hastie
& Tibshirani, 1990). These two kinds of structure are
demonstrated in rows 2 and 4 of ﬁgure 2, respectively.

Multiplication Multiplying kernels allows us to ac-
count for interactions between diﬀerent input dimen-
sions or diﬀerent notions of similarity. For instance,
in multidimensional data, the multiplicative kernel
SE1 × SE3 represents a smoothly varying function of
dimensions 1 and 3 which is not constrained to be

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

additive. In univariate data, multiplying a kernel by
SE gives a way of converting global structure to local
structure. For example, Per corresponds to globally
periodic structure, whereas Per × SE corresponds to
locally periodic structure, as shown in row 1 of ﬁgure 2.

Many architectures for learning complex functions,
such as convolutional networks (LeCun et al., 1989)
and sum-product networks (Poon & Domingos, 2011),
include units which compute AND-like and OR-like
operations. Composite kernels can be viewed in this
way too. A sum of kernels can be understood as an
OR-like operation: two points are considered similar
if either kernel has a high value. Similarly, multiply-
ing kernels is an AND-like operation, since two points
are considered similar only if both kernels have high
values. Since we are applying these operations to the
similarity functions rather than the regression func-
tions themselves, compositions of even a few base ker-
nels are able to capture complex relationships in data
which do not have a simple parametric form.

Example expressions
In addition to the examples
given in Figure 2, many common motifs of supervised
learning can be captured using sums and products of
one-dimensional base kernels:

Lin
Lin × Lin × . . .

Bayesian linear regression
Bayesian polynomial regression
Generalized Fourier decomposition Per + Per + . . .
(cid:80)D
Generalized additive models
Automatic relevance determination (cid:81)D
Linear trend with local deviations
Linearly growing amplitude

SEd
SEd
d=1
Lin + SE
Lin × SE

d=1

We use the term ‘generalized Fourier decomposition’
to express that the periodic functions expressible by a
gp with a periodic kernel are not limited to sinusoids.

3. Searching over structures

(1) Any subexpression S can be replaced with S + B,

where B is any base kernel family.

(2) Any subexpression S can be replaced with S × B,

where B is any base kernel family.

(3) Any base kernel B may be replaced with any other

base kernel family B(cid:48).

These operators can generate all possible algebraic ex-
pressions. To see this, observe that if we restricted
the + and × rules only to apply to base kernel fam-
ilies, we would obtain a context-free grammar (CFG)
which generates the set of algebraic expressions. How-
ever, the more general versions of these rules allow
more ﬂexibility in the search procedure, which is use-
ful because the CFG derivation may not be the most
straightforward way to arrive at a kernel family.

Our algorithm searches over this space using a greedy
search: at each stage, we choose the highest scoring
kernel and expand it by applying all possible operators.

Our search operators are motivated by strategies re-
searchers often use to construct kernels. In particular,

• One can look for structure, e.g. periodicity, in the
residuals of a model, and then extend the model
to capture that structure. This corresponds to
applying rule (1).

• One can start with structure, e.g. linearity, which
is assumed to hold globally, but ﬁnd that it only
holds locally. This corresponds to applying rule
(2) to obtain the structure shown in rows 1 and 3
of ﬁgure 2.

• One can add features incrementally, analogous to
algorithms like boosting, backﬁtting, or forward
selection. This corresponds to applying rules (1)
or (2) to dimensions not yet included in the model.

As discussed above, we can construct a wide variety of
kernel structures compositionally by adding and mul-
tiplying a small number of base kernels. In particular,
we consider the four base kernel families discussed in
Section 2: SE, Per, Lin, and RQ. Any algebraic ex-
pression combining these kernels using the operations
+ and × deﬁnes a kernel family, whose parameters are
the concatenation of the parameters for the base kernel
families.

Our search procedure begins by proposing all base ker-
nel families applied to all input dimensions. We allow
the following search operators over our set of expres-
sions:

Scoring kernel
families Choosing kernel struc-
tures requires a criterion for evaluating structures. We
choose marginal likelihood as our criterion, since it bal-
ances the ﬁt and complexity of a model (Rasmussen &
Ghahramani, 2001). Conditioned on kernel parame-
ters, the marginal likelihood of a gp can be computed
analytically. However, to evaluate a kernel family we
must integrate over kernel parameters. We approxi-
mate this intractable integral with the Bayesian infor-
mation criterion (Schwarz, 1978) after ﬁrst optimizing
to ﬁnd the maximum-likelihood kernel parameters.

Unfortunately, optimizing over parameters is not a
convex optimization problem, and the space can have

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

many local optima. For example, in data with pe-
riodic structure, integer multiples of the true period
(i.e. harmonics) are often local optima. To alleviate
this diﬃculty, we take advantage of our search proce-
dure to provide reasonable initializations: all of the
parameters which were part of the previous kernel are
initialized to their previous values. All parameters are
then optimized using conjugate gradients, randomly
restarting the newly introduced parameters. This pro-
cedure is not guaranteed to ﬁnd the global optimum,
but it implements the commonly used heuristic of it-
eratively modeling residuals.

4. Related Work

Nonparametric regression in high dimensions
Nonparametric regression methods such as splines, lo-
cally weighted regression, and gp regression are pop-
ular because they are capable of learning arbitrary
smooth functions of the data. Unfortunately, they suf-
fer from the curse of dimensionality: it is very diﬃcult
for the basic versions of these methods to generalize
well in more than a few dimensions. Applying non-
parametric methods in high-dimensional spaces can
require imposing additional structure on the model.

One such structure is additivity. Generalized addi-
tive models (GAM) assume the regression function is a
transformed sum of functions deﬁned on the individual
dimensions: E[f (x)] = g−1((cid:80)D
d=1 fd(xd)). These mod-
els have a limited compositional form, but one which is
interpretable and often generalizes well. In our gram-
mar, we can capture analogous structure through sums
of base kernels along diﬀerent dimensions.

It is possible to add more ﬂexibility to additive mod-
els by considering higher-order interactions between
diﬀerent dimensions. Additive Gaussian processes
(Duvenaud et al., 2011) are a gp model whose ker-
nel implicitly sums over all possible products of one-
dimensional base kernels. Plate (1999) constructs a gp
with a composite kernel, summing an SE kernel along
each dimension, with an SE-ARD kernel (i.e. a prod-
uct of SE over all dimensions). Both of these models
can be expressed in our grammar.

A closely related procedure is
smoothing-splines
ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-
ear combinations of splines along each dimension, all
pairs of dimensions, and possibly higher-order com-
binations. Because the number of terms to consider
grows exponentially in the order,
in practice, only
terms of ﬁrst and second order are usually considered.

Semiparametric regression (e.g. Ruppert et al., 2003)
attempts to combine interpretability with ﬂexibility by

building a composite model out of an interpretable,
parametric part (such as linear regression) and a
‘catch-all’ nonparametric part (such as a gp with an
SE kernel). In our approach, this can be represented
as a sum of SE and Lin.

Kernel learning There is a large body of work at-
tempting to construct a rich kernel through a weighted
sum of base kernels (e.g. Christoudias et al., 2009;
Bach, 2009). While these approaches ﬁnd the optimal
solution in polynomial time, speed comes at a cost: the
component kernels, as well as their hyperparameters,
must be speciﬁed in advance.

Another approach to kernel learning is to learn an em-
bedding of the data points. Lawrence (2005) learns an
embedding of the data into a low-dimensional space,
and constructs a ﬁxed kernel structure over that space.
This model is typically used in unsupervised tasks and
requires an expensive integration or optimisation over
potential embeddings when generalizing to test points.
Salakhutdinov & Hinton (2008) use a deep neural net-
work to learn an embedding; this is a ﬂexible approach
to kernel learning but relies upon ﬁnding structure in
the input density, p(x). Instead we focus on domains
where most of the interesting structure is in f(x).

Wilson & Adams (2013) derive kernels of the form
SE × cos(x − x(cid:48)), forming a basis for stationary ker-
nels. These kernels share similarities with SE × Per
but can express negative prior correlation, and could
usefully be included in our grammar.

Diosan et al. (2007) and Bing et al. (2010) learn com-
posite kernels for support vector machines and rel-
evance vector machines, using genetic search algo-
rithms. Our work employs a Bayesian search criterion,
and goes beyond this prior work by demonstrating the
interpretability of the structure implied by composite
kernels, and how such structure allows for extrapola-
tion.

Structure discovery There have been several at-
tempts to uncover the structural form of a dataset by
searching over a grammar of structures. For example,
(Schmidt & Lipson, 2009), (Todorovski & Dzeroski,
1997) and (Washio et al., 1999) attempt to learn para-
metric forms of equations to describe time series, or
relations between quantities. Because we learn expres-
sions describing the covariance structure rather than
the functions themselves, we are able to capture struc-
ture which does not have a simple parametric form.

Kemp & Tenenbaum (2008) learned the structural
form of a graph used to model human similarity judg-
ments. Examples of graphs included planes, trees, and

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

cylinders. Some of their discrete graph structures have
continous analogues in our own space; e.g. SE1 × SE2
and SE1 × Per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

Grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. This model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. Our work is tackling a similar problem, but in a
supervised setting.

5. Structure discovery in time series

To investigate our method’s ability to discover struc-
ture, we ran the kernel search on several time-series.

As discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. This provides another method
of visualizing the learned structures. In particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. For example,

SE × (RQ + Lin) = SE × RQ + SE × Lin.

We visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. The
search was run to depth 10, using the base kernels from
Section 2.

Mauna Loa atmospheric CO2 Using our method,
we analyzed records of carbon dioxide levels recorded
at the Mauna Loa observatory. Since this dataset was
analyzed in detail by Rasmussen & Williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

=

+

+

+

Figure 3. Posterior mean and variance for diﬀerent depths
of kernel search. The dashed line marks the extent of the
dataset. In the ﬁrst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
Next, a periodic component is added, and the extrapolation
improves. At depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

Figure 4. First row: The posterior on the Mauna Loa
dataset, after a search of depth 10. Subsequent rows show
the automatic decomposition of the time series. The de-
compositions shows long-term, yearly periodic, medium-
term anomaly components, and residuals, respectively. In
the third row, the scale has been changed in order to clearly
show the yearly periodic structure.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

=

+

+

+

Figure 5. Full posterior and residuals on the solar irradi-
ance dataset.

Figure 3 shows the posterior mean and variance on
this dataset as the search depth increases. While the
data can be smoothly interpolated by a single base
kernel model, the extrapolations improve dramatically
as the increased search depth allows more structure to
be included.

Figure 4 shows the ﬁnal model chosen by our method,
together with its decomposition into additive compo-
nents. The ﬁnal model exhibits both plausible ex-
trapolation and interpretable components: a long-
term trend, annual periodicity and medium-term devi-
ations; the same components chosen by Rasmussen &
Williams (2006). We also plot the residuals, observing
that there is little obvious structure left in the data.

Airline passenger data Figure 6 shows the decom-
position produced by applying our method to monthly
totals of international airline passengers (Box et al.,
1976). We observe similar components to the pre-
vious dataset: a long term trend, annual periodicity
and medium-term deviations.
In addition, the com-
posite kernel captures the near-linearity of the long-
term trend, and the linearly growing amplitude of the
annual oscillations.

Solar irradiance Data Finally, we analyzed annual
solar irradiation data from 1610 to 2011 (Lean et al.,
1995). The posterior and residuals of the learned ker-
nel are shown in ﬁgure 5.

Figure 6. First row: The airline dataset and posterior after
a search of depth 10. Subsequent rows: Additive decom-
position of posterior into long-term smooth trend, yearly
variation, and short-term deviations. Due to the linear ker-
nel, the marginal variance grows over time, making this a
heteroskedastic model.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

None of the models in our search space are capable
of parsimoniously representing the lack of variation
from 1645 to 1715. Despite this, our approach fails
gracefully: the learned kernel still captures the peri-
odic structure, and the quickly growing posterior vari-
ance demonstrates that the model is uncertain about
long term structure.

6. Validation on synthetic data

We validated our method’s ability to recover known
structure on a set of synthetic datasets. For several
composite kernel expressions, we constructed synthetic
data by ﬁrst sampling 300 points uniformly at random,
then sampling function values at those points from a
gp prior. We then added i.i.d. Gaussian noise to the
functions, at various signal-to-noise ratios (SNR).

Table 1 lists the true kernels we used to generate the
data. Subscripts indicate which dimension each kernel
was applied to. Subsequent columns show the dimen-
sionality D of the input space, and the kernels chosen
by our search for diﬀerent SNRs. Dashes - indicate
that no kernel had a higher marginal likelihood than
modeling the data as i.i.d. Gaussian noise.

For the highest SNR, the method ﬁnds all relevant
structure in all but one test. The reported additional
linear structure is explainable by the fact that func-
tions sampled from SE kernels with long length scales
occasionally have near-linear trends. As the noise
increases, our method generally backs oﬀ to simpler
structures.

7. Quantitative evaluation

In addition to the qualitative evaluation in section 5,
we investigated quantitatively how our method per-
forms on both extrapolation and interpolation tasks.

7.1. Extrapolation

We compared the extrapolation capabilities of our
model against standard baselines2. Dividing the air-
line dataset into contiguous training and test sets, we
computed the predictive mean-squared-error (MSE) of
each method. We varied the size of the training set
from the ﬁrst 10% to the ﬁrst 90% of the data.

Figure 7 shows the learning curves of linear regres-
sion, a variety of ﬁxed kernel family gp models, and
our method. gp models with only SE and Per ker-
nels did not capture the long-term trends, since the
best parameter values in terms of gp marginal like-
lihood only capture short term structure. Linear re-
gression approximately captured the long-term trend,

airline
Figure 7. Extrapolation
dataset. We plot test-set MSE as a function of the fraction
of the dataset used for training.

performance

the

on

but quickly plateaued in predictive performance. The
more richly structured gp models (SE + Per and
SE × Per) eventually captured more structure and
performed better, but the full structures discovered
by our search outperformed the other approaches in
terms of predictive performance for all data amounts.

7.2. High-dimensional prediction

To evaluate the predictive accuracy of our method in
a high-dimensional setting, we extended the compari-
son of (Duvenaud et al., 2011) to include our method.
We performed 10 fold cross validation on 5 datasets 3
comparing 5 methods in terms of MSE and predictive
likelihood. Our structure search was run up to depth
10, using the SE and RQ base kernel families.

The comparison included three methods with ﬁxed
kernel families: Additive gps, Generalized Additive
Models (GAM), and a gp with a standard SE kernel
using Automatic Relevance Determination (gp SE-
ARD). Also included was the related kernel-search
method of Hierarchical Kernel Learning (HKL).

Results are presented in table 2. Our method outper-
formed the next-best method in each test, although
not substantially.

All gp hyperparameter tuning was performed by au-

2 In one dimension, the predictive means of all baseline
methods in table 2 are identical to that of a gp with an SE
kernel.

3The data sets had dimensionalities ranging from 4 to
13, and the number of data points ranged from 150 to 450.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

Table 1. Kernels chosen by our method on synthetic data generated using known kernel structures. D denotes the
dimension of the functions being modeled. SNR indicates the signal-to-noise ratio. Dashes - indicate no structure.

True Kernel
SE + RQ
Lin × Per
SE1 + RQ
2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 + SE4)

D
1
1
2
3
4
4
4

SNR = 10
SE
Lin × Per
SE1 + SE2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 × Lin3 × Lin1 + SE4)

SNR = 1
SE × Per
Lin × Per
Lin1 + SE2
SE2 × Per1 + SE3
Lin1 × SE2
SE1 + SE2 × SE3
(SE1 + SE2) × SE3 × SE4

SNR = 0.1
SE
SE
Lin1
-
Lin2
SE1
-

Table 2. Comparison of multidimensional regression performance. Bold results are not signiﬁcantly diﬀerent from the
best-performing method in each experiment, in a paired t-test with a p-value of 5%.

Method
Linear Regression
GAM
HKL
gp SE-ARD
gp Additive
Structure Search

bach
1.031
1.259
0.199
0.045
0.045
0.044

Mean Squared Error (MSE)
concrete puma
0.641
0.404
0.598
0.149
0.346
0.147
0.317
0.157
0.316
0.089
0.315
0.087

servo
0.523
0.281
0.199
0.126
0.110
0.102

housing bach
2.430
0.289
1.708
0.161
-
0.151
−0.131 0.398
0.092
−0.131 0.114
0.102
−0.141 0.065
0.082

Negative Log-Likelihood
servo
1.678
0.800
-
0.429
0.309
0.265

concrete puma
1.881
1.403
1.195
0.467
-
-
0.843
0.841
0.840

housing
1.052
0.457
-
0.207
0.194
0.059

tomated calls to the GPML toolbox4; Python code to
perform all experiments is available on github5.

ric regression and classiﬁcation methods accessible to
non-experts.

8. Discussion

“It would be very nice to have a formal
apparatus that gives us some ‘optimal’ way of
recognizing unusual phenomena and invent-
ing new classes of hypotheses that are most
likely to contain the true one; but this re-
mains an art for the creative human mind.”
E. T. Jaynes, 1985

Towards the goal of automating the choice of kernel
family, we introduced a space of composite kernels de-
ﬁned compositionally as sums and products of a small
number of base kernels. The set of models included in
this space includes many standard regression models.
We proposed a search procedure for this space of ker-
nels which parallels the process of scientiﬁc discovery.

We found that the learned structures are often capa-
ble of accurate extrapolation in complex time-series
datasets, and are competitive with widely used kernel
classes and kernel combination methods on a variety
of prediction tasks. The learned kernels often yield de-
compositions of a signal into diverse and interpretable
components, enabling model-checking by humans. We
believe that a data-driven approach to choosing kernel
structures automatically can help make nonparamet-

4Available at www.gaussianprocess.org/gpml/code/
5 github.com/jamesrobertlloyd/gp-structure-search

Acknowledgements

We thank Carl Rasmussen and Andrew G. Wilson for
helpful discussions. This work was funded in part by
NSERC, EPSRC grant EP/I036575/1, and Google.

Appendix

Kernel deﬁnitions For scalar-valued inputs, the
squared exponential (SE), periodic (Per),
linear
(Lin), and rational quadratic (RQ) kernels are deﬁned
as follows:

kSE(x, x(cid:48)) =

kPer(x, x(cid:48)) = σ2 exp

kLin(x, x(cid:48)) =

kRQ(x, x(cid:48)) =

σ2 exp
(cid:16)

(cid:17)

(cid:16)

(cid:17)

− (x−x(cid:48))2
2(cid:96)2
− 2 sin2(π(x−x(cid:48))/p)
(cid:96)2
v(x − (cid:96))(x(cid:48) − (cid:96))
(cid:17)−α
1 + (x−x(cid:48))2
2α(cid:96)2

σ2
b + σ2
σ2 (cid:16)

Posterior decomposition We can analytically de-
compose a gp posterior distribution over additive com-
ponents using the following identity: The conditional
distribution of a Gaussian vector f1 conditioned on its
sum with another Gaussian vector f = f1 + f2 where
f1 ∼ N (µ1, K1) and f2 ∼ N (µ2, K2) is given by

f1|f ∼ N (cid:0)µ1 + K1
K1 − K1

T(K1 + K2)−1 (f − µ1 − µ2) ,
T(K1 + K2)−1K1

(cid:1).

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

References

Bach, F. Exploring large feature spaces with hierarchi-
cal multiple kernel learning. In Advances in Neural
Information Processing Systems, pp. 105–112. 2009.

Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization
method for RVM.
In International Conference on
Computer and Automation Engineering (ICCAE),
volume 4, pp. 419–423, 2010.

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time

series analysis: forecasting and control. 1976.

Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-
nical report, EECS Department, University of Cali-
fornia, Berkeley, 2009.

Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving
kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007, pp.
19–24. IEEE, 2007.

Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-
In Advances in Neural

ditive Gaussian processes.
Information Processing Systems, 2011.

Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-
plore a large space of model structures. In Uncer-
tainty in Artiﬁcial Intelligence, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989.

Plate, T.A. Accuracy versus interpretability in ﬂexible
modeling: Implementing a tradeoﬀ using Gaussian
process models. Behaviormetrika, 26:29–50, 1999.
ISSN 0385-7417.

Poon, H. and Domingos, P. Sum-product networks:
a new deep architecture. In Conference on Uncer-
tainty in AI, 2011.

Rasmussen, C.E. and Ghahramani, Z. Occam’s razor.
In Advances in Neural Information Processing Sys-
tems, 2001.

Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-
cesses for Machine Learning. The MIT Press, Cam-
bridge, MA, USA, 2006.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-
metric regression, volume 12. Cambridge University
Press, 2003.

Salakhutdinov, R. and Hinton, G. Using deep belief
nets to learn covariance kernels for Gaussian pro-
cesses. Advances in Neural information processing
systems, 20:1249–1256, 2008.

Schmidt, M. and Lipson, H. Distilling free-form natu-
ral laws from experimental data. Science, 324(5923):
81–85, 2009.

Gu, C. Smoothing spline ANOVA models. Springer

Schwarz, G. Estimating the dimension of a model. The

Verlag, 2002. ISBN 0387953531.

Annals of Statistics, 6(2):461–464, 1978.

Hastie, T.J. and Tibshirani, R.J. Generalized additive

models. Chapman & Hall/CRC, 1990.

Jaynes, E. T. Highly informative priors. In Proceedings
of the Second International Meeting on Bayesian
Statistics, 1985.

Kemp, C. and Tenenbaum, J.B.

The discovery
of structural form. Proceedings of the National
Academy of Sciences, 105(31):10687–10692, 2008.

Lawrence, N. Probabilistic non-linear principal com-
ponent analysis with gaussian process latent variable
models. The Journal of Machine Learning Research,
6:1783–1816, 2005.

Lean, J., Beer, J., and Bradley, R. Reconstruction of
solar irradiance since 1610: Implications for climate
change. Geophysical Research Letters, 22(23):3195–
3198, 1995.

Todorovski, L. and Dzeroski, S. Declarative bias in
equation discovery. In International Conference on
Machine Learning, pp. 376–384, 1997.

Wahba, G.

Spline models for observational data.
ISBN

Society for Industrial Mathematics, 1990.
0898712440.

Washio, T., Motoda, H., Niwa, Y., et al. Discover-
ing admissible model equations from observed data
based on scale-types and identity constraints.
In
International Joint Conference On Artiﬁcal Intelli-
gence, volume 16, pp. 772–779, 1999.

Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern
discovery and extrapolation.
Technical Report
arXiv:1302.4245 [stat.ML], February 2013.

Structure Discovery in Nonparametric Regression through
Compositional Kernel Search

3
1
0
2
 
y
a
M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
2
9
4
.
2
0
3
1
:
v
i
X
r
a

David Duvenaud∗†
James Robert Lloyd∗†
Roger Grosse‡
Joshua B. Tenenbaum‡
Zoubin Ghahramani†

Abstract
Despite its importance, choosing the struc-
tural form of the kernel
in nonparametric
regression remains a black art. We deﬁne
a space of kernel structures which are built
compositionally by adding and multiplying a
small number of base kernels. We present a
method for searching over this space of struc-
tures which mirrors the scientiﬁc discovery
process. The learned structures can often
decompose functions into interpretable com-
ponents and enable long-range extrapolation
on time-series datasets. Our structure search
method outperforms many widely used ker-
nels and kernel combination methods on a
variety of prediction tasks.

1. Introduction

Kernel-based nonparametric models, such as support
vector machines and Gaussian processes (gps), have
been one of the dominant paradigms for supervised
machine learning over the last 20 years. These meth-
ods depend on deﬁning a kernel function, k(x, x(cid:48)),
which speciﬁes how similar or correlated outputs y and
y(cid:48) are expected to be at two inputs x and x(cid:48). By deﬁn-
ing the measure of similarity between inputs, the ker-
nel determines the pattern of inductive generalization.

Most existing techniques pose kernel
learning as
a (possibly high-dimensional) parameter estimation
problem. Examples include learning hyperparameters
(Rasmussen & Williams, 2006), linear combinations of
ﬁxed kernels (Bach, 2009), and mappings from the in-
put space to an embedding space (Salakhutdinov &
Hinton, 2008).

‡MIT. Proceedings
†U. Cambridge.
∗Equal contribution.
of the 30 th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28.

dkd23@cam.ac.uk
jrl44@cam.ac.uk
rgrosse@mit.edu
jbt@mit.edu
zoubin@eng.cam.ac.uk

However, to apply existing kernel learning algorithms,
the user must specify the parametric form of the ker-
nel, and this can require considerable expertise, as well
as trial and error.

To make kernel learning more generally applicable, we
reframe the kernel learning problem as one of structure
discovery, and automate the choice of kernel form. In
particular, we formulate a space of kernel structures
deﬁned compositionally in terms of sums and prod-
ucts of a small number of base kernel structures. This
provides an expressive modeling language which con-
cisely captures many widely used techniques for con-
structing kernels. We focus on Gaussian process re-
gression, where the kernel speciﬁes a covariance func-
tion, because the Bayesian framework is a convenient
way to formalize structure discovery. Borrowing dis-
crete search techniques which have proved successful in
equation discovery (Todorovski & Dzeroski, 1997) and
unsupervised learning (Grosse et al., 2012), we auto-
matically search over this space of kernel structures
using marginal likelihood as the search criterion.

We found that our structure discovery algorithm is
able to automatically recover known structures from
synthetic data as well as plausible structures for a va-
riety of real-world datasets. On a variety of time series
datasets, the learned kernels yield decompositions of
the unknown function into interpretable components
that enable accurate extrapolation beyond the range
of the observations. Furthermore, the automatically
discovered kernels outperform a variety of widely used
kernel classes and kernel combination methods on su-
pervised prediction tasks.

While we focus on Gaussian process regression, we be-
lieve our kernel search method can be extended to
other supervised learning frameworks such as classi-
ﬁcation or ordinal regression, or to other kinds of ker-
nel architectures such as kernel SVMs. We hope that
the algorithm developed in this paper will help replace

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

the current and often opaque art of kernel engineering
with a more transparent science of automated kernel
construction.

scripts, e.g. SE2 represents an SE kernel over the
second dimension of x.

2. Expressing structure through kernels

Gaussian process models use a kernel to deﬁne
the covariance between any two function values:
Cov(y, y(cid:48)) = k(x, x(cid:48)). The kernel speciﬁes which struc-
tures are likely under the gp prior, which in turn de-
termines the generalization properties of the model. In
this section, we review the ways in which kernel fam-
ilies1can be composed to express diverse priors over
functions.

There has been signiﬁcant work on constructing gp
kernels and analyzing their properties, summarized in
Chapter 4 of (Rasmussen & Williams, 2006). Com-
monly used kernels families include the squared expo-
nential (SE), periodic (Per), linear (Lin), and ratio-
nal quadratic (RQ) (see Figure 1 and the appendix).

Lin × Lin

quadratic
functions

SE × Per

locally
periodic

Lin + Per

periodic
with trend

SE + Per

periodic
with noise

Lin × SE

increasing
variation

Lin × Per

growing
amplitude

Squared-
exp (SE)

local
variation

Periodic
(Per)

repeating
structure

SE1 + SE2

f1(x1)
+f2(x2)

SE1 × SE2

f (x1, x2)

Linear
(Lin)

linear
functions

Rational-
quadratic(RQ)

multi-scale
variation

Figure 1. Left and third columns: base kernels k(·, 0). Sec-
ond and fourth columns: draws from a gp with each repec-
tive kernel. The x-axis has the same range on all plots.

Composing Kernels Positive semideﬁnite kernels
(i.e. those which deﬁne valid covariance functions) are
closed under addition and multiplication. This allows
one to create richly structured and interpretable ker-
nels from well understood base components.

All of the base kernels we use are one-dimensional; ker-
nels over multidimensional inputs are constructed by
adding and multiplying kernels over individual dimen-
sions. These dimensions are represented using sub-

1When unclear from context, we use ‘kernel family’ to
refer to the parametric forms of the functions given in the
appendix. A kernel is a kernel family with all of the pa-
rameters speciﬁed.

Figure 2. Examples of structures expressible by composite
kernels. Left column and third columns: composite kernels
k(·, 0). Plots have same meaning as in Figure 1.

Summation By summing kernels, we can model
the data as a superposition of
independent func-
tions, possibly representing diﬀerent structures. Sup-
pose functions f1, f2 are draw from independent gp
priors, f1 ∼ GP(µ1, k1), f2 ∼ GP(µ2, k2). Then
f := f1 + f2 ∼ GP(µ1 + µ2, k1 + k2).

In time series models, sums of kernels can express su-
perposition of diﬀerent processes, possibly operating
at diﬀerent scales. In multiple dimensions, summing
kernels gives additive structure over diﬀerent dimen-
sions, similar to generalized additive models (Hastie
& Tibshirani, 1990). These two kinds of structure are
demonstrated in rows 2 and 4 of ﬁgure 2, respectively.

Multiplication Multiplying kernels allows us to ac-
count for interactions between diﬀerent input dimen-
sions or diﬀerent notions of similarity. For instance,
in multidimensional data, the multiplicative kernel
SE1 × SE3 represents a smoothly varying function of
dimensions 1 and 3 which is not constrained to be

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

additive. In univariate data, multiplying a kernel by
SE gives a way of converting global structure to local
structure. For example, Per corresponds to globally
periodic structure, whereas Per × SE corresponds to
locally periodic structure, as shown in row 1 of ﬁgure 2.

Many architectures for learning complex functions,
such as convolutional networks (LeCun et al., 1989)
and sum-product networks (Poon & Domingos, 2011),
include units which compute AND-like and OR-like
operations. Composite kernels can be viewed in this
way too. A sum of kernels can be understood as an
OR-like operation: two points are considered similar
if either kernel has a high value. Similarly, multiply-
ing kernels is an AND-like operation, since two points
are considered similar only if both kernels have high
values. Since we are applying these operations to the
similarity functions rather than the regression func-
tions themselves, compositions of even a few base ker-
nels are able to capture complex relationships in data
which do not have a simple parametric form.

Example expressions
In addition to the examples
given in Figure 2, many common motifs of supervised
learning can be captured using sums and products of
one-dimensional base kernels:

Lin
Lin × Lin × . . .

Bayesian linear regression
Bayesian polynomial regression
Generalized Fourier decomposition Per + Per + . . .
(cid:80)D
Generalized additive models
Automatic relevance determination (cid:81)D
Linear trend with local deviations
Linearly growing amplitude

SEd
SEd
d=1
Lin + SE
Lin × SE

d=1

We use the term ‘generalized Fourier decomposition’
to express that the periodic functions expressible by a
gp with a periodic kernel are not limited to sinusoids.

3. Searching over structures

(1) Any subexpression S can be replaced with S + B,

where B is any base kernel family.

(2) Any subexpression S can be replaced with S × B,

where B is any base kernel family.

(3) Any base kernel B may be replaced with any other

base kernel family B(cid:48).

These operators can generate all possible algebraic ex-
pressions. To see this, observe that if we restricted
the + and × rules only to apply to base kernel fam-
ilies, we would obtain a context-free grammar (CFG)
which generates the set of algebraic expressions. How-
ever, the more general versions of these rules allow
more ﬂexibility in the search procedure, which is use-
ful because the CFG derivation may not be the most
straightforward way to arrive at a kernel family.

Our algorithm searches over this space using a greedy
search: at each stage, we choose the highest scoring
kernel and expand it by applying all possible operators.

Our search operators are motivated by strategies re-
searchers often use to construct kernels. In particular,

• One can look for structure, e.g. periodicity, in the
residuals of a model, and then extend the model
to capture that structure. This corresponds to
applying rule (1).

• One can start with structure, e.g. linearity, which
is assumed to hold globally, but ﬁnd that it only
holds locally. This corresponds to applying rule
(2) to obtain the structure shown in rows 1 and 3
of ﬁgure 2.

• One can add features incrementally, analogous to
algorithms like boosting, backﬁtting, or forward
selection. This corresponds to applying rules (1)
or (2) to dimensions not yet included in the model.

As discussed above, we can construct a wide variety of
kernel structures compositionally by adding and mul-
tiplying a small number of base kernels. In particular,
we consider the four base kernel families discussed in
Section 2: SE, Per, Lin, and RQ. Any algebraic ex-
pression combining these kernels using the operations
+ and × deﬁnes a kernel family, whose parameters are
the concatenation of the parameters for the base kernel
families.

Our search procedure begins by proposing all base ker-
nel families applied to all input dimensions. We allow
the following search operators over our set of expres-
sions:

Scoring kernel
families Choosing kernel struc-
tures requires a criterion for evaluating structures. We
choose marginal likelihood as our criterion, since it bal-
ances the ﬁt and complexity of a model (Rasmussen &
Ghahramani, 2001). Conditioned on kernel parame-
ters, the marginal likelihood of a gp can be computed
analytically. However, to evaluate a kernel family we
must integrate over kernel parameters. We approxi-
mate this intractable integral with the Bayesian infor-
mation criterion (Schwarz, 1978) after ﬁrst optimizing
to ﬁnd the maximum-likelihood kernel parameters.

Unfortunately, optimizing over parameters is not a
convex optimization problem, and the space can have

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

many local optima. For example, in data with pe-
riodic structure, integer multiples of the true period
(i.e. harmonics) are often local optima. To alleviate
this diﬃculty, we take advantage of our search proce-
dure to provide reasonable initializations: all of the
parameters which were part of the previous kernel are
initialized to their previous values. All parameters are
then optimized using conjugate gradients, randomly
restarting the newly introduced parameters. This pro-
cedure is not guaranteed to ﬁnd the global optimum,
but it implements the commonly used heuristic of it-
eratively modeling residuals.

4. Related Work

Nonparametric regression in high dimensions
Nonparametric regression methods such as splines, lo-
cally weighted regression, and gp regression are pop-
ular because they are capable of learning arbitrary
smooth functions of the data. Unfortunately, they suf-
fer from the curse of dimensionality: it is very diﬃcult
for the basic versions of these methods to generalize
well in more than a few dimensions. Applying non-
parametric methods in high-dimensional spaces can
require imposing additional structure on the model.

One such structure is additivity. Generalized addi-
tive models (GAM) assume the regression function is a
transformed sum of functions deﬁned on the individual
dimensions: E[f (x)] = g−1((cid:80)D
d=1 fd(xd)). These mod-
els have a limited compositional form, but one which is
interpretable and often generalizes well. In our gram-
mar, we can capture analogous structure through sums
of base kernels along diﬀerent dimensions.

It is possible to add more ﬂexibility to additive mod-
els by considering higher-order interactions between
diﬀerent dimensions. Additive Gaussian processes
(Duvenaud et al., 2011) are a gp model whose ker-
nel implicitly sums over all possible products of one-
dimensional base kernels. Plate (1999) constructs a gp
with a composite kernel, summing an SE kernel along
each dimension, with an SE-ARD kernel (i.e. a prod-
uct of SE over all dimensions). Both of these models
can be expressed in our grammar.

A closely related procedure is
smoothing-splines
ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-
ear combinations of splines along each dimension, all
pairs of dimensions, and possibly higher-order com-
binations. Because the number of terms to consider
grows exponentially in the order,
in practice, only
terms of ﬁrst and second order are usually considered.

Semiparametric regression (e.g. Ruppert et al., 2003)
attempts to combine interpretability with ﬂexibility by

building a composite model out of an interpretable,
parametric part (such as linear regression) and a
‘catch-all’ nonparametric part (such as a gp with an
SE kernel). In our approach, this can be represented
as a sum of SE and Lin.

Kernel learning There is a large body of work at-
tempting to construct a rich kernel through a weighted
sum of base kernels (e.g. Christoudias et al., 2009;
Bach, 2009). While these approaches ﬁnd the optimal
solution in polynomial time, speed comes at a cost: the
component kernels, as well as their hyperparameters,
must be speciﬁed in advance.

Another approach to kernel learning is to learn an em-
bedding of the data points. Lawrence (2005) learns an
embedding of the data into a low-dimensional space,
and constructs a ﬁxed kernel structure over that space.
This model is typically used in unsupervised tasks and
requires an expensive integration or optimisation over
potential embeddings when generalizing to test points.
Salakhutdinov & Hinton (2008) use a deep neural net-
work to learn an embedding; this is a ﬂexible approach
to kernel learning but relies upon ﬁnding structure in
the input density, p(x). Instead we focus on domains
where most of the interesting structure is in f(x).

Wilson & Adams (2013) derive kernels of the form
SE × cos(x − x(cid:48)), forming a basis for stationary ker-
nels. These kernels share similarities with SE × Per
but can express negative prior correlation, and could
usefully be included in our grammar.

Diosan et al. (2007) and Bing et al. (2010) learn com-
posite kernels for support vector machines and rel-
evance vector machines, using genetic search algo-
rithms. Our work employs a Bayesian search criterion,
and goes beyond this prior work by demonstrating the
interpretability of the structure implied by composite
kernels, and how such structure allows for extrapola-
tion.

Structure discovery There have been several at-
tempts to uncover the structural form of a dataset by
searching over a grammar of structures. For example,
(Schmidt & Lipson, 2009), (Todorovski & Dzeroski,
1997) and (Washio et al., 1999) attempt to learn para-
metric forms of equations to describe time series, or
relations between quantities. Because we learn expres-
sions describing the covariance structure rather than
the functions themselves, we are able to capture struc-
ture which does not have a simple parametric form.

Kemp & Tenenbaum (2008) learned the structural
form of a graph used to model human similarity judg-
ments. Examples of graphs included planes, trees, and

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

cylinders. Some of their discrete graph structures have
continous analogues in our own space; e.g. SE1 × SE2
and SE1 × Per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

Grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. This model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. Our work is tackling a similar problem, but in a
supervised setting.

5. Structure discovery in time series

To investigate our method’s ability to discover struc-
ture, we ran the kernel search on several time-series.

As discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. This provides another method
of visualizing the learned structures. In particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. For example,

SE × (RQ + Lin) = SE × RQ + SE × Lin.

We visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. The
search was run to depth 10, using the base kernels from
Section 2.

Mauna Loa atmospheric CO2 Using our method,
we analyzed records of carbon dioxide levels recorded
at the Mauna Loa observatory. Since this dataset was
analyzed in detail by Rasmussen & Williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

=

+

+

+

Figure 3. Posterior mean and variance for diﬀerent depths
of kernel search. The dashed line marks the extent of the
dataset. In the ﬁrst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
Next, a periodic component is added, and the extrapolation
improves. At depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

Figure 4. First row: The posterior on the Mauna Loa
dataset, after a search of depth 10. Subsequent rows show
the automatic decomposition of the time series. The de-
compositions shows long-term, yearly periodic, medium-
term anomaly components, and residuals, respectively. In
the third row, the scale has been changed in order to clearly
show the yearly periodic structure.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

=

+

+

+

Figure 5. Full posterior and residuals on the solar irradi-
ance dataset.

Figure 3 shows the posterior mean and variance on
this dataset as the search depth increases. While the
data can be smoothly interpolated by a single base
kernel model, the extrapolations improve dramatically
as the increased search depth allows more structure to
be included.

Figure 4 shows the ﬁnal model chosen by our method,
together with its decomposition into additive compo-
nents. The ﬁnal model exhibits both plausible ex-
trapolation and interpretable components: a long-
term trend, annual periodicity and medium-term devi-
ations; the same components chosen by Rasmussen &
Williams (2006). We also plot the residuals, observing
that there is little obvious structure left in the data.

Airline passenger data Figure 6 shows the decom-
position produced by applying our method to monthly
totals of international airline passengers (Box et al.,
1976). We observe similar components to the pre-
vious dataset: a long term trend, annual periodicity
and medium-term deviations.
In addition, the com-
posite kernel captures the near-linearity of the long-
term trend, and the linearly growing amplitude of the
annual oscillations.

Solar irradiance Data Finally, we analyzed annual
solar irradiation data from 1610 to 2011 (Lean et al.,
1995). The posterior and residuals of the learned ker-
nel are shown in ﬁgure 5.

Figure 6. First row: The airline dataset and posterior after
a search of depth 10. Subsequent rows: Additive decom-
position of posterior into long-term smooth trend, yearly
variation, and short-term deviations. Due to the linear ker-
nel, the marginal variance grows over time, making this a
heteroskedastic model.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

None of the models in our search space are capable
of parsimoniously representing the lack of variation
from 1645 to 1715. Despite this, our approach fails
gracefully: the learned kernel still captures the peri-
odic structure, and the quickly growing posterior vari-
ance demonstrates that the model is uncertain about
long term structure.

6. Validation on synthetic data

We validated our method’s ability to recover known
structure on a set of synthetic datasets. For several
composite kernel expressions, we constructed synthetic
data by ﬁrst sampling 300 points uniformly at random,
then sampling function values at those points from a
gp prior. We then added i.i.d. Gaussian noise to the
functions, at various signal-to-noise ratios (SNR).

Table 1 lists the true kernels we used to generate the
data. Subscripts indicate which dimension each kernel
was applied to. Subsequent columns show the dimen-
sionality D of the input space, and the kernels chosen
by our search for diﬀerent SNRs. Dashes - indicate
that no kernel had a higher marginal likelihood than
modeling the data as i.i.d. Gaussian noise.

For the highest SNR, the method ﬁnds all relevant
structure in all but one test. The reported additional
linear structure is explainable by the fact that func-
tions sampled from SE kernels with long length scales
occasionally have near-linear trends. As the noise
increases, our method generally backs oﬀ to simpler
structures.

7. Quantitative evaluation

In addition to the qualitative evaluation in section 5,
we investigated quantitatively how our method per-
forms on both extrapolation and interpolation tasks.

7.1. Extrapolation

We compared the extrapolation capabilities of our
model against standard baselines2. Dividing the air-
line dataset into contiguous training and test sets, we
computed the predictive mean-squared-error (MSE) of
each method. We varied the size of the training set
from the ﬁrst 10% to the ﬁrst 90% of the data.

Figure 7 shows the learning curves of linear regres-
sion, a variety of ﬁxed kernel family gp models, and
our method. gp models with only SE and Per ker-
nels did not capture the long-term trends, since the
best parameter values in terms of gp marginal like-
lihood only capture short term structure. Linear re-
gression approximately captured the long-term trend,

airline
Figure 7. Extrapolation
dataset. We plot test-set MSE as a function of the fraction
of the dataset used for training.

performance

the

on

but quickly plateaued in predictive performance. The
more richly structured gp models (SE + Per and
SE × Per) eventually captured more structure and
performed better, but the full structures discovered
by our search outperformed the other approaches in
terms of predictive performance for all data amounts.

7.2. High-dimensional prediction

To evaluate the predictive accuracy of our method in
a high-dimensional setting, we extended the compari-
son of (Duvenaud et al., 2011) to include our method.
We performed 10 fold cross validation on 5 datasets 3
comparing 5 methods in terms of MSE and predictive
likelihood. Our structure search was run up to depth
10, using the SE and RQ base kernel families.

The comparison included three methods with ﬁxed
kernel families: Additive gps, Generalized Additive
Models (GAM), and a gp with a standard SE kernel
using Automatic Relevance Determination (gp SE-
ARD). Also included was the related kernel-search
method of Hierarchical Kernel Learning (HKL).

Results are presented in table 2. Our method outper-
formed the next-best method in each test, although
not substantially.

All gp hyperparameter tuning was performed by au-

2 In one dimension, the predictive means of all baseline
methods in table 2 are identical to that of a gp with an SE
kernel.

3The data sets had dimensionalities ranging from 4 to
13, and the number of data points ranged from 150 to 450.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

Table 1. Kernels chosen by our method on synthetic data generated using known kernel structures. D denotes the
dimension of the functions being modeled. SNR indicates the signal-to-noise ratio. Dashes - indicate no structure.

True Kernel
SE + RQ
Lin × Per
SE1 + RQ
2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 + SE4)

D
1
1
2
3
4
4
4

SNR = 10
SE
Lin × Per
SE1 + SE2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 × Lin3 × Lin1 + SE4)

SNR = 1
SE × Per
Lin × Per
Lin1 + SE2
SE2 × Per1 + SE3
Lin1 × SE2
SE1 + SE2 × SE3
(SE1 + SE2) × SE3 × SE4

SNR = 0.1
SE
SE
Lin1
-
Lin2
SE1
-

Table 2. Comparison of multidimensional regression performance. Bold results are not signiﬁcantly diﬀerent from the
best-performing method in each experiment, in a paired t-test with a p-value of 5%.

Method
Linear Regression
GAM
HKL
gp SE-ARD
gp Additive
Structure Search

bach
1.031
1.259
0.199
0.045
0.045
0.044

Mean Squared Error (MSE)
concrete puma
0.641
0.404
0.598
0.149
0.346
0.147
0.317
0.157
0.316
0.089
0.315
0.087

servo
0.523
0.281
0.199
0.126
0.110
0.102

housing bach
2.430
0.289
1.708
0.161
-
0.151
−0.131 0.398
0.092
−0.131 0.114
0.102
−0.141 0.065
0.082

Negative Log-Likelihood
servo
1.678
0.800
-
0.429
0.309
0.265

concrete puma
1.881
1.403
1.195
0.467
-
-
0.843
0.841
0.840

housing
1.052
0.457
-
0.207
0.194
0.059

tomated calls to the GPML toolbox4; Python code to
perform all experiments is available on github5.

ric regression and classiﬁcation methods accessible to
non-experts.

8. Discussion

“It would be very nice to have a formal
apparatus that gives us some ‘optimal’ way of
recognizing unusual phenomena and invent-
ing new classes of hypotheses that are most
likely to contain the true one; but this re-
mains an art for the creative human mind.”
E. T. Jaynes, 1985

Towards the goal of automating the choice of kernel
family, we introduced a space of composite kernels de-
ﬁned compositionally as sums and products of a small
number of base kernels. The set of models included in
this space includes many standard regression models.
We proposed a search procedure for this space of ker-
nels which parallels the process of scientiﬁc discovery.

We found that the learned structures are often capa-
ble of accurate extrapolation in complex time-series
datasets, and are competitive with widely used kernel
classes and kernel combination methods on a variety
of prediction tasks. The learned kernels often yield de-
compositions of a signal into diverse and interpretable
components, enabling model-checking by humans. We
believe that a data-driven approach to choosing kernel
structures automatically can help make nonparamet-

4Available at www.gaussianprocess.org/gpml/code/
5 github.com/jamesrobertlloyd/gp-structure-search

Acknowledgements

We thank Carl Rasmussen and Andrew G. Wilson for
helpful discussions. This work was funded in part by
NSERC, EPSRC grant EP/I036575/1, and Google.

Appendix

Kernel deﬁnitions For scalar-valued inputs, the
squared exponential (SE), periodic (Per),
linear
(Lin), and rational quadratic (RQ) kernels are deﬁned
as follows:

kSE(x, x(cid:48)) =

kPer(x, x(cid:48)) = σ2 exp

kLin(x, x(cid:48)) =

kRQ(x, x(cid:48)) =

σ2 exp
(cid:16)

(cid:17)

(cid:16)

(cid:17)

− (x−x(cid:48))2
2(cid:96)2
− 2 sin2(π(x−x(cid:48))/p)
(cid:96)2
v(x − (cid:96))(x(cid:48) − (cid:96))
(cid:17)−α
1 + (x−x(cid:48))2
2α(cid:96)2

σ2
b + σ2
σ2 (cid:16)

Posterior decomposition We can analytically de-
compose a gp posterior distribution over additive com-
ponents using the following identity: The conditional
distribution of a Gaussian vector f1 conditioned on its
sum with another Gaussian vector f = f1 + f2 where
f1 ∼ N (µ1, K1) and f2 ∼ N (µ2, K2) is given by

f1|f ∼ N (cid:0)µ1 + K1
K1 − K1

T(K1 + K2)−1 (f − µ1 − µ2) ,
T(K1 + K2)−1K1

(cid:1).

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

References

Bach, F. Exploring large feature spaces with hierarchi-
cal multiple kernel learning. In Advances in Neural
Information Processing Systems, pp. 105–112. 2009.

Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization
method for RVM.
In International Conference on
Computer and Automation Engineering (ICCAE),
volume 4, pp. 419–423, 2010.

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time

series analysis: forecasting and control. 1976.

Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-
nical report, EECS Department, University of Cali-
fornia, Berkeley, 2009.

Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving
kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007, pp.
19–24. IEEE, 2007.

Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-
In Advances in Neural

ditive Gaussian processes.
Information Processing Systems, 2011.

Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-
plore a large space of model structures. In Uncer-
tainty in Artiﬁcial Intelligence, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989.

Plate, T.A. Accuracy versus interpretability in ﬂexible
modeling: Implementing a tradeoﬀ using Gaussian
process models. Behaviormetrika, 26:29–50, 1999.
ISSN 0385-7417.

Poon, H. and Domingos, P. Sum-product networks:
a new deep architecture. In Conference on Uncer-
tainty in AI, 2011.

Rasmussen, C.E. and Ghahramani, Z. Occam’s razor.
In Advances in Neural Information Processing Sys-
tems, 2001.

Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-
cesses for Machine Learning. The MIT Press, Cam-
bridge, MA, USA, 2006.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-
metric regression, volume 12. Cambridge University
Press, 2003.

Salakhutdinov, R. and Hinton, G. Using deep belief
nets to learn covariance kernels for Gaussian pro-
cesses. Advances in Neural information processing
systems, 20:1249–1256, 2008.

Schmidt, M. and Lipson, H. Distilling free-form natu-
ral laws from experimental data. Science, 324(5923):
81–85, 2009.

Gu, C. Smoothing spline ANOVA models. Springer

Schwarz, G. Estimating the dimension of a model. The

Verlag, 2002. ISBN 0387953531.

Annals of Statistics, 6(2):461–464, 1978.

Hastie, T.J. and Tibshirani, R.J. Generalized additive

models. Chapman & Hall/CRC, 1990.

Jaynes, E. T. Highly informative priors. In Proceedings
of the Second International Meeting on Bayesian
Statistics, 1985.

Kemp, C. and Tenenbaum, J.B.

The discovery
of structural form. Proceedings of the National
Academy of Sciences, 105(31):10687–10692, 2008.

Lawrence, N. Probabilistic non-linear principal com-
ponent analysis with gaussian process latent variable
models. The Journal of Machine Learning Research,
6:1783–1816, 2005.

Lean, J., Beer, J., and Bradley, R. Reconstruction of
solar irradiance since 1610: Implications for climate
change. Geophysical Research Letters, 22(23):3195–
3198, 1995.

Todorovski, L. and Dzeroski, S. Declarative bias in
equation discovery. In International Conference on
Machine Learning, pp. 376–384, 1997.

Wahba, G.

Spline models for observational data.
ISBN

Society for Industrial Mathematics, 1990.
0898712440.

Washio, T., Motoda, H., Niwa, Y., et al. Discover-
ing admissible model equations from observed data
based on scale-types and identity constraints.
In
International Joint Conference On Artiﬁcal Intelli-
gence, volume 16, pp. 772–779, 1999.

Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern
discovery and extrapolation.
Technical Report
arXiv:1302.4245 [stat.ML], February 2013.

Structure Discovery in Nonparametric Regression through
Compositional Kernel Search

3
1
0
2
 
y
a
M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
2
9
4
.
2
0
3
1
:
v
i
X
r
a

David Duvenaud∗†
James Robert Lloyd∗†
Roger Grosse‡
Joshua B. Tenenbaum‡
Zoubin Ghahramani†

Abstract
Despite its importance, choosing the struc-
tural form of the kernel
in nonparametric
regression remains a black art. We deﬁne
a space of kernel structures which are built
compositionally by adding and multiplying a
small number of base kernels. We present a
method for searching over this space of struc-
tures which mirrors the scientiﬁc discovery
process. The learned structures can often
decompose functions into interpretable com-
ponents and enable long-range extrapolation
on time-series datasets. Our structure search
method outperforms many widely used ker-
nels and kernel combination methods on a
variety of prediction tasks.

1. Introduction

Kernel-based nonparametric models, such as support
vector machines and Gaussian processes (gps), have
been one of the dominant paradigms for supervised
machine learning over the last 20 years. These meth-
ods depend on deﬁning a kernel function, k(x, x(cid:48)),
which speciﬁes how similar or correlated outputs y and
y(cid:48) are expected to be at two inputs x and x(cid:48). By deﬁn-
ing the measure of similarity between inputs, the ker-
nel determines the pattern of inductive generalization.

Most existing techniques pose kernel
learning as
a (possibly high-dimensional) parameter estimation
problem. Examples include learning hyperparameters
(Rasmussen & Williams, 2006), linear combinations of
ﬁxed kernels (Bach, 2009), and mappings from the in-
put space to an embedding space (Salakhutdinov &
Hinton, 2008).

‡MIT. Proceedings
†U. Cambridge.
∗Equal contribution.
of the 30 th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28.

dkd23@cam.ac.uk
jrl44@cam.ac.uk
rgrosse@mit.edu
jbt@mit.edu
zoubin@eng.cam.ac.uk

However, to apply existing kernel learning algorithms,
the user must specify the parametric form of the ker-
nel, and this can require considerable expertise, as well
as trial and error.

To make kernel learning more generally applicable, we
reframe the kernel learning problem as one of structure
discovery, and automate the choice of kernel form. In
particular, we formulate a space of kernel structures
deﬁned compositionally in terms of sums and prod-
ucts of a small number of base kernel structures. This
provides an expressive modeling language which con-
cisely captures many widely used techniques for con-
structing kernels. We focus on Gaussian process re-
gression, where the kernel speciﬁes a covariance func-
tion, because the Bayesian framework is a convenient
way to formalize structure discovery. Borrowing dis-
crete search techniques which have proved successful in
equation discovery (Todorovski & Dzeroski, 1997) and
unsupervised learning (Grosse et al., 2012), we auto-
matically search over this space of kernel structures
using marginal likelihood as the search criterion.

We found that our structure discovery algorithm is
able to automatically recover known structures from
synthetic data as well as plausible structures for a va-
riety of real-world datasets. On a variety of time series
datasets, the learned kernels yield decompositions of
the unknown function into interpretable components
that enable accurate extrapolation beyond the range
of the observations. Furthermore, the automatically
discovered kernels outperform a variety of widely used
kernel classes and kernel combination methods on su-
pervised prediction tasks.

While we focus on Gaussian process regression, we be-
lieve our kernel search method can be extended to
other supervised learning frameworks such as classi-
ﬁcation or ordinal regression, or to other kinds of ker-
nel architectures such as kernel SVMs. We hope that
the algorithm developed in this paper will help replace

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

the current and often opaque art of kernel engineering
with a more transparent science of automated kernel
construction.

scripts, e.g. SE2 represents an SE kernel over the
second dimension of x.

2. Expressing structure through kernels

Gaussian process models use a kernel to deﬁne
the covariance between any two function values:
Cov(y, y(cid:48)) = k(x, x(cid:48)). The kernel speciﬁes which struc-
tures are likely under the gp prior, which in turn de-
termines the generalization properties of the model. In
this section, we review the ways in which kernel fam-
ilies1can be composed to express diverse priors over
functions.

There has been signiﬁcant work on constructing gp
kernels and analyzing their properties, summarized in
Chapter 4 of (Rasmussen & Williams, 2006). Com-
monly used kernels families include the squared expo-
nential (SE), periodic (Per), linear (Lin), and ratio-
nal quadratic (RQ) (see Figure 1 and the appendix).

Lin × Lin

quadratic
functions

SE × Per

locally
periodic

Lin + Per

periodic
with trend

SE + Per

periodic
with noise

Lin × SE

increasing
variation

Lin × Per

growing
amplitude

Squared-
exp (SE)

local
variation

Periodic
(Per)

repeating
structure

SE1 + SE2

f1(x1)
+f2(x2)

SE1 × SE2

f (x1, x2)

Linear
(Lin)

linear
functions

Rational-
quadratic(RQ)

multi-scale
variation

Figure 1. Left and third columns: base kernels k(·, 0). Sec-
ond and fourth columns: draws from a gp with each repec-
tive kernel. The x-axis has the same range on all plots.

Composing Kernels Positive semideﬁnite kernels
(i.e. those which deﬁne valid covariance functions) are
closed under addition and multiplication. This allows
one to create richly structured and interpretable ker-
nels from well understood base components.

All of the base kernels we use are one-dimensional; ker-
nels over multidimensional inputs are constructed by
adding and multiplying kernels over individual dimen-
sions. These dimensions are represented using sub-

1When unclear from context, we use ‘kernel family’ to
refer to the parametric forms of the functions given in the
appendix. A kernel is a kernel family with all of the pa-
rameters speciﬁed.

Figure 2. Examples of structures expressible by composite
kernels. Left column and third columns: composite kernels
k(·, 0). Plots have same meaning as in Figure 1.

Summation By summing kernels, we can model
the data as a superposition of
independent func-
tions, possibly representing diﬀerent structures. Sup-
pose functions f1, f2 are draw from independent gp
priors, f1 ∼ GP(µ1, k1), f2 ∼ GP(µ2, k2). Then
f := f1 + f2 ∼ GP(µ1 + µ2, k1 + k2).

In time series models, sums of kernels can express su-
perposition of diﬀerent processes, possibly operating
at diﬀerent scales. In multiple dimensions, summing
kernels gives additive structure over diﬀerent dimen-
sions, similar to generalized additive models (Hastie
& Tibshirani, 1990). These two kinds of structure are
demonstrated in rows 2 and 4 of ﬁgure 2, respectively.

Multiplication Multiplying kernels allows us to ac-
count for interactions between diﬀerent input dimen-
sions or diﬀerent notions of similarity. For instance,
in multidimensional data, the multiplicative kernel
SE1 × SE3 represents a smoothly varying function of
dimensions 1 and 3 which is not constrained to be

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

additive. In univariate data, multiplying a kernel by
SE gives a way of converting global structure to local
structure. For example, Per corresponds to globally
periodic structure, whereas Per × SE corresponds to
locally periodic structure, as shown in row 1 of ﬁgure 2.

Many architectures for learning complex functions,
such as convolutional networks (LeCun et al., 1989)
and sum-product networks (Poon & Domingos, 2011),
include units which compute AND-like and OR-like
operations. Composite kernels can be viewed in this
way too. A sum of kernels can be understood as an
OR-like operation: two points are considered similar
if either kernel has a high value. Similarly, multiply-
ing kernels is an AND-like operation, since two points
are considered similar only if both kernels have high
values. Since we are applying these operations to the
similarity functions rather than the regression func-
tions themselves, compositions of even a few base ker-
nels are able to capture complex relationships in data
which do not have a simple parametric form.

Example expressions
In addition to the examples
given in Figure 2, many common motifs of supervised
learning can be captured using sums and products of
one-dimensional base kernels:

Lin
Lin × Lin × . . .

Bayesian linear regression
Bayesian polynomial regression
Generalized Fourier decomposition Per + Per + . . .
(cid:80)D
Generalized additive models
Automatic relevance determination (cid:81)D
Linear trend with local deviations
Linearly growing amplitude

SEd
SEd
d=1
Lin + SE
Lin × SE

d=1

We use the term ‘generalized Fourier decomposition’
to express that the periodic functions expressible by a
gp with a periodic kernel are not limited to sinusoids.

3. Searching over structures

(1) Any subexpression S can be replaced with S + B,

where B is any base kernel family.

(2) Any subexpression S can be replaced with S × B,

where B is any base kernel family.

(3) Any base kernel B may be replaced with any other

base kernel family B(cid:48).

These operators can generate all possible algebraic ex-
pressions. To see this, observe that if we restricted
the + and × rules only to apply to base kernel fam-
ilies, we would obtain a context-free grammar (CFG)
which generates the set of algebraic expressions. How-
ever, the more general versions of these rules allow
more ﬂexibility in the search procedure, which is use-
ful because the CFG derivation may not be the most
straightforward way to arrive at a kernel family.

Our algorithm searches over this space using a greedy
search: at each stage, we choose the highest scoring
kernel and expand it by applying all possible operators.

Our search operators are motivated by strategies re-
searchers often use to construct kernels. In particular,

• One can look for structure, e.g. periodicity, in the
residuals of a model, and then extend the model
to capture that structure. This corresponds to
applying rule (1).

• One can start with structure, e.g. linearity, which
is assumed to hold globally, but ﬁnd that it only
holds locally. This corresponds to applying rule
(2) to obtain the structure shown in rows 1 and 3
of ﬁgure 2.

• One can add features incrementally, analogous to
algorithms like boosting, backﬁtting, or forward
selection. This corresponds to applying rules (1)
or (2) to dimensions not yet included in the model.

As discussed above, we can construct a wide variety of
kernel structures compositionally by adding and mul-
tiplying a small number of base kernels. In particular,
we consider the four base kernel families discussed in
Section 2: SE, Per, Lin, and RQ. Any algebraic ex-
pression combining these kernels using the operations
+ and × deﬁnes a kernel family, whose parameters are
the concatenation of the parameters for the base kernel
families.

Our search procedure begins by proposing all base ker-
nel families applied to all input dimensions. We allow
the following search operators over our set of expres-
sions:

Scoring kernel
families Choosing kernel struc-
tures requires a criterion for evaluating structures. We
choose marginal likelihood as our criterion, since it bal-
ances the ﬁt and complexity of a model (Rasmussen &
Ghahramani, 2001). Conditioned on kernel parame-
ters, the marginal likelihood of a gp can be computed
analytically. However, to evaluate a kernel family we
must integrate over kernel parameters. We approxi-
mate this intractable integral with the Bayesian infor-
mation criterion (Schwarz, 1978) after ﬁrst optimizing
to ﬁnd the maximum-likelihood kernel parameters.

Unfortunately, optimizing over parameters is not a
convex optimization problem, and the space can have

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

many local optima. For example, in data with pe-
riodic structure, integer multiples of the true period
(i.e. harmonics) are often local optima. To alleviate
this diﬃculty, we take advantage of our search proce-
dure to provide reasonable initializations: all of the
parameters which were part of the previous kernel are
initialized to their previous values. All parameters are
then optimized using conjugate gradients, randomly
restarting the newly introduced parameters. This pro-
cedure is not guaranteed to ﬁnd the global optimum,
but it implements the commonly used heuristic of it-
eratively modeling residuals.

4. Related Work

Nonparametric regression in high dimensions
Nonparametric regression methods such as splines, lo-
cally weighted regression, and gp regression are pop-
ular because they are capable of learning arbitrary
smooth functions of the data. Unfortunately, they suf-
fer from the curse of dimensionality: it is very diﬃcult
for the basic versions of these methods to generalize
well in more than a few dimensions. Applying non-
parametric methods in high-dimensional spaces can
require imposing additional structure on the model.

One such structure is additivity. Generalized addi-
tive models (GAM) assume the regression function is a
transformed sum of functions deﬁned on the individual
dimensions: E[f (x)] = g−1((cid:80)D
d=1 fd(xd)). These mod-
els have a limited compositional form, but one which is
interpretable and often generalizes well. In our gram-
mar, we can capture analogous structure through sums
of base kernels along diﬀerent dimensions.

It is possible to add more ﬂexibility to additive mod-
els by considering higher-order interactions between
diﬀerent dimensions. Additive Gaussian processes
(Duvenaud et al., 2011) are a gp model whose ker-
nel implicitly sums over all possible products of one-
dimensional base kernels. Plate (1999) constructs a gp
with a composite kernel, summing an SE kernel along
each dimension, with an SE-ARD kernel (i.e. a prod-
uct of SE over all dimensions). Both of these models
can be expressed in our grammar.

A closely related procedure is
smoothing-splines
ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-
ear combinations of splines along each dimension, all
pairs of dimensions, and possibly higher-order com-
binations. Because the number of terms to consider
grows exponentially in the order,
in practice, only
terms of ﬁrst and second order are usually considered.

Semiparametric regression (e.g. Ruppert et al., 2003)
attempts to combine interpretability with ﬂexibility by

building a composite model out of an interpretable,
parametric part (such as linear regression) and a
‘catch-all’ nonparametric part (such as a gp with an
SE kernel). In our approach, this can be represented
as a sum of SE and Lin.

Kernel learning There is a large body of work at-
tempting to construct a rich kernel through a weighted
sum of base kernels (e.g. Christoudias et al., 2009;
Bach, 2009). While these approaches ﬁnd the optimal
solution in polynomial time, speed comes at a cost: the
component kernels, as well as their hyperparameters,
must be speciﬁed in advance.

Another approach to kernel learning is to learn an em-
bedding of the data points. Lawrence (2005) learns an
embedding of the data into a low-dimensional space,
and constructs a ﬁxed kernel structure over that space.
This model is typically used in unsupervised tasks and
requires an expensive integration or optimisation over
potential embeddings when generalizing to test points.
Salakhutdinov & Hinton (2008) use a deep neural net-
work to learn an embedding; this is a ﬂexible approach
to kernel learning but relies upon ﬁnding structure in
the input density, p(x). Instead we focus on domains
where most of the interesting structure is in f(x).

Wilson & Adams (2013) derive kernels of the form
SE × cos(x − x(cid:48)), forming a basis for stationary ker-
nels. These kernels share similarities with SE × Per
but can express negative prior correlation, and could
usefully be included in our grammar.

Diosan et al. (2007) and Bing et al. (2010) learn com-
posite kernels for support vector machines and rel-
evance vector machines, using genetic search algo-
rithms. Our work employs a Bayesian search criterion,
and goes beyond this prior work by demonstrating the
interpretability of the structure implied by composite
kernels, and how such structure allows for extrapola-
tion.

Structure discovery There have been several at-
tempts to uncover the structural form of a dataset by
searching over a grammar of structures. For example,
(Schmidt & Lipson, 2009), (Todorovski & Dzeroski,
1997) and (Washio et al., 1999) attempt to learn para-
metric forms of equations to describe time series, or
relations between quantities. Because we learn expres-
sions describing the covariance structure rather than
the functions themselves, we are able to capture struc-
ture which does not have a simple parametric form.

Kemp & Tenenbaum (2008) learned the structural
form of a graph used to model human similarity judg-
ments. Examples of graphs included planes, trees, and

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

cylinders. Some of their discrete graph structures have
continous analogues in our own space; e.g. SE1 × SE2
and SE1 × Per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

Grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. This model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. Our work is tackling a similar problem, but in a
supervised setting.

5. Structure discovery in time series

To investigate our method’s ability to discover struc-
ture, we ran the kernel search on several time-series.

As discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. This provides another method
of visualizing the learned structures. In particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. For example,

SE × (RQ + Lin) = SE × RQ + SE × Lin.

We visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. The
search was run to depth 10, using the base kernels from
Section 2.

Mauna Loa atmospheric CO2 Using our method,
we analyzed records of carbon dioxide levels recorded
at the Mauna Loa observatory. Since this dataset was
analyzed in detail by Rasmussen & Williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

=

+

+

+

Figure 3. Posterior mean and variance for diﬀerent depths
of kernel search. The dashed line marks the extent of the
dataset. In the ﬁrst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
Next, a periodic component is added, and the extrapolation
improves. At depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

Figure 4. First row: The posterior on the Mauna Loa
dataset, after a search of depth 10. Subsequent rows show
the automatic decomposition of the time series. The de-
compositions shows long-term, yearly periodic, medium-
term anomaly components, and residuals, respectively. In
the third row, the scale has been changed in order to clearly
show the yearly periodic structure.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

=

+

+

+

Figure 5. Full posterior and residuals on the solar irradi-
ance dataset.

Figure 3 shows the posterior mean and variance on
this dataset as the search depth increases. While the
data can be smoothly interpolated by a single base
kernel model, the extrapolations improve dramatically
as the increased search depth allows more structure to
be included.

Figure 4 shows the ﬁnal model chosen by our method,
together with its decomposition into additive compo-
nents. The ﬁnal model exhibits both plausible ex-
trapolation and interpretable components: a long-
term trend, annual periodicity and medium-term devi-
ations; the same components chosen by Rasmussen &
Williams (2006). We also plot the residuals, observing
that there is little obvious structure left in the data.

Airline passenger data Figure 6 shows the decom-
position produced by applying our method to monthly
totals of international airline passengers (Box et al.,
1976). We observe similar components to the pre-
vious dataset: a long term trend, annual periodicity
and medium-term deviations.
In addition, the com-
posite kernel captures the near-linearity of the long-
term trend, and the linearly growing amplitude of the
annual oscillations.

Solar irradiance Data Finally, we analyzed annual
solar irradiation data from 1610 to 2011 (Lean et al.,
1995). The posterior and residuals of the learned ker-
nel are shown in ﬁgure 5.

Figure 6. First row: The airline dataset and posterior after
a search of depth 10. Subsequent rows: Additive decom-
position of posterior into long-term smooth trend, yearly
variation, and short-term deviations. Due to the linear ker-
nel, the marginal variance grows over time, making this a
heteroskedastic model.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

None of the models in our search space are capable
of parsimoniously representing the lack of variation
from 1645 to 1715. Despite this, our approach fails
gracefully: the learned kernel still captures the peri-
odic structure, and the quickly growing posterior vari-
ance demonstrates that the model is uncertain about
long term structure.

6. Validation on synthetic data

We validated our method’s ability to recover known
structure on a set of synthetic datasets. For several
composite kernel expressions, we constructed synthetic
data by ﬁrst sampling 300 points uniformly at random,
then sampling function values at those points from a
gp prior. We then added i.i.d. Gaussian noise to the
functions, at various signal-to-noise ratios (SNR).

Table 1 lists the true kernels we used to generate the
data. Subscripts indicate which dimension each kernel
was applied to. Subsequent columns show the dimen-
sionality D of the input space, and the kernels chosen
by our search for diﬀerent SNRs. Dashes - indicate
that no kernel had a higher marginal likelihood than
modeling the data as i.i.d. Gaussian noise.

For the highest SNR, the method ﬁnds all relevant
structure in all but one test. The reported additional
linear structure is explainable by the fact that func-
tions sampled from SE kernels with long length scales
occasionally have near-linear trends. As the noise
increases, our method generally backs oﬀ to simpler
structures.

7. Quantitative evaluation

In addition to the qualitative evaluation in section 5,
we investigated quantitatively how our method per-
forms on both extrapolation and interpolation tasks.

7.1. Extrapolation

We compared the extrapolation capabilities of our
model against standard baselines2. Dividing the air-
line dataset into contiguous training and test sets, we
computed the predictive mean-squared-error (MSE) of
each method. We varied the size of the training set
from the ﬁrst 10% to the ﬁrst 90% of the data.

Figure 7 shows the learning curves of linear regres-
sion, a variety of ﬁxed kernel family gp models, and
our method. gp models with only SE and Per ker-
nels did not capture the long-term trends, since the
best parameter values in terms of gp marginal like-
lihood only capture short term structure. Linear re-
gression approximately captured the long-term trend,

airline
Figure 7. Extrapolation
dataset. We plot test-set MSE as a function of the fraction
of the dataset used for training.

performance

the

on

but quickly plateaued in predictive performance. The
more richly structured gp models (SE + Per and
SE × Per) eventually captured more structure and
performed better, but the full structures discovered
by our search outperformed the other approaches in
terms of predictive performance for all data amounts.

7.2. High-dimensional prediction

To evaluate the predictive accuracy of our method in
a high-dimensional setting, we extended the compari-
son of (Duvenaud et al., 2011) to include our method.
We performed 10 fold cross validation on 5 datasets 3
comparing 5 methods in terms of MSE and predictive
likelihood. Our structure search was run up to depth
10, using the SE and RQ base kernel families.

The comparison included three methods with ﬁxed
kernel families: Additive gps, Generalized Additive
Models (GAM), and a gp with a standard SE kernel
using Automatic Relevance Determination (gp SE-
ARD). Also included was the related kernel-search
method of Hierarchical Kernel Learning (HKL).

Results are presented in table 2. Our method outper-
formed the next-best method in each test, although
not substantially.

All gp hyperparameter tuning was performed by au-

2 In one dimension, the predictive means of all baseline
methods in table 2 are identical to that of a gp with an SE
kernel.

3The data sets had dimensionalities ranging from 4 to
13, and the number of data points ranged from 150 to 450.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

Table 1. Kernels chosen by our method on synthetic data generated using known kernel structures. D denotes the
dimension of the functions being modeled. SNR indicates the signal-to-noise ratio. Dashes - indicate no structure.

True Kernel
SE + RQ
Lin × Per
SE1 + RQ
2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 + SE4)

D
1
1
2
3
4
4
4

SNR = 10
SE
Lin × Per
SE1 + SE2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 × Lin3 × Lin1 + SE4)

SNR = 1
SE × Per
Lin × Per
Lin1 + SE2
SE2 × Per1 + SE3
Lin1 × SE2
SE1 + SE2 × SE3
(SE1 + SE2) × SE3 × SE4

SNR = 0.1
SE
SE
Lin1
-
Lin2
SE1
-

Table 2. Comparison of multidimensional regression performance. Bold results are not signiﬁcantly diﬀerent from the
best-performing method in each experiment, in a paired t-test with a p-value of 5%.

Method
Linear Regression
GAM
HKL
gp SE-ARD
gp Additive
Structure Search

bach
1.031
1.259
0.199
0.045
0.045
0.044

Mean Squared Error (MSE)
concrete puma
0.641
0.404
0.598
0.149
0.346
0.147
0.317
0.157
0.316
0.089
0.315
0.087

servo
0.523
0.281
0.199
0.126
0.110
0.102

housing bach
2.430
0.289
1.708
0.161
-
0.151
−0.131 0.398
0.092
−0.131 0.114
0.102
−0.141 0.065
0.082

Negative Log-Likelihood
servo
1.678
0.800
-
0.429
0.309
0.265

concrete puma
1.881
1.403
1.195
0.467
-
-
0.843
0.841
0.840

housing
1.052
0.457
-
0.207
0.194
0.059

tomated calls to the GPML toolbox4; Python code to
perform all experiments is available on github5.

ric regression and classiﬁcation methods accessible to
non-experts.

8. Discussion

“It would be very nice to have a formal
apparatus that gives us some ‘optimal’ way of
recognizing unusual phenomena and invent-
ing new classes of hypotheses that are most
likely to contain the true one; but this re-
mains an art for the creative human mind.”
E. T. Jaynes, 1985

Towards the goal of automating the choice of kernel
family, we introduced a space of composite kernels de-
ﬁned compositionally as sums and products of a small
number of base kernels. The set of models included in
this space includes many standard regression models.
We proposed a search procedure for this space of ker-
nels which parallels the process of scientiﬁc discovery.

We found that the learned structures are often capa-
ble of accurate extrapolation in complex time-series
datasets, and are competitive with widely used kernel
classes and kernel combination methods on a variety
of prediction tasks. The learned kernels often yield de-
compositions of a signal into diverse and interpretable
components, enabling model-checking by humans. We
believe that a data-driven approach to choosing kernel
structures automatically can help make nonparamet-

4Available at www.gaussianprocess.org/gpml/code/
5 github.com/jamesrobertlloyd/gp-structure-search

Acknowledgements

We thank Carl Rasmussen and Andrew G. Wilson for
helpful discussions. This work was funded in part by
NSERC, EPSRC grant EP/I036575/1, and Google.

Appendix

Kernel deﬁnitions For scalar-valued inputs, the
squared exponential (SE), periodic (Per),
linear
(Lin), and rational quadratic (RQ) kernels are deﬁned
as follows:

kSE(x, x(cid:48)) =

kPer(x, x(cid:48)) = σ2 exp

kLin(x, x(cid:48)) =

kRQ(x, x(cid:48)) =

σ2 exp
(cid:16)

(cid:17)

(cid:16)

(cid:17)

− (x−x(cid:48))2
2(cid:96)2
− 2 sin2(π(x−x(cid:48))/p)
(cid:96)2
v(x − (cid:96))(x(cid:48) − (cid:96))
(cid:17)−α
1 + (x−x(cid:48))2
2α(cid:96)2

σ2
b + σ2
σ2 (cid:16)

Posterior decomposition We can analytically de-
compose a gp posterior distribution over additive com-
ponents using the following identity: The conditional
distribution of a Gaussian vector f1 conditioned on its
sum with another Gaussian vector f = f1 + f2 where
f1 ∼ N (µ1, K1) and f2 ∼ N (µ2, K2) is given by

f1|f ∼ N (cid:0)µ1 + K1
K1 − K1

T(K1 + K2)−1 (f − µ1 − µ2) ,
T(K1 + K2)−1K1

(cid:1).

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

References

Bach, F. Exploring large feature spaces with hierarchi-
cal multiple kernel learning. In Advances in Neural
Information Processing Systems, pp. 105–112. 2009.

Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization
method for RVM.
In International Conference on
Computer and Automation Engineering (ICCAE),
volume 4, pp. 419–423, 2010.

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time

series analysis: forecasting and control. 1976.

Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-
nical report, EECS Department, University of Cali-
fornia, Berkeley, 2009.

Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving
kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007, pp.
19–24. IEEE, 2007.

Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-
In Advances in Neural

ditive Gaussian processes.
Information Processing Systems, 2011.

Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-
plore a large space of model structures. In Uncer-
tainty in Artiﬁcial Intelligence, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989.

Plate, T.A. Accuracy versus interpretability in ﬂexible
modeling: Implementing a tradeoﬀ using Gaussian
process models. Behaviormetrika, 26:29–50, 1999.
ISSN 0385-7417.

Poon, H. and Domingos, P. Sum-product networks:
a new deep architecture. In Conference on Uncer-
tainty in AI, 2011.

Rasmussen, C.E. and Ghahramani, Z. Occam’s razor.
In Advances in Neural Information Processing Sys-
tems, 2001.

Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-
cesses for Machine Learning. The MIT Press, Cam-
bridge, MA, USA, 2006.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-
metric regression, volume 12. Cambridge University
Press, 2003.

Salakhutdinov, R. and Hinton, G. Using deep belief
nets to learn covariance kernels for Gaussian pro-
cesses. Advances in Neural information processing
systems, 20:1249–1256, 2008.

Schmidt, M. and Lipson, H. Distilling free-form natu-
ral laws from experimental data. Science, 324(5923):
81–85, 2009.

Gu, C. Smoothing spline ANOVA models. Springer

Schwarz, G. Estimating the dimension of a model. The

Verlag, 2002. ISBN 0387953531.

Annals of Statistics, 6(2):461–464, 1978.

Hastie, T.J. and Tibshirani, R.J. Generalized additive

models. Chapman & Hall/CRC, 1990.

Jaynes, E. T. Highly informative priors. In Proceedings
of the Second International Meeting on Bayesian
Statistics, 1985.

Kemp, C. and Tenenbaum, J.B.

The discovery
of structural form. Proceedings of the National
Academy of Sciences, 105(31):10687–10692, 2008.

Lawrence, N. Probabilistic non-linear principal com-
ponent analysis with gaussian process latent variable
models. The Journal of Machine Learning Research,
6:1783–1816, 2005.

Lean, J., Beer, J., and Bradley, R. Reconstruction of
solar irradiance since 1610: Implications for climate
change. Geophysical Research Letters, 22(23):3195–
3198, 1995.

Todorovski, L. and Dzeroski, S. Declarative bias in
equation discovery. In International Conference on
Machine Learning, pp. 376–384, 1997.

Wahba, G.

Spline models for observational data.
ISBN

Society for Industrial Mathematics, 1990.
0898712440.

Washio, T., Motoda, H., Niwa, Y., et al. Discover-
ing admissible model equations from observed data
based on scale-types and identity constraints.
In
International Joint Conference On Artiﬁcal Intelli-
gence, volume 16, pp. 772–779, 1999.

Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern
discovery and extrapolation.
Technical Report
arXiv:1302.4245 [stat.ML], February 2013.

Structure Discovery in Nonparametric Regression through
Compositional Kernel Search

3
1
0
2
 
y
a
M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
2
9
4
.
2
0
3
1
:
v
i
X
r
a

David Duvenaud∗†
James Robert Lloyd∗†
Roger Grosse‡
Joshua B. Tenenbaum‡
Zoubin Ghahramani†

Abstract
Despite its importance, choosing the struc-
tural form of the kernel
in nonparametric
regression remains a black art. We deﬁne
a space of kernel structures which are built
compositionally by adding and multiplying a
small number of base kernels. We present a
method for searching over this space of struc-
tures which mirrors the scientiﬁc discovery
process. The learned structures can often
decompose functions into interpretable com-
ponents and enable long-range extrapolation
on time-series datasets. Our structure search
method outperforms many widely used ker-
nels and kernel combination methods on a
variety of prediction tasks.

1. Introduction

Kernel-based nonparametric models, such as support
vector machines and Gaussian processes (gps), have
been one of the dominant paradigms for supervised
machine learning over the last 20 years. These meth-
ods depend on deﬁning a kernel function, k(x, x(cid:48)),
which speciﬁes how similar or correlated outputs y and
y(cid:48) are expected to be at two inputs x and x(cid:48). By deﬁn-
ing the measure of similarity between inputs, the ker-
nel determines the pattern of inductive generalization.

Most existing techniques pose kernel
learning as
a (possibly high-dimensional) parameter estimation
problem. Examples include learning hyperparameters
(Rasmussen & Williams, 2006), linear combinations of
ﬁxed kernels (Bach, 2009), and mappings from the in-
put space to an embedding space (Salakhutdinov &
Hinton, 2008).

‡MIT. Proceedings
†U. Cambridge.
∗Equal contribution.
of the 30 th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28.

dkd23@cam.ac.uk
jrl44@cam.ac.uk
rgrosse@mit.edu
jbt@mit.edu
zoubin@eng.cam.ac.uk

However, to apply existing kernel learning algorithms,
the user must specify the parametric form of the ker-
nel, and this can require considerable expertise, as well
as trial and error.

To make kernel learning more generally applicable, we
reframe the kernel learning problem as one of structure
discovery, and automate the choice of kernel form. In
particular, we formulate a space of kernel structures
deﬁned compositionally in terms of sums and prod-
ucts of a small number of base kernel structures. This
provides an expressive modeling language which con-
cisely captures many widely used techniques for con-
structing kernels. We focus on Gaussian process re-
gression, where the kernel speciﬁes a covariance func-
tion, because the Bayesian framework is a convenient
way to formalize structure discovery. Borrowing dis-
crete search techniques which have proved successful in
equation discovery (Todorovski & Dzeroski, 1997) and
unsupervised learning (Grosse et al., 2012), we auto-
matically search over this space of kernel structures
using marginal likelihood as the search criterion.

We found that our structure discovery algorithm is
able to automatically recover known structures from
synthetic data as well as plausible structures for a va-
riety of real-world datasets. On a variety of time series
datasets, the learned kernels yield decompositions of
the unknown function into interpretable components
that enable accurate extrapolation beyond the range
of the observations. Furthermore, the automatically
discovered kernels outperform a variety of widely used
kernel classes and kernel combination methods on su-
pervised prediction tasks.

While we focus on Gaussian process regression, we be-
lieve our kernel search method can be extended to
other supervised learning frameworks such as classi-
ﬁcation or ordinal regression, or to other kinds of ker-
nel architectures such as kernel SVMs. We hope that
the algorithm developed in this paper will help replace

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

the current and often opaque art of kernel engineering
with a more transparent science of automated kernel
construction.

scripts, e.g. SE2 represents an SE kernel over the
second dimension of x.

2. Expressing structure through kernels

Gaussian process models use a kernel to deﬁne
the covariance between any two function values:
Cov(y, y(cid:48)) = k(x, x(cid:48)). The kernel speciﬁes which struc-
tures are likely under the gp prior, which in turn de-
termines the generalization properties of the model. In
this section, we review the ways in which kernel fam-
ilies1can be composed to express diverse priors over
functions.

There has been signiﬁcant work on constructing gp
kernels and analyzing their properties, summarized in
Chapter 4 of (Rasmussen & Williams, 2006). Com-
monly used kernels families include the squared expo-
nential (SE), periodic (Per), linear (Lin), and ratio-
nal quadratic (RQ) (see Figure 1 and the appendix).

Lin × Lin

quadratic
functions

SE × Per

locally
periodic

Lin + Per

periodic
with trend

SE + Per

periodic
with noise

Lin × SE

increasing
variation

Lin × Per

growing
amplitude

Squared-
exp (SE)

local
variation

Periodic
(Per)

repeating
structure

SE1 + SE2

f1(x1)
+f2(x2)

SE1 × SE2

f (x1, x2)

Linear
(Lin)

linear
functions

Rational-
quadratic(RQ)

multi-scale
variation

Figure 1. Left and third columns: base kernels k(·, 0). Sec-
ond and fourth columns: draws from a gp with each repec-
tive kernel. The x-axis has the same range on all plots.

Composing Kernels Positive semideﬁnite kernels
(i.e. those which deﬁne valid covariance functions) are
closed under addition and multiplication. This allows
one to create richly structured and interpretable ker-
nels from well understood base components.

All of the base kernels we use are one-dimensional; ker-
nels over multidimensional inputs are constructed by
adding and multiplying kernels over individual dimen-
sions. These dimensions are represented using sub-

1When unclear from context, we use ‘kernel family’ to
refer to the parametric forms of the functions given in the
appendix. A kernel is a kernel family with all of the pa-
rameters speciﬁed.

Figure 2. Examples of structures expressible by composite
kernels. Left column and third columns: composite kernels
k(·, 0). Plots have same meaning as in Figure 1.

Summation By summing kernels, we can model
the data as a superposition of
independent func-
tions, possibly representing diﬀerent structures. Sup-
pose functions f1, f2 are draw from independent gp
priors, f1 ∼ GP(µ1, k1), f2 ∼ GP(µ2, k2). Then
f := f1 + f2 ∼ GP(µ1 + µ2, k1 + k2).

In time series models, sums of kernels can express su-
perposition of diﬀerent processes, possibly operating
at diﬀerent scales. In multiple dimensions, summing
kernels gives additive structure over diﬀerent dimen-
sions, similar to generalized additive models (Hastie
& Tibshirani, 1990). These two kinds of structure are
demonstrated in rows 2 and 4 of ﬁgure 2, respectively.

Multiplication Multiplying kernels allows us to ac-
count for interactions between diﬀerent input dimen-
sions or diﬀerent notions of similarity. For instance,
in multidimensional data, the multiplicative kernel
SE1 × SE3 represents a smoothly varying function of
dimensions 1 and 3 which is not constrained to be

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

additive. In univariate data, multiplying a kernel by
SE gives a way of converting global structure to local
structure. For example, Per corresponds to globally
periodic structure, whereas Per × SE corresponds to
locally periodic structure, as shown in row 1 of ﬁgure 2.

Many architectures for learning complex functions,
such as convolutional networks (LeCun et al., 1989)
and sum-product networks (Poon & Domingos, 2011),
include units which compute AND-like and OR-like
operations. Composite kernels can be viewed in this
way too. A sum of kernels can be understood as an
OR-like operation: two points are considered similar
if either kernel has a high value. Similarly, multiply-
ing kernels is an AND-like operation, since two points
are considered similar only if both kernels have high
values. Since we are applying these operations to the
similarity functions rather than the regression func-
tions themselves, compositions of even a few base ker-
nels are able to capture complex relationships in data
which do not have a simple parametric form.

Example expressions
In addition to the examples
given in Figure 2, many common motifs of supervised
learning can be captured using sums and products of
one-dimensional base kernels:

Lin
Lin × Lin × . . .

Bayesian linear regression
Bayesian polynomial regression
Generalized Fourier decomposition Per + Per + . . .
(cid:80)D
Generalized additive models
Automatic relevance determination (cid:81)D
Linear trend with local deviations
Linearly growing amplitude

SEd
SEd
d=1
Lin + SE
Lin × SE

d=1

We use the term ‘generalized Fourier decomposition’
to express that the periodic functions expressible by a
gp with a periodic kernel are not limited to sinusoids.

3. Searching over structures

(1) Any subexpression S can be replaced with S + B,

where B is any base kernel family.

(2) Any subexpression S can be replaced with S × B,

where B is any base kernel family.

(3) Any base kernel B may be replaced with any other

base kernel family B(cid:48).

These operators can generate all possible algebraic ex-
pressions. To see this, observe that if we restricted
the + and × rules only to apply to base kernel fam-
ilies, we would obtain a context-free grammar (CFG)
which generates the set of algebraic expressions. How-
ever, the more general versions of these rules allow
more ﬂexibility in the search procedure, which is use-
ful because the CFG derivation may not be the most
straightforward way to arrive at a kernel family.

Our algorithm searches over this space using a greedy
search: at each stage, we choose the highest scoring
kernel and expand it by applying all possible operators.

Our search operators are motivated by strategies re-
searchers often use to construct kernels. In particular,

• One can look for structure, e.g. periodicity, in the
residuals of a model, and then extend the model
to capture that structure. This corresponds to
applying rule (1).

• One can start with structure, e.g. linearity, which
is assumed to hold globally, but ﬁnd that it only
holds locally. This corresponds to applying rule
(2) to obtain the structure shown in rows 1 and 3
of ﬁgure 2.

• One can add features incrementally, analogous to
algorithms like boosting, backﬁtting, or forward
selection. This corresponds to applying rules (1)
or (2) to dimensions not yet included in the model.

As discussed above, we can construct a wide variety of
kernel structures compositionally by adding and mul-
tiplying a small number of base kernels. In particular,
we consider the four base kernel families discussed in
Section 2: SE, Per, Lin, and RQ. Any algebraic ex-
pression combining these kernels using the operations
+ and × deﬁnes a kernel family, whose parameters are
the concatenation of the parameters for the base kernel
families.

Our search procedure begins by proposing all base ker-
nel families applied to all input dimensions. We allow
the following search operators over our set of expres-
sions:

Scoring kernel
families Choosing kernel struc-
tures requires a criterion for evaluating structures. We
choose marginal likelihood as our criterion, since it bal-
ances the ﬁt and complexity of a model (Rasmussen &
Ghahramani, 2001). Conditioned on kernel parame-
ters, the marginal likelihood of a gp can be computed
analytically. However, to evaluate a kernel family we
must integrate over kernel parameters. We approxi-
mate this intractable integral with the Bayesian infor-
mation criterion (Schwarz, 1978) after ﬁrst optimizing
to ﬁnd the maximum-likelihood kernel parameters.

Unfortunately, optimizing over parameters is not a
convex optimization problem, and the space can have

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

many local optima. For example, in data with pe-
riodic structure, integer multiples of the true period
(i.e. harmonics) are often local optima. To alleviate
this diﬃculty, we take advantage of our search proce-
dure to provide reasonable initializations: all of the
parameters which were part of the previous kernel are
initialized to their previous values. All parameters are
then optimized using conjugate gradients, randomly
restarting the newly introduced parameters. This pro-
cedure is not guaranteed to ﬁnd the global optimum,
but it implements the commonly used heuristic of it-
eratively modeling residuals.

4. Related Work

Nonparametric regression in high dimensions
Nonparametric regression methods such as splines, lo-
cally weighted regression, and gp regression are pop-
ular because they are capable of learning arbitrary
smooth functions of the data. Unfortunately, they suf-
fer from the curse of dimensionality: it is very diﬃcult
for the basic versions of these methods to generalize
well in more than a few dimensions. Applying non-
parametric methods in high-dimensional spaces can
require imposing additional structure on the model.

One such structure is additivity. Generalized addi-
tive models (GAM) assume the regression function is a
transformed sum of functions deﬁned on the individual
dimensions: E[f (x)] = g−1((cid:80)D
d=1 fd(xd)). These mod-
els have a limited compositional form, but one which is
interpretable and often generalizes well. In our gram-
mar, we can capture analogous structure through sums
of base kernels along diﬀerent dimensions.

It is possible to add more ﬂexibility to additive mod-
els by considering higher-order interactions between
diﬀerent dimensions. Additive Gaussian processes
(Duvenaud et al., 2011) are a gp model whose ker-
nel implicitly sums over all possible products of one-
dimensional base kernels. Plate (1999) constructs a gp
with a composite kernel, summing an SE kernel along
each dimension, with an SE-ARD kernel (i.e. a prod-
uct of SE over all dimensions). Both of these models
can be expressed in our grammar.

A closely related procedure is
smoothing-splines
ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-
ear combinations of splines along each dimension, all
pairs of dimensions, and possibly higher-order com-
binations. Because the number of terms to consider
grows exponentially in the order,
in practice, only
terms of ﬁrst and second order are usually considered.

Semiparametric regression (e.g. Ruppert et al., 2003)
attempts to combine interpretability with ﬂexibility by

building a composite model out of an interpretable,
parametric part (such as linear regression) and a
‘catch-all’ nonparametric part (such as a gp with an
SE kernel). In our approach, this can be represented
as a sum of SE and Lin.

Kernel learning There is a large body of work at-
tempting to construct a rich kernel through a weighted
sum of base kernels (e.g. Christoudias et al., 2009;
Bach, 2009). While these approaches ﬁnd the optimal
solution in polynomial time, speed comes at a cost: the
component kernels, as well as their hyperparameters,
must be speciﬁed in advance.

Another approach to kernel learning is to learn an em-
bedding of the data points. Lawrence (2005) learns an
embedding of the data into a low-dimensional space,
and constructs a ﬁxed kernel structure over that space.
This model is typically used in unsupervised tasks and
requires an expensive integration or optimisation over
potential embeddings when generalizing to test points.
Salakhutdinov & Hinton (2008) use a deep neural net-
work to learn an embedding; this is a ﬂexible approach
to kernel learning but relies upon ﬁnding structure in
the input density, p(x). Instead we focus on domains
where most of the interesting structure is in f(x).

Wilson & Adams (2013) derive kernels of the form
SE × cos(x − x(cid:48)), forming a basis for stationary ker-
nels. These kernels share similarities with SE × Per
but can express negative prior correlation, and could
usefully be included in our grammar.

Diosan et al. (2007) and Bing et al. (2010) learn com-
posite kernels for support vector machines and rel-
evance vector machines, using genetic search algo-
rithms. Our work employs a Bayesian search criterion,
and goes beyond this prior work by demonstrating the
interpretability of the structure implied by composite
kernels, and how such structure allows for extrapola-
tion.

Structure discovery There have been several at-
tempts to uncover the structural form of a dataset by
searching over a grammar of structures. For example,
(Schmidt & Lipson, 2009), (Todorovski & Dzeroski,
1997) and (Washio et al., 1999) attempt to learn para-
metric forms of equations to describe time series, or
relations between quantities. Because we learn expres-
sions describing the covariance structure rather than
the functions themselves, we are able to capture struc-
ture which does not have a simple parametric form.

Kemp & Tenenbaum (2008) learned the structural
form of a graph used to model human similarity judg-
ments. Examples of graphs included planes, trees, and

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

cylinders. Some of their discrete graph structures have
continous analogues in our own space; e.g. SE1 × SE2
and SE1 × Per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

Grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. This model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. Our work is tackling a similar problem, but in a
supervised setting.

5. Structure discovery in time series

To investigate our method’s ability to discover struc-
ture, we ran the kernel search on several time-series.

As discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. This provides another method
of visualizing the learned structures. In particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. For example,

SE × (RQ + Lin) = SE × RQ + SE × Lin.

We visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. The
search was run to depth 10, using the base kernels from
Section 2.

Mauna Loa atmospheric CO2 Using our method,
we analyzed records of carbon dioxide levels recorded
at the Mauna Loa observatory. Since this dataset was
analyzed in detail by Rasmussen & Williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

=

+

+

+

Figure 3. Posterior mean and variance for diﬀerent depths
of kernel search. The dashed line marks the extent of the
dataset. In the ﬁrst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
Next, a periodic component is added, and the extrapolation
improves. At depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

Figure 4. First row: The posterior on the Mauna Loa
dataset, after a search of depth 10. Subsequent rows show
the automatic decomposition of the time series. The de-
compositions shows long-term, yearly periodic, medium-
term anomaly components, and residuals, respectively. In
the third row, the scale has been changed in order to clearly
show the yearly periodic structure.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

=

+

+

+

Figure 5. Full posterior and residuals on the solar irradi-
ance dataset.

Figure 3 shows the posterior mean and variance on
this dataset as the search depth increases. While the
data can be smoothly interpolated by a single base
kernel model, the extrapolations improve dramatically
as the increased search depth allows more structure to
be included.

Figure 4 shows the ﬁnal model chosen by our method,
together with its decomposition into additive compo-
nents. The ﬁnal model exhibits both plausible ex-
trapolation and interpretable components: a long-
term trend, annual periodicity and medium-term devi-
ations; the same components chosen by Rasmussen &
Williams (2006). We also plot the residuals, observing
that there is little obvious structure left in the data.

Airline passenger data Figure 6 shows the decom-
position produced by applying our method to monthly
totals of international airline passengers (Box et al.,
1976). We observe similar components to the pre-
vious dataset: a long term trend, annual periodicity
and medium-term deviations.
In addition, the com-
posite kernel captures the near-linearity of the long-
term trend, and the linearly growing amplitude of the
annual oscillations.

Solar irradiance Data Finally, we analyzed annual
solar irradiation data from 1610 to 2011 (Lean et al.,
1995). The posterior and residuals of the learned ker-
nel are shown in ﬁgure 5.

Figure 6. First row: The airline dataset and posterior after
a search of depth 10. Subsequent rows: Additive decom-
position of posterior into long-term smooth trend, yearly
variation, and short-term deviations. Due to the linear ker-
nel, the marginal variance grows over time, making this a
heteroskedastic model.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

None of the models in our search space are capable
of parsimoniously representing the lack of variation
from 1645 to 1715. Despite this, our approach fails
gracefully: the learned kernel still captures the peri-
odic structure, and the quickly growing posterior vari-
ance demonstrates that the model is uncertain about
long term structure.

6. Validation on synthetic data

We validated our method’s ability to recover known
structure on a set of synthetic datasets. For several
composite kernel expressions, we constructed synthetic
data by ﬁrst sampling 300 points uniformly at random,
then sampling function values at those points from a
gp prior. We then added i.i.d. Gaussian noise to the
functions, at various signal-to-noise ratios (SNR).

Table 1 lists the true kernels we used to generate the
data. Subscripts indicate which dimension each kernel
was applied to. Subsequent columns show the dimen-
sionality D of the input space, and the kernels chosen
by our search for diﬀerent SNRs. Dashes - indicate
that no kernel had a higher marginal likelihood than
modeling the data as i.i.d. Gaussian noise.

For the highest SNR, the method ﬁnds all relevant
structure in all but one test. The reported additional
linear structure is explainable by the fact that func-
tions sampled from SE kernels with long length scales
occasionally have near-linear trends. As the noise
increases, our method generally backs oﬀ to simpler
structures.

7. Quantitative evaluation

In addition to the qualitative evaluation in section 5,
we investigated quantitatively how our method per-
forms on both extrapolation and interpolation tasks.

7.1. Extrapolation

We compared the extrapolation capabilities of our
model against standard baselines2. Dividing the air-
line dataset into contiguous training and test sets, we
computed the predictive mean-squared-error (MSE) of
each method. We varied the size of the training set
from the ﬁrst 10% to the ﬁrst 90% of the data.

Figure 7 shows the learning curves of linear regres-
sion, a variety of ﬁxed kernel family gp models, and
our method. gp models with only SE and Per ker-
nels did not capture the long-term trends, since the
best parameter values in terms of gp marginal like-
lihood only capture short term structure. Linear re-
gression approximately captured the long-term trend,

airline
Figure 7. Extrapolation
dataset. We plot test-set MSE as a function of the fraction
of the dataset used for training.

performance

the

on

but quickly plateaued in predictive performance. The
more richly structured gp models (SE + Per and
SE × Per) eventually captured more structure and
performed better, but the full structures discovered
by our search outperformed the other approaches in
terms of predictive performance for all data amounts.

7.2. High-dimensional prediction

To evaluate the predictive accuracy of our method in
a high-dimensional setting, we extended the compari-
son of (Duvenaud et al., 2011) to include our method.
We performed 10 fold cross validation on 5 datasets 3
comparing 5 methods in terms of MSE and predictive
likelihood. Our structure search was run up to depth
10, using the SE and RQ base kernel families.

The comparison included three methods with ﬁxed
kernel families: Additive gps, Generalized Additive
Models (GAM), and a gp with a standard SE kernel
using Automatic Relevance Determination (gp SE-
ARD). Also included was the related kernel-search
method of Hierarchical Kernel Learning (HKL).

Results are presented in table 2. Our method outper-
formed the next-best method in each test, although
not substantially.

All gp hyperparameter tuning was performed by au-

2 In one dimension, the predictive means of all baseline
methods in table 2 are identical to that of a gp with an SE
kernel.

3The data sets had dimensionalities ranging from 4 to
13, and the number of data points ranged from 150 to 450.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

Table 1. Kernels chosen by our method on synthetic data generated using known kernel structures. D denotes the
dimension of the functions being modeled. SNR indicates the signal-to-noise ratio. Dashes - indicate no structure.

True Kernel
SE + RQ
Lin × Per
SE1 + RQ
2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 + SE4)

D
1
1
2
3
4
4
4

SNR = 10
SE
Lin × Per
SE1 + SE2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 × Lin3 × Lin1 + SE4)

SNR = 1
SE × Per
Lin × Per
Lin1 + SE2
SE2 × Per1 + SE3
Lin1 × SE2
SE1 + SE2 × SE3
(SE1 + SE2) × SE3 × SE4

SNR = 0.1
SE
SE
Lin1
-
Lin2
SE1
-

Table 2. Comparison of multidimensional regression performance. Bold results are not signiﬁcantly diﬀerent from the
best-performing method in each experiment, in a paired t-test with a p-value of 5%.

Method
Linear Regression
GAM
HKL
gp SE-ARD
gp Additive
Structure Search

bach
1.031
1.259
0.199
0.045
0.045
0.044

Mean Squared Error (MSE)
concrete puma
0.641
0.404
0.598
0.149
0.346
0.147
0.317
0.157
0.316
0.089
0.315
0.087

servo
0.523
0.281
0.199
0.126
0.110
0.102

housing bach
2.430
0.289
1.708
0.161
-
0.151
−0.131 0.398
0.092
−0.131 0.114
0.102
−0.141 0.065
0.082

Negative Log-Likelihood
servo
1.678
0.800
-
0.429
0.309
0.265

concrete puma
1.881
1.403
1.195
0.467
-
-
0.843
0.841
0.840

housing
1.052
0.457
-
0.207
0.194
0.059

tomated calls to the GPML toolbox4; Python code to
perform all experiments is available on github5.

ric regression and classiﬁcation methods accessible to
non-experts.

8. Discussion

“It would be very nice to have a formal
apparatus that gives us some ‘optimal’ way of
recognizing unusual phenomena and invent-
ing new classes of hypotheses that are most
likely to contain the true one; but this re-
mains an art for the creative human mind.”
E. T. Jaynes, 1985

Towards the goal of automating the choice of kernel
family, we introduced a space of composite kernels de-
ﬁned compositionally as sums and products of a small
number of base kernels. The set of models included in
this space includes many standard regression models.
We proposed a search procedure for this space of ker-
nels which parallels the process of scientiﬁc discovery.

We found that the learned structures are often capa-
ble of accurate extrapolation in complex time-series
datasets, and are competitive with widely used kernel
classes and kernel combination methods on a variety
of prediction tasks. The learned kernels often yield de-
compositions of a signal into diverse and interpretable
components, enabling model-checking by humans. We
believe that a data-driven approach to choosing kernel
structures automatically can help make nonparamet-

4Available at www.gaussianprocess.org/gpml/code/
5 github.com/jamesrobertlloyd/gp-structure-search

Acknowledgements

We thank Carl Rasmussen and Andrew G. Wilson for
helpful discussions. This work was funded in part by
NSERC, EPSRC grant EP/I036575/1, and Google.

Appendix

Kernel deﬁnitions For scalar-valued inputs, the
squared exponential (SE), periodic (Per),
linear
(Lin), and rational quadratic (RQ) kernels are deﬁned
as follows:

kSE(x, x(cid:48)) =

kPer(x, x(cid:48)) = σ2 exp

kLin(x, x(cid:48)) =

kRQ(x, x(cid:48)) =

σ2 exp
(cid:16)

(cid:17)

(cid:16)

(cid:17)

− (x−x(cid:48))2
2(cid:96)2
− 2 sin2(π(x−x(cid:48))/p)
(cid:96)2
v(x − (cid:96))(x(cid:48) − (cid:96))
(cid:17)−α
1 + (x−x(cid:48))2
2α(cid:96)2

σ2
b + σ2
σ2 (cid:16)

Posterior decomposition We can analytically de-
compose a gp posterior distribution over additive com-
ponents using the following identity: The conditional
distribution of a Gaussian vector f1 conditioned on its
sum with another Gaussian vector f = f1 + f2 where
f1 ∼ N (µ1, K1) and f2 ∼ N (µ2, K2) is given by

f1|f ∼ N (cid:0)µ1 + K1
K1 − K1

T(K1 + K2)−1 (f − µ1 − µ2) ,
T(K1 + K2)−1K1

(cid:1).

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

References

Bach, F. Exploring large feature spaces with hierarchi-
cal multiple kernel learning. In Advances in Neural
Information Processing Systems, pp. 105–112. 2009.

Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization
method for RVM.
In International Conference on
Computer and Automation Engineering (ICCAE),
volume 4, pp. 419–423, 2010.

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time

series analysis: forecasting and control. 1976.

Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-
nical report, EECS Department, University of Cali-
fornia, Berkeley, 2009.

Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving
kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007, pp.
19–24. IEEE, 2007.

Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-
In Advances in Neural

ditive Gaussian processes.
Information Processing Systems, 2011.

Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-
plore a large space of model structures. In Uncer-
tainty in Artiﬁcial Intelligence, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989.

Plate, T.A. Accuracy versus interpretability in ﬂexible
modeling: Implementing a tradeoﬀ using Gaussian
process models. Behaviormetrika, 26:29–50, 1999.
ISSN 0385-7417.

Poon, H. and Domingos, P. Sum-product networks:
a new deep architecture. In Conference on Uncer-
tainty in AI, 2011.

Rasmussen, C.E. and Ghahramani, Z. Occam’s razor.
In Advances in Neural Information Processing Sys-
tems, 2001.

Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-
cesses for Machine Learning. The MIT Press, Cam-
bridge, MA, USA, 2006.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-
metric regression, volume 12. Cambridge University
Press, 2003.

Salakhutdinov, R. and Hinton, G. Using deep belief
nets to learn covariance kernels for Gaussian pro-
cesses. Advances in Neural information processing
systems, 20:1249–1256, 2008.

Schmidt, M. and Lipson, H. Distilling free-form natu-
ral laws from experimental data. Science, 324(5923):
81–85, 2009.

Gu, C. Smoothing spline ANOVA models. Springer

Schwarz, G. Estimating the dimension of a model. The

Verlag, 2002. ISBN 0387953531.

Annals of Statistics, 6(2):461–464, 1978.

Hastie, T.J. and Tibshirani, R.J. Generalized additive

models. Chapman & Hall/CRC, 1990.

Jaynes, E. T. Highly informative priors. In Proceedings
of the Second International Meeting on Bayesian
Statistics, 1985.

Kemp, C. and Tenenbaum, J.B.

The discovery
of structural form. Proceedings of the National
Academy of Sciences, 105(31):10687–10692, 2008.

Lawrence, N. Probabilistic non-linear principal com-
ponent analysis with gaussian process latent variable
models. The Journal of Machine Learning Research,
6:1783–1816, 2005.

Lean, J., Beer, J., and Bradley, R. Reconstruction of
solar irradiance since 1610: Implications for climate
change. Geophysical Research Letters, 22(23):3195–
3198, 1995.

Todorovski, L. and Dzeroski, S. Declarative bias in
equation discovery. In International Conference on
Machine Learning, pp. 376–384, 1997.

Wahba, G.

Spline models for observational data.
ISBN

Society for Industrial Mathematics, 1990.
0898712440.

Washio, T., Motoda, H., Niwa, Y., et al. Discover-
ing admissible model equations from observed data
based on scale-types and identity constraints.
In
International Joint Conference On Artiﬁcal Intelli-
gence, volume 16, pp. 772–779, 1999.

Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern
discovery and extrapolation.
Technical Report
arXiv:1302.4245 [stat.ML], February 2013.

Structure Discovery in Nonparametric Regression through
Compositional Kernel Search

3
1
0
2
 
y
a
M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
2
9
4
.
2
0
3
1
:
v
i
X
r
a

David Duvenaud∗†
James Robert Lloyd∗†
Roger Grosse‡
Joshua B. Tenenbaum‡
Zoubin Ghahramani†

Abstract
Despite its importance, choosing the struc-
tural form of the kernel
in nonparametric
regression remains a black art. We deﬁne
a space of kernel structures which are built
compositionally by adding and multiplying a
small number of base kernels. We present a
method for searching over this space of struc-
tures which mirrors the scientiﬁc discovery
process. The learned structures can often
decompose functions into interpretable com-
ponents and enable long-range extrapolation
on time-series datasets. Our structure search
method outperforms many widely used ker-
nels and kernel combination methods on a
variety of prediction tasks.

1. Introduction

Kernel-based nonparametric models, such as support
vector machines and Gaussian processes (gps), have
been one of the dominant paradigms for supervised
machine learning over the last 20 years. These meth-
ods depend on deﬁning a kernel function, k(x, x(cid:48)),
which speciﬁes how similar or correlated outputs y and
y(cid:48) are expected to be at two inputs x and x(cid:48). By deﬁn-
ing the measure of similarity between inputs, the ker-
nel determines the pattern of inductive generalization.

Most existing techniques pose kernel
learning as
a (possibly high-dimensional) parameter estimation
problem. Examples include learning hyperparameters
(Rasmussen & Williams, 2006), linear combinations of
ﬁxed kernels (Bach, 2009), and mappings from the in-
put space to an embedding space (Salakhutdinov &
Hinton, 2008).

‡MIT. Proceedings
†U. Cambridge.
∗Equal contribution.
of the 30 th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28.

dkd23@cam.ac.uk
jrl44@cam.ac.uk
rgrosse@mit.edu
jbt@mit.edu
zoubin@eng.cam.ac.uk

However, to apply existing kernel learning algorithms,
the user must specify the parametric form of the ker-
nel, and this can require considerable expertise, as well
as trial and error.

To make kernel learning more generally applicable, we
reframe the kernel learning problem as one of structure
discovery, and automate the choice of kernel form. In
particular, we formulate a space of kernel structures
deﬁned compositionally in terms of sums and prod-
ucts of a small number of base kernel structures. This
provides an expressive modeling language which con-
cisely captures many widely used techniques for con-
structing kernels. We focus on Gaussian process re-
gression, where the kernel speciﬁes a covariance func-
tion, because the Bayesian framework is a convenient
way to formalize structure discovery. Borrowing dis-
crete search techniques which have proved successful in
equation discovery (Todorovski & Dzeroski, 1997) and
unsupervised learning (Grosse et al., 2012), we auto-
matically search over this space of kernel structures
using marginal likelihood as the search criterion.

We found that our structure discovery algorithm is
able to automatically recover known structures from
synthetic data as well as plausible structures for a va-
riety of real-world datasets. On a variety of time series
datasets, the learned kernels yield decompositions of
the unknown function into interpretable components
that enable accurate extrapolation beyond the range
of the observations. Furthermore, the automatically
discovered kernels outperform a variety of widely used
kernel classes and kernel combination methods on su-
pervised prediction tasks.

While we focus on Gaussian process regression, we be-
lieve our kernel search method can be extended to
other supervised learning frameworks such as classi-
ﬁcation or ordinal regression, or to other kinds of ker-
nel architectures such as kernel SVMs. We hope that
the algorithm developed in this paper will help replace

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

the current and often opaque art of kernel engineering
with a more transparent science of automated kernel
construction.

scripts, e.g. SE2 represents an SE kernel over the
second dimension of x.

2. Expressing structure through kernels

Gaussian process models use a kernel to deﬁne
the covariance between any two function values:
Cov(y, y(cid:48)) = k(x, x(cid:48)). The kernel speciﬁes which struc-
tures are likely under the gp prior, which in turn de-
termines the generalization properties of the model. In
this section, we review the ways in which kernel fam-
ilies1can be composed to express diverse priors over
functions.

There has been signiﬁcant work on constructing gp
kernels and analyzing their properties, summarized in
Chapter 4 of (Rasmussen & Williams, 2006). Com-
monly used kernels families include the squared expo-
nential (SE), periodic (Per), linear (Lin), and ratio-
nal quadratic (RQ) (see Figure 1 and the appendix).

Lin × Lin

quadratic
functions

SE × Per

locally
periodic

Lin + Per

periodic
with trend

SE + Per

periodic
with noise

Lin × SE

increasing
variation

Lin × Per

growing
amplitude

Squared-
exp (SE)

local
variation

Periodic
(Per)

repeating
structure

SE1 + SE2

f1(x1)
+f2(x2)

SE1 × SE2

f (x1, x2)

Linear
(Lin)

linear
functions

Rational-
quadratic(RQ)

multi-scale
variation

Figure 1. Left and third columns: base kernels k(·, 0). Sec-
ond and fourth columns: draws from a gp with each repec-
tive kernel. The x-axis has the same range on all plots.

Composing Kernels Positive semideﬁnite kernels
(i.e. those which deﬁne valid covariance functions) are
closed under addition and multiplication. This allows
one to create richly structured and interpretable ker-
nels from well understood base components.

All of the base kernels we use are one-dimensional; ker-
nels over multidimensional inputs are constructed by
adding and multiplying kernels over individual dimen-
sions. These dimensions are represented using sub-

1When unclear from context, we use ‘kernel family’ to
refer to the parametric forms of the functions given in the
appendix. A kernel is a kernel family with all of the pa-
rameters speciﬁed.

Figure 2. Examples of structures expressible by composite
kernels. Left column and third columns: composite kernels
k(·, 0). Plots have same meaning as in Figure 1.

Summation By summing kernels, we can model
the data as a superposition of
independent func-
tions, possibly representing diﬀerent structures. Sup-
pose functions f1, f2 are draw from independent gp
priors, f1 ∼ GP(µ1, k1), f2 ∼ GP(µ2, k2). Then
f := f1 + f2 ∼ GP(µ1 + µ2, k1 + k2).

In time series models, sums of kernels can express su-
perposition of diﬀerent processes, possibly operating
at diﬀerent scales. In multiple dimensions, summing
kernels gives additive structure over diﬀerent dimen-
sions, similar to generalized additive models (Hastie
& Tibshirani, 1990). These two kinds of structure are
demonstrated in rows 2 and 4 of ﬁgure 2, respectively.

Multiplication Multiplying kernels allows us to ac-
count for interactions between diﬀerent input dimen-
sions or diﬀerent notions of similarity. For instance,
in multidimensional data, the multiplicative kernel
SE1 × SE3 represents a smoothly varying function of
dimensions 1 and 3 which is not constrained to be

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

additive. In univariate data, multiplying a kernel by
SE gives a way of converting global structure to local
structure. For example, Per corresponds to globally
periodic structure, whereas Per × SE corresponds to
locally periodic structure, as shown in row 1 of ﬁgure 2.

Many architectures for learning complex functions,
such as convolutional networks (LeCun et al., 1989)
and sum-product networks (Poon & Domingos, 2011),
include units which compute AND-like and OR-like
operations. Composite kernels can be viewed in this
way too. A sum of kernels can be understood as an
OR-like operation: two points are considered similar
if either kernel has a high value. Similarly, multiply-
ing kernels is an AND-like operation, since two points
are considered similar only if both kernels have high
values. Since we are applying these operations to the
similarity functions rather than the regression func-
tions themselves, compositions of even a few base ker-
nels are able to capture complex relationships in data
which do not have a simple parametric form.

Example expressions
In addition to the examples
given in Figure 2, many common motifs of supervised
learning can be captured using sums and products of
one-dimensional base kernels:

Lin
Lin × Lin × . . .

Bayesian linear regression
Bayesian polynomial regression
Generalized Fourier decomposition Per + Per + . . .
(cid:80)D
Generalized additive models
Automatic relevance determination (cid:81)D
Linear trend with local deviations
Linearly growing amplitude

SEd
SEd
d=1
Lin + SE
Lin × SE

d=1

We use the term ‘generalized Fourier decomposition’
to express that the periodic functions expressible by a
gp with a periodic kernel are not limited to sinusoids.

3. Searching over structures

(1) Any subexpression S can be replaced with S + B,

where B is any base kernel family.

(2) Any subexpression S can be replaced with S × B,

where B is any base kernel family.

(3) Any base kernel B may be replaced with any other

base kernel family B(cid:48).

These operators can generate all possible algebraic ex-
pressions. To see this, observe that if we restricted
the + and × rules only to apply to base kernel fam-
ilies, we would obtain a context-free grammar (CFG)
which generates the set of algebraic expressions. How-
ever, the more general versions of these rules allow
more ﬂexibility in the search procedure, which is use-
ful because the CFG derivation may not be the most
straightforward way to arrive at a kernel family.

Our algorithm searches over this space using a greedy
search: at each stage, we choose the highest scoring
kernel and expand it by applying all possible operators.

Our search operators are motivated by strategies re-
searchers often use to construct kernels. In particular,

• One can look for structure, e.g. periodicity, in the
residuals of a model, and then extend the model
to capture that structure. This corresponds to
applying rule (1).

• One can start with structure, e.g. linearity, which
is assumed to hold globally, but ﬁnd that it only
holds locally. This corresponds to applying rule
(2) to obtain the structure shown in rows 1 and 3
of ﬁgure 2.

• One can add features incrementally, analogous to
algorithms like boosting, backﬁtting, or forward
selection. This corresponds to applying rules (1)
or (2) to dimensions not yet included in the model.

As discussed above, we can construct a wide variety of
kernel structures compositionally by adding and mul-
tiplying a small number of base kernels. In particular,
we consider the four base kernel families discussed in
Section 2: SE, Per, Lin, and RQ. Any algebraic ex-
pression combining these kernels using the operations
+ and × deﬁnes a kernel family, whose parameters are
the concatenation of the parameters for the base kernel
families.

Our search procedure begins by proposing all base ker-
nel families applied to all input dimensions. We allow
the following search operators over our set of expres-
sions:

Scoring kernel
families Choosing kernel struc-
tures requires a criterion for evaluating structures. We
choose marginal likelihood as our criterion, since it bal-
ances the ﬁt and complexity of a model (Rasmussen &
Ghahramani, 2001). Conditioned on kernel parame-
ters, the marginal likelihood of a gp can be computed
analytically. However, to evaluate a kernel family we
must integrate over kernel parameters. We approxi-
mate this intractable integral with the Bayesian infor-
mation criterion (Schwarz, 1978) after ﬁrst optimizing
to ﬁnd the maximum-likelihood kernel parameters.

Unfortunately, optimizing over parameters is not a
convex optimization problem, and the space can have

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

many local optima. For example, in data with pe-
riodic structure, integer multiples of the true period
(i.e. harmonics) are often local optima. To alleviate
this diﬃculty, we take advantage of our search proce-
dure to provide reasonable initializations: all of the
parameters which were part of the previous kernel are
initialized to their previous values. All parameters are
then optimized using conjugate gradients, randomly
restarting the newly introduced parameters. This pro-
cedure is not guaranteed to ﬁnd the global optimum,
but it implements the commonly used heuristic of it-
eratively modeling residuals.

4. Related Work

Nonparametric regression in high dimensions
Nonparametric regression methods such as splines, lo-
cally weighted regression, and gp regression are pop-
ular because they are capable of learning arbitrary
smooth functions of the data. Unfortunately, they suf-
fer from the curse of dimensionality: it is very diﬃcult
for the basic versions of these methods to generalize
well in more than a few dimensions. Applying non-
parametric methods in high-dimensional spaces can
require imposing additional structure on the model.

One such structure is additivity. Generalized addi-
tive models (GAM) assume the regression function is a
transformed sum of functions deﬁned on the individual
dimensions: E[f (x)] = g−1((cid:80)D
d=1 fd(xd)). These mod-
els have a limited compositional form, but one which is
interpretable and often generalizes well. In our gram-
mar, we can capture analogous structure through sums
of base kernels along diﬀerent dimensions.

It is possible to add more ﬂexibility to additive mod-
els by considering higher-order interactions between
diﬀerent dimensions. Additive Gaussian processes
(Duvenaud et al., 2011) are a gp model whose ker-
nel implicitly sums over all possible products of one-
dimensional base kernels. Plate (1999) constructs a gp
with a composite kernel, summing an SE kernel along
each dimension, with an SE-ARD kernel (i.e. a prod-
uct of SE over all dimensions). Both of these models
can be expressed in our grammar.

A closely related procedure is
smoothing-splines
ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-
ear combinations of splines along each dimension, all
pairs of dimensions, and possibly higher-order com-
binations. Because the number of terms to consider
grows exponentially in the order,
in practice, only
terms of ﬁrst and second order are usually considered.

Semiparametric regression (e.g. Ruppert et al., 2003)
attempts to combine interpretability with ﬂexibility by

building a composite model out of an interpretable,
parametric part (such as linear regression) and a
‘catch-all’ nonparametric part (such as a gp with an
SE kernel). In our approach, this can be represented
as a sum of SE and Lin.

Kernel learning There is a large body of work at-
tempting to construct a rich kernel through a weighted
sum of base kernels (e.g. Christoudias et al., 2009;
Bach, 2009). While these approaches ﬁnd the optimal
solution in polynomial time, speed comes at a cost: the
component kernels, as well as their hyperparameters,
must be speciﬁed in advance.

Another approach to kernel learning is to learn an em-
bedding of the data points. Lawrence (2005) learns an
embedding of the data into a low-dimensional space,
and constructs a ﬁxed kernel structure over that space.
This model is typically used in unsupervised tasks and
requires an expensive integration or optimisation over
potential embeddings when generalizing to test points.
Salakhutdinov & Hinton (2008) use a deep neural net-
work to learn an embedding; this is a ﬂexible approach
to kernel learning but relies upon ﬁnding structure in
the input density, p(x). Instead we focus on domains
where most of the interesting structure is in f(x).

Wilson & Adams (2013) derive kernels of the form
SE × cos(x − x(cid:48)), forming a basis for stationary ker-
nels. These kernels share similarities with SE × Per
but can express negative prior correlation, and could
usefully be included in our grammar.

Diosan et al. (2007) and Bing et al. (2010) learn com-
posite kernels for support vector machines and rel-
evance vector machines, using genetic search algo-
rithms. Our work employs a Bayesian search criterion,
and goes beyond this prior work by demonstrating the
interpretability of the structure implied by composite
kernels, and how such structure allows for extrapola-
tion.

Structure discovery There have been several at-
tempts to uncover the structural form of a dataset by
searching over a grammar of structures. For example,
(Schmidt & Lipson, 2009), (Todorovski & Dzeroski,
1997) and (Washio et al., 1999) attempt to learn para-
metric forms of equations to describe time series, or
relations between quantities. Because we learn expres-
sions describing the covariance structure rather than
the functions themselves, we are able to capture struc-
ture which does not have a simple parametric form.

Kemp & Tenenbaum (2008) learned the structural
form of a graph used to model human similarity judg-
ments. Examples of graphs included planes, trees, and

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

cylinders. Some of their discrete graph structures have
continous analogues in our own space; e.g. SE1 × SE2
and SE1 × Per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

Grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. This model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. Our work is tackling a similar problem, but in a
supervised setting.

5. Structure discovery in time series

To investigate our method’s ability to discover struc-
ture, we ran the kernel search on several time-series.

As discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. This provides another method
of visualizing the learned structures. In particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. For example,

SE × (RQ + Lin) = SE × RQ + SE × Lin.

We visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. The
search was run to depth 10, using the base kernels from
Section 2.

Mauna Loa atmospheric CO2 Using our method,
we analyzed records of carbon dioxide levels recorded
at the Mauna Loa observatory. Since this dataset was
analyzed in detail by Rasmussen & Williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

=

+

+

+

Figure 3. Posterior mean and variance for diﬀerent depths
of kernel search. The dashed line marks the extent of the
dataset. In the ﬁrst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
Next, a periodic component is added, and the extrapolation
improves. At depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

Figure 4. First row: The posterior on the Mauna Loa
dataset, after a search of depth 10. Subsequent rows show
the automatic decomposition of the time series. The de-
compositions shows long-term, yearly periodic, medium-
term anomaly components, and residuals, respectively. In
the third row, the scale has been changed in order to clearly
show the yearly periodic structure.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

=

+

+

+

Figure 5. Full posterior and residuals on the solar irradi-
ance dataset.

Figure 3 shows the posterior mean and variance on
this dataset as the search depth increases. While the
data can be smoothly interpolated by a single base
kernel model, the extrapolations improve dramatically
as the increased search depth allows more structure to
be included.

Figure 4 shows the ﬁnal model chosen by our method,
together with its decomposition into additive compo-
nents. The ﬁnal model exhibits both plausible ex-
trapolation and interpretable components: a long-
term trend, annual periodicity and medium-term devi-
ations; the same components chosen by Rasmussen &
Williams (2006). We also plot the residuals, observing
that there is little obvious structure left in the data.

Airline passenger data Figure 6 shows the decom-
position produced by applying our method to monthly
totals of international airline passengers (Box et al.,
1976). We observe similar components to the pre-
vious dataset: a long term trend, annual periodicity
and medium-term deviations.
In addition, the com-
posite kernel captures the near-linearity of the long-
term trend, and the linearly growing amplitude of the
annual oscillations.

Solar irradiance Data Finally, we analyzed annual
solar irradiation data from 1610 to 2011 (Lean et al.,
1995). The posterior and residuals of the learned ker-
nel are shown in ﬁgure 5.

Figure 6. First row: The airline dataset and posterior after
a search of depth 10. Subsequent rows: Additive decom-
position of posterior into long-term smooth trend, yearly
variation, and short-term deviations. Due to the linear ker-
nel, the marginal variance grows over time, making this a
heteroskedastic model.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

None of the models in our search space are capable
of parsimoniously representing the lack of variation
from 1645 to 1715. Despite this, our approach fails
gracefully: the learned kernel still captures the peri-
odic structure, and the quickly growing posterior vari-
ance demonstrates that the model is uncertain about
long term structure.

6. Validation on synthetic data

We validated our method’s ability to recover known
structure on a set of synthetic datasets. For several
composite kernel expressions, we constructed synthetic
data by ﬁrst sampling 300 points uniformly at random,
then sampling function values at those points from a
gp prior. We then added i.i.d. Gaussian noise to the
functions, at various signal-to-noise ratios (SNR).

Table 1 lists the true kernels we used to generate the
data. Subscripts indicate which dimension each kernel
was applied to. Subsequent columns show the dimen-
sionality D of the input space, and the kernels chosen
by our search for diﬀerent SNRs. Dashes - indicate
that no kernel had a higher marginal likelihood than
modeling the data as i.i.d. Gaussian noise.

For the highest SNR, the method ﬁnds all relevant
structure in all but one test. The reported additional
linear structure is explainable by the fact that func-
tions sampled from SE kernels with long length scales
occasionally have near-linear trends. As the noise
increases, our method generally backs oﬀ to simpler
structures.

7. Quantitative evaluation

In addition to the qualitative evaluation in section 5,
we investigated quantitatively how our method per-
forms on both extrapolation and interpolation tasks.

7.1. Extrapolation

We compared the extrapolation capabilities of our
model against standard baselines2. Dividing the air-
line dataset into contiguous training and test sets, we
computed the predictive mean-squared-error (MSE) of
each method. We varied the size of the training set
from the ﬁrst 10% to the ﬁrst 90% of the data.

Figure 7 shows the learning curves of linear regres-
sion, a variety of ﬁxed kernel family gp models, and
our method. gp models with only SE and Per ker-
nels did not capture the long-term trends, since the
best parameter values in terms of gp marginal like-
lihood only capture short term structure. Linear re-
gression approximately captured the long-term trend,

airline
Figure 7. Extrapolation
dataset. We plot test-set MSE as a function of the fraction
of the dataset used for training.

performance

the

on

but quickly plateaued in predictive performance. The
more richly structured gp models (SE + Per and
SE × Per) eventually captured more structure and
performed better, but the full structures discovered
by our search outperformed the other approaches in
terms of predictive performance for all data amounts.

7.2. High-dimensional prediction

To evaluate the predictive accuracy of our method in
a high-dimensional setting, we extended the compari-
son of (Duvenaud et al., 2011) to include our method.
We performed 10 fold cross validation on 5 datasets 3
comparing 5 methods in terms of MSE and predictive
likelihood. Our structure search was run up to depth
10, using the SE and RQ base kernel families.

The comparison included three methods with ﬁxed
kernel families: Additive gps, Generalized Additive
Models (GAM), and a gp with a standard SE kernel
using Automatic Relevance Determination (gp SE-
ARD). Also included was the related kernel-search
method of Hierarchical Kernel Learning (HKL).

Results are presented in table 2. Our method outper-
formed the next-best method in each test, although
not substantially.

All gp hyperparameter tuning was performed by au-

2 In one dimension, the predictive means of all baseline
methods in table 2 are identical to that of a gp with an SE
kernel.

3The data sets had dimensionalities ranging from 4 to
13, and the number of data points ranged from 150 to 450.

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

Table 1. Kernels chosen by our method on synthetic data generated using known kernel structures. D denotes the
dimension of the functions being modeled. SNR indicates the signal-to-noise ratio. Dashes - indicate no structure.

True Kernel
SE + RQ
Lin × Per
SE1 + RQ
2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 + SE4)

D
1
1
2
3
4
4
4

SNR = 10
SE
Lin × Per
SE1 + SE2
SE1 + SE2 × Per1 + SE3
SE1 × SE2
SE1 × SE2 + SE2 × SE3
(SE1 + SE2) × (SE3 × Lin3 × Lin1 + SE4)

SNR = 1
SE × Per
Lin × Per
Lin1 + SE2
SE2 × Per1 + SE3
Lin1 × SE2
SE1 + SE2 × SE3
(SE1 + SE2) × SE3 × SE4

SNR = 0.1
SE
SE
Lin1
-
Lin2
SE1
-

Table 2. Comparison of multidimensional regression performance. Bold results are not signiﬁcantly diﬀerent from the
best-performing method in each experiment, in a paired t-test with a p-value of 5%.

Method
Linear Regression
GAM
HKL
gp SE-ARD
gp Additive
Structure Search

bach
1.031
1.259
0.199
0.045
0.045
0.044

Mean Squared Error (MSE)
concrete puma
0.641
0.404
0.598
0.149
0.346
0.147
0.317
0.157
0.316
0.089
0.315
0.087

servo
0.523
0.281
0.199
0.126
0.110
0.102

housing bach
2.430
0.289
1.708
0.161
-
0.151
−0.131 0.398
0.092
−0.131 0.114
0.102
−0.141 0.065
0.082

Negative Log-Likelihood
servo
1.678
0.800
-
0.429
0.309
0.265

concrete puma
1.881
1.403
1.195
0.467
-
-
0.843
0.841
0.840

housing
1.052
0.457
-
0.207
0.194
0.059

tomated calls to the GPML toolbox4; Python code to
perform all experiments is available on github5.

ric regression and classiﬁcation methods accessible to
non-experts.

8. Discussion

“It would be very nice to have a formal
apparatus that gives us some ‘optimal’ way of
recognizing unusual phenomena and invent-
ing new classes of hypotheses that are most
likely to contain the true one; but this re-
mains an art for the creative human mind.”
E. T. Jaynes, 1985

Towards the goal of automating the choice of kernel
family, we introduced a space of composite kernels de-
ﬁned compositionally as sums and products of a small
number of base kernels. The set of models included in
this space includes many standard regression models.
We proposed a search procedure for this space of ker-
nels which parallels the process of scientiﬁc discovery.

We found that the learned structures are often capa-
ble of accurate extrapolation in complex time-series
datasets, and are competitive with widely used kernel
classes and kernel combination methods on a variety
of prediction tasks. The learned kernels often yield de-
compositions of a signal into diverse and interpretable
components, enabling model-checking by humans. We
believe that a data-driven approach to choosing kernel
structures automatically can help make nonparamet-

4Available at www.gaussianprocess.org/gpml/code/
5 github.com/jamesrobertlloyd/gp-structure-search

Acknowledgements

We thank Carl Rasmussen and Andrew G. Wilson for
helpful discussions. This work was funded in part by
NSERC, EPSRC grant EP/I036575/1, and Google.

Appendix

Kernel deﬁnitions For scalar-valued inputs, the
squared exponential (SE), periodic (Per),
linear
(Lin), and rational quadratic (RQ) kernels are deﬁned
as follows:

kSE(x, x(cid:48)) =

kPer(x, x(cid:48)) = σ2 exp

kLin(x, x(cid:48)) =

kRQ(x, x(cid:48)) =

σ2 exp
(cid:16)

(cid:17)

(cid:16)

(cid:17)

− (x−x(cid:48))2
2(cid:96)2
− 2 sin2(π(x−x(cid:48))/p)
(cid:96)2
v(x − (cid:96))(x(cid:48) − (cid:96))
(cid:17)−α
1 + (x−x(cid:48))2
2α(cid:96)2

σ2
b + σ2
σ2 (cid:16)

Posterior decomposition We can analytically de-
compose a gp posterior distribution over additive com-
ponents using the following identity: The conditional
distribution of a Gaussian vector f1 conditioned on its
sum with another Gaussian vector f = f1 + f2 where
f1 ∼ N (µ1, K1) and f2 ∼ N (µ2, K2) is given by

f1|f ∼ N (cid:0)µ1 + K1
K1 − K1

T(K1 + K2)−1 (f − µ1 − µ2) ,
T(K1 + K2)−1K1

(cid:1).

Structure Discovery in Nonparametric Regression through Compositional Kernel Search

References

Bach, F. Exploring large feature spaces with hierarchi-
cal multiple kernel learning. In Advances in Neural
Information Processing Systems, pp. 105–112. 2009.

Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization
method for RVM.
In International Conference on
Computer and Automation Engineering (ICCAE),
volume 4, pp. 419–423, 2010.

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time

series analysis: forecasting and control. 1976.

Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-
nical report, EECS Department, University of Cali-
fornia, Berkeley, 2009.

Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving
kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007, pp.
19–24. IEEE, 2007.

Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-
In Advances in Neural

ditive Gaussian processes.
Information Processing Systems, 2011.

Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-
plore a large space of model structures. In Uncer-
tainty in Artiﬁcial Intelligence, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541–551, 1989.

Plate, T.A. Accuracy versus interpretability in ﬂexible
modeling: Implementing a tradeoﬀ using Gaussian
process models. Behaviormetrika, 26:29–50, 1999.
ISSN 0385-7417.

Poon, H. and Domingos, P. Sum-product networks:
a new deep architecture. In Conference on Uncer-
tainty in AI, 2011.

Rasmussen, C.E. and Ghahramani, Z. Occam’s razor.
In Advances in Neural Information Processing Sys-
tems, 2001.

Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-
cesses for Machine Learning. The MIT Press, Cam-
bridge, MA, USA, 2006.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-
metric regression, volume 12. Cambridge University
Press, 2003.

Salakhutdinov, R. and Hinton, G. Using deep belief
nets to learn covariance kernels for Gaussian pro-
cesses. Advances in Neural information processing
systems, 20:1249–1256, 2008.

Schmidt, M. and Lipson, H. Distilling free-form natu-
ral laws from experimental data. Science, 324(5923):
81–85, 2009.

Gu, C. Smoothing spline ANOVA models. Springer

Schwarz, G. Estimating the dimension of a model. The

Verlag, 2002. ISBN 0387953531.

Annals of Statistics, 6(2):461–464, 1978.

Hastie, T.J. and Tibshirani, R.J. Generalized additive

models. Chapman & Hall/CRC, 1990.

Jaynes, E. T. Highly informative priors. In Proceedings
of the Second International Meeting on Bayesian
Statistics, 1985.

Kemp, C. and Tenenbaum, J.B.

The discovery
of structural form. Proceedings of the National
Academy of Sciences, 105(31):10687–10692, 2008.

Lawrence, N. Probabilistic non-linear principal com-
ponent analysis with gaussian process latent variable
models. The Journal of Machine Learning Research,
6:1783–1816, 2005.

Lean, J., Beer, J., and Bradley, R. Reconstruction of
solar irradiance since 1610: Implications for climate
change. Geophysical Research Letters, 22(23):3195–
3198, 1995.

Todorovski, L. and Dzeroski, S. Declarative bias in
equation discovery. In International Conference on
Machine Learning, pp. 376–384, 1997.

Wahba, G.

Spline models for observational data.
ISBN

Society for Industrial Mathematics, 1990.
0898712440.

Washio, T., Motoda, H., Niwa, Y., et al. Discover-
ing admissible model equations from observed data
based on scale-types and identity constraints.
In
International Joint Conference On Artiﬁcal Intelli-
gence, volume 16, pp. 772–779, 1999.

Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern
discovery and extrapolation.
Technical Report
arXiv:1302.4245 [stat.ML], February 2013.

