Safe Grid Search with Optimal Complexity

Eugene Ndiaye 1 Tam Le 1 Olivier Fercoq 2 Joseph Salmon 3 Ichiro Takeuchi 4

9
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
1
7
4
5
0
.
0
1
8
1
:
v
i
X
r
a

≥

Abstract
Popular machine learning estimators involve reg-
ularization parameters that can be challenging to
tune, and standard strategies rely on grid search
for this task. In this paper, we revisit the tech-
niques of approximating the regularization path
up to predeﬁned tolerance (cid:15) in a uniﬁed frame-
work and show that its complexity is O(1/ d√(cid:15))
2 and
for uniformly convex loss of order d
O(1/√(cid:15)) for Generalized Self-Concordant func-
tions. This framework encompasses least-squares
but also logistic regression, a case that as far as
we know was not handled as precisely in previ-
ous works. We leverage our technique to provide
reﬁned bounds on the validation error as well as
a practical algorithm for hyperparameter tuning.
The latter has global convergence guarantee when
targeting a prescribed accuracy on the validation
set. Last but not least, our approach helps reliev-
ing the practitioner from the (often neglected) task
of selecting a stopping criterion when optimizing
over the training set: our method automatically
calibrates this criterion based on the targeted ac-
curacy on the validation set.

1. Introduction

Various machine learning problems are formulated as mini-
mization of an empirical loss function f plus a regularization
function Ω whose calibration is controlled by a hyperparam-
eter λ. The choice of λ is crucial in practice since it directly
inﬂuences the generalization performance of the estimator,
i.e., its score on unseen data sets. The most popular method
in such a context is cross-validation (or some variant, see
(Arlot & Celisse, 2010) for a detailed review). For sim-
plicity, we investigate here the simplest case, the holdout
version. It consists in splitting the data in two parts: on

1Riken AIP 2LTCI, T´el´ecom ParisTech, Universit´e Paris-Saclay
3IMAG, Univ Montpellier, CNRS, Montpellier, France 4Nagoya
Institute of Technology. Correspondence to: E. Ndiaye <eu-
gene.ndiaye@riken.jp>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

the ﬁrst part (training set) the method is trained for a pre-
deﬁned collection of candidates ΛT :=
,
}
and on the second part (validation set), the best parameter
is selected among the T candidates.

λ0, . . . , λT −1
{

For a piecewise quadratic loss f and a piecewise linear
regularization Ω (e.g., the Lasso estimator), Osborne et al.
(2000); Rosset & Zhu (2007) have shown that the set of
solutions follows a piecewise linear curve w.r.t. to the pa-
rameter λ. There are several algorithms that can generate
the full path by maintaining optimality conditions when
the regularization parameter varies. This is what LARS is
performing for Lasso (Efron et al., 2004), but similar ap-
proaches exist for SVM (Hastie et al., 2004) or generalized
linear models (GLM) (Park & Hastie, 2007). Unfortunately,
these methods have some drawbacks that can be critical in
many situations:

their worst case complexity, i.e., the number of linear
•
segments, is exponential in the dimension p of the problem
(G¨artner et al., 2012) leading to unpractical algorithms. Re-
cently, Li & Singer (2018) have shown that for some speciﬁc
design matrix with n observations, a polynomial complexity
p6) can be obtained. Note that even in a more
of O(n
favorable cases of linear complexity in p, the exact path can
be expensive to compute when the dimension p is large.

×

they suffer from numerical instabilities due to multiple
•
and expensive inversion of ill-conditioned matrix. As a
result, these algorithms may fail before exploring the entire
path, a common issue for small regularization parameter.

they lack ﬂexibility when it comes at incorporating dif-
•
ferent statistical learning tasks because they usually rely on
speciﬁc algebra to handle the structure of the regularization
and loss functions. As far as we know, they can be applied
only to a limited number of cases and we are not aware of a
general framework that bypasses these issues.

they cannot beneﬁt of early stopping. Following Bottou &
•
Bousquet (2008), it is not necessary to optimize below the
statistical error for suitable generalization. Exact regulariza-
tion path algorithms need to maintain optimality conditions
as the hyperparameter varies, which is time consuming.

To overcome these issues, an (cid:15)-approximation of the solu-
tion path was proposed and optimal complexity was proven
to be O(1/(cid:15)) by (Giesen et al., 2010) in a fairly general

Safe Grid Search with Optimal Complexity

fi(z)
f ∗
i (u)
Vf ∗,x(u)

Lasso
(yi − z)2/2
((u − yi)2 − y2
(cid:107)u(cid:107)2

2/2

i )/2

Logistic regr.
log(1 + ez) − yiz
Nh(u + yi)

w4((cid:107)u(cid:107)2

x/(cid:107)u(cid:107)2)(cid:107)u(cid:107)2
u

Table 1. w4(τ ) = (1−τ ) log(1−τ )+τ
τ 2
x) log(1

and Nh(x) = x log(x) + (1

−

x)

−

setting. Then, Mairal & Yu (2012) provided an interesting
algorithm whose complexity is O(1/√(cid:15)) for the Lasso case.
The latter result was then extended by (Giesen et al., 2012)
to objective function with quadratic lower bound while pro-
viding a lower and upper bound of order O(1/√(cid:15)). Unfor-
tunately, these assumptions fail to hold for many problems,
including logistic regression or Huber loss.

Following such ideas, (Shibagaki et al., 2015) have pro-
posed, for classiﬁcation problems, to approximate the regu-
larization path on the hold-out cross-validation error. Indeed,
the latter is a more natural criterion to monitor when one
aims at selecting a hyperparameter guaranteed to achieve the
best validation error. The main idea is to construct upper and
lower bounds of the validation error as simple functions of
the regularization parameter. Hence by sequentially varying
the parameters, one can estimate a range of parameter for
which the validation error gap (i.e., the difference with the
validation error achieved by the best parameter) is smaller
than an accuracy (cid:15)v > 0.

Contributions. We revisit the approximation of the so-
lution and validation path in a uniﬁed framework under
general regularity assumptions commonly met in machine
learning. We encompass both classiﬁcation and regression
problems and provide a complexity analysis along with ex-
plicit optimality guarantees. We highlight the relationship
between the regularity of the loss function and the complex-
ity of the approximation path. We prove that its complex-
ity is O(1/ d√(cid:15)) for uniformly convex loss of order d
2
(see Bauschke & Combettes (2011, Deﬁnition 10.5)) and
O(1/√(cid:15)) for the logistic loss thanks to a reﬁned measure
of its curvature throughout its Generalized Self-Concordant
properties (Sun & Tran-Dinh, 2017). As far as we know,
the previously known approximation path algorithms cannot
handle these cases. We provide an algorithm with global
convergence property for selecting a hyperparameter with
a validation error (cid:15)v-close to the optimal hyperparameter
from a given grid. We bring a natural stopping criterion
when optimizing over the training set making this criterion
automatically calibrated.

≥

Our implementation is available at https://github.
com/EugeneNdiaye/safe_grid_search.

Notation. Given a proper, closed and convex function
Rn :
f : Rn

, we denote dom f =

+

R

→

∪ {

∞}

x
{

∈

Figure 1. Illustration of the approximation path for the Lasso at
accuracy (cid:15) = (cid:107)y(cid:107)2
2 /20. We choose λmax = (cid:107)X (cid:62)y(cid:107)∞ and
λmin = λmax/50. The shaded gray region shows the interval
where any (cid:15)-path must lie. The exact path is computed with the
LassoLars on diabetes data from sklearn.

z

(cid:107)

(cid:112)

(cid:104)∇

∞}

∞}

∇
2f (x)z, z

2f (x) at any x

∈
. The Fenchel-

. If f is a twice continuously differentiable

f (x) < +
function with positive deﬁnite Hessian
dom f , we denote
(cid:107)x =
(cid:105)
Legendre transform of f is the function f ∗ : Rn
x∗, x
deﬁned by f ∗(x∗) = supx∈dom f (cid:104)
+

R
∪
f (x).
{
The support function of a nonempty set C is deﬁned as
. If C is closed, convex and contains
σC(x) = supc∈C(cid:104)
c, x
(cid:105)
0, we deﬁne its polar as σ◦
. We
for any non zero integer T .
denote by [T ] the set
1, . . . , T
{
Rn and the design matrix
The vector of observations is y
Rn×p has n observations row-wise,
X = [x1, . . . , xn](cid:62)
and p features (column-wise).

C(x∗) = supσC (x)≤1(cid:104)

x∗, x
(cid:105)

→
(cid:105) −

}
∈

∈

2. Problem setup

Let us consider the class of regularized learning methods
expressed as convex optimization problems, such as (regu-
larized) GLM (McCullagh & Nelder, 1989):

ˆβ(λ)

arg min
β∈Rp

∈

f (Xβ) + λΩ(β)
(cid:123)(cid:122)
(cid:125)
(cid:124)
Pλ(β)

(Primal).

(1)

) = 1

i∈[n] fi(x(cid:62)

We highlight two important cases: the regularized least-
squares and logistic regression where the loss functions are
written as an empirical risk f (Xβ) = (cid:80)
i β) with
the fi’s given in Table 1. The penalty term is often used to
incorporate prior knowledges by enforcing a certain regular-
ity on the solutions. For instance, choosing a Ridge penalty
2
(Hoerl & Kennard, 1970) Ω(
2 improves the stabil-
2 (cid:107)·(cid:107)
·
ity of the resolution of inverse problems while Ω(
(cid:107)·(cid:107)1
imposes sparsity at the feature level, a motivation that led to
the Lasso estimator (Tibshirani, 1996); see also (Bach et al.,
2012) for extensions to other structured penalties.
In practice, obtaining ˆβ(λ), an exact solution to Problem (1)
is unpractical and one aims achieving a prescribed precision
(cid:15) > 0. More precisely, a (primal) vector β(λ) := β(λ,(cid:15)) (we
will drop the dependency in (cid:15) for readability) is referred
to as an (cid:15)-solution for λ if its (primal) objective value is
optimal at precision (cid:15):

) =
·

Pλ(β(λ))

Pλ( ˆβ(λ))

−

(cid:15) .

≤

(2)

Safe Grid Search with Optimal Complexity

We recall and illustrate the notion of approximation path in
Figure 1 as described by Giesen et al. (2012).
Deﬁnition 1 ((cid:15)-path). A set
⊂
a parameter range [λmin, λmax] if

Rp is called an (cid:15)-path for

P

(cid:15)

λ

∀

∈

[λmin, λmax],

an (cid:15)-solution β(λ)

(cid:15) .

(3)

∃

∈ P

We call path complexity T(cid:15) the cardinality of the (cid:15)-path.

To achieve the targeted (cid:15)-precision in (2) over a whole path
and construct an (cid:15)-path 1, we rely on duality gap evaluations.
For that, we compute (cid:15)c-solutions2 (for an accuracy (cid:15)c < (cid:15))
over a ﬁnite grid, and then we control the gap variations
w.r.t. λ to achieve the prescribed (cid:15)-precision over the whole
range [λmin, λmax]; see Algorithm 1. We now recall the
Fenchel duality (Rockafellar, 1997, Chapter 31):

ˆθ(λ)

arg max

θ∈Rn −
(cid:124)

∈

f ∗(

λθ)

−

λΩ∗(X (cid:62)θ)
(cid:125)

−
(cid:123)(cid:122)
Dλ(θ)

(Dual). (4)

For a primal/dual pair (β, θ)
dom Dλ, the dual-
∈
ity gap is the difference between primal and dual objectives:

dom Pλ

×

λ(β, θ) = f (Xβ) + f ∗(

G
−
Weak duality yields Dλ(θ)

λθ) + λ(Ω(β) + Ω∗(X (cid:62)θ)) .

Pλ(β)

−

λ(β, θ) ,

≤ G

(5)

λ(cid:48)(β(λ), θ(λ))

explaining the interest of the duality gap as an optimal-
ity certiﬁcate. Using (5), we can safely construct an ap-
proximation path for Problem (1) : if β(λ) is an (cid:15)-solution
for λ, it is guaranteed to remain one for all parameters
λ(cid:48) such that
(cid:15). Since the function
≤
λ(cid:48)
λ(cid:48)(β(λ), θ(λ)) does not exhibit a simple dependence
in λ, we rely on an upper bound on the gap encoding the
structural regularity of the loss function (e.g., 1-dimensional
quadratics for strongly convex functions). This bound con-
trols the optimization error as λ varies while preserving
optimal complexity on the approximation path.

(cid:55)→ G

G

Pλ(β) and

≤
Pλ( ˆβ(λ))

3. Bounds and approximation path

We introduce the tools to design an approximation path.

3.1. Preliminary results and technical tools

Deﬁnition 2. Given a differentiable function f and x
∈
) be non negative functions
dom f , let
) and
f,x(
f,x(
·
·
f,x-
f,x-convex (resp.
that vanish at 0. We say that f is
smooth) at x when Inequality (6) (resp. (7)) is satisﬁed for
any z

dom f

U

U

V

V

∈
f,x(z
f,x(z

U

x)

f (z)

f (x)

−

≤

−

− (cid:104)∇

f (x), z

x

(cid:105)

−

,

−

x)

f (z)

f (x)
.
V
1note that such a path depends on exact solutions ˆβ(λ)’s
2the c stands for computational in (cid:15)c

f (x), z

− (cid:104)∇

≥

−

−

x

(cid:105)

(6)

(7)

This extends µ-strong convexity and ν-smoothness (Nes-
terov, 2004) and encompasses smooth uniformly convex
losses and generalized self-concordant ones.

Smooth uniformly convex case:

In this case, we have

U

f,x(z
f,x(z

−

x) =

x) =

U

(

(

z
(cid:107)
z
(cid:107)

−

x

x

),
(cid:107)
),
(cid:107)

V

V

∞

V
) and
(
·

−
−
) are increasing from [0, +
(
·

) to
where
U
] vanishing at 0; see Az´e & Penot (1995). Exam-
[0, +
(t) = µ
d td
ples of such functions are
U
where d, µ and ν are positive constants. The case d = 2
corresponds to strong convexity and smoothness; in general
they are called uniformly convex of order d, see (Juditski &
Nesterov, 2014) or (Bauschke & Combettes, 2011, Ch. 10.2
and 18.5) for details.

∞
(t) = ν

d td and

V

Generalized self-concordant case:
f is (Mf , ν)-generalized self-concordant of order ν
dom f and
and Mf

3 convex function
2

C
Rn:

u, v

0 if

≥

x

a

≥
∀
∈
(cid:12)
3f (x)[v]u, u
(cid:12)
(cid:105)

(cid:12)
(cid:12)

(cid:104)∇

Mf

≤

∀
u
(cid:107)
(cid:107)

∈
2
v
x (cid:107)

ν−2
x
(cid:107)

v
(cid:107)

3−ν
2
(cid:107)

.

In this case, Sun & Tran-Dinh (2017, Proposition 10) have
shown that one could write:

f,x(y

f,x(y

U

V

−

−

x) = wν(

dν(x, y))

−
x) = wν(dν(x, y))

y
(cid:107)
y

−
x

(cid:107)

−

2
x ,
x
(cid:107)
2
x ,
(cid:107)

where the last equality holds if dν(x, y) < 1 for the case
) are re-
) and dν(
ν > 2. Closed-form expressions of wν(
·
·
called in Appendix for logistic, quadratic and power losses.

Approximating the duality gap path. Assume we have
constructed primal/dual feasible vectors for a ﬁnite grid
, i.e., we have at
of parameters ΛT =
λ0, . . . , λT −1
our disposal (β(λt), θ(λt)) for all λt
ΛT . Let us de-
λtθ(λt),
note
t =
f ∗(ζt)). For any function φ :
∆t = f (Xβ(λt))
Rn
[0, +

λt(β(λt), θ(λt)), and for ζt =

−
R, we deﬁne

∇
] that vanishes at 0, ρ

}
∈

f (

−

G

G

{

→

∞
Qt,φ(ρ) =

t +ρ

(∆t

·

− G

G

ζt) .

ρ

−

·

(8)

∈
t) + φ(

G

t and ∆t represent a measure of the optimization
The terms
error at λt. The notation introduced in (8) will be convenient
to write concisely upper and lower bounds on the duality gap.
This is the goal of the next lemma which leverages regularity
of the loss function f , as introduced in Deﬁnition 2. This
provides control on how the duality gap deviates when one
evaluates it for another (close) parameter λ.
Lemma 1. We assume that
X (cid:62)θ(λt)
convex)3, then for ρ = 1

∈
f ∗ -smooth (resp.
λ/λt, the right (resp.

dom f ∗ and
f ∗ -
U
left)

−
If f ∗ is

dom Ω∗.

λθ(λt)

∈

V

−
3we drop x in Uf,x and write Uf if no ambiguity holds.

Safe Grid Search with Optimal Complexity

(a) Uniform unilateral approximation path (as in Proposition 3)

(b) Bilateral approximation path (as in Proposition 4)

Figure 2. Illustration of the construction of (cid:15)-paths for the Lasso on synthetic dataset generated with sklearn as X, y =
make regression(n = 30, p = 150) at accuracy (cid:15) = (cid:107)y(cid:107)2
2 /40 and (cid:15)c = (cid:15)/10. We choose λmax = (cid:107)X (cid:62)y(cid:107)∞ and λmin = λmax/20.
For Lasso the bounds are piece-wise quadratic. The shaded gray regions correspond to the regions where the true value of the duality gap
lies. We obtain a path complexity of T(cid:15) = 6 (resp. T(cid:15) = 4) for the unilateral (resp. bilateral) path over [λmin, λmax].

hand side of Inequality (9) holds true

Qt,Uf ∗ (ρ)

λ(β(λt), θ(λt))

Qt,Vf ∗ (ρ) .

(9)

≤ G

≤

Proof. Proof for this result and for other propositions and
theorems are deferred to the Appendix.

V

U

f ∗ ) for the upper
f ∗ (resp.
The function φ, chosen as
(resp.
lower) bound, essentially captures the regularity
needed to approximate the duality gap at λ when using pri-
mal/dual vector (β(λt), θ(λt)) for λt close to λ. When the
function satisﬁes both inequalities, tightness of the bounds
f ∗ of the dual loss
can be related to the conditioning
f ∗ /
U
V
2
1
f ∗. Equality holds for
2 (least-squares),
2 (cid:107)·(cid:107)
≡
showing the tightness of the bounds.

≡ V

U

f ∗

f ∗

≤

λ(β(λt), θ(λt))

G
(cid:15) where ρ = 1

(cid:15) as soon as
From Lemma 1, we have
≤
λ/λt varies with λ. Hence,
Qt,Vf ∗ (ρ)
−
we obtain the following proposition that allows to track the
regularization path for an arbitrary precision on the duality
gap. It proceeds by choosing the largest ρ = ρt such that
the upper bound in Equation (9) remains below (cid:15) and leads
to Algorithm 1 for computing an (cid:15)-path.

t

t((cid:15)), 1 + ρr
ρ(cid:96)

G
≤
t ((cid:15))(cid:3), we have

Proposition 1 (Grid for a prescribed precision).
Given (β(λt), θ(λt)) such that
(cid:2)1
λt
−
×
t((cid:15)) (resp. ρr
where ρ(cid:96)
Qt,Vf ∗ (ρ)

∈
(cid:15)
G
t ((cid:15))) is the largest non-negative ρ s.t.

(cid:15)c < (cid:15), for all λ
λ(β(λt), θ(λt))

−
Conversely, given a grid4 of T parameters ΛT =
, we deﬁne (cid:15)ΛT , the error of the approx-
λ0, . . . , λT −1
{
imation path on [λmin, λmax] by using a piecewise constant
approximation of the map λ

(cid:15) (resp. Qt,Vf ∗ (

λ(β(λt), θ(λt)):

(cid:15)).

ρ)

≤

≤

≤

}

(cid:55)→ G

(cid:15)ΛT :=

max
λ∈[λmin,λmax]

min
λt∈ΛT G

λ(β(λt), θ(λt)) .

(10)

4we assume a decreasing order λt+1 < λt, reﬂecting common

practices for GLM, e.g., for the Lasso.

This error is however difﬁcult to evaluate in practice so we
rely on a tight upper bound based on Lemma 1 that often
leads to closed-form expressions.
Proposition 2 (Precision for a given grid). Given a grid
of parameters ΛT , the set
is an (cid:15)ΛT -
∈
λ(cid:63)
t /λt) where for all
path with (cid:15)ΛT ≤
t is the largest λ
[λt+1, λt] such
t
0, . . . , T
−
∈ {
that Qt,Vf ∗ (1
Qt+1,Vf ∗ (1
−

maxt∈[T ] Qt,Vf ∗ (1
, λ(cid:63)
1
}
λ/λt)

β(λ) : λ
{

∈
λ/λt+1).

ΛT

−

−

≥

}

Construction of dual feasible vector. We rely on gradi-
ent rescaling to produce a dual feasible vector:
Lemma 2. For any β(λt)

Rp, the vector

∈

f (Xβ(λt))

−∇
dom Ω∗ (X (cid:62)
dom f ∗, X (cid:62)θ(λt)

∇

θ(λt) =

max(λt, σ◦

f (Xβ(λt)))

,

λθ(λt)

is feasible:
Remark 1. When the regularization is a norm, Ω(
·
then σ◦

dom Ω∗ is the associated dual norm

−

∈

∈

dom Ω∗.

) =

The dual θ(λt) in Lemma 2 implies that
to 0 when β(λt) converges to ˆβ(λt) (Ndiaye et al., 2017).

G

(cid:107)·(cid:107)

(cid:107)·(cid:107)∗.
t and ∆t converge

Finding ρ. Following Proposition 1, a 1-dimensional
equation Qt,Vf ∗ (ρ) = (cid:15) needs to be solved to obtain an
(cid:15)-path. This can be done efﬁciently at high precision by
numerical solvers if no explicit solution is available.

As a corollary from Lemma 1 and Proposition 2, we recover
the analysis by Giesen et al. (2012):
Corollary 1. If the function f ∗ is ν
(ρ(cid:96)
closed-form expressions:

2-smooth, the left
2 (cid:107)·(cid:107)
t ) step sizes deﬁned in Proposition 1 have

t) and right (ρr

(cid:113)

2νδt

ρ(cid:96)
t =

where δt := (cid:15)
to δt = (cid:15)

−

(cid:113)

2νδt

˜δt

, ρr

t =

t −

(cid:107)
ζt

ζt
(cid:107)
ν

2 + ˜δ2
2
(cid:107)
t and ˜δt := ∆t
− G
(cid:15)c and ˜δt = 0 when max(
G

− G

(cid:107)

t + ˜δt

,

ζt
(cid:107)
ν

2 + ˜δ2
(cid:107)
2
ζt
(cid:107)

(cid:107)
t. This is simpliﬁed
(cid:15)c.
t, ∆t)

≤

Safe Grid Search with Optimal Complexity

Algorithm 1 training path
Input: f, Ω, (cid:15), (cid:15)c, [λmin, λmax]
Initialization: t = 0, λ0 = λmax, Λ = {λmax}
repeat

Get β(λt) solving (1) to accuracy Gt ≤ (cid:15)c < (cid:15)
Compute the step size ρ(cid:96)
Set λt+1 = max(λt × (1 − ρ(cid:96)
Λ ← Λ ∪ {λt+1} and t ← t + 1

t), λmin)

t((cid:15)) following Proposition 3, 4, 5.

until λt ≤ λmin
Return: {β(λt) : λt ∈ Λ}

3.2. Discretization strategies

We now establish new strategies for the exploration of the
hyperparameter space in the search for an (cid:15)-path.

For regularized learning methods, it is customary to start
from a large regularizer5 λ0 = λmax and then to perform
the computation of ˆβ(λt+1) after the one of ˆβ(λt), until the
smallest parameter of interest λmin is reached. Models
are generally computed by increasing complexity, allowing
important speed-ups due to warm start (Friedman et al.,
2007) when the λ’s are close to each other. Knowing λt, we
provide a recursive strategy to construct λt+1.

Adaptive unilateral. The strategy we call unilateral con-
sists in computing the new parameter as λt+1 = λt
ρ(cid:96)
t((cid:15))) as in Proposition 1.
Proposition 3 (Unilateral approximation path). Assume
that f ∗ is
f ∗ -smooth. We construct the grid of param-
eters Λ(u)((cid:15)) =
by

(1

×

−

V

λ0, . . . , λT(cid:15)−1
{

}
λt+1 = λt

λ0 = λmax,

(1

×

−

ρ(cid:96)
t((cid:15))) ,

and (β(λt), θ(λt)) s.t.
β(λt) : λt

G
Λ(u)((cid:15))
}

(cid:15)c < (cid:15) for all t. Then, the set

t
≤
is an (cid:15)-path for Problem (1).

∈

{
This strategy is illustrated in Figure 2(a) on a Lasso case, and
stands as a generic one to compute an approximation path
for loss functions satisfying assumptions in Deﬁnition 2.

Adaptive bilateral. For uniformly convex functions, we
can make a larger step by combining the information given
by the left and right step sizes. Indeed, let us assume that
we explore the parameter range [λmin, λmax]. Starting from
a parameter λt, we deﬁne the next step, given by Propo-
ρ(cid:96)
sition 1, λ(cid:96)
λ(cid:96)
t). Then it exists λt(cid:48)
t such
t := λt(1
−
≤
t(cid:48)) = λ(cid:96)
that λr
t(cid:48) := λt(cid:48)(1 + ρr
t. Thus a larger step can be
t(cid:48)). However ρr
done by using λt(cid:48) = λt
(1
t(cid:48)
−
depends on the (approximated) solution β(λt(cid:48) ) that we do
not know before optimizing the problem for parameter λt(cid:48)
when computing sequentially the grid points in decreasing
order i.e., λt(cid:48)
λt. We overcome this issue in Lemma 3
by (upper) bounding all the constants in Qt(cid:48),Vf ∗ (ρ) that

t)/(1 + ρr
ρ(cid:96)

≤

×

5for the Lasso one often chooses λ0 = λmax := (cid:13)

(cid:13)X (cid:62)y(cid:13)

(cid:13)∞

depend on the solution β(λt(cid:48) ), by constants involving only
information given by β(λt).
Lemma 3. Assuming f uniformly smooth yields
−1(cid:0)f (Xβ(λt)) +
f (Xβ(λt(cid:48) ))
(cid:101)Rt, where (cid:101)Rt :=
(cid:1). If additionally f is uniformly convex, this yields
((cid:15)c) as well as
:= (cid:101)Rt

(cid:101)∆t, where (cid:101)∆t

(cid:107)∇
2(cid:15)c
ρ(cid:96)
t((cid:15))
∆t(cid:48)

∗
f
V

∗
(cid:107)

≤

−1
f

≤

λ(β(λt(cid:48) ), θ(λt(cid:48) ))

Qt(cid:48),Vf ∗ (ρ)

≤

≤

G

× U
(cid:101)Qt,Vf ∗ (ρ), where

(cid:101)Qt,Vf ∗ (ρ) = (cid:15)c + ρ

( (cid:101)∆t

(cid:15)c) +

·

−

(cid:16)

f ∗

V

(cid:101)Rt

ρ

|

| ·

(cid:17)

.

t ((cid:15))

t((cid:15)) + ˜ρr
ρ(cid:96)
1 + ˜ρr
t ((cid:15))

t ((cid:15)) =

, where ρ(cid:96)

Let us now deﬁne ρ(b)
deﬁned in Proposition 1 and ˜ρr
ρ such that (cid:101)Qt,Vf ∗ (ρ)
Proposition 4 (Bilateral Approximation Path). Assume that
f is uniformly convex and smooth. We construct the grid
Λ(b)((cid:15)) =
by

t ((cid:15)) is the largest non negative

(cid:15) in Lemma 3.

t((cid:15)) is

≤

λ0, . . . , λT(cid:15)−1
{

}

λ0 = λmax,

λt+1 = λt

(1

×

−

ρ(b)
t ((cid:15))) ,

and (β(λt), θ(λt)) s.t.
Λ(b)((cid:15))
β(λt) : λt

(cid:15)c < (cid:15) for all t. Then the set

t
is an (cid:15)-path for Problem (1).

≤

{
This strategy is illustrated in Figure 2(b) on a Lasso example.

∈

G
}

Uniform unilateral and bilateral.
In some cases, it may
be advantageous to have access to a predeﬁned grid be-
fore launching a hyperparameter selection procedure such
as hyperband (Li et al., 2017) or for parallel computa-
tions. Given the initial information from the initialization
(β(λ0), θ(λ0)), we can build a uniform grid that guarantees
an (cid:15)-approximation before solving any optimization prob-
Indeed, by applying Lemma 3 at t = 0, we have
lem.
0((cid:15)) (resp.
(cid:15) (resp.

λ(β(λt), θ(λt))
(cid:101)Q0,Vf ∗ (ρ). We can deﬁne (cid:101)ρ(cid:96)
G
(cid:101)ρr
0((cid:15))) as the largest non-negative ρ s.t. (cid:101)Q0,Vf ∗ (ρ)
(cid:101)Q0,Vf ∗ (

(cid:15)) and also

ρ)

≤

≤

−

≤
(cid:40)

ρ0((cid:15)) =

(cid:101)ρ(cid:96)
0((cid:15))
0((cid:15))+(cid:101)ρr
(cid:101)ρ(cid:96)
1+ ˜ρr
0((cid:15))

0((cid:15))

for unilateral path,

for bilateral path.

(11)

Proposition 5 (Uniform approximation path).
Assume that f is uniformly convex and smooth, and deﬁne
the grid Λ(0)((cid:15)) =

by

λ0, . . . , λT(cid:15)−1
{
λ0 = λmax,

λt+1 = λt

}

(1

ρ0((cid:15))) ,

×

−
(cid:15)c < (cid:15). Then the set
is an (cid:15)-path for Problem (1) with

≤

G

t

and
t
∀
β(λt)
{
∈
at most T(cid:15) grid points where

[T ], (β(λt), θ(λt)) s.t.
Λ(0)((cid:15))
}

∈
: λt

T(cid:15) =

(cid:22) log(λmin/λmax)
ρ0((cid:15)))

log(1

(cid:23)

.

−

(12)

Safe Grid Search with Optimal Complexity

(a) (cid:96)1 least-squares regression on climate data set NCEP/NCAR
Reanalysis; n = 814 observations and p = 73577 features.

(b) (cid:96)1 logistic regression on leukemia data set with n = 72
observations and p = 7129 features.

Figure 3. Computation of the approximation path to reach the same error than the default grid ((cid:15) = 10−4 (cid:107)y(cid:107)2 for the least-squares case
and (cid:15) = 10−4 min(n1, n2)/n where ni is the number of observations in the class i ∈ {0, 1}, for the logistic case). We have used the
same (vanilla) coordinate descent optimization solver with warm start between parameters for all grids. Note that a smaller grid do not
imply faster computation, as the interplay with the warm-start can be intricate in our sequential approach.

3.3. Limitations of previous framework

Previous algorithms for computing (cid:15)-paths have been ini-
tially developed with a complexity of O(1/(cid:15)) (Clarkson,
2010; Giesen et al., 2010) in a large class of problems. Yet,
losses arising in machine learning have often nicer regular-
ities that can be exploited. This is all the more striking in
the Lasso case where a better complexity in O(1/√(cid:15)) was
obtained by Mairal & Yu (2012); Giesen et al. (2012).

The relation between path complexity and regularity of the
objective function remains unclear and previous methods
do not apply to all popular learning problems. For instance
the dual loss f ∗ of the logistic regression is not uniformly
smooth. So to apply the previous theory, one needs to
optimize on a (potentially badly pre-selected) compact set.

∈

−

Let us consider the one dimensional toy example where
R, X = Idp, y =
1 and the loss func-
p = 1, β
2f (β) =
tion f (Xβ) = log(1 + exp(β)). We have,
exp(β)/(1 + exp(β))2. Then for Problem (1), since
Pλ( ˆβ(λ))
ˆβ(λ)
[0, log(2)/λ] and
|
a smoothness constant νf ∗
exp(λ) for the dual can be
≈
estimated at each step. This leads to an unreasonable algo-
rithm with tiny step sizes in Corollary 1. Also, the algorithm
proposed by Giesen et al. (2012) can not be applied for the
logistic loss since the dual function is not polynomial.

Pλ(0), we have

| ∈

∇

≤

Our proposed algorithm does not suffer from such limi-
tations and we introduce a ﬁner analysis that takes into
account the regularity of the loss functions.

3.4. Complexity and regularity

Lower bound on path complexity. For our method, the
lower bound on the duality gap quantiﬁes how close the from

Proposition 1 is from the best possible one can achieve for
smooth loss functions. Indeed, at optimal solution, we have
t = ∆t = 0. Thus the largest possible step — starting
G
at λt and moving in decreasing order — is given by the
smallest λ
ρ) > (cid:15) where
U
λt ˆθ(λt). Hence, any algorithm for computing an
ˆζt =
(cid:15)-path for
f ∗ -uniformly convex dual loss, have necessarily
a complexity of order at least O(1/

[λmin, λt] such that

f ∗ (

ˆζt

−

−

×

∈

U

−1
f ∗ ((cid:15))).

U

Upper bounds. We remind that we write T(cid:15) for the com-
plexity of our proposed approximation path i.e., the cardi-
nality of the grid returned by Algorithm 1. In the following
proposition, we propose a bound on the complexity w.r.t. the
regularity of the loss function. Discussions on the constants
and assumptions are provided in the Appendix.
Proposition 6 (Approximation path: complexity).
Assuming that max(
t, ∆t)
exists an explicit constant Cf ((cid:15)c) > 0 such that

(cid:15)c < (cid:15) at each step t, there

≤

G

T(cid:15)

log

≤

(cid:17)

(cid:16) λmax
λmin

×

where for all t > 0, the function

,

Cf ((cid:15)c)
f ∗ ((cid:15)
(cid:15)c)
f ∗ is deﬁned by

−

W

W

(13)

) =
f ∗ (
·

W






−1
f ∗ (
·
,
·

V
√

), if f is uniformly convex and smooth
if f is Generalized Self-Concordant
and uniformly-smooth.

Moreover, Cf ((cid:15)c) is an uniform upper bound of
the path, that tends to a constant Cf when (cid:15)c goes to 0.

(cid:107)∗ along

ζt
(cid:107)

Proposition 6 applied to the special case when f is ν-smooth

for
and µ-strongly convex reads log
the complexity for any data X, y. This is not explicitly
dependent on the dimension n and p and are more scalable.

f (Xβ(λ0))
(cid:15)−(cid:15)c

(cid:16) λmax
λmin

(cid:17)(cid:113) ν
µ

Safe Grid Search with Optimal Complexity

Algorithm 2 (cid:15)v-path for Validation Set

Input: f, Ω, (cid:15)v, [λmin, λmax]
Compute (cid:15)v,µ as in Proposition 7
Λ((cid:15)v,µ) = training path (f, Ω, (cid:15)v,µ, [λmin, λmax])
Return: Λ((cid:15)v,µ)

4. Validation path

To achieve good generalization performance, estimators de-
ﬁned as solutions of Problem 1 require a careful adjustment
of λ to balance data-ﬁtting and regularization. A standard
approach to calibrate such a parameter is to select it by com-
paring the validation errors on a ﬁnite grid (say with K-fold
cross-validation). Unfortunately, it is often difﬁcult to de-
termine a priori the grid limits, the number of λ’s (number
of points in the grid) or how they should be distributed to
achieve low validation error.

Considering the validation data (X (cid:48), y(cid:48)) with n(cid:48) observa-
Rp:
tions and loss6

, we deﬁne the validation error for β

L

Ev(β) =

(y(cid:48), X (cid:48)β) .

L
For selecting a hyperparameter, we leverage our approxima-
tion path to solve the bi-level problem

∈

(14)

arg min
λ∈[λmin,λmax]

Ev( ˆβ(λ)) =

(y(cid:48), X (cid:48) ˆβ(λ))

L

s.t. ˆβ(λ)

arg min
β∈Rp

∈

f (Xβ) + λΩ(β) .

Recent works have addressed this problem by using gradient-
based algorithms, see for instance Pedregosa (2016);
Franceschi et al. (2018) who have shown promising results
in computational time and scalability w.r.t. multiple hyper-
parameters. Yet, they require assumptions such as smooth-
ness of the validation function Ev and non-singular Hessian
of the inner optimization problem at optimal values which
are difﬁcult to check in practice since they depend on the
optimal solutions ˆβ(λ). Moreover, they can only guarantee
convergence to stationary point.

In this section, we generalize the approach of (Shibagaki
et al., 2015) and show that with a safe and simple explo-
ration of the parameter space, our algorithm has a global
convergence property. For that, we assume the following
conditions on the validation loss and on the inner optimiza-
tion objective throughout the section:

A1

A2

(a, b)

(a, c)

|L
The function β

− L

(b, c) for any a, b, c
| ≤ L
Pλ(β) is µ-strongly convex.

∈

Rn.

(cid:55)→

6the data-ﬁtting terms might differ from training to testing; for
instance for logistic regression the (cid:96)0/1-loss is used for validation
but the logistic function is optimized at training.

The assumption on the loss function is veriﬁed for norms
(regression) and indicator functions (classiﬁcation). Indeed,
, A1 corresponds to the triangle inequal-
if
b
(cid:107)
i=1 1aibi<0, since for
ity. For the (cid:96)0/1-loss
1sv<0, one has
any real s, u and v,

(a, b) = 1
n
1uv<0

(a, b) =

a
(cid:107)

(cid:80)n

−

L

|

L
1us<0
n
(cid:88)

−

1
n

−

i=1

| ≤
(cid:12)
(cid:12)
(cid:12) ≤

1
n

n
(cid:88)

i=1

1aibi<0

1aici<0

1bici<0 .

(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

Deﬁnition 3. Given a primal solution ˆβ(λ) for parameter λ
and a primal point β(λt) returned by an algorithm, we deﬁne
the gap on the validation error between λ and λt as

∆Ev(λt, λ) :=

(cid:12)
(cid:12)Ev( ˆβ(λ))

(cid:12)
Ev(β(λt))
(cid:12) .

(15)

−

Suppose we have ﬁxed a tolerance (cid:15)v on the gap on valida-
tion error i.e., ∆Ev(λt, λ)
(cid:15)v. Based on Assumption A1,
if there is a region
λ that contains the optimal solution
ˆβ(λ) at parameter λ, then we have

R

≤

∆Ev(λt, λ)

(X (cid:48) ˆβ(λ), X (cid:48)β(λt))

≤ L

max
β∈Rλ L

≤

(X (cid:48)β, X (cid:48)β(λt)) .

λ as a ball.

A simple strategy consists in choosing
Lemma 4 (Gap safe region Ndiaye et al. (2017)). Under
A2, any primal solution ˆβ(λ) belongs to the Euclidean ball
with center β(λt) and radius
(cid:114) 2

R

rt,µ(λ) =

λ(β(λt), θ(λt)) .

(16)

µ G

Such a safe ball leveraging duality gap has been proved
useful to speed-up sparse optimization solvers. The improve
performance relies on the ability to identify the sparsity
structure of the optimal solutions; approaches of this type
are referred to as safe screening rules as they provide safe
certiﬁcates for such structures (El Ghaoui et al., 2012; Fer-
coq et al., 2015; Shibagaki et al., 2016; Ndiaye et al., 2017).

Since the radius in Equation (16) depends explicitly on the
duality gap, we can sequentially track a range of parameters
for which the gap on the validation error remains below a
prescribed tolerance by controlling the optimization error.
Proposition 7 (Grid for prescribed validation error). Under
[n(cid:48)] an index
Assumptions A1 and A2, let us deﬁne for i
(cid:16) x(cid:48)(cid:62)

(cid:17)2

∈

in the test set, ξi =

i β(λt)
(cid:107)x(cid:48)
i(cid:107)

and

(cid:15)v,µ =






µ
2 ×
µ
2 ×

(cid:17)2

(cid:16) (cid:15)v
(cid:107)X (cid:48)(cid:107)

,

ξ((cid:98)n(cid:15)v(cid:99)+1),

(regression)

(classiﬁcation)

(17)

n(cid:15)v

where ξ((cid:98)n(cid:15)v(cid:99)+1) is the (
ξi’s. Given (β(λt), θ(λt)) such that
∆Ev(λt, λ)
≤
(cid:2)1
t((cid:15)v,µ), 1 + ρr
ρ(cid:96)
λt
are deﬁned in Proposition 1.

≤
(cid:15)v for all parameter λ in the interval
t ((cid:15)v,µ)(cid:3) , where ρ(cid:96)
t((cid:15)v,µ), ρr

+ 1)-th smallest value of
(cid:15)v,µ, we have

t ((cid:15)v,µ)

−

×

G

(cid:99)

(cid:98)

t

Safe Grid Search with Optimal Complexity

(a) Synthetic data set generated using the sklearn command
X, y = make sparse uncorrelated(n = 30, p = 50).

(b) Synthetic data set generated using the sklearn command
X, y = make regression(n = 500, p = 5000).

Figure 4. Safe selection of the optimal hyperparameter for Enet on the validation set (30% of the observations). The targeted accuracy (cid:15)v is
reﬁned from δv ×10 to δv/10 with δv = maxλt∈Λ Ev(β(λt))−minλt∈Λ Ev(β(λt)) and Λ is the default grid between λmax = (cid:107)X (cid:62)y(cid:107)∞
and λmin = λmax/100 of size T = 200. The stars represent the worst case solution amount the one generated by Algorithm 2 (with
bilateral path). For loose precision suboptimal parameters are identiﬁed, but better ones are found as the accuracy (cid:15)v decreases.

(cid:15)v as soon as

Remark 2 (Stopping criterion for training). For the current
parameter λt, ∆Ev(λt, λt)
(cid:15)v,µ,
≤
which gives us a stopping criterion for optimizing on the
training part (X, y) relative to the desired accuracy (cid:15)v on
the validation data (X (cid:48), y(cid:48)). This has the appealing property
of relieving the practitioner from selecting the stopping
criterion (cid:15)c when optimizing on the training set.

≤

G

t

Algorithm 2 outputs a discrete set of parameters Λ((cid:15)v,µ)
β(λt) for λt
is an (cid:15)v-path for the
such that
Λ((cid:15)v,µ)
}
validation error. Thus, for any λ
[λmin, λmax], there
∈
exists λt

Λ((cid:15)v,µ) such that

∈

{

∈

Ev(β(λt))

(cid:15)v

−

≤

Ev( ˆβ(λ)) .

(18)

The following proposition is obtained by taking the mini-
mum on both sides of the inequality.
Proposition 8. Under Assumptions A1 and A2, the set
β(λt) for λt

is an (cid:15)v-path for the error and

Λ((cid:15)v,µ)

{

min
λt∈Λ((cid:15)v,µ)

∈
Ev(β(λt))

}

−

min
λ∈[λmin,λmax]

Ev( ˆβ(λ))

(cid:15)v .

≤

numbers of grid points T(cid:15) needed to achieve such a precision.
Our experiments were conducted on the leukemia dataset,
available in sklearn and the climate dataset NCEP/NCAR
Reanalysis (Kalnay et al., 1996). The optimization al-
gorithms are the same for all the grid, hence we compare
only the grid construction impact. Results are reported in
Figure 3 for classiﬁcation and regression problem. Our
approach leads to better guarantees for approximating the
regularization path w.r.t. the default grid and often signiﬁ-
cant gain in computing time.

Figure 4 illustrates convergence for Elastic Net (Enet) (Zou
& Hastie, 2005), on synthetic data generated by sklearn
as random regression problems make regression and
make sparse uncorrelated (Celeux et al., 2012).
For a decreasing levels of validation error, we represent
the λ selected by our algorithm and its corresponding safe
interval. Even when the validation curve is non smooth and
non convex, the output of the safe grid search converges to
the global minimum as stated in Proposition 8.

5. Numerical experiments

6. Conclusion

We illustrate our method on (cid:96)1-regularized least squares and
logistic regression by comparing the computational times
and number of grid points needed to compute an (cid:15)-path for
a given range [λmin, λmax] for several strategies.

The ”Default grid” is the one used by default in the packages
glmnet (Friedman et al., 2010) and sklearn (Pedregosa
10−δt/(T −1)
et al., 2011). It is deﬁned as λt = λmax
(here δ = 3). The proposed grids are the adaptive unilat-
eral/bilateral and uniform unilateral/bilateral grids that are
deﬁned in Propositions 3, 4 and 5.

×

Thanks to Proposition 2, we measure the approximation path
error (cid:15) of the default grid of size T and report the times and

We have shown how to efﬁciently construct one dimensional
grids of regularization parameters for convex risk minimiza-
tion, and to get an automatic calibration, optimal in term of
hold-out test error. Future research could examine how to
adapt our framework to address multi-dimensional parame-
ter grids. This case is all the more interesting that it naturally
arises when addressing non-convex problems, e.g., MCP or
SCAD, with re-weighted (cid:96)1-minimization. Approximation
of a full path then requires to optimize up to precision (cid:15)c at
each step, even for non promising hyperparameter, which
is time consuming. Combining our approach with safe
elimination procedures could provide faster hyperparameter
selection algorithms.

Safe Grid Search with Optimal Complexity

Acknowledgements

This work was supported by the Chair Machine Learning
for Big Data at T´el´ecom ParisTech. TL acknowledges the
support of JSPS KAKENHI Grant number 17K12745.

References

G¨artner, B., Jaggi, M., and Maria, C. An exponential lower
bound on the complexity of regularization paths. Journal
of Computational Geometry, 2012.

Giesen, J., Jaggi, M., and Laue, S. Approximating param-
In European

eterized convex optimization problems.
Symposium on Algorithms, pp. 524–535, 2010.

Arlot, S. and Celisse, A. A survey of cross-validation pro-
cedures for model selection. Statistics surveys, 4:40–79,
2010.

Giesen, J., M¨uller, J. K., Laue, S., and Swiercy, S. Approxi-
mating concavely parameterized optimization problems.
In NIPS, pp. 2105–2113, 2012.

Az´e, D. and Penot, J.-P. Uniformly convex and uniformly
smooth convex functions. In Annales de la facult´e des sci-
ences de Toulouse, pp. 705–730. Universit´e Paul Sabatier,
1995.

Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Convex
optimization with sparsity-inducing norms. Foundations
and Trends in Machine Learning, 4(1):1–106, 2012.

Bauschke, H. H. and Combettes, P. L. Convex analysis and
monotone operator theory in Hilbert spaces. Springer,
New York, 2011. ISBN 978-1-4419-9466-0.

Bottou, L. and Bousquet, O. The tradeoffs of large scale

learning. In NIPS, pp. 161–168, 2008.

Celeux, G., Anbari, M. E., Marin, J.-M., and Robert, C. P.
Regularization in regression: comparing bayesian and
frequentist methods in a poorly informative situation.
Bayesian Analysis, 7(2):477–502, 2012.

Clarkson, K. L. Coresets, sparse greedy approximation,
and the frank-wolfe algorithm. ACM Transactions on
Algorithms (TALG), 6(4):63:1–63:30, 2010.

Efron, B., Hastie, T., Johnstone, I. M., and Tibshirani, R.
Least angle regression. Ann. Statist., 32(2):407–499,
2004. With discussion, and a rejoinder by the authors.

El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature elim-
ination in sparse supervised learning. J. Paciﬁc Optim., 8
(4):667–698, 2012.

Fercoq, O., Gramfort, A., and Salmon, J. Mind the duality
gap: safer rules for the lasso. In ICML, pp. 333–342,
2015.

Franceschi, L., Frasconi, P., Salzo, S., and Pontil, M. Bilevel
programming for hyperparameter optimization and meta-
learning. In ICML, pp. 1563–1572, 2018.

Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire
regularization path for the support vector machine. J.
Mach. Learn. Res., 5:1391–1415, 2004.

Hoerl, A. E. and Kennard, R. W. Ridge regression: Biased
estimation for nonorthogonal problems. Technometrics,
12(1):55–67, 1970.

Juditski, A. and Nesterov, Y. Primal-dual subgradient meth-
ods for minimizing uniformly convex functions. arXiv
preprint arXiv:1401.1792, 2014.

Kalnay, E., Kanamitsu, M., Kistler, R., Collins, W., Deaven,
D., Gandin, L., Iredell, M., Saha, S., White, G., Woollen,
J., et al. The NCEP/NCAR 40-year reanalysis project.
Bulletin of the American meteorological Society, 77(3):
437–471, 1996.

Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and
Talwalkar, A. Hyperband: A novel bandit-based approach
to hyperparameter optimization. The Journal of Machine
Learning Research, 18(1):6765–6816, 2017.

Li, Y. and Singer, Y. The well tempered lasso. ICML, 2018.

Mairal, J. and Yu, B. Complexity analysis of the lasso

regularization path. In ICML, pp. 353–360, 2012.

McCullagh, P. and Nelder, J. A. Generalized Linear Models.

Chapman & Hall, 1989.

Ndiaye, E. Safe optimization algorithms for variable selec-
tion and hyperparameter tuning. PhD thesis, Universit´e
Paris-Saclay, 2018.

Ndiaye, E., Fercoq, O., Gramfort, A., and Salmon, J. Gap
safe screening rules for sparsity enforcing penalties. J.
Mach. Learn. Res., 18(128):1–33, 2017.

Friedman, J., Hastie, T., H¨oﬂing, H., and Tibshirani, R.
Pathwise coordinate optimization. Ann. Appl. Stat., 1(2):
302–332, 2007.

Nesterov, Y. Introductory lectures on convex optimization,
volume 87 of Applied Optimization. Kluwer Academic
Publishers, Boston, MA, 2004.

Friedman, J., Hastie, T., and Tibshirani, R. Regularization
paths for generalized linear models via coordinate descent.
J. Stat. Softw., 33(1):1, 2010.

Osborne, M. R., Presnell, B., and Turlach, B. A. A new
approach to variable selection in least squares problems.
IMA J. Numer. Anal., 20(3):389–403, 2000.

Safe Grid Search with Optimal Complexity

Park, M. Y. and Hastie, T. L1-regularization path algorithm
for generalized linear models. J. Roy. Statist. Soc. Ser. B,
69(4):659–677, 2007.

Pedregosa, F. Hyperparameter optimization with approxi-

mate gradient. In ICML, pp. 737–746, 2016.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. J. Mach. Learn.
Res., 12:2825–2830, 2011.

Rockafellar, R. T. Convex analysis. Princeton Landmarks in
Mathematics. Princeton University Press, Princeton, NJ,
1997. Reprint of the 1970 original, Princeton Paperbacks.

Rosset, S. and Zhu, J. Piecewise linear regularized solution

paths. Ann. Statist., 35(3):1012–1030, 2007.

Shibagaki, A., Suzuki, Y., Karasuyama, M., and Takeuchi,
I. Regularization path of cross-validation error lower
bounds. In NIPS, pp. 1666–1674, 2015.

Shibagaki, A., Karasuyama, M., Hatano, K., and Takeuchi,
I. Simultaneous safe screening of features and samples in
doubly sparse modeling. In ICML, pp. 1577–1586, 2016.

Sun, T. and Tran-Dinh, Q. Generalized self-concordant func-
tions: A recipe for newton-type methods. Mathematical
Programming, 2017.

Tibshirani, R. Regression shrinkage and selection via the
lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267–288, 1996.

Zou, H. and Hastie, T. Regularization and variable selection
via the elastic net. J. Roy. Statist. Soc. Ser. B, 67(2):
301–320, 2005.

Safe Grid Search with Optimal Complexity

7. Appendix

7.1. Generalized self-concordant functions

Proposition 9 (Sun & Tran-Dinh (2017), Proposition 10). If (Mf , ν)-generalized self concordant, then

wν(

dν(x, y))

−

y
(cid:107)

−

x
(cid:107)

2
x ≤

f (y)

f (x)

−

− (cid:104)∇

f (x), y

x

wν(dν(x, y))

−

(cid:105) ≤

y
(cid:107)

−

x

2
x ,
(cid:107)

where the right-hand side inequality holds if dν(x, y) < 1 for the case ν > 2 and where

and

dν(x, y) :=

(cid:40)

Mf
(cid:0) ν

(cid:107)
2 −

y
x
(cid:107)2
−
1(cid:1) Mf
y
(cid:107)

x

3−ν
2
(cid:107)

y
(cid:107)

x

(cid:107)

−

ν−2
x

−

if ν = 2,
if ν > 2,

wν(τ ) :=






eτ −τ −1
τ 2

−τ −log(1−τ )
τ 2

(1−τ ) log(1−τ )+τ
τ 2
(cid:104) ν−2
(cid:17) 1
(cid:16) ν−2
2(3−ν)τ
τ
4−ν

if ν = 2,
if ν = 3,
if ν = 4,

otherwise.

(cid:16)

(1

−

2(3−ν)
2−ν

τ )

(cid:17)
1

(cid:105)
1

−

−

(19)

(20)

(21)

The dual of the logistic loss is Generalized Self-Concordant with Mf ∗ = 1, ν = 4. Power loss function fi(z) = (yi
for q

(1, 2), popular in robust regression, is covered with Mf =

z)q
2−q . We refer to (Sun & Tran-Dinh,

, ν = 2(3−q)

−

2−q
(2−q)√q(q−1)

∈

2017) for more details and examples.

Remark 3. Note that the relation between
f is not always explicit. Uniform convexity and smoothness
V
do not always hold simultaneously for primal f and dual functions f ∗ and one needs to carefully consider the regularity
∗
f ∗ for uniformly smooth (Az´e & Penot, 1995, Coroll. 2.7) and for self
of the functions used. In general, we have
concordant, ν + ν∗ = 6 for ν∗

(0, 6) (Sun & Tran-Dinh, 2017, Prop 6).

f (cid:63) and

f =

f (cid:63) ,

f ,

U

U

U

V

V

∈

Figure 5. Illustration of the functions in self concordant bounds Equation (21)

Safe Grid Search with Optimal Complexity

7.2. Useful convexity inequalities

Lemma 5 (Fenchel-Young inequalities). Let f be a continuously differentiable function. For all x, x∗, we have

with equality if and only if x∗ =
and f ∗ is differentiable at x∗, Inequality (23) (resp. Inequality (24)) holds true:

f (x) (or equivalently x

∂f ∗(x∗)). Moreover, if f is

∇

∈

U

f,x-convex (resp.

f,x-smooth)

V

f (x) + f ∗(x∗)

x∗, x
(cid:105)

,

≥ (cid:104)

f (x) + f ∗(x∗)
f (x) + f ∗(x∗)

x∗, x
(cid:105)
x∗, x
(cid:105)

≥ (cid:104)

≤ (cid:104)

+

+

U

V

f,x(x
f,x(x

− ∇

− ∇

f ∗(x∗)) ,
f ∗(x∗)) .

Proof. We have from the

f,x-convexity and the equality f (z) + f ∗(

f (z)) =

f (z), z

U

f ∗(

f (z)) +

−

∇

U
We conclude by applying the inequality at z =
bound (24).

(cid:104)∇

∇

−

f (z), x
(cid:105)

+

f,x(x

z) = f (z) +

∇

∇
f (z), x

(cid:104)∇
z

(cid:105)

−

+

U

(cid:105)
f,x(x

(cid:104)∇

z)

f (x) .

−

≤

f ∗(x∗) and remark that

f (z) = x∗. The same proof holds for the upper

Applying Fenchel-Young Inequalities (23) and (24) give the following bounds.

Lemma 6. We assume that
that f is

f -convex (resp.

U

−
∈
f -smooth).

V

∈

λθ

Dom(f ∗) and X (cid:62)θ

Dom(Ω∗). Then, the Inequality (25) (resp. (26)) hold provided

λ(cid:101)Ω(β, θ) +

f (Xβ

U
f (Xβ

f ∗(

− ∇

f ∗(

− ∇

−

λθ))

−
λθ)) ,

≤ G

≥ G

λ(β, θ)

λ(β, θ) ,

λ(cid:101)Ω(β, θ) +

V
X (cid:62)θ

.

(cid:105)

β,

(cid:104)

−

where (cid:101)Ω(β, θ) = Ω(β) + Ω∗(X (cid:62)θ) +

Proof. We apply the Fenchel-Young Inequality (23) to obtain

G

λ(β, θ) = f (Xβ) + f ∗(
+
Xβ,
U
f ∗(

−
f (Xβ

≥ (cid:104)
=

λθ

−

(cid:105)

λθ) + λ(Ω(β) + Ω∗(X (cid:62)θ))
f ∗(

f (Xβ

λθ)) + λ (cid:0)Ω(β) + Ω∗(X (cid:62)θ) +

λθ)) + λ(Ω(β) + Ω∗(X (cid:62)θ))
X (cid:62)θ

− ∇

β,

−

(cid:1) .
(cid:105)

− ∇
The same technique applies for the upper bound with the Fenchel-Young Inequality (24).

−

U

(cid:104)

−

Remark 4. From the Fenchel-Young Inequality (22), we have Ω(β) + Ω∗(X (cid:62)θ)
non negative.

≥ (cid:104)

β, X (cid:62)θ

, so the lower bound is always
(cid:105)

7.3. Proof of the bounds for the approximation path error

Lemma 1. We assume that
ρ = 1

−

∈

λ/λt, the right (resp. left) hand side of Inequality (27) holds true

λθ(λt)

dom f ∗ and X (cid:62)θ(λt)

dom Ω∗. If f ∗ is

f ∗ -smooth (resp.

f ∗ -convex), then, for

∈

V

U

−

Qt,Uf ∗ (ρ)

λ(β(λt), θ(λt))

Qt,Vf ∗ (ρ) .

≤ G

≤

Proof. We recall that

t :=

λt(β(λt), θ(λt)) and we denote for simplicity

G

G
λt
λ :=

G

G

λ(β(λt), θ(λt))

and

Γt := Ω(β(λt)) + Ω∗(X (cid:62)θ(λt)) .

By deﬁnition for any (β, θ)
following holds

∈

dom Pλ

dom Dλ we have

λ(β, θ) = f (Xβ) + f ∗(

λθ) + λ(Ω(β) + Ω∗(X (cid:62)θ)), so the

×

G

−

1
λt

t

[
G

−

f (Xβ(λt))

f ∗(

λtθ(λt))] = Γt .

−

−

(22)

(23)

(24)

(25)

(26)

(27)

(28)

Safe Grid Search with Optimal Complexity

Hence using Equality (28) in the deﬁnition of

λt
λ , we have:

G

λθ(λt)) + λΓt

G

λt
λ = f (Xβ(λt)) + f ∗(
λ
λt

λ
λt G

(28)
=

t +

−

(cid:18)

1

−
(cid:19)

[f (Xβ(λt)) + f ∗(

λtθ(λt))] + f ∗(

λθ(λt))

f ∗(

λtθ(λt)) .

−

−

−

−

Let us write the proof for the upper bound (the proof for the lower bound is similar). We apply the smoothness property and
the Fenchel-Young Inequality (24) to the function f ∗(
) with z =
·
λ
λt

λθ(λt) and x = ζt :=
λ)θ(λt)(cid:17)

λtθ(λt) to obtain

λ
λt G

λt
λ ≤

∆t +

f ∗,ζt

t +

(λt

−

−

−

−

(cid:18)

(cid:19)

V

(cid:16)

G

1

,

where we have used the equality case in the Fenchel-Young Inequality (22) to get:

(cid:104)∇

V

∆t = f (Xβ(λt)) + f ∗(ζt) +

f ∗(ζt),

= f (Xβ(λt))

ζt

−

(cid:105)

f (

−

∇

f ∗(ζt)) .

We conclude by noticing that λ
deﬁnition of Qt,φ, from Equation (8), applied to φ =

λt G

t +

λ
λt

−

1

∆t =

G
f ∗ .

(cid:16)

(cid:17)

(cid:16)

t +

1

(cid:17)

λ
λt

(∆t

−

− G

t), that ζt =

λtθ(λt) and thanks to the

−

Proposition 1 (Grid for a prescribed precision). Given (β(λt), θ(λt)) such that
(cid:2)1
t((cid:15)), 1 + ρr
ρ(cid:96)
−
Qt,Vf ∗ (ρ)

t ((cid:15))(cid:3), we have
ρ)

λ(β(λt), θ(λt))
(cid:15)).

t((cid:15)) (resp. ρr

(cid:15) (resp. Qt,Vf ∗ (

(cid:15) where ρ(cid:96)

≤

G
≤

−

≤

t

(cid:15)c < (cid:15), for all λ
×
t ((cid:15))) is the largest non-negative ρ s.t.

λt

≤

∈

G

Proof. From Lemma 1, we have
(cid:2)inf

G
λ(cid:48) : Qt,Vf ∗ (1

λ

∈

{

λ(β(λt), θ(λt))

Qt,Vf ∗ (ρ) = Qt,Vf ∗ (1

λ/λt). Then,

λ(β(λt), θ(λt))

(cid:15) for

≤
(cid:15), λ(cid:48)

λ(cid:48)/λt)

−

≤

λt

,

}

≤

sup
{

−
λ(cid:48) : Qt,Vf ∗ (1

G
λ(cid:48)/λt)

−

(cid:15), λ(cid:48)

λt

≥

≤

≤
(cid:3) .
}

Proposition 2 (Precision for a Given Grid). Given a grid of parameter ΛT , the set
(cid:15)ΛT ≤
Qt+1,Vf ∗ (1

λ(cid:63)
t /λt) where for all t

maxt∈[T ] Qt,Vf ∗ (1

t is the largest λ

λ/λt+1).

1], λ(cid:63)

[T

−

−

∈

∈

β(λ) : λ

ΛT

is an (cid:15)ΛT -path and

{

∈
[λt+1, λt] such that Qt,Vf ∗ (1

}

λ/λt)

−

≥

−

λ(β(λt), θ(λt))
Proof. From the upper bound
G
∪t∈[0:T −1][λt+1, λt], we have thanks to the Deﬁnition of (cid:15)Λt in Equation (10):
parameter set as [λmin, λmax] =
(cid:15)Λt ≤
Qt,Vf ∗ (1

max
t∈[0:T −1]

Qt,Vf ∗ (1

min
λt∈ΛT

λ/λt)

λ/λt) for all λ and λt, and since one can partition the

sup
λ∈[λt+1,λt]

−

−

≤

max
t∈[0:T −1]

sup
λ∈[λt+1,λt]

≤

min
t(cid:48)∈{t+1,t}

Qt(cid:48),Vf ∗ (1

λ/λt(cid:48)) .

−

where the last inequality holds since

is a subset of ΛT . Let us deﬁne

λt+1, λt
{

}

λ

Qt+1,Vf ∗ (1
[λt+1, λt], ψt(λ) := min
{

∈
The quantity Qt+1,Vf ∗ (1
supλ∈[λt+1,λt] ψt(λ) is reached for λ(cid:63)

−

∀

λ/λt+1) (resp. Qt,Vf ∗ (1

t , the largest λ satisfying

−

λ/λt+1), Qt,Vf ∗ (1

λ/λt)

.

−
λ/λt)) is monotonically increasing w.r.t. λ (resp. decreasing), so

−

}

Qt,Vf ∗ (1

λ/λt)

Qt+1,Vf ∗ (1

λ/λt+1) .

−

≥

−

Corollary 1. If the function f ∗ is ν
closed-form expressions:

2 (cid:107)·(cid:107)

2-smooth, the left (ρ(cid:96)

t) and right (ρr

t ) step sizes deﬁned in Proposition 1 have

(cid:113)

2νδt

ρ(cid:96)
t =

˜δt

t −

ζt
(cid:107)
ν

2 + ˜δ2
(cid:107)
2
ζt
(cid:107)

(cid:107)

(cid:113)

2νδt

and ρr

t =

t + ˜δt

,

ζt
(cid:107)
ν

2 + ˜δ2
2
(cid:107)

(cid:107)
ζt

(cid:107)

where δt := (cid:15)

t and ˜δt := ∆t

− G

t.

− G

Safe Grid Search with Optimal Complexity

Proof. If f ∗ is ν

2-smooth (which is equivalent to f is 1

2-strongly convex), we have from Lemma 1

2 (cid:107)·(cid:107)

G
Hence we conclude by solving in ρ the inequality Qt,Vf ∗ (ρ)

≤

λ(β(λt), θ(λt))

Qt,Vf ∗ (ρ) =

t +ρ(∆t

t) +

νρ2
2 (cid:107)

ζt

(cid:107)

2 .

− G

2ν (cid:107)·(cid:107)

G

≤

(cid:15).

Lemma 2. For any β(λt)

Rp, the vector

∈

θ(λt) =

f (Xβ(λt))

max(λt, σ◦

−∇
dom Ω∗ (X (cid:62)

f (Xβ(λt)))

∇

,

is feasible:

λθ(λt)

dom f ∗, X (cid:62)θ(λt)

dom Ω∗.

−

∈

∈

Proof. The proof of this result and the convergence of the sequence of dual points is given in Proposition 11 and lemma 5 of
(Ndiaye, 2018, Chapter 2).

Variation of the loss function along the path
Lemma 7. Let β(λt) (resp. β(λt(cid:48) )) be an (cid:15)-solution at parameter λt (resp. λt(cid:48)), then we have

(cid:19) (cid:16)

(cid:18)

λt(cid:48)
λt

1

−

f (Xβ(λt(cid:48) ))

f (Xβ(λt))

−

(cid:17)

t(cid:48) +

≤ G

λt(cid:48)
λt G

t

.

where

s :=

G

G

λs(β(λs), θ(λs)) for s

t, t(cid:48)

Proof. Denote (cid:15) =

t and (cid:15)(cid:48) =

G

. Moreover, the mapping λ
}

∈ {

(cid:55)→
t(cid:48). Since β(λt(cid:48) ) is an (cid:15)(cid:48)-solution and ˆβ(λt(cid:48) ) is optimal at parameter λt(cid:48), we have:
G

f (X ˆβ(λ)) is non-increasing.

f (Xβ(λt(cid:48) )) + λt(cid:48)Ω(β(λt(cid:48) ))

(cid:15)(cid:48)

f (X ˆβ(λt(cid:48) )) + λt(cid:48)Ω( ˆβ(λt(cid:48) ))

f (Xβ(λt)) + λt(cid:48)Ω(β(λt)) .

Moreover,

f (Xβ(λt)) + λt(cid:48)Ω(β(λt)) =

(cid:16)

(cid:17)
f (Xβ(λt)) + λtΩ(β(λt))

+

f (Xβ(λt))

−

≤

λt(cid:48)
λt
λt(cid:48)
λt
λt(cid:48)
λt

(cid:16)

(cid:16)

≤

≤

(cid:16)

λt(cid:48)
λt

−

≤
(cid:16)

f (X ˆβ(λt)) + λtΩ( ˆβ(λt)) + (cid:15)

+

1

f (Xβ(λt))

f (Xβ(λ)) + λtΩ(β(λ)) + (cid:15)

+

1

f (Xβ(λt)) .

≤

(cid:19)

λt(cid:48)
λt

(cid:18)
1

(cid:17)

−
(cid:18)

(cid:19)

λt(cid:48)
λt
(cid:19)

−
λt(cid:48)
λt

−

(cid:17)

(cid:18)

(cid:17)

(cid:18)

(cid:19)

λt(cid:48)
λt

−

The last inequality comes from the optimality of ˆβ(λt) at parameter λt. Hence,

f (Xβ(λt(cid:48) )) + λt(cid:48)Ω(β(λt(cid:48) ))

(cid:15)(cid:48)

f (Xβ(λt(cid:48) )) + λtΩ(β(λt(cid:48) )) + (cid:15)

+

1

f (Xβ(λt)) .

At optimality, (cid:15) = 0 and we can deduce that

1

(cid:17)

λ
λt

−

f (X ˆβ(λ))

(cid:16)

1

≤

−

(cid:17)

λ
λt

f (X ˆβ(λt)), hence the second result.

Bounding the gradient along the path

We can furthermore bound the norm of the gradient of the loss when the parameter λ varies.
Lemma 8. For x

f,x-smooth, then writing

Dom(f ), if f is

∗
f,x = (

f,x)∗ for the Fenchel-Legendre transform, one has

∈

V

∗
f,x(

V

−∇

f (x))

≤

V
f (x)

V
inf
z

−

f (z) .

Proof. From the smoothness of f , we have

f (z)

inf
z

inf
z

≤

(f (x) +

f (x), z

(cid:104)∇

x

+

f,x(z

−

(cid:105)

V

−

x)) = f (x)

f,x)∗(
(
V

−

−∇

f (x)) .

Safe Grid Search with Optimal Complexity

A direct application of Lemma 8 and Lemma 7 yields:
Lemma 9. Assume that f is uniformly smooth and let β(λt(cid:48) ) (resp. β(λt)) be an (cid:15)-solution at parameter λt(cid:48) (resp. λt). Then
for δ(cid:15)(λt(cid:48), λt) := λt+λt(cid:48)

λt−λt(cid:48) (cid:15), we have

At optimality (cid:15) = 0 and so δ(cid:15)(λt(cid:48), λt) = 0 and we have

∗
f (

V

−∇

f (Xβ(λt(cid:48) )))

f (Xβ(λt)) + δ(cid:15)(λt(cid:48), λt) .

≤

Lemma 3. Assuming f uniformly smooth yields

additionally f is uniformly convex, this yields ∆t(cid:48)
Qt(cid:48),Vf ∗ (ρ)

(cid:101)Qt,Vf ∗ (ρ), where

≤

∗
f (

V

−∇

f (X ˆβ(λt(cid:48) )))

f (X ˆβ(λt)) .

≤
f (Xβ(λt(cid:48) ))
∗
(cid:107)
(cid:101)∆t, where (cid:101)∆t := (cid:101)Rt

≤

(cid:107)∇

≤

(cid:101)Rt, where (cid:101)Rt :=

∗
f
V
((cid:15)c) as well as

−1(cid:0)f (Xβ(λt)) + 2(cid:15)c
ρ(cid:96)
t((cid:15))

(cid:1). If

λ(β(λt(cid:48) ), θ(λt(cid:48) ))

G

≤

−1
f
× U

Proof. If f is uniformly smooth, from Lemma 9, we have:

(cid:101)Qt,Vf ∗ (ρ) = (cid:15)c + ρ

( (cid:101)∆t

(cid:15)c) +

·

−

(cid:16)

f ∗

V

(cid:101)Rt

ρ

|

| ·

(cid:17)

.

∗
f (

V

−∇

f (Xβ(λt(cid:48) )))

f (Xβ(λt(cid:48) ))
∗
(cid:107)

(cid:107)∇

≤

f (Xβ(λt)) + δ(cid:15)(λt(cid:48), λt)
2(cid:15)
ρ(cid:96)
t((cid:15))

f (Xβ(λt)) +

(cid:18)

−1

∗
f
≤ V

(cid:19)

,

where the ﬁrst line follows from Lemma 9 and the second follows from the fact that for the function

f =

V

V ◦ (cid:107)·(cid:107)

, we have

∗
f := (

f )∗ =

V

∗

V

◦ (cid:107)·(cid:107)∗. Finally, since λt(cid:48)

≤

V

λt, then

δ(cid:15)(λt(cid:48), λt)

2(cid:15)λt

≤

λt

λt(cid:48)

−

=

2(cid:15)
ρ(cid:96)
t((cid:15))

.

Since f is convex, we have

∆t := f (Xβ(λt))

f (

f ∗(

λtθ(λt)))

≤ −(cid:104)∇

f (Xβ(λt)),

f ∗(

λtθ(λt))

Xβ(λt)

∇

−

−

(cid:105)

−
f (Xβ(λt))
f (Xβ(λt))

∇

−
f ∗(
× (cid:107)∇
−1
f

(
G

× U

∗
(cid:107)
∗
(cid:107)

≤ (cid:107)∇

≤ (cid:107)∇

λtθ(λt))

Xβ(λt)

−
λt(β(λt), θ(λt))) .

−

(cid:107)

where the two last inequalities come respectively from Holder inequality and Lemma 6.

The bound on the duality gap directly comes from the bounds on ∆t(cid:48) and the norm of the gradient.

7.4. Proof of the complexity bound

Proposition 5. Assume that f is uniformly convex and smooth, and deﬁne the grid Λ(0)((cid:15)) =

λ0, . . . , λT(cid:15)−1
{

}

by

[T ], (β(λt), θ(λt)) s.t.

and
T(cid:15) grid points where

t
∀

∈

t

G

≤

(cid:15)c < (cid:15). Then the set

λ0 = λmax,

λt+1 = λt

(1

ρ0((cid:15))) ,

×

−

Λ(0)((cid:15))

}

β(λt) : λt
{

∈
(cid:23)
(cid:22) log(λmin/λmax)
ρ0((cid:15)))

log(1

.

−

T(cid:15) =

is an (cid:15)-path for Problem (1) with at most

(29)

Proof. By construction, given any two consecutive grid point λt and λt+1, we have
t+1 are smaller than (cid:15).
Moreover, since β(λt) is an (cid:15)-solution for any λ in the interval [λt+1, λt] whose union forms a covering of [λmin, λmax], we
conclude that the uniform grid is an (cid:15)-path.

t and

G

G

By deﬁnition, λmin = λT(cid:15), λmax = λ0 and ρ0((cid:15)) = 1
λT(cid:15) = λ0

ρ0((cid:15)))T(cid:15).

(1

−

∈

×

−

λt+1/λt

(0, 1). The conclusion follows from the fact that

Safe Grid Search with Optimal Complexity

Now we present the proof for the general case. We denote T(cid:15) the cardinality of the grid returned by Algorithm 1 and let
(ρt)t∈[0:T(cid:15)−1] be the set of step size needed to cover the interval [λmin, λmax]. Using ρt = 1

, we have

λt+1
λt

−

log

(cid:19)

(cid:18) λmax
λmin

= log

(cid:33)

(cid:32)T(cid:15)−1
(cid:89)

t=0

λt
λt+1

=

log

T(cid:15)−1
(cid:88)

t=0

(cid:19)

.

(cid:18) 1
1

−

ρt

Hence, denoting ρmin((cid:15)) = mint∈[0:T(cid:15)−1] ρt, we have

T(cid:15)

ρmin((cid:15))

log

×

≤

(cid:19)

(cid:18) λmax
λmin

.

(30)

We assume that we explore the parameter range in decreasing order. Also, to simplify the complexity analysis we will
suppose that at each step λt, we have solved the optimization problem with two measures of accuracy
(cid:15)c
for (cid:15)c < (cid:15).

(cid:15)c and ∆t

≤

≤

G

t

Remark 5. It is important to note that the usual stop criterion at each step t is
additional constraint ∆t
and ∆t converge to zero.

≤

(cid:15)c can be satisﬁed by any converging optimization solver (e.g., coordinate descent) since both

t

G

≤

(cid:15)c which is used in our algorithm. The
t

G

Then we recall from Lemma 1 that
Since ρmin((cid:15)) = mint∈[0:T(cid:15)−1] ρt = mint∈[0:T(cid:15)−1] sup
ρ : Qt,Vf ∗ (ρ)
{

Qt,Vf ∗ (ρ) which is smaller than (cid:15) as soon as
(cid:15)

, then
}

λ(β(λt), θ(λt))

≤

≤

G

f ∗,ζt(

ζt

ρ)

−

·

(cid:15)

(cid:15)c.

≤

−

V

ρmin((cid:15))

min
t∈[0:T(cid:15)−1]

sup
{

ρ :

≥

f ∗,ζt(

ζt

ρ)

−

·

V

(cid:15)

≤

−

.

(cid:15)c

}

(31)

Hence the complexity of the path is bounded as follows.

Proposition 6 (Complexity of the approximation path). Assuming that max(
Cf ((cid:15)c) > 0 such that

G

≤

t, ∆t)

(cid:15)c < (cid:15) at each step t, there exists

T(cid:15)

log

≤

(cid:19)

(cid:18) λmax
λmin

Cf ((cid:15)c)
f ∗ ((cid:15)

−

,

(cid:15)c)

×

W

where for all t > 0, the function

f ∗ is deﬁned by

W

(cid:40)

−1
f ∗ ,
,
·

V
√

f ∗ =

W

if f is uniformly convex and smooth,
if f is Generalized Self-Concordant and uniformly-smooth.

Moreover,

where

and

Cf ((cid:15)c) =

(cid:40)

(cid:101)R0,
(cid:113) ¯R0

wν (−Bf ) ,

if f is uniformly convex and smooth,

if f is Generalized Self-Concordant and uniformly-smooth.

(cid:18)

−1

(cid:101)R0 =

∗
f
V

(cid:19)

2(cid:15)c
ρ(cid:96)
0((cid:15))

f (Xβ(λ0)) +

, and ¯R0 = f (Xβ(λ0)) +

2(cid:15)c
ρ(cid:96)
0((cid:15))

+ (cid:15)c

dν(z) : z
Bf = sup
{

∈

Rn, ψ(z)

¯R0,

≤

z
(cid:107)

(cid:107)∗ ≤

(cid:101)R0

}

.

Proof. In the uniformly convex case,
that

V

f ∗,ζt(

ζt

ρ) =

f ∗ (ρ

−

·

V

ζt

(cid:107)∗), hence we can deduce from Equation (30) and (31)

(cid:107)

1

T(cid:15)

≤

ρmin((cid:15)) ×

log

(cid:19)

(cid:18) λmax
λmin

(cid:19)

(cid:18) λmax
λmin

×

log

≤

maxt∈[0:N(cid:15)−1] (cid:107)
−1
(cid:15)c)
f ∗ ((cid:15)

ζt

V

−

(cid:107)∗

,

Safe Grid Search with Optimal Complexity

so we just need to uniformly bound

ζs
(cid:107)

(cid:107)∗. By construction of the dual point Lemma 2, we have:

ζt
(cid:107)

(cid:107)∗ =

max(λt, σ◦

dom Ω∗ (X (cid:62)

f (Xβ(λt))) (cid:107)∇

f (Xβ(λt))

∗
(cid:107)

≤ (cid:107)∇

f (Xβ(λt))
∗
(cid:107)

≤

(cid:101)R0 ,

(32)

λt

∇

where the last inequality comes from Lemma 3 applied at λt and λ0.

) in Equation (21) are increasing and
For the Generalized Self-Concordant case, we ﬁrst recall that the functions wν(
·
[0, aν] (in fact aν = 1 for the logistic
wν(0) = 1/2. Then there exists a positive constant aν such that wν(τ )
∈
ρ2
regression). Thus, provided ρdν(ζt)
≤

aν, we can derive the bound

1 for τ
ρ)
ζt

≤
f ∗ (
−

2
ζt
(cid:107)

ζt
(cid:107)

×

≤

V

.

Like in the uniformly convex case, in order to get the complexity of the (cid:15)-path, we also need a uniform bound on
taking (6) on f ∗ with x = ζt and z = 0, we obtain

ζt

(cid:107)

(cid:107)ζt

. By

wν(

dν(ζt))

−

2
ζt =

ζt
(cid:107)

(cid:107)

U

f ∗,ζt(

ζt)

−

f ∗(0)

f ∗(ζt)

−
f (Xβ(λt)) + (cid:15)c

f ∗(ζt),

ζt

−

− (cid:104)∇

f (Xβ(λ0)) +

≤

≤

≤

(cid:105)

= f (
2(cid:15)c
ρ(cid:96)
0((cid:15))

f ∗(ζt)) = f (Xβ(λt))

∆t

−

∇
+ (cid:15)c =: ¯R0 ,

where we used the inequality case of Fenchel-Young Inequality and the fact that f ∗(0) =
ψ(ζt) :=
ζt)
closed. Recalling Equation (32), we have

−
¯R0. Since the function ψ is continuous, then its level set is closed i.e.,
(cid:101)R0. Then we have

f ∗,ζt(

≤

−

ζt

U

inf f = 0. This shows that
Rn : ψ(z)
is

¯R0

z
{

∈

≤

}

(cid:107)

(cid:107)∗ ≤

dν(ζt)

sup
z∈Hf

≤

dν(z) =: Bf where

f :=

H

z

{

∈

Rn : ψ(z)

¯R0

≤

} ∩ {

z :

z
(cid:107)

(cid:107)∗ ≤

(cid:101)R0

}

is a compact set.

Since the function wν(
·
2
ζt ≤
(cid:107)
√

This implies that

ζt
(cid:107)

Whence ρmin((cid:15))

mint

(cid:15)−(cid:15)c
(cid:107)ζt(cid:107)ζt

≥

) is increasing, we have wν(

ζt
(cid:107)
wν (−Bf ) . Thus, provided ρdν(ζt)

Bf )

¯R0

−

wν(

dν(ζt))

2
ζt ≤
(cid:107)
¯aν, we can derive the bound

2
ζt ≤
(cid:107)

ζt
(cid:107)

−

¯R0.

≤

f ∗ (ζt

ρ)

ρ2

ζt

.

2
ζt
(cid:107)

(cid:107)

≤

. Hence the complexity is bounded as T(cid:15)

log

×
V
(cid:17) √ ¯R0/wν (−Bf )
√

.

(cid:15)−(cid:15)c

(cid:16) λmax
λmin

≤

7.5. Proof of the validation error bounds

Proposition 7 (Grid for a prescribed validation error). Suppose that we have solved problem (1) for a parameter λt up to
accuracy

ξ((cid:15)v, µ, X (cid:48)), then we have ∆Ev(λt, λ)

λt(β(λt), θ(λt))

G

≤

λ

λt

(cid:2)1

(cid:15)v for all
≤
t (ξ((cid:15)v, µ, X (cid:48)))(cid:3) ,
t(ξ((cid:15)v, µ, X (cid:48))), 1 + ρr
ρ(cid:96)

where ρ(cid:96)

t((cid:15)) and ρr

×
t ((cid:15)) for (cid:15) > 0 are deﬁned in Proposition 1.

−

∈

Proof. We distinguish the two cases of interest: classiﬁcation and regression.

Case where the loss function is a norm:

•

we have

max
β∈B(β(λt),r) L

(X (cid:48)β, X (cid:48)β(λt)) =

max
β∈B(β(λt),r)(cid:107)

X (cid:48)(β

β(λt))

−

rλ,µ

X (cid:48)
(cid:107)

(cid:107)

,

(cid:107) ≤

where rλ,µ is the duality gap safe radius deﬁned in Equation (16). Hence by using the bounds on the duality gap in
Lemma 1, we can ensure ∆Ev(λt, λ)

λ/λt such that Qt,Vf ∗ (ρ)

(cid:15)v for all ρ = 1

µ(cid:15)2
2(cid:107)X (cid:48)(cid:107)2 .
v

≤

−

≤

Case where the loss function is the indicator function:

•

using the inequality
β

2ab
(β(λt), r) we have:

−

≤

∈ B

(a

b)2

b2 for a = x(cid:48)(cid:62)

i β and b = x(cid:48)(cid:62)

i β(λt) and

x(cid:48)(cid:62)
|

i (β

−

β(λt))

r

x(cid:48)
(cid:107)

i

(cid:107)

| ≤

for all

−
−
i β)(x(cid:48)(cid:62)
2(x(cid:48)(cid:62)

−

i β(λt))

(r

x(cid:48)
(cid:107)

i

)2
(cid:107)

−

≤

(x(cid:48)(cid:62)

i β(λt))2 .

Hence we obtain the following upper bound

Safe Grid Search with Optimal Complexity

max
β∈B(β(λt),r) L

(X (cid:48)β, X (cid:48)β(λt)) =

max
β∈B(β(λt),r)

1(x(cid:48)(cid:62)

i β(λt))(x(cid:48)(cid:62)

i β)<0 ≤

1|x(cid:48)(cid:62)

i β(λt)|≤r(cid:107)x(cid:48)

i(cid:107) .

1
n

n
(cid:88)

i=1

By using the bound on the duality gap, we can ensure ∆Ev(λ0, λ)

(cid:15)v for all λ such that:

≤



i


∈

#

[n] : ξi :=

(cid:33)2

µ
2

(cid:32)

x(cid:48)(cid:62)

i β(λt)
x(cid:48)
i(cid:107)
(cid:107)

Qt,Vf ∗ (1

λ/λt)

−

≤

n(cid:15)v

.

(cid:99)

By denoting (cid:0)ξ(i)
values i.e., we choose λ such that:

(cid:1)

i∈[n] the (increasing) ordered sequence, we need the inequality to be true for at most the

n(cid:15)v

ﬁrst

(cid:98)

(cid:99)

1
n

n
(cid:88)

i=1



 ≤ (cid:98)

Qt,Vf ∗

1

(cid:18)

(cid:19)

λ
λt

−

<

µ
2

(cid:32)

x(cid:48)(cid:62)
((cid:98)n(cid:15)v(cid:99)+1)β(λt)
(cid:13)
(cid:13)
(cid:13)x(cid:48)
(cid:13)

((cid:98)n(cid:15)v(cid:99)+1)

(cid:33)2

.

7.6. Additional Experiments

We add an additional experiments in large scale data with n = 16087 observations and p = 1668737 features.

Figure 6. (cid:96)1 least-squares regression on the ﬁnancial dataset E2006-log1p (available in libsvm) with n = 16087 observations and
p = 1668737 features. We have used the same (vanilla) coordinate descent optimization solver with warm start between parameters for
all grids. Note that a smaller grid do not imply faster computation, as the interplay with the warm-start can be intricate in our sequential
approach.

