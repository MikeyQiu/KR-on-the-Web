Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Jaron J. R. Lee 1 Ilya Shpitser 1

0
2
0
2
 
r
p
A
 
5
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
5
1
1
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Causal inference quantiﬁes cause-effect relation-
ships by estimating counterfactual parameters
from data. This entails using identiﬁcation the-
ory to establish a link between counterfactual pa-
rameters of interest and distributions from which
data is available. A line of work characterized
non-parametric identiﬁcation for a wide variety
of causal parameters in terms of the observed
data distribution. More recently, identiﬁcation
results have been extended to settings where ex-
perimental data from interventional distributions
In this paper, we use Single
is also available.
World Intervention Graphs and a nested factor-
ization of models associated with mixed graphs
to give a very simple view of existing identiﬁca-
tion theory for experimental data. We use this
view to yield general identiﬁcation algorithms for
settings where the input distributions consist of
an arbitrary set of observational and experimen-
tal distributions, including marginal and condi-
tional distributions. We show that for problems
where inputs are interventional marginal distribu-
tions of a certain type (ancestral marginals), our
algorithm is complete.

1. Introduction

Causal inference quantiﬁes cause-effect relationships using
parameters associated with counterfactual responses to an
intervention operation, where variables are set to values,
possibly contrary to fact. This operation is denoted by do(.)
in (Pearl, 2009). In statistics and public health, counterfac-
tual responses, or potential outcomes are denoted as Y (a),
which reads “the variable Y had the set of variables A
been set to values a.” Cause-effect relationships are quan-
tiﬁed by low dimensional parameters of counterfactual dis-
tributions. For example, the average causal effect (ACE)
E[Y (a)] − E[Y (a′)] quantiﬁes the impact of treatment vari-

1Department of Computer Science, Johns Hopkins University,
Baltimore, Maryland, USA. Correspondence to: Jaron J. R. Lee
<jaron.lee@jhu.edu>.

ables A on the outcome Y by comparing means in a hypo-
thetical randomized controlled trial where the treatments in
one arm are set to a, and in another arm to a′.
Counterfactual outcomes Y (a) are linked to factual out-
comes Y using the consistency property which states that
for any unit in the data where A is observed to equal a,
Y (a) = Y . However, values of Y (a′) for such units are
unobserved if a′ 6= a, leading to the fundamental problem
of causal inference. This problem is addressed by causal
models, which use assumptions on the joint distribution of
factual and counterfactual random variables to express de-
sired causal parameters as functionals of the observed data
distribution.

As a simple example (known in the literature as the con-
ditional ignorability model), if observed variables include
the treatment A, outcome Y , and a set of baseline covari-
ates C, and these covariates sufﬁce to adjust for confound-
ing (meaning that the conditional ignorability assumption
Y (a) ⊥⊥ A | C holds), then under the positivity condition
p(a | C) > 0 for all a, the ACE is identiﬁed by the adjust-
ment formula:

E[Y (a)] − E[Y (a′)] = E[E[Y |a, C] − E[Y |a′, C]].

A complete theory has been developed that uses as-
sumptions in a causal model to check which interven-
tional distributions are identiﬁed, and express all identiﬁ-
able interventional distributions as functionals of the ob-
served data (Tian & Pearl, 2002; Shpitser & Pearl, 2006;
Huang & Valtorta, 2006). If the causal model yields the ob-
served data distribution that admits a factorization with re-
spect to a graph, then identiﬁed interventional distributions
are always equal to modiﬁed factorizations of an appro-
priate graphical model representing the observed data dis-
tribution (Lauritzen & Richardson, 2002; Richardson et al.,
2017).

A natural generalization considered the problem of identi-
ﬁcation from surrogate experiments where a target causal
parameter is expressed in terms of a set of distributions aris-
ing from performing experiments on a particular population
(including possibly the “null experiment,” which recovers
the observed data distribution). A line of work gave in-
creasingly general identiﬁcation algorithms for this prob-
lem (Bareinboim & Pearl, 2012; Lee et al., 2019).

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

In this paper, we show that Single World Intervention
Graphs (SWIGs) (Richardson & Robins, 2013), and the
nested factorization of mixed graphs (Richardson et al.,
2017) yield a very simple view of the theory of identiﬁ-
cation from experimental data. We use this view to give a
series of general algorithms for identiﬁcation in terms of ar-
bitrary sets of observational or interventional distributions.
In addition, we show that for a particular class of inputs,
our algorithm is complete.

1.1. Motivating Example

Consider a causal model represented by a graph shown in
Fig. 1a, where directed edges denote causation, and bidi-
rected edges denote the presence of a hidden common
cause 1. Here Y is a health outcome, namely the pres-
ence of cardiovascular disease, X1 denotes whether a hip
replacement was performed, and X2 denotes whether an
atrial valve replacement was performed. Furthermore, let
W denote the ability to walk (inﬂuenced by whether hip
replacement was performed), and U denote an aspect of
heart health (such as valve regurgitation). Hip problems
and heart problems do not have any direct causal connec-
tion, but are certainly confounded by a patient’s general
health. Plausibly, a hip replacement which causes a patient
to be unable to walk would certainly impact their overall
health, and could contribute to the development of cardio-
vascular disease. Additionally, the measure U and heart
disease Y is confounded by the doctor’s latent knowledge
of the patient’s health.

We want to learn how hip surgery and valve replace-
ment surgeries affect cardiovascular disease by consider-
ing the distribution p(Y (x1, x2)). Given the type of un-
observed confounding present in the problem, existing re-
sults in Shpitser & Pearl (2006) imply that this distribu-
tion is not identiﬁed from the observed data distribution
p(X1, X2, W, U, Y ).

However, suppose that we have access to a data set where
patients (from the population we wished to consider)
elected to be randomized to hip replacement versus a
non-invasive alternative treatment, and to another data
set whre patients elected to be randomized to valve
replacement versus a non-invasive alternative treatment.
Data from these RCTs is represented by interventional dis-
tributions p(Y (x1), X2(x1), W (x1), U (x1), Y (x1)), and
p(Y (x2), X1(x2), W (x2), U (x2), Y (x2)),
respectively.
Graphs representing these two RCTs, called mutilated
graphs (Pearl, 2009), are shown in Figs. 1b and 1c. The
gID algorithm, described in Lee et al. (2019) is able to
identify the target distribution p(X1, X2, W, U, Y ) from
the two input interventional distributions above.

1This example is inspired by Fig. 1 in Lee et al. (2019).

While gID was proven sound and complete, it has the
limitation that requires that every input distribution con-
tains every observed variable: either as an outcome, or an
intervened-on treatment. This limits its utility for certain
types of causal inference problems, as we now illustrate.
Suppose that the available RCTs were performed by sepa-
rate research groups with differing data collection policies.
For example, the RCT studying hip surgery was scoped
only for its impact on walking ability, yielding the distri-
bution p(W (x1)), represented by Fig. 1d In practice, we
should not expect all studies used for analysis to contain all
variables relevant in the problem. Marginal distributions
are not valid inputs for gID, and require a more general al-
gorithm.

In this paper, we consider extensions to the gID algorithm
that are able to identify interventional distributions given
increasingly arbitrary interventional distributions as inputs.
We build up to the most general algorithm by considering
how gID generalizes in a number of special cases.

The paper is organized as follows. We introduce neces-
sary preliminaries in Section 2, reformulate the existing
identiﬁcation algorithm for experimental distribution in-
puts (Lee et al., 2019), as well as our generalizations in
Section 3, and describe completeness results in Section
4. Section 5 contains our conclusions. We defer proofs of
all results to the Appendix.

2. Graphs and Graphical Models

Let capital letters X denote random variables, and let lower
case letters x values. Sets of random variables are denoted
V, and sets of values v. For a subset A ⊆ V, vA denotes
the subset of values in v of variables in A. Domains of X
and X are denoted by XX and XX, respectively.

siblings

graph G are

and
denoted

descendants,
a

standard genealogic
children,
of X in

relations on graphs:
We
use
an-
parents,
by
cestors
respec-
paG(X), chG(X), deG(X), siG(X), anG(X),
These relations are deﬁned
tively (Lauritzen, 1996).
disjunctively for sets, e.g. paG(X) ≡
X∈X paG(X).
We will also deﬁne the set of strict parents as follows:
G(X) = paG(X) \ X. Given any vertex V in an
pas
ADMG G, deﬁne the ordered Markov blanket of V as
mbG(V ) ≡ (disG(V ) ∪ paG(disG(V ))) \ {V }. Given a
graph G with vertex set V, and S ⊆ V, deﬁne the induced
subgraph GS to be a graph containing the vertex set S and
all edges in G among elements in S.

S

We will consider directed acyclic graphs (DAGs), which
are graphs with directed edges and no directed cycles, and
acyclic directed mixed graphs (ADMGs), which are graphs
with directed and bidirected edges and no directed cycles.
A bidirected connected component in an ADMG is called

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

X1 X2

x1

X2

X1

x2

x2

x1

x2

W (x1)

U

W

U (x2)

x1

W

W (x1)

U (x2)

Y (x1)

Y (x2)

W (x1)

Y (x2)

Y (x1, x2)

(a) G

(b) G1

(c) G2

(d) G3

(e) G4

(f) G5

Figure1. A graph G and corresponding interventional distributions for the example given in Section 1.1.

W

U

Y

a district (also known as a c-component). A set of dis-
tricts of an ADMG G with vertex set V, which we will
denote by D(G), partitions V. The district of V ∈ V in
G(V) is denoted by disG(V)(V ). By convention, for any
X, anG(X) ∩ deG(X) ∩ disG(X) = {X}.
A statistical model of DAG G with vertex set V is the set
of all distributions p(V) that Markov-factorize according
to G, as follows: p(V) =

V ∈V p(V | paG(V )).

Q

Causal models are sets of distributions on counterfactual
random variables. For some Y ∈ V, A ⊆ V\{Y }, a coun-
terfactual random variable Y (a) reads “value of Y had A
been set, possibly contrary to fact, to a.” For convenience
we will denote distributions over multiple counterfactuals
p(Y1(a), . . . , Yk(a))) as p({Y1, . . . , Yk}(a)), or p(Y(a))
if Y ≡ {Y1, . . . , Yk}. The same distribution had been de-
noted by p(Y|do(a)) in (Pearl, 2009).
Causal models of a DAG G(V) are deﬁned on counter-
factual variables V (aV ), for all aV ∈ XpaG(V ).
In this
paper, we use Pearl’s functional model for a DAG G(V),
which is deﬁned by the restriction that the sets of vari-
ables {V (aV) | aV ∈ XpaG(V )} for every V ∈ V
are mutually independent. Under this model, for ev-
ery A, the distribution p(V(a)) is identiﬁed by a modi-
ﬁed DAG factorization known as the extended g-formula:

V ∈V p(V |apaG (V )∩A, paG(V )).

Conditional independences in p(V(a)) implied by a causal
Q
DAG model may be read off from a special DAG called
a Single World Intervention Graph (SWIG). Given a set
A ⊆ V of variables and an assignment a to those variables,
a SWIG G(V(a)) is constructed from G(V) by splitting
all vertices in A into a random half and a ﬁxed half, with
the random half inheriting all edges with an incoming ar-
rowhead and the ﬁxed half inheriting all outgoing directed
edges. Then, all random vertices Vi are re-labelled as Vi(a)
or equivalently as Vi(ai), where ai consists of the values
of ﬁxed ﬁxed vertices that are ancestors of Vi in the split
graph; the latter labelling is referred to as the minimal la-
belling of the SWIG. Under standard causal models of a
DAG, the interventional distribution p(V(a)) factorizes as

follows with respect to the SWIG G(V(a)):

p(V (a)|{W (a) : W ∈ paG(V(a))(V (a)) \ a}),

YV (a)∈V(a)
where each p(V (a)|{W (a) : W ∈ paG(V(a))(V (a)) \ a})
is only a function of a that are parents of V (a) (this qual-
iﬁcation is substantive and potentially deﬁnes restrictions).
This factorization allows us to use standard d-separation
relations on the SWIG G(V(a)) (that potentially allow
one of the endpoints to be a ﬁxed vertex, and treat all
other ﬁxed vertices as conditioned on) to discover condi-
tional independence or exclusion restrictions on p(V(a)).
See Richardson & Robins (2013); Malinsky et al. (2019)
for more details.

Most causal models in practice contain hidden variables,
which signiﬁcantly complicates identiﬁcation theory. An
interventional distribution p(Y(a)) may not be identiﬁed at
all from the observed marginal distribution in hidden vari-
able models. However, if p(Y(a)) is identiﬁed, it is equal
to a modiﬁed factorization of a graphical model associated
with a certain mixed graph, just as identiﬁed p(Y(a)) are
equal to a modiﬁed DAG factorization if the causal model
is fully observed.

A hidden variable causal model of a DAG is represented
by a DAG G with vertices V ∪ H, with V represent-
ing observed variables, and H representing hidden vari-
ables. Given a hidden variable DAG G(V ∪ H), the mixed
graph we will be interested in is called a latent projec-
tion of G(V ∪ H) on V, and will be denoted by G(V)
(by analogy with marginal distribution notation in proba-
bility theory). G(V) is an ADMG with vertices V, a di-
rected edge between any Vi, Vj linked by a directed path in
G(V ∪ H) where all intermediate vertices are in H, and a
bidirected edge between any Vi, Vj linked by a marginally
d-connected path in G(V ∪ H) where all intermediate ver-
tices are in H, the ﬁrst edge is directed into Vi, and the last
edge is directed into Vj.

The latent projection operation generalizes in the natural
way to SWIGs. Just as a latent projection G(V) of a hid-
den variable DAG G(V ∪ H) represents the structure of a
marginal distribution p(V)) obtained from p(V ∪ H), so

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

does a latent projection G(V) of a SWIG G({V ∪ H}(a))
represents the structure of a marginal counterfactual distri-
bution p(V(a)) obtained from p({V ∪ H}(a)).
A marginal SWIG G(V(a)) may be constructed from the
SWIG G({V∪H}(a)) (itself constructed from a latent vari-
able DAG G(V ∪H) by splitting vertices in A) by “project-
ing out” variables H(a) using the standard latent projection
construction. Note that the operations of splitting vertices
that yield SWIGs, and projecting out vertices correspond-
ing to hidden variables commute provided all split vertices
correspond to observed variables (Malinsky et al., 2019).
A hidden variable DAG G(V ∪ H) may be used to deﬁne
a factorization on marginal distributions p(V) in terms of
the DAG as: p(V) =
V ∈V∪H p(V | paG(V )). How-
ever, such a factorization is difﬁcult to work with in causal
Q
inference applications, since the corresponding likelihood
is difﬁcult to specify correctly, and leads to a model with
singularities (Drton, 2009).

P

H

A principled alternative is to deﬁne a factorization of a
marginal distribution p(V) directly on the latent projection
ADMG G(V). Such a nested Markov factorization, de-
scribed in Richardson et al. (2017) completely avoids mod-
eling hidden variables, and leads to a regular likelihood
in special cases (Evans & Richardson, 2018; Shpitser et al.,
2018), while capturing all equality constraints a hidden
variable DAG factorization imposes on the observed mar-
gin p(V) (Evans, 2018). In addition, p(Y(a)) identiﬁed in
a hidden variable causal model represented by G(V ∪ H) is
always equal to a modiﬁed version of a nested factorization
(Richardson et al., 2017) associated with G(V), which we
brieﬂy describe.

2.1. The Nested Markov Factorization

The nested factorization of p(V) with respect to an ADMG
G(V) links Markov kernels derived from p(V) to condi-
tional graphs derived from G(V) via a graphical and prob-
abilistic ﬁxing operator.
A conditional ADMG (CADMG) is a graph G(V, W) with
random vertices V, and ﬁxed vertices W, directed and
bidirected edges, no directed cycles, and no edges with ar-
rowheads into any element of W. All genealogic relations
transfer from ADMGs to CADMGs without change, except
districts in a CADMG are deﬁned only on the set V. A
CADMG without bidirected edges is called a conditional
DAG (CDAG).
A kernel qV(V|W) is a mapping from XW to normal-
ized densities over V. A conditional distribution is a ker-
nel, although some kernels are not conditional distributions
C p(Y |a, C)p(C) is a ker-
– for example, qY (Y |a) =
nel arising under conditional ignorability that is not equal
P
to p(Y |a) unless A is marginally independent of C. For

P

any A ⊆ V, we deﬁne the following shorthand notation:
V\A qV(V|W), qV(V \ A|A, W) ≡
qV(A|W) ≡
qV(V|W)/qV(A|W).
V ∈ V is said to be ﬁxable in G(V, W) if deG(V ) ∩
disG(V ) = ∅. We deﬁne a ﬁxing operator φV (G) for
graphs, and a ﬁxing operator φV (q; G) for kernels.
Given a CADMG G(V, W), with a ﬁxable V ∈ V,
φV (G(V, W)) yields a new CADMG G(V \ {V }, W ∪
{V }) obtained from G(V, W) by moving V from V
to W, and removing all edges with arrowheads into V .
Given a kernel qV(V|W), and a CADMG G(V, W),
φV (qV(V|W), G(V, W)) yields a new kernel:

qV\{V }(V \ {V }|W ∪ {V }) ≡

qV(V|W)
qV(V | mbG(V ), W)

.

A sequence hV1, . . . , Vki is said to be valid in G(V, W)
if V1 ﬁxable in G(V, W), V2 is ﬁxable in φV1 (G(V, W)),
and so on. If any two sequences σ1, σ2 for the same set
S ⊆ V are ﬁxable in G, they lead to the same CADMG.
As a result, we extend the graph ﬁxing operator to a set
S: φS(G). This operator is deﬁned as applying the vertex
ﬁxing operation in any valid sequence on elements in S.

Given a sequence σS, deﬁne η(σS) to be the ﬁrst element
in σS, and τ (σS) to be the subsequence of σS containing
all but the ﬁrst element.

to sequences:
We extend the kernel ﬁxing operator
in S valid in
given a sequence σS on elements
G(V, W), φσS (qV(V|W), G(V, W))
is deﬁned to
be equal to qV(V|W) if σS is the empty sequence, and
φτ (σS)(φη(σS )(qV(V|W); G(V, W)), φη(σS )(G(V, W)))
otherwise.
Given a CADMG G(V, W), a set R ⊆ V is called reach-
able if there exists a sequence for V \ R valid in G(V, W).
A set R reachable in G(V, W) is intrinsic in G(V, W) if
φV\R(G) contains a single district, R itself. The set of
intrinsic sets in a CADMG G is denoted by I(G).
A distribution p(V) is said to obey the nested Markov fac-
torization with respect to the ADMG G(V) if there exists
G(S)) : S ∈ I(G)}}
a set of kernels of the form {qS(S| pas
such that for every valid sequence σR for a reachable set R
in G, we have:

φσR (p(V); G(V)) =

qD(D| pas

G(D)).

D∈D(φR(G(V)))
Y

If a distribution obeys this factorization,
then for any
reachable R, any two valid sequences on R applied to
p(V) yield the same kernel qR(R|V \ R). Hence, ker-
nel ﬁxing may be deﬁned on sets, just as graph ﬁxing.
In this case, for every D ∈ I(G), qD(D| pas
G(D)) ≡
φV\D(p(V); G(V)).

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

One of the consequences of the nested factorization is the
so called district factorization or c-component factoriza-
tion:

p(V) =

qD(D| pas

G(D))

D∈D(G(V))
Y

=

D∈D(G(V))  
Y

D∈D
Y

p(D| pre≺(D))

,

!

where pre≺(D) is the set of predecessors of D according
to a topological total ordering ≺. Note that each factor
D∈D p(D | pre≺(D)) is only a function of D ∪ paG(D)

under the nested factorization.
Q
If p(V∪H) Markov factorizes relative to a DAG G(V∪H),
then the marginal distribution p(V) nested Markov fac-
torizes relative to the latent projection ADMG G(V). A
global Markov property has been deﬁned for models obey-
ing this factorization (Richardson et al., 2017), and it is
known to logically imply all equality constraints imposed
on a marginal distribution by a hidden variable DAG.

It is known that in a hidden variable causal model, not every
interventional distribution p(Y(a)) is identiﬁed. However,
every p(Y(a)) identiﬁed from p(V) can be expressed as a
modiﬁed nested factorization as follows:

p(Y(a)) =

p(D| do(pas

G(D)))|A=a

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

=

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

φV\D(p(V); G(V))|A=a,

where Y∗ ≡ anG(V(a))(Y) \ a, and G(Y∗(a)) is the latent
projection of the SWIG G(V(a)) onto Y∗(a). This mod-
iﬁed factorization yields a particularly simple view of the
ID algorithm (Tian & Pearl, 2002; Shpitser & Pearl, 2006),
one that we extend to identiﬁcation algorithms that treat in-
terventional distributions as known inputs.

3. Algorithms for Identiﬁcation from
Interventional Distribution Inputs

A SWIG G(V(a)) obtained from a DAG G(V) may be
viewed as a conditional DAG with random vertices V and
ﬁxed vertices a. Similarly, a marginal SWIG G(V(a)) is
a conditional ADMG with random vertices V and ﬁxed
vertices a. Deﬁnitions of ﬁxability, as well as reachable
and intrinsic sets carry over to marginal SWIGs without
change.
In fact, by a simple extention of Lemma 56 of
(Richardson et al., 2017), we can show that if p({V ∪
H}(a)) factorizes with respect to a SWIG G({V ∪ H}(a)),
then p(V(a)) admits the following nested SWIG factoriza-
tion with respect to the latent projection SWIG G(V(a)).
For every set R ⊆ V reachable in G(V(a)), the kernel

φV\R(p(V(a)); G(V(a))) factorizes as

qD(D(a)| pas

G(V(a))(D(a)))

D∈D(φV\R(G(V(a))))
Y

=

φV\D(p(V(a)); G(V(a))),

D∈D(φV\R(G(V(a))))
Y
term qD(D(a)| pas

each

where
is
only a function of those elements of a that are in
paG(V(a))(D(a)). These terms correspond to the set of
intrinsic sets in the SWIG I(G(V(a))).

G(V(a))(D(a)))

In other words, under standard causal models of a DAG
with hidden variables G(H ∪ V), marginal SWIGs G(S(a))
represent represent structure of a marginal counterfactual
p(S(a)), for any S ⊆ V using the SWIG version of the
nested Markov factorization.

3.1. The gID algorithm as a modiﬁed nested

factorization with respect to a set of SWIGs

Checking
to
alent

if

p(Y(a))

checking

is
whether
G(D)))|A=a

identiﬁed

is
p(Y∗(a))
is

equiv-
=
identiﬁed,

D∈D(G(Y∗(a))) p(D(pas
where Y∗ ≡ anG(V(a))(Y) \ a.
Q
The ID algorithm described above simply checks whether
each district D in the marginal SWIG G(Y∗(a)) corre-
sponds to an intrinsic set in G(V). If so, it obtains the cor-
responding distribution p(D(paG(D))) via the appropriate
factor in the nested Markov factorization, which in turn is a
functional of p(V) obtained by the ﬁxing operator φ(.). If
not, the distribution p(Y(a)) turns out not to be identiﬁed
from p(V).
If we have access to interventional distributions Z ≡
{p(Vi(bi))}k
i=1 for Vi(bi) ≡ (V \ Bi)(bi) instead of
p(V), then it is possible to identify distributions p(Y(a))
by checking whether every district D in some SWIG
G(Y∗(a)) is in the set of intrinsic sets I(G(Vi(bi))) for
It is possible that D is reachable in
some i ∈ 1, . . . k.
multiple interventional distributions p(Vi(bi)) – if so, we
choose any one of the indices i to assign to D, hence-
forth denoted iD. The corresponding interventional distri-
bution is denoted p(ViD (biD )). Since D ∈ I(G(Vi(bi))),
p(D(do(pas
G(D)))) may be obtained by a sequence of ﬁx-
ing operations on p(ViD (biD )). This gives rise to the fol-
lowing result.

Lemma 1. Fix a hidden variable causal model repre-
sented by an ADMG G(V), and a set of interventional
distributions Z ≡ {p(Vi(bi))}k
i=1 where Vi = V \ Bi.
Then, p(Y(a)) is identiﬁed if and only if for each D ∈
D(G(Y∗(a))) where Y∗ ≡ anG(V(a))(Y) \ a, there exists
at least one iD ∈ {1, . . . , k} such that p(ViD (biD )) ∈ Z
and D ∈ I(G(ViD (biD ))). Moreover, if p(Y(a)) is iden-

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

tiﬁed, it is equal to:

Y∗\Y
X

D∈
Y
D(G(Y∗(a)))

φViD \D(p(ViD (biD )); G(ViD (biD )))|A=a.

This formulation is equivalent to one given in Lee et al.
(2019) (the proof appears in the Appendix). An in-
volved proof 2 in Lee et al. (2019) also shows gID is
complete, meaning that if some D exists such that the
ﬁxing operator cannot be used to obtain the distribution
p(D|do(pas
G(D))) from any available interventional dis-
tribution and corresponding SWIG, then p(Y(a)) is not
identiﬁed from those distributions.

Applying Lemma 1 to identiﬁcation of p(Y (x1, x2)) in
Fig. 1a with access to Z = {p({V \ X1}(x1)), p({V \
X2}(x2))} (represented by Figs. 1b and 1c) yields

p(Y (x2)|U (x2), W )p(U (x2))p(W (x1))

X
W,U

The details are provided in the Appendix.

3.2. aID: Identiﬁcation with Ancestral Marginal

Interventional Distributions

The gID algorithm described above inherits the attractive
feature of the ID algorithm that identiﬁcation reduces to
checking district pieces of a special set Y∗ corresponding
to causally relevant ancestors of Y. The limitation of gID is
the requirement that interventional distribution inputs take
the form of p(Vi(bi)), where Vi and Bi are disjoint sets
and their union yields V. In reality, as discussed in Section
1.1, interventional distributions that are likely to be avail-
able will only be functions of p(Vi(bi)) – for example, a
marginal distribution.
Deﬁnition 1. (Ancestrality) Given a SWIG G(V(a)), a set
of random vertices S(a) ⊆ V(a) is said to be ancestral if
whenever S(a) ∈ S(a), then anG(V(a))(S(a)) \ a ⊆ S(a).

We have the following result, which states that the gID al-
gorithm may be adapted without loss of generality to the
setting where all inputs are marginal interventional distri-
butions with a particular property – namely, that they are
ancestral in their corresponding SWIG.

Lemma 2. Fix a hidden variable causal model represented
by an ADMG G(V), and a set of interventional distribu-
tions Z ≡ {p(Si(bi))}k
i=1, such that each Si(bi) is ances-
tral in G(V(bi)). Then p(Y(a)) is identiﬁed if and only if
for each D ∈ D(G(Y∗(a))) where Y∗ ≡ anG(V(a))(Y) \

2A minor edge case was not covered in this proof - namely
the thicket construction is invalid for root sets with exactly one
variable. We provide a correction in the Appendix.

a, there exists at least one iD ∈ {1, . . . , k} such that
p(SiD (biD )) ∈ Z and D ∈ I(G(SiD (biD ))). Moreover,
if p(Y(a)) is identiﬁed, it is equal to:

φSiD \D(p(SiD (biD )); G(SiD (biD )))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Applying Lemma 2 to identiﬁcation of p(Y (x1, x2)) in
Fig. 1a with Z = {p(W (x1)), p({W, U, Y, X1}(x2))} (rep-
resented by Figs. 1d and 1c) yields

p(Y (x2) | W, U (x2), X1)p(U (x2) | X1) × p(W (x1)).

X
W,U

The details are provided in the Appendix.

In the remainder of the paper, we consider increasingly gen-
eral identiﬁcations algorithms for p(Y(a)) that allow arbi-
trary marginal distributions obtained from p(Vi(bi)), and
then conditionals distributions obtained from p(Vi(bi)) to
be used as inputs.

3.3. mID: Identiﬁcation with Marginal Interventional

Distributions

If marginal interventional distributions given as input are
not ancestral in the corresponding SWIG, the identiﬁca-
tion algorithm becomes considerably more complicated.
In particular, it is no longer sufﬁcient to consider the set
Y∗ ≡ anG(V(a))(Y) \ a. Consider the following example.
In Fig. 1a we wish to identify p(Y (x1, x2)).
If we con-
sider the set Y ∗ = {U, Y, W } to try to identify this dis-
tribution, we will conclude that the required intrinsic sets
are D(GY∗ ) = {{U, Y }, {W }}, which means that we
must identify the corresponding interventional distributions
p({U, Y }(w, x2)), p(W (x1)). However, assume that we
only have access to the (non-ancestral) marginal interven-
tional distributions Z = {p(W (x1)), p(Y (x2), W (x2))},
represented by Figs. 1d and
In this
case, using the aID algorithm below will fail to identify
p(Y (x1, x2)) since p({U, Y }(w, x2)) is not identiﬁable
from any distribution in Z, since none of them have any
information on U .

1e respectively.

However,
it
is possible to identify p(Y (x1, x2)) from
Z via a larger set Y ′
that contains Y but not U (de-
spite the fact that U (x1, x2) is an ancestor of Y (x1, x2)
in the SWIG G(V(x1, x2))).
let Y′ ≡
anG((V\{U})(a))(Y) \ a = {Y, W }. Then, D(GY′ ) =
is easy to verify that {W } ∈
{{Y }, {W }}.
I(G(W (x1))), and {Y } ∈ I(G(Y (w, x2))), and thus
p(Y (x1, x2)) is identiﬁed from Z.

Speciﬁcally,

It

The algorithm schema we present here considers all possi-
ble subsets Y′ of Y∗ that also include Y.
Lemma 3. Given a hidden variable causal model repre-
sented by an ADMG G(V), and a set of interventional dis-

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

tributions Z ≡ {p(Si(bi))}k
i=1, then p(Y(a)) is identi-
ﬁed from Z if there exists Y′ ⊆ P(Y∗ \ Y) ∪ Y,3 such
that for each D ∈ D(G(Y′(a))), we can ﬁnd at least one
iD such that p(SiD (biD )) ∈ Z, pas
G(SiD (biD ))(D) =
G(Y′)(D), and D ∈ I(G(SiD (biD ))). Moreover, if
pas
p(Y(a)) is identiﬁed, the identifying formula is:

φSiD \D(p(SiD (biD )); G(SiD (biD )))|A=a.

D∈D(G(Y′(a)))
Y

Y′\Y
X
Note that evaluating whether an appropriate Y′ exists is in-
tractable in general. We emphasize this point by calling
this an algorithm schema rather than a tractable algorithm.
A polynomial time check that would discover an appropri-
ate Y′, if it exists, is currently an open question.

Applying Lemma
Z = {p(W (x1)), p({Y, W }(x2))} yields
W )p(W (x1)). The details are provided in the Appendix.

3 to identifying p(Y (x1, x2)) with
W p(Y (x2) |

P

3.4. eID: Identiﬁcation with Arbitrary Conditional

Interventional Distributions

We now consider the case where interventional distribu-
tions might arise as arbitrary marginal or conditional dis-
tributions. Conditional distributions can arise if data were
collected on a subset of a population (e.g. an RCT is con-
ducted with speciﬁc enrollment criteria). We consider any
combination of distributions of the form p(Si(bi)|Ci(bi))
(available at all levels of Ci(bi)), and distributions of the
form p(Si(bi)|Ci(bi) = ci) (available only at a speciﬁc
set of values ci of Ci(bi)).

Considering interventional conditional distributions creates
additional complications. First, identiﬁcation may have to
proceed in an interventional distribution where some vari-
ables are also conditioned on. We adapt the algorithm in
Bareinboim & Tian (2015) for this task, rephrasing it in
terms of intrinsic sets and a modiﬁcation of the ﬁxing op-
erator (adapted from Bhattacharya et al. (2019)). Second,
identiﬁcation of an interventional distribution from condi-
tionals may in general require us to “stitch together” mul-
tiple distributions via the chain rule of probability. We ad-
dress this issue by a preprocessing step applied to the input
set Z that uses the chain rule and model restrictions to con-
struct all additional distributions not already present in Z.
Deﬁnition 2. A variable A ⊆ V is selection ﬁxable (s-
ﬁxable) in a CADMG G(V, W) given conditioned vari-
ables C ⊆ V if C∩deG(V,W)(A) = ∅ and deG(V,W)(A)∩
disG(V,W)(A) = {A}.
Deﬁnition 3. If A ⊆ V is s-ﬁxable in a CADMG G(V, W)
where C ⊆ V is conditioned, deﬁne the s-ﬁxing operator
φC
A (G(V, W)) as φA(G(V, W)) (the ordinary ﬁxing oper-
ator on graphs), yielding G(V \ {A}, W ∪ {A}).

Note that if C ⊆ V is conditioned in G(V, W), C ⊆ V \
{A} is conditioned in φC
A (G(V, W)).
Deﬁnition 4. If A ⊆ V is s-ﬁxable in a CADMG G(V, W)
where C ⊆ V is conditioned, associated with a kernel
qV(V \ C | W = w, C = c), deﬁne the s-ﬁxing oper-
ator on kernels as:

C
A (qV(V \ C | W, C = c), G(V, W))
φ
qV(V \ C | W = w, C = c)
qV(A | mbG(V,W)(A) \ C, C = c, W = w)

≡

The s-ﬁxing operation of A given a conditioned set C, and
the conditioning operation on C commute, in the following
sense.

Lemma 4. If A ⊆ V is s-ﬁxable in a CADMG G(V, W)
where C ⊆ V is conditioned, associated with a kernel
qV(V \ C | W = w, C = c), then

qV\{A}(V \ (C ∪ {A})|W ∪ {A}, C = c) =

C
A (qV(V \ C|W, C = c), G(V, W)), with
φ

qV\{A}(V \ {A}|W ∪ {A}) ≡ φA(qV(V|W), G(V, W)).

Deﬁnition 5. A sequence σA of elements in A is s-ﬁxable
in G(V, W) given a conditioned C ⊆ V if either A = ∅,
or η(σA) is s-ﬁxable in G(V, W), and τ (σA) is s-ﬁxable
in φC

η(σA)(G(V, W)).

A is s-ﬁxable in G(V, W) if there exists an s-ﬁxable se-
quence σA in G(V, W).

The following is a version of the ID algorithm when the in-
put distribution has selection (conditioning on a particular
value), in terms of the s-ﬁxing operator.

Lemma 5. Given a SWIG G(S(b)), and the corre-
sponding interventional distribution p(S(b)), let Y∗ =
(anG(S(b))(Y) \ a). Let Y ⊆ S, and A ⊆ S ∪ B.
p(Y(a)) is identiﬁed from p({S \ C}(b)|C(b) = c) if
deG(S(b))(Y∗) ∩ C = ∅, cpaG(Y∗)∩A is consistent with
a, bpaG (Y∗)∩A is consistent with a, and for each district
D ∈ D(G(Y∗(a))), there exists a set ZD ∈ S \ C, such
that D ⊆ ZD, ZD is s-ﬁxable in G(S(b)) (given a con-
ditioned C) by a sequence σZD that ﬁxes ¯D ⊆ ZD last,
where ¯D is a district in φC
σZD \ ¯D(G(S(b))), and D is reach-
able in φC
σZD \ ¯D(G(S(b))).
If p(Y(a)) is identiﬁed, it is equal to

qD(D| pas

G(S(b))(D))|A=a,

3P(S) for any set S is the power set of S.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

where for each D ∈ D(G(Y∗(a))),

qD(D|pas
q ¯D( ¯D| pas

G(S(b))(D)) ≡ φ ¯D\D(q ¯D( ¯D|pas
G( ¯D)) ≡ Y
D∈ ¯D

qS\(ZD\¯D)(S\(ZD \ ¯D)|ZD \ ¯D) ≡

G( ¯D));φS\¯D(G(S(b))))

qS\(ZD\ ¯D)(D| mb∗(D), ZD \ ¯D)

C

φ

ZD\¯D(p({S \ C}(b)|C(b) = c; G(S(b)))),

with mb∗(D) deﬁned as mbφZD\ ¯D(G(S(b)))(D) intersected
with elements in ¯D earlier than D in any reverse topologi-
cal order in φZD\ ¯D(G(S(b))).
Deﬁnition 6. The set Z is said to be chain rule
for any p(Si(bi)|Ci(bi)) ∈ Z, and a
closed if
i (bi) of Ci(bi),
i (bi) ˙∪C2
partition C1
there exists
p(Sj(bj)|Cj (bj)) ∈ Z such that p(C1
i (bi)) =
i (bj), Cj(bj )), under the given
p(C1
i (bi)) ∈
i (b)|Ci(bi) \ C1
causal model, then p(Si(bi), C1
Z.

i (bj)|Sj(bj ) \ C1

if
i (bi)|C2

Any set of conditional counterfactual distributions Z may
also be made chain rule closed without loss of generality,
and the required equality may be established by rules of
po-calculus in (Malinsky et al., 2019).

Lemma 6. Fix a hidden variable causal model represented
by an ADMG G(V), and a chain rule closed set of distri-
butions Z = {p(Si(bi)|Ci(bi))}k
i=1 (with some possibly
available only at a level ci). Then p(Y(a)) is identiﬁed
from Z if there exists Y′ ⊆ P(Y∗ \ Y) ∪ Y, such that
for for each D ∈ D(G(Y′(a))), we can ﬁnd at least one
G({SiD ∪CiD }(biD ))(D) = pas
G(Y′)(D),
iD such that pas
and p(D | do(pas
G(SiD (biD )∪CiD (biD ))(D))) is identiﬁed
from p(Si(bi)|Ci(bi)) evaluated at ci consistent with a us-
ing Lemma 5. Moreover, if p(Y(a)) is identiﬁed, it is equal
to:

p(D(pas

G(SiD (biD )∪CiD (biD ))(D)))|A=a

Y′\Y
X

D∈D(G(Y′(a)))
Y
=

Y′\Y
X

D∈D(G(Y′(a)))
Y
GY′ (D)) is obtained from applying

where each qD(D| pas
Lemma 5 to the appropriate element of Z.

A worked example of Lemma 6 is provided in the Ap-
pendix.

4. Completeness

An identiﬁcation algorithm is considered complete if it fails
only when no computable functional exists.

We consider a proof of completeness for aID. The aim is
to demonstrate that aID will only fail when there exists a

structure in the graph that inhibits identiﬁcation, by allow-
ing the creation of two models M1, M2 which agree on
the input distributions, but disagree on a causal effect.
For disjoint sets A, Y, the causal effect p(Y(a)) is not
identiﬁed from a set of ancestral marginal distributions
Z = {p(Si(bi))}k
i=1 if there exist distinct causal mod-
els M1, M2 such that p1(Si(bi)) = p2(Si(bi)) for all
i ∈ {1, . . . , k}, but p1(Y(a)) 6= p2(Y(a)).
We consider a set of distributions ¯Z = {p(Vi(bi))}k
i=1,
where the interventions bi are identical to those in Z. Pre-
cisely, we construct arbitrary p({Vi \ S}(bi) | Si(bi)) for
i = 1, . . . , k. These distributions are combined with mod-
els M1, M2 as

p1(Vi(bi)) = p1(Si(bi))p({Vi \ S}(bi) | Si(bi)),
p2(Vi(bi)) = p2(Si(bi))p({Vi \ S}(bi) | Si(bi)).

This construction is the input to gID - interventional distri-
butions over V. Logically, if a causal query p(Y(a)) fails
against Z using aID, it can either fail with ¯Z using gID, or
succeed with gID. In the former case, the thicket construc-
tion proving non-identiﬁcation in gID can be adapted by
marginalization to prove non-identiﬁcation in aID (proved
in Lemma 7). In the latter case, we are required to demon-
strate that the failure of aID was due to some graphical ob-
ject preventing identiﬁcation. Due to the ancestrality prop-
erty of distributions in Z, it happens that this object is also
a thicket (proved in Lemma 1).
Lemma 7. If a causal query p(Y(a)) fails from Z using
aID, and fails from ¯Z using gID, then this causal query is
not identiﬁed.
Lemma 8. If a causal query p(Y(a)) fails from Z using
aID, but succeeds from ¯Z using gID, then a thicket con-
struction demonstrating non-identiﬁability applies.

The above results taken together establish completeness of
aID.

5. Conclusions

In this paper we used Single World Intervention Graphs
(SWIGs) (Richardson & Robins, 2013), the potential out-
comes calculus (Malinsky et al., 2019), and the nested
Markov factorization of mixed graphs (Richardson et al.,
2017) to yield a set of increasingly general algorithms for
identiﬁcation of counterfactual distributions given an arbi-
trary set of counterfactual or observed data distributions as
inputs. These results generalize a previous algorithm de-
scribed in (Lee et al., 2019). In addition, we show that for
a class of marginal counterfactual distribution inputs, our
algorithm is complete.

qD(D| pas

GY′ (D))|A=a,

Theorem 1. aID is complete.

Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Malinsky, D., Shpitser, I., and Richardson, T. S. A poten-
tial outcomes calculus for identifying conditional path-
In Proceedings of the 22nd Interna-
speciﬁc effects.
tional Conference on Artiﬁcial Intelligence and Statis-
tics, 2019.

Pearl, J. Causality: Models, Reasoning, and Inference.
Cambridge University Press, 2 edition, 2009. ISBN 978-
0521895606.

Richardson, T. S. and Robins, J. M. Single world inter-
vention graphs (SWIGs): A uniﬁcation of the counter-
factual and graphical approaches to causality. preprint:
http://www.csss.washington.edu/Papers/wp128.pdf,
2013.

Richardson, T. S., Evans, R. J., Robins, J. M., and Shpitser,
I. Nested Markov properties for acyclic directed mixed
graphs, 2017. Working paper.

Shpitser, I. and Pearl, J.

Identiﬁcation of joint interven-
tional distributions in recursive semi-Markovian causal
In Proceedings of the Twenty-First National
models.
Conference on Artiﬁcial Intelligence (AAAI-06). AAAI
Press, Palo Alto, 2006.

Shpitser, I., Evans, R. J., and Richardson, T. S. Acyclic
In Pro-
linear sems obey the nested markov property.
ceedings of the 34th Annual Conference on Uncertainty
in Artiﬁcial Intelligence (UAI-18), 2018.

Tian, J. and Pearl, J. On the testable implications of causal
In Proceedings of the
models with hidden variables.
Eighteenth Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI-02), volume 18, pp. 519–527. AUAI Press,
Corvallis, Oregon, 2002.

Tsiatis, A.

Semiparametric Theory and Missing Data.

Springer-Verlag New York, 1st edition edition, 2006.

Since our algorithm formulation relies on the nested
Markov factorization of mixed graphs, it naturally lends
itself to parametric statistical inference for cases where
nested Markov likelihoods have been formulated, such as
discrete and Gaussian data. Giving estimators for function-
als identiﬁed by our algorithms for likelihoods for more
general types of data, as well as deriving semi-parametric
estimators (Tsiatis, 2006) are obvious areas of future work.

important open problems include showing
In addition,
whether all algorithms we describe are complete, as well
as developing efﬁcient implementations in software.

References

Bareinboim, E. and Pearl, J. Causal inference by surrogate
experiments: z-identiﬁability. In Proceedings of the 28th
Conference on Uncertainty in Artiﬁcial Intelligence, pp.
113–120. AUAI Press, 2012.

Bareinboim, E. and Tian, J. Recovering causal effects from
selection bias. In Proceedings of the 29th AAAI Confer-
ence on Artiﬁcial Intelligence, pp. 3475–3481, 2015.

Bhattacharya, R., Nabi, R., Shpitser, I., and Robins, J. M.
Identiﬁcation In Missing Data Models Represented By
Directed Acyclic Graphs. In Proceedings of the Thirty-
Fifth Conference on Uncertainty in Artiﬁcial Intelli-
gence, pp. 10. AUAI Press, 2019.

Drton, M. Likelihood ratio tests and singularities. Annals

of Statistics, 37(2):979–1012, 2009.

Evans, R. J. Margins of discrete Bayesian networks.
The Annals of Statistics, 46(6A):2623–2656, December
2018.
doi: 10.1214/
17-AOS1631.

ISSN 0090-5364, 2168-8966.

Evans, R. J. and Richardson, T. S. Smooth, identiﬁable su-
permodels of discrete DAG models with latent variables.
Bernoulli, 2018. (to appear).

Huang, Y. and Valtorta, M.

Identiﬁability in causal
bayesian networks: A sound and complete algorithm.
In Twenty-First National Conference on Artiﬁcial Intel-
ligence, 2006.

Lauritzen, S. L. Graphical Models. Oxford, U.K.: Claren-

don, 1996.

Lauritzen, S. L. and Richardson, T. S. Chain graph models
and their causal interpretations (with discussion). Jour-
nal of the Royal Statistical Society: Series B, 64:321–
361, 2002.

Lee, S., Correa, J. D., and Bareinboim, E. General Identiﬁ-
ability with Arbitrary Surrogate Experiments. Proceed-
ings of the Conference on Uncertainty in Artiﬁcial Intel-
ligence, pp. 10, 2019.

0
2
0
2
 
r
p
A
 
5
1

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
5
1
1
0
.
4
0
0
2
:
v
i
X
r
a

Appendix to Identiﬁcation Methods With Arbitrary Interventional
Distributions as Inputs

Jaron J. R. Lee 1 Ilya Shpitser 1

1. Proofs of Soundness

We begin by reproducing the proof of soundness of the ID algorithm in Richardson et al. (2017), which will serve as a
framework for an alternative proof of soundness for gID and other soundness proofs of generalizations of gID we described.

Theorem 2. Let G(H ∪ V) be a causal DAG with a latent projection G(V). For A ˙∪Y ⊂ V, let Y∗ = anG(V(a))(Y) \ a.
If D(G(Y∗(a))) ⊆ I(G(V)) then p(Y(a)) is identiﬁed and given by

φV\D(p(V); G(V))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Proof: Let A∗ = V \ Y∗ ⊇ A. By Lemma 53 in (Richardson et al., 2017), p(Y∗ | do(A)) = p(Y∗ | do(A∗)).
Since G(H ∪ V)) is a DAG, any set is ﬁxable, in particular A∗. Let G∗(H ∪ (V \ A∗), A∗) be the CDAG obtained from
φA∗ (G(H ∪ V)).
Given a graph G(V ∪ H), deﬁne σH to be the operator that creates a latent projection that removes vertices in H. By
Corollary 53 in (Richardson et al., 2017), ﬁxing (φ) and latent projections(σ) operators in any CDAG commute, in other
words, σH(φA∗ (G(H ∪ V))) = φA∗ (σH(G(H ∪ V))).
Since G∗(Y∗, A∗) = σH(φA∗ (G(H ∪ V))), and by deﬁnition of induced subgraphs, G(V)Y∗ = (φA∗ (G(V)))Y∗ , this
means that G(V)Y∗ = G∗(Y∗, A∗)Y∗ . Therefore D(G(V)Y∗ ) = D(G(Y∗, A∗)). Further, by deﬁnition of SWIG latent
projections and induced subgraphs, D(G(V)Y∗ ) = D(G(Y∗(a))).
HD. HD is
For every district D ∈ D(G∗(Y∗, A∗)), deﬁne HD ≡ H ∩ anG(H∪V)D∪H (D)), and H∗ =
the set of variables h ∈ H for which there exists a vertex d ∈ D and a directed path h → . . . → d in G(H ∪ V) on which
excepting d all vertices are in H.
The construction of HD implies the following three corollaries.

D∈D(G∗(Y∗,A∗))

S

Corollary 1. If D, D′ ∈ D(G∗(Y∗, A∗)) and D 6= D′, then HD ∩ DD′ = ∅.

Proof: This is because if the intersection was not empty, the two districts D, D′ would have a single H ∈ H pointing into
both of them.
(cid:3)

Corollary 2. For each D ∈ D(G∗(Y∗, A∗)) we have pas

G(H∪V)(D ∪ HD) ∩ H∗ = HD.

Corollary 3. Y∗ ∪ H∗ is ancestral in G(H ∪ V) so if V ∈ Y∗ ∪ H∗, pas

G(H∪V)(V ) ∩ H ⊆ H∗.

Then,

}

}

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

p(V | pas

G(H∪V)(V )) ×

p(V | pas

G(H∪V)(V ))

p(Y∗ | doG(H∪V)(A∗))

p(V | pas

G(H∪V)(V ))

H
X

V ∈H∪Y∗
Y

H∗
X

V ∈H∗∪Y∗
Y

=

=

=

=

H\H∗
X

YV ∈H\H∗

p(V | pas
G(H∪V)(V ))
|

=1

{z

H∗
X

D∈D(G∗(Y∗,A∗)) Y

Y

V ∈D∪HD

D∈D(G∗(Y∗,A∗))  
Y

V ∈D∪HD

HD Y
X

p(V | pas

G(H∪V(V ))

!

The ﬁrst equality follows from applying g-formula to a DAG and then marginalizing hidden variables H. The second
equality follows from Corollary 3 since these hidden variables do not point into any D ∈ D(G∗(Y∗, A∗)). This means
that V ∈ H∗ ∪ Y∗ do not have parents from H \ H∗. The third equality follows from Corollary 1 which allows us to factor
the V ∈ H∗ ∪ Y∗ by districts. The fourth equality follows from Corollary 2, which allows us to partition the H∗ by their
districts under the sum.
For any given D,

p(V | pas

G(H∪V)(V )) ×

p(V | pas

G(H∪V)(V ))

p(V | pas

G(H∪V)(v))

V ∈D∪HD

HD Y
X
=

V ∈D∪HD

HD Y
X

p(V | pas

G(H∪V)(V ))

|

V ∈D∪H
Y
φV\D(p(H ∪ V); G(H ∪ V))

=

=

H
X

H
X

H\HD Yv∈H\HD
X

=1

{z

The ﬁrst equality follows from Corollaries 2, and 3, the deﬁnition of HD, and the fact that pas
HD) = ∅.
To see that this last fact is true, let K ∈ H \ HD. We show by contradiction that this K cannot also be in pas
G(H∪V)(D ∪
HD) If K is a parent of D, then K ∈ HD since K is a hidden variable with a direct path to a district. This is a contradiction.
If K is a parent of HD, then there exists a path from K to some element H ∈ HD, and from H to D. In particular, all
intermediate elements of this path are hidden. Therefore there is a directed path of hidden variables from K to D, which
implies that K ∈ HD. This is also a contradiction.
Lemma 55 of Richardson et al. (2017) showed that ﬁxing a set A and marginalizing a set H in a kernel qV(V ∪ H|W)
commutes, provided this kernel is nested Markov relative to a CADMG G(V ∪ H, W), and A is a ﬁxable set in the latent
projection G(V, W). It follows that

G(H∪V)(D ∪ HD) ∩ (H \

φV\D(p(H ∪ V); G(H ∪ V)) = φV\D(p(V); G(V))

(1)

and

H
X

The conclusion follows since

p(Y∗ | doG(H∪V)(A∗)) =

φV\D(p(V); G(V)).

p(Y | doG(H∪V)(A)) =

p(Y∗ | doG(H∪V)(A∗)).

D∈D(G∗(Y∗,A∗))
Y

Y∗\Y
X

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

(cid:3)
Lemma 9. Fix a causal model associated with a DAG G(V ∪ H) deﬁned on all one-step-ahead counterfactuals of the
form V (bV ), where bV are elements in XpaG(V∪H)(V ) and V ∈ V ∪ H.
Consider the marginal SWIG G({S ∪ H}(a)) for any A ⊆ V, S ≡ V \ A. Then the set of counterfactuals V (bV ), where
consistent with a, and V ∈ S ∪ H forms a causal model associated with a (conditional)
bV are elements in XpaG({S∪H}(a))
DAG G({S ∪ H}(a)).
Moreover, any counterfactual random variable T(c), where T ⊆ S ∪ H, and a ⊆ c is equal in the two causal models.

Proof: This follows by standard structural equation model semantics of causal models, coupled with deﬁnition of interven-
tions via structural equation replacement.
(cid:3)
Lemma 10. Fix a causal model associated with a DAG G(V ∪ H) deﬁned on all one-step-ahead counterfactuals of the
form V (bV ), where bV are elements in XpaG(V∪H)(V ) and V ∈ V ∪ H.
Consider a subset A ⊆ V ∪ H ancestral in G(V ∪ H). Then the set of counterfactuals V (bV ), where bV are elements in
XpaG(A)
Moreover, any counterfactual random variable T(c), where T, C ⊆ A is equal in the two causal models.

, and V ∈ A forms a causal model associated with G(A).

Proof: This follows by standard structural equation model semantics of causal models, coupled with deﬁnition of ancestral
sets.
(cid:3)
Lemma ?? Fix a hidden variable causal model represented by an ADMG G(V), and a set of interventional distributions
Z ≡ {p(Vi(bi))}k
i=1 where Vi = V \ Bi. Then, p(Y(a)) is identiﬁed if and only if for each D ∈ D(G(Y∗(a))) where
Y∗ ≡ anG(V(a))(Y) \ a, there exists at least one iD ∈ {1, . . . , k} such that p(ViD (biD )) ∈ Z and D ∈ I(G(ViD (biD ))).
Moreover, if p(Y(a)) is identiﬁed, it is equal to:

Proof: Under the given causal model,

φViD \D(p(ViD (biD )); G(ViD (biD )))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

p(Y(a)) =

p(D(pas

G(D)))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Lemma 9 immediately implies, by Theorem 2, that the ID algorithm is sound applied to the marginal SWIG G({V\A}(a))
and any corresponding interventional distribution p(S(b)), provided S ⊆ V \ A, and a ⊆ b.
Fix D ∈ D(G(Y∗(a))), and let p(ViD (biD )) be the distribution used in the algorithm to obtain p(D(pas

G(D))). Then

p(D(pas

G(D))) = φViD \D(p(ViD ); G(ViD (biD )))|A=a,

since this is simply the output of the ID algorithm applied to the query p(D(pas
G(D))) with a known distribution
p(ViD (biD )) and the corresponding latent projection graph G(biD )) as inputs. The fact that D ∈ I(G(ViD (biD )))
G(D))) is invariant in the original
implies p(D(pas
causal model yielding the latent projection G(V), and the new causal model yielding the marginal SWIG G(ViD (biD )).

G(D))) is identiﬁable from p(ViD (biD )), and Lemma 9 implies p(D(pas

(cid:3)

Lemma ?? Fix a hidden variable causal model represented by an ADMG G(V), and a set of interventional distributions
Z ≡ {p(Si(bi))}k
i=1, such that each Si(bi) is ancestral in G(V(a)). Then p(Y(a)) is identiﬁed if and only if for each
D ∈ D(G(Y∗(a))) where Y∗ ≡ anG(V(a))(Y) \ a, there exists at least one iD ∈ {1, . . . , k} such that p(SiD (biD )) ∈ Z
and D ∈ I(G(SiD (biD ))). Moreover, if p(Y(a)) is identiﬁed, it is equal to:

φSiD \D(p(SiD (biD )); G(SiD (biD )))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Proof: Under the given causal model,

p(Y(a)) =

p(D(pas

G(D)))|A=a.

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

Fix D ∈ D(G(Y∗(a))), and the corresponding p(SiD (biD )) ∈ Z. Fix any G(V ∪ H) representing a causal model, of
which G(V) is a latent projection.
Let S∗ ≡ anG(V∪H)(SiD ). By deﬁnition, the set S∗ is ancestral in G(V ∪ H). By Lemma 10, the DAG G(S∗) forms a
causal model with respect to the appropriate set of counterfactuals. By Lemma 9, the marginal SWIG G({S∗ \ BiD }(biD ))
forms a causal model with respect to the appropriate set of counterfactuals.
This implies the ID algorithm is sound applied to the marginal SWIG G(SiD (biD )) obtained as a latent projection of
G({S∗ \ BiD }(biD )), and any corresponding interventional distribution p(SiD (biD )). This implies that in the causal
model associated with the marginal SWIG G(SiD (biD )),

p(D(pas

G(D))) = φViD \D(p(SiD ); G(SiD (biD )))|A=a.

Lemma 10 implies p(D(pas
new causal model yielding the marginal SWIG G(SiD (biD )). This establishes our conclusion.

G(D))) is invariant in the original causal model yielding the latent projection G(V), and the

Lemma ?? Given a hidden variable causal model represented by an ADMG G(V), and a set of interventional distributions
i=1, then p(Y(a)) is identiﬁed from Z if there exists Y′ ⊆ P(Y∗ \ Y) ∪ Y,1 such that for each
Z ≡ {p(Si(bi))}k
D ∈ D(G(Y′(a))), we can ﬁnd at least one iD such that p(SiD (biD )) ∈ Z, pas
G(Y′)(D), and
D ∈ I(G(SiD (biD ))). Moreover, if p(Y(a)) is identiﬁed, the identifying formula is:

G(SiD (biD ))(D) = pas

(cid:3)

Proof: Under the given causal model,

φSiD \D(p(SiD (biD )); G(SiD (biD )))|A=a.

Y′\Y
X

D∈D(G(Y′(a)))
Y

p(Y(a)) =

p(D(pas

G(D)))|A=a.

Y′\Y
X

D∈D(G(Y′(a)))
Y

Note here that unlike previous proofs, the SWIG G(Y′(a)) is potentially a latent projection of the SWIG G(Y∗(a)).
Nevertheless, since in any underlying causal model associated with a hidden variable DAG G(V ∪ H), the distribution
p(V(a) ∪ H(a) is Markov relative to G(V(a) ∪ H(a)), implies p(Y′(a)) is nested Markov relative to G(Y′(a)), which
in turn implies the district factorization above.
Fix D ∈ D(G(Y′(a))), and the corresponding p(SiD (biD )) ∈ Z. Fix any G(V ∪ H) representing a causal model, of
which G(V) is a latent projection.
Let S∗ ≡ anG(V∪H)(SiD ). By deﬁnition, the set S∗ is ancestral in G(V ∪ H). By Lemma 10, the DAG G(S∗) forms a
causal model with respect to the appropriate set of counterfactuals. By Lemma 9, the marginal SWIG G({S∗ \ BiD }(biD ))
forms a causal model with respect to the appropriate set of counterfactuals.
This implies the ID algorithm is sound applied to the marginal SWIG G(SiD (biD )) obtained as a latent projection of
G({S∗ \ BiD }(biD )), and any corresponding interventional distribution p(SiD (biD )). This implies that in the causal
model associated with the marginal SWIG G(SiD (biD )),

p(D(pas

G(Y′(a))(D))) = φViD \D(p(SiD ); G(SiD (biD )))|A=a.

That p(D(pas
model yielding the marginal SWIG G(SiD (biD )) follows from the assumption that pas
This establishes our conclusion.

G(Y′(a))(D))) is invariant in the original causal model yielding the latent projection G(V), and the new causal
G(SiD (biD ))(D).

G(Y′(a))(D)) = pas

1P(S) for any set S is the power set of S.

(cid:3)

(cid:3)

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Lemma ?? If A ⊆ V is s-ﬁxable in a CADMG G(V, W) where C ⊆ V is conditioned, associated with a kernel
qV(V \ C | W = w, C = c), then

Proof: Consider ﬁrst conditioning on C = c in qV(V | W). By the deﬁnition of conditioning in a kernel, we obtain

The subsequent s-ﬁxing on A results in

qV\{A}(V \ (C ∪ {A})|W ∪ {A}, C = c) =
C
A (qV(V \ C|W, C = c), G(V, W)), with
φ
qV\{A}(V \ {A}|W ∪ {A}) ≡ φA(qV(V|W), G(V, W)).

qV(V \ C | W, C = c) =

qV(V | W)
V\C qV(V | W)

P

C=c
(cid:12)
(cid:12)
(cid:12)

qV(V \ C | W, C = c)
qV(A | W, ndG(A) \ (C ∪ W), C = c)
= qV(V \ (ndG(A) ˙∪{A}) | A, ndG(A) \ (C ∪ W), C = c, W)

× qV(ndG(A) \ (C ∪ W) | W, C = c)

qV(V | W)
qV(A | ndG(A) \ W, W)
= qV(V \ (ndG(A) ∪ {A}) | A, ndG(A)) \ W, W)

× qV(nd(A)) \ W | W)

where C ⊆ ndG(A) by the deﬁnition of s-ﬁxing, and where equation 10 in Richardson et al. (2017) was applied.
Conversely, consider ﬁrst ﬁxing A in qV(V | W). This is deﬁned as

by equation 10 of Richardson et al. (2017), where C ⊆ ndG(A).
Conditioning on C and setting C = c in this distribution gives

qV(V \ (ndG(A) ∪ {A}) | A, ndG(A) \ (C ∪ W), C = c, W)

× qV(nd(A) \ (C ∪ W) | W, C = c)

by applying the deﬁnition of conditioning, which demonstrates the required commutativity property.

Lemma ?? Given a SWIG G(S(b)), and the corresponding interventional distribution p(S(b)), let Y∗ = (anG(S(b))(Y)\
a). Let Y ⊆ S, and A ⊆ S ∪ B. p(Y(a)) is identiﬁed from p({S \ C}(b)|C(b) = c) if deG(S(b))(Y∗) ∩ C = ∅,
cpaG (Y∗)∩A is consistent with a, bpaG(Y∗)∩A is consistent with a, and for each district D ∈ D(G(Y∗(a))), there exists
a set ZD ∈ S \ C, such that D ⊆ ZD, ZD is s-ﬁxable in G(S(b)) (given a conditioned C) by a sequence σZD that ﬁxes
¯D ⊆ ZD last, where ¯D is a district in φC
σZD \ ¯D(G(S(b))), and D is reachable in φC

σZD \ ¯D(G(S(b))).

If p(Y(a)) is identiﬁed, it is equal to

where for each D ∈ D(G(Y∗(a))),

qD(D| pas

G(S(b))(D))|A=a,

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

qD(D| pas

G(S(b))(D)) ≡ φ ¯D\D(q ¯D( ¯D| pas
q ¯D( ¯D| pas
G( ¯D)) ≡ Y
D∈ ¯D

G( ¯D)); φS\ ¯D(G(S(b))))
qS\(ZD\ ¯D)(D| mb∗(D), ZD \ ¯D)

qS\(ZD\¯D)(S\(ZD \ ¯D)|ZD \ ¯D) ≡
ZD\¯D(p({S \ C}(b|C(b) = c); G(S(b)))),

C

φ

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

with mb∗(D) deﬁned as mbφZD\ ¯D(G(S(b)))(D) intersected with elements in ¯D earlier than D in any reverse topological
order in φZD\ ¯D(G(S(b))).
Proof: Since s-ﬁxability implies ﬁxability, under the given causal model, we have

φσV\D (p(S(b)), G(S(b)))|A=a,

Y∗\Y
X

D∈D(G(Y∗(a)))
Y

D∈ ¯D qS\(ZD\ ¯D)(D| mb∗(D), ZD \ ¯D) correspond, via chain rule,

for any valid sequence σV\D for every D.
The kernel obtained by s-ﬁxing ZD \ ¯D is equal to the kernel we obtain by ﬁxing had there been no selection bias, by
inductive application of Lemma ??, and assumption of s-ﬁxability of ZD by sequence σ (and thus of ZD \ ¯D, since
elements in ¯D are ﬁxed last by σ).
Since p(S(b)) is nested Markov with respect to G(S(b)), and since ¯D is s-ﬁxable once we s-ﬁx ZD \ D,
the
G( ¯D)) ≡
terms in
φS\ ¯D(p(S(b)); G(S(b))). Speciﬁcally, this is established by induction by noting that at every stage only childless un-
ﬁxed elements in ¯D may be ﬁxed, using the nested Markov property to note ﬁxing every element in ¯D is equivalent to
marginalization, and using chain rule. That each such element is s-ﬁxable rather than just ﬁxable implies the resulting ob-
ject is independent of all variables in C and thus their product is equal to φS\ ¯D(p(S(b)); G(S(b))) (note the usual ﬁxing
operator, rather than the s-ﬁxing operator).
Since p(S(b)) is nested Markov with respect to G(S(b)), this kernel is nested Markov relative to φS\ ¯D(G(S(b))), which
G( ¯D)); φS\ ¯D(G(S(b)))), is sound
means the last step of the algorithm, namely qD(D| pas
since the ID algorithm expressed via the ﬁxing operator is sound. This has been shown in (?).

G(S(b))(D)) ≡ φ ¯D\D(q ¯D( ¯D| pas

to the kernel q ¯D( ¯D| pas

Q

Lemma ?? Fix a hidden variable causal model represented by an ADMG G(V), and a chain rule closed set of dis-
tributions Z = {p(Si(bi)|Ci(bi))}k
i=1 (with some possibly available only at a level ci). Then p(Y(a)) is identiﬁed
from Z if there exists Y′ ⊆ P(Y∗ \ Y) ∪ Y, such that for for each D ∈ D(G(Y′(a))), we can ﬁnd at least one iD
G(Y′)(D), and p(D | do(pas
G(SiD (biD )∪CiD (biD ))(D))) is identiﬁed from
such that pas
p(Si(bi)|Ci(bi)) evaluated at ci consistent with a using Lemma ??. Moreover, if p(Y(a)) is identiﬁed, it is equal to:

G({SiD ∪CiD }(biD ))(D) = pas

(cid:3)

where each qD(D| pas
under the given causal model,

GY′ (D)) is obtained from applying Lemma ?? to the appropriate element of Z. Proof: As before,

p(D(pas

G(SiD (biD )∪CiD (biD ))(D)))|A=a

Y′\Y
X

D∈D(G(Y′(a)))
Y
=

Y′\Y
X

D∈D(G(Y′(a)))
Y

qD(D| pas

GY′ (D))|A=a,

p(Y(a)) =

p(D(pas

G(D)))|A=a.

Y′\Y
X

D∈D(G(Y′(a)))
Y

Fix D ∈ D(G(Y′(a))), and the corresponding p({SiD \ CiD }(biD )|CiD (biD )) ∈ Z. Fix any G(V ∪ H) representing a
causal model, of which G(V) is a latent projection.
Let S∗ ≡ anG(V∪H)(SiD ∪ CiD ). By deﬁnition, the set S∗ is ancestral in G(V ∪ H). By Lemma 10, the DAG G(S∗) forms
a causal model with respect to the appropriate set of counterfactuals. By Lemma 9, the marginal SWIG G({S∗\BiD}(biD ))
forms a causal model with respect to the appropriate set of counterfactuals.
This implies that we can apply the results of Lemma ?? to the marginal SWIG G({SiD ∪ CiD }(biD )) obtained as a latent
projection of G({S∗ \ BiD }(biD )), and any corresponding conditional interventional distribution p(SiD (biD )|CiD (biD ))
to yield p(D(pas
G(Y′(a))(D))) is invariant in the original causal model yielding the latent
projection G(V), and the new causal model yielding the marginal SWIG G(SiD (biD )) follows from the assumption that
pas
(cid:3)

G(Y′(a))(D))). That p(D(pas

G(SiD (biD ))(D). This establishes our conclusion.

G(Y′(a))(D)) = pas

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

2. Proofs of Completeness

Lemma ?? If a causal query p(Y(a)) fails from Z using aID, and fails from ¯Z using gID, then this causal query is not
identiﬁed from Z.
Proof: Both aID and gID will fail on some intrinsic set D ∈ D(GY∗ ). For this intrinsic set D, we can construct a thicket
(Lee et al., 2019) from the point where gID fails. On this thicket we can construct models M1, M2 with the property
that they will agree on interventional distributions from ¯Z including D and pa(D), but p1(D | do(pa(D))) 6= p2(D |
do(pa(D))). The construction will align on elements from Z, as these are marginals of elements of ¯Z.
(cid:3)
Lemma ?? If a causal query p(Y(a)) fails from Z using aID, but succeeds from ¯Z using gID, then this causal query is
not identiﬁed from Z.

Proof: If identiﬁcation fails for aID but succeeds for gID, then we can ﬁnd at least one such required intrinsic set D with
associated parents paG(D) which exists in ¯Z but not in Z.
For each p(Vi(bi)) ∈ ¯Z and p(Si(bi)) ∈ Z, the kernel qVi(D | Vi \ D) corresponding to intrinsic set and parents
D ∪ paG(D) is:

1. absent in p(Vi(bi)) and p(Si(bi))
2. present in p(Vi(bi)), but absent in p(Si(bi)) because parts of D ∪ paG(D) were marginalized
3. present in p(Vi(bi)), but absent in p(Si(bi)) because D was not reachable

where present means that there exist a valid ﬁxing order for the kernel in the speciﬁed distribution, and absent means no
such order exists.
Note that by ancestrality of ¯Z, paGY∗ (D) is equal to paG(Vi(bi))(D) if D ⊆ Vi(bi) for all i, and similarly for if Vi(bi)
is replaced with Si(bi).
We claim case 3 cannot happen. Consider the variables D∪paG(D) which are present (but not reachable) in p(Si(bi)) ∈ Z.
D is reachable in some p(Vi(bi)) ∈ ¯Z, which is a distribution over possibly more variables. Further, by ancestrality of Z,
the set difference of Vi(bi)\Si(bi) must be descendants of D∪paG(D). To identify p(D | do(paG(D))) from p(Vi(bi)),
this involves ﬁxing on all parents of the intrinsic set, plus all descendants of the set (i.e. marginalization of descendants).
However, D will also be intrinsic in p(Si(bi)), since the interventional distribution will differ from p(Vi(bi)) only by
descendants (which have been marginalized). The ancestral property of the interventional distributions guarantees that the
parents of this intrinsic set are present, in ﬁxed or ﬁxable form. Thus if D ∪ paG(D) is present in p(Vi(bi)), then it
is also present in p(Si(bi)). Therefore, only cases 1 and 2 can exist. The implication is that the appropriate witness to
non-identiﬁcation is a thicket. We demonstrate this by outlining a constructive proof.
Assume that the aID algorithm fails on some district D, with some set of interventional marginals Z, and some arbitrary
completion of those marginals ¯Z. One possibility is that D ∪ paG(D) falls into case 1 for each p(Si(bi)) and p(Vi(bi))
respectively. In this case, the thicket construction is exactly as outlined in Lee et al. (2019). This thicket construction
will show that p1(Vi(bi)) = p2(Vi(bi)) for all i, but the causal effects disagree - p1(D(paG(D))) 6= p2(D(paG(D))).
Marginalizing Vi \ Si over each p(Vi(bi)) in both M1, M2 gives p1(Si(bi)) = p2(Si(bi)) for all i, by deﬁnition. This
proves non-identiﬁability because we have supplied two sets of distributions which agree observationally, but disagree on
the desired causal effect.
On another extreme, we may ﬁnd that D ∪ paG(D) falls into case 2 for some p(Si(bi)) and p(Vi(bi)). In such a case,
non-identiﬁcation is straightforward. The thicket construction applies as we can construct p1(Si(bi)) = p2(Si(bi)) by
marginalizing Vi \ Si for each i, but the causal effects disagree - p1(D(paG(D))) 6= p2(D(paG(D))).
In practice, cases 1 and 2 may be present - but as the procedure for handling the cases is the same, the above argument
extends to handling such D.
The proof of Lemma 3 in Lee et al. (2019) provides an explicit construction of this thicket, once we have extended Z to ¯Z.
When gID fails, it will have as arguments the causal effect p(D | do(V \ D)) = p(D | do(paG(D))) = p(D(paG(D))),
graph GY∗ , and the set of extended distributions Z.

(cid:3)

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

A

B

C

D

Figure2. Nested Markov Factorization Example

Theorem ?? aID is complete.
Proof: aID receives as inputs the interventional distributions Z and the desired causal effect p(Y(a)) in some graph G. The
algorithm will fail if for some intrinsic set D with parents paG(D), the kernel q(D | V \ D) cannot be obtained via a valid
ﬁxing sequence from any distribution p(Si(bi)) ∈ Z, for D ∈ D(GY∗ )
We can construct the completion of the interventions, denoted ¯Z.
If the desired intrinsic set D with parents paG(D)) fails to be found using gID, then by Lemma ?? we can create a thicket
and models M1, M2 demonstrating that p(Y(a)) could not be identiﬁed.
If the desired intrinsic set D with parents paG(D)) is identiﬁed using gID, then by Lemma ?? we can show that this can
only happen in cases where a thicket construction applies, and that p(Y(a)) could not be identiﬁed.

Hence, if aID fails, the causal effect is not identiﬁed.

(cid:3)

3. Equivalence of gID and one-line gID

Section 1 provides proof that the one-line gID algorithm is sound. It remains to show that whenever the one-line gID
algorithm fails, there exists a thicket preventing identiﬁcation.

Line 6 of Algorithm 1 in (Lee et al., 2019) is equivalent to the expression for one-line gID (Lemma 1). Both algorithms
have the following steps: there is a summation over variables which are ancestors of Y - guaranteed by Line 3 in Algorithm
1, and the deﬁnition of Y∗ in Lemma 1; a product over districts in the graph G(a); and an operation which attempts to
obtain p(D | do(pa(D))) in one of the interventional distributions available. This ﬁnal operation relies on the ID algorithm
applied to each interventional distribution - either in its original recursive form (Shpitser & Pearl, 2006) or as the one-line
version using the nested Markov factorization (Richardson et al., 2017).

4. Examples

4.1. Nested Markov factorization

We demonstrate the nested Markov factorisation of some distribution p(A, B, C, D) with corresponding graph

We ﬁrst consider all reachable sets, of which there are 13. Out of these sets, 5 are intrinsic (these are listed ﬁrst), and
correspond to intrinsic Markov kernels making up the nested Markov factorization of all reachable sets. Every reachable
set corresponds to a distribution equal to a particular product of intrinsic Markov kernels.

• {A}, qA(A) ≡ p(A). This is an intrinsic set.

• {B}, qB(B|A) ≡ p(B|A). This is an intrinsic set.

• {C}, qC (C|B, A) ≡ p(C|B, A). This is an intrinsic set.

• {D}, qD(D|C) ≡

B p(D|C, B, A)p(B|A). This is an intrinsic set.

• {B, D}, qB,D(B, D|C, A) ≡ p(D|C, B, A)p(B|A). This is an intrinsic set.

P

• {A, B}, qA(A) · qB(B|A). This is a reachable (but not intrinsic) set.

• {A, C}, qA(A) · qC (C|B, A). This is a reachable (but not intrinsic) set.

• {B, C}, qB(B|A) · qC (C|B, A). This is a reachable (but not intrinsic) set.

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

• {A, D}, qD(D|C) · qA(A). This is a reachable (but not intrinsic) set.

• {A, B, C}, qA(A) · qB(B|A) · qC (C|B, A). This is a reachable (but not intrinsic) set.

• {A, B, D}, qA(A) · qB,D(B, D|C, A). This is a reachable (but not intrinsic) set.

• {B, C, D}, qC (C|B, A) · qB,D(B, D|C, A). This is a reachable (but not intrinsic) set.

• {A, B, C, D}, qA(A) · qC (C|B, A) · qB,D(B, D|C, A). This is a reachable (but not intrinsic) set.

Note that the kernel qD(D|C) is not a function of A under the model, although it may appear that it might be at ﬁrst glance,
B p(D | A, B, C)p(B | A). This is a generalized
based on the form of its functional in terms of p(A, B, C, D), namely
independence constraint or a Verma constraint (?).

P

4.2. gID example

To illustrate gID reformulated using the nested Markov factorization (as given in Lemma ??), we consider the problem
of identifying p(Y(a)) = p(Y (x1, x2)) in Fig. ??, with access to Z = {p(V1(b1)) = p({VX1 (x1)), p(V2(b2)) =
p({VX2 (x2))}, represented by Figs. ?? and ?? respectively.
The districts of G(Y∗) are D(G(Y∗) = {{Y, U }, {W }}. The required intrinsic Markov kernels will be of the form
qV(D | paG(D)). These are qV(Y, U | W, X2), qV(W | X1).
We consider D1 = {Y, U }. This set is reachable in G(Vx2 (x2)) by ﬁxing W and then X1, so iD1 = 2. Then, the intrinsic
Markov kernel can be recovered as

φV2\D1 (p(V2(b2)); G(V2(b2)))|A=a
= φ{X1,W }(p(VX2 (x2)); G(VX2 (x2)))|X1=x1,X2=x2

= φX1

= φX1

= φX1

p(VX2 (x2))
p(W | X1, U (x2))
p(Y (x2) | X1, U (x2), W )p(X1, U (x2)); G((VX2 \ {W })(x2))
(cid:17)

; G((VX2 \ {W })(x2))
(cid:17)

|X1=x1,X2=x2

qV(Y (x2), X1, U (x2) | W ); G((VX2 \ {W })(x2))
(cid:17)

|X1=x1,X2=x2

(cid:16)

(cid:16)

(cid:16)

=

=

qV(Y (x2), X1, U (x2) | W )
qV(X1 | W )

|X1=x1,X2=x2

p(Y (x2) | X1, U (x2), W )p(X1, U (x2))
p(X1)

|X1=x1,X2=x2

= p(Y (x2) | X1 = x1, U (x2), W )p(U (x2) | X1 = x1)

|X1=x1,X2=x2

By similar logic, D2 = {W } is reachable in G(VX2 (x2)) by ﬁxing Y (x1), U, X2 in that order, so iD2 = 1. The reader
can verify that the intrinsic Markov kernel is

φV1\D2 (p(V1(b1)); G(V1(b1)))|A=a
= φ{Y (x1),U,X2}(p(VX1 (x1)); G(VX1 (x1)))|A=a
= qV(W | X1, U (x2), W )
= p(W (x1))

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Hence, p(Y (x1, x2)) is

φViD \D(p(ViD (biD )); G(ViD (biD )))|A=a

p(Y (x2) | X1 = x1, U (x2), W )p(U (x2) | X1 = x1)p(W (x1)),

D∈D(G(Y∗(a)))
Y

Y∗\Y
X
=

W,U
X

W,U
X

=

p(Y (x2) | U (x2), W )p(U (x2))p(W (x1)).

where the last equality holds by the Markov properties, since Y ⊥⊥ X1 | U, W , and U | X1 in G({V \ {X2}}), represented
by Fig. ??.

4.3. aID example

To illustrate the aID algorithm presented in Lemma ??, consider again the problem of identifying p(Y (x1, x2)) in Fig. ??,
where

Z ={p(S1(b1)) = p(W (x1)),

p(S2(b2)) = p({W, U, Y, X1}(x2))},

corresponding to Figs. ?? and ?? respectively. Marginal distributions are not considered valid inputs to gID, causing it to
trivially fail. However, the intrinsic Markov kernels might still be identiﬁed.
The districts of G(Y∗) are D(G(Y∗) = {{Y, U }, {W }}. The required intrinsic Markov kernels will be of the form
qV(D | paG(D)). These are qV(Y, U | W, X2), qV(W | X1).
Considering D1 = {Y, U }, we notice that this set is reachable in G(S2(b2)) by ﬁxing W (x2), and then X1. The corre-
sponding intrinsic Markov kernel is

qV\{Y (x2),U(x2)}(Y (x2), U (x2) | W (x2)),

which is equal to

φS2\D1 (p(S2(b2)); G(S2(b2)))|A=a
= φX1,W (p(S2(b2)); G(S2(b2)))|A=a
= φX1 (p(Y (x2) | W, U (x2), X1)p(U (x2), X1); φW G(S2(b2))),

=

p(Y (x2) | W, U (x2), X1)p(X1, U (x2))
p(X1)
= p(Y (x2) | W, U (x2), X1)p(U (x2) | X1)

D2 = {W } is reachable in G(S1(b1)), without any ﬁxing operations. In this case,

qV\{W }(W (x1)) = φS1\D2 (p(W (x1)); G(S1(b1)))|A=a

= p(W (x1))

since S1 \ D2 = ∅.

Therefore, p(Y (x1, x2)) is

p(Y (x2) | W, U (x2), X1)p(U (x2) | X1) × p(W (x1))

W,U
X

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

4.4. mID example

Once again, we return to the problem of identifying p(Y (x1, x2)) in Fig. ??, where our interventional distributions are

Z ={p(S1(b1)) = p(W (x1))

p(S2(b2)) = p({Y, W }(x2))}

given by Figs. ?? and ??. This differs from the aID example because the marginal distribution represented by G(S2(b2))
is not ancestral with respect to G(V(x1, x2)).
The algorithm is functionally identical to aID, with the exception that it is applied over each Y′ ⊆ P(Y∗ \ Y) ∪ Y
We ﬁrst verify that the algorithm fails when Y′ ≡ Y∗. In this case, the districts of G(Y′) are D(G(Y′)) = {{Y, U }, {W }}.
We immediately see that since U is not available in Z, the algorithm will fail.
Then, we consider Y′ ≡ Y∗ \ {U }. The distrcts of G(Y′) are D(G(Y′)) = {{Y }, {W }}. Since all variables of D(G(Y′))
are present in Z, it is possible that it is identiﬁed.
For D1 = {Y }, this is reachable in G(S2(b2)), and the corresponding kernel is given by

φS2\D1 (p({Y, W }(x2)); G({Y, W }(x2)))|X1=x1,X2=x2
= φ{W }(p({Y, W }(x2)); G({Y, W }(x2)))|X1=x1,X2=x2

=

p(Y (x2), W )
p(W )
= p(Y (x2) | W ))

|X1=x1,X2=x2

The procedure for obtaining the kernel associated with D2 = {W } is exactly as described in the aID example. The
corresponding kernel is

Therefore, p(Y (x1, x2)) is identiﬁed as

4.5. eID example

p(W (x1))

p(Y (x2) | W )p(W (x1)).

W
X

To demonstrate eID using Lemma ??, we consider identifying p({R1, R2}(x1, x2)) in G represented by Figure 3a from
conditional marginal interventional distributions represented by Figs. 3b and 3c. In particular,

Z′ = {p({J, R1, W2, X2}(x1) | C(x1) = c, {W1, R1}(x1)),

p(R1(x1) | W1(x1)),
p(W1(x1) | C(x1) = c),
p({J, R1, R2, W2, C}(x2))}

where Z′ has not been chain rule closed. Unless otherwise speciﬁed, we assume that we have conditional distributions
available at all levels of conditioning.

Recall that to make a set of distributions chain rule closed, it may be necessary to apply the rules of po-calculus
(Malinsky et al., 2019). In particular, we can apply rule 1 to p(R1(x1) | W1(x1)). Since R1(x1) ⊥⊥ C(x1) | W1(x1)
in G({V \ {X1})(x1)), rule 1 states that

p(R1(x1) | W1(x1)) = p(R1(x1) | W1(x1), C(x1))

= p(R1(x1) | W1(x1), C(x1) = c))

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Then, applying chain rule we notice that the ﬁrst three distributions may be combined in the following manner:

p({J, R1, R2, W1, W2, X2}(x1) | C(x1) = c) = p({J, R1, W2, X2}(x1) | C(x1) = c, {W1, R1}(x1))

× p(R1(x1) | W1(x1), C(x1) = c))
× p(W1(x1) | C(x1) = c)

After one chain rule closure step, we obtain

Z ={p(S1(b1) | C1(b1) = c1) = p({J, R1, R2, W1, W2, X2}(x1) | C(x1) = c),

p(S2(b2) | C2(b2) = c2) = p({J, R1, R2, W2, C}(x2)),
p({J, R1, W2, X2}(x1) | C(x1) = c, {W1, R1}(x1)),
p(R1(x1) | W1(x1)),
p(W1(x1) | C(x1) = c),
. . .}

where we recognise that there may be other distributions attainable by further chain rule closures, but that only the ﬁrst two
interventional distributions are sufﬁcient for this problem.
Ordinarily, the algorithm iterates over Y′ ∈ P(Y∗ \ Y) ∪ Y. For Y′ ≡ Y∗ = {J, R1, R2, W2, W1}, the relevant districts
are D(G(Y∗)) = {R1, R2, W1}, {W }, {J}}. In this example we will ﬁnd that it is sufﬁcient to consider Y∗ only, but this
is not true in general as per the mID example.
We now consider the problem of identifying p(D1 | do(paG(D))) using Lemma ??. For this indented block, assume Y∗
to be scoped with reference to Lemma ??, and not Lemma ??.

For D1 = {R1, R2, W1}, we ﬁnd that the corresponding Markov kernel p(D1 | do(paG(D1))) is identiﬁed
from p(S1(b1) \ C1 | C1(b1) = c1)) = p({J, R1, R2, W1, W2, X2}(x1) | C(x1) = c), using Lemma ??. We
ﬁnd that all consistency conditions in the Lemma are satisﬁed. Further, Y∗ = {{R1, R2, W1}}. For this district
D = {R1, R2, W1}, we consider ZD = {W2, X2, R2, W1, R1}, and ¯D = D, and show that these are suitable
candidates (in general, one would need to search over all such candidates ZD to verify if they were suitable).
First, ZD ∈ S \ C = {R1, R2, W1, W2, X2}. Second, D ⊆ ZD. Third, ZD is s-ﬁxable in G({V \ {X1}}(x1))
with a sequence that ﬁxes ¯D last – we may ﬁx ZD \ D = {W2, X2}, and then members of ¯D = {R2, R1, W1}
in the order presented. Fourth, one can verify that ¯D is a district in φC
σZD \ ¯D(G({V \ {X1}}(x1))) – ﬁxing
ZD = {W2, X2} does not disrupt the district D. Fifth, D is reachable in φC
σZD \ ¯D(G({V \ {X1}}(x1))) since
D = ¯D. Then, applying the rest of the Lemma, we note that for our single D:

qD(D| pas

G(S(b))(D)) = φ ¯D\D(q ¯D( ¯D| pas

G( ¯D)); φS\ ¯D(G(S(b))))

does not do anything since D = ¯D.
Then,

qS\(ZD\¯D)(S\(ZD\ ¯D)|ZD \ ¯D)
≡ φ

C

ZD\¯D(p(S(b | C(b) = c); G(S(b))))
X2,W2 (p(({V \ {X1, C}}(x1) | C(x1) = c); G({V \ {X1}}(x1)))

C

= φ

= φ

C
X2 (

p({R1, R2, W1, W2, J, X2}(x1) | C(x1) = c)
p(W2 | C(x1) = c, J, R1, W1, X2)

; φC

W2 G({V \ {X1}}(x1)))

C

= φ

X2 (p({R2}(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))p({R1, W1, X2, J}(x1) | C(x1) = c); φC

W2 G({V \ {X1}}(x1)))

Hence,

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)
= p({R2}(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))p({R1, W1, J}(x1) | C(x1) = c)

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Finally, we note that for the topological order W1 ≺ R1 ≺ R2,

q ¯D( ¯D| pas

G( ¯D)) ≡

qS\(ZD\ ¯D)(D| mb∗(D), ZD \ ¯D)

YD∈ ¯D

= qR1,R2,W1,J (W1 | J, W2, X2)qR1,R2,W1,J (R1 | W1, J, W2, X2)qR1,R2,W1,J (R2 | R1, W1, J, W2, X2)

Expanding each of these factors by appealing to the deﬁnition of conditioning in a kernel, we obtain

qR1,R2,W1,J (W1 | J, W2, X2) =

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

R1,R2,W1

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))p({R1, W1, J}(x1) | C(x1) = c)
P
p(J(x1) | C(x1) = c)

qR1,R2,W1,J (R1, | W1, J, W2, X2) =

= p(W1(x1) | C(x1) = c, J(x1))

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

R1,R2

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))p({R1, W1, J}(x1) | C(x1) = c)
P
p({W1, J}(x1) | C(x1) = c)

=

R1,R2
X

R1,R2
X

=

R2
X

R2
X

qR1,R2,W1,J (R2 | R1, W1, J, W2, X2) =

= p(R1(x1) | C(x1) = c, {W1, J}(x1))

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

qR1,R2,W1,J (R1, R2, W1, J | W2, X2)

R2

=

p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))p({R1, W1, J}(x1) | C(x1) = c)
P
p({R1, W1, J}(x1) | C(x1) = c)

= p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))

which means that

p(D1 | do(paG(D1))) = p(W1(x1) | C(x1) = c, J(x1))

× p(R1(x1) | C(x1) = c, {W1, J}(x1))
× p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))

As Lemma ?? is functionally equivalent to the RC algorithm presented in Bareinboim & Tian (2015), the reader should be
able to recover the same kernel for D1 either way.
For D2 = {W2}, we ﬁnd that its corresponding Markov kernel is identiﬁed from p(S2(b2) \ C2 | C2(b2) = c2) =
p({R1, R2, W2, J, C}(x2)), in a straightforward manner as there are no conditionals. Applying mID techniques,

p(D2 | do(paG(S2(b2)∪C2(b2))(D2)))
= p(W2 | do(X2))
= φR1,R2,J,C(p({R1, R2, W2, J, C}(x2)) = c); G(S2(b2)))

=

p({R1, R2, W2, J, C}(x2))

R2,R2,J,C
X
= p(W2(x2))

For D3 = {J}, we note that p(J | do(paG J)) = p(J) is obtained from p(S2(b2) \ C2 | C2(b2) = c2) =
p({R1, R2, W2, J, C}(x2)) by

p(J) =

p({R1, R2, W2, J, C}(x2))

R2,R2,W2,C
X

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

X2

X1

X2

X2

W1

J

C W2

W1

J

C W2

J

C W2

X1

R1

R2

R1

R2

R1

R2

(a) G(V)

(b) G((V \ {X1})(x2))

(c) G({R1, R2, C, W2}(x2))

Figure3. Example for eID

Applying the primary equation of Lemma ??, the causal effect p({R1, R2}(x1, x2)) is given by

p(W1(x1) | C(x1) = c, J(x1))

W1,W2,J (cid:16)
X
× p(R1(x1) | C(x1) = c, {W1, J}(x1))

× p(R2(x1) | C(x1) = c, {R1, W1, W2, X2, J}(x1))

p(W2(x2))

p(J)

(cid:17)(cid:16)

(cid:17)(cid:16)

(cid:17)

5. Amending the Thicket Construction

Throughout this paper we rely on the thicket as a structure which is a witness to the non-identiﬁcation of causal effects.
The thicket was originally proposed in Deﬁnition 6 of Lee et al. (2019), along with an explicit construction relying on the
thicket of two models demonstrating non-identiﬁability of a query. We note that whilst the construction is in principle
correct, there exists a minor edge case which we address now.
The construction proposed by Lee et al. (2019) only requires that the root set R be non-empty, but in the construction of
the two models it is also required that U′R ( the UCs connected to R in the thicket) also be non-empty. It is only possible
to have bidirected edges between root nodes if there are at least two root nodes. Therefore, the thicket construction of two
models agreeing on the observed distribution p(V) whilst disagreeing on p(R(a)) is not deﬁned if R contains only one
element, since bidirected edges are not deﬁned on a single node. Since we cannot assume the cardinality of R in general,
we amend the construction accordingly in this section.
Let G be a graph where R is a single node R. We delete R, and in its place create ˜R1 inheriting all directed edges, and
˜R2 inheriting all bidirected edges. ˜R1 and ˜R2 are connected by a bidirected edge so that ˜R = { ˜R1, ˜R2} is a valid root set.
The resulting graph ˜G is a valid thicket, since it remains one district, each hedge has variables with only one child (where
hedges are drawn from the original hedges in G), and the root set is a minimal district.
We can apply the existing construction to this thicket as U′R now contains a single bidirected edge between ˜R1 and ˜R2.
We deﬁne Rn = ˜R1 × ˜R2, where × denotes the cartesian product, and treat it as a single node which inherits all edges in
˜R. Deﬁne this new graph as G×.
Lemma 11. If M1 and M2 deﬁned on ˜G agree on available distributions and disagree on a causal effect, then so do M1
and M2 deﬁned on G×.

Proof: Consider M1, M2. By construction they disagree on p(R(t′ = 0) = 0) where T′ ⊆ T, such that all hedgelets
are intervened upon. Then p1(R(t′ = 0) = 0) > p2(R(t′ = 0) = 0). Since the cartesian product is an injective map, it
must be the case that p1(Rn(t′ = 0) = 0) > p2(Rn(t′ = 0) = 0). Additionally, M1, M2 also agree on any distribution
p(R(t′ = 0) = 0) where T′ ⊆ T, such that at least one hedgelet is not intervened upon. Again, by the injectivity of the
cartesian product, it follows that p1(Rn(t′ = 0) = 0) = p2(Rn(t′ = 0) = 0).
(cid:3)

References

Artiﬁcial Intelligence, pp. 7, 2015.

Bareinboim, E. and Tian, J. Recovering Causal Effects from Selection Bias. Proceedings of the AAAI Conference on

Appendix to Identiﬁcation Methods With Arbitrary Interventional Distributions as Inputs

Lee, S., Correa, J. D., and Bareinboim, E. General Identiﬁability with Arbitrary Surrogate Experiments. Proceedings of

the Conference on Uncertainty in Artiﬁcial Intelligence, pp. 10, 2019.

Malinsky, D., Shpitser, I., and Richardson, T. A Potential Outcomes Calculus for Identifying Conditional Path-Speciﬁc

Effects. International Conference on Artiﬁcial Intelligence and Statistics, pp. 9, 2019.

Richardson, T. S., Evans, R. J., Robins, J. M., and Shpitser, I. Nested Markov Properties for Acyclic Directed Mixed

Graphs. arXiv:1701.06686 [stat], January 2017.

Shpitser, I. and Pearl, J. Identiﬁcation of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models.

Proceedings of the National Conference on Artiﬁcial Intelligence, pp. 8, 2006.

