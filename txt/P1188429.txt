Adaptive Semi-supervised Learning for Cross-domain
Sentiment Classiﬁcation

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore
‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg
‡d.dahlmeier@sap.com

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
3
5
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We consider the cross-domain sentiment clas-
siﬁcation problem, where a sentiment classi-
ﬁer is to be learned from a source domain and
to be generalized to a target domain. Our ap-
proach explicitly minimizes the distance be-
tween the source and the target instances in
an embedded feature space. With the differ-
ence between source and target minimized,
we then exploit additional information from
the target domain by consolidating the idea
of semi-supervised learning, for which, we
jointly employ two regularizations – entropy
minimization and self-ensemble bootstrapping
– to incorporate the unlabeled target data for
classiﬁer reﬁnement. Our experimental results
demonstrate that the proposed approach can
better leverage unlabeled data from the target
domain and achieve substantial improvements
over baseline methods in various experimental
settings.

1

Introduction

In practice, it is often difﬁcult and costly to anno-
tate sufﬁcient training data for diverse application
domains on-the-ﬂy. We may have sufﬁcient la-
beled data in an existing domain (called the source
domain), but very few or no labeled data in a
new domain (called the target domain). This issue
has motivated research on cross-domain sentiment
classiﬁcation, where knowledge in the source do-
main is transferred to the target domain in order to
alleviate the required labeling effort.

One key challenge of domain adaptation is that
data in the source and target domains are drawn
from different distributions. Thus, adaptation per-
formance will decline with an increase in distribu-
tion difference. Speciﬁcally, in sentiment analy-
sis, reviews of different products have different vo-
cabulary. For instance, restaurants reviews would
contain opinion words such as “tender”, “tasty”, or

“undercooked” and movie reviews would contain
“thrilling”, “horriﬁc”, or “hilarious”. The intersec-
tion between these two sets of opinion words could
be small which makes domain adaptation difﬁcult.
Several techniques have been proposed for ad-
dressing the problem of domain shifting. The
aim is to bridge the source and target domains
by learning domain-invariant feature representa-
tions so that a classiﬁer trained on a source do-
main can be adapted to another target domain.
In cross-domain sentiment classiﬁcation, many
works (Blitzer et al., 2007; Pan et al., 2010; Zhou
et al., 2015; Wu and Huang, 2016; Yu and Jiang,
2016) utilize a key intuition that domain-speciﬁc
features could be aligned with the help of domain-
invariant features (pivot features). For instance,
“hilarious” and “tasty” could be aligned as both
of them are relevant to “good”.

Despite their promising results,

these works
share two major limitations. First, they highly de-
pend on the heuristic selection of pivot features,
which may be sensitive to different applications.
Thus the learned new representations may not ef-
fectively reduce the domain difference. Further-
more, these works only utilize the unlabeled tar-
get data for representation learning while the sen-
timent classiﬁer was solely trained on the source
domain. There have not been many studies on ex-
ploiting unlabeled target data for reﬁning the clas-
siﬁer, even though it may contain beneﬁcial infor-
mation. How to effectively leverage unlabeled tar-
get data still remains an important challenge for
domain adaptation.

In this work, we argue that the information
from unlabeled target data is beneﬁcial for do-
main adaptation and we propose a novel Domain
Adaptive Semi-supervised learning framework
(DAS) to better exploit it. Our main intuition is
to treat the problem as a semi-supervised learn-
ing task by considering target instances as unla-

beled data, assuming the domain distance can be
effectively reduced through domain-invariant rep-
resentation learning. Speciﬁcally, the proposed
approach jointly performs feature adaptation and
semi-supervised learning in a multi-task learning
setting. For feature adaptation, it explicitly mini-
mizes the distance between the encoded represen-
tations of the two domains. On this basis, two
semi-supervised regularizations – entropy mini-
mization and self-ensemble bootstrapping – are
jointly employed to exploit unlabeled target data
for classiﬁer reﬁnement.

We evaluate our method rigorously under multi-
ple experimental settings by taking label distribu-
tion and corpus size into consideration. The re-
sults show that our model is able to obtain sig-
niﬁcant improvements over strong baselines. We
also demonstrate through a series of analysis that
the proposed method beneﬁts greatly from incor-
porating unlabeled target data via semi-supervised
learning, which is consistent with our motivation.
Our datasets and source code can be obtained from
https://github.com/ruidan/DAS.

2 Related Work

Domain Adaptation: The majority of feature
adaptation methods for sentiment analysis rely on
a key intuition that even though certain opinion
words are completely distinct for each domain,
they can be aligned if they have high correlation
with some domain-invariant opinion words (pivot
words) such as “excellent” or “terrible”. Blitzer
et al. (2007) proposed a method based on struc-
tural correspondence learning (SCL), which uses
pivot feature prediction to induce a projected fea-
ture space that works well for both the source and
the target domains. The pivot words are selected in
a way to cover common domain-invariant opinion
words. Subsequent research aims to better align
the domain-speciﬁc words (Pan et al., 2010; He
et al., 2011; Wu and Huang, 2016) such that the
domain discrepancy could be reduced. More re-
cently, Yu and Jiang (2016) borrow the idea of
pivot feature prediction from SCL and extend it
to a neural network-based solution with auxiliary
tasks.
In their experiment, substantial improve-
ment over SCL has been observed due to the use
of real-valued word embeddings. Unsupervised
representation learning with deep neural networks
(DNN) such as denoising autoencoders has also
been explored for feature adaptation (Glorot et al.,

2011; Chen et al., 2012; Yang and Eisenstein,
2014). It has been shown that DNNs could learn
transferable representations that disentangle the
underlying factors of variation behind data sam-
ples.

Although the aforementioned methods aim to
reduce the domain discrepancy, they do not explic-
itly minimize the distance between distributions,
and some of them highly rely on the selection of
pivot features. In our method, we formally con-
struct an objective for this purpose. Similar ideas
have been explored in many computer vision prob-
lems, where the representations of the underlying
domains are encouraged to be similar through ex-
plicit objectives (Tzeng et al., 2014; Ganin and
Lempitsky, 2015; Long et al., 2015; Zhuang et al.,
2015; Long et al., 2017) such as maximum mean
discrepancy (MMD) (Gretton et al., 2012). In NLP
tasks, Li et al. (2017) and Chen et al. (2017) both
proposed using adversarial training framework for
reducing domain difference. In their model, a sub-
network is added as a domain discriminator while
deep features are learned to confuse the discrim-
inator. The feature adaptation component in our
model shares similar intuition with MMD and ad-
versary training. We will show a detailed compar-
ison with them in our experiments.
Semi-supervised Learning: We attempt to treat
domain adaptation as a semi-supervised learning
task by considering the target instances as unla-
beled data. Some efforts have been initiated on
transfer learning from unlabeled data (Dai et al.,
2007; Jiang and Zhai, 2007; Wu et al., 2009).
In our model, we reduce the domain discrep-
ancy by feature adaptation, and thereafter adopt
semi-supervised learning techniques to learn from
unlabeled data. Primarily motivated by (Grand-
valet and Bengio, 2004) and (Laine and Aila,
2017), we employed entropy minimization and
self-ensemble bootstrapping as regularizations to
incorporate unlabeled data. Our experimental re-
sults show that both methods are effective when
jointly trained with the feature adaptation objec-
tive, which conﬁrms to our motivation.

3 Model Description

3.1 Notations and Model Overview

We conduct most of our experiments under an un-
supervised domain adaptation setting, where we
have no labeled data from the target domain. Con-
sider two sets Ds and Dt. Ds = {x(s)
i=1 is
i

i }|ns

, y(s)

i }|nt

from the source domain with ns labeled examples,
where yi ∈ RC is a one-hot vector representation
of sentiment label and C denotes the number of
classes. Dt = {x(t)
i=1 is from the target domain
with nt unlabeled examples. N = ns + nt denotes
the total number of training documents including
both labeled and unlabeled1. We aim to learn a
sentiment classiﬁer from Ds and Dt such that the
classiﬁer would work well on the target domain.
We also present some results under a setting where
we assume that a small number of labeled target
examples are available (see Figure 3).

For the proposed model, we denote G parame-
terized by θg as a neural-based feature encoder that
maps documents from both domains to a shared
feature space, and F parameterized by θf as a
fully connected layer with softmax activation serv-
ing as the sentiment classiﬁer. We aim to learn fea-
ture representations that are domain-invariant and
at the same time discriminative on both domains,
thus we simultaneously consider three factors in
our objective: (1) minimize the classiﬁcation error
on the labeled source examples; (2) minimize the
domain discrepancy; and (3) leverage unlabeled
data via semi-supervised learning.

Suppose we already have the encoded features
of documents {ξ(s,t)
i=1 (see
Section 4.1), the objective function for purpose (1)
is thus the cross entropy loss on the labeled source
examples

= G(x(s,t)

; θg)}|N

i

i

L = −

1
ns

ns(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(s)
y(s)

i (j)

(1)

i

i = F(ξ(s)

where ˜y(s)
; θf ) denotes the predicted la-
bel distribution. In the following subsections, we
will explain how to perform feature adaptation and
domain adaptive semi-supervised learning in de-
tails for purpose (2) and (3) respectively.

3.2 Feature Adaptation

Unlike prior works (Blitzer et al., 2007; Yu and
Jiang, 2016), our method does not attempt to align
domain-speciﬁc words through pivot words.
In
our preliminary experiments, we found that word
embeddings pre-trained on a large corpus are able
to adequately capture this information. As we will

1Note that unlabeled source examples can also be in-
cluded for training. In that case, N = ns + nt + ns(cid:48) where
ns(cid:48) denotes the number of unlabeled source examples. This
corresponds to our experimental setting 2. For simplicity, we
only consider ns and nt in our description.

later show in our experiments, even without adap-
tation, a naive neural network classiﬁer with pre-
trained word embeddings can already achieve rea-
sonably good results.

i }nt

i }|ns

i=1 and {ξ(t)

We attempt to explicitly minimize the distance
between the source and target feature represen-
tations ({ξ(s)
i=1). A few meth-
ods from literature can be applied such as Maxi-
mum Mean Discrepancy (MMD) (Gretton et al.,
2012) or adversary training (Li et al., 2017; Chen
et al., 2017). The main idea of MMD is to esti-
mate the distance between two distributions as the
distance between sample means of the projected
embeddings in Hilbert space. MMD is implicitly
computed through a characteristic kernel, which is
used to ensure that the sample mean is injective,
leading to the MMD being zero if and only if the
distributions are identical. In our implementation,
we skip the mapping procedure induced by a char-
acteristic kernel for simplifying the computation
and learning. We simply estimate the distribution
distance as the distance between the sample means
in the current embedding space. Although this ap-
proximation cannot preserve all statistical features
of the underlying distributions, we ﬁnd it performs
comparably to MMD on our problem. The follow-
ing equations formally describe the feature adap-
tation loss J :

J = KL(gs||gt) + KL(gt||gs)
g(cid:48)
s
(cid:107)g(cid:48)
s(cid:107)1

g(cid:48)
s =

1
ns

ξ(s)
i

gs =

ns(cid:88)

,

i=1

nt(cid:88)

i=1

1
nt

g(cid:48)
t =

ξ(t)
i

,

gt =

g(cid:48)
t
(cid:107)g(cid:48)
t(cid:107)1

(2)

(3)

(4)

s and g(cid:48)

L1 normalization is applied on the mean represen-
tations g(cid:48)
t, rescaling the vectors such that
all entries sum to 1. We adopt a symmetric ver-
sion of KL divergence (Zhuang et al., 2015) as the
distance function. Given two distribution vectors
P, Q ∈ Rk, KL(P||Q) = (cid:80)k

i=1 P(i) log( P(i)

Q(i) ).

3.3 Domain Adaptive Semi-supervised

Learning (DAS)

We attempt to exploit the information in target
data through semi-supervised learning objectives,
which are jointly trained with L and J . Normally,
to incorporate target data, we can minimize the
cross entropy loss between the true label distri-
butions y(t)
and the predicted label distributions
i

i = F(ξ(t)
˜y(t)
; θf ) over target samples. The chal-
i
lenge here is that y(t)
is unknown, and thus we
i
attempt to estimate it via semi-supervised learn-
ing. We use entropy minimization and bootstrap-
ping for this purpose. We will later show in our
experiments that both methods are effective, and
jointly employing them overall yields the best re-
sults.
Entropy Minimization: In this method, y(t)
is
i
estimated as the predicted label distribution ˜y(t)
,
i
which is a function of θg and θf . The loss can thus
be written as

Γ = −

1
nt

nt(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(t)
˜y(t)

i (j)

(5)

Assume the domain discrepancy can be effectively
reduced through feature adaptation, by minimiz-
ing the entropy penalty, training of the classiﬁer
is inﬂuenced by the unlabeled target data and will
generally maximize the margins between the tar-
get examples and the decision boundaries, increas-
ing the prediction conﬁdence on the target domain.

training.

Self-ensemble Bootstrapping: Another way to
estimate y(t)
corresponds to bootstrapping. The
i
idea is to estimate the unknown labels as the
predictions of the model learned from the pre-
vious round of
Bootstrapping has
been explored for domain adaptation in previous
works (Jiang and Zhai, 2007; Wu et al., 2009).
However, in their methods, domain discrepancy
was not explicitly minimized via feature adap-
tation. Applying bootstrapping or other semi-
supervised learning techniques in this case may
worsen the results as the classiﬁer can perform
quite bad on the target data.

Inspired by the ensembling method proposed
in (Laine and Aila, 2017), we estimate y(t)
by
i
forming ensemble predictions of labels during
training, using the outputs on different training
epochs. The loss is formulated as follows:

Ω = −

1
N

N
(cid:88)

C
(cid:88)

i=1

j=1

˜z(s,t)
i

(j) log ˜y(s,t)

(j)

i

(6)

where ˜z denotes the estimated labels computed on
the ensemble predictions from different epochs.
It serves
The loss is applied on all documents.
for bootstrapping on the unlabeled target data, and
it also serves as a regularization that encourages

Algorithm 1 Pseudocode for training DAS
Require: Ds, Dt, G, F
Require: α = ensembling momentum, 0 ≤ α < 1
Require: w(t) = weight ramp-up function

Z ← 0[N ×C]
˜z ← 0[N ×C]
for t ∈ [1, max-epochs] do

for each minibatch B(s), B(t), B(u) in

Ds, Dt, {x(s,t)

}|N

i=1 do

i
compute loss L on [xi∈B(s), yi∈B(s)]
compute loss J on [xi∈B(s), xj∈B(t)]
compute loss Γ on xi∈B(t)
compute loss Ω on [xi∈B(u), ˜zi∈B(u)]
overall-loss ← L + λ1J + λ2Γ + w(t)Ω
update network parameters

i ← F(G(xi)), for i ∈ N

end for
Z(cid:48)
Z ← αZ + (1 − α)Z(cid:48)
˜z ← one-hot-vectors(Z)

end for

the network predictions to be consistent in differ-
ent training epochs. Ω is jointly trained with L,
J , and Γ. Algorithm 1 illustrates the overall train-
ing process of the proposed domain adaptive semi-
supervised learning (DAS) framework.

t

In Algorithm 1, λ1, λ2, and w(t) are weights
to balance the effects of J , Γ, and Ω respectively.
λ1 and λ2 are constant hyper-parameters. We set
max-epochs )2]λ3 as a Gaus-
w(t) = exp[−5(1 −
sian curve to ramp up the weight from 0 to λ3.
This is to ensure the ramp-up of the bootstrapping
loss component is slow enough in the beginning
of the training. After each training epoch, we com-
pute Z(cid:48)
i which denotes the predictions made by the
network in current epoch, and then the ensemble
prediction Zi is updated as a weighted average of
the outputs from previous epochs and the current
epoch, with recent epochs having larger weight.
For generating estimated labels ˜zi, Zi is converted
to a one-hot vector where the entry with the maxi-
mum value is set to one and other entries are set to
zeros. The self-ensemble bootstrapping is a gener-
alized version of bootstrappings that only use the
outputs from the previous round of training (Jiang
and Zhai, 2007; Wu et al., 2009). The ensemble
prediction is likely to be closer to the correct, un-
known labels of the target data.

Domain
Book

Electronics

Beauty

Music

#Pos
2000
4824
2000
4817
2000
4709
2000
4441

#Neg
2000
513
2000
694
2000
616
2000
785

#Neu Total
6000
2000
6000
663
6000
2000
6000
489
6000
2000
6000
675
6000
2000
6000
774

Set 1
Set 2
Set 1
Set 2
Set 1
Set 2
Set 1
Set 2

(a) Small-scale datasets

Domain
IMDB
Yelp
Cell Phone
Baby

#Pos
55,242
155,625
148,657
126,525

#Neg
11,735
29,597
24,343
17,012

#Neu
17,942
45,941
21,439
17,255

Total
84,919
231,163
194,439
160,792

(b) Large-scale datasets

Table 1: Summary of datasets.

4 Experiments

4.1 CNN Encoder Implementation

We have left the feature encoder G unspeciﬁed,
In
for which, a few options can be considered.
our implementation, we adopt a one-layer CNN
structure from previous works (Kim, 2014; Yu and
Jiang, 2016), as it has been demonstrated to work
well for sentiment classiﬁcation tasks. Given a re-
view document x = (x1, x2, ..., xn) consisting of
n words, we begin by associating each word with
a continuous word embedding (Mikolov et al.,
2013) ex from an embedding matrix E ∈ RV ×d,
where V is the vocabulary size and d is the embed-
ding dimension. E is jointly updated with other
network parameters during training. Given a win-
dow of dense word embeddings ex1, ex2, ..., exl,
the convolution layer ﬁrst concatenates these vec-
tors to form a vector ˆx of length ld and then the
output vector is computed by Equation (7):

Conv(ˆx) = f (W · ˆx + b)

(7)

θg = {W, b} is the parameter set of the en-
coder G and is shared across all windows of the
sequence. f is an element-wise non-linear activa-
tion function. The convolution operation can cap-
ture local contextual dependencies of the input se-
quence and the extracted feature vectors are sim-
ilar to n-grams. After the convolution operation
is applied to the whole sequence, we obtain a list
of hidden vectors H = (h1, h2, ..., hn). A max-
over-time pooling layer is applied to obtain the ﬁ-
nal vector representation ξ of the input document.

4.2 Datasets and Experimental Settings

Existing benchmark datasets such as the Amazon
benchmark (Blitzer et al., 2007) typically remove

reviews with neutral labels in both domains. This
is problematic as the label information of the tar-
get domain is not accessible in an unsupervised
domain adaptation setting. Furthermore, remov-
ing neutral instances may bias the dataset favor-
ably for max-margin-based algorithms like ours,
since the resulting dataset has all uncertain labels
removed, leaving only high conﬁdence examples.
Therefore, we construct new datasets by ourselves.
The results on the original Amazon benchmark is
qualitatively similar, and we present them in Ap-
pendix A for completeness since most of previous
works reported results on it.

Small-scale datasets: Our new dataset was de-
rived from the large-scale Amazon datasets2 re-
leased by McAuley et al. (2015). It contains four
domains3: Book (BK), Electronics (E), Beauty
(BT), and Music (M). Each domain contains two
datasets. Set 1 contains 6000 instances with ex-
actly balanced class labels, and set 2 contains
6000 instances that are randomly sampled from
the large dataset, preserving the original label dis-
tribution, which we believe better reﬂects the label
distribution in real life. The examples in these two
sets do not overlap. Detailed statistics of the gen-
erated datasets are given in Table 1a.

In all our experiments on the small-scale
datasets, we use set 1 of the source domain as the
only source with sentiment label information dur-
ing training, and we evaluate the trained model on
set 1 of the target domain. Since we cannot con-
trol the label distribution of unlabeled data during
training, we consider two different settings:
Setting (1): Only set 1 of the target domain is used
as the unlabeled set. This tells us how the method
performs in a condition when the target domain
has a close-to-balanced label distribution. As we
also evaluate on set 1 of the target domain, this is
also considered as a transductive setting.
Setting (2): Set 2 from both the source and target
domains are used as unlabeled sets. Since set 2 is
directly sampled from millions of reviews, it better
reﬂects real-life sentiment distribution.

Large-scale datasets: We further conduct ex-
periments on four much larger datasets: IMDB4

2http://jmcauley.ucsd.edu/data/amazon/
3The original reviews were rated on a 5-point scale. We
label them with rating < 3, > 3, and = 3 as negative, posi-
tive, and neutral respectively.

4IMDB is rated on a 10-point scale, and we label reviews
with rating < 5, > 6, and = 5/6 as negative, positive, and
neutral respectively.

(I), Yelp2014 (Y), Cell Phone (C), and Baby
(B). IMDB and Yelp2014 were previously used
in (Tang et al., 2015; Yang et al., 2017). Cell
phone and Baby are from the large-scale Amazon
dataset (McAuley et al., 2015; He and McAuley,
2016). Detailed statistics are summarized in Ta-
ble 1b. We keep all reviews in the original datasets
and consider a transductive setting where all target
examples are used for both training (without la-
bel information) and evaluation. We perform sam-
pling to balance the classes of labeled source data
in each minibatch B(s) during training.

4.3 Selection of Development Set

Ideally, the development set should be drawn from
the same distribution as the test set. However, un-
der the unsupervised domain adaptation setting,
we do not have any labeled target data at training
phase which could be used as development set. In
all of our experiments, for each pair of domains,
we instead sample 1000 examples from the train-
ing set of the source domain as development set.
We train the network for a ﬁxed number of epochs,
and the model with the minimum classiﬁcation er-
ror on this development set is saved for evaluation.
This approach works well on most of the problems
since the target domain is supposed to behave like
the source domain if the domain difference is ef-
fectively reduced.

Another problem is how to select the values for
hyper-parameters. If we tune λ1 and λ2 directly
on the development set from the source domain,
most likely both of them will be set to 0, as un-
labeled target data is not helpful for improving in-
domain accuracy of the source domain. Other neu-
ral network models also have the same problem for
hyper-parameter tuning. Therefore, our strategy is
to use the development set from the target domain
to optimize λ1 and λ2 for one problem (e.g., we
only do this on E→BK), and ﬁx their values on the
other problems. This setting assumes that we have
at least two labeled domains such that we can op-
timize the hyper-parameters, and then we ﬁx them
for other new unlabeled domains to transfer to.

4.4 Training Details and Hyper-parameters

We initialize word embeddings using the 300-
dimension GloVe vectors supplied by Pennington
et al., (2014), which were trained on 840 billion
tokens from the Common Crawl. For each pair of
domains, the vocabulary consists of the top 10000
most frequent words. For words in the vocabulary

but not present in the pre-trained embeddings, we
randomly initialize them.

We set hyper-parameters of

the CNN en-
coder following previous works (Kim, 2014; Yu
and Jiang, 2016) without speciﬁc tuning on our
datasets. The window size is set to 3 and the size
of the hidden layer is set to 300. The nonlinear
activation function is Relu. For regularization, we
also follow their settings and employ dropout with
probability set to 0.5 on ξi before feeding it to the
output layer F, and constrain the l2-norm of the
weight vector θf , setting its max norm to 3.

On the small-scale datasets and the Aamzon
benchmark, λ1 and λ2 are set to 200 and 1,
respectively,
tuned on the development set of
task E→BK under setting 1. On the large-scale
datasets, λ1 and λ2 are set to 500 and 0.2, re-
tuned on I→Y. We use a Gaussian
spectively,
curve w(t) = exp[−5(1 − t
)2]λ3 to ramp up
the weight of the bootstrapping loss Ω from 0 to
λ3, where tmax denotes the maximum number of
training epochs. We train 30 epochs for all exper-
iments. We set λ3 to 3 and α to 0.5 for all experi-
ments.

tmax

The batch size is set to 50 on the small-scale
datasets and the Amazon benchmark. We increase
the batch size to 250 on the large-scale datasets to
reduce the number of iterations. RMSProp opti-
mizer with learning rate set to 0.0005 is used for
all experiments.

4.5 Models for Comparison

We compare with the following baselines:

(1) Naive: A non-domain-adaptive baseline
with bag-of-words representations and SVM clas-
siﬁer trained on the source domain.

(2) mSDA (Chen et al., 2012): This is the state-
of-the-art method based on discrete input features.
Top 1000 bag-of-words features are kept as pivot
features. We set the number of stacked layers to 3
and the corruption probability to 0.5.

(3) NaiveNN: This is a non-domain-adaptive
CNN trained on source domain, which is a variant
of our model by setting λ1, λ2, and λ3 to zeros.

(4) AuxNN (Yu and Jiang, 2016): This is a neu-
ral model that exploits auxiliary tasks, which has
achieved state-of-the-art results on cross-domain
sentiment classiﬁcation. The sentence encoder
used in this model is the same as ours.

(5) ADAN (Chen et al., 2017): This method
exploits adversarial training to reduce representa-

(a) Accuracy on the small-scale dataset under setting 1.

(b) Accuracy on the small-scale dataset under setting 2.

(c) Macro-F1 on the large-scale dataset.

Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is
signiﬁcantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test.

tion difference between domains. The original pa-
per uses a simple feedforward network as encoder.
For fair comparison, we replace it with our CNN-
based encoder. We train 5 iterations on the dis-
criminator per iteration on the encoder and senti-
ment classiﬁer as suggested in their paper.

(6) MMD: MMD has been widely used for min-
imizing domain discrepancy on images. In those
works (Tzeng et al., 2014; Long et al., 2017), vari-
ants of deep CNNs are used for encoding images
and the MMDs of multiple layers are jointly mini-
mized. In NLP, adding more layers of CNNs may
not be very helpful and thus those models from
image-related tasks can not be directly applied
to our problem. To compare with MMD-based
method, we train a model that jointly minimize
the classiﬁcation loss L on the source domain and
i=1} and {ξ(t)
MMD between {ξ(s)
|nt
|ns
i=1}. For
computing MMD, we use a Gaussian RBF which
is a common choice for characteristic kernel.

i

i

In addition to the above baselines, we also show
results of different variants of our model. DAS
as shown in Algorithm 1 denotes our full model.
DAS-EM denotes the model with only entropy

minimization for semi-supervised learning (set
λ3 = 0). DAS-SE denotes the model with only
self-ensemble bootstrapping for semi-supervised
learning (set λ2 = 0). FANN (feature-adaptation
neural network) denotes the model without semi-
supervised learning performed (set both λ2 and λ3
to zeros).

4.6 Main Results

Figure 15 shows the comparison of adaptation re-
sults (see Appendix B for the exact numerical
numbers). We report classiﬁcation accuracy on
the small-scale dataset. For the large-scale dataset,
macro-F1 is instead used since the label distribu-
tion in the test set is extremely unbalanced. Key
observations are summarized as follows. (1) Both
DAS-EM and DAS-SE perform better in most
cases compared with ADAN, MDD, and FANN,
in which only feature adaptation is performed.
This demonstrates the effectiveness of the pro-

5We exclude results of Naive, mSDA and AuxNN on the
large-scale dataset. Both Naive and mSDA have difﬁculties
to scale up to the large dataset. AuxNN relies on manually
selecting positive and negative pivots before training.

Figure 2: Accuracy vs. percentage of unlabeled target training examples.

Figure 3: Accuracy vs. number of labeled target training examples.

posed domain adaptive semi-supervised learning
framework. DAS-EM is more effective than DAS-
SE in most cases, and the full model DAS with
both techniques jointly employed overall has the
(2) When comparing the two
best performance.
settings on the small-scale dataset, all domain-
adaptive methods6 generally perform better under
setting 1. In setting 1, the target examples are bal-
anced in classes, which can provide more diverse
opinion-related features. However, when consid-
ering unsupervised domain adaptation, we should
not presume the label distribution of the unlabeled
data. Thus, it is necessary to conduct experiments
using datasets that reﬂect real-life sentiment dis-
tribution as what we did on setting2 and the large-
scale dataset. Unfortunately, this is ignored by
most of previous works. (3) Word-embeddings are
very helpful, as we can see even NaiveNN can sub-
stantially outperform mSDA on most tasks.

To see the effect of semi-supervised learning
alone, we also conduct experiments by setting
λ1 = 0 to eliminate the effect of feature adapta-
tion. Both entropy minimization and bootstrap-
ping perform very badly in this setting. En-
tropy minimization gives almost random predic-
tions with accuracy below 0.4, and the results
of bootstrapping are also much lower compared
to NaiveNN. This suggests that the feature adap-
tation component is essential. Without it,
the
learned target representations are less meaning-
ful and discriminative. Applying semi-supervised

6Results of Naive and NaiveNN do not change under both

settings as they are only trained on the source domain.

learning in this case is likely to worsen the results.

4.7 Further Analysis

In Figure 2, we show the change of accuracy with
respect to the percentage of unlabeled data used
for training on three particular problems under set-
ting 1. The value at x = 0 denotes the accuracies
of NaiveNN which does not utilize any target data.
For DAS, we observe a nonlinear increasing trend
where the accuracy quickly improves at the be-
ginning, and then gradually stabilizes. For other
methods, this trend is less obvious, and adding
more unlabeled data sometimes even worsen the
results. This ﬁnding again suggests that the pro-
posed approach can better exploit the information
from unlabeled data.

We also conduct experiments under a setting
with a small number of labeled target examples
available. Figure 3 shows the change of accuracy
with respect to the number of labeled target exam-
ples added for training. We can observe that DAS
is still more effective under this setting, while the
performance differences to other methods gradu-
ally decrease with the increasing number of la-
beled target examples.

4.8 CNN Filter Analysis

In this subsection, we aim to better understand
DAS by analyzing sentiment-related CNN ﬁlters.
To do that, 1) we ﬁrst select a list of the most re-
lated CNN ﬁlters for predicting each sentiment la-
bel (positive, negative neutral). Those ﬁlters can
be identiﬁed according to the learned weights θf

best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

(a) NaiveNN

(b) FANN

(c) DAS

Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the
5 most positive-sentiment-related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

of the output layer F. Higher weight indicates
stronger relatedness. 2) Recall that in our im-
plementation, each CNN ﬁlter has a window size
of 3 with Relu activation. We can thus represent
each selected ﬁlter as a ranked list of trigrams with
highest activation values.

We analyze the CNN ﬁlters

learned by
NaiveNN, FANN and DAS respectively on task
E→BT under setting 1. We focus on E→BT for
study because electronics and beauty are very dif-
ferent domains and each of them has a diverse
set of domain-speciﬁc sentiment expressions. For
each method, we identify the top 10 most related
ﬁlters for each sentiment label, and extract the top
trigrams of each selected ﬁlter on both source and
target domains. Since labeled source examples are
used for training, we ﬁnd the ﬁlters learned by the
three methods capture similar expressions on the
source domain, containing both domain-invariant
and domain-speciﬁc trigrams. On the target do-
main, DAS captures more target-speciﬁc expres-
sions compared to the other two methods. Due
to space limitation, we only present a small sub-
set of positive-sentiment-related ﬁlters in Table 2.
The complete results are provided in Appendix C.
From Table 2, we can observe that the ﬁlters
learned by NaiveNN are almost unable to cap-
ture target-speciﬁc sentiment expressions, while
FANN is able to capture limited target-speciﬁc
words such as “clean” and “scent”. The ﬁlters
learned by DAS are more domain-adaptive, cap-
turing diverse sentiment expressions in the target
domain.

5 Conclusion

In this work, we propose DAS, a novel frame-
work that jointly performs feature adaptation and
semi-supervised learning. We have demonstrated
through multiple experiments that DAS can better
leverage unlabeled data, and achieve substantial
improvements over baseline methods. We have
also shown that feature adaptation is an essen-
tial component, without which, semi-supervised
learning is not able to function properly. The pro-
posed framework could be potentially adapted to
other domain adaptation tasks, which is the focus
of our future studies.

References

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: domain adaptation for sentiment classiﬁ-
In Annual Meeting of the Association for
cation.
Computational Linguistics.

Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In The 29th Interna-
tional Conference on Machine Learning.

Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie,
and Kilian Weinberger. 2017. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁer. In Arxiv e-prints arXiv:1606.01614.

Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classiﬁers for
text classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: a deep learning approach. In The 28th
International Conference on Machine Learning.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
In
supervised learning by entropy minimization.
Neural Information Processing Systems.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander Smola. 2012. A
kernel two-sample test. Journal of Machine Learn-
ing Research, 13:723–773.

Ruining He and Julian McAuley. 2016. Ups and
downs: modeling the visual evolution of fashion
In
trends with one-class collaborative ﬁltering.
WWW.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classiﬁcation. In ACL.

Jing Jiang and ChengXiang Zhai. 2007.

Instance
weighting for domain adaptation in NLP. In Annual
Meeting of the Association for Computational Lin-
guistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Conference on Empirical
Methods in Natural Language Processing.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In International
Conference on Learning Representation.

Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial mem-
ory network for cross-domain sentiment classiﬁca-
tion. In The 26th International Joint Conference on
Artiﬁcial Intelligence.

Mingsheng Long, Yue Cao,

Jianmin Wang, and
Michael I. Jordan. 2015. Learning transferable fea-
In Interna-
tures with deep adaptation networks.
tional Conference on Machine Learning.

Mingsheng Long, Han Zhu,

Jianmin Wang, and
Michael I. Jordan. 2017. Deep transfer learning with
joint adaptation networks. In International Confer-
ence on Machine Learning.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
ommendations on styles and substitutes. In The 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Neural Information Processing Systems.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classiﬁcation via spectral feature alignment. In
The 19th International World Wide Web Conference.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representation of users and products for
document level sentiment classiﬁcation. In Annual
Meeting of the Association for Computational Lin-
guistics.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
maximizing for domain invariance. In Arxiv e-prints
arXiv:1412.3474.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Conference on Empirical Meth-
ods in Natural Language Processing.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Annual
Meeting of the Association for Computational Lin-
guistics.

Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In Conference on
Empirical Methods in Natural Language Process-
ing.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Annual Meeting of the Association
for Computational Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
In Conference on Empirical
timent classiﬁcation.
Methods in Natural Language Processing.

Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-
aohua Tony Hu. 2015. Linking heterogeneous input
features with pivots for domain adaptation. In The
24th International Joint Conference on Artiﬁcial In-
telligence.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016. Bi-transferring deep neural
networks for domain adaptation. In Annual Meeting
of the Association for Computational Linguistics.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: transfer learning with deep autoencoders.
In The 24th International Joint Conference on Arti-
ﬁcial Intelligence.

S

D
E
K
B
E
K
B
D
K
B
D
E

T

B
B
B
D
D
D
E
E
E
K
K
K

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE
82.10∗
77.75
79.60
82.35
79.75
82.15∗
75.80
80.05
85.95
79.45∗
79.50
83.80

82.00∗
80.25∗
79.95
82.65
81.40∗
81.65∗
80.25∗
81.40∗
85.70
81.55∗
80.80
84.50

80.30
77.25
79.20
81.65
79.55
76.90
76.75
79.25
85.60
77.55
78.00
83.85

81.05
78.65
79.70
82.00
80.10
79.35
76.45
80.20
85.75
75.20
79.70
81.75

81.70
78.55
79.25
82.30
79.70
80.45
77.60
79.70
86.85
76.10
77.35
83.95

80.80
78.00
77.85
81.75
80.65
78.90
76.40
77.55
84.05
78.10
80.05
84.15

78.50
76.15
75.65
80.60
76.30
76.05
75.55
76.00
84.20
75.95
76.30
84.45

75.20
68.85
70.00
77.15
69.50
71.40
72.15
71.65
79.75
73.50
72.00
82.80

81.10
77.95
77.75
80.80
77.00
79.35
76.20
76.60
84.85
77.40
78.55
84.95

Average

73.66

77.98

79.38

79.85

80.29

80.00

79.65

81.84

80.68

DAS
82.05∗
80.00∗
80.05∗
82.75∗
80.15
81.40∗
81.15∗
81.55∗
85.80
82.25∗
81.50∗
84.85

81.96

Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations
are reported for each neural method. ∗ indicates that the proposed method (DAS-EM, DAS-SE, DAS) is
signiﬁcantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

A Results on Amazon Benchmark

C CNN Filter Analysis Full Results

As mentioned in Section 4.8, we conduct CNN ﬁl-
ter analysis on NaiveNN, FANN, and DAS. For
each method, we identify the top 10 most related
ﬁlters for positive, negative, neutral sentiment la-
bels respectively, and then represent each selected
ﬁlter as a ranked list of trigrams with the highest
activation values on it. Table 5, 6, 7 in the fol-
lowing pages illustrate the trigrams from the tar-
get domain (beauty) captured by the selected ﬁl-
ters learned on E→BT for each method.

We can observe that compared to NaiveNN and
FANN, DAS is able to capture a more diverse set
of relevant sentiment expressions on the target do-
main for each sentiment label. This observation is
consistent with our motivation. Since NaiveNN,
FANN and other baseline methods solely train
the sentiment classiﬁer on the source domain, the
learned encoder is not able to produce discrimina-
tive features on the target domain. DAS addresses
this problem by reﬁning the classiﬁer on the tar-
get domain with semi-supervised learning, and
the overall objective forces the encoder to learn
feature representations that are not only domain-
invariant but also discriminative on both domains.

Most previous works (Blitzer et al., 2007; Pan
et al., 2010; Glorot et al., 2011; Chen et al.,
2012; Zhou et al., 2016) carried out experiments
on the Amazon benchmark released by Blitzer
et al.
(2007). The dataset contains 4 different
domains: Book (B), DVDs (D), Electronics (E),
and Kitchen (K). Following their experimental set-
tings, we consider the binary classiﬁcation task to
predict whether a review is positive or negative
on the target domain. Each domain consists of
1000 positive and 1000 negative reviews respec-
tively. We also allow 4000 unlabeled reviews to
be used for both the source and the target domains,
of which the positive and negative reviews are bal-
anced as well, following the settings in previous
works. We construct 12 cross-domain sentiment
classiﬁcation tasks and split the labeled data in
each domain into a training set of 1600 reviews
and a test set of 400 reviews. The classiﬁer is
trained on the training set of the source domain
and is evaluated on the test set of the target do-
main. The comparison results are shown in Ta-
ble 3.

B Numerical Results of Figure 1

Due to space limitation, we only show results in
ﬁgures in the paper. All numerical numbers used
for plotting Figure 1 are presented in Table 4. We
can observe that DAS-EM, DAS-SE, and DAS
all achieve substantial improvements over baseline
methods under different settings.

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

BK 49.07
E
BK 48.17
BT
M BK 45.20
46.43
E
BK
53.63
E
BT
37.93
E
M
45.57
BK BT
48.43
E
BT
39.42
M BT
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

BK 49.07
E
BK 48.17
BT
M BK 45.20
E
46.43
BK
E
BT
53.63
37.93
M
E
BK BT
45.57
48.43
E
BT
M BT
39.43
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

55.13
53.53
49.22
48.22
57.32
38.13
50.77
53.13
39.37
47.88
47.88
49.62
49.18

52.88
47.65
48.33
47.07
55.12
37.40
49.63
51.98
37.73
45.97
45.12
45.78
47.06

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

60.62
59.86
60.43
48.45
60.98
49.60
48.67
55.58
48.65
48.87
51.19
53.11
53.84

57.72
58.46
58.15
48.22
59.08
49.43
47.80
54.37
46.92
48.79
52.31
53.55
52.98

63.32
65.62
62.87
47.42
63.13
46.57
46.14
50.98
44.26
51.10
50.23
55.35
53.92

57.07
59.78
58.67
49.48
59.45
47.00
47.52
51.28
45.73
50.20
52.57
54.68
52.79

60.38
59.66
60.20
53.32
60.53
51.55
49.48
54.83
48.35
53.04
51.81
54.43
54.80

57.43
56.17
57.08
45.42
60.24
48.72
45.43
54.92
46.68
48.76
51.50
54.55
52.23

59.59
59.28
57.65
51.27
60.62
47.23
50.24
56.78
48.89
52.35
52.14
53.84
54.15

56.43
57.98
57.75
51.95
58.67
48.92
49.83
55.42
48.48
49.47
48.18
53.41
53.04

66.48∗
66.78∗
69.63∗
58.59∗
65.71∗
55.88∗
49.49
61.53∗
47.65
55.47∗
58.28∗
60.95∗
59.74

57.78
61.17∗
58.62
54.51∗
61.27
51.28∗
53.72∗
53.10
47.18
52.37∗
53.63∗
56.24∗
55.07

(a) Accuracy on the small-scale dataset under setting 1

DAS
67.12∗
66.53
70.31∗
58.73∗
66.14∗
55.78∗
51.30∗
60.76∗
50.66∗
55.98∗
59.06∗
60.5∗
60.24

55.20
63.32∗
60.77∗
53.92∗
59.83
52.88∗
54.67∗
56.05∗
49.73∗
53.52∗
55.38∗
56.02∗
55.94

62.37
61.17
65.24∗
55.15∗
61.78
53.22∗
54.23∗
59.52∗
50.67∗
55.13∗
55.60∗
56.90∗
57.58

58.93
60.17∗
58.25
52.47∗
61.42
51.18∗
51.23∗
56.43∗
51.57∗
52.68∗
52.25
56.23∗
55.23

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

DAS

(b) Accuracy on the small-scale dataset under setting 2

S

T

Y
I
C
I
B
I
I
Y
C
Y
B
Y
I
C
Y
C
B
C
I
B
Y
B
B
C
Average

NaiveNN ADAN MMD FANN DAS-EM DAS-SE
56.66∗
55.18
54.30
58.72∗
59.14
58.43
51.97∗
54.67∗
59.98
50.81
52.95
58.12
55.91

55.04
57.27∗
57.31∗
57.92∗
61.17
59.94∗
53.46∗
53.48
59.84
48.84
52.87
57.74
56.24

54.16
53.35
51.40
56.52
60.81
58.77
50.49
53.12
61.23
47.35
54.43
60.52
55.18

54.46
53.07
52.39
56.30
56.02
55.72
51.04
51.86
60.19
48.17
53.54
55.56
54.02

55.52
55.07
54.64
52.57
60.70
58.42
47.27
52.53
59.91
46.34
50.82
59.99
54.48

53.01
51.84
45.85
55.46
61.22
56.86
50.38
53.87
59.48
50.05
54.73
60.47
54.43

DAS
58.54∗
57.28∗
58.02∗
58.92∗
61.39
61.87∗
53.38∗
55.44∗
59.76
48.84
52.91
59.75
57.18

(c) Macro-F1 scores on the large-scale dataset

Table 4: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signiﬁcantly
better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

1
best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

2
highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

3
nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

4
beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

5
since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

6
store-and-am
cleanser-and-am
olay-and-am
daily-and-need
shower-and-noticed

7
ofﬁce-setting-thanks
locks-shimmering-color
dirty-blonde-color
victoria-secrets-gorgeous
dirty-pinkish-color

8
car-washes-!
price-in-stores
products-are-priced
car-and-burning
from-our-store

(a) NaiveNN

9
speed-is-perfect
buttons-are-perfect
unit-is-superb
spray-is-perfect
coverage-is-excellent

10
!-i-recommend
!-i-highly
shower-i-slather
spots-i-needed
best-i-use

1
prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

2
so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

3
purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

4
feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

5
are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

6
shower-or-cleaning
water-onto-my
bleach-your-towels
pump-onto-my
water-great-for

8

7
deﬁnitely-purchase-again more-affordable-price
deﬁnitely-buy-again
perfect-for-my
deﬁnitely-order-again
super-happy-to

a-perfect-length
an-exceptional-value
’ve-enjoyed-it
pretty-decent-layer

9
absolutely-wonderful-!
perfect-for-running
concealer-for-my
moisturizing-for-my
super-glue-even

10
felt-cleaner-than
ﬂat-iron-through
rubbed-grease-on
deeply-cleans-my
being-cleaner-after

1
bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

2
love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

3
feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

4
cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

5
excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

6
shower-lather-wonderfully
affordable-cleans-nicely
peels-great-price
daughter-loves-this
cleans-great-smells

7
highly-recommend-!
deﬁnitely-recommend-!
love-love-!
highly-advise-!
time-advise-!

8
excellent-fragrance-and
fantastic-for-daytime
wonderfully-moisturizing-and
lathers-great-cleans
delightful-shampoo-works

10
9
its-unique-smoothing
forgeous-gragrance-mist
smooth-luxurious-texture wonderful-bedtime-scent
’s-extremely-gentle
’s-affordable-combination
absorbs-quite-well

love-essie-polish
perfect-beautiful-shimmer
fantastic-coverage-hydrates

(b) FANN

(c) DAS

Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
pads-ruined-my
highly-disappointed-and
dryers-blew-my
completely-worthless-didn’t
am-disappointed-and

2
simply-threw-out
reviewer-pointed-out
extracts-broke-into
actually-threw-out
clips-barely-keep

3
hours-after-trying
minutes-after-rinsing
disappointed-after-trying
lips-after-trying
dry-after-shampooing

4
junk-drawer-∗
refund-time-!
total-fake-wen
waste-your-time
total-fail-!

5
contacted-manufacturer-about
minutes-not-worth
’ve-owned-this
hour-unless-it
results-they-claim

6
were-awful-garbage
what-awful-garbage
and-utter-waste
are-absolute-garbage
piece-of-junk

7
two-failed-attempts
a-mistake-save
a-deﬁnite-return
a-pathetic-limp
a-total-disappointment

8
auto-ship-sent
am-returning-to
am-unable-to
am-pale-ghost
got-returned-and

10
broke-don’t-ﬁx
sent-me-expired

9
refund-and-dispose
refund-spend-your
wouldn’t-recommend-! wearing-false-eyelashes
not-buy-dunhill
not-worth-returning

a-temporary-ﬁx
a-disappointment-cheap

(a) NaiveNN

2
the-worse-mascaras

1
nasty-sunburn-lol
bother-returning-them it-caused-patchy
fails-miserably-at
minutes-auric-needs
severely-burned-me

lifeless-disaster-enter
it-fails-miserably
feel-worse-leaving

3
stale-very-unhappy
were-horrible-failures
send-this-crap
were-awful-garbage
were-horribly-red

4
actually-hurts-your
didn’t-bother-returning
it-hurts-your
didn’t-exist-in
skin-horribly-after

5
a-return-label
stay-away-completely
like-bug-quit
a-defective-brown
’d-refund-the

6
worse-with-exercise
worse-and-after
unable-to-return
worse-my-face
poorly-in-step

7
not-stink-your
mistake-save-your
nothing-!-by
nothing-happened-!
nothing-save-your

8
it-fails-miserably
is-ineffective-apart
but-horribly-unhealthy
a-pathetic-limp
a-worse-job

9
got-progressively-worse
gave-opposite-result
another-epic-fail
got-horribly-painful
was-downright-painful

10
stopped-working-for
uncomfortable-i-returned
i-am-returning
stopped-working-shortly
not-waterproof-makeup

1
poorly-designed-product
defective-dryer-promising
disgusting-smelling-thing
hurts-your-scalp
hurts-your-hair

2
a-refund-spend
a-refund-save
i-regret-spending
just-wouldn’t-spend
looked-washed-out

3
completely-waste-of
of-junk-*
were-awful-garbage
worthless-waste-of
throwing-money-away

4
smells-disgusting-!
smells-horribly-like
does-not-straighten
’s-false-advertising
a-disgusting-cheap

5
burning-rubber-stench
began-smelling-vomit
reaction-and-wasted
control-and-smelled
using-this-disgusting

6
super-irritating-!
strong-reaction-and
really-burned-and
very-pasty-and
super-streaky-and

7
got-promptly-broke
after-ive-washed
after-several-attempts
this-stuff-stinks
again-i-threw

8
sore-and-painful
is-simply-irritating
tight-and-uncomfortable
drying-and-irritating
goopy-and-unpleasant

9
it-caused-patchy
layer-hydrogenated-alcohols
the-harmful-uva
my-severe-dark
a-allergic-reaction

10
painful-it-hurt
unnecessary-health-risks
uncomfortable-to-wear
stinging-your-eyes
unbearable-to-wear

(b) FANN

(c) DAS

Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
purpose-cologne-splash
other-hanae-mori
the-mavala-peeled
avoid-hair-pulling
cause-rashes-stinging

2
okay-cord-was
cocamide-dea-is
coily-conditioner-is
ﬂaky-dandruff-is
quickly-cord-is

3
hands-feet-elbows
been-sealed-tight
stainless-steel-blackhead
severely-tight-chest
thick-nasty-callouses

4
aggressive-in-general
pimples-in-general
biotin-in-general
dimethicone-is-terrible
but-in-general

5
but-its-okay
it-moisturizes-okay
but-moisturizes-keeps
but-don’t-expect
it-lathers-ok

6
pretty-damaged-from
daughter-suffers-from
teenager-suffers-from
tissue-damage-during
the-damage-on

7
darker-olive-complexion
stronger-healthier-or
natural-ingredient-however
vitamin-enriched-color
natural-ingredients-∗

8
doesn’t-mind-pushing
kinda-doesn’t-its
kinda-kinky-coily
okay-job-of
intended-purpose-that

9
producto-por-los
unstuck-frownies-∗
they-push-∗
uva-rays-uva
tend-to-slip

10
feeling-didn’t-last
curls-didn’t-last
extra-uv-protection
garnier-fructis-curl
the-mavala-after

(a) NaiveNN

1
worse-and-after
worse-before-improving
unable-to-return
unless-your-entire
horrible-in-execution

2
maybe-a-refund
ok-mask-i
ok-pining-it
ok-try-i
ok-tho-i

3
very-disappointing-waste
ok-but-clean
ok-but-will
ok-but-didn’t
ok-nothing-special

4
my-ears-are
my-neck-line
cause-unsightly-beads
my-sporadic-line
your-ear-is

5
pretty-neutral-neither
ok-so-if
ok-during-pregnancy
kinda-annoying-if
ok-this-seems

6
uncomfortable-i-returned
weak-they-bend
claimed-faulty-∗
suffers-from-wind
as-defective-∗

7
sticky-lathers-and
quickly-deep-cleans
but-elegant-bottle
beat-the-price
and-reasonably-priced

8
some-fading-when
real-disappointment-the
especially-noticeable-after
progressively-worse-during
style-unfortunately-the

9
are-very-painful
are-less-painful
are-a-pain
about-sum-damage
offered-no-pain

10
its-also-convenient
that-also-my
that-may-make
that-allows-your
its-helpful-to

1
’m-kinda-pale
a-terrible-headache
but-kinda-annoying
’m-kinda-mad
i-kinda-stopped

2
darker-but-nope
gray-didn’t-cover
makeup-doesn’t-sweat
dark-spots-around
moist-but-thats

3
ok-but-horrible
ok-but-didn’t
okay-but-doesn’t
okay-however-it
unfortunately-straight

4
noticeable-i-avoid
however-i-lean
but-otherwise-ok
but-im-deciding
however-i-prefer

5
same-result-mediocre
it-caused-patchy
doesn’t-cause-ﬂare
the-harmful-uva
rather-unpleasant-smell

6
kinda-annoying-if
pretty-bad-breakage
my-slight-discoloration
smells-kinda-bad
look-kinda-crappy

7
brutal-winter-however
summer-color-however
beige-shade-however
is-okay-however
bit-greasy-however

8
higher-rating-because
slight-burnt-rubber
noticeable-tan-since
somewhat-pale-affect
kinda-pale-so

9
nothing-for-odor
kinda-recommend-this
not-recommend-if
noticeable-but-non
nothing-special-moderate

10
but-darker-*
slightly-darker-shade
somewhat-pale-affect
but-somewhat-heavy
bit-dull-heavy

(b) FANN

(c) DAS

Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

Adaptive Semi-supervised Learning for Cross-domain
Sentiment Classiﬁcation

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore
‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg
‡d.dahlmeier@sap.com

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
3
5
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We consider the cross-domain sentiment clas-
siﬁcation problem, where a sentiment classi-
ﬁer is to be learned from a source domain and
to be generalized to a target domain. Our ap-
proach explicitly minimizes the distance be-
tween the source and the target instances in
an embedded feature space. With the differ-
ence between source and target minimized,
we then exploit additional information from
the target domain by consolidating the idea
of semi-supervised learning, for which, we
jointly employ two regularizations – entropy
minimization and self-ensemble bootstrapping
– to incorporate the unlabeled target data for
classiﬁer reﬁnement. Our experimental results
demonstrate that the proposed approach can
better leverage unlabeled data from the target
domain and achieve substantial improvements
over baseline methods in various experimental
settings.

1

Introduction

In practice, it is often difﬁcult and costly to anno-
tate sufﬁcient training data for diverse application
domains on-the-ﬂy. We may have sufﬁcient la-
beled data in an existing domain (called the source
domain), but very few or no labeled data in a
new domain (called the target domain). This issue
has motivated research on cross-domain sentiment
classiﬁcation, where knowledge in the source do-
main is transferred to the target domain in order to
alleviate the required labeling effort.

One key challenge of domain adaptation is that
data in the source and target domains are drawn
from different distributions. Thus, adaptation per-
formance will decline with an increase in distribu-
tion difference. Speciﬁcally, in sentiment analy-
sis, reviews of different products have different vo-
cabulary. For instance, restaurants reviews would
contain opinion words such as “tender”, “tasty”, or

“undercooked” and movie reviews would contain
“thrilling”, “horriﬁc”, or “hilarious”. The intersec-
tion between these two sets of opinion words could
be small which makes domain adaptation difﬁcult.
Several techniques have been proposed for ad-
dressing the problem of domain shifting. The
aim is to bridge the source and target domains
by learning domain-invariant feature representa-
tions so that a classiﬁer trained on a source do-
main can be adapted to another target domain.
In cross-domain sentiment classiﬁcation, many
works (Blitzer et al., 2007; Pan et al., 2010; Zhou
et al., 2015; Wu and Huang, 2016; Yu and Jiang,
2016) utilize a key intuition that domain-speciﬁc
features could be aligned with the help of domain-
invariant features (pivot features). For instance,
“hilarious” and “tasty” could be aligned as both
of them are relevant to “good”.

Despite their promising results,

these works
share two major limitations. First, they highly de-
pend on the heuristic selection of pivot features,
which may be sensitive to different applications.
Thus the learned new representations may not ef-
fectively reduce the domain difference. Further-
more, these works only utilize the unlabeled tar-
get data for representation learning while the sen-
timent classiﬁer was solely trained on the source
domain. There have not been many studies on ex-
ploiting unlabeled target data for reﬁning the clas-
siﬁer, even though it may contain beneﬁcial infor-
mation. How to effectively leverage unlabeled tar-
get data still remains an important challenge for
domain adaptation.

In this work, we argue that the information
from unlabeled target data is beneﬁcial for do-
main adaptation and we propose a novel Domain
Adaptive Semi-supervised learning framework
(DAS) to better exploit it. Our main intuition is
to treat the problem as a semi-supervised learn-
ing task by considering target instances as unla-

beled data, assuming the domain distance can be
effectively reduced through domain-invariant rep-
resentation learning. Speciﬁcally, the proposed
approach jointly performs feature adaptation and
semi-supervised learning in a multi-task learning
setting. For feature adaptation, it explicitly mini-
mizes the distance between the encoded represen-
tations of the two domains. On this basis, two
semi-supervised regularizations – entropy mini-
mization and self-ensemble bootstrapping – are
jointly employed to exploit unlabeled target data
for classiﬁer reﬁnement.

We evaluate our method rigorously under multi-
ple experimental settings by taking label distribu-
tion and corpus size into consideration. The re-
sults show that our model is able to obtain sig-
niﬁcant improvements over strong baselines. We
also demonstrate through a series of analysis that
the proposed method beneﬁts greatly from incor-
porating unlabeled target data via semi-supervised
learning, which is consistent with our motivation.
Our datasets and source code can be obtained from
https://github.com/ruidan/DAS.

2 Related Work

Domain Adaptation: The majority of feature
adaptation methods for sentiment analysis rely on
a key intuition that even though certain opinion
words are completely distinct for each domain,
they can be aligned if they have high correlation
with some domain-invariant opinion words (pivot
words) such as “excellent” or “terrible”. Blitzer
et al. (2007) proposed a method based on struc-
tural correspondence learning (SCL), which uses
pivot feature prediction to induce a projected fea-
ture space that works well for both the source and
the target domains. The pivot words are selected in
a way to cover common domain-invariant opinion
words. Subsequent research aims to better align
the domain-speciﬁc words (Pan et al., 2010; He
et al., 2011; Wu and Huang, 2016) such that the
domain discrepancy could be reduced. More re-
cently, Yu and Jiang (2016) borrow the idea of
pivot feature prediction from SCL and extend it
to a neural network-based solution with auxiliary
tasks.
In their experiment, substantial improve-
ment over SCL has been observed due to the use
of real-valued word embeddings. Unsupervised
representation learning with deep neural networks
(DNN) such as denoising autoencoders has also
been explored for feature adaptation (Glorot et al.,

2011; Chen et al., 2012; Yang and Eisenstein,
2014). It has been shown that DNNs could learn
transferable representations that disentangle the
underlying factors of variation behind data sam-
ples.

Although the aforementioned methods aim to
reduce the domain discrepancy, they do not explic-
itly minimize the distance between distributions,
and some of them highly rely on the selection of
pivot features. In our method, we formally con-
struct an objective for this purpose. Similar ideas
have been explored in many computer vision prob-
lems, where the representations of the underlying
domains are encouraged to be similar through ex-
plicit objectives (Tzeng et al., 2014; Ganin and
Lempitsky, 2015; Long et al., 2015; Zhuang et al.,
2015; Long et al., 2017) such as maximum mean
discrepancy (MMD) (Gretton et al., 2012). In NLP
tasks, Li et al. (2017) and Chen et al. (2017) both
proposed using adversarial training framework for
reducing domain difference. In their model, a sub-
network is added as a domain discriminator while
deep features are learned to confuse the discrim-
inator. The feature adaptation component in our
model shares similar intuition with MMD and ad-
versary training. We will show a detailed compar-
ison with them in our experiments.
Semi-supervised Learning: We attempt to treat
domain adaptation as a semi-supervised learning
task by considering the target instances as unla-
beled data. Some efforts have been initiated on
transfer learning from unlabeled data (Dai et al.,
2007; Jiang and Zhai, 2007; Wu et al., 2009).
In our model, we reduce the domain discrep-
ancy by feature adaptation, and thereafter adopt
semi-supervised learning techniques to learn from
unlabeled data. Primarily motivated by (Grand-
valet and Bengio, 2004) and (Laine and Aila,
2017), we employed entropy minimization and
self-ensemble bootstrapping as regularizations to
incorporate unlabeled data. Our experimental re-
sults show that both methods are effective when
jointly trained with the feature adaptation objec-
tive, which conﬁrms to our motivation.

3 Model Description

3.1 Notations and Model Overview

We conduct most of our experiments under an un-
supervised domain adaptation setting, where we
have no labeled data from the target domain. Con-
sider two sets Ds and Dt. Ds = {x(s)
i=1 is
i

i }|ns

, y(s)

i }|nt

from the source domain with ns labeled examples,
where yi ∈ RC is a one-hot vector representation
of sentiment label and C denotes the number of
classes. Dt = {x(t)
i=1 is from the target domain
with nt unlabeled examples. N = ns + nt denotes
the total number of training documents including
both labeled and unlabeled1. We aim to learn a
sentiment classiﬁer from Ds and Dt such that the
classiﬁer would work well on the target domain.
We also present some results under a setting where
we assume that a small number of labeled target
examples are available (see Figure 3).

For the proposed model, we denote G parame-
terized by θg as a neural-based feature encoder that
maps documents from both domains to a shared
feature space, and F parameterized by θf as a
fully connected layer with softmax activation serv-
ing as the sentiment classiﬁer. We aim to learn fea-
ture representations that are domain-invariant and
at the same time discriminative on both domains,
thus we simultaneously consider three factors in
our objective: (1) minimize the classiﬁcation error
on the labeled source examples; (2) minimize the
domain discrepancy; and (3) leverage unlabeled
data via semi-supervised learning.

Suppose we already have the encoded features
of documents {ξ(s,t)
i=1 (see
Section 4.1), the objective function for purpose (1)
is thus the cross entropy loss on the labeled source
examples

= G(x(s,t)

; θg)}|N

i

i

L = −

1
ns

ns(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(s)
y(s)

i (j)

(1)

i

i = F(ξ(s)

where ˜y(s)
; θf ) denotes the predicted la-
bel distribution. In the following subsections, we
will explain how to perform feature adaptation and
domain adaptive semi-supervised learning in de-
tails for purpose (2) and (3) respectively.

3.2 Feature Adaptation

Unlike prior works (Blitzer et al., 2007; Yu and
Jiang, 2016), our method does not attempt to align
domain-speciﬁc words through pivot words.
In
our preliminary experiments, we found that word
embeddings pre-trained on a large corpus are able
to adequately capture this information. As we will

1Note that unlabeled source examples can also be in-
cluded for training. In that case, N = ns + nt + ns(cid:48) where
ns(cid:48) denotes the number of unlabeled source examples. This
corresponds to our experimental setting 2. For simplicity, we
only consider ns and nt in our description.

later show in our experiments, even without adap-
tation, a naive neural network classiﬁer with pre-
trained word embeddings can already achieve rea-
sonably good results.

i }nt

i }|ns

i=1 and {ξ(t)

We attempt to explicitly minimize the distance
between the source and target feature represen-
tations ({ξ(s)
i=1). A few meth-
ods from literature can be applied such as Maxi-
mum Mean Discrepancy (MMD) (Gretton et al.,
2012) or adversary training (Li et al., 2017; Chen
et al., 2017). The main idea of MMD is to esti-
mate the distance between two distributions as the
distance between sample means of the projected
embeddings in Hilbert space. MMD is implicitly
computed through a characteristic kernel, which is
used to ensure that the sample mean is injective,
leading to the MMD being zero if and only if the
distributions are identical. In our implementation,
we skip the mapping procedure induced by a char-
acteristic kernel for simplifying the computation
and learning. We simply estimate the distribution
distance as the distance between the sample means
in the current embedding space. Although this ap-
proximation cannot preserve all statistical features
of the underlying distributions, we ﬁnd it performs
comparably to MMD on our problem. The follow-
ing equations formally describe the feature adap-
tation loss J :

J = KL(gs||gt) + KL(gt||gs)
g(cid:48)
s
(cid:107)g(cid:48)
s(cid:107)1

g(cid:48)
s =

1
ns

ξ(s)
i

gs =

ns(cid:88)

,

i=1

nt(cid:88)

i=1

1
nt

g(cid:48)
t =

ξ(t)
i

,

gt =

g(cid:48)
t
(cid:107)g(cid:48)
t(cid:107)1

(2)

(3)

(4)

s and g(cid:48)

L1 normalization is applied on the mean represen-
tations g(cid:48)
t, rescaling the vectors such that
all entries sum to 1. We adopt a symmetric ver-
sion of KL divergence (Zhuang et al., 2015) as the
distance function. Given two distribution vectors
P, Q ∈ Rk, KL(P||Q) = (cid:80)k

i=1 P(i) log( P(i)

Q(i) ).

3.3 Domain Adaptive Semi-supervised

Learning (DAS)

We attempt to exploit the information in target
data through semi-supervised learning objectives,
which are jointly trained with L and J . Normally,
to incorporate target data, we can minimize the
cross entropy loss between the true label distri-
butions y(t)
and the predicted label distributions
i

i = F(ξ(t)
˜y(t)
; θf ) over target samples. The chal-
i
lenge here is that y(t)
is unknown, and thus we
i
attempt to estimate it via semi-supervised learn-
ing. We use entropy minimization and bootstrap-
ping for this purpose. We will later show in our
experiments that both methods are effective, and
jointly employing them overall yields the best re-
sults.
Entropy Minimization: In this method, y(t)
is
i
estimated as the predicted label distribution ˜y(t)
,
i
which is a function of θg and θf . The loss can thus
be written as

Γ = −

1
nt

nt(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(t)
˜y(t)

i (j)

(5)

Assume the domain discrepancy can be effectively
reduced through feature adaptation, by minimiz-
ing the entropy penalty, training of the classiﬁer
is inﬂuenced by the unlabeled target data and will
generally maximize the margins between the tar-
get examples and the decision boundaries, increas-
ing the prediction conﬁdence on the target domain.

training.

Self-ensemble Bootstrapping: Another way to
estimate y(t)
corresponds to bootstrapping. The
i
idea is to estimate the unknown labels as the
predictions of the model learned from the pre-
vious round of
Bootstrapping has
been explored for domain adaptation in previous
works (Jiang and Zhai, 2007; Wu et al., 2009).
However, in their methods, domain discrepancy
was not explicitly minimized via feature adap-
tation. Applying bootstrapping or other semi-
supervised learning techniques in this case may
worsen the results as the classiﬁer can perform
quite bad on the target data.

Inspired by the ensembling method proposed
in (Laine and Aila, 2017), we estimate y(t)
by
i
forming ensemble predictions of labels during
training, using the outputs on different training
epochs. The loss is formulated as follows:

Ω = −

1
N

N
(cid:88)

C
(cid:88)

i=1

j=1

˜z(s,t)
i

(j) log ˜y(s,t)

(j)

i

(6)

where ˜z denotes the estimated labels computed on
the ensemble predictions from different epochs.
It serves
The loss is applied on all documents.
for bootstrapping on the unlabeled target data, and
it also serves as a regularization that encourages

Algorithm 1 Pseudocode for training DAS
Require: Ds, Dt, G, F
Require: α = ensembling momentum, 0 ≤ α < 1
Require: w(t) = weight ramp-up function

Z ← 0[N ×C]
˜z ← 0[N ×C]
for t ∈ [1, max-epochs] do

for each minibatch B(s), B(t), B(u) in

Ds, Dt, {x(s,t)

}|N

i=1 do

i
compute loss L on [xi∈B(s), yi∈B(s)]
compute loss J on [xi∈B(s), xj∈B(t)]
compute loss Γ on xi∈B(t)
compute loss Ω on [xi∈B(u), ˜zi∈B(u)]
overall-loss ← L + λ1J + λ2Γ + w(t)Ω
update network parameters

i ← F(G(xi)), for i ∈ N

end for
Z(cid:48)
Z ← αZ + (1 − α)Z(cid:48)
˜z ← one-hot-vectors(Z)

end for

the network predictions to be consistent in differ-
ent training epochs. Ω is jointly trained with L,
J , and Γ. Algorithm 1 illustrates the overall train-
ing process of the proposed domain adaptive semi-
supervised learning (DAS) framework.

t

In Algorithm 1, λ1, λ2, and w(t) are weights
to balance the effects of J , Γ, and Ω respectively.
λ1 and λ2 are constant hyper-parameters. We set
max-epochs )2]λ3 as a Gaus-
w(t) = exp[−5(1 −
sian curve to ramp up the weight from 0 to λ3.
This is to ensure the ramp-up of the bootstrapping
loss component is slow enough in the beginning
of the training. After each training epoch, we com-
pute Z(cid:48)
i which denotes the predictions made by the
network in current epoch, and then the ensemble
prediction Zi is updated as a weighted average of
the outputs from previous epochs and the current
epoch, with recent epochs having larger weight.
For generating estimated labels ˜zi, Zi is converted
to a one-hot vector where the entry with the maxi-
mum value is set to one and other entries are set to
zeros. The self-ensemble bootstrapping is a gener-
alized version of bootstrappings that only use the
outputs from the previous round of training (Jiang
and Zhai, 2007; Wu et al., 2009). The ensemble
prediction is likely to be closer to the correct, un-
known labels of the target data.

Domain
Book

Electronics

Beauty

Music

#Pos
2000
4824
2000
4817
2000
4709
2000
4441

#Neg
2000
513
2000
694
2000
616
2000
785

#Neu Total
6000
2000
6000
663
6000
2000
6000
489
6000
2000
6000
675
6000
2000
6000
774

Set 1
Set 2
Set 1
Set 2
Set 1
Set 2
Set 1
Set 2

(a) Small-scale datasets

Domain
IMDB
Yelp
Cell Phone
Baby

#Pos
55,242
155,625
148,657
126,525

#Neg
11,735
29,597
24,343
17,012

#Neu
17,942
45,941
21,439
17,255

Total
84,919
231,163
194,439
160,792

(b) Large-scale datasets

Table 1: Summary of datasets.

4 Experiments

4.1 CNN Encoder Implementation

We have left the feature encoder G unspeciﬁed,
In
for which, a few options can be considered.
our implementation, we adopt a one-layer CNN
structure from previous works (Kim, 2014; Yu and
Jiang, 2016), as it has been demonstrated to work
well for sentiment classiﬁcation tasks. Given a re-
view document x = (x1, x2, ..., xn) consisting of
n words, we begin by associating each word with
a continuous word embedding (Mikolov et al.,
2013) ex from an embedding matrix E ∈ RV ×d,
where V is the vocabulary size and d is the embed-
ding dimension. E is jointly updated with other
network parameters during training. Given a win-
dow of dense word embeddings ex1, ex2, ..., exl,
the convolution layer ﬁrst concatenates these vec-
tors to form a vector ˆx of length ld and then the
output vector is computed by Equation (7):

Conv(ˆx) = f (W · ˆx + b)

(7)

θg = {W, b} is the parameter set of the en-
coder G and is shared across all windows of the
sequence. f is an element-wise non-linear activa-
tion function. The convolution operation can cap-
ture local contextual dependencies of the input se-
quence and the extracted feature vectors are sim-
ilar to n-grams. After the convolution operation
is applied to the whole sequence, we obtain a list
of hidden vectors H = (h1, h2, ..., hn). A max-
over-time pooling layer is applied to obtain the ﬁ-
nal vector representation ξ of the input document.

4.2 Datasets and Experimental Settings

Existing benchmark datasets such as the Amazon
benchmark (Blitzer et al., 2007) typically remove

reviews with neutral labels in both domains. This
is problematic as the label information of the tar-
get domain is not accessible in an unsupervised
domain adaptation setting. Furthermore, remov-
ing neutral instances may bias the dataset favor-
ably for max-margin-based algorithms like ours,
since the resulting dataset has all uncertain labels
removed, leaving only high conﬁdence examples.
Therefore, we construct new datasets by ourselves.
The results on the original Amazon benchmark is
qualitatively similar, and we present them in Ap-
pendix A for completeness since most of previous
works reported results on it.

Small-scale datasets: Our new dataset was de-
rived from the large-scale Amazon datasets2 re-
leased by McAuley et al. (2015). It contains four
domains3: Book (BK), Electronics (E), Beauty
(BT), and Music (M). Each domain contains two
datasets. Set 1 contains 6000 instances with ex-
actly balanced class labels, and set 2 contains
6000 instances that are randomly sampled from
the large dataset, preserving the original label dis-
tribution, which we believe better reﬂects the label
distribution in real life. The examples in these two
sets do not overlap. Detailed statistics of the gen-
erated datasets are given in Table 1a.

In all our experiments on the small-scale
datasets, we use set 1 of the source domain as the
only source with sentiment label information dur-
ing training, and we evaluate the trained model on
set 1 of the target domain. Since we cannot con-
trol the label distribution of unlabeled data during
training, we consider two different settings:
Setting (1): Only set 1 of the target domain is used
as the unlabeled set. This tells us how the method
performs in a condition when the target domain
has a close-to-balanced label distribution. As we
also evaluate on set 1 of the target domain, this is
also considered as a transductive setting.
Setting (2): Set 2 from both the source and target
domains are used as unlabeled sets. Since set 2 is
directly sampled from millions of reviews, it better
reﬂects real-life sentiment distribution.

Large-scale datasets: We further conduct ex-
periments on four much larger datasets: IMDB4

2http://jmcauley.ucsd.edu/data/amazon/
3The original reviews were rated on a 5-point scale. We
label them with rating < 3, > 3, and = 3 as negative, posi-
tive, and neutral respectively.

4IMDB is rated on a 10-point scale, and we label reviews
with rating < 5, > 6, and = 5/6 as negative, positive, and
neutral respectively.

(I), Yelp2014 (Y), Cell Phone (C), and Baby
(B). IMDB and Yelp2014 were previously used
in (Tang et al., 2015; Yang et al., 2017). Cell
phone and Baby are from the large-scale Amazon
dataset (McAuley et al., 2015; He and McAuley,
2016). Detailed statistics are summarized in Ta-
ble 1b. We keep all reviews in the original datasets
and consider a transductive setting where all target
examples are used for both training (without la-
bel information) and evaluation. We perform sam-
pling to balance the classes of labeled source data
in each minibatch B(s) during training.

4.3 Selection of Development Set

Ideally, the development set should be drawn from
the same distribution as the test set. However, un-
der the unsupervised domain adaptation setting,
we do not have any labeled target data at training
phase which could be used as development set. In
all of our experiments, for each pair of domains,
we instead sample 1000 examples from the train-
ing set of the source domain as development set.
We train the network for a ﬁxed number of epochs,
and the model with the minimum classiﬁcation er-
ror on this development set is saved for evaluation.
This approach works well on most of the problems
since the target domain is supposed to behave like
the source domain if the domain difference is ef-
fectively reduced.

Another problem is how to select the values for
hyper-parameters. If we tune λ1 and λ2 directly
on the development set from the source domain,
most likely both of them will be set to 0, as un-
labeled target data is not helpful for improving in-
domain accuracy of the source domain. Other neu-
ral network models also have the same problem for
hyper-parameter tuning. Therefore, our strategy is
to use the development set from the target domain
to optimize λ1 and λ2 for one problem (e.g., we
only do this on E→BK), and ﬁx their values on the
other problems. This setting assumes that we have
at least two labeled domains such that we can op-
timize the hyper-parameters, and then we ﬁx them
for other new unlabeled domains to transfer to.

4.4 Training Details and Hyper-parameters

We initialize word embeddings using the 300-
dimension GloVe vectors supplied by Pennington
et al., (2014), which were trained on 840 billion
tokens from the Common Crawl. For each pair of
domains, the vocabulary consists of the top 10000
most frequent words. For words in the vocabulary

but not present in the pre-trained embeddings, we
randomly initialize them.

We set hyper-parameters of

the CNN en-
coder following previous works (Kim, 2014; Yu
and Jiang, 2016) without speciﬁc tuning on our
datasets. The window size is set to 3 and the size
of the hidden layer is set to 300. The nonlinear
activation function is Relu. For regularization, we
also follow their settings and employ dropout with
probability set to 0.5 on ξi before feeding it to the
output layer F, and constrain the l2-norm of the
weight vector θf , setting its max norm to 3.

On the small-scale datasets and the Aamzon
benchmark, λ1 and λ2 are set to 200 and 1,
respectively,
tuned on the development set of
task E→BK under setting 1. On the large-scale
datasets, λ1 and λ2 are set to 500 and 0.2, re-
tuned on I→Y. We use a Gaussian
spectively,
curve w(t) = exp[−5(1 − t
)2]λ3 to ramp up
the weight of the bootstrapping loss Ω from 0 to
λ3, where tmax denotes the maximum number of
training epochs. We train 30 epochs for all exper-
iments. We set λ3 to 3 and α to 0.5 for all experi-
ments.

tmax

The batch size is set to 50 on the small-scale
datasets and the Amazon benchmark. We increase
the batch size to 250 on the large-scale datasets to
reduce the number of iterations. RMSProp opti-
mizer with learning rate set to 0.0005 is used for
all experiments.

4.5 Models for Comparison

We compare with the following baselines:

(1) Naive: A non-domain-adaptive baseline
with bag-of-words representations and SVM clas-
siﬁer trained on the source domain.

(2) mSDA (Chen et al., 2012): This is the state-
of-the-art method based on discrete input features.
Top 1000 bag-of-words features are kept as pivot
features. We set the number of stacked layers to 3
and the corruption probability to 0.5.

(3) NaiveNN: This is a non-domain-adaptive
CNN trained on source domain, which is a variant
of our model by setting λ1, λ2, and λ3 to zeros.

(4) AuxNN (Yu and Jiang, 2016): This is a neu-
ral model that exploits auxiliary tasks, which has
achieved state-of-the-art results on cross-domain
sentiment classiﬁcation. The sentence encoder
used in this model is the same as ours.

(5) ADAN (Chen et al., 2017): This method
exploits adversarial training to reduce representa-

(a) Accuracy on the small-scale dataset under setting 1.

(b) Accuracy on the small-scale dataset under setting 2.

(c) Macro-F1 on the large-scale dataset.

Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is
signiﬁcantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test.

tion difference between domains. The original pa-
per uses a simple feedforward network as encoder.
For fair comparison, we replace it with our CNN-
based encoder. We train 5 iterations on the dis-
criminator per iteration on the encoder and senti-
ment classiﬁer as suggested in their paper.

(6) MMD: MMD has been widely used for min-
imizing domain discrepancy on images. In those
works (Tzeng et al., 2014; Long et al., 2017), vari-
ants of deep CNNs are used for encoding images
and the MMDs of multiple layers are jointly mini-
mized. In NLP, adding more layers of CNNs may
not be very helpful and thus those models from
image-related tasks can not be directly applied
to our problem. To compare with MMD-based
method, we train a model that jointly minimize
the classiﬁcation loss L on the source domain and
i=1} and {ξ(t)
MMD between {ξ(s)
|nt
|ns
i=1}. For
computing MMD, we use a Gaussian RBF which
is a common choice for characteristic kernel.

i

i

In addition to the above baselines, we also show
results of different variants of our model. DAS
as shown in Algorithm 1 denotes our full model.
DAS-EM denotes the model with only entropy

minimization for semi-supervised learning (set
λ3 = 0). DAS-SE denotes the model with only
self-ensemble bootstrapping for semi-supervised
learning (set λ2 = 0). FANN (feature-adaptation
neural network) denotes the model without semi-
supervised learning performed (set both λ2 and λ3
to zeros).

4.6 Main Results

Figure 15 shows the comparison of adaptation re-
sults (see Appendix B for the exact numerical
numbers). We report classiﬁcation accuracy on
the small-scale dataset. For the large-scale dataset,
macro-F1 is instead used since the label distribu-
tion in the test set is extremely unbalanced. Key
observations are summarized as follows. (1) Both
DAS-EM and DAS-SE perform better in most
cases compared with ADAN, MDD, and FANN,
in which only feature adaptation is performed.
This demonstrates the effectiveness of the pro-

5We exclude results of Naive, mSDA and AuxNN on the
large-scale dataset. Both Naive and mSDA have difﬁculties
to scale up to the large dataset. AuxNN relies on manually
selecting positive and negative pivots before training.

Figure 2: Accuracy vs. percentage of unlabeled target training examples.

Figure 3: Accuracy vs. number of labeled target training examples.

posed domain adaptive semi-supervised learning
framework. DAS-EM is more effective than DAS-
SE in most cases, and the full model DAS with
both techniques jointly employed overall has the
(2) When comparing the two
best performance.
settings on the small-scale dataset, all domain-
adaptive methods6 generally perform better under
setting 1. In setting 1, the target examples are bal-
anced in classes, which can provide more diverse
opinion-related features. However, when consid-
ering unsupervised domain adaptation, we should
not presume the label distribution of the unlabeled
data. Thus, it is necessary to conduct experiments
using datasets that reﬂect real-life sentiment dis-
tribution as what we did on setting2 and the large-
scale dataset. Unfortunately, this is ignored by
most of previous works. (3) Word-embeddings are
very helpful, as we can see even NaiveNN can sub-
stantially outperform mSDA on most tasks.

To see the effect of semi-supervised learning
alone, we also conduct experiments by setting
λ1 = 0 to eliminate the effect of feature adapta-
tion. Both entropy minimization and bootstrap-
ping perform very badly in this setting. En-
tropy minimization gives almost random predic-
tions with accuracy below 0.4, and the results
of bootstrapping are also much lower compared
to NaiveNN. This suggests that the feature adap-
tation component is essential. Without it,
the
learned target representations are less meaning-
ful and discriminative. Applying semi-supervised

6Results of Naive and NaiveNN do not change under both

settings as they are only trained on the source domain.

learning in this case is likely to worsen the results.

4.7 Further Analysis

In Figure 2, we show the change of accuracy with
respect to the percentage of unlabeled data used
for training on three particular problems under set-
ting 1. The value at x = 0 denotes the accuracies
of NaiveNN which does not utilize any target data.
For DAS, we observe a nonlinear increasing trend
where the accuracy quickly improves at the be-
ginning, and then gradually stabilizes. For other
methods, this trend is less obvious, and adding
more unlabeled data sometimes even worsen the
results. This ﬁnding again suggests that the pro-
posed approach can better exploit the information
from unlabeled data.

We also conduct experiments under a setting
with a small number of labeled target examples
available. Figure 3 shows the change of accuracy
with respect to the number of labeled target exam-
ples added for training. We can observe that DAS
is still more effective under this setting, while the
performance differences to other methods gradu-
ally decrease with the increasing number of la-
beled target examples.

4.8 CNN Filter Analysis

In this subsection, we aim to better understand
DAS by analyzing sentiment-related CNN ﬁlters.
To do that, 1) we ﬁrst select a list of the most re-
lated CNN ﬁlters for predicting each sentiment la-
bel (positive, negative neutral). Those ﬁlters can
be identiﬁed according to the learned weights θf

best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

(a) NaiveNN

(b) FANN

(c) DAS

Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the
5 most positive-sentiment-related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

of the output layer F. Higher weight indicates
stronger relatedness. 2) Recall that in our im-
plementation, each CNN ﬁlter has a window size
of 3 with Relu activation. We can thus represent
each selected ﬁlter as a ranked list of trigrams with
highest activation values.

We analyze the CNN ﬁlters

learned by
NaiveNN, FANN and DAS respectively on task
E→BT under setting 1. We focus on E→BT for
study because electronics and beauty are very dif-
ferent domains and each of them has a diverse
set of domain-speciﬁc sentiment expressions. For
each method, we identify the top 10 most related
ﬁlters for each sentiment label, and extract the top
trigrams of each selected ﬁlter on both source and
target domains. Since labeled source examples are
used for training, we ﬁnd the ﬁlters learned by the
three methods capture similar expressions on the
source domain, containing both domain-invariant
and domain-speciﬁc trigrams. On the target do-
main, DAS captures more target-speciﬁc expres-
sions compared to the other two methods. Due
to space limitation, we only present a small sub-
set of positive-sentiment-related ﬁlters in Table 2.
The complete results are provided in Appendix C.
From Table 2, we can observe that the ﬁlters
learned by NaiveNN are almost unable to cap-
ture target-speciﬁc sentiment expressions, while
FANN is able to capture limited target-speciﬁc
words such as “clean” and “scent”. The ﬁlters
learned by DAS are more domain-adaptive, cap-
turing diverse sentiment expressions in the target
domain.

5 Conclusion

In this work, we propose DAS, a novel frame-
work that jointly performs feature adaptation and
semi-supervised learning. We have demonstrated
through multiple experiments that DAS can better
leverage unlabeled data, and achieve substantial
improvements over baseline methods. We have
also shown that feature adaptation is an essen-
tial component, without which, semi-supervised
learning is not able to function properly. The pro-
posed framework could be potentially adapted to
other domain adaptation tasks, which is the focus
of our future studies.

References

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: domain adaptation for sentiment classiﬁ-
In Annual Meeting of the Association for
cation.
Computational Linguistics.

Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In The 29th Interna-
tional Conference on Machine Learning.

Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie,
and Kilian Weinberger. 2017. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁer. In Arxiv e-prints arXiv:1606.01614.

Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classiﬁers for
text classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: a deep learning approach. In The 28th
International Conference on Machine Learning.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
In
supervised learning by entropy minimization.
Neural Information Processing Systems.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander Smola. 2012. A
kernel two-sample test. Journal of Machine Learn-
ing Research, 13:723–773.

Ruining He and Julian McAuley. 2016. Ups and
downs: modeling the visual evolution of fashion
In
trends with one-class collaborative ﬁltering.
WWW.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classiﬁcation. In ACL.

Jing Jiang and ChengXiang Zhai. 2007.

Instance
weighting for domain adaptation in NLP. In Annual
Meeting of the Association for Computational Lin-
guistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Conference on Empirical
Methods in Natural Language Processing.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In International
Conference on Learning Representation.

Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial mem-
ory network for cross-domain sentiment classiﬁca-
tion. In The 26th International Joint Conference on
Artiﬁcial Intelligence.

Mingsheng Long, Yue Cao,

Jianmin Wang, and
Michael I. Jordan. 2015. Learning transferable fea-
In Interna-
tures with deep adaptation networks.
tional Conference on Machine Learning.

Mingsheng Long, Han Zhu,

Jianmin Wang, and
Michael I. Jordan. 2017. Deep transfer learning with
joint adaptation networks. In International Confer-
ence on Machine Learning.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
ommendations on styles and substitutes. In The 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Neural Information Processing Systems.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classiﬁcation via spectral feature alignment. In
The 19th International World Wide Web Conference.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representation of users and products for
document level sentiment classiﬁcation. In Annual
Meeting of the Association for Computational Lin-
guistics.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
maximizing for domain invariance. In Arxiv e-prints
arXiv:1412.3474.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Conference on Empirical Meth-
ods in Natural Language Processing.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Annual
Meeting of the Association for Computational Lin-
guistics.

Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In Conference on
Empirical Methods in Natural Language Process-
ing.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Annual Meeting of the Association
for Computational Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
In Conference on Empirical
timent classiﬁcation.
Methods in Natural Language Processing.

Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-
aohua Tony Hu. 2015. Linking heterogeneous input
features with pivots for domain adaptation. In The
24th International Joint Conference on Artiﬁcial In-
telligence.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016. Bi-transferring deep neural
networks for domain adaptation. In Annual Meeting
of the Association for Computational Linguistics.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: transfer learning with deep autoencoders.
In The 24th International Joint Conference on Arti-
ﬁcial Intelligence.

S

D
E
K
B
E
K
B
D
K
B
D
E

T

B
B
B
D
D
D
E
E
E
K
K
K

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE
82.10∗
77.75
79.60
82.35
79.75
82.15∗
75.80
80.05
85.95
79.45∗
79.50
83.80

82.00∗
80.25∗
79.95
82.65
81.40∗
81.65∗
80.25∗
81.40∗
85.70
81.55∗
80.80
84.50

80.30
77.25
79.20
81.65
79.55
76.90
76.75
79.25
85.60
77.55
78.00
83.85

81.05
78.65
79.70
82.00
80.10
79.35
76.45
80.20
85.75
75.20
79.70
81.75

81.70
78.55
79.25
82.30
79.70
80.45
77.60
79.70
86.85
76.10
77.35
83.95

80.80
78.00
77.85
81.75
80.65
78.90
76.40
77.55
84.05
78.10
80.05
84.15

78.50
76.15
75.65
80.60
76.30
76.05
75.55
76.00
84.20
75.95
76.30
84.45

75.20
68.85
70.00
77.15
69.50
71.40
72.15
71.65
79.75
73.50
72.00
82.80

81.10
77.95
77.75
80.80
77.00
79.35
76.20
76.60
84.85
77.40
78.55
84.95

Average

73.66

77.98

79.38

79.85

80.29

80.00

79.65

81.84

80.68

DAS
82.05∗
80.00∗
80.05∗
82.75∗
80.15
81.40∗
81.15∗
81.55∗
85.80
82.25∗
81.50∗
84.85

81.96

Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations
are reported for each neural method. ∗ indicates that the proposed method (DAS-EM, DAS-SE, DAS) is
signiﬁcantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

A Results on Amazon Benchmark

C CNN Filter Analysis Full Results

As mentioned in Section 4.8, we conduct CNN ﬁl-
ter analysis on NaiveNN, FANN, and DAS. For
each method, we identify the top 10 most related
ﬁlters for positive, negative, neutral sentiment la-
bels respectively, and then represent each selected
ﬁlter as a ranked list of trigrams with the highest
activation values on it. Table 5, 6, 7 in the fol-
lowing pages illustrate the trigrams from the tar-
get domain (beauty) captured by the selected ﬁl-
ters learned on E→BT for each method.

We can observe that compared to NaiveNN and
FANN, DAS is able to capture a more diverse set
of relevant sentiment expressions on the target do-
main for each sentiment label. This observation is
consistent with our motivation. Since NaiveNN,
FANN and other baseline methods solely train
the sentiment classiﬁer on the source domain, the
learned encoder is not able to produce discrimina-
tive features on the target domain. DAS addresses
this problem by reﬁning the classiﬁer on the tar-
get domain with semi-supervised learning, and
the overall objective forces the encoder to learn
feature representations that are not only domain-
invariant but also discriminative on both domains.

Most previous works (Blitzer et al., 2007; Pan
et al., 2010; Glorot et al., 2011; Chen et al.,
2012; Zhou et al., 2016) carried out experiments
on the Amazon benchmark released by Blitzer
et al.
(2007). The dataset contains 4 different
domains: Book (B), DVDs (D), Electronics (E),
and Kitchen (K). Following their experimental set-
tings, we consider the binary classiﬁcation task to
predict whether a review is positive or negative
on the target domain. Each domain consists of
1000 positive and 1000 negative reviews respec-
tively. We also allow 4000 unlabeled reviews to
be used for both the source and the target domains,
of which the positive and negative reviews are bal-
anced as well, following the settings in previous
works. We construct 12 cross-domain sentiment
classiﬁcation tasks and split the labeled data in
each domain into a training set of 1600 reviews
and a test set of 400 reviews. The classiﬁer is
trained on the training set of the source domain
and is evaluated on the test set of the target do-
main. The comparison results are shown in Ta-
ble 3.

B Numerical Results of Figure 1

Due to space limitation, we only show results in
ﬁgures in the paper. All numerical numbers used
for plotting Figure 1 are presented in Table 4. We
can observe that DAS-EM, DAS-SE, and DAS
all achieve substantial improvements over baseline
methods under different settings.

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

BK 49.07
E
BK 48.17
BT
M BK 45.20
46.43
E
BK
53.63
E
BT
37.93
E
M
45.57
BK BT
48.43
E
BT
39.42
M BT
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

BK 49.07
E
BK 48.17
BT
M BK 45.20
E
46.43
BK
E
BT
53.63
37.93
M
E
BK BT
45.57
48.43
E
BT
M BT
39.43
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

55.13
53.53
49.22
48.22
57.32
38.13
50.77
53.13
39.37
47.88
47.88
49.62
49.18

52.88
47.65
48.33
47.07
55.12
37.40
49.63
51.98
37.73
45.97
45.12
45.78
47.06

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

60.62
59.86
60.43
48.45
60.98
49.60
48.67
55.58
48.65
48.87
51.19
53.11
53.84

57.72
58.46
58.15
48.22
59.08
49.43
47.80
54.37
46.92
48.79
52.31
53.55
52.98

63.32
65.62
62.87
47.42
63.13
46.57
46.14
50.98
44.26
51.10
50.23
55.35
53.92

57.07
59.78
58.67
49.48
59.45
47.00
47.52
51.28
45.73
50.20
52.57
54.68
52.79

60.38
59.66
60.20
53.32
60.53
51.55
49.48
54.83
48.35
53.04
51.81
54.43
54.80

57.43
56.17
57.08
45.42
60.24
48.72
45.43
54.92
46.68
48.76
51.50
54.55
52.23

59.59
59.28
57.65
51.27
60.62
47.23
50.24
56.78
48.89
52.35
52.14
53.84
54.15

56.43
57.98
57.75
51.95
58.67
48.92
49.83
55.42
48.48
49.47
48.18
53.41
53.04

66.48∗
66.78∗
69.63∗
58.59∗
65.71∗
55.88∗
49.49
61.53∗
47.65
55.47∗
58.28∗
60.95∗
59.74

57.78
61.17∗
58.62
54.51∗
61.27
51.28∗
53.72∗
53.10
47.18
52.37∗
53.63∗
56.24∗
55.07

(a) Accuracy on the small-scale dataset under setting 1

DAS
67.12∗
66.53
70.31∗
58.73∗
66.14∗
55.78∗
51.30∗
60.76∗
50.66∗
55.98∗
59.06∗
60.5∗
60.24

55.20
63.32∗
60.77∗
53.92∗
59.83
52.88∗
54.67∗
56.05∗
49.73∗
53.52∗
55.38∗
56.02∗
55.94

62.37
61.17
65.24∗
55.15∗
61.78
53.22∗
54.23∗
59.52∗
50.67∗
55.13∗
55.60∗
56.90∗
57.58

58.93
60.17∗
58.25
52.47∗
61.42
51.18∗
51.23∗
56.43∗
51.57∗
52.68∗
52.25
56.23∗
55.23

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

DAS

(b) Accuracy on the small-scale dataset under setting 2

S

T

Y
I
C
I
B
I
I
Y
C
Y
B
Y
I
C
Y
C
B
C
I
B
Y
B
B
C
Average

NaiveNN ADAN MMD FANN DAS-EM DAS-SE
56.66∗
55.18
54.30
58.72∗
59.14
58.43
51.97∗
54.67∗
59.98
50.81
52.95
58.12
55.91

55.04
57.27∗
57.31∗
57.92∗
61.17
59.94∗
53.46∗
53.48
59.84
48.84
52.87
57.74
56.24

54.16
53.35
51.40
56.52
60.81
58.77
50.49
53.12
61.23
47.35
54.43
60.52
55.18

54.46
53.07
52.39
56.30
56.02
55.72
51.04
51.86
60.19
48.17
53.54
55.56
54.02

55.52
55.07
54.64
52.57
60.70
58.42
47.27
52.53
59.91
46.34
50.82
59.99
54.48

53.01
51.84
45.85
55.46
61.22
56.86
50.38
53.87
59.48
50.05
54.73
60.47
54.43

DAS
58.54∗
57.28∗
58.02∗
58.92∗
61.39
61.87∗
53.38∗
55.44∗
59.76
48.84
52.91
59.75
57.18

(c) Macro-F1 scores on the large-scale dataset

Table 4: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signiﬁcantly
better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

1
best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

2
highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

3
nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

4
beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

5
since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

6
store-and-am
cleanser-and-am
olay-and-am
daily-and-need
shower-and-noticed

7
ofﬁce-setting-thanks
locks-shimmering-color
dirty-blonde-color
victoria-secrets-gorgeous
dirty-pinkish-color

8
car-washes-!
price-in-stores
products-are-priced
car-and-burning
from-our-store

(a) NaiveNN

9
speed-is-perfect
buttons-are-perfect
unit-is-superb
spray-is-perfect
coverage-is-excellent

10
!-i-recommend
!-i-highly
shower-i-slather
spots-i-needed
best-i-use

1
prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

2
so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

3
purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

4
feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

5
are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

6
shower-or-cleaning
water-onto-my
bleach-your-towels
pump-onto-my
water-great-for

8

7
deﬁnitely-purchase-again more-affordable-price
deﬁnitely-buy-again
perfect-for-my
deﬁnitely-order-again
super-happy-to

a-perfect-length
an-exceptional-value
’ve-enjoyed-it
pretty-decent-layer

9
absolutely-wonderful-!
perfect-for-running
concealer-for-my
moisturizing-for-my
super-glue-even

10
felt-cleaner-than
ﬂat-iron-through
rubbed-grease-on
deeply-cleans-my
being-cleaner-after

1
bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

2
love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

3
feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

4
cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

5
excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

6
shower-lather-wonderfully
affordable-cleans-nicely
peels-great-price
daughter-loves-this
cleans-great-smells

7
highly-recommend-!
deﬁnitely-recommend-!
love-love-!
highly-advise-!
time-advise-!

8
excellent-fragrance-and
fantastic-for-daytime
wonderfully-moisturizing-and
lathers-great-cleans
delightful-shampoo-works

10
9
its-unique-smoothing
forgeous-gragrance-mist
smooth-luxurious-texture wonderful-bedtime-scent
’s-extremely-gentle
’s-affordable-combination
absorbs-quite-well

love-essie-polish
perfect-beautiful-shimmer
fantastic-coverage-hydrates

(b) FANN

(c) DAS

Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
pads-ruined-my
highly-disappointed-and
dryers-blew-my
completely-worthless-didn’t
am-disappointed-and

2
simply-threw-out
reviewer-pointed-out
extracts-broke-into
actually-threw-out
clips-barely-keep

3
hours-after-trying
minutes-after-rinsing
disappointed-after-trying
lips-after-trying
dry-after-shampooing

4
junk-drawer-∗
refund-time-!
total-fake-wen
waste-your-time
total-fail-!

5
contacted-manufacturer-about
minutes-not-worth
’ve-owned-this
hour-unless-it
results-they-claim

6
were-awful-garbage
what-awful-garbage
and-utter-waste
are-absolute-garbage
piece-of-junk

7
two-failed-attempts
a-mistake-save
a-deﬁnite-return
a-pathetic-limp
a-total-disappointment

8
auto-ship-sent
am-returning-to
am-unable-to
am-pale-ghost
got-returned-and

10
broke-don’t-ﬁx
sent-me-expired

9
refund-and-dispose
refund-spend-your
wouldn’t-recommend-! wearing-false-eyelashes
not-buy-dunhill
not-worth-returning

a-temporary-ﬁx
a-disappointment-cheap

(a) NaiveNN

2
the-worse-mascaras

1
nasty-sunburn-lol
bother-returning-them it-caused-patchy
fails-miserably-at
minutes-auric-needs
severely-burned-me

lifeless-disaster-enter
it-fails-miserably
feel-worse-leaving

3
stale-very-unhappy
were-horrible-failures
send-this-crap
were-awful-garbage
were-horribly-red

4
actually-hurts-your
didn’t-bother-returning
it-hurts-your
didn’t-exist-in
skin-horribly-after

5
a-return-label
stay-away-completely
like-bug-quit
a-defective-brown
’d-refund-the

6
worse-with-exercise
worse-and-after
unable-to-return
worse-my-face
poorly-in-step

7
not-stink-your
mistake-save-your
nothing-!-by
nothing-happened-!
nothing-save-your

8
it-fails-miserably
is-ineffective-apart
but-horribly-unhealthy
a-pathetic-limp
a-worse-job

9
got-progressively-worse
gave-opposite-result
another-epic-fail
got-horribly-painful
was-downright-painful

10
stopped-working-for
uncomfortable-i-returned
i-am-returning
stopped-working-shortly
not-waterproof-makeup

1
poorly-designed-product
defective-dryer-promising
disgusting-smelling-thing
hurts-your-scalp
hurts-your-hair

2
a-refund-spend
a-refund-save
i-regret-spending
just-wouldn’t-spend
looked-washed-out

3
completely-waste-of
of-junk-*
were-awful-garbage
worthless-waste-of
throwing-money-away

4
smells-disgusting-!
smells-horribly-like
does-not-straighten
’s-false-advertising
a-disgusting-cheap

5
burning-rubber-stench
began-smelling-vomit
reaction-and-wasted
control-and-smelled
using-this-disgusting

6
super-irritating-!
strong-reaction-and
really-burned-and
very-pasty-and
super-streaky-and

7
got-promptly-broke
after-ive-washed
after-several-attempts
this-stuff-stinks
again-i-threw

8
sore-and-painful
is-simply-irritating
tight-and-uncomfortable
drying-and-irritating
goopy-and-unpleasant

9
it-caused-patchy
layer-hydrogenated-alcohols
the-harmful-uva
my-severe-dark
a-allergic-reaction

10
painful-it-hurt
unnecessary-health-risks
uncomfortable-to-wear
stinging-your-eyes
unbearable-to-wear

(b) FANN

(c) DAS

Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
purpose-cologne-splash
other-hanae-mori
the-mavala-peeled
avoid-hair-pulling
cause-rashes-stinging

2
okay-cord-was
cocamide-dea-is
coily-conditioner-is
ﬂaky-dandruff-is
quickly-cord-is

3
hands-feet-elbows
been-sealed-tight
stainless-steel-blackhead
severely-tight-chest
thick-nasty-callouses

4
aggressive-in-general
pimples-in-general
biotin-in-general
dimethicone-is-terrible
but-in-general

5
but-its-okay
it-moisturizes-okay
but-moisturizes-keeps
but-don’t-expect
it-lathers-ok

6
pretty-damaged-from
daughter-suffers-from
teenager-suffers-from
tissue-damage-during
the-damage-on

7
darker-olive-complexion
stronger-healthier-or
natural-ingredient-however
vitamin-enriched-color
natural-ingredients-∗

8
doesn’t-mind-pushing
kinda-doesn’t-its
kinda-kinky-coily
okay-job-of
intended-purpose-that

9
producto-por-los
unstuck-frownies-∗
they-push-∗
uva-rays-uva
tend-to-slip

10
feeling-didn’t-last
curls-didn’t-last
extra-uv-protection
garnier-fructis-curl
the-mavala-after

(a) NaiveNN

1
worse-and-after
worse-before-improving
unable-to-return
unless-your-entire
horrible-in-execution

2
maybe-a-refund
ok-mask-i
ok-pining-it
ok-try-i
ok-tho-i

3
very-disappointing-waste
ok-but-clean
ok-but-will
ok-but-didn’t
ok-nothing-special

4
my-ears-are
my-neck-line
cause-unsightly-beads
my-sporadic-line
your-ear-is

5
pretty-neutral-neither
ok-so-if
ok-during-pregnancy
kinda-annoying-if
ok-this-seems

6
uncomfortable-i-returned
weak-they-bend
claimed-faulty-∗
suffers-from-wind
as-defective-∗

7
sticky-lathers-and
quickly-deep-cleans
but-elegant-bottle
beat-the-price
and-reasonably-priced

8
some-fading-when
real-disappointment-the
especially-noticeable-after
progressively-worse-during
style-unfortunately-the

9
are-very-painful
are-less-painful
are-a-pain
about-sum-damage
offered-no-pain

10
its-also-convenient
that-also-my
that-may-make
that-allows-your
its-helpful-to

1
’m-kinda-pale
a-terrible-headache
but-kinda-annoying
’m-kinda-mad
i-kinda-stopped

2
darker-but-nope
gray-didn’t-cover
makeup-doesn’t-sweat
dark-spots-around
moist-but-thats

3
ok-but-horrible
ok-but-didn’t
okay-but-doesn’t
okay-however-it
unfortunately-straight

4
noticeable-i-avoid
however-i-lean
but-otherwise-ok
but-im-deciding
however-i-prefer

5
same-result-mediocre
it-caused-patchy
doesn’t-cause-ﬂare
the-harmful-uva
rather-unpleasant-smell

6
kinda-annoying-if
pretty-bad-breakage
my-slight-discoloration
smells-kinda-bad
look-kinda-crappy

7
brutal-winter-however
summer-color-however
beige-shade-however
is-okay-however
bit-greasy-however

8
higher-rating-because
slight-burnt-rubber
noticeable-tan-since
somewhat-pale-affect
kinda-pale-so

9
nothing-for-odor
kinda-recommend-this
not-recommend-if
noticeable-but-non
nothing-special-moderate

10
but-darker-*
slightly-darker-shade
somewhat-pale-affect
but-somewhat-heavy
bit-dull-heavy

(b) FANN

(c) DAS

Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

Adaptive Semi-supervised Learning for Cross-domain
Sentiment Classiﬁcation

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore
‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg
‡d.dahlmeier@sap.com

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
3
5
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We consider the cross-domain sentiment clas-
siﬁcation problem, where a sentiment classi-
ﬁer is to be learned from a source domain and
to be generalized to a target domain. Our ap-
proach explicitly minimizes the distance be-
tween the source and the target instances in
an embedded feature space. With the differ-
ence between source and target minimized,
we then exploit additional information from
the target domain by consolidating the idea
of semi-supervised learning, for which, we
jointly employ two regularizations – entropy
minimization and self-ensemble bootstrapping
– to incorporate the unlabeled target data for
classiﬁer reﬁnement. Our experimental results
demonstrate that the proposed approach can
better leverage unlabeled data from the target
domain and achieve substantial improvements
over baseline methods in various experimental
settings.

1

Introduction

In practice, it is often difﬁcult and costly to anno-
tate sufﬁcient training data for diverse application
domains on-the-ﬂy. We may have sufﬁcient la-
beled data in an existing domain (called the source
domain), but very few or no labeled data in a
new domain (called the target domain). This issue
has motivated research on cross-domain sentiment
classiﬁcation, where knowledge in the source do-
main is transferred to the target domain in order to
alleviate the required labeling effort.

One key challenge of domain adaptation is that
data in the source and target domains are drawn
from different distributions. Thus, adaptation per-
formance will decline with an increase in distribu-
tion difference. Speciﬁcally, in sentiment analy-
sis, reviews of different products have different vo-
cabulary. For instance, restaurants reviews would
contain opinion words such as “tender”, “tasty”, or

“undercooked” and movie reviews would contain
“thrilling”, “horriﬁc”, or “hilarious”. The intersec-
tion between these two sets of opinion words could
be small which makes domain adaptation difﬁcult.
Several techniques have been proposed for ad-
dressing the problem of domain shifting. The
aim is to bridge the source and target domains
by learning domain-invariant feature representa-
tions so that a classiﬁer trained on a source do-
main can be adapted to another target domain.
In cross-domain sentiment classiﬁcation, many
works (Blitzer et al., 2007; Pan et al., 2010; Zhou
et al., 2015; Wu and Huang, 2016; Yu and Jiang,
2016) utilize a key intuition that domain-speciﬁc
features could be aligned with the help of domain-
invariant features (pivot features). For instance,
“hilarious” and “tasty” could be aligned as both
of them are relevant to “good”.

Despite their promising results,

these works
share two major limitations. First, they highly de-
pend on the heuristic selection of pivot features,
which may be sensitive to different applications.
Thus the learned new representations may not ef-
fectively reduce the domain difference. Further-
more, these works only utilize the unlabeled tar-
get data for representation learning while the sen-
timent classiﬁer was solely trained on the source
domain. There have not been many studies on ex-
ploiting unlabeled target data for reﬁning the clas-
siﬁer, even though it may contain beneﬁcial infor-
mation. How to effectively leverage unlabeled tar-
get data still remains an important challenge for
domain adaptation.

In this work, we argue that the information
from unlabeled target data is beneﬁcial for do-
main adaptation and we propose a novel Domain
Adaptive Semi-supervised learning framework
(DAS) to better exploit it. Our main intuition is
to treat the problem as a semi-supervised learn-
ing task by considering target instances as unla-

beled data, assuming the domain distance can be
effectively reduced through domain-invariant rep-
resentation learning. Speciﬁcally, the proposed
approach jointly performs feature adaptation and
semi-supervised learning in a multi-task learning
setting. For feature adaptation, it explicitly mini-
mizes the distance between the encoded represen-
tations of the two domains. On this basis, two
semi-supervised regularizations – entropy mini-
mization and self-ensemble bootstrapping – are
jointly employed to exploit unlabeled target data
for classiﬁer reﬁnement.

We evaluate our method rigorously under multi-
ple experimental settings by taking label distribu-
tion and corpus size into consideration. The re-
sults show that our model is able to obtain sig-
niﬁcant improvements over strong baselines. We
also demonstrate through a series of analysis that
the proposed method beneﬁts greatly from incor-
porating unlabeled target data via semi-supervised
learning, which is consistent with our motivation.
Our datasets and source code can be obtained from
https://github.com/ruidan/DAS.

2 Related Work

Domain Adaptation: The majority of feature
adaptation methods for sentiment analysis rely on
a key intuition that even though certain opinion
words are completely distinct for each domain,
they can be aligned if they have high correlation
with some domain-invariant opinion words (pivot
words) such as “excellent” or “terrible”. Blitzer
et al. (2007) proposed a method based on struc-
tural correspondence learning (SCL), which uses
pivot feature prediction to induce a projected fea-
ture space that works well for both the source and
the target domains. The pivot words are selected in
a way to cover common domain-invariant opinion
words. Subsequent research aims to better align
the domain-speciﬁc words (Pan et al., 2010; He
et al., 2011; Wu and Huang, 2016) such that the
domain discrepancy could be reduced. More re-
cently, Yu and Jiang (2016) borrow the idea of
pivot feature prediction from SCL and extend it
to a neural network-based solution with auxiliary
tasks.
In their experiment, substantial improve-
ment over SCL has been observed due to the use
of real-valued word embeddings. Unsupervised
representation learning with deep neural networks
(DNN) such as denoising autoencoders has also
been explored for feature adaptation (Glorot et al.,

2011; Chen et al., 2012; Yang and Eisenstein,
2014). It has been shown that DNNs could learn
transferable representations that disentangle the
underlying factors of variation behind data sam-
ples.

Although the aforementioned methods aim to
reduce the domain discrepancy, they do not explic-
itly minimize the distance between distributions,
and some of them highly rely on the selection of
pivot features. In our method, we formally con-
struct an objective for this purpose. Similar ideas
have been explored in many computer vision prob-
lems, where the representations of the underlying
domains are encouraged to be similar through ex-
plicit objectives (Tzeng et al., 2014; Ganin and
Lempitsky, 2015; Long et al., 2015; Zhuang et al.,
2015; Long et al., 2017) such as maximum mean
discrepancy (MMD) (Gretton et al., 2012). In NLP
tasks, Li et al. (2017) and Chen et al. (2017) both
proposed using adversarial training framework for
reducing domain difference. In their model, a sub-
network is added as a domain discriminator while
deep features are learned to confuse the discrim-
inator. The feature adaptation component in our
model shares similar intuition with MMD and ad-
versary training. We will show a detailed compar-
ison with them in our experiments.
Semi-supervised Learning: We attempt to treat
domain adaptation as a semi-supervised learning
task by considering the target instances as unla-
beled data. Some efforts have been initiated on
transfer learning from unlabeled data (Dai et al.,
2007; Jiang and Zhai, 2007; Wu et al., 2009).
In our model, we reduce the domain discrep-
ancy by feature adaptation, and thereafter adopt
semi-supervised learning techniques to learn from
unlabeled data. Primarily motivated by (Grand-
valet and Bengio, 2004) and (Laine and Aila,
2017), we employed entropy minimization and
self-ensemble bootstrapping as regularizations to
incorporate unlabeled data. Our experimental re-
sults show that both methods are effective when
jointly trained with the feature adaptation objec-
tive, which conﬁrms to our motivation.

3 Model Description

3.1 Notations and Model Overview

We conduct most of our experiments under an un-
supervised domain adaptation setting, where we
have no labeled data from the target domain. Con-
sider two sets Ds and Dt. Ds = {x(s)
i=1 is
i

i }|ns

, y(s)

i }|nt

from the source domain with ns labeled examples,
where yi ∈ RC is a one-hot vector representation
of sentiment label and C denotes the number of
classes. Dt = {x(t)
i=1 is from the target domain
with nt unlabeled examples. N = ns + nt denotes
the total number of training documents including
both labeled and unlabeled1. We aim to learn a
sentiment classiﬁer from Ds and Dt such that the
classiﬁer would work well on the target domain.
We also present some results under a setting where
we assume that a small number of labeled target
examples are available (see Figure 3).

For the proposed model, we denote G parame-
terized by θg as a neural-based feature encoder that
maps documents from both domains to a shared
feature space, and F parameterized by θf as a
fully connected layer with softmax activation serv-
ing as the sentiment classiﬁer. We aim to learn fea-
ture representations that are domain-invariant and
at the same time discriminative on both domains,
thus we simultaneously consider three factors in
our objective: (1) minimize the classiﬁcation error
on the labeled source examples; (2) minimize the
domain discrepancy; and (3) leverage unlabeled
data via semi-supervised learning.

Suppose we already have the encoded features
of documents {ξ(s,t)
i=1 (see
Section 4.1), the objective function for purpose (1)
is thus the cross entropy loss on the labeled source
examples

= G(x(s,t)

; θg)}|N

i

i

L = −

1
ns

ns(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(s)
y(s)

i (j)

(1)

i

i = F(ξ(s)

where ˜y(s)
; θf ) denotes the predicted la-
bel distribution. In the following subsections, we
will explain how to perform feature adaptation and
domain adaptive semi-supervised learning in de-
tails for purpose (2) and (3) respectively.

3.2 Feature Adaptation

Unlike prior works (Blitzer et al., 2007; Yu and
Jiang, 2016), our method does not attempt to align
domain-speciﬁc words through pivot words.
In
our preliminary experiments, we found that word
embeddings pre-trained on a large corpus are able
to adequately capture this information. As we will

1Note that unlabeled source examples can also be in-
cluded for training. In that case, N = ns + nt + ns(cid:48) where
ns(cid:48) denotes the number of unlabeled source examples. This
corresponds to our experimental setting 2. For simplicity, we
only consider ns and nt in our description.

later show in our experiments, even without adap-
tation, a naive neural network classiﬁer with pre-
trained word embeddings can already achieve rea-
sonably good results.

i }nt

i }|ns

i=1 and {ξ(t)

We attempt to explicitly minimize the distance
between the source and target feature represen-
tations ({ξ(s)
i=1). A few meth-
ods from literature can be applied such as Maxi-
mum Mean Discrepancy (MMD) (Gretton et al.,
2012) or adversary training (Li et al., 2017; Chen
et al., 2017). The main idea of MMD is to esti-
mate the distance between two distributions as the
distance between sample means of the projected
embeddings in Hilbert space. MMD is implicitly
computed through a characteristic kernel, which is
used to ensure that the sample mean is injective,
leading to the MMD being zero if and only if the
distributions are identical. In our implementation,
we skip the mapping procedure induced by a char-
acteristic kernel for simplifying the computation
and learning. We simply estimate the distribution
distance as the distance between the sample means
in the current embedding space. Although this ap-
proximation cannot preserve all statistical features
of the underlying distributions, we ﬁnd it performs
comparably to MMD on our problem. The follow-
ing equations formally describe the feature adap-
tation loss J :

J = KL(gs||gt) + KL(gt||gs)
g(cid:48)
s
(cid:107)g(cid:48)
s(cid:107)1

g(cid:48)
s =

1
ns

ξ(s)
i

gs =

ns(cid:88)

,

i=1

nt(cid:88)

i=1

1
nt

g(cid:48)
t =

ξ(t)
i

,

gt =

g(cid:48)
t
(cid:107)g(cid:48)
t(cid:107)1

(2)

(3)

(4)

s and g(cid:48)

L1 normalization is applied on the mean represen-
tations g(cid:48)
t, rescaling the vectors such that
all entries sum to 1. We adopt a symmetric ver-
sion of KL divergence (Zhuang et al., 2015) as the
distance function. Given two distribution vectors
P, Q ∈ Rk, KL(P||Q) = (cid:80)k

i=1 P(i) log( P(i)

Q(i) ).

3.3 Domain Adaptive Semi-supervised

Learning (DAS)

We attempt to exploit the information in target
data through semi-supervised learning objectives,
which are jointly trained with L and J . Normally,
to incorporate target data, we can minimize the
cross entropy loss between the true label distri-
butions y(t)
and the predicted label distributions
i

i = F(ξ(t)
˜y(t)
; θf ) over target samples. The chal-
i
lenge here is that y(t)
is unknown, and thus we
i
attempt to estimate it via semi-supervised learn-
ing. We use entropy minimization and bootstrap-
ping for this purpose. We will later show in our
experiments that both methods are effective, and
jointly employing them overall yields the best re-
sults.
Entropy Minimization: In this method, y(t)
is
i
estimated as the predicted label distribution ˜y(t)
,
i
which is a function of θg and θf . The loss can thus
be written as

Γ = −

1
nt

nt(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(t)
˜y(t)

i (j)

(5)

Assume the domain discrepancy can be effectively
reduced through feature adaptation, by minimiz-
ing the entropy penalty, training of the classiﬁer
is inﬂuenced by the unlabeled target data and will
generally maximize the margins between the tar-
get examples and the decision boundaries, increas-
ing the prediction conﬁdence on the target domain.

training.

Self-ensemble Bootstrapping: Another way to
estimate y(t)
corresponds to bootstrapping. The
i
idea is to estimate the unknown labels as the
predictions of the model learned from the pre-
vious round of
Bootstrapping has
been explored for domain adaptation in previous
works (Jiang and Zhai, 2007; Wu et al., 2009).
However, in their methods, domain discrepancy
was not explicitly minimized via feature adap-
tation. Applying bootstrapping or other semi-
supervised learning techniques in this case may
worsen the results as the classiﬁer can perform
quite bad on the target data.

Inspired by the ensembling method proposed
in (Laine and Aila, 2017), we estimate y(t)
by
i
forming ensemble predictions of labels during
training, using the outputs on different training
epochs. The loss is formulated as follows:

Ω = −

1
N

N
(cid:88)

C
(cid:88)

i=1

j=1

˜z(s,t)
i

(j) log ˜y(s,t)

(j)

i

(6)

where ˜z denotes the estimated labels computed on
the ensemble predictions from different epochs.
It serves
The loss is applied on all documents.
for bootstrapping on the unlabeled target data, and
it also serves as a regularization that encourages

Algorithm 1 Pseudocode for training DAS
Require: Ds, Dt, G, F
Require: α = ensembling momentum, 0 ≤ α < 1
Require: w(t) = weight ramp-up function

Z ← 0[N ×C]
˜z ← 0[N ×C]
for t ∈ [1, max-epochs] do

for each minibatch B(s), B(t), B(u) in

Ds, Dt, {x(s,t)

}|N

i=1 do

i
compute loss L on [xi∈B(s), yi∈B(s)]
compute loss J on [xi∈B(s), xj∈B(t)]
compute loss Γ on xi∈B(t)
compute loss Ω on [xi∈B(u), ˜zi∈B(u)]
overall-loss ← L + λ1J + λ2Γ + w(t)Ω
update network parameters

i ← F(G(xi)), for i ∈ N

end for
Z(cid:48)
Z ← αZ + (1 − α)Z(cid:48)
˜z ← one-hot-vectors(Z)

end for

the network predictions to be consistent in differ-
ent training epochs. Ω is jointly trained with L,
J , and Γ. Algorithm 1 illustrates the overall train-
ing process of the proposed domain adaptive semi-
supervised learning (DAS) framework.

t

In Algorithm 1, λ1, λ2, and w(t) are weights
to balance the effects of J , Γ, and Ω respectively.
λ1 and λ2 are constant hyper-parameters. We set
max-epochs )2]λ3 as a Gaus-
w(t) = exp[−5(1 −
sian curve to ramp up the weight from 0 to λ3.
This is to ensure the ramp-up of the bootstrapping
loss component is slow enough in the beginning
of the training. After each training epoch, we com-
pute Z(cid:48)
i which denotes the predictions made by the
network in current epoch, and then the ensemble
prediction Zi is updated as a weighted average of
the outputs from previous epochs and the current
epoch, with recent epochs having larger weight.
For generating estimated labels ˜zi, Zi is converted
to a one-hot vector where the entry with the maxi-
mum value is set to one and other entries are set to
zeros. The self-ensemble bootstrapping is a gener-
alized version of bootstrappings that only use the
outputs from the previous round of training (Jiang
and Zhai, 2007; Wu et al., 2009). The ensemble
prediction is likely to be closer to the correct, un-
known labels of the target data.

Domain
Book

Electronics

Beauty

Music

#Pos
2000
4824
2000
4817
2000
4709
2000
4441

#Neg
2000
513
2000
694
2000
616
2000
785

#Neu Total
6000
2000
6000
663
6000
2000
6000
489
6000
2000
6000
675
6000
2000
6000
774

Set 1
Set 2
Set 1
Set 2
Set 1
Set 2
Set 1
Set 2

(a) Small-scale datasets

Domain
IMDB
Yelp
Cell Phone
Baby

#Pos
55,242
155,625
148,657
126,525

#Neg
11,735
29,597
24,343
17,012

#Neu
17,942
45,941
21,439
17,255

Total
84,919
231,163
194,439
160,792

(b) Large-scale datasets

Table 1: Summary of datasets.

4 Experiments

4.1 CNN Encoder Implementation

We have left the feature encoder G unspeciﬁed,
In
for which, a few options can be considered.
our implementation, we adopt a one-layer CNN
structure from previous works (Kim, 2014; Yu and
Jiang, 2016), as it has been demonstrated to work
well for sentiment classiﬁcation tasks. Given a re-
view document x = (x1, x2, ..., xn) consisting of
n words, we begin by associating each word with
a continuous word embedding (Mikolov et al.,
2013) ex from an embedding matrix E ∈ RV ×d,
where V is the vocabulary size and d is the embed-
ding dimension. E is jointly updated with other
network parameters during training. Given a win-
dow of dense word embeddings ex1, ex2, ..., exl,
the convolution layer ﬁrst concatenates these vec-
tors to form a vector ˆx of length ld and then the
output vector is computed by Equation (7):

Conv(ˆx) = f (W · ˆx + b)

(7)

θg = {W, b} is the parameter set of the en-
coder G and is shared across all windows of the
sequence. f is an element-wise non-linear activa-
tion function. The convolution operation can cap-
ture local contextual dependencies of the input se-
quence and the extracted feature vectors are sim-
ilar to n-grams. After the convolution operation
is applied to the whole sequence, we obtain a list
of hidden vectors H = (h1, h2, ..., hn). A max-
over-time pooling layer is applied to obtain the ﬁ-
nal vector representation ξ of the input document.

4.2 Datasets and Experimental Settings

Existing benchmark datasets such as the Amazon
benchmark (Blitzer et al., 2007) typically remove

reviews with neutral labels in both domains. This
is problematic as the label information of the tar-
get domain is not accessible in an unsupervised
domain adaptation setting. Furthermore, remov-
ing neutral instances may bias the dataset favor-
ably for max-margin-based algorithms like ours,
since the resulting dataset has all uncertain labels
removed, leaving only high conﬁdence examples.
Therefore, we construct new datasets by ourselves.
The results on the original Amazon benchmark is
qualitatively similar, and we present them in Ap-
pendix A for completeness since most of previous
works reported results on it.

Small-scale datasets: Our new dataset was de-
rived from the large-scale Amazon datasets2 re-
leased by McAuley et al. (2015). It contains four
domains3: Book (BK), Electronics (E), Beauty
(BT), and Music (M). Each domain contains two
datasets. Set 1 contains 6000 instances with ex-
actly balanced class labels, and set 2 contains
6000 instances that are randomly sampled from
the large dataset, preserving the original label dis-
tribution, which we believe better reﬂects the label
distribution in real life. The examples in these two
sets do not overlap. Detailed statistics of the gen-
erated datasets are given in Table 1a.

In all our experiments on the small-scale
datasets, we use set 1 of the source domain as the
only source with sentiment label information dur-
ing training, and we evaluate the trained model on
set 1 of the target domain. Since we cannot con-
trol the label distribution of unlabeled data during
training, we consider two different settings:
Setting (1): Only set 1 of the target domain is used
as the unlabeled set. This tells us how the method
performs in a condition when the target domain
has a close-to-balanced label distribution. As we
also evaluate on set 1 of the target domain, this is
also considered as a transductive setting.
Setting (2): Set 2 from both the source and target
domains are used as unlabeled sets. Since set 2 is
directly sampled from millions of reviews, it better
reﬂects real-life sentiment distribution.

Large-scale datasets: We further conduct ex-
periments on four much larger datasets: IMDB4

2http://jmcauley.ucsd.edu/data/amazon/
3The original reviews were rated on a 5-point scale. We
label them with rating < 3, > 3, and = 3 as negative, posi-
tive, and neutral respectively.

4IMDB is rated on a 10-point scale, and we label reviews
with rating < 5, > 6, and = 5/6 as negative, positive, and
neutral respectively.

(I), Yelp2014 (Y), Cell Phone (C), and Baby
(B). IMDB and Yelp2014 were previously used
in (Tang et al., 2015; Yang et al., 2017). Cell
phone and Baby are from the large-scale Amazon
dataset (McAuley et al., 2015; He and McAuley,
2016). Detailed statistics are summarized in Ta-
ble 1b. We keep all reviews in the original datasets
and consider a transductive setting where all target
examples are used for both training (without la-
bel information) and evaluation. We perform sam-
pling to balance the classes of labeled source data
in each minibatch B(s) during training.

4.3 Selection of Development Set

Ideally, the development set should be drawn from
the same distribution as the test set. However, un-
der the unsupervised domain adaptation setting,
we do not have any labeled target data at training
phase which could be used as development set. In
all of our experiments, for each pair of domains,
we instead sample 1000 examples from the train-
ing set of the source domain as development set.
We train the network for a ﬁxed number of epochs,
and the model with the minimum classiﬁcation er-
ror on this development set is saved for evaluation.
This approach works well on most of the problems
since the target domain is supposed to behave like
the source domain if the domain difference is ef-
fectively reduced.

Another problem is how to select the values for
hyper-parameters. If we tune λ1 and λ2 directly
on the development set from the source domain,
most likely both of them will be set to 0, as un-
labeled target data is not helpful for improving in-
domain accuracy of the source domain. Other neu-
ral network models also have the same problem for
hyper-parameter tuning. Therefore, our strategy is
to use the development set from the target domain
to optimize λ1 and λ2 for one problem (e.g., we
only do this on E→BK), and ﬁx their values on the
other problems. This setting assumes that we have
at least two labeled domains such that we can op-
timize the hyper-parameters, and then we ﬁx them
for other new unlabeled domains to transfer to.

4.4 Training Details and Hyper-parameters

We initialize word embeddings using the 300-
dimension GloVe vectors supplied by Pennington
et al., (2014), which were trained on 840 billion
tokens from the Common Crawl. For each pair of
domains, the vocabulary consists of the top 10000
most frequent words. For words in the vocabulary

but not present in the pre-trained embeddings, we
randomly initialize them.

We set hyper-parameters of

the CNN en-
coder following previous works (Kim, 2014; Yu
and Jiang, 2016) without speciﬁc tuning on our
datasets. The window size is set to 3 and the size
of the hidden layer is set to 300. The nonlinear
activation function is Relu. For regularization, we
also follow their settings and employ dropout with
probability set to 0.5 on ξi before feeding it to the
output layer F, and constrain the l2-norm of the
weight vector θf , setting its max norm to 3.

On the small-scale datasets and the Aamzon
benchmark, λ1 and λ2 are set to 200 and 1,
respectively,
tuned on the development set of
task E→BK under setting 1. On the large-scale
datasets, λ1 and λ2 are set to 500 and 0.2, re-
tuned on I→Y. We use a Gaussian
spectively,
curve w(t) = exp[−5(1 − t
)2]λ3 to ramp up
the weight of the bootstrapping loss Ω from 0 to
λ3, where tmax denotes the maximum number of
training epochs. We train 30 epochs for all exper-
iments. We set λ3 to 3 and α to 0.5 for all experi-
ments.

tmax

The batch size is set to 50 on the small-scale
datasets and the Amazon benchmark. We increase
the batch size to 250 on the large-scale datasets to
reduce the number of iterations. RMSProp opti-
mizer with learning rate set to 0.0005 is used for
all experiments.

4.5 Models for Comparison

We compare with the following baselines:

(1) Naive: A non-domain-adaptive baseline
with bag-of-words representations and SVM clas-
siﬁer trained on the source domain.

(2) mSDA (Chen et al., 2012): This is the state-
of-the-art method based on discrete input features.
Top 1000 bag-of-words features are kept as pivot
features. We set the number of stacked layers to 3
and the corruption probability to 0.5.

(3) NaiveNN: This is a non-domain-adaptive
CNN trained on source domain, which is a variant
of our model by setting λ1, λ2, and λ3 to zeros.

(4) AuxNN (Yu and Jiang, 2016): This is a neu-
ral model that exploits auxiliary tasks, which has
achieved state-of-the-art results on cross-domain
sentiment classiﬁcation. The sentence encoder
used in this model is the same as ours.

(5) ADAN (Chen et al., 2017): This method
exploits adversarial training to reduce representa-

(a) Accuracy on the small-scale dataset under setting 1.

(b) Accuracy on the small-scale dataset under setting 2.

(c) Macro-F1 on the large-scale dataset.

Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is
signiﬁcantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test.

tion difference between domains. The original pa-
per uses a simple feedforward network as encoder.
For fair comparison, we replace it with our CNN-
based encoder. We train 5 iterations on the dis-
criminator per iteration on the encoder and senti-
ment classiﬁer as suggested in their paper.

(6) MMD: MMD has been widely used for min-
imizing domain discrepancy on images. In those
works (Tzeng et al., 2014; Long et al., 2017), vari-
ants of deep CNNs are used for encoding images
and the MMDs of multiple layers are jointly mini-
mized. In NLP, adding more layers of CNNs may
not be very helpful and thus those models from
image-related tasks can not be directly applied
to our problem. To compare with MMD-based
method, we train a model that jointly minimize
the classiﬁcation loss L on the source domain and
i=1} and {ξ(t)
MMD between {ξ(s)
|nt
|ns
i=1}. For
computing MMD, we use a Gaussian RBF which
is a common choice for characteristic kernel.

i

i

In addition to the above baselines, we also show
results of different variants of our model. DAS
as shown in Algorithm 1 denotes our full model.
DAS-EM denotes the model with only entropy

minimization for semi-supervised learning (set
λ3 = 0). DAS-SE denotes the model with only
self-ensemble bootstrapping for semi-supervised
learning (set λ2 = 0). FANN (feature-adaptation
neural network) denotes the model without semi-
supervised learning performed (set both λ2 and λ3
to zeros).

4.6 Main Results

Figure 15 shows the comparison of adaptation re-
sults (see Appendix B for the exact numerical
numbers). We report classiﬁcation accuracy on
the small-scale dataset. For the large-scale dataset,
macro-F1 is instead used since the label distribu-
tion in the test set is extremely unbalanced. Key
observations are summarized as follows. (1) Both
DAS-EM and DAS-SE perform better in most
cases compared with ADAN, MDD, and FANN,
in which only feature adaptation is performed.
This demonstrates the effectiveness of the pro-

5We exclude results of Naive, mSDA and AuxNN on the
large-scale dataset. Both Naive and mSDA have difﬁculties
to scale up to the large dataset. AuxNN relies on manually
selecting positive and negative pivots before training.

Figure 2: Accuracy vs. percentage of unlabeled target training examples.

Figure 3: Accuracy vs. number of labeled target training examples.

posed domain adaptive semi-supervised learning
framework. DAS-EM is more effective than DAS-
SE in most cases, and the full model DAS with
both techniques jointly employed overall has the
(2) When comparing the two
best performance.
settings on the small-scale dataset, all domain-
adaptive methods6 generally perform better under
setting 1. In setting 1, the target examples are bal-
anced in classes, which can provide more diverse
opinion-related features. However, when consid-
ering unsupervised domain adaptation, we should
not presume the label distribution of the unlabeled
data. Thus, it is necessary to conduct experiments
using datasets that reﬂect real-life sentiment dis-
tribution as what we did on setting2 and the large-
scale dataset. Unfortunately, this is ignored by
most of previous works. (3) Word-embeddings are
very helpful, as we can see even NaiveNN can sub-
stantially outperform mSDA on most tasks.

To see the effect of semi-supervised learning
alone, we also conduct experiments by setting
λ1 = 0 to eliminate the effect of feature adapta-
tion. Both entropy minimization and bootstrap-
ping perform very badly in this setting. En-
tropy minimization gives almost random predic-
tions with accuracy below 0.4, and the results
of bootstrapping are also much lower compared
to NaiveNN. This suggests that the feature adap-
tation component is essential. Without it,
the
learned target representations are less meaning-
ful and discriminative. Applying semi-supervised

6Results of Naive and NaiveNN do not change under both

settings as they are only trained on the source domain.

learning in this case is likely to worsen the results.

4.7 Further Analysis

In Figure 2, we show the change of accuracy with
respect to the percentage of unlabeled data used
for training on three particular problems under set-
ting 1. The value at x = 0 denotes the accuracies
of NaiveNN which does not utilize any target data.
For DAS, we observe a nonlinear increasing trend
where the accuracy quickly improves at the be-
ginning, and then gradually stabilizes. For other
methods, this trend is less obvious, and adding
more unlabeled data sometimes even worsen the
results. This ﬁnding again suggests that the pro-
posed approach can better exploit the information
from unlabeled data.

We also conduct experiments under a setting
with a small number of labeled target examples
available. Figure 3 shows the change of accuracy
with respect to the number of labeled target exam-
ples added for training. We can observe that DAS
is still more effective under this setting, while the
performance differences to other methods gradu-
ally decrease with the increasing number of la-
beled target examples.

4.8 CNN Filter Analysis

In this subsection, we aim to better understand
DAS by analyzing sentiment-related CNN ﬁlters.
To do that, 1) we ﬁrst select a list of the most re-
lated CNN ﬁlters for predicting each sentiment la-
bel (positive, negative neutral). Those ﬁlters can
be identiﬁed according to the learned weights θf

best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

(a) NaiveNN

(b) FANN

(c) DAS

Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the
5 most positive-sentiment-related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

of the output layer F. Higher weight indicates
stronger relatedness. 2) Recall that in our im-
plementation, each CNN ﬁlter has a window size
of 3 with Relu activation. We can thus represent
each selected ﬁlter as a ranked list of trigrams with
highest activation values.

We analyze the CNN ﬁlters

learned by
NaiveNN, FANN and DAS respectively on task
E→BT under setting 1. We focus on E→BT for
study because electronics and beauty are very dif-
ferent domains and each of them has a diverse
set of domain-speciﬁc sentiment expressions. For
each method, we identify the top 10 most related
ﬁlters for each sentiment label, and extract the top
trigrams of each selected ﬁlter on both source and
target domains. Since labeled source examples are
used for training, we ﬁnd the ﬁlters learned by the
three methods capture similar expressions on the
source domain, containing both domain-invariant
and domain-speciﬁc trigrams. On the target do-
main, DAS captures more target-speciﬁc expres-
sions compared to the other two methods. Due
to space limitation, we only present a small sub-
set of positive-sentiment-related ﬁlters in Table 2.
The complete results are provided in Appendix C.
From Table 2, we can observe that the ﬁlters
learned by NaiveNN are almost unable to cap-
ture target-speciﬁc sentiment expressions, while
FANN is able to capture limited target-speciﬁc
words such as “clean” and “scent”. The ﬁlters
learned by DAS are more domain-adaptive, cap-
turing diverse sentiment expressions in the target
domain.

5 Conclusion

In this work, we propose DAS, a novel frame-
work that jointly performs feature adaptation and
semi-supervised learning. We have demonstrated
through multiple experiments that DAS can better
leverage unlabeled data, and achieve substantial
improvements over baseline methods. We have
also shown that feature adaptation is an essen-
tial component, without which, semi-supervised
learning is not able to function properly. The pro-
posed framework could be potentially adapted to
other domain adaptation tasks, which is the focus
of our future studies.

References

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: domain adaptation for sentiment classiﬁ-
In Annual Meeting of the Association for
cation.
Computational Linguistics.

Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In The 29th Interna-
tional Conference on Machine Learning.

Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie,
and Kilian Weinberger. 2017. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁer. In Arxiv e-prints arXiv:1606.01614.

Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classiﬁers for
text classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: a deep learning approach. In The 28th
International Conference on Machine Learning.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
In
supervised learning by entropy minimization.
Neural Information Processing Systems.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander Smola. 2012. A
kernel two-sample test. Journal of Machine Learn-
ing Research, 13:723–773.

Ruining He and Julian McAuley. 2016. Ups and
downs: modeling the visual evolution of fashion
In
trends with one-class collaborative ﬁltering.
WWW.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classiﬁcation. In ACL.

Jing Jiang and ChengXiang Zhai. 2007.

Instance
weighting for domain adaptation in NLP. In Annual
Meeting of the Association for Computational Lin-
guistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Conference on Empirical
Methods in Natural Language Processing.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In International
Conference on Learning Representation.

Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial mem-
ory network for cross-domain sentiment classiﬁca-
tion. In The 26th International Joint Conference on
Artiﬁcial Intelligence.

Mingsheng Long, Yue Cao,

Jianmin Wang, and
Michael I. Jordan. 2015. Learning transferable fea-
In Interna-
tures with deep adaptation networks.
tional Conference on Machine Learning.

Mingsheng Long, Han Zhu,

Jianmin Wang, and
Michael I. Jordan. 2017. Deep transfer learning with
joint adaptation networks. In International Confer-
ence on Machine Learning.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
ommendations on styles and substitutes. In The 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Neural Information Processing Systems.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classiﬁcation via spectral feature alignment. In
The 19th International World Wide Web Conference.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representation of users and products for
document level sentiment classiﬁcation. In Annual
Meeting of the Association for Computational Lin-
guistics.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
maximizing for domain invariance. In Arxiv e-prints
arXiv:1412.3474.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Conference on Empirical Meth-
ods in Natural Language Processing.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Annual
Meeting of the Association for Computational Lin-
guistics.

Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In Conference on
Empirical Methods in Natural Language Process-
ing.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Annual Meeting of the Association
for Computational Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
In Conference on Empirical
timent classiﬁcation.
Methods in Natural Language Processing.

Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-
aohua Tony Hu. 2015. Linking heterogeneous input
features with pivots for domain adaptation. In The
24th International Joint Conference on Artiﬁcial In-
telligence.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016. Bi-transferring deep neural
networks for domain adaptation. In Annual Meeting
of the Association for Computational Linguistics.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: transfer learning with deep autoencoders.
In The 24th International Joint Conference on Arti-
ﬁcial Intelligence.

S

D
E
K
B
E
K
B
D
K
B
D
E

T

B
B
B
D
D
D
E
E
E
K
K
K

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE
82.10∗
77.75
79.60
82.35
79.75
82.15∗
75.80
80.05
85.95
79.45∗
79.50
83.80

82.00∗
80.25∗
79.95
82.65
81.40∗
81.65∗
80.25∗
81.40∗
85.70
81.55∗
80.80
84.50

80.30
77.25
79.20
81.65
79.55
76.90
76.75
79.25
85.60
77.55
78.00
83.85

81.05
78.65
79.70
82.00
80.10
79.35
76.45
80.20
85.75
75.20
79.70
81.75

81.70
78.55
79.25
82.30
79.70
80.45
77.60
79.70
86.85
76.10
77.35
83.95

80.80
78.00
77.85
81.75
80.65
78.90
76.40
77.55
84.05
78.10
80.05
84.15

78.50
76.15
75.65
80.60
76.30
76.05
75.55
76.00
84.20
75.95
76.30
84.45

75.20
68.85
70.00
77.15
69.50
71.40
72.15
71.65
79.75
73.50
72.00
82.80

81.10
77.95
77.75
80.80
77.00
79.35
76.20
76.60
84.85
77.40
78.55
84.95

Average

73.66

77.98

79.38

79.85

80.29

80.00

79.65

81.84

80.68

DAS
82.05∗
80.00∗
80.05∗
82.75∗
80.15
81.40∗
81.15∗
81.55∗
85.80
82.25∗
81.50∗
84.85

81.96

Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations
are reported for each neural method. ∗ indicates that the proposed method (DAS-EM, DAS-SE, DAS) is
signiﬁcantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

A Results on Amazon Benchmark

C CNN Filter Analysis Full Results

As mentioned in Section 4.8, we conduct CNN ﬁl-
ter analysis on NaiveNN, FANN, and DAS. For
each method, we identify the top 10 most related
ﬁlters for positive, negative, neutral sentiment la-
bels respectively, and then represent each selected
ﬁlter as a ranked list of trigrams with the highest
activation values on it. Table 5, 6, 7 in the fol-
lowing pages illustrate the trigrams from the tar-
get domain (beauty) captured by the selected ﬁl-
ters learned on E→BT for each method.

We can observe that compared to NaiveNN and
FANN, DAS is able to capture a more diverse set
of relevant sentiment expressions on the target do-
main for each sentiment label. This observation is
consistent with our motivation. Since NaiveNN,
FANN and other baseline methods solely train
the sentiment classiﬁer on the source domain, the
learned encoder is not able to produce discrimina-
tive features on the target domain. DAS addresses
this problem by reﬁning the classiﬁer on the tar-
get domain with semi-supervised learning, and
the overall objective forces the encoder to learn
feature representations that are not only domain-
invariant but also discriminative on both domains.

Most previous works (Blitzer et al., 2007; Pan
et al., 2010; Glorot et al., 2011; Chen et al.,
2012; Zhou et al., 2016) carried out experiments
on the Amazon benchmark released by Blitzer
et al.
(2007). The dataset contains 4 different
domains: Book (B), DVDs (D), Electronics (E),
and Kitchen (K). Following their experimental set-
tings, we consider the binary classiﬁcation task to
predict whether a review is positive or negative
on the target domain. Each domain consists of
1000 positive and 1000 negative reviews respec-
tively. We also allow 4000 unlabeled reviews to
be used for both the source and the target domains,
of which the positive and negative reviews are bal-
anced as well, following the settings in previous
works. We construct 12 cross-domain sentiment
classiﬁcation tasks and split the labeled data in
each domain into a training set of 1600 reviews
and a test set of 400 reviews. The classiﬁer is
trained on the training set of the source domain
and is evaluated on the test set of the target do-
main. The comparison results are shown in Ta-
ble 3.

B Numerical Results of Figure 1

Due to space limitation, we only show results in
ﬁgures in the paper. All numerical numbers used
for plotting Figure 1 are presented in Table 4. We
can observe that DAS-EM, DAS-SE, and DAS
all achieve substantial improvements over baseline
methods under different settings.

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

BK 49.07
E
BK 48.17
BT
M BK 45.20
46.43
E
BK
53.63
E
BT
37.93
E
M
45.57
BK BT
48.43
E
BT
39.42
M BT
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

BK 49.07
E
BK 48.17
BT
M BK 45.20
E
46.43
BK
E
BT
53.63
37.93
M
E
BK BT
45.57
48.43
E
BT
M BT
39.43
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

55.13
53.53
49.22
48.22
57.32
38.13
50.77
53.13
39.37
47.88
47.88
49.62
49.18

52.88
47.65
48.33
47.07
55.12
37.40
49.63
51.98
37.73
45.97
45.12
45.78
47.06

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

60.62
59.86
60.43
48.45
60.98
49.60
48.67
55.58
48.65
48.87
51.19
53.11
53.84

57.72
58.46
58.15
48.22
59.08
49.43
47.80
54.37
46.92
48.79
52.31
53.55
52.98

63.32
65.62
62.87
47.42
63.13
46.57
46.14
50.98
44.26
51.10
50.23
55.35
53.92

57.07
59.78
58.67
49.48
59.45
47.00
47.52
51.28
45.73
50.20
52.57
54.68
52.79

60.38
59.66
60.20
53.32
60.53
51.55
49.48
54.83
48.35
53.04
51.81
54.43
54.80

57.43
56.17
57.08
45.42
60.24
48.72
45.43
54.92
46.68
48.76
51.50
54.55
52.23

59.59
59.28
57.65
51.27
60.62
47.23
50.24
56.78
48.89
52.35
52.14
53.84
54.15

56.43
57.98
57.75
51.95
58.67
48.92
49.83
55.42
48.48
49.47
48.18
53.41
53.04

66.48∗
66.78∗
69.63∗
58.59∗
65.71∗
55.88∗
49.49
61.53∗
47.65
55.47∗
58.28∗
60.95∗
59.74

57.78
61.17∗
58.62
54.51∗
61.27
51.28∗
53.72∗
53.10
47.18
52.37∗
53.63∗
56.24∗
55.07

(a) Accuracy on the small-scale dataset under setting 1

DAS
67.12∗
66.53
70.31∗
58.73∗
66.14∗
55.78∗
51.30∗
60.76∗
50.66∗
55.98∗
59.06∗
60.5∗
60.24

55.20
63.32∗
60.77∗
53.92∗
59.83
52.88∗
54.67∗
56.05∗
49.73∗
53.52∗
55.38∗
56.02∗
55.94

62.37
61.17
65.24∗
55.15∗
61.78
53.22∗
54.23∗
59.52∗
50.67∗
55.13∗
55.60∗
56.90∗
57.58

58.93
60.17∗
58.25
52.47∗
61.42
51.18∗
51.23∗
56.43∗
51.57∗
52.68∗
52.25
56.23∗
55.23

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

DAS

(b) Accuracy on the small-scale dataset under setting 2

S

T

Y
I
C
I
B
I
I
Y
C
Y
B
Y
I
C
Y
C
B
C
I
B
Y
B
B
C
Average

NaiveNN ADAN MMD FANN DAS-EM DAS-SE
56.66∗
55.18
54.30
58.72∗
59.14
58.43
51.97∗
54.67∗
59.98
50.81
52.95
58.12
55.91

55.04
57.27∗
57.31∗
57.92∗
61.17
59.94∗
53.46∗
53.48
59.84
48.84
52.87
57.74
56.24

54.16
53.35
51.40
56.52
60.81
58.77
50.49
53.12
61.23
47.35
54.43
60.52
55.18

54.46
53.07
52.39
56.30
56.02
55.72
51.04
51.86
60.19
48.17
53.54
55.56
54.02

55.52
55.07
54.64
52.57
60.70
58.42
47.27
52.53
59.91
46.34
50.82
59.99
54.48

53.01
51.84
45.85
55.46
61.22
56.86
50.38
53.87
59.48
50.05
54.73
60.47
54.43

DAS
58.54∗
57.28∗
58.02∗
58.92∗
61.39
61.87∗
53.38∗
55.44∗
59.76
48.84
52.91
59.75
57.18

(c) Macro-F1 scores on the large-scale dataset

Table 4: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signiﬁcantly
better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

1
best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

2
highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

3
nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

4
beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

5
since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

6
store-and-am
cleanser-and-am
olay-and-am
daily-and-need
shower-and-noticed

7
ofﬁce-setting-thanks
locks-shimmering-color
dirty-blonde-color
victoria-secrets-gorgeous
dirty-pinkish-color

8
car-washes-!
price-in-stores
products-are-priced
car-and-burning
from-our-store

(a) NaiveNN

9
speed-is-perfect
buttons-are-perfect
unit-is-superb
spray-is-perfect
coverage-is-excellent

10
!-i-recommend
!-i-highly
shower-i-slather
spots-i-needed
best-i-use

1
prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

2
so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

3
purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

4
feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

5
are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

6
shower-or-cleaning
water-onto-my
bleach-your-towels
pump-onto-my
water-great-for

8

7
deﬁnitely-purchase-again more-affordable-price
deﬁnitely-buy-again
perfect-for-my
deﬁnitely-order-again
super-happy-to

a-perfect-length
an-exceptional-value
’ve-enjoyed-it
pretty-decent-layer

9
absolutely-wonderful-!
perfect-for-running
concealer-for-my
moisturizing-for-my
super-glue-even

10
felt-cleaner-than
ﬂat-iron-through
rubbed-grease-on
deeply-cleans-my
being-cleaner-after

1
bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

2
love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

3
feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

4
cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

5
excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

6
shower-lather-wonderfully
affordable-cleans-nicely
peels-great-price
daughter-loves-this
cleans-great-smells

7
highly-recommend-!
deﬁnitely-recommend-!
love-love-!
highly-advise-!
time-advise-!

8
excellent-fragrance-and
fantastic-for-daytime
wonderfully-moisturizing-and
lathers-great-cleans
delightful-shampoo-works

10
9
its-unique-smoothing
forgeous-gragrance-mist
smooth-luxurious-texture wonderful-bedtime-scent
’s-extremely-gentle
’s-affordable-combination
absorbs-quite-well

love-essie-polish
perfect-beautiful-shimmer
fantastic-coverage-hydrates

(b) FANN

(c) DAS

Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
pads-ruined-my
highly-disappointed-and
dryers-blew-my
completely-worthless-didn’t
am-disappointed-and

2
simply-threw-out
reviewer-pointed-out
extracts-broke-into
actually-threw-out
clips-barely-keep

3
hours-after-trying
minutes-after-rinsing
disappointed-after-trying
lips-after-trying
dry-after-shampooing

4
junk-drawer-∗
refund-time-!
total-fake-wen
waste-your-time
total-fail-!

5
contacted-manufacturer-about
minutes-not-worth
’ve-owned-this
hour-unless-it
results-they-claim

6
were-awful-garbage
what-awful-garbage
and-utter-waste
are-absolute-garbage
piece-of-junk

7
two-failed-attempts
a-mistake-save
a-deﬁnite-return
a-pathetic-limp
a-total-disappointment

8
auto-ship-sent
am-returning-to
am-unable-to
am-pale-ghost
got-returned-and

10
broke-don’t-ﬁx
sent-me-expired

9
refund-and-dispose
refund-spend-your
wouldn’t-recommend-! wearing-false-eyelashes
not-buy-dunhill
not-worth-returning

a-temporary-ﬁx
a-disappointment-cheap

(a) NaiveNN

2
the-worse-mascaras

1
nasty-sunburn-lol
bother-returning-them it-caused-patchy
fails-miserably-at
minutes-auric-needs
severely-burned-me

lifeless-disaster-enter
it-fails-miserably
feel-worse-leaving

3
stale-very-unhappy
were-horrible-failures
send-this-crap
were-awful-garbage
were-horribly-red

4
actually-hurts-your
didn’t-bother-returning
it-hurts-your
didn’t-exist-in
skin-horribly-after

5
a-return-label
stay-away-completely
like-bug-quit
a-defective-brown
’d-refund-the

6
worse-with-exercise
worse-and-after
unable-to-return
worse-my-face
poorly-in-step

7
not-stink-your
mistake-save-your
nothing-!-by
nothing-happened-!
nothing-save-your

8
it-fails-miserably
is-ineffective-apart
but-horribly-unhealthy
a-pathetic-limp
a-worse-job

9
got-progressively-worse
gave-opposite-result
another-epic-fail
got-horribly-painful
was-downright-painful

10
stopped-working-for
uncomfortable-i-returned
i-am-returning
stopped-working-shortly
not-waterproof-makeup

1
poorly-designed-product
defective-dryer-promising
disgusting-smelling-thing
hurts-your-scalp
hurts-your-hair

2
a-refund-spend
a-refund-save
i-regret-spending
just-wouldn’t-spend
looked-washed-out

3
completely-waste-of
of-junk-*
were-awful-garbage
worthless-waste-of
throwing-money-away

4
smells-disgusting-!
smells-horribly-like
does-not-straighten
’s-false-advertising
a-disgusting-cheap

5
burning-rubber-stench
began-smelling-vomit
reaction-and-wasted
control-and-smelled
using-this-disgusting

6
super-irritating-!
strong-reaction-and
really-burned-and
very-pasty-and
super-streaky-and

7
got-promptly-broke
after-ive-washed
after-several-attempts
this-stuff-stinks
again-i-threw

8
sore-and-painful
is-simply-irritating
tight-and-uncomfortable
drying-and-irritating
goopy-and-unpleasant

9
it-caused-patchy
layer-hydrogenated-alcohols
the-harmful-uva
my-severe-dark
a-allergic-reaction

10
painful-it-hurt
unnecessary-health-risks
uncomfortable-to-wear
stinging-your-eyes
unbearable-to-wear

(b) FANN

(c) DAS

Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
purpose-cologne-splash
other-hanae-mori
the-mavala-peeled
avoid-hair-pulling
cause-rashes-stinging

2
okay-cord-was
cocamide-dea-is
coily-conditioner-is
ﬂaky-dandruff-is
quickly-cord-is

3
hands-feet-elbows
been-sealed-tight
stainless-steel-blackhead
severely-tight-chest
thick-nasty-callouses

4
aggressive-in-general
pimples-in-general
biotin-in-general
dimethicone-is-terrible
but-in-general

5
but-its-okay
it-moisturizes-okay
but-moisturizes-keeps
but-don’t-expect
it-lathers-ok

6
pretty-damaged-from
daughter-suffers-from
teenager-suffers-from
tissue-damage-during
the-damage-on

7
darker-olive-complexion
stronger-healthier-or
natural-ingredient-however
vitamin-enriched-color
natural-ingredients-∗

8
doesn’t-mind-pushing
kinda-doesn’t-its
kinda-kinky-coily
okay-job-of
intended-purpose-that

9
producto-por-los
unstuck-frownies-∗
they-push-∗
uva-rays-uva
tend-to-slip

10
feeling-didn’t-last
curls-didn’t-last
extra-uv-protection
garnier-fructis-curl
the-mavala-after

(a) NaiveNN

1
worse-and-after
worse-before-improving
unable-to-return
unless-your-entire
horrible-in-execution

2
maybe-a-refund
ok-mask-i
ok-pining-it
ok-try-i
ok-tho-i

3
very-disappointing-waste
ok-but-clean
ok-but-will
ok-but-didn’t
ok-nothing-special

4
my-ears-are
my-neck-line
cause-unsightly-beads
my-sporadic-line
your-ear-is

5
pretty-neutral-neither
ok-so-if
ok-during-pregnancy
kinda-annoying-if
ok-this-seems

6
uncomfortable-i-returned
weak-they-bend
claimed-faulty-∗
suffers-from-wind
as-defective-∗

7
sticky-lathers-and
quickly-deep-cleans
but-elegant-bottle
beat-the-price
and-reasonably-priced

8
some-fading-when
real-disappointment-the
especially-noticeable-after
progressively-worse-during
style-unfortunately-the

9
are-very-painful
are-less-painful
are-a-pain
about-sum-damage
offered-no-pain

10
its-also-convenient
that-also-my
that-may-make
that-allows-your
its-helpful-to

1
’m-kinda-pale
a-terrible-headache
but-kinda-annoying
’m-kinda-mad
i-kinda-stopped

2
darker-but-nope
gray-didn’t-cover
makeup-doesn’t-sweat
dark-spots-around
moist-but-thats

3
ok-but-horrible
ok-but-didn’t
okay-but-doesn’t
okay-however-it
unfortunately-straight

4
noticeable-i-avoid
however-i-lean
but-otherwise-ok
but-im-deciding
however-i-prefer

5
same-result-mediocre
it-caused-patchy
doesn’t-cause-ﬂare
the-harmful-uva
rather-unpleasant-smell

6
kinda-annoying-if
pretty-bad-breakage
my-slight-discoloration
smells-kinda-bad
look-kinda-crappy

7
brutal-winter-however
summer-color-however
beige-shade-however
is-okay-however
bit-greasy-however

8
higher-rating-because
slight-burnt-rubber
noticeable-tan-since
somewhat-pale-affect
kinda-pale-so

9
nothing-for-odor
kinda-recommend-this
not-recommend-if
noticeable-but-non
nothing-special-moderate

10
but-darker-*
slightly-darker-shade
somewhat-pale-affect
but-somewhat-heavy
bit-dull-heavy

(b) FANN

(c) DAS

Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

Adaptive Semi-supervised Learning for Cross-domain
Sentiment Classiﬁcation

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore
‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg
‡d.dahlmeier@sap.com

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
3
5
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We consider the cross-domain sentiment clas-
siﬁcation problem, where a sentiment classi-
ﬁer is to be learned from a source domain and
to be generalized to a target domain. Our ap-
proach explicitly minimizes the distance be-
tween the source and the target instances in
an embedded feature space. With the differ-
ence between source and target minimized,
we then exploit additional information from
the target domain by consolidating the idea
of semi-supervised learning, for which, we
jointly employ two regularizations – entropy
minimization and self-ensemble bootstrapping
– to incorporate the unlabeled target data for
classiﬁer reﬁnement. Our experimental results
demonstrate that the proposed approach can
better leverage unlabeled data from the target
domain and achieve substantial improvements
over baseline methods in various experimental
settings.

1

Introduction

In practice, it is often difﬁcult and costly to anno-
tate sufﬁcient training data for diverse application
domains on-the-ﬂy. We may have sufﬁcient la-
beled data in an existing domain (called the source
domain), but very few or no labeled data in a
new domain (called the target domain). This issue
has motivated research on cross-domain sentiment
classiﬁcation, where knowledge in the source do-
main is transferred to the target domain in order to
alleviate the required labeling effort.

One key challenge of domain adaptation is that
data in the source and target domains are drawn
from different distributions. Thus, adaptation per-
formance will decline with an increase in distribu-
tion difference. Speciﬁcally, in sentiment analy-
sis, reviews of different products have different vo-
cabulary. For instance, restaurants reviews would
contain opinion words such as “tender”, “tasty”, or

“undercooked” and movie reviews would contain
“thrilling”, “horriﬁc”, or “hilarious”. The intersec-
tion between these two sets of opinion words could
be small which makes domain adaptation difﬁcult.
Several techniques have been proposed for ad-
dressing the problem of domain shifting. The
aim is to bridge the source and target domains
by learning domain-invariant feature representa-
tions so that a classiﬁer trained on a source do-
main can be adapted to another target domain.
In cross-domain sentiment classiﬁcation, many
works (Blitzer et al., 2007; Pan et al., 2010; Zhou
et al., 2015; Wu and Huang, 2016; Yu and Jiang,
2016) utilize a key intuition that domain-speciﬁc
features could be aligned with the help of domain-
invariant features (pivot features). For instance,
“hilarious” and “tasty” could be aligned as both
of them are relevant to “good”.

Despite their promising results,

these works
share two major limitations. First, they highly de-
pend on the heuristic selection of pivot features,
which may be sensitive to different applications.
Thus the learned new representations may not ef-
fectively reduce the domain difference. Further-
more, these works only utilize the unlabeled tar-
get data for representation learning while the sen-
timent classiﬁer was solely trained on the source
domain. There have not been many studies on ex-
ploiting unlabeled target data for reﬁning the clas-
siﬁer, even though it may contain beneﬁcial infor-
mation. How to effectively leverage unlabeled tar-
get data still remains an important challenge for
domain adaptation.

In this work, we argue that the information
from unlabeled target data is beneﬁcial for do-
main adaptation and we propose a novel Domain
Adaptive Semi-supervised learning framework
(DAS) to better exploit it. Our main intuition is
to treat the problem as a semi-supervised learn-
ing task by considering target instances as unla-

beled data, assuming the domain distance can be
effectively reduced through domain-invariant rep-
resentation learning. Speciﬁcally, the proposed
approach jointly performs feature adaptation and
semi-supervised learning in a multi-task learning
setting. For feature adaptation, it explicitly mini-
mizes the distance between the encoded represen-
tations of the two domains. On this basis, two
semi-supervised regularizations – entropy mini-
mization and self-ensemble bootstrapping – are
jointly employed to exploit unlabeled target data
for classiﬁer reﬁnement.

We evaluate our method rigorously under multi-
ple experimental settings by taking label distribu-
tion and corpus size into consideration. The re-
sults show that our model is able to obtain sig-
niﬁcant improvements over strong baselines. We
also demonstrate through a series of analysis that
the proposed method beneﬁts greatly from incor-
porating unlabeled target data via semi-supervised
learning, which is consistent with our motivation.
Our datasets and source code can be obtained from
https://github.com/ruidan/DAS.

2 Related Work

Domain Adaptation: The majority of feature
adaptation methods for sentiment analysis rely on
a key intuition that even though certain opinion
words are completely distinct for each domain,
they can be aligned if they have high correlation
with some domain-invariant opinion words (pivot
words) such as “excellent” or “terrible”. Blitzer
et al. (2007) proposed a method based on struc-
tural correspondence learning (SCL), which uses
pivot feature prediction to induce a projected fea-
ture space that works well for both the source and
the target domains. The pivot words are selected in
a way to cover common domain-invariant opinion
words. Subsequent research aims to better align
the domain-speciﬁc words (Pan et al., 2010; He
et al., 2011; Wu and Huang, 2016) such that the
domain discrepancy could be reduced. More re-
cently, Yu and Jiang (2016) borrow the idea of
pivot feature prediction from SCL and extend it
to a neural network-based solution with auxiliary
tasks.
In their experiment, substantial improve-
ment over SCL has been observed due to the use
of real-valued word embeddings. Unsupervised
representation learning with deep neural networks
(DNN) such as denoising autoencoders has also
been explored for feature adaptation (Glorot et al.,

2011; Chen et al., 2012; Yang and Eisenstein,
2014). It has been shown that DNNs could learn
transferable representations that disentangle the
underlying factors of variation behind data sam-
ples.

Although the aforementioned methods aim to
reduce the domain discrepancy, they do not explic-
itly minimize the distance between distributions,
and some of them highly rely on the selection of
pivot features. In our method, we formally con-
struct an objective for this purpose. Similar ideas
have been explored in many computer vision prob-
lems, where the representations of the underlying
domains are encouraged to be similar through ex-
plicit objectives (Tzeng et al., 2014; Ganin and
Lempitsky, 2015; Long et al., 2015; Zhuang et al.,
2015; Long et al., 2017) such as maximum mean
discrepancy (MMD) (Gretton et al., 2012). In NLP
tasks, Li et al. (2017) and Chen et al. (2017) both
proposed using adversarial training framework for
reducing domain difference. In their model, a sub-
network is added as a domain discriminator while
deep features are learned to confuse the discrim-
inator. The feature adaptation component in our
model shares similar intuition with MMD and ad-
versary training. We will show a detailed compar-
ison with them in our experiments.
Semi-supervised Learning: We attempt to treat
domain adaptation as a semi-supervised learning
task by considering the target instances as unla-
beled data. Some efforts have been initiated on
transfer learning from unlabeled data (Dai et al.,
2007; Jiang and Zhai, 2007; Wu et al., 2009).
In our model, we reduce the domain discrep-
ancy by feature adaptation, and thereafter adopt
semi-supervised learning techniques to learn from
unlabeled data. Primarily motivated by (Grand-
valet and Bengio, 2004) and (Laine and Aila,
2017), we employed entropy minimization and
self-ensemble bootstrapping as regularizations to
incorporate unlabeled data. Our experimental re-
sults show that both methods are effective when
jointly trained with the feature adaptation objec-
tive, which conﬁrms to our motivation.

3 Model Description

3.1 Notations and Model Overview

We conduct most of our experiments under an un-
supervised domain adaptation setting, where we
have no labeled data from the target domain. Con-
sider two sets Ds and Dt. Ds = {x(s)
i=1 is
i

i }|ns

, y(s)

i }|nt

from the source domain with ns labeled examples,
where yi ∈ RC is a one-hot vector representation
of sentiment label and C denotes the number of
classes. Dt = {x(t)
i=1 is from the target domain
with nt unlabeled examples. N = ns + nt denotes
the total number of training documents including
both labeled and unlabeled1. We aim to learn a
sentiment classiﬁer from Ds and Dt such that the
classiﬁer would work well on the target domain.
We also present some results under a setting where
we assume that a small number of labeled target
examples are available (see Figure 3).

For the proposed model, we denote G parame-
terized by θg as a neural-based feature encoder that
maps documents from both domains to a shared
feature space, and F parameterized by θf as a
fully connected layer with softmax activation serv-
ing as the sentiment classiﬁer. We aim to learn fea-
ture representations that are domain-invariant and
at the same time discriminative on both domains,
thus we simultaneously consider three factors in
our objective: (1) minimize the classiﬁcation error
on the labeled source examples; (2) minimize the
domain discrepancy; and (3) leverage unlabeled
data via semi-supervised learning.

Suppose we already have the encoded features
of documents {ξ(s,t)
i=1 (see
Section 4.1), the objective function for purpose (1)
is thus the cross entropy loss on the labeled source
examples

= G(x(s,t)

; θg)}|N

i

i

L = −

1
ns

ns(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(s)
y(s)

i (j)

(1)

i

i = F(ξ(s)

where ˜y(s)
; θf ) denotes the predicted la-
bel distribution. In the following subsections, we
will explain how to perform feature adaptation and
domain adaptive semi-supervised learning in de-
tails for purpose (2) and (3) respectively.

3.2 Feature Adaptation

Unlike prior works (Blitzer et al., 2007; Yu and
Jiang, 2016), our method does not attempt to align
domain-speciﬁc words through pivot words.
In
our preliminary experiments, we found that word
embeddings pre-trained on a large corpus are able
to adequately capture this information. As we will

1Note that unlabeled source examples can also be in-
cluded for training. In that case, N = ns + nt + ns(cid:48) where
ns(cid:48) denotes the number of unlabeled source examples. This
corresponds to our experimental setting 2. For simplicity, we
only consider ns and nt in our description.

later show in our experiments, even without adap-
tation, a naive neural network classiﬁer with pre-
trained word embeddings can already achieve rea-
sonably good results.

i }nt

i }|ns

i=1 and {ξ(t)

We attempt to explicitly minimize the distance
between the source and target feature represen-
tations ({ξ(s)
i=1). A few meth-
ods from literature can be applied such as Maxi-
mum Mean Discrepancy (MMD) (Gretton et al.,
2012) or adversary training (Li et al., 2017; Chen
et al., 2017). The main idea of MMD is to esti-
mate the distance between two distributions as the
distance between sample means of the projected
embeddings in Hilbert space. MMD is implicitly
computed through a characteristic kernel, which is
used to ensure that the sample mean is injective,
leading to the MMD being zero if and only if the
distributions are identical. In our implementation,
we skip the mapping procedure induced by a char-
acteristic kernel for simplifying the computation
and learning. We simply estimate the distribution
distance as the distance between the sample means
in the current embedding space. Although this ap-
proximation cannot preserve all statistical features
of the underlying distributions, we ﬁnd it performs
comparably to MMD on our problem. The follow-
ing equations formally describe the feature adap-
tation loss J :

J = KL(gs||gt) + KL(gt||gs)
g(cid:48)
s
(cid:107)g(cid:48)
s(cid:107)1

g(cid:48)
s =

1
ns

ξ(s)
i

gs =

ns(cid:88)

,

i=1

nt(cid:88)

i=1

1
nt

g(cid:48)
t =

ξ(t)
i

,

gt =

g(cid:48)
t
(cid:107)g(cid:48)
t(cid:107)1

(2)

(3)

(4)

s and g(cid:48)

L1 normalization is applied on the mean represen-
tations g(cid:48)
t, rescaling the vectors such that
all entries sum to 1. We adopt a symmetric ver-
sion of KL divergence (Zhuang et al., 2015) as the
distance function. Given two distribution vectors
P, Q ∈ Rk, KL(P||Q) = (cid:80)k

i=1 P(i) log( P(i)

Q(i) ).

3.3 Domain Adaptive Semi-supervised

Learning (DAS)

We attempt to exploit the information in target
data through semi-supervised learning objectives,
which are jointly trained with L and J . Normally,
to incorporate target data, we can minimize the
cross entropy loss between the true label distri-
butions y(t)
and the predicted label distributions
i

i = F(ξ(t)
˜y(t)
; θf ) over target samples. The chal-
i
lenge here is that y(t)
is unknown, and thus we
i
attempt to estimate it via semi-supervised learn-
ing. We use entropy minimization and bootstrap-
ping for this purpose. We will later show in our
experiments that both methods are effective, and
jointly employing them overall yields the best re-
sults.
Entropy Minimization: In this method, y(t)
is
i
estimated as the predicted label distribution ˜y(t)
,
i
which is a function of θg and θf . The loss can thus
be written as

Γ = −

1
nt

nt(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(t)
˜y(t)

i (j)

(5)

Assume the domain discrepancy can be effectively
reduced through feature adaptation, by minimiz-
ing the entropy penalty, training of the classiﬁer
is inﬂuenced by the unlabeled target data and will
generally maximize the margins between the tar-
get examples and the decision boundaries, increas-
ing the prediction conﬁdence on the target domain.

training.

Self-ensemble Bootstrapping: Another way to
estimate y(t)
corresponds to bootstrapping. The
i
idea is to estimate the unknown labels as the
predictions of the model learned from the pre-
vious round of
Bootstrapping has
been explored for domain adaptation in previous
works (Jiang and Zhai, 2007; Wu et al., 2009).
However, in their methods, domain discrepancy
was not explicitly minimized via feature adap-
tation. Applying bootstrapping or other semi-
supervised learning techniques in this case may
worsen the results as the classiﬁer can perform
quite bad on the target data.

Inspired by the ensembling method proposed
in (Laine and Aila, 2017), we estimate y(t)
by
i
forming ensemble predictions of labels during
training, using the outputs on different training
epochs. The loss is formulated as follows:

Ω = −

1
N

N
(cid:88)

C
(cid:88)

i=1

j=1

˜z(s,t)
i

(j) log ˜y(s,t)

(j)

i

(6)

where ˜z denotes the estimated labels computed on
the ensemble predictions from different epochs.
It serves
The loss is applied on all documents.
for bootstrapping on the unlabeled target data, and
it also serves as a regularization that encourages

Algorithm 1 Pseudocode for training DAS
Require: Ds, Dt, G, F
Require: α = ensembling momentum, 0 ≤ α < 1
Require: w(t) = weight ramp-up function

Z ← 0[N ×C]
˜z ← 0[N ×C]
for t ∈ [1, max-epochs] do

for each minibatch B(s), B(t), B(u) in

Ds, Dt, {x(s,t)

}|N

i=1 do

i
compute loss L on [xi∈B(s), yi∈B(s)]
compute loss J on [xi∈B(s), xj∈B(t)]
compute loss Γ on xi∈B(t)
compute loss Ω on [xi∈B(u), ˜zi∈B(u)]
overall-loss ← L + λ1J + λ2Γ + w(t)Ω
update network parameters

i ← F(G(xi)), for i ∈ N

end for
Z(cid:48)
Z ← αZ + (1 − α)Z(cid:48)
˜z ← one-hot-vectors(Z)

end for

the network predictions to be consistent in differ-
ent training epochs. Ω is jointly trained with L,
J , and Γ. Algorithm 1 illustrates the overall train-
ing process of the proposed domain adaptive semi-
supervised learning (DAS) framework.

t

In Algorithm 1, λ1, λ2, and w(t) are weights
to balance the effects of J , Γ, and Ω respectively.
λ1 and λ2 are constant hyper-parameters. We set
max-epochs )2]λ3 as a Gaus-
w(t) = exp[−5(1 −
sian curve to ramp up the weight from 0 to λ3.
This is to ensure the ramp-up of the bootstrapping
loss component is slow enough in the beginning
of the training. After each training epoch, we com-
pute Z(cid:48)
i which denotes the predictions made by the
network in current epoch, and then the ensemble
prediction Zi is updated as a weighted average of
the outputs from previous epochs and the current
epoch, with recent epochs having larger weight.
For generating estimated labels ˜zi, Zi is converted
to a one-hot vector where the entry with the maxi-
mum value is set to one and other entries are set to
zeros. The self-ensemble bootstrapping is a gener-
alized version of bootstrappings that only use the
outputs from the previous round of training (Jiang
and Zhai, 2007; Wu et al., 2009). The ensemble
prediction is likely to be closer to the correct, un-
known labels of the target data.

Domain
Book

Electronics

Beauty

Music

#Pos
2000
4824
2000
4817
2000
4709
2000
4441

#Neg
2000
513
2000
694
2000
616
2000
785

#Neu Total
6000
2000
6000
663
6000
2000
6000
489
6000
2000
6000
675
6000
2000
6000
774

Set 1
Set 2
Set 1
Set 2
Set 1
Set 2
Set 1
Set 2

(a) Small-scale datasets

Domain
IMDB
Yelp
Cell Phone
Baby

#Pos
55,242
155,625
148,657
126,525

#Neg
11,735
29,597
24,343
17,012

#Neu
17,942
45,941
21,439
17,255

Total
84,919
231,163
194,439
160,792

(b) Large-scale datasets

Table 1: Summary of datasets.

4 Experiments

4.1 CNN Encoder Implementation

We have left the feature encoder G unspeciﬁed,
In
for which, a few options can be considered.
our implementation, we adopt a one-layer CNN
structure from previous works (Kim, 2014; Yu and
Jiang, 2016), as it has been demonstrated to work
well for sentiment classiﬁcation tasks. Given a re-
view document x = (x1, x2, ..., xn) consisting of
n words, we begin by associating each word with
a continuous word embedding (Mikolov et al.,
2013) ex from an embedding matrix E ∈ RV ×d,
where V is the vocabulary size and d is the embed-
ding dimension. E is jointly updated with other
network parameters during training. Given a win-
dow of dense word embeddings ex1, ex2, ..., exl,
the convolution layer ﬁrst concatenates these vec-
tors to form a vector ˆx of length ld and then the
output vector is computed by Equation (7):

Conv(ˆx) = f (W · ˆx + b)

(7)

θg = {W, b} is the parameter set of the en-
coder G and is shared across all windows of the
sequence. f is an element-wise non-linear activa-
tion function. The convolution operation can cap-
ture local contextual dependencies of the input se-
quence and the extracted feature vectors are sim-
ilar to n-grams. After the convolution operation
is applied to the whole sequence, we obtain a list
of hidden vectors H = (h1, h2, ..., hn). A max-
over-time pooling layer is applied to obtain the ﬁ-
nal vector representation ξ of the input document.

4.2 Datasets and Experimental Settings

Existing benchmark datasets such as the Amazon
benchmark (Blitzer et al., 2007) typically remove

reviews with neutral labels in both domains. This
is problematic as the label information of the tar-
get domain is not accessible in an unsupervised
domain adaptation setting. Furthermore, remov-
ing neutral instances may bias the dataset favor-
ably for max-margin-based algorithms like ours,
since the resulting dataset has all uncertain labels
removed, leaving only high conﬁdence examples.
Therefore, we construct new datasets by ourselves.
The results on the original Amazon benchmark is
qualitatively similar, and we present them in Ap-
pendix A for completeness since most of previous
works reported results on it.

Small-scale datasets: Our new dataset was de-
rived from the large-scale Amazon datasets2 re-
leased by McAuley et al. (2015). It contains four
domains3: Book (BK), Electronics (E), Beauty
(BT), and Music (M). Each domain contains two
datasets. Set 1 contains 6000 instances with ex-
actly balanced class labels, and set 2 contains
6000 instances that are randomly sampled from
the large dataset, preserving the original label dis-
tribution, which we believe better reﬂects the label
distribution in real life. The examples in these two
sets do not overlap. Detailed statistics of the gen-
erated datasets are given in Table 1a.

In all our experiments on the small-scale
datasets, we use set 1 of the source domain as the
only source with sentiment label information dur-
ing training, and we evaluate the trained model on
set 1 of the target domain. Since we cannot con-
trol the label distribution of unlabeled data during
training, we consider two different settings:
Setting (1): Only set 1 of the target domain is used
as the unlabeled set. This tells us how the method
performs in a condition when the target domain
has a close-to-balanced label distribution. As we
also evaluate on set 1 of the target domain, this is
also considered as a transductive setting.
Setting (2): Set 2 from both the source and target
domains are used as unlabeled sets. Since set 2 is
directly sampled from millions of reviews, it better
reﬂects real-life sentiment distribution.

Large-scale datasets: We further conduct ex-
periments on four much larger datasets: IMDB4

2http://jmcauley.ucsd.edu/data/amazon/
3The original reviews were rated on a 5-point scale. We
label them with rating < 3, > 3, and = 3 as negative, posi-
tive, and neutral respectively.

4IMDB is rated on a 10-point scale, and we label reviews
with rating < 5, > 6, and = 5/6 as negative, positive, and
neutral respectively.

(I), Yelp2014 (Y), Cell Phone (C), and Baby
(B). IMDB and Yelp2014 were previously used
in (Tang et al., 2015; Yang et al., 2017). Cell
phone and Baby are from the large-scale Amazon
dataset (McAuley et al., 2015; He and McAuley,
2016). Detailed statistics are summarized in Ta-
ble 1b. We keep all reviews in the original datasets
and consider a transductive setting where all target
examples are used for both training (without la-
bel information) and evaluation. We perform sam-
pling to balance the classes of labeled source data
in each minibatch B(s) during training.

4.3 Selection of Development Set

Ideally, the development set should be drawn from
the same distribution as the test set. However, un-
der the unsupervised domain adaptation setting,
we do not have any labeled target data at training
phase which could be used as development set. In
all of our experiments, for each pair of domains,
we instead sample 1000 examples from the train-
ing set of the source domain as development set.
We train the network for a ﬁxed number of epochs,
and the model with the minimum classiﬁcation er-
ror on this development set is saved for evaluation.
This approach works well on most of the problems
since the target domain is supposed to behave like
the source domain if the domain difference is ef-
fectively reduced.

Another problem is how to select the values for
hyper-parameters. If we tune λ1 and λ2 directly
on the development set from the source domain,
most likely both of them will be set to 0, as un-
labeled target data is not helpful for improving in-
domain accuracy of the source domain. Other neu-
ral network models also have the same problem for
hyper-parameter tuning. Therefore, our strategy is
to use the development set from the target domain
to optimize λ1 and λ2 for one problem (e.g., we
only do this on E→BK), and ﬁx their values on the
other problems. This setting assumes that we have
at least two labeled domains such that we can op-
timize the hyper-parameters, and then we ﬁx them
for other new unlabeled domains to transfer to.

4.4 Training Details and Hyper-parameters

We initialize word embeddings using the 300-
dimension GloVe vectors supplied by Pennington
et al., (2014), which were trained on 840 billion
tokens from the Common Crawl. For each pair of
domains, the vocabulary consists of the top 10000
most frequent words. For words in the vocabulary

but not present in the pre-trained embeddings, we
randomly initialize them.

We set hyper-parameters of

the CNN en-
coder following previous works (Kim, 2014; Yu
and Jiang, 2016) without speciﬁc tuning on our
datasets. The window size is set to 3 and the size
of the hidden layer is set to 300. The nonlinear
activation function is Relu. For regularization, we
also follow their settings and employ dropout with
probability set to 0.5 on ξi before feeding it to the
output layer F, and constrain the l2-norm of the
weight vector θf , setting its max norm to 3.

On the small-scale datasets and the Aamzon
benchmark, λ1 and λ2 are set to 200 and 1,
respectively,
tuned on the development set of
task E→BK under setting 1. On the large-scale
datasets, λ1 and λ2 are set to 500 and 0.2, re-
tuned on I→Y. We use a Gaussian
spectively,
curve w(t) = exp[−5(1 − t
)2]λ3 to ramp up
the weight of the bootstrapping loss Ω from 0 to
λ3, where tmax denotes the maximum number of
training epochs. We train 30 epochs for all exper-
iments. We set λ3 to 3 and α to 0.5 for all experi-
ments.

tmax

The batch size is set to 50 on the small-scale
datasets and the Amazon benchmark. We increase
the batch size to 250 on the large-scale datasets to
reduce the number of iterations. RMSProp opti-
mizer with learning rate set to 0.0005 is used for
all experiments.

4.5 Models for Comparison

We compare with the following baselines:

(1) Naive: A non-domain-adaptive baseline
with bag-of-words representations and SVM clas-
siﬁer trained on the source domain.

(2) mSDA (Chen et al., 2012): This is the state-
of-the-art method based on discrete input features.
Top 1000 bag-of-words features are kept as pivot
features. We set the number of stacked layers to 3
and the corruption probability to 0.5.

(3) NaiveNN: This is a non-domain-adaptive
CNN trained on source domain, which is a variant
of our model by setting λ1, λ2, and λ3 to zeros.

(4) AuxNN (Yu and Jiang, 2016): This is a neu-
ral model that exploits auxiliary tasks, which has
achieved state-of-the-art results on cross-domain
sentiment classiﬁcation. The sentence encoder
used in this model is the same as ours.

(5) ADAN (Chen et al., 2017): This method
exploits adversarial training to reduce representa-

(a) Accuracy on the small-scale dataset under setting 1.

(b) Accuracy on the small-scale dataset under setting 2.

(c) Macro-F1 on the large-scale dataset.

Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is
signiﬁcantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test.

tion difference between domains. The original pa-
per uses a simple feedforward network as encoder.
For fair comparison, we replace it with our CNN-
based encoder. We train 5 iterations on the dis-
criminator per iteration on the encoder and senti-
ment classiﬁer as suggested in their paper.

(6) MMD: MMD has been widely used for min-
imizing domain discrepancy on images. In those
works (Tzeng et al., 2014; Long et al., 2017), vari-
ants of deep CNNs are used for encoding images
and the MMDs of multiple layers are jointly mini-
mized. In NLP, adding more layers of CNNs may
not be very helpful and thus those models from
image-related tasks can not be directly applied
to our problem. To compare with MMD-based
method, we train a model that jointly minimize
the classiﬁcation loss L on the source domain and
i=1} and {ξ(t)
MMD between {ξ(s)
|nt
|ns
i=1}. For
computing MMD, we use a Gaussian RBF which
is a common choice for characteristic kernel.

i

i

In addition to the above baselines, we also show
results of different variants of our model. DAS
as shown in Algorithm 1 denotes our full model.
DAS-EM denotes the model with only entropy

minimization for semi-supervised learning (set
λ3 = 0). DAS-SE denotes the model with only
self-ensemble bootstrapping for semi-supervised
learning (set λ2 = 0). FANN (feature-adaptation
neural network) denotes the model without semi-
supervised learning performed (set both λ2 and λ3
to zeros).

4.6 Main Results

Figure 15 shows the comparison of adaptation re-
sults (see Appendix B for the exact numerical
numbers). We report classiﬁcation accuracy on
the small-scale dataset. For the large-scale dataset,
macro-F1 is instead used since the label distribu-
tion in the test set is extremely unbalanced. Key
observations are summarized as follows. (1) Both
DAS-EM and DAS-SE perform better in most
cases compared with ADAN, MDD, and FANN,
in which only feature adaptation is performed.
This demonstrates the effectiveness of the pro-

5We exclude results of Naive, mSDA and AuxNN on the
large-scale dataset. Both Naive and mSDA have difﬁculties
to scale up to the large dataset. AuxNN relies on manually
selecting positive and negative pivots before training.

Figure 2: Accuracy vs. percentage of unlabeled target training examples.

Figure 3: Accuracy vs. number of labeled target training examples.

posed domain adaptive semi-supervised learning
framework. DAS-EM is more effective than DAS-
SE in most cases, and the full model DAS with
both techniques jointly employed overall has the
(2) When comparing the two
best performance.
settings on the small-scale dataset, all domain-
adaptive methods6 generally perform better under
setting 1. In setting 1, the target examples are bal-
anced in classes, which can provide more diverse
opinion-related features. However, when consid-
ering unsupervised domain adaptation, we should
not presume the label distribution of the unlabeled
data. Thus, it is necessary to conduct experiments
using datasets that reﬂect real-life sentiment dis-
tribution as what we did on setting2 and the large-
scale dataset. Unfortunately, this is ignored by
most of previous works. (3) Word-embeddings are
very helpful, as we can see even NaiveNN can sub-
stantially outperform mSDA on most tasks.

To see the effect of semi-supervised learning
alone, we also conduct experiments by setting
λ1 = 0 to eliminate the effect of feature adapta-
tion. Both entropy minimization and bootstrap-
ping perform very badly in this setting. En-
tropy minimization gives almost random predic-
tions with accuracy below 0.4, and the results
of bootstrapping are also much lower compared
to NaiveNN. This suggests that the feature adap-
tation component is essential. Without it,
the
learned target representations are less meaning-
ful and discriminative. Applying semi-supervised

6Results of Naive and NaiveNN do not change under both

settings as they are only trained on the source domain.

learning in this case is likely to worsen the results.

4.7 Further Analysis

In Figure 2, we show the change of accuracy with
respect to the percentage of unlabeled data used
for training on three particular problems under set-
ting 1. The value at x = 0 denotes the accuracies
of NaiveNN which does not utilize any target data.
For DAS, we observe a nonlinear increasing trend
where the accuracy quickly improves at the be-
ginning, and then gradually stabilizes. For other
methods, this trend is less obvious, and adding
more unlabeled data sometimes even worsen the
results. This ﬁnding again suggests that the pro-
posed approach can better exploit the information
from unlabeled data.

We also conduct experiments under a setting
with a small number of labeled target examples
available. Figure 3 shows the change of accuracy
with respect to the number of labeled target exam-
ples added for training. We can observe that DAS
is still more effective under this setting, while the
performance differences to other methods gradu-
ally decrease with the increasing number of la-
beled target examples.

4.8 CNN Filter Analysis

In this subsection, we aim to better understand
DAS by analyzing sentiment-related CNN ﬁlters.
To do that, 1) we ﬁrst select a list of the most re-
lated CNN ﬁlters for predicting each sentiment la-
bel (positive, negative neutral). Those ﬁlters can
be identiﬁed according to the learned weights θf

best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

(a) NaiveNN

(b) FANN

(c) DAS

Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the
5 most positive-sentiment-related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

of the output layer F. Higher weight indicates
stronger relatedness. 2) Recall that in our im-
plementation, each CNN ﬁlter has a window size
of 3 with Relu activation. We can thus represent
each selected ﬁlter as a ranked list of trigrams with
highest activation values.

We analyze the CNN ﬁlters

learned by
NaiveNN, FANN and DAS respectively on task
E→BT under setting 1. We focus on E→BT for
study because electronics and beauty are very dif-
ferent domains and each of them has a diverse
set of domain-speciﬁc sentiment expressions. For
each method, we identify the top 10 most related
ﬁlters for each sentiment label, and extract the top
trigrams of each selected ﬁlter on both source and
target domains. Since labeled source examples are
used for training, we ﬁnd the ﬁlters learned by the
three methods capture similar expressions on the
source domain, containing both domain-invariant
and domain-speciﬁc trigrams. On the target do-
main, DAS captures more target-speciﬁc expres-
sions compared to the other two methods. Due
to space limitation, we only present a small sub-
set of positive-sentiment-related ﬁlters in Table 2.
The complete results are provided in Appendix C.
From Table 2, we can observe that the ﬁlters
learned by NaiveNN are almost unable to cap-
ture target-speciﬁc sentiment expressions, while
FANN is able to capture limited target-speciﬁc
words such as “clean” and “scent”. The ﬁlters
learned by DAS are more domain-adaptive, cap-
turing diverse sentiment expressions in the target
domain.

5 Conclusion

In this work, we propose DAS, a novel frame-
work that jointly performs feature adaptation and
semi-supervised learning. We have demonstrated
through multiple experiments that DAS can better
leverage unlabeled data, and achieve substantial
improvements over baseline methods. We have
also shown that feature adaptation is an essen-
tial component, without which, semi-supervised
learning is not able to function properly. The pro-
posed framework could be potentially adapted to
other domain adaptation tasks, which is the focus
of our future studies.

References

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: domain adaptation for sentiment classiﬁ-
In Annual Meeting of the Association for
cation.
Computational Linguistics.

Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In The 29th Interna-
tional Conference on Machine Learning.

Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie,
and Kilian Weinberger. 2017. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁer. In Arxiv e-prints arXiv:1606.01614.

Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classiﬁers for
text classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: a deep learning approach. In The 28th
International Conference on Machine Learning.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
In
supervised learning by entropy minimization.
Neural Information Processing Systems.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander Smola. 2012. A
kernel two-sample test. Journal of Machine Learn-
ing Research, 13:723–773.

Ruining He and Julian McAuley. 2016. Ups and
downs: modeling the visual evolution of fashion
In
trends with one-class collaborative ﬁltering.
WWW.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classiﬁcation. In ACL.

Jing Jiang and ChengXiang Zhai. 2007.

Instance
weighting for domain adaptation in NLP. In Annual
Meeting of the Association for Computational Lin-
guistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Conference on Empirical
Methods in Natural Language Processing.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In International
Conference on Learning Representation.

Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial mem-
ory network for cross-domain sentiment classiﬁca-
tion. In The 26th International Joint Conference on
Artiﬁcial Intelligence.

Mingsheng Long, Yue Cao,

Jianmin Wang, and
Michael I. Jordan. 2015. Learning transferable fea-
In Interna-
tures with deep adaptation networks.
tional Conference on Machine Learning.

Mingsheng Long, Han Zhu,

Jianmin Wang, and
Michael I. Jordan. 2017. Deep transfer learning with
joint adaptation networks. In International Confer-
ence on Machine Learning.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
ommendations on styles and substitutes. In The 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Neural Information Processing Systems.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classiﬁcation via spectral feature alignment. In
The 19th International World Wide Web Conference.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representation of users and products for
document level sentiment classiﬁcation. In Annual
Meeting of the Association for Computational Lin-
guistics.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
maximizing for domain invariance. In Arxiv e-prints
arXiv:1412.3474.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Conference on Empirical Meth-
ods in Natural Language Processing.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Annual
Meeting of the Association for Computational Lin-
guistics.

Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In Conference on
Empirical Methods in Natural Language Process-
ing.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Annual Meeting of the Association
for Computational Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
In Conference on Empirical
timent classiﬁcation.
Methods in Natural Language Processing.

Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-
aohua Tony Hu. 2015. Linking heterogeneous input
features with pivots for domain adaptation. In The
24th International Joint Conference on Artiﬁcial In-
telligence.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016. Bi-transferring deep neural
networks for domain adaptation. In Annual Meeting
of the Association for Computational Linguistics.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: transfer learning with deep autoencoders.
In The 24th International Joint Conference on Arti-
ﬁcial Intelligence.

S

D
E
K
B
E
K
B
D
K
B
D
E

T

B
B
B
D
D
D
E
E
E
K
K
K

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE
82.10∗
77.75
79.60
82.35
79.75
82.15∗
75.80
80.05
85.95
79.45∗
79.50
83.80

82.00∗
80.25∗
79.95
82.65
81.40∗
81.65∗
80.25∗
81.40∗
85.70
81.55∗
80.80
84.50

81.05
78.65
79.70
82.00
80.10
79.35
76.45
80.20
85.75
75.20
79.70
81.75

80.30
77.25
79.20
81.65
79.55
76.90
76.75
79.25
85.60
77.55
78.00
83.85

81.70
78.55
79.25
82.30
79.70
80.45
77.60
79.70
86.85
76.10
77.35
83.95

80.80
78.00
77.85
81.75
80.65
78.90
76.40
77.55
84.05
78.10
80.05
84.15

78.50
76.15
75.65
80.60
76.30
76.05
75.55
76.00
84.20
75.95
76.30
84.45

75.20
68.85
70.00
77.15
69.50
71.40
72.15
71.65
79.75
73.50
72.00
82.80

81.10
77.95
77.75
80.80
77.00
79.35
76.20
76.60
84.85
77.40
78.55
84.95

Average

73.66

77.98

79.38

79.85

80.29

80.00

79.65

81.84

80.68

DAS
82.05∗
80.00∗
80.05∗
82.75∗
80.15
81.40∗
81.15∗
81.55∗
85.80
82.25∗
81.50∗
84.85

81.96

Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations
are reported for each neural method. ∗ indicates that the proposed method (DAS-EM, DAS-SE, DAS) is
signiﬁcantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

A Results on Amazon Benchmark

C CNN Filter Analysis Full Results

As mentioned in Section 4.8, we conduct CNN ﬁl-
ter analysis on NaiveNN, FANN, and DAS. For
each method, we identify the top 10 most related
ﬁlters for positive, negative, neutral sentiment la-
bels respectively, and then represent each selected
ﬁlter as a ranked list of trigrams with the highest
activation values on it. Table 5, 6, 7 in the fol-
lowing pages illustrate the trigrams from the tar-
get domain (beauty) captured by the selected ﬁl-
ters learned on E→BT for each method.

We can observe that compared to NaiveNN and
FANN, DAS is able to capture a more diverse set
of relevant sentiment expressions on the target do-
main for each sentiment label. This observation is
consistent with our motivation. Since NaiveNN,
FANN and other baseline methods solely train
the sentiment classiﬁer on the source domain, the
learned encoder is not able to produce discrimina-
tive features on the target domain. DAS addresses
this problem by reﬁning the classiﬁer on the tar-
get domain with semi-supervised learning, and
the overall objective forces the encoder to learn
feature representations that are not only domain-
invariant but also discriminative on both domains.

Most previous works (Blitzer et al., 2007; Pan
et al., 2010; Glorot et al., 2011; Chen et al.,
2012; Zhou et al., 2016) carried out experiments
on the Amazon benchmark released by Blitzer
et al.
(2007). The dataset contains 4 different
domains: Book (B), DVDs (D), Electronics (E),
and Kitchen (K). Following their experimental set-
tings, we consider the binary classiﬁcation task to
predict whether a review is positive or negative
on the target domain. Each domain consists of
1000 positive and 1000 negative reviews respec-
tively. We also allow 4000 unlabeled reviews to
be used for both the source and the target domains,
of which the positive and negative reviews are bal-
anced as well, following the settings in previous
works. We construct 12 cross-domain sentiment
classiﬁcation tasks and split the labeled data in
each domain into a training set of 1600 reviews
and a test set of 400 reviews. The classiﬁer is
trained on the training set of the source domain
and is evaluated on the test set of the target do-
main. The comparison results are shown in Ta-
ble 3.

B Numerical Results of Figure 1

Due to space limitation, we only show results in
ﬁgures in the paper. All numerical numbers used
for plotting Figure 1 are presented in Table 4. We
can observe that DAS-EM, DAS-SE, and DAS
all achieve substantial improvements over baseline
methods under different settings.

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

BK 49.07
E
BK 48.17
BT
M BK 45.20
46.43
E
BK
53.63
E
BT
37.93
E
M
45.57
BK BT
48.43
E
BT
39.42
M BT
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

BK 49.07
E
BK 48.17
BT
M BK 45.20
E
46.43
BK
E
BT
53.63
37.93
M
E
BK BT
45.57
48.43
E
BT
M BT
39.43
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

55.13
53.53
49.22
48.22
57.32
38.13
50.77
53.13
39.37
47.88
47.88
49.62
49.18

52.88
47.65
48.33
47.07
55.12
37.40
49.63
51.98
37.73
45.97
45.12
45.78
47.06

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

60.62
59.86
60.43
48.45
60.98
49.60
48.67
55.58
48.65
48.87
51.19
53.11
53.84

57.72
58.46
58.15
48.22
59.08
49.43
47.80
54.37
46.92
48.79
52.31
53.55
52.98

63.32
65.62
62.87
47.42
63.13
46.57
46.14
50.98
44.26
51.10
50.23
55.35
53.92

57.07
59.78
58.67
49.48
59.45
47.00
47.52
51.28
45.73
50.20
52.57
54.68
52.79

60.38
59.66
60.20
53.32
60.53
51.55
49.48
54.83
48.35
53.04
51.81
54.43
54.80

57.43
56.17
57.08
45.42
60.24
48.72
45.43
54.92
46.68
48.76
51.50
54.55
52.23

59.59
59.28
57.65
51.27
60.62
47.23
50.24
56.78
48.89
52.35
52.14
53.84
54.15

56.43
57.98
57.75
51.95
58.67
48.92
49.83
55.42
48.48
49.47
48.18
53.41
53.04

66.48∗
66.78∗
69.63∗
58.59∗
65.71∗
55.88∗
49.49
61.53∗
47.65
55.47∗
58.28∗
60.95∗
59.74

57.78
61.17∗
58.62
54.51∗
61.27
51.28∗
53.72∗
53.10
47.18
52.37∗
53.63∗
56.24∗
55.07

(a) Accuracy on the small-scale dataset under setting 1

DAS
67.12∗
66.53
70.31∗
58.73∗
66.14∗
55.78∗
51.30∗
60.76∗
50.66∗
55.98∗
59.06∗
60.5∗
60.24

55.20
63.32∗
60.77∗
53.92∗
59.83
52.88∗
54.67∗
56.05∗
49.73∗
53.52∗
55.38∗
56.02∗
55.94

62.37
61.17
65.24∗
55.15∗
61.78
53.22∗
54.23∗
59.52∗
50.67∗
55.13∗
55.60∗
56.90∗
57.58

58.93
60.17∗
58.25
52.47∗
61.42
51.18∗
51.23∗
56.43∗
51.57∗
52.68∗
52.25
56.23∗
55.23

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

DAS

(b) Accuracy on the small-scale dataset under setting 2

S

T

Y
I
C
I
B
I
I
Y
C
Y
B
Y
I
C
Y
C
B
C
I
B
Y
B
B
C
Average

NaiveNN ADAN MMD FANN DAS-EM DAS-SE
56.66∗
55.18
54.30
58.72∗
59.14
58.43
51.97∗
54.67∗
59.98
50.81
52.95
58.12
55.91

55.04
57.27∗
57.31∗
57.92∗
61.17
59.94∗
53.46∗
53.48
59.84
48.84
52.87
57.74
56.24

54.16
53.35
51.40
56.52
60.81
58.77
50.49
53.12
61.23
47.35
54.43
60.52
55.18

54.46
53.07
52.39
56.30
56.02
55.72
51.04
51.86
60.19
48.17
53.54
55.56
54.02

55.52
55.07
54.64
52.57
60.70
58.42
47.27
52.53
59.91
46.34
50.82
59.99
54.48

53.01
51.84
45.85
55.46
61.22
56.86
50.38
53.87
59.48
50.05
54.73
60.47
54.43

DAS
58.54∗
57.28∗
58.02∗
58.92∗
61.39
61.87∗
53.38∗
55.44∗
59.76
48.84
52.91
59.75
57.18

(c) Macro-F1 scores on the large-scale dataset

Table 4: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signiﬁcantly
better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

1
best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

2
highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

3
nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

4
beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

5
since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

6
store-and-am
cleanser-and-am
olay-and-am
daily-and-need
shower-and-noticed

7
ofﬁce-setting-thanks
locks-shimmering-color
dirty-blonde-color
victoria-secrets-gorgeous
dirty-pinkish-color

8
car-washes-!
price-in-stores
products-are-priced
car-and-burning
from-our-store

(a) NaiveNN

9
speed-is-perfect
buttons-are-perfect
unit-is-superb
spray-is-perfect
coverage-is-excellent

10
!-i-recommend
!-i-highly
shower-i-slather
spots-i-needed
best-i-use

1
prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

2
so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

3
purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

4
feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

5
are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

6
shower-or-cleaning
water-onto-my
bleach-your-towels
pump-onto-my
water-great-for

8

7
deﬁnitely-purchase-again more-affordable-price
deﬁnitely-buy-again
perfect-for-my
deﬁnitely-order-again
super-happy-to

a-perfect-length
an-exceptional-value
’ve-enjoyed-it
pretty-decent-layer

9
absolutely-wonderful-!
perfect-for-running
concealer-for-my
moisturizing-for-my
super-glue-even

10
felt-cleaner-than
ﬂat-iron-through
rubbed-grease-on
deeply-cleans-my
being-cleaner-after

1
bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

2
love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

3
feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

4
cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

5
excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

6
shower-lather-wonderfully
affordable-cleans-nicely
peels-great-price
daughter-loves-this
cleans-great-smells

7
highly-recommend-!
deﬁnitely-recommend-!
love-love-!
highly-advise-!
time-advise-!

8
excellent-fragrance-and
fantastic-for-daytime
wonderfully-moisturizing-and
lathers-great-cleans
delightful-shampoo-works

10
9
its-unique-smoothing
forgeous-gragrance-mist
smooth-luxurious-texture wonderful-bedtime-scent
’s-extremely-gentle
’s-affordable-combination
absorbs-quite-well

love-essie-polish
perfect-beautiful-shimmer
fantastic-coverage-hydrates

(b) FANN

(c) DAS

Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
pads-ruined-my
highly-disappointed-and
dryers-blew-my
completely-worthless-didn’t
am-disappointed-and

2
simply-threw-out
reviewer-pointed-out
extracts-broke-into
actually-threw-out
clips-barely-keep

3
hours-after-trying
minutes-after-rinsing
disappointed-after-trying
lips-after-trying
dry-after-shampooing

4
junk-drawer-∗
refund-time-!
total-fake-wen
waste-your-time
total-fail-!

5
contacted-manufacturer-about
minutes-not-worth
’ve-owned-this
hour-unless-it
results-they-claim

6
were-awful-garbage
what-awful-garbage
and-utter-waste
are-absolute-garbage
piece-of-junk

7
two-failed-attempts
a-mistake-save
a-deﬁnite-return
a-pathetic-limp
a-total-disappointment

8
auto-ship-sent
am-returning-to
am-unable-to
am-pale-ghost
got-returned-and

10
broke-don’t-ﬁx
sent-me-expired

9
refund-and-dispose
refund-spend-your
wouldn’t-recommend-! wearing-false-eyelashes
not-buy-dunhill
not-worth-returning

a-temporary-ﬁx
a-disappointment-cheap

(a) NaiveNN

2
the-worse-mascaras

1
nasty-sunburn-lol
bother-returning-them it-caused-patchy
fails-miserably-at
minutes-auric-needs
severely-burned-me

lifeless-disaster-enter
it-fails-miserably
feel-worse-leaving

3
stale-very-unhappy
were-horrible-failures
send-this-crap
were-awful-garbage
were-horribly-red

4
actually-hurts-your
didn’t-bother-returning
it-hurts-your
didn’t-exist-in
skin-horribly-after

5
a-return-label
stay-away-completely
like-bug-quit
a-defective-brown
’d-refund-the

6
worse-with-exercise
worse-and-after
unable-to-return
worse-my-face
poorly-in-step

7
not-stink-your
mistake-save-your
nothing-!-by
nothing-happened-!
nothing-save-your

8
it-fails-miserably
is-ineffective-apart
but-horribly-unhealthy
a-pathetic-limp
a-worse-job

9
got-progressively-worse
gave-opposite-result
another-epic-fail
got-horribly-painful
was-downright-painful

10
stopped-working-for
uncomfortable-i-returned
i-am-returning
stopped-working-shortly
not-waterproof-makeup

1
poorly-designed-product
defective-dryer-promising
disgusting-smelling-thing
hurts-your-scalp
hurts-your-hair

2
a-refund-spend
a-refund-save
i-regret-spending
just-wouldn’t-spend
looked-washed-out

3
completely-waste-of
of-junk-*
were-awful-garbage
worthless-waste-of
throwing-money-away

4
smells-disgusting-!
smells-horribly-like
does-not-straighten
’s-false-advertising
a-disgusting-cheap

5
burning-rubber-stench
began-smelling-vomit
reaction-and-wasted
control-and-smelled
using-this-disgusting

6
super-irritating-!
strong-reaction-and
really-burned-and
very-pasty-and
super-streaky-and

7
got-promptly-broke
after-ive-washed
after-several-attempts
this-stuff-stinks
again-i-threw

8
sore-and-painful
is-simply-irritating
tight-and-uncomfortable
drying-and-irritating
goopy-and-unpleasant

9
it-caused-patchy
layer-hydrogenated-alcohols
the-harmful-uva
my-severe-dark
a-allergic-reaction

10
painful-it-hurt
unnecessary-health-risks
uncomfortable-to-wear
stinging-your-eyes
unbearable-to-wear

(b) FANN

(c) DAS

Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
purpose-cologne-splash
other-hanae-mori
the-mavala-peeled
avoid-hair-pulling
cause-rashes-stinging

2
okay-cord-was
cocamide-dea-is
coily-conditioner-is
ﬂaky-dandruff-is
quickly-cord-is

3
hands-feet-elbows
been-sealed-tight
stainless-steel-blackhead
severely-tight-chest
thick-nasty-callouses

4
aggressive-in-general
pimples-in-general
biotin-in-general
dimethicone-is-terrible
but-in-general

5
but-its-okay
it-moisturizes-okay
but-moisturizes-keeps
but-don’t-expect
it-lathers-ok

6
pretty-damaged-from
daughter-suffers-from
teenager-suffers-from
tissue-damage-during
the-damage-on

7
darker-olive-complexion
stronger-healthier-or
natural-ingredient-however
vitamin-enriched-color
natural-ingredients-∗

8
doesn’t-mind-pushing
kinda-doesn’t-its
kinda-kinky-coily
okay-job-of
intended-purpose-that

9
producto-por-los
unstuck-frownies-∗
they-push-∗
uva-rays-uva
tend-to-slip

10
feeling-didn’t-last
curls-didn’t-last
extra-uv-protection
garnier-fructis-curl
the-mavala-after

(a) NaiveNN

1
worse-and-after
worse-before-improving
unable-to-return
unless-your-entire
horrible-in-execution

2
maybe-a-refund
ok-mask-i
ok-pining-it
ok-try-i
ok-tho-i

3
very-disappointing-waste
ok-but-clean
ok-but-will
ok-but-didn’t
ok-nothing-special

4
my-ears-are
my-neck-line
cause-unsightly-beads
my-sporadic-line
your-ear-is

5
pretty-neutral-neither
ok-so-if
ok-during-pregnancy
kinda-annoying-if
ok-this-seems

6
uncomfortable-i-returned
weak-they-bend
claimed-faulty-∗
suffers-from-wind
as-defective-∗

7
sticky-lathers-and
quickly-deep-cleans
but-elegant-bottle
beat-the-price
and-reasonably-priced

8
some-fading-when
real-disappointment-the
especially-noticeable-after
progressively-worse-during
style-unfortunately-the

9
are-very-painful
are-less-painful
are-a-pain
about-sum-damage
offered-no-pain

10
its-also-convenient
that-also-my
that-may-make
that-allows-your
its-helpful-to

1
’m-kinda-pale
a-terrible-headache
but-kinda-annoying
’m-kinda-mad
i-kinda-stopped

2
darker-but-nope
gray-didn’t-cover
makeup-doesn’t-sweat
dark-spots-around
moist-but-thats

3
ok-but-horrible
ok-but-didn’t
okay-but-doesn’t
okay-however-it
unfortunately-straight

4
noticeable-i-avoid
however-i-lean
but-otherwise-ok
but-im-deciding
however-i-prefer

5
same-result-mediocre
it-caused-patchy
doesn’t-cause-ﬂare
the-harmful-uva
rather-unpleasant-smell

6
kinda-annoying-if
pretty-bad-breakage
my-slight-discoloration
smells-kinda-bad
look-kinda-crappy

7
brutal-winter-however
summer-color-however
beige-shade-however
is-okay-however
bit-greasy-however

8
higher-rating-because
slight-burnt-rubber
noticeable-tan-since
somewhat-pale-affect
kinda-pale-so

9
nothing-for-odor
kinda-recommend-this
not-recommend-if
noticeable-but-non
nothing-special-moderate

10
but-darker-*
slightly-darker-shade
somewhat-pale-affect
but-somewhat-heavy
bit-dull-heavy

(b) FANN

(c) DAS

Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

Adaptive Semi-supervised Learning for Cross-domain
Sentiment Classiﬁcation

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore
‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg
‡d.dahlmeier@sap.com

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
3
5
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We consider the cross-domain sentiment clas-
siﬁcation problem, where a sentiment classi-
ﬁer is to be learned from a source domain and
to be generalized to a target domain. Our ap-
proach explicitly minimizes the distance be-
tween the source and the target instances in
an embedded feature space. With the differ-
ence between source and target minimized,
we then exploit additional information from
the target domain by consolidating the idea
of semi-supervised learning, for which, we
jointly employ two regularizations – entropy
minimization and self-ensemble bootstrapping
– to incorporate the unlabeled target data for
classiﬁer reﬁnement. Our experimental results
demonstrate that the proposed approach can
better leverage unlabeled data from the target
domain and achieve substantial improvements
over baseline methods in various experimental
settings.

1

Introduction

In practice, it is often difﬁcult and costly to anno-
tate sufﬁcient training data for diverse application
domains on-the-ﬂy. We may have sufﬁcient la-
beled data in an existing domain (called the source
domain), but very few or no labeled data in a
new domain (called the target domain). This issue
has motivated research on cross-domain sentiment
classiﬁcation, where knowledge in the source do-
main is transferred to the target domain in order to
alleviate the required labeling effort.

One key challenge of domain adaptation is that
data in the source and target domains are drawn
from different distributions. Thus, adaptation per-
formance will decline with an increase in distribu-
tion difference. Speciﬁcally, in sentiment analy-
sis, reviews of different products have different vo-
cabulary. For instance, restaurants reviews would
contain opinion words such as “tender”, “tasty”, or

“undercooked” and movie reviews would contain
“thrilling”, “horriﬁc”, or “hilarious”. The intersec-
tion between these two sets of opinion words could
be small which makes domain adaptation difﬁcult.
Several techniques have been proposed for ad-
dressing the problem of domain shifting. The
aim is to bridge the source and target domains
by learning domain-invariant feature representa-
tions so that a classiﬁer trained on a source do-
main can be adapted to another target domain.
In cross-domain sentiment classiﬁcation, many
works (Blitzer et al., 2007; Pan et al., 2010; Zhou
et al., 2015; Wu and Huang, 2016; Yu and Jiang,
2016) utilize a key intuition that domain-speciﬁc
features could be aligned with the help of domain-
invariant features (pivot features). For instance,
“hilarious” and “tasty” could be aligned as both
of them are relevant to “good”.

Despite their promising results,

these works
share two major limitations. First, they highly de-
pend on the heuristic selection of pivot features,
which may be sensitive to different applications.
Thus the learned new representations may not ef-
fectively reduce the domain difference. Further-
more, these works only utilize the unlabeled tar-
get data for representation learning while the sen-
timent classiﬁer was solely trained on the source
domain. There have not been many studies on ex-
ploiting unlabeled target data for reﬁning the clas-
siﬁer, even though it may contain beneﬁcial infor-
mation. How to effectively leverage unlabeled tar-
get data still remains an important challenge for
domain adaptation.

In this work, we argue that the information
from unlabeled target data is beneﬁcial for do-
main adaptation and we propose a novel Domain
Adaptive Semi-supervised learning framework
(DAS) to better exploit it. Our main intuition is
to treat the problem as a semi-supervised learn-
ing task by considering target instances as unla-

beled data, assuming the domain distance can be
effectively reduced through domain-invariant rep-
resentation learning. Speciﬁcally, the proposed
approach jointly performs feature adaptation and
semi-supervised learning in a multi-task learning
setting. For feature adaptation, it explicitly mini-
mizes the distance between the encoded represen-
tations of the two domains. On this basis, two
semi-supervised regularizations – entropy mini-
mization and self-ensemble bootstrapping – are
jointly employed to exploit unlabeled target data
for classiﬁer reﬁnement.

We evaluate our method rigorously under multi-
ple experimental settings by taking label distribu-
tion and corpus size into consideration. The re-
sults show that our model is able to obtain sig-
niﬁcant improvements over strong baselines. We
also demonstrate through a series of analysis that
the proposed method beneﬁts greatly from incor-
porating unlabeled target data via semi-supervised
learning, which is consistent with our motivation.
Our datasets and source code can be obtained from
https://github.com/ruidan/DAS.

2 Related Work

Domain Adaptation: The majority of feature
adaptation methods for sentiment analysis rely on
a key intuition that even though certain opinion
words are completely distinct for each domain,
they can be aligned if they have high correlation
with some domain-invariant opinion words (pivot
words) such as “excellent” or “terrible”. Blitzer
et al. (2007) proposed a method based on struc-
tural correspondence learning (SCL), which uses
pivot feature prediction to induce a projected fea-
ture space that works well for both the source and
the target domains. The pivot words are selected in
a way to cover common domain-invariant opinion
words. Subsequent research aims to better align
the domain-speciﬁc words (Pan et al., 2010; He
et al., 2011; Wu and Huang, 2016) such that the
domain discrepancy could be reduced. More re-
cently, Yu and Jiang (2016) borrow the idea of
pivot feature prediction from SCL and extend it
to a neural network-based solution with auxiliary
tasks.
In their experiment, substantial improve-
ment over SCL has been observed due to the use
of real-valued word embeddings. Unsupervised
representation learning with deep neural networks
(DNN) such as denoising autoencoders has also
been explored for feature adaptation (Glorot et al.,

2011; Chen et al., 2012; Yang and Eisenstein,
2014). It has been shown that DNNs could learn
transferable representations that disentangle the
underlying factors of variation behind data sam-
ples.

Although the aforementioned methods aim to
reduce the domain discrepancy, they do not explic-
itly minimize the distance between distributions,
and some of them highly rely on the selection of
pivot features. In our method, we formally con-
struct an objective for this purpose. Similar ideas
have been explored in many computer vision prob-
lems, where the representations of the underlying
domains are encouraged to be similar through ex-
plicit objectives (Tzeng et al., 2014; Ganin and
Lempitsky, 2015; Long et al., 2015; Zhuang et al.,
2015; Long et al., 2017) such as maximum mean
discrepancy (MMD) (Gretton et al., 2012). In NLP
tasks, Li et al. (2017) and Chen et al. (2017) both
proposed using adversarial training framework for
reducing domain difference. In their model, a sub-
network is added as a domain discriminator while
deep features are learned to confuse the discrim-
inator. The feature adaptation component in our
model shares similar intuition with MMD and ad-
versary training. We will show a detailed compar-
ison with them in our experiments.
Semi-supervised Learning: We attempt to treat
domain adaptation as a semi-supervised learning
task by considering the target instances as unla-
beled data. Some efforts have been initiated on
transfer learning from unlabeled data (Dai et al.,
2007; Jiang and Zhai, 2007; Wu et al., 2009).
In our model, we reduce the domain discrep-
ancy by feature adaptation, and thereafter adopt
semi-supervised learning techniques to learn from
unlabeled data. Primarily motivated by (Grand-
valet and Bengio, 2004) and (Laine and Aila,
2017), we employed entropy minimization and
self-ensemble bootstrapping as regularizations to
incorporate unlabeled data. Our experimental re-
sults show that both methods are effective when
jointly trained with the feature adaptation objec-
tive, which conﬁrms to our motivation.

3 Model Description

3.1 Notations and Model Overview

We conduct most of our experiments under an un-
supervised domain adaptation setting, where we
have no labeled data from the target domain. Con-
sider two sets Ds and Dt. Ds = {x(s)
i=1 is
i

i }|ns

, y(s)

i }|nt

from the source domain with ns labeled examples,
where yi ∈ RC is a one-hot vector representation
of sentiment label and C denotes the number of
classes. Dt = {x(t)
i=1 is from the target domain
with nt unlabeled examples. N = ns + nt denotes
the total number of training documents including
both labeled and unlabeled1. We aim to learn a
sentiment classiﬁer from Ds and Dt such that the
classiﬁer would work well on the target domain.
We also present some results under a setting where
we assume that a small number of labeled target
examples are available (see Figure 3).

For the proposed model, we denote G parame-
terized by θg as a neural-based feature encoder that
maps documents from both domains to a shared
feature space, and F parameterized by θf as a
fully connected layer with softmax activation serv-
ing as the sentiment classiﬁer. We aim to learn fea-
ture representations that are domain-invariant and
at the same time discriminative on both domains,
thus we simultaneously consider three factors in
our objective: (1) minimize the classiﬁcation error
on the labeled source examples; (2) minimize the
domain discrepancy; and (3) leverage unlabeled
data via semi-supervised learning.

Suppose we already have the encoded features
of documents {ξ(s,t)
i=1 (see
Section 4.1), the objective function for purpose (1)
is thus the cross entropy loss on the labeled source
examples

= G(x(s,t)

; θg)}|N

i

i

L = −

1
ns

ns(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(s)
y(s)

i (j)

(1)

i

i = F(ξ(s)

where ˜y(s)
; θf ) denotes the predicted la-
bel distribution. In the following subsections, we
will explain how to perform feature adaptation and
domain adaptive semi-supervised learning in de-
tails for purpose (2) and (3) respectively.

3.2 Feature Adaptation

Unlike prior works (Blitzer et al., 2007; Yu and
Jiang, 2016), our method does not attempt to align
domain-speciﬁc words through pivot words.
In
our preliminary experiments, we found that word
embeddings pre-trained on a large corpus are able
to adequately capture this information. As we will

1Note that unlabeled source examples can also be in-
cluded for training. In that case, N = ns + nt + ns(cid:48) where
ns(cid:48) denotes the number of unlabeled source examples. This
corresponds to our experimental setting 2. For simplicity, we
only consider ns and nt in our description.

later show in our experiments, even without adap-
tation, a naive neural network classiﬁer with pre-
trained word embeddings can already achieve rea-
sonably good results.

i }nt

i }|ns

i=1 and {ξ(t)

We attempt to explicitly minimize the distance
between the source and target feature represen-
tations ({ξ(s)
i=1). A few meth-
ods from literature can be applied such as Maxi-
mum Mean Discrepancy (MMD) (Gretton et al.,
2012) or adversary training (Li et al., 2017; Chen
et al., 2017). The main idea of MMD is to esti-
mate the distance between two distributions as the
distance between sample means of the projected
embeddings in Hilbert space. MMD is implicitly
computed through a characteristic kernel, which is
used to ensure that the sample mean is injective,
leading to the MMD being zero if and only if the
distributions are identical. In our implementation,
we skip the mapping procedure induced by a char-
acteristic kernel for simplifying the computation
and learning. We simply estimate the distribution
distance as the distance between the sample means
in the current embedding space. Although this ap-
proximation cannot preserve all statistical features
of the underlying distributions, we ﬁnd it performs
comparably to MMD on our problem. The follow-
ing equations formally describe the feature adap-
tation loss J :

J = KL(gs||gt) + KL(gt||gs)
g(cid:48)
s
(cid:107)g(cid:48)
s(cid:107)1

g(cid:48)
s =

1
ns

ξ(s)
i

gs =

ns(cid:88)

,

i=1

nt(cid:88)

i=1

1
nt

g(cid:48)
t =

ξ(t)
i

,

gt =

g(cid:48)
t
(cid:107)g(cid:48)
t(cid:107)1

(2)

(3)

(4)

s and g(cid:48)

L1 normalization is applied on the mean represen-
tations g(cid:48)
t, rescaling the vectors such that
all entries sum to 1. We adopt a symmetric ver-
sion of KL divergence (Zhuang et al., 2015) as the
distance function. Given two distribution vectors
P, Q ∈ Rk, KL(P||Q) = (cid:80)k

i=1 P(i) log( P(i)

Q(i) ).

3.3 Domain Adaptive Semi-supervised

Learning (DAS)

We attempt to exploit the information in target
data through semi-supervised learning objectives,
which are jointly trained with L and J . Normally,
to incorporate target data, we can minimize the
cross entropy loss between the true label distri-
butions y(t)
and the predicted label distributions
i

i = F(ξ(t)
˜y(t)
; θf ) over target samples. The chal-
i
lenge here is that y(t)
is unknown, and thus we
i
attempt to estimate it via semi-supervised learn-
ing. We use entropy minimization and bootstrap-
ping for this purpose. We will later show in our
experiments that both methods are effective, and
jointly employing them overall yields the best re-
sults.
Entropy Minimization: In this method, y(t)
is
i
estimated as the predicted label distribution ˜y(t)
,
i
which is a function of θg and θf . The loss can thus
be written as

Γ = −

1
nt

nt(cid:88)

C
(cid:88)

i=1

j=1

i (j) log ˜y(t)
˜y(t)

i (j)

(5)

Assume the domain discrepancy can be effectively
reduced through feature adaptation, by minimiz-
ing the entropy penalty, training of the classiﬁer
is inﬂuenced by the unlabeled target data and will
generally maximize the margins between the tar-
get examples and the decision boundaries, increas-
ing the prediction conﬁdence on the target domain.

training.

Self-ensemble Bootstrapping: Another way to
estimate y(t)
corresponds to bootstrapping. The
i
idea is to estimate the unknown labels as the
predictions of the model learned from the pre-
vious round of
Bootstrapping has
been explored for domain adaptation in previous
works (Jiang and Zhai, 2007; Wu et al., 2009).
However, in their methods, domain discrepancy
was not explicitly minimized via feature adap-
tation. Applying bootstrapping or other semi-
supervised learning techniques in this case may
worsen the results as the classiﬁer can perform
quite bad on the target data.

Inspired by the ensembling method proposed
in (Laine and Aila, 2017), we estimate y(t)
by
i
forming ensemble predictions of labels during
training, using the outputs on different training
epochs. The loss is formulated as follows:

Ω = −

1
N

N
(cid:88)

C
(cid:88)

i=1

j=1

˜z(s,t)
i

(j) log ˜y(s,t)

(j)

i

(6)

where ˜z denotes the estimated labels computed on
the ensemble predictions from different epochs.
It serves
The loss is applied on all documents.
for bootstrapping on the unlabeled target data, and
it also serves as a regularization that encourages

Algorithm 1 Pseudocode for training DAS
Require: Ds, Dt, G, F
Require: α = ensembling momentum, 0 ≤ α < 1
Require: w(t) = weight ramp-up function

Z ← 0[N ×C]
˜z ← 0[N ×C]
for t ∈ [1, max-epochs] do

for each minibatch B(s), B(t), B(u) in

Ds, Dt, {x(s,t)

}|N

i=1 do

i
compute loss L on [xi∈B(s), yi∈B(s)]
compute loss J on [xi∈B(s), xj∈B(t)]
compute loss Γ on xi∈B(t)
compute loss Ω on [xi∈B(u), ˜zi∈B(u)]
overall-loss ← L + λ1J + λ2Γ + w(t)Ω
update network parameters

i ← F(G(xi)), for i ∈ N

end for
Z(cid:48)
Z ← αZ + (1 − α)Z(cid:48)
˜z ← one-hot-vectors(Z)

end for

the network predictions to be consistent in differ-
ent training epochs. Ω is jointly trained with L,
J , and Γ. Algorithm 1 illustrates the overall train-
ing process of the proposed domain adaptive semi-
supervised learning (DAS) framework.

t

In Algorithm 1, λ1, λ2, and w(t) are weights
to balance the effects of J , Γ, and Ω respectively.
λ1 and λ2 are constant hyper-parameters. We set
max-epochs )2]λ3 as a Gaus-
w(t) = exp[−5(1 −
sian curve to ramp up the weight from 0 to λ3.
This is to ensure the ramp-up of the bootstrapping
loss component is slow enough in the beginning
of the training. After each training epoch, we com-
pute Z(cid:48)
i which denotes the predictions made by the
network in current epoch, and then the ensemble
prediction Zi is updated as a weighted average of
the outputs from previous epochs and the current
epoch, with recent epochs having larger weight.
For generating estimated labels ˜zi, Zi is converted
to a one-hot vector where the entry with the maxi-
mum value is set to one and other entries are set to
zeros. The self-ensemble bootstrapping is a gener-
alized version of bootstrappings that only use the
outputs from the previous round of training (Jiang
and Zhai, 2007; Wu et al., 2009). The ensemble
prediction is likely to be closer to the correct, un-
known labels of the target data.

Domain
Book

Electronics

Beauty

Music

#Pos
2000
4824
2000
4817
2000
4709
2000
4441

#Neg
2000
513
2000
694
2000
616
2000
785

#Neu Total
6000
2000
6000
663
6000
2000
6000
489
6000
2000
6000
675
6000
2000
6000
774

Set 1
Set 2
Set 1
Set 2
Set 1
Set 2
Set 1
Set 2

(a) Small-scale datasets

Domain
IMDB
Yelp
Cell Phone
Baby

#Pos
55,242
155,625
148,657
126,525

#Neg
11,735
29,597
24,343
17,012

#Neu
17,942
45,941
21,439
17,255

Total
84,919
231,163
194,439
160,792

(b) Large-scale datasets

Table 1: Summary of datasets.

4 Experiments

4.1 CNN Encoder Implementation

We have left the feature encoder G unspeciﬁed,
In
for which, a few options can be considered.
our implementation, we adopt a one-layer CNN
structure from previous works (Kim, 2014; Yu and
Jiang, 2016), as it has been demonstrated to work
well for sentiment classiﬁcation tasks. Given a re-
view document x = (x1, x2, ..., xn) consisting of
n words, we begin by associating each word with
a continuous word embedding (Mikolov et al.,
2013) ex from an embedding matrix E ∈ RV ×d,
where V is the vocabulary size and d is the embed-
ding dimension. E is jointly updated with other
network parameters during training. Given a win-
dow of dense word embeddings ex1, ex2, ..., exl,
the convolution layer ﬁrst concatenates these vec-
tors to form a vector ˆx of length ld and then the
output vector is computed by Equation (7):

Conv(ˆx) = f (W · ˆx + b)

(7)

θg = {W, b} is the parameter set of the en-
coder G and is shared across all windows of the
sequence. f is an element-wise non-linear activa-
tion function. The convolution operation can cap-
ture local contextual dependencies of the input se-
quence and the extracted feature vectors are sim-
ilar to n-grams. After the convolution operation
is applied to the whole sequence, we obtain a list
of hidden vectors H = (h1, h2, ..., hn). A max-
over-time pooling layer is applied to obtain the ﬁ-
nal vector representation ξ of the input document.

4.2 Datasets and Experimental Settings

Existing benchmark datasets such as the Amazon
benchmark (Blitzer et al., 2007) typically remove

reviews with neutral labels in both domains. This
is problematic as the label information of the tar-
get domain is not accessible in an unsupervised
domain adaptation setting. Furthermore, remov-
ing neutral instances may bias the dataset favor-
ably for max-margin-based algorithms like ours,
since the resulting dataset has all uncertain labels
removed, leaving only high conﬁdence examples.
Therefore, we construct new datasets by ourselves.
The results on the original Amazon benchmark is
qualitatively similar, and we present them in Ap-
pendix A for completeness since most of previous
works reported results on it.

Small-scale datasets: Our new dataset was de-
rived from the large-scale Amazon datasets2 re-
leased by McAuley et al. (2015). It contains four
domains3: Book (BK), Electronics (E), Beauty
(BT), and Music (M). Each domain contains two
datasets. Set 1 contains 6000 instances with ex-
actly balanced class labels, and set 2 contains
6000 instances that are randomly sampled from
the large dataset, preserving the original label dis-
tribution, which we believe better reﬂects the label
distribution in real life. The examples in these two
sets do not overlap. Detailed statistics of the gen-
erated datasets are given in Table 1a.

In all our experiments on the small-scale
datasets, we use set 1 of the source domain as the
only source with sentiment label information dur-
ing training, and we evaluate the trained model on
set 1 of the target domain. Since we cannot con-
trol the label distribution of unlabeled data during
training, we consider two different settings:
Setting (1): Only set 1 of the target domain is used
as the unlabeled set. This tells us how the method
performs in a condition when the target domain
has a close-to-balanced label distribution. As we
also evaluate on set 1 of the target domain, this is
also considered as a transductive setting.
Setting (2): Set 2 from both the source and target
domains are used as unlabeled sets. Since set 2 is
directly sampled from millions of reviews, it better
reﬂects real-life sentiment distribution.

Large-scale datasets: We further conduct ex-
periments on four much larger datasets: IMDB4

2http://jmcauley.ucsd.edu/data/amazon/
3The original reviews were rated on a 5-point scale. We
label them with rating < 3, > 3, and = 3 as negative, posi-
tive, and neutral respectively.

4IMDB is rated on a 10-point scale, and we label reviews
with rating < 5, > 6, and = 5/6 as negative, positive, and
neutral respectively.

(I), Yelp2014 (Y), Cell Phone (C), and Baby
(B). IMDB and Yelp2014 were previously used
in (Tang et al., 2015; Yang et al., 2017). Cell
phone and Baby are from the large-scale Amazon
dataset (McAuley et al., 2015; He and McAuley,
2016). Detailed statistics are summarized in Ta-
ble 1b. We keep all reviews in the original datasets
and consider a transductive setting where all target
examples are used for both training (without la-
bel information) and evaluation. We perform sam-
pling to balance the classes of labeled source data
in each minibatch B(s) during training.

4.3 Selection of Development Set

Ideally, the development set should be drawn from
the same distribution as the test set. However, un-
der the unsupervised domain adaptation setting,
we do not have any labeled target data at training
phase which could be used as development set. In
all of our experiments, for each pair of domains,
we instead sample 1000 examples from the train-
ing set of the source domain as development set.
We train the network for a ﬁxed number of epochs,
and the model with the minimum classiﬁcation er-
ror on this development set is saved for evaluation.
This approach works well on most of the problems
since the target domain is supposed to behave like
the source domain if the domain difference is ef-
fectively reduced.

Another problem is how to select the values for
hyper-parameters. If we tune λ1 and λ2 directly
on the development set from the source domain,
most likely both of them will be set to 0, as un-
labeled target data is not helpful for improving in-
domain accuracy of the source domain. Other neu-
ral network models also have the same problem for
hyper-parameter tuning. Therefore, our strategy is
to use the development set from the target domain
to optimize λ1 and λ2 for one problem (e.g., we
only do this on E→BK), and ﬁx their values on the
other problems. This setting assumes that we have
at least two labeled domains such that we can op-
timize the hyper-parameters, and then we ﬁx them
for other new unlabeled domains to transfer to.

4.4 Training Details and Hyper-parameters

We initialize word embeddings using the 300-
dimension GloVe vectors supplied by Pennington
et al., (2014), which were trained on 840 billion
tokens from the Common Crawl. For each pair of
domains, the vocabulary consists of the top 10000
most frequent words. For words in the vocabulary

but not present in the pre-trained embeddings, we
randomly initialize them.

We set hyper-parameters of

the CNN en-
coder following previous works (Kim, 2014; Yu
and Jiang, 2016) without speciﬁc tuning on our
datasets. The window size is set to 3 and the size
of the hidden layer is set to 300. The nonlinear
activation function is Relu. For regularization, we
also follow their settings and employ dropout with
probability set to 0.5 on ξi before feeding it to the
output layer F, and constrain the l2-norm of the
weight vector θf , setting its max norm to 3.

On the small-scale datasets and the Aamzon
benchmark, λ1 and λ2 are set to 200 and 1,
respectively,
tuned on the development set of
task E→BK under setting 1. On the large-scale
datasets, λ1 and λ2 are set to 500 and 0.2, re-
tuned on I→Y. We use a Gaussian
spectively,
curve w(t) = exp[−5(1 − t
)2]λ3 to ramp up
the weight of the bootstrapping loss Ω from 0 to
λ3, where tmax denotes the maximum number of
training epochs. We train 30 epochs for all exper-
iments. We set λ3 to 3 and α to 0.5 for all experi-
ments.

tmax

The batch size is set to 50 on the small-scale
datasets and the Amazon benchmark. We increase
the batch size to 250 on the large-scale datasets to
reduce the number of iterations. RMSProp opti-
mizer with learning rate set to 0.0005 is used for
all experiments.

4.5 Models for Comparison

We compare with the following baselines:

(1) Naive: A non-domain-adaptive baseline
with bag-of-words representations and SVM clas-
siﬁer trained on the source domain.

(2) mSDA (Chen et al., 2012): This is the state-
of-the-art method based on discrete input features.
Top 1000 bag-of-words features are kept as pivot
features. We set the number of stacked layers to 3
and the corruption probability to 0.5.

(3) NaiveNN: This is a non-domain-adaptive
CNN trained on source domain, which is a variant
of our model by setting λ1, λ2, and λ3 to zeros.

(4) AuxNN (Yu and Jiang, 2016): This is a neu-
ral model that exploits auxiliary tasks, which has
achieved state-of-the-art results on cross-domain
sentiment classiﬁcation. The sentence encoder
used in this model is the same as ours.

(5) ADAN (Chen et al., 2017): This method
exploits adversarial training to reduce representa-

(a) Accuracy on the small-scale dataset under setting 1.

(b) Accuracy on the small-scale dataset under setting 2.

(c) Macro-F1 on the large-scale dataset.

Figure 1: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (either of DAS, DAS-EM, DAS-SE) is
signiﬁcantly better than other baselines (baseline 1-6) with p < 0.05 based on one-tailed unpaired t-test.

tion difference between domains. The original pa-
per uses a simple feedforward network as encoder.
For fair comparison, we replace it with our CNN-
based encoder. We train 5 iterations on the dis-
criminator per iteration on the encoder and senti-
ment classiﬁer as suggested in their paper.

(6) MMD: MMD has been widely used for min-
imizing domain discrepancy on images. In those
works (Tzeng et al., 2014; Long et al., 2017), vari-
ants of deep CNNs are used for encoding images
and the MMDs of multiple layers are jointly mini-
mized. In NLP, adding more layers of CNNs may
not be very helpful and thus those models from
image-related tasks can not be directly applied
to our problem. To compare with MMD-based
method, we train a model that jointly minimize
the classiﬁcation loss L on the source domain and
i=1} and {ξ(t)
MMD between {ξ(s)
|nt
|ns
i=1}. For
computing MMD, we use a Gaussian RBF which
is a common choice for characteristic kernel.

i

i

In addition to the above baselines, we also show
results of different variants of our model. DAS
as shown in Algorithm 1 denotes our full model.
DAS-EM denotes the model with only entropy

minimization for semi-supervised learning (set
λ3 = 0). DAS-SE denotes the model with only
self-ensemble bootstrapping for semi-supervised
learning (set λ2 = 0). FANN (feature-adaptation
neural network) denotes the model without semi-
supervised learning performed (set both λ2 and λ3
to zeros).

4.6 Main Results

Figure 15 shows the comparison of adaptation re-
sults (see Appendix B for the exact numerical
numbers). We report classiﬁcation accuracy on
the small-scale dataset. For the large-scale dataset,
macro-F1 is instead used since the label distribu-
tion in the test set is extremely unbalanced. Key
observations are summarized as follows. (1) Both
DAS-EM and DAS-SE perform better in most
cases compared with ADAN, MDD, and FANN,
in which only feature adaptation is performed.
This demonstrates the effectiveness of the pro-

5We exclude results of Naive, mSDA and AuxNN on the
large-scale dataset. Both Naive and mSDA have difﬁculties
to scale up to the large dataset. AuxNN relies on manually
selecting positive and negative pivots before training.

Figure 2: Accuracy vs. percentage of unlabeled target training examples.

Figure 3: Accuracy vs. number of labeled target training examples.

posed domain adaptive semi-supervised learning
framework. DAS-EM is more effective than DAS-
SE in most cases, and the full model DAS with
both techniques jointly employed overall has the
(2) When comparing the two
best performance.
settings on the small-scale dataset, all domain-
adaptive methods6 generally perform better under
setting 1. In setting 1, the target examples are bal-
anced in classes, which can provide more diverse
opinion-related features. However, when consid-
ering unsupervised domain adaptation, we should
not presume the label distribution of the unlabeled
data. Thus, it is necessary to conduct experiments
using datasets that reﬂect real-life sentiment dis-
tribution as what we did on setting2 and the large-
scale dataset. Unfortunately, this is ignored by
most of previous works. (3) Word-embeddings are
very helpful, as we can see even NaiveNN can sub-
stantially outperform mSDA on most tasks.

To see the effect of semi-supervised learning
alone, we also conduct experiments by setting
λ1 = 0 to eliminate the effect of feature adapta-
tion. Both entropy minimization and bootstrap-
ping perform very badly in this setting. En-
tropy minimization gives almost random predic-
tions with accuracy below 0.4, and the results
of bootstrapping are also much lower compared
to NaiveNN. This suggests that the feature adap-
tation component is essential. Without it,
the
learned target representations are less meaning-
ful and discriminative. Applying semi-supervised

6Results of Naive and NaiveNN do not change under both

settings as they are only trained on the source domain.

learning in this case is likely to worsen the results.

4.7 Further Analysis

In Figure 2, we show the change of accuracy with
respect to the percentage of unlabeled data used
for training on three particular problems under set-
ting 1. The value at x = 0 denotes the accuracies
of NaiveNN which does not utilize any target data.
For DAS, we observe a nonlinear increasing trend
where the accuracy quickly improves at the be-
ginning, and then gradually stabilizes. For other
methods, this trend is less obvious, and adding
more unlabeled data sometimes even worsen the
results. This ﬁnding again suggests that the pro-
posed approach can better exploit the information
from unlabeled data.

We also conduct experiments under a setting
with a small number of labeled target examples
available. Figure 3 shows the change of accuracy
with respect to the number of labeled target exam-
ples added for training. We can observe that DAS
is still more effective under this setting, while the
performance differences to other methods gradu-
ally decrease with the increasing number of la-
beled target examples.

4.8 CNN Filter Analysis

In this subsection, we aim to better understand
DAS by analyzing sentiment-related CNN ﬁlters.
To do that, 1) we ﬁrst select a list of the most re-
lated CNN ﬁlters for predicting each sentiment la-
bel (positive, negative neutral). Those ﬁlters can
be identiﬁed according to the learned weights θf

best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

(a) NaiveNN

(b) FANN

(c) DAS

Table 2: Comparison of the top trigrams (each column) from the target domain (beauty) captured by the
5 most positive-sentiment-related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

of the output layer F. Higher weight indicates
stronger relatedness. 2) Recall that in our im-
plementation, each CNN ﬁlter has a window size
of 3 with Relu activation. We can thus represent
each selected ﬁlter as a ranked list of trigrams with
highest activation values.

We analyze the CNN ﬁlters

learned by
NaiveNN, FANN and DAS respectively on task
E→BT under setting 1. We focus on E→BT for
study because electronics and beauty are very dif-
ferent domains and each of them has a diverse
set of domain-speciﬁc sentiment expressions. For
each method, we identify the top 10 most related
ﬁlters for each sentiment label, and extract the top
trigrams of each selected ﬁlter on both source and
target domains. Since labeled source examples are
used for training, we ﬁnd the ﬁlters learned by the
three methods capture similar expressions on the
source domain, containing both domain-invariant
and domain-speciﬁc trigrams. On the target do-
main, DAS captures more target-speciﬁc expres-
sions compared to the other two methods. Due
to space limitation, we only present a small sub-
set of positive-sentiment-related ﬁlters in Table 2.
The complete results are provided in Appendix C.
From Table 2, we can observe that the ﬁlters
learned by NaiveNN are almost unable to cap-
ture target-speciﬁc sentiment expressions, while
FANN is able to capture limited target-speciﬁc
words such as “clean” and “scent”. The ﬁlters
learned by DAS are more domain-adaptive, cap-
turing diverse sentiment expressions in the target
domain.

5 Conclusion

In this work, we propose DAS, a novel frame-
work that jointly performs feature adaptation and
semi-supervised learning. We have demonstrated
through multiple experiments that DAS can better
leverage unlabeled data, and achieve substantial
improvements over baseline methods. We have
also shown that feature adaptation is an essen-
tial component, without which, semi-supervised
learning is not able to function properly. The pro-
posed framework could be potentially adapted to
other domain adaptation tasks, which is the focus
of our future studies.

References

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: domain adaptation for sentiment classiﬁ-
In Annual Meeting of the Association for
cation.
Computational Linguistics.

Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In The 29th Interna-
tional Conference on Machine Learning.

Xilun Chen, Yu Sun, Ben Athiwarakun, Claire Cardie,
and Kilian Weinberger. 2017. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁer. In Arxiv e-prints arXiv:1606.01614.

Wenyuan Dai, Gui rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classiﬁers for
text classiﬁcation. In AAAI Conference on Artiﬁcial
Intelligence.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: a deep learning approach. In The 28th
International Conference on Machine Learning.

Yves Grandvalet and Yoshua Bengio. 2004. Semi-
In
supervised learning by entropy minimization.
Neural Information Processing Systems.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander Smola. 2012. A
kernel two-sample test. Journal of Machine Learn-
ing Research, 13:723–773.

Ruining He and Julian McAuley. 2016. Ups and
downs: modeling the visual evolution of fashion
In
trends with one-class collaborative ﬁltering.
WWW.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classiﬁcation. In ACL.

Jing Jiang and ChengXiang Zhai. 2007.

Instance
weighting for domain adaptation in NLP. In Annual
Meeting of the Association for Computational Lin-
guistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Conference on Empirical
Methods in Natural Language Processing.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In International
Conference on Learning Representation.

Zheng Li, Yun Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial mem-
ory network for cross-domain sentiment classiﬁca-
tion. In The 26th International Joint Conference on
Artiﬁcial Intelligence.

Mingsheng Long, Yue Cao,

Jianmin Wang, and
Michael I. Jordan. 2015. Learning transferable fea-
In Interna-
tures with deep adaptation networks.
tional Conference on Machine Learning.

Mingsheng Long, Han Zhu,

Jianmin Wang, and
Michael I. Jordan. 2017. Deep transfer learning with
joint adaptation networks. In International Confer-
ence on Machine Learning.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
ommendations on styles and substitutes. In The 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Neural Information Processing Systems.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classiﬁcation via spectral feature alignment. In
The 19th International World Wide Web Conference.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representation of users and products for
document level sentiment classiﬁcation. In Annual
Meeting of the Association for Computational Lin-
guistics.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
maximizing for domain invariance. In Arxiv e-prints
arXiv:1412.3474.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Conference on Empirical Meth-
ods in Natural Language Processing.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Annual
Meeting of the Association for Computational Lin-
guistics.

Wei Yang, Wei Lu, and Vincent W. Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In Conference on
Empirical Methods in Natural Language Process-
ing.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Annual Meeting of the Association
for Computational Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
In Conference on Empirical
timent classiﬁcation.
Methods in Natural Language Processing.

Guangyou Zhou, Tingting He, Wensheng Wu, and Xi-
aohua Tony Hu. 2015. Linking heterogeneous input
features with pivots for domain adaptation. In The
24th International Joint Conference on Artiﬁcial In-
telligence.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016. Bi-transferring deep neural
networks for domain adaptation. In Annual Meeting
of the Association for Computational Linguistics.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: transfer learning with deep autoencoders.
In The 24th International Joint Conference on Arti-
ﬁcial Intelligence.

S

D
E
K
B
E
K
B
D
K
B
D
E

T

B
B
B
D
D
D
E
E
E
K
K
K

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE
82.10∗
77.75
79.60
82.35
79.75
82.15∗
75.80
80.05
85.95
79.45∗
79.50
83.80

82.00∗
80.25∗
79.95
82.65
81.40∗
81.65∗
80.25∗
81.40∗
85.70
81.55∗
80.80
84.50

81.05
78.65
79.70
82.00
80.10
79.35
76.45
80.20
85.75
75.20
79.70
81.75

80.30
77.25
79.20
81.65
79.55
76.90
76.75
79.25
85.60
77.55
78.00
83.85

81.70
78.55
79.25
82.30
79.70
80.45
77.60
79.70
86.85
76.10
77.35
83.95

80.80
78.00
77.85
81.75
80.65
78.90
76.40
77.55
84.05
78.10
80.05
84.15

75.20
68.85
70.00
77.15
69.50
71.40
72.15
71.65
79.75
73.50
72.00
82.80

78.50
76.15
75.65
80.60
76.30
76.05
75.55
76.00
84.20
75.95
76.30
84.45

81.10
77.95
77.75
80.80
77.00
79.35
76.20
76.60
84.85
77.40
78.55
84.95

Average

73.66

77.98

79.38

79.85

80.29

80.00

79.65

81.84

80.68

DAS
82.05∗
80.00∗
80.05∗
82.75∗
80.15
81.40∗
81.15∗
81.55∗
85.80
82.25∗
81.50∗
84.85

81.96

Table 3: Accuracies on the Amazon benchmark. Average results over 5 runs with random initializations
are reported for each neural method. ∗ indicates that the proposed method (DAS-EM, DAS-SE, DAS) is
signiﬁcantly better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

A Results on Amazon Benchmark

C CNN Filter Analysis Full Results

As mentioned in Section 4.8, we conduct CNN ﬁl-
ter analysis on NaiveNN, FANN, and DAS. For
each method, we identify the top 10 most related
ﬁlters for positive, negative, neutral sentiment la-
bels respectively, and then represent each selected
ﬁlter as a ranked list of trigrams with the highest
activation values on it. Table 5, 6, 7 in the fol-
lowing pages illustrate the trigrams from the tar-
get domain (beauty) captured by the selected ﬁl-
ters learned on E→BT for each method.

We can observe that compared to NaiveNN and
FANN, DAS is able to capture a more diverse set
of relevant sentiment expressions on the target do-
main for each sentiment label. This observation is
consistent with our motivation. Since NaiveNN,
FANN and other baseline methods solely train
the sentiment classiﬁer on the source domain, the
learned encoder is not able to produce discrimina-
tive features on the target domain. DAS addresses
this problem by reﬁning the classiﬁer on the tar-
get domain with semi-supervised learning, and
the overall objective forces the encoder to learn
feature representations that are not only domain-
invariant but also discriminative on both domains.

Most previous works (Blitzer et al., 2007; Pan
et al., 2010; Glorot et al., 2011; Chen et al.,
2012; Zhou et al., 2016) carried out experiments
on the Amazon benchmark released by Blitzer
et al.
(2007). The dataset contains 4 different
domains: Book (B), DVDs (D), Electronics (E),
and Kitchen (K). Following their experimental set-
tings, we consider the binary classiﬁcation task to
predict whether a review is positive or negative
on the target domain. Each domain consists of
1000 positive and 1000 negative reviews respec-
tively. We also allow 4000 unlabeled reviews to
be used for both the source and the target domains,
of which the positive and negative reviews are bal-
anced as well, following the settings in previous
works. We construct 12 cross-domain sentiment
classiﬁcation tasks and split the labeled data in
each domain into a training set of 1600 reviews
and a test set of 400 reviews. The classiﬁer is
trained on the training set of the source domain
and is evaluated on the test set of the target do-
main. The comparison results are shown in Ta-
ble 3.

B Numerical Results of Figure 1

Due to space limitation, we only show results in
ﬁgures in the paper. All numerical numbers used
for plotting Figure 1 are presented in Table 4. We
can observe that DAS-EM, DAS-SE, and DAS
all achieve substantial improvements over baseline
methods under different settings.

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

BK 49.07
E
BK 48.17
BT
M BK 45.20
46.43
E
BK
53.63
E
BT
37.93
E
M
45.57
BK BT
48.43
E
BT
39.42
M BT
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

BK 49.07
E
BK 48.17
BT
M BK 45.20
E
46.43
BK
E
BT
53.63
37.93
M
E
BK BT
45.57
48.43
E
BT
M BT
39.43
BK M 43.32
M 41.83
E
BT M 43.55
45.21
Average

55.13
53.53
49.22
48.22
57.32
38.13
50.77
53.13
39.37
47.88
47.88
49.62
49.18

52.88
47.65
48.33
47.07
55.12
37.40
49.63
51.98
37.73
45.97
45.12
45.78
47.06

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

58.26
58.48
57.10
47.15
58.77
47.28
48.35
54.07
47.23
47.67
50.21
50.27
52.07

60.62
59.86
60.43
48.45
60.98
49.60
48.67
55.58
48.65
48.87
51.19
53.11
53.84

57.72
58.46
58.15
48.22
59.08
49.43
47.80
54.37
46.92
48.79
52.31
53.55
52.98

63.32
65.62
62.87
47.42
63.13
46.57
46.14
50.98
44.26
51.10
50.23
55.35
53.92

57.07
59.78
58.67
49.48
59.45
47.00
47.52
51.28
45.73
50.20
52.57
54.68
52.79

60.38
59.66
60.20
53.32
60.53
51.55
49.48
54.83
48.35
53.04
51.81
54.43
54.80

57.43
56.17
57.08
45.42
60.24
48.72
45.43
54.92
46.68
48.76
51.50
54.55
52.23

59.59
59.28
57.65
51.27
60.62
47.23
50.24
56.78
48.89
52.35
52.14
53.84
54.15

56.43
57.98
57.75
51.95
58.67
48.92
49.83
55.42
48.48
49.47
48.18
53.41
53.04

66.48∗
66.78∗
69.63∗
58.59∗
65.71∗
55.88∗
49.49
61.53∗
47.65
55.47∗
58.28∗
60.95∗
59.74

57.78
61.17∗
58.62
54.51∗
61.27
51.28∗
53.72∗
53.10
47.18
52.37∗
53.63∗
56.24∗
55.07

(a) Accuracy on the small-scale dataset under setting 1

DAS
67.12∗
66.53
70.31∗
58.73∗
66.14∗
55.78∗
51.30∗
60.76∗
50.66∗
55.98∗
59.06∗
60.5∗
60.24

55.20
63.32∗
60.77∗
53.92∗
59.83
52.88∗
54.67∗
56.05∗
49.73∗
53.52∗
55.38∗
56.02∗
55.94

62.37
61.17
65.24∗
55.15∗
61.78
53.22∗
54.23∗
59.52∗
50.67∗
55.13∗
55.60∗
56.90∗
57.58

58.93
60.17∗
58.25
52.47∗
61.42
51.18∗
51.23∗
56.43∗
51.57∗
52.68∗
52.25
56.23∗
55.23

S

T

Naive mSDA NaiveNN AuxNN ADAN MMD FANN DAS-EM DAS-SE

DAS

(b) Accuracy on the small-scale dataset under setting 2

S

T

Y
I
C
I
B
I
I
Y
C
Y
B
Y
I
C
Y
C
B
C
I
B
Y
B
B
C
Average

NaiveNN ADAN MMD FANN DAS-EM DAS-SE
56.66∗
55.18
54.30
58.72∗
59.14
58.43
51.97∗
54.67∗
59.98
50.81
52.95
58.12
55.91

55.04
57.27∗
57.31∗
57.92∗
61.17
59.94∗
53.46∗
53.48
59.84
48.84
52.87
57.74
56.24

54.16
53.35
51.40
56.52
60.81
58.77
50.49
53.12
61.23
47.35
54.43
60.52
55.18

54.46
53.07
52.39
56.30
56.02
55.72
51.04
51.86
60.19
48.17
53.54
55.56
54.02

55.52
55.07
54.64
52.57
60.70
58.42
47.27
52.53
59.91
46.34
50.82
59.99
54.48

53.01
51.84
45.85
55.46
61.22
56.86
50.38
53.87
59.48
50.05
54.73
60.47
54.43

DAS
58.54∗
57.28∗
58.02∗
58.92∗
61.39
61.87∗
53.38∗
55.44∗
59.76
48.84
52.91
59.75
57.18

(c) Macro-F1 scores on the large-scale dataset

Table 4: Performance comparison. Average results over 5 runs with random initializations are reported
for each neural method. ∗ indicates that the proposed method (DAS, DAS-EM, DAS-SE) is signiﬁcantly
better than other baselines with p < 0.05 based on one-tailed unpaired t-test.

1
best-value-at
good-value-at
perfect-product-for
great-product-at
amazing-product-∗

2
highly-recommend-!
highly-advise-!
gogeous-absolutely-perfect
love-love-love
highly-recommend-for

3
nars-are-amazing
ulta-are-fantastic
length-are-so
expected-in-perfect
setting-works-perfect

4
beauty-store-suggested
durable-machine-and
perfect-length-and
great-store-on
beauty-store-for

5
since-i-love
years-i-love
bonus-i-love
appearance-i-love
relaxing-i-love

6
store-and-am
cleanser-and-am
olay-and-am
daily-and-need
shower-and-noticed

7
ofﬁce-setting-thanks
locks-shimmering-color
dirty-blonde-color
victoria-secrets-gorgeous
dirty-pinkish-color

8
car-washes-!
price-in-stores
products-are-priced
car-and-burning
from-our-store

(a) NaiveNN

9
speed-is-perfect
buttons-are-perfect
unit-is-superb
spray-is-perfect
coverage-is-excellent

10
!-i-recommend
!-i-highly
shower-i-slather
spots-i-needed
best-i-use

1
prices-my-favorite
brands-my-favorite
very-great-stores
great-bottle-also
scent-pleasantly-ﬂoral

2
so-nicely-!
more-affordable-price
shampoo-a-perfect
an-excellent-value
really-enjoy-it

3
purchase-thanks-!
buy-again-!
without-hesitation-!
buy-this-!
discount-too-!

4
feel-wonderfully-clean
on-nicely-builds
polish-easy-and
felt-cleanser-than
honestly-perfect-it

5
are-really-cleaning
washing-and-cleaning
really-good-shampoo
deeply-cleans-my
totally-moisturize-our

6
shower-or-cleaning
water-onto-my
bleach-your-towels
pump-onto-my
water-great-for

8

7
deﬁnitely-purchase-again more-affordable-price
deﬁnitely-buy-again
perfect-for-my
deﬁnitely-order-again
super-happy-to

a-perfect-length
an-exceptional-value
’ve-enjoyed-it
pretty-decent-layer

9
absolutely-wonderful-!
perfect-for-running
concealer-for-my
moisturizing-for-my
super-glue-even

10
felt-cleaner-than
ﬂat-iron-through
rubbed-grease-on
deeply-cleans-my
being-cleaner-after

1
bath-’s-wonderful
all-pretty-affordable
it-delivers-fabulous
and-blends-nicely
heats-quickly-love

2
love-fruity-sweet
absorb-really-nicely
shower-lather-wonderfully
*-smells-fantastic
and-clean-excellent

3
feeling-smooth-radiant
love-lavender-scented
am-very-grateful
love-fruity-fragrances
perfect-beautiful-shimmer

4
cleans-thoroughly-*
loving-this-soap
bed-of-love
shower-!-*
radiant-daily-moisturizer

5
excellent-everyday-lotion
affordable-cleans-nicely
fantastic-base-coat
nice-gentle-scrub
surprisingly-safe-on

6
shower-lather-wonderfully
affordable-cleans-nicely
peels-great-price
daughter-loves-this
cleans-great-smells

7
highly-recommend-!
deﬁnitely-recommend-!
love-love-!
highly-advise-!
time-advise-!

8
excellent-fragrance-and
fantastic-for-daytime
wonderfully-moisturizing-and
lathers-great-cleans
delightful-shampoo-works

10
9
its-unique-smoothing
forgeous-gragrance-mist
smooth-luxurious-texture wonderful-bedtime-scent
’s-extremely-gentle
’s-affordable-combination
absorbs-quite-well

love-essie-polish
perfect-beautiful-shimmer
fantastic-coverage-hydrates

(b) FANN

(c) DAS

Table 5: Top 5 trigrams from the target domain (beauty) captured by the top 10 most positive-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
pads-ruined-my
highly-disappointed-and
dryers-blew-my
completely-worthless-didn’t
am-disappointed-and

2
simply-threw-out
reviewer-pointed-out
extracts-broke-into
actually-threw-out
clips-barely-keep

3
hours-after-trying
minutes-after-rinsing
disappointed-after-trying
lips-after-trying
dry-after-shampooing

4
junk-drawer-∗
refund-time-!
total-fake-wen
waste-your-time
total-fail-!

5
contacted-manufacturer-about
minutes-not-worth
’ve-owned-this
hour-unless-it
results-they-claim

6
were-awful-garbage
what-awful-garbage
and-utter-waste
are-absolute-garbage
piece-of-junk

7
two-failed-attempts
a-mistake-save
a-deﬁnite-return
a-pathetic-limp
a-total-disappointment

8
auto-ship-sent
am-returning-to
am-unable-to
am-pale-ghost
got-returned-and

10
broke-don’t-ﬁx
sent-me-expired

9
refund-and-dispose
refund-spend-your
wouldn’t-recommend-! wearing-false-eyelashes
not-buy-dunhill
not-worth-returning

a-temporary-ﬁx
a-disappointment-cheap

(a) NaiveNN

2
the-worse-mascaras

1
nasty-sunburn-lol
bother-returning-them it-caused-patchy
fails-miserably-at
minutes-auric-needs
severely-burned-me

lifeless-disaster-enter
it-fails-miserably
feel-worse-leaving

3
stale-very-unhappy
were-horrible-failures
send-this-crap
were-awful-garbage
were-horribly-red

4
actually-hurts-your
didn’t-bother-returning
it-hurts-your
didn’t-exist-in
skin-horribly-after

5
a-return-label
stay-away-completely
like-bug-quit
a-defective-brown
’d-refund-the

6
worse-with-exercise
worse-and-after
unable-to-return
worse-my-face
poorly-in-step

7
not-stink-your
mistake-save-your
nothing-!-by
nothing-happened-!
nothing-save-your

8
it-fails-miserably
is-ineffective-apart
but-horribly-unhealthy
a-pathetic-limp
a-worse-job

9
got-progressively-worse
gave-opposite-result
another-epic-fail
got-horribly-painful
was-downright-painful

10
stopped-working-for
uncomfortable-i-returned
i-am-returning
stopped-working-shortly
not-waterproof-makeup

1
poorly-designed-product
defective-dryer-promising
disgusting-smelling-thing
hurts-your-scalp
hurts-your-hair

2
a-refund-spend
a-refund-save
i-regret-spending
just-wouldn’t-spend
looked-washed-out

3
completely-waste-of
of-junk-*
were-awful-garbage
worthless-waste-of
throwing-money-away

4
smells-disgusting-!
smells-horribly-like
does-not-straighten
’s-false-advertising
a-disgusting-cheap

5
burning-rubber-stench
began-smelling-vomit
reaction-and-wasted
control-and-smelled
using-this-disgusting

6
super-irritating-!
strong-reaction-and
really-burned-and
very-pasty-and
super-streaky-and

7
got-promptly-broke
after-ive-washed
after-several-attempts
this-stuff-stinks
again-i-threw

8
sore-and-painful
is-simply-irritating
tight-and-uncomfortable
drying-and-irritating
goopy-and-unpleasant

9
it-caused-patchy
layer-hydrogenated-alcohols
the-harmful-uva
my-severe-dark
a-allergic-reaction

10
painful-it-hurt
unnecessary-health-risks
uncomfortable-to-wear
stinging-your-eyes
unbearable-to-wear

(b) FANN

(c) DAS

Table 6: Top 5 trigrams from the target domain (beauty) captured by the top 10 most negative-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

1
purpose-cologne-splash
other-hanae-mori
the-mavala-peeled
avoid-hair-pulling
cause-rashes-stinging

2
okay-cord-was
cocamide-dea-is
coily-conditioner-is
ﬂaky-dandruff-is
quickly-cord-is

3
hands-feet-elbows
been-sealed-tight
stainless-steel-blackhead
severely-tight-chest
thick-nasty-callouses

4
aggressive-in-general
pimples-in-general
biotin-in-general
dimethicone-is-terrible
but-in-general

5
but-its-okay
it-moisturizes-okay
but-moisturizes-keeps
but-don’t-expect
it-lathers-ok

6
pretty-damaged-from
daughter-suffers-from
teenager-suffers-from
tissue-damage-during
the-damage-on

7
darker-olive-complexion
stronger-healthier-or
natural-ingredient-however
vitamin-enriched-color
natural-ingredients-∗

8
doesn’t-mind-pushing
kinda-doesn’t-its
kinda-kinky-coily
okay-job-of
intended-purpose-that

9
producto-por-los
unstuck-frownies-∗
they-push-∗
uva-rays-uva
tend-to-slip

10
feeling-didn’t-last
curls-didn’t-last
extra-uv-protection
garnier-fructis-curl
the-mavala-after

(a) NaiveNN

1
worse-and-after
worse-before-improving
unable-to-return
unless-your-entire
horrible-in-execution

2
maybe-a-refund
ok-mask-i
ok-pining-it
ok-try-i
ok-tho-i

3
very-disappointing-waste
ok-but-clean
ok-but-will
ok-but-didn’t
ok-nothing-special

4
my-ears-are
my-neck-line
cause-unsightly-beads
my-sporadic-line
your-ear-is

5
pretty-neutral-neither
ok-so-if
ok-during-pregnancy
kinda-annoying-if
ok-this-seems

6
uncomfortable-i-returned
weak-they-bend
claimed-faulty-∗
suffers-from-wind
as-defective-∗

7
sticky-lathers-and
quickly-deep-cleans
but-elegant-bottle
beat-the-price
and-reasonably-priced

8
some-fading-when
real-disappointment-the
especially-noticeable-after
progressively-worse-during
style-unfortunately-the

9
are-very-painful
are-less-painful
are-a-pain
about-sum-damage
offered-no-pain

10
its-also-convenient
that-also-my
that-may-make
that-allows-your
its-helpful-to

1
’m-kinda-pale
a-terrible-headache
but-kinda-annoying
’m-kinda-mad
i-kinda-stopped

2
darker-but-nope
gray-didn’t-cover
makeup-doesn’t-sweat
dark-spots-around
moist-but-thats

3
ok-but-horrible
ok-but-didn’t
okay-but-doesn’t
okay-however-it
unfortunately-straight

4
noticeable-i-avoid
however-i-lean
but-otherwise-ok
but-im-deciding
however-i-prefer

5
same-result-mediocre
it-caused-patchy
doesn’t-cause-ﬂare
the-harmful-uva
rather-unpleasant-smell

6
kinda-annoying-if
pretty-bad-breakage
my-slight-discoloration
smells-kinda-bad
look-kinda-crappy

7
brutal-winter-however
summer-color-however
beige-shade-however
is-okay-however
bit-greasy-however

8
higher-rating-because
slight-burnt-rubber
noticeable-tan-since
somewhat-pale-affect
kinda-pale-so

9
nothing-for-odor
kinda-recommend-this
not-recommend-if
noticeable-but-non
nothing-special-moderate

10
but-darker-*
slightly-darker-shade
somewhat-pale-affect
but-somewhat-heavy
bit-dull-heavy

(b) FANN

(c) DAS

Table 7: Top 5 trigrams from the target domain (beauty) captured by the top 10 most neutral-sentiment-
related CNN ﬁlters learned on E→BT. ∗ denotes a padding.

