IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed
Models for Inﬂection

Ben Peters† and Andr´e F.T. Martins†‡
†Instituto de Telecomunicac¸ ˜oes, Lisbon, Portugal
‡Unbabel, Lisbon, Portugal
benzurdopeters@gmail.com, andre.martins@unbabel.com

Abstract

the

paper

presents

Instituto

This
de
Telecomunicac¸ ˜oes–Instituto Superior T´ecnico
submission to Task 1 of the SIGMORPHON
2019 Shared Task. Our models combine
sparse sequence-to-sequence models with a
two-headed attention mechanism that learns
separate attention distributions for the lemma
and inﬂectional tags. Among submissions to
Task 1, our models rank second and third.
Despite the low data setting of the task (only
100 in-language training examples),
they
learn plausible inﬂection patterns and often
concentrate all probability mass into a small
set of hypotheses, making beam search exact.

1

Introduction

Morphological inﬂection is the task of producing
an inﬂected form, given a lemma and a set of
inﬂectional tags. A widespread approach to the
task is the attention-based sequence-to-sequence
model (seq2seq; Bahdanau et al., 2015; Kann and
Sch¨utze, 2016); such models perform well but are
difﬁcult to interpret. To mitigate this shortcom-
ing, we employ an alternative architecture which
combines sparse seq2seq modeling (Peters et al.,
2019) with two-headed attention that attends sep-
arately to the lemma and inﬂectional tags ( ´Acs,
2018). The attention and output distributions are
computed with the sparsemax function and mod-
els are trained to minimize sparsemax loss (Mar-
tins and Astudillo, 2016). Sparsemax, unlike soft-
max, can assign exactly zero attention weight to
irrelevant source tokens and exactly zero probabil-
ity to implausible hypotheses. We apply our mod-
els to Task 1 at the SIGMORPHON 2019 Shared
Task (McCarthy et al., 2019), which extends mor-
phological inﬂection to a cross-lingual setting. We
present two sparse seq2seq architectures:

• DOUBLEATTN (it-ist-01-1) is a reim-
plementation of the two-headed attention

model ( ´Acs, 2018) which substitutes sparse-
max and its loss for softmax and cross en-
tropy loss. It uses separate encoders and at-
tention heads for the lemma and inﬂections,
and concatenates the outputs of the attention
heads.

• GATEDATTN (it-ist-02-1) replaces the
attention concatenation with a sparse gate
which interpolates the lemma and inﬂection
attention. The intuition is that the lemma and
inﬂectional tags are not likely to be equally
important at all time steps. For example, in
a sufﬁxing language, the ﬁrst several gener-
ated characters are likely to be identical to the
lemma; inﬂectional tags are not relevant. The
sparse gate allows the model to learn to shift
focus between the two attentions while ignor-
ing the other at a given time step.

GATEDATTN and DOUBLEATTN rank second
and third, respectively, among submissions to
Task 1.
In addition, their behavior is highly in-
terpretable: they mostly learn to attend to a single
lemma hidden state at a time, progressing mono-
tonically from left to right, while their inﬂection
attention learns patterns which reﬂect underlying
morphological structure. The sparse output layer
often allows the model to concentrate all probabil-
ity mass into a single hypothesis, providing a cer-
tiﬁcate that decoding is exact. Our analysis shows
that sparsity is also highly predictive of perfor-
mance on the shared task metrics, showing that the
models “know what they know”.

2 Models

Our architecture is mostly the same as a standard
RNN-based seq2seq model with attention. In this
section, we outline the changes needed to extend
this model to use sparsemax and two-headed at-
tention in a multilingual setting.

2.1 Sparsemax

Our models’ sparsity comes from the sparsemax
function (Martins and Astudillo, 2016), which
computes the Euclidean projection of a vector z ∈
Rn onto the n–dimensional probability simplex
(cid:52)n := {p ∈ Rn : p ≥ 0, 1(cid:62)p = 1}:

sparsemax(z) := argmin
p∈(cid:52)n

(cid:107)p − z(cid:107)2

(1)

Like softmax, sparsemax converts an arbitrary
real-valued vector into a probability distribution.
The critical difference is that sparsemax can as-
sign exactly zero probability, whereas softmax is
strictly positive. Sparsemax is differentiable al-
most everywhere and can be computed quickly,
allowing its use as a drop-in replacement for soft-
It has previously been used in seq2seq
max.
for computing both attention weights (Malaviya
et al., 2018) and output probabilities (Peters et al.,
2019). Sparse attention is attractive in morpholog-
ical inﬂection because it resembles hard attention,
which has been successful on the task (Aharoni
and Goldberg, 2017; Wu et al., 2018).

2.2 Encoder–Decoder Model

Multilingual embeddings Each encoder and
decoder uses an embedding layer to convert one-
hot token representations into dense embeddings.
To account for the bilingual nature of Task 1, each
of our embedding layers contains two look-up ta-
bles: one for the sequence of input tokens, and
the other for the language of the sequence. At
each time step, the current token’s embedding is
concatenated to a language embedding.1 Each
encoder and decoder uses a separate embedding
layer; no weights are tied. Characters and inﬂec-
tional tags use embeddings of size Dc, while lan-
guage embeddings are of size D(cid:96). Thus the total
embedding size is D = Dc + D(cid:96).

Encoders The lemma and inﬂection encoders
are both bidirectional LSTMs
(Graves and
Schmidhuber, 2005). An encoder’s forward and
backward hidden states are concatenated, forming
a sequence of source hidden states. We set the size
of these hidden states as D in all experiments.

Decoder The decoder is a unidirectional LSTM
(Hochreiter and Schmidhuber, 1997) with input

1The language embedding is the same at all time steps

within an example; there is no code-switching in this task.

feeding (Luong et al., 2015). At time step t, it
computes a hidden state st ∈ RD. Conditioned
on st and the hidden state sequences from the
lemma and inﬂection encoders, a two-headed at-
tention mechanism then computes an attentional
hidden state ˜st ∈ RD. The decoder LSTM is ini-
tialized only with the lemma encoder’s state.

Attention head At time t, an attention head
computes a context vector ct ∈ RD conditioned
on the decoder state st and an encoder state se-
quence H = [h1, . . . , hJ ]. A head consists of
two modular components:

1. Alignment Compute a vector a ∈ RJ of
alignment scores between st and H. We
use the general attention scorer (Luong et al.,
2015), which computes aj := s(cid:62)

t Wahj.

2. Context Compute the context vector ct as
a weighted sum of H: ct
j=1 πjhj,
where π = sparsemax(a) is a sparse vector
of alignment scores in the simplex.

:= (cid:80)J

In Luong et al.’s single-headed attention, the at-
tentional hidden state ˜st = tanh(Ws[ct; st]) is
computed by a concatenation layer from the con-
text vector and pre-attention hidden state. How-
ever, our two-headed attention mechanism pro-
duces two context vectors and so must be com-
puted differently. We use two different formula-
tions, which we describe next.

DOUBLEATTN uses the same strategy for com-
bining multiple context vectors as ´Acs (2018): the
lemma and inﬂection context vectors ut and vt
and the target hidden state st are inputs to a con-
catenation layer:

˜st = tanh(Wd[ut; vt; st])

(2)

where Wd ∈ RD×3D.

GATEDATTN,
on the other hand, computes sep-
arate candidate attentional hidden states for the
two context vectors:

˜sut = tanh(Wu[ut; st])
˜svt = tanh(Wv[vt; st])

(3)

(4)

where Wu, Wv ∈ RD×2D. We deﬁne gate
weights Wg ∈ R2×3D and gate bias bg ∈ R2 and

Model

DOUBLEATTN
GATEDATTN

Baseline (Wu and Cotterell, 2019)

48.549

48.999
50.179

1.2918
1.3209

1.3281

Acc. ↑

Lev. Dist. ↓

Hyperparameter

Character embedding size
Tag embedding size
Language embedding size
RNN size
Lemma encoder layers
Inﬂection encoder layers
Dropout

Value

180
180
20
200
2
{1, 2}
{0.3, 0.4, 0.5}

Table 1: Task 1 test results on the SIGMORPHON
2019 Shared Task, averaged across language pairs.

use a sparse gate to compute weights pt ∈ (cid:52)2 for
the two candidate states:

pt = sparsemax(Wg[ut; vt; st] + bg)

(5)

We then stack the two candidate states ˜sut and ˜svt
into a matrix ˜St ∈ R2×D and use the gate weights
to compute ˜st as a weighted sum of them:

˜st = pt ˜S

(6)

Just as a two-dimensional softmax is equivalent
to a sigmoid, this two-dimensional sparsemax is
a hard sigmoid, as was pointed out by Martins and
Astudillo (2016). It provides extra interpretability
in the form of a three-way answer about what is
relevant at a time step: the lemma, the inﬂections,
or both.

Sparse outputs After the attentional hidden
state is computed, an output layer computes scores
for each output type z = Wz ˜s + bz. These are
then converted into a sparse probability distribu-
tion p(cid:63) = sparsemax(z). The model is trained
to minimize the sparsemax loss (Martins and As-
tudillo, 2016), deﬁned as

1
2

Lsparsemax(y, z) :=

((cid:107)ey − z(cid:107)2 − (cid:107)p(cid:63) − z(cid:107)2)
(7)
where y is the index of the gold target and ey is
a one-hot vector. The sparsemax loss is differen-
tiable, convex, and has a margin, and its gradient
is sparse. Although softmax-based models use the
cross entropy loss, this is not possible for our mod-
els because the cross entropy loss is inﬁnite when
the model assigns zero probability to the gold tar-
get.

3 Results

Our test results are shown in Table 1. Our two
models ranked second and third among ofﬁcial
submissions to Task 1.

Table 2: Hyperparameters for all models. Bracketed
values were tuned individually for each language pair.

3.1 Experimental set-up

Each model was trained with early stopping for a
maximum of 30 epochs with a batch size of 64. We
used the Adam optimizer (Kingma and Ba, 2015)
with a learning rate of 10−3, which was halved
when validation accuracy failed to improve for
three consecutive epochs. We tuned the dropout
and the number of inﬂection encoder layers on a
separate grid for each language pair. Our hyperpa-
rameter ranges are shown in Table 2. At test time,
we decoded with a beam size of 5. We oversam-
pled the low resource training data 100 times and
did not use synthetic data or ﬁlter the corpora. We
implemented our models in PyTorch (Paszke et al.,
2017) with a codebase derived from OpenNMT-py
(Klein et al., 2017).2

Hyperparameters for Uzbek The Uzbek train-
ing set contains only 1060 examples, much
smaller than the other high resource corpora, and
initial results with Uzbek language pairs were
poor. We improved performance by oversampling
the Uzbek data 10 times (yielding roughly the
same high-low balance as the other pairs) and re-
ducing the initial learning rate to 10−4.

4 Analysis

Next we interpret our models’ behavior on a selec-
tion of language pairs from Task 1.

4.1 Sparse Attention

Table 3 shows the sparsity of our attention mech-
anisms, averaged across language families. The
attention is extremely sparse, especially over the
lemma: models attend to fewer than 1.1 lemma
characters per target character on average. Spar-
sity is more varied between language families in
the inﬂection attention. This may be explained by

2Our code is available at https://github.com/

deep-spin/SIGMORPHON2019.

Family

Afro-Asiatic
Baltic
Celtic
Dravidian
Germanic
Greek
Indo-Iranian
Murrinhpatha
Niger-Congo
NW Caucasian
Quechua
Romance
Slavic
Turkic
Uralic

DOUBLEATTN
Lemma

GATEDATTN

Inﬂ. Lemma

Inﬂ. Total

#Langs

1.11
1.02
1.23
1.69
1.05
1.15
1.10
1.16
1.07
1.02
1.24
1.01
1.08
1.06
1.03

1.55
1.33
1.54
1.42
1.56
1.52
1.35
1.28
1.30
1.08
1.19
1.27
1.31
1.09
1.21

1.14
1.02
1.20
1.86
1.05
1.18
1.11
1.15
1.04
1.02
1.09
1.02
1.07
1.08
1.04

1.27
1.34
1.59
1.35
1.39
1.62
1.37
1.35
1.09
1.20
1.23
1.30
1.35
1.15
1.31

1.62
1.54
2.00
1.95
1.61
2.11
1.67
1.71
1.11
1.25
1.42
1.39
1.55
1.35
1.48

1.56

5
1
12
1
17
1
7
1
1
2
1
10
9
20
12

Overall

1.09

1.33

1.09

1.33

100

Table 3: Average number of positions with nonzero attention per target time step on the Task 1 development sets,
grouped by the family of the low resource language. For DOUBLEATTN, this is simply averaged over all target
time steps. For GATEDATTN, the lemma attention nonzeros are summed only over time steps in which the gate
is active for the lemma, and similarly for the inﬂection attention nonzeros. The ‘Total’ column for GATEDATTN
indicates the average number of nonzeros over all time steps after accounting for the gate.

typological differences between languages, which
we next analyze in detail.

Turkic languages
are characterized by concate-
native inﬂections (Bickel and Nichols, 2013b)
which represent individual features (monoexpo-
nence; Bickel and Nichols, 2013a). Monoexpo-
nence should allow the inﬂection attention to con-
centrate on a single tag at a time, and Table 3 con-
ﬁrms that Turkic inﬂection attention is among the
sparsest for both DOUBLEATTN and GATEDATTN
models. The Azeri attention plot in Figure 1 illus-
trates that the inﬂection attention usually focuses
on a single morpheme at a time, with some dis-
crepancies at morpheme boundaries, where other
tags may be relevant because of voicing assimi-
lation rules. Furthermore, the sparse gate gener-
ally allows the model to focus on only one atten-
tion head at a time: in the Azeri example, there
is only one position at which both attention heads
are used. This position is the ﬁnal consonant of
the lemma, which appears to change because of
a phonological environment created by the sufﬁx.
The shared task results suggest that sparse inﬂec-
tion attention is a good inductive bias for aggluti-
native languages: one of our models has the best

test accuracy among task submissions on 11 of
20 pairs where the low resource language is Tur-
kic and 11 of 12 pairs in the typologically similar
Uralic languages.

Germanic languages
present different chal-
lenges, which may explain our models’ less sparse
inﬂection attention. Often several inﬂections are
fused into a single afﬁx; a familiar example is the
German sufﬁx st, which marks a verb as present
tense, second person, and singular, but has no sep-
arable parts that represent these features individ-
ually. The North Frisian plot in Figure 1 demon-
strates the less sparse nature of Germanic inﬂec-
tion attention. Producing “wulst” from the lemma
“wel” requires both a sufﬁx and a change to the
lemma, and multiple inﬂectional tags are attended
to at several time steps. The fusional nature of
the morphology means there is not a clear align-
ment between the inﬂected sequence and the tags.
This in reﬂected in the fact that at many time steps,
DOUBLEATTN and GATEDATTN disagree about
which tags to attend to. Unlike in the Turkic ex-
ample, GATEDATTN’s gate usually gives weight
to both attention heads. This makes sense because
the inﬂection requires a change to the lemma, not

Figure 1: Pairs of attention plots for Azeri (left) and North Frisian (right) words with models trained on the Turkish–
Azeri and Danish–North Frisian language pairs. Within each pair, the left plot comes from the DOUBLEATTN
model and the right plot from GATEDATTN. Gray squares have zero attention weight. The dashed vertical line
separates the lemma and inﬂection attention heads. Attention values in the GATEDATTN plots are scaled by gate
weights, which is why there is no inﬂection attention for the ﬁrst several positions of the Azeri word.

just a sufﬁx that follows it.

4.2 Sparse Output Layer

Sparse output probabilities provide tools for anal-
ysis that are not available to softmax-based mod-
els: when no hypotheses are pruned, they provide
a certiﬁcate that beam search is exact; when only
one hypothesis is possible, this gives an indication
of the model’s certainty; and when probability is
distributed among a small set of hypotheses, it is
easy to reason about what phenomena continue to
confuse the model.

Certainty When the probability distribution is
completely concentrated at each time step,
the
model will be able to generate only one hypoth-
esis, regardless of the beam width. When this
happens for a particular input, the model can be
said to be certain for that input. This also triv-
ially guarantees that beam search is exact because
no hypotheses have been pruned. As Figure 2
shows, certainty is a strong indication of perfor-
mance. This suggests future work using certainty
as a validation metric alternative to accuracy and
loss.

Interpretable ambiguity Our Turkish–Azeri
GATEDATTN model demonstrates that there is
also useful information to be gleaned from the
cases where the model produces multiple hypothe-
ses. Of the 100 examples in the development set,
GATEDATTN concentrated all probability mass
into a single hypothesis for 79 of them, but the
other 21 examples exhibit ambiguities that have
linguistically plausible interpretations:

Figure 2: Percentage of inputs for which the selected
GATEDATTN model concentrates all probability into a
single hypothesis compared to the word-level accuracy
on the development set. Each point is a language pair.

• Consonant alternations In 13 of the 21 ex-
amples, the hypotheses differ in their treat-
ment of stop consonants, which have very
similar phonological alternations in the two
languages that are represented in orthogra-
phy. The ambiguity is a sign that the model
has not mastered Azeri phonological rules.
Nine of the examples concern lemma-ﬁnal
“k” and “q”, which have slightly different
rules in Azeri than Turkish.3

3This judgment is based on inspection of the Azeri data

and prior knowledge of Turkish.

72.7%
e

c¸

i

27.3%
@

c

c

67.5%
e
32.5%
@
52.0%
e
48.0%
@

k

k
k

k

</s>

</s>
</s>

</s>

Figure 3: The Turkish–Azeri GATEDATTN model’s full
beam search for the Azeri lemma “ic¸mek” and the tags
V 3 SG FUT. All other sequences have zero proba-
bility. The correct form is “ic¸@c@k”, while the model
prefers “ic¸ecek”, which would be correct with Turkish
vowel harmony rules.

• Vowel harmony In two other examples, the
model produced multiple guesses for the
vowels in the future tense marker. One of
these examples is shown in Figure 3. In both
instances, Azeri vowel harmony rules would
generate “@” in the sufﬁx, but the model in-
stead produced “e”, which is correct with
Turkish vowel harmony. This shows the in-
ﬂuence of the high resource language.

• Other cases The last six non-certain exam-
ples consist of a loanword with an unusual
character sequence, two instances where one
hypothesis has the wrong possessive sufﬁx,
and two where a hypothesis inserts or drops
a character. The top prediction was nonethe-
less correct in all six.

This sort of analysis is not possible with tradi-
tional dense models because probability can never
become concentrated in a small set of hypotheses
and it is impossible to separate legitimate ambigu-
ities from the long tail of implausible hypotheses.

Paradigm completion Figure 3 suggests that
our models do a good job of concentrating prob-
ability in a small number of hypotheses. This
raises the question of whether, by underspecify-
ing the inﬂectional tags, the set of possible hy-
potheses in the beam can be made to resemble
a lemma’s complete paradigm. To investigate,
we trained monolingual models with the English,
German, and Turkish data from the high resource
setting of Task 1 of the CoNLL–SIGMORPHON
2018 Shared Task (Cotterell et al., 2018). We used
mostly the same hyperparameters as for this year’s
submission, except that there are no language em-
beddings, and the inﬂection tags are not used and
the models have single-headed attention over the
lemma sequences. We increased the beam width to

10 in order to accomodate the models’ greater un-
certainty. With English, this often works well: for
the regular verb “jitter”, the model’s only possible
hypotheses are “jittered”, “jittering”, “jitters”, and
“jitter”, which is the complete paradigm. Irregu-
lar verbs often have a handful of other hypotheses,
and sometimes the beam gives some probability
to misspellings. Something similar can be seen
in German, although the beam rarely contains all
surface forms. For German nouns, the beam often
shows uncertainty about plural formation: the hy-
potheses for “Nadelbaum” include “Nadelbaume”,
“Nadelb¨aume”, and “Nadelb¨aumer”, all of which
are plausible German plurals. Turkish has very
large paradigms, so in general it is not possible
to ﬁt all forms into a beam of any reasonable size.
However, the hypotheses in the beam do typically
correspond to correct forms.

5 Conclusion

We presented a new style of seq2seq model which
brings together two-headed attention ( ´Acs, 2018)
and sparse modeling for morphological inﬂection
(Peters et al., 2019). Our models learn sparse at-
tention distributions in both attention heads. Their
sparse probability distribution over hypotheses of-
ten allows beam search to become exact, while the
remaining ambiguities often have a clear linguistic
interpretation. The two versions of our model rank
second and third among submissions to Task 1.

Acknowledgments

This work was supported by the European Re-
search Council (ERC StG DeepSPIN 758969),
and by the Fundac¸ ˜ao para a Ciˆencia e Tecnolo-
gia through contracts UID/EEA/50008/2019 and
CMUPERI/TIC/0046/2014 (GoLocal). We thank
Gonc¸alo Correia, Aitor Egurtzegi, Erick Fonseca,
Pedro Martins, Tsvetomila Mihaylova, Vlad Nic-
ulae, Marcos Treviso, and the anonymous review-
ers, for helpful discussion and feedback.

References

Judit

´Acs. 2018. BME-HAS system for CoNLL–
SIGMORPHON 2018 shared task: Universal mor-
phological reinﬂection. Proceedings of the CoNLL
SIGMORPHON 2018 Shared Task: Universal Mor-
phological Reinﬂection, pages 121–126.

Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inﬂection generation with hard monotonic atten-
tion. In Proc. ACL.

shared task: Crosslinguality and context in morphol-
In Proceedings of the 16th SIGMORPHON
ogy.
Workshop on Computational Research in Phonetics,
Phonology, and Morphology”, Florence, Italy. As-
sociation for Computational Linguistics.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.
In Proc. NeurIPS Autodiff Workshop.

Ben Peters, Vlad Niculae, and Andr´e FT Martins. 2019.
In Proc.

Sparse Sequence-to-Sequence Models.
ACL.

Shijie Wu and Ryan Cotterell. 2019.

Exact Hard
Monotonic Attention for Character-Level Transduc-
tion. Proc. ACL.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level
transduction. In Proc. EMNLP.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

Balthasar Bickel and Johanna Nichols. 2013a. Ex-
ponence of selected inﬂectional formatives.
In
Matthew S. Dryer and Martin Haspelmath, editors,
The World Atlas of Language Structures Online.
Max Planck Institute for Evolutionary Anthropol-
ogy, Leipzig.

Balthasar Bickel and Johanna Nichols. 2013b. Fusion
of selected inﬂectional formatives.
In Matthew S.
Dryer and Martin Haspelmath, editors, The World
Atlas of Language Structures Online. Max Planck
Institute for Evolutionary Anthropology, Leipzig.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
G˙eraldine Walther, Ekaterina Vylomova, Arya D
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, David Yarowsky,
Jason Eisner, and Mans Hulden. 2018.
The
CoNLL–SIGMORPHON 2018 shared task: Uni-
versal morphological reinﬂection. Proc. CoNLL–
SIGMORPHON.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Katharina Kann and Hinrich Sch¨utze. 2016. Single-
Model Encoder-Decoder with Explicit Morphologi-
cal Representation for Reinﬂection. In Proc. ACL.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proc. ICLR.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
arXiv e-prints.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proc. EMNLP.

Chaitanya Malaviya, Pedro Ferreira, and Andr´e FT
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proc. ACL.

Andr´e FT Martins and Ram´on Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
of attention and multi-label classiﬁcation. In Proc.
of ICML.

Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu,
Chaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-
rett Nicolai, Christo Kirov, Miikka Silfverberg, Se-
bastian Mielke, Jeffrey Heinz, Ryan Cotterell, and
Mans Hulden. 2019. The SIGMORPHON 2019

