8
1
0
2
 
y
a
M
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
2
1
6
0
.
5
0
8
1
:
v
i
X
r
a

Narrative Modeling with Memory Chains and Semantic Supervision

Fei Liu

Trevor Cohn

Timothy Baldwin

School of Computing and Information Systems
The University of Melbourne
Victoria, Australia
fliu3@student.unimelb.edu.au
t.cohn@unimelb.edu.au tb@ldwin.net

Abstract

Story comprehension requires a deep se-
mantic understanding of
the narrative,
making it a challenging task. Inspired by
previous studies on ROC Story Cloze Test,
we propose a novel method, tracking var-
ious semantic aspects with external neural
memory chains while encouraging each
to focus on a particular semantic aspect.
Evaluated on the task of story ending pre-
diction, our model demonstrates superior
performance to a collection of competitive
baselines, setting a new state of the art. 1

1 Introduction

(Winograd,

has
artiﬁcial

narrative
long-standing

in
1972; Turner,

comprehension
challenge

been
Story
in-
a
1994;
telligence
Schubert and Hwang, 2000). The difﬁculties of
this task arise from the necessity for understand-
ing not only narratives, but also commonsense
and normative social behaviour (Charniak, 1972).
Of particular interest in this paper is the work
by Mostafazadeh et al. (2016) on understanding
commonsense stories in the form of a Story Cloze
Test: given a short story, we must predict the most
coherent sentential ending from two options (e.g.
see Figure 1).

Many attempts have been made to solve
this problem, based either on linear classiﬁers
with handcrafted features (Schwartz et al., 2017;
Chaturvedi et al., 2017), or representation learning
via deep learning models (Mihaylov and Frank,
2017; Bugert et al., 2017; Mostafazadeh et al.,
2017). Another widely used component of com-
petitive systems is language model-based features,
which require training on large corpora in the story
domain.

Context: Sam loved his old belt. He matched it with
everything. Unfortunately he gained too much weight.
It became too small.
Coherent Ending: Sam went on a diet.
Incoherent Ending: Sam was happy.

Figure 1: Story Cloze Test example.

The

current

approach

of
state-of-the-art
Chaturvedi et al. (2017) is based on understanding
the context from three perspectives: (1) event
sequence, (2) sentiment trajectory, and (3) topic
consistency. Chaturvedi et al. (2017) adopt exter-
nal tools to recognise relevant aspect-triggering
words, and manually design features to incorpo-
rate them into the classiﬁer. While identifying
triggers has been made easy by the use of various
linguistic resources, crafting such features is
time consuming and requires domain-speciﬁc
knowledge along with repeated experimentation.

Inspired by the argument for tracking the dy-
namics of events, sentiment and topic, we propose
to address the issues identiﬁed above with mul-
tiple external memory chains, each responsible
for a single aspect. Building on Recurrent Entity
Networks (EntNets), a superior framework to
LSTMs demonstrated by Henaff et al. (2017) for
reasoning-focused question answering and cloze-
style reading comprehension, we introduce a novel
multi-task learning objective, encouraging each
chain to focus on a particular aspect. While still
making use of external linguistic resources, we do
not extract or design features from them but rather
utilise such tools to generate labels. The generated
labels are then used to guide training so that each
chain focuses on tracking a particular aspect. At
test time, our model is free of feature engineering
such that, once trained, it can be easily deployed to
unseen data without preprocessing. Moreover, our

1Code available at http://github.com/liufly/narrative-modeling.

approach also differs in the lack of a ROC Stories
language model component, eliminating the need
for large, domain-speciﬁc training corpora.

Evaluated on the task of Story Cloze Test, our
model outperforms a collection of competitive
baselines, achieving state-of-the-art performance
under more modest data requirements.

2 Methodology

In the story cloze test, given a story of length
L, consisting of a sequence of context sentences
c = {c1, c2, . . . , cL}, we are interested in predict-
ing the coherent ending to the story out of two end-
ing options o1 and o2. Following previous studies
(Schwartz et al., 2017), we frame this problem as a
binary classiﬁcation task. Assuming o1 and o2 are
the logical and inconsistent endings respectively
with their associated labels being y1 and y2, we
assign y1 = 1 and y2 = 0. At test time, given
a pair of possible endings, the system returns the
one with the higher score as the prediction. In this
section, we ﬁrst describe the model architecture
and then detail how we identify aspect-triggering
words in text and incorporate them in training.

2.1 Proposed Model

i,

−→h

−→h

i−1) where w

First, to take neighbouring contexts into account,
we process context sentences and ending options
at the word level with a bi-directional gated recur-
rent unit (“Bi-GRU”: Chung et al. (2014)):
i =
−−→
GRU(w
i is the embedding of
the i-th word in the story or ending option. The
backward hidden representation
i is computed
analogously but with a different set of GRU pa-
←−h
rameters. We simply take the sum h
i
as the representation at time i. An ending option,
denoted o, is represented by taking the sum of the
ﬁnal states of the same Bi-GRU over the ending
option word sequence.

i =

i +

−→h

←−h

This representation is then taken as input
to a Recurrent Entity Network (“EntNet”:
Henaff et al. (2017)), capable of tracking the state
of the world with external memory. Developed in
the context of machine reading comprehension,
EntNet maintains a number of “memory chains”
— one for each entity — and dynamically up-
dates the representations of them as it progresses
through a story. Here, we borrow the concept of
“entity” and use each chain to track a unique as-
pect. An illustration of EntNet is provided in
Figure 2.

key kj

˜mj
i

φ

L

update
gate
gj
i

σ

C

⊙

+

memory mj

i−1

hi

mj
i

Figure 2: Illustration of EntNet with a single
memory chain at time i. φ and σ represent Equa-
tions 1 and 2, while circled nodes L, C, ⊙ and
+ depict the location, content terms, Hadamard
product, and addition, resp.

Memory update candidate. At every time step
i, the internal memory update candidate
i for
the j-th memory chain is computed as:
−→U −→mj

−→W h

−→
˜mj

−→V kj +

−→
˜mj

(1)

i = φ(

i−1 +

i)

where kj is the embedding for the j-th entity
−→W are trainable parameters, and
(key),
φ is the parametric ReLU (He et al., 2015).

−→V and

−→U ,

Memory update. The update of each memory
chain is controlled by a gating mechanism:

−→g j
−→
˚mj

i = σ(h
i = −→mj

i · kj + h
i−1 + −→g j
i ⊙

i · −→mj
−→
˜mj
i

i−1)

(2)

(3)

where ⊙ denotes Hadamard product (resulting
−→
˚mj
i ),
in the unnormalised memory representation
and σ is the sigmoid function. The gate −→g j
i con-
trols how much the memory chain should be up-
dated, a decision factoring in two elements: (1) the
“location” term, quantifying how related the cur-
rent input h
i (the output of the Bi-GRU at time i)
is to the key kj of the entity being tracked; and (2)
the “content” term, measuring the similarity be-
tween the input and the current state −→mj
i−1 of the
entity tracked by the j-th memory chain.

i =

−→
˚mj

Normalisation. We normalise each memory
−→
−→
representation: −→mj
˚mj
˚mj
i k where k
i k de-
i /k
−→
˚mj
notes the Euclidean norm of
i . In doing so, we
allow the model to forget in the sense that, as −→mj
i
is constrained to be lying on the surface of the unit
sphere, adding new information carried by
i and
then performing normalisation inevitably causes
forgetting in that the cosine distance between the
original and updated memory decreases.

−→
˜mj

In contrast to EntNet, we
Bi-directionality.
apply the above steps in both directions, scanning
a story both forward and backward, with arrows
denoting the processing direction. ←−mj
i is computed
analogously to −→mj
i but with a different set of pa-
i + ←−mj
i = −→mj
rameters, and mj
i . We further deﬁne
gj
i to be the average of the gate values of both di-
rections at time i for the j-th chain:

i = (−→g j
gj

i + ←−g j

i )/2

(4)

which will be used for semantic supervision.

Final classiﬁer. The ﬁnal prediction ˆy to an end-
ing option given its context story is performed by
incorporating the last states of all memory chains
in the form of a weighted sum u:

pj = softmax((kj)⊤Watto)
u = X
j

pjmj
T

(5)

(6)

where T denotes the total number of words in a
story and Watt is a trainable weight matrix. We
then transform u to get the ﬁnal prediction:

ˆy = σ(Rφ(Hu + o))

(7)

where R and H are trainable weight matrices.

2.2 Semantically Motivated Memory Chains

In order to encourage each chain to focus on a
particular aspect, we supervise the activation of
each update gate (Equation 2) with binary labels.
Intuitively, for the sentiment chain, a sentiment-
bearing word (e.g. amazing) receives a label of 1,
allowing the model to open the gate and therefore
change the internal state relevant to this aspect,
while a neutral one (e.g. school) should not trig-
ger the activation with an assigned label of 0. To
achieve this, we use three memory chains to in-
dependently track each of: (1) event sequence, (2)
sentiment trajectory, and (3) topical consistency.
To supervise the memory update gates of these
chains, we design three sequences of binary labels:
lj = {lj
T } for j ∈ [1, 3] representing
event, sentiment, and topic, and lj
i ∈ {0, 1}. The
label at time i for the j-th aspect is only assigned
a value of 1 if the word is a trigger for that particu-
lar aspect: lj
i = 0. During train-
ing, we utilise such sequences of binary labels to
supervise the memory update gate activations of
each chain. Speciﬁcally, each chain is encouraged

i = 1; otherwise lj

2, . . . , lj

1, lj

Ricky fell while hiking in the woods

lEvent
lSentiment
lT opic

0
0
1

1
1
1

0
0
0

1
0
1

0
0
0

0
0
0

1
0
1

Table 1: An example of the linguistically moti-
vated memory chain supervision binary labels.

to open its gate only when it sees a trigger bearing
information semantically sensitive to that particu-
lar aspect. Note that at test time, we do not apply
such supervision. This effectively becomes a bi-
nary tagging task for each memory chain indepen-
dently and results in individual memory chains be-
ing sensitive to only a speciﬁc set of triggers bear-
ing one of the three types of signals.

While largely inspired by Chaturvedi et al.
(2017), our approach differs in how we extract
and use such features. Also note that, while still
making use of external linguistic resources to de-
tect trigger words, our approach lets the mem-
ory chains decide how such words should in-
ﬂuence the ﬁnal prediction, as opposed to the
handcrafted conditional probability features of
Chaturvedi et al. (2017).

To identify the trigger words, we use external
linguistic tools, and mark trigger words for each
aspect with a label of 1. An example is presented
in Table 1, noting that the same word can act as
trigger for multiple aspects.

Event sequence. We parse each sentence into
representation with SEMAFOR
its FrameNet
(Das et al., 2010), and identify each frame target
(word or phrase tokens evoking a frame).

Sentiment
trajectory. Following
Chaturvedi et al. (2017), we utilise a pre-compiled
list of sentiment words (Liu et al., 2005). To take
negation into account, we parse each sentence
with the Stanford Core NLP dependency
parser (Manning et al., 2014; Chen and Manning,
2014) and include negation words as trigger
words.

Topical consistency. We process each sen-
tence with the Stanford Core NLP POS tag-
ger and identify nouns and verbs,
following
Chaturvedi et al. (2017).

2.3 Training Loss

In addition to the cross entropy loss of the ﬁnal
prediction of right/wrong endings, we also take

into account the memory update gate supervision
of each chain by adding the second term. More for-
mally, the model is trained to minimise the loss:

L = XEntropy(y, ˆy) + α X
i,j

XEntropy(lj

i , gj
i )

where ˆy and gj
i are deﬁned in Equations 7 and 4 re-
spectively, y is the gold label for the current ending
option o, and lj
i is the semantic supervision binary
label at time i for the j-th memory chain. In our
experiments, we empirically set α to 0.5 based on
development data.

3 Experiments

3.1 Experimental Setup

Dataset. To test the effectiveness of our model,
we employ the Story Cloze Test dataset of
Mostafazadeh et al. (2016). The development and
test set each consist of 1,871 4-sentence stories,
each with a pair of ending options. Consistent with
previous studies, we split the development set into
a training and validation set (for early stopping),
resulting in 1,683 and 188 in each set, resp. Note
that while most current approaches make use of
the much larger training set, comprised of 100K 5-
sentence ROC stories (with coherent endings only,
also released as part of the dataset) to train a lan-
guage model, we make no use of this data.

Model conﬁguration. We initialise our model
with word2vec embeddings (300-D, pre-trained
on 100B Google News articles, not updated dur-
ing training: Mikolov et al. (2013a,b)). In addition
to the three supervised chains, we also add a “free”
chain, unconstrained to any semantic aspect.

Training is carried out over 200 epochs with
the FTRL optimiser (McMahan et al., 2013) and a
batch size of 128 and learning rate of 0.1. We use
the following hyper-parameters for weight matri-
ces in both directions: R ∈ R300×1, H, U, V, W
are all matrices of size R300×300, and hidden size
of the Bi-GRU is 300. Dropout is applied to the
output of φ in the ﬁnal classiﬁer (Equation 7) with
a rate of 0.2. Moreover, we employ the technique
introduced by Gal and Ghahramani (2016) where
the same dropout mask is applied at every step to
the input w
i to the
memory chains with rates of 0.5 and 0.2 respec-
tively. Lastly, to curb overﬁtting, we regularise the
last layer (Equation 7) with an L2 penalty on its
weights: λkRk where λ = 0.001.

i to the Bi-GRU and the input h

Model

Acc.

SD

DSSM
58.5 —
TBMIHAYLOV 72.8 —
MSAP
75.2 —
HCM
77.6 —
EntNet †
77.6 ±0.5
78.5 ±0.5
Our model†

Table 2: Performance on the Story Cloze test set.
Bold = best performance, † = ave. accuracy over 5
runs, SD = standard deviation, “—” = not reported.

Evaluation. Following previous
studies, we
evaluate the performance in terms of coherent
ending prediction accuracy: #correct
#total . Here, we re-
port the average accuracy over 5 runs with dif-
ferent random seeds. Echoing Melis et al. (2018),
we also observe some variance in model perfor-
mance even if training is carried out with the same
random seed, which is largely due to the non-
deterministic ordering of ﬂoating-point operations
in our environment (Tensorflow (Abadi et al.,
2015) with a single GPU). Therefore, to account
for the randomness, we further train our model 5
times for each random seed and select the model
with the best validation performance.

We

2017):

a
the

collection

(Mostafazadeh et al.,

of
against
benchmark
strong baselines,
top-3 sys-
including
tems of the 2017 LSDSem workshop shared
MSAP
task
(Schwartz et al., 2017), HCM (Chaturvedi et al.,
2017)2, and TBMIHAYLOV (Mihaylov and Frank,
2017). The ﬁrst two primarily rely on a sim-
ple logistic regression classiﬁer, both taking
linguistic and probability features from a ROC
Stories domain-speciﬁc neural language model.
TBMIHAYLOV is LSTM-based; we also include
DSSM (Mostafazadeh et al., 2016). We addi-
tionally implement a bi-directional EntNet
(Henaff et al., 2017) with the
same hyper-
parameter settings as our model and no semantic
supervision.3

3.2 Results and Discussion

The experimental results are shown in Table 2.

State-of-the-art results. Our model outper-
forms a collection of strong baselines, setting a

2We take the performance reported in a subsequent paper,

3.2% better than the original submission to the shared task.

3EntNet is also subject to the same model selection cri-

terion described above.

Model

Our model
—bi-directionality
—all semantic supervision

—event sequence
—sentiment trajectory
—topical consistency
—free chain

Acc.

SD

78.5 ±0.5
77.8 ±0.7
77.6 ±0.5
78.7 ±0.2
75.9 ±0.4
77.3 ±0.4
77.0 ±0.4

Table 3: Ablation study. Average accuracy over
5 runs on the Story Cloze test set (all subject to
the same model selection criterion as our model).
Bold: best performance. SD: standard deviation.

performance down to 77.8, is inferior to its bi-
directional cousin. Removing all semantic super-
vision (resulting in 4 free memory chains) is
also damaging to the accuracy (dropping to 77.6).
types of semantic super-
Among the different
vision, we see various degrees of performance
degradation, with removing sentiment trajectory
being the most detrimental, reﬂecting its value
to the task. Interestingly, we observe improve-
ment when removing event sequence supervision
from consideration. We suspect that this is mainly
due to the noise introduced by the rather inaccu-
rate FrameNet representation output of SEMAFOR
(F1 = 61.4% in frame identiﬁcation as reported in
Das et al. (2010)). While it is possible that replac-
ing SEMAFOR with the SemLM approach (with
the extended frame deﬁnition to include explicit
discourse markers, e.g. but) in Peng and Roth
(2016) and Chaturvedi et al. (2017) may poten-
tially reduce the amount of noise, we leave this
exercise for future work.

4 Conclusion

In this paper, we have proposed a novel model
for tracking various semantic aspects with exter-
nal memory chains. While requiring less domain-
speciﬁc training data, our model achieves state-
of-the-art performance on the task of ROC Story
Cloze ending prediction, beating a collection of
strong baselines.

Acknowledgments

We thank the anonymous reviewers for their
valuable feedback, and gratefully acknowledge
the support of Australian Government Research
Training Program Scholarship. This work was

Event Key
Sentiment Key
Event Word Sentiment Word Topic Word

Topic Key Free Key

Figure 3: t-SNE scatter plot of aspect-triggering
words (output representations h
i by Bi-GRU,
500 randomly sampled from each aspect) and the
learned keys. EntNet (left) vs. our model (right).
Best viewed in colour.

new state of the art. Note that this is achieved with-
out any external linguistic resources at test time or
domain-speciﬁc language model-based probabil-
ity features, highlighting the effectiveness of the
proposed model.

EntNet vs. our model. We see clear improve-
ments of the proposed method over EntNet,
an absolute gain of 0.9% in accuracy. This vali-
dates the hypothesis that encouraging each mem-
ory chain to focus on a unique, well-deﬁned aspect
is beneﬁcial.

Discussion. To better understand the beneﬁts of
the proposed method, we visualise the learned
word representations (output representation of the
Bi-GRU, h
i) and keys between EntNet and our
model in Figure 3. With EntNet, while senti-
ment words form a loose cluster, words bearing
event and topic signal are placed in close prox-
imity and are largely indistinguishable. With our
model, on the other hand, the degree of separation
is much clearer. The intersection of a small por-
tion of the event and topic groups is largely due to
the fact that both aspects include verbs. Another
interesting perspective is the location of the au-
tomatically learned keys (displayed as diamonds):
while all the keys with EntNet end up overlap-
ping, indicating little difference among them, the
keys learned by our method demonstrate seman-
tic diversity, with each placed within its respective
cluster. Note that the free key is learned to com-
plement the event key, a difﬁcult challenge given
the two disjoint clusters.

Ablation study. We further perform a ablation
study to analyse the utility of each component
in Table 3. The uni-directional variant, with its

also supported in part by the Australian Research
Council.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur,
Josh Levenberg, Dandelion Man´e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya
Sutskever, Kunal Talwar, Paul Tucker, Vincent Van-
houcke, Vijay Vasudevan, Fernanda Vi´egas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Mar-
tin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015.
TensorFlow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.

Michael Bugert, Yevgeniy Puzikov, Andreas R¨uckl´e,
Judith Eckle-Kohler, Teresa Martin, Eugenio
Mart´ınez-C´amara, Daniil Sorokin, Maxime Peyrard,
and Iryna Gurevych. 2017. LSDSem 2017: Explor-
ing data generation methods for the story cloze test.
In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics (LSDSem 2017), pages 56–61, Valencia,
Spain.

Eugene Charniak. 1972. Toward a model of chil-
dren”s story comprehension.
Technical report,
Massachusetts Institute of Technology, Cambridge,
USA.

Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
2017. Story comprehension for predicting what hap-
pens next. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2017), pages 1603–1614, Copen-
hagen, Denmark.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), pages 740–750, Doha, Qatar.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Proceedings of the NIPS 2014 Deep Learn-
ing and Representation Learning Workshop.

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
In Human Language Technologies: The
parsing.
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT 2010), pages 948–956, Los
Angeles, USA.

Yarin Gal and Zoubin Ghahramani. 2016. A theo-
retically grounded application of dropout in recur-
rent neural networks. In Proceedings of Advances
in Neural Information Processing Systems (NIPS
2016), pages 1027–1035, Barcelona, Spain.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectiﬁers: Surpass-
ing human-level performance on ImageNet classiﬁ-
In Proceedings of the 2015 IEEE Interna-
cation.
tional Conference on Computer Vision (ICCV 2015),
pages 1026–1034, Washington, DC, USA.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the world
In Proceed-
state with recurrent entity networks.
ings of the 5th International Conference on Learn-
ing Representations (ICLR 2017), Toulon, France.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Proceedings of the 14th Interna-
tional Conference on World Wide Web (WWW 2005),
pages 342–351, Chiba, Japan.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

H. Brendan McMahan, Gary Holt, David Sculley,
Michael Young, Dietmar Ebner, Julian Grady,
Lan Nie, Todd Phillips, Eugene Davydov, Daniel
Golovin, et al. 2013. Ad click prediction: A view
from the trenches. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2013), pages
1222–1230, Chicago, USA.

G´abor Melis, Chris Dyer, and Phil Blunsom. 2018.
On the state of the art of evaluation in neural lan-
In Proceedings of the 6th Inter-
guage models.
national Conference on Learning Representations
(ICLR 2018), Vancouver, Canada.

Todor Mihaylov and Anette Frank. 2017. Story cloze
ending selection baselines and data examination. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 87–92, Valencia, Spain.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word represen-
In Proceedings of the 1st
tations in vector space.
International Conference on Learning Representa-
tions (ICLR 2013), Scottsdale, USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the 27th Annual Conference
on Neural Information Processing Systems (NIPS
2013), pages 3111–3119, Stateline, USA.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding of
In Proceedings of the 2016
commonsense stories.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2016), pages
839–849, San Diego, USA.

Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael Chambers, and James Allen. 2017. LS-
DSem 2017 shared task: The story cloze test.
In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 46–51, Valencia, Spain.

Haoruo Peng and Dan Roth. 2016. Two discourse
driven language models for semantics. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (ACL 2016), pages
290–300, Berlin, Germany.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic logic meets little red riding hood: A com-
prehensive, natural representation for language un-
derstanding.
In Lucja Iwanska and Stuart C.
Shapiro, editors, Natural Language Processing and
Knowledge Representation: Language for Knowl-
edge and Knowledge for Language, pages 111–174.
MIT/AAAI Press, Menlo Park, USA.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. The
effect of different writing tasks on linguistic style:
A case study of the ROC story cloze task. In Pro-
ceedings of the 21st Conference on Computational
Natural Language Learning (CoNLL 2017), pages
15–25, Vancouver, Canada.

Scott R. Turner. 1994.

The Creative Process: A
Computer Model of Storytelling and Creativity.
Lawrence Erlbaum, Hillsdale, USA.

Terry Winograd. 1972. Understanding natural lan-

guage. Cognitive Psychology, 3(1):1–191.

8
1
0
2
 
y
a
M
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
2
1
6
0
.
5
0
8
1
:
v
i
X
r
a

Narrative Modeling with Memory Chains and Semantic Supervision

Fei Liu

Trevor Cohn

Timothy Baldwin

School of Computing and Information Systems
The University of Melbourne
Victoria, Australia
fliu3@student.unimelb.edu.au
t.cohn@unimelb.edu.au tb@ldwin.net

Abstract

Story comprehension requires a deep se-
mantic understanding of
the narrative,
making it a challenging task. Inspired by
previous studies on ROC Story Cloze Test,
we propose a novel method, tracking var-
ious semantic aspects with external neural
memory chains while encouraging each
to focus on a particular semantic aspect.
Evaluated on the task of story ending pre-
diction, our model demonstrates superior
performance to a collection of competitive
baselines, setting a new state of the art. 1

1 Introduction

(Winograd,

has
artiﬁcial

narrative
long-standing

in
1972; Turner,

comprehension
challenge

been
Story
in-
a
1994;
telligence
Schubert and Hwang, 2000). The difﬁculties of
this task arise from the necessity for understand-
ing not only narratives, but also commonsense
and normative social behaviour (Charniak, 1972).
Of particular interest in this paper is the work
by Mostafazadeh et al. (2016) on understanding
commonsense stories in the form of a Story Cloze
Test: given a short story, we must predict the most
coherent sentential ending from two options (e.g.
see Figure 1).

Many attempts have been made to solve
this problem, based either on linear classiﬁers
with handcrafted features (Schwartz et al., 2017;
Chaturvedi et al., 2017), or representation learning
via deep learning models (Mihaylov and Frank,
2017; Bugert et al., 2017; Mostafazadeh et al.,
2017). Another widely used component of com-
petitive systems is language model-based features,
which require training on large corpora in the story
domain.

Context: Sam loved his old belt. He matched it with
everything. Unfortunately he gained too much weight.
It became too small.
Coherent Ending: Sam went on a diet.
Incoherent Ending: Sam was happy.

Figure 1: Story Cloze Test example.

The

current

approach

of
state-of-the-art
Chaturvedi et al. (2017) is based on understanding
the context from three perspectives: (1) event
sequence, (2) sentiment trajectory, and (3) topic
consistency. Chaturvedi et al. (2017) adopt exter-
nal tools to recognise relevant aspect-triggering
words, and manually design features to incorpo-
rate them into the classiﬁer. While identifying
triggers has been made easy by the use of various
linguistic resources, crafting such features is
time consuming and requires domain-speciﬁc
knowledge along with repeated experimentation.

Inspired by the argument for tracking the dy-
namics of events, sentiment and topic, we propose
to address the issues identiﬁed above with mul-
tiple external memory chains, each responsible
for a single aspect. Building on Recurrent Entity
Networks (EntNets), a superior framework to
LSTMs demonstrated by Henaff et al. (2017) for
reasoning-focused question answering and cloze-
style reading comprehension, we introduce a novel
multi-task learning objective, encouraging each
chain to focus on a particular aspect. While still
making use of external linguistic resources, we do
not extract or design features from them but rather
utilise such tools to generate labels. The generated
labels are then used to guide training so that each
chain focuses on tracking a particular aspect. At
test time, our model is free of feature engineering
such that, once trained, it can be easily deployed to
unseen data without preprocessing. Moreover, our

1Code available at http://github.com/liufly/narrative-modeling.

approach also differs in the lack of a ROC Stories
language model component, eliminating the need
for large, domain-speciﬁc training corpora.

Evaluated on the task of Story Cloze Test, our
model outperforms a collection of competitive
baselines, achieving state-of-the-art performance
under more modest data requirements.

2 Methodology

In the story cloze test, given a story of length
L, consisting of a sequence of context sentences
c = {c1, c2, . . . , cL}, we are interested in predict-
ing the coherent ending to the story out of two end-
ing options o1 and o2. Following previous studies
(Schwartz et al., 2017), we frame this problem as a
binary classiﬁcation task. Assuming o1 and o2 are
the logical and inconsistent endings respectively
with their associated labels being y1 and y2, we
assign y1 = 1 and y2 = 0. At test time, given
a pair of possible endings, the system returns the
one with the higher score as the prediction. In this
section, we ﬁrst describe the model architecture
and then detail how we identify aspect-triggering
words in text and incorporate them in training.

2.1 Proposed Model

i,

−→h

−→h

i−1) where w

First, to take neighbouring contexts into account,
we process context sentences and ending options
at the word level with a bi-directional gated recur-
rent unit (“Bi-GRU”: Chung et al. (2014)):
i =
−−→
GRU(w
i is the embedding of
the i-th word in the story or ending option. The
backward hidden representation
i is computed
analogously but with a different set of GRU pa-
←−h
rameters. We simply take the sum h
i
as the representation at time i. An ending option,
denoted o, is represented by taking the sum of the
ﬁnal states of the same Bi-GRU over the ending
option word sequence.

i =

i +

−→h

←−h

This representation is then taken as input
to a Recurrent Entity Network (“EntNet”:
Henaff et al. (2017)), capable of tracking the state
of the world with external memory. Developed in
the context of machine reading comprehension,
EntNet maintains a number of “memory chains”
— one for each entity — and dynamically up-
dates the representations of them as it progresses
through a story. Here, we borrow the concept of
“entity” and use each chain to track a unique as-
pect. An illustration of EntNet is provided in
Figure 2.

key kj

˜mj
i

φ

L

update
gate
gj
i

σ

C

⊙

+

memory mj

i−1

hi

mj
i

Figure 2: Illustration of EntNet with a single
memory chain at time i. φ and σ represent Equa-
tions 1 and 2, while circled nodes L, C, ⊙ and
+ depict the location, content terms, Hadamard
product, and addition, resp.

Memory update candidate. At every time step
i, the internal memory update candidate
i for
the j-th memory chain is computed as:
−→U −→mj

−→W h

−→
˜mj

−→V kj +

−→
˜mj

(1)

i = φ(

i−1 +

i)

where kj is the embedding for the j-th entity
−→W are trainable parameters, and
(key),
φ is the parametric ReLU (He et al., 2015).

−→V and

−→U ,

Memory update. The update of each memory
chain is controlled by a gating mechanism:

−→g j
−→
˚mj

i = σ(h
i = −→mj

i · kj + h
i−1 + −→g j
i ⊙

i · −→mj
−→
˜mj
i

i−1)

(2)

(3)

where ⊙ denotes Hadamard product (resulting
−→
˚mj
i ),
in the unnormalised memory representation
and σ is the sigmoid function. The gate −→g j
i con-
trols how much the memory chain should be up-
dated, a decision factoring in two elements: (1) the
“location” term, quantifying how related the cur-
rent input h
i (the output of the Bi-GRU at time i)
is to the key kj of the entity being tracked; and (2)
the “content” term, measuring the similarity be-
tween the input and the current state −→mj
i−1 of the
entity tracked by the j-th memory chain.

i =

−→
˚mj

Normalisation. We normalise each memory
−→
−→
representation: −→mj
˚mj
˚mj
i k where k
i k de-
i /k
−→
˚mj
notes the Euclidean norm of
i . In doing so, we
allow the model to forget in the sense that, as −→mj
i
is constrained to be lying on the surface of the unit
sphere, adding new information carried by
i and
then performing normalisation inevitably causes
forgetting in that the cosine distance between the
original and updated memory decreases.

−→
˜mj

In contrast to EntNet, we
Bi-directionality.
apply the above steps in both directions, scanning
a story both forward and backward, with arrows
denoting the processing direction. ←−mj
i is computed
analogously to −→mj
i but with a different set of pa-
i + ←−mj
i = −→mj
rameters, and mj
i . We further deﬁne
gj
i to be the average of the gate values of both di-
rections at time i for the j-th chain:

i = (−→g j
gj

i + ←−g j

i )/2

(4)

which will be used for semantic supervision.

Final classiﬁer. The ﬁnal prediction ˆy to an end-
ing option given its context story is performed by
incorporating the last states of all memory chains
in the form of a weighted sum u:

pj = softmax((kj)⊤Watto)
u = X
j

pjmj
T

(5)

(6)

where T denotes the total number of words in a
story and Watt is a trainable weight matrix. We
then transform u to get the ﬁnal prediction:

ˆy = σ(Rφ(Hu + o))

(7)

where R and H are trainable weight matrices.

2.2 Semantically Motivated Memory Chains

In order to encourage each chain to focus on a
particular aspect, we supervise the activation of
each update gate (Equation 2) with binary labels.
Intuitively, for the sentiment chain, a sentiment-
bearing word (e.g. amazing) receives a label of 1,
allowing the model to open the gate and therefore
change the internal state relevant to this aspect,
while a neutral one (e.g. school) should not trig-
ger the activation with an assigned label of 0. To
achieve this, we use three memory chains to in-
dependently track each of: (1) event sequence, (2)
sentiment trajectory, and (3) topical consistency.
To supervise the memory update gates of these
chains, we design three sequences of binary labels:
lj = {lj
T } for j ∈ [1, 3] representing
event, sentiment, and topic, and lj
i ∈ {0, 1}. The
label at time i for the j-th aspect is only assigned
a value of 1 if the word is a trigger for that particu-
lar aspect: lj
i = 0. During train-
ing, we utilise such sequences of binary labels to
supervise the memory update gate activations of
each chain. Speciﬁcally, each chain is encouraged

i = 1; otherwise lj

2, . . . , lj

1, lj

Ricky fell while hiking in the woods

lEvent
lSentiment
lT opic

0
0
1

1
1
1

0
0
0

1
0
1

0
0
0

0
0
0

1
0
1

Table 1: An example of the linguistically moti-
vated memory chain supervision binary labels.

to open its gate only when it sees a trigger bearing
information semantically sensitive to that particu-
lar aspect. Note that at test time, we do not apply
such supervision. This effectively becomes a bi-
nary tagging task for each memory chain indepen-
dently and results in individual memory chains be-
ing sensitive to only a speciﬁc set of triggers bear-
ing one of the three types of signals.

While largely inspired by Chaturvedi et al.
(2017), our approach differs in how we extract
and use such features. Also note that, while still
making use of external linguistic resources to de-
tect trigger words, our approach lets the mem-
ory chains decide how such words should in-
ﬂuence the ﬁnal prediction, as opposed to the
handcrafted conditional probability features of
Chaturvedi et al. (2017).

To identify the trigger words, we use external
linguistic tools, and mark trigger words for each
aspect with a label of 1. An example is presented
in Table 1, noting that the same word can act as
trigger for multiple aspects.

Event sequence. We parse each sentence into
representation with SEMAFOR
its FrameNet
(Das et al., 2010), and identify each frame target
(word or phrase tokens evoking a frame).

Sentiment
trajectory. Following
Chaturvedi et al. (2017), we utilise a pre-compiled
list of sentiment words (Liu et al., 2005). To take
negation into account, we parse each sentence
with the Stanford Core NLP dependency
parser (Manning et al., 2014; Chen and Manning,
2014) and include negation words as trigger
words.

Topical consistency. We process each sen-
tence with the Stanford Core NLP POS tag-
ger and identify nouns and verbs,
following
Chaturvedi et al. (2017).

2.3 Training Loss

In addition to the cross entropy loss of the ﬁnal
prediction of right/wrong endings, we also take

into account the memory update gate supervision
of each chain by adding the second term. More for-
mally, the model is trained to minimise the loss:

L = XEntropy(y, ˆy) + α X
i,j

XEntropy(lj

i , gj
i )

where ˆy and gj
i are deﬁned in Equations 7 and 4 re-
spectively, y is the gold label for the current ending
option o, and lj
i is the semantic supervision binary
label at time i for the j-th memory chain. In our
experiments, we empirically set α to 0.5 based on
development data.

3 Experiments

3.1 Experimental Setup

Dataset. To test the effectiveness of our model,
we employ the Story Cloze Test dataset of
Mostafazadeh et al. (2016). The development and
test set each consist of 1,871 4-sentence stories,
each with a pair of ending options. Consistent with
previous studies, we split the development set into
a training and validation set (for early stopping),
resulting in 1,683 and 188 in each set, resp. Note
that while most current approaches make use of
the much larger training set, comprised of 100K 5-
sentence ROC stories (with coherent endings only,
also released as part of the dataset) to train a lan-
guage model, we make no use of this data.

Model conﬁguration. We initialise our model
with word2vec embeddings (300-D, pre-trained
on 100B Google News articles, not updated dur-
ing training: Mikolov et al. (2013a,b)). In addition
to the three supervised chains, we also add a “free”
chain, unconstrained to any semantic aspect.

Training is carried out over 200 epochs with
the FTRL optimiser (McMahan et al., 2013) and a
batch size of 128 and learning rate of 0.1. We use
the following hyper-parameters for weight matri-
ces in both directions: R ∈ R300×1, H, U, V, W
are all matrices of size R300×300, and hidden size
of the Bi-GRU is 300. Dropout is applied to the
output of φ in the ﬁnal classiﬁer (Equation 7) with
a rate of 0.2. Moreover, we employ the technique
introduced by Gal and Ghahramani (2016) where
the same dropout mask is applied at every step to
the input w
i to the
memory chains with rates of 0.5 and 0.2 respec-
tively. Lastly, to curb overﬁtting, we regularise the
last layer (Equation 7) with an L2 penalty on its
weights: λkRk where λ = 0.001.

i to the Bi-GRU and the input h

Model

Acc.

SD

DSSM
58.5 —
TBMIHAYLOV 72.8 —
MSAP
75.2 —
HCM
77.6 —
EntNet †
77.6 ±0.5
78.5 ±0.5
Our model†

Table 2: Performance on the Story Cloze test set.
Bold = best performance, † = ave. accuracy over 5
runs, SD = standard deviation, “—” = not reported.

Evaluation. Following previous
studies, we
evaluate the performance in terms of coherent
ending prediction accuracy: #correct
#total . Here, we re-
port the average accuracy over 5 runs with dif-
ferent random seeds. Echoing Melis et al. (2018),
we also observe some variance in model perfor-
mance even if training is carried out with the same
random seed, which is largely due to the non-
deterministic ordering of ﬂoating-point operations
in our environment (Tensorflow (Abadi et al.,
2015) with a single GPU). Therefore, to account
for the randomness, we further train our model 5
times for each random seed and select the model
with the best validation performance.

We

2017):

a
the

collection

(Mostafazadeh et al.,

of
against
benchmark
strong baselines,
top-3 sys-
including
tems of the 2017 LSDSem workshop shared
MSAP
task
(Schwartz et al., 2017), HCM (Chaturvedi et al.,
2017)2, and TBMIHAYLOV (Mihaylov and Frank,
2017). The ﬁrst two primarily rely on a sim-
ple logistic regression classiﬁer, both taking
linguistic and probability features from a ROC
Stories domain-speciﬁc neural language model.
TBMIHAYLOV is LSTM-based; we also include
DSSM (Mostafazadeh et al., 2016). We addi-
tionally implement a bi-directional EntNet
(Henaff et al., 2017) with the
same hyper-
parameter settings as our model and no semantic
supervision.3

3.2 Results and Discussion

The experimental results are shown in Table 2.

State-of-the-art results. Our model outper-
forms a collection of strong baselines, setting a

2We take the performance reported in a subsequent paper,

3.2% better than the original submission to the shared task.

3EntNet is also subject to the same model selection cri-

terion described above.

Model

Our model
—bi-directionality
—all semantic supervision

—event sequence
—sentiment trajectory
—topical consistency
—free chain

Acc.

SD

78.5 ±0.5
77.8 ±0.7
77.6 ±0.5
78.7 ±0.2
75.9 ±0.4
77.3 ±0.4
77.0 ±0.4

Table 3: Ablation study. Average accuracy over
5 runs on the Story Cloze test set (all subject to
the same model selection criterion as our model).
Bold: best performance. SD: standard deviation.

performance down to 77.8, is inferior to its bi-
directional cousin. Removing all semantic super-
vision (resulting in 4 free memory chains) is
also damaging to the accuracy (dropping to 77.6).
types of semantic super-
Among the different
vision, we see various degrees of performance
degradation, with removing sentiment trajectory
being the most detrimental, reﬂecting its value
to the task. Interestingly, we observe improve-
ment when removing event sequence supervision
from consideration. We suspect that this is mainly
due to the noise introduced by the rather inaccu-
rate FrameNet representation output of SEMAFOR
(F1 = 61.4% in frame identiﬁcation as reported in
Das et al. (2010)). While it is possible that replac-
ing SEMAFOR with the SemLM approach (with
the extended frame deﬁnition to include explicit
discourse markers, e.g. but) in Peng and Roth
(2016) and Chaturvedi et al. (2017) may poten-
tially reduce the amount of noise, we leave this
exercise for future work.

4 Conclusion

In this paper, we have proposed a novel model
for tracking various semantic aspects with exter-
nal memory chains. While requiring less domain-
speciﬁc training data, our model achieves state-
of-the-art performance on the task of ROC Story
Cloze ending prediction, beating a collection of
strong baselines.

Acknowledgments

We thank the anonymous reviewers for their
valuable feedback, and gratefully acknowledge
the support of Australian Government Research
Training Program Scholarship. This work was

Event Key
Sentiment Key
Event Word Sentiment Word Topic Word

Topic Key Free Key

Figure 3: t-SNE scatter plot of aspect-triggering
words (output representations h
i by Bi-GRU,
500 randomly sampled from each aspect) and the
learned keys. EntNet (left) vs. our model (right).
Best viewed in colour.

new state of the art. Note that this is achieved with-
out any external linguistic resources at test time or
domain-speciﬁc language model-based probabil-
ity features, highlighting the effectiveness of the
proposed model.

EntNet vs. our model. We see clear improve-
ments of the proposed method over EntNet,
an absolute gain of 0.9% in accuracy. This vali-
dates the hypothesis that encouraging each mem-
ory chain to focus on a unique, well-deﬁned aspect
is beneﬁcial.

Discussion. To better understand the beneﬁts of
the proposed method, we visualise the learned
word representations (output representation of the
Bi-GRU, h
i) and keys between EntNet and our
model in Figure 3. With EntNet, while senti-
ment words form a loose cluster, words bearing
event and topic signal are placed in close prox-
imity and are largely indistinguishable. With our
model, on the other hand, the degree of separation
is much clearer. The intersection of a small por-
tion of the event and topic groups is largely due to
the fact that both aspects include verbs. Another
interesting perspective is the location of the au-
tomatically learned keys (displayed as diamonds):
while all the keys with EntNet end up overlap-
ping, indicating little difference among them, the
keys learned by our method demonstrate seman-
tic diversity, with each placed within its respective
cluster. Note that the free key is learned to com-
plement the event key, a difﬁcult challenge given
the two disjoint clusters.

Ablation study. We further perform a ablation
study to analyse the utility of each component
in Table 3. The uni-directional variant, with its

also supported in part by the Australian Research
Council.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur,
Josh Levenberg, Dandelion Man´e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya
Sutskever, Kunal Talwar, Paul Tucker, Vincent Van-
houcke, Vijay Vasudevan, Fernanda Vi´egas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Mar-
tin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015.
TensorFlow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.

Michael Bugert, Yevgeniy Puzikov, Andreas R¨uckl´e,
Judith Eckle-Kohler, Teresa Martin, Eugenio
Mart´ınez-C´amara, Daniil Sorokin, Maxime Peyrard,
and Iryna Gurevych. 2017. LSDSem 2017: Explor-
ing data generation methods for the story cloze test.
In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics (LSDSem 2017), pages 56–61, Valencia,
Spain.

Eugene Charniak. 1972. Toward a model of chil-
dren”s story comprehension.
Technical report,
Massachusetts Institute of Technology, Cambridge,
USA.

Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
2017. Story comprehension for predicting what hap-
pens next. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2017), pages 1603–1614, Copen-
hagen, Denmark.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), pages 740–750, Doha, Qatar.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Proceedings of the NIPS 2014 Deep Learn-
ing and Representation Learning Workshop.

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
In Human Language Technologies: The
parsing.
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT 2010), pages 948–956, Los
Angeles, USA.

Yarin Gal and Zoubin Ghahramani. 2016. A theo-
retically grounded application of dropout in recur-
rent neural networks. In Proceedings of Advances
in Neural Information Processing Systems (NIPS
2016), pages 1027–1035, Barcelona, Spain.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectiﬁers: Surpass-
ing human-level performance on ImageNet classiﬁ-
In Proceedings of the 2015 IEEE Interna-
cation.
tional Conference on Computer Vision (ICCV 2015),
pages 1026–1034, Washington, DC, USA.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the world
In Proceed-
state with recurrent entity networks.
ings of the 5th International Conference on Learn-
ing Representations (ICLR 2017), Toulon, France.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Proceedings of the 14th Interna-
tional Conference on World Wide Web (WWW 2005),
pages 342–351, Chiba, Japan.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

H. Brendan McMahan, Gary Holt, David Sculley,
Michael Young, Dietmar Ebner, Julian Grady,
Lan Nie, Todd Phillips, Eugene Davydov, Daniel
Golovin, et al. 2013. Ad click prediction: A view
from the trenches. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2013), pages
1222–1230, Chicago, USA.

G´abor Melis, Chris Dyer, and Phil Blunsom. 2018.
On the state of the art of evaluation in neural lan-
In Proceedings of the 6th Inter-
guage models.
national Conference on Learning Representations
(ICLR 2018), Vancouver, Canada.

Todor Mihaylov and Anette Frank. 2017. Story cloze
ending selection baselines and data examination. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 87–92, Valencia, Spain.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word represen-
In Proceedings of the 1st
tations in vector space.
International Conference on Learning Representa-
tions (ICLR 2013), Scottsdale, USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the 27th Annual Conference
on Neural Information Processing Systems (NIPS
2013), pages 3111–3119, Stateline, USA.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding of
In Proceedings of the 2016
commonsense stories.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2016), pages
839–849, San Diego, USA.

Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael Chambers, and James Allen. 2017. LS-
DSem 2017 shared task: The story cloze test.
In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 46–51, Valencia, Spain.

Haoruo Peng and Dan Roth. 2016. Two discourse
driven language models for semantics. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (ACL 2016), pages
290–300, Berlin, Germany.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic logic meets little red riding hood: A com-
prehensive, natural representation for language un-
derstanding.
In Lucja Iwanska and Stuart C.
Shapiro, editors, Natural Language Processing and
Knowledge Representation: Language for Knowl-
edge and Knowledge for Language, pages 111–174.
MIT/AAAI Press, Menlo Park, USA.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. The
effect of different writing tasks on linguistic style:
A case study of the ROC story cloze task. In Pro-
ceedings of the 21st Conference on Computational
Natural Language Learning (CoNLL 2017), pages
15–25, Vancouver, Canada.

Scott R. Turner. 1994.

The Creative Process: A
Computer Model of Storytelling and Creativity.
Lawrence Erlbaum, Hillsdale, USA.

Terry Winograd. 1972. Understanding natural lan-

guage. Cognitive Psychology, 3(1):1–191.

