7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

−

−

=

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

=

−

+

−

−

=

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

=

−

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

{

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

7

3

4

5

2

6

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

=

−

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

−

−

−

=

+

=

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

=

−

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

{

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

7

6

4

2

3

5

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

−

=

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

−

−

−

+

=

=

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

=

−

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

{

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

7

6

4

5

3

2

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

−

=

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

+

−

=

−

=

−

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

−

=

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

{

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

4

2

3

5

7

6

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

−

=

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

=

−

−

−

−

+

=

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

−

=

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

{

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

2

5

3

6

7

4

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

=

−

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

=

−

−

−

−

=

+

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

=

−

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

}

}

{

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

7

2

5

3

4

6

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

7
1
0
2
 
v
o
N
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
1
3
1
7
0
.
5
0
7
1
:
v
i
X
r
a

Streaming Sparse Gaussian Process Approximations

Thang D. Bui∗

Cuong V. Nguyen∗

Richard E. Turner

Department of Engineering, University of Cambridge, UK
{tdb40,vcn22,ret26}@cam.ac.uk

Abstract

Sparse pseudo-point approximations for Gaussian process (GP) models provide a
suite of methods that support deployment of GPs in the large data regime and en-
able analytic intractabilities to be sidestepped. However, the ﬁeld lacks a principled
method to handle streaming data in which both the posterior distribution over func-
tion values and the hyperparameter estimates are updated in an online fashion. The
small number of existing approaches either use suboptimal hand-crafted heuristics
for hyperparameter learning, or suffer from catastrophic forgetting or slow updating
when new data arrive. This paper develops a new principled framework for de-
ploying Gaussian process probabilistic models in the streaming setting, providing
methods for learning hyperparameters and optimising pseudo-input locations. The
proposed framework is assessed using synthetic and real-world datasets.

1

Introduction

Probabilistic models employing Gaussian processes have become a standard approach to solving
many machine learning tasks, thanks largely to the modelling ﬂexibility, robustness to overﬁtting, and
well-calibrated uncertainty estimates afforded by the approach [1]. One of the pillars of the modern
Gaussian process probabilistic modelling approach is a set of sparse approximation schemes that
(N 2) for
allow the prohibitive computational cost of GP methods, typically
prediction where N is the number of training points, to be substantially reduced whilst still retaining
accuracy. Arguably the most important and inﬂuential approximations of this sort are pseudo-point
N pseudo-points to summarise the observational
approximation schemes that employ a set of M
(cid:28)
(M 2) for training and prediction,
(N M 2) and
data thereby reducing computational costs to
O
respectively [2, 3]. Stochastic optimisation methods that employ mini-batches of training data can
be used to further reduce computational costs [4, 5, 6, 7], allowing GPs to be scaled to datasets
comprising millions of data points.

(N 3) for training and

O

O

O

The focus of this paper is to provide a comprehensive framework for deploying the Gaussian process
probabilistic modelling approach to streaming data. That is, data that arrive sequentially in an online
fashion, possibly in small batches, and whose number are not known a priori (and indeed may be
inﬁnite). The vast majority of previous work has focussed exclusively on the batch setting and there
is not a satisfactory framework that supports learning and approximation in the streaming setting.
A naïve approach might simply incorporate each new datum as they arrived into an ever-growing
dataset and retrain the GP model from scratch each time. With inﬁnite computational resources, this
approach is optimal, but in the majority of practical settings, it is intractable. A feasible alternative
would train on just the most recent K training data points, but this completely ignores potentially
large amounts of informative training data and it does not provide a method for incorporating the
old model into the new one which would save computation (except perhaps through initialisation of
the hyperparameters). Existing, sparse approximation schemes could be applied in the same manner,
but they merely allow K to be increased, rather than allowing all previous data to be leveraged, and
again do not utilise intermediate approximate ﬁts.

∗These authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

What is needed is a method for performing learning and sparse approximation that incrementally
updates the previously ﬁt model using the new data. Such an approach would utilise all the previous
training data (as they will have been incorporated into the previously ﬁt model) and leverage as much
of the previous computation as possible at each stage (since the algorithm only requires access to the
data at the current time point). Existing stochastic sparse approximation methods could potentially
be used by collecting the streamed data into mini-batches. However, the assumptions underpinning
these methods are ill-suited to the streaming setting and they perform poorly (see sections 2 and 4).

This paper provides a new principled framework for deploying Gaussian process probabilistic models
in the streaming setting. The framework subsumes Csató and Opper’s two seminal approaches to
online regression [8, 9] that were based upon the variational free energy (VFE) and expectation
propagation (EP) approaches to approximate inference respectively. In the new framework, these
algorithms are recovered as special cases. We also provide principled methods for learning hyperpa-
rameters (learning was not treated in the original work and the extension is non-trivial) and optimising
pseudo-input locations (previously handled via hand-crafted heuristics). The approach also relates to
the streaming variational Bayes framework [10]. We review background material in the next section
and detail the technical contribution in section 3, followed by several experiments on synthetic and
real-world data in section 4.

2 Background

Regression models that employ Gaussian processes are state of the art for many datasets [11]. In
this paper we focus on the simplest GP regression model as a test case of the streaming framework
N
for inference and learning. Given N input and real-valued output pairs
n=1, a standard GP
regression model assumes yn = f (xn) + (cid:15)n, where f is an unknown function that is corrupted by
y). Typically, f is assumed to be drawn from a zero-mean
Gaussian observation noise (cid:15)n ∼ N
θ)) whose covariance function depends on hyperparameters θ. In this
GP prior f
θ) can be computed
simple model, the posterior over f , p(f
|
n=1).2 However,
N
yn}
analytically (here we have collected the observations into a vector y =
{
these quantities present a computational challenge resulting in an O(N 3) complexity for maximum
likelihood training and O(N 2) per test point for prediction.

y, θ), and the marginal likelihood p(y
|

xn, yn}

,
(0, k(
·

(0, σ2

∼ GP

·|

{

This prohibitive complexity of exact learning and inference in GP models has driven the development
of many sparse approximation frameworks [12, 13]. In this paper, we focus on the variational free
energy approximation scheme [3, 14] which lower bounds the marginal likelihood of the data using a
variational distribution q(f ) over the latent function:

(cid:90)

log p(y

θ) = log
|

(cid:90)

|

≥

df p(y, f

θ)

df q(f ) log

p(y, f
θ)
|
q(f )

=

vfe(q, θ).

F

(1)

F

vfe(q, θ) = log p(y

] denotes the Kullback–Leibler
Since
divergence, maximising this lower bound with respect to q(f ) guarantees the approximate posterior
gets closer to the exact posterior p(f
vfe(q, θ) approximates
y, θ). Moreover, the variational bound
|
the marginal likelihood and can be used for learning the hyperparameters θ.

y, θ)], where KL[
|

KL[q(f )

θ)
|

p(f

·||·

−

F

||

In order to arrive at a computationally tractable method, the approximate posterior is parameterized
via a set of M pseudo-points u that are a subset of the function values f =
and which will
summarise the data. Speciﬁcally, the approximate posterior is assumed to be q(f ) = p(f(cid:54)=u
u, θ)q(u),
|
u, θ) is the prior distribution of the
where q(u) is a variational distribution over u and p(f(cid:54)=u
remaining latent function values. This assumption allows the following critical cancellation that
results in a computationally tractable lower bound:
(cid:90)

f(cid:54)=u, u
}
{

|

vfe(q(u), θ) =

df q(f ) log

F

p(y

|

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
f, θ)p(u
θ)
u, θ)
p(f(cid:54)=u
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
|
|
u, θ)q(u)
p(f(cid:54)=u
|
(cid:90)
(cid:88)

=

KL[q(u)

−

p(u
|

||

θ)] +

du q(u)p(fn|

u, θ) log p(yn|

fn, θ),

n
where fn = f (xn) is the latent function value at xn. For the simple GP regression model considered
here, closed-form expressions for the optimal variational approximation qvfe(f ) and the optimal

2The dependence on the inputs {xn}N
suppressed throughout to lighten the notation.

n=1 of the posterior, marginal likelihood, and other quantities is

2

variational bound

vfe(q(u), θ) (also called the ‘collapsed’ bound) are available:

vfe(θ) = maxq(u)F
F
p(f(cid:54)=u
qvfe(f )

p(f

y, θ)
|

≈

log p(y

θ)

|

≈ F

∝
vfe(θ) = log

(y; KfuK−1

u, θ)p(u
|

θ)

N
|
uuKuf + σ2
(y; 0, KfuK−1

uuu, σ2
yI),
1
2σ2
y

yI)

−

N

(cid:88)

n

(knn −

KnuK−1

uuKun),

where f is the latent function values at training points, and Kf1f2 is the covariance matrix between
the latent function values f1 and f2. Critically, the approach leads to O(N M 2) complexity for
approximate maximum likelihood learning and O(M 2) per test point for prediction. In order for this
method to perform well, it is necessary to adapt the pseudo-point input locations, e.g. by optimising
the variational free energy, so that the pseudo-data distribute themselves over the training data.

Alternatively, stochastic optimisation may be applied directly to the original, uncollapsed version of
the bound [4, 15]. In particular, an unbiased estimate of the variational lower bound can be obtained
using a small number of training points randomly drawn from the training set:

(cid:90)

(cid:88)

N
B
|

vfe(q(u), θ)

KL[q(u)

F

≈ −

θ)] +
p(u
|

||

du q(u)p(fn|

u, θ) log p(yn|

fn, θ).

|
Since the optimal approximation is Gaussian as shown above, q(u) is often posited as a Gaussian
distribution and its parameters are updated by following the (noisy) gradients of the stochastic
estimate of the variational lower bound. By passing through the training set a sufﬁcient number
of times, the variational distribution converges to the optimal solution above, given appropriately
decaying learning rates [4].

yn∈B

In principle, the stochastic uncollapsed approach is applicable to the streaming setting as it reﬁnes an
approximate posterior based on mini-batches of data that can be considered to arrive sequentially
(here N would be the number of data points seen so far). However, it is unsuited to this task since
stochastic optimisation assumes that the data subsampling process is uniformly random, that the
training set is revisited multiple times, and it typically makes a single gradient update per mini-batch.
These assumptions are incompatible with the streaming setting: continuously arriving data are not
typically drawn iid from the input distribution (consider an evolving time-series, for example); the
data can only be touched once by the algorithm and not revisited due to computational constraints;
each mini-batch needs to be processed intensively as it will not be revisited (multiple gradient
steps would normally be required, for example, and this runs the risk of forgetting old data without
delicately tuning the learning rates). In the following sections, we shall discuss how to tackle these
challenges through a novel online inference and learning procedure, and demonstrate the efﬁcacy of
this method over the uncollapsed approach and naïve online versions of the collapsed approach.

3 Streaming sparse GP (SSGP) approximation using variational inference

The general situation assumed in this paper is that data arrive sequentially so that at each step new
data points ynew are added to the old dataset yold. The goal is to approximate the marginal likelihood
and the posterior of the latent process at each step, which can be used for anytime prediction. The
hyperparameters will also be adjusted online. Importantly, we assume that we can only access the
current data points ynew directly for computational reasons (it might be too expensive to hold yold
and x1:Nold in memory, for example, or approximations made at the previous step must be reused
to reduce computational overhead). So the effect of the old data on the current posterior must be
propagated through the previous posterior. We will now develop a new sparse variational free energy
approximation for this purpose, that compactly summarises the old data via pseudo-points. The
pseudo-inputs will also be adjusted online since this is critical as new parts of the input space will be
revealed over time. The framework is easily extensible to more complex non-linear models.

3.1 Online variational free energy inference and learning

Consider an approximation to the true posterior at the previous step, qold(f ), which must be updated
to form the new approximation qnew(f ),

qold(f )

p(f

yold) =
|

≈

p(f

θold)p(yold
|

f ),
|

1
1(θold)

Z

qnew(f )

p(f

yold, ynew) =
|

≈

p(f

θnew)p(yold
|

f )p(ynew
|

|

f ).

(2)

(3)

1
2(θnew)

Z

3

f )
|

≈ Z

the updated exact posterior p(f

Whilst
new data through their likelihoods,
rectly.
p(yold

1(θold)qold(f )/p(f

|

yold, ynew) balances the contribution of old and
f ) di-
|
that is

the new approximation cannot access p(yold
f ) by inverting eq. (2),
|

θold). Substituting this into eq. (3) yields,

Instead, we can ﬁnd an approximation of p(yold

|

|

.

f )

(4)

p(f

ˆp(f

θnew)p(ynew

1(θold)
2(θnew)

yold, ynew) = Z
|
Z

qold(f )
θold)
p(f
|
Although it is tempting to use this as the new posterior, qnew(f ) = ˆp(f
yold, ynew), this recovers
|
exact GP regression with ﬁxed hyperparameters (see section 3.3) and it is intractable. So, instead, we
consider a variational update that projects the distribution back to a tractable form using pseudo-data.
At this stage we allow the pseudo-data input locations in the new approximation to differ from those
in the old one. This is required if new regions of input space are gradually revealed, as for example
in typical time-series applications. Let a = f (zold) and b = f (znew) be the function values at the
pseudo-inputs before and after seeing new data. Note that the number of pseudo-points, Ma =
and
|
Mb =
are not necessarily restricted to be the same. The form of the approximate posterior mirrors
that in the batch case, that is, the previous approximate posterior, qold(f ) = p(f(cid:54)=a
a, θold)qold(a)
(a; ma, Sa). The new posterior approximation takes the same form,
where we assume qold(a) =
but with the new pseudo-points and new hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)qnew(b).
|
Similar to the batch case, this approximate inference problem can be turned into an optimisation
problem using variational inference. Speciﬁcally, consider

b
|
|

a
|

N

|

|

(cid:90)

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

(cid:90)

+

df qnew(f )

log

f ) qold(f )
p(f |θold)
|
θold)qnew(b)
p(a
|
θnew)qold(a)p(ynew

p(b
|

(5)

(cid:21)

.

f )
|

2(θnew)
1(θold)

= log Z
Z

Since the KL divergence is non-negative, the second term in the expression above is the negative
yold)), or the
approximate lower bound of the online log marginal likelihood (as
1
Z
|
w.r.t. q(b) equal to 0, the
(qnew(f ), θnew). By setting the derivative of
variational free energy
optimal approximate posterior can be obtained for the regression case,3

p(ynew

Z
F

2/

≈

F

qvfe(b)

p(b) exp

(cid:16) (cid:90)

∝

∝

p(b)

N

da p(a

b) log
|

+

df p(f

b) log p(ynew
|

|

f )

(cid:90)

qold(a)
θold)
p(a
|

(cid:17)

(6)

(7)

(ˆy; Kˆf bK−1

bbb, Σˆy,vfe),

where f is the latent function values at the new training points,

ˆy =

(cid:21)

(cid:20)
ynew
DaS−1

a ma

, Kˆf b =

, Σˆy,vfe =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
0
yI
0 Da

, Da = (S−1

K(cid:48)−1

aa )−1.

a −

The negative variational free energy is also analytically available,

(θ) = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy,vfe)

F

N

KfbK−1

bbKbf ) + ∆a; where

(8)

K(cid:48)
|

|

−

log

+ log

+ log

Sa
|

a Qa] + const.
2∆a =
Equations (7) and (8) provide the complete recipe for online posterior update and hyperparameter
learning in the streaming setting. The computational complexity and memory overhead of the new
method is of the same order as the uncollapsed stochastic variational inference approach. The
procedure is demonstrated on a toy regression example as shown in ﬁg. 1[Left].

a )ma

Da
|

a −

aa|

−

|

tr[D−1

S−1

1
2σ2
y
a(S−1

tr(Kﬀ

−
a DaS−1

−
+ m(cid:124)

3.2 Online α-divergence inference and learning

One obvious extension of the online approach discussed above replaces the KL divergence in
eq. (11) with a more general α-divergence [16]. This does not affect tractability:
the opti-
mal form of the approximate posterior can be obtained analytically for the regression case,
qpep(b)

p(b)

∝

N

Σˆy,pep =

(ˆy; Kˆf bK−1
(cid:20)σ2

bbb, Σˆy,pep) where
KfbK−1

yI + αdiag(Kﬀ
−
0

bbKbf )

0
KabK−1

bbKba)

(cid:21)

.

(9)

Da + α(Kaa

−

3Note that we have dropped θnew from p(b|θnew), p(a|b, θnew) and p(f |b, θnew) to lighten the notation.

4

Figure 1: [Left] SSGP inference and learning on a toy time-series using the VFE approach. The black
crosses are data points (past points are greyed out), the red circles are pseudo-points, and blue lines
and shaded areas are the marginal predictive means and conﬁdence intervals at test points. [Right]
Log-likelihood of test data as training data arrives for different α values, for the pseudo periodic
dataset (see section 4.2). We observed that α = 0.01 is virtually identical to VFE. Dark lines are
means over 4 splits and shaded lines are results for each split. Best viewed in colour.

0 (compare to eq. (7)) since then the α-divergence is
This reduces back to the variational case as α
equivalent to the KL divergence. The approximate online log marginal likelihood is also analytically
tractable and recovers the variational case when α

0. Full details are provided in the appendix.

→

3.3 Connections to previous work and special cases

→

This section brieﬂy highlights connections between the new framework and existing approaches
including Power Expectation Propagation (Power-EP), Expectation Propagation (EP), Assumed
Density Filtering (ADF), and streaming variational Bayes.

Recent work has uniﬁed a range of batch sparse GP approximations as special cases of the Power-EP
algorithm [13]. The online α-divergence approach to inference and learning described in the last
section is equivalent to running a forward ﬁltering pass of Power-EP. In other words, the current work
generalizes the unifying framework to the streaming setting.

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for sparse GP
regression recovers the batch solutions provided by Power-EP. In other words, only a single pass
through the data is necessary for Power-EP to converge in sparse GP regression. For the case α = 1,
which is called Expectation Propagation, we recover the seminal work by Csató and Opper [8].
For the variational free energy case (equivalently where α
0) we recover the seminal work by
Csató [9]. The new framework can be seen to extend these methods to allow principled learning and
pseudo-input optimisation. Interestingly, in the setting where hyperparameters and the pseudo-inputs
are ﬁxed, if pseudo-points are added at each stage at the new data input locations, the method returns
the true posterior and marginal likelihood (see appendix).

→

For ﬁxed hyperparameters and pseudo-points, the new VFE framework is equivalent to the application
of streaming variational Bayes (VB) or online variational inference [10, 17, 18] to the GP setting in
which the previous posterior plays a role of an effective prior for the new data. Similarly, the equivalent
algorithm when α = 1 is called Assumed Density Filtering [19]. When the hyperparameters are
updated, the new method proposed here is different from streaming VB and standard application of
ADF, as the new method propagates approximations to just the old likelihood terms and not the prior.
Importantly, we found vanilla application of the streaming VB framework performed catastrophically
for hyperparameter learning, so the modiﬁcation is critical.

4 Experiments

In this section, the SSGP method is evaluated in terms of speed, memory usage, and accuracy (log-
likelihood and error). The method was implemented on GPﬂow [20] and compared against GPﬂow’s
version of the following baselines: exact GP (GP), sparse GP using the collapsed bound (SGP), and
stochastic variational inference using the uncollapsed bound (SVI). In all the experiments, the RBF
kernel with ARD lengthscales is used, but this is not a limitation required by the new methods. An im-
plementation of the proposed method can be found at http://github.com/thangbui/streaming_sparse_gp.
Full experimental results and additional discussion points are included in the appendix.

4.1 Synthetic data

Comparing α-divergences. We ﬁrst consider the general online α-divergence inference and learning
framework and compare the performance of different α values on a toy online regression dataset

5

in ﬁg. 1[Right]. Whilst the variational approach performs well, adapting pseudo-inputs to cover
new regions of input space as they are revealed, algorithms using higher α values perform more
poorly. Interestingly this appears to be related to the tendency for EP, in batch settings, to clump
pseudo-inputs on top of one another [21]. Here the effect is much more extreme as the clumps
accumulate over time, leading to a shortage of pseudo-points if the input range of the data increases.
Although heuristics could be introduced to break up the clumps, this result suggests that using small α
values for online inference and learning might be more appropriate (this recommendation differs from
the batch setting where intermediate settings of α around 0.5 are best [13]). Due to these ﬁndings, for
the rest of the paper, we focus on the variational case.

Hyperparameter learning. We generated multiple time-series from GPs with known hyperpa-
rameters and observation noises, and tracked the hyperparameters learnt by the proposed online
variational free energy method and exact GP regression. Overall, SSGP can track and learn good
hyperparameters, and if there are sufﬁcient pseudo-points, it performs comparatively to full GP on
the entire dataset. Interestingly, all models including full GP regression tend to learn bigger noise
variances as any discrepancy in the true and learned function values is absorbed into this parameter.

4.2 Speed versus accuracy

In this experiment, we compare SSGP to the baselines (GP, SGP, and SVI) in terms of a speed-
accuracy trade-off where the mean marginal log-likelihood (MLL) and the root mean squared error
(RMSE) are plotted against the accumulated running time of each method after each iteration. The
comparison is performed on two time-series datasets and a spatial dataset.

Time-series data. We ﬁrst consider modelling a segment of the pseudo periodic synthetic dataset
[22], previously used for testing indexing schemes in time-series databases. The segment contains
24,000 time-steps. Training and testing sets are chosen interleaved so that their sizes are both 12,000.
The second dataset is an audio signal prediction dataset, produced from the TIMIT database [23] and
previously used to evaluate GP approximations [24]. The signal was shifted down to the baseband
and a segment of length 18,000 was used to produce interleaved training and testing sets containing
9,000 time steps. For both datasets, we linearly scale the input time steps to the range [0, 10].

All algorithms are assessed in the mini-batch streaming setting with data ynew arriving in batches
of size 300 and 500 taken in order from the time-series. The ﬁrst 1,000 examples are used as an
initial training set to obtain a reasonable starting model for each algorithm. In this experiment, we
use memory-limited versions of GP and SGP that store the last 3,000 examples. This number was
chosen so that the running times of these algorithms match those of SSGP or are slightly higher. For
all sparse methods (SSGP, SGP, and SVI), we run the experiments with 100 and 200 pseudo-points.

For SVI, we allow the algorithm to make 100 stochastic gradient updates during each iteration and
run preliminary experiments to compare 3 learning rates r = 0.001, 0.01, and 0.1. The preliminary
results showed that the performance of SVI was not signiﬁcantly altered and so we only present the
results for r = 0.1.

Figure 2 shows the plots of the accumulated running time (total training and testing time up until the
current iteration) against the MLL and RMSE for the considered algorithms. It is clear that SSGP
signiﬁcantly outperforms the other methods both in terms of the MLL and RMSE, once sufﬁcient
training data have arrived. The performance of SSGP improves when the number of pseudo-points
increases, but the algorithm runs more slowly. In contrast, the performance of GP and SGP, even after
seeing more data or using more pseudo-points, does not increase signiﬁcantly since they can only
model a limited amount of data (the last 3,000 examples).

Spatial data. The second set of experiments consider the OS Terrain 50 dataset that contains spot
heights of landscapes in Great Britain computed on a grid.4 A block of 200
200 points was split
into 10,000 training examples and 30,000 interleaved testing examples. Mini-batches of data of size
750 and 1,000 arrive in spatial order. The ﬁrst 1,000 examples were used as an initial training set.
For this dataset, we allow GP and SGP to remember the last 7,500 examples and use 400 and 600
pseudo-points for the sparse models. Figure 3 shows the results for this dataset. SSGP performs
better than the other baselines in terms of the RMSE although it is worse than GP and SGP in terms
of the MLL.

×

4The dataset is available at: https://data.gov.uk/dataset/os-terrain-50-dtm.

6

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 2: Results for time-series datasets with batch sizes 300 and 500. Pluses and circles indicate
the results for M = 100, 200 pseudo-points respectively. For each algorithm (except for GP), the
solid and dashed lines are the efﬁcient frontier curves for M = 100, 200 respectively.

4.3 Memory usage versus accuracy

Besides running time, memory usage is another important factor that should be considered. In this
experiment, we compare the memory usage of SSGP against GP and SGP on the Terrain dataset
above with batch size 750 and M = 600 pseudo-points. We allow GP and SGP to use the last 2,000
and 6,000 examples for training, respectively. These numbers were chosen so that the memory usage
of the two baselines roughly matches that of SSGP. Figure 4 plots the maximum memory usage of
the three methods against the MLL and RMSE. From the ﬁgure, SSGP requires small memory usage
while it can achieve comparable or better MLL and RMSE than GP and SGP.

4.4 Binary classiﬁcation

We show a preliminary result for GP models with non-Gaussian likelihoods, in particular, a binary
classiﬁcation model on the benchmark banana dataset. As the optimal form for the approximate
posterior is not analytically tractable, the uncollapsed variational free energy is optimised numerically.
The predictions made by SSGP in a non-iid streaming setting are shown in ﬁg. 12. SSGP performs
well and achieves the performance of the batch sparse variational method [5].

7

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 3: Results for spatial data (see ﬁg. 2 for the legend). Pluses/solid lines and circles/dashed lines
indicate the results for M = 400, 600 pseudo-points respectively.

Figure 4: Memory usage of SSGP (blue), GP (magenta) and SGP (red) against MLL and RMSE.

Figure 5: SSGP inference and learning on a binary classiﬁcation task in a non-iid streaming setting.
The right-most plot shows the prediction made by using sparse variational inference on full training
data [5] for comparison. Past observations are greyed out. The pseudo-points are shown as black dots
and the black curves show the decision boundary.

5 Summary

We have introduced a novel online inference and learning framework for Gaussian process models.
The framework uniﬁes disparate methods in the literature and greatly extends them, allowing se-
quential updates of the approximate posterior and online hyperparameter optimisation in a principled
manner. The proposed approach outperforms existing approaches on a wide range of regression
datasets and shows promising results on a binary classiﬁcation dataset. A more thorough investigation
on models with non-Gaussian likelihoods is left as future work. We believe that this framework will
be particularly useful for efﬁcient deployment of GPs in sequential decision making problems such
as active learning, Bayesian optimisation, and reinforcement learning.

8

Acknowledgements

The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful
comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship.
Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.

References
[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006.

[2] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural

Information Processing Systems (NIPS), 2006.

[3] M. K. Titsias, “Variational learning of inducing variables in sparse Gaussian processes,” in International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[4] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Conference on Uncertainty

in Artiﬁcial Intelligence (UAI), 2013.

[5] J. Hensman, A. G. D. G. Matthews, and Z. Ghahramani, “Scalable variational Gaussian process classiﬁca-

tion,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

[6] A. Dezfouli and E. V. Bonilla, “Scalable inference for Gaussian process models with black-box likelihoods,”

in Advances in Neural Information Processing Systems (NIPS), 2015.

[7] D. Hernández-Lobato and J. M. Hernández-Lobato, “Scalable Gaussian process classiﬁcation via ex-
pectation propagation,” in International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
2016.

[8] L. Csató and M. Opper, “Sparse online Gaussian processes,” Neural Computation, 2002.

[9] L. Csató, Gaussian Processes – Iterative Sparse Approximations. PhD thesis, Aston University, 2002.

[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan, “Streaming variational Bayes,” in

Advances in Neural Information Processing Systems (NIPS), 2013.

[11] T. D. Bui, D. Hernández-Lobato, J. M. Hernández-Lobato, Y. Li, and R. E. Turner, “Deep Gaussian
processes for regression using approximate expectation propagation,” in International Conference on
Machine Learning (ICML), 2016.

[12] J. Quiñonero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process

regression,” The Journal of Machine Learning Research, 2005.

[13] T. D. Bui, J. Yan, and R. E. Turner, “A unifying framework for Gaussian process pseudo-point approxima-

tions using power expectation propagation,” Journal of Machine Learning Research, 2017.

[14] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, “On sparse variational methods and
the Kullback-Leibler divergence between stochastic processes,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

[15] C.-A. Cheng and B. Boots, “Incremental variational sparse Gaussian process regression,” in Advances in

Neural Information Processing Systems (NIPS), 2016.

[16] T. Minka, “Power EP,” tech. rep., Microsoft Research, Cambridge, 2004.

[17] Z. Ghahramani and H. Attias, “Online variational Bayesian learning,” in NIPS Workshop on Online

Learning, 2000.

[18] M.-A. Sato, “Online model selection based on the variational Bayes,” Neural Computation, 2001.

[19] M. Opper, “A Bayesian approach to online learning,” in On-Line Learning in Neural Networks, 1999.

[20] A. G. D. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-Villagrá, Z. Ghahra-
mani, and J. Hensman, “GPﬂow: A Gaussian process library using TensorFlow,” Journal of Machine
Learning Research, 2017.

[21] M. Bauer, M. van der Wilk, and C. E. Rasmussen, “Understanding probabilistic sparse Gaussian process

approximations,” in Advances in Neural Information Processing Systems (NIPS), 2016.

[22] E. J. Keogh and M. J. Pazzani, “An indexing scheme for fast similarity search in large time series databases,”

in International Conference on Scientiﬁc and Statistical Database Management, 1999.

[23] J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, N. Dahlgren, and V. Zue, “TIMIT acoustic-phonetic

continuous speech corpus LDC93S1,” Philadelphia: Linguistic Data Consortium, 1993.

[24] T. D. Bui and R. E. Turner, “Tree-structured Gaussian process approximations,” in Advances in Neural

Information Processing Systems (NIPS), 2014.

9

Appendices

A More discussions on the paper

A.1 Can the variational lower bound be derived using Jensen’s inequality?

Yes. There are two equivalent ways of deriving VI:

1. Applying Jensen’s inequality directly to the log marginal likelihood.
2. Explicitly writing down the KL(q

p), noting that it is non-negative and rearranging to get

the same bound as in (1).

(cid:107)

(1) is often used in traditional VI literature. Many recent papers (e.g. [4] and our paper) use (2).

A.2 Comparison to [9]

It is not clear how to compare to [9] fairly since it does not provide methods for learning hyperparam-
eters and their framework does not support such an extension. Accurate hyperparameter learning is
required for real datasets like those in the paper. So [9] performs extremely poorly unless suitable
settings for the hyperparameters can be guessed from the ﬁrst batch of data. Furthermore, our paper
goes beyond [9] by providing a method for optimising pseudo-inputs which has been shown to
substantially improve upon the heuristics used in [9] in the batch setting [2].

A.3 Are SVI or the stream-based method performing differently due to different

approximations?

No. Conventional SVI is fundamentally unsuited to the streaming setting and it performs very poorly
practically compared to both the collapsed and uncollapsed versions of our method. The SVI learning
rates require a lot of dataset and iteration speciﬁc tuning so the new data can be revisited multiple
times without forgetting old data. The uncollapsed versions of our method do not require tuning of
this sort and perform just as well as the collapsed version given sufﬁcient updates.

A.4 Are pseudo-points appropriate for streaming settings?

In any setting (batch/streaming), pseudo-point approximations require the pseudo-points to cover the
input space occupied by the data. This means they can be inappropriate for very long time-series or
very high-dimensional inputs. This is a general issue with the approximation class. The development
of new pseudo-point approximations to handle very large numbers of pseudo-points is a key and
active research area [24], but orthogonal to our focus in this paper. A moving window could be
introduced so just recent data are modelled (as we use for SGP/GP) but the utility of this depends on
the task. Here we assume all input regions must be modelled which is problematic for windowing.

A.5 A possible explanation on why all models including full GP regression tend to learn

bigger noise variances

This is a bias that arises because the learned functions are more discrepant from the training data than
the true function and so the learned observation noise inﬂates to accommodate the mismatch.

A.6 Are the hyperparameters learned in the time-series and spatial data experiments?

Yes, hyperparameters and pseudo-inputs are optimised using the online variational free energy. This
is absolutely central to our approach and the key difference to [9, 8].

A.7 Why is there a non-monotonic behaviour in ﬁg. 4 in the main text?

This occurs because at some point the GP/SGP memory window cannot cover all observed data.
Some parts of the input space are then missed, leading to decreasing performance.

B Variational free energy approach for streaming sparse GP regression

B.1 The variational lower bound

Let a = f (zold) and b = f (znew) be the function values at the pseudo-inputs before and after seeing
a, θold)q(a), can be used to ﬁnd the approximate
new data. The previous posterior, qold(f ) = p(f(cid:54)=a

|

10

likelihood given by old observations as follows,

p(yold

f )

|

≈

qold(f )p(yold
θold)
|
θold)
p(f

|

as

qold(f )

p(f

θold)p(yold
|
θold)
p(yold

|

f )

.

≈

|

(10)

Substituting this into the posterior that we want to target gives us:

p(f

yold, ynew) =

|

p(f

f )p(ynew
θnew)p(yold
|
|
θnew)
p(ynew, yold
|

|

f )

p(f

θnew)qold(f )p(yold

≈

p(f

θold)p(ynew, yold

|

|

|

θold)p(ynew
θnew)
|

f )
|

.

The new posterior approximation takes the same form, but with the new pseudo-points and new
hyperparameters: qnew(f ) = p(f(cid:54)=b
b, θnew)q(b). This approximate posterior can be obtained by
|
minimising the KL divergence,

KL[qnew(f )

ˆp(f

yold, ynew)] =

df qnew(f ) log

||

|

(cid:90)

p(f(cid:54)=b
Z1(θold)
Z2(θnew) p(f

(cid:90)

+

df qnew(f )

b, θnew)qnew(b)
|
θnew)p(ynew
|
(cid:20)

f ) qold(f )
p(f |θold)
θold)qnew(b)

|
p(a
|
θnew)qold(a)p(ynew
p(b
|

log

(11)

(cid:21)

.

f )
|
(12)

2(θnew)
1(θold)

= log Z
Z

The last equation above is obtained by noting that p(f
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
a, θold)qold(a)
p(f(cid:54)=a
(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)(cid:104)
|
θold)
a, θold)p(a
p(f(cid:54)=a
|
|

qold(f )
θold)
p(f
|

=

=

|
qold(a)
θold)
p(a
|

.

θnew)/p(f(cid:54)=b
|

θnew) and
b, θnew) = p(b
|

Since the KL divergence is non-negative, the second term in (12) is the negative lower bound of
(qnew(f )). We can
the approximate online log marginal likelihood, or the variational free energy,
decompose the bound as follows,

F

(cid:90)

(cid:20)

(qnew(f )) =

df qnew(f )

log

F

(cid:21)

θold)qnew(b)

p(a
|
θnew)qold(a)p(ynew
p(b
|
(cid:90)

|

f )

qnew(f ) log p(ynew

= KL(q(b)

θnew))

p(b
|

||
+ KL(qnew(a)

−
qold(a))

||

−

f )
|
p(a
|

||

KL(qnew(a)

θold)).

(14)

(13)

The ﬁrst two terms form the batch variational bound if the current batch is the whole training data,
and the last two terms constrain the posterior to take into account the old likelihood (through the
approximate posterior and the prior).

B.2 Derivation of the optimal posterior update and the collapsed bound

The aim is to ﬁnd the new approximate posterior qnew(f ) such that the free energy is minimised.
This is achieved by setting the derivative of
(cid:20)

and a Lagrange term 5 w.r.t. q(b) equal 0,

F

(cid:21)

(cid:90)

d
F
dq(b)

+ λ =

df(cid:54)=bp(f(cid:54)=b

log

b)
|

p(a
θold)q(b)
|
θnew)q(a) −
p(b
|

log p(y

f )

+ 1 + λ = 0,

(15)

|

resulting in,

1

(cid:16) (cid:90)

qopt(b) =

p(b) exp

q(a)

(cid:90)

b) log
dap(a
|

+

df p(f

b) log p(y
|

f )
|

(cid:17)

.

(16)

p(a

θold)
C
|
Note that we have dropped θnew from p(b
b, θnew) and p(f
θnew), p(a
|
|
|
notation. Substituting the above result into the variational free energy leads to
F
We now consider the exponents in the optimal qopt(b), noting that q(a) =
aa )−1, Qf = Kﬀ
aa), and denoting Da = (S−1
p(a
θold) =
|
Qa = Kaa
(cid:90)

(a; 0, K(cid:48)
KabK−1

bbKba,

a −

N
−

K(cid:48)−1

−

(qopt(f )) =

b, θnew) to lighten the
.
−
C
(a; ma, Sa) and
bbKbf , and

N
KfbK−1

log

(17)

E1 =

dap(a

p(a
|
5to ensure q(b) is normalised

b) log
|

q(a)

θold)

11

(a

−

−

ma)(cid:124)S−1

a (a

ma) + a(cid:124)K(cid:48)−1
aa a

−

da

(a; KabK−1

(cid:16)
bbb, Qa)

Sa
log |
K(cid:48)
−
|
bbb, Da) + ∆1,

|
aa|

a ma; KabK−1

N
(DaS−1

=

(cid:90)

1
2

= log
(cid:90)

N
df p(f

E2 =

=

df

(f ; KfbK−1

(y; f , σ2I)

b) log p(y
|

f )
|

= log

(cid:90)

−

−

N
log

N
(y; KfbK−1
Sa
|
K(cid:48)
aa||
|
1
2σ2 tr(Qf ).

|
Da

|

2∆1 =

∆2 =

bbb, Qf ) log

N
bbb, σ2I) + ∆2,
+ m(cid:124)

aS−1

a DaS−1

a ma

tr[D−1

a Qa]

m(cid:124)

aS−1

a ma + Ma log(2π), (22)

−

−

Putting these results back into the optimal q(b), we obtain:

qopt(b)

p(b)

(ˆy, Kˆf bK−1
(b; Kbˆf (Kˆf bK−1

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

N

∝
=

Kbˆf (Kˆf bK−1

−

(24)
bbKbˆf + Σˆy)−1Kˆf b) (25)

N

where

ˆy =

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)σ2
yI
0
0 Da

.

The negative variational free energy, which is the lower bound of the log marginal likelihood, can
also be derived,

= log

= log

C

N

F

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy) + ∆1 + ∆2.

B.3 Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

B.3.1 The variational free energy

The ﬁrst term in eq. (27) can be written as follows,

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

1 = log

F

=

N
N + Ma
2

−
Let Sy = Kˆf bK−1
log

log(2π)

1
2
(cid:124)
bbKbˆf + Σˆy and Kbb = LbL
b, using the matrix determinant lemma, we obtain,

bbKbˆf + Σˆy)−1 ˆy.

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy

Kˆf bK−1

log

| −

1
2

−

(29)

|

b Kbˆf Σ−1
+ log

ˆy Kˆf bL−(cid:124)
b |

I + L−1

b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
.
b |

|

Let D = I + L−1

b Kbˆf Σ−1

Sy

|

bbKbˆf + Σˆy
I + L−1

|

|

= log

= log

Kˆf bK−1
|
+ log
Σˆy
|
|
|
= N log σ2
y + log
ˆy Kˆf bL−(cid:124)
1
σ2
y

ˆy Kˆf b =

Da
|

|
b . Note that,

Kbˆf Σ−1

Kbf Kfb + KbaS−1

a Kab

KbaK(cid:48)−1

aa Kab.

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1
ˆy ,

leading to,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1

ˆy ˆy.

−

12

(cid:17)

(18)

(19)

(20)

(21)

(23)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

Note that,

ˆy(cid:124)Σ−1

ˆy ˆy =

y(cid:124)y + m(cid:124)

aS−1

a DaS−1

a ma,

1
σ2
y

and c = Kbˆf Σ−1

ˆy ˆy =

1
σ2 Kbf y + KbaS−1

a ma.

Substituting these results back into equation eq. (27),

log(2πσ2)

=

F

N
2

−
1
2

log

Sa

+

|

|

−

1
2
K(cid:48)
|

log

D

|

aa| −

1
2σ2 y(cid:124)y +
tr[D−1
a Qa]

| −
1
2

1
2
1
2

−

c(cid:124)L−(cid:124)

b D−1L−1
b c
1
2σ2 tr(Qf ).

a ma

aS−1

−

m(cid:124)

−

log

1
2

B.3.2 Prediction

We revisit and rewrite the optimal variational distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

N

∝
=

N

−1(b; K−1

bbKbˆf Σ−1

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb).

The predictive covariance at some test points s is:

Vss = Kss

= Kss

= Kss

−

−

−

KsbK−1
KsbK−1
KsbK−1

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−(cid:124)

b Kbˆf Σ−1
b Kbs.

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

And the predictive mean is:

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

b Kbˆf Σ−1
b Kbˆf Σ−1

ˆy Kˆf bL−(cid:124)
ˆy ˆy.

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

C Power-EP for streaming sparse Gaussian process regression

Similar to the variational approach above, we also use a = f (zold) and b = f (znew) as pseudo-
outputs before and after seeing new data. The exact posterior upon observing new data is

p(f

y, yold) =

p(f(cid:54)=a

|

1

Z
1

Z

a, θold)q(a)p(y
|

|

f )

q(a)

p(a

θold)
|

p(y

f ).

|

=

p(f

θold)

|

p(f

y, yold)

|

p(f

θnew)

|

1

Z

≈

q(a)

θold)

p(a
|

p(y

f ).

|

q(f )

p(f

θnew)q1(b)q2(b),
|

∝

In addition, we assume that the hyperparameters do not change signiﬁcantly after each online update
and as a result, the exact posterior can be approximated by:

We posit the following approximate posterior, which mirrors the form of the exact posterior,

where q1(b) and q2(b) are the approximate effect that
f ) have on the posterior,
respectively. Next we describe steps to obtain the closed-form expressions for the approximate factors
and the approximate marginal likelihood.

p(a|θold) and p(y

|

q(a)

13

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

C.1

q1(b)

The cavity and tilted distributions are:

qcav,1(f ) = p(f )q1−α

1
= p(f(cid:54)=a,b

and ˜q1(f ) = p(f(cid:54)=a,b

(b)q2(b)
b)q1−α
b)p(b)q2(b)p(a
1
|
b)q1−α
b)p(b)q2(b)p(a
1
|

|

|

(b)

(b)

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

.

θold) =
|

N

(a; 0, K(cid:48)

aa), leading to:

We note that, q(a) =

(a; ma, Sa) and p(a

N

(cid:19)α

(cid:18) q(a)
p(a
|
where ˆma = DaS−1

= C1

θold)

N

a ma,

(a; ˆma, ˆSa)

ˆSa =

Da,

1
α

Da = (S−1
a −
C1 = (2π)M/2

K(cid:48)−1

aa )−1,
α/2
K(cid:48)
|

aa|

|

−α/2

Sa

|

ˆSa

|

1/2 exp(
|

α
2

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma).

a −

Let Σa = Da + αQa. Note that:

As a result,

b) =

p(a
|

N

(a; KabK−1

bbb; Kaa

KabK−1

bbKba) =

(a; Wab, Qa).

−

N

(cid:90)

b)
dap(a
|

(cid:18) q(a)
p(a
|

θold)

(cid:19)α

(cid:90)

=

daC1

(a; ˆma, ˆSa)

N

N
( ˆma; Wab, Σa/α).

= C1

N

(a; Wab, Qa)

Since this is the contribution towards the posterior from a, it needs to match qα
that is,

1 (b) at convergence,

q1(b)

[C1

( ˆma; Wab, Σa/α)]1/α

∝
=
=

N
N

N
( ˆma; Wab, α(Σa/α))
( ˆma; Wab, Σa).

In addition, we can compute:
(cid:90)

log ˜Z1 = log

df ˜q1(f )

= log C1

= log C1

N

( ˆma; Wamcav, Σa/α + WaVcavW(cid:124)
a)
M
2
−
−
a(Σa/α + WaVcavW(cid:124)
cavW(cid:124)

a)−1 ˆma

log(2π)

Σa/α + WaVcavW(cid:124)

m(cid:124)

log

1
2

a| −
cavW(cid:124)

|

1
2

+ m(cid:124)

1
2

−

ˆm(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)

a)−1Wamcav.
(68)

Note that:

V−1 = V−1
V−1m = V−1

cav + W(cid:124)
cavmcav + W(cid:124)

a(Σa/α)−1Wa,

a(Σa/α)−1 ˆma.

Using matrix inversion lemma gives

V = Vcav

VcavW(cid:124)

a(Σa/α + WaVcavW(cid:124)

a)−1WaVcav.

−

Using matrix determinant lemma gives

V−1
|

|

=

V−1
|

cav||

(Σa/α)−1

Σa/α + WaVcavW(cid:124)
a|
||

.

14

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)
(64)
(65)

(66)

(67)

(69)

(70)

(71)

(72)

We can expand terms in log ˜Z1 above as follows:

log ˜Z1A =

log

Σa/α + WaVcavW(cid:124)
a|
|

1
2
1
2

−

−
1
2

=

=

log

(log

V−1
|

log

V−1
|

| −
1
2

cav| −
1
Vcav
V
2
|
|
a(Σa/α + WaVcavW(cid:124)
ˆm(cid:124)

log

| −

| −

log

(Σa/α)−1
|

)
|

.

log

(Σa/α)
|
|
a)−1 ˆma

−

=

−

=

m(cid:124)

cavW(cid:124)

log ˜Z1B =

log ˜Z1D =

1
2
1
ˆm(cid:124)
2
−
log ˜Z1C = m(cid:124)
cavW(cid:124)
cavW(cid:124)
= m(cid:124)
1
m(cid:124)
2
1
2
1
2
−
1
ˆm(cid:124)
2
ˆma(Σa/α)−1Wam
ˆm(cid:124)
ˆm(cid:124)
ˆm(cid:124)
m(cid:124)

log ˜Z1DA =
=
log ˜Z1DA1 =
=

cavmcav +

cavmcav +

cavV−1

cavV−1

m(cid:124)

−

−

−

=

−

+

=

−

a(Σa/α)−1 ˆma +

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

1
2

a(Σa/α)−1WaVW(cid:124)
a)−1Wamcav

a(Σa/α)−1 ˆma.

a)−1 ˆma

a(Σa/α + WaVcavW(cid:124)
a(Σa/α)−1 ˆma

m(cid:124)

cavW(cid:124)
−
a(Σa/α + WaVcavW(cid:124)
1
2
1
2

m(cid:124)V−1m

cavV−1

m(cid:124)

cavVV−1

cavmcav

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma

ˆma(Σa/α)−1Wam.

−

cavmcav

a(Σa/α)−1WaVV−1
a((Σa/α)−1)WaVV−1
a(Σa/α)−1Wa(I
cavW(cid:124)

−
cavmcav
VW(cid:124)
a(Σa/α)−1 ˆma + m(cid:124)

−

ˆm(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

aΣa/α)−1Wa)mcav
cavW(cid:124)

a(Σa/α)−1WaVW(cid:124)

a(Σa/α)−1 ˆma.

which results in:

−

C.2

q2(b)

log ˜Z1 + φcav,1

−

φpost = log C1

log(2π)

log

(Σa/α)

1
2

−

|

1
2

| −

M
2

−

ˆm(cid:124)

a(Σa/α)−1 ˆma.

(88)

We repeat the above procedure to ﬁnd q2(b). The cavity and tilted distributions are,

qcav,2(f ) = p(f )q1(b)q1−α

(b)

2
= p(f(cid:54)=f ,b|b)p(b)q1(b)p(f |b)q1−α

(b)
2
b)q1−α
b)p(b)q1(b)p(a
2
|

|

and ˜q2(f ) = p(f(cid:54)=f ,b

(b)pα(y

f )
|

We note that, p(y

f ) =

(y; f , σ2

yI) leading to,

|

N

(y; f , ˆSy)

pα(y

f ) = C2
|
σ2
where ˆSy =
y
α

N

I

C2 = (2πσ2

y)N (1−α)/2α−N/2

Let Σy = σ2

yI + αQf . Note that,

As a result,

p(f

b) =
|

N

(cid:90)

(f ; KfbK−1

bbb; Kﬀ

KfbK−1

bbKbf ) =

(a; Wf b, Qf )

N

dap(f

b)pα(y
|

f ) =
|

df C2

(y; f , ˆSy)

(f ; Wf b, B)

N

N

−

(cid:90)

15

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Since this is the contribution towards the posterior from y, it needs to match qα(b) at convergence,
that is,

= C2

(y; Wf b, ˆSy + Qf )

N

q2(b)

(y; Wf b, ˆSy + Qf )

(cid:105)1/α

(cid:104)
C2

N

∝
=
=

N
N

(y; Wf b, α(Σy/α))
(y; Wf b, Σy)

In addition, we can compute,
(cid:90)

log ˜Z2 = log

df ˜q2(f )

= log C2

= log C2

+ m(cid:124)

cavW

N

−

log(2π)

(y; Wf mcav, Σy/α + Wf VcavW
N
2
(cid:124)
f (Σy/α + Wf VcavW

f )−1y

log

1
2

−

(cid:124)

Σy/α + Wf VcavW
|

m(cid:124)

cavW

(cid:124)
f )

1
2

−

1
2

y(cid:124)(Σy/α + Wf VcavW

(cid:124)
f | −
(cid:124)
f (Σy/α + Wf VcavW

(cid:124)

f )−1Wf mcav
(103)

By following the exact procedure as shown above for q1(b), we can obtain,

log ˜Z2 + φcav,2

−

φpost = log C2

log(2π)

log

(Σy/α)

1
2

−

|

1
2

| −

N
2

−

y(cid:124)(Σy/α)−1y

(104)

C.3 Approximate posterior

Putting the above results together gives the approximate posterior over b as follows,

qopt(b)

p(b)q1(b)q2(b)
p(b)

(ˆy, Kˆf bK−1
(a; Kbˆf (Kˆf bK−1

N

∝

∝
=

N

bbb, Σˆy)
bbKbˆf + Σˆy)−1 ˆy, Kbb

−

Kbˆf (Kˆf bK−1

bbKbˆf + Σˆy)−1Kˆf b)

where

ˆy =

(cid:21)

(cid:20) y
ya

=

(cid:21)

(cid:20)
y
DaS−1
a ma

, Kˆf b =

, Σˆy =

(cid:21)

(cid:20)Kfb
Kab

(cid:21)

(cid:20)Σy

0
0 Σa

,

and Σy = σ2I + αdiagQf , and Σa = Da + αQa.

C.4 Approximate marginal likelihood

The Power-EP procedure above also provides us an approximation to the marginal likelihood, which
can be used to optimise the hyperparameters and the pseudo-inputs,

= φpost

φprior +

F

−

1
α

(log ˜Z1 + φcav,1

φpost) +

(log ˜Z2 + φcav,2

φpost)

(109)

−

1
α

−

1
2

1
2

1
2

ˆy(cid:124)Σ−1

ˆy ˆy +

y(cid:124)Σ−1

y y +

y(cid:124)
aΣ−1

a ya

I + αD−1
|

a Qa

| −

1
2

y(cid:124)
aΣ−1

a ya +

m(cid:124)

a[S−1

a DaS−1

S−1

a ]ma

a −

Note that,

∆0 = φpost

=

=

1
2

−
1
α
1
2

∆1 =

φprior
1
2

+

|

−
V
|

log

m(cid:124)V−1m

−

+

log

Σˆy

1
1
2
2
|
(log ˜Z1 + φcav,1

|

log

+

Σa|
|
φpost)

1
2
1
2

log

log

Kbb
|

|
Σy| −
|

1
2

=

log |

−

log

K(cid:48)
aa|
Sa
|
|

1
2α

−

16

(97)

(98)

(99)
(100)

(101)

(102)

(cid:124)

f )−1y

(105)

(106)

(107)

(108)

(110)

(111)

(112)

(113)

(114)

1
α

N
2

∆2 =

(log ˜Z2 + φcav,2

φpost)

−
N (1

=

log(2π) +

−
Therefore,

α)

−
2α

log(σ2
y)

1
2α

−

log

Σy

|

| −

1
2

y(cid:124)Σ−1

y y

= log

(ˆy; 0, Σˆy) +

F

N

1
2
Ma
2

+

+

log

K(cid:48)
|

aa| −

log(2π) +

N (1

α)

−
2α

log(σ2
y)

log

+

log

1
Sa
2
|
|
m(cid:124)
a DaS−1
a[S−1

1
2
1
2

a −

−
Σa| −
S−1

|

1

α

−
2α
1
2α

a ]ma

log

Σy

|
|
I + αD−1

a Qa

|

log

|

The limit as α tends to 0 is the variational free energy in eq. (27). This is achieved similar to the
batch case as detailed in [13] and by further observing that as α

0,

1
2α

log

I + αD−1
|

a Qa

| ≈

→
log(1 + αtr(D−1
a Qa) +

(α2))

O

1
2α
1
2

≈

tr(D−1

a Qa)

C.5

Implementation

In this section, we provide efﬁcient and numerical stable forms for a practical implementation of the
above results.

C.5.1 The Power-EP approximate marginal likelihood

The ﬁrst term in eq. (117) can be written as follows,

1 = log

(ˆy; 0, Kˆf bK−1

bbKbˆf + Σˆy)

F

−

=

N
N + Ma
2
Let denote Sy = Kˆf bK−1
D = I + L−1

b Kbˆf Σ−1

log(2π)

1
2

−

log

Kˆf bK−1

bbKbˆf + Σˆy

|

(cid:124)
bbKbˆf + Σˆy, Kbb = LbL

1
2
b, Qa = LqL(cid:124)

| −

ˆy(cid:124)(Kˆf bK−1

bbKbˆf + Σˆy)−1 ˆy (121)

q, Ma = I + αL(cid:124)

qD−1

a Lq and

b . By using the matrix determinant lemma, we obtain,

ˆy Kˆf bL−(cid:124)
log

Sy
|

|

= log

= log

= log

Kˆf bK−1
+ log
Σˆy
Σy

bbKbˆf + Σˆy
I + L−1
|
Σa
|

+ log

b Kbˆf Σ−1
D
+ log
|

|

|

|

|

|

|

|

|

ˆy Kˆf bL−(cid:124)
b |

Note that,

Kbˆf Σ−1
Kbf Σ−1
KbaΣ−1

y Kfb + KbaΣ−1
ˆy Kˆf b = Kbf Σ−1
y Kfb = Kbf (σ2
yI + αQf )−1Kfb
a Kab = Kba(Da + αQa)−1Kab

a Kab

= Kba(D−1
= KbaD−1

a −
a Kab

αD−1

a Lq[I + αL(cid:124)
αKbaD−1

qD−1
a LqM−1

a Lq]−1L(cid:124)
a L(cid:124)
qD−1

a Kab

qD−1

a )Kab

−

Using the matrix inversion lemma gives us,
y = (Kˆf bK−1
bbKbˆf + Σˆy)−1
S−1
ˆy Kˆf bL−(cid:124)
Σ−1
= Σ−1

ˆy −

b D−1L−1

b Kbˆf Σ−1

ˆy

leading to,

Note that,

ˆy(cid:124)S−1

y ˆy = ˆy(cid:124)Σ−1
ˆy ˆy

ˆy(cid:124)Σ−1

ˆy Kˆf bL−(cid:124)

b D−1L−1

b Kbˆf Σ−1
ˆy ˆy

−

(115)

(116)

(117)

(118)

(119)

(120)

(122)

(123)

(124)

(125)

(126)

(127)

(128)

(129)

(130)

(131)

(132)

(133)

ˆy(cid:124)Σ−1

ˆy ˆy = y(cid:124)Σ−1

y y + y(cid:124)

aΣ−1
ya

ya

17

y(cid:124)Σ−1
y(cid:124)
aΣ−1
ya

y y = y(cid:124)(σ2
yI + αQf )−1y
ya = y(cid:124)
a(Da + αQa)−1ya
= m(cid:124)
a D−1
aS−1
a ma
and c = Kbˆf Σ−1
ˆy ˆy
y y + KbaΣ−1
= Kbf Σ−1
y y + KbaS−1
= Kbf Σ−1

a S−1

−

a ya
a ma

αm(cid:124)

aS−1

a LqM−1

a L(cid:124)

qS−1

a ma

−
Substituting these results back into equation eq. (117),

αKbaD−1

a LqM−1

a L(cid:124)

qS−1

a ma

y(cid:124)Σ−1

y y +

αm(cid:124)

aS−1

a L(cid:124)

a ma +

c(cid:124)L−(cid:124)

b D−1L−1
b c

=

F

−

1
2
1
2
N (1

log

−

+

1
2
1
2

a LqM−1
1
2
α

| −
1

log

Sa
|

−
2α

log

Σy
|

−

+

qS−1
1
2
N
2

| −

|

1
2
K(cid:48)
|

log(2π)

Σy
|

α)

log

D
|

| −
log(σ2
y)

−
2α

log

1
2α

aa| −

log

Ma

|

| −

1
2

m(cid:124)

aS−1

a ma

C.5.2 Prediction

We revisit and rewrite the optimal approximate distribution, qopt(b), using its natural parameters:

qopt(b)

p(b)

(ˆy, Kˆf bK−1

bbb, Σˆy)

∝
=

N

N

−1(b; K−1

bbKbˆf Σ−1
The predictive covariance at some test points s is,
bb(K−1
b (I + L−1
b D−1L−(cid:124)

bbKbs + KsbK−1
bbKbs + KsbL−(cid:124)
bbKbs + KsbL−(cid:124)

KsbK−1
KsbK−1
KsbK−1

Vss = Kss

= Kss

= Kss

−

−

And, the predictive mean,

−

ˆy ˆy, K−1

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1
bb)

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ˆy Kˆf bL−(cid:124)

b )−1L−(cid:124)

bbKbs

bb)−1K−1
b Kbs

b Kbˆf Σ−1
b Kbs

bb + K−1

bbKbˆf Σ−1

ˆy Kˆf bK−1

ms = KsbK−1
= KsbL−(cid:124)
= KsbL−(cid:124)

bb(K−1
b (I + L−1
b D−1L−1

ˆy Kˆf bL−(cid:124)
b Kbˆf Σ−1
b Kbˆf Σ−1
ˆy ˆy

bb)−1K−1

bbKbˆf Σ−1
ˆy ˆy
b Kbˆf Σ−1
ˆy ˆy

b )−1L−1

D Equivalence results

When the hyperparameters and the pseudo-inputs are ﬁxed, α-divergence inference for streaming
sparse GP regression recovers the batch solutions provided by Power-EP with the same α value. In
other words, only a single pass through the data is necessary for Power-EP to converge in sparse GP
regression. This result is in a similar vein to the equivalence between sequential inference and batch
inference in full GP regression, when the hyperparameters are kept ﬁxed. As an illustrative example,
assume that za = zb and θ is kept ﬁxed, and
are the ﬁrst and second data
batches respectively. The optimal variational update gives,

x1, y1
{

x2, y2

and

{

}

}

q1(a)

p(a) exp

df1p(f1

a) log p(y1

|

f1)
|

(cid:90)

(cid:90)

(cid:90)

∝

∝

q2(a)

q1(a) exp

df2p(f2

a) log p(y2
|

|

f2)

∝

p(a) exp

df p(f

a) log p(y

f )

(150)

|

|

where y =
. Equation (150) is exactly identical to the optimal variational
}
approximation for the batch case of [3], when we group all data batches into one. A similar procedure
can be shown for Power-EP. We demonstrate this equivalence in ﬁg. 6.

y1, y2
{

f1, f2
{

and f =

}

In addition, in the setting where hyperparameters and the pseudo-inputs are ﬁxed, if pseudo-points
are added at each stage at the new data input locations, the method returns the true posterior and
marginal likelihood. This equivalence is demonstrated in ﬁg. 7.

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

18

Figure 6: Equivalence between the streaming variational approximation and the batch variational
approximation when hyperparameters and pseudo-inputs are ﬁxed. The inset numbers are the
approximate marginal likelihood (the variational free energy) for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E Extra experimental results

E.1 Hyperparameter learning on synthetic data

In this experiment, we generated several time series from GPs with known kernel hyperparameters and
observation noise. We tracked the hyperparameters as the streaming algorithm learns and plot their
traces in ﬁgs. 8 and 9. It could be seen that for the smaller lengthscale, we need more pseudo-points
to cover the input space and to learn correct hyperparameters. Interestingly, all models including
full GP regression on the entire dataset tend to learn bigger noise variances. Overall, the proposed
streaming method can track and learn good hyperparameters; and if there is enough pseudo-points,
this method performs comparatively to full GP on the entire dataset.

E.2 Learning and inference on a toy time series

As shown in the main text, we construct a synthetic time series to demonstrate the learning procedure
as data arrives sequentially. Figures 10 and 11 show the results for non-iid and iid streaming settings
respectively.

E.3 Binary classiﬁcation

We consider a binary classiﬁcation task on the benchmark banana dataset. In particular, we test two
streaming settings, non-iid and iid, as shown in ﬁgs. 12 and 13 respectively. In all cases, the streaming
algorithm performs well and reaches the performance of the batch case using a sparse variational
method [5] (as shown in the right-most plots).

19

Figure 7: Equivalence between the streaming variational approximation and the exact GP regression
when hyperparameters and pseudo-inputs are ﬁxed, and the pseudo-points are at the training points.
The inset numbers are the (approximate) marginal likelihood for each model. Note that the numbers
in the batch case are the cumulative sum of the numbers on the left for the streaming case. Small
differences, if any, are merely due to numerical computation.

E.4 Sensitivity to the order of the data

We consider the classiﬁcation task above but now with more (smaller) mini-batches and the order
of the batches are varied. The aim is to evaluate the sensitivity of the algorithm to the order of the
data. The classiﬁcation errors as data arrive are included in table 1 and are consistent with what we
included in the main text.

Order/Index

Left to Right
Right to Left
Random
Batch

Table 1: Classiﬁcation errors as data arrive in different orders
1

7

6

2

5

4

3

8

9

10

0.255
0.255
0.5025

0.145
0.1475
0.2775

0.1325
0.1325
0.26

0.1225
0.12
0.2725

0.1075
0.105
0.2875

0.11
0.1025
0.1975

0.105
0.0975
0.1125

0.1
0.0925
0.125

0.0925
0.09
0.105

0.0875
0.095
0.095
0.095

E.5 Additional plots for the time-series and spatial datasets

In this section, we plot the mean marginal log-likelihood and RMSE against the number of batches
for the models in the “speed versus accuracy” experiment in the main text. Fig. 14 shows the results
for the time-series datasets while ﬁg. 15 shows the results for the spatial datasets.

20

Figure 8: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.5. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

Figure 9: Learnt hyperparameters on a time series dataset, that was generated from a GP with an
exponentiated quadratic kernel and with a lengthscale of 0.8. Note the y
axis show the difference
between the learnt values and the groundtruth.

−

21

Figure 10: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in a non-iid setting. The black crosses are data points (past points are greyed out),
the red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means
and conﬁdence intervals at test points.

22

Figure 11: Online regression on a toy time series using variational inference (top) and Power-EP with
α = 0.5 (bottom), in an iid setting. The black crosses are data points (past points are greyed out), the
red circles are pseudo-points, and blue lines and shaded areas are the marginal predictive means and
conﬁdence intervals at test points.

Figure 12: Classifying binary data in a non-iid streaming setting. The right-most plot shows the
prediction made by using sparse variational inference on full training data.

23

Figure 13: Classifying binary data in an iid streaming setting. The right-most plot shows the prediction
made by using sparse variational inference on full training data.

pseudo periodic data, batch size = 300

pseudo periodic data, batch size = 500

audio data, batch size = 300

audio data, batch size = 500

Figure 14: Results for time-series datasets with batch sizes 300 and 500. The solid and dashed lines
are for M = 100, 200 respectively.

24

terrain data, batch size = 750

terrain data, batch size = 1000

Figure 15: Results for spatial data (see ﬁg. 14 for the legend). Solid and dashed lines indicate the
results for M = 400, 600 pseudo-points respectively.

25

