No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling

Xin Wang∗, Wenhu Chen∗, Yuan-Fang Wang , William Yang Wang
University of California, Santa Barbara
{xwang,wenhuchen,yfwang,william}@cs.ucsb.edu

8
1
0
2
 
l
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
0
6
1
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Though impressive results have been
achieved in visual captioning,
the task
of generating abstract stories from photo
streams is still a little-tapped problem.
Different from captions, stories have more
expressive language styles and contain
many imaginary concepts that do not ap-
pear in the images. Thus it poses chal-
lenges to behavioral cloning algorithms.
Furthermore, due to the limitations of au-
tomatic metrics on evaluating story qual-
ity, reinforcement learning methods with
hand-crafted rewards also face difﬁcul-
ties in gaining an overall performance
boost. Therefore, we propose an Adver-
sarial REward Learning (AREL) frame-
work to learn an implicit reward function
from human demonstrations, and then op-
timize policy search with the learned re-
ward function. Though automatic eval-
uation indicates slight performance boost
over state-of-the-art (SOTA) methods in
cloning expert behaviors, human evalua-
tion shows that our approach achieves sig-
niﬁcant improvement in generating more
human-like stories than SOTA systems.1

1

Introduction

Recently, increasing attention has been focused
on visual captioning (Chen et al., 2015, 2016; Xu
et al., 2016; Wang et al., 2018c), which aims at
describing the content of an image or a video.
Though it has achieved impressive results, its ca-
pability of performing human-like understanding
is still restrictive. To further investigate machine’s

∗ Equal contribution
1Code
released

is

littlekobe/AREL

at https://github.com/

Figure 1: An example of visual storytelling and
visual captioning. Both captions and stories are
shown here: each image is captioned with one sen-
tence, and we also demonstrate two diversiﬁed sto-
ries that match the same image sequence.

capabilities in understanding more complicated vi-
sual scenarios and composing more structured ex-
pressions, visual storytelling (Huang et al., 2016)
has been proposed. Visual captioning is aimed at
depicting the concrete content of the images, and
its expression style is rather simple. In contrast,
visual storytelling goes one step further: it sum-
marizes the idea of a photo stream and tells a story
about it. Figure 1 shows an example of visual
captioning and visual storytelling. We have ob-
served that stories contain rich emotions (excited,
happy, not want) and imagination (siblings, par-
ents, school, car). It, therefore, requires the capa-
bility to associate with concepts that do not explic-
itly appear in the images. Moreover, stories are
more subjective, so there barely exists standard

templates for storytelling. As shown in Figure 1,
the same photo stream can be paired with diverse
stories, different from each other. This heavily in-
creases the evaluation difﬁculty.

So far, prior work for visual storytelling (Huang
et al., 2016; Yu et al., 2017b) is mainly inspired
by the success of visual captioning. Nevertheless,
because these methods are trained by maximizing
the likelihood of the observed data pairs, they are
restricted to generate simple and plain description
with limited expressive patterns. In order to cope
with the challenges and produce more human-like
descriptions, Rennie et al. (2017) have proposed
a reinforcement learning framework. However, in
the scenario of visual storytelling, the common re-
inforced captioning methods are facing great chal-
lenges since the hand-crafted rewards based on
string matches are either too biased or too sparse
to drive the policy search. For instance, we used
the METEOR (Banerjee and Lavie, 2005) score
as the reward to reinforce our policy and found
that though the METEOR score is signiﬁcantly
improved, the other scores are severely harmed.
Here we showcase an adversarial example with an
average METEOR score as high as 40.2:

We had a great time to have a lot of the.
They were to be a of the. They were to be in
the. The and it were to be the. The, and it
were to be the.

Apparently, the machine is gaming the metrics.
Conversely, when using some other metrics (e.g.
BLEU, CIDEr) to evaluate the stories, we observe
an opposite behavior: many relevant and coherent
stories are receiving a very low score (nearly zero).
In order to resolve the strong bias brought by
the hand-coded evaluation metrics in RL training
and produce more human-like stories, we propose
an Adversarial REward Learning (AREL) frame-
work for visual storytelling. We draw our inspi-
ration from recent progress in inverse reinforce-
ment learning (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017) and propose the AREL algo-
rithm to learn a more intelligent reward function.
Speciﬁcally, we ﬁrst incorporate a Boltzmann dis-
tribution to associate reward learning with distri-
bution approximation, then design the adversarial
process with two models – a policy model and a
reward model. The policy model performs the
primitive actions and produces the story sequence,
while the reward model is responsible for learning

the implicit reward function from human demon-
strations. The learned reward function would be
employed to optimize the policy in return.

For evaluation, we conduct both automatic met-
rics and human evaluation but observe a poor cor-
relation between them. Particularly, our method
gains slight performance boost over the base-
line systems on automatic metrics; human evalu-
ation, however, indicates signiﬁcant performance
boost. Thus we further discuss the limitations
of the metrics and validate the superiority of our
AREL method in performing more intelligent un-
derstanding of the visual scenes and generating
more human-like stories.

Our main contributions are four-fold:

• We propose an adversarial reward learning
framework and apply it to boost visual story
generation.

• We evaluate our approach on the Visual
Storytelling (VIST) dataset and achieve the
state-of-the-art results on automatic metrics.

• We empirically demonstrate that automatic
metrics are not perfect for either training or
evaluation.

• We design and perform a comprehensive
human evaluation via Amazon Mechanical
Turk, which demonstrates the superiority of
the generated stories of our method on rele-
vance, expressiveness, and concreteness.

2 Related Work

Visual Storytelling Visual storytelling is the
task of generating a narrative story from a photo
stream, which requires a deeper understanding
of the event ﬂow in the stream. Park and Kim
(2015) has done some pioneering research on sto-
rytelling. Chen et al. (2017) proposed a multi-
modal approach for storyline generation to pro-
duce a stream of entities instead of human-like de-
scriptions. Recently, a more sophisticated dataset
for visual storytelling (VIST) has been released
to explore a more human-like understanding of
grounded stories (Huang et al., 2016). Yu et al.
(2017b) proposes a multi-task learning algorithm
for both album summarization and paragraph gen-
eration, achieving the best results on the VIST
dataset. But these methods are still based on be-
havioral cloning and lack the ability to generate
more structured stories.

Reinforcement Learning in Sequence Genera-
tion Recently, reinforcement learning (RL) has
gained its popularity in many sequence generation
tasks such as machine translation (Bahdanau et al.,
2016), visual captioning (Ren et al., 2017; Wang
et al., 2018b), summarization (Paulus et al., 2017;
Chen et al., 2018), etc. The common wisdom of
using RL is to view generating a word as an ac-
tion and aim at maximizing the expected return
by optimizing its policy. As pointed in (Ranzato
et al., 2015), traditional maximum likelihood al-
gorithm is prone to exposure bias and label bias,
while the RL agent exposes the generative model
to its own distribution and thus can perform bet-
ter. But these works usually utilize hand-crafted
metric scores as the reward to optimize the model,
which fails to learn more implicit semantics due to
the limitations of automatic metrics.

Rethinking Automatic Metrics Automatic
metrics, including BLEU (Papineni et al., 2002),
CIDEr (Vedantam et al., 2015), METEOR (Baner-
jee and Lavie, 2005), and ROUGE (Lin, 2004),
have been widely applied to the sequence gener-
ation tasks. Using automatic metrics can ensure
rapid prototyping and testing new models with
fewer expensive human evaluation. However, they
have been criticized to be biased and correlate
poorly with human judgments, especially in many
generative tasks like response generation (Lowe
et al., 2017; Liu et al., 2016), dialogue sys-
tem (Bruni and Fern´andez, 2017) and machine
translation (Callison-Burch et al., 2006). The
naive overlap-counting methods are not able
to reﬂect many semantic properties in natural
language, such as coherence, expressiveness, etc.

Generative Adversarial Network Generative
adversarial network (GAN) (Goodfellow et al.,
2014) is a very popular approach for estimating
intractable probabilities, which sidestep the difﬁ-
culty by alternately training two models to play a
min-max two-player game:

min
D

max
G

E
x∼pdata

[log D(x)] + E
z∼pz

[log D(G(z))] ,

where G is the generator and D is the discrimina-
tor, and z is the latent variable. Recently, GAN
has quickly been adopted to tackle discrete prob-
lems (Yu et al., 2017a; Dai et al., 2017; Wang et al.,
2018a). The basic idea is to use Monte Carlo pol-
icy gradient estimation (Williams, 1992) to update
the parameters of the generator.

Figure 2: AREL framework for visual storytelling.

Inverse Reinforcement Learning Reinforce-
ment learning is known to be hindered by the
need for an extensive feature and reward engi-
neering, especially under the unknown dynamics.
Therefore, inverse reinforcement learning (IRL)
has been proposed to infer expert’s reward func-
tion. Previous IRL approaches include maximum
margin approaches (Abbeel and Ng, 2004; Ratliff
et al., 2006) and probabilistic approaches (Ziebart,
2010; Ziebart et al., 2008). Recently, adversarial
inverse reinforcement learning methods provide
an efﬁcient and scalable promise for automatic re-
ward acquisition (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017; Henderson et al., 2017).
These approaches utilize the connection between
IRL and energy-based model and associate every
data with a scalar energy value by using Boltz-
mann distribution pθ(x) ∝ exp(−Eθ(x)).
In-
spired by these methods, we propose a practical
AREL approach for visual storytelling to uncover
a robust reward function from human demonstra-
tions and thus help produce human-like stories.

3 Our Approach

3.1 Problem Statement

Here we consider the task of visual storytelling,
whose objective is to output a word sequence W =
(w1, w1, · · · , wT ), wt ∈ V given an input image
stream of 5 ordered images I = (I1, I2, · · · , I5),
where V is the vocabulary of all output token.
We formulate the generation as a markov deci-
sion process and design a reinforcement learning
framework to tackle it. As described in Figure 2,
our AREL framework is mainly composed of two
modules: a policy model πβ(W ) and a reward
model Rθ(W ). The policy model takes an image
sequence I as the input and performs sequential
actions (choosing words w from the vocabulary V)
to form a narrative story W . The reward model

Figure 3: Overview of the policy model. The vi-
sual encoder is a bidirectional GRU, which en-
codes the high-level visual features extracted from
the input images. Its outputs are then fed into the
RNN decoders to generate sentences in parallel.
Finally, we concatenate all the generated sentences
as a full story. Note that the ﬁve decoders share the
same weights.

is optimized by the adversarial objective (see Sec-
tion 3.3) and aims at deriving a human-like reward
from both human-annotated stories and sampled
predictions.

3.2 Model

Policy Model As is shown in Figure 3, the pol-
icy model is a CNN-RNN architecture. We ﬁst
feed the photo stream I = (I1, · · · , I5) into a
pretrained CNN and extract their high-level image
features. We then employ a visual encoder to fur-
ther encode the image features as context vectors
−→
hi = [
hi ]. The visual encoder is a bidirectional
gated recurrent units (GRU).

←−
hi ;

In the decoding stage, we feed each context vec-
tor hi into a GRU-RNN decoder to generate a sub-
story Wi. Formally, the generation process can be
written as:

t = GRU(si
t−1, [wi
si
1:t−1) = sof tmax(Wssi

t−1, hi]) ,
t + bs) ,

(1)

(2)

πβ(wi

t|wi

where si
t denotes the t-th hidden state of i-th de-
coder. We concatenate the previous token wi
t−1
and the context vector hi as the input. Ws and
bs are the projection matrix and bias, which out-
put a probability distribution over the whole vo-
cabulary V. Eventually, the ﬁnal story W is the
concatenation of the sub-stories Wi. β denotes all
the parameters of the encoder, the decoder, and the
output layer.

Figure 4: Overview of the reward model. Our re-
ward model is a CNN-based architecture, which
utilizes convolution kernels with size 2, 3 and 4
to extract bigram, trigram and 4-gram representa-
tions from the input sequence embeddings. Once
the sentence representation is learned, it will be
concatenated with the visual representation of the
input image, and then be fed into the ﬁnal FC layer
to obtain the reward.

Reward Model The reward model Rθ(W ) is a
CNN-based architecture (see Figure 4). Instead of
giving an overall score for the whole story, we ap-
ply the reward model to different story parts (sub-
stories) Wi and compute partial rewards, where
i = 1, · · · , 5. We observe that the partial rewards
are more ﬁne-grained and can provide better guid-
ance for the policy model.

We ﬁrst query the word embeddings of the sub-
story (one sentence in most cases). Next, multi-
ple convolutional layers with different kernel sizes
are used to extract the n-grams features, which
are then projected into the sentence-level repre-
sentation space by pooling layers (the design here
In addition to the
is inspired by Kim (2014)).
textual features, evaluating the quality of a story
should also consider the image features for rele-
vance. Therefore, we then combine the sentence
representation with the visual feature of the input
image through concatenation and feed them into
the ﬁnal fully connected decision layer.
In the
end, the reward model outputs an estimated reward
value Rθ(W ). The process can be written in for-
mula:

Rθ(W ) = φ(Wr(fconv(W ) + WiICN N ) + br),

(3)

where φ denotes the non-linear projection func-
tion, Wr, br denote the weight and bias in the
output layer, and fconv denotes the operations in
CNN. ICN N is the high-level visual feature ex-
tracted from the image, and Wi projects it into the

sentence representation space. θ includes all the
parameters above.

3.3 Learning

Reward Boltzmann Distribution In order to
associate story distribution with reward function,
we apply EBM to deﬁne a Reward Boltzmann dis-
tribution:

Algorithm 1 The AREL Algorithm.

1: for episode ← 1 to N do
2:

collect story W by executing policy πθ
if Train-Reward then
θ ← θ − η × ∂Jθ
else if Train-Policy then

∂θ (see Equation 9)

collect story ˜W from empirical pe
β ← β − η × ∂Jβ
∂β (see Equation 9)

3:

4:

5:

6:

7:

pθ(W ) =

exp(Rθ(W ))
Zθ

,

(4)

end if

8:
9: end for

Where W is the word sequence of the story and
pθ(W ) is the approximate data distribution, and
Zθ = (cid:80)
exp(Rθ(W )) denotes the partition func-
W
tion. According to the energy-based model (Le-
Cun et al., 2006), the optimal reward function
R∗(W ) is achieved when the Reward-Boltzmann
distribution equals to the “real” data distribution
pθ(W ) = p∗(W ).

Adversarial Reward Learning We ﬁrst intro-
1(W ∈D)
duce an empirical distribution pe(W ) =
|D|
to represent the empirical distribution of the train-
ing data, where D denotes the dataset with |D| sto-
ries and 1 denotes an indicator function. We use
this empirical distribution as the “good” examples,
which provides the evidence for the reward func-
tion to learn from.

In order to approximate the Reward Boltzmann
distribution towards the “real” data distribution
p∗(W ), we design a min-max two-player game,
where the Reward Boltzmann distribution pθ aims
at maximizing the its similarity with empirical
distribution pe while minimizing that with the
“faked” data generated from policy model πβ. On
the contrary, the policy distribution πβ tries to
maximize its similarity with the Boltzmann dis-
tribution pθ. Formally, the adversarial objective
function is deﬁned as
max
β

KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W )) .

min
θ

We further decompose it into two parts. First,
because the objective Jβ of the story genera-
tion policy is to maximize its similarity with
the Boltzmann distribution pθ, the optimal policy
that minimizes KL-divergence is thus π(W ) ∼
exp(Rθ(W )), meaning if Rθ is optimal, the op-
timal πβ = π∗. In formula,

Jβ = − KL(πβ(W )||pθ(W ))

=

E
W ∼πβ (W )

[Rθ(W )] − log Zθ + H(πβ(W )) ,

(6)

where H denotes the entropy of the policy model.
On the other hand, the objective Jθ of the re-
ward function is to distinguish between human-
annotated stories and machine-generated stories.
Hence it is trying to minimize the KL-divergence
with the empirical distribution pe and maximize
the KL-divergence with the approximated policy
distribution πβ:

Jθ =KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W ))

(cid:88)

=

[pe(W )Rθ(W ) − πβ(W )Rθ(W )]

W
+ log Zθ − log Zθ − H(pe) + H(πβ) ,

(7)

Since H(πβ) and H(pe) are irrelevant to θ, we
denote them as constant C. It is also worth not-
ing that with negative sampling in the optimization
of the KL-divergence, the computation of the in-
tractable partition function Zθ is bypassed. There-
fore, the objective Jθ can be further derived as

Jθ =

E
W ∼pe(W )

[Rθ(W )] −

E
W ∼πβ (W )

[Rθ(W )] + C . (8)

Here we propose to use stochastic gradient de-
scent to optimize these two models alternately.
Formally, the gradients can be written as

(5)

∂Jθ
∂θ

∂Jβ
∂β

=

E
[
W ∼pe(W )

∂Rθ(W )
∂θ

] −

E
[
W ∼πβ (W )

∂Rθ(W )
∂θ

] ,

=

E
W ∼πβ (W )

(Rθ(W ) − log πβ(W ) − b)

∂ log πβ(W )
∂β

,

(9)

where b is the estimated baseline to reduce vari-
ance during REINFORCE training.

Training & Testing As described in Algo-
rithm 1, we introduce an alternating algorithm to
train these two models using stochastic gradient
descent. During testing, the policy model is used
with beam search to produce the story.

4 Experiments and Analysis

4.1 Experimental Setup

VIST Dataset The VIST dataset (Huang et al.,
2016) is the ﬁrst dataset for sequential vision-to-
language tasks including visual storytelling, which
consists of 10,117 Flickr albums with 210,819
unique photos.
In this paper, we mainly evalu-
ate our AREL method on this dataset. After ﬁlter-
ing the broken images2, there are 40,098 training,
4,988 validation, and 5,050 testing samples. Each
sample contains one story that describes 5 selected
images from a photo album (mostly one sentence
per image). And the same album is paired with 5
different stories as references. In our experiments,
we used the same split settings as in (Huang et al.,
2016; Yu et al., 2017b) for a fair comparison. Dur-
ing our experiments, we apply two kinds of non-
linear functions φ for the discriminator, namely
SoftSign function (f (x) = x
1+|x| ) and Hyper-
bolic function (f (x) = sinhx
coshx ). We found that
unbounded non-linear functions like ReLU func-
tion (Glorot et al., 2011) will lead to severe vibra-
tions and instabilities during training, therefore we
resort to the bounded functions.

Evaluation Metrics
In order to comprehen-
sively evaluate our method on storytelling dataset,
we adopt both the automatic metrics and human
evaluation as our criterion. Four diverse automatic
metrics are used in our experiments: BLEU, ME-
TEOR, ROUGE-L, and CIDEr. We utilize the
open source evaluation code3 used in (Yu et al.,
2017b). For human evaluation, we employ the
Amazon Mechanical Turk to perform two kinds of
user studies (see Section 4.3 for more details).

employ

Training Details We
pretrained
ResNet-152 model (He et al., 2016) to extract
image features from the photostream. We built a
vocabulary of size 9,837 to include words appear-
ing more than three times in the training set. More
training details can be found at Appendix B.

4.2 Automatic Evaluation

In this section, we compare our AREL method
with the state-of-the-art methods as well as stan-
dard reinforcement learning algorithms on auto-

2There are only 3 (out of 21,075) broken images in the
test set, which basically has no inﬂuence on the ﬁnal results.
Moreover, Yu et al. (2017b) also removed the 3 pictures, so it
is a fair comparison.

3https://github.com/lichengunc/vist_eval

Method

B-1 B-2 B-3 B-4 M R

C

Huang et al.
Yu et al.
XE-ss
GAN
AREL-s-50
AREL-t-50
AREL-s-100
AREL-t-100

-
-

-
-

-
21.0

-
-
31.4
-
34.1 29.5 7.5
-
62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.8 38.8 23.0 14.0 35.0 29.5 9.0
62.9 38.4 22.7 14.0 34.9 29.4 9.1
63.4 39.0 23.1 14.1 35.2 29.6 9.5
64.0 38.6 22.3 13.2 35.1 29.3 9.6
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 1: Automatic evaluation on the VIST
dataset. We report BLEU (B), METEOR (M),
ROUGH-L (R), and CIDEr (C) scores of the
SOTA systems and the models we implemented,
including XE-ss, GAN and AREL. AREL-s-N de-
notes AREL models with SoftSign as output acti-
vation and alternate frequency as N, while AREL-
t-N denoting AREL models with Hyperbolic as the
output activation (N = 50 or 100).

matic evaluation metrics. Then we further discuss
the limitations of the hand-crafted metrics on eval-
uating human-like stories.

Comparison with SOTA on Automatic Metrics
In Table 1, we compare our method with Huang
et al. (2016) and Yu et al. (2017b), which report
achieving best-known results on the VIST dataset.
We ﬁrst implement a strong baseline model (XE-
ss), which share the same architecture with our
policy model but is trained with cross-entropy loss
and scheduled sampling. Besides, we adopt the
traditional generative adversarial training for com-
parison (GAN). As shown in Table 1, our XE-
ss model already outperforms the best-known re-
sults on the VIST dataset, and the GAN model can
bring a performance boost. We then use the XE-
ss model to initialize our policy model and further
train it with AREL. Evidently, our AREL model
performs the best and achieves the new state-of-
the-art results across all metrics.

But, compared with the XE-ss model, the per-
formance gain is minor, especially on METEOR
and ROUGE-L scores. However, in Sec. 4.3, the
extensive human evaluation has indicated that our
AREL framework brings a signiﬁcant improve-
ment on generating human-like stories over the
XE-ss model. The inconsistency of automatic
evaluation and human evaluation lead to a suspect
that these hand-crafted metrics lack the ability to
fully evaluate stories’ quality due to the compli-
cated characteristics of the stories. Therefore, we
conduct experiments to analyze and discuss the

Method

B-1 B-2 B-3 B-4 M R

C

XE-ss
BLEU-RL
METEOR-RL
ROUGE-RL
CIDEr-RL
AREL (best)

62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.1 38.0 22.6 13.9 34.6 29.0 8.9
40.2 30.0 1.2
68.1 35.0 15.4
6.8
27.0 33.8
58.1 18.5
0
0
1.6
61.9 37.8 22.5 13.8 34.9 29.7 8.1
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 2: Comparison with different RL mod-
els with different metric scores as the rewards.
We report the average scores of the AREL mod-
els as AREL (avg). Although METEOR-RL and
ROUGE-RL models achieve the highest scores
on their own metrics, the underlined scores are
severely damaged. Actually, they are gaming their
own metrics with nonsense sentences.

defects of the automatic metrics in section 4.2.

Limitations of Automatic Metrics String-
match-based automatic metrics are not perfect
and fail to evaluate some semantic characteristics
of the stories (e.g. expressiveness and coherence).
In order to conﬁrm our conjecture, we utilize
automatic metrics as rewards to reinforce the
model with policy gradient.
The quantitative
results are demonstrated in Table 1.

Apparently, METEOR-RL and ROUGE-RL are
severely ill-posed: they obtain the highest scores
on their own metrics but damage the other met-
rics severely. We observe that these models are
actually overﬁtting to a given metric while losing
the overall coherence and semantical correctness.
Same as METEOR score, there is also an adver-
sarial example for ROUGE-L4, which is nonsense
but achieves an average ROUGE-L score of 33.8.
Besides, as can be seen in Table 1, after rein-
forced training, BLEU-RL and CIDEr-RL do not
bring a consistent improvement over the XE-ss
model. We plot the histogram distributions of both
BLEU-3 and CIDEr scores on the test set in Fig-
ure 5. An interesting fact is that there are a large
number of samples with nearly zero score on both
metrics. However, we observed those “zero-score”
samples are not pointless results; instead, lots of
them make sense and deserve a better score than
zero. Here is a “zero-score” example on BLEU-3:

I had a great time at the restaurant today.
The food was delicious. I had a lot of food.
The food was delicious. I had a great time.

4An adversarial example for ROUGE-L: we the was a .
and to the . we the was a . and to the . we the was a . and to
the . we the was a . and to the . we the was a . and to the .

Method
XE-ss
BLEU-RL
CIDEr-RL
GAN
AREL

Win
Lose
22.4% 71.7%
23.4% 67.9%
13.8% 80.3%
34.3% 60.5%
38.4% 54.2%

Unsure
5.9%
8.7%
5.9%
5.2%
7.4%

Table 3: Turing test results.

The corresponding reference is

The table of food was a pleasure to see!
Our food is both nutritious and beautiful!
Our chicken was especially tasty! We love
greens as they taste great and are healthy!
The fruit was a colorful display that tanta-
lized our palette.

Although the prediction is not as good as the ref-
erence, it is actually coherent and relevant to the
theme “food and eating”, which showcases the de-
feats of using BLEU and CIDEr scores as a reward
for RL training.

Moreover, we compare the human evaluation
scores with these two metric scores in Figure 5.
Noticeably, both BLEU-3 and CIDEr have a poor
correlation with the human evaluation scores.
Their distributions are more biased and thus can-
not fully reﬂect the quality of the generated sto-
ries. In terms of BLEU, it is extremely hard for
machines to produce the exact 3-gram or 4-gram
matching, so the scores are too low to provide use-
ful guidance. CIDEr measures the similarity of a
sentence to the majority of the references. How-
ever, the references to the same image sequence
are photostream different from each other, so the
score is very low and not suitable for this task. In
contrast, our AREL framework can lean a more
robust reward function from human-annotated sto-
ries, which is able to provide better guidance to
the policy and thus improves its performances over
different metrics.

Visualization of The Learned Rewards
In Fig-
ure 6, we visualize the learned reward function for
both ground truth and generated stories. Evidently,
the AREL model is able to learn a smoother re-
ward function that can distinguish the generated
stories from human annotations. In other words,
the learned reward function is more in line with
human perception and thus can encourage the
model to explore more diverse language styles and
expressions.

Figure 5: Metric score distributions. We plot the histogram distributions of BLEU-3 and CIDEr scores
on the test set, as well as the human evaluation score distribution on the test samples. We use the Turing
test results to calculate the human evaluation scores (see Section 4.3). Basically, 0.2 score is given if the
generated story wins the Turing test, 0.1 for tie, and 0 if losing. Each sample has 5 scores from 5 judges,
and we use the sum as the human evaluation score, so it is in the range [0, 1].

Choice (%)
Relevance
Expressiveness
Concreteness

AREL vs XE-ss
AREL XE-ss Tie
61.7
13.2
66.1
15.1
63.9
15.8

25.1
18.8
20.3

AREL vs BLEU-RL
AREL BLEU-RL Tie
55.8
16.3
59.1
14.5
60.1
13.6

27.9
26.4
26.3

AREL vs CIDEr-RL
AREL CIDEr-RL Tie
56.1
15.7
59.1
14.3
59.5
15.9

28.2
26.6
24.6

AREL vs GAN
AREL GAN Tie
52.9
35.8 11.3
48.5
32.2 19.3
49.8
35.8 14.4

Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model
in generating more human-like stories than the SOTA methods.

is prone to the vanishing gradient issue. Analyti-
cally, our method does not suffer from these two
common issues and thus is able converge to op-
timum solutions more easily. From Table 1 we
can observe slight gains of using AREL over GAN
with automatic metrics, but we further deploy hu-
man evaluation for a better comparison.

4.3 Human Evaluation

Automatic metrics cannot fully evaluate the ca-
pability of our AREL method. Therefore, we
perform two different kinds of human evaluation
studies on Amazon Mechanical Turk: Turing test
and pairwise human evaluation. For both tasks,
we use 150 stories (750 images) sampled from the
test set, each assigned to 5 workers to eliminate
human variance. We batch six items as one assign-
ment and insert an additional assignment as a san-
ity check. Besides, the order of the options within
each item is shufﬂed to make a fair comparison.

Turing Test We ﬁrst conduct ﬁve indepen-
dent Turing tests for XE-ss, BLEU-RL, CIDEr-
RL, GAN, and AREL models, during which the
worker is given one human-annotated sample and
one machine-generated sample, and needs to de-
cide which is human-annotated. As shown in Ta-
ble 3, our AREL model signiﬁcantly outperforms
all the other baseline models in the Turing test: it
has much more chances to fool AMT worker (the

Figure 6: Visualization of the learned rewards on
both the ground-truth stories and the stories gen-
erated by our AREL model. The generated sto-
ries are receiving lower averaged scores than the
human-annotated ones.

Comparison with GAN We here compare our
method with vanilla GAN (Goodfellow et al.,
2014), whose update rules for the generator can
be generally classiﬁed into two categories. We
demonstrate their corresponding objectives and
ours as follows:

GAN 1 : Jβ = E

[− log Rθ(W )] ,

GAN 2 :

ours :

W ∼pβ

Jβ = E

W ∼pβ

Jβ = E

W ∼pβ

[log(1 − Rθ(W ))] ,

[−Rθ(W )] .

As discussed in Arjovsky et al. (2017), GAN 1 is
prone to the unstable gradient issue and GAN 2

Figure 7: Qualitative comparison example with XE-ss. The direct comparison votes (AREL:XE-ss:Tie)
were 5:0:0 on Relevance, 4:0:1 on Expressiveness, and 5:0:0 on Concreteness.

ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN
= 45.8%:28.3%:32.1%:19.7%:39.5%), which con-
ﬁrms the superiority of our AREL framework in
generating human-like stories. Unlike automatic
metric evaluation, the Turing test has indicated
a much larger margin between AREL and other
competing algorithms. Thus, we empirically con-
ﬁrm that metrics are not perfect in evaluating many
implicit semantic properties of natural language.
Besides, the Turing test of our AREL model re-
veals that nearly half of the workers are fooled by
our machine generation, indicating a preliminary
success toward generating human-like stories.

Pairwise Comparison In order to have a clear
comparison with competing algorithms with re-
spect to different semantic features of the sto-
ries, we further perform four pairwise compar-
ison tests: AREL vs XE-ss/BLEU-RL/CIDEr-
RL/GAN. For each photostream, the worker is
presented with two generated stories and asked to
make decisions from the three aspects: relevance5,
expressiveness6 and concreteness7. This head-to-
head compete is designed to help us understand in
what aspect our model outperforms the competing
algorithms, which is displayed in Table 4.

Consistently on all the three comparisons, a
large majority of the AREL stories trumps the
competing systems with respect to their relevance,

5Relevance: the story accurately describes what is hap-

pening in the image sequence and covers the main objects.

6Expressiveness: coherence, grammatically and semanti-

cally correct, no repetition, expressive language style.

7Concreteness: the story should narrate concretely what

is in the image rather than giving very general descriptions.

expressiveness, and concreteness. Therefore, it
empirically conﬁrms that our generated stories are
more relevant to the image sequences, more coher-
ent and concrete than the other algorithms, which
however is not explicitly reﬂected by the auto-
matic metric evaluation.

4.4 Qualitative Analysis

Figure 7 gives a qualitative comparison example
between AREL and XE-ss models. Looking at the
individual sentences, it is obvious that our results
are more grammatically and semantically correct.
Then connecting the sentences together, we ob-
serve that the AREL story is more coherent and
describes the photo stream more accurately. Thus,
our AREL model signiﬁcantly surpasses the XE-
ss model on all the three aspects of the qualitative
example. Besides, it won the Turing test (3 out 5
AMT workers think the AREL story is created by
a human). In the appendix, we also show a nega-
tive case that fails the Turing test.

5 Conclusion

In this paper, we not only introduce a novel ad-
versarial reward learning algorithm to generate
more human-like stories given image sequences,
but also empirically analyze the limitations of the
automatic metrics for story evaluation. We believe
there are still lots of improvement space in the
narrative paragraph generation tasks, like how to
better simulate human imagination to create more
vivid and diversiﬁed stories.

Acknowledgment

We thank Adobe Research for supporting our lan-
guage and vision research. We would also like
to thank Licheng Yu for clarifying the details
of his paper and the anonymous reviewers for
their thoughtful comments. This research was
sponsored in part by the Army Research Labora-
tory under cooperative agreements W911NF09-2-
0053. The views and conclusions contained herein
are those of the authors and should not be inter-
preted as representing the ofﬁcial policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References

Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international confer-
ence on Machine learning, page 1. ACM.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
arXiv preprint

tou. 2017. Wasserstein gan.
arXiv:1701.07875.

Wenhu Chen, Aur´elien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.

Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo,
Jing Dai, and Chang-Tien Lu. 2017. Multi-
modal storytelling via generative adversarial imita-
In Proceedings of the Twenty-Sixth
tion learning.
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-17, pages 3967–3973.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
In The IEEE Interna-
tions via a conditional gan.
tional Conference on Computer Vision (ICCV).

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. arXiv preprint
arXiv:1611.03852.

Justin Fu, Katie Luo, and Sergey Levine. 2017.
in-
arXiv preprint

rewards with adversarial

learning.

Learning robust
verse reinforcement
arXiv:1710.11248.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks. In Pro-
ceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 315–
323.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Elia Bruni and Raquel Fern´andez. 2017. Adversarial
evaluation for open-domain dialogue generation. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 284–288.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.

Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon,
David Meger, Joelle Pineau, and Doina Precup.
2017. Optiongan: Learning joint reward-policy op-
tions using generative adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1709.06683.

Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui
Zhang, Mu Li, and Ming Zhou. 2018. Generative
bridging network for neural sequence prediction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1706–1715. Associ-
ation for Computational Linguistics.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neural
Information Processing Systems, pages 4565–4573.

Ting-Hao K. Huang,

Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual

storytelling.
North American Chapter of
Computational Linguistics (NAACL 2016).

In 15th Annual Conference of the
the Association for

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,
and F Huang. 2006. A tutorial on energy-based
learning. Predicting structured data, 1(0).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. arXiv
preprint arXiv:1708.07149.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and
Tao Mei. 2018a. Show, reward and tell: Automatic
generation of narrative paragraph from photo stream
by adversarial training. AAAI.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video caption-
ing via hierarchical reinforcement learning. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xin Wang, Yuan-Fang Wang, and William Yang Wang.
2018c. Watch, listen, and describe: Globally and lo-
cally aligned cross-modal attentions for video cap-
tioning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 795–801.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
In Proceedings of the IEEE
video and language.
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in Neural Information Processing Sys-
tems, pages 73–81.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017a. Seqgan: Sequence generative adversarial
In AAAI, pages 2852–
nets with policy gradient.
2858.

Licheng Yu, Mohit Bansal, and Tamara Berg. 2017b.
Hierarchically-attentive rnn for album summariza-
In Proceedings of the 2017
tion and storytelling.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 966–971, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
In AAAI, volume 8, pages
reinforcement learning.
1433–1438. Chicago, IL, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning.
In
Proceedings of the 23rd international conference on
Machine learning, pages 729–736. ACM.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceeding of IEEE conference on Computer Vision and
Pattern Recognition (CVPR).

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
In The
sequence training for image captioning.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Appendix

A Error Analysis

Failure Case in Turing Test
In Figure 8, we
presented a negative example that failed the Turing
test (4 out of 5 made the correct decision). Com-
pared with the human-generated story, our AREL
story lacked emotion and imagination and thus can
be easily distinguished. For example, the real hu-
man gave the band a nickname “very loud band”
and told a more amusing story. Though we have
made encouraging progress on generating human-
like stories, further research of creating diversiﬁed
stories is still needed.

Data Bias From the experiments, we observe
that there exist some severe data bias issues in
the VIST dataset, such as gender bias and event
bias.
In the training set, the ratio of male and
female’s appearances is 2.06:1, and it is 2.16:1
in the test set.
the models aggravate the gender
bias to 3.44:1. Besides, because all the images
are collected from Flickr, there is also an event
bias issue. We count three most frequent events:
party, wedding, and graduation, whose ratios are
6.51:2.36:1 on the training set and 4.54:2.42:1 on
the test set. However, their ratio on the testing
results is 10.69:2.22:1. Clearly, the models tend
to magnify the inﬂuence of the largest majority.
These bias issues remain to be studied for future
work.

B Training Details

Our model is implemented on PyTorch and con-
sists of two parts – a policy model and a reward
model. The policy model is implemented with
a multiple-RNN architecture. Each RNN model
is responsible for generating a sub-story for each
photo in the stream. But the weights are tied
to minimize the memory consumption. The im-
age features are extracted from the pre-trained
ResNet-152 model8. The visual encoder receives
the ResNet-152 features and uses recurrent neu-
ral network to understand the temporal dynamics
and represents them as hidden state vectors, which
is further fed into the decoder to generate stories.
The reward model is based on convolutional neu-
ral network and uses convolution kernels to extract
semantic features for prediction. Here we give the
detailed description of our system:

8https://github.com/KaimingHe/

deep-residual-networks

• Visual Encoder:

the visual encoder is a bi-
directional GRU model with hidden dimen-
sion of 256 for each direction. we concate-
nate the bi-directional states and form a 512
dimension vector for the story generator. The
input album is composed of ﬁve images, and
each image is used as separate input to differ-
ent RNN decoders.

• Decoder: The decoder is a single-layer GRU
model with hidden dimension of 512. The
recurrent decoder model receives the output
from the visual encoder as the ﬁrst input, and
then at the following time steps, it receives
the last predicted token as input or uses the
ground truth as input. During scheduled sam-
pling, we use a sampling probability to de-
cide which action to take.

• Reward Model: we use a convolutional neu-
ral network to extract n-gram features from
the story embedding and stretch them into a
ﬂattened vector. The embedding size of input
story is 128, and the ﬁlter dimension of CNN
is also 128. Here we use three kernels with
window size 2, 3, 4, each with a stride size
of 1. We use a pooling size of 2 to shrink the
extracted outputs and ﬂatten them as a vector.
Finally, we project this vector into a single
cell indicating the predicted reward value.

During training, we ﬁrst pre-train a schedule-
sampling model with a batch size of 64 with
NVIDIA Titan X GPU. The warm-up process
takes roughly 5-10 hours, and then we select the
best model to initialize our AREL policy model.
Finally, we use alternating training strategy to op-
timize both the policy model and the reward model
with a learning rate of 2e-4 using Adam optimiza-
tion algorithm. During test time, we use a beam
size of 3 to approximate the whole search space,
we force the beam search to proceed more than 5
steps and no more than 110 steps. Once we reach
the EOS token, the algorithm stops and we com-
pare the results with human-annotated corpus us-
ing 4 different automatic evaluation metrics.

C Amazon Mechanical Turk

We used AMT to perform two surveys, one picks
a more human-like story. We asked the worker
to answers 8 questions within 30 minutes, and
we pay 5 workers to work on the same sheet to

Figure 8: Failure case in Turing test. 4 out of 5 workers correctly recognized the human-created story
and 1 person mistakenly chose AREL story.

eliminate human-to-human bias. Here we demon-
strate the Turing survey form in Figure 9. Besides,
we also perform a head-to-head comparison with
other algorithms, we demonstrate the survey form
in Figure 10.

Figure 9: Turing Survey Form

Figure 10: Pairwise Comparison Form

No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling

Xin Wang∗, Wenhu Chen∗, Yuan-Fang Wang , William Yang Wang
University of California, Santa Barbara
{xwang,wenhuchen,yfwang,william}@cs.ucsb.edu

8
1
0
2
 
l
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
0
6
1
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Though impressive results have been
achieved in visual captioning,
the task
of generating abstract stories from photo
streams is still a little-tapped problem.
Different from captions, stories have more
expressive language styles and contain
many imaginary concepts that do not ap-
pear in the images. Thus it poses chal-
lenges to behavioral cloning algorithms.
Furthermore, due to the limitations of au-
tomatic metrics on evaluating story qual-
ity, reinforcement learning methods with
hand-crafted rewards also face difﬁcul-
ties in gaining an overall performance
boost. Therefore, we propose an Adver-
sarial REward Learning (AREL) frame-
work to learn an implicit reward function
from human demonstrations, and then op-
timize policy search with the learned re-
ward function. Though automatic eval-
uation indicates slight performance boost
over state-of-the-art (SOTA) methods in
cloning expert behaviors, human evalua-
tion shows that our approach achieves sig-
niﬁcant improvement in generating more
human-like stories than SOTA systems.1

1

Introduction

Recently, increasing attention has been focused
on visual captioning (Chen et al., 2015, 2016; Xu
et al., 2016; Wang et al., 2018c), which aims at
describing the content of an image or a video.
Though it has achieved impressive results, its ca-
pability of performing human-like understanding
is still restrictive. To further investigate machine’s

∗ Equal contribution
1Code
released

is

littlekobe/AREL

at https://github.com/

Figure 1: An example of visual storytelling and
visual captioning. Both captions and stories are
shown here: each image is captioned with one sen-
tence, and we also demonstrate two diversiﬁed sto-
ries that match the same image sequence.

capabilities in understanding more complicated vi-
sual scenarios and composing more structured ex-
pressions, visual storytelling (Huang et al., 2016)
has been proposed. Visual captioning is aimed at
depicting the concrete content of the images, and
its expression style is rather simple. In contrast,
visual storytelling goes one step further: it sum-
marizes the idea of a photo stream and tells a story
about it. Figure 1 shows an example of visual
captioning and visual storytelling. We have ob-
served that stories contain rich emotions (excited,
happy, not want) and imagination (siblings, par-
ents, school, car). It, therefore, requires the capa-
bility to associate with concepts that do not explic-
itly appear in the images. Moreover, stories are
more subjective, so there barely exists standard

templates for storytelling. As shown in Figure 1,
the same photo stream can be paired with diverse
stories, different from each other. This heavily in-
creases the evaluation difﬁculty.

So far, prior work for visual storytelling (Huang
et al., 2016; Yu et al., 2017b) is mainly inspired
by the success of visual captioning. Nevertheless,
because these methods are trained by maximizing
the likelihood of the observed data pairs, they are
restricted to generate simple and plain description
with limited expressive patterns. In order to cope
with the challenges and produce more human-like
descriptions, Rennie et al. (2017) have proposed
a reinforcement learning framework. However, in
the scenario of visual storytelling, the common re-
inforced captioning methods are facing great chal-
lenges since the hand-crafted rewards based on
string matches are either too biased or too sparse
to drive the policy search. For instance, we used
the METEOR (Banerjee and Lavie, 2005) score
as the reward to reinforce our policy and found
that though the METEOR score is signiﬁcantly
improved, the other scores are severely harmed.
Here we showcase an adversarial example with an
average METEOR score as high as 40.2:

We had a great time to have a lot of the.
They were to be a of the. They were to be in
the. The and it were to be the. The, and it
were to be the.

Apparently, the machine is gaming the metrics.
Conversely, when using some other metrics (e.g.
BLEU, CIDEr) to evaluate the stories, we observe
an opposite behavior: many relevant and coherent
stories are receiving a very low score (nearly zero).
In order to resolve the strong bias brought by
the hand-coded evaluation metrics in RL training
and produce more human-like stories, we propose
an Adversarial REward Learning (AREL) frame-
work for visual storytelling. We draw our inspi-
ration from recent progress in inverse reinforce-
ment learning (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017) and propose the AREL algo-
rithm to learn a more intelligent reward function.
Speciﬁcally, we ﬁrst incorporate a Boltzmann dis-
tribution to associate reward learning with distri-
bution approximation, then design the adversarial
process with two models – a policy model and a
reward model. The policy model performs the
primitive actions and produces the story sequence,
while the reward model is responsible for learning

the implicit reward function from human demon-
strations. The learned reward function would be
employed to optimize the policy in return.

For evaluation, we conduct both automatic met-
rics and human evaluation but observe a poor cor-
relation between them. Particularly, our method
gains slight performance boost over the base-
line systems on automatic metrics; human evalu-
ation, however, indicates signiﬁcant performance
boost. Thus we further discuss the limitations
of the metrics and validate the superiority of our
AREL method in performing more intelligent un-
derstanding of the visual scenes and generating
more human-like stories.

Our main contributions are four-fold:

• We propose an adversarial reward learning
framework and apply it to boost visual story
generation.

• We evaluate our approach on the Visual
Storytelling (VIST) dataset and achieve the
state-of-the-art results on automatic metrics.

• We empirically demonstrate that automatic
metrics are not perfect for either training or
evaluation.

• We design and perform a comprehensive
human evaluation via Amazon Mechanical
Turk, which demonstrates the superiority of
the generated stories of our method on rele-
vance, expressiveness, and concreteness.

2 Related Work

Visual Storytelling Visual storytelling is the
task of generating a narrative story from a photo
stream, which requires a deeper understanding
of the event ﬂow in the stream. Park and Kim
(2015) has done some pioneering research on sto-
rytelling. Chen et al. (2017) proposed a multi-
modal approach for storyline generation to pro-
duce a stream of entities instead of human-like de-
scriptions. Recently, a more sophisticated dataset
for visual storytelling (VIST) has been released
to explore a more human-like understanding of
grounded stories (Huang et al., 2016). Yu et al.
(2017b) proposes a multi-task learning algorithm
for both album summarization and paragraph gen-
eration, achieving the best results on the VIST
dataset. But these methods are still based on be-
havioral cloning and lack the ability to generate
more structured stories.

Reinforcement Learning in Sequence Genera-
tion Recently, reinforcement learning (RL) has
gained its popularity in many sequence generation
tasks such as machine translation (Bahdanau et al.,
2016), visual captioning (Ren et al., 2017; Wang
et al., 2018b), summarization (Paulus et al., 2017;
Chen et al., 2018), etc. The common wisdom of
using RL is to view generating a word as an ac-
tion and aim at maximizing the expected return
by optimizing its policy. As pointed in (Ranzato
et al., 2015), traditional maximum likelihood al-
gorithm is prone to exposure bias and label bias,
while the RL agent exposes the generative model
to its own distribution and thus can perform bet-
ter. But these works usually utilize hand-crafted
metric scores as the reward to optimize the model,
which fails to learn more implicit semantics due to
the limitations of automatic metrics.

Rethinking Automatic Metrics Automatic
metrics, including BLEU (Papineni et al., 2002),
CIDEr (Vedantam et al., 2015), METEOR (Baner-
jee and Lavie, 2005), and ROUGE (Lin, 2004),
have been widely applied to the sequence gener-
ation tasks. Using automatic metrics can ensure
rapid prototyping and testing new models with
fewer expensive human evaluation. However, they
have been criticized to be biased and correlate
poorly with human judgments, especially in many
generative tasks like response generation (Lowe
et al., 2017; Liu et al., 2016), dialogue sys-
tem (Bruni and Fern´andez, 2017) and machine
translation (Callison-Burch et al., 2006). The
naive overlap-counting methods are not able
to reﬂect many semantic properties in natural
language, such as coherence, expressiveness, etc.

Generative Adversarial Network Generative
adversarial network (GAN) (Goodfellow et al.,
2014) is a very popular approach for estimating
intractable probabilities, which sidestep the difﬁ-
culty by alternately training two models to play a
min-max two-player game:

min
D

max
G

E
x∼pdata

[log D(x)] + E
z∼pz

[log D(G(z))] ,

where G is the generator and D is the discrimina-
tor, and z is the latent variable. Recently, GAN
has quickly been adopted to tackle discrete prob-
lems (Yu et al., 2017a; Dai et al., 2017; Wang et al.,
2018a). The basic idea is to use Monte Carlo pol-
icy gradient estimation (Williams, 1992) to update
the parameters of the generator.

Figure 2: AREL framework for visual storytelling.

Inverse Reinforcement Learning Reinforce-
ment learning is known to be hindered by the
need for an extensive feature and reward engi-
neering, especially under the unknown dynamics.
Therefore, inverse reinforcement learning (IRL)
has been proposed to infer expert’s reward func-
tion. Previous IRL approaches include maximum
margin approaches (Abbeel and Ng, 2004; Ratliff
et al., 2006) and probabilistic approaches (Ziebart,
2010; Ziebart et al., 2008). Recently, adversarial
inverse reinforcement learning methods provide
an efﬁcient and scalable promise for automatic re-
ward acquisition (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017; Henderson et al., 2017).
These approaches utilize the connection between
IRL and energy-based model and associate every
data with a scalar energy value by using Boltz-
mann distribution pθ(x) ∝ exp(−Eθ(x)).
In-
spired by these methods, we propose a practical
AREL approach for visual storytelling to uncover
a robust reward function from human demonstra-
tions and thus help produce human-like stories.

3 Our Approach

3.1 Problem Statement

Here we consider the task of visual storytelling,
whose objective is to output a word sequence W =
(w1, w1, · · · , wT ), wt ∈ V given an input image
stream of 5 ordered images I = (I1, I2, · · · , I5),
where V is the vocabulary of all output token.
We formulate the generation as a markov deci-
sion process and design a reinforcement learning
framework to tackle it. As described in Figure 2,
our AREL framework is mainly composed of two
modules: a policy model πβ(W ) and a reward
model Rθ(W ). The policy model takes an image
sequence I as the input and performs sequential
actions (choosing words w from the vocabulary V)
to form a narrative story W . The reward model

Figure 3: Overview of the policy model. The vi-
sual encoder is a bidirectional GRU, which en-
codes the high-level visual features extracted from
the input images. Its outputs are then fed into the
RNN decoders to generate sentences in parallel.
Finally, we concatenate all the generated sentences
as a full story. Note that the ﬁve decoders share the
same weights.

is optimized by the adversarial objective (see Sec-
tion 3.3) and aims at deriving a human-like reward
from both human-annotated stories and sampled
predictions.

3.2 Model

Policy Model As is shown in Figure 3, the pol-
icy model is a CNN-RNN architecture. We ﬁst
feed the photo stream I = (I1, · · · , I5) into a
pretrained CNN and extract their high-level image
features. We then employ a visual encoder to fur-
ther encode the image features as context vectors
−→
hi = [
hi ]. The visual encoder is a bidirectional
gated recurrent units (GRU).

←−
hi ;

In the decoding stage, we feed each context vec-
tor hi into a GRU-RNN decoder to generate a sub-
story Wi. Formally, the generation process can be
written as:

t = GRU(si
t−1, [wi
si
1:t−1) = sof tmax(Wssi

t−1, hi]) ,
t + bs) ,

(1)

(2)

πβ(wi

t|wi

where si
t denotes the t-th hidden state of i-th de-
coder. We concatenate the previous token wi
t−1
and the context vector hi as the input. Ws and
bs are the projection matrix and bias, which out-
put a probability distribution over the whole vo-
cabulary V. Eventually, the ﬁnal story W is the
concatenation of the sub-stories Wi. β denotes all
the parameters of the encoder, the decoder, and the
output layer.

Figure 4: Overview of the reward model. Our re-
ward model is a CNN-based architecture, which
utilizes convolution kernels with size 2, 3 and 4
to extract bigram, trigram and 4-gram representa-
tions from the input sequence embeddings. Once
the sentence representation is learned, it will be
concatenated with the visual representation of the
input image, and then be fed into the ﬁnal FC layer
to obtain the reward.

Reward Model The reward model Rθ(W ) is a
CNN-based architecture (see Figure 4). Instead of
giving an overall score for the whole story, we ap-
ply the reward model to different story parts (sub-
stories) Wi and compute partial rewards, where
i = 1, · · · , 5. We observe that the partial rewards
are more ﬁne-grained and can provide better guid-
ance for the policy model.

We ﬁrst query the word embeddings of the sub-
story (one sentence in most cases). Next, multi-
ple convolutional layers with different kernel sizes
are used to extract the n-grams features, which
are then projected into the sentence-level repre-
sentation space by pooling layers (the design here
In addition to the
is inspired by Kim (2014)).
textual features, evaluating the quality of a story
should also consider the image features for rele-
vance. Therefore, we then combine the sentence
representation with the visual feature of the input
image through concatenation and feed them into
the ﬁnal fully connected decision layer.
In the
end, the reward model outputs an estimated reward
value Rθ(W ). The process can be written in for-
mula:

Rθ(W ) = φ(Wr(fconv(W ) + WiICN N ) + br),

(3)

where φ denotes the non-linear projection func-
tion, Wr, br denote the weight and bias in the
output layer, and fconv denotes the operations in
CNN. ICN N is the high-level visual feature ex-
tracted from the image, and Wi projects it into the

sentence representation space. θ includes all the
parameters above.

3.3 Learning

Reward Boltzmann Distribution In order to
associate story distribution with reward function,
we apply EBM to deﬁne a Reward Boltzmann dis-
tribution:

Algorithm 1 The AREL Algorithm.

1: for episode ← 1 to N do
2:

collect story W by executing policy πθ
if Train-Reward then
θ ← θ − η × ∂Jθ
else if Train-Policy then

∂θ (see Equation 9)

collect story ˜W from empirical pe
β ← β − η × ∂Jβ
∂β (see Equation 9)

3:

4:

5:

6:

7:

pθ(W ) =

exp(Rθ(W ))
Zθ

,

(4)

end if

8:
9: end for

Where W is the word sequence of the story and
pθ(W ) is the approximate data distribution, and
Zθ = (cid:80)
exp(Rθ(W )) denotes the partition func-
W
tion. According to the energy-based model (Le-
Cun et al., 2006), the optimal reward function
R∗(W ) is achieved when the Reward-Boltzmann
distribution equals to the “real” data distribution
pθ(W ) = p∗(W ).

Adversarial Reward Learning We ﬁrst intro-
1(W ∈D)
duce an empirical distribution pe(W ) =
|D|
to represent the empirical distribution of the train-
ing data, where D denotes the dataset with |D| sto-
ries and 1 denotes an indicator function. We use
this empirical distribution as the “good” examples,
which provides the evidence for the reward func-
tion to learn from.

In order to approximate the Reward Boltzmann
distribution towards the “real” data distribution
p∗(W ), we design a min-max two-player game,
where the Reward Boltzmann distribution pθ aims
at maximizing the its similarity with empirical
distribution pe while minimizing that with the
“faked” data generated from policy model πβ. On
the contrary, the policy distribution πβ tries to
maximize its similarity with the Boltzmann dis-
tribution pθ. Formally, the adversarial objective
function is deﬁned as
max
β

KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W )) .

min
θ

We further decompose it into two parts. First,
because the objective Jβ of the story genera-
tion policy is to maximize its similarity with
the Boltzmann distribution pθ, the optimal policy
that minimizes KL-divergence is thus π(W ) ∼
exp(Rθ(W )), meaning if Rθ is optimal, the op-
timal πβ = π∗. In formula,

Jβ = − KL(πβ(W )||pθ(W ))

=

E
W ∼πβ (W )

[Rθ(W )] − log Zθ + H(πβ(W )) ,

(6)

where H denotes the entropy of the policy model.
On the other hand, the objective Jθ of the re-
ward function is to distinguish between human-
annotated stories and machine-generated stories.
Hence it is trying to minimize the KL-divergence
with the empirical distribution pe and maximize
the KL-divergence with the approximated policy
distribution πβ:

Jθ =KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W ))

(cid:88)

=

[pe(W )Rθ(W ) − πβ(W )Rθ(W )]

W
+ log Zθ − log Zθ − H(pe) + H(πβ) ,

(7)

Since H(πβ) and H(pe) are irrelevant to θ, we
denote them as constant C. It is also worth not-
ing that with negative sampling in the optimization
of the KL-divergence, the computation of the in-
tractable partition function Zθ is bypassed. There-
fore, the objective Jθ can be further derived as

Jθ =

E
W ∼pe(W )

[Rθ(W )] −

E
W ∼πβ (W )

[Rθ(W )] + C . (8)

Here we propose to use stochastic gradient de-
scent to optimize these two models alternately.
Formally, the gradients can be written as

(5)

∂Jθ
∂θ

∂Jβ
∂β

=

E
[
W ∼pe(W )

∂Rθ(W )
∂θ

] −

E
[
W ∼πβ (W )

∂Rθ(W )
∂θ

] ,

=

E
W ∼πβ (W )

(Rθ(W ) − log πβ(W ) − b)

∂ log πβ(W )
∂β

,

(9)

where b is the estimated baseline to reduce vari-
ance during REINFORCE training.

Training & Testing As described in Algo-
rithm 1, we introduce an alternating algorithm to
train these two models using stochastic gradient
descent. During testing, the policy model is used
with beam search to produce the story.

4 Experiments and Analysis

4.1 Experimental Setup

VIST Dataset The VIST dataset (Huang et al.,
2016) is the ﬁrst dataset for sequential vision-to-
language tasks including visual storytelling, which
consists of 10,117 Flickr albums with 210,819
unique photos.
In this paper, we mainly evalu-
ate our AREL method on this dataset. After ﬁlter-
ing the broken images2, there are 40,098 training,
4,988 validation, and 5,050 testing samples. Each
sample contains one story that describes 5 selected
images from a photo album (mostly one sentence
per image). And the same album is paired with 5
different stories as references. In our experiments,
we used the same split settings as in (Huang et al.,
2016; Yu et al., 2017b) for a fair comparison. Dur-
ing our experiments, we apply two kinds of non-
linear functions φ for the discriminator, namely
SoftSign function (f (x) = x
1+|x| ) and Hyper-
bolic function (f (x) = sinhx
coshx ). We found that
unbounded non-linear functions like ReLU func-
tion (Glorot et al., 2011) will lead to severe vibra-
tions and instabilities during training, therefore we
resort to the bounded functions.

Evaluation Metrics
In order to comprehen-
sively evaluate our method on storytelling dataset,
we adopt both the automatic metrics and human
evaluation as our criterion. Four diverse automatic
metrics are used in our experiments: BLEU, ME-
TEOR, ROUGE-L, and CIDEr. We utilize the
open source evaluation code3 used in (Yu et al.,
2017b). For human evaluation, we employ the
Amazon Mechanical Turk to perform two kinds of
user studies (see Section 4.3 for more details).

employ

Training Details We
pretrained
ResNet-152 model (He et al., 2016) to extract
image features from the photostream. We built a
vocabulary of size 9,837 to include words appear-
ing more than three times in the training set. More
training details can be found at Appendix B.

4.2 Automatic Evaluation

In this section, we compare our AREL method
with the state-of-the-art methods as well as stan-
dard reinforcement learning algorithms on auto-

2There are only 3 (out of 21,075) broken images in the
test set, which basically has no inﬂuence on the ﬁnal results.
Moreover, Yu et al. (2017b) also removed the 3 pictures, so it
is a fair comparison.

3https://github.com/lichengunc/vist_eval

Method

B-1 B-2 B-3 B-4 M R

C

Huang et al.
Yu et al.
XE-ss
GAN
AREL-s-50
AREL-t-50
AREL-s-100
AREL-t-100

-
-

-
-

-
21.0

-
-
31.4
-
34.1 29.5 7.5
-
62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.8 38.8 23.0 14.0 35.0 29.5 9.0
62.9 38.4 22.7 14.0 34.9 29.4 9.1
63.4 39.0 23.1 14.1 35.2 29.6 9.5
64.0 38.6 22.3 13.2 35.1 29.3 9.6
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 1: Automatic evaluation on the VIST
dataset. We report BLEU (B), METEOR (M),
ROUGH-L (R), and CIDEr (C) scores of the
SOTA systems and the models we implemented,
including XE-ss, GAN and AREL. AREL-s-N de-
notes AREL models with SoftSign as output acti-
vation and alternate frequency as N, while AREL-
t-N denoting AREL models with Hyperbolic as the
output activation (N = 50 or 100).

matic evaluation metrics. Then we further discuss
the limitations of the hand-crafted metrics on eval-
uating human-like stories.

Comparison with SOTA on Automatic Metrics
In Table 1, we compare our method with Huang
et al. (2016) and Yu et al. (2017b), which report
achieving best-known results on the VIST dataset.
We ﬁrst implement a strong baseline model (XE-
ss), which share the same architecture with our
policy model but is trained with cross-entropy loss
and scheduled sampling. Besides, we adopt the
traditional generative adversarial training for com-
parison (GAN). As shown in Table 1, our XE-
ss model already outperforms the best-known re-
sults on the VIST dataset, and the GAN model can
bring a performance boost. We then use the XE-
ss model to initialize our policy model and further
train it with AREL. Evidently, our AREL model
performs the best and achieves the new state-of-
the-art results across all metrics.

But, compared with the XE-ss model, the per-
formance gain is minor, especially on METEOR
and ROUGE-L scores. However, in Sec. 4.3, the
extensive human evaluation has indicated that our
AREL framework brings a signiﬁcant improve-
ment on generating human-like stories over the
XE-ss model. The inconsistency of automatic
evaluation and human evaluation lead to a suspect
that these hand-crafted metrics lack the ability to
fully evaluate stories’ quality due to the compli-
cated characteristics of the stories. Therefore, we
conduct experiments to analyze and discuss the

Method

B-1 B-2 B-3 B-4 M R

C

XE-ss
BLEU-RL
METEOR-RL
ROUGE-RL
CIDEr-RL
AREL (best)

62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.1 38.0 22.6 13.9 34.6 29.0 8.9
40.2 30.0 1.2
68.1 35.0 15.4
6.8
27.0 33.8
58.1 18.5
0
0
1.6
61.9 37.8 22.5 13.8 34.9 29.7 8.1
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 2: Comparison with different RL mod-
els with different metric scores as the rewards.
We report the average scores of the AREL mod-
els as AREL (avg). Although METEOR-RL and
ROUGE-RL models achieve the highest scores
on their own metrics, the underlined scores are
severely damaged. Actually, they are gaming their
own metrics with nonsense sentences.

defects of the automatic metrics in section 4.2.

Limitations of Automatic Metrics String-
match-based automatic metrics are not perfect
and fail to evaluate some semantic characteristics
of the stories (e.g. expressiveness and coherence).
In order to conﬁrm our conjecture, we utilize
automatic metrics as rewards to reinforce the
model with policy gradient.
The quantitative
results are demonstrated in Table 1.

Apparently, METEOR-RL and ROUGE-RL are
severely ill-posed: they obtain the highest scores
on their own metrics but damage the other met-
rics severely. We observe that these models are
actually overﬁtting to a given metric while losing
the overall coherence and semantical correctness.
Same as METEOR score, there is also an adver-
sarial example for ROUGE-L4, which is nonsense
but achieves an average ROUGE-L score of 33.8.
Besides, as can be seen in Table 1, after rein-
forced training, BLEU-RL and CIDEr-RL do not
bring a consistent improvement over the XE-ss
model. We plot the histogram distributions of both
BLEU-3 and CIDEr scores on the test set in Fig-
ure 5. An interesting fact is that there are a large
number of samples with nearly zero score on both
metrics. However, we observed those “zero-score”
samples are not pointless results; instead, lots of
them make sense and deserve a better score than
zero. Here is a “zero-score” example on BLEU-3:

I had a great time at the restaurant today.
The food was delicious. I had a lot of food.
The food was delicious. I had a great time.

4An adversarial example for ROUGE-L: we the was a .
and to the . we the was a . and to the . we the was a . and to
the . we the was a . and to the . we the was a . and to the .

Method
XE-ss
BLEU-RL
CIDEr-RL
GAN
AREL

Win
Lose
22.4% 71.7%
23.4% 67.9%
13.8% 80.3%
34.3% 60.5%
38.4% 54.2%

Unsure
5.9%
8.7%
5.9%
5.2%
7.4%

Table 3: Turing test results.

The corresponding reference is

The table of food was a pleasure to see!
Our food is both nutritious and beautiful!
Our chicken was especially tasty! We love
greens as they taste great and are healthy!
The fruit was a colorful display that tanta-
lized our palette.

Although the prediction is not as good as the ref-
erence, it is actually coherent and relevant to the
theme “food and eating”, which showcases the de-
feats of using BLEU and CIDEr scores as a reward
for RL training.

Moreover, we compare the human evaluation
scores with these two metric scores in Figure 5.
Noticeably, both BLEU-3 and CIDEr have a poor
correlation with the human evaluation scores.
Their distributions are more biased and thus can-
not fully reﬂect the quality of the generated sto-
ries. In terms of BLEU, it is extremely hard for
machines to produce the exact 3-gram or 4-gram
matching, so the scores are too low to provide use-
ful guidance. CIDEr measures the similarity of a
sentence to the majority of the references. How-
ever, the references to the same image sequence
are photostream different from each other, so the
score is very low and not suitable for this task. In
contrast, our AREL framework can lean a more
robust reward function from human-annotated sto-
ries, which is able to provide better guidance to
the policy and thus improves its performances over
different metrics.

Visualization of The Learned Rewards
In Fig-
ure 6, we visualize the learned reward function for
both ground truth and generated stories. Evidently,
the AREL model is able to learn a smoother re-
ward function that can distinguish the generated
stories from human annotations. In other words,
the learned reward function is more in line with
human perception and thus can encourage the
model to explore more diverse language styles and
expressions.

Figure 5: Metric score distributions. We plot the histogram distributions of BLEU-3 and CIDEr scores
on the test set, as well as the human evaluation score distribution on the test samples. We use the Turing
test results to calculate the human evaluation scores (see Section 4.3). Basically, 0.2 score is given if the
generated story wins the Turing test, 0.1 for tie, and 0 if losing. Each sample has 5 scores from 5 judges,
and we use the sum as the human evaluation score, so it is in the range [0, 1].

Choice (%)
Relevance
Expressiveness
Concreteness

AREL vs XE-ss
AREL XE-ss Tie
61.7
13.2
66.1
15.1
63.9
15.8

25.1
18.8
20.3

AREL vs BLEU-RL
AREL BLEU-RL Tie
55.8
16.3
59.1
14.5
60.1
13.6

27.9
26.4
26.3

AREL vs CIDEr-RL
AREL CIDEr-RL Tie
56.1
15.7
59.1
14.3
59.5
15.9

28.2
26.6
24.6

AREL vs GAN
AREL GAN Tie
52.9
35.8 11.3
48.5
32.2 19.3
49.8
35.8 14.4

Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model
in generating more human-like stories than the SOTA methods.

is prone to the vanishing gradient issue. Analyti-
cally, our method does not suffer from these two
common issues and thus is able converge to op-
timum solutions more easily. From Table 1 we
can observe slight gains of using AREL over GAN
with automatic metrics, but we further deploy hu-
man evaluation for a better comparison.

4.3 Human Evaluation

Automatic metrics cannot fully evaluate the ca-
pability of our AREL method. Therefore, we
perform two different kinds of human evaluation
studies on Amazon Mechanical Turk: Turing test
and pairwise human evaluation. For both tasks,
we use 150 stories (750 images) sampled from the
test set, each assigned to 5 workers to eliminate
human variance. We batch six items as one assign-
ment and insert an additional assignment as a san-
ity check. Besides, the order of the options within
each item is shufﬂed to make a fair comparison.

Turing Test We ﬁrst conduct ﬁve indepen-
dent Turing tests for XE-ss, BLEU-RL, CIDEr-
RL, GAN, and AREL models, during which the
worker is given one human-annotated sample and
one machine-generated sample, and needs to de-
cide which is human-annotated. As shown in Ta-
ble 3, our AREL model signiﬁcantly outperforms
all the other baseline models in the Turing test: it
has much more chances to fool AMT worker (the

Figure 6: Visualization of the learned rewards on
both the ground-truth stories and the stories gen-
erated by our AREL model. The generated sto-
ries are receiving lower averaged scores than the
human-annotated ones.

Comparison with GAN We here compare our
method with vanilla GAN (Goodfellow et al.,
2014), whose update rules for the generator can
be generally classiﬁed into two categories. We
demonstrate their corresponding objectives and
ours as follows:

GAN 1 : Jβ = E

[− log Rθ(W )] ,

GAN 2 :

ours :

W ∼pβ

Jβ = E

W ∼pβ

Jβ = E

W ∼pβ

[log(1 − Rθ(W ))] ,

[−Rθ(W )] .

As discussed in Arjovsky et al. (2017), GAN 1 is
prone to the unstable gradient issue and GAN 2

Figure 7: Qualitative comparison example with XE-ss. The direct comparison votes (AREL:XE-ss:Tie)
were 5:0:0 on Relevance, 4:0:1 on Expressiveness, and 5:0:0 on Concreteness.

ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN
= 45.8%:28.3%:32.1%:19.7%:39.5%), which con-
ﬁrms the superiority of our AREL framework in
generating human-like stories. Unlike automatic
metric evaluation, the Turing test has indicated
a much larger margin between AREL and other
competing algorithms. Thus, we empirically con-
ﬁrm that metrics are not perfect in evaluating many
implicit semantic properties of natural language.
Besides, the Turing test of our AREL model re-
veals that nearly half of the workers are fooled by
our machine generation, indicating a preliminary
success toward generating human-like stories.

Pairwise Comparison In order to have a clear
comparison with competing algorithms with re-
spect to different semantic features of the sto-
ries, we further perform four pairwise compar-
ison tests: AREL vs XE-ss/BLEU-RL/CIDEr-
RL/GAN. For each photostream, the worker is
presented with two generated stories and asked to
make decisions from the three aspects: relevance5,
expressiveness6 and concreteness7. This head-to-
head compete is designed to help us understand in
what aspect our model outperforms the competing
algorithms, which is displayed in Table 4.

Consistently on all the three comparisons, a
large majority of the AREL stories trumps the
competing systems with respect to their relevance,

5Relevance: the story accurately describes what is hap-

pening in the image sequence and covers the main objects.

6Expressiveness: coherence, grammatically and semanti-

cally correct, no repetition, expressive language style.

7Concreteness: the story should narrate concretely what

is in the image rather than giving very general descriptions.

expressiveness, and concreteness. Therefore, it
empirically conﬁrms that our generated stories are
more relevant to the image sequences, more coher-
ent and concrete than the other algorithms, which
however is not explicitly reﬂected by the auto-
matic metric evaluation.

4.4 Qualitative Analysis

Figure 7 gives a qualitative comparison example
between AREL and XE-ss models. Looking at the
individual sentences, it is obvious that our results
are more grammatically and semantically correct.
Then connecting the sentences together, we ob-
serve that the AREL story is more coherent and
describes the photo stream more accurately. Thus,
our AREL model signiﬁcantly surpasses the XE-
ss model on all the three aspects of the qualitative
example. Besides, it won the Turing test (3 out 5
AMT workers think the AREL story is created by
a human). In the appendix, we also show a nega-
tive case that fails the Turing test.

5 Conclusion

In this paper, we not only introduce a novel ad-
versarial reward learning algorithm to generate
more human-like stories given image sequences,
but also empirically analyze the limitations of the
automatic metrics for story evaluation. We believe
there are still lots of improvement space in the
narrative paragraph generation tasks, like how to
better simulate human imagination to create more
vivid and diversiﬁed stories.

Acknowledgment

We thank Adobe Research for supporting our lan-
guage and vision research. We would also like
to thank Licheng Yu for clarifying the details
of his paper and the anonymous reviewers for
their thoughtful comments. This research was
sponsored in part by the Army Research Labora-
tory under cooperative agreements W911NF09-2-
0053. The views and conclusions contained herein
are those of the authors and should not be inter-
preted as representing the ofﬁcial policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References

Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international confer-
ence on Machine learning, page 1. ACM.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
arXiv preprint

tou. 2017. Wasserstein gan.
arXiv:1701.07875.

Wenhu Chen, Aur´elien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.

Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo,
Jing Dai, and Chang-Tien Lu. 2017. Multi-
modal storytelling via generative adversarial imita-
In Proceedings of the Twenty-Sixth
tion learning.
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-17, pages 3967–3973.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
In The IEEE Interna-
tions via a conditional gan.
tional Conference on Computer Vision (ICCV).

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. arXiv preprint
arXiv:1611.03852.

Justin Fu, Katie Luo, and Sergey Levine. 2017.
in-
arXiv preprint

rewards with adversarial

learning.

Learning robust
verse reinforcement
arXiv:1710.11248.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks. In Pro-
ceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 315–
323.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Elia Bruni and Raquel Fern´andez. 2017. Adversarial
evaluation for open-domain dialogue generation. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 284–288.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.

Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon,
David Meger, Joelle Pineau, and Doina Precup.
2017. Optiongan: Learning joint reward-policy op-
tions using generative adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1709.06683.

Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui
Zhang, Mu Li, and Ming Zhou. 2018. Generative
bridging network for neural sequence prediction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1706–1715. Associ-
ation for Computational Linguistics.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neural
Information Processing Systems, pages 4565–4573.

Ting-Hao K. Huang,

Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual

storytelling.
North American Chapter of
Computational Linguistics (NAACL 2016).

In 15th Annual Conference of the
the Association for

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,
and F Huang. 2006. A tutorial on energy-based
learning. Predicting structured data, 1(0).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. arXiv
preprint arXiv:1708.07149.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and
Tao Mei. 2018a. Show, reward and tell: Automatic
generation of narrative paragraph from photo stream
by adversarial training. AAAI.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video caption-
ing via hierarchical reinforcement learning. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xin Wang, Yuan-Fang Wang, and William Yang Wang.
2018c. Watch, listen, and describe: Globally and lo-
cally aligned cross-modal attentions for video cap-
tioning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 795–801.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
In Proceedings of the IEEE
video and language.
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in Neural Information Processing Sys-
tems, pages 73–81.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017a. Seqgan: Sequence generative adversarial
In AAAI, pages 2852–
nets with policy gradient.
2858.

Licheng Yu, Mohit Bansal, and Tamara Berg. 2017b.
Hierarchically-attentive rnn for album summariza-
In Proceedings of the 2017
tion and storytelling.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 966–971, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
In AAAI, volume 8, pages
reinforcement learning.
1433–1438. Chicago, IL, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning.
In
Proceedings of the 23rd international conference on
Machine learning, pages 729–736. ACM.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceeding of IEEE conference on Computer Vision and
Pattern Recognition (CVPR).

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
In The
sequence training for image captioning.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Appendix

A Error Analysis

Failure Case in Turing Test
In Figure 8, we
presented a negative example that failed the Turing
test (4 out of 5 made the correct decision). Com-
pared with the human-generated story, our AREL
story lacked emotion and imagination and thus can
be easily distinguished. For example, the real hu-
man gave the band a nickname “very loud band”
and told a more amusing story. Though we have
made encouraging progress on generating human-
like stories, further research of creating diversiﬁed
stories is still needed.

Data Bias From the experiments, we observe
that there exist some severe data bias issues in
the VIST dataset, such as gender bias and event
bias.
In the training set, the ratio of male and
female’s appearances is 2.06:1, and it is 2.16:1
in the test set.
the models aggravate the gender
bias to 3.44:1. Besides, because all the images
are collected from Flickr, there is also an event
bias issue. We count three most frequent events:
party, wedding, and graduation, whose ratios are
6.51:2.36:1 on the training set and 4.54:2.42:1 on
the test set. However, their ratio on the testing
results is 10.69:2.22:1. Clearly, the models tend
to magnify the inﬂuence of the largest majority.
These bias issues remain to be studied for future
work.

B Training Details

Our model is implemented on PyTorch and con-
sists of two parts – a policy model and a reward
model. The policy model is implemented with
a multiple-RNN architecture. Each RNN model
is responsible for generating a sub-story for each
photo in the stream. But the weights are tied
to minimize the memory consumption. The im-
age features are extracted from the pre-trained
ResNet-152 model8. The visual encoder receives
the ResNet-152 features and uses recurrent neu-
ral network to understand the temporal dynamics
and represents them as hidden state vectors, which
is further fed into the decoder to generate stories.
The reward model is based on convolutional neu-
ral network and uses convolution kernels to extract
semantic features for prediction. Here we give the
detailed description of our system:

8https://github.com/KaimingHe/

deep-residual-networks

• Visual Encoder:

the visual encoder is a bi-
directional GRU model with hidden dimen-
sion of 256 for each direction. we concate-
nate the bi-directional states and form a 512
dimension vector for the story generator. The
input album is composed of ﬁve images, and
each image is used as separate input to differ-
ent RNN decoders.

• Decoder: The decoder is a single-layer GRU
model with hidden dimension of 512. The
recurrent decoder model receives the output
from the visual encoder as the ﬁrst input, and
then at the following time steps, it receives
the last predicted token as input or uses the
ground truth as input. During scheduled sam-
pling, we use a sampling probability to de-
cide which action to take.

• Reward Model: we use a convolutional neu-
ral network to extract n-gram features from
the story embedding and stretch them into a
ﬂattened vector. The embedding size of input
story is 128, and the ﬁlter dimension of CNN
is also 128. Here we use three kernels with
window size 2, 3, 4, each with a stride size
of 1. We use a pooling size of 2 to shrink the
extracted outputs and ﬂatten them as a vector.
Finally, we project this vector into a single
cell indicating the predicted reward value.

During training, we ﬁrst pre-train a schedule-
sampling model with a batch size of 64 with
NVIDIA Titan X GPU. The warm-up process
takes roughly 5-10 hours, and then we select the
best model to initialize our AREL policy model.
Finally, we use alternating training strategy to op-
timize both the policy model and the reward model
with a learning rate of 2e-4 using Adam optimiza-
tion algorithm. During test time, we use a beam
size of 3 to approximate the whole search space,
we force the beam search to proceed more than 5
steps and no more than 110 steps. Once we reach
the EOS token, the algorithm stops and we com-
pare the results with human-annotated corpus us-
ing 4 different automatic evaluation metrics.

C Amazon Mechanical Turk

We used AMT to perform two surveys, one picks
a more human-like story. We asked the worker
to answers 8 questions within 30 minutes, and
we pay 5 workers to work on the same sheet to

Figure 8: Failure case in Turing test. 4 out of 5 workers correctly recognized the human-created story
and 1 person mistakenly chose AREL story.

eliminate human-to-human bias. Here we demon-
strate the Turing survey form in Figure 9. Besides,
we also perform a head-to-head comparison with
other algorithms, we demonstrate the survey form
in Figure 10.

Figure 9: Turing Survey Form

Figure 10: Pairwise Comparison Form

No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling

Xin Wang∗, Wenhu Chen∗, Yuan-Fang Wang , William Yang Wang
University of California, Santa Barbara
{xwang,wenhuchen,yfwang,william}@cs.ucsb.edu

8
1
0
2
 
l
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
0
6
1
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Though impressive results have been
achieved in visual captioning,
the task
of generating abstract stories from photo
streams is still a little-tapped problem.
Different from captions, stories have more
expressive language styles and contain
many imaginary concepts that do not ap-
pear in the images. Thus it poses chal-
lenges to behavioral cloning algorithms.
Furthermore, due to the limitations of au-
tomatic metrics on evaluating story qual-
ity, reinforcement learning methods with
hand-crafted rewards also face difﬁcul-
ties in gaining an overall performance
boost. Therefore, we propose an Adver-
sarial REward Learning (AREL) frame-
work to learn an implicit reward function
from human demonstrations, and then op-
timize policy search with the learned re-
ward function. Though automatic eval-
uation indicates slight performance boost
over state-of-the-art (SOTA) methods in
cloning expert behaviors, human evalua-
tion shows that our approach achieves sig-
niﬁcant improvement in generating more
human-like stories than SOTA systems.1

1

Introduction

Recently, increasing attention has been focused
on visual captioning (Chen et al., 2015, 2016; Xu
et al., 2016; Wang et al., 2018c), which aims at
describing the content of an image or a video.
Though it has achieved impressive results, its ca-
pability of performing human-like understanding
is still restrictive. To further investigate machine’s

∗ Equal contribution
1Code
released

is

littlekobe/AREL

at https://github.com/

Figure 1: An example of visual storytelling and
visual captioning. Both captions and stories are
shown here: each image is captioned with one sen-
tence, and we also demonstrate two diversiﬁed sto-
ries that match the same image sequence.

capabilities in understanding more complicated vi-
sual scenarios and composing more structured ex-
pressions, visual storytelling (Huang et al., 2016)
has been proposed. Visual captioning is aimed at
depicting the concrete content of the images, and
its expression style is rather simple. In contrast,
visual storytelling goes one step further: it sum-
marizes the idea of a photo stream and tells a story
about it. Figure 1 shows an example of visual
captioning and visual storytelling. We have ob-
served that stories contain rich emotions (excited,
happy, not want) and imagination (siblings, par-
ents, school, car). It, therefore, requires the capa-
bility to associate with concepts that do not explic-
itly appear in the images. Moreover, stories are
more subjective, so there barely exists standard

templates for storytelling. As shown in Figure 1,
the same photo stream can be paired with diverse
stories, different from each other. This heavily in-
creases the evaluation difﬁculty.

So far, prior work for visual storytelling (Huang
et al., 2016; Yu et al., 2017b) is mainly inspired
by the success of visual captioning. Nevertheless,
because these methods are trained by maximizing
the likelihood of the observed data pairs, they are
restricted to generate simple and plain description
with limited expressive patterns. In order to cope
with the challenges and produce more human-like
descriptions, Rennie et al. (2017) have proposed
a reinforcement learning framework. However, in
the scenario of visual storytelling, the common re-
inforced captioning methods are facing great chal-
lenges since the hand-crafted rewards based on
string matches are either too biased or too sparse
to drive the policy search. For instance, we used
the METEOR (Banerjee and Lavie, 2005) score
as the reward to reinforce our policy and found
that though the METEOR score is signiﬁcantly
improved, the other scores are severely harmed.
Here we showcase an adversarial example with an
average METEOR score as high as 40.2:

We had a great time to have a lot of the.
They were to be a of the. They were to be in
the. The and it were to be the. The, and it
were to be the.

Apparently, the machine is gaming the metrics.
Conversely, when using some other metrics (e.g.
BLEU, CIDEr) to evaluate the stories, we observe
an opposite behavior: many relevant and coherent
stories are receiving a very low score (nearly zero).
In order to resolve the strong bias brought by
the hand-coded evaluation metrics in RL training
and produce more human-like stories, we propose
an Adversarial REward Learning (AREL) frame-
work for visual storytelling. We draw our inspi-
ration from recent progress in inverse reinforce-
ment learning (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017) and propose the AREL algo-
rithm to learn a more intelligent reward function.
Speciﬁcally, we ﬁrst incorporate a Boltzmann dis-
tribution to associate reward learning with distri-
bution approximation, then design the adversarial
process with two models – a policy model and a
reward model. The policy model performs the
primitive actions and produces the story sequence,
while the reward model is responsible for learning

the implicit reward function from human demon-
strations. The learned reward function would be
employed to optimize the policy in return.

For evaluation, we conduct both automatic met-
rics and human evaluation but observe a poor cor-
relation between them. Particularly, our method
gains slight performance boost over the base-
line systems on automatic metrics; human evalu-
ation, however, indicates signiﬁcant performance
boost. Thus we further discuss the limitations
of the metrics and validate the superiority of our
AREL method in performing more intelligent un-
derstanding of the visual scenes and generating
more human-like stories.

Our main contributions are four-fold:

• We propose an adversarial reward learning
framework and apply it to boost visual story
generation.

• We evaluate our approach on the Visual
Storytelling (VIST) dataset and achieve the
state-of-the-art results on automatic metrics.

• We empirically demonstrate that automatic
metrics are not perfect for either training or
evaluation.

• We design and perform a comprehensive
human evaluation via Amazon Mechanical
Turk, which demonstrates the superiority of
the generated stories of our method on rele-
vance, expressiveness, and concreteness.

2 Related Work

Visual Storytelling Visual storytelling is the
task of generating a narrative story from a photo
stream, which requires a deeper understanding
of the event ﬂow in the stream. Park and Kim
(2015) has done some pioneering research on sto-
rytelling. Chen et al. (2017) proposed a multi-
modal approach for storyline generation to pro-
duce a stream of entities instead of human-like de-
scriptions. Recently, a more sophisticated dataset
for visual storytelling (VIST) has been released
to explore a more human-like understanding of
grounded stories (Huang et al., 2016). Yu et al.
(2017b) proposes a multi-task learning algorithm
for both album summarization and paragraph gen-
eration, achieving the best results on the VIST
dataset. But these methods are still based on be-
havioral cloning and lack the ability to generate
more structured stories.

Reinforcement Learning in Sequence Genera-
tion Recently, reinforcement learning (RL) has
gained its popularity in many sequence generation
tasks such as machine translation (Bahdanau et al.,
2016), visual captioning (Ren et al., 2017; Wang
et al., 2018b), summarization (Paulus et al., 2017;
Chen et al., 2018), etc. The common wisdom of
using RL is to view generating a word as an ac-
tion and aim at maximizing the expected return
by optimizing its policy. As pointed in (Ranzato
et al., 2015), traditional maximum likelihood al-
gorithm is prone to exposure bias and label bias,
while the RL agent exposes the generative model
to its own distribution and thus can perform bet-
ter. But these works usually utilize hand-crafted
metric scores as the reward to optimize the model,
which fails to learn more implicit semantics due to
the limitations of automatic metrics.

Rethinking Automatic Metrics Automatic
metrics, including BLEU (Papineni et al., 2002),
CIDEr (Vedantam et al., 2015), METEOR (Baner-
jee and Lavie, 2005), and ROUGE (Lin, 2004),
have been widely applied to the sequence gener-
ation tasks. Using automatic metrics can ensure
rapid prototyping and testing new models with
fewer expensive human evaluation. However, they
have been criticized to be biased and correlate
poorly with human judgments, especially in many
generative tasks like response generation (Lowe
et al., 2017; Liu et al., 2016), dialogue sys-
tem (Bruni and Fern´andez, 2017) and machine
translation (Callison-Burch et al., 2006). The
naive overlap-counting methods are not able
to reﬂect many semantic properties in natural
language, such as coherence, expressiveness, etc.

Generative Adversarial Network Generative
adversarial network (GAN) (Goodfellow et al.,
2014) is a very popular approach for estimating
intractable probabilities, which sidestep the difﬁ-
culty by alternately training two models to play a
min-max two-player game:

min
D

max
G

E
x∼pdata

[log D(x)] + E
z∼pz

[log D(G(z))] ,

where G is the generator and D is the discrimina-
tor, and z is the latent variable. Recently, GAN
has quickly been adopted to tackle discrete prob-
lems (Yu et al., 2017a; Dai et al., 2017; Wang et al.,
2018a). The basic idea is to use Monte Carlo pol-
icy gradient estimation (Williams, 1992) to update
the parameters of the generator.

Figure 2: AREL framework for visual storytelling.

Inverse Reinforcement Learning Reinforce-
ment learning is known to be hindered by the
need for an extensive feature and reward engi-
neering, especially under the unknown dynamics.
Therefore, inverse reinforcement learning (IRL)
has been proposed to infer expert’s reward func-
tion. Previous IRL approaches include maximum
margin approaches (Abbeel and Ng, 2004; Ratliff
et al., 2006) and probabilistic approaches (Ziebart,
2010; Ziebart et al., 2008). Recently, adversarial
inverse reinforcement learning methods provide
an efﬁcient and scalable promise for automatic re-
ward acquisition (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017; Henderson et al., 2017).
These approaches utilize the connection between
IRL and energy-based model and associate every
data with a scalar energy value by using Boltz-
mann distribution pθ(x) ∝ exp(−Eθ(x)).
In-
spired by these methods, we propose a practical
AREL approach for visual storytelling to uncover
a robust reward function from human demonstra-
tions and thus help produce human-like stories.

3 Our Approach

3.1 Problem Statement

Here we consider the task of visual storytelling,
whose objective is to output a word sequence W =
(w1, w1, · · · , wT ), wt ∈ V given an input image
stream of 5 ordered images I = (I1, I2, · · · , I5),
where V is the vocabulary of all output token.
We formulate the generation as a markov deci-
sion process and design a reinforcement learning
framework to tackle it. As described in Figure 2,
our AREL framework is mainly composed of two
modules: a policy model πβ(W ) and a reward
model Rθ(W ). The policy model takes an image
sequence I as the input and performs sequential
actions (choosing words w from the vocabulary V)
to form a narrative story W . The reward model

Figure 3: Overview of the policy model. The vi-
sual encoder is a bidirectional GRU, which en-
codes the high-level visual features extracted from
the input images. Its outputs are then fed into the
RNN decoders to generate sentences in parallel.
Finally, we concatenate all the generated sentences
as a full story. Note that the ﬁve decoders share the
same weights.

is optimized by the adversarial objective (see Sec-
tion 3.3) and aims at deriving a human-like reward
from both human-annotated stories and sampled
predictions.

3.2 Model

Policy Model As is shown in Figure 3, the pol-
icy model is a CNN-RNN architecture. We ﬁst
feed the photo stream I = (I1, · · · , I5) into a
pretrained CNN and extract their high-level image
features. We then employ a visual encoder to fur-
ther encode the image features as context vectors
−→
hi = [
hi ]. The visual encoder is a bidirectional
gated recurrent units (GRU).

←−
hi ;

In the decoding stage, we feed each context vec-
tor hi into a GRU-RNN decoder to generate a sub-
story Wi. Formally, the generation process can be
written as:

t = GRU(si
t−1, [wi
si
1:t−1) = sof tmax(Wssi

t−1, hi]) ,
t + bs) ,

(1)

(2)

πβ(wi

t|wi

where si
t denotes the t-th hidden state of i-th de-
coder. We concatenate the previous token wi
t−1
and the context vector hi as the input. Ws and
bs are the projection matrix and bias, which out-
put a probability distribution over the whole vo-
cabulary V. Eventually, the ﬁnal story W is the
concatenation of the sub-stories Wi. β denotes all
the parameters of the encoder, the decoder, and the
output layer.

Figure 4: Overview of the reward model. Our re-
ward model is a CNN-based architecture, which
utilizes convolution kernels with size 2, 3 and 4
to extract bigram, trigram and 4-gram representa-
tions from the input sequence embeddings. Once
the sentence representation is learned, it will be
concatenated with the visual representation of the
input image, and then be fed into the ﬁnal FC layer
to obtain the reward.

Reward Model The reward model Rθ(W ) is a
CNN-based architecture (see Figure 4). Instead of
giving an overall score for the whole story, we ap-
ply the reward model to different story parts (sub-
stories) Wi and compute partial rewards, where
i = 1, · · · , 5. We observe that the partial rewards
are more ﬁne-grained and can provide better guid-
ance for the policy model.

We ﬁrst query the word embeddings of the sub-
story (one sentence in most cases). Next, multi-
ple convolutional layers with different kernel sizes
are used to extract the n-grams features, which
are then projected into the sentence-level repre-
sentation space by pooling layers (the design here
In addition to the
is inspired by Kim (2014)).
textual features, evaluating the quality of a story
should also consider the image features for rele-
vance. Therefore, we then combine the sentence
representation with the visual feature of the input
image through concatenation and feed them into
the ﬁnal fully connected decision layer.
In the
end, the reward model outputs an estimated reward
value Rθ(W ). The process can be written in for-
mula:

Rθ(W ) = φ(Wr(fconv(W ) + WiICN N ) + br),

(3)

where φ denotes the non-linear projection func-
tion, Wr, br denote the weight and bias in the
output layer, and fconv denotes the operations in
CNN. ICN N is the high-level visual feature ex-
tracted from the image, and Wi projects it into the

sentence representation space. θ includes all the
parameters above.

3.3 Learning

Reward Boltzmann Distribution In order to
associate story distribution with reward function,
we apply EBM to deﬁne a Reward Boltzmann dis-
tribution:

Algorithm 1 The AREL Algorithm.

1: for episode ← 1 to N do
2:

collect story W by executing policy πθ
if Train-Reward then
θ ← θ − η × ∂Jθ
else if Train-Policy then

∂θ (see Equation 9)

collect story ˜W from empirical pe
β ← β − η × ∂Jβ
∂β (see Equation 9)

3:

4:

5:

6:

7:

pθ(W ) =

exp(Rθ(W ))
Zθ

,

(4)

end if

8:
9: end for

Where W is the word sequence of the story and
pθ(W ) is the approximate data distribution, and
Zθ = (cid:80)
exp(Rθ(W )) denotes the partition func-
W
tion. According to the energy-based model (Le-
Cun et al., 2006), the optimal reward function
R∗(W ) is achieved when the Reward-Boltzmann
distribution equals to the “real” data distribution
pθ(W ) = p∗(W ).

Adversarial Reward Learning We ﬁrst intro-
1(W ∈D)
duce an empirical distribution pe(W ) =
|D|
to represent the empirical distribution of the train-
ing data, where D denotes the dataset with |D| sto-
ries and 1 denotes an indicator function. We use
this empirical distribution as the “good” examples,
which provides the evidence for the reward func-
tion to learn from.

In order to approximate the Reward Boltzmann
distribution towards the “real” data distribution
p∗(W ), we design a min-max two-player game,
where the Reward Boltzmann distribution pθ aims
at maximizing the its similarity with empirical
distribution pe while minimizing that with the
“faked” data generated from policy model πβ. On
the contrary, the policy distribution πβ tries to
maximize its similarity with the Boltzmann dis-
tribution pθ. Formally, the adversarial objective
function is deﬁned as
max
β

KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W )) .

min
θ

We further decompose it into two parts. First,
because the objective Jβ of the story genera-
tion policy is to maximize its similarity with
the Boltzmann distribution pθ, the optimal policy
that minimizes KL-divergence is thus π(W ) ∼
exp(Rθ(W )), meaning if Rθ is optimal, the op-
timal πβ = π∗. In formula,

Jβ = − KL(πβ(W )||pθ(W ))

=

E
W ∼πβ (W )

[Rθ(W )] − log Zθ + H(πβ(W )) ,

(6)

where H denotes the entropy of the policy model.
On the other hand, the objective Jθ of the re-
ward function is to distinguish between human-
annotated stories and machine-generated stories.
Hence it is trying to minimize the KL-divergence
with the empirical distribution pe and maximize
the KL-divergence with the approximated policy
distribution πβ:

Jθ =KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W ))

(cid:88)

=

[pe(W )Rθ(W ) − πβ(W )Rθ(W )]

W
+ log Zθ − log Zθ − H(pe) + H(πβ) ,

(7)

Since H(πβ) and H(pe) are irrelevant to θ, we
denote them as constant C. It is also worth not-
ing that with negative sampling in the optimization
of the KL-divergence, the computation of the in-
tractable partition function Zθ is bypassed. There-
fore, the objective Jθ can be further derived as

Jθ =

E
W ∼pe(W )

[Rθ(W )] −

E
W ∼πβ (W )

[Rθ(W )] + C . (8)

Here we propose to use stochastic gradient de-
scent to optimize these two models alternately.
Formally, the gradients can be written as

(5)

∂Jθ
∂θ

∂Jβ
∂β

=

E
[
W ∼pe(W )

∂Rθ(W )
∂θ

] −

E
[
W ∼πβ (W )

∂Rθ(W )
∂θ

] ,

=

E
W ∼πβ (W )

(Rθ(W ) − log πβ(W ) − b)

∂ log πβ(W )
∂β

,

(9)

where b is the estimated baseline to reduce vari-
ance during REINFORCE training.

Training & Testing As described in Algo-
rithm 1, we introduce an alternating algorithm to
train these two models using stochastic gradient
descent. During testing, the policy model is used
with beam search to produce the story.

4 Experiments and Analysis

4.1 Experimental Setup

VIST Dataset The VIST dataset (Huang et al.,
2016) is the ﬁrst dataset for sequential vision-to-
language tasks including visual storytelling, which
consists of 10,117 Flickr albums with 210,819
unique photos.
In this paper, we mainly evalu-
ate our AREL method on this dataset. After ﬁlter-
ing the broken images2, there are 40,098 training,
4,988 validation, and 5,050 testing samples. Each
sample contains one story that describes 5 selected
images from a photo album (mostly one sentence
per image). And the same album is paired with 5
different stories as references. In our experiments,
we used the same split settings as in (Huang et al.,
2016; Yu et al., 2017b) for a fair comparison. Dur-
ing our experiments, we apply two kinds of non-
linear functions φ for the discriminator, namely
SoftSign function (f (x) = x
1+|x| ) and Hyper-
bolic function (f (x) = sinhx
coshx ). We found that
unbounded non-linear functions like ReLU func-
tion (Glorot et al., 2011) will lead to severe vibra-
tions and instabilities during training, therefore we
resort to the bounded functions.

Evaluation Metrics
In order to comprehen-
sively evaluate our method on storytelling dataset,
we adopt both the automatic metrics and human
evaluation as our criterion. Four diverse automatic
metrics are used in our experiments: BLEU, ME-
TEOR, ROUGE-L, and CIDEr. We utilize the
open source evaluation code3 used in (Yu et al.,
2017b). For human evaluation, we employ the
Amazon Mechanical Turk to perform two kinds of
user studies (see Section 4.3 for more details).

employ

Training Details We
pretrained
ResNet-152 model (He et al., 2016) to extract
image features from the photostream. We built a
vocabulary of size 9,837 to include words appear-
ing more than three times in the training set. More
training details can be found at Appendix B.

4.2 Automatic Evaluation

In this section, we compare our AREL method
with the state-of-the-art methods as well as stan-
dard reinforcement learning algorithms on auto-

2There are only 3 (out of 21,075) broken images in the
test set, which basically has no inﬂuence on the ﬁnal results.
Moreover, Yu et al. (2017b) also removed the 3 pictures, so it
is a fair comparison.

3https://github.com/lichengunc/vist_eval

Method

B-1 B-2 B-3 B-4 M R

C

Huang et al.
Yu et al.
XE-ss
GAN
AREL-s-50
AREL-t-50
AREL-s-100
AREL-t-100

-
-

-
-

-
21.0

-
-
31.4
-
34.1 29.5 7.5
-
62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.8 38.8 23.0 14.0 35.0 29.5 9.0
62.9 38.4 22.7 14.0 34.9 29.4 9.1
63.4 39.0 23.1 14.1 35.2 29.6 9.5
64.0 38.6 22.3 13.2 35.1 29.3 9.6
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 1: Automatic evaluation on the VIST
dataset. We report BLEU (B), METEOR (M),
ROUGH-L (R), and CIDEr (C) scores of the
SOTA systems and the models we implemented,
including XE-ss, GAN and AREL. AREL-s-N de-
notes AREL models with SoftSign as output acti-
vation and alternate frequency as N, while AREL-
t-N denoting AREL models with Hyperbolic as the
output activation (N = 50 or 100).

matic evaluation metrics. Then we further discuss
the limitations of the hand-crafted metrics on eval-
uating human-like stories.

Comparison with SOTA on Automatic Metrics
In Table 1, we compare our method with Huang
et al. (2016) and Yu et al. (2017b), which report
achieving best-known results on the VIST dataset.
We ﬁrst implement a strong baseline model (XE-
ss), which share the same architecture with our
policy model but is trained with cross-entropy loss
and scheduled sampling. Besides, we adopt the
traditional generative adversarial training for com-
parison (GAN). As shown in Table 1, our XE-
ss model already outperforms the best-known re-
sults on the VIST dataset, and the GAN model can
bring a performance boost. We then use the XE-
ss model to initialize our policy model and further
train it with AREL. Evidently, our AREL model
performs the best and achieves the new state-of-
the-art results across all metrics.

But, compared with the XE-ss model, the per-
formance gain is minor, especially on METEOR
and ROUGE-L scores. However, in Sec. 4.3, the
extensive human evaluation has indicated that our
AREL framework brings a signiﬁcant improve-
ment on generating human-like stories over the
XE-ss model. The inconsistency of automatic
evaluation and human evaluation lead to a suspect
that these hand-crafted metrics lack the ability to
fully evaluate stories’ quality due to the compli-
cated characteristics of the stories. Therefore, we
conduct experiments to analyze and discuss the

Method

B-1 B-2 B-3 B-4 M R

C

XE-ss
BLEU-RL
METEOR-RL
ROUGE-RL
CIDEr-RL
AREL (best)

62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.1 38.0 22.6 13.9 34.6 29.0 8.9
40.2 30.0 1.2
68.1 35.0 15.4
6.8
27.0 33.8
58.1 18.5
0
0
1.6
61.9 37.8 22.5 13.8 34.9 29.7 8.1
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 2: Comparison with different RL mod-
els with different metric scores as the rewards.
We report the average scores of the AREL mod-
els as AREL (avg). Although METEOR-RL and
ROUGE-RL models achieve the highest scores
on their own metrics, the underlined scores are
severely damaged. Actually, they are gaming their
own metrics with nonsense sentences.

defects of the automatic metrics in section 4.2.

Limitations of Automatic Metrics String-
match-based automatic metrics are not perfect
and fail to evaluate some semantic characteristics
of the stories (e.g. expressiveness and coherence).
In order to conﬁrm our conjecture, we utilize
automatic metrics as rewards to reinforce the
model with policy gradient.
The quantitative
results are demonstrated in Table 1.

Apparently, METEOR-RL and ROUGE-RL are
severely ill-posed: they obtain the highest scores
on their own metrics but damage the other met-
rics severely. We observe that these models are
actually overﬁtting to a given metric while losing
the overall coherence and semantical correctness.
Same as METEOR score, there is also an adver-
sarial example for ROUGE-L4, which is nonsense
but achieves an average ROUGE-L score of 33.8.
Besides, as can be seen in Table 1, after rein-
forced training, BLEU-RL and CIDEr-RL do not
bring a consistent improvement over the XE-ss
model. We plot the histogram distributions of both
BLEU-3 and CIDEr scores on the test set in Fig-
ure 5. An interesting fact is that there are a large
number of samples with nearly zero score on both
metrics. However, we observed those “zero-score”
samples are not pointless results; instead, lots of
them make sense and deserve a better score than
zero. Here is a “zero-score” example on BLEU-3:

I had a great time at the restaurant today.
The food was delicious. I had a lot of food.
The food was delicious. I had a great time.

4An adversarial example for ROUGE-L: we the was a .
and to the . we the was a . and to the . we the was a . and to
the . we the was a . and to the . we the was a . and to the .

Method
XE-ss
BLEU-RL
CIDEr-RL
GAN
AREL

Win
Lose
22.4% 71.7%
23.4% 67.9%
13.8% 80.3%
34.3% 60.5%
38.4% 54.2%

Unsure
5.9%
8.7%
5.9%
5.2%
7.4%

Table 3: Turing test results.

The corresponding reference is

The table of food was a pleasure to see!
Our food is both nutritious and beautiful!
Our chicken was especially tasty! We love
greens as they taste great and are healthy!
The fruit was a colorful display that tanta-
lized our palette.

Although the prediction is not as good as the ref-
erence, it is actually coherent and relevant to the
theme “food and eating”, which showcases the de-
feats of using BLEU and CIDEr scores as a reward
for RL training.

Moreover, we compare the human evaluation
scores with these two metric scores in Figure 5.
Noticeably, both BLEU-3 and CIDEr have a poor
correlation with the human evaluation scores.
Their distributions are more biased and thus can-
not fully reﬂect the quality of the generated sto-
ries. In terms of BLEU, it is extremely hard for
machines to produce the exact 3-gram or 4-gram
matching, so the scores are too low to provide use-
ful guidance. CIDEr measures the similarity of a
sentence to the majority of the references. How-
ever, the references to the same image sequence
are photostream different from each other, so the
score is very low and not suitable for this task. In
contrast, our AREL framework can lean a more
robust reward function from human-annotated sto-
ries, which is able to provide better guidance to
the policy and thus improves its performances over
different metrics.

Visualization of The Learned Rewards
In Fig-
ure 6, we visualize the learned reward function for
both ground truth and generated stories. Evidently,
the AREL model is able to learn a smoother re-
ward function that can distinguish the generated
stories from human annotations. In other words,
the learned reward function is more in line with
human perception and thus can encourage the
model to explore more diverse language styles and
expressions.

Figure 5: Metric score distributions. We plot the histogram distributions of BLEU-3 and CIDEr scores
on the test set, as well as the human evaluation score distribution on the test samples. We use the Turing
test results to calculate the human evaluation scores (see Section 4.3). Basically, 0.2 score is given if the
generated story wins the Turing test, 0.1 for tie, and 0 if losing. Each sample has 5 scores from 5 judges,
and we use the sum as the human evaluation score, so it is in the range [0, 1].

Choice (%)
Relevance
Expressiveness
Concreteness

AREL vs XE-ss
AREL XE-ss Tie
61.7
13.2
66.1
15.1
63.9
15.8

25.1
18.8
20.3

AREL vs BLEU-RL
AREL BLEU-RL Tie
55.8
16.3
59.1
14.5
60.1
13.6

27.9
26.4
26.3

AREL vs CIDEr-RL
AREL CIDEr-RL Tie
56.1
15.7
59.1
14.3
59.5
15.9

28.2
26.6
24.6

AREL vs GAN
AREL GAN Tie
52.9
35.8 11.3
48.5
32.2 19.3
49.8
35.8 14.4

Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model
in generating more human-like stories than the SOTA methods.

is prone to the vanishing gradient issue. Analyti-
cally, our method does not suffer from these two
common issues and thus is able converge to op-
timum solutions more easily. From Table 1 we
can observe slight gains of using AREL over GAN
with automatic metrics, but we further deploy hu-
man evaluation for a better comparison.

4.3 Human Evaluation

Automatic metrics cannot fully evaluate the ca-
pability of our AREL method. Therefore, we
perform two different kinds of human evaluation
studies on Amazon Mechanical Turk: Turing test
and pairwise human evaluation. For both tasks,
we use 150 stories (750 images) sampled from the
test set, each assigned to 5 workers to eliminate
human variance. We batch six items as one assign-
ment and insert an additional assignment as a san-
ity check. Besides, the order of the options within
each item is shufﬂed to make a fair comparison.

Turing Test We ﬁrst conduct ﬁve indepen-
dent Turing tests for XE-ss, BLEU-RL, CIDEr-
RL, GAN, and AREL models, during which the
worker is given one human-annotated sample and
one machine-generated sample, and needs to de-
cide which is human-annotated. As shown in Ta-
ble 3, our AREL model signiﬁcantly outperforms
all the other baseline models in the Turing test: it
has much more chances to fool AMT worker (the

Figure 6: Visualization of the learned rewards on
both the ground-truth stories and the stories gen-
erated by our AREL model. The generated sto-
ries are receiving lower averaged scores than the
human-annotated ones.

Comparison with GAN We here compare our
method with vanilla GAN (Goodfellow et al.,
2014), whose update rules for the generator can
be generally classiﬁed into two categories. We
demonstrate their corresponding objectives and
ours as follows:

GAN 1 : Jβ = E

[− log Rθ(W )] ,

GAN 2 :

ours :

W ∼pβ

Jβ = E

W ∼pβ

Jβ = E

W ∼pβ

[log(1 − Rθ(W ))] ,

[−Rθ(W )] .

As discussed in Arjovsky et al. (2017), GAN 1 is
prone to the unstable gradient issue and GAN 2

Figure 7: Qualitative comparison example with XE-ss. The direct comparison votes (AREL:XE-ss:Tie)
were 5:0:0 on Relevance, 4:0:1 on Expressiveness, and 5:0:0 on Concreteness.

ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN
= 45.8%:28.3%:32.1%:19.7%:39.5%), which con-
ﬁrms the superiority of our AREL framework in
generating human-like stories. Unlike automatic
metric evaluation, the Turing test has indicated
a much larger margin between AREL and other
competing algorithms. Thus, we empirically con-
ﬁrm that metrics are not perfect in evaluating many
implicit semantic properties of natural language.
Besides, the Turing test of our AREL model re-
veals that nearly half of the workers are fooled by
our machine generation, indicating a preliminary
success toward generating human-like stories.

Pairwise Comparison In order to have a clear
comparison with competing algorithms with re-
spect to different semantic features of the sto-
ries, we further perform four pairwise compar-
ison tests: AREL vs XE-ss/BLEU-RL/CIDEr-
RL/GAN. For each photostream, the worker is
presented with two generated stories and asked to
make decisions from the three aspects: relevance5,
expressiveness6 and concreteness7. This head-to-
head compete is designed to help us understand in
what aspect our model outperforms the competing
algorithms, which is displayed in Table 4.

Consistently on all the three comparisons, a
large majority of the AREL stories trumps the
competing systems with respect to their relevance,

5Relevance: the story accurately describes what is hap-

pening in the image sequence and covers the main objects.

6Expressiveness: coherence, grammatically and semanti-

cally correct, no repetition, expressive language style.

7Concreteness: the story should narrate concretely what

is in the image rather than giving very general descriptions.

expressiveness, and concreteness. Therefore, it
empirically conﬁrms that our generated stories are
more relevant to the image sequences, more coher-
ent and concrete than the other algorithms, which
however is not explicitly reﬂected by the auto-
matic metric evaluation.

4.4 Qualitative Analysis

Figure 7 gives a qualitative comparison example
between AREL and XE-ss models. Looking at the
individual sentences, it is obvious that our results
are more grammatically and semantically correct.
Then connecting the sentences together, we ob-
serve that the AREL story is more coherent and
describes the photo stream more accurately. Thus,
our AREL model signiﬁcantly surpasses the XE-
ss model on all the three aspects of the qualitative
example. Besides, it won the Turing test (3 out 5
AMT workers think the AREL story is created by
a human). In the appendix, we also show a nega-
tive case that fails the Turing test.

5 Conclusion

In this paper, we not only introduce a novel ad-
versarial reward learning algorithm to generate
more human-like stories given image sequences,
but also empirically analyze the limitations of the
automatic metrics for story evaluation. We believe
there are still lots of improvement space in the
narrative paragraph generation tasks, like how to
better simulate human imagination to create more
vivid and diversiﬁed stories.

Acknowledgment

We thank Adobe Research for supporting our lan-
guage and vision research. We would also like
to thank Licheng Yu for clarifying the details
of his paper and the anonymous reviewers for
their thoughtful comments. This research was
sponsored in part by the Army Research Labora-
tory under cooperative agreements W911NF09-2-
0053. The views and conclusions contained herein
are those of the authors and should not be inter-
preted as representing the ofﬁcial policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References

Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international confer-
ence on Machine learning, page 1. ACM.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
arXiv preprint

tou. 2017. Wasserstein gan.
arXiv:1701.07875.

Wenhu Chen, Aur´elien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.

Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo,
Jing Dai, and Chang-Tien Lu. 2017. Multi-
modal storytelling via generative adversarial imita-
In Proceedings of the Twenty-Sixth
tion learning.
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-17, pages 3967–3973.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
In The IEEE Interna-
tions via a conditional gan.
tional Conference on Computer Vision (ICCV).

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. arXiv preprint
arXiv:1611.03852.

Justin Fu, Katie Luo, and Sergey Levine. 2017.
in-
arXiv preprint

rewards with adversarial

learning.

Learning robust
verse reinforcement
arXiv:1710.11248.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks. In Pro-
ceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 315–
323.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Elia Bruni and Raquel Fern´andez. 2017. Adversarial
evaluation for open-domain dialogue generation. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 284–288.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.

Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon,
David Meger, Joelle Pineau, and Doina Precup.
2017. Optiongan: Learning joint reward-policy op-
tions using generative adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1709.06683.

Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui
Zhang, Mu Li, and Ming Zhou. 2018. Generative
bridging network for neural sequence prediction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1706–1715. Associ-
ation for Computational Linguistics.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neural
Information Processing Systems, pages 4565–4573.

Ting-Hao K. Huang,

Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual

storytelling.
North American Chapter of
Computational Linguistics (NAACL 2016).

In 15th Annual Conference of the
the Association for

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,
and F Huang. 2006. A tutorial on energy-based
learning. Predicting structured data, 1(0).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. arXiv
preprint arXiv:1708.07149.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and
Tao Mei. 2018a. Show, reward and tell: Automatic
generation of narrative paragraph from photo stream
by adversarial training. AAAI.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video caption-
ing via hierarchical reinforcement learning. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xin Wang, Yuan-Fang Wang, and William Yang Wang.
2018c. Watch, listen, and describe: Globally and lo-
cally aligned cross-modal attentions for video cap-
tioning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 795–801.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
In Proceedings of the IEEE
video and language.
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in Neural Information Processing Sys-
tems, pages 73–81.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017a. Seqgan: Sequence generative adversarial
In AAAI, pages 2852–
nets with policy gradient.
2858.

Licheng Yu, Mohit Bansal, and Tamara Berg. 2017b.
Hierarchically-attentive rnn for album summariza-
In Proceedings of the 2017
tion and storytelling.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 966–971, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
In AAAI, volume 8, pages
reinforcement learning.
1433–1438. Chicago, IL, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning.
In
Proceedings of the 23rd international conference on
Machine learning, pages 729–736. ACM.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceeding of IEEE conference on Computer Vision and
Pattern Recognition (CVPR).

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
In The
sequence training for image captioning.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Appendix

A Error Analysis

Failure Case in Turing Test
In Figure 8, we
presented a negative example that failed the Turing
test (4 out of 5 made the correct decision). Com-
pared with the human-generated story, our AREL
story lacked emotion and imagination and thus can
be easily distinguished. For example, the real hu-
man gave the band a nickname “very loud band”
and told a more amusing story. Though we have
made encouraging progress on generating human-
like stories, further research of creating diversiﬁed
stories is still needed.

Data Bias From the experiments, we observe
that there exist some severe data bias issues in
the VIST dataset, such as gender bias and event
bias.
In the training set, the ratio of male and
female’s appearances is 2.06:1, and it is 2.16:1
in the test set.
the models aggravate the gender
bias to 3.44:1. Besides, because all the images
are collected from Flickr, there is also an event
bias issue. We count three most frequent events:
party, wedding, and graduation, whose ratios are
6.51:2.36:1 on the training set and 4.54:2.42:1 on
the test set. However, their ratio on the testing
results is 10.69:2.22:1. Clearly, the models tend
to magnify the inﬂuence of the largest majority.
These bias issues remain to be studied for future
work.

B Training Details

Our model is implemented on PyTorch and con-
sists of two parts – a policy model and a reward
model. The policy model is implemented with
a multiple-RNN architecture. Each RNN model
is responsible for generating a sub-story for each
photo in the stream. But the weights are tied
to minimize the memory consumption. The im-
age features are extracted from the pre-trained
ResNet-152 model8. The visual encoder receives
the ResNet-152 features and uses recurrent neu-
ral network to understand the temporal dynamics
and represents them as hidden state vectors, which
is further fed into the decoder to generate stories.
The reward model is based on convolutional neu-
ral network and uses convolution kernels to extract
semantic features for prediction. Here we give the
detailed description of our system:

8https://github.com/KaimingHe/

deep-residual-networks

• Visual Encoder:

the visual encoder is a bi-
directional GRU model with hidden dimen-
sion of 256 for each direction. we concate-
nate the bi-directional states and form a 512
dimension vector for the story generator. The
input album is composed of ﬁve images, and
each image is used as separate input to differ-
ent RNN decoders.

• Decoder: The decoder is a single-layer GRU
model with hidden dimension of 512. The
recurrent decoder model receives the output
from the visual encoder as the ﬁrst input, and
then at the following time steps, it receives
the last predicted token as input or uses the
ground truth as input. During scheduled sam-
pling, we use a sampling probability to de-
cide which action to take.

• Reward Model: we use a convolutional neu-
ral network to extract n-gram features from
the story embedding and stretch them into a
ﬂattened vector. The embedding size of input
story is 128, and the ﬁlter dimension of CNN
is also 128. Here we use three kernels with
window size 2, 3, 4, each with a stride size
of 1. We use a pooling size of 2 to shrink the
extracted outputs and ﬂatten them as a vector.
Finally, we project this vector into a single
cell indicating the predicted reward value.

During training, we ﬁrst pre-train a schedule-
sampling model with a batch size of 64 with
NVIDIA Titan X GPU. The warm-up process
takes roughly 5-10 hours, and then we select the
best model to initialize our AREL policy model.
Finally, we use alternating training strategy to op-
timize both the policy model and the reward model
with a learning rate of 2e-4 using Adam optimiza-
tion algorithm. During test time, we use a beam
size of 3 to approximate the whole search space,
we force the beam search to proceed more than 5
steps and no more than 110 steps. Once we reach
the EOS token, the algorithm stops and we com-
pare the results with human-annotated corpus us-
ing 4 different automatic evaluation metrics.

C Amazon Mechanical Turk

We used AMT to perform two surveys, one picks
a more human-like story. We asked the worker
to answers 8 questions within 30 minutes, and
we pay 5 workers to work on the same sheet to

Figure 8: Failure case in Turing test. 4 out of 5 workers correctly recognized the human-created story
and 1 person mistakenly chose AREL story.

eliminate human-to-human bias. Here we demon-
strate the Turing survey form in Figure 9. Besides,
we also perform a head-to-head comparison with
other algorithms, we demonstrate the survey form
in Figure 10.

Figure 9: Turing Survey Form

Figure 10: Pairwise Comparison Form

No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling

Xin Wang∗, Wenhu Chen∗, Yuan-Fang Wang , William Yang Wang
University of California, Santa Barbara
{xwang,wenhuchen,yfwang,william}@cs.ucsb.edu

8
1
0
2
 
l
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
0
6
1
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Though impressive results have been
achieved in visual captioning,
the task
of generating abstract stories from photo
streams is still a little-tapped problem.
Different from captions, stories have more
expressive language styles and contain
many imaginary concepts that do not ap-
pear in the images. Thus it poses chal-
lenges to behavioral cloning algorithms.
Furthermore, due to the limitations of au-
tomatic metrics on evaluating story qual-
ity, reinforcement learning methods with
hand-crafted rewards also face difﬁcul-
ties in gaining an overall performance
boost. Therefore, we propose an Adver-
sarial REward Learning (AREL) frame-
work to learn an implicit reward function
from human demonstrations, and then op-
timize policy search with the learned re-
ward function. Though automatic eval-
uation indicates slight performance boost
over state-of-the-art (SOTA) methods in
cloning expert behaviors, human evalua-
tion shows that our approach achieves sig-
niﬁcant improvement in generating more
human-like stories than SOTA systems.1

1

Introduction

Recently, increasing attention has been focused
on visual captioning (Chen et al., 2015, 2016; Xu
et al., 2016; Wang et al., 2018c), which aims at
describing the content of an image or a video.
Though it has achieved impressive results, its ca-
pability of performing human-like understanding
is still restrictive. To further investigate machine’s

∗ Equal contribution
1Code
released

is

littlekobe/AREL

at https://github.com/

Figure 1: An example of visual storytelling and
visual captioning. Both captions and stories are
shown here: each image is captioned with one sen-
tence, and we also demonstrate two diversiﬁed sto-
ries that match the same image sequence.

capabilities in understanding more complicated vi-
sual scenarios and composing more structured ex-
pressions, visual storytelling (Huang et al., 2016)
has been proposed. Visual captioning is aimed at
depicting the concrete content of the images, and
its expression style is rather simple. In contrast,
visual storytelling goes one step further: it sum-
marizes the idea of a photo stream and tells a story
about it. Figure 1 shows an example of visual
captioning and visual storytelling. We have ob-
served that stories contain rich emotions (excited,
happy, not want) and imagination (siblings, par-
ents, school, car). It, therefore, requires the capa-
bility to associate with concepts that do not explic-
itly appear in the images. Moreover, stories are
more subjective, so there barely exists standard

templates for storytelling. As shown in Figure 1,
the same photo stream can be paired with diverse
stories, different from each other. This heavily in-
creases the evaluation difﬁculty.

So far, prior work for visual storytelling (Huang
et al., 2016; Yu et al., 2017b) is mainly inspired
by the success of visual captioning. Nevertheless,
because these methods are trained by maximizing
the likelihood of the observed data pairs, they are
restricted to generate simple and plain description
with limited expressive patterns. In order to cope
with the challenges and produce more human-like
descriptions, Rennie et al. (2017) have proposed
a reinforcement learning framework. However, in
the scenario of visual storytelling, the common re-
inforced captioning methods are facing great chal-
lenges since the hand-crafted rewards based on
string matches are either too biased or too sparse
to drive the policy search. For instance, we used
the METEOR (Banerjee and Lavie, 2005) score
as the reward to reinforce our policy and found
that though the METEOR score is signiﬁcantly
improved, the other scores are severely harmed.
Here we showcase an adversarial example with an
average METEOR score as high as 40.2:

We had a great time to have a lot of the.
They were to be a of the. They were to be in
the. The and it were to be the. The, and it
were to be the.

Apparently, the machine is gaming the metrics.
Conversely, when using some other metrics (e.g.
BLEU, CIDEr) to evaluate the stories, we observe
an opposite behavior: many relevant and coherent
stories are receiving a very low score (nearly zero).
In order to resolve the strong bias brought by
the hand-coded evaluation metrics in RL training
and produce more human-like stories, we propose
an Adversarial REward Learning (AREL) frame-
work for visual storytelling. We draw our inspi-
ration from recent progress in inverse reinforce-
ment learning (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017) and propose the AREL algo-
rithm to learn a more intelligent reward function.
Speciﬁcally, we ﬁrst incorporate a Boltzmann dis-
tribution to associate reward learning with distri-
bution approximation, then design the adversarial
process with two models – a policy model and a
reward model. The policy model performs the
primitive actions and produces the story sequence,
while the reward model is responsible for learning

the implicit reward function from human demon-
strations. The learned reward function would be
employed to optimize the policy in return.

For evaluation, we conduct both automatic met-
rics and human evaluation but observe a poor cor-
relation between them. Particularly, our method
gains slight performance boost over the base-
line systems on automatic metrics; human evalu-
ation, however, indicates signiﬁcant performance
boost. Thus we further discuss the limitations
of the metrics and validate the superiority of our
AREL method in performing more intelligent un-
derstanding of the visual scenes and generating
more human-like stories.

Our main contributions are four-fold:

• We propose an adversarial reward learning
framework and apply it to boost visual story
generation.

• We evaluate our approach on the Visual
Storytelling (VIST) dataset and achieve the
state-of-the-art results on automatic metrics.

• We empirically demonstrate that automatic
metrics are not perfect for either training or
evaluation.

• We design and perform a comprehensive
human evaluation via Amazon Mechanical
Turk, which demonstrates the superiority of
the generated stories of our method on rele-
vance, expressiveness, and concreteness.

2 Related Work

Visual Storytelling Visual storytelling is the
task of generating a narrative story from a photo
stream, which requires a deeper understanding
of the event ﬂow in the stream. Park and Kim
(2015) has done some pioneering research on sto-
rytelling. Chen et al. (2017) proposed a multi-
modal approach for storyline generation to pro-
duce a stream of entities instead of human-like de-
scriptions. Recently, a more sophisticated dataset
for visual storytelling (VIST) has been released
to explore a more human-like understanding of
grounded stories (Huang et al., 2016). Yu et al.
(2017b) proposes a multi-task learning algorithm
for both album summarization and paragraph gen-
eration, achieving the best results on the VIST
dataset. But these methods are still based on be-
havioral cloning and lack the ability to generate
more structured stories.

Reinforcement Learning in Sequence Genera-
tion Recently, reinforcement learning (RL) has
gained its popularity in many sequence generation
tasks such as machine translation (Bahdanau et al.,
2016), visual captioning (Ren et al., 2017; Wang
et al., 2018b), summarization (Paulus et al., 2017;
Chen et al., 2018), etc. The common wisdom of
using RL is to view generating a word as an ac-
tion and aim at maximizing the expected return
by optimizing its policy. As pointed in (Ranzato
et al., 2015), traditional maximum likelihood al-
gorithm is prone to exposure bias and label bias,
while the RL agent exposes the generative model
to its own distribution and thus can perform bet-
ter. But these works usually utilize hand-crafted
metric scores as the reward to optimize the model,
which fails to learn more implicit semantics due to
the limitations of automatic metrics.

Rethinking Automatic Metrics Automatic
metrics, including BLEU (Papineni et al., 2002),
CIDEr (Vedantam et al., 2015), METEOR (Baner-
jee and Lavie, 2005), and ROUGE (Lin, 2004),
have been widely applied to the sequence gener-
ation tasks. Using automatic metrics can ensure
rapid prototyping and testing new models with
fewer expensive human evaluation. However, they
have been criticized to be biased and correlate
poorly with human judgments, especially in many
generative tasks like response generation (Lowe
et al., 2017; Liu et al., 2016), dialogue sys-
tem (Bruni and Fern´andez, 2017) and machine
translation (Callison-Burch et al., 2006). The
naive overlap-counting methods are not able
to reﬂect many semantic properties in natural
language, such as coherence, expressiveness, etc.

Generative Adversarial Network Generative
adversarial network (GAN) (Goodfellow et al.,
2014) is a very popular approach for estimating
intractable probabilities, which sidestep the difﬁ-
culty by alternately training two models to play a
min-max two-player game:

min
D

max
G

E
x∼pdata

[log D(x)] + E
z∼pz

[log D(G(z))] ,

where G is the generator and D is the discrimina-
tor, and z is the latent variable. Recently, GAN
has quickly been adopted to tackle discrete prob-
lems (Yu et al., 2017a; Dai et al., 2017; Wang et al.,
2018a). The basic idea is to use Monte Carlo pol-
icy gradient estimation (Williams, 1992) to update
the parameters of the generator.

Figure 2: AREL framework for visual storytelling.

Inverse Reinforcement Learning Reinforce-
ment learning is known to be hindered by the
need for an extensive feature and reward engi-
neering, especially under the unknown dynamics.
Therefore, inverse reinforcement learning (IRL)
has been proposed to infer expert’s reward func-
tion. Previous IRL approaches include maximum
margin approaches (Abbeel and Ng, 2004; Ratliff
et al., 2006) and probabilistic approaches (Ziebart,
2010; Ziebart et al., 2008). Recently, adversarial
inverse reinforcement learning methods provide
an efﬁcient and scalable promise for automatic re-
ward acquisition (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017; Henderson et al., 2017).
These approaches utilize the connection between
IRL and energy-based model and associate every
data with a scalar energy value by using Boltz-
mann distribution pθ(x) ∝ exp(−Eθ(x)).
In-
spired by these methods, we propose a practical
AREL approach for visual storytelling to uncover
a robust reward function from human demonstra-
tions and thus help produce human-like stories.

3 Our Approach

3.1 Problem Statement

Here we consider the task of visual storytelling,
whose objective is to output a word sequence W =
(w1, w1, · · · , wT ), wt ∈ V given an input image
stream of 5 ordered images I = (I1, I2, · · · , I5),
where V is the vocabulary of all output token.
We formulate the generation as a markov deci-
sion process and design a reinforcement learning
framework to tackle it. As described in Figure 2,
our AREL framework is mainly composed of two
modules: a policy model πβ(W ) and a reward
model Rθ(W ). The policy model takes an image
sequence I as the input and performs sequential
actions (choosing words w from the vocabulary V)
to form a narrative story W . The reward model

Figure 3: Overview of the policy model. The vi-
sual encoder is a bidirectional GRU, which en-
codes the high-level visual features extracted from
the input images. Its outputs are then fed into the
RNN decoders to generate sentences in parallel.
Finally, we concatenate all the generated sentences
as a full story. Note that the ﬁve decoders share the
same weights.

is optimized by the adversarial objective (see Sec-
tion 3.3) and aims at deriving a human-like reward
from both human-annotated stories and sampled
predictions.

3.2 Model

Policy Model As is shown in Figure 3, the pol-
icy model is a CNN-RNN architecture. We ﬁst
feed the photo stream I = (I1, · · · , I5) into a
pretrained CNN and extract their high-level image
features. We then employ a visual encoder to fur-
ther encode the image features as context vectors
−→
hi = [
hi ]. The visual encoder is a bidirectional
gated recurrent units (GRU).

←−
hi ;

In the decoding stage, we feed each context vec-
tor hi into a GRU-RNN decoder to generate a sub-
story Wi. Formally, the generation process can be
written as:

t = GRU(si
t−1, [wi
si
1:t−1) = sof tmax(Wssi

t−1, hi]) ,
t + bs) ,

(1)

(2)

πβ(wi

t|wi

where si
t denotes the t-th hidden state of i-th de-
coder. We concatenate the previous token wi
t−1
and the context vector hi as the input. Ws and
bs are the projection matrix and bias, which out-
put a probability distribution over the whole vo-
cabulary V. Eventually, the ﬁnal story W is the
concatenation of the sub-stories Wi. β denotes all
the parameters of the encoder, the decoder, and the
output layer.

Figure 4: Overview of the reward model. Our re-
ward model is a CNN-based architecture, which
utilizes convolution kernels with size 2, 3 and 4
to extract bigram, trigram and 4-gram representa-
tions from the input sequence embeddings. Once
the sentence representation is learned, it will be
concatenated with the visual representation of the
input image, and then be fed into the ﬁnal FC layer
to obtain the reward.

Reward Model The reward model Rθ(W ) is a
CNN-based architecture (see Figure 4). Instead of
giving an overall score for the whole story, we ap-
ply the reward model to different story parts (sub-
stories) Wi and compute partial rewards, where
i = 1, · · · , 5. We observe that the partial rewards
are more ﬁne-grained and can provide better guid-
ance for the policy model.

We ﬁrst query the word embeddings of the sub-
story (one sentence in most cases). Next, multi-
ple convolutional layers with different kernel sizes
are used to extract the n-grams features, which
are then projected into the sentence-level repre-
sentation space by pooling layers (the design here
In addition to the
is inspired by Kim (2014)).
textual features, evaluating the quality of a story
should also consider the image features for rele-
vance. Therefore, we then combine the sentence
representation with the visual feature of the input
image through concatenation and feed them into
the ﬁnal fully connected decision layer.
In the
end, the reward model outputs an estimated reward
value Rθ(W ). The process can be written in for-
mula:

Rθ(W ) = φ(Wr(fconv(W ) + WiICN N ) + br),

(3)

where φ denotes the non-linear projection func-
tion, Wr, br denote the weight and bias in the
output layer, and fconv denotes the operations in
CNN. ICN N is the high-level visual feature ex-
tracted from the image, and Wi projects it into the

sentence representation space. θ includes all the
parameters above.

3.3 Learning

Reward Boltzmann Distribution In order to
associate story distribution with reward function,
we apply EBM to deﬁne a Reward Boltzmann dis-
tribution:

Algorithm 1 The AREL Algorithm.

1: for episode ← 1 to N do
2:

collect story W by executing policy πθ
if Train-Reward then
θ ← θ − η × ∂Jθ
else if Train-Policy then

∂θ (see Equation 9)

collect story ˜W from empirical pe
β ← β − η × ∂Jβ
∂β (see Equation 9)

3:

4:

5:

6:

7:

pθ(W ) =

exp(Rθ(W ))
Zθ

,

(4)

end if

8:
9: end for

Where W is the word sequence of the story and
pθ(W ) is the approximate data distribution, and
Zθ = (cid:80)
exp(Rθ(W )) denotes the partition func-
W
tion. According to the energy-based model (Le-
Cun et al., 2006), the optimal reward function
R∗(W ) is achieved when the Reward-Boltzmann
distribution equals to the “real” data distribution
pθ(W ) = p∗(W ).

Adversarial Reward Learning We ﬁrst intro-
1(W ∈D)
duce an empirical distribution pe(W ) =
|D|
to represent the empirical distribution of the train-
ing data, where D denotes the dataset with |D| sto-
ries and 1 denotes an indicator function. We use
this empirical distribution as the “good” examples,
which provides the evidence for the reward func-
tion to learn from.

In order to approximate the Reward Boltzmann
distribution towards the “real” data distribution
p∗(W ), we design a min-max two-player game,
where the Reward Boltzmann distribution pθ aims
at maximizing the its similarity with empirical
distribution pe while minimizing that with the
“faked” data generated from policy model πβ. On
the contrary, the policy distribution πβ tries to
maximize its similarity with the Boltzmann dis-
tribution pθ. Formally, the adversarial objective
function is deﬁned as
max
β

KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W )) .

min
θ

We further decompose it into two parts. First,
because the objective Jβ of the story genera-
tion policy is to maximize its similarity with
the Boltzmann distribution pθ, the optimal policy
that minimizes KL-divergence is thus π(W ) ∼
exp(Rθ(W )), meaning if Rθ is optimal, the op-
timal πβ = π∗. In formula,

Jβ = − KL(πβ(W )||pθ(W ))

=

E
W ∼πβ (W )

[Rθ(W )] − log Zθ + H(πβ(W )) ,

(6)

where H denotes the entropy of the policy model.
On the other hand, the objective Jθ of the re-
ward function is to distinguish between human-
annotated stories and machine-generated stories.
Hence it is trying to minimize the KL-divergence
with the empirical distribution pe and maximize
the KL-divergence with the approximated policy
distribution πβ:

Jθ =KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W ))

(cid:88)

=

[pe(W )Rθ(W ) − πβ(W )Rθ(W )]

W
+ log Zθ − log Zθ − H(pe) + H(πβ) ,

(7)

Since H(πβ) and H(pe) are irrelevant to θ, we
denote them as constant C. It is also worth not-
ing that with negative sampling in the optimization
of the KL-divergence, the computation of the in-
tractable partition function Zθ is bypassed. There-
fore, the objective Jθ can be further derived as

Jθ =

E
W ∼pe(W )

[Rθ(W )] −

E
W ∼πβ (W )

[Rθ(W )] + C . (8)

Here we propose to use stochastic gradient de-
scent to optimize these two models alternately.
Formally, the gradients can be written as

(5)

∂Jθ
∂θ

∂Jβ
∂β

=

E
[
W ∼pe(W )

∂Rθ(W )
∂θ

] −

E
[
W ∼πβ (W )

∂Rθ(W )
∂θ

] ,

=

E
W ∼πβ (W )

(Rθ(W ) − log πβ(W ) − b)

∂ log πβ(W )
∂β

,

(9)

where b is the estimated baseline to reduce vari-
ance during REINFORCE training.

Training & Testing As described in Algo-
rithm 1, we introduce an alternating algorithm to
train these two models using stochastic gradient
descent. During testing, the policy model is used
with beam search to produce the story.

4 Experiments and Analysis

4.1 Experimental Setup

VIST Dataset The VIST dataset (Huang et al.,
2016) is the ﬁrst dataset for sequential vision-to-
language tasks including visual storytelling, which
consists of 10,117 Flickr albums with 210,819
unique photos.
In this paper, we mainly evalu-
ate our AREL method on this dataset. After ﬁlter-
ing the broken images2, there are 40,098 training,
4,988 validation, and 5,050 testing samples. Each
sample contains one story that describes 5 selected
images from a photo album (mostly one sentence
per image). And the same album is paired with 5
different stories as references. In our experiments,
we used the same split settings as in (Huang et al.,
2016; Yu et al., 2017b) for a fair comparison. Dur-
ing our experiments, we apply two kinds of non-
linear functions φ for the discriminator, namely
SoftSign function (f (x) = x
1+|x| ) and Hyper-
bolic function (f (x) = sinhx
coshx ). We found that
unbounded non-linear functions like ReLU func-
tion (Glorot et al., 2011) will lead to severe vibra-
tions and instabilities during training, therefore we
resort to the bounded functions.

Evaluation Metrics
In order to comprehen-
sively evaluate our method on storytelling dataset,
we adopt both the automatic metrics and human
evaluation as our criterion. Four diverse automatic
metrics are used in our experiments: BLEU, ME-
TEOR, ROUGE-L, and CIDEr. We utilize the
open source evaluation code3 used in (Yu et al.,
2017b). For human evaluation, we employ the
Amazon Mechanical Turk to perform two kinds of
user studies (see Section 4.3 for more details).

employ

Training Details We
pretrained
ResNet-152 model (He et al., 2016) to extract
image features from the photostream. We built a
vocabulary of size 9,837 to include words appear-
ing more than three times in the training set. More
training details can be found at Appendix B.

4.2 Automatic Evaluation

In this section, we compare our AREL method
with the state-of-the-art methods as well as stan-
dard reinforcement learning algorithms on auto-

2There are only 3 (out of 21,075) broken images in the
test set, which basically has no inﬂuence on the ﬁnal results.
Moreover, Yu et al. (2017b) also removed the 3 pictures, so it
is a fair comparison.

3https://github.com/lichengunc/vist_eval

Method

B-1 B-2 B-3 B-4 M R

C

Huang et al.
Yu et al.
XE-ss
GAN
AREL-s-50
AREL-t-50
AREL-s-100
AREL-t-100

-
-

-
-

-
21.0

-
-
31.4
-
34.1 29.5 7.5
-
62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.8 38.8 23.0 14.0 35.0 29.5 9.0
62.9 38.4 22.7 14.0 34.9 29.4 9.1
63.4 39.0 23.1 14.1 35.2 29.6 9.5
64.0 38.6 22.3 13.2 35.1 29.3 9.6
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 1: Automatic evaluation on the VIST
dataset. We report BLEU (B), METEOR (M),
ROUGH-L (R), and CIDEr (C) scores of the
SOTA systems and the models we implemented,
including XE-ss, GAN and AREL. AREL-s-N de-
notes AREL models with SoftSign as output acti-
vation and alternate frequency as N, while AREL-
t-N denoting AREL models with Hyperbolic as the
output activation (N = 50 or 100).

matic evaluation metrics. Then we further discuss
the limitations of the hand-crafted metrics on eval-
uating human-like stories.

Comparison with SOTA on Automatic Metrics
In Table 1, we compare our method with Huang
et al. (2016) and Yu et al. (2017b), which report
achieving best-known results on the VIST dataset.
We ﬁrst implement a strong baseline model (XE-
ss), which share the same architecture with our
policy model but is trained with cross-entropy loss
and scheduled sampling. Besides, we adopt the
traditional generative adversarial training for com-
parison (GAN). As shown in Table 1, our XE-
ss model already outperforms the best-known re-
sults on the VIST dataset, and the GAN model can
bring a performance boost. We then use the XE-
ss model to initialize our policy model and further
train it with AREL. Evidently, our AREL model
performs the best and achieves the new state-of-
the-art results across all metrics.

But, compared with the XE-ss model, the per-
formance gain is minor, especially on METEOR
and ROUGE-L scores. However, in Sec. 4.3, the
extensive human evaluation has indicated that our
AREL framework brings a signiﬁcant improve-
ment on generating human-like stories over the
XE-ss model. The inconsistency of automatic
evaluation and human evaluation lead to a suspect
that these hand-crafted metrics lack the ability to
fully evaluate stories’ quality due to the compli-
cated characteristics of the stories. Therefore, we
conduct experiments to analyze and discuss the

Method

B-1 B-2 B-3 B-4 M R

C

XE-ss
BLEU-RL
METEOR-RL
ROUGE-RL
CIDEr-RL
AREL (best)

62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.1 38.0 22.6 13.9 34.6 29.0 8.9
40.2 30.0 1.2
68.1 35.0 15.4
6.8
27.0 33.8
58.1 18.5
0
0
1.6
61.9 37.8 22.5 13.8 34.9 29.7 8.1
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 2: Comparison with different RL mod-
els with different metric scores as the rewards.
We report the average scores of the AREL mod-
els as AREL (avg). Although METEOR-RL and
ROUGE-RL models achieve the highest scores
on their own metrics, the underlined scores are
severely damaged. Actually, they are gaming their
own metrics with nonsense sentences.

defects of the automatic metrics in section 4.2.

Limitations of Automatic Metrics String-
match-based automatic metrics are not perfect
and fail to evaluate some semantic characteristics
of the stories (e.g. expressiveness and coherence).
In order to conﬁrm our conjecture, we utilize
automatic metrics as rewards to reinforce the
model with policy gradient.
The quantitative
results are demonstrated in Table 1.

Apparently, METEOR-RL and ROUGE-RL are
severely ill-posed: they obtain the highest scores
on their own metrics but damage the other met-
rics severely. We observe that these models are
actually overﬁtting to a given metric while losing
the overall coherence and semantical correctness.
Same as METEOR score, there is also an adver-
sarial example for ROUGE-L4, which is nonsense
but achieves an average ROUGE-L score of 33.8.
Besides, as can be seen in Table 1, after rein-
forced training, BLEU-RL and CIDEr-RL do not
bring a consistent improvement over the XE-ss
model. We plot the histogram distributions of both
BLEU-3 and CIDEr scores on the test set in Fig-
ure 5. An interesting fact is that there are a large
number of samples with nearly zero score on both
metrics. However, we observed those “zero-score”
samples are not pointless results; instead, lots of
them make sense and deserve a better score than
zero. Here is a “zero-score” example on BLEU-3:

I had a great time at the restaurant today.
The food was delicious. I had a lot of food.
The food was delicious. I had a great time.

4An adversarial example for ROUGE-L: we the was a .
and to the . we the was a . and to the . we the was a . and to
the . we the was a . and to the . we the was a . and to the .

Method
XE-ss
BLEU-RL
CIDEr-RL
GAN
AREL

Win
Lose
22.4% 71.7%
23.4% 67.9%
13.8% 80.3%
34.3% 60.5%
38.4% 54.2%

Unsure
5.9%
8.7%
5.9%
5.2%
7.4%

Table 3: Turing test results.

The corresponding reference is

The table of food was a pleasure to see!
Our food is both nutritious and beautiful!
Our chicken was especially tasty! We love
greens as they taste great and are healthy!
The fruit was a colorful display that tanta-
lized our palette.

Although the prediction is not as good as the ref-
erence, it is actually coherent and relevant to the
theme “food and eating”, which showcases the de-
feats of using BLEU and CIDEr scores as a reward
for RL training.

Moreover, we compare the human evaluation
scores with these two metric scores in Figure 5.
Noticeably, both BLEU-3 and CIDEr have a poor
correlation with the human evaluation scores.
Their distributions are more biased and thus can-
not fully reﬂect the quality of the generated sto-
ries. In terms of BLEU, it is extremely hard for
machines to produce the exact 3-gram or 4-gram
matching, so the scores are too low to provide use-
ful guidance. CIDEr measures the similarity of a
sentence to the majority of the references. How-
ever, the references to the same image sequence
are photostream different from each other, so the
score is very low and not suitable for this task. In
contrast, our AREL framework can lean a more
robust reward function from human-annotated sto-
ries, which is able to provide better guidance to
the policy and thus improves its performances over
different metrics.

Visualization of The Learned Rewards
In Fig-
ure 6, we visualize the learned reward function for
both ground truth and generated stories. Evidently,
the AREL model is able to learn a smoother re-
ward function that can distinguish the generated
stories from human annotations. In other words,
the learned reward function is more in line with
human perception and thus can encourage the
model to explore more diverse language styles and
expressions.

Figure 5: Metric score distributions. We plot the histogram distributions of BLEU-3 and CIDEr scores
on the test set, as well as the human evaluation score distribution on the test samples. We use the Turing
test results to calculate the human evaluation scores (see Section 4.3). Basically, 0.2 score is given if the
generated story wins the Turing test, 0.1 for tie, and 0 if losing. Each sample has 5 scores from 5 judges,
and we use the sum as the human evaluation score, so it is in the range [0, 1].

Choice (%)
Relevance
Expressiveness
Concreteness

AREL vs XE-ss
AREL XE-ss Tie
61.7
13.2
66.1
15.1
63.9
15.8

25.1
18.8
20.3

AREL vs BLEU-RL
AREL BLEU-RL Tie
55.8
16.3
59.1
14.5
60.1
13.6

27.9
26.4
26.3

AREL vs CIDEr-RL
AREL CIDEr-RL Tie
56.1
15.7
59.1
14.3
59.5
15.9

28.2
26.6
24.6

AREL vs GAN
AREL GAN Tie
52.9
35.8 11.3
48.5
32.2 19.3
49.8
35.8 14.4

Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model
in generating more human-like stories than the SOTA methods.

is prone to the vanishing gradient issue. Analyti-
cally, our method does not suffer from these two
common issues and thus is able converge to op-
timum solutions more easily. From Table 1 we
can observe slight gains of using AREL over GAN
with automatic metrics, but we further deploy hu-
man evaluation for a better comparison.

4.3 Human Evaluation

Automatic metrics cannot fully evaluate the ca-
pability of our AREL method. Therefore, we
perform two different kinds of human evaluation
studies on Amazon Mechanical Turk: Turing test
and pairwise human evaluation. For both tasks,
we use 150 stories (750 images) sampled from the
test set, each assigned to 5 workers to eliminate
human variance. We batch six items as one assign-
ment and insert an additional assignment as a san-
ity check. Besides, the order of the options within
each item is shufﬂed to make a fair comparison.

Turing Test We ﬁrst conduct ﬁve indepen-
dent Turing tests for XE-ss, BLEU-RL, CIDEr-
RL, GAN, and AREL models, during which the
worker is given one human-annotated sample and
one machine-generated sample, and needs to de-
cide which is human-annotated. As shown in Ta-
ble 3, our AREL model signiﬁcantly outperforms
all the other baseline models in the Turing test: it
has much more chances to fool AMT worker (the

Figure 6: Visualization of the learned rewards on
both the ground-truth stories and the stories gen-
erated by our AREL model. The generated sto-
ries are receiving lower averaged scores than the
human-annotated ones.

Comparison with GAN We here compare our
method with vanilla GAN (Goodfellow et al.,
2014), whose update rules for the generator can
be generally classiﬁed into two categories. We
demonstrate their corresponding objectives and
ours as follows:

GAN 1 : Jβ = E

[− log Rθ(W )] ,

GAN 2 :

ours :

W ∼pβ

Jβ = E

W ∼pβ

Jβ = E

W ∼pβ

[log(1 − Rθ(W ))] ,

[−Rθ(W )] .

As discussed in Arjovsky et al. (2017), GAN 1 is
prone to the unstable gradient issue and GAN 2

Figure 7: Qualitative comparison example with XE-ss. The direct comparison votes (AREL:XE-ss:Tie)
were 5:0:0 on Relevance, 4:0:1 on Expressiveness, and 5:0:0 on Concreteness.

ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN
= 45.8%:28.3%:32.1%:19.7%:39.5%), which con-
ﬁrms the superiority of our AREL framework in
generating human-like stories. Unlike automatic
metric evaluation, the Turing test has indicated
a much larger margin between AREL and other
competing algorithms. Thus, we empirically con-
ﬁrm that metrics are not perfect in evaluating many
implicit semantic properties of natural language.
Besides, the Turing test of our AREL model re-
veals that nearly half of the workers are fooled by
our machine generation, indicating a preliminary
success toward generating human-like stories.

Pairwise Comparison In order to have a clear
comparison with competing algorithms with re-
spect to different semantic features of the sto-
ries, we further perform four pairwise compar-
ison tests: AREL vs XE-ss/BLEU-RL/CIDEr-
RL/GAN. For each photostream, the worker is
presented with two generated stories and asked to
make decisions from the three aspects: relevance5,
expressiveness6 and concreteness7. This head-to-
head compete is designed to help us understand in
what aspect our model outperforms the competing
algorithms, which is displayed in Table 4.

Consistently on all the three comparisons, a
large majority of the AREL stories trumps the
competing systems with respect to their relevance,

5Relevance: the story accurately describes what is hap-

pening in the image sequence and covers the main objects.

6Expressiveness: coherence, grammatically and semanti-

cally correct, no repetition, expressive language style.

7Concreteness: the story should narrate concretely what

is in the image rather than giving very general descriptions.

expressiveness, and concreteness. Therefore, it
empirically conﬁrms that our generated stories are
more relevant to the image sequences, more coher-
ent and concrete than the other algorithms, which
however is not explicitly reﬂected by the auto-
matic metric evaluation.

4.4 Qualitative Analysis

Figure 7 gives a qualitative comparison example
between AREL and XE-ss models. Looking at the
individual sentences, it is obvious that our results
are more grammatically and semantically correct.
Then connecting the sentences together, we ob-
serve that the AREL story is more coherent and
describes the photo stream more accurately. Thus,
our AREL model signiﬁcantly surpasses the XE-
ss model on all the three aspects of the qualitative
example. Besides, it won the Turing test (3 out 5
AMT workers think the AREL story is created by
a human). In the appendix, we also show a nega-
tive case that fails the Turing test.

5 Conclusion

In this paper, we not only introduce a novel ad-
versarial reward learning algorithm to generate
more human-like stories given image sequences,
but also empirically analyze the limitations of the
automatic metrics for story evaluation. We believe
there are still lots of improvement space in the
narrative paragraph generation tasks, like how to
better simulate human imagination to create more
vivid and diversiﬁed stories.

Acknowledgment

We thank Adobe Research for supporting our lan-
guage and vision research. We would also like
to thank Licheng Yu for clarifying the details
of his paper and the anonymous reviewers for
their thoughtful comments. This research was
sponsored in part by the Army Research Labora-
tory under cooperative agreements W911NF09-2-
0053. The views and conclusions contained herein
are those of the authors and should not be inter-
preted as representing the ofﬁcial policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References

Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international confer-
ence on Machine learning, page 1. ACM.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
arXiv preprint

tou. 2017. Wasserstein gan.
arXiv:1701.07875.

Wenhu Chen, Aur´elien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.

Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo,
Jing Dai, and Chang-Tien Lu. 2017. Multi-
modal storytelling via generative adversarial imita-
In Proceedings of the Twenty-Sixth
tion learning.
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-17, pages 3967–3973.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
In The IEEE Interna-
tions via a conditional gan.
tional Conference on Computer Vision (ICCV).

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. arXiv preprint
arXiv:1611.03852.

Justin Fu, Katie Luo, and Sergey Levine. 2017.
in-
arXiv preprint

rewards with adversarial

learning.

Learning robust
verse reinforcement
arXiv:1710.11248.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks. In Pro-
ceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 315–
323.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Elia Bruni and Raquel Fern´andez. 2017. Adversarial
evaluation for open-domain dialogue generation. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 284–288.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.

Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon,
David Meger, Joelle Pineau, and Doina Precup.
2017. Optiongan: Learning joint reward-policy op-
tions using generative adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1709.06683.

Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui
Zhang, Mu Li, and Ming Zhou. 2018. Generative
bridging network for neural sequence prediction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1706–1715. Associ-
ation for Computational Linguistics.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neural
Information Processing Systems, pages 4565–4573.

Ting-Hao K. Huang,

Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual

storytelling.
North American Chapter of
Computational Linguistics (NAACL 2016).

In 15th Annual Conference of the
the Association for

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,
and F Huang. 2006. A tutorial on energy-based
learning. Predicting structured data, 1(0).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. arXiv
preprint arXiv:1708.07149.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and
Tao Mei. 2018a. Show, reward and tell: Automatic
generation of narrative paragraph from photo stream
by adversarial training. AAAI.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video caption-
ing via hierarchical reinforcement learning. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xin Wang, Yuan-Fang Wang, and William Yang Wang.
2018c. Watch, listen, and describe: Globally and lo-
cally aligned cross-modal attentions for video cap-
tioning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 795–801.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
In Proceedings of the IEEE
video and language.
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in Neural Information Processing Sys-
tems, pages 73–81.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017a. Seqgan: Sequence generative adversarial
In AAAI, pages 2852–
nets with policy gradient.
2858.

Licheng Yu, Mohit Bansal, and Tamara Berg. 2017b.
Hierarchically-attentive rnn for album summariza-
In Proceedings of the 2017
tion and storytelling.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 966–971, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
In AAAI, volume 8, pages
reinforcement learning.
1433–1438. Chicago, IL, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning.
In
Proceedings of the 23rd international conference on
Machine learning, pages 729–736. ACM.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceeding of IEEE conference on Computer Vision and
Pattern Recognition (CVPR).

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
In The
sequence training for image captioning.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Appendix

A Error Analysis

Failure Case in Turing Test
In Figure 8, we
presented a negative example that failed the Turing
test (4 out of 5 made the correct decision). Com-
pared with the human-generated story, our AREL
story lacked emotion and imagination and thus can
be easily distinguished. For example, the real hu-
man gave the band a nickname “very loud band”
and told a more amusing story. Though we have
made encouraging progress on generating human-
like stories, further research of creating diversiﬁed
stories is still needed.

Data Bias From the experiments, we observe
that there exist some severe data bias issues in
the VIST dataset, such as gender bias and event
bias.
In the training set, the ratio of male and
female’s appearances is 2.06:1, and it is 2.16:1
in the test set.
the models aggravate the gender
bias to 3.44:1. Besides, because all the images
are collected from Flickr, there is also an event
bias issue. We count three most frequent events:
party, wedding, and graduation, whose ratios are
6.51:2.36:1 on the training set and 4.54:2.42:1 on
the test set. However, their ratio on the testing
results is 10.69:2.22:1. Clearly, the models tend
to magnify the inﬂuence of the largest majority.
These bias issues remain to be studied for future
work.

B Training Details

Our model is implemented on PyTorch and con-
sists of two parts – a policy model and a reward
model. The policy model is implemented with
a multiple-RNN architecture. Each RNN model
is responsible for generating a sub-story for each
photo in the stream. But the weights are tied
to minimize the memory consumption. The im-
age features are extracted from the pre-trained
ResNet-152 model8. The visual encoder receives
the ResNet-152 features and uses recurrent neu-
ral network to understand the temporal dynamics
and represents them as hidden state vectors, which
is further fed into the decoder to generate stories.
The reward model is based on convolutional neu-
ral network and uses convolution kernels to extract
semantic features for prediction. Here we give the
detailed description of our system:

8https://github.com/KaimingHe/

deep-residual-networks

• Visual Encoder:

the visual encoder is a bi-
directional GRU model with hidden dimen-
sion of 256 for each direction. we concate-
nate the bi-directional states and form a 512
dimension vector for the story generator. The
input album is composed of ﬁve images, and
each image is used as separate input to differ-
ent RNN decoders.

• Decoder: The decoder is a single-layer GRU
model with hidden dimension of 512. The
recurrent decoder model receives the output
from the visual encoder as the ﬁrst input, and
then at the following time steps, it receives
the last predicted token as input or uses the
ground truth as input. During scheduled sam-
pling, we use a sampling probability to de-
cide which action to take.

• Reward Model: we use a convolutional neu-
ral network to extract n-gram features from
the story embedding and stretch them into a
ﬂattened vector. The embedding size of input
story is 128, and the ﬁlter dimension of CNN
is also 128. Here we use three kernels with
window size 2, 3, 4, each with a stride size
of 1. We use a pooling size of 2 to shrink the
extracted outputs and ﬂatten them as a vector.
Finally, we project this vector into a single
cell indicating the predicted reward value.

During training, we ﬁrst pre-train a schedule-
sampling model with a batch size of 64 with
NVIDIA Titan X GPU. The warm-up process
takes roughly 5-10 hours, and then we select the
best model to initialize our AREL policy model.
Finally, we use alternating training strategy to op-
timize both the policy model and the reward model
with a learning rate of 2e-4 using Adam optimiza-
tion algorithm. During test time, we use a beam
size of 3 to approximate the whole search space,
we force the beam search to proceed more than 5
steps and no more than 110 steps. Once we reach
the EOS token, the algorithm stops and we com-
pare the results with human-annotated corpus us-
ing 4 different automatic evaluation metrics.

C Amazon Mechanical Turk

We used AMT to perform two surveys, one picks
a more human-like story. We asked the worker
to answers 8 questions within 30 minutes, and
we pay 5 workers to work on the same sheet to

Figure 8: Failure case in Turing test. 4 out of 5 workers correctly recognized the human-created story
and 1 person mistakenly chose AREL story.

eliminate human-to-human bias. Here we demon-
strate the Turing survey form in Figure 9. Besides,
we also perform a head-to-head comparison with
other algorithms, we demonstrate the survey form
in Figure 10.

Figure 9: Turing Survey Form

Figure 10: Pairwise Comparison Form

No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling

Xin Wang∗, Wenhu Chen∗, Yuan-Fang Wang , William Yang Wang
University of California, Santa Barbara
{xwang,wenhuchen,yfwang,william}@cs.ucsb.edu

8
1
0
2
 
l
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
2
v
0
6
1
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Though impressive results have been
achieved in visual captioning,
the task
of generating abstract stories from photo
streams is still a little-tapped problem.
Different from captions, stories have more
expressive language styles and contain
many imaginary concepts that do not ap-
pear in the images. Thus it poses chal-
lenges to behavioral cloning algorithms.
Furthermore, due to the limitations of au-
tomatic metrics on evaluating story qual-
ity, reinforcement learning methods with
hand-crafted rewards also face difﬁcul-
ties in gaining an overall performance
boost. Therefore, we propose an Adver-
sarial REward Learning (AREL) frame-
work to learn an implicit reward function
from human demonstrations, and then op-
timize policy search with the learned re-
ward function. Though automatic eval-
uation indicates slight performance boost
over state-of-the-art (SOTA) methods in
cloning expert behaviors, human evalua-
tion shows that our approach achieves sig-
niﬁcant improvement in generating more
human-like stories than SOTA systems.1

1

Introduction

Recently, increasing attention has been focused
on visual captioning (Chen et al., 2015, 2016; Xu
et al., 2016; Wang et al., 2018c), which aims at
describing the content of an image or a video.
Though it has achieved impressive results, its ca-
pability of performing human-like understanding
is still restrictive. To further investigate machine’s

∗ Equal contribution
1Code
released

is

littlekobe/AREL

at https://github.com/

Figure 1: An example of visual storytelling and
visual captioning. Both captions and stories are
shown here: each image is captioned with one sen-
tence, and we also demonstrate two diversiﬁed sto-
ries that match the same image sequence.

capabilities in understanding more complicated vi-
sual scenarios and composing more structured ex-
pressions, visual storytelling (Huang et al., 2016)
has been proposed. Visual captioning is aimed at
depicting the concrete content of the images, and
its expression style is rather simple. In contrast,
visual storytelling goes one step further: it sum-
marizes the idea of a photo stream and tells a story
about it. Figure 1 shows an example of visual
captioning and visual storytelling. We have ob-
served that stories contain rich emotions (excited,
happy, not want) and imagination (siblings, par-
ents, school, car). It, therefore, requires the capa-
bility to associate with concepts that do not explic-
itly appear in the images. Moreover, stories are
more subjective, so there barely exists standard

templates for storytelling. As shown in Figure 1,
the same photo stream can be paired with diverse
stories, different from each other. This heavily in-
creases the evaluation difﬁculty.

So far, prior work for visual storytelling (Huang
et al., 2016; Yu et al., 2017b) is mainly inspired
by the success of visual captioning. Nevertheless,
because these methods are trained by maximizing
the likelihood of the observed data pairs, they are
restricted to generate simple and plain description
with limited expressive patterns. In order to cope
with the challenges and produce more human-like
descriptions, Rennie et al. (2017) have proposed
a reinforcement learning framework. However, in
the scenario of visual storytelling, the common re-
inforced captioning methods are facing great chal-
lenges since the hand-crafted rewards based on
string matches are either too biased or too sparse
to drive the policy search. For instance, we used
the METEOR (Banerjee and Lavie, 2005) score
as the reward to reinforce our policy and found
that though the METEOR score is signiﬁcantly
improved, the other scores are severely harmed.
Here we showcase an adversarial example with an
average METEOR score as high as 40.2:

We had a great time to have a lot of the.
They were to be a of the. They were to be in
the. The and it were to be the. The, and it
were to be the.

Apparently, the machine is gaming the metrics.
Conversely, when using some other metrics (e.g.
BLEU, CIDEr) to evaluate the stories, we observe
an opposite behavior: many relevant and coherent
stories are receiving a very low score (nearly zero).
In order to resolve the strong bias brought by
the hand-coded evaluation metrics in RL training
and produce more human-like stories, we propose
an Adversarial REward Learning (AREL) frame-
work for visual storytelling. We draw our inspi-
ration from recent progress in inverse reinforce-
ment learning (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017) and propose the AREL algo-
rithm to learn a more intelligent reward function.
Speciﬁcally, we ﬁrst incorporate a Boltzmann dis-
tribution to associate reward learning with distri-
bution approximation, then design the adversarial
process with two models – a policy model and a
reward model. The policy model performs the
primitive actions and produces the story sequence,
while the reward model is responsible for learning

the implicit reward function from human demon-
strations. The learned reward function would be
employed to optimize the policy in return.

For evaluation, we conduct both automatic met-
rics and human evaluation but observe a poor cor-
relation between them. Particularly, our method
gains slight performance boost over the base-
line systems on automatic metrics; human evalu-
ation, however, indicates signiﬁcant performance
boost. Thus we further discuss the limitations
of the metrics and validate the superiority of our
AREL method in performing more intelligent un-
derstanding of the visual scenes and generating
more human-like stories.

Our main contributions are four-fold:

• We propose an adversarial reward learning
framework and apply it to boost visual story
generation.

• We evaluate our approach on the Visual
Storytelling (VIST) dataset and achieve the
state-of-the-art results on automatic metrics.

• We empirically demonstrate that automatic
metrics are not perfect for either training or
evaluation.

• We design and perform a comprehensive
human evaluation via Amazon Mechanical
Turk, which demonstrates the superiority of
the generated stories of our method on rele-
vance, expressiveness, and concreteness.

2 Related Work

Visual Storytelling Visual storytelling is the
task of generating a narrative story from a photo
stream, which requires a deeper understanding
of the event ﬂow in the stream. Park and Kim
(2015) has done some pioneering research on sto-
rytelling. Chen et al. (2017) proposed a multi-
modal approach for storyline generation to pro-
duce a stream of entities instead of human-like de-
scriptions. Recently, a more sophisticated dataset
for visual storytelling (VIST) has been released
to explore a more human-like understanding of
grounded stories (Huang et al., 2016). Yu et al.
(2017b) proposes a multi-task learning algorithm
for both album summarization and paragraph gen-
eration, achieving the best results on the VIST
dataset. But these methods are still based on be-
havioral cloning and lack the ability to generate
more structured stories.

Reinforcement Learning in Sequence Genera-
tion Recently, reinforcement learning (RL) has
gained its popularity in many sequence generation
tasks such as machine translation (Bahdanau et al.,
2016), visual captioning (Ren et al., 2017; Wang
et al., 2018b), summarization (Paulus et al., 2017;
Chen et al., 2018), etc. The common wisdom of
using RL is to view generating a word as an ac-
tion and aim at maximizing the expected return
by optimizing its policy. As pointed in (Ranzato
et al., 2015), traditional maximum likelihood al-
gorithm is prone to exposure bias and label bias,
while the RL agent exposes the generative model
to its own distribution and thus can perform bet-
ter. But these works usually utilize hand-crafted
metric scores as the reward to optimize the model,
which fails to learn more implicit semantics due to
the limitations of automatic metrics.

Rethinking Automatic Metrics Automatic
metrics, including BLEU (Papineni et al., 2002),
CIDEr (Vedantam et al., 2015), METEOR (Baner-
jee and Lavie, 2005), and ROUGE (Lin, 2004),
have been widely applied to the sequence gener-
ation tasks. Using automatic metrics can ensure
rapid prototyping and testing new models with
fewer expensive human evaluation. However, they
have been criticized to be biased and correlate
poorly with human judgments, especially in many
generative tasks like response generation (Lowe
et al., 2017; Liu et al., 2016), dialogue sys-
tem (Bruni and Fern´andez, 2017) and machine
translation (Callison-Burch et al., 2006). The
naive overlap-counting methods are not able
to reﬂect many semantic properties in natural
language, such as coherence, expressiveness, etc.

Generative Adversarial Network Generative
adversarial network (GAN) (Goodfellow et al.,
2014) is a very popular approach for estimating
intractable probabilities, which sidestep the difﬁ-
culty by alternately training two models to play a
min-max two-player game:

min
D

max
G

E
x∼pdata

[log D(x)] + E
z∼pz

[log D(G(z))] ,

where G is the generator and D is the discrimina-
tor, and z is the latent variable. Recently, GAN
has quickly been adopted to tackle discrete prob-
lems (Yu et al., 2017a; Dai et al., 2017; Wang et al.,
2018a). The basic idea is to use Monte Carlo pol-
icy gradient estimation (Williams, 1992) to update
the parameters of the generator.

Figure 2: AREL framework for visual storytelling.

Inverse Reinforcement Learning Reinforce-
ment learning is known to be hindered by the
need for an extensive feature and reward engi-
neering, especially under the unknown dynamics.
Therefore, inverse reinforcement learning (IRL)
has been proposed to infer expert’s reward func-
tion. Previous IRL approaches include maximum
margin approaches (Abbeel and Ng, 2004; Ratliff
et al., 2006) and probabilistic approaches (Ziebart,
2010; Ziebart et al., 2008). Recently, adversarial
inverse reinforcement learning methods provide
an efﬁcient and scalable promise for automatic re-
ward acquisition (Ho and Ermon, 2016; Finn et al.,
2016; Fu et al., 2017; Henderson et al., 2017).
These approaches utilize the connection between
IRL and energy-based model and associate every
data with a scalar energy value by using Boltz-
mann distribution pθ(x) ∝ exp(−Eθ(x)).
In-
spired by these methods, we propose a practical
AREL approach for visual storytelling to uncover
a robust reward function from human demonstra-
tions and thus help produce human-like stories.

3 Our Approach

3.1 Problem Statement

Here we consider the task of visual storytelling,
whose objective is to output a word sequence W =
(w1, w1, · · · , wT ), wt ∈ V given an input image
stream of 5 ordered images I = (I1, I2, · · · , I5),
where V is the vocabulary of all output token.
We formulate the generation as a markov deci-
sion process and design a reinforcement learning
framework to tackle it. As described in Figure 2,
our AREL framework is mainly composed of two
modules: a policy model πβ(W ) and a reward
model Rθ(W ). The policy model takes an image
sequence I as the input and performs sequential
actions (choosing words w from the vocabulary V)
to form a narrative story W . The reward model

Figure 3: Overview of the policy model. The vi-
sual encoder is a bidirectional GRU, which en-
codes the high-level visual features extracted from
the input images. Its outputs are then fed into the
RNN decoders to generate sentences in parallel.
Finally, we concatenate all the generated sentences
as a full story. Note that the ﬁve decoders share the
same weights.

is optimized by the adversarial objective (see Sec-
tion 3.3) and aims at deriving a human-like reward
from both human-annotated stories and sampled
predictions.

3.2 Model

Policy Model As is shown in Figure 3, the pol-
icy model is a CNN-RNN architecture. We ﬁst
feed the photo stream I = (I1, · · · , I5) into a
pretrained CNN and extract their high-level image
features. We then employ a visual encoder to fur-
ther encode the image features as context vectors
−→
hi = [
hi ]. The visual encoder is a bidirectional
gated recurrent units (GRU).

←−
hi ;

In the decoding stage, we feed each context vec-
tor hi into a GRU-RNN decoder to generate a sub-
story Wi. Formally, the generation process can be
written as:

t = GRU(si
t−1, [wi
si
1:t−1) = sof tmax(Wssi

t−1, hi]) ,
t + bs) ,

(1)

(2)

πβ(wi

t|wi

where si
t denotes the t-th hidden state of i-th de-
coder. We concatenate the previous token wi
t−1
and the context vector hi as the input. Ws and
bs are the projection matrix and bias, which out-
put a probability distribution over the whole vo-
cabulary V. Eventually, the ﬁnal story W is the
concatenation of the sub-stories Wi. β denotes all
the parameters of the encoder, the decoder, and the
output layer.

Figure 4: Overview of the reward model. Our re-
ward model is a CNN-based architecture, which
utilizes convolution kernels with size 2, 3 and 4
to extract bigram, trigram and 4-gram representa-
tions from the input sequence embeddings. Once
the sentence representation is learned, it will be
concatenated with the visual representation of the
input image, and then be fed into the ﬁnal FC layer
to obtain the reward.

Reward Model The reward model Rθ(W ) is a
CNN-based architecture (see Figure 4). Instead of
giving an overall score for the whole story, we ap-
ply the reward model to different story parts (sub-
stories) Wi and compute partial rewards, where
i = 1, · · · , 5. We observe that the partial rewards
are more ﬁne-grained and can provide better guid-
ance for the policy model.

We ﬁrst query the word embeddings of the sub-
story (one sentence in most cases). Next, multi-
ple convolutional layers with different kernel sizes
are used to extract the n-grams features, which
are then projected into the sentence-level repre-
sentation space by pooling layers (the design here
In addition to the
is inspired by Kim (2014)).
textual features, evaluating the quality of a story
should also consider the image features for rele-
vance. Therefore, we then combine the sentence
representation with the visual feature of the input
image through concatenation and feed them into
the ﬁnal fully connected decision layer.
In the
end, the reward model outputs an estimated reward
value Rθ(W ). The process can be written in for-
mula:

Rθ(W ) = φ(Wr(fconv(W ) + WiICN N ) + br),

(3)

where φ denotes the non-linear projection func-
tion, Wr, br denote the weight and bias in the
output layer, and fconv denotes the operations in
CNN. ICN N is the high-level visual feature ex-
tracted from the image, and Wi projects it into the

sentence representation space. θ includes all the
parameters above.

3.3 Learning

Reward Boltzmann Distribution In order to
associate story distribution with reward function,
we apply EBM to deﬁne a Reward Boltzmann dis-
tribution:

Algorithm 1 The AREL Algorithm.

1: for episode ← 1 to N do
2:

collect story W by executing policy πθ
if Train-Reward then
θ ← θ − η × ∂Jθ
else if Train-Policy then

∂θ (see Equation 9)

collect story ˜W from empirical pe
β ← β − η × ∂Jβ
∂β (see Equation 9)

3:

4:

5:

6:

7:

pθ(W ) =

exp(Rθ(W ))
Zθ

,

(4)

end if

8:
9: end for

Where W is the word sequence of the story and
pθ(W ) is the approximate data distribution, and
Zθ = (cid:80)
exp(Rθ(W )) denotes the partition func-
W
tion. According to the energy-based model (Le-
Cun et al., 2006), the optimal reward function
R∗(W ) is achieved when the Reward-Boltzmann
distribution equals to the “real” data distribution
pθ(W ) = p∗(W ).

Adversarial Reward Learning We ﬁrst intro-
1(W ∈D)
duce an empirical distribution pe(W ) =
|D|
to represent the empirical distribution of the train-
ing data, where D denotes the dataset with |D| sto-
ries and 1 denotes an indicator function. We use
this empirical distribution as the “good” examples,
which provides the evidence for the reward func-
tion to learn from.

In order to approximate the Reward Boltzmann
distribution towards the “real” data distribution
p∗(W ), we design a min-max two-player game,
where the Reward Boltzmann distribution pθ aims
at maximizing the its similarity with empirical
distribution pe while minimizing that with the
“faked” data generated from policy model πβ. On
the contrary, the policy distribution πβ tries to
maximize its similarity with the Boltzmann dis-
tribution pθ. Formally, the adversarial objective
function is deﬁned as
max
β

KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W )) .

min
θ

We further decompose it into two parts. First,
because the objective Jβ of the story genera-
tion policy is to maximize its similarity with
the Boltzmann distribution pθ, the optimal policy
that minimizes KL-divergence is thus π(W ) ∼
exp(Rθ(W )), meaning if Rθ is optimal, the op-
timal πβ = π∗. In formula,

Jβ = − KL(πβ(W )||pθ(W ))

=

E
W ∼πβ (W )

[Rθ(W )] − log Zθ + H(πβ(W )) ,

(6)

where H denotes the entropy of the policy model.
On the other hand, the objective Jθ of the re-
ward function is to distinguish between human-
annotated stories and machine-generated stories.
Hence it is trying to minimize the KL-divergence
with the empirical distribution pe and maximize
the KL-divergence with the approximated policy
distribution πβ:

Jθ =KL(pe(W )||pθ(W )) − KL(πβ(W )||pθ(W ))

(cid:88)

=

[pe(W )Rθ(W ) − πβ(W )Rθ(W )]

W
+ log Zθ − log Zθ − H(pe) + H(πβ) ,

(7)

Since H(πβ) and H(pe) are irrelevant to θ, we
denote them as constant C. It is also worth not-
ing that with negative sampling in the optimization
of the KL-divergence, the computation of the in-
tractable partition function Zθ is bypassed. There-
fore, the objective Jθ can be further derived as

Jθ =

E
W ∼pe(W )

[Rθ(W )] −

E
W ∼πβ (W )

[Rθ(W )] + C . (8)

Here we propose to use stochastic gradient de-
scent to optimize these two models alternately.
Formally, the gradients can be written as

(5)

∂Jθ
∂θ

∂Jβ
∂β

=

E
[
W ∼pe(W )

∂Rθ(W )
∂θ

] −

E
[
W ∼πβ (W )

∂Rθ(W )
∂θ

] ,

=

E
W ∼πβ (W )

(Rθ(W ) − log πβ(W ) − b)

∂ log πβ(W )
∂β

,

(9)

where b is the estimated baseline to reduce vari-
ance during REINFORCE training.

Training & Testing As described in Algo-
rithm 1, we introduce an alternating algorithm to
train these two models using stochastic gradient
descent. During testing, the policy model is used
with beam search to produce the story.

4 Experiments and Analysis

4.1 Experimental Setup

VIST Dataset The VIST dataset (Huang et al.,
2016) is the ﬁrst dataset for sequential vision-to-
language tasks including visual storytelling, which
consists of 10,117 Flickr albums with 210,819
unique photos.
In this paper, we mainly evalu-
ate our AREL method on this dataset. After ﬁlter-
ing the broken images2, there are 40,098 training,
4,988 validation, and 5,050 testing samples. Each
sample contains one story that describes 5 selected
images from a photo album (mostly one sentence
per image). And the same album is paired with 5
different stories as references. In our experiments,
we used the same split settings as in (Huang et al.,
2016; Yu et al., 2017b) for a fair comparison. Dur-
ing our experiments, we apply two kinds of non-
linear functions φ for the discriminator, namely
SoftSign function (f (x) = x
1+|x| ) and Hyper-
bolic function (f (x) = sinhx
coshx ). We found that
unbounded non-linear functions like ReLU func-
tion (Glorot et al., 2011) will lead to severe vibra-
tions and instabilities during training, therefore we
resort to the bounded functions.

Evaluation Metrics
In order to comprehen-
sively evaluate our method on storytelling dataset,
we adopt both the automatic metrics and human
evaluation as our criterion. Four diverse automatic
metrics are used in our experiments: BLEU, ME-
TEOR, ROUGE-L, and CIDEr. We utilize the
open source evaluation code3 used in (Yu et al.,
2017b). For human evaluation, we employ the
Amazon Mechanical Turk to perform two kinds of
user studies (see Section 4.3 for more details).

employ

Training Details We
pretrained
ResNet-152 model (He et al., 2016) to extract
image features from the photostream. We built a
vocabulary of size 9,837 to include words appear-
ing more than three times in the training set. More
training details can be found at Appendix B.

4.2 Automatic Evaluation

In this section, we compare our AREL method
with the state-of-the-art methods as well as stan-
dard reinforcement learning algorithms on auto-

2There are only 3 (out of 21,075) broken images in the
test set, which basically has no inﬂuence on the ﬁnal results.
Moreover, Yu et al. (2017b) also removed the 3 pictures, so it
is a fair comparison.

3https://github.com/lichengunc/vist_eval

Method

B-1 B-2 B-3 B-4 M R

C

Huang et al.
Yu et al.
XE-ss
GAN
AREL-s-50
AREL-t-50
AREL-s-100
AREL-t-100

-
-

-
-

-
21.0

-
-
31.4
-
34.1 29.5 7.5
-
62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.8 38.8 23.0 14.0 35.0 29.5 9.0
62.9 38.4 22.7 14.0 34.9 29.4 9.1
63.4 39.0 23.1 14.1 35.2 29.6 9.5
64.0 38.6 22.3 13.2 35.1 29.3 9.6
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 1: Automatic evaluation on the VIST
dataset. We report BLEU (B), METEOR (M),
ROUGH-L (R), and CIDEr (C) scores of the
SOTA systems and the models we implemented,
including XE-ss, GAN and AREL. AREL-s-N de-
notes AREL models with SoftSign as output acti-
vation and alternate frequency as N, while AREL-
t-N denoting AREL models with Hyperbolic as the
output activation (N = 50 or 100).

matic evaluation metrics. Then we further discuss
the limitations of the hand-crafted metrics on eval-
uating human-like stories.

Comparison with SOTA on Automatic Metrics
In Table 1, we compare our method with Huang
et al. (2016) and Yu et al. (2017b), which report
achieving best-known results on the VIST dataset.
We ﬁrst implement a strong baseline model (XE-
ss), which share the same architecture with our
policy model but is trained with cross-entropy loss
and scheduled sampling. Besides, we adopt the
traditional generative adversarial training for com-
parison (GAN). As shown in Table 1, our XE-
ss model already outperforms the best-known re-
sults on the VIST dataset, and the GAN model can
bring a performance boost. We then use the XE-
ss model to initialize our policy model and further
train it with AREL. Evidently, our AREL model
performs the best and achieves the new state-of-
the-art results across all metrics.

But, compared with the XE-ss model, the per-
formance gain is minor, especially on METEOR
and ROUGE-L scores. However, in Sec. 4.3, the
extensive human evaluation has indicated that our
AREL framework brings a signiﬁcant improve-
ment on generating human-like stories over the
XE-ss model. The inconsistency of automatic
evaluation and human evaluation lead to a suspect
that these hand-crafted metrics lack the ability to
fully evaluate stories’ quality due to the compli-
cated characteristics of the stories. Therefore, we
conduct experiments to analyze and discuss the

Method

B-1 B-2 B-3 B-4 M R

C

XE-ss
BLEU-RL
METEOR-RL
ROUGE-RL
CIDEr-RL
AREL (best)

62.3 38.2 22.5 13.7 34.8 29.7 8.7
62.1 38.0 22.6 13.9 34.6 29.0 8.9
40.2 30.0 1.2
68.1 35.0 15.4
6.8
27.0 33.8
58.1 18.5
0
0
1.6
61.9 37.8 22.5 13.8 34.9 29.7 8.1
63.8 39.1 23.2 14.1 35.0 29.5 9.4

Table 2: Comparison with different RL mod-
els with different metric scores as the rewards.
We report the average scores of the AREL mod-
els as AREL (avg). Although METEOR-RL and
ROUGE-RL models achieve the highest scores
on their own metrics, the underlined scores are
severely damaged. Actually, they are gaming their
own metrics with nonsense sentences.

defects of the automatic metrics in section 4.2.

Limitations of Automatic Metrics String-
match-based automatic metrics are not perfect
and fail to evaluate some semantic characteristics
of the stories (e.g. expressiveness and coherence).
In order to conﬁrm our conjecture, we utilize
automatic metrics as rewards to reinforce the
model with policy gradient.
The quantitative
results are demonstrated in Table 1.

Apparently, METEOR-RL and ROUGE-RL are
severely ill-posed: they obtain the highest scores
on their own metrics but damage the other met-
rics severely. We observe that these models are
actually overﬁtting to a given metric while losing
the overall coherence and semantical correctness.
Same as METEOR score, there is also an adver-
sarial example for ROUGE-L4, which is nonsense
but achieves an average ROUGE-L score of 33.8.
Besides, as can be seen in Table 1, after rein-
forced training, BLEU-RL and CIDEr-RL do not
bring a consistent improvement over the XE-ss
model. We plot the histogram distributions of both
BLEU-3 and CIDEr scores on the test set in Fig-
ure 5. An interesting fact is that there are a large
number of samples with nearly zero score on both
metrics. However, we observed those “zero-score”
samples are not pointless results; instead, lots of
them make sense and deserve a better score than
zero. Here is a “zero-score” example on BLEU-3:

I had a great time at the restaurant today.
The food was delicious. I had a lot of food.
The food was delicious. I had a great time.

4An adversarial example for ROUGE-L: we the was a .
and to the . we the was a . and to the . we the was a . and to
the . we the was a . and to the . we the was a . and to the .

Method
XE-ss
BLEU-RL
CIDEr-RL
GAN
AREL

Win
Lose
22.4% 71.7%
23.4% 67.9%
13.8% 80.3%
34.3% 60.5%
38.4% 54.2%

Unsure
5.9%
8.7%
5.9%
5.2%
7.4%

Table 3: Turing test results.

The corresponding reference is

The table of food was a pleasure to see!
Our food is both nutritious and beautiful!
Our chicken was especially tasty! We love
greens as they taste great and are healthy!
The fruit was a colorful display that tanta-
lized our palette.

Although the prediction is not as good as the ref-
erence, it is actually coherent and relevant to the
theme “food and eating”, which showcases the de-
feats of using BLEU and CIDEr scores as a reward
for RL training.

Moreover, we compare the human evaluation
scores with these two metric scores in Figure 5.
Noticeably, both BLEU-3 and CIDEr have a poor
correlation with the human evaluation scores.
Their distributions are more biased and thus can-
not fully reﬂect the quality of the generated sto-
ries. In terms of BLEU, it is extremely hard for
machines to produce the exact 3-gram or 4-gram
matching, so the scores are too low to provide use-
ful guidance. CIDEr measures the similarity of a
sentence to the majority of the references. How-
ever, the references to the same image sequence
are photostream different from each other, so the
score is very low and not suitable for this task. In
contrast, our AREL framework can lean a more
robust reward function from human-annotated sto-
ries, which is able to provide better guidance to
the policy and thus improves its performances over
different metrics.

Visualization of The Learned Rewards
In Fig-
ure 6, we visualize the learned reward function for
both ground truth and generated stories. Evidently,
the AREL model is able to learn a smoother re-
ward function that can distinguish the generated
stories from human annotations. In other words,
the learned reward function is more in line with
human perception and thus can encourage the
model to explore more diverse language styles and
expressions.

Figure 5: Metric score distributions. We plot the histogram distributions of BLEU-3 and CIDEr scores
on the test set, as well as the human evaluation score distribution on the test samples. We use the Turing
test results to calculate the human evaluation scores (see Section 4.3). Basically, 0.2 score is given if the
generated story wins the Turing test, 0.1 for tie, and 0 if losing. Each sample has 5 scores from 5 judges,
and we use the sum as the human evaluation score, so it is in the range [0, 1].

Choice (%)
Relevance
Expressiveness
Concreteness

AREL vs XE-ss
AREL XE-ss Tie
61.7
13.2
66.1
15.1
63.9
15.8

25.1
18.8
20.3

AREL vs BLEU-RL
AREL BLEU-RL Tie
55.8
16.3
59.1
14.5
60.1
13.6

27.9
26.4
26.3

AREL vs CIDEr-RL
AREL CIDEr-RL Tie
56.1
15.7
59.1
14.3
59.5
15.9

28.2
26.6
24.6

AREL vs GAN
AREL GAN Tie
52.9
35.8 11.3
48.5
32.2 19.3
49.8
35.8 14.4

Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model
in generating more human-like stories than the SOTA methods.

is prone to the vanishing gradient issue. Analyti-
cally, our method does not suffer from these two
common issues and thus is able converge to op-
timum solutions more easily. From Table 1 we
can observe slight gains of using AREL over GAN
with automatic metrics, but we further deploy hu-
man evaluation for a better comparison.

4.3 Human Evaluation

Automatic metrics cannot fully evaluate the ca-
pability of our AREL method. Therefore, we
perform two different kinds of human evaluation
studies on Amazon Mechanical Turk: Turing test
and pairwise human evaluation. For both tasks,
we use 150 stories (750 images) sampled from the
test set, each assigned to 5 workers to eliminate
human variance. We batch six items as one assign-
ment and insert an additional assignment as a san-
ity check. Besides, the order of the options within
each item is shufﬂed to make a fair comparison.

Turing Test We ﬁrst conduct ﬁve indepen-
dent Turing tests for XE-ss, BLEU-RL, CIDEr-
RL, GAN, and AREL models, during which the
worker is given one human-annotated sample and
one machine-generated sample, and needs to de-
cide which is human-annotated. As shown in Ta-
ble 3, our AREL model signiﬁcantly outperforms
all the other baseline models in the Turing test: it
has much more chances to fool AMT worker (the

Figure 6: Visualization of the learned rewards on
both the ground-truth stories and the stories gen-
erated by our AREL model. The generated sto-
ries are receiving lower averaged scores than the
human-annotated ones.

Comparison with GAN We here compare our
method with vanilla GAN (Goodfellow et al.,
2014), whose update rules for the generator can
be generally classiﬁed into two categories. We
demonstrate their corresponding objectives and
ours as follows:

GAN 1 : Jβ = E

[− log Rθ(W )] ,

GAN 2 :

ours :

W ∼pβ

Jβ = E

W ∼pβ

Jβ = E

W ∼pβ

[log(1 − Rθ(W ))] ,

[−Rθ(W )] .

As discussed in Arjovsky et al. (2017), GAN 1 is
prone to the unstable gradient issue and GAN 2

Figure 7: Qualitative comparison example with XE-ss. The direct comparison votes (AREL:XE-ss:Tie)
were 5:0:0 on Relevance, 4:0:1 on Expressiveness, and 5:0:0 on Concreteness.

ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN
= 45.8%:28.3%:32.1%:19.7%:39.5%), which con-
ﬁrms the superiority of our AREL framework in
generating human-like stories. Unlike automatic
metric evaluation, the Turing test has indicated
a much larger margin between AREL and other
competing algorithms. Thus, we empirically con-
ﬁrm that metrics are not perfect in evaluating many
implicit semantic properties of natural language.
Besides, the Turing test of our AREL model re-
veals that nearly half of the workers are fooled by
our machine generation, indicating a preliminary
success toward generating human-like stories.

Pairwise Comparison In order to have a clear
comparison with competing algorithms with re-
spect to different semantic features of the sto-
ries, we further perform four pairwise compar-
ison tests: AREL vs XE-ss/BLEU-RL/CIDEr-
RL/GAN. For each photostream, the worker is
presented with two generated stories and asked to
make decisions from the three aspects: relevance5,
expressiveness6 and concreteness7. This head-to-
head compete is designed to help us understand in
what aspect our model outperforms the competing
algorithms, which is displayed in Table 4.

Consistently on all the three comparisons, a
large majority of the AREL stories trumps the
competing systems with respect to their relevance,

5Relevance: the story accurately describes what is hap-

pening in the image sequence and covers the main objects.

6Expressiveness: coherence, grammatically and semanti-

cally correct, no repetition, expressive language style.

7Concreteness: the story should narrate concretely what

is in the image rather than giving very general descriptions.

expressiveness, and concreteness. Therefore, it
empirically conﬁrms that our generated stories are
more relevant to the image sequences, more coher-
ent and concrete than the other algorithms, which
however is not explicitly reﬂected by the auto-
matic metric evaluation.

4.4 Qualitative Analysis

Figure 7 gives a qualitative comparison example
between AREL and XE-ss models. Looking at the
individual sentences, it is obvious that our results
are more grammatically and semantically correct.
Then connecting the sentences together, we ob-
serve that the AREL story is more coherent and
describes the photo stream more accurately. Thus,
our AREL model signiﬁcantly surpasses the XE-
ss model on all the three aspects of the qualitative
example. Besides, it won the Turing test (3 out 5
AMT workers think the AREL story is created by
a human). In the appendix, we also show a nega-
tive case that fails the Turing test.

5 Conclusion

In this paper, we not only introduce a novel ad-
versarial reward learning algorithm to generate
more human-like stories given image sequences,
but also empirically analyze the limitations of the
automatic metrics for story evaluation. We believe
there are still lots of improvement space in the
narrative paragraph generation tasks, like how to
better simulate human imagination to create more
vivid and diversiﬁed stories.

Acknowledgment

We thank Adobe Research for supporting our lan-
guage and vision research. We would also like
to thank Licheng Yu for clarifying the details
of his paper and the anonymous reviewers for
their thoughtful comments. This research was
sponsored in part by the Army Research Labora-
tory under cooperative agreements W911NF09-2-
0053. The views and conclusions contained herein
are those of the authors and should not be inter-
preted as representing the ofﬁcial policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notice herein.

References

Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international confer-
ence on Machine learning, page 1. ACM.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
arXiv preprint

tou. 2017. Wasserstein gan.
arXiv:1701.07875.

Wenhu Chen, Aur´elien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325.

Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo,
Jing Dai, and Chang-Tien Lu. 2017. Multi-
modal storytelling via generative adversarial imita-
In Proceedings of the Twenty-Sixth
tion learning.
International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-17, pages 3967–3973.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
In The IEEE Interna-
tions via a conditional gan.
tional Conference on Computer Vision (ICCV).

Chelsea Finn, Paul Christiano, Pieter Abbeel, and
Sergey Levine. 2016. A connection between gen-
erative adversarial networks, inverse reinforcement
learning, and energy-based models. arXiv preprint
arXiv:1611.03852.

Justin Fu, Katie Luo, and Sergey Levine. 2017.
in-
arXiv preprint

rewards with adversarial

learning.

Learning robust
verse reinforcement
arXiv:1710.11248.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks. In Pro-
ceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, pages 315–
323.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Elia Bruni and Raquel Fern´andez. 2017. Adversarial
evaluation for open-domain dialogue generation. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 284–288.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.

Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon,
David Meger, Joelle Pineau, and Doina Precup.
2017. Optiongan: Learning joint reward-policy op-
tions using generative adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1709.06683.

Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui
Zhang, Mu Li, and Ming Zhou. 2018. Generative
bridging network for neural sequence prediction. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1706–1715. Associ-
ation for Computational Linguistics.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neural
Information Processing Systems, pages 4565–4573.

Ting-Hao K. Huang,

Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He,
Pushmeet Kohli, Dhruv Batra, et al. 2016. Visual

storytelling.
North American Chapter of
Computational Linguistics (NAACL 2016).

In 15th Annual Conference of the
the Association for

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,
and F Huang. 2006. A tutorial on energy-based
learning. Predicting structured data, 1(0).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

Ryan Lowe, Michael Noseworthy, Iulian V Serban,
Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic turing
test: Learning to evaluate dialogue responses. arXiv
preprint arXiv:1708.07149.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and
Tao Mei. 2018a. Show, reward and tell: Automatic
generation of narrative paragraph from photo stream
by adversarial training. AAAI.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video caption-
ing via hierarchical reinforcement learning. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xin Wang, Yuan-Fang Wang, and William Yang Wang.
2018c. Watch, listen, and describe: Globally and lo-
cally aligned cross-modal attentions for video cap-
tioning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 795–801.
Association for Computational Linguistics.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
In Proceedings of the IEEE
video and language.
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Cesc C Park and Gunhee Kim. 2015. Expressing an
image stream with a sequence of natural sentences.
In Advances in Neural Information Processing Sys-
tems, pages 73–81.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017a. Seqgan: Sequence generative adversarial
In AAAI, pages 2852–
nets with policy gradient.
2858.

Licheng Yu, Mohit Bansal, and Tamara Berg. 2017b.
Hierarchically-attentive rnn for album summariza-
In Proceedings of the 2017
tion and storytelling.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 966–971, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
In AAAI, volume 8, pages
reinforcement learning.
1433–1438. Chicago, IL, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2006. Maximum margin planning.
In
Proceedings of the 23rd international conference on
Machine learning, pages 729–736. ACM.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceeding of IEEE conference on Computer Vision and
Pattern Recognition (CVPR).

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
In The
sequence training for image captioning.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Appendix

A Error Analysis

Failure Case in Turing Test
In Figure 8, we
presented a negative example that failed the Turing
test (4 out of 5 made the correct decision). Com-
pared with the human-generated story, our AREL
story lacked emotion and imagination and thus can
be easily distinguished. For example, the real hu-
man gave the band a nickname “very loud band”
and told a more amusing story. Though we have
made encouraging progress on generating human-
like stories, further research of creating diversiﬁed
stories is still needed.

Data Bias From the experiments, we observe
that there exist some severe data bias issues in
the VIST dataset, such as gender bias and event
bias.
In the training set, the ratio of male and
female’s appearances is 2.06:1, and it is 2.16:1
in the test set.
the models aggravate the gender
bias to 3.44:1. Besides, because all the images
are collected from Flickr, there is also an event
bias issue. We count three most frequent events:
party, wedding, and graduation, whose ratios are
6.51:2.36:1 on the training set and 4.54:2.42:1 on
the test set. However, their ratio on the testing
results is 10.69:2.22:1. Clearly, the models tend
to magnify the inﬂuence of the largest majority.
These bias issues remain to be studied for future
work.

B Training Details

Our model is implemented on PyTorch and con-
sists of two parts – a policy model and a reward
model. The policy model is implemented with
a multiple-RNN architecture. Each RNN model
is responsible for generating a sub-story for each
photo in the stream. But the weights are tied
to minimize the memory consumption. The im-
age features are extracted from the pre-trained
ResNet-152 model8. The visual encoder receives
the ResNet-152 features and uses recurrent neu-
ral network to understand the temporal dynamics
and represents them as hidden state vectors, which
is further fed into the decoder to generate stories.
The reward model is based on convolutional neu-
ral network and uses convolution kernels to extract
semantic features for prediction. Here we give the
detailed description of our system:

8https://github.com/KaimingHe/

deep-residual-networks

• Visual Encoder:

the visual encoder is a bi-
directional GRU model with hidden dimen-
sion of 256 for each direction. we concate-
nate the bi-directional states and form a 512
dimension vector for the story generator. The
input album is composed of ﬁve images, and
each image is used as separate input to differ-
ent RNN decoders.

• Decoder: The decoder is a single-layer GRU
model with hidden dimension of 512. The
recurrent decoder model receives the output
from the visual encoder as the ﬁrst input, and
then at the following time steps, it receives
the last predicted token as input or uses the
ground truth as input. During scheduled sam-
pling, we use a sampling probability to de-
cide which action to take.

• Reward Model: we use a convolutional neu-
ral network to extract n-gram features from
the story embedding and stretch them into a
ﬂattened vector. The embedding size of input
story is 128, and the ﬁlter dimension of CNN
is also 128. Here we use three kernels with
window size 2, 3, 4, each with a stride size
of 1. We use a pooling size of 2 to shrink the
extracted outputs and ﬂatten them as a vector.
Finally, we project this vector into a single
cell indicating the predicted reward value.

During training, we ﬁrst pre-train a schedule-
sampling model with a batch size of 64 with
NVIDIA Titan X GPU. The warm-up process
takes roughly 5-10 hours, and then we select the
best model to initialize our AREL policy model.
Finally, we use alternating training strategy to op-
timize both the policy model and the reward model
with a learning rate of 2e-4 using Adam optimiza-
tion algorithm. During test time, we use a beam
size of 3 to approximate the whole search space,
we force the beam search to proceed more than 5
steps and no more than 110 steps. Once we reach
the EOS token, the algorithm stops and we com-
pare the results with human-annotated corpus us-
ing 4 different automatic evaluation metrics.

C Amazon Mechanical Turk

We used AMT to perform two surveys, one picks
a more human-like story. We asked the worker
to answers 8 questions within 30 minutes, and
we pay 5 workers to work on the same sheet to

Figure 8: Failure case in Turing test. 4 out of 5 workers correctly recognized the human-created story
and 1 person mistakenly chose AREL story.

eliminate human-to-human bias. Here we demon-
strate the Turing survey form in Figure 9. Besides,
we also perform a head-to-head comparison with
other algorithms, we demonstrate the survey form
in Figure 10.

Figure 9: Turing Survey Form

Figure 10: Pairwise Comparison Form

