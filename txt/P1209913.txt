Integrating Relation Constraints with Neural Relation Extractors

Yuan Ye, Yansong Feng*, Bingfeng Luo,
Yuxuan Lai, Dongyan Zhao
Wangxuan Institute of Computer Technology, Peking University, China
{pkuyeyuan, fengyansong, bf luo, erutan, zhaodongyan}@pku.edu.cn

9
1
0
2
 
v
o
N
 
6
2
 
 
]
L
C
.
s
c
[
 
 
1
v
3
9
4
1
1
.
1
1
9
1
:
v
i
X
r
a

Abstract

Recent years have seen rapid progress in identifying prede-
ﬁned relationship between entity pairs using neural networks
(NNs). However, such models often make predictions for
each entity pair individually, thus often fail to solve the in-
consistency among different predictions, which can be char-
acterized by discrete relation constraints. These constraints
are often deﬁned over combinations of entity-relation-entity
triples, since there often lack of explicitly well-deﬁned type
and cardinality requirements for the relations. In this paper,
we propose a uniﬁed framework to integrate relation con-
straints with NNs by introducing a new loss term, Constraint
Loss. Particularly, we develop two efﬁcient methods to cap-
ture how well the local predictions from multiple instance
pairs satisfy the relation constraints. Experiments on both En-
glish and Chinese datasets show that our approach can help
NNs learn from discrete relation constraints to reduce in-
consistency among local predictions, and outperform popular
neural relation extraction (NRE) models even enhanced with
extra post-processing. Our source code and datasets will be
released at https://github.com/PKUYeYuan/Constraint-Loss-
AAAI-2020.

Introduction
Relation extraction (RE) aims to extract predeﬁned relations
between two marked entities in plain texts, and its success
can beneﬁt many knowledge base (KB) related tasks like
knowledge base population (KBP) (Suchanek et al. 2013;
Wu, He, and Hu 2018), question answering (QA) (Dai, Li,
and Xu 2016; Yu et al. 2017; Lai et al. 2019) and etc.

Most existing works investigate the RE task in a classiﬁca-
tion style. A sentence marked with a given pair of entities is
fed to a classiﬁer to decide their relationship, also called the
sentence-level RE. Another related setup is to feed a group
of sentences containing the given entity pair to the classiﬁer,
called the bag-level RE. We should note that both sentence-
level RE and bag-level RE make predictions for each en-
tity pair individually and locally. However, when we look at
the model outputs globally, there are always contradictions
among different predictions, such as an entity is regarded
as the object of both Country and City, two different cities

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

have been labeled as Capital for one country and so on. To
alleviate these local contradictions, Chen et al. 2018 collect
constraints on the type and cardinality requirements of re-
lations, such as whether two relations should not have the
same type of subject (object), or whether a relation should
not have multiple subjects (objects) given its object (sub-
ject). Further, in the inference stage, they use integer linear
programming (ILP) to ﬁlter and adjust the local predictions
that are inconsistent with these constraints. Basically, ILP
operates in a post-processing way to copy with contradic-
tory predictions, but there is no way to provide feedback to
the original RE model.

In fact, it would be of great importance to utilize those
constraints to backwards improve the original RE models.
For example, enhanced with various attention or pooling
mechanisms, most current neural network extraction models
have shown promising performance on benchmark datasets,
but they still suffer from inconsistent local predictions (Chen
et al. 2018). If those relation constraints can be learned dur-
ing model training, that will help to further improve the over-
all performance, and we may no longer need a separate post-
processing step as ILP does.

However, directly integrating relation constraints with
NRE models is not a trivial task: (1) relation constraints
are not deﬁned regarding a single prediction, but often over
combinations of instances, thus it is not easy to ﬁnd appro-
priate representations for those constraints; (2) it is not easy
to evaluate how well pairwise predictions match the con-
straints in a batch, and it is not clear how to feed the infor-
mation back to the NRE models.

To tackle the challenges, we propose a uniﬁed framework
to ﬂexibly integrate relation constraints with NRE models
by introducing a loss term Constraint Loss. Concretely, we
develop two methods denoted as Coherent and Semantic to
construct Constraint Loss from different perspectives. Co-
herent captures how well pairwise predictions match the
constraints from an overall perspective, and Semantic pays
more attention to which speciﬁc rule in the constraints the
pairwise predictions should satisfy. In addition, we encode
relation constraints into different representations for each
method. Notably, Constraint Loss is regarded as a rule-based
regularization term within a batch instead of regularizing

each instance, since the relation constraints are often deﬁned
over combinations of local predictions. Moreover, our ap-
proach does not bring extra cost to the inference phase and
can be adapted to most existing NRE models without explicit
modiﬁcations to their structures, as it only utilizes the out-
puts from the NRE model as well as relation constraints to
obtain Constraint Loss and provides feedback to the NRE
model through backward propagation. Experiments on both
Chinese and English datasets show that our approach can
help popular NRE models learn from the constraints and out-
performs state-of-the-art methods even enhanced with ILP
post-processing. Moreover, jointing our approach and ILP
achieves further improvement which demonstrates that our
approach and the ILP post-processing exploit complemen-
tary aspects from the constraints.

The main contributions of this paper include: (1) We pro-
pose a uniﬁed framework to effectively integrate NRE mod-
els with relation constraints without interfering the inherent
NRE structure. (2) We develop two efﬁcient methods to cap-
ture the inconsistency between local NRE outputs and re-
lation constraints, which are used as a loss term to help the
NRE training. (3) We provide thoroughly experimental study
on different datasets and base models. The results show that
our approach is effective and exploits the constraints from
different perspectives with ILP.

Related Work

Since annotating high-quality relational facts in sentences is
laborious and time-consuming, RE is usually investigated in
the distant supervision (DS) paradigm, where datasets are
automatically constructed by aligning existing KB triples <
subj, r, obj > 1 with a large text corpus (Mintz et al. 2009).
However, the automatically constructed dataset suffers the
wrong labeling problem, where the sentence that mentions
the two target entities may not express the relation they hold
in KB, thus contains many false positive labels (Riedel, Yao,
and McCallum 2010). To alleviate the wrong labeling prob-
lem, RE is usually investigated in the multi-instance learning
(MIL) framework which considers RE task at bag-level and
holds the at-least-one hypothesis, thinking that there exists at
least one sentence which expresses the entity pair’s relation
in its corresponding sentence bag (Hoffmann et al. 2011;
Surdeanu et al. 2012; Suchanek et al. 2013).

As neural networks have been widely used, an increasing
number of researches for RE have been proposed under MIL
framework. Zeng et al. 2014 use a convolution neural net-
work (CNN) to automatically extract features and Zeng et al.
2015 use a piece-wise convolution neural network (PCNN)
to capture structural information by inherently splitting a
sentence into three segments according to the two target en-
tities. Furthermore, Lin et al. 2016 proposed sentence-level
attention-based models (ACNN, APCNN) to dynamically re-
duce the weights of noisy sentences. And there also exists
many NN based works improving the RE performance by
utilizing external information, such as syntactic information

1We use subj, obj and r to denote subject, object and relation

for a KB triple, respectively, in the rest of this paper.

(He et al. 2018), entity description (Ji et al. 2017), relation
aliases (Vashishth et al. 2018) and etc.

In addition, there are many works focusing on combining
NNs with precise logic rules to harness ﬂexibility and reduce
uninterpretability of the neural models. Hu et al. 2016 make
use of ﬁrst-order logic (FOL) to express the constraints and
propose a teacher-student network that could project predic-
tion probability into a rule-regularized subspace and trans-
fer the information of logic rules into the weights of neural
models. Xu et al. 2018 put forward a semantic loss frame-
work, which bridges between neural output vectors and log-
ical constraints by evaluating how close the neural network
is to satisfying the constraints on its output with a loss term.
And Luo et al. 2018 develop novel methods to exploit the
rich expressiveness of regular expressions at different lev-
els within a NN, showing that the combination signiﬁcantly
enhances the learning effectiveness when a small number of
training examples are available.

However, using these frameworks on RE is not straightfor-
ward. Speciﬁcally, Hu et al. 2016 directly project prediction
probability of instance as they can assess how well a single
instance’s prediction satisﬁes the rules, while constraints in
RE are non-local and we could not examine each instance
individually for the violation of constraints. Luo et al. 2018
need the regular expressions to provide keyword informa-
tion and get a priori category prediction, however, generat-
ing high-quality regular expressions from RE datasets is not
easy. For Xu et al. 2018, since our constraints are related to
the combination of instances rather than a single instance,
to utilize the semantic loss framework, we need to ﬁnd ap-
propriate representations for various relation constraints and
evaluate the neural output in a pairwise way.

Relation Constraints
Since many KBs do not have a well-deﬁned typing sys-
tem and explicit argument cardinality requirements, sim-
ilar in spirit with Chen et al. 2018, our relation con-
straints are deﬁned over the combination of two triples:<
subjm, rm, objm > and < subjn, rn, objn >. 2 We derive
the type and cardinality constraints from existing KB to im-
plicitly capture the expected type and cardinality require-
ments on the arguments of a relation. One can surely employ
human annotators to collect such constraints.

Type Constraints. Type constraints implicitly express the
types of subjects and objects that a speciﬁc relation could
have. For example, the subject and object types for relation
almaMater should be PERSON and SCHOOL, respectively,
and we take positive rules [almaMater and knownFor could
have the same subject type] and [almaMater and employer
could have the same object type] to implicitly encode al-
maMater’s subject and object type requirements.

Speciﬁcally, we use entity sharing between different re-
lations to implicitly capture the expected argument type of

2The main difference is that our constraints are considered as
positive rules where we expect the relation predictions to fall in,
while the constraints in Chen et al. 2018 are considered as inviolate
rules that the local predictions should not break.

each relation. If the subject (or object) set of relation ri in
KB has an intersection with those of rj, then we consider
ri and rj could have the same expected subject (or object)
type. We thereby assign relation pairs (ri, rj) into Cts if
they are expected to have the same subject type, into Cto if
they are expected to have the same object type, and assign it
into Ctso if the subject type of one relation is expected to
be same as the object type of the other.

Cardinality Constraints. Cardinality constraints indicate
the cardinality requirements on a relation’s arguments. For
example, relation almaMater could have multiple subjects
(PERSON) when its object (SCHOOL) is given.

Speciﬁcally, for each predeﬁned relation ri, we collect all
triples containing ri, and count the number of the triples that
have multiple objects (subjects) for each subject (object).
Then, we assign relation ri into Ccs if it can have multiple
subjects for a given object, into Cco if it can have multiple
objects for a given subject.

Finally, we get 5 sub-category constraint sets. We use Cφ
to represent a single set, Ct∗ ∈ {Cts, Cto, Ctso} to repre-
sent a type constraint set, and Cc∗ ∈ {Ccs, Cco} to repre-
sent a cardinality constraint set. Note that our relation con-
straints are deﬁned to examine whether a pair of subject-
relation-object triples can hold at the same time from dif-
ferent perspectives. To make our constraints clearer, we list
some rules for each constraint set in Table 1.

Set

C ts
C to
C tso
C cs
C co

Sampled Positive Rules

(almaMater, knowFor), (city, region), (spouse, child)
(almaMater, owner), (city, hometown), (capital, city)
(birthPlace, capital), (child, spouse), (city, country)
almaMater, country, city, hometown
foundationPerson, child, knownFor, product

Table 1: Example rules for each constraint set Cφ.

Our Approach
As shown in Fig. 1, our framework consists of two main
components, a base NRE model and the Constraint Loss Cal-
culator (CLC). The CLC module is designed to integrate the
relation constraints with NRE models, which does not rely
on speciﬁc NRE architectures and can work in a plug-and-
play fashion.

Base NRE Model
While our framework can work with most existing relation
extractors, in this paper, we take the most popular neural
relation extractors, ACNN and APCNN (Lin et al. 2016), as
our base extractors. 3

ACNN uses convolution neural networks with max-
pooling layer to capture the most signiﬁcant features from

3We do not use the most recently neural models, such as Feng
et al. 2018, Qin, Xu, and Wang 2018 and Jia et al. 2019, as our base
model, as they focused more on noise reduction which is not within
the scope of this paper.

Figure 1: Framework overview. For each mini-batch, the
Constraint Loss is calculated by evaluating the predicted
probability pθ(Y |X) according to the relation constraints.

a sentence. Then, an attention layer is used to selectively ag-
gregate individual representations from a bag of sentences
into a sentence bag embedding, which is fed to a softmax
classiﬁer to predict the relation distribution pθ(Y |X).

APCNN is an extension of ACNN. Speciﬁcally, APCNN
divides the convolution output into three segments based on
the positions of the two given entities and devises a piece-
wise max-pooling layer to produce sentence representation.

Constraint Loss Calculator (CLC)
Given the inherent nature of our relation constraints, the
CLC can not evaluate a single subject-relation-object pre-
diction against our constraint sets, we thus operate our CLC
in a mini-batch wise way. Speciﬁcally, in each batch, we in-
tegrate the relation constraints with the NRE output by in-
troducing a loss term, Constraint Loss, which is designed
to regulate the NRE model to learn from those constraints,
e.g., not to violate the positive rules in the constraints. As
shown in Fig. 1, to calculate Constraint Loss, we ﬁrst collect
the NRE output probability pθ(Y |X) within the batch, and
then the CLC takes pθ(Y |X) and relation constraints as
input to obtain Constraint Loss, which should reﬂect the in-
consistency among all local predictions pθ(Y |X) accord-
ing to our relation constraints. Finally, the total loss for back
propagation consists of two parts: the original NRE loss (LO)
and the Constraint Loss (LC):

Ltotal = LO + λLC

where λ is a weight coefﬁcient.

Particularly, the key task of CLC is to evaluate how well
the current NRE output probabilities pθ(Y |X) satisfy our
relation constraints. We solve this problem in two steps. We
ﬁrst calculate a local loss term L(pm, pn, Cφ) for a pair
of local predictions, i.e., pm and pn for the mth and nth
instances, respectively, 4 against the constraint set Cφ. Sec-
ondly, we aggregate all local loss terms to obtain the batch-
wise Constraint Loss. Here, we develop two methods to cal-

4We use pm to represent the probability output of the neural

model on the mth instance in a batch.

Figure 2: A running example of our two CLC modules, Semancit and Coherent. To exhibit the main process clearly, we simplify
the example and only consider 4 relations, a mini-batch with 3 instances and two constraint sets (Cts, Ccs). The whole process
of CLC contains 3 steps, we take Semantic as an example. First, we represent constraint set Cts (Ccs) as a vector set U ts (U cs)
and each vector represents a single rule. Then, we feed NRE output and the vector set into local loss calculator, getting the local
loss L(pm, pn, Cφ) (using Eq. 4) for each pair of instances within a batch. Finally, Constraint Loss is obtained by aggregating
all instance pairs in the batch. The main difference between Semantic and Coherent is that Coherent represents constraint set
into one vector while Semantic represents it into a vector set, utilizing relation constraints from different perspectives.

culate Constraint Loss from different perspectives, denoted
as Coherent (Coh) and Semantic (Sem), respectively.

Coherent (Coh)
In this method, we calculate Constraint Loss by evaluating
the coherence between the NRE output and a constraint set.
Note that this method only requires the NRE outputs to be
more consistent with one constraint set as a whole, but does
not explicitly push the NRE model to update according to
speciﬁc positive rules in this set.

Representing Constraint Sets. We encode a constraint
set into one single binary vector. Since the positive rules in
the type and cardinality constraint set have different forms,
we represent them in slightly different ways.

For a type constraint set Ct∗, we construct a binary vec-
tor vt∗, where vt∗
i,j indicates whether relation pair (ri, rj)
belongs to Ct∗, i.e., vt∗
i,j = 1 if (ri, rj) ∈ Ct∗ and vt∗
i,j = 0
if (ri, rj) (cid:54)∈ Ct∗. Take Cts illustrated in Fig. 2 as an exam-
ple, since (almaMater, city) ∈ Cts, vts

0,1 is set to 1.

For a cardinality constraint set Cc∗, we construct a binary
indicates whether relation ri belongs
i = 0 if ri (cid:54)∈ Cc∗.

i = 1 if ri ∈ Cc∗ and vc∗

vector vc∗, where vc∗
i
to Cc∗, i.e., vc∗
Again, in Fig.2, vcs

0 is set to 1, since almaMater ∈ Ccs.

Thus, for each one of the 5 sub-category constraint sets,
we build one single representation vector, resulting in 5 vec-
tors {vts, vto, vtso, vcs, vco}. And the dimensions of vt∗
and vc∗ are |R|2 and |R|, respectively, where |R| is the size
of the relation set.

nth instances, within a batch. Our expectation is that coher-
ent local prediction pairs should satisfy our constraint sets.
Again, we deal with the type constraint sets and cardinality
constraint sets separately.

Thus, for a type constraint set Ct∗ represented by vt∗,

the local loss, L(pm, pn, Ct∗), can be written as:
i pn
i,jpm
vt∗
j )

L(pm, pn, Ct∗) = −I mn

t∗ log(

(cid:88)

(1)

i,j

∈ {0, 1} indicates whether

where I mn
to calculate
t∗
L(pm, pn, Ct∗). Take I mn
as an example, for triple pair
ts
(subjm, rm, objm) and (subjn, rn, objn), we set I mn
ts = 1,
if subjm = subjn which means the two triples have the
same subject type and corresponding predicted relation pair
i pn
should satisfy Cts; otherwise, we assign 0 to I mn
j
can be considered as the probability that the base NRE model
predicts relation ri and rj for the mth and nth instances, re-
spectively.

ts .5 pm

For cardinality constraint set Cc∗ represented by vc∗, the

local loss, L(pm, pn, Cc∗), can be written as:
i pm
vc∗

L(pm, pn, Cc∗) = −I mn

c∗ log(

(cid:88)

i pn
i )

(2)

i
is an indicator similar to I mn
t∗

where I mn
i is seen
c∗
as the possibility that the base NRE predicts relation ri for
both the mth and nth instances.

and pm

i pn

Aggregation. To obtain the batch-wise Constraint Loss,
we simply sum all the local loss terms L(pm, pn, Cφ) in a
batch to get the total constraint loss LC (Eq. 3).
(cid:88)
L(pm, pn, Cφ)

(cid:88)

(cid:88)

(3)

LC =

Local Loss Calculation. Now, we proceed to calculate the
loss term for a pair of local predictions, e.g., the mth and

φ
5Detailed assignment for I mn

m

n

φ

can be found in Appendix.

Semantic (Sem)
In this method, we pay more attention to which speciﬁc rules
in the constraint sets the pairwise local predictions should
satisfy. Our intuition is that, for each of our constraint set,
good local predictions should follow one rule in that set,
while bad ones may not ﬁnd any rules to satisfy. This may
push the NRE model to effectively learn from speciﬁc rules
in a more focused way.

Representing Constraints. To represent the rules in the
constraint sets more precisely, we encode each rule c ∈ Cφ
into a single binary vector u, thus, the whole set is repre-
sented as a vector set U φ, shown as in Fig. 2. Again, since
the rules in Ct∗ and Cc∗ have different forms, we represent
them in different ways.

For each type rule (ri, rj) ∈ Ct∗, the representation vec-
tor u is a binary vector whose ith and jth dimensions are
set to 1 and the rest are set to 0. Take Cts in Fig. 2 as an
example, the rule (almaMater, city) ∈ Cts is encoded as a
vector whose ﬁrst two dimensions are set to 1.

For each cardinality rule ri ∈ Cc∗, the representation
vector u is a binary vector whose ith dimension is set to 1
and the rest are set to 0. In Fig. 2, the rule almaMater ∈ Ccs
is represented by a vector, where only the ﬁrst dimension is
set to 1.

Different from Coherent, here we construct one vector set
to represent each sub-category constraint sets, resulting in 5
vector sets {U ts, U to, U tso, U cs, U co}. And each single
rule is represented by a |R|-dim binary vector.

Local Loss Calculation.
Inspired by the semantic loss
function (SL) introduced in Xu et al. 2018, which operates
on a single output, we adapt the original SL to deal with pair-
wise instances over different kinds of constraints. We design
the new local loss term as:

L(pm, pn, Cφ) = −I mn

φ log

(cid:88)

f (pm, pn, c)

(4)

φ

c∈Cφ
where I mn
is an indicator same as before and f is a score
function reﬂecting how well the pairwise predictions match
a single rule c ∈ Cφ. Since the rules in Ct∗ and Cc∗ are
encoded in different ways, we calculate f for type constraint
sets and cardinality constraint sets separately.

Thus, for a rule c in type constraint set Ct∗, the score

function f (pm, pn, c) can be calculated by:
i − pm
i pn
i
(cid:89)
(1 − qi)

qi = pm
i + pn
(cid:89)
qi

f (pm, pn, c) =

(5)

ui=1
where u is the vector representation of c and qi is the prob-
ability that base NRE model predicts relation ri for at least
one of the mth and the nth instances.

ui=0

For a rule c in cardinality constraint

set Cc∗,

f (pm, pn, c) can be calculated by:
i pn
pm
i

f (pm, pn, c) =

(cid:89)

(cid:89)

ui=1

ui=0

(1 − pm

i pn
i )

(6)

i pn

where pm
relation ri for both the mth and the nth instances.

i means the probability that NRE model predicts

Aggregation. We use the same method as Coherent to per-
form aggregation according to Eq. 3.

Note that Coherent handles the constraint set as a whole
and treats each single rule in that set equally, while Semantic
treats all rules in a constraint set as mutually exclusive and
makes the pairwise predictions more satisfying one certain
rule in that set. Take Ccs as an example, in Eq. 2, Coherent
just simply increases the probabilities of corresponding re-
lation pairs for all positive rules, and each rule has the same
inﬂuence on the summation. However, in Eq. 6, for a poten-
tially satisﬁed rule, Semantic not only tries to increase the
probabilities of its corresponding relation pair, but also low-
ers the probabilities of the rest. That is, there would not exist
pair-wise local predictions which satisfy two positive rules
well in one constraint set at the same time, since if the high
probabilities of a relation pair have the positive effect on one
speciﬁc rule, it has negative effect on all the others.

Experiments
Our experiments are designed to answer the following ques-
tions: (1) whether our approach can effectively utilize the re-
lation constraints to improve the extraction performance? (2)
which CLC module performs better, Coherent or Semantic?
(3) which is the better way to utilize the relation constraints,
learning or post-processing?

Datasets

We evaluate our approach on both English and Chinese
datasets constructed by Chen et al. 2018. The English one
is constructed by mapping triples in DBpedia (Bizer et al.
2009) to sentences in the New York Times Corpus. It has 51
relations, about 50k triples, 134k sentences for training and
30k triples, 53k sentences for testing. The Chinese dataset is
built by mapping the triples of HudongBaiKe, a large Chi-
nese encyclopedia, with four Chinese economic newspapers.
It contains 28 relations, about 60k triples, 120k sentences for
training and 40k triples, 83k sentences for testing.

We automatically collect relation constraints for English
and Chinese datasets based on corresponding KBs. In total,
we obtain 541 rules for the English dataset and 110 rules for
the Chinese one.

Here we do not use the popular RE dataset created
by Riedel, Yao, and McCallum 2010, since it is produced
with an earlier version of Freebase which is not avail-
able now, and makes it impossible to automatically col-
lect the constraints. Secondly, Riedel’s dataset is dominated
by three big relations: location/contains, /people/nationality
and /people/place lived, covering about 60% of all KB
triples. Therefore, there are not enough data related to other
relations for us to collect constraints.

Setup
Following common practice in the RE community (Ji et al.
2017; He et al. 2018), we report the model performance by
both precision-recall (PR) curve and Precision@N (P@N).
We also report the average score of P@N (Mean).

Figure 3: The PR curves of our approach on two datasets with ACNN and APCNN as base models.

The main goal of our work is to explore whether our ap-
proach can help neural models effectively learn from dis-
crete relation constraints. Therefore, the ﬁrst baseline mod-
els are the two most popular base NRE models, ACNN and
APCNN. We also compare with the base NRE models en-
hanced with a post-processing ILP step, ACNN+ILP and
APCNN+ILP, which can be considered as state-of-the-art
constraint-based RE solutions.

We use a grid search to tune our hyper parameters, includ-
ing the weight coefﬁcient λ. Details about our hyper param-
eters are reported in Appendix.

Main Results
Our main results are summarized in Fig. 3 and Table 2. As
shown in Fig. 3, we can see that both the red and green dot
lines are lifted above the solid black lines, showing that after
equipped with our CLC modules, i.e., Coherent and Seman-
tic, both ACNN and APCNN obtain signiﬁcant improvement
on the English and Chinese datasets. This indicates our CLC
module actually helps the base NRE models beneﬁt from
properly utilizing the relation constraints, without interfer-
ence to the base models.

However, we ﬁnd that our approach obtains different lev-
els of improvement on the two datasets. On the Chinese
one, as shown in Table 2, with our Semantic version CLC,
APCNN(Sem) gains 4.9% improvement in Mean compared
to APCNN, but, on the English dataset, it only receives 0.5%
in Mean. Similar trends are also found for the Coherent ver-
sion and the ACNN base model. The better performance gain
on the Chinese dataset is mainly because its relation deﬁni-
tions are more clear compared to that of the English dataset.
For example, in English dataset, there are 8 relations whose
object could be any LOCATION, such as birthPlace, while
only 3 similar relations exist in Chinese dataset.

In addition, we investigate the performance improvement
when applying our CLC module to different base NRE mod-
els. Although both ACNN and APCNN are improved by
our CLC module in various datasets, we can still observe
that ACNN generally receives more performance improve-
ment compared with the APCNN base model. Taking the
Semantic method as an example, as shown in Table 2, on
the English dataset, ACNN(Sem) obtains 2.2% performance
improvement in Mean against ACNN, while APCNN(Sem)
only fetches 0.5% improvement. And similar trends can be
found in the Coherent method and on the Chinese dataset.
The more improvement when taking the ACNN as base NRE
model is because, compared with ACNN, APCNN itself is

designed to take the entity-aware sentence structure infor-
mation into account, thus can extract more effective features
that, to some extent, can implicitly capture part of the argu-
ments’ type and cardinality requirements of a relation, leav-
ing relatively less space for our CLC module to improve.

Comparing Coherent and Semantic This paper presents
two different methods, Coh and Sem, to represent and inte-
grate the relation constraints, both of which can lead to sub-
stantial improvement with both base models and datasets.
Speciﬁcally, as shown in Table 2, Sem brings slightly more
improvement than Coh in most of the settings, e.g., on Chi-
nese dataset, APCNN(Sem) obtain about 0.2% more gains
(4.9% vs 4.7%) in Mean than APCNN(Coh). We think the
reason is that Sem provides a more precise treatment for
the constraints, e.g., embedding each rule with a vector and
trying to evaluate the NRE output against one speciﬁc rule,
while Coh represents all rules in a sub-category with one
single vector and evaluates the output against whole set of
rules, which is admittedly a more coarse fashion.

Learning? or Post-processing? Previous works show
that ILP can effectively solve the inconsistency among pre-
dictions in a post-processing fashion (Chen et al. 2018).

Now we discuss which is the better way to utilize the re-
lation constraints, our CLC module or traditional ILP post-
processing. As shown in Table 2, both APCNN(Sem) and
APCNN(Coh) outperforms APCNN+ILP by at least 0.1% on
the English dataset and 1.0% on the Chinese dataset. Similar
trends can be also found for ACNN(Sem) and ACNN(Coh).
This shows that helping base NRE models to learn from the
relation constraints can generally bring more improvement,
thus utilizes the constraints more effectively compared to
utilizing those constraints in a post-processing way.

We can also apply ILP as a post-processing step to our
approach, since our CLC module works in the model train-
ing phase, and leaves the testing phase as it is. Interestingly,
as shown in Table 2, with an extra ILP post-processing, both
Coh and Sem obtain further improvement with different base
NRE models on different datasets. This indicates that our
CLC module still may not fully exploit the useful informa-
tion behind the relation constraints. The reasons may be that
our approach and the ILP post-processing exploit the rela-
tion constraints from different perspectives. For example,
our CLC operates in a mini-batch level during training, that
is a relatively local view, but ILP post-processing directly
optimizes the model output in a slightly global view.

Moreover, in Table 3, we ﬁnd that applying ILP to our

Model Name

P@100

P@200

P@300 Mean ∆Base

P@100

P@200

P@300 Mean ∆Base

English Dataset

Chinese Dataset

ACNN
ACNN(Coh)
ACNN(Sem)
ACNN+ILP
ACNN(Coh)+ILP
ACNN(Sem)+ILP

APCNN
APCNN(Coh)
APCNN(Sem)
APCNN+ILP
APCNN(Coh)+ILP
APCNN(Sem)+ILP

96.70
97.39
97.62
97.87
97.73
98.17

100
100
100
100
100
100

92.61
93.78
95.87
94.36
94.51
96.6

98.97
99.57
100
99.13
100
100

91.72
90.69
94.12
93.16
91.29
95.48

97.41
97.33
97.95
97.55
98.03
98.39

93.68
93.96
95.87
95.13
94.51
96.75

98.79
98.97
99.32
98.89
99.34
99.46

–
+0.3
+2.2
+1.5
+0.8
+3.1

–
+0.2
+0.5
+0.1
+0.6
+0.7

89.08
95.86
95.97
93.75
97.09
97.73

92.96
98.88
100
96.06
99.07
100

86.89
94.86
94.61
92.18
96.18
96.40

91.75
96.00
96.97
95.15
96.17
97.67

84.52
93.04
93.53
90.10
94.01
94.43

91.08
94.98
93.42
94.63
95.16
94.25

86.83
94.59
94.70
92.01
95.76
96.18

91.93
96.62
96.80
95.28
96.79
97.31

–
+7.8
+8.1
+5.2
+9.0
+9.4

–
+4.7
+4.9
+3.4
+4.9
+5.4

Table 2: Summary P@N(%) scores of our approach on two datasets with ACNN and APCNN as base models. ∆Base indicates
the difference between mentioned model and the base NRE model (ACNN in the top and APCNN in the bottom). And the name
with +ILP means that we perform ILP over the model’s outputs as an extra post-processing.

CLC enhanced model receives relatively less gain com-
pared to applying ILP to the base model, e.g., 0.5% for
APCNN(Sem) v.s. 3.4% for APCNN on the Chinese dataset.
This observation may indicate that our approach has pushed
NRE base models to learn part of the useful information be-
hind relation constraints, leaving fewer inconsistent outputs
for ILP post-processing to ﬁlter out. On the other hand, this
observation shows again that our CLC approach and the ILP
post-processing exploit complementary aspects from the re-
lation constraints, and our CLC module could be further im-
proved by taking more global optimization into account.

ACNN
ACNN(Coh)
ACNN(Sem)
APCNN
APCNN(Coh)
APCNN(Sem)

English

Chinese

Mean ∆ILP Mean ∆ILP
+5.2
93.68
+1.2
93.86
95.87
+1.5
+3.4
98.79
+0.2
98.97
99.32
+0.5

86.83
94.59
94.70
91.93
96.62
96.80

+1.5
+0.6
+0.9
+0.1
+0.4
+0.1

Table 3: Relative improvement of different models in Mean.
∆ILP is the performance difference between the mentioned
model and the same model with an extra ILP step. For ex-
ample, ∆ILP corresponding to raw ACNN indicates that ap-
plying ILP to ACNN obtains 1.5% and 5.2% gain in Mean
on English and Chinese dataset, respectively.

More Analysis

To better understand what our approach learns from the con-
straints, we take a deep look at the outputs of APCNN and
APCNN(Sem) on the test split of the Chinese dataset. First,
we count the total number of contradictory pairwise predic-
tions and ﬁnd that applying our Semantic method to APCNN
achieves a reduction of 5,966 violations, 28.0% of the total6.

6Detailed numbers per category are reported in Appendix.

This indicates our approach has pushed the base NRE mod-
els to learn from the relation constraints. However, there are
still many remaining violations since our approach operates
during training in a soft and local way, compared to ILP dur-
ing testing.

Another observation is that our approach actually reduces
the violations related to each relations, and especially does
better when there are tighter requirements on the relation’s
arguments. For example, APCNN(Sem) reduces 89.6% vio-
lations for relation locationState compared to APCNN, but
for locationRegion, it only reduces 36.3%. This is because
the relation constraints may indicate more clear arguments’
type requirements for locationState than those of location-
Region, which are captured by our CLC module to push into
the base NRE during training.

Conclusion

In this paper, we propose a uniﬁed framework to effec-
tively integrate discrete relation constraints with neural net-
works for relation extraction. Speciﬁcally, we develop two
approaches to evaluate how well NRE predictions satisfy our
relation constraints in a batch-wise, from both general and
precise perspectives. We explore our approach on English
and Chinese dataset, and the experimental results show that
our approach can help the base NRE models to effectively
learn from the discrete relation constraints, and outperform
popular NRE models as well as their ILP enhanced versions.
Our study reveals that learning with the constraints can bet-
ter utilize the constraints from a different perspective com-
pared to the ILP post-processing method.

Acknowledgments

We thank anonymous reviewers for their valuable sugges-
tions. This work is supported in part by the National Hi-Tech
R&D Program of China (2018YFC0831900) and the NSFC
Grants (No.61672057, 61672058). For any correspondence,
please contact Yansong Feng.

Appendix

Parameter Settings
In the experiment, both ACNN and APCNN use convolution
window size 3, sentence embedding size 256, position em-
bedding size 5 and batch size 50. The word embedding size
is 50 and 300 for the English and Chinese dataset, respec-
tively. We use Adam with learning rate 0.001 to train our
model. And we ﬁne-tune the constraint loss coefﬁcients for
each experimental settings, reported in Table 4

English dataset

Chinese dataset

ACNN

APCNN

ACNN

APCNN

Sem 1 × 10−3
5 × 10−3
Coh

1 × 10−4
1 × 10−4

5 × 10−4
1 × 10−4

1 × 10−5
1 × 10−4

Table 4: The value of coeffecient λ for each experimental
settings.

Local Loss Calculating Indicators
In this section, we list the assignment methods for all
I mn
φ which indicates whether to calculate local loss term
L(pm, pn, Cφ) for the combination of the mth and nth
instances, < subjm, rm, objm > and < subjn, rn, objn >,
within a batch.

I mn
ts =

(cid:26) 1,
0,

subjm = subjn;
subjm (cid:54)= subjn.

(7)

where subjm = subjn means the two triples have the same
subject type, thus, corresponding predicted relation pair may
be contradictory with Cts.
(cid:26) 1,
0,

objm = objn;
objm (cid:54)= objn.

I mn
to =

(8)

where objm = objn means the two triples have the same
object type, thus, corresponding predicted relation pair may
be contradictory with Cto.

I mn
tso =

(cid:26) 1,
0,

subjm = objn||objm = subjn;
otherwise.

(9)

where subjm = objn||objm = subjn means the subject
type of one relation is same as the object type of the other,
thus, corresponding predicted relation pair may be contra-
dictory with Ctso.

I mn
cs =

(cid:26) 1,
0,

subjm (cid:54)= subjn&&objm = objn;
otherwise.

(10)

where subjm (cid:54)= subjn&&objm = objn means that for a
given object, there are multiple subjects, thus, corresponding
predicted relation pair may be contradictory with Ccs.

I mn
co =

(cid:26) 0,
1,

objm (cid:54)= objn&&subjm = subjn;
otherwise.

(11)

where objm (cid:54)= objn&&subjm = subjn means that for a
given subject, there are multiple objects, thus, corresponding
predicted relation pair may be contradictory with Cco.

Statistics on Violations for Each Constraint Set

In this section, we collect the number of violations for
each speciﬁc constraint set among the relation predictions
of APCNN and APCNN(Sem) on Chinese dataset, shown as
in Table 5.

Cts

Cto

Ctso

APCNN
APCNN(Sem)

850
596

11,183
6,772

7,636
6,573

Ccs

1,464
1,259

Cco

209
176

Total

21,342
15,376

Table 5: Statistics on predicted relation pairs which are con-
tradictory with constraint set Cφ for test data of Chinese
dataset.

Further Discussions on Training Procedure

First, adjusting the coefﬁcient λ of our constraint loss by a
dynamic mechanism during training would be helpful. Par-
ticularly, we use Eq.12 to dynamic adjust λ.

λ = −α ∗

2 ∗ |Ecur − 0.5 ∗ Etotal|
Etotal

+ α

(12)

where α is a constant which represents the max value of λ,
Ecur and Etotal represent the current epoch number and the
total epoch number, respectively. By Eq.12, we make λ ﬁrst
rise and then fall, since we think the NRE model should more
focus on the original loss at the start of training, and the in-
ﬂuence of relation constraints should decrease after the NRE
model has learned relation constraints pretty well. We apply
this dynamic mechanism on English dataset with APCNN
as base model, and achieve 99.33% compared to 99.32% of
constant λ in Mean. We think may be a more nicely dynamic
mechanism which captures the inherent of combining rela-
tion constraints with NRE models could fetch more improve-
ment.

In addition, organizing related instances into a same mini-
batch seems to be helpful too, while how to make the reor-
ganized data evenly distributed and maintaining the random-
ness of data at the same time, is very challenging. We leave
this modiﬁcation into the future work.

References
[Bizer et al. 2009] Bizer, C.; Lehmann, J.; Kobilarov, G.;
Auer, S.; Becker, C.; Cyganiak, R.; and Hellmann, S. 2009.
Dbpedia-a crystallization point for the web of data. Web Se-
mantics: science, services and agents on the world wide web
7(3):154–165.
[Chen et al. 2018] Chen, L.; Feng, Y.; Huang, S.; Luo, B.;
and Zhao, D. 2018. Encoding implicit relation requirements
for relation extraction: A joint inference approach. Artiﬁcial
Intelligence 265:45–66.
[Dai, Li, and Xu 2016] Dai, Z.; Li, L.; and Xu, W. 2016. Cfo:
Conditional focused neural question answering with large-
scale knowledge bases. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, 800–810.

knowledge base construction. SIGMOD Records journal,
March.
[Surdeanu et al. 2012] Surdeanu, M.; Tibshirani, J.; Nallap-
ati, R.; and Manning, C. D. 2012. Multi-instance multi-label
learning for relation extraction. In Proceedings of the 2012
joint conference on empirical methods in natural language
processing and computational natural language learning,
455–465. Association for Computational Linguistics.
[Vashishth et al. 2018] Vashishth, S.; Joshi, R.; Prayaga,
S. S.; Bhattacharyya, C.; and Talukdar, P. 2018. Reside: Im-
proving distantly-supervised neural relation extraction using
side information. arXiv preprint arXiv:1812.04361.
[Wu, He, and Hu 2018] Wu, G.; He, Y.; and Hu, X. 2018.
Entity linking: an issue to extract corresponding entity with
knowledge base. IEEE Access 6:6220–6231.
[Xu et al. 2016] Xu, K.; Reddy, S.; Feng, Y.; Huang, S.; and
Zhao, D. 2016. Question answering on Freebase via rela-
tion extraction and textual evidence. In Proceedings of the
54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2326–2336. Berlin,
Germany: Association for Computational Linguistics.
[Xu et al. 2018] Xu, J.; Zhang, Z.; Friedman, T.; Liang, Y.;
and den Broeck, G. V. 2018. A semantic loss function
for deep learning with symbolic knowledge. In Proceedings
of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July
10-15, 2018, 5498–5507.
[Yu et al. 2017] Yu, M.; Yin, W.; Hasan, K. S.; dos Santos,
C.; Xiang, B.; and Zhou, B. 2017. Improved neural relation
detection for knowledge base question answering. In Pro-
ceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), 571–
581. Vancouver, Canada: Association for Computational
Linguistics.
[Zeng et al. 2014] Zeng, D.; Liu, K.; Lai, S.; Zhou, G.; and
Zhao, J. 2014. Relation classiﬁcation via convolutional
deep neural network. In Proceedings of COLING 2014, the
25th International Conference on Computational Linguis-
tics: Technical Papers, 2335–2344.
[Zeng et al. 2015] Zeng, D.; Liu, K.; Chen, Y.; and Zhao,
J. 2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In EMNLP, 1753–
1762.

[Feng et al. 2018] Feng, J.; Huang, M.; Zhao, L.; Yang, Y.;
and Zhu, X. 2018. Reinforcement learning for relation clas-
siﬁcation from noisy data. In Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence.
[He et al. 2018] He, Z.; Chen, W.; Li, Z.; Zhang, M.; Zhang,
W.; and Zhang, M. 2018. See: Syntax-aware entity embed-
ding for neural relation extraction. In Thirty-Second AAAI
Conference on Artiﬁcial Intelligence.
[Hoffmann et al. 2011] Hoffmann, R.; Zhang, C.; Ling, X.;
Zettlemoyer, L.; and Weld, D. S. 2011. Knowledge-based
weak supervision for information extraction of overlapping
relations. In Proceedings of ACL, 541–550.
[Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing,
E. 2016. Harnessing deep neural networks with logic rules.
arXiv preprint arXiv:1603.06318.
[Ji et al. 2017] Ji, G.; Liu, K.; He, S.; and Zhao, J. 2017. Dis-
tant supervision for relation extraction with sentence-level
attention and entity descriptions. In Thirty-First AAAI Con-
ference on Artiﬁcial Intelligence.
[Jia et al. 2019] Jia, W.; Dai, D.; Xiao, X.; and Wu, H. 2019.
Arnor: Attention regularization based noise reduction for
distant supervision relation classiﬁcation. In Proceedings of
the 57th Conference of the Association for Computational
Linguistics, 1399–1408.
[Lai et al. 2019] Lai, Y.; Feng, Y.; Yu, X.; Wang, Z.; Xu, K.;
and Zhao, D. 2019. Lattice cnns for matching based chinese
question answering. arXiv preprint arXiv:1902.09087.
[Lin et al. 2016] Lin, Y.; Shen, S.; Liu, Z.; Luan, H.; and Sun,
M. 2016. Neural relation extraction with selective attention
over instances. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1:
Long Papers), volume 1, 2124–2133.
[Luo et al. 2018] Luo, B.; Feng, Y.; Wang, Z.; Huang, S.;
Yan, R.; and Zhao, D. 2018. Marrying up regular expres-
sions with neural networks: A case study for spoken lan-
In Proceedings of the 56th Annual
guage understanding.
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, 2083–2093.
[Mintz et al. 2009] Mintz, M.; Bills, S.; Snow, R.; and Juraf-
sky, D. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of
the AFNLP: Volume 2-Volume 2, 1003–1011. Association
for Computational Linguistics.
[Qin, Xu, and Wang 2018] Qin, P.; Xu, W.; and Wang,
W. Y.
training
for distant supervision relation extraction. arXiv preprint
arXiv:1805.09929.
[Riedel, Yao, and McCallum 2010] Riedel, S.; Yao, L.; and
McCallum, A. 2010. Modeling relations and their men-
tions without labeled text. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases,
148–163. Springer.
[Suchanek et al. 2013] Suchanek, F.; Fan, J.; Hoffmann, R.;
Riedel, S.; and Talukdar, P. P. 2013. Advances in automated

Dsgan: generative adversarial

2018.

