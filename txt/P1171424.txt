8
1
0
2
 
r
a

M
 
5
2
 
 
]

G
L
.
s
c
[
 
 
5
v
7
6
1
4
0
.
0
1
6
1
:
v
i
X
r
a

Tensorial Mixture Models

Or Sharir
Department of Computer Science
The Hebrew University of Jerusalem
Israel
or.sharir@cs.huji.ac.il

Ronen Tamari
Department of Computer Science
The Hebrew University of Jerusalem
Israel
ronent@cs.huji.ac.il

Nadav Cohen
Department of Computer Science
The Hebrew University of Jerusalem
Israel
cohennadav@cs.huji.ac.il

Amnon Shashua
Department of Computer Science
The Hebrew University of Jerusalem
Israel
shashua@cs.huji.ac.il

Abstract

Casting neural networks in generative frameworks is a highly sought-after en-
deavor these days. Contemporary methods, such as Generative Adversarial Net-
works, capture some of the generative capabilities, but not all. In particular, they
lack the ability of tractable marginalization, and thus are not suitable for many
tasks. Other methods, based on arithmetic circuits and sum-product networks, do
allow tractable marginalization, but their performance is challenged by the need to
learn the structure of a circuit. Building on the tractability of arithmetic circuits,
we leverage concepts from tensor analysis, and derive a family of generative mod-
els we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolu-
tional network structure, and in addition, lend themselves to theoretical analyses
that allow comprehensive understanding of the relation between their structure and
their expressive properties. We thus obtain a generative model that is tractable on
one hand, and on the other hand, allows effective representation of rich distribu-
tions in an easily controlled manner. These two capabilities are brought together
in the task of classiﬁcation under missing data, where TMMs deliver state of the
art accuracies with seamless implementation and design.

1

Introduction

There have been many attempts in recent years to marry generative models with neural networks,
including successful methods, such as Generative Adversarial Networks [19], Variational Auto-
Encoders [24], NADE [38], and PixelRNN [39]. Though each of the above methods has demon-
strated its usefulness on some tasks, it is yet unclear if their advantage strictly lies in their generative
nature or some other attribute. More broadly, we ask if combining generative models with neural
networks could lead to methods who have a clear advantage over purely discriminative models.

|

On the most fundamental level, if X stands for an instance and Y for its class, generative mod-
els learn P(X, Y ), from which we can also infer P(Y
X), while discriminative models learn only
P(Y
X). It might not be immediately apparent if this sole difference leads to any advantage. In Ng
and Jordan [31], this question was studied with respect to the sample complexity, proving that under
some cases it can be signiﬁcantly lesser in favor of the generative classiﬁer. We wish to highlight
a more clear cut case, by examining the problem of classiﬁcation under missing data – where the
value of some of the entries of X are unknown at prediction time. Under these settings, discrimi-
native classiﬁers typically rely on some form of data imputation, i.e. ﬁlling missing values by some
auxiliary method prior to prediction. Generative classiﬁers, on the other hand, are naturally suited

|

to handle missing values through marginalization – effectively assessing every possible completion
of the missing values. Moreover, under mild assumptions, this method is optimal regardless of the
process by which values become missing (see sec. 3).

It is evident that such application of generative models assumes we can efﬁciently and exactly com-
pute P(X, Y ), a process known as tractable inference. Moreover, it assumes we may efﬁciently
marginalize over any subset of X, a procedure we refer to as tractable marginalization. Not all
generative models have both of these properties, and speciﬁcally not the ones mentioned in the be-
ginning of this section. Known models that do possess these properties, e.g. Latent Tree Model [30],
have other limitations. A detailed discussion can be found in sec. 4, but in broad terms, all known
generative models possess one of the following shortcomings: (i) they are insufﬁciently expressive
to model high-dimensional data (images, audio, etc.), (ii) they require explicitly designing all the
dependencies of the data, or (iii) they do not have tractable marginalization. Models based on neural
networks typically solve (i) and (ii), but are incapable of (iii), while more classical methods, e.g.
mixture models, solve (iii) but suffer from (i) and (ii).

There is a long history of specifying tractable generative models through arithmetic circuits and sum-
product networks [12, 34] – computational graphs comprised solely of product and weighted sum
nodes. To address the shortcomings above, we take a similar approach, but go one step further and
leverage tensor analysis to distill it to a speciﬁc family of models we call Tensorial Mixture Models.
A Tensorial Mixture Model assumes a convolutional network structure, but as opposed to previous
methods tying generative models with neural networks, lends itself to theoretical analyses that allow
a thorough understanding of the relation between its structure and its expressive properties. We
thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective
representation of rich distributions in an easily controlled manner.

2 Tensorial Mixture Models

(cid:80)

M
d=1

P(d)P(x

One of the simplest types of tractable generative models are mixture models, where the probability
distribution is deﬁned the convex combination of M mixing components (e.g. Normal distribu-
tions): P(x) =
d; θd). Mixture models are very easy to learn, and many of them
|
are able to approximate any probability distribution, given sufﬁcient number of components, ren-
dering them suitable for a variety of tasks. The disadvantage of classic mixture models is that they
do not scale will to high dimensional data (“curse of dimensionality”). To address this challenge,
we extend mixture models, leveraging the fact many high dimensional domains (e.g. images) are
typically comprised of small, simple local structures. We represent a high dimensional instance as
Rs (called lo-
X = (x1, . . . , xN ) – an N -length sequence of s-dimensional vectors x1, . . . , xN ∈
cal structures). X is typically thought of as an image, where each local structure xi corresponds to a
local patch from that image, where no two patches are overlapping. We assume that the distribution
of individual local structures can be efﬁciently modeled by some mixture model of few components,
which for natural image patches, was shown to be the case [42]. Formally, for all i
[N ] there
di; θdi), where di is a hidden variable specifying the matching
exists di ∈
component for the i-th local structure. The probability density of sampling X is thus described by:

[M ] such that xi ∼

P (x
|

∈

P (X) =

P (d1, . . . , dN )

M

d1,...,dN =1

N

i=1

P (xi|

di; θdi)

(cid:88)

(cid:89)

(1)

where P (d1, . . . , dN ) represents the prior probability of assigning components d1, . . . , dN to their
respective local structures x1, . . . , xN . As with classical mixture models, any probability density
function P(X) could be approximated arbitrarily well by eq. 1, as M
At ﬁrst glance, eq. 1 seems to be impractical, having an exponential number of terms. In the lit-
erature, this equation is known as the “Network Polynomial” [12], and the traditional method to
overcome its intractability is to express P (d1, . . . , dN ) by an arithmetic circuit, or sum-product
networks, following certain constraints (decomposable and complete). We augment this method
by viewing P (d1, . . . , dN ) from an algebraic perspective, treating it as a tensor of order N and
Ad1,...,dN speciﬁed by N indices
dimension M in each mode, i.e., as a multi-dimensional array,
d1, . . . , dN , each ranging in [M ], where [M ]
P (d1, . . . , dN )
≡{
as the prior tensor. Under this perspective, eq. 1 can be thought of as a mixture model with tensorial
mixing weights, thus we call the arising models Tensorial Mixture Models, or TMMs for short.

Ad1,...,dN ≡

. We refer to

(see app. A).

1, . . . , M

→ ∞

}

2

Figure 1: A generative variant of Convolutional Arithmetic Circuits.

2.1 Tensor Factorization, Tractability, and Convolutional Arithmetic Circuits

Not only is it intractable to compute eq. 1, but it is also impossible to even store the prior tensor.
We argue that addressing the latter is intrinsically tied to addressing the former. For example, if
we impose a sparsity constraint on the prior tensor, then we only need to compute the few non-
zero terms of eq. 1. TMMs with sparsity constraints can represent common generative models,
e.g. GMMs (see app. B). However, they do not take full advantage of the prior tensor. Instead, we
consider constraining TMMs with prior tensors that adhere to non-negative low-rank factorizations.

= v(1)

takes a rank-1 form, i.e. there
We begin by examining the simplest case, where the prior tensor
N
exist vectors v(1), . . . , v(N )
, or in tensor product nota-
d = P (di=d) as a probability over di, and so
tion,
i P (di), then it reveals that imposing a rank-1 constraint is actually equivalent
P (d1, . . . , dN ) =
to assuming the hidden variables d1, . . . , dN are statistically independent. Applying it to eq. 1 re-
sults in the tractable form P (X) =
di, θdi), or in other words, a product
of mixture models. Despite the familiar setting, this strict assumption severely limits expressivity.

RM such that
v(N ). If we interpret1 v(i)

d=1 P (di=d)P (xi|

Ad1,...,dN =

A
i=1 v(i)

⊗ · · · ⊗

N
i=1

(cid:81)

(cid:81)

A

∈

M

di

In a broader setting, we look at general factorization schemes that given sufﬁcient resources could
represent any tensor. Namely, the CANDECOMP/PARAFAC (CP) and the Hierarchical Tucker (HT)
factorizations. The CP factorization is simply a sum of rank-1 tensors, extending the previous case,
and HT factorization can be seen as a recursive application of CP (see def. in app. C). Since both
factorization schemes are solely based on product and weighted sum operations, they could be re-
alized through arithmetic circuits. As shown by Cohen et al. [9], this gives rise to a speciﬁc class
of convolutional networks named Convolutional Arithmetic Circuits (ConvACs), which consist of
1-convolutions, non-overlapping product pooling layers, and linear activations. More speciﬁ-
1
×
cally, the CP factorization corresponds to shallow ConvACs, HT corresponds to deep ConvACs, and
the number of channels in each layer corresponds to the respective concept of “rank” in each factor-
ization scheme. In general, when a tensor factorization is applied to eq. 1, inference is equivalent to
M,N
d=1,i=1, in what we call the
ﬁrst computing the likelihoods of all mixing components
representation layer, followed by a ConvAC. A complete network is illustrated in ﬁg. 1.

P (xi|
{

d; θd)
}

(cid:81)

(cid:80)

When restricting the prior tensor of eq. 1 to a factorization, we must
ensure it represents actual probabilities, i.e. it is non-negative and
its entries sum to one. This can be addressed through a restriction
to non-negative factorizations, which translates to limiting the pa-
rameters of each convolutional kernel to the simplex. There is a
vast literature on the relations between non-negative factorizations
and generative models [21, 30]. As opposed to most of these works,
we apply factorizations merely to derive our model and analyze its
expressivity – not for learning its parameters (see sec. 2.3).

From a generative perspective, the restriction of convolutional ker-
nels to the simplex results in a latent tree graphical model, as illus-
trated in ﬁg. 2. Each hidden layer in the ConvAC network – a pair
of convolution and pooling operations, corresponds to a transition
between two levels in the tree. More speciﬁcally, each level is com-
prised of multiple latent variables, one for each spatial position in
the input to a hidden layer in the network. Each latent variable in
the input to the l-th layer takes values in [rl

Figure 2: Graphical model
description of HT-TMM

1A represents a probability, and w.l.o.g. we can assume all entries of v(i) are non-negative and (cid:80)M

d=1 v(i)

d =1

1] – the number of channels in the layer that precedes it.

−

3

Pooling operations in the network correspond to the parent-child relationships in the tree – a set of
latent variables are siblings with a shared parent in the tree, if they are positioned in the same pooling
window in the network. The weights of convolution operations correspond to the transition matrix
between a parent and each of its children, i.e. if Hp is the parent latent variable, taking values in
Hp=c)=w(c)
1], then P (Hchild=i
[rl], and Hchild is one of its child variables, taking values in [rl
,
|
where w(c) is the 1
1 convolutional kernel for the c-th output channel. With the above graphical
representation in place, we can easily draw samples from our model.

×

−

i

To conclude this subsection, by leveraging an algebraic perspective of the network polynomial
(eq. 1), we show that tractability is related to the tensor properties of the priors, and in particular,
that low rank factorizations are equivalent to inference via ConvACs. The application of arithmetic
circuits to achieve tractability is by itself not a novelty. However, the particular convolutional arith-
metic circuits we propose lead to a comprehensive understanding of representational abilities, and
as a result, to a straightforward architectural design of TMMs.

2.2 Controlling the Expressivity and Inductive Bias of TMMs

As discussed in sec. 1, it is not enough for a generative model to be tractable – it must also be
sufﬁciently expressive, and moreover, we must also be able to understand how its structure affects
its expressivity. In this section we explain how our algebraic perspective enables us to achieve this.

To begin with, since we derived our model by factorizing the prior tensor, it immediately follows
that given sufﬁcient number of channels in the ConvAC, i.e. given sufﬁcient ranks in the tensor fac-
torization, any distribution could be approximated arbitrarily well (assuming M is allowed to grow).
In short, this amounts to saying that TMMs are universal. Though many other generative models are
known to be universal, it is typically not clear how one may assess what a given structure of ﬁnite
size can and cannot express. In contrast, the expressivity of ConvACs has been throughly studied in
a series of works [9, 8, 11, 27], each of which examined a different attribute of its structure. In Cohen
et al. [9] it was proven that ConvACs exhibit the Depth Efﬁciency property, i.e. deep networks are
exponentially more expressive than shallow ones. In Cohen and Shashua [8] it was shown that deep
networks can efﬁciently model some input correlations but not all, and that by designing appropriate
pooling schemes, different preferences may be encoded, i.e. the inductive bias may be controlled. In
Cohen et al. [11] this result was extended to more complex connectivity patterns, involving mixtures
of pooling schemes. Finally, in Levine et al. [27], an exact relation between the number of channels
and the correlations supported by a network has been found, enabling tight control over expressiv-
ity and inductive bias. All of these results are brought forth by the relations of ConvACs to tensor
factorizations. They allow TMMs to be analyzed and designed in much more principled ways than
alternative high-dimensional generative models.2

2.3 Classiﬁcation and Learning

|

∈

Y =y) for each y

TMMs realized through ConvACs, sharing many of the same traits as ConvNets, are especially suit-
able to serve as classiﬁers. We begin by introducing a class variable Y , and model the conditional
likelihood P(X
[K]. Though it is possible to have separate generative models for
each class, it is much more efﬁcient to leverage the relation to ConvNets and use a shared ConvAC
instead, which is equivalent to a joint-factorization of the prior tensors for all classes. This results
in a single network, where instead of a single scalar output representing P(X), multiple outputs are
driven by the network, representing P(X
Y =y) for each class y. Predicting the class of a given
|
instance is carried through Maximum A-Posteriori, i.e. by returning the most likely class. In the
common setting of uniform class priors, i.e. P(Y =y)
1
K , this corresponds to classiﬁcation by max-
imal network output, as customary with ConvNets. We note that in practice, na¨ıve implementation of
ConvACs is not numerically stable3, and this is treated by performing all computations in log-space,
which transforms ConvACs into SimNets – a recently introduced deep learning architecture [7, 10].

≡

S
Suppose now that we are given a training set S =
|i=1 of instances and
[K])
|
}
labels, and would like to ﬁt the parameters Θ of our model according to the Maximum Likelihood

(Rs)N , Y (i)

(X (i)
{

∈

∈

2 As a demonstration of the fact that ConvAC analyses are not affected by the non-negativity and normal-
ization restrictions of our generative variant, we prove in app. D that the Depth Efﬁciency property still holds.
3Since high degree polynomials (as computed by ACs) are susceptible to numerical underﬂow or overﬂow.

4

−

principle, or equivalently, by minimizing the Negative Log-Likelihood (NLL) loss function:
E[

log PΘ(X, Y )]. The latter can be factorized into two separate loss terms:

L

(Θ) =

(Θ) = E[

log PΘ(Y

X)] + E[
|

log PΘ(X)]

L

−

−

−
where E[
log PΘ(Y
X)], which we refer to as the discriminative loss, is commonly known as
|
log PΘ(X)], which corresponds to maximizing the prior likelihood
the cross-entropy loss, and E[
P(X), has no analogy in standard discriminative classiﬁcation.
It is this term that captures the
generative nature of the model, and we accordingly refer to it as the generative loss. Now, let
NΘ(X (i); y):= log PΘ(X (i)
Y =y) stand for the y’th output of the SimNet (ConvAC in log-space)
|
realizing our model with parameters Θ. In the case of uniform class priors (P(Y =y)
1/K), the
empirical estimation of

(Θ) may be written as:

≡

−

L

−

log

S
|
|
i=1

1
S
|

(Θ; S) =

eNΘ(X (i);Y (i))
K
y=1 eNΘ(X (i);y) −
This objective includes the standard softmax loss as its ﬁrst term, and an additional generative loss
as its second. Rather than employing dedicated Maximum Likelihood methods for training (e.g. Ex-
pectation Minimization), we leverage once more the resemblance between our networks and Con-
vNets, and optimize the above objective using Stochastic Gradient Descent (SGD).

eNΘ(X (i);y)

| (cid:88)

| (cid:88)

S
|
|
i=1

1
S

(cid:88)

log

(2)

(cid:80)

y=1

K

|

L

3 Classiﬁcation under Missing Data through Marginalization

A major advantage of generative models over discriminative ones lies in their ability to cope with
missing data, speciﬁcally in the context of classiﬁcation. By and large, discriminative methods ei-
ther attempt to complete missing parts of the data before classiﬁcation (a process known as data
imputation), or learn directly to classify data with missing values [28]. The ﬁrst of these approaches
relies on the quality of data completion, a much more difﬁcult task than the original one of classiﬁ-
cation under missing data. Even if the completion was optimal, the resulting classiﬁer is known to
be sub-optimal (see app. E). The second approach does not rely on data completion, but nonetheless
assumes that the distribution of missing values at train and test times are similar, a condition which
often does not hold in practice. Indeed, Globerson and Roweis [17] coined the term “nightmare at
test time” to refer to the common situation where a classiﬁer must cope with missing data whose
distribution is different from that encountered in training.

As opposed to discriminative methods, generative models are endowed with a natural mechanism for
classiﬁcation under missing data. Namely, a generative model can simply marginalize over missing
values, effectively classifying under all possible completions, weighing each completion according
to its probability. This, however, requires tractable inference and marginalization. We have already
shown in sec. 2 that TMMs support the former, and will show in sec. F that marginalization can be
just as efﬁcient. Beforehand, we lay out the formulation of classiﬁcation under missing data.

,

X

Y
) the joint distribution of (

D
is drawn conditioned on

be a random vector in Rs representing an object, and let
(
X

Let
be a random variable in [K]
representing its label. Denote by
[K])
,
∈
speciﬁc realizations thereof. Assume that after sampling a speciﬁc instance (x, y), a random binary
s
vector
0, 1
}
=x). xi is considered missing if mi is equal
(realization of
(
·|X
Q
m, whose i’th coordinate is
to zero, and observed otherwise. Formally, we consider the vector x
deﬁned to hold xi if mi=1, and the wildcard
if mi=0. The classiﬁcation task is then to predict y
given access solely to x

=x. More concretely, we sample a binary mask m

) according to a distribution

), and by (x

Rs, y

m.

M

M

∈{

(cid:12)

X

X

Y

Y

∈

∗

X

M

=m
=m

(
M
Q
(
M
Q

|X
|X
is independent of the missing values in

Following the works of Rubin [36], Little and Rubin [28], we consider three cases for the miss-
=x): missing completely at random (MCAR), where
is inde-
ingness distribution
=x) is a function of m but not of x; missing at random (MAR),
pendent of
, i.e.
where
=x) is a function of both m
=m
and x, but is not affected by changes in xi if mi=0; and missing not at random (MNAR), covering
the rest of the distributions for which
=x) is a
function of both m and x, which at least sometimes is sensitive to changes in xi when mi=0.
Let P be the joint distribution of the object
P(

|X
depends on missing values in

, and missingness mask

X
=m) =

(
M
Q

Y
=x,

:
M

, label

(
Q

, i.e.

=m

, i.e.

=x)

=m

=y)

=x,

=y,

M

M

M

|X

X

X

(
X

D

Y

(
M

· Q

|X

X

Y

M

(cid:12)

5

Rs and m

∈

∈{

0, 1
}

s, denote by o(x, m) the event where the random vector

coincides
For given x
with x on the coordinates i for which mi=1. For example, if m is an all-zero vector, o(x, m) covers
the entire probability space, and if m is an all-one vector, o(x, m) corresponds to the event
=x.
X
With these notations in hand, we are now ready to characterize the optimal predictor in the presence
of missing data. The proofs are common knowledge, but provided in app. E for completeness.
Claim 1. For any data distribution
rule in terms of 0-1 loss is given by predicting the class y
P(

, the optimal classiﬁcation
=y

Q
[K], that maximizes P(

and missingness distribution

=y), for an instance x

o(x, m))
|

m.

D

X

Y

∈

·

M

o(x, m),

=m
|
Y
When the distribution
to as the marginalized Bayes predictor:
Corollary 1. Under the conditions of claim 1, if the distribution
classiﬁcation rule may be written as:

Q

(cid:12)

Q

is MAR (or MCAR), the optimal classiﬁer admits a simpler form, referred

is MAR (or MCAR), the optimal

h∗(x

m) = argmaxy

(cid:12)

P(

Y

|

=y

o(x, m))

(3)

Corollary 1 indicates that in the MAR setting, which is frequently encountered in practice, optimal
classiﬁcation does not require prior knowledge regarding the missingness distribution
. As long
as one is able to realize the marginalized Bayes predictor (eq. 3), or equivalently, to compute the
likelihoods of observed values conditioned on labels (P(o(x, m)
Y =y)), classiﬁcation under miss-
|
ing data is guaranteed to be optimal, regardless of the corruption process taking place. This is in
stark contrast to discriminative methods, which require access to the missingness distribution during
training, and thus are not able to cope with unknown conditions at test time.

Q

Most of this section dealt with the task of prediction given an input with missing data, where we
assumed we had access to a “clean” training set, and only faced missingness during prediction.
However, many times we wish to tackle the reverse task, where the training set itself is riddled with
missing data. Tractability leads to an advantage here as well: under the MAR assumption, learning
from missing data with the marginalized likelihood objective results in an unbiased classiﬁer [28].

In the case of TMMs, marginalizing over missing values is just as efﬁcient as plain inference – re-
quires only a single pass through the corresponding network. The exact mechanism is carried out in
similar fashion as in sum-product networks, and is covered in app. F. Accordingly, the marginalized
Bayes predictor (eq. 3) is realized efﬁciently, and classiﬁcation under missing data (in the MAR
setting) is optimal (under generative assumption), regardless of the missingness distribution.

4 Related Works

There are many generative models realized through neural networks, and convolutional networks
in particular, e.g. Generative Adversarial Networks [19], Variational Auto-Encoders [24], and
NADE [38]. However, most do not posses tractable inference, and of the few that do, non posses
tractable marginalization over any set of variables. Due to limits of space, we defer the discussion
on the above to app. G, and in the remainder of this section focus instead on the most relevant works.

As mentioned in sec. 2, we build on the approach of specifying generative models through Arith-
metic Circuits (ACs) [12], and speciﬁcally, our model is a strict subclass of the well-known Sum-
Product Networks (SPNs) [34], under the decomposable and complete restrictions. Where our work
differs is in our algebraic approach to eq. 1, which gives rise to a speciﬁc structure of ACs, called
ConvACs, and a deep theory regarding their expressivity and inductive bias (see sec. 2.2). In contrast
to the structure we proposed, the current literature on general SPNs does not prescribe any speciﬁc
structures, and its theory is limited to either very speciﬁc instances [14], or very broad classes, e.g
ﬁxed-depth circuits [29]. In the early works on SPNs, specialized networks of complex structure
were designed for each task based mainly on heuristics, often bearing little resemblance to common
neural networks. Contemporary works have since moved on to focus mainly on learning the struc-
ture of SPNs directly from data [33, 16, 1, 35], leading to improved results in many domains. Despite
that, only few published studies have applied this method to natural domains (images, audio, etc.),
on which only limited performance, compared to other common methods, was reported, speciﬁcally
on the MNIST dataset [1]. The above suggests that choosing the right architecture of general SPNs,
at least on some domains, remains to be an unsolved problem. In addition, both the previously
studied manually-designed SPNs, as well as ones with a learned structure, lead to models, which

6

n=

0

25

50

75

100

125

150

96.4
97.8
Table 1: Prediction for each two-class task of MNIST digits, under feature deletion noise.

97.9
HT-TMM 98.5

97.5
98.2

94.1
96.5

89.2
93.9

80.9
87.1

70.2
76.3

LP

ptest

0.25

0.50

0.75

0.90

0.95

0.99

ptrain
0.25
0.50
0.75
0.90
0.95
0.99

i.i.d. (rand)
rects (rand)

98.9
99.1
98.9
97.6
95.7
87.3

98.7
98.2

97.8
98.6
98.7
97.5
95.6
86.7

98.4
95.7

78.9
94.6
97.2
96.7
94.8
85.0

97.0
83.2

32.4
68.1
83.9
89.0
88.3
78.2

87.6
54.7

17.6
37.9
56.4
71.0
74.0
66.2

70.6
35.8

11.0
12.9
16.7
21.3
30.5
31.3

29.6
17.5

(a) MNIST with i.i.d. corruption

(b) MNIST with missing rectangles.

Figure 3: We examine ConvNets trained on one missingness distribution while tested on others.
“(rand)” denotes training on distributions with randomized parameters. (a) i.i.d. corruption: trained
with probability ptrain and tested on ptest. (b) missing rectangles: training on randomized distributions
(rand) compared to training on the same (ﬁxed) missing rectangles distribution.

according to recent works on GPU-optimized algorithms [4], cannot be efﬁciently implemented due
to their irregular memory access patterns. This is in stark contrast to our model, which leverages
the same patterns as modern ConvNets, and thus enjoys similar run-time performance. An addi-
tional difference in our work is that we manage to successfully train our model using standard SGD.
Even though this approach has already been considered by Poon and Domingos [34], they deemed
it lacking and advocated for specialized optimization algorithms instead.

Outside the realm of generative networks, tractable graphical models, e.g. Latent Tree Mod-
els (LTMs) [30], are the most common method for tractable inference. Similar to SPNs, it is not
straightforward to ﬁnd the proper structure of graphical models for a particular problem, and most
of the same arguments apply here as well. Nevertheless, it is noteworthy that recent progress in
structure and parameters learning of LTMs [22, 3] was also brought forth by connections to tensor
factorizations, similar to our approach. Unlike the aforementioned algorithms, we utilize tensor
factorizations solely for deriving our model and analyzing its expressivity, while leaving learning
to SGD – the most successful method for training neural networks. Leveraging their perspective to
analyze the optimization properties of our model is viewed as a promising avenue for future research.

5 Experiments

We demonstrate the properties of TMMs through both qualitative and quantitative experiments. In
sec. 5.1 we present state of the art results on image classiﬁcation under missing data, with robustness
to various missingness distributions. In sec. 5.2 we show that our results are not limited to images, by
applying TMMs for speech recognition. Finally, in app. H we show visualizations of samples drawn
from TMMs, shedding light on their generative nature. Our implementation, based on Caffe [23] and
MAPS [4] (toolbox for efﬁcient GPU code generation), as well as all other code for reproducing our
experiments, are available at: https://github.com/HUJI-Deep/Generative-ConvACs. Ex-
tended details regarding the experiments are provided in app. I.

5.1

Image Classiﬁcation under Missing Data

In this section we experiment on two datasets: MNIST [25] for digit classiﬁcation, and small
NORB [26] for 3D object recognition. In our results, we refer to models using shallow networks as
CP-TMM, and to those using deep networks as HT-TMM, in accordance with the respective tensor
factorizations (see sec. 2). The theory discussed in sec. 2.2 guided our exact choice of architectures.
Namely, we used the fact [27] that the capacity to model either short- or long-range correlations in
the input, is related to the number of channels in the beginning or end of a network, respectively. In
MNIST, discriminating between digits has more to do with long-range correlations than the basic
strokes digits are made of, hence we chose to start with few channels and end with many – layer

7

(a) MNIST with i.i.d. corruption.

(b) MNIST with missing rectangles.

(c) NORB with i.i.d. corruption.

(d) NORB with missing rectangles.

Figure 4: Blind classiﬁcation under missing data. (a,c) Testing i.i.d. corruption with probability p
for each pixel. (b,d) Testing missing rectangles corruption with n missing rectangles, each of width
and hight equal to W . (*) Based on the published results [18]. (
†

) Data imputation algorithms.

widths were set to 64-128-256-512. In contrast, the classes of NORB differ in much ﬁner details,
requiring more channels in the ﬁrst layers, hence layer widths were set to 256-256-256-512. In both
cases, M = 32 Gaussian mixing components were used.

We begin by comparing our generative approach to missing data against classical methods, namely,
methods based on Globerson and Roweis [17]. They regard missing data as “feature deletion” noise,
replace missing entries by zeros, and devise a learning algorithm over linear predictors that takes
the number of missing features, n, into account. The method was later improved by Dekel and
Shamir [13]. We compare TMMs to the latter, with n non-zero pixels randomly chosen and changed
to zero, in the two-class prediction task derived from each pair of MNIST digits. Due to limits of
their implementation, only 300 images per digit are used for training. Despite this, and the fact that
the evaluated scenario is of the MNAR type (on which optimality is not guaranteed – see sec. 3),
we achieve signiﬁcantly better results (see table 1), and unlike their method, which requires several
classiﬁers and knowing n, we use a single TMM with no prior knowledge.

∈

Heading on to multi-class prediction under missing data, we focus on the challenging “blind” setting,
where the missingness distribution at test time is completely unknown during training. We simulate
[0, 1] of
two kinds of MAR missingness distributions: (i) an i.i.d. mask with a ﬁxed probability p
dropping each pixel, and (ii) a mask composed of the union of n (possibly overlapping) rectangles
of width and height W , each positioned randomly in the image (uniform distribution). We ﬁrst
demonstrate that purely discriminative classiﬁers cannot generalize to all missingness distributions,
by training the standard LeNeT ConvNet [25] on one set of distributions and then testing it on others
(see ﬁg. 3). Next, we present our main results. We compare our model against three different
approaches. First, as a baseline, we use K-Nearest Neighbors (KNN) to vote on the most likely
class, augmented with an l2-metric that disregards missing coordinates. KNN actually scores better
than most methods, but its missingness-aware distance metric prevents the common memory and
runtime optimizations, making it impractical for real-world settings. Second, we test various data-
imputation methods, ranging from simply ﬁlling missing pixels with zeros or their mean, to modern
generative models suited to inpainting. Data imputation is followed by a ConvNet prediction on
the completed image.
In general, we ﬁnd that this approach only works well when few pixels
are missing. Finally, we test generative classiﬁers other than our model, including MP-DBM and
SPN (sum-product networks). MP-DBM is notable for being limited to approximations, and its
results show the importance of using exact inference instead. For SPN, we have augmented the
model from Poon and Domingos [34] with a class variable Y , and trained it to maximize the joint
probability P (X, Y ) using the code of Zhao et al. [41]. The inferior performance of SPN suggests

8

that the structure of TMMs, which are in fact a special case, is advantageous. Due to limitations of
available public code and time, not all methods were tested on all datasets and distributions. See
ﬁg. 4 for the complete results.

To conclude, TMMs signiﬁcantly outperform all other methods tested on image classiﬁcation with
missing data. Although they are a special case of SPNs, their particular structure appears to be
more effective than ones existing in the literature. We attribute this superiority to the fact that their
architectural design is backed by comprehensive theoretical studies (see sec. 2.2).

5.2 Speech Recognition under Missing Data

To demonstrate the versatility of TMMs, we also conducted limited experiments on the TIMIT
speech recognition dataset, following the same protocols as in sec. 5.1. We trained a TMM and a
standard ConvNet on 256ms windows of raw data at 16Hz sample rate to predict the phoneme at the
center of a window. Both the TMM and the ConvNet reached 78% accuracy on the clean dataset,
but when half of the audio is missing i.i.d., accuracy of the ConvNet with mean imputation drops
to 34%, while the TMM remains at 63%. Utilizing common audio inpainting methods [2] only
improves accuracy of the ConvNet to 48%, well below that of TMM.

6 Summary

This paper focuses on generative models which admit tractable inference and marginalization, ca-
pabilities that lie outside the realm of contemporary neural network-based generative methods. We
build on prior works on tractable models based on arithmetic circuits and sum-product networks,
and leverage concepts from tensor analysis to derive a sub-class of models we call Tensorial Mixture
Models (TMMs). In contrast to existing methods, our algebraic approach leads to a comprehensive
understanding of the relation between model structure and representational properties. In practice,
utilizing this understanding for the design of TMMs has led to state of the art performance in clas-
siﬁcation under missing data. We are currently investigating several avenues for future research,
including semi-supervised learning, and examining more intricate ConvAC architectures, such as
the ones suggested by Cohen et al. [11]).

This work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12 and
by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google
Fellowship in Machine Learning.

Acknowledgments

References

[1] Tameem Adel, David Balduzzi, and Ali Ghodsi. Learning the Structure of Sum-Product Networks via an

SVD-based Algorithm. UAI, 2015.

[2] A Adler, V Emiya, M G Jafari, and M Elad. Audio inpainting.

IEEE Trans. on Audio, Speech and

Language Processing, 20:922–932, March 2012.

[3] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research (), 15(1):2773–2832,
2014.

[4] Tal Ben-Nun, Ely Levy, Amnon Barak, and Eri Rubin. Memory Access Patterns: The Missing Piece of
the Multi-GPU Puzzle. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, pages 19:1–19:12. ACM, 2015.

[5] Yoshua Bengio, ´Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep Generative Stochas-

tic Networks Trainable by Backprop. In International Conference on Machine Learning, 2014.

[6] Richard Caron and Tim Traynor. The Zero Set of a Polynomial. WSMR Report 05-02, 2005.

[7] Nadav Cohen and Amnon Shashua. SimNets: A Generalization of Convolutional Networks. In Advances

in Neural Information Processing Systems NIPS, Deep Learning Workshop, 2014.

9

[8] Nadav Cohen and Amnon Shashua.

Inductive Bias of Deep Convolutional Networks through Pooling

Geometry. In International Conference on Learning Representations ICLR, April 2017.

[9] Nadav Cohen, Or Sharir, and Amnon Shashua. On the Expressive Power of Deep Learning: A Tensor

Analysis. In Conference on Learning Theory COLT, May 2016.

[10] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep SimNets. In Computer Vision and Pattern Recogni-

tion CVPR, May 2016.

[11] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting Dilated Convolutional Networks with

Mixed Tensor Decompositions. arXiv.org, 2017.

[12] Adnan Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM (JACM),

50(3):280–305, May 2003.

[13] Ofer Dekel and Ohad Shamir. Learning to classify with missing and corrupted features. In International

Conference on Machine Learning. ACM, 2008.

[14] Olivier Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks. Advances in Neural

Information Processing Systems, pages 666–674, 2011.

[15] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estima-

tion. arXiv.org, October 2014.

on Machine Learning, 2013.

[16] R Gens and P M Domingos. Learning the Structure of Sum-Product Networks. Internation Conference

[17] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Inter-

national Conference on Machine Learning. ACM, 2006.

[18] Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-Prediction Deep Boltzmann

Machines. Advances in Neural Information Processing Systems, 2013.

[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing
Systems, 2014.

[20] W Hackbusch and S K¨uhn. A New Scheme for the Tensor Representation. Journal of Fourier Analysis

and Applications, 15(5):706–722, 2009.

[21] Thomas Hofmann. Probabilistic latent semantic analysis. Morgan Kaufmann Publishers Inc., July 1999.

[22] Furong Huang, Niranjan U N, Ioakeim Perros, Robert Chen, Jimeng Sun, and Anima Anandkumar. Scal-
able Latent Tree Model and its Application to Health Analytics. In NIPS Machine Learning for Healthcare
Workshop, 2015.

[23] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional Architecture for Fast Feature Embedding. CoRR
abs/1202.2745, cs.CV, 2014.

[24] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on

Learning Representations, 2014.

[25] Yan LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-

ment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[26] Yann LeCun, Fu Jie Huang, and L´eon Bottou. Learning Methods for Generic Object Recognition with

Invariance to Pose and Lighting. Computer Vision and Pattern Recognition, 2004.

[27] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep Learning and Quantum Entangle-

ment: Fundamental Connections with Implications to Network Design. arXiv.org, April 2017.

[28] Roderick J A Little and Donald B Rubin. Statistical analysis with missing data (2nd edition). John Wiley

& Sons, Inc., September 2002.

CoRR abs/1202.2745, cs.LG, 2014.

[29] James Martens and Venkatesh Medabalimi. On the Expressive Efﬁciency of Sum Product Networks.

[30] Rapha¨el Mourad, Christine Sinoquet, Nevin Lianwen Zhang, Tengfei Liu, and Philippe Leray. A Survey

on Latent Tree Models and Applications. J. Artif. Intell. Res. (), cs.LG:157–203, 2013.

10

[31] Andrew Y Ng and Michael I Jordan. On Discriminative vs. Generative Classiﬁers: A comparison of
logistic regression and naive Bayes. In Advances in Neural Information Processing Systems NIPS, Deep
Learning Workshop, 2002.

[32] F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, and E Duchesnay.
Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research (), 12:2825–2830,
2011.

[33] Robert Peharz, Bernhard C Geiger, and Franz Pernkopf. Greedy Part-Wise Learning of Sum-Product
Networks. In Machine Learning and Knowledge Discovery in Databases, pages 612–627. Springer Berlin
Heidelberg, Berlin, Heidelberg, September 2013.

[34] Hoifung Poon and Pedro Domingos. Sum-Product Networks: A New Deep Architecture. In Uncertainty

in Artiﬁcail Intelligence, 2011.

Variable Interactions. ICML, 2014.

[35] Amirmohammad Rooshenas and Daniel Lowd. Learning Sum-Product Networks with Direct and Indirect

[36] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, December 1976.

[37] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. Internation Conference on Machine Learning, 2015.

[38] Benigno Uria, Marc-Alexandre C ˆo t ´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural Autore-
gressive Distribution Estimation. Journal of Machine Learning Research (), 17(205):1–37, 2016.

[39] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In

International Conference on Machine Learning, 2016.

[40] Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In European

Conference on Computer Vision. Springer International Publishing, 2014.

[41] Han Zhao, Pascal Poupart, and Geoff Gordon. A Uniﬁed Approach for Learning the Parameters of
Sum-Product Networks. In Advances in Neural Information Processing Systems NIPS, Deep Learning
Workshop, 2016.

[42] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.

ICCV, pages 479–486, 2011.

11

A The Universality of Tensorial Mixture Models

In this section we prove the universality property of Generative ConvACs, as discussed in sec. 2. We begin by
taking note from functional analysis and deﬁne a new property called PDF total set, which is similar in concept
to a total set, followed by proving that this property is invariant under the cartesian product of functions, which
entails the universality of these models as a corollary.
Deﬁnition 1. Let F be a set of PDFs over Rs. F is PDF total iff for any PDF h(x) over Rs and for all (cid:15) > 0
there exists M ∈ N, {f1(x), . . . , fM (x)} ⊂ F and w ∈ (cid:52)M −1 s.t.
< (cid:15). In
other words, a set is a PDF total set if its convex span is a dense set under L1 norm.
Claim 2. Let F be a set of PDFs over Rs and let F ⊗N = {(cid:81)N
the product space (Rs)N . If F is a PDF total set then F ⊗N is PDF total set.

i=1 fi(x)|∀i, fi(x) ∈ F} be a set of PDFs over

(cid:13)
(cid:13)h(x) − (cid:80)M
(cid:13)

i=1 wifi(x)

(cid:13)
(cid:13)
(cid:13)1

Proof. If F is the set of Gaussian PDFs over Rs with diagonal covariance matrices, which is known to be a
PDF total set, then F ⊗N is the set of Gaussian PDFs over (Rs)N with diagonal covariance matrices and the
claim is trivially true.
Otherwise, let h(x1, . . . , xN ) be a PDF over (Rs)N and let (cid:15) > 0. From the above, there exists K ∈ N,
w ∈ (cid:52)M1−1 and a set of diagonal Gaussians {gij(x)}i∈[M1],j∈[N ] s.t.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gij(xj)

g(x) −

M1(cid:88)

N
(cid:89)

(cid:15)
2

(4)

wi

<

j=1

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

Additionally, since F is a PDF total set then there exists M2 ∈ N, {fk(x)}k∈[M2] ⊂ F and {wij ∈
(cid:13)
(cid:13)gij(x) − (cid:80)M2
(cid:52)M2−1}i∈[M1],j∈[N ] s.t.
(cid:13)
2N ,
from which it is trivially proven using a telescopic sum and the triangle inequality that:

for all i ∈ [M1], j ∈ [N ] it holds that

(cid:13)
(cid:13)
k=1 wijkfk(x)
(cid:13)1

< (cid:15)

i=1
From eq. 4, eq. 5 the triangle inequality it holds that:

j=1

i=1

j=1

k=1

N
(cid:89)

M1(cid:88)

N
(cid:89)

M2(cid:88)

wi

gij(x) −

wi

wijkfk(xj)

<

(5)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

(cid:15)
2

g(x) −

Ak1,...,kN

fkj (xj)

< (cid:15)

M2(cid:88)

k1,...,kN =1

N
(cid:89)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

where Ak1,...,kN = (cid:80)M1
{(cid:81)N

j=1 fkj (xj)}k1∈[M2],...,kN ∈[M2] ⊂ F ⊗N and w = vec(A) completes the proof.

(cid:81)N

j=1 wijkj which holds (cid:80)M2

k1,...,kN =1 Ak1,...,kN = 1. Taking M = M N
2 ,

Corollary 2. Let F be a PDF total set of PDFs over Rs, then the family of Generative ConvACs with mixture
components from F can approximate any P DF over (Rs)N arbitrarily well, given arbitrarily many compo-
nents.

B TMMs with Sparsity Constraints Can Represent Gaussian Mixture

Models

As discussed in sec. 2, TMMs become tractable when a sparsity constraint is imposed on the priors tensor, i.e.
most of the entries of the tensors are replaced with zeros. In this section, we demonstrate that under such a case,
TMMs can represent Gaussian Mixture Models with diagonal covariance matrices, probably the most common
type of mixture models.

With the same notations as sec. 2, assume the number of mixing components of the TMM is M = N · K for
some K ∈ N, let {N (x; µki, diag(σ2
be these components, and ﬁnally, assume the prior tensor has
the following structure:

ki))}K,N

k,i

P (d1, . . . , dN ) =

(cid:40)

wk ∀i ∈ [N ], di=N ·(k−1)+i
0

Otherwise

then eq. 1 reduces to:

(cid:88)K

(cid:89)N

wk

P (X) =

N (x; µki, diag(σ2
k = ((σ2
˜σ2
which is equivalent to a diagonal GMM with mixing weights w ∈ (cid:52)K−1 (where (cid:52)K−1 is the K-dimensional
simplex) and Gaussian mixture components with means { ˜µk}K

k=1
kN )T )T
k1)T , . . . , (σ2

k=1 and covariances {diag( ˜σ2

wkN (x; ˜µk, diag( ˜σ2

k1, . . . , µT

˜µk = (µT

ki)) =

kN )T

k)}K

k=1.

k))

k=1

i=1

(cid:88)K

(cid:13)
M1(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1 wi

12

C Background on Tensor Factorizations

In this section we establish the minimal background in the ﬁeld of tensor analysis required for following our
work. A tensor is best thought of as a multi-dimensional array Ad1,...,dN ∈ R, where ∀i ∈ [N ], di ∈ [Mi].
The number of indexing entries in the array, which are also called modes, is referred to as the order of the
tensor. The number of values an index of a particular mode can take is referred to as the dimension of the mode.
The tensor A ∈ RM1⊗...⊗MN mentioned above is thus of order N with dimension Mi in its i-th mode. For
our purposes we typically assume that M1 = . . . = MN = M , and simply denote it as A ∈ (RM )⊗N .

The fundamental operator in tensor analysis is the tensor product. The tensor product operator, denoted by ⊗,
is a generalization of outer product of vectors (1-ordered vectors) to any pair of tensors. Speciﬁcally, let A and
B be tensors of order P and Q respectively, then the tensor product A ⊗ B results in a tensor of order P + Q,
deﬁned by: (A ⊗ B)d1,...,dP +Q = Ad1,...,dP · BdP +1,...,dP +Q .
The main concept from tensor analysis we use in our work is that of tensor decompositions. The most straight-
forward and common tensor decomposition format is the rank-1 decomposition, also known as a CANDE-
COMP/PARAFAC decomposition, or in short, a CP decomposition. The CP decomposition is a natural exten-
sion of low-rank matrix decomposition to general tensors, both built upon the concept of a linear combination
of rank-1 elements. Similarly to matrices, tensors of the form v(1) ⊗ · · · ⊗ v(N ), where v(i) ∈ RMi are
non-zero vectors, are regarded as N -ordered rank-1 tensors, thus the rank-Z CP decomposition of a tensor A
is naturally deﬁned by:

A =

azaz,1 ⊗ · · · ⊗ az,N

Z
(cid:88)

z=1

Z
(cid:88)

⇒ Ad1,...,dN =

N
(cid:89)

az

az,i
di

z=1

i=1

where {az,i ∈ RMi }N,Z
i=1,z=1 and a ∈ RZ are the parameters of the decomposition. As mentioned above,
for N = 2 it is equivalent to low-order matrix factorization. It is simple to show that any tensor A can be
represented by the CP decomposition for some Z, where the minimal such Z is known as its tensor rank.

Another decomposition we will use in this paper is of a hierarchical nature and known as the Hierarchical Tucker
decomposition [20], which we will refer to as HT decomposition. While the CP decomposition combines
vectors into higher order tensors in a single step, the HT decomposition does that more gradually, combining
vectors into matrices, these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion.
Speciﬁcally, the following describes the recursive formula of the HT decomposition4 for a tensor A ∈ (RM )⊗N
where N = 2L, i.e. N is a power of two5:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

· · ·

· · ·

r0(cid:88)

α=1

rl
1
(cid:88)
−

α=1

rL
2
(cid:88)
−

α=1

rL
1
(cid:88)
−

α=1

φl,j,γ =

al,j,γ
α

φl−1,2j−1,α
(cid:124)
(cid:123)(cid:122)
(cid:125)
order 2l

1

−

⊗ φl−1,2j,α
(cid:125)
(cid:123)(cid:122)
(cid:124)
1
order 2l

−

φL−1,j,γ =

aL−1,j,γ
α

φL−2,2j−1,α
(cid:123)(cid:122)
(cid:125)
(cid:124)
order N
4

⊗ φL−2,2j,α
(cid:123)(cid:122)
(cid:125)
order N
4

(cid:124)

A =

α φL−1,1,α
aL
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

⊗ φL−1,2,α
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

(6)

(7)

where the parameters of the decomposition are the vectors {al,j,γ∈Rrl
1 }l∈{0,...,L−1},j∈[N/2l],γ∈[rl] and the
top level vector aL ∈ RrL
1 , and the scalars r0, . . . , rL−1 ∈ N are referred to as the ranks of the decompo-
sition. Similar to the CP decomposition, any tensor can be represented by an HT decomposition. Moreover,

−

−

4 More precisely, we use a special case of the canonical HT decomposition as presented in Hackbusch and
K¨uhn [20]. In the terminology of the latter, the matrices Al,j,γ are diagonal and equal to diag(al,j,γ) (using
the notations from eq. 7).

5The requirement for N to be a power of two is solely for simplifying the deﬁnition of the HT decomposi-
tion. More generally, instead of deﬁning it through a complete binary tree describing the order of operations,
the canonical decomposition can use any balanced binary tree.

13

any given CP decomposition can be converted to an HT decomposition by only a polynomial increase in the
number of parameters.

Finally, since we are dealing with generative models, the tensors we study are non-negative and sum to one, i.e.
the vectorization of A (rearranging its entries to the shape of a vector), denoted by vec(A), is constrained to lie
in the multi-dimensional simplex, denoted by:

(cid:110)

(cid:52)k :=

x ∈ Rk+1|

(cid:88)k+1
i=1

xi = 1, ∀i ∈ [k + 1] : xi ≥ 0

(cid:111)

(8)

D Proof for the Depth Efﬁciency of Convolutional Arithmetic Circuits with

Simplex Constraints

In this section we prove that the depth efﬁciency property of ConvACs that was proved in Cohen et al. [9]
applies also to the generative variant of ConvACs we have introduced in sec. 2. Our analysis relies on basic
knowledge of tensor analysis and its relation to ConvACs, speciﬁcally, that the concept of “ranks” of each
factorization scheme is equivalent to the number of channels in these networks. For completeness, we provide
a short introduction to tensor analysis in app. C. The

We prove the following theorem, which is the generative analog of theorem 1 from [9]:
Theorem 1. Let Ay be a tensor of order N and dimension M in each mode, generated by the recursive
formulas in eq. 7, under the simplex constraints introduced in sec. 2. Deﬁne r := min{r0, M }, and consider
the space of all possible conﬁgurations for the parameters of the decomposition – {al,j,γ ∈ (cid:52)rl
1−1}l,j,γ.
In this space, the generated tensor Ay will have CP-rank of at least rN/2 almost everywhere (w.r.t. the product
measure of simplex spaces). Put differently, the conﬁgurations for which the CP-rank of Ay is less than rN/2
form a set of measure zero. The exact same result holds if we constrain the composition to be “shared”, i.e. set
al,j,γ ≡ al,γ and consider the space of {al,γ ∈ (cid:52)rl

1−1}l,γ conﬁgurations.

−

−

The only differences between ConvACs and their generative counter-parts are the simplex constraints applied
to the parameters of the models, which necessitate a careful treatment to the measure theoretical arguments of
the original proof. More speciﬁcally, while the k-dimensional simplex (cid:52)k is a subset of the k + 1-dimensional
space Rk+1, it has a zero measure with respect to the Lebesgue measure over Rk+1. The standard method
to deﬁne a measure over (cid:52)k is by the Lebesgue measure over Rk of its projection to that space, i.e.
let
λ : Rk → R be the Lebesgue measure over Rk, p : Rk+1 → Rk, p(x) = (x1, . . . , xk)T be a projection,
and A ⊂ (cid:52)k be a subset of the simplex, then the latter’s measure is deﬁned as λ(p(A)). Notice that p((cid:52)k)
has a positive measure, and moreover that p is invertible over the set p((cid:52)k), and that its inverse is given by
p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
i=1 xi). In our case, the parameter space is the cartesian product
of several simplex spaces of different dimensions, for each of them the measure is deﬁned as above, and the
measure over their cartesian product is uniquely deﬁned by the product measure. Though standard, the choice
of the projection function p above could be seen as a limitation, however, the set of zero measure sets in (cid:52)k
is identical for any reasonable choice of a projection π (e.g. all polynomial mappings). More speciﬁcally, for
any projection π : Rk+1 → Rk that is invertible over π((cid:52)k), π−1 is differentiable, and the Jacobian of π−1
is bounded over π((cid:52)k), then a subset A ⊂ (cid:52)k is of measure zero w.r.t. the projection π iff it is of measure
zero w.r.t. p (as deﬁned above). This implies that if we sample the weights of the generative decomposition
(eq. 7 with simplex constraints) by a continuous distribution, a property that holds with probability 1 under the
standard parameterization (projection p), will hold with probability 1 under any reasonable parameterization.

We now state and prove a lemma that will be needed for our proof of theorem 1.
Lemma 1. Let M, N, K ∈ N, 1 ≤ r ≤ min{M, N } and a polynomial mapping A : RK → RM ×N (i.e.
for every i ∈ [M ], j ∈ [N ] then Aij : Rk → R is a polynomial function). If there exists a point x ∈ RK s.t.
rank (A(x)) ≥ r, then the set {x ∈ RK |rank (A(x)) < r} has zero measure.

Proof. Remember that rank (A(x)) ≥ r iff there exits a non-zero r × r minor of A(x), which is polynomial
(cid:1) be the number of minors in A,
in the entries of A(x), and so it is polynomial in x as well. Let c = (cid:0)M
denote the minors by {fi(x)}c
i=1 fi(x)2. It thus holds that
f (x) = 0 iff for all i ∈ [c] it holds that fi(x) = 0, i.e. f (x) = 0 iff rank (A(x)) < r.

i=1, and deﬁne the polynomial function f (x) = (cid:80)c

(cid:1) · (cid:0)N

r

r

Now, f (x) is a polynomial in the entries of x, and so it either vanishes on a set of zero measure, or it is
the zero polynomial (see Caron and Traynor [6] for proof). Since we assumed that there exists x ∈ RK s.t.
rank(A(x)) ≥ r, the latter option is not possible.

Following the work of Cohen et al. [9], our main proof relies on following notations and facts:

14

• We denote by [A] the matricization of an N -order tensor A (for simplicity, N is assumed to be
even), where rows and columns correspond to odd and even modes, respectively. Speciﬁcally, if
A ∈ RM1×···MN , the matrix [A] has M1 · M3 · . . . · MN −1 rows and M2 · M4 · . . . · MN columns,
rearranging the entries of the tensor such that Ad1,...,dN is stored in row index 1 + (cid:80)N/2
i=1(d2i−1 −
1) (cid:81)N/2
j=i+1 M2j. Additionally, the matriciza-
tion is a linear operator, i.e. for all scalars α1, α2 and tensors A1, A2 with the order and dimensions
in every mode, it holds that [α1A1 + α2A2] = α1[A1] + α2[A2].

j=i+1 M2j−1 and column index 1 + (cid:80)N/2

i=1(d2i − 1) (cid:81)N/2

• The relation between the Kronecker product (denoted by (cid:12)) and the tensor product (denoted by ⊗)

is given by [A ⊗ B] = [A] (cid:12) [B].

• For any two matrices A and B, it holds that rank (A (cid:12) B) = rank (A) · rank (B).

• Let Z be the CP-rank of A, then it holds that rank ([A]) ≤ Z (see [9] for proof).

Proof of theorem 1. Stemming from the above stated facts, to show that the CP-rank of Ay is at least rN/2, it
is sufﬁcient to examine its matricization [Ay] and prove that rank ([Ay]) ≥ rN/2.
Notice from the construction of [Ay], according to the recursive formula of the HT-decomposition, that
its entires are polynomial in the parameters of the decomposition, its dimensions are M N/2 each and that
1 ≤ rN/2 ≤ M N/2. In accordance with the discussion on the measure of simplex spaces, for each vector
1−1, and notice that
parameter al,j,γ ∈ (cid:52)rl
p−1(˜al,j,γ) is a polynomial mapping6 w.r.t. ˜al,j,γ. Thus, [Ay] is a polynomial mapping w.r.t. the projected
parameters {˜al,j,γ}l,j,γ, and using lemma 1 it is sufﬁcient to show that there exists a set of parameters for
which rank ([Ay]) ≥ rN/2.

1−1, we instead examine its projection ˜al,j,γ = p(al,j,γ) ∈ Rrl

−

−

Denoting for convenience φL,1,1 := Ay and rL = 1, we will construct by induction over l = 1, ..., L a
set of parameters, {al,j,γ}l,j,γ, for which the ranks of the matrices {[φl,j,γ]}j∈[N/2l],γ∈[rl] are at least r2l/2,
while enforcing the simplex constraints on the parameters. More so, we’ll construct these parameters s.t.
al,j,γ = al,γ, thus proving both the ”unshared” and ”shared” cases.

For the case l = 1 we have:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

and let a1,j,γ
and so

α =

r

1α
≤
r

and a0,j,α
i

= 1α=i for all i, j, γ and α ≤ M , and a0,j,α

i

= 1i=1 for all i and α > M ,

1/r
0
which means rank (cid:0)[φ1,j,γ](cid:1) = r, while preserving the simplex constraints, which proves our inductive hy-
pothesis for l = 1.

i = j ∧ i ≤ r
Otherwise

[φ1,j,γ]i,j =

≥ r2l

−

1/2 for all j(cid:48) ∈ [N/2l

1] and γ(cid:48) ∈ [rl−1]. For some speciﬁc choice

−

[φl−1,j(cid:48),γ(cid:48) ]
Assume now that rank
of j ∈ [N/2l] and γ ∈ [rl] we have:

(cid:16)

(cid:17)

r0(cid:88)

α=1

(cid:40)

φl,j,γ =

α φl−1,2j−1,α ⊗ φl−1,2j,α
al,j,γ

=⇒ [φl,j,γ] =

al,j,γ
α

[φl−1,2j−1,α] (cid:12) [φl−1,2j,α]

Denote Mα := [φl−1,2j−1,α] (cid:12) [φl−1,2j,α] for α = 1, ..., rl−1. By our inductive assumption, and by the
general property rank (A (cid:12) B) = rank (A) · rank (B), we have that the ranks of all matrices Mα are at least
r2l
α=1 al,j,γ
· Mα, and noticing that {Mα} do not depend on
α = 1α=1, and thus φl,j,γ = M1, which is of rank r2l/2. This completes the proof
al,j,γ, we simply pick al,j,γ
of the theorem.

1/2 = r2l/2. Writing [φl,j,γ] = (cid:80)rl

1/2 · r2l

α

−

−

−

1

From the perspective of ConvACs with simplex constraints, theorem 1 leads to the following corollary:

6As we mentioned earlier, p is invertible only over p((cid:52)k),

p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
as deﬁned here over the entire range Rk−1, even where it does not serve as the inverse of p.

for which its inverse is given by
i=1 xi). However, to simpliﬁed the proof and notations, we use p−1

rl
1
(cid:88)
−

α=1

rl
1
(cid:88)
−

α=1

15

Corollary 3. Assume the mixing components M = {fi(x) ∈ L2(R2) ∩ L1(Rs)}M
i=1 are square integrable7
probability density functions, which form a linearly independent set. Consider a deep ConvAC model with
simplex constraints of polynomial size whose parameters are drawn at random by some continuous distribution.
Then, with probability 1, the distribution realized by this network requires an exponential size in order to be
realized (or approximated w.r.t. the L2 distance) by the shallow ConvAC model with simplex constraints. The
claim holds regardless of whether the parameters of the deep model are shared or not.

Proof. Given a coefﬁcient tensor A, the CP-rank of A is a lower bound on the number of channels (of its next
to last layer) required to represent that tensor by the ConvAC following the CP factorization. Additionally,
since the mixing components are linearly independent, their products {(cid:81)N
i=1 fi(xi)|fi ∈ M} are linearly
independent as well, which entails that any distribution representable by the generative variant of ConvAC
with mixing components M has a unique coefﬁcient tensor A. From theorem 1, the set of parameters of a
deep ConvAC model (under the simplex constraints) with a coefﬁcient tensor of a polynomial CP-rank – the
requirement for a polynomially-sized shallow ConvAC model with simplex constraints realizing that same
distribution exactly – forms a set of measure zero.

It is left to prove, that not only is it impossible to exactly represent a distribution with an exponential coefﬁcient
tensor by a shallow model, it is also impossible to approximate it. This follows directly from lemma 7 in
appendix B of Cohen et al. [9], as our case meets the requirement of that lemma.

E Proof for the Optimality of Marginalized Bayes Predictor

In this section we give short proofs for the claims from sec. 3, on the optimality of the marginalized Bayes
predictor under missing-at-random (MAR) distribution, when the missingness mechanism is unknown, as well
as the general case when we do not add additional assumptions. In addition, we will also present a counter ex-
ample proving data imputation results lead to suboptimal classiﬁcation performance. We begin by introducing
several notations that augment the notations already introduced in the body of the article.
Given a speciﬁc mask realization m ∈ {0, 1}s, we use the following notations to denote partial assignments
to the random vector X . For the observed indices of X , i.e. the indices for which mi = 1, we denote a partial
assignment by X \ m = xo, where xo ∈ Rdo is a vector of length do equal to the number of observed indices.
Similarly, we denote by X ∩ m = xm a partial assignment to the missing indices according to m, where
xm ∈ Rdm is a vector of length dm equal to the number of missing indices. As an example of the notation,
for given realizations x ∈ Rs and m ∈ {0, 1}s, we deﬁned in sec. 3 the event o(x, m), which using current
notation is marked by the partial assignment X \ m = xo where xo matches the observed values of the vector
x according to m.

With the above notations in place, we move on to prove claim 1, which describes the general solution to the
optimal prediction rule given both the data and missingness distributions, and without adding any additional
assumptions.

7It is important to note that most commonly used distribution functions are square integrable, e.g. most

members of the exponential family such as the Gaussian distribution.

16

Proof of claim 1. Fix an arbitrary prediction rule h. We will show that L(h∗) ≤ L(h), where L is the expected
0-1 loss.

1 − L(h)=E(x,m,y)∼(X ,M,Y)[1h(x(cid:12)m)=y]

P(M=m, X =x, Y=y)1h(x(cid:12)m)=ydx

(cid:88)

(cid:90)

(cid:88)

m∈{0,1}s

y∈[k]

Rs

(cid:88)

(cid:90)

(cid:88)

(cid:90)

Rdo

Rdm

y∈[k]

m∈{0,1}s
P(M=m, X \m=xo, X ∩m=xm, Y=y)1h(x⊗m)=ydxodxm

1h(x(cid:12)m)=ydxo

(cid:88)

(cid:90)

(cid:88)

y∈[k]

Rdo

m∈{0,1}s
(cid:90)

Rdm
(cid:88)

(cid:90)

(cid:88)

Rdo

m∈{0,1}s

(cid:88)

y∈[k]
(cid:90)

Rdo

P(M=m, X \m=xo, X ∩m=xm, Y=y)dxm

1h(x(cid:12)m)=yP(M=m, X \m=xo, Y=y)dxo

P(X \m=xo)

1h(x(cid:12)m)=yP(Y=y|X \m=xo)

(cid:88)

=

=

=1

=2

=3

≤4

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo
(cid:88)

(cid:88)

(cid:90)

P(X \m=xo)

Rdo

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo

=1 − L(h∗)

1h∗(x(cid:12)m)=yP(Y=y|X \m=xo)

Where (1) is because the output of h(x (cid:12) m) is independent of the missing values, (2) by marginalization,
(3) by conditional probability deﬁnition and (4) because by deﬁnition h∗(x (cid:12) m) maximizes the expression
P(Y=y|X \m=xo)P(M=m|X \m=xo, Y=y) w.r.t.
the possible values of y for ﬁxed vectors m and xo.
Finally, by replacing integrals with sums, the proof holds exactly the same when instances (X ) are discrete.

We now continue and prove corollary 1, a direct implication of claim 1 which shows that in the MAR setting,
the missingness distribution can be ignored, and the optimal prediction rule is given by the marginalized Bayes
predictor.

Proof of corollary 1. Using the same notation as in the previous proof, and denoting by xo the partial vector
containing the observed values of x (cid:12) m, the following holds:

P(M=m|o(x, m), Y=y) := P(M=m|X \m=xo, Y=y)

P(M=m, X ∩ m=xm|X \m=xo, Y=y)dxm

(cid:90)

(cid:90)

=

=

Rdm

P(X ∩m=xm|X \m=xo, Y=y)

Rdm
· P(M=m|X ∩m=xm, X \m=xo, Y=y)dxm
(cid:90)

P(X ∩m=xm|X \m=xo, Y=y)

=1

Rdm
· P(M=m|X ∩m=xm, X \m=xo)dxm
(cid:90)

=2

P(X ∩m=xm|X \m=xo, Y=y) · P(M=m|X \m=xo)dxm

Rdm

=P(M=m|X \m=xo)

P(X ∩m=xm|X \m=xo, Y=y)dxm

(cid:90)

Rdm

=P(M=m|o(x, m))

Where (1) is due to the independence assumption of the events Y = y and M = m conditioned on X = x,
while noting that (X \ m = xo) ∧ (X ∩ m = xm) is a complete assignment of X . (2) is due to the MAR
assumption, i.e. that for a given m and xo it holds for all xm ∈ Rdm :

P(M=m|X \m=xo, X ∩m=xm) = P(M=m|X \m=xo)

17

Y Weight

Probability ((cid:15) = 10−

4)

X1 X2
0
0
1
0
0
1
1
1
0
0
1
0
0
1
1
1

0
0
0
0
1
1
1
1

1

1

−
1

(cid:15)

(cid:15)

−
1
0
1 + (cid:15)
0
1 + (cid:15)

16.665%
16.667%
16.665%
16.667%
0.000%
16.668%
0.000%
16.668%
2
0, 1

Table 2: Data distribution over the space
the sub-optimality of classiﬁcation through data-imputation (proof of claim 3).

0, 1
}
{

X × Y

× {

=

}

that serves as the example for

We have shown that P(M=m|o(x, m), Y = y) does not depend on y, and thus does not affect the optimal
prediction rule in claim 1. It may therefore be dropped, and we obtain the marginalized Bayes predictor.

Having proved that in the MAR setting, classiﬁcation through marginalization leads to optimal performance,
we now move on to show that the same is not true for classiﬁcation through data-imputation. Though there are
many methods to perform data-imputation, i.e. to complete missing values given the observed ones, all of these
methods can be seen as the solution of the following optimization problem, or more typically its approximation:

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48))

Where g(x (cid:12) m) is the most likely completion of x (cid:12) m. When data-imputation is carried out for classiﬁcation
purposes, one is often interested in data-imputation conditioned on a given class Y = y, i.e.:

g(x (cid:12) m; y) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48)|Y = y)

Given a classiﬁer h : Rs → [K] and an instance x with missing values according to m, classiﬁcation through
data-imputation is simply the result of applying h on the output of g. When h is the optimal classiﬁer for
complete data, i.e. the Bayes predictor, we end up with one of the following prediction rules:

Unconditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m))

Conditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m; y))

y

y

Claim 3. There exists a data distribution D and MAR missingness distribution Q s.t. the accuracy of classi-
ﬁcation through data-imputation is almost half the accuracy of the optimal marginalized Bayes predictor, with
an absolute gap of more than 33 percentage points.

Proof. For simplicity, we will give an example for a discrete distribution over
the binary set
X × Y = {0, 1}2 × {0, 1}. Let 1 > (cid:15) > 0 be some small positive number, and we deﬁne D according to table 2,
where each triplet (x1, x2, y) ∈ X ×Y is assigned a positive weight, which through normalization deﬁnes a
distribution over X ×Y. The missingness distribution Q is deﬁned s.t. PQ(M1 = 1, M2 = 0|X = x) = 1 for
all x ∈ X , i.e. X1 is always observed and X2 is always missing, which is a trivial MAR distribution. Given the
above data distribution D, we can easily calculate the exact accuracy of the optimal data-imputation classiﬁer
and the marginalized Bayes predictor under the missingness distribution Q, as well as the standard Bayes pre-
dictor under full-observability. First notice that whether we apply conditional or unconditional data-imputation,
and whether X1 is equal to 0 or 1, the completion will always be X2 = 1 and the predicted class will always
be Y = 1. Since the data-imputation classiﬁers always predict the same class Y = 1 regardless of their input,
(for (cid:15) = 10−4 it equals approximately
the probability of success is simply the probability P (Y = 1) = 1+(cid:15)
3
33.337%). Similarly, the marginalized Bayes predictor always predicts Y = 0 regardless of its input, and so
(for (cid:15) = 10−4 it equals approximately 66.663%), which is
its probability of success is P (Y = 0) = 2−(cid:15)
3
almost double the accuracy achieved by the data-imputation classiﬁer. Additionally, notice that the marginal-
ized Bayes predictor achieves almost the same accuracy as the Bayes predictor under full-observability, which
equals exactly 2
3 .

18

F Efﬁcient Marginalization with Tensorial Mixture Models

As discussed above, with generative models optimal classiﬁcation under missing data (in the MAR setting) is
oblivious to the speciﬁc missingness distribution. However, it requires tractable marginalization over missing
values. In this section we show that TMMs bring forth extremely efﬁcient marginalization, requiring only a
single forward pass through the corresponding ConvAC.

Recall from sec. 2 and 2.3 that a TMM classiﬁer realizes the following form:

P (x1, . . . , xN |Y =y) =

P (d1, . . . , dN |Y =y)

P (xi|di; θdi )

(9)

(cid:88)M

d1,...,dN

(cid:89)N

i=1

Suppose now that only the local structures xi1 . . . xiV are observed, and we would like to marginalize over the
rest. Integrating eq. 9 gives:

P (xi1 , . . . , xiV |Y =y) =

P (d1, . . . , dN |Y =y)

(cid:88)M

d1,...,dN

(cid:89)V

v=1

P (xiv |div ; θdiv

)

from which it is evident that the same network used to compute P (x1, . . . , xN |Y =y), can be used to compute
P (xi1 , . . . , xiV |Y =y) – all it requires is a slight adaptation of the representation layer. Namely, the latter
would represent observed values through the usual likelihoods, whereas missing (marginalized) values would
now be represented via constant ones:
(cid:40)

rep(i, d) =

1
P (xi|d; Θ)

, xi is missing (marginalized)
, xi is visible (not marginalized)

More generally, to marginalize over individual coordinates of the local structure xi, it is sufﬁcient to replace
rep(i, d) by its respective marginalized mixing component.

To conclude, with TMMs marginalizing over missing values is just as efﬁcient as plain inference – requires
only a single pass through the corresponding network. Accordingly, the marginalized Bayes predictor (eq. 3)
is realized efﬁciently, and classiﬁcation under missing data (in the MAR setting) is optimal (under generative
assumption), regardless of the missingness distribution.

G Extended Discussion on Generative Models Based on Neural Networks

There are many generative models realized through neural networks, and convolutional networks in particular.
Of these models, one of the most successful to date is the method of Generative Adversarial Networks [19],
where a network is trained to generate instances from the data distribution, through a two-player mini-max
game. While there are numerous applications for learning to generate data points, e.g. inpainting and super-
resolution, it cannot be used for computing the likelihood of the data. Other generative networks do offer
inference, but only approximate. Variational Auto-Encoders [24] use a variational lower-bound on the likeli-
hood function. GSNs [5], DPMs [37] and MPDBMs [18] are additional methods along this line. The latter
is especially noteworthy for being a generative classiﬁer that can approximate the marginal likelihoods condi-
tioned on each class, and for being tested on classiﬁcation under missing data.

Some generative neural networks are capable of tractable inference, but not of tractable marginalization. Dinh
et al. [15] suggest a method for designing neural networks that realize an invertible transformation from a
Inverting the network brings forth tractable inference, yet par-
simple distribution to the data distribution.
tial integration of its density function is still intractable. Another popular method for tractable inference,
central to both PixelRNN [39] and NADE [38], is the factorization of the probability distribution according
to P(x1, . . . , xd) = (cid:81)d
P(xi|xi−1, . . . , x1), and realization of P(xi|xi−1, . . . , x1) as a neural network.
Based on this construction, certain marginal distributions are indeed tractable to compute, but most are not.
Orderless-NADE partially addresses this issue by using ensembles of models over different orderings of its
input. However, it can only estimate the marginal distributions, and has no classiﬁer analogue that can compute
class-conditional marginal likelihoods, as required for classiﬁcation under missing data.

i=1

H Image Generation and Network Visualization

Following the graphical model perspective of our models allows us to not only generate random instances from
the distribution, but to also generate the most likely patches for each neuron in the network, effectively explain-
ing its role in the classiﬁcation process. We remind the reader that every neuron in the network corresponds to a
possible assignment of a latent variable in the graphical model. By looking for the most likely assignments for
each of its child nodes in the graphical tree model, we can generate a patch that describes that neuron. Unlike
similar suggested methods to visualize neural networks [40], often relying on brute-force search or on solving
some optimization problem to ﬁnd the most likely image, our method emerges naturally from the probabilistic
interpretation of our model.

19

Figure 5: Generated digits samples from the HT-TMM model trained on the MNIST dataset.

Figure 6: Visualization of the HT-TMM model. Each of the images above visualize a different
layer of the model and consists of several samples generated from latent variables at different spatial
locations conditioned on randomly selected channels. The leftmost image shows samples taken
from the 5th layer which consists of just a single latent variable with 512 channels. The center
image shows samples taken from the 4th layer, which consists of 2
2 grid of latent variables with
256 channels each. The image is divided to 4 quadrants, each contains samples taken from the
respective latent variable at that position. The rightmost image shows samples from the 3rd layer,
which consists of 4
4 grid of latent variables with 128 channels, and the image is similarly spatial
divided into different areas matching the latent variables of the layer.

×

×

20

In ﬁg. 5, we can see conditional samples generates for each digit, while in ﬁg. 6 we can see a visualization of the
top-level layers of network, where each small patch matches a different neuron in the network. The common
wisdom of how ConvNets work is by assuming that simple low-level features are composed together to create
more and more complex features, where each subsequent layer denotes features of higher abstraction – the
visualization of our network clearly demonstrate this hypothesis to be true for our case, showing small strokes
iteratively being composed into complete digits.

I Detailed Description of the Experiments

Experiments are meaningful only if they could be reproduced by other proﬁcient individuals. Providing suf-
ﬁcient details to enable others to replicate our results is the goal of this section. We hope to accomplish this
by making our code public, as well as documenting our experiments to a sufﬁcient degree allowing for their
reproduction from scratch. Our complete implementation of the models presented in this paper, as well as
our modiﬁcations to other open-source projects and scripts used in the process of conducting our experiments,
are available at our Github repository: https://github.com/HUJI-Deep/Generative-ConvACs. We ad-
ditionally wish to invite readers to contact the authors, if they deem the following details insufﬁcient in their
process to reproduce our results.

I.1 Description of Methods

In the following we give concise descriptions of each classiﬁcation method we have used in our experiments.
The results of the experiment on MP-DBM [18] were taken directly from the paper and were not conducted
by us, hence we do not cover it in this section. We direct the reader to that article for exact details on how to
reproduce their results.

I.1.1 Robust Linear Classiﬁer

In [13], binary linear classiﬁers were trained by formulating their optimization as a quadric program under the
constraint that some of its features could be deleted, i.e. their original value was changed to zero. While the
original source code was never published, the authors have kindly agreed to share with us their code, which
we used to reproduced their results, but on larger datasets. The algorithm has only a couple hyper-parameters,
which were chosen by a grid-search through a cross-validation process. For details on the exact protocol for
testing binary classiﬁers on missing data, please see sec. I.2.1.

I.1.2 K-Nearest Neighbors

K-Nearest Neighbors (KNN) is a classical machine learning algorithm used for both regression and classiﬁca-
tion tasks. Its underlying mechanism is ﬁnding the k nearest examples (called neighbors) from the training set,
(x1, y1), . . . , (xk, yk) ∈ S, according to some metric function d(·, ·) : X × X → R+, after which a summa-
rizing function f is applied to the targets of the k nearest neighbors to produce the output y∗ = f (y1, . . . , yk).
When KNN is used for classiﬁcation, f is typically the majority voting function, returning the class found in
most of the k nearest neighbors.

In our experiments we use KNN for classiﬁcation under missing data, where the training set consists of com-
plete examples with no missing data, but at classiﬁcation time the inputs have missing values. Given an input
with missing values x (cid:12) m and an example x(cid:48) from the training set, we use a modiﬁed Euclidean distance
metric, where we compare the distance only against the non-missing coordinates of x, i.e. the metric is deﬁned
by d(x(cid:48), x (cid:12) m) = (cid:80)
i − xi)2. Through a process of cross-validation we have chosen k = 5 for
all of our experiments. Our implementation of KNN is based on the popular scikit-learn python library [32].

i:mi=1 (x(cid:48)

I.1.3 Convolutional Neural Networks

The most widespread and successful discriminative method nowadays are Convolutional Neural Net-
works (ConvNets). Standard ConvNets are represented by a computational graph consisted of different kinds
of nodes, called layers, with a convolutional-like operators applied to their inputs, followed by a non-linear
point-wise activation function, e.g. max(0, x) known as ReLU.

For our experiments on MNIST, both with and without missing data, we have used the LeNeT ConvNet ar-
chitecture [25] that is bundled with Caffe [23], trained for 20,000 iterations using SGD with 0.9 momentum
and 0.01 base learning rate, which remained constant for 10,000 iterations, followed by a linear decrease to
0.001 for another 5,000 iterations, followed by a linear decrease to 0 learning rate for the remaining 5,000
iterations. The model also used l2-regularization (also known as weight decay), which was chosen through
cross-validation for each experiment separately. No other modiﬁcations were made to the model or its training
procedure.

21

For our experiments on NORB, we have used an ensemble of 3 ConvNets, each using the following architecture:
5×5 convolution with 128 output channels, 3×3 max pooling with stride 2, ReLU activation, 5×5 convolution
with 128 output channels, ReLU activation, dropout layer with probability 0.5, 3×3 average pooling with
stride 2, 5×5 convolution with 256 output channels, ReLU activation, dropout layer with probability 0.5,
3×3 average pooling with stride 2, fully-connected layer with 768 output channels, ReLU activation, dropout
layer with probability 0.5, and ends with fully-connected layer with 5 output channels. The stereo images
were represented as a two-channel input image when fed to the network. During training we have used data
augmentation consisting of randomly scaling and rotation transforms. The networks were trained for 40,000
iterations using SGD with 0.99 momentum and 0.001 base learning rate, which remained constant for 30,000
iterations, followed by a linear decrease to 0.0001 for 6000 iterations, followed by a linear decrease to 0 learning
rate for the remaining 4,000 iterations. The model also used 0.0001 weight decay for additional regularization.

When ConvNets were trained on images containing missing values, we passed the network the original image
with missing values zeroed out, and an additional binary image as a separate channel, containing 1 for missing
values at the same spatial position, and 0 otherwise – this missing data format is sometimes known as ﬂag
data imputation. Other formats for representing missing values were tested (e.g. just using zeros for missing
values), however, the above scheme performed signiﬁcantly better than other formats. In our experiments, we
assumed that the training set was complete and missing values were only present in the test set. In order to
design ConvNets that are robust against speciﬁc missingness distributions, we have simulated missing values
during training, sampling a different mask of missing values for each image in each mini-batch. As covered
in sec. 5, the results of training ConvNets directly on simulated missingness distributions resulted in classiﬁers
which were biased towards the speciﬁc distribution used in training, and performed worse on other distributions
compared to ConvNets trained on the same distribution.

In addition to training ConvNets directly on missing data, we have also used them as the classiﬁer for testing
different data imputation methods, as describe in the next section.

I.1.4 Classiﬁcation Through Data Imputation

The most common method for handling missing data, while leveraging available discriminative classiﬁers, is
through the application of data imputation – an algorithm for the completion of missing values – and then
passing the results to a classiﬁer trained on uncorrupted dataset. We have tested ﬁve different types of data
imputation algorithms:

• Zero data imputation: replacing every missing value by zero.

• Mean data imputation: replacing every missing value by the mean value computed over the dataset.

• Generative data imputation: training a generative model and using it to complete the missing values
by ﬁnding the most likely instance that coincides with the observed values, i.e. solving the following

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i,mi=1→x(cid:48)i=xi

P (X = x(cid:48))

We have tested the following generative models:

– Generative Stochastic Networks (GSN) [5]: We have used their original source code from
https://github.com/yaoli/GSN, and trained their example model on MNIST for 1000
epochs. Whereas in the original article they have tested completing only the left or right side of
a given image, we have modiﬁed their code to support general masks. Our modiﬁed implemen-
tation can be found at https://github.com/HUJI-Deep/GSN.

– Non-linear Independent Components Estimation (NICE) [15]: We have used their original
source code from https://github.com/laurent-dinh/nice, and trained it on MNIST us-
ing their example code without changes. Similarly to our modiﬁcation to the GSN code, here
too we have adapted their code to support general masks over the input. Additionally, their orig-
inal inpainting code required 110,000 iterations, which we have reduced to just 8,000 iterations,
since the effect on classiﬁcation accuracy was marginal. For the NORB dataset, we have used
their CIFAR10 example, with lower learning rate of 10−4. Our modiﬁed code can be found at
https://github.com/HUJI-Deep/nice.

– Diffusion Probabilistic Models (DPM) [37]: We have user their original source code
from https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models, and
trained it on MNIST using their example code without changes. Similarly to our modiﬁ-
cations to GSN, we have add support for a general mask of missing values, but other than
that kept the rest of the parameters for inpainting unchanged. For NORB we have used
the same model as MNIST. We have tried using their CIFAR10 example for NORB, how-
ever, it produced exceptions during training. Our modiﬁed code can be found at https:
//github.com/HUJI-Deep/Diffusion-Probabilistic-Models.

22

I.1.5 Tensorial Mixture Models

For a complete theoretical description of our model please see the body of the article. Our models were
implemented by performing all intermediate computations in log-space, using numerically aware operations. In
practiced, that meant our models were realized by the SimNets architecture [7, 10], which consists of Similarity
layers representing gaussian distributions, MEX layers representing weighted sums performed on log-space
input and outputs, as well as standard pooling operations. The learned parameters of the MEX layers are called
offsets, which represents the weights of the weighted sum, but saved in log-space. The parameters of the MEX
layers can be optionally shared between spatial regions, or alternatively left with no parameter sharing at all.
Additionally, when used to implement our generative models, the offsets are normalized to have a soft-max (i.e.,
log (cid:0)(cid:80)

i exp(xi)(cid:1)) of zero.

The network architectures we have tested in this article, consists of M different Gaussian mixture components
with diagonal covariance matrices, over non-overlapping patches of the input of size 2 × 2, which were imple-
mented by a similarity layer as speciﬁed by the SimNets architecture, but with an added gaussian normalization
term.

We ﬁrst describe the architectures used for the MNIST dataset. For the CP-TMM model, we used M = 800,
and following the similarity layer is a 1 × 1 MEX layer with no parameter sharing over spatial regions and
10 output channels. The model ends with a global sum pooling operation, followed by another 1 × 1 MEX
layer with 10 outputs, one for each class. The HT-TMM model starts with the similarity layer with M = 32,
followed by a sequence of four pairs of 1 × 1 MEX layer followed by 2 × 2 sum pooling layer, and after the
pairs and additional 1 × 1 MEX layer lowering the outputs of the model to 10 outputs as the number of classes.
The number of output channels for each MEX layer are as follows 64-128-256-512-10. All the MEX layers in
this network do not use parameter sharing, except the ﬁrst MEX layer, which uses a repeated sharing pattern
of 2 × 2 offsets, that analogous to a 2 × 2 convolution layer with stride 2. Both models were trained with the
losses described in sec. 2.3, using the Adam SGD variant for optimizing the parameters, with a base learning
rate of 0.03, and β1 = β2 = 0.9. The models were trained for 25,000 iterations, where the learning rate was
dropped by 0.1 after 20,000 iterations.

For the NORB dataset, we have trained only the HT-TMM model with M = 128 for the similarity layer. The
MEX layers use the same parameter sharing scheme as the one for MNIST, and the number of output channels
for each MEX layer are as follows: 256-256-256-512-5. Training was identical to the MNIST models, with the
exception of using 40,000 iterations instead of just 25,000. Additionally, we have used an ensemble of 4 models
trained separately, each trained using a different generative loss weight (see below for more information). We
have also used the same data augmentation methods (scaling and rotation) which were used in training the
ConvNets for NORB used in this article.

i x2

The standard L2 weight regularization (sometimes known as weight decay) did not work well on our mod-
els, which lead us to adapt it to better ﬁt to log-space weights, by minimizing λ (cid:80)
i (exp (xi))2 instead of
λ||x||2 = λ (cid:80)
i , where the parameter λ was chosen through cross-validation. Additionally, since even
with large values of λ our model was still overﬁtting, we have added another form of regularization in the form
of random marginalization layers. A random marginalization layer, is similar in concept to dropout, but instead
of zeroing activations completely in random, it choses spatial locations at random, and then zero out the activa-
tions at those locations for all the channels. Under our model, zeroing all the activations in a layer at a speciﬁc
location, is equivalent to marginalizing over all the inputs for the receptive ﬁeld for that respective location.
We have used random marginalization layers in between all our layers during training, where the probability
for zeroing out activations was chosen through cross-validation for each layer separately. Though it might raise
concern that random marginalization layers could lead to biased results toward the missingness distributions
we have tested it on, in practice the addition of those layers only helped improve our results under cases where
only few pixels where missing.

Finally, we wish to discuss a few optimization tricks which had a minor effects compared to the above, but were
nevertheless very useful in achieving slightly better results. First, instead of optimizing directly the objective
deﬁned by eq. 2, we add smoothing parameter β between the two terms, as follows:

Θ∗ = argmin

−

log

Θ

|S|
(cid:88)

i=1

eNΘ(X(i);Y (i))
y=1 eNΘ(X(i);y)

(cid:80)K

|S|
(cid:88)

K
(cid:88)

− β

log

eNΘ(X(i);y)

i=1

y=1

setting β too low diminish the generative capabilities of our models, while setting it too high diminish the
discriminative performance. Through cross-validation, we decided on the value β = 0.01 for the models
trained on MNIST, while for NORB we have used a different value of β for each of the models, ranging in
{0.01, 0.1, 0.5, 1}. Second, we found that performance increased if we normalized activations before applying
the 1 × 1 MEX operations. Speciﬁcally, we calculate the soft-max over the channels for each spatial location

23

which we call the activation norm, and then subtract it from every respective activation. After applying the
MEX operation, we add back the activation norm. Though might not be obvious at ﬁrst, subtracting a constant
from the input of a MEX operation and adding it to its output is equivalent does not change the mathematical
operation. However, it does resolve the numerical issue of adding very large activations to very small offsets,
which might result in a loss of precision. Finally, we are applying our model in different translations of the input
and then average the class predictions. Since our model can marginalize over inputs, we do not need to crop
the original image, and instead mask the unknown parts after translation as missing. Applying a similar trick
to standard ConvNets on MNIST does not seem to improve their results. We believe this method is especially
ﬁt to our model, is because it does not have a natural treatment of overlapping patches like ConvNets do, and
because it is able to marginalize over missing pixels easily, not limiting it just to crop translation as is typically
done.

I.2 Description of Experiments

In this section we will give a detailed description of the protocol we have used during our experiments.

I.2.1 Binary Digit Classiﬁcation under Feature Deletion Missing Data

This experiment focuses on the binary classiﬁcation problem derived from MNIST, by limiting the number of
classes to two different digits at a time. We use the same non-zero feature deletion distribution as suggested by
Globerson and Roweis [17], i.e. for a given image we uniformly sample a set of N non-zero pixels from the
image (if the image has less than N non-zero pixels then they are non-zero pixels are chosen), and replace their
values with zeros. This type of missingness distribution falls under the MNAR type deﬁned in sec.3.

We test values of N in {0, 25, 50, 75, 100, 125, 150}. For a given value of N , we train a separate classiﬁer
on each digit pair classiﬁer on a randomly picked subset of the dataset containing 300 images per digit (600
total). During training we use a ﬁxed validation set with 1000 images per digit. After picking the best classiﬁer
according to the validation set, the classiﬁer is tested against a test set with a 1000 images per digits with a
randomly chosen missing values according to the value of N . This experiment is repeated 10 times for each
digit pair, each time using a different subset for the training set, and a new corrupted test set. After conducting
all the different experiments, all the accuracies are averaged for each value of N , which are reported in table 1.

I.2.2 Multi-class Digit Classiﬁcation under MAR Missing Data

This experiment focuses on the complete multi-class digit classiﬁcation of the MNIST dataset, in the presence
of missing data according to different missingness distributions. Under this setting, only the test set contains
missing values, whereas the training set does not. We test two kinds of missingness distributions, which both
fall under the MAR type deﬁned in sec.3. The ﬁrst kind, which we call i.i.d. corruption, each pixel is missing
with a ﬁxed probability p. the second kind, which we call missing rectangles corruption, The positions of N
rectangles of width W or chosen uniformly in the picture, where the rectangles can overlap one another. During
the training stage, the models to be tested are not to be biased toward the speciﬁc missingness distributions we
have chosen, and during the test stage, the same classiﬁer is tested against all types of missingness distributions,
and without supplying it with the parameters or type of the missingness distribution it is tested against. This
rule prevent the use of ConvNets trained on simulated missingness distributions. To demonstrate that the latter
lead to biased classiﬁers, we have conducted a separate experiment just for ConvNets, where the previous rule is
ignored, and we train a separate ConvNet classiﬁer on each type and parameter of the missingness distributions
we have used. We then tested each of those ConvNets on all other missingness distributions, the results of
which are in ﬁg. 3, which conﬁrmed our hypothesis.

24

8
1
0
2
 
r
a

M
 
5
2
 
 
]

G
L
.
s
c
[
 
 
5
v
7
6
1
4
0
.
0
1
6
1
:
v
i
X
r
a

Tensorial Mixture Models

Or Sharir
Department of Computer Science
The Hebrew University of Jerusalem
Israel
or.sharir@cs.huji.ac.il

Ronen Tamari
Department of Computer Science
The Hebrew University of Jerusalem
Israel
ronent@cs.huji.ac.il

Nadav Cohen
Department of Computer Science
The Hebrew University of Jerusalem
Israel
cohennadav@cs.huji.ac.il

Amnon Shashua
Department of Computer Science
The Hebrew University of Jerusalem
Israel
shashua@cs.huji.ac.il

Abstract

Casting neural networks in generative frameworks is a highly sought-after en-
deavor these days. Contemporary methods, such as Generative Adversarial Net-
works, capture some of the generative capabilities, but not all. In particular, they
lack the ability of tractable marginalization, and thus are not suitable for many
tasks. Other methods, based on arithmetic circuits and sum-product networks, do
allow tractable marginalization, but their performance is challenged by the need to
learn the structure of a circuit. Building on the tractability of arithmetic circuits,
we leverage concepts from tensor analysis, and derive a family of generative mod-
els we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolu-
tional network structure, and in addition, lend themselves to theoretical analyses
that allow comprehensive understanding of the relation between their structure and
their expressive properties. We thus obtain a generative model that is tractable on
one hand, and on the other hand, allows effective representation of rich distribu-
tions in an easily controlled manner. These two capabilities are brought together
in the task of classiﬁcation under missing data, where TMMs deliver state of the
art accuracies with seamless implementation and design.

1

Introduction

There have been many attempts in recent years to marry generative models with neural networks,
including successful methods, such as Generative Adversarial Networks [19], Variational Auto-
Encoders [24], NADE [38], and PixelRNN [39]. Though each of the above methods has demon-
strated its usefulness on some tasks, it is yet unclear if their advantage strictly lies in their generative
nature or some other attribute. More broadly, we ask if combining generative models with neural
networks could lead to methods who have a clear advantage over purely discriminative models.

|

On the most fundamental level, if X stands for an instance and Y for its class, generative mod-
els learn P(X, Y ), from which we can also infer P(Y
X), while discriminative models learn only
P(Y
X). It might not be immediately apparent if this sole difference leads to any advantage. In Ng
and Jordan [31], this question was studied with respect to the sample complexity, proving that under
some cases it can be signiﬁcantly lesser in favor of the generative classiﬁer. We wish to highlight
a more clear cut case, by examining the problem of classiﬁcation under missing data – where the
value of some of the entries of X are unknown at prediction time. Under these settings, discrimi-
native classiﬁers typically rely on some form of data imputation, i.e. ﬁlling missing values by some
auxiliary method prior to prediction. Generative classiﬁers, on the other hand, are naturally suited

|

to handle missing values through marginalization – effectively assessing every possible completion
of the missing values. Moreover, under mild assumptions, this method is optimal regardless of the
process by which values become missing (see sec. 3).

It is evident that such application of generative models assumes we can efﬁciently and exactly com-
pute P(X, Y ), a process known as tractable inference. Moreover, it assumes we may efﬁciently
marginalize over any subset of X, a procedure we refer to as tractable marginalization. Not all
generative models have both of these properties, and speciﬁcally not the ones mentioned in the be-
ginning of this section. Known models that do possess these properties, e.g. Latent Tree Model [30],
have other limitations. A detailed discussion can be found in sec. 4, but in broad terms, all known
generative models possess one of the following shortcomings: (i) they are insufﬁciently expressive
to model high-dimensional data (images, audio, etc.), (ii) they require explicitly designing all the
dependencies of the data, or (iii) they do not have tractable marginalization. Models based on neural
networks typically solve (i) and (ii), but are incapable of (iii), while more classical methods, e.g.
mixture models, solve (iii) but suffer from (i) and (ii).

There is a long history of specifying tractable generative models through arithmetic circuits and sum-
product networks [12, 34] – computational graphs comprised solely of product and weighted sum
nodes. To address the shortcomings above, we take a similar approach, but go one step further and
leverage tensor analysis to distill it to a speciﬁc family of models we call Tensorial Mixture Models.
A Tensorial Mixture Model assumes a convolutional network structure, but as opposed to previous
methods tying generative models with neural networks, lends itself to theoretical analyses that allow
a thorough understanding of the relation between its structure and its expressive properties. We
thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective
representation of rich distributions in an easily controlled manner.

2 Tensorial Mixture Models

(cid:80)

M
d=1

P(d)P(x

One of the simplest types of tractable generative models are mixture models, where the probability
distribution is deﬁned the convex combination of M mixing components (e.g. Normal distribu-
tions): P(x) =
d; θd). Mixture models are very easy to learn, and many of them
|
are able to approximate any probability distribution, given sufﬁcient number of components, ren-
dering them suitable for a variety of tasks. The disadvantage of classic mixture models is that they
do not scale will to high dimensional data (“curse of dimensionality”). To address this challenge,
we extend mixture models, leveraging the fact many high dimensional domains (e.g. images) are
typically comprised of small, simple local structures. We represent a high dimensional instance as
Rs (called lo-
X = (x1, . . . , xN ) – an N -length sequence of s-dimensional vectors x1, . . . , xN ∈
cal structures). X is typically thought of as an image, where each local structure xi corresponds to a
local patch from that image, where no two patches are overlapping. We assume that the distribution
of individual local structures can be efﬁciently modeled by some mixture model of few components,
which for natural image patches, was shown to be the case [42]. Formally, for all i
[N ] there
di; θdi), where di is a hidden variable specifying the matching
exists di ∈
component for the i-th local structure. The probability density of sampling X is thus described by:

[M ] such that xi ∼

P (x
|

∈

P (X) =

P (d1, . . . , dN )

M

d1,...,dN =1

N

i=1

P (xi|

di; θdi)

(cid:88)

(cid:89)

(1)

where P (d1, . . . , dN ) represents the prior probability of assigning components d1, . . . , dN to their
respective local structures x1, . . . , xN . As with classical mixture models, any probability density
function P(X) could be approximated arbitrarily well by eq. 1, as M
At ﬁrst glance, eq. 1 seems to be impractical, having an exponential number of terms. In the lit-
erature, this equation is known as the “Network Polynomial” [12], and the traditional method to
overcome its intractability is to express P (d1, . . . , dN ) by an arithmetic circuit, or sum-product
networks, following certain constraints (decomposable and complete). We augment this method
by viewing P (d1, . . . , dN ) from an algebraic perspective, treating it as a tensor of order N and
Ad1,...,dN speciﬁed by N indices
dimension M in each mode, i.e., as a multi-dimensional array,
d1, . . . , dN , each ranging in [M ], where [M ]
P (d1, . . . , dN )
≡{
as the prior tensor. Under this perspective, eq. 1 can be thought of as a mixture model with tensorial
mixing weights, thus we call the arising models Tensorial Mixture Models, or TMMs for short.

Ad1,...,dN ≡

. We refer to

(see app. A).

1, . . . , M

→ ∞

}

2

Figure 1: A generative variant of Convolutional Arithmetic Circuits.

2.1 Tensor Factorization, Tractability, and Convolutional Arithmetic Circuits

Not only is it intractable to compute eq. 1, but it is also impossible to even store the prior tensor.
We argue that addressing the latter is intrinsically tied to addressing the former. For example, if
we impose a sparsity constraint on the prior tensor, then we only need to compute the few non-
zero terms of eq. 1. TMMs with sparsity constraints can represent common generative models,
e.g. GMMs (see app. B). However, they do not take full advantage of the prior tensor. Instead, we
consider constraining TMMs with prior tensors that adhere to non-negative low-rank factorizations.

= v(1)

takes a rank-1 form, i.e. there
We begin by examining the simplest case, where the prior tensor
N
exist vectors v(1), . . . , v(N )
, or in tensor product nota-
d = P (di=d) as a probability over di, and so
tion,
i P (di), then it reveals that imposing a rank-1 constraint is actually equivalent
P (d1, . . . , dN ) =
to assuming the hidden variables d1, . . . , dN are statistically independent. Applying it to eq. 1 re-
sults in the tractable form P (X) =
di, θdi), or in other words, a product
of mixture models. Despite the familiar setting, this strict assumption severely limits expressivity.

RM such that
v(N ). If we interpret1 v(i)

d=1 P (di=d)P (xi|

Ad1,...,dN =

A
i=1 v(i)

⊗ · · · ⊗

N
i=1

(cid:81)

(cid:81)

A

∈

M

di

In a broader setting, we look at general factorization schemes that given sufﬁcient resources could
represent any tensor. Namely, the CANDECOMP/PARAFAC (CP) and the Hierarchical Tucker (HT)
factorizations. The CP factorization is simply a sum of rank-1 tensors, extending the previous case,
and HT factorization can be seen as a recursive application of CP (see def. in app. C). Since both
factorization schemes are solely based on product and weighted sum operations, they could be re-
alized through arithmetic circuits. As shown by Cohen et al. [9], this gives rise to a speciﬁc class
of convolutional networks named Convolutional Arithmetic Circuits (ConvACs), which consist of
1-convolutions, non-overlapping product pooling layers, and linear activations. More speciﬁ-
1
×
cally, the CP factorization corresponds to shallow ConvACs, HT corresponds to deep ConvACs, and
the number of channels in each layer corresponds to the respective concept of “rank” in each factor-
ization scheme. In general, when a tensor factorization is applied to eq. 1, inference is equivalent to
M,N
d=1,i=1, in what we call the
ﬁrst computing the likelihoods of all mixing components
representation layer, followed by a ConvAC. A complete network is illustrated in ﬁg. 1.

P (xi|
{

d; θd)
}

(cid:81)

(cid:80)

When restricting the prior tensor of eq. 1 to a factorization, we must
ensure it represents actual probabilities, i.e. it is non-negative and
its entries sum to one. This can be addressed through a restriction
to non-negative factorizations, which translates to limiting the pa-
rameters of each convolutional kernel to the simplex. There is a
vast literature on the relations between non-negative factorizations
and generative models [21, 30]. As opposed to most of these works,
we apply factorizations merely to derive our model and analyze its
expressivity – not for learning its parameters (see sec. 2.3).

From a generative perspective, the restriction of convolutional ker-
nels to the simplex results in a latent tree graphical model, as illus-
trated in ﬁg. 2. Each hidden layer in the ConvAC network – a pair
of convolution and pooling operations, corresponds to a transition
between two levels in the tree. More speciﬁcally, each level is com-
prised of multiple latent variables, one for each spatial position in
the input to a hidden layer in the network. Each latent variable in
the input to the l-th layer takes values in [rl

Figure 2: Graphical model
description of HT-TMM

1A represents a probability, and w.l.o.g. we can assume all entries of v(i) are non-negative and (cid:80)M

d=1 v(i)

d =1

1] – the number of channels in the layer that precedes it.

−

3

Pooling operations in the network correspond to the parent-child relationships in the tree – a set of
latent variables are siblings with a shared parent in the tree, if they are positioned in the same pooling
window in the network. The weights of convolution operations correspond to the transition matrix
between a parent and each of its children, i.e. if Hp is the parent latent variable, taking values in
Hp=c)=w(c)
1], then P (Hchild=i
[rl], and Hchild is one of its child variables, taking values in [rl
,
|
where w(c) is the 1
1 convolutional kernel for the c-th output channel. With the above graphical
representation in place, we can easily draw samples from our model.

×

−

i

To conclude this subsection, by leveraging an algebraic perspective of the network polynomial
(eq. 1), we show that tractability is related to the tensor properties of the priors, and in particular,
that low rank factorizations are equivalent to inference via ConvACs. The application of arithmetic
circuits to achieve tractability is by itself not a novelty. However, the particular convolutional arith-
metic circuits we propose lead to a comprehensive understanding of representational abilities, and
as a result, to a straightforward architectural design of TMMs.

2.2 Controlling the Expressivity and Inductive Bias of TMMs

As discussed in sec. 1, it is not enough for a generative model to be tractable – it must also be
sufﬁciently expressive, and moreover, we must also be able to understand how its structure affects
its expressivity. In this section we explain how our algebraic perspective enables us to achieve this.

To begin with, since we derived our model by factorizing the prior tensor, it immediately follows
that given sufﬁcient number of channels in the ConvAC, i.e. given sufﬁcient ranks in the tensor fac-
torization, any distribution could be approximated arbitrarily well (assuming M is allowed to grow).
In short, this amounts to saying that TMMs are universal. Though many other generative models are
known to be universal, it is typically not clear how one may assess what a given structure of ﬁnite
size can and cannot express. In contrast, the expressivity of ConvACs has been throughly studied in
a series of works [9, 8, 11, 27], each of which examined a different attribute of its structure. In Cohen
et al. [9] it was proven that ConvACs exhibit the Depth Efﬁciency property, i.e. deep networks are
exponentially more expressive than shallow ones. In Cohen and Shashua [8] it was shown that deep
networks can efﬁciently model some input correlations but not all, and that by designing appropriate
pooling schemes, different preferences may be encoded, i.e. the inductive bias may be controlled. In
Cohen et al. [11] this result was extended to more complex connectivity patterns, involving mixtures
of pooling schemes. Finally, in Levine et al. [27], an exact relation between the number of channels
and the correlations supported by a network has been found, enabling tight control over expressiv-
ity and inductive bias. All of these results are brought forth by the relations of ConvACs to tensor
factorizations. They allow TMMs to be analyzed and designed in much more principled ways than
alternative high-dimensional generative models.2

2.3 Classiﬁcation and Learning

|

∈

Y =y) for each y

TMMs realized through ConvACs, sharing many of the same traits as ConvNets, are especially suit-
able to serve as classiﬁers. We begin by introducing a class variable Y , and model the conditional
likelihood P(X
[K]. Though it is possible to have separate generative models for
each class, it is much more efﬁcient to leverage the relation to ConvNets and use a shared ConvAC
instead, which is equivalent to a joint-factorization of the prior tensors for all classes. This results
in a single network, where instead of a single scalar output representing P(X), multiple outputs are
driven by the network, representing P(X
Y =y) for each class y. Predicting the class of a given
|
instance is carried through Maximum A-Posteriori, i.e. by returning the most likely class. In the
common setting of uniform class priors, i.e. P(Y =y)
1
K , this corresponds to classiﬁcation by max-
imal network output, as customary with ConvNets. We note that in practice, na¨ıve implementation of
ConvACs is not numerically stable3, and this is treated by performing all computations in log-space,
which transforms ConvACs into SimNets – a recently introduced deep learning architecture [7, 10].

≡

S
Suppose now that we are given a training set S =
|i=1 of instances and
[K])
|
}
labels, and would like to ﬁt the parameters Θ of our model according to the Maximum Likelihood

(Rs)N , Y (i)

(X (i)
{

∈

∈

2 As a demonstration of the fact that ConvAC analyses are not affected by the non-negativity and normal-
ization restrictions of our generative variant, we prove in app. D that the Depth Efﬁciency property still holds.
3Since high degree polynomials (as computed by ACs) are susceptible to numerical underﬂow or overﬂow.

4

−

principle, or equivalently, by minimizing the Negative Log-Likelihood (NLL) loss function:
E[

log PΘ(X, Y )]. The latter can be factorized into two separate loss terms:

L

(Θ) =

(Θ) = E[

log PΘ(Y

X)] + E[
|

log PΘ(X)]

L

−

−

−
where E[
log PΘ(Y
X)], which we refer to as the discriminative loss, is commonly known as
|
log PΘ(X)], which corresponds to maximizing the prior likelihood
the cross-entropy loss, and E[
P(X), has no analogy in standard discriminative classiﬁcation.
It is this term that captures the
generative nature of the model, and we accordingly refer to it as the generative loss. Now, let
NΘ(X (i); y):= log PΘ(X (i)
Y =y) stand for the y’th output of the SimNet (ConvAC in log-space)
|
realizing our model with parameters Θ. In the case of uniform class priors (P(Y =y)
1/K), the
empirical estimation of

(Θ) may be written as:

≡

−

L

−

log

S
|
|
i=1

1
S
|

(Θ; S) =

eNΘ(X (i);Y (i))
K
y=1 eNΘ(X (i);y) −
This objective includes the standard softmax loss as its ﬁrst term, and an additional generative loss
as its second. Rather than employing dedicated Maximum Likelihood methods for training (e.g. Ex-
pectation Minimization), we leverage once more the resemblance between our networks and Con-
vNets, and optimize the above objective using Stochastic Gradient Descent (SGD).

eNΘ(X (i);y)

| (cid:88)

| (cid:88)

S
|
|
i=1

1
S

(cid:88)

log

(2)

(cid:80)

y=1

K

|

L

3 Classiﬁcation under Missing Data through Marginalization

A major advantage of generative models over discriminative ones lies in their ability to cope with
missing data, speciﬁcally in the context of classiﬁcation. By and large, discriminative methods ei-
ther attempt to complete missing parts of the data before classiﬁcation (a process known as data
imputation), or learn directly to classify data with missing values [28]. The ﬁrst of these approaches
relies on the quality of data completion, a much more difﬁcult task than the original one of classiﬁ-
cation under missing data. Even if the completion was optimal, the resulting classiﬁer is known to
be sub-optimal (see app. E). The second approach does not rely on data completion, but nonetheless
assumes that the distribution of missing values at train and test times are similar, a condition which
often does not hold in practice. Indeed, Globerson and Roweis [17] coined the term “nightmare at
test time” to refer to the common situation where a classiﬁer must cope with missing data whose
distribution is different from that encountered in training.

As opposed to discriminative methods, generative models are endowed with a natural mechanism for
classiﬁcation under missing data. Namely, a generative model can simply marginalize over missing
values, effectively classifying under all possible completions, weighing each completion according
to its probability. This, however, requires tractable inference and marginalization. We have already
shown in sec. 2 that TMMs support the former, and will show in sec. F that marginalization can be
just as efﬁcient. Beforehand, we lay out the formulation of classiﬁcation under missing data.

,

X

Y
) the joint distribution of (

D
is drawn conditioned on

be a random vector in Rs representing an object, and let
(
X

Let
be a random variable in [K]
representing its label. Denote by
[K])
,
∈
speciﬁc realizations thereof. Assume that after sampling a speciﬁc instance (x, y), a random binary
s
vector
0, 1
}
=x). xi is considered missing if mi is equal
(realization of
(
·|X
Q
m, whose i’th coordinate is
to zero, and observed otherwise. Formally, we consider the vector x
deﬁned to hold xi if mi=1, and the wildcard
if mi=0. The classiﬁcation task is then to predict y
given access solely to x

=x. More concretely, we sample a binary mask m

) according to a distribution

), and by (x

Rs, y

m.

M

M

∈{

(cid:12)

X

X

Y

Y

∈

∗

X

M

=m
=m

(
M
Q
(
M
Q

|X
|X
is independent of the missing values in

Following the works of Rubin [36], Little and Rubin [28], we consider three cases for the miss-
=x): missing completely at random (MCAR), where
is inde-
ingness distribution
=x) is a function of m but not of x; missing at random (MAR),
pendent of
, i.e.
where
=x) is a function of both m
=m
and x, but is not affected by changes in xi if mi=0; and missing not at random (MNAR), covering
the rest of the distributions for which
=x) is a
function of both m and x, which at least sometimes is sensitive to changes in xi when mi=0.
Let P be the joint distribution of the object
P(

|X
depends on missing values in

, and missingness mask

X
=m) =

(
M
Q

Y
=x,

:
M

, label

(
Q

, i.e.

=m

, i.e.

=x)

=m

=y)

=x,

=y,

M

M

M

|X

X

X

(
X

D

Y

(
M

· Q

|X

X

Y

M

(cid:12)

5

Rs and m

∈

∈{

0, 1
}

s, denote by o(x, m) the event where the random vector

coincides
For given x
with x on the coordinates i for which mi=1. For example, if m is an all-zero vector, o(x, m) covers
the entire probability space, and if m is an all-one vector, o(x, m) corresponds to the event
=x.
X
With these notations in hand, we are now ready to characterize the optimal predictor in the presence
of missing data. The proofs are common knowledge, but provided in app. E for completeness.
Claim 1. For any data distribution
rule in terms of 0-1 loss is given by predicting the class y
P(

, the optimal classiﬁcation
=y

Q
[K], that maximizes P(

and missingness distribution

=y), for an instance x

o(x, m))
|

m.

D

X

Y

∈

·

M

o(x, m),

=m
|
Y
When the distribution
to as the marginalized Bayes predictor:
Corollary 1. Under the conditions of claim 1, if the distribution
classiﬁcation rule may be written as:

Q

(cid:12)

Q

is MAR (or MCAR), the optimal classiﬁer admits a simpler form, referred

is MAR (or MCAR), the optimal

h∗(x

m) = argmaxy

(cid:12)

P(

Y

|

=y

o(x, m))

(3)

Corollary 1 indicates that in the MAR setting, which is frequently encountered in practice, optimal
classiﬁcation does not require prior knowledge regarding the missingness distribution
. As long
as one is able to realize the marginalized Bayes predictor (eq. 3), or equivalently, to compute the
likelihoods of observed values conditioned on labels (P(o(x, m)
Y =y)), classiﬁcation under miss-
|
ing data is guaranteed to be optimal, regardless of the corruption process taking place. This is in
stark contrast to discriminative methods, which require access to the missingness distribution during
training, and thus are not able to cope with unknown conditions at test time.

Q

Most of this section dealt with the task of prediction given an input with missing data, where we
assumed we had access to a “clean” training set, and only faced missingness during prediction.
However, many times we wish to tackle the reverse task, where the training set itself is riddled with
missing data. Tractability leads to an advantage here as well: under the MAR assumption, learning
from missing data with the marginalized likelihood objective results in an unbiased classiﬁer [28].

In the case of TMMs, marginalizing over missing values is just as efﬁcient as plain inference – re-
quires only a single pass through the corresponding network. The exact mechanism is carried out in
similar fashion as in sum-product networks, and is covered in app. F. Accordingly, the marginalized
Bayes predictor (eq. 3) is realized efﬁciently, and classiﬁcation under missing data (in the MAR
setting) is optimal (under generative assumption), regardless of the missingness distribution.

4 Related Works

There are many generative models realized through neural networks, and convolutional networks
in particular, e.g. Generative Adversarial Networks [19], Variational Auto-Encoders [24], and
NADE [38]. However, most do not posses tractable inference, and of the few that do, non posses
tractable marginalization over any set of variables. Due to limits of space, we defer the discussion
on the above to app. G, and in the remainder of this section focus instead on the most relevant works.

As mentioned in sec. 2, we build on the approach of specifying generative models through Arith-
metic Circuits (ACs) [12], and speciﬁcally, our model is a strict subclass of the well-known Sum-
Product Networks (SPNs) [34], under the decomposable and complete restrictions. Where our work
differs is in our algebraic approach to eq. 1, which gives rise to a speciﬁc structure of ACs, called
ConvACs, and a deep theory regarding their expressivity and inductive bias (see sec. 2.2). In contrast
to the structure we proposed, the current literature on general SPNs does not prescribe any speciﬁc
structures, and its theory is limited to either very speciﬁc instances [14], or very broad classes, e.g
ﬁxed-depth circuits [29]. In the early works on SPNs, specialized networks of complex structure
were designed for each task based mainly on heuristics, often bearing little resemblance to common
neural networks. Contemporary works have since moved on to focus mainly on learning the struc-
ture of SPNs directly from data [33, 16, 1, 35], leading to improved results in many domains. Despite
that, only few published studies have applied this method to natural domains (images, audio, etc.),
on which only limited performance, compared to other common methods, was reported, speciﬁcally
on the MNIST dataset [1]. The above suggests that choosing the right architecture of general SPNs,
at least on some domains, remains to be an unsolved problem. In addition, both the previously
studied manually-designed SPNs, as well as ones with a learned structure, lead to models, which

6

n=

0

25

50

75

100

125

150

96.4
97.8
Table 1: Prediction for each two-class task of MNIST digits, under feature deletion noise.

97.9
HT-TMM 98.5

97.5
98.2

94.1
96.5

89.2
93.9

80.9
87.1

70.2
76.3

LP

ptest

0.25

0.50

0.75

0.90

0.95

0.99

ptrain
0.25
0.50
0.75
0.90
0.95
0.99

i.i.d. (rand)
rects (rand)

98.9
99.1
98.9
97.6
95.7
87.3

98.7
98.2

97.8
98.6
98.7
97.5
95.6
86.7

98.4
95.7

78.9
94.6
97.2
96.7
94.8
85.0

97.0
83.2

32.4
68.1
83.9
89.0
88.3
78.2

87.6
54.7

17.6
37.9
56.4
71.0
74.0
66.2

70.6
35.8

11.0
12.9
16.7
21.3
30.5
31.3

29.6
17.5

(a) MNIST with i.i.d. corruption

(b) MNIST with missing rectangles.

Figure 3: We examine ConvNets trained on one missingness distribution while tested on others.
“(rand)” denotes training on distributions with randomized parameters. (a) i.i.d. corruption: trained
with probability ptrain and tested on ptest. (b) missing rectangles: training on randomized distributions
(rand) compared to training on the same (ﬁxed) missing rectangles distribution.

according to recent works on GPU-optimized algorithms [4], cannot be efﬁciently implemented due
to their irregular memory access patterns. This is in stark contrast to our model, which leverages
the same patterns as modern ConvNets, and thus enjoys similar run-time performance. An addi-
tional difference in our work is that we manage to successfully train our model using standard SGD.
Even though this approach has already been considered by Poon and Domingos [34], they deemed
it lacking and advocated for specialized optimization algorithms instead.

Outside the realm of generative networks, tractable graphical models, e.g. Latent Tree Mod-
els (LTMs) [30], are the most common method for tractable inference. Similar to SPNs, it is not
straightforward to ﬁnd the proper structure of graphical models for a particular problem, and most
of the same arguments apply here as well. Nevertheless, it is noteworthy that recent progress in
structure and parameters learning of LTMs [22, 3] was also brought forth by connections to tensor
factorizations, similar to our approach. Unlike the aforementioned algorithms, we utilize tensor
factorizations solely for deriving our model and analyzing its expressivity, while leaving learning
to SGD – the most successful method for training neural networks. Leveraging their perspective to
analyze the optimization properties of our model is viewed as a promising avenue for future research.

5 Experiments

We demonstrate the properties of TMMs through both qualitative and quantitative experiments. In
sec. 5.1 we present state of the art results on image classiﬁcation under missing data, with robustness
to various missingness distributions. In sec. 5.2 we show that our results are not limited to images, by
applying TMMs for speech recognition. Finally, in app. H we show visualizations of samples drawn
from TMMs, shedding light on their generative nature. Our implementation, based on Caffe [23] and
MAPS [4] (toolbox for efﬁcient GPU code generation), as well as all other code for reproducing our
experiments, are available at: https://github.com/HUJI-Deep/Generative-ConvACs. Ex-
tended details regarding the experiments are provided in app. I.

5.1

Image Classiﬁcation under Missing Data

In this section we experiment on two datasets: MNIST [25] for digit classiﬁcation, and small
NORB [26] for 3D object recognition. In our results, we refer to models using shallow networks as
CP-TMM, and to those using deep networks as HT-TMM, in accordance with the respective tensor
factorizations (see sec. 2). The theory discussed in sec. 2.2 guided our exact choice of architectures.
Namely, we used the fact [27] that the capacity to model either short- or long-range correlations in
the input, is related to the number of channels in the beginning or end of a network, respectively. In
MNIST, discriminating between digits has more to do with long-range correlations than the basic
strokes digits are made of, hence we chose to start with few channels and end with many – layer

7

(a) MNIST with i.i.d. corruption.

(b) MNIST with missing rectangles.

(c) NORB with i.i.d. corruption.

(d) NORB with missing rectangles.

Figure 4: Blind classiﬁcation under missing data. (a,c) Testing i.i.d. corruption with probability p
for each pixel. (b,d) Testing missing rectangles corruption with n missing rectangles, each of width
and hight equal to W . (*) Based on the published results [18]. (
†

) Data imputation algorithms.

widths were set to 64-128-256-512. In contrast, the classes of NORB differ in much ﬁner details,
requiring more channels in the ﬁrst layers, hence layer widths were set to 256-256-256-512. In both
cases, M = 32 Gaussian mixing components were used.

We begin by comparing our generative approach to missing data against classical methods, namely,
methods based on Globerson and Roweis [17]. They regard missing data as “feature deletion” noise,
replace missing entries by zeros, and devise a learning algorithm over linear predictors that takes
the number of missing features, n, into account. The method was later improved by Dekel and
Shamir [13]. We compare TMMs to the latter, with n non-zero pixels randomly chosen and changed
to zero, in the two-class prediction task derived from each pair of MNIST digits. Due to limits of
their implementation, only 300 images per digit are used for training. Despite this, and the fact that
the evaluated scenario is of the MNAR type (on which optimality is not guaranteed – see sec. 3),
we achieve signiﬁcantly better results (see table 1), and unlike their method, which requires several
classiﬁers and knowing n, we use a single TMM with no prior knowledge.

∈

Heading on to multi-class prediction under missing data, we focus on the challenging “blind” setting,
where the missingness distribution at test time is completely unknown during training. We simulate
[0, 1] of
two kinds of MAR missingness distributions: (i) an i.i.d. mask with a ﬁxed probability p
dropping each pixel, and (ii) a mask composed of the union of n (possibly overlapping) rectangles
of width and height W , each positioned randomly in the image (uniform distribution). We ﬁrst
demonstrate that purely discriminative classiﬁers cannot generalize to all missingness distributions,
by training the standard LeNeT ConvNet [25] on one set of distributions and then testing it on others
(see ﬁg. 3). Next, we present our main results. We compare our model against three different
approaches. First, as a baseline, we use K-Nearest Neighbors (KNN) to vote on the most likely
class, augmented with an l2-metric that disregards missing coordinates. KNN actually scores better
than most methods, but its missingness-aware distance metric prevents the common memory and
runtime optimizations, making it impractical for real-world settings. Second, we test various data-
imputation methods, ranging from simply ﬁlling missing pixels with zeros or their mean, to modern
generative models suited to inpainting. Data imputation is followed by a ConvNet prediction on
the completed image.
In general, we ﬁnd that this approach only works well when few pixels
are missing. Finally, we test generative classiﬁers other than our model, including MP-DBM and
SPN (sum-product networks). MP-DBM is notable for being limited to approximations, and its
results show the importance of using exact inference instead. For SPN, we have augmented the
model from Poon and Domingos [34] with a class variable Y , and trained it to maximize the joint
probability P (X, Y ) using the code of Zhao et al. [41]. The inferior performance of SPN suggests

8

that the structure of TMMs, which are in fact a special case, is advantageous. Due to limitations of
available public code and time, not all methods were tested on all datasets and distributions. See
ﬁg. 4 for the complete results.

To conclude, TMMs signiﬁcantly outperform all other methods tested on image classiﬁcation with
missing data. Although they are a special case of SPNs, their particular structure appears to be
more effective than ones existing in the literature. We attribute this superiority to the fact that their
architectural design is backed by comprehensive theoretical studies (see sec. 2.2).

5.2 Speech Recognition under Missing Data

To demonstrate the versatility of TMMs, we also conducted limited experiments on the TIMIT
speech recognition dataset, following the same protocols as in sec. 5.1. We trained a TMM and a
standard ConvNet on 256ms windows of raw data at 16Hz sample rate to predict the phoneme at the
center of a window. Both the TMM and the ConvNet reached 78% accuracy on the clean dataset,
but when half of the audio is missing i.i.d., accuracy of the ConvNet with mean imputation drops
to 34%, while the TMM remains at 63%. Utilizing common audio inpainting methods [2] only
improves accuracy of the ConvNet to 48%, well below that of TMM.

6 Summary

This paper focuses on generative models which admit tractable inference and marginalization, ca-
pabilities that lie outside the realm of contemporary neural network-based generative methods. We
build on prior works on tractable models based on arithmetic circuits and sum-product networks,
and leverage concepts from tensor analysis to derive a sub-class of models we call Tensorial Mixture
Models (TMMs). In contrast to existing methods, our algebraic approach leads to a comprehensive
understanding of the relation between model structure and representational properties. In practice,
utilizing this understanding for the design of TMMs has led to state of the art performance in clas-
siﬁcation under missing data. We are currently investigating several avenues for future research,
including semi-supervised learning, and examining more intricate ConvAC architectures, such as
the ones suggested by Cohen et al. [11]).

This work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12 and
by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google
Fellowship in Machine Learning.

Acknowledgments

References

[1] Tameem Adel, David Balduzzi, and Ali Ghodsi. Learning the Structure of Sum-Product Networks via an

SVD-based Algorithm. UAI, 2015.

[2] A Adler, V Emiya, M G Jafari, and M Elad. Audio inpainting.

IEEE Trans. on Audio, Speech and

Language Processing, 20:922–932, March 2012.

[3] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research (), 15(1):2773–2832,
2014.

[4] Tal Ben-Nun, Ely Levy, Amnon Barak, and Eri Rubin. Memory Access Patterns: The Missing Piece of
the Multi-GPU Puzzle. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, pages 19:1–19:12. ACM, 2015.

[5] Yoshua Bengio, ´Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep Generative Stochas-

tic Networks Trainable by Backprop. In International Conference on Machine Learning, 2014.

[6] Richard Caron and Tim Traynor. The Zero Set of a Polynomial. WSMR Report 05-02, 2005.

[7] Nadav Cohen and Amnon Shashua. SimNets: A Generalization of Convolutional Networks. In Advances

in Neural Information Processing Systems NIPS, Deep Learning Workshop, 2014.

9

[8] Nadav Cohen and Amnon Shashua.

Inductive Bias of Deep Convolutional Networks through Pooling

Geometry. In International Conference on Learning Representations ICLR, April 2017.

[9] Nadav Cohen, Or Sharir, and Amnon Shashua. On the Expressive Power of Deep Learning: A Tensor

Analysis. In Conference on Learning Theory COLT, May 2016.

[10] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep SimNets. In Computer Vision and Pattern Recogni-

tion CVPR, May 2016.

[11] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting Dilated Convolutional Networks with

Mixed Tensor Decompositions. arXiv.org, 2017.

[12] Adnan Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM (JACM),

50(3):280–305, May 2003.

[13] Ofer Dekel and Ohad Shamir. Learning to classify with missing and corrupted features. In International

Conference on Machine Learning. ACM, 2008.

[14] Olivier Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks. Advances in Neural

Information Processing Systems, pages 666–674, 2011.

[15] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estima-

tion. arXiv.org, October 2014.

on Machine Learning, 2013.

[16] R Gens and P M Domingos. Learning the Structure of Sum-Product Networks. Internation Conference

[17] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Inter-

national Conference on Machine Learning. ACM, 2006.

[18] Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-Prediction Deep Boltzmann

Machines. Advances in Neural Information Processing Systems, 2013.

[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing
Systems, 2014.

[20] W Hackbusch and S K¨uhn. A New Scheme for the Tensor Representation. Journal of Fourier Analysis

and Applications, 15(5):706–722, 2009.

[21] Thomas Hofmann. Probabilistic latent semantic analysis. Morgan Kaufmann Publishers Inc., July 1999.

[22] Furong Huang, Niranjan U N, Ioakeim Perros, Robert Chen, Jimeng Sun, and Anima Anandkumar. Scal-
able Latent Tree Model and its Application to Health Analytics. In NIPS Machine Learning for Healthcare
Workshop, 2015.

[23] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional Architecture for Fast Feature Embedding. CoRR
abs/1202.2745, cs.CV, 2014.

[24] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on

Learning Representations, 2014.

[25] Yan LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-

ment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[26] Yann LeCun, Fu Jie Huang, and L´eon Bottou. Learning Methods for Generic Object Recognition with

Invariance to Pose and Lighting. Computer Vision and Pattern Recognition, 2004.

[27] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep Learning and Quantum Entangle-

ment: Fundamental Connections with Implications to Network Design. arXiv.org, April 2017.

[28] Roderick J A Little and Donald B Rubin. Statistical analysis with missing data (2nd edition). John Wiley

& Sons, Inc., September 2002.

CoRR abs/1202.2745, cs.LG, 2014.

[29] James Martens and Venkatesh Medabalimi. On the Expressive Efﬁciency of Sum Product Networks.

[30] Rapha¨el Mourad, Christine Sinoquet, Nevin Lianwen Zhang, Tengfei Liu, and Philippe Leray. A Survey

on Latent Tree Models and Applications. J. Artif. Intell. Res. (), cs.LG:157–203, 2013.

10

[31] Andrew Y Ng and Michael I Jordan. On Discriminative vs. Generative Classiﬁers: A comparison of
logistic regression and naive Bayes. In Advances in Neural Information Processing Systems NIPS, Deep
Learning Workshop, 2002.

[32] F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, and E Duchesnay.
Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research (), 12:2825–2830,
2011.

[33] Robert Peharz, Bernhard C Geiger, and Franz Pernkopf. Greedy Part-Wise Learning of Sum-Product
Networks. In Machine Learning and Knowledge Discovery in Databases, pages 612–627. Springer Berlin
Heidelberg, Berlin, Heidelberg, September 2013.

[34] Hoifung Poon and Pedro Domingos. Sum-Product Networks: A New Deep Architecture. In Uncertainty

in Artiﬁcail Intelligence, 2011.

Variable Interactions. ICML, 2014.

[35] Amirmohammad Rooshenas and Daniel Lowd. Learning Sum-Product Networks with Direct and Indirect

[36] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, December 1976.

[37] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. Internation Conference on Machine Learning, 2015.

[38] Benigno Uria, Marc-Alexandre C ˆo t ´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural Autore-
gressive Distribution Estimation. Journal of Machine Learning Research (), 17(205):1–37, 2016.

[39] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In

International Conference on Machine Learning, 2016.

[40] Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In European

Conference on Computer Vision. Springer International Publishing, 2014.

[41] Han Zhao, Pascal Poupart, and Geoff Gordon. A Uniﬁed Approach for Learning the Parameters of
Sum-Product Networks. In Advances in Neural Information Processing Systems NIPS, Deep Learning
Workshop, 2016.

[42] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.

ICCV, pages 479–486, 2011.

11

A The Universality of Tensorial Mixture Models

In this section we prove the universality property of Generative ConvACs, as discussed in sec. 2. We begin by
taking note from functional analysis and deﬁne a new property called PDF total set, which is similar in concept
to a total set, followed by proving that this property is invariant under the cartesian product of functions, which
entails the universality of these models as a corollary.
Deﬁnition 1. Let F be a set of PDFs over Rs. F is PDF total iff for any PDF h(x) over Rs and for all (cid:15) > 0
there exists M ∈ N, {f1(x), . . . , fM (x)} ⊂ F and w ∈ (cid:52)M −1 s.t.
< (cid:15). In
other words, a set is a PDF total set if its convex span is a dense set under L1 norm.
Claim 2. Let F be a set of PDFs over Rs and let F ⊗N = {(cid:81)N
the product space (Rs)N . If F is a PDF total set then F ⊗N is PDF total set.

i=1 fi(x)|∀i, fi(x) ∈ F} be a set of PDFs over

(cid:13)
(cid:13)h(x) − (cid:80)M
(cid:13)

i=1 wifi(x)

(cid:13)
(cid:13)
(cid:13)1

Proof. If F is the set of Gaussian PDFs over Rs with diagonal covariance matrices, which is known to be a
PDF total set, then F ⊗N is the set of Gaussian PDFs over (Rs)N with diagonal covariance matrices and the
claim is trivially true.
Otherwise, let h(x1, . . . , xN ) be a PDF over (Rs)N and let (cid:15) > 0. From the above, there exists K ∈ N,
w ∈ (cid:52)M1−1 and a set of diagonal Gaussians {gij(x)}i∈[M1],j∈[N ] s.t.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gij(xj)

g(x) −

M1(cid:88)

N
(cid:89)

(cid:15)
2

(4)

wi

<

j=1

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

Additionally, since F is a PDF total set then there exists M2 ∈ N, {fk(x)}k∈[M2] ⊂ F and {wij ∈
(cid:13)
(cid:13)gij(x) − (cid:80)M2
(cid:52)M2−1}i∈[M1],j∈[N ] s.t.
(cid:13)
2N ,
from which it is trivially proven using a telescopic sum and the triangle inequality that:

for all i ∈ [M1], j ∈ [N ] it holds that

(cid:13)
(cid:13)
k=1 wijkfk(x)
(cid:13)1

< (cid:15)

i=1
From eq. 4, eq. 5 the triangle inequality it holds that:

j=1

i=1

j=1

k=1

N
(cid:89)

M1(cid:88)

N
(cid:89)

M2(cid:88)

wi

gij(x) −

wi

wijkfk(xj)

<

(5)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

(cid:15)
2

g(x) −

Ak1,...,kN

fkj (xj)

< (cid:15)

M2(cid:88)

k1,...,kN =1

N
(cid:89)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

where Ak1,...,kN = (cid:80)M1
{(cid:81)N

j=1 fkj (xj)}k1∈[M2],...,kN ∈[M2] ⊂ F ⊗N and w = vec(A) completes the proof.

(cid:81)N

j=1 wijkj which holds (cid:80)M2

k1,...,kN =1 Ak1,...,kN = 1. Taking M = M N
2 ,

Corollary 2. Let F be a PDF total set of PDFs over Rs, then the family of Generative ConvACs with mixture
components from F can approximate any P DF over (Rs)N arbitrarily well, given arbitrarily many compo-
nents.

B TMMs with Sparsity Constraints Can Represent Gaussian Mixture

Models

As discussed in sec. 2, TMMs become tractable when a sparsity constraint is imposed on the priors tensor, i.e.
most of the entries of the tensors are replaced with zeros. In this section, we demonstrate that under such a case,
TMMs can represent Gaussian Mixture Models with diagonal covariance matrices, probably the most common
type of mixture models.

With the same notations as sec. 2, assume the number of mixing components of the TMM is M = N · K for
some K ∈ N, let {N (x; µki, diag(σ2
be these components, and ﬁnally, assume the prior tensor has
the following structure:

ki))}K,N

k,i

P (d1, . . . , dN ) =

(cid:40)

wk ∀i ∈ [N ], di=N ·(k−1)+i
0

Otherwise

then eq. 1 reduces to:

(cid:88)K

(cid:89)N

wk

P (X) =

N (x; µki, diag(σ2
k = ((σ2
˜σ2
which is equivalent to a diagonal GMM with mixing weights w ∈ (cid:52)K−1 (where (cid:52)K−1 is the K-dimensional
simplex) and Gaussian mixture components with means { ˜µk}K

k=1
kN )T )T
k1)T , . . . , (σ2

k=1 and covariances {diag( ˜σ2

wkN (x; ˜µk, diag( ˜σ2

k1, . . . , µT

˜µk = (µT

ki)) =

kN )T

k)}K

k=1.

k))

k=1

i=1

(cid:88)K

(cid:13)
M1(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1 wi

12

C Background on Tensor Factorizations

In this section we establish the minimal background in the ﬁeld of tensor analysis required for following our
work. A tensor is best thought of as a multi-dimensional array Ad1,...,dN ∈ R, where ∀i ∈ [N ], di ∈ [Mi].
The number of indexing entries in the array, which are also called modes, is referred to as the order of the
tensor. The number of values an index of a particular mode can take is referred to as the dimension of the mode.
The tensor A ∈ RM1⊗...⊗MN mentioned above is thus of order N with dimension Mi in its i-th mode. For
our purposes we typically assume that M1 = . . . = MN = M , and simply denote it as A ∈ (RM )⊗N .

The fundamental operator in tensor analysis is the tensor product. The tensor product operator, denoted by ⊗,
is a generalization of outer product of vectors (1-ordered vectors) to any pair of tensors. Speciﬁcally, let A and
B be tensors of order P and Q respectively, then the tensor product A ⊗ B results in a tensor of order P + Q,
deﬁned by: (A ⊗ B)d1,...,dP +Q = Ad1,...,dP · BdP +1,...,dP +Q .
The main concept from tensor analysis we use in our work is that of tensor decompositions. The most straight-
forward and common tensor decomposition format is the rank-1 decomposition, also known as a CANDE-
COMP/PARAFAC decomposition, or in short, a CP decomposition. The CP decomposition is a natural exten-
sion of low-rank matrix decomposition to general tensors, both built upon the concept of a linear combination
of rank-1 elements. Similarly to matrices, tensors of the form v(1) ⊗ · · · ⊗ v(N ), where v(i) ∈ RMi are
non-zero vectors, are regarded as N -ordered rank-1 tensors, thus the rank-Z CP decomposition of a tensor A
is naturally deﬁned by:

A =

azaz,1 ⊗ · · · ⊗ az,N

Z
(cid:88)

z=1

Z
(cid:88)

⇒ Ad1,...,dN =

N
(cid:89)

az

az,i
di

z=1

i=1

where {az,i ∈ RMi }N,Z
i=1,z=1 and a ∈ RZ are the parameters of the decomposition. As mentioned above,
for N = 2 it is equivalent to low-order matrix factorization. It is simple to show that any tensor A can be
represented by the CP decomposition for some Z, where the minimal such Z is known as its tensor rank.

Another decomposition we will use in this paper is of a hierarchical nature and known as the Hierarchical Tucker
decomposition [20], which we will refer to as HT decomposition. While the CP decomposition combines
vectors into higher order tensors in a single step, the HT decomposition does that more gradually, combining
vectors into matrices, these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion.
Speciﬁcally, the following describes the recursive formula of the HT decomposition4 for a tensor A ∈ (RM )⊗N
where N = 2L, i.e. N is a power of two5:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

· · ·

· · ·

r0(cid:88)

α=1

rl
1
(cid:88)
−

α=1

rL
2
(cid:88)
−

α=1

rL
1
(cid:88)
−

α=1

φl,j,γ =

al,j,γ
α

φl−1,2j−1,α
(cid:124)
(cid:123)(cid:122)
(cid:125)
order 2l

1

−

⊗ φl−1,2j,α
(cid:125)
(cid:123)(cid:122)
(cid:124)
1
order 2l

−

φL−1,j,γ =

aL−1,j,γ
α

φL−2,2j−1,α
(cid:123)(cid:122)
(cid:125)
(cid:124)
order N
4

⊗ φL−2,2j,α
(cid:123)(cid:122)
(cid:125)
order N
4

(cid:124)

A =

α φL−1,1,α
aL
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

⊗ φL−1,2,α
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

(6)

(7)

where the parameters of the decomposition are the vectors {al,j,γ∈Rrl
1 }l∈{0,...,L−1},j∈[N/2l],γ∈[rl] and the
top level vector aL ∈ RrL
1 , and the scalars r0, . . . , rL−1 ∈ N are referred to as the ranks of the decompo-
sition. Similar to the CP decomposition, any tensor can be represented by an HT decomposition. Moreover,

−

−

4 More precisely, we use a special case of the canonical HT decomposition as presented in Hackbusch and
K¨uhn [20]. In the terminology of the latter, the matrices Al,j,γ are diagonal and equal to diag(al,j,γ) (using
the notations from eq. 7).

5The requirement for N to be a power of two is solely for simplifying the deﬁnition of the HT decomposi-
tion. More generally, instead of deﬁning it through a complete binary tree describing the order of operations,
the canonical decomposition can use any balanced binary tree.

13

any given CP decomposition can be converted to an HT decomposition by only a polynomial increase in the
number of parameters.

Finally, since we are dealing with generative models, the tensors we study are non-negative and sum to one, i.e.
the vectorization of A (rearranging its entries to the shape of a vector), denoted by vec(A), is constrained to lie
in the multi-dimensional simplex, denoted by:

(cid:110)

(cid:52)k :=

x ∈ Rk+1|

(cid:88)k+1
i=1

xi = 1, ∀i ∈ [k + 1] : xi ≥ 0

(cid:111)

(8)

D Proof for the Depth Efﬁciency of Convolutional Arithmetic Circuits with

Simplex Constraints

In this section we prove that the depth efﬁciency property of ConvACs that was proved in Cohen et al. [9]
applies also to the generative variant of ConvACs we have introduced in sec. 2. Our analysis relies on basic
knowledge of tensor analysis and its relation to ConvACs, speciﬁcally, that the concept of “ranks” of each
factorization scheme is equivalent to the number of channels in these networks. For completeness, we provide
a short introduction to tensor analysis in app. C. The

We prove the following theorem, which is the generative analog of theorem 1 from [9]:
Theorem 1. Let Ay be a tensor of order N and dimension M in each mode, generated by the recursive
formulas in eq. 7, under the simplex constraints introduced in sec. 2. Deﬁne r := min{r0, M }, and consider
the space of all possible conﬁgurations for the parameters of the decomposition – {al,j,γ ∈ (cid:52)rl
1−1}l,j,γ.
In this space, the generated tensor Ay will have CP-rank of at least rN/2 almost everywhere (w.r.t. the product
measure of simplex spaces). Put differently, the conﬁgurations for which the CP-rank of Ay is less than rN/2
form a set of measure zero. The exact same result holds if we constrain the composition to be “shared”, i.e. set
al,j,γ ≡ al,γ and consider the space of {al,γ ∈ (cid:52)rl

1−1}l,γ conﬁgurations.

−

−

The only differences between ConvACs and their generative counter-parts are the simplex constraints applied
to the parameters of the models, which necessitate a careful treatment to the measure theoretical arguments of
the original proof. More speciﬁcally, while the k-dimensional simplex (cid:52)k is a subset of the k + 1-dimensional
space Rk+1, it has a zero measure with respect to the Lebesgue measure over Rk+1. The standard method
to deﬁne a measure over (cid:52)k is by the Lebesgue measure over Rk of its projection to that space, i.e.
let
λ : Rk → R be the Lebesgue measure over Rk, p : Rk+1 → Rk, p(x) = (x1, . . . , xk)T be a projection,
and A ⊂ (cid:52)k be a subset of the simplex, then the latter’s measure is deﬁned as λ(p(A)). Notice that p((cid:52)k)
has a positive measure, and moreover that p is invertible over the set p((cid:52)k), and that its inverse is given by
p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
i=1 xi). In our case, the parameter space is the cartesian product
of several simplex spaces of different dimensions, for each of them the measure is deﬁned as above, and the
measure over their cartesian product is uniquely deﬁned by the product measure. Though standard, the choice
of the projection function p above could be seen as a limitation, however, the set of zero measure sets in (cid:52)k
is identical for any reasonable choice of a projection π (e.g. all polynomial mappings). More speciﬁcally, for
any projection π : Rk+1 → Rk that is invertible over π((cid:52)k), π−1 is differentiable, and the Jacobian of π−1
is bounded over π((cid:52)k), then a subset A ⊂ (cid:52)k is of measure zero w.r.t. the projection π iff it is of measure
zero w.r.t. p (as deﬁned above). This implies that if we sample the weights of the generative decomposition
(eq. 7 with simplex constraints) by a continuous distribution, a property that holds with probability 1 under the
standard parameterization (projection p), will hold with probability 1 under any reasonable parameterization.

We now state and prove a lemma that will be needed for our proof of theorem 1.
Lemma 1. Let M, N, K ∈ N, 1 ≤ r ≤ min{M, N } and a polynomial mapping A : RK → RM ×N (i.e.
for every i ∈ [M ], j ∈ [N ] then Aij : Rk → R is a polynomial function). If there exists a point x ∈ RK s.t.
rank (A(x)) ≥ r, then the set {x ∈ RK |rank (A(x)) < r} has zero measure.

Proof. Remember that rank (A(x)) ≥ r iff there exits a non-zero r × r minor of A(x), which is polynomial
(cid:1) be the number of minors in A,
in the entries of A(x), and so it is polynomial in x as well. Let c = (cid:0)M
denote the minors by {fi(x)}c
i=1 fi(x)2. It thus holds that
f (x) = 0 iff for all i ∈ [c] it holds that fi(x) = 0, i.e. f (x) = 0 iff rank (A(x)) < r.

i=1, and deﬁne the polynomial function f (x) = (cid:80)c

(cid:1) · (cid:0)N

r

r

Now, f (x) is a polynomial in the entries of x, and so it either vanishes on a set of zero measure, or it is
the zero polynomial (see Caron and Traynor [6] for proof). Since we assumed that there exists x ∈ RK s.t.
rank(A(x)) ≥ r, the latter option is not possible.

Following the work of Cohen et al. [9], our main proof relies on following notations and facts:

14

• We denote by [A] the matricization of an N -order tensor A (for simplicity, N is assumed to be
even), where rows and columns correspond to odd and even modes, respectively. Speciﬁcally, if
A ∈ RM1×···MN , the matrix [A] has M1 · M3 · . . . · MN −1 rows and M2 · M4 · . . . · MN columns,
rearranging the entries of the tensor such that Ad1,...,dN is stored in row index 1 + (cid:80)N/2
i=1(d2i−1 −
1) (cid:81)N/2
j=i+1 M2j. Additionally, the matriciza-
tion is a linear operator, i.e. for all scalars α1, α2 and tensors A1, A2 with the order and dimensions
in every mode, it holds that [α1A1 + α2A2] = α1[A1] + α2[A2].

j=i+1 M2j−1 and column index 1 + (cid:80)N/2

i=1(d2i − 1) (cid:81)N/2

• The relation between the Kronecker product (denoted by (cid:12)) and the tensor product (denoted by ⊗)

is given by [A ⊗ B] = [A] (cid:12) [B].

• For any two matrices A and B, it holds that rank (A (cid:12) B) = rank (A) · rank (B).

• Let Z be the CP-rank of A, then it holds that rank ([A]) ≤ Z (see [9] for proof).

Proof of theorem 1. Stemming from the above stated facts, to show that the CP-rank of Ay is at least rN/2, it
is sufﬁcient to examine its matricization [Ay] and prove that rank ([Ay]) ≥ rN/2.
Notice from the construction of [Ay], according to the recursive formula of the HT-decomposition, that
its entires are polynomial in the parameters of the decomposition, its dimensions are M N/2 each and that
1 ≤ rN/2 ≤ M N/2. In accordance with the discussion on the measure of simplex spaces, for each vector
1−1, and notice that
parameter al,j,γ ∈ (cid:52)rl
p−1(˜al,j,γ) is a polynomial mapping6 w.r.t. ˜al,j,γ. Thus, [Ay] is a polynomial mapping w.r.t. the projected
parameters {˜al,j,γ}l,j,γ, and using lemma 1 it is sufﬁcient to show that there exists a set of parameters for
which rank ([Ay]) ≥ rN/2.

1−1, we instead examine its projection ˜al,j,γ = p(al,j,γ) ∈ Rrl

−

−

Denoting for convenience φL,1,1 := Ay and rL = 1, we will construct by induction over l = 1, ..., L a
set of parameters, {al,j,γ}l,j,γ, for which the ranks of the matrices {[φl,j,γ]}j∈[N/2l],γ∈[rl] are at least r2l/2,
while enforcing the simplex constraints on the parameters. More so, we’ll construct these parameters s.t.
al,j,γ = al,γ, thus proving both the ”unshared” and ”shared” cases.

For the case l = 1 we have:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

and let a1,j,γ
and so

α =

r

1α
≤
r

and a0,j,α
i

= 1α=i for all i, j, γ and α ≤ M , and a0,j,α

i

= 1i=1 for all i and α > M ,

1/r
0
which means rank (cid:0)[φ1,j,γ](cid:1) = r, while preserving the simplex constraints, which proves our inductive hy-
pothesis for l = 1.

i = j ∧ i ≤ r
Otherwise

[φ1,j,γ]i,j =

≥ r2l

−

1/2 for all j(cid:48) ∈ [N/2l

1] and γ(cid:48) ∈ [rl−1]. For some speciﬁc choice

−

[φl−1,j(cid:48),γ(cid:48) ]
Assume now that rank
of j ∈ [N/2l] and γ ∈ [rl] we have:

(cid:16)

(cid:17)

r0(cid:88)

α=1

(cid:40)

φl,j,γ =

α φl−1,2j−1,α ⊗ φl−1,2j,α
al,j,γ

=⇒ [φl,j,γ] =

al,j,γ
α

[φl−1,2j−1,α] (cid:12) [φl−1,2j,α]

Denote Mα := [φl−1,2j−1,α] (cid:12) [φl−1,2j,α] for α = 1, ..., rl−1. By our inductive assumption, and by the
general property rank (A (cid:12) B) = rank (A) · rank (B), we have that the ranks of all matrices Mα are at least
r2l
α=1 al,j,γ
· Mα, and noticing that {Mα} do not depend on
α = 1α=1, and thus φl,j,γ = M1, which is of rank r2l/2. This completes the proof
al,j,γ, we simply pick al,j,γ
of the theorem.

1/2 = r2l/2. Writing [φl,j,γ] = (cid:80)rl

1/2 · r2l

α

−

−

−

1

From the perspective of ConvACs with simplex constraints, theorem 1 leads to the following corollary:

6As we mentioned earlier, p is invertible only over p((cid:52)k),

p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
as deﬁned here over the entire range Rk−1, even where it does not serve as the inverse of p.

for which its inverse is given by
i=1 xi). However, to simpliﬁed the proof and notations, we use p−1

rl
1
(cid:88)
−

α=1

rl
1
(cid:88)
−

α=1

15

Corollary 3. Assume the mixing components M = {fi(x) ∈ L2(R2) ∩ L1(Rs)}M
i=1 are square integrable7
probability density functions, which form a linearly independent set. Consider a deep ConvAC model with
simplex constraints of polynomial size whose parameters are drawn at random by some continuous distribution.
Then, with probability 1, the distribution realized by this network requires an exponential size in order to be
realized (or approximated w.r.t. the L2 distance) by the shallow ConvAC model with simplex constraints. The
claim holds regardless of whether the parameters of the deep model are shared or not.

Proof. Given a coefﬁcient tensor A, the CP-rank of A is a lower bound on the number of channels (of its next
to last layer) required to represent that tensor by the ConvAC following the CP factorization. Additionally,
since the mixing components are linearly independent, their products {(cid:81)N
i=1 fi(xi)|fi ∈ M} are linearly
independent as well, which entails that any distribution representable by the generative variant of ConvAC
with mixing components M has a unique coefﬁcient tensor A. From theorem 1, the set of parameters of a
deep ConvAC model (under the simplex constraints) with a coefﬁcient tensor of a polynomial CP-rank – the
requirement for a polynomially-sized shallow ConvAC model with simplex constraints realizing that same
distribution exactly – forms a set of measure zero.

It is left to prove, that not only is it impossible to exactly represent a distribution with an exponential coefﬁcient
tensor by a shallow model, it is also impossible to approximate it. This follows directly from lemma 7 in
appendix B of Cohen et al. [9], as our case meets the requirement of that lemma.

E Proof for the Optimality of Marginalized Bayes Predictor

In this section we give short proofs for the claims from sec. 3, on the optimality of the marginalized Bayes
predictor under missing-at-random (MAR) distribution, when the missingness mechanism is unknown, as well
as the general case when we do not add additional assumptions. In addition, we will also present a counter ex-
ample proving data imputation results lead to suboptimal classiﬁcation performance. We begin by introducing
several notations that augment the notations already introduced in the body of the article.
Given a speciﬁc mask realization m ∈ {0, 1}s, we use the following notations to denote partial assignments
to the random vector X . For the observed indices of X , i.e. the indices for which mi = 1, we denote a partial
assignment by X \ m = xo, where xo ∈ Rdo is a vector of length do equal to the number of observed indices.
Similarly, we denote by X ∩ m = xm a partial assignment to the missing indices according to m, where
xm ∈ Rdm is a vector of length dm equal to the number of missing indices. As an example of the notation,
for given realizations x ∈ Rs and m ∈ {0, 1}s, we deﬁned in sec. 3 the event o(x, m), which using current
notation is marked by the partial assignment X \ m = xo where xo matches the observed values of the vector
x according to m.

With the above notations in place, we move on to prove claim 1, which describes the general solution to the
optimal prediction rule given both the data and missingness distributions, and without adding any additional
assumptions.

7It is important to note that most commonly used distribution functions are square integrable, e.g. most

members of the exponential family such as the Gaussian distribution.

16

Proof of claim 1. Fix an arbitrary prediction rule h. We will show that L(h∗) ≤ L(h), where L is the expected
0-1 loss.

1 − L(h)=E(x,m,y)∼(X ,M,Y)[1h(x(cid:12)m)=y]

P(M=m, X =x, Y=y)1h(x(cid:12)m)=ydx

(cid:88)

(cid:90)

(cid:88)

m∈{0,1}s

y∈[k]

Rs

(cid:88)

(cid:90)

(cid:88)

(cid:90)

Rdo

Rdm

y∈[k]

m∈{0,1}s
P(M=m, X \m=xo, X ∩m=xm, Y=y)1h(x⊗m)=ydxodxm

1h(x(cid:12)m)=ydxo

(cid:88)

(cid:90)

(cid:88)

y∈[k]

Rdo

m∈{0,1}s
(cid:90)

Rdm
(cid:88)

(cid:90)

(cid:88)

Rdo

m∈{0,1}s

(cid:88)

y∈[k]
(cid:90)

Rdo

P(M=m, X \m=xo, X ∩m=xm, Y=y)dxm

1h(x(cid:12)m)=yP(M=m, X \m=xo, Y=y)dxo

P(X \m=xo)

1h(x(cid:12)m)=yP(Y=y|X \m=xo)

(cid:88)

=

=

=1

=2

=3

≤4

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo
(cid:88)

(cid:88)

(cid:90)

P(X \m=xo)

Rdo

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo

=1 − L(h∗)

1h∗(x(cid:12)m)=yP(Y=y|X \m=xo)

Where (1) is because the output of h(x (cid:12) m) is independent of the missing values, (2) by marginalization,
(3) by conditional probability deﬁnition and (4) because by deﬁnition h∗(x (cid:12) m) maximizes the expression
P(Y=y|X \m=xo)P(M=m|X \m=xo, Y=y) w.r.t.
the possible values of y for ﬁxed vectors m and xo.
Finally, by replacing integrals with sums, the proof holds exactly the same when instances (X ) are discrete.

We now continue and prove corollary 1, a direct implication of claim 1 which shows that in the MAR setting,
the missingness distribution can be ignored, and the optimal prediction rule is given by the marginalized Bayes
predictor.

Proof of corollary 1. Using the same notation as in the previous proof, and denoting by xo the partial vector
containing the observed values of x (cid:12) m, the following holds:

P(M=m|o(x, m), Y=y) := P(M=m|X \m=xo, Y=y)

P(M=m, X ∩ m=xm|X \m=xo, Y=y)dxm

(cid:90)

(cid:90)

=

=

Rdm

P(X ∩m=xm|X \m=xo, Y=y)

Rdm
· P(M=m|X ∩m=xm, X \m=xo, Y=y)dxm
(cid:90)

P(X ∩m=xm|X \m=xo, Y=y)

=1

Rdm
· P(M=m|X ∩m=xm, X \m=xo)dxm
(cid:90)

=2

P(X ∩m=xm|X \m=xo, Y=y) · P(M=m|X \m=xo)dxm

Rdm

=P(M=m|X \m=xo)

P(X ∩m=xm|X \m=xo, Y=y)dxm

(cid:90)

Rdm

=P(M=m|o(x, m))

Where (1) is due to the independence assumption of the events Y = y and M = m conditioned on X = x,
while noting that (X \ m = xo) ∧ (X ∩ m = xm) is a complete assignment of X . (2) is due to the MAR
assumption, i.e. that for a given m and xo it holds for all xm ∈ Rdm :

P(M=m|X \m=xo, X ∩m=xm) = P(M=m|X \m=xo)

17

Y Weight

Probability ((cid:15) = 10−

4)

X1 X2
0
0
1
0
0
1
1
1
0
0
1
0
0
1
1
1

0
0
0
0
1
1
1
1

1

1

−
1

(cid:15)

(cid:15)

−
1
0
1 + (cid:15)
0
1 + (cid:15)

16.665%
16.667%
16.665%
16.667%
0.000%
16.668%
0.000%
16.668%
2
0, 1

Table 2: Data distribution over the space
the sub-optimality of classiﬁcation through data-imputation (proof of claim 3).

0, 1
}
{

X × Y

× {

=

}

that serves as the example for

We have shown that P(M=m|o(x, m), Y = y) does not depend on y, and thus does not affect the optimal
prediction rule in claim 1. It may therefore be dropped, and we obtain the marginalized Bayes predictor.

Having proved that in the MAR setting, classiﬁcation through marginalization leads to optimal performance,
we now move on to show that the same is not true for classiﬁcation through data-imputation. Though there are
many methods to perform data-imputation, i.e. to complete missing values given the observed ones, all of these
methods can be seen as the solution of the following optimization problem, or more typically its approximation:

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48))

Where g(x (cid:12) m) is the most likely completion of x (cid:12) m. When data-imputation is carried out for classiﬁcation
purposes, one is often interested in data-imputation conditioned on a given class Y = y, i.e.:

g(x (cid:12) m; y) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48)|Y = y)

Given a classiﬁer h : Rs → [K] and an instance x with missing values according to m, classiﬁcation through
data-imputation is simply the result of applying h on the output of g. When h is the optimal classiﬁer for
complete data, i.e. the Bayes predictor, we end up with one of the following prediction rules:

Unconditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m))

Conditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m; y))

y

y

Claim 3. There exists a data distribution D and MAR missingness distribution Q s.t. the accuracy of classi-
ﬁcation through data-imputation is almost half the accuracy of the optimal marginalized Bayes predictor, with
an absolute gap of more than 33 percentage points.

Proof. For simplicity, we will give an example for a discrete distribution over
the binary set
X × Y = {0, 1}2 × {0, 1}. Let 1 > (cid:15) > 0 be some small positive number, and we deﬁne D according to table 2,
where each triplet (x1, x2, y) ∈ X ×Y is assigned a positive weight, which through normalization deﬁnes a
distribution over X ×Y. The missingness distribution Q is deﬁned s.t. PQ(M1 = 1, M2 = 0|X = x) = 1 for
all x ∈ X , i.e. X1 is always observed and X2 is always missing, which is a trivial MAR distribution. Given the
above data distribution D, we can easily calculate the exact accuracy of the optimal data-imputation classiﬁer
and the marginalized Bayes predictor under the missingness distribution Q, as well as the standard Bayes pre-
dictor under full-observability. First notice that whether we apply conditional or unconditional data-imputation,
and whether X1 is equal to 0 or 1, the completion will always be X2 = 1 and the predicted class will always
be Y = 1. Since the data-imputation classiﬁers always predict the same class Y = 1 regardless of their input,
(for (cid:15) = 10−4 it equals approximately
the probability of success is simply the probability P (Y = 1) = 1+(cid:15)
3
33.337%). Similarly, the marginalized Bayes predictor always predicts Y = 0 regardless of its input, and so
(for (cid:15) = 10−4 it equals approximately 66.663%), which is
its probability of success is P (Y = 0) = 2−(cid:15)
3
almost double the accuracy achieved by the data-imputation classiﬁer. Additionally, notice that the marginal-
ized Bayes predictor achieves almost the same accuracy as the Bayes predictor under full-observability, which
equals exactly 2
3 .

18

F Efﬁcient Marginalization with Tensorial Mixture Models

As discussed above, with generative models optimal classiﬁcation under missing data (in the MAR setting) is
oblivious to the speciﬁc missingness distribution. However, it requires tractable marginalization over missing
values. In this section we show that TMMs bring forth extremely efﬁcient marginalization, requiring only a
single forward pass through the corresponding ConvAC.

Recall from sec. 2 and 2.3 that a TMM classiﬁer realizes the following form:

P (x1, . . . , xN |Y =y) =

P (d1, . . . , dN |Y =y)

P (xi|di; θdi )

(9)

(cid:88)M

d1,...,dN

(cid:89)N

i=1

Suppose now that only the local structures xi1 . . . xiV are observed, and we would like to marginalize over the
rest. Integrating eq. 9 gives:

P (xi1 , . . . , xiV |Y =y) =

P (d1, . . . , dN |Y =y)

(cid:88)M

d1,...,dN

(cid:89)V

v=1

P (xiv |div ; θdiv

)

from which it is evident that the same network used to compute P (x1, . . . , xN |Y =y), can be used to compute
P (xi1 , . . . , xiV |Y =y) – all it requires is a slight adaptation of the representation layer. Namely, the latter
would represent observed values through the usual likelihoods, whereas missing (marginalized) values would
now be represented via constant ones:
(cid:40)

rep(i, d) =

1
P (xi|d; Θ)

, xi is missing (marginalized)
, xi is visible (not marginalized)

More generally, to marginalize over individual coordinates of the local structure xi, it is sufﬁcient to replace
rep(i, d) by its respective marginalized mixing component.

To conclude, with TMMs marginalizing over missing values is just as efﬁcient as plain inference – requires
only a single pass through the corresponding network. Accordingly, the marginalized Bayes predictor (eq. 3)
is realized efﬁciently, and classiﬁcation under missing data (in the MAR setting) is optimal (under generative
assumption), regardless of the missingness distribution.

G Extended Discussion on Generative Models Based on Neural Networks

There are many generative models realized through neural networks, and convolutional networks in particular.
Of these models, one of the most successful to date is the method of Generative Adversarial Networks [19],
where a network is trained to generate instances from the data distribution, through a two-player mini-max
game. While there are numerous applications for learning to generate data points, e.g. inpainting and super-
resolution, it cannot be used for computing the likelihood of the data. Other generative networks do offer
inference, but only approximate. Variational Auto-Encoders [24] use a variational lower-bound on the likeli-
hood function. GSNs [5], DPMs [37] and MPDBMs [18] are additional methods along this line. The latter
is especially noteworthy for being a generative classiﬁer that can approximate the marginal likelihoods condi-
tioned on each class, and for being tested on classiﬁcation under missing data.

Some generative neural networks are capable of tractable inference, but not of tractable marginalization. Dinh
et al. [15] suggest a method for designing neural networks that realize an invertible transformation from a
Inverting the network brings forth tractable inference, yet par-
simple distribution to the data distribution.
tial integration of its density function is still intractable. Another popular method for tractable inference,
central to both PixelRNN [39] and NADE [38], is the factorization of the probability distribution according
to P(x1, . . . , xd) = (cid:81)d
P(xi|xi−1, . . . , x1), and realization of P(xi|xi−1, . . . , x1) as a neural network.
Based on this construction, certain marginal distributions are indeed tractable to compute, but most are not.
Orderless-NADE partially addresses this issue by using ensembles of models over different orderings of its
input. However, it can only estimate the marginal distributions, and has no classiﬁer analogue that can compute
class-conditional marginal likelihoods, as required for classiﬁcation under missing data.

i=1

H Image Generation and Network Visualization

Following the graphical model perspective of our models allows us to not only generate random instances from
the distribution, but to also generate the most likely patches for each neuron in the network, effectively explain-
ing its role in the classiﬁcation process. We remind the reader that every neuron in the network corresponds to a
possible assignment of a latent variable in the graphical model. By looking for the most likely assignments for
each of its child nodes in the graphical tree model, we can generate a patch that describes that neuron. Unlike
similar suggested methods to visualize neural networks [40], often relying on brute-force search or on solving
some optimization problem to ﬁnd the most likely image, our method emerges naturally from the probabilistic
interpretation of our model.

19

Figure 5: Generated digits samples from the HT-TMM model trained on the MNIST dataset.

Figure 6: Visualization of the HT-TMM model. Each of the images above visualize a different
layer of the model and consists of several samples generated from latent variables at different spatial
locations conditioned on randomly selected channels. The leftmost image shows samples taken
from the 5th layer which consists of just a single latent variable with 512 channels. The center
image shows samples taken from the 4th layer, which consists of 2
2 grid of latent variables with
256 channels each. The image is divided to 4 quadrants, each contains samples taken from the
respective latent variable at that position. The rightmost image shows samples from the 3rd layer,
which consists of 4
4 grid of latent variables with 128 channels, and the image is similarly spatial
divided into different areas matching the latent variables of the layer.

×

×

20

In ﬁg. 5, we can see conditional samples generates for each digit, while in ﬁg. 6 we can see a visualization of the
top-level layers of network, where each small patch matches a different neuron in the network. The common
wisdom of how ConvNets work is by assuming that simple low-level features are composed together to create
more and more complex features, where each subsequent layer denotes features of higher abstraction – the
visualization of our network clearly demonstrate this hypothesis to be true for our case, showing small strokes
iteratively being composed into complete digits.

I Detailed Description of the Experiments

Experiments are meaningful only if they could be reproduced by other proﬁcient individuals. Providing suf-
ﬁcient details to enable others to replicate our results is the goal of this section. We hope to accomplish this
by making our code public, as well as documenting our experiments to a sufﬁcient degree allowing for their
reproduction from scratch. Our complete implementation of the models presented in this paper, as well as
our modiﬁcations to other open-source projects and scripts used in the process of conducting our experiments,
are available at our Github repository: https://github.com/HUJI-Deep/Generative-ConvACs. We ad-
ditionally wish to invite readers to contact the authors, if they deem the following details insufﬁcient in their
process to reproduce our results.

I.1 Description of Methods

In the following we give concise descriptions of each classiﬁcation method we have used in our experiments.
The results of the experiment on MP-DBM [18] were taken directly from the paper and were not conducted
by us, hence we do not cover it in this section. We direct the reader to that article for exact details on how to
reproduce their results.

I.1.1 Robust Linear Classiﬁer

In [13], binary linear classiﬁers were trained by formulating their optimization as a quadric program under the
constraint that some of its features could be deleted, i.e. their original value was changed to zero. While the
original source code was never published, the authors have kindly agreed to share with us their code, which
we used to reproduced their results, but on larger datasets. The algorithm has only a couple hyper-parameters,
which were chosen by a grid-search through a cross-validation process. For details on the exact protocol for
testing binary classiﬁers on missing data, please see sec. I.2.1.

I.1.2 K-Nearest Neighbors

K-Nearest Neighbors (KNN) is a classical machine learning algorithm used for both regression and classiﬁca-
tion tasks. Its underlying mechanism is ﬁnding the k nearest examples (called neighbors) from the training set,
(x1, y1), . . . , (xk, yk) ∈ S, according to some metric function d(·, ·) : X × X → R+, after which a summa-
rizing function f is applied to the targets of the k nearest neighbors to produce the output y∗ = f (y1, . . . , yk).
When KNN is used for classiﬁcation, f is typically the majority voting function, returning the class found in
most of the k nearest neighbors.

In our experiments we use KNN for classiﬁcation under missing data, where the training set consists of com-
plete examples with no missing data, but at classiﬁcation time the inputs have missing values. Given an input
with missing values x (cid:12) m and an example x(cid:48) from the training set, we use a modiﬁed Euclidean distance
metric, where we compare the distance only against the non-missing coordinates of x, i.e. the metric is deﬁned
by d(x(cid:48), x (cid:12) m) = (cid:80)
i − xi)2. Through a process of cross-validation we have chosen k = 5 for
all of our experiments. Our implementation of KNN is based on the popular scikit-learn python library [32].

i:mi=1 (x(cid:48)

I.1.3 Convolutional Neural Networks

The most widespread and successful discriminative method nowadays are Convolutional Neural Net-
works (ConvNets). Standard ConvNets are represented by a computational graph consisted of different kinds
of nodes, called layers, with a convolutional-like operators applied to their inputs, followed by a non-linear
point-wise activation function, e.g. max(0, x) known as ReLU.

For our experiments on MNIST, both with and without missing data, we have used the LeNeT ConvNet ar-
chitecture [25] that is bundled with Caffe [23], trained for 20,000 iterations using SGD with 0.9 momentum
and 0.01 base learning rate, which remained constant for 10,000 iterations, followed by a linear decrease to
0.001 for another 5,000 iterations, followed by a linear decrease to 0 learning rate for the remaining 5,000
iterations. The model also used l2-regularization (also known as weight decay), which was chosen through
cross-validation for each experiment separately. No other modiﬁcations were made to the model or its training
procedure.

21

For our experiments on NORB, we have used an ensemble of 3 ConvNets, each using the following architecture:
5×5 convolution with 128 output channels, 3×3 max pooling with stride 2, ReLU activation, 5×5 convolution
with 128 output channels, ReLU activation, dropout layer with probability 0.5, 3×3 average pooling with
stride 2, 5×5 convolution with 256 output channels, ReLU activation, dropout layer with probability 0.5,
3×3 average pooling with stride 2, fully-connected layer with 768 output channels, ReLU activation, dropout
layer with probability 0.5, and ends with fully-connected layer with 5 output channels. The stereo images
were represented as a two-channel input image when fed to the network. During training we have used data
augmentation consisting of randomly scaling and rotation transforms. The networks were trained for 40,000
iterations using SGD with 0.99 momentum and 0.001 base learning rate, which remained constant for 30,000
iterations, followed by a linear decrease to 0.0001 for 6000 iterations, followed by a linear decrease to 0 learning
rate for the remaining 4,000 iterations. The model also used 0.0001 weight decay for additional regularization.

When ConvNets were trained on images containing missing values, we passed the network the original image
with missing values zeroed out, and an additional binary image as a separate channel, containing 1 for missing
values at the same spatial position, and 0 otherwise – this missing data format is sometimes known as ﬂag
data imputation. Other formats for representing missing values were tested (e.g. just using zeros for missing
values), however, the above scheme performed signiﬁcantly better than other formats. In our experiments, we
assumed that the training set was complete and missing values were only present in the test set. In order to
design ConvNets that are robust against speciﬁc missingness distributions, we have simulated missing values
during training, sampling a different mask of missing values for each image in each mini-batch. As covered
in sec. 5, the results of training ConvNets directly on simulated missingness distributions resulted in classiﬁers
which were biased towards the speciﬁc distribution used in training, and performed worse on other distributions
compared to ConvNets trained on the same distribution.

In addition to training ConvNets directly on missing data, we have also used them as the classiﬁer for testing
different data imputation methods, as describe in the next section.

I.1.4 Classiﬁcation Through Data Imputation

The most common method for handling missing data, while leveraging available discriminative classiﬁers, is
through the application of data imputation – an algorithm for the completion of missing values – and then
passing the results to a classiﬁer trained on uncorrupted dataset. We have tested ﬁve different types of data
imputation algorithms:

• Zero data imputation: replacing every missing value by zero.

• Mean data imputation: replacing every missing value by the mean value computed over the dataset.

• Generative data imputation: training a generative model and using it to complete the missing values
by ﬁnding the most likely instance that coincides with the observed values, i.e. solving the following

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i,mi=1→x(cid:48)i=xi

P (X = x(cid:48))

We have tested the following generative models:

– Generative Stochastic Networks (GSN) [5]: We have used their original source code from
https://github.com/yaoli/GSN, and trained their example model on MNIST for 1000
epochs. Whereas in the original article they have tested completing only the left or right side of
a given image, we have modiﬁed their code to support general masks. Our modiﬁed implemen-
tation can be found at https://github.com/HUJI-Deep/GSN.

– Non-linear Independent Components Estimation (NICE) [15]: We have used their original
source code from https://github.com/laurent-dinh/nice, and trained it on MNIST us-
ing their example code without changes. Similarly to our modiﬁcation to the GSN code, here
too we have adapted their code to support general masks over the input. Additionally, their orig-
inal inpainting code required 110,000 iterations, which we have reduced to just 8,000 iterations,
since the effect on classiﬁcation accuracy was marginal. For the NORB dataset, we have used
their CIFAR10 example, with lower learning rate of 10−4. Our modiﬁed code can be found at
https://github.com/HUJI-Deep/nice.

– Diffusion Probabilistic Models (DPM) [37]: We have user their original source code
from https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models, and
trained it on MNIST using their example code without changes. Similarly to our modiﬁ-
cations to GSN, we have add support for a general mask of missing values, but other than
that kept the rest of the parameters for inpainting unchanged. For NORB we have used
the same model as MNIST. We have tried using their CIFAR10 example for NORB, how-
ever, it produced exceptions during training. Our modiﬁed code can be found at https:
//github.com/HUJI-Deep/Diffusion-Probabilistic-Models.

22

I.1.5 Tensorial Mixture Models

For a complete theoretical description of our model please see the body of the article. Our models were
implemented by performing all intermediate computations in log-space, using numerically aware operations. In
practiced, that meant our models were realized by the SimNets architecture [7, 10], which consists of Similarity
layers representing gaussian distributions, MEX layers representing weighted sums performed on log-space
input and outputs, as well as standard pooling operations. The learned parameters of the MEX layers are called
offsets, which represents the weights of the weighted sum, but saved in log-space. The parameters of the MEX
layers can be optionally shared between spatial regions, or alternatively left with no parameter sharing at all.
Additionally, when used to implement our generative models, the offsets are normalized to have a soft-max (i.e.,
log (cid:0)(cid:80)

i exp(xi)(cid:1)) of zero.

The network architectures we have tested in this article, consists of M different Gaussian mixture components
with diagonal covariance matrices, over non-overlapping patches of the input of size 2 × 2, which were imple-
mented by a similarity layer as speciﬁed by the SimNets architecture, but with an added gaussian normalization
term.

We ﬁrst describe the architectures used for the MNIST dataset. For the CP-TMM model, we used M = 800,
and following the similarity layer is a 1 × 1 MEX layer with no parameter sharing over spatial regions and
10 output channels. The model ends with a global sum pooling operation, followed by another 1 × 1 MEX
layer with 10 outputs, one for each class. The HT-TMM model starts with the similarity layer with M = 32,
followed by a sequence of four pairs of 1 × 1 MEX layer followed by 2 × 2 sum pooling layer, and after the
pairs and additional 1 × 1 MEX layer lowering the outputs of the model to 10 outputs as the number of classes.
The number of output channels for each MEX layer are as follows 64-128-256-512-10. All the MEX layers in
this network do not use parameter sharing, except the ﬁrst MEX layer, which uses a repeated sharing pattern
of 2 × 2 offsets, that analogous to a 2 × 2 convolution layer with stride 2. Both models were trained with the
losses described in sec. 2.3, using the Adam SGD variant for optimizing the parameters, with a base learning
rate of 0.03, and β1 = β2 = 0.9. The models were trained for 25,000 iterations, where the learning rate was
dropped by 0.1 after 20,000 iterations.

For the NORB dataset, we have trained only the HT-TMM model with M = 128 for the similarity layer. The
MEX layers use the same parameter sharing scheme as the one for MNIST, and the number of output channels
for each MEX layer are as follows: 256-256-256-512-5. Training was identical to the MNIST models, with the
exception of using 40,000 iterations instead of just 25,000. Additionally, we have used an ensemble of 4 models
trained separately, each trained using a different generative loss weight (see below for more information). We
have also used the same data augmentation methods (scaling and rotation) which were used in training the
ConvNets for NORB used in this article.

i x2

The standard L2 weight regularization (sometimes known as weight decay) did not work well on our mod-
els, which lead us to adapt it to better ﬁt to log-space weights, by minimizing λ (cid:80)
i (exp (xi))2 instead of
λ||x||2 = λ (cid:80)
i , where the parameter λ was chosen through cross-validation. Additionally, since even
with large values of λ our model was still overﬁtting, we have added another form of regularization in the form
of random marginalization layers. A random marginalization layer, is similar in concept to dropout, but instead
of zeroing activations completely in random, it choses spatial locations at random, and then zero out the activa-
tions at those locations for all the channels. Under our model, zeroing all the activations in a layer at a speciﬁc
location, is equivalent to marginalizing over all the inputs for the receptive ﬁeld for that respective location.
We have used random marginalization layers in between all our layers during training, where the probability
for zeroing out activations was chosen through cross-validation for each layer separately. Though it might raise
concern that random marginalization layers could lead to biased results toward the missingness distributions
we have tested it on, in practice the addition of those layers only helped improve our results under cases where
only few pixels where missing.

Finally, we wish to discuss a few optimization tricks which had a minor effects compared to the above, but were
nevertheless very useful in achieving slightly better results. First, instead of optimizing directly the objective
deﬁned by eq. 2, we add smoothing parameter β between the two terms, as follows:

Θ∗ = argmin

−

log

Θ

|S|
(cid:88)

i=1

eNΘ(X(i);Y (i))
y=1 eNΘ(X(i);y)

(cid:80)K

|S|
(cid:88)

K
(cid:88)

− β

log

eNΘ(X(i);y)

i=1

y=1

setting β too low diminish the generative capabilities of our models, while setting it too high diminish the
discriminative performance. Through cross-validation, we decided on the value β = 0.01 for the models
trained on MNIST, while for NORB we have used a different value of β for each of the models, ranging in
{0.01, 0.1, 0.5, 1}. Second, we found that performance increased if we normalized activations before applying
the 1 × 1 MEX operations. Speciﬁcally, we calculate the soft-max over the channels for each spatial location

23

which we call the activation norm, and then subtract it from every respective activation. After applying the
MEX operation, we add back the activation norm. Though might not be obvious at ﬁrst, subtracting a constant
from the input of a MEX operation and adding it to its output is equivalent does not change the mathematical
operation. However, it does resolve the numerical issue of adding very large activations to very small offsets,
which might result in a loss of precision. Finally, we are applying our model in different translations of the input
and then average the class predictions. Since our model can marginalize over inputs, we do not need to crop
the original image, and instead mask the unknown parts after translation as missing. Applying a similar trick
to standard ConvNets on MNIST does not seem to improve their results. We believe this method is especially
ﬁt to our model, is because it does not have a natural treatment of overlapping patches like ConvNets do, and
because it is able to marginalize over missing pixels easily, not limiting it just to crop translation as is typically
done.

I.2 Description of Experiments

In this section we will give a detailed description of the protocol we have used during our experiments.

I.2.1 Binary Digit Classiﬁcation under Feature Deletion Missing Data

This experiment focuses on the binary classiﬁcation problem derived from MNIST, by limiting the number of
classes to two different digits at a time. We use the same non-zero feature deletion distribution as suggested by
Globerson and Roweis [17], i.e. for a given image we uniformly sample a set of N non-zero pixels from the
image (if the image has less than N non-zero pixels then they are non-zero pixels are chosen), and replace their
values with zeros. This type of missingness distribution falls under the MNAR type deﬁned in sec.3.

We test values of N in {0, 25, 50, 75, 100, 125, 150}. For a given value of N , we train a separate classiﬁer
on each digit pair classiﬁer on a randomly picked subset of the dataset containing 300 images per digit (600
total). During training we use a ﬁxed validation set with 1000 images per digit. After picking the best classiﬁer
according to the validation set, the classiﬁer is tested against a test set with a 1000 images per digits with a
randomly chosen missing values according to the value of N . This experiment is repeated 10 times for each
digit pair, each time using a different subset for the training set, and a new corrupted test set. After conducting
all the different experiments, all the accuracies are averaged for each value of N , which are reported in table 1.

I.2.2 Multi-class Digit Classiﬁcation under MAR Missing Data

This experiment focuses on the complete multi-class digit classiﬁcation of the MNIST dataset, in the presence
of missing data according to different missingness distributions. Under this setting, only the test set contains
missing values, whereas the training set does not. We test two kinds of missingness distributions, which both
fall under the MAR type deﬁned in sec.3. The ﬁrst kind, which we call i.i.d. corruption, each pixel is missing
with a ﬁxed probability p. the second kind, which we call missing rectangles corruption, The positions of N
rectangles of width W or chosen uniformly in the picture, where the rectangles can overlap one another. During
the training stage, the models to be tested are not to be biased toward the speciﬁc missingness distributions we
have chosen, and during the test stage, the same classiﬁer is tested against all types of missingness distributions,
and without supplying it with the parameters or type of the missingness distribution it is tested against. This
rule prevent the use of ConvNets trained on simulated missingness distributions. To demonstrate that the latter
lead to biased classiﬁers, we have conducted a separate experiment just for ConvNets, where the previous rule is
ignored, and we train a separate ConvNet classiﬁer on each type and parameter of the missingness distributions
we have used. We then tested each of those ConvNets on all other missingness distributions, the results of
which are in ﬁg. 3, which conﬁrmed our hypothesis.

24

8
1
0
2
 
r
a

M
 
5
2
 
 
]

G
L
.
s
c
[
 
 
5
v
7
6
1
4
0
.
0
1
6
1
:
v
i
X
r
a

Tensorial Mixture Models

Or Sharir
Department of Computer Science
The Hebrew University of Jerusalem
Israel
or.sharir@cs.huji.ac.il

Ronen Tamari
Department of Computer Science
The Hebrew University of Jerusalem
Israel
ronent@cs.huji.ac.il

Nadav Cohen
Department of Computer Science
The Hebrew University of Jerusalem
Israel
cohennadav@cs.huji.ac.il

Amnon Shashua
Department of Computer Science
The Hebrew University of Jerusalem
Israel
shashua@cs.huji.ac.il

Abstract

Casting neural networks in generative frameworks is a highly sought-after en-
deavor these days. Contemporary methods, such as Generative Adversarial Net-
works, capture some of the generative capabilities, but not all. In particular, they
lack the ability of tractable marginalization, and thus are not suitable for many
tasks. Other methods, based on arithmetic circuits and sum-product networks, do
allow tractable marginalization, but their performance is challenged by the need to
learn the structure of a circuit. Building on the tractability of arithmetic circuits,
we leverage concepts from tensor analysis, and derive a family of generative mod-
els we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolu-
tional network structure, and in addition, lend themselves to theoretical analyses
that allow comprehensive understanding of the relation between their structure and
their expressive properties. We thus obtain a generative model that is tractable on
one hand, and on the other hand, allows effective representation of rich distribu-
tions in an easily controlled manner. These two capabilities are brought together
in the task of classiﬁcation under missing data, where TMMs deliver state of the
art accuracies with seamless implementation and design.

1

Introduction

There have been many attempts in recent years to marry generative models with neural networks,
including successful methods, such as Generative Adversarial Networks [19], Variational Auto-
Encoders [24], NADE [38], and PixelRNN [39]. Though each of the above methods has demon-
strated its usefulness on some tasks, it is yet unclear if their advantage strictly lies in their generative
nature or some other attribute. More broadly, we ask if combining generative models with neural
networks could lead to methods who have a clear advantage over purely discriminative models.

|

On the most fundamental level, if X stands for an instance and Y for its class, generative mod-
els learn P(X, Y ), from which we can also infer P(Y
X), while discriminative models learn only
P(Y
X). It might not be immediately apparent if this sole difference leads to any advantage. In Ng
and Jordan [31], this question was studied with respect to the sample complexity, proving that under
some cases it can be signiﬁcantly lesser in favor of the generative classiﬁer. We wish to highlight
a more clear cut case, by examining the problem of classiﬁcation under missing data – where the
value of some of the entries of X are unknown at prediction time. Under these settings, discrimi-
native classiﬁers typically rely on some form of data imputation, i.e. ﬁlling missing values by some
auxiliary method prior to prediction. Generative classiﬁers, on the other hand, are naturally suited

|

to handle missing values through marginalization – effectively assessing every possible completion
of the missing values. Moreover, under mild assumptions, this method is optimal regardless of the
process by which values become missing (see sec. 3).

It is evident that such application of generative models assumes we can efﬁciently and exactly com-
pute P(X, Y ), a process known as tractable inference. Moreover, it assumes we may efﬁciently
marginalize over any subset of X, a procedure we refer to as tractable marginalization. Not all
generative models have both of these properties, and speciﬁcally not the ones mentioned in the be-
ginning of this section. Known models that do possess these properties, e.g. Latent Tree Model [30],
have other limitations. A detailed discussion can be found in sec. 4, but in broad terms, all known
generative models possess one of the following shortcomings: (i) they are insufﬁciently expressive
to model high-dimensional data (images, audio, etc.), (ii) they require explicitly designing all the
dependencies of the data, or (iii) they do not have tractable marginalization. Models based on neural
networks typically solve (i) and (ii), but are incapable of (iii), while more classical methods, e.g.
mixture models, solve (iii) but suffer from (i) and (ii).

There is a long history of specifying tractable generative models through arithmetic circuits and sum-
product networks [12, 34] – computational graphs comprised solely of product and weighted sum
nodes. To address the shortcomings above, we take a similar approach, but go one step further and
leverage tensor analysis to distill it to a speciﬁc family of models we call Tensorial Mixture Models.
A Tensorial Mixture Model assumes a convolutional network structure, but as opposed to previous
methods tying generative models with neural networks, lends itself to theoretical analyses that allow
a thorough understanding of the relation between its structure and its expressive properties. We
thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective
representation of rich distributions in an easily controlled manner.

2 Tensorial Mixture Models

(cid:80)

M
d=1

P(d)P(x

One of the simplest types of tractable generative models are mixture models, where the probability
distribution is deﬁned the convex combination of M mixing components (e.g. Normal distribu-
tions): P(x) =
d; θd). Mixture models are very easy to learn, and many of them
|
are able to approximate any probability distribution, given sufﬁcient number of components, ren-
dering them suitable for a variety of tasks. The disadvantage of classic mixture models is that they
do not scale will to high dimensional data (“curse of dimensionality”). To address this challenge,
we extend mixture models, leveraging the fact many high dimensional domains (e.g. images) are
typically comprised of small, simple local structures. We represent a high dimensional instance as
Rs (called lo-
X = (x1, . . . , xN ) – an N -length sequence of s-dimensional vectors x1, . . . , xN ∈
cal structures). X is typically thought of as an image, where each local structure xi corresponds to a
local patch from that image, where no two patches are overlapping. We assume that the distribution
of individual local structures can be efﬁciently modeled by some mixture model of few components,
which for natural image patches, was shown to be the case [42]. Formally, for all i
[N ] there
di; θdi), where di is a hidden variable specifying the matching
exists di ∈
component for the i-th local structure. The probability density of sampling X is thus described by:

[M ] such that xi ∼

P (x
|

∈

P (X) =

P (d1, . . . , dN )

M

d1,...,dN =1

N

i=1

P (xi|

di; θdi)

(cid:88)

(cid:89)

(1)

where P (d1, . . . , dN ) represents the prior probability of assigning components d1, . . . , dN to their
respective local structures x1, . . . , xN . As with classical mixture models, any probability density
function P(X) could be approximated arbitrarily well by eq. 1, as M
At ﬁrst glance, eq. 1 seems to be impractical, having an exponential number of terms. In the lit-
erature, this equation is known as the “Network Polynomial” [12], and the traditional method to
overcome its intractability is to express P (d1, . . . , dN ) by an arithmetic circuit, or sum-product
networks, following certain constraints (decomposable and complete). We augment this method
by viewing P (d1, . . . , dN ) from an algebraic perspective, treating it as a tensor of order N and
Ad1,...,dN speciﬁed by N indices
dimension M in each mode, i.e., as a multi-dimensional array,
d1, . . . , dN , each ranging in [M ], where [M ]
P (d1, . . . , dN )
≡{
as the prior tensor. Under this perspective, eq. 1 can be thought of as a mixture model with tensorial
mixing weights, thus we call the arising models Tensorial Mixture Models, or TMMs for short.

Ad1,...,dN ≡

. We refer to

(see app. A).

1, . . . , M

→ ∞

}

2

Figure 1: A generative variant of Convolutional Arithmetic Circuits.

2.1 Tensor Factorization, Tractability, and Convolutional Arithmetic Circuits

Not only is it intractable to compute eq. 1, but it is also impossible to even store the prior tensor.
We argue that addressing the latter is intrinsically tied to addressing the former. For example, if
we impose a sparsity constraint on the prior tensor, then we only need to compute the few non-
zero terms of eq. 1. TMMs with sparsity constraints can represent common generative models,
e.g. GMMs (see app. B). However, they do not take full advantage of the prior tensor. Instead, we
consider constraining TMMs with prior tensors that adhere to non-negative low-rank factorizations.

= v(1)

takes a rank-1 form, i.e. there
We begin by examining the simplest case, where the prior tensor
N
exist vectors v(1), . . . , v(N )
, or in tensor product nota-
d = P (di=d) as a probability over di, and so
tion,
i P (di), then it reveals that imposing a rank-1 constraint is actually equivalent
P (d1, . . . , dN ) =
to assuming the hidden variables d1, . . . , dN are statistically independent. Applying it to eq. 1 re-
sults in the tractable form P (X) =
di, θdi), or in other words, a product
of mixture models. Despite the familiar setting, this strict assumption severely limits expressivity.

RM such that
v(N ). If we interpret1 v(i)

d=1 P (di=d)P (xi|

Ad1,...,dN =

A
i=1 v(i)

⊗ · · · ⊗

N
i=1

(cid:81)

(cid:81)

A

∈

M

di

In a broader setting, we look at general factorization schemes that given sufﬁcient resources could
represent any tensor. Namely, the CANDECOMP/PARAFAC (CP) and the Hierarchical Tucker (HT)
factorizations. The CP factorization is simply a sum of rank-1 tensors, extending the previous case,
and HT factorization can be seen as a recursive application of CP (see def. in app. C). Since both
factorization schemes are solely based on product and weighted sum operations, they could be re-
alized through arithmetic circuits. As shown by Cohen et al. [9], this gives rise to a speciﬁc class
of convolutional networks named Convolutional Arithmetic Circuits (ConvACs), which consist of
1-convolutions, non-overlapping product pooling layers, and linear activations. More speciﬁ-
1
×
cally, the CP factorization corresponds to shallow ConvACs, HT corresponds to deep ConvACs, and
the number of channels in each layer corresponds to the respective concept of “rank” in each factor-
ization scheme. In general, when a tensor factorization is applied to eq. 1, inference is equivalent to
M,N
d=1,i=1, in what we call the
ﬁrst computing the likelihoods of all mixing components
representation layer, followed by a ConvAC. A complete network is illustrated in ﬁg. 1.

P (xi|
{

d; θd)
}

(cid:81)

(cid:80)

When restricting the prior tensor of eq. 1 to a factorization, we must
ensure it represents actual probabilities, i.e. it is non-negative and
its entries sum to one. This can be addressed through a restriction
to non-negative factorizations, which translates to limiting the pa-
rameters of each convolutional kernel to the simplex. There is a
vast literature on the relations between non-negative factorizations
and generative models [21, 30]. As opposed to most of these works,
we apply factorizations merely to derive our model and analyze its
expressivity – not for learning its parameters (see sec. 2.3).

From a generative perspective, the restriction of convolutional ker-
nels to the simplex results in a latent tree graphical model, as illus-
trated in ﬁg. 2. Each hidden layer in the ConvAC network – a pair
of convolution and pooling operations, corresponds to a transition
between two levels in the tree. More speciﬁcally, each level is com-
prised of multiple latent variables, one for each spatial position in
the input to a hidden layer in the network. Each latent variable in
the input to the l-th layer takes values in [rl

Figure 2: Graphical model
description of HT-TMM

1A represents a probability, and w.l.o.g. we can assume all entries of v(i) are non-negative and (cid:80)M

d=1 v(i)

d =1

1] – the number of channels in the layer that precedes it.

−

3

Pooling operations in the network correspond to the parent-child relationships in the tree – a set of
latent variables are siblings with a shared parent in the tree, if they are positioned in the same pooling
window in the network. The weights of convolution operations correspond to the transition matrix
between a parent and each of its children, i.e. if Hp is the parent latent variable, taking values in
Hp=c)=w(c)
1], then P (Hchild=i
[rl], and Hchild is one of its child variables, taking values in [rl
,
|
where w(c) is the 1
1 convolutional kernel for the c-th output channel. With the above graphical
representation in place, we can easily draw samples from our model.

×

−

i

To conclude this subsection, by leveraging an algebraic perspective of the network polynomial
(eq. 1), we show that tractability is related to the tensor properties of the priors, and in particular,
that low rank factorizations are equivalent to inference via ConvACs. The application of arithmetic
circuits to achieve tractability is by itself not a novelty. However, the particular convolutional arith-
metic circuits we propose lead to a comprehensive understanding of representational abilities, and
as a result, to a straightforward architectural design of TMMs.

2.2 Controlling the Expressivity and Inductive Bias of TMMs

As discussed in sec. 1, it is not enough for a generative model to be tractable – it must also be
sufﬁciently expressive, and moreover, we must also be able to understand how its structure affects
its expressivity. In this section we explain how our algebraic perspective enables us to achieve this.

To begin with, since we derived our model by factorizing the prior tensor, it immediately follows
that given sufﬁcient number of channels in the ConvAC, i.e. given sufﬁcient ranks in the tensor fac-
torization, any distribution could be approximated arbitrarily well (assuming M is allowed to grow).
In short, this amounts to saying that TMMs are universal. Though many other generative models are
known to be universal, it is typically not clear how one may assess what a given structure of ﬁnite
size can and cannot express. In contrast, the expressivity of ConvACs has been throughly studied in
a series of works [9, 8, 11, 27], each of which examined a different attribute of its structure. In Cohen
et al. [9] it was proven that ConvACs exhibit the Depth Efﬁciency property, i.e. deep networks are
exponentially more expressive than shallow ones. In Cohen and Shashua [8] it was shown that deep
networks can efﬁciently model some input correlations but not all, and that by designing appropriate
pooling schemes, different preferences may be encoded, i.e. the inductive bias may be controlled. In
Cohen et al. [11] this result was extended to more complex connectivity patterns, involving mixtures
of pooling schemes. Finally, in Levine et al. [27], an exact relation between the number of channels
and the correlations supported by a network has been found, enabling tight control over expressiv-
ity and inductive bias. All of these results are brought forth by the relations of ConvACs to tensor
factorizations. They allow TMMs to be analyzed and designed in much more principled ways than
alternative high-dimensional generative models.2

2.3 Classiﬁcation and Learning

|

∈

Y =y) for each y

TMMs realized through ConvACs, sharing many of the same traits as ConvNets, are especially suit-
able to serve as classiﬁers. We begin by introducing a class variable Y , and model the conditional
likelihood P(X
[K]. Though it is possible to have separate generative models for
each class, it is much more efﬁcient to leverage the relation to ConvNets and use a shared ConvAC
instead, which is equivalent to a joint-factorization of the prior tensors for all classes. This results
in a single network, where instead of a single scalar output representing P(X), multiple outputs are
driven by the network, representing P(X
Y =y) for each class y. Predicting the class of a given
|
instance is carried through Maximum A-Posteriori, i.e. by returning the most likely class. In the
common setting of uniform class priors, i.e. P(Y =y)
1
K , this corresponds to classiﬁcation by max-
imal network output, as customary with ConvNets. We note that in practice, na¨ıve implementation of
ConvACs is not numerically stable3, and this is treated by performing all computations in log-space,
which transforms ConvACs into SimNets – a recently introduced deep learning architecture [7, 10].

≡

S
Suppose now that we are given a training set S =
|i=1 of instances and
[K])
|
}
labels, and would like to ﬁt the parameters Θ of our model according to the Maximum Likelihood

(Rs)N , Y (i)

(X (i)
{

∈

∈

2 As a demonstration of the fact that ConvAC analyses are not affected by the non-negativity and normal-
ization restrictions of our generative variant, we prove in app. D that the Depth Efﬁciency property still holds.
3Since high degree polynomials (as computed by ACs) are susceptible to numerical underﬂow or overﬂow.

4

−

principle, or equivalently, by minimizing the Negative Log-Likelihood (NLL) loss function:
E[

log PΘ(X, Y )]. The latter can be factorized into two separate loss terms:

L

(Θ) =

(Θ) = E[

log PΘ(Y

X)] + E[
|

log PΘ(X)]

L

−

−

−
where E[
log PΘ(Y
X)], which we refer to as the discriminative loss, is commonly known as
|
log PΘ(X)], which corresponds to maximizing the prior likelihood
the cross-entropy loss, and E[
P(X), has no analogy in standard discriminative classiﬁcation.
It is this term that captures the
generative nature of the model, and we accordingly refer to it as the generative loss. Now, let
NΘ(X (i); y):= log PΘ(X (i)
Y =y) stand for the y’th output of the SimNet (ConvAC in log-space)
|
realizing our model with parameters Θ. In the case of uniform class priors (P(Y =y)
1/K), the
empirical estimation of

(Θ) may be written as:

≡

−

L

−

log

S
|
|
i=1

1
S
|

(Θ; S) =

eNΘ(X (i);Y (i))
K
y=1 eNΘ(X (i);y) −
This objective includes the standard softmax loss as its ﬁrst term, and an additional generative loss
as its second. Rather than employing dedicated Maximum Likelihood methods for training (e.g. Ex-
pectation Minimization), we leverage once more the resemblance between our networks and Con-
vNets, and optimize the above objective using Stochastic Gradient Descent (SGD).

eNΘ(X (i);y)

| (cid:88)

| (cid:88)

S
|
|
i=1

1
S

(cid:88)

log

(2)

(cid:80)

y=1

K

|

L

3 Classiﬁcation under Missing Data through Marginalization

A major advantage of generative models over discriminative ones lies in their ability to cope with
missing data, speciﬁcally in the context of classiﬁcation. By and large, discriminative methods ei-
ther attempt to complete missing parts of the data before classiﬁcation (a process known as data
imputation), or learn directly to classify data with missing values [28]. The ﬁrst of these approaches
relies on the quality of data completion, a much more difﬁcult task than the original one of classiﬁ-
cation under missing data. Even if the completion was optimal, the resulting classiﬁer is known to
be sub-optimal (see app. E). The second approach does not rely on data completion, but nonetheless
assumes that the distribution of missing values at train and test times are similar, a condition which
often does not hold in practice. Indeed, Globerson and Roweis [17] coined the term “nightmare at
test time” to refer to the common situation where a classiﬁer must cope with missing data whose
distribution is different from that encountered in training.

As opposed to discriminative methods, generative models are endowed with a natural mechanism for
classiﬁcation under missing data. Namely, a generative model can simply marginalize over missing
values, effectively classifying under all possible completions, weighing each completion according
to its probability. This, however, requires tractable inference and marginalization. We have already
shown in sec. 2 that TMMs support the former, and will show in sec. F that marginalization can be
just as efﬁcient. Beforehand, we lay out the formulation of classiﬁcation under missing data.

,

X

Y
) the joint distribution of (

D
is drawn conditioned on

be a random vector in Rs representing an object, and let
(
X

Let
be a random variable in [K]
representing its label. Denote by
[K])
,
∈
speciﬁc realizations thereof. Assume that after sampling a speciﬁc instance (x, y), a random binary
s
vector
0, 1
}
=x). xi is considered missing if mi is equal
(realization of
(
·|X
Q
m, whose i’th coordinate is
to zero, and observed otherwise. Formally, we consider the vector x
deﬁned to hold xi if mi=1, and the wildcard
if mi=0. The classiﬁcation task is then to predict y
given access solely to x

=x. More concretely, we sample a binary mask m

) according to a distribution

), and by (x

Rs, y

m.

M

M

∈{

(cid:12)

X

X

Y

Y

∈

∗

X

M

=m
=m

(
M
Q
(
M
Q

|X
|X
is independent of the missing values in

Following the works of Rubin [36], Little and Rubin [28], we consider three cases for the miss-
=x): missing completely at random (MCAR), where
is inde-
ingness distribution
=x) is a function of m but not of x; missing at random (MAR),
pendent of
, i.e.
where
=x) is a function of both m
=m
and x, but is not affected by changes in xi if mi=0; and missing not at random (MNAR), covering
the rest of the distributions for which
=x) is a
function of both m and x, which at least sometimes is sensitive to changes in xi when mi=0.
Let P be the joint distribution of the object
P(

|X
depends on missing values in

, and missingness mask

X
=m) =

(
M
Q

Y
=x,

:
M

, label

(
Q

, i.e.

=m

, i.e.

=x)

=m

=y)

=x,

=y,

M

M

M

|X

X

X

(
X

D

Y

(
M

· Q

|X

X

Y

M

(cid:12)

5

Rs and m

∈

∈{

0, 1
}

s, denote by o(x, m) the event where the random vector

coincides
For given x
with x on the coordinates i for which mi=1. For example, if m is an all-zero vector, o(x, m) covers
the entire probability space, and if m is an all-one vector, o(x, m) corresponds to the event
=x.
X
With these notations in hand, we are now ready to characterize the optimal predictor in the presence
of missing data. The proofs are common knowledge, but provided in app. E for completeness.
Claim 1. For any data distribution
rule in terms of 0-1 loss is given by predicting the class y
P(

, the optimal classiﬁcation
=y

Q
[K], that maximizes P(

and missingness distribution

=y), for an instance x

o(x, m))
|

o(x, m),

m.

D

X

Y

∈

·

M

=m
|
Y
When the distribution
to as the marginalized Bayes predictor:
Corollary 1. Under the conditions of claim 1, if the distribution
classiﬁcation rule may be written as:

Q

(cid:12)

Q

is MAR (or MCAR), the optimal classiﬁer admits a simpler form, referred

is MAR (or MCAR), the optimal

h∗(x

m) = argmaxy

(cid:12)

P(

Y

|

=y

o(x, m))

(3)

Corollary 1 indicates that in the MAR setting, which is frequently encountered in practice, optimal
classiﬁcation does not require prior knowledge regarding the missingness distribution
. As long
as one is able to realize the marginalized Bayes predictor (eq. 3), or equivalently, to compute the
likelihoods of observed values conditioned on labels (P(o(x, m)
Y =y)), classiﬁcation under miss-
|
ing data is guaranteed to be optimal, regardless of the corruption process taking place. This is in
stark contrast to discriminative methods, which require access to the missingness distribution during
training, and thus are not able to cope with unknown conditions at test time.

Q

Most of this section dealt with the task of prediction given an input with missing data, where we
assumed we had access to a “clean” training set, and only faced missingness during prediction.
However, many times we wish to tackle the reverse task, where the training set itself is riddled with
missing data. Tractability leads to an advantage here as well: under the MAR assumption, learning
from missing data with the marginalized likelihood objective results in an unbiased classiﬁer [28].

In the case of TMMs, marginalizing over missing values is just as efﬁcient as plain inference – re-
quires only a single pass through the corresponding network. The exact mechanism is carried out in
similar fashion as in sum-product networks, and is covered in app. F. Accordingly, the marginalized
Bayes predictor (eq. 3) is realized efﬁciently, and classiﬁcation under missing data (in the MAR
setting) is optimal (under generative assumption), regardless of the missingness distribution.

4 Related Works

There are many generative models realized through neural networks, and convolutional networks
in particular, e.g. Generative Adversarial Networks [19], Variational Auto-Encoders [24], and
NADE [38]. However, most do not posses tractable inference, and of the few that do, non posses
tractable marginalization over any set of variables. Due to limits of space, we defer the discussion
on the above to app. G, and in the remainder of this section focus instead on the most relevant works.

As mentioned in sec. 2, we build on the approach of specifying generative models through Arith-
metic Circuits (ACs) [12], and speciﬁcally, our model is a strict subclass of the well-known Sum-
Product Networks (SPNs) [34], under the decomposable and complete restrictions. Where our work
differs is in our algebraic approach to eq. 1, which gives rise to a speciﬁc structure of ACs, called
ConvACs, and a deep theory regarding their expressivity and inductive bias (see sec. 2.2). In contrast
to the structure we proposed, the current literature on general SPNs does not prescribe any speciﬁc
structures, and its theory is limited to either very speciﬁc instances [14], or very broad classes, e.g
ﬁxed-depth circuits [29]. In the early works on SPNs, specialized networks of complex structure
were designed for each task based mainly on heuristics, often bearing little resemblance to common
neural networks. Contemporary works have since moved on to focus mainly on learning the struc-
ture of SPNs directly from data [33, 16, 1, 35], leading to improved results in many domains. Despite
that, only few published studies have applied this method to natural domains (images, audio, etc.),
on which only limited performance, compared to other common methods, was reported, speciﬁcally
on the MNIST dataset [1]. The above suggests that choosing the right architecture of general SPNs,
at least on some domains, remains to be an unsolved problem. In addition, both the previously
studied manually-designed SPNs, as well as ones with a learned structure, lead to models, which

6

n=

0

25

50

75

100

125

150

96.4
97.8
Table 1: Prediction for each two-class task of MNIST digits, under feature deletion noise.

97.9
HT-TMM 98.5

89.2
93.9

97.5
98.2

94.1
96.5

80.9
87.1

70.2
76.3

LP

ptest

0.25

0.50

0.75

0.90

0.95

0.99

ptrain
0.25
0.50
0.75
0.90
0.95
0.99

i.i.d. (rand)
rects (rand)

98.9
99.1
98.9
97.6
95.7
87.3

98.7
98.2

97.8
98.6
98.7
97.5
95.6
86.7

98.4
95.7

78.9
94.6
97.2
96.7
94.8
85.0

97.0
83.2

32.4
68.1
83.9
89.0
88.3
78.2

87.6
54.7

17.6
37.9
56.4
71.0
74.0
66.2

70.6
35.8

11.0
12.9
16.7
21.3
30.5
31.3

29.6
17.5

(a) MNIST with i.i.d. corruption

(b) MNIST with missing rectangles.

Figure 3: We examine ConvNets trained on one missingness distribution while tested on others.
“(rand)” denotes training on distributions with randomized parameters. (a) i.i.d. corruption: trained
with probability ptrain and tested on ptest. (b) missing rectangles: training on randomized distributions
(rand) compared to training on the same (ﬁxed) missing rectangles distribution.

according to recent works on GPU-optimized algorithms [4], cannot be efﬁciently implemented due
to their irregular memory access patterns. This is in stark contrast to our model, which leverages
the same patterns as modern ConvNets, and thus enjoys similar run-time performance. An addi-
tional difference in our work is that we manage to successfully train our model using standard SGD.
Even though this approach has already been considered by Poon and Domingos [34], they deemed
it lacking and advocated for specialized optimization algorithms instead.

Outside the realm of generative networks, tractable graphical models, e.g. Latent Tree Mod-
els (LTMs) [30], are the most common method for tractable inference. Similar to SPNs, it is not
straightforward to ﬁnd the proper structure of graphical models for a particular problem, and most
of the same arguments apply here as well. Nevertheless, it is noteworthy that recent progress in
structure and parameters learning of LTMs [22, 3] was also brought forth by connections to tensor
factorizations, similar to our approach. Unlike the aforementioned algorithms, we utilize tensor
factorizations solely for deriving our model and analyzing its expressivity, while leaving learning
to SGD – the most successful method for training neural networks. Leveraging their perspective to
analyze the optimization properties of our model is viewed as a promising avenue for future research.

5 Experiments

We demonstrate the properties of TMMs through both qualitative and quantitative experiments. In
sec. 5.1 we present state of the art results on image classiﬁcation under missing data, with robustness
to various missingness distributions. In sec. 5.2 we show that our results are not limited to images, by
applying TMMs for speech recognition. Finally, in app. H we show visualizations of samples drawn
from TMMs, shedding light on their generative nature. Our implementation, based on Caffe [23] and
MAPS [4] (toolbox for efﬁcient GPU code generation), as well as all other code for reproducing our
experiments, are available at: https://github.com/HUJI-Deep/Generative-ConvACs. Ex-
tended details regarding the experiments are provided in app. I.

5.1

Image Classiﬁcation under Missing Data

In this section we experiment on two datasets: MNIST [25] for digit classiﬁcation, and small
NORB [26] for 3D object recognition. In our results, we refer to models using shallow networks as
CP-TMM, and to those using deep networks as HT-TMM, in accordance with the respective tensor
factorizations (see sec. 2). The theory discussed in sec. 2.2 guided our exact choice of architectures.
Namely, we used the fact [27] that the capacity to model either short- or long-range correlations in
the input, is related to the number of channels in the beginning or end of a network, respectively. In
MNIST, discriminating between digits has more to do with long-range correlations than the basic
strokes digits are made of, hence we chose to start with few channels and end with many – layer

7

(a) MNIST with i.i.d. corruption.

(b) MNIST with missing rectangles.

(c) NORB with i.i.d. corruption.

(d) NORB with missing rectangles.

Figure 4: Blind classiﬁcation under missing data. (a,c) Testing i.i.d. corruption with probability p
for each pixel. (b,d) Testing missing rectangles corruption with n missing rectangles, each of width
and hight equal to W . (*) Based on the published results [18]. (
†

) Data imputation algorithms.

widths were set to 64-128-256-512. In contrast, the classes of NORB differ in much ﬁner details,
requiring more channels in the ﬁrst layers, hence layer widths were set to 256-256-256-512. In both
cases, M = 32 Gaussian mixing components were used.

We begin by comparing our generative approach to missing data against classical methods, namely,
methods based on Globerson and Roweis [17]. They regard missing data as “feature deletion” noise,
replace missing entries by zeros, and devise a learning algorithm over linear predictors that takes
the number of missing features, n, into account. The method was later improved by Dekel and
Shamir [13]. We compare TMMs to the latter, with n non-zero pixels randomly chosen and changed
to zero, in the two-class prediction task derived from each pair of MNIST digits. Due to limits of
their implementation, only 300 images per digit are used for training. Despite this, and the fact that
the evaluated scenario is of the MNAR type (on which optimality is not guaranteed – see sec. 3),
we achieve signiﬁcantly better results (see table 1), and unlike their method, which requires several
classiﬁers and knowing n, we use a single TMM with no prior knowledge.

∈

Heading on to multi-class prediction under missing data, we focus on the challenging “blind” setting,
where the missingness distribution at test time is completely unknown during training. We simulate
[0, 1] of
two kinds of MAR missingness distributions: (i) an i.i.d. mask with a ﬁxed probability p
dropping each pixel, and (ii) a mask composed of the union of n (possibly overlapping) rectangles
of width and height W , each positioned randomly in the image (uniform distribution). We ﬁrst
demonstrate that purely discriminative classiﬁers cannot generalize to all missingness distributions,
by training the standard LeNeT ConvNet [25] on one set of distributions and then testing it on others
(see ﬁg. 3). Next, we present our main results. We compare our model against three different
approaches. First, as a baseline, we use K-Nearest Neighbors (KNN) to vote on the most likely
class, augmented with an l2-metric that disregards missing coordinates. KNN actually scores better
than most methods, but its missingness-aware distance metric prevents the common memory and
runtime optimizations, making it impractical for real-world settings. Second, we test various data-
imputation methods, ranging from simply ﬁlling missing pixels with zeros or their mean, to modern
generative models suited to inpainting. Data imputation is followed by a ConvNet prediction on
the completed image.
In general, we ﬁnd that this approach only works well when few pixels
are missing. Finally, we test generative classiﬁers other than our model, including MP-DBM and
SPN (sum-product networks). MP-DBM is notable for being limited to approximations, and its
results show the importance of using exact inference instead. For SPN, we have augmented the
model from Poon and Domingos [34] with a class variable Y , and trained it to maximize the joint
probability P (X, Y ) using the code of Zhao et al. [41]. The inferior performance of SPN suggests

8

that the structure of TMMs, which are in fact a special case, is advantageous. Due to limitations of
available public code and time, not all methods were tested on all datasets and distributions. See
ﬁg. 4 for the complete results.

To conclude, TMMs signiﬁcantly outperform all other methods tested on image classiﬁcation with
missing data. Although they are a special case of SPNs, their particular structure appears to be
more effective than ones existing in the literature. We attribute this superiority to the fact that their
architectural design is backed by comprehensive theoretical studies (see sec. 2.2).

5.2 Speech Recognition under Missing Data

To demonstrate the versatility of TMMs, we also conducted limited experiments on the TIMIT
speech recognition dataset, following the same protocols as in sec. 5.1. We trained a TMM and a
standard ConvNet on 256ms windows of raw data at 16Hz sample rate to predict the phoneme at the
center of a window. Both the TMM and the ConvNet reached 78% accuracy on the clean dataset,
but when half of the audio is missing i.i.d., accuracy of the ConvNet with mean imputation drops
to 34%, while the TMM remains at 63%. Utilizing common audio inpainting methods [2] only
improves accuracy of the ConvNet to 48%, well below that of TMM.

6 Summary

This paper focuses on generative models which admit tractable inference and marginalization, ca-
pabilities that lie outside the realm of contemporary neural network-based generative methods. We
build on prior works on tractable models based on arithmetic circuits and sum-product networks,
and leverage concepts from tensor analysis to derive a sub-class of models we call Tensorial Mixture
Models (TMMs). In contrast to existing methods, our algebraic approach leads to a comprehensive
understanding of the relation between model structure and representational properties. In practice,
utilizing this understanding for the design of TMMs has led to state of the art performance in clas-
siﬁcation under missing data. We are currently investigating several avenues for future research,
including semi-supervised learning, and examining more intricate ConvAC architectures, such as
the ones suggested by Cohen et al. [11]).

This work is supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12 and
by the European Research Council (TheoryDL project). Nadav Cohen is supported by a Google
Fellowship in Machine Learning.

Acknowledgments

References

[1] Tameem Adel, David Balduzzi, and Ali Ghodsi. Learning the Structure of Sum-Product Networks via an

SVD-based Algorithm. UAI, 2015.

[2] A Adler, V Emiya, M G Jafari, and M Elad. Audio inpainting.

IEEE Trans. on Audio, Speech and

Language Processing, 20:922–932, March 2012.

[3] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research (), 15(1):2773–2832,
2014.

[4] Tal Ben-Nun, Ely Levy, Amnon Barak, and Eri Rubin. Memory Access Patterns: The Missing Piece of
the Multi-GPU Puzzle. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, pages 19:1–19:12. ACM, 2015.

[5] Yoshua Bengio, ´Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep Generative Stochas-

tic Networks Trainable by Backprop. In International Conference on Machine Learning, 2014.

[6] Richard Caron and Tim Traynor. The Zero Set of a Polynomial. WSMR Report 05-02, 2005.

[7] Nadav Cohen and Amnon Shashua. SimNets: A Generalization of Convolutional Networks. In Advances

in Neural Information Processing Systems NIPS, Deep Learning Workshop, 2014.

9

[8] Nadav Cohen and Amnon Shashua.

Inductive Bias of Deep Convolutional Networks through Pooling

Geometry. In International Conference on Learning Representations ICLR, April 2017.

[9] Nadav Cohen, Or Sharir, and Amnon Shashua. On the Expressive Power of Deep Learning: A Tensor

Analysis. In Conference on Learning Theory COLT, May 2016.

[10] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep SimNets. In Computer Vision and Pattern Recogni-

tion CVPR, May 2016.

[11] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting Dilated Convolutional Networks with

Mixed Tensor Decompositions. arXiv.org, 2017.

[12] Adnan Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM (JACM),

50(3):280–305, May 2003.

[13] Ofer Dekel and Ohad Shamir. Learning to classify with missing and corrupted features. In International

Conference on Machine Learning. ACM, 2008.

[14] Olivier Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks. Advances in Neural

Information Processing Systems, pages 666–674, 2011.

[15] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estima-

tion. arXiv.org, October 2014.

on Machine Learning, 2013.

[16] R Gens and P M Domingos. Learning the Structure of Sum-Product Networks. Internation Conference

[17] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Inter-

national Conference on Machine Learning. ACM, 2006.

[18] Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-Prediction Deep Boltzmann

Machines. Advances in Neural Information Processing Systems, 2013.

[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. Advances in Neural Information Processing
Systems, 2014.

[20] W Hackbusch and S K¨uhn. A New Scheme for the Tensor Representation. Journal of Fourier Analysis

and Applications, 15(5):706–722, 2009.

[21] Thomas Hofmann. Probabilistic latent semantic analysis. Morgan Kaufmann Publishers Inc., July 1999.

[22] Furong Huang, Niranjan U N, Ioakeim Perros, Robert Chen, Jimeng Sun, and Anima Anandkumar. Scal-
able Latent Tree Model and its Application to Health Analytics. In NIPS Machine Learning for Healthcare
Workshop, 2015.

[23] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional Architecture for Fast Feature Embedding. CoRR
abs/1202.2745, cs.CV, 2014.

[24] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on

Learning Representations, 2014.

[25] Yan LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-

ment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[26] Yann LeCun, Fu Jie Huang, and L´eon Bottou. Learning Methods for Generic Object Recognition with

Invariance to Pose and Lighting. Computer Vision and Pattern Recognition, 2004.

[27] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep Learning and Quantum Entangle-

ment: Fundamental Connections with Implications to Network Design. arXiv.org, April 2017.

[28] Roderick J A Little and Donald B Rubin. Statistical analysis with missing data (2nd edition). John Wiley

& Sons, Inc., September 2002.

CoRR abs/1202.2745, cs.LG, 2014.

[29] James Martens and Venkatesh Medabalimi. On the Expressive Efﬁciency of Sum Product Networks.

[30] Rapha¨el Mourad, Christine Sinoquet, Nevin Lianwen Zhang, Tengfei Liu, and Philippe Leray. A Survey

on Latent Tree Models and Applications. J. Artif. Intell. Res. (), cs.LG:157–203, 2013.

10

[31] Andrew Y Ng and Michael I Jordan. On Discriminative vs. Generative Classiﬁers: A comparison of
logistic regression and naive Bayes. In Advances in Neural Information Processing Systems NIPS, Deep
Learning Workshop, 2002.

[32] F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, and E Duchesnay.
Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research (), 12:2825–2830,
2011.

[33] Robert Peharz, Bernhard C Geiger, and Franz Pernkopf. Greedy Part-Wise Learning of Sum-Product
Networks. In Machine Learning and Knowledge Discovery in Databases, pages 612–627. Springer Berlin
Heidelberg, Berlin, Heidelberg, September 2013.

[34] Hoifung Poon and Pedro Domingos. Sum-Product Networks: A New Deep Architecture. In Uncertainty

in Artiﬁcail Intelligence, 2011.

Variable Interactions. ICML, 2014.

[35] Amirmohammad Rooshenas and Daniel Lowd. Learning Sum-Product Networks with Direct and Indirect

[36] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, December 1976.

[37] Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. Internation Conference on Machine Learning, 2015.

[38] Benigno Uria, Marc-Alexandre C ˆo t ´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural Autore-
gressive Distribution Estimation. Journal of Machine Learning Research (), 17(205):1–37, 2016.

[39] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In

International Conference on Machine Learning, 2016.

[40] Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In European

Conference on Computer Vision. Springer International Publishing, 2014.

[41] Han Zhao, Pascal Poupart, and Geoff Gordon. A Uniﬁed Approach for Learning the Parameters of
Sum-Product Networks. In Advances in Neural Information Processing Systems NIPS, Deep Learning
Workshop, 2016.

[42] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.

ICCV, pages 479–486, 2011.

11

A The Universality of Tensorial Mixture Models

In this section we prove the universality property of Generative ConvACs, as discussed in sec. 2. We begin by
taking note from functional analysis and deﬁne a new property called PDF total set, which is similar in concept
to a total set, followed by proving that this property is invariant under the cartesian product of functions, which
entails the universality of these models as a corollary.
Deﬁnition 1. Let F be a set of PDFs over Rs. F is PDF total iff for any PDF h(x) over Rs and for all (cid:15) > 0
there exists M ∈ N, {f1(x), . . . , fM (x)} ⊂ F and w ∈ (cid:52)M −1 s.t.
< (cid:15). In
other words, a set is a PDF total set if its convex span is a dense set under L1 norm.
Claim 2. Let F be a set of PDFs over Rs and let F ⊗N = {(cid:81)N
the product space (Rs)N . If F is a PDF total set then F ⊗N is PDF total set.

i=1 fi(x)|∀i, fi(x) ∈ F} be a set of PDFs over

(cid:13)
(cid:13)h(x) − (cid:80)M
(cid:13)

i=1 wifi(x)

(cid:13)
(cid:13)
(cid:13)1

Proof. If F is the set of Gaussian PDFs over Rs with diagonal covariance matrices, which is known to be a
PDF total set, then F ⊗N is the set of Gaussian PDFs over (Rs)N with diagonal covariance matrices and the
claim is trivially true.
Otherwise, let h(x1, . . . , xN ) be a PDF over (Rs)N and let (cid:15) > 0. From the above, there exists K ∈ N,
w ∈ (cid:52)M1−1 and a set of diagonal Gaussians {gij(x)}i∈[M1],j∈[N ] s.t.
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gij(xj)

g(x) −

M1(cid:88)

N
(cid:89)

(cid:15)
2

(4)

wi

<

j=1

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

Additionally, since F is a PDF total set then there exists M2 ∈ N, {fk(x)}k∈[M2] ⊂ F and {wij ∈
(cid:13)
(cid:13)gij(x) − (cid:80)M2
(cid:52)M2−1}i∈[M1],j∈[N ] s.t.
(cid:13)
2N ,
from which it is trivially proven using a telescopic sum and the triangle inequality that:

for all i ∈ [M1], j ∈ [N ] it holds that

(cid:13)
(cid:13)
k=1 wijkfk(x)
(cid:13)1

< (cid:15)

i=1
From eq. 4, eq. 5 the triangle inequality it holds that:

j=1

i=1

j=1

k=1

N
(cid:89)

M1(cid:88)

N
(cid:89)

M2(cid:88)

wi

gij(x) −

wi

wijkfk(xj)

<

(5)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

(cid:15)
2

g(x) −

Ak1,...,kN

fkj (xj)

< (cid:15)

M2(cid:88)

k1,...,kN =1

N
(cid:89)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

where Ak1,...,kN = (cid:80)M1
{(cid:81)N

j=1 fkj (xj)}k1∈[M2],...,kN ∈[M2] ⊂ F ⊗N and w = vec(A) completes the proof.

(cid:81)N

j=1 wijkj which holds (cid:80)M2

k1,...,kN =1 Ak1,...,kN = 1. Taking M = M N
2 ,

Corollary 2. Let F be a PDF total set of PDFs over Rs, then the family of Generative ConvACs with mixture
components from F can approximate any P DF over (Rs)N arbitrarily well, given arbitrarily many compo-
nents.

B TMMs with Sparsity Constraints Can Represent Gaussian Mixture

Models

As discussed in sec. 2, TMMs become tractable when a sparsity constraint is imposed on the priors tensor, i.e.
most of the entries of the tensors are replaced with zeros. In this section, we demonstrate that under such a case,
TMMs can represent Gaussian Mixture Models with diagonal covariance matrices, probably the most common
type of mixture models.

With the same notations as sec. 2, assume the number of mixing components of the TMM is M = N · K for
some K ∈ N, let {N (x; µki, diag(σ2
be these components, and ﬁnally, assume the prior tensor has
the following structure:

ki))}K,N

k,i

P (d1, . . . , dN ) =

(cid:40)

wk ∀i ∈ [N ], di=N ·(k−1)+i
0

Otherwise

then eq. 1 reduces to:

(cid:88)K

(cid:89)N

wk

P (X) =

N (x; µki, diag(σ2
k = ((σ2
˜σ2
which is equivalent to a diagonal GMM with mixing weights w ∈ (cid:52)K−1 (where (cid:52)K−1 is the K-dimensional
simplex) and Gaussian mixture components with means { ˜µk}K

k=1
kN )T )T
k1)T , . . . , (σ2

k=1 and covariances {diag( ˜σ2

wkN (x; ˜µk, diag( ˜σ2

k1, . . . , µT

˜µk = (µT

ki)) =

kN )T

k)}K

k=1.

k))

k=1

i=1

(cid:88)K

(cid:13)
M1(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1 wi

12

C Background on Tensor Factorizations

In this section we establish the minimal background in the ﬁeld of tensor analysis required for following our
work. A tensor is best thought of as a multi-dimensional array Ad1,...,dN ∈ R, where ∀i ∈ [N ], di ∈ [Mi].
The number of indexing entries in the array, which are also called modes, is referred to as the order of the
tensor. The number of values an index of a particular mode can take is referred to as the dimension of the mode.
The tensor A ∈ RM1⊗...⊗MN mentioned above is thus of order N with dimension Mi in its i-th mode. For
our purposes we typically assume that M1 = . . . = MN = M , and simply denote it as A ∈ (RM )⊗N .

The fundamental operator in tensor analysis is the tensor product. The tensor product operator, denoted by ⊗,
is a generalization of outer product of vectors (1-ordered vectors) to any pair of tensors. Speciﬁcally, let A and
B be tensors of order P and Q respectively, then the tensor product A ⊗ B results in a tensor of order P + Q,
deﬁned by: (A ⊗ B)d1,...,dP +Q = Ad1,...,dP · BdP +1,...,dP +Q .
The main concept from tensor analysis we use in our work is that of tensor decompositions. The most straight-
forward and common tensor decomposition format is the rank-1 decomposition, also known as a CANDE-
COMP/PARAFAC decomposition, or in short, a CP decomposition. The CP decomposition is a natural exten-
sion of low-rank matrix decomposition to general tensors, both built upon the concept of a linear combination
of rank-1 elements. Similarly to matrices, tensors of the form v(1) ⊗ · · · ⊗ v(N ), where v(i) ∈ RMi are
non-zero vectors, are regarded as N -ordered rank-1 tensors, thus the rank-Z CP decomposition of a tensor A
is naturally deﬁned by:

A =

azaz,1 ⊗ · · · ⊗ az,N

Z
(cid:88)

z=1

Z
(cid:88)

⇒ Ad1,...,dN =

N
(cid:89)

az

az,i
di

z=1

i=1

where {az,i ∈ RMi }N,Z
i=1,z=1 and a ∈ RZ are the parameters of the decomposition. As mentioned above,
for N = 2 it is equivalent to low-order matrix factorization. It is simple to show that any tensor A can be
represented by the CP decomposition for some Z, where the minimal such Z is known as its tensor rank.

Another decomposition we will use in this paper is of a hierarchical nature and known as the Hierarchical Tucker
decomposition [20], which we will refer to as HT decomposition. While the CP decomposition combines
vectors into higher order tensors in a single step, the HT decomposition does that more gradually, combining
vectors into matrices, these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion.
Speciﬁcally, the following describes the recursive formula of the HT decomposition4 for a tensor A ∈ (RM )⊗N
where N = 2L, i.e. N is a power of two5:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

· · ·

· · ·

r0(cid:88)

α=1

rl
1
(cid:88)
−

α=1

rL
2
(cid:88)
−

α=1

rL
1
(cid:88)
−

α=1

φl,j,γ =

al,j,γ
α

φl−1,2j−1,α
(cid:124)
(cid:123)(cid:122)
(cid:125)
order 2l

1

−

⊗ φl−1,2j,α
(cid:125)
(cid:123)(cid:122)
(cid:124)
1
order 2l

−

φL−1,j,γ =

aL−1,j,γ
α

φL−2,2j−1,α
(cid:123)(cid:122)
(cid:125)
(cid:124)
order N
4

⊗ φL−2,2j,α
(cid:123)(cid:122)
(cid:125)
order N
4

(cid:124)

A =

α φL−1,1,α
aL
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

⊗ φL−1,2,α
(cid:123)(cid:122)
(cid:125)
order N
2

(cid:124)

(6)

(7)

where the parameters of the decomposition are the vectors {al,j,γ∈Rrl
1 }l∈{0,...,L−1},j∈[N/2l],γ∈[rl] and the
top level vector aL ∈ RrL
1 , and the scalars r0, . . . , rL−1 ∈ N are referred to as the ranks of the decompo-
sition. Similar to the CP decomposition, any tensor can be represented by an HT decomposition. Moreover,

−

−

4 More precisely, we use a special case of the canonical HT decomposition as presented in Hackbusch and
K¨uhn [20]. In the terminology of the latter, the matrices Al,j,γ are diagonal and equal to diag(al,j,γ) (using
the notations from eq. 7).

5The requirement for N to be a power of two is solely for simplifying the deﬁnition of the HT decomposi-
tion. More generally, instead of deﬁning it through a complete binary tree describing the order of operations,
the canonical decomposition can use any balanced binary tree.

13

any given CP decomposition can be converted to an HT decomposition by only a polynomial increase in the
number of parameters.

Finally, since we are dealing with generative models, the tensors we study are non-negative and sum to one, i.e.
the vectorization of A (rearranging its entries to the shape of a vector), denoted by vec(A), is constrained to lie
in the multi-dimensional simplex, denoted by:

(cid:110)

(cid:52)k :=

x ∈ Rk+1|

(cid:88)k+1
i=1

xi = 1, ∀i ∈ [k + 1] : xi ≥ 0

(cid:111)

(8)

D Proof for the Depth Efﬁciency of Convolutional Arithmetic Circuits with

Simplex Constraints

In this section we prove that the depth efﬁciency property of ConvACs that was proved in Cohen et al. [9]
applies also to the generative variant of ConvACs we have introduced in sec. 2. Our analysis relies on basic
knowledge of tensor analysis and its relation to ConvACs, speciﬁcally, that the concept of “ranks” of each
factorization scheme is equivalent to the number of channels in these networks. For completeness, we provide
a short introduction to tensor analysis in app. C. The

We prove the following theorem, which is the generative analog of theorem 1 from [9]:
Theorem 1. Let Ay be a tensor of order N and dimension M in each mode, generated by the recursive
formulas in eq. 7, under the simplex constraints introduced in sec. 2. Deﬁne r := min{r0, M }, and consider
the space of all possible conﬁgurations for the parameters of the decomposition – {al,j,γ ∈ (cid:52)rl
1−1}l,j,γ.
In this space, the generated tensor Ay will have CP-rank of at least rN/2 almost everywhere (w.r.t. the product
measure of simplex spaces). Put differently, the conﬁgurations for which the CP-rank of Ay is less than rN/2
form a set of measure zero. The exact same result holds if we constrain the composition to be “shared”, i.e. set
al,j,γ ≡ al,γ and consider the space of {al,γ ∈ (cid:52)rl

1−1}l,γ conﬁgurations.

−

−

The only differences between ConvACs and their generative counter-parts are the simplex constraints applied
to the parameters of the models, which necessitate a careful treatment to the measure theoretical arguments of
the original proof. More speciﬁcally, while the k-dimensional simplex (cid:52)k is a subset of the k + 1-dimensional
space Rk+1, it has a zero measure with respect to the Lebesgue measure over Rk+1. The standard method
to deﬁne a measure over (cid:52)k is by the Lebesgue measure over Rk of its projection to that space, i.e.
let
λ : Rk → R be the Lebesgue measure over Rk, p : Rk+1 → Rk, p(x) = (x1, . . . , xk)T be a projection,
and A ⊂ (cid:52)k be a subset of the simplex, then the latter’s measure is deﬁned as λ(p(A)). Notice that p((cid:52)k)
has a positive measure, and moreover that p is invertible over the set p((cid:52)k), and that its inverse is given by
p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
i=1 xi). In our case, the parameter space is the cartesian product
of several simplex spaces of different dimensions, for each of them the measure is deﬁned as above, and the
measure over their cartesian product is uniquely deﬁned by the product measure. Though standard, the choice
of the projection function p above could be seen as a limitation, however, the set of zero measure sets in (cid:52)k
is identical for any reasonable choice of a projection π (e.g. all polynomial mappings). More speciﬁcally, for
any projection π : Rk+1 → Rk that is invertible over π((cid:52)k), π−1 is differentiable, and the Jacobian of π−1
is bounded over π((cid:52)k), then a subset A ⊂ (cid:52)k is of measure zero w.r.t. the projection π iff it is of measure
zero w.r.t. p (as deﬁned above). This implies that if we sample the weights of the generative decomposition
(eq. 7 with simplex constraints) by a continuous distribution, a property that holds with probability 1 under the
standard parameterization (projection p), will hold with probability 1 under any reasonable parameterization.

We now state and prove a lemma that will be needed for our proof of theorem 1.
Lemma 1. Let M, N, K ∈ N, 1 ≤ r ≤ min{M, N } and a polynomial mapping A : RK → RM ×N (i.e.
for every i ∈ [M ], j ∈ [N ] then Aij : Rk → R is a polynomial function). If there exists a point x ∈ RK s.t.
rank (A(x)) ≥ r, then the set {x ∈ RK |rank (A(x)) < r} has zero measure.

Proof. Remember that rank (A(x)) ≥ r iff there exits a non-zero r × r minor of A(x), which is polynomial
(cid:1) be the number of minors in A,
in the entries of A(x), and so it is polynomial in x as well. Let c = (cid:0)M
denote the minors by {fi(x)}c
i=1 fi(x)2. It thus holds that
f (x) = 0 iff for all i ∈ [c] it holds that fi(x) = 0, i.e. f (x) = 0 iff rank (A(x)) < r.

i=1, and deﬁne the polynomial function f (x) = (cid:80)c

(cid:1) · (cid:0)N

r

r

Now, f (x) is a polynomial in the entries of x, and so it either vanishes on a set of zero measure, or it is
the zero polynomial (see Caron and Traynor [6] for proof). Since we assumed that there exists x ∈ RK s.t.
rank(A(x)) ≥ r, the latter option is not possible.

Following the work of Cohen et al. [9], our main proof relies on following notations and facts:

14

• We denote by [A] the matricization of an N -order tensor A (for simplicity, N is assumed to be
even), where rows and columns correspond to odd and even modes, respectively. Speciﬁcally, if
A ∈ RM1×···MN , the matrix [A] has M1 · M3 · . . . · MN −1 rows and M2 · M4 · . . . · MN columns,
rearranging the entries of the tensor such that Ad1,...,dN is stored in row index 1 + (cid:80)N/2
i=1(d2i−1 −
1) (cid:81)N/2
j=i+1 M2j. Additionally, the matriciza-
tion is a linear operator, i.e. for all scalars α1, α2 and tensors A1, A2 with the order and dimensions
in every mode, it holds that [α1A1 + α2A2] = α1[A1] + α2[A2].

j=i+1 M2j−1 and column index 1 + (cid:80)N/2

i=1(d2i − 1) (cid:81)N/2

• The relation between the Kronecker product (denoted by (cid:12)) and the tensor product (denoted by ⊗)

is given by [A ⊗ B] = [A] (cid:12) [B].

• For any two matrices A and B, it holds that rank (A (cid:12) B) = rank (A) · rank (B).

• Let Z be the CP-rank of A, then it holds that rank ([A]) ≤ Z (see [9] for proof).

Proof of theorem 1. Stemming from the above stated facts, to show that the CP-rank of Ay is at least rN/2, it
is sufﬁcient to examine its matricization [Ay] and prove that rank ([Ay]) ≥ rN/2.
Notice from the construction of [Ay], according to the recursive formula of the HT-decomposition, that
its entires are polynomial in the parameters of the decomposition, its dimensions are M N/2 each and that
1 ≤ rN/2 ≤ M N/2. In accordance with the discussion on the measure of simplex spaces, for each vector
1−1, and notice that
parameter al,j,γ ∈ (cid:52)rl
p−1(˜al,j,γ) is a polynomial mapping6 w.r.t. ˜al,j,γ. Thus, [Ay] is a polynomial mapping w.r.t. the projected
parameters {˜al,j,γ}l,j,γ, and using lemma 1 it is sufﬁcient to show that there exists a set of parameters for
which rank ([Ay]) ≥ rN/2.

1−1, we instead examine its projection ˜al,j,γ = p(al,j,γ) ∈ Rrl

−

−

Denoting for convenience φL,1,1 := Ay and rL = 1, we will construct by induction over l = 1, ..., L a
set of parameters, {al,j,γ}l,j,γ, for which the ranks of the matrices {[φl,j,γ]}j∈[N/2l],γ∈[rl] are at least r2l/2,
while enforcing the simplex constraints on the parameters. More so, we’ll construct these parameters s.t.
al,j,γ = al,γ, thus proving both the ”unshared” and ”shared” cases.

For the case l = 1 we have:

φ1,j,γ =

a1,j,γ
α

a0,2j−1,α ⊗ a0,2j,α

and let a1,j,γ
and so

α =

r

1α
≤
r

and a0,j,α
i

= 1α=i for all i, j, γ and α ≤ M , and a0,j,α

i

= 1i=1 for all i and α > M ,

1/r
0
which means rank (cid:0)[φ1,j,γ](cid:1) = r, while preserving the simplex constraints, which proves our inductive hy-
pothesis for l = 1.

i = j ∧ i ≤ r
Otherwise

[φ1,j,γ]i,j =

≥ r2l

−

1/2 for all j(cid:48) ∈ [N/2l

1] and γ(cid:48) ∈ [rl−1]. For some speciﬁc choice

−

[φl−1,j(cid:48),γ(cid:48) ]
Assume now that rank
of j ∈ [N/2l] and γ ∈ [rl] we have:

(cid:16)

(cid:17)

r0(cid:88)

α=1

(cid:40)

φl,j,γ =

α φl−1,2j−1,α ⊗ φl−1,2j,α
al,j,γ

=⇒ [φl,j,γ] =

al,j,γ
α

[φl−1,2j−1,α] (cid:12) [φl−1,2j,α]

Denote Mα := [φl−1,2j−1,α] (cid:12) [φl−1,2j,α] for α = 1, ..., rl−1. By our inductive assumption, and by the
general property rank (A (cid:12) B) = rank (A) · rank (B), we have that the ranks of all matrices Mα are at least
r2l
α=1 al,j,γ
· Mα, and noticing that {Mα} do not depend on
α = 1α=1, and thus φl,j,γ = M1, which is of rank r2l/2. This completes the proof
al,j,γ, we simply pick al,j,γ
of the theorem.

1/2 = r2l/2. Writing [φl,j,γ] = (cid:80)rl

1/2 · r2l

α

−

−

−

1

From the perspective of ConvACs with simplex constraints, theorem 1 leads to the following corollary:

6As we mentioned earlier, p is invertible only over p((cid:52)k),

p−1(x1, . . . , xk) = (x1, . . . , xk, 1 − (cid:80)k
as deﬁned here over the entire range Rk−1, even where it does not serve as the inverse of p.

for which its inverse is given by
i=1 xi). However, to simpliﬁed the proof and notations, we use p−1

rl
1
(cid:88)
−

α=1

rl
1
(cid:88)
−

α=1

15

Corollary 3. Assume the mixing components M = {fi(x) ∈ L2(R2) ∩ L1(Rs)}M
i=1 are square integrable7
probability density functions, which form a linearly independent set. Consider a deep ConvAC model with
simplex constraints of polynomial size whose parameters are drawn at random by some continuous distribution.
Then, with probability 1, the distribution realized by this network requires an exponential size in order to be
realized (or approximated w.r.t. the L2 distance) by the shallow ConvAC model with simplex constraints. The
claim holds regardless of whether the parameters of the deep model are shared or not.

Proof. Given a coefﬁcient tensor A, the CP-rank of A is a lower bound on the number of channels (of its next
to last layer) required to represent that tensor by the ConvAC following the CP factorization. Additionally,
since the mixing components are linearly independent, their products {(cid:81)N
i=1 fi(xi)|fi ∈ M} are linearly
independent as well, which entails that any distribution representable by the generative variant of ConvAC
with mixing components M has a unique coefﬁcient tensor A. From theorem 1, the set of parameters of a
deep ConvAC model (under the simplex constraints) with a coefﬁcient tensor of a polynomial CP-rank – the
requirement for a polynomially-sized shallow ConvAC model with simplex constraints realizing that same
distribution exactly – forms a set of measure zero.

It is left to prove, that not only is it impossible to exactly represent a distribution with an exponential coefﬁcient
tensor by a shallow model, it is also impossible to approximate it. This follows directly from lemma 7 in
appendix B of Cohen et al. [9], as our case meets the requirement of that lemma.

E Proof for the Optimality of Marginalized Bayes Predictor

In this section we give short proofs for the claims from sec. 3, on the optimality of the marginalized Bayes
predictor under missing-at-random (MAR) distribution, when the missingness mechanism is unknown, as well
as the general case when we do not add additional assumptions. In addition, we will also present a counter ex-
ample proving data imputation results lead to suboptimal classiﬁcation performance. We begin by introducing
several notations that augment the notations already introduced in the body of the article.
Given a speciﬁc mask realization m ∈ {0, 1}s, we use the following notations to denote partial assignments
to the random vector X . For the observed indices of X , i.e. the indices for which mi = 1, we denote a partial
assignment by X \ m = xo, where xo ∈ Rdo is a vector of length do equal to the number of observed indices.
Similarly, we denote by X ∩ m = xm a partial assignment to the missing indices according to m, where
xm ∈ Rdm is a vector of length dm equal to the number of missing indices. As an example of the notation,
for given realizations x ∈ Rs and m ∈ {0, 1}s, we deﬁned in sec. 3 the event o(x, m), which using current
notation is marked by the partial assignment X \ m = xo where xo matches the observed values of the vector
x according to m.

With the above notations in place, we move on to prove claim 1, which describes the general solution to the
optimal prediction rule given both the data and missingness distributions, and without adding any additional
assumptions.

7It is important to note that most commonly used distribution functions are square integrable, e.g. most

members of the exponential family such as the Gaussian distribution.

16

Proof of claim 1. Fix an arbitrary prediction rule h. We will show that L(h∗) ≤ L(h), where L is the expected
0-1 loss.

1 − L(h)=E(x,m,y)∼(X ,M,Y)[1h(x(cid:12)m)=y]

P(M=m, X =x, Y=y)1h(x(cid:12)m)=ydx

(cid:88)

(cid:90)

(cid:88)

m∈{0,1}s

y∈[k]

Rs

(cid:88)

(cid:90)

(cid:88)

(cid:90)

Rdo

Rdm

y∈[k]

m∈{0,1}s
P(M=m, X \m=xo, X ∩m=xm, Y=y)1h(x⊗m)=ydxodxm

1h(x(cid:12)m)=ydxo

(cid:88)

(cid:90)

(cid:88)

y∈[k]

Rdo

m∈{0,1}s
(cid:90)

Rdm
(cid:88)

(cid:90)

(cid:88)

Rdo

m∈{0,1}s

(cid:88)

y∈[k]
(cid:90)

Rdo

P(M=m, X \m=xo, X ∩m=xm, Y=y)dxm

1h(x(cid:12)m)=yP(M=m, X \m=xo, Y=y)dxo

P(X \m=xo)

1h(x(cid:12)m)=yP(Y=y|X \m=xo)

(cid:88)

=

=

=1

=2

=3

≤4

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo
(cid:88)

(cid:88)

(cid:90)

P(X \m=xo)

Rdo

m∈{0,1}s
y∈[k]
P(M=m|X \m=xo, Y=y)dxo

=1 − L(h∗)

1h∗(x(cid:12)m)=yP(Y=y|X \m=xo)

Where (1) is because the output of h(x (cid:12) m) is independent of the missing values, (2) by marginalization,
(3) by conditional probability deﬁnition and (4) because by deﬁnition h∗(x (cid:12) m) maximizes the expression
P(Y=y|X \m=xo)P(M=m|X \m=xo, Y=y) w.r.t.
the possible values of y for ﬁxed vectors m and xo.
Finally, by replacing integrals with sums, the proof holds exactly the same when instances (X ) are discrete.

We now continue and prove corollary 1, a direct implication of claim 1 which shows that in the MAR setting,
the missingness distribution can be ignored, and the optimal prediction rule is given by the marginalized Bayes
predictor.

Proof of corollary 1. Using the same notation as in the previous proof, and denoting by xo the partial vector
containing the observed values of x (cid:12) m, the following holds:

P(M=m|o(x, m), Y=y) := P(M=m|X \m=xo, Y=y)

P(M=m, X ∩ m=xm|X \m=xo, Y=y)dxm

(cid:90)

(cid:90)

=

=

Rdm

P(X ∩m=xm|X \m=xo, Y=y)

Rdm
· P(M=m|X ∩m=xm, X \m=xo, Y=y)dxm
(cid:90)

P(X ∩m=xm|X \m=xo, Y=y)

=1

Rdm
· P(M=m|X ∩m=xm, X \m=xo)dxm
(cid:90)

=2

P(X ∩m=xm|X \m=xo, Y=y) · P(M=m|X \m=xo)dxm

Rdm

=P(M=m|X \m=xo)

P(X ∩m=xm|X \m=xo, Y=y)dxm

(cid:90)

Rdm

=P(M=m|o(x, m))

Where (1) is due to the independence assumption of the events Y = y and M = m conditioned on X = x,
while noting that (X \ m = xo) ∧ (X ∩ m = xm) is a complete assignment of X . (2) is due to the MAR
assumption, i.e. that for a given m and xo it holds for all xm ∈ Rdm :

P(M=m|X \m=xo, X ∩m=xm) = P(M=m|X \m=xo)

17

Y Weight

Probability ((cid:15) = 10−

4)

X1 X2
0
0
1
0
0
1
1
1
0
0
1
0
0
1
1
1

0
0
0
0
1
1
1
1

1

1

−
1

(cid:15)

(cid:15)

−
1
0
1 + (cid:15)
0
1 + (cid:15)

16.665%
16.667%
16.665%
16.667%
0.000%
16.668%
0.000%
16.668%
2
0, 1

Table 2: Data distribution over the space
the sub-optimality of classiﬁcation through data-imputation (proof of claim 3).

0, 1
}
{

X × Y

× {

=

}

that serves as the example for

We have shown that P(M=m|o(x, m), Y = y) does not depend on y, and thus does not affect the optimal
prediction rule in claim 1. It may therefore be dropped, and we obtain the marginalized Bayes predictor.

Having proved that in the MAR setting, classiﬁcation through marginalization leads to optimal performance,
we now move on to show that the same is not true for classiﬁcation through data-imputation. Though there are
many methods to perform data-imputation, i.e. to complete missing values given the observed ones, all of these
methods can be seen as the solution of the following optimization problem, or more typically its approximation:

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48))

Where g(x (cid:12) m) is the most likely completion of x (cid:12) m. When data-imputation is carried out for classiﬁcation
purposes, one is often interested in data-imputation conditioned on a given class Y = y, i.e.:

g(x (cid:12) m; y) =

argmax
x(cid:48)∈Rs∧∀i:mi=1→x(cid:48)i=xi

P(X = x(cid:48)|Y = y)

Given a classiﬁer h : Rs → [K] and an instance x with missing values according to m, classiﬁcation through
data-imputation is simply the result of applying h on the output of g. When h is the optimal classiﬁer for
complete data, i.e. the Bayes predictor, we end up with one of the following prediction rules:

Unconditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m))

Conditional: h(x (cid:12) m) = argmax

P(Y = y|X = g(x (cid:12) m; y))

y

y

Claim 3. There exists a data distribution D and MAR missingness distribution Q s.t. the accuracy of classi-
ﬁcation through data-imputation is almost half the accuracy of the optimal marginalized Bayes predictor, with
an absolute gap of more than 33 percentage points.

Proof. For simplicity, we will give an example for a discrete distribution over
the binary set
X × Y = {0, 1}2 × {0, 1}. Let 1 > (cid:15) > 0 be some small positive number, and we deﬁne D according to table 2,
where each triplet (x1, x2, y) ∈ X ×Y is assigned a positive weight, which through normalization deﬁnes a
distribution over X ×Y. The missingness distribution Q is deﬁned s.t. PQ(M1 = 1, M2 = 0|X = x) = 1 for
all x ∈ X , i.e. X1 is always observed and X2 is always missing, which is a trivial MAR distribution. Given the
above data distribution D, we can easily calculate the exact accuracy of the optimal data-imputation classiﬁer
and the marginalized Bayes predictor under the missingness distribution Q, as well as the standard Bayes pre-
dictor under full-observability. First notice that whether we apply conditional or unconditional data-imputation,
and whether X1 is equal to 0 or 1, the completion will always be X2 = 1 and the predicted class will always
be Y = 1. Since the data-imputation classiﬁers always predict the same class Y = 1 regardless of their input,
(for (cid:15) = 10−4 it equals approximately
the probability of success is simply the probability P (Y = 1) = 1+(cid:15)
3
33.337%). Similarly, the marginalized Bayes predictor always predicts Y = 0 regardless of its input, and so
(for (cid:15) = 10−4 it equals approximately 66.663%), which is
its probability of success is P (Y = 0) = 2−(cid:15)
3
almost double the accuracy achieved by the data-imputation classiﬁer. Additionally, notice that the marginal-
ized Bayes predictor achieves almost the same accuracy as the Bayes predictor under full-observability, which
equals exactly 2
3 .

18

F Efﬁcient Marginalization with Tensorial Mixture Models

As discussed above, with generative models optimal classiﬁcation under missing data (in the MAR setting) is
oblivious to the speciﬁc missingness distribution. However, it requires tractable marginalization over missing
values. In this section we show that TMMs bring forth extremely efﬁcient marginalization, requiring only a
single forward pass through the corresponding ConvAC.

Recall from sec. 2 and 2.3 that a TMM classiﬁer realizes the following form:

P (x1, . . . , xN |Y =y) =

P (d1, . . . , dN |Y =y)

P (xi|di; θdi )

(9)

(cid:88)M

d1,...,dN

(cid:89)N

i=1

Suppose now that only the local structures xi1 . . . xiV are observed, and we would like to marginalize over the
rest. Integrating eq. 9 gives:

P (xi1 , . . . , xiV |Y =y) =

P (d1, . . . , dN |Y =y)

(cid:88)M

d1,...,dN

(cid:89)V

v=1

P (xiv |div ; θdiv

)

from which it is evident that the same network used to compute P (x1, . . . , xN |Y =y), can be used to compute
P (xi1 , . . . , xiV |Y =y) – all it requires is a slight adaptation of the representation layer. Namely, the latter
would represent observed values through the usual likelihoods, whereas missing (marginalized) values would
now be represented via constant ones:
(cid:40)

rep(i, d) =

1
P (xi|d; Θ)

, xi is missing (marginalized)
, xi is visible (not marginalized)

More generally, to marginalize over individual coordinates of the local structure xi, it is sufﬁcient to replace
rep(i, d) by its respective marginalized mixing component.

To conclude, with TMMs marginalizing over missing values is just as efﬁcient as plain inference – requires
only a single pass through the corresponding network. Accordingly, the marginalized Bayes predictor (eq. 3)
is realized efﬁciently, and classiﬁcation under missing data (in the MAR setting) is optimal (under generative
assumption), regardless of the missingness distribution.

G Extended Discussion on Generative Models Based on Neural Networks

There are many generative models realized through neural networks, and convolutional networks in particular.
Of these models, one of the most successful to date is the method of Generative Adversarial Networks [19],
where a network is trained to generate instances from the data distribution, through a two-player mini-max
game. While there are numerous applications for learning to generate data points, e.g. inpainting and super-
resolution, it cannot be used for computing the likelihood of the data. Other generative networks do offer
inference, but only approximate. Variational Auto-Encoders [24] use a variational lower-bound on the likeli-
hood function. GSNs [5], DPMs [37] and MPDBMs [18] are additional methods along this line. The latter
is especially noteworthy for being a generative classiﬁer that can approximate the marginal likelihoods condi-
tioned on each class, and for being tested on classiﬁcation under missing data.

Some generative neural networks are capable of tractable inference, but not of tractable marginalization. Dinh
et al. [15] suggest a method for designing neural networks that realize an invertible transformation from a
Inverting the network brings forth tractable inference, yet par-
simple distribution to the data distribution.
tial integration of its density function is still intractable. Another popular method for tractable inference,
central to both PixelRNN [39] and NADE [38], is the factorization of the probability distribution according
to P(x1, . . . , xd) = (cid:81)d
P(xi|xi−1, . . . , x1), and realization of P(xi|xi−1, . . . , x1) as a neural network.
Based on this construction, certain marginal distributions are indeed tractable to compute, but most are not.
Orderless-NADE partially addresses this issue by using ensembles of models over different orderings of its
input. However, it can only estimate the marginal distributions, and has no classiﬁer analogue that can compute
class-conditional marginal likelihoods, as required for classiﬁcation under missing data.

i=1

H Image Generation and Network Visualization

Following the graphical model perspective of our models allows us to not only generate random instances from
the distribution, but to also generate the most likely patches for each neuron in the network, effectively explain-
ing its role in the classiﬁcation process. We remind the reader that every neuron in the network corresponds to a
possible assignment of a latent variable in the graphical model. By looking for the most likely assignments for
each of its child nodes in the graphical tree model, we can generate a patch that describes that neuron. Unlike
similar suggested methods to visualize neural networks [40], often relying on brute-force search or on solving
some optimization problem to ﬁnd the most likely image, our method emerges naturally from the probabilistic
interpretation of our model.

19

Figure 5: Generated digits samples from the HT-TMM model trained on the MNIST dataset.

Figure 6: Visualization of the HT-TMM model. Each of the images above visualize a different
layer of the model and consists of several samples generated from latent variables at different spatial
locations conditioned on randomly selected channels. The leftmost image shows samples taken
from the 5th layer which consists of just a single latent variable with 512 channels. The center
image shows samples taken from the 4th layer, which consists of 2
2 grid of latent variables with
256 channels each. The image is divided to 4 quadrants, each contains samples taken from the
respective latent variable at that position. The rightmost image shows samples from the 3rd layer,
which consists of 4
4 grid of latent variables with 128 channels, and the image is similarly spatial
divided into different areas matching the latent variables of the layer.

×

×

20

In ﬁg. 5, we can see conditional samples generates for each digit, while in ﬁg. 6 we can see a visualization of the
top-level layers of network, where each small patch matches a different neuron in the network. The common
wisdom of how ConvNets work is by assuming that simple low-level features are composed together to create
more and more complex features, where each subsequent layer denotes features of higher abstraction – the
visualization of our network clearly demonstrate this hypothesis to be true for our case, showing small strokes
iteratively being composed into complete digits.

I Detailed Description of the Experiments

Experiments are meaningful only if they could be reproduced by other proﬁcient individuals. Providing suf-
ﬁcient details to enable others to replicate our results is the goal of this section. We hope to accomplish this
by making our code public, as well as documenting our experiments to a sufﬁcient degree allowing for their
reproduction from scratch. Our complete implementation of the models presented in this paper, as well as
our modiﬁcations to other open-source projects and scripts used in the process of conducting our experiments,
are available at our Github repository: https://github.com/HUJI-Deep/Generative-ConvACs. We ad-
ditionally wish to invite readers to contact the authors, if they deem the following details insufﬁcient in their
process to reproduce our results.

I.1 Description of Methods

In the following we give concise descriptions of each classiﬁcation method we have used in our experiments.
The results of the experiment on MP-DBM [18] were taken directly from the paper and were not conducted
by us, hence we do not cover it in this section. We direct the reader to that article for exact details on how to
reproduce their results.

I.1.1 Robust Linear Classiﬁer

In [13], binary linear classiﬁers were trained by formulating their optimization as a quadric program under the
constraint that some of its features could be deleted, i.e. their original value was changed to zero. While the
original source code was never published, the authors have kindly agreed to share with us their code, which
we used to reproduced their results, but on larger datasets. The algorithm has only a couple hyper-parameters,
which were chosen by a grid-search through a cross-validation process. For details on the exact protocol for
testing binary classiﬁers on missing data, please see sec. I.2.1.

I.1.2 K-Nearest Neighbors

K-Nearest Neighbors (KNN) is a classical machine learning algorithm used for both regression and classiﬁca-
tion tasks. Its underlying mechanism is ﬁnding the k nearest examples (called neighbors) from the training set,
(x1, y1), . . . , (xk, yk) ∈ S, according to some metric function d(·, ·) : X × X → R+, after which a summa-
rizing function f is applied to the targets of the k nearest neighbors to produce the output y∗ = f (y1, . . . , yk).
When KNN is used for classiﬁcation, f is typically the majority voting function, returning the class found in
most of the k nearest neighbors.

In our experiments we use KNN for classiﬁcation under missing data, where the training set consists of com-
plete examples with no missing data, but at classiﬁcation time the inputs have missing values. Given an input
with missing values x (cid:12) m and an example x(cid:48) from the training set, we use a modiﬁed Euclidean distance
metric, where we compare the distance only against the non-missing coordinates of x, i.e. the metric is deﬁned
by d(x(cid:48), x (cid:12) m) = (cid:80)
i − xi)2. Through a process of cross-validation we have chosen k = 5 for
all of our experiments. Our implementation of KNN is based on the popular scikit-learn python library [32].

i:mi=1 (x(cid:48)

I.1.3 Convolutional Neural Networks

The most widespread and successful discriminative method nowadays are Convolutional Neural Net-
works (ConvNets). Standard ConvNets are represented by a computational graph consisted of different kinds
of nodes, called layers, with a convolutional-like operators applied to their inputs, followed by a non-linear
point-wise activation function, e.g. max(0, x) known as ReLU.

For our experiments on MNIST, both with and without missing data, we have used the LeNeT ConvNet ar-
chitecture [25] that is bundled with Caffe [23], trained for 20,000 iterations using SGD with 0.9 momentum
and 0.01 base learning rate, which remained constant for 10,000 iterations, followed by a linear decrease to
0.001 for another 5,000 iterations, followed by a linear decrease to 0 learning rate for the remaining 5,000
iterations. The model also used l2-regularization (also known as weight decay), which was chosen through
cross-validation for each experiment separately. No other modiﬁcations were made to the model or its training
procedure.

21

For our experiments on NORB, we have used an ensemble of 3 ConvNets, each using the following architecture:
5×5 convolution with 128 output channels, 3×3 max pooling with stride 2, ReLU activation, 5×5 convolution
with 128 output channels, ReLU activation, dropout layer with probability 0.5, 3×3 average pooling with
stride 2, 5×5 convolution with 256 output channels, ReLU activation, dropout layer with probability 0.5,
3×3 average pooling with stride 2, fully-connected layer with 768 output channels, ReLU activation, dropout
layer with probability 0.5, and ends with fully-connected layer with 5 output channels. The stereo images
were represented as a two-channel input image when fed to the network. During training we have used data
augmentation consisting of randomly scaling and rotation transforms. The networks were trained for 40,000
iterations using SGD with 0.99 momentum and 0.001 base learning rate, which remained constant for 30,000
iterations, followed by a linear decrease to 0.0001 for 6000 iterations, followed by a linear decrease to 0 learning
rate for the remaining 4,000 iterations. The model also used 0.0001 weight decay for additional regularization.

When ConvNets were trained on images containing missing values, we passed the network the original image
with missing values zeroed out, and an additional binary image as a separate channel, containing 1 for missing
values at the same spatial position, and 0 otherwise – this missing data format is sometimes known as ﬂag
data imputation. Other formats for representing missing values were tested (e.g. just using zeros for missing
values), however, the above scheme performed signiﬁcantly better than other formats. In our experiments, we
assumed that the training set was complete and missing values were only present in the test set. In order to
design ConvNets that are robust against speciﬁc missingness distributions, we have simulated missing values
during training, sampling a different mask of missing values for each image in each mini-batch. As covered
in sec. 5, the results of training ConvNets directly on simulated missingness distributions resulted in classiﬁers
which were biased towards the speciﬁc distribution used in training, and performed worse on other distributions
compared to ConvNets trained on the same distribution.

In addition to training ConvNets directly on missing data, we have also used them as the classiﬁer for testing
different data imputation methods, as describe in the next section.

I.1.4 Classiﬁcation Through Data Imputation

The most common method for handling missing data, while leveraging available discriminative classiﬁers, is
through the application of data imputation – an algorithm for the completion of missing values – and then
passing the results to a classiﬁer trained on uncorrupted dataset. We have tested ﬁve different types of data
imputation algorithms:

• Zero data imputation: replacing every missing value by zero.

• Mean data imputation: replacing every missing value by the mean value computed over the dataset.

• Generative data imputation: training a generative model and using it to complete the missing values
by ﬁnding the most likely instance that coincides with the observed values, i.e. solving the following

g(x (cid:12) m) =

argmax
x(cid:48)∈Rs∧∀i,mi=1→x(cid:48)i=xi

P (X = x(cid:48))

We have tested the following generative models:

– Generative Stochastic Networks (GSN) [5]: We have used their original source code from
https://github.com/yaoli/GSN, and trained their example model on MNIST for 1000
epochs. Whereas in the original article they have tested completing only the left or right side of
a given image, we have modiﬁed their code to support general masks. Our modiﬁed implemen-
tation can be found at https://github.com/HUJI-Deep/GSN.

– Non-linear Independent Components Estimation (NICE) [15]: We have used their original
source code from https://github.com/laurent-dinh/nice, and trained it on MNIST us-
ing their example code without changes. Similarly to our modiﬁcation to the GSN code, here
too we have adapted their code to support general masks over the input. Additionally, their orig-
inal inpainting code required 110,000 iterations, which we have reduced to just 8,000 iterations,
since the effect on classiﬁcation accuracy was marginal. For the NORB dataset, we have used
their CIFAR10 example, with lower learning rate of 10−4. Our modiﬁed code can be found at
https://github.com/HUJI-Deep/nice.

– Diffusion Probabilistic Models (DPM) [37]: We have user their original source code
from https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models, and
trained it on MNIST using their example code without changes. Similarly to our modiﬁ-
cations to GSN, we have add support for a general mask of missing values, but other than
that kept the rest of the parameters for inpainting unchanged. For NORB we have used
the same model as MNIST. We have tried using their CIFAR10 example for NORB, how-
ever, it produced exceptions during training. Our modiﬁed code can be found at https:
//github.com/HUJI-Deep/Diffusion-Probabilistic-Models.

22

I.1.5 Tensorial Mixture Models

For a complete theoretical description of our model please see the body of the article. Our models were
implemented by performing all intermediate computations in log-space, using numerically aware operations. In
practiced, that meant our models were realized by the SimNets architecture [7, 10], which consists of Similarity
layers representing gaussian distributions, MEX layers representing weighted sums performed on log-space
input and outputs, as well as standard pooling operations. The learned parameters of the MEX layers are called
offsets, which represents the weights of the weighted sum, but saved in log-space. The parameters of the MEX
layers can be optionally shared between spatial regions, or alternatively left with no parameter sharing at all.
Additionally, when used to implement our generative models, the offsets are normalized to have a soft-max (i.e.,
log (cid:0)(cid:80)

i exp(xi)(cid:1)) of zero.

The network architectures we have tested in this article, consists of M different Gaussian mixture components
with diagonal covariance matrices, over non-overlapping patches of the input of size 2 × 2, which were imple-
mented by a similarity layer as speciﬁed by the SimNets architecture, but with an added gaussian normalization
term.

We ﬁrst describe the architectures used for the MNIST dataset. For the CP-TMM model, we used M = 800,
and following the similarity layer is a 1 × 1 MEX layer with no parameter sharing over spatial regions and
10 output channels. The model ends with a global sum pooling operation, followed by another 1 × 1 MEX
layer with 10 outputs, one for each class. The HT-TMM model starts with the similarity layer with M = 32,
followed by a sequence of four pairs of 1 × 1 MEX layer followed by 2 × 2 sum pooling layer, and after the
pairs and additional 1 × 1 MEX layer lowering the outputs of the model to 10 outputs as the number of classes.
The number of output channels for each MEX layer are as follows 64-128-256-512-10. All the MEX layers in
this network do not use parameter sharing, except the ﬁrst MEX layer, which uses a repeated sharing pattern
of 2 × 2 offsets, that analogous to a 2 × 2 convolution layer with stride 2. Both models were trained with the
losses described in sec. 2.3, using the Adam SGD variant for optimizing the parameters, with a base learning
rate of 0.03, and β1 = β2 = 0.9. The models were trained for 25,000 iterations, where the learning rate was
dropped by 0.1 after 20,000 iterations.

For the NORB dataset, we have trained only the HT-TMM model with M = 128 for the similarity layer. The
MEX layers use the same parameter sharing scheme as the one for MNIST, and the number of output channels
for each MEX layer are as follows: 256-256-256-512-5. Training was identical to the MNIST models, with the
exception of using 40,000 iterations instead of just 25,000. Additionally, we have used an ensemble of 4 models
trained separately, each trained using a different generative loss weight (see below for more information). We
have also used the same data augmentation methods (scaling and rotation) which were used in training the
ConvNets for NORB used in this article.

i x2

The standard L2 weight regularization (sometimes known as weight decay) did not work well on our mod-
els, which lead us to adapt it to better ﬁt to log-space weights, by minimizing λ (cid:80)
i (exp (xi))2 instead of
λ||x||2 = λ (cid:80)
i , where the parameter λ was chosen through cross-validation. Additionally, since even
with large values of λ our model was still overﬁtting, we have added another form of regularization in the form
of random marginalization layers. A random marginalization layer, is similar in concept to dropout, but instead
of zeroing activations completely in random, it choses spatial locations at random, and then zero out the activa-
tions at those locations for all the channels. Under our model, zeroing all the activations in a layer at a speciﬁc
location, is equivalent to marginalizing over all the inputs for the receptive ﬁeld for that respective location.
We have used random marginalization layers in between all our layers during training, where the probability
for zeroing out activations was chosen through cross-validation for each layer separately. Though it might raise
concern that random marginalization layers could lead to biased results toward the missingness distributions
we have tested it on, in practice the addition of those layers only helped improve our results under cases where
only few pixels where missing.

Finally, we wish to discuss a few optimization tricks which had a minor effects compared to the above, but were
nevertheless very useful in achieving slightly better results. First, instead of optimizing directly the objective
deﬁned by eq. 2, we add smoothing parameter β between the two terms, as follows:

Θ∗ = argmin

−

log

Θ

|S|
(cid:88)

i=1

eNΘ(X(i);Y (i))
y=1 eNΘ(X(i);y)

(cid:80)K

|S|
(cid:88)

K
(cid:88)

− β

log

eNΘ(X(i);y)

i=1

y=1

setting β too low diminish the generative capabilities of our models, while setting it too high diminish the
discriminative performance. Through cross-validation, we decided on the value β = 0.01 for the models
trained on MNIST, while for NORB we have used a different value of β for each of the models, ranging in
{0.01, 0.1, 0.5, 1}. Second, we found that performance increased if we normalized activations before applying
the 1 × 1 MEX operations. Speciﬁcally, we calculate the soft-max over the channels for each spatial location

23

which we call the activation norm, and then subtract it from every respective activation. After applying the
MEX operation, we add back the activation norm. Though might not be obvious at ﬁrst, subtracting a constant
from the input of a MEX operation and adding it to its output is equivalent does not change the mathematical
operation. However, it does resolve the numerical issue of adding very large activations to very small offsets,
which might result in a loss of precision. Finally, we are applying our model in different translations of the input
and then average the class predictions. Since our model can marginalize over inputs, we do not need to crop
the original image, and instead mask the unknown parts after translation as missing. Applying a similar trick
to standard ConvNets on MNIST does not seem to improve their results. We believe this method is especially
ﬁt to our model, is because it does not have a natural treatment of overlapping patches like ConvNets do, and
because it is able to marginalize over missing pixels easily, not limiting it just to crop translation as is typically
done.

I.2 Description of Experiments

In this section we will give a detailed description of the protocol we have used during our experiments.

I.2.1 Binary Digit Classiﬁcation under Feature Deletion Missing Data

This experiment focuses on the binary classiﬁcation problem derived from MNIST, by limiting the number of
classes to two different digits at a time. We use the same non-zero feature deletion distribution as suggested by
Globerson and Roweis [17], i.e. for a given image we uniformly sample a set of N non-zero pixels from the
image (if the image has less than N non-zero pixels then they are non-zero pixels are chosen), and replace their
values with zeros. This type of missingness distribution falls under the MNAR type deﬁned in sec.3.

We test values of N in {0, 25, 50, 75, 100, 125, 150}. For a given value of N , we train a separate classiﬁer
on each digit pair classiﬁer on a randomly picked subset of the dataset containing 300 images per digit (600
total). During training we use a ﬁxed validation set with 1000 images per digit. After picking the best classiﬁer
according to the validation set, the classiﬁer is tested against a test set with a 1000 images per digits with a
randomly chosen missing values according to the value of N . This experiment is repeated 10 times for each
digit pair, each time using a different subset for the training set, and a new corrupted test set. After conducting
all the different experiments, all the accuracies are averaged for each value of N , which are reported in table 1.

I.2.2 Multi-class Digit Classiﬁcation under MAR Missing Data

This experiment focuses on the complete multi-class digit classiﬁcation of the MNIST dataset, in the presence
of missing data according to different missingness distributions. Under this setting, only the test set contains
missing values, whereas the training set does not. We test two kinds of missingness distributions, which both
fall under the MAR type deﬁned in sec.3. The ﬁrst kind, which we call i.i.d. corruption, each pixel is missing
with a ﬁxed probability p. the second kind, which we call missing rectangles corruption, The positions of N
rectangles of width W or chosen uniformly in the picture, where the rectangles can overlap one another. During
the training stage, the models to be tested are not to be biased toward the speciﬁc missingness distributions we
have chosen, and during the test stage, the same classiﬁer is tested against all types of missingness distributions,
and without supplying it with the parameters or type of the missingness distribution it is tested against. This
rule prevent the use of ConvNets trained on simulated missingness distributions. To demonstrate that the latter
lead to biased classiﬁers, we have conducted a separate experiment just for ConvNets, where the previous rule is
ignored, and we train a separate ConvNet classiﬁer on each type and parameter of the missingness distributions
we have used. We then tested each of those ConvNets on all other missingness distributions, the results of
which are in ﬁg. 3, which conﬁrmed our hypothesis.

24

