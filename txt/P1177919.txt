7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

7
1
0
2
 
v
o
N
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
7
4
6
2
0
.
3
0
7
1
:
v
i
X
r
a

Streaming Weak Submodularity:
Interpreting Neural Networks on the Fly

Ethan R. Elenberg1, Alexandros G. Dimakis1, Moran Feldman2, and Amin Karbasi3

1Department of Electrical and Computer Engineering
The University of Texas at Austin
elenberg@utexas.edu, dimakis@austin.utexas.edu
2Department of Mathematics and Computer Science
Open University of Israel
moranfe@openu.ac.il
3Department of Electrical Engineering, Department of Computer Science
Yale University
amin.karbasi@yale.edu

November 27, 2017

Abstract

In many machine learning applications, it is important to explain the predictions of a black-box
classiﬁer. For example, why does a deep neural network assign an image to a particular class? We cast
interpretability of black-box classiﬁers as a combinatorial maximization problem and propose an eﬃcient
streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru
et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of
random stream order and a weakly submodular objective function. This is the ﬁrst such theoretical
guarantee for this general class of functions, and we also show that no such algorithm exists for a worst
case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster
than the state-of-the-art LIME framework of Ribeiro et al. [2016].

1

Introduction

Consider the following combinatorial optimization problem. Given a ground set N of N elements and a set
function f : 2N (cid:55)→ R≥0, ﬁnd the set S of size k which maximizes f (S). This formulation is at the heart
of many machine learning applications such as sparse regression, data summarization, facility location, and
graphical model inference. Although the problem is intractable in general, if f is assumed to be submodular
then many approximation algorithms have been shown to perform provably within a constant factor from
the best solution.

Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in main
memory, or when a single function evaluation is time consuming. In our main application, each function
evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast,
streaming algorithms make a small number of passes (often only one) over the data and have sublinear space
complexity, and thus, are ideal for tasks of the above kind.

Recent ideas, algorithms, and techniques from submodular set function theory have been used to derive
similar results in much more general settings. For example, Elenberg et al. [2016a] used the concept of weak
submodularity to derive approximation and parameter recovery guarantees for nonlinear sparse regression.

1

Thus, a natural question is whether recent results on streaming algorithms for maximizing submodular
functions [Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015] extend to the weakly
submodular setting.

This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm for
any class of approximately submodular functions. We use key algorithmic components of Sieve-Streaming
[Badanidiyuru et al., 2014], namely greedy thresholding and binary search, combined with a novel analysis to
prove a constant factor approximation for γ-weakly submodular functions (deﬁned in Section 3). Speciﬁcally,
our contributions are as follows.

• An impossibility result showing that, even for 0.5-weakly submodular objectives, no randomized stream-
ing algorithm which uses o(N ) memory can have a constant approximation ratio when the ground set
elements arrive in a worst case order.

• Streak: a greedy, deterministic streaming algorithm for maximizing γ-weakly submodular functions
2 − e−γ/2)

which uses O(ε−1k log k) memory and has an approximation ratio of (1−ε) γ
when the ground set elements arrive in a random order.

2 ·(3−e−γ/2 −2

√

• An experimental evaluation of our algorithm in two applications: nonlinear sparse regression using

pairwise products of features and interpretability of black-box neural network classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for worst
case stream order.

One advantage of our approach is that, while our approximation guarantees are in terms of γ, our
algorithm Streak runs without requiring prior knowledge about the value of γ. This is important since
the weak submodularity parameter γ is hard to compute, especially in streaming applications, as a single
element can alter γ drastically.

We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al.,
2016]. For that purpose, we deﬁne a new set function maximization problem similar to LIME [Ribeiro et al.,
2016] and apply our framework to approximately maximize this function. Experimentally, we ﬁnd that our
interpretability method produces explanations of similar quality as LIME, but runs approximately 10 times
faster.

2 Related Work

Monotone submodular set function maximization has been well studied, starting with the classical analysis
of greedy forward selection subject to a matroid constraint [Nemhauser et al., 1978; Fisher et al., 1978].
For the special case of a uniform matroid constraint, the greedy algorithm achieves an approximation ratio
of 1 − 1/e [Fisher et al., 1978], and a more involved algorithm obtains this ratio also for general matroid
constraints [Călinescu et al., 2011]. In general, no polynomial-time algorithm can have a better approximation
ratio even for a uniform matroid constraint [Nemhauser and Wolsey, 1978; Feige, 1998]. However, it is possible
to improve upon this bound when the data obeys some additional guarantees [Conforti and Cornuéjols, 1984;
Vondrák, 2010; Sviridenko et al., 2015]. For maximizing nonnegative, not necessarily monotone, submodular
functions subject to a general matroid constraint, the state-of-the-art randomized algorithm achieves an
approximation ratio of 0.385 [Buchbinder and Feldman, 2016b]. Moreover, for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and Feldman,
2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys on submodular
function theory.

A recent line of work aims to develop new algorithms for optimizing submodular functions suitable for
large-scale machine learning applications. Algorithmic advances of this kind include Stochastic-Greedy
[Mirzasoleiman et al., 2015], Sieve-Streaming [Badanidiyuru et al., 2014], and several distributed ap-
proaches [Mirzasoleiman et al., 2013; Barbosa et al., 2015, 2016; Pan et al., 2014; Khanna et al., 2017b]. Our
algorithm extends ideas found in Sieve-Streaming and uses a diﬀerent analysis to handle more general
functions. Additionally, submodular set functions have been used to prove guarantees for online and active

2

learning problems [Hoi et al., 2006; Wei et al., 2015; Buchbinder et al., 2015]. Speciﬁcally, in the online set-
ting corresponding to our setting (i.e., maximizing a monotone function subject to a cardinality constraint),
Chan et al. [2017] achieve a competitive ratio of about 0.3178 when the function is submodular.

The concept of weak submodularity was introduced in Krause and Cevher [2010]; Das and Kempe [2011],
where it was applied to the speciﬁc problem of feature selection in linear regression. Their main results state
that if the data covariance matrix is not too correlated (using either incoherence or restricted eigenvalue
assumptions), then maximizing the goodness of ﬁt f (S) = R2
S as a function of the feature set S is weakly
submodular. This leads to constant factor approximation guarantees for several greedy algorithms. Weak
submodularity was connected with Restricted Strong Convexity in Elenberg et al. [2016a,b]. This showed that
the same assumptions which imply the success of regularization also lead to guarantees on greedy algorithms.
This framework was later used for additional algorithms and applications [Khanna et al., 2017a,b]. Other
approximate versions of submodularity were used for greedy selection problems in Horel and Singer [2016];
Hassidim and Singer [2017]; Altschuler et al. [2016]; Bian et al. [2017]. To the best of our knowledge, this is
the ﬁrst analysis of streaming algorithms for approximately submodular set functions.

Increased interest in interpretable machine learning models has led to extensive study of sparse feature
selection methods. For example, Bahmani et al. [2013] consider greedy algorithms for logistic regression,
and Yang et al. [2016] solve a more general problem using (cid:96)1 regularization. Recently, Ribeiro et al. [2016]
developed a framework called LIME for interpreting black-box neural networks, and Sundararajan et al.
[2017] proposed a method that requires access to the network’s gradients with respect to its inputs. We
compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters, and all big O notation
is assumed to be scaling with respect to N (the number of elements in the input stream). Given a set function
f , we often use the discrete derivative f (B | A) (cid:44) f (A ∪ B) − f (A). f is monotone if f (B | A) ≥ 0, ∀A, B
and nonnegative if f (A) ≥ 0, ∀A. Using this notation one can deﬁne weakly submodular functions based on
the following ratio.

Deﬁnition 3.1 (Weak Submodularity, adapted from Das and Kempe [2011]). A monotone nonnegative set
function f : 2N (cid:55)→ R≥0 is called γ-weakly submodular for an integer r if

γ ≤ γr (cid:44) min
L,S⊆N :
|L|,|S\L|≤r

(cid:80)

j∈S\L f (j | L)
f (S | L)

,

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete derivatives.

It is easy to show that f is submodular if and only if γ|N | = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns a set S
has approximation ratio R ∈ [0, 1] if E[f (S)] ≥ R · f (OP T ), where OP T is the optimal solution and the
expectation is over the random decisions of the algorithm and the randomness of the input stream order
(when it is random).

Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that given
oracle access to a nonnegative set function f : 2N (cid:55)→ R≥0 maintains at most o(N ) elements in memory and
returns a set S of size at most k approximating

up to an approximation ratio R(γk). Ideally, this approximation ratio should be as large as possible, and we
also want it to be a function of γk and nothing else. In particular, we want it to be independent of k and N .
To simplify notation, we use γ in place of γk in the rest of the paper. Additionally, proofs for all our

theoretical results are deferred to the Appendix.

max
|T |≤k

f (T ) ,

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant approxi-
mation ratio against a worst case stream order, we ﬁrst need to construct a weakly submodular set function
fk. Later we use it to construct a bad instance for any given streaming algorithm.

Fix some k ≥ 1, and consider the ground set Nk = {ui, vi}k

i=1. For ease of notation, let us deﬁne for

every subset S ⊆ Nk

u(S) = |S ∩ {ui}k

i=1| ,

v(S) = |S ∩ {vi}k

i=1| .

Now we deﬁne the following set function:

fk(S) = min{2 · u(S) + 1, 2 · v(S)} ∀ S ⊆ Nk .

Lemma 4.1. fk is nonnegative, monotone and 0.5-weakly submodular for the integer |Nk|.

Since |Nk| = 2k, the maximum value of fk is fk(Nk) = 2 · v(Nk) = 2k. We now extend the ground set
of fk by adding to it an arbitrary large number d of dummy elements which do not aﬀect fk at all. Clearly,
this does not aﬀect the properties of fk proved in Lemma 4.1. However, the introduction of dummy elements
allows us to assume that k is an arbitrary small value compared to N , which is necessary for the proof of
the next theorem. In a nutshell, this proof is based on the observation that the elements of {ui}k
i=1 are
indistinguishable from the dummy elements as long as no element of {vi}k

i=1 has arrived yet.

Theorem 4.2. For every constant c ∈ (0, 1] there is a large enough k such that no randomized streaming
algorithm that uses o(N ) memory to solve max|S|≤2k fk(S) has an approximation ratio of c for a worst case
stream order.

We note that fk has strong properties. In particular, Lemma 4.1 implies that it is 0.5-weakly submodular
for every 0 ≤ r ≤ |N |. In contrast, the algorithm we show later assumes weak submodularity only for the
cardinality constraint k. Thus, the above theorem implies that worst case stream order precludes a constant
approximation ratio even for functions with much stronger properties compared to what is necessary for
getting a constant approximation ratio when the order is random.

The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In other
words, once the algorithm decides to discard an element from its memory, this element is gone forever, which
is a standard assumption for streaming algorithms. Thus, the theorem does not apply to algorithms that
use multiple passes over N , or non-streaming algorithms that use o(N ) writable memory, and their analysis
remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model in
which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1 such a
streaming algorithm assuming access to a value τ which approximates aγ · f (OP T ), where a is a shorthand
2 − e−γ/2 − 1)/2. Then, in Section 5.2 we explain how this assumption can be removed to obtain
for a = (
Streak and bound its approximation ratio, space complexity, and running time.

√

5.1 Algorithm with access to τ

Consider Algorithm 1. In addition to the input instance, this algorithm gets a parameter τ ∈ [0, aγ ·f (OP T )].
One should think of τ as close to aγ · f (OP T ), although the following analysis of the algorithm does not
rely on it. We provide an outline of the proof, but defer the technical details to the Appendix.

Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

√

τ
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

(cid:112)

= τ · (

2 − e−γ/2 − 1) .

4

Algorithm 1 Threshold Greedy(f, k, τ )

Let S ← ∅.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S) ≥ τ /k then

Update S ← S ∪ {u}.

end if
end while
return: S

Proof (Sketch). Let E be the event that f (S) < τ , where S is the output produced by Algorithm 1. Clearly
f (S) ≥ τ whenever E does not occur, and thus, it is possible to lower bound the expected value of f (S)
using E as follows.

Observation 5.2. Let S denote the output of Algorithm 1, then E[f (S)] ≥ (1 − Pr[E]) · τ .

The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another lower
bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this proposition is
the next observation, which implies that the solution produced by Algorithm 1 is always of size smaller than
k when E happens.

Observation 5.3. If at some point Algorithm 1 has a set S of size k, then f (S) ≥ τ .

The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it is a
random element out of all the OP T elements that did not arrive yet.

Proposition 5.4. For the set S produced by Algorithm 1,

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:16)

·

1
2

The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the theorem
is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally, the former happens when Pr[E] ≤
2 − e−γ/2.
2 −

2 − e−γ/2 and the later when Pr[E] ≥ 2 −

√

√

5.2 Algorithm without access to τ

In this section we explain how to get an algorithm which does not depend on τ . Instead, Streak (Algo-
rithm 2) receives an accuracy parameter ε ∈ (0, 1). Then, it uses ε to run several instances of Algorithm 1
stored in a collection denoted by I. The algorithm maintains two variables throughout its execution: m is
the maximum value of a singleton set corresponding to an element that the algorithm already observed, and
um references an arbitrary element satisfying f (um) = m.

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for τ , and it no longer should contain such an instance, then the instance
is simply removed. In contrast, if I did not contain an instance of Algorithm 1 with a given value for τ ,
and it should now contain such an instance, then a new instance with this value for τ is created. Finally, if
I contained an instance of Algorithm 1 with a given value for τ , and it should continue to contain such an
instance, then this instance remains in I as is.

Theorem 5.5. The approximation ratio of Streak is at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

5

Algorithm 2 Streak(f, k, ε)

Let m ← 0, and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Let u be the next element.
if f (u) ≥ m then

Update m ← f (u) and um ← u.

end if
Update I so that it contains an instance of Algorithm 1 with τ = x for every x ∈ {(1 − ε)i | i ∈
Z and (1 − ε)m/(9k2) ≤ (1 − ε)i ≤ mk}, as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton set
{um}.

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1 whose τ
provides a good approximation for aγ · f (OP T ), and thus, this instance of Algorithm 1 should (up to some
technical details) produce a good output set in accordance with Theorem 5.1.

It remains to analyze the space complexity and running time of Streak. We concentrate on bounding
the number of elements Streak keeps in its memory at any given time, as this amount dominates the space
complexity as long as we assume that the space necessary to keep an element is at least as large as the space
necessary to keep each one of the numbers used by the algorithm.

Theorem 5.6. The space complexity of Streak is O(ε−1k log k) elements.

The running time of Algorithm 1 is O(N f ) where, abusing notation, f is the running time of a single
oracle evaluation of f . Therefore, the running time of Streak is O(N f ε−1 log k) since it uses at every given
time only O(ε−1 log k) instances of the former algorithm. Given multiple threads, this can be improved to
O(N f + ε−1 log k) by running the O(ε−1 log k) instances of Algorithm 1 in parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression, Phishing dataset with pairwise feature products. Our algorithm is comparable
to LocalSearch in both log likelihood and generalization accuracy, with much lower running time and
number of model ﬁts in most cases. Results averaged over 40 iterations, error bars show 1 standard deviation.

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression, Phishing dataset with pairwise feature products, k = 80 features.
By varying the parameter ε, our algorithm captures a time-accuracy tradeoﬀ between RandomSubset and
LocalSearch. Results averaged over 40 iterations, standard deviation shown with error bars. 2(b): Running
times of interpretability algorithms on the Inception V3 network, N = 30, k = 5. Streaming maximization
runs 10 times faster than the LIME framework. Results averaged over 40 total iterations using 8 example
explanations, error bars show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment, a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from the
Phishing dataset [Lichman, 2013]. This setup is known to be weakly submodular under mild data assumptions
[Elenberg et al., 2016a]. First, the categorical features are one-hot encoded, increasing the feature dimension
to 68. Then, all pairwise products are added for a total of N = 4692 features. To reduce computational
cost, feature products are generated and added to the stream on-the-ﬂy as needed. We compare with 2 other
algorithms. RandomSubset selects the ﬁrst k features from the random stream. LocalSearch ﬁrst ﬁlls a
buﬀer with the ﬁrst k features, and then swaps each incoming feature with the feature from the buﬀer which
yields the largest nonnegative improvement.

Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RandomSubset,
LocalSearch, and our Streak algorithm for ε = {0.75, 0.1} and k = {20, 40, 80}. As expected, the
RandomSubset algorithm has much larger variation since its performance depends highly on the random
stream order. It also performs signiﬁcantly worse than LocalSearch for both metrics, whereas Streak
is comparable for most parameter choices. Figure 1(b) shows two measures of computational cost: running
time and the number of oracle evaluations (regression ﬁts). We note Streak scales better as k increases;
for example, Streak with k = 80 and ε = 0.1 (ε = 0.75) runs in about 70% (5%) of the time it takes to run
LocalSearch with k = 40. Interestingly, our speedups are more substantial with respect to running time.
In some cases Streak actually ﬁts more regressions than LocalSearch, but still manages to be faster. We
attribute this to the fact that nearly all of LocalSearch’s regressions involve k features, which are slower
than many of the small regressions called by Streak.

Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and ε ∈ [0.05, 0.75]. By varying
the precision ε, we achieve a gradual tradeoﬀ between speed and performance. This shows that Streak can
reduce the running time by over an order of magnitude with minimal impact on the ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally, we
begin with the Inception V3 deep neural network [Szegedy et al., 2016] trained on ImageNet. We use this
network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by adding a ﬁnal
softmax layer and retraining the network.

We compare our approach to the LIME framework [Ribeiro et al., 2016] for developing sparse, inter-
pretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of interpretable
features. Here, the features are superpixels determined by the SLIC image segmentation algorithm [Achanta

7

et al., 2012] (regions from any other segmentation would also suﬃce). The number of superpixels is bounded
by N = 30. After a feature selection step, a ﬁnal regression is performed on only the selected features. The
following feature selection methods are supplied by LIME: 1. Highest Weights: ﬁts a full regression and keep
the k features with largest coeﬃcients. 2. Forward Selection: standard greedy forward selection. 3. Lasso:
(cid:96)1 regularization.

We introduce a novel method for black-box interpretability that is similar to but simpler than LIME. As
before, we segment an image into N superpixels. Then, for a subset S of those regions we can create a new
image that contains only these regions and feed this into the black-box classiﬁer. For a given model M , an
input image I, and a label L1 we ask for an explanation: why did model M label image I with label L1.
We propose the following solution to this problem. Consider the set function f (S) giving the likelihood that
image I(S) has label L1. We approximately solve

max
|S|≤k

f (S) ,

using Streak. Intuitively, we are limiting the number of superpixels to k so that the output will include only
the most important superpixels, and thus, will represent an interpretable explanation. In our experiments
we set k = 5.

Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor sub-
modular in general. Still, we ﬁnd that the greedy maximization algorithm produces very good explanations
for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the Appendix. Figure 2(b)
shows that our algorithm is much faster than the LIME approach. This is primarily because LIME relies on
generating and classifying a large set of randomly perturbed example images.

7 Conclusions

We propose Streak, the ﬁrst streaming algorithm for maximizing weakly submodular functions, and prove
that it achieves a constant factor approximation assuming a random stream order. This is useful when the
set function is not submodular and, additionally, takes a long time to evaluate or has a very large ground set.
Conversely, we show that under a worst case stream order no algorithm with memory sublinear in the ground
set size has a constant factor approximation. We formulate interpretability of black-box neural networks as
set function maximization, and show that Streak provides interpretable explanations faster than previous
approaches. We also show experimentally that Streak trades oﬀ accuracy and running time in nonlinear
sparse regression.

One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5, which are
nontrivial but somewhat loose. For example, there is a gap between the theoretical guarantee of the state-
of-the-art algorithm for submodular functions and our bound for γ = 1. However, as our algorithm performs
the same computation as that state-of-the-art algorithm when the function is submodular, this gap is solely
an analysis issue. Hence, the real theoretical performance of our algorithm is better than what we have been
able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award
(D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We have
used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these four input
images the ﬂower types were correctly classiﬁed (from (a) to (d): rose, sunﬂower, daisy, and daisy). We ask
the question of interpretability: why did this model classify this image as rose. We are using our framework
(and the recent prior work LIME [Ribeiro et al., 2016]) to see which parts of the image the neural network
is looking at for these classiﬁcation tasks. As can be seen Streak correctly identiﬁes the ﬂower parts of
the images while some LIME variations do not. More importantly, Streak is creating subsampled images
on-the-ﬂy, and hence, runs approximately 10 times faster. Since interpretability tasks perform multiple calls
to the black-box model, the running times can be quite signiﬁcant.

9

References

Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. SLIC
Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 34(11):2274–2282, 2012.

Jason Altschuler, Aditya Bhaskara, Gang (Thomas) Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza
Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed Algorithms. In ICML,
pages 2539–2548, 2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations

and Trends in Machine Learning, 6, 2013.

Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming Sub-

modular Maximization: Massive Data Summarization on the Fly. In KDD, pages 671–680, 2014.

Sohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization. Journal

of Machine Learning Research, 14:807–841, 2013.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The Power of Randomization:

Distributed Submodular Maximization on Massive Datasets. In ICML, pages 1236–1244, 2015.

Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. A New Framework for Distributed

Submodular Maximization. In FOCS, pages 645–654, 2016.

Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed Non-
convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS, pages 111–120,
2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization Problems. In

SODA, pages 392–403, 2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric Tech-

nique. CoRR, abs/1611.03253, 2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online Submodular Maximization with Preemption.

In SODA, pages 1202–1216, 2015.

Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a Monotone Submodular

Function Subject to a Matroid Constraint. SIAM J. Comput., 40(6):1740–1766, 2011.

T-H. Hubert Chan, Zhiyi Huang, Shaofeng H.-C. Jiang, Ning Kang, and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In SODA,
pages 1204–1223, 2017.

Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming Algorithms for Submodular Function

Maximization. In ICALP, pages 318–330, 2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions, matroids and the greedy algorithm:
Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Math-
ematics, 7(3):251–274, March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset Selection,

Sparse Approximation and Dictionary Selection. In ICML, pages 1057–1064, 2011.

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. CoRR, abs/1612.00804, 2016a. URL http://arxiv.org/abs/
1612.00804.

10

Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand Negahban. Restricted Strong Con-
vexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with Structure,
2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM), 45(4):634–652,

1998.

2017.

Marshall L. Fisher, George L. Nemhauser, and Laurence A. Wolsey. An analysis of approximations for
maximizing submodular set functions–II. In M. L. Balinski and A. J. Hoﬀman, editors, Polyhedral Com-
binatorics: Dedicated to the memory of D.R. Fulkerson, pages 73–87. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT, pages 1069–1122,

Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch Mode Active Learning and its Application

to Medical Image Classiﬁcation. In ICML, pages 417–424, 2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS, 2016.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Joydeep Ghosh, and Sahand Negahban. On

Approximation Guarantees for Greedy Low Rank Optimization. In ICML, pages 1837–1846, 2017a.

Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand Negahban, and Joydeep Ghosh. Scalable

Greedy Support Selection via Weak Submodularity. In AISTATS, pages 1560–1568, 2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In ICML,

pages 567–574, 2010.

to Hard Problems, 3:71–104, 2014.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical Approaches

Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.

Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed Submodular Maxi-

mization: Identifying Representative Elements in Massive Data. NIPS, pages 2049–2057, 2013.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas Krause.

Lazier Than Lazy Greedy. In AAAI, pages 1812–1818, 2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum of a

Submodular Set Function. Math. Oper. Res., 3(3):177–188, August 1978.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for

maximizing submodular set functions–I. Mathematical Programming, 14(1):265–294, 1978.

Xinghao Pan, Stefanie Jegelka, Joseph E. Gonzalez, Joseph K. Bradley, and Michael I. Jordan. Parallel

Double Greedy Submodular Maximization. In NIPS, pages 118–126, 2014.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.

“Why Should I Trust You?” Explaining the

Predictions of Any Classiﬁer. In KDD, pages 1135–1144, 2016.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In ICML,

pages 3319–3328, 2017.

Maxim Sviridenko, Jan Vondrák, and Justin Ward. Optimal approximation for submodular and supermod-

ular optimization with bounded curvature. In SODA, pages 1134–1148, 2015.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the

Inception Architecture for Computer Vision. In CVPR, pages 2818–2826, 2016.

11

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23, pages

253–266, 2010.

ICML, pages 1954–1963, 2015.

Kai Wei, Iyer Rishabh, and Jeﬀ Bilmes. Submodularity in Data Subset Selection and Active Learning.

Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, and Tong Zhang. Sparse Nonlinear Regression:

Parameter Estimation and Asymptotic Inference. ICML, pages 2472–2481, 2016.

12

A Appendix

A.1 Proof of Lemma 4.1

The nonnegativity and monotonicity of fk follow immediately from the fact that u(S) and v(S) have these
properties. Thus, it remains to prove that fk is 0.5-weakly submodular for |Nk|, i.e., that for every pair of
arbitrary sets S, L ⊆ Nk it holds that

(cid:88)

w∈S\L

fk(w | L) ≥ 0.5 · fk(S | L) .

There are two cases to consider. The ﬁrst case is that fk(L) = 2 · u(L) + 1. In this case S \ L must contain
at least (cid:100)fk(S | L)/2(cid:101) elements of {ui}k
i=1. Additionally, the marginal contribution to L of every element of
{ui}k

i=1 which does not belong to L is at least 1. Thus, we get

The second case is that fk(L) = 2 · v(L). In this case S \ L must contain at least (cid:100)fk(S | L)/2(cid:101) elements of
{vi}k
i=1 which does not belong
to L is at least 1. Thus, we get in this case again

i=1, and in addition, the marginal contribution to L of every element of {vi}k

(cid:88)

w∈S\L

(cid:88)

w∈S\L

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {ui}k

i=1|

w∈(S\L)∩{ui}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

fk(w | L) ≥

(cid:88)

fk(w | L) ≥ |(S \ L) ∩ {vi}k

i=1|

w∈(S\L)∩{vi}k

i=1

≥ (cid:100)fk(S | L)/2(cid:101) ≥ 0.5 · fk(S | L) .

A.2 Proof of Theorem 4.2
Consider an arbitrary (randomized) streaming algorithm ALG aiming to maximize fk(S) subject to the
cardinality constraint |S| ≤ 2k. Since ALG uses o(N ) memory, we can guarantee, by choosing a large
enough d, that ALG uses no more than (c/4) · N memory. In order to show that ALG performs poorly,
consider the case that it gets ﬁrst the elements of {ui}k
i=1 and the dummy elements (in some order to be
determined later), and only then it gets the elements of {vi}k
i=1. The next lemma shows that some order of
the elements of {ui}k

i=1 and the dummy elements is bad for ALG.

Lemma A.1. There is an order for the elements of {ui}k
in expectation ALG returns at most (c/2) · k elements of {ui}k

i=1.

i=1 and the dummy elements which guarantees that

Proof. Let W be the set of the elements of {ui}k
i=1 and the dummy elements. Observe that the value of fk
for every subset of W is 0. Thus, ALG has no way to diﬀerentiate between the elements of W until it views
i=1, which implies that the probability of every element w ∈ W to remain in ALG’s
the ﬁrst element of {vi}k
memory until the moment that the ﬁrst element of {vi}k
i=1 arrives is determined only by w’s arrival position.
Hence, by choosing an appropriate arrival order one can guarantee that the sum of the probabilities of the
elements of {ui}k

i=1 to be at the memory of ALG at this point is at most

kM
|W |

≤

k(c/4) · N
k + d

=

k(c/4) · (2k + d)
k + d

≤

kc
2

,

where M is the amount of memory ALG uses.

The expected value of the solution produced by ALG for the stream order provided by Lemma A.1 is at

most ck + 1. Hence, its approximation ratio for k > 1/c is at most

ck + 1
2k

=

+

< c .

c
2

1
2k

13

A.3 Proof of Observation 5.3

Algorithm 1 adds an element u to the set S only when the marginal contribution of u with respect to S is
at least τ /k. Thus, it is always true that

f (S) ≥

τ · |S|
k

.

A.4 Proof of Proposition 5.4
We begin by proving several intermediate lemmas. Recall that γ (cid:44) γk, and notice that by the monotonicity
of f we may assume that OP T is of size k. For every 0 ≤ i ≤ |OP T | = k, let OP Ti be the random set
consisting of the last i elements of OP T according to the input order. Note that OP Ti is simply a uniformly
random subset of OP T of size i. Thus, we can lower bound its expected value as follows.
Lemma A.2. For every 0 ≤ i ≤ k, E[f (OP Ti)] ≥ [1 − (1 − γ/k)i] · f (OP T ).
Proof. We prove the lemma by induction on i. For i = 0 the lemma follows from the nonnegativity of f
since

f (OP T0) ≥ 0 = [1 − (1 − γ/k)0] · f (OP T ) .

Assume now that the lemma holds for some 0 ≤ i − 1 < k, and let us prove it holds also for i. Since
OP Ti−1 is a uniformly random subset of OP T of size i − 1, and OP Ti is a uniformly random subset of OP T
of size i, we can think of OP Ti as obtained from OP Ti−1 by adding to this set a uniformly random element
of OP T \ OP Ti−1. Taking this point of view, we get, for every set T ⊆ OP T of size i − 1,

E[f (OP Ti) | OP Ti−1 = T ] = f (T ) +

(cid:80)

u∈OP T \T f (u | T )
|OP T \ T |

1
k

γ
k
(cid:17)

≥ f (T ) +

·

f (u | T )

(cid:88)

u∈OP T \T

≥ f (T ) +

· f (OP T \ T | T )

(cid:16)

=

1 −

γ
k

γ
k

· f (T ) +

· f (OP T ) ,

where the last inequality holds by the γ-weak submodularity of f . Taking expectation over the set OP Ti−1,
the last inequality becomes

E[f (OP Ti)] ≥

1 −

· f (OP T )

≥

1 −

· f (OP T ) +

· f (OP T )

γ
k

(cid:17)

(cid:17)

E[f (OP Ti−1)] +
(cid:20)

γ
k
(cid:17)i−1(cid:21)

(cid:16)

·

1 −

1 −

γ
k

(cid:16)

(cid:16)

(cid:20)

γ
k
γ
k
(cid:16)

=

1 −

1 −

· f (OP T ) ,

(cid:17)i(cid:21)

γ
k

where the second inequality follows from the induction hypothesis.

Let us now denote by o1, o2, . . . , ok the k elements of OP T in the order in which they arrive, and, for every
1 ≤ i ≤ k, let Si be the set S of Algorithm 1 immediately before the algorithm receives oi. Additionally,
let Ai be an event ﬁxing the arrival time of oi, the set of elements arriving before oi and the order in which
they arrive. Note that conditioned on Ai, the sets Si and OP Tk−i+1 are both deterministic.
Lemma A.3. For every 1 ≤ i ≤ k and event Ai, E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)], where
OP Tk−i+1 and Si represent the deterministic values these sets take given Ai.

Proof. By the monotonicity and γ-weak submodularity of f , we get

(cid:88)

u∈OP Tk−i+1

f (u | Si) ≥ γ · f (OP Tk−i+1 | Si)

= γ · [f (OP Tk−i+1 ∪ Si) − f (Si)]
≥ γ · [f (OP Tk−i+1) − f (Si)] .

14

Since oi is a uniformly random element of OP Tk−i+1, even conditioned on Ai, the last inequality implies

E[f (oi | Si) | Ai] =

(cid:80)

(cid:80)

u∈OP Tk−i+1

f (u | Si)

k − i + 1

u∈OP Tk−i+1

f (u | Si)

k

γ · [f (OP Tk−i+1) − f (Si)]
k

.

≥

≥

Let ∆i be the increase in the value of S in the iteration of Algorithm 1 in which it gets oi.

Lemma A.4. Fix 1 ≤ i ≤ k and event Ai, and let OP Tk−i+1 and Si represent the deterministic values
these sets take given Ai. If f (Si) < τ , then E[∆i | Ai] ≥ [γ · f (OP Tk−i+1) − 2τ ]/k.

Proof. Notice that by Observation 5.3 the fact that f (Si) < τ implies that Si contains less than k elements.
Thus, conditioned on Ai, Algorithm 1 adds oi to S whenever f (oi | Si) ≥ τ /k, which means that

One implication of the last equality is

(cid:40)

∆i =

f (oi | Si)
0

if f (oi | Si) ≥ τ /k ,
otherwise .

E[∆i | Ai] ≥ E[f (oi | Si) | Ai] − τ /k ,

which intuitively means that the contribution to E[f (oi | Si) | Ai] of values of f (oi | Si) which are too small
to make the algorithm add oi to S is at most τ /k. The lemma now follows by observing that Lemma A.3
and the fact that f (Si) < τ guarantee

E[f (oi | Si) | Ai] ≥ (γ/k) · [f (OP Tk−i+1) − f (Si)]

> (γ/k) · [f (OP Tk−i+1) − τ ]
≥ [γ · f (OP Tk−i+1) − τ ]/k .

We are now ready to put everything together and get a lower bound on E[∆i].

Lemma A.5. For every 1 ≤ i ≤ k,

E[∆i] ≥

γ · [Pr[E] − (1 − γ/k)k−i+1] · f (OP T ) − 2τ
k

.

Proof. Let Ei be the event that f (Si) < τ . Clearly Ei is the disjoint union of the events Ai which imply
f (Si) < τ , and thus, by Lemma A.4,

E[∆i | Ei] ≥ [γ · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

Note that ∆i is always nonnegative due to the monotonicity of f . Thus,

E[∆i] = Pr[Ei] · E[∆i | Ei] + Pr[ ¯Ei] · E[∆i | ¯Ei] ≥ Pr[Ei] · E[∆i | Ei]

≥ [γ · Pr[Ei] · E[f (OP Tk−i+1) | Ei] − 2τ ]/k .

It now remains to lower bound the expression Pr[Ei] · E[f (OP Tk−i+1) | Ei] on the rightmost hand side of

the last inequality.

Pr[Ei] · E[f (OP Tk−i+1) | Ei] = E[f (OP Tk−i+1)] − Pr[ ¯Ei] · E[f (OP Tk−i+1) | ¯Ei]

≥ [1 − (1 − γ/k)k−i+1 − (1 − Pr[Ei])] · f (OP T )
≥ [Pr[E] − (1 − γ/k)k−i+1] · f (OP T )

where the ﬁrst inequality follows from Lemma A.2 and the monotonicity of f , and the second inequality
holds since E implies Ei which means that Pr[Ei] ≥ Pr[E] for every 1 ≤ i ≤ k.

15

Proposition 5.4 follows quite easily from the last lemma.

Proof of Proposition 5.4. Lemma A.5 implies, for every 1 ≤ i ≤ (cid:100)k/2(cid:101),

E[∆i] ≥

f (OP T )[Pr[E] − (1 − γ/k)k−(cid:100)k/2(cid:101)+1] −

γ
k
γ
k
(cid:16)

≥

≥

f (OP T )[Pr[E] − (1 − γ/k)k/2] −

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

/k .

2τ
k

2τ
k
(cid:17)

The deﬁnition of ∆i and the monotonicity of f imply together

E[f (S)] ≥

E[∆i]

b
(cid:88)

i=1

for every integer 1 ≤ b ≤ k. In particular, for b = (cid:100)k/2(cid:101), we get

E[f (S)] ≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

≥

γ · [Pr[E] − e−γ/2] · f (OP T ) − 2τ

.

(cid:17)

(cid:17)

(cid:16)

(cid:16)

·

·

b
k
1
2

A.5 Proof of Theorem 5.1

In this section we combine the previous results to prove Theorem 5.1. Recall that Observation 5.2 and
Proposition 5.4 give two lower bounds on E[f (S)] that depend on Pr[E]. The following lemmata use these
lower bounds to derive another lower bound on this quantity which is independent of Pr[E]. For ease of the
reading, we use in this section the shorthand γ(cid:48) = e−γ/2.

Lemma A.6. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≥ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Proposition 5.4,

E[f (S)] ≥

1
2
1
2
1
2
τ
2a
τ
a

·

≥

=

≥

=

(cid:104)

(cid:110)

· {γ · [Pr[E] − γ(cid:48)] · f (OP T ) − 2τ }
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48)(cid:105)

γ ·

γ ·

(cid:110)

(cid:104)

·

·

· f (OP T ) − 2τ

(cid:111)

· f (OP T ) − ((cid:112)2 − γ(cid:48) − 1) ·

(cid:111)

τ
a

(cid:110)
2 − (cid:112)2 − γ(cid:48) − γ(cid:48) − (cid:112)2 − γ(cid:48) + 1

(cid:111)

·

√

2 − γ(cid:48)

,

3 − γ(cid:48) − 2
2
√

where the ﬁrst equality holds since a = (
τ .

2 − γ(cid:48) − 1)/2, and the last inequality holds since aγ · f (OP T ) ≥

Lemma A.7. E[f (S)] ≥ τ

2a (3 − γ(cid:48) − 2

2 − γ(cid:48)) = τ

√

√
a · 3−e−γ/2−2

2

2−e−γ/2

whenever Pr[E] ≤ 2 −

2 − γ(cid:48).

√

Proof. By the lower bound given by Observation 5.2,

E[f (S)] ≥ (1 − Pr[E]) · τ ≥

(cid:16)(cid:112)2 − γ(cid:48) − 1

(cid:17)

·

=

(cid:16)

1 − 2 + (cid:112)2 − γ(cid:48)
√

(cid:17)

2 − γ(cid:48) − 1
2

·

τ
a

=

· τ

3 − γ(cid:48) − 2
2

√

2 − γ(cid:48)

·

τ
a

.

Combining Lemmata A.6 and A.7 we get the theorem.

16

A.6 Proof of Theorem 5.5

There are two cases to consider. If γ < 4/3 · k−1, then we use the following simple observation.

Observation A.8. The ﬁnal value of the variable m is f max (cid:44) max{f (u) | u ∈ N } ≥ γ

k · f (OP T ).

Proof. The way m is updated by Algorithm 2 guarantees that its ﬁnal value is f max. To see why the other
part of the observation is also true, note that the γ-weak submodularity of f implies

f max ≥ max{f (u) | u ∈ OP T } = f (∅) + max{f (u | ∅) | u ∈ OP T }

≥ f (∅) +

f (u | ∅) ≥ f (∅) +

f (OP T | ∅) ≥

· f (OP T ) .

γ
k

γ
k

1
k

(cid:88)

u∈OP T

By Observation A.8, the value of the solution produced by Streak is at least

f (um) = m ≥

· f (OP T ) ≥

· f (OP T )

γ
k

3γ2
4

≥ (1 − ε)γ ·

· f (OP T )

3(γ/2)
2

≥ (1 − ε)γ ·

≥ (1 − ε)γ ·

3 − 3e−γ/2
2

3 − e−γ/2 − 2
2

· f (OP T )

√

2 − e−γ/2

· f (OP T ) ,

where the second to last inequality holds since 1−γ/2 ≤ e−γ/2, and the last inequality holds since e−γ+e−γ/2 ≤
2.

It remains to consider the case γ ≥ 4/3 · k−1, which has a somewhat more involved proof. Observe that
the approximation ratio of Streak is 1 whenever f (OP T ) = 0 because the value of any set, including the
output set of the algorithm, is nonnegative. Thus, we can safely assume in the rest of the analysis of the
approximation ratio of Algorithm 2 that f (OP T ) > 0.

Let τ ∗ be the maximal value in the set {(1−ε)i | i ∈ Z} which is not larger than aγ ·f (OP T ). Note that τ ∗
exists by our assumption that f (OP T ) > 0. Moreover, we also have (1−ε)·aγ ·f (OP T ) < τ ∗ ≤ aγ ·f (OP T ).
The following lemma gives an interesting property of τ ∗. To understand the lemma, it is important to note
that the set of values for τ in the instances of Algorithm 1 appearing in the ﬁnal collection I is deterministic
because the ﬁnal value of m is always f max.

Lemma A.9. If there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak terminates, then in
expectation Streak has an approximation ratio of at least

(1 − ε)γ ·

3 − e−γ/2 − 2
2

√

2 − e−γ/2

.

Proof. Consider a value of τ for which there is an instance of Algorithm 1 in I when Algorithm 2 terminates,
and consider the moment that Algorithm 2 created this instance. Since the instance was not created earlier,
we get that m was smaller than τ /k before this point. In other words, the marginal contribution of every
element that appeared before this point to the empty set was less than τ /k. Thus, even if the instance had
been created earlier it would not have taken any previous elements.

An important corollary of the above observation is that the output of every instance of Algorithm 1 that
appears in I when Streak terminates is equal to the output it would have had if it had been executed on
the entire input stream from its beginning (rather than just from the point in which it was created). Since we
assume that there is an instance of Algorithm 1 with τ = τ ∗ in the ﬁnal collection I, we get by Theorem 5.1
that the expected value of the output of this instance is at least
√

√

τ ∗
a

·

3 − e−γ/2 − 2
2

2 − e−γ/2

> (1 − ε)γ · f (OP T ) ·

3 − e−γ/2 − 2
2

2 − e−γ/2

.

The lemma now follows since the output of Streak is always at least as good as the output of each one of
the instances of Algorithm 1 in its collection I.

17

We complement the last lemma with the next one.

Lemma A.10. If γ ≥ 4/3 · k−1, then there is an instance of Algorithm 1 with τ = τ ∗ in I when Streak
terminates.

Proof. We begin by bounding the ﬁnal value of m. By Observation A.8 this ﬁnal value is f max ≥ γ
k ·f (OP T ).
On the other hand, f (u) ≤ f (OP T ) for every element u ∈ N since {u} is a possible candidate to be OP T ,
which implies f max ≤ f (OP T ). Thus, the ﬁnal collection I contains an instance of Algorithm 1 for every
value of τ within the set

(cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f max/(9k2) ≤ (1 − ε)i ≤ f max · k(cid:9)

⊇ (cid:8)(1 − ε)i | i ∈ Z and (1 − ε) · f (OP T )/(9k2) ≤ (1 − ε)i ≤ γ · f (OP T )(cid:9) .

To see that τ ∗ belongs to the last set, we need to verify that it obeys the two inequalities deﬁning this set.
On the one hand, a = (

2 − e−γ/2 − 1)/2 < 1 implies

√

On the other hand, γ ≥ 4/3 · k−1 and 1 − e−γ/2 ≥ γ/2 − γ2/8 imply

τ ∗ ≤ aγ · f (OP T ) ≤ γ · f (OP T ) .

τ ∗ > (1 − ε) · aγ · f (OP T ) = (1 − ε) · (

2 − e−γ/2 − 1) · γ · f (OP T )/2

(cid:112)

≥ (1 − ε) · ((cid:112)1 + γ/2 − γ2/8 − 1) · γ · f (OP T )/2
≥ (1 − ε) · ((cid:112)1 + γ/4 + γ2/64 − 1) · γ · f (OP T )/2
= (1 − ε) · ((cid:112)(1 + γ/8)2 − 1) · γ · f (OP T )/2 ≥ (1 − ε) · γ2 · f (OP T )/16
≥ (1 − ε) · f (OP T )/(9k2) .

Combining Lemmata A.9 and A.10 we get the desired guarantee on the approximation ratio of Streak.

A.7 Proof of Theorem 5.6
Observe that Streak keeps only one element (um) in addition to the elements maintained by the instances
of Algorithm 1 in I. Moreover, Algorithm 1 keeps at any given time at most O(k) elements since the set S it
maintains can never contain more than k elements. Thus, it is enough to show that the collection I contains
at every given time at most O(ε−1 log k) instances of Algorithm 1. If m = 0 then this is trivial since I = ∅.
Thus, it is enough to consider the case m > 0. Note that in this case

We now need to upper bound ln(1 − ε). Recall that 1 − ε ≤ e−ε. Thus, ln(1 − ε) ≤ −ε. Plugging this into
the previous inequality gives

|I| ≤ 1 − log1−ε

mk
(1 − ε)m/(9k2)

= 2 −

ln(9k3)
ln(1 − ε)

= 2 −

ln 9 + 3 ln k
ln(1 − ε)

= 2 −

O(ln k)
ln(1 − ε)

.

|I| ≤ 2 −

= 2 + O(ε−1 ln k) = O(ε−1 ln k) .

O(ln k)
−ε

18

A.8 Additional Experiments

(a)

(b)

(c)

(d)

Figure 4: In addition to the experiment in Section 6.2, we also replaced LIME’s default feature selection
algorithms with Streak and then ﬁt the same sparse regression on the selected superpixels. This method
is captioned “LIME + Streak.” Since LIME ﬁts a series of nested regression models, the corresponding
set function is guaranteed to be monotone, but is not necessarily submodular. We see that results look
qualitatively similar and are in some instances better than the default methods. However, the running time
of this approach is similar to the other LIME algorithms.

19

(a)

(b)

Figure 5: Here we used the same setup described in Figure 4, but compared explanations for predicting 2
diﬀerent classes for the same base image: 5(a) the highest likelihood label (sunﬂower) and 5(b) the second-
highest likelihood label (rose). All algorithms perform similarly for the sunﬂower label, but our algorithms
identify the most rose-like parts of the image.

20

