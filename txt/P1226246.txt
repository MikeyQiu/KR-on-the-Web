Knowledge Graph Embedding with Iterative Guidance from Soft Rules

Shu Guo1,2, Quan Wang1,2,3

∗, Lihong Wang4, Bin Wang1,2, Li Guo1,2

1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3State Key Laboratory of Information Security, Chinese Academy of Sciences
4National Computer Network Emergency Response Technical Team & Coordination Center of China

7
1
0
2
 
v
o
N
 
0
3
 
 
]
I

A
.
s
c
[
 
 
1
v
1
3
2
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Embedding knowledge graphs (KGs) into continuous vector
spaces is a focus of current research. Combining such an em-
bedding model with logic rules has recently attracted increas-
ing attention. Most previous attempts made a one-time injec-
tion of logic rules, ignoring the interactive nature between
embedding learning and logical inference. And they focused
only on hard rules, which always hold with no exception and
usually require extensive manual effort to create or validate.
In this paper, we propose Rule-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from 1) labeled triples that have been directly
observed in a given KG, 2) unlabeled triples whose labels are
going to be predicted iteratively, and 3) soft rules with vari-
ous conﬁdence levels extracted automatically from the KG. In
the learning process, RUGE iteratively queries rules to obtain
soft labels for unlabeled triples, and integrates such newly la-
beled triples to update the embedding model. Through this
iterative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings. We evalu-
ate RUGE in link prediction on Freebase and YAGO. Exper-
imental results show that: 1) with rule knowledge injected it-
eratively, RUGE achieves signiﬁcant and consistent improve-
ments over state-of-the-art baselines; and 2) despite their un-
certainties, automatically extracted soft rules are highly bene-
ﬁcial to KG embedding, even those with moderate conﬁdence
levels. The code and data used for this paper can be obtained
from https://github.com/iieir-km/RUGE.

Introduction
Knowledge graphs (KGs) such as WordNet (Miller 1995),
Freebase (Bollacker et al. 2008), YAGO (Suchanek, Kas-
neci, and Weikum 2007), and NELL (Carlson et al. 2010)
are extremely useful resources for many AI related applica-
tions. A KG is a multi-relational graph composed of entities
as nodes and relations as different types of edges. Each edge
is represented as a triple (head entity, relation, tail entity),
indicating that there is a speciﬁc relation between two enti-
ties, e.g., (Paris, CapitalOf, France). Although effective
in representing structured data, the underlying symbolic na-
ture of such triples often makes KGs hard to manipulate.

∗Corresponding author: Quan Wang (wangquan@iie.ac.cn).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recently, a new research direction termed as knowledge
graph embedding has been proposed and quickly received
massive attention (Nickel, Tresp, and Kriegel 2011; Bordes
et al. 2013; Wang et al. 2014; Lin et al. 2015b; Yang et al.
2015; Nickel, Rosasco, and Poggio 2016; Trouillon et al.
2016). The key idea is to embed entities and relations in a
KG into a low-dimensional continuous vector space, so as
to simplify the manipulation while preserving the inherent
structure of the KG. Such embeddings contain rich semantic
information, and can beneﬁt a broad range of downstream
applications (Weston et al. 2013; Bordes et al. 2014; Zhang
et al. 2016; Xiong, Power, and Callan 2017).

Traditional methods performed embedding based solely
on triples observed in a KG. But considering the power of
logic rules in knowledge acquisition and inference, combin-
ing embedding models with logic rules has become a focus
of current research (Rockt¨aschel et al. 2014; Vendrov et al.
2015; Wang and Cohen 2016; Hu et al. 2016). Wang et al.
(2015) and Wei et al. (2015) tried to use embedding models
and logic rules for KG completion. But in their work, rules
are modeled separately from embedding models, and would
not help to learn more predictive embeddings. Rockt¨aschel
et al. (2015) and Guo et al. (2016) then devised joint learning
paradigms which can inject ﬁrst-order logic (FOL) into KG
embedding. Demeester et al. (2016) further proposed lifted
rule injection to avoid the costly propositionalization of FOL
rules. Although these joint models are able to learn better
embeddings after integrating logic rules, they still have their
drawbacks and restrictions.

First of all, these joint models made a one-time injection
of logic rules, taking them as additional rule-based training
instances (Rockt¨aschel, Singh, and Riedel 2015) or regular-
ization terms (Demeester, Rockt¨aschel, and Riedel 2016).
We argue that rules can better enhance KG embedding, how-
ever, in an iterative manner. Given the learned embeddings
and their rough predictions, rules can be used to reﬁne the
predictions and infer new facts. The newly inferred facts, in
turn, will help to learn better embeddings and more accurate
logical inference. Previous methods fail to model such in-
teractions between embedding models and logic rules. Fur-
thermore, they focused only on hard rules which always hold
with no exception. Such rules usually require extensive man-
ual effort to create or validate. Actually, besides hard rules,
a signiﬁcant amount of background information can be en-

proach is quite generic and ﬂexible. It can integrate various
types of rules with different conﬁdence levels to enhance a
good variety of KG embedding models.

Related Work
Recent years have witnessed increasing interest in learning
distributed representations for entities and relations in KGs,
a.k.a. KG embedding. Various techniques have been devised
for this task, e.g., translation-based models which take re-
lations as translating operations between head and tail enti-
ties (Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015b),
simple compositional models which match compositions of
head-tail entity pairs with their relations (Nickel, Tresp, and
Kriegel 2011; Yang et al. 2015; Nickel, Rosasco, and Pog-
gio 2016; Trouillon et al. 2016), and neural networks which
further introduce non-linear layers and deep architectures
(Socher et al. 2013; Bordes et al. 2014; Dong et al. 2014;
Liu et al. 2016). Among these techniques, ComplEx (Trouil-
lon et al. 2016), a compositional model which represents
entities and relations as complex-valued vectors, achieves a
very good trade-off between accuracy and efﬁciency. Most
of the currently available techniques perform the embedding
task based solely on triples observed in a KG. Some recent
work further tried to use other information, e.g., entity types
(Guo et al. 2015; Xie, Liu, and Sun 2016) and textual de-
scriptions (Xie et al. 2016; Xiao, Huang, and Zhu 2017), to
learn more predictive embeddings. See (Wang et al. 2017)
for a thorough review of KG embedding techniques.

Given the power of logic rules in knowledge acquisition
and inference, combining KG embedding with logic rules
becomes a focus of current research. Wang et al. (2015) and
Wei et al. (2015) devised pipelined frameworks which use
logic rules to further reﬁne predictions made by embedding
models. In their work, rules will not help to learn better em-
beddings. Rockt¨aschel et al. (2015) and Guo et al. (2016)
then tried to learn KG embeddings jointly from triples and
propositionalized FOL rules. Demeester et al. (2016) further
proposed lifted rule injection to avoid the costly proposi-
tionalization. These joint models, however, made a one-time
injection of logic rules, ignoring the interactive nature be-
tween embedding learning and logical inference. Moreover,
they can only handle hard rules which are usually manually
created or validated.

Besides logic rules, relation paths which can be regarded
as Horn clauses and get a strong connection to logical infer-
ence (Gardner, Talukdar, and Mitchell 2015), have also been
studied in KG embedding (Neelakantan, Roth, and McCal-
lum 2015; Lin et al. 2015a; Guu, Miller, and Liang 2015).
But in these methods, relation paths are incorporated, again,
in a one-time manner. Our approach, in contrast, iteratively
injects knowledge contained in logic rules into KG embed-
ding, and is able to handle soft rules with various conﬁdence
levels extracted automatically from KGs.

Combining logic rules with distributed representations is
also an active research topic in other contexts outside KGs.
Faruqui et al. (2014) tried to inject ontological knowledge
from WordNet into word embeddings. Vendrov et al. (2015)
introduced order-embedding to model the partial order struc-
ture of hypernymy, textual entailment, and image caption-

Figure 1: Framework overview. RUGE enables an embed-
ding model to learn simultaneously from labeled triples, un-
labeled triples, and soft rules in an iterative manner, where
each iteration alternates between a soft label prediction stage
and an embedding rectiﬁcation stage.

coded as soft rules, e.g., “a person is very likely (but not nec-
essarily) to have a nationality of the country where he/she
was born”. Soft rules can be extracted automatically and ef-
ﬁciently via modern rule mining systems (Gal´arraga et al.
2013; Gal´arraga et al. 2015). Yet, despite this merit, soft
rules have not been well studied in previous methods.

This paper proposes RUle-Guided Embedding (RUGE), a
novel paradigm of KG embedding with iterative guidance
from soft rules. As sketched in Fig. 1, it enables an embed-
ding model to learn simultaneously from 1) labeled triples
that have been directly observed in a given KG, 2) unlabeled
triples whose labels are going to be predicted iteratively, and
3) soft rules with different conﬁdence levels extracted auto-
matically from the KG. During each iteration of the learning
process, the model alternates between a soft label prediction
stage and an embedding rectiﬁcation stage. The former uses
currently learned embeddings and soft rules to predict soft
labels for unlabeled triples, and the latter further integrates
both labeled and unlabeled triples (with hard and soft labels
respectively) to update current embeddings. Through this it-
erative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings.

We empirically evaluate RUGE on large scale public KGs,
namely Freebase and YAGO. Experimental results reveal
that: 1) by incorporating logic rules, RUGE signiﬁcantly and
consistently improves over state-of-the-art basic embedding
models (without rules); 2) compared to those one-time in-
jection schemes studied before, the iterative injection strat-
egy maximizes the utility of logic rules for KG embedding,
and indeed achieves substantially better performance; 3) de-
spite the uncertainties, automatically extracted soft rules are
highly beneﬁcial to KG embedding, even those with moder-
ate conﬁdence levels.

The contributions of this paper are threefold. 1) We devise
a novel paradigm of KG embedding which iteratively injects
logic rules into the learned embeddings. To our knowledge,
this is the ﬁrst work that models interactions between em-
bedding learning and logical inference in a principled frame-
work. 2) We demonstrate the usefulness of automatically ex-
tracted soft rules in KG embedding, thereby eliminating the
requirement of laborious manual rule creation. 3) Our ap-

ing. Hu et al. (2016) proposed to enhance various types of
neural networks with FOL rules. All these studies demon-
strate the capability of logic rules to enhance distributed rep-
resentation learning.

Rule-Guided Knowledge Graph Embedding
This section introduces RUle-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. During each iteration, the
model alternates between a soft label prediction stage and
an embedding rectiﬁcation stage. Fig. 1 sketches this overall
framework. In what follows, we ﬁrst describe our learning
resources, and then detail the two alternating stages.

O

and

, where

(ei, rk, ej)
}
{

Learning Resources
Suppose we are given a KG with a set of triples observed,
=
i.e.,
. Each triple is composed of two en-
and their relation rk ∈ R
tities ei, ej ∈ E
R
are the sets of entities and relations respectively. We obtain
our learning resources (i.e., labeled triples, unlabeled triples,
and soft rules) and model them as follows.
Labeled Triples. We take the triples observed in
as pos-
itive ones. For each positive triple (ei, rk, ej), we randomly
corrupt the head ei or the tail ej, to form a negative triple
and e(cid:48)j ∈
(e(cid:48)i, rk, ej) or (ei, rk, e(cid:48)j), where e(cid:48)i ∈ E \ {
. We denote a labeled triple as x(cid:96), and associate with
E \ {
it a label y(cid:96) = 1 if x(cid:96) is positive, and y(cid:96) = 0 otherwise. Let
=
denote the set of these labeled triples (along

ei}

O

E

ej}
(x(cid:96), y(cid:96))
}

{

{

U

F

=

=

O

(fp, λp)
}

L
with their labels).
Unlabeled Triples. Besides the labeled triples, we collect a
, where xu = (ei, rk, ej)
xu}
set of unlabeled triples
{
indicates an unlabeled triple. In fact, all the triples that have
not been observed in
can be taken as unlabeled ones. But
in this paper, we consider only those encoded in the conclu-
sion of a soft rule, as detailed below.
Soft Rules. We also consider a set of FOL rules with differ-
P
ent conﬁdence levels, denoted as
p=1. Here,
fp is the p-th logic rule deﬁned over the given KG, repre-
(x, rt, y),
sented, e.g., in the form of
∀
stating that two entities linked by relation rs might also
be linked by relation rt. The left-hand side of the im-
” is called the premise, and the right-hand
plication “
side the conclusion. In this paper, we restrict fp
to be
a Horn clause rule, where the conclusion contains only
a single atom and the premise is a conjunction of sev-
eral atoms. The conﬁdence level of rule fp is denoted
[0, 1]. Rules with higher conﬁdence levels are
as λp ∈
more likely to hold, and a conﬁdence level of λp = 1
indicates a hard rule which always holds with no excep-
tion. Such rules as well as their conﬁdence levels can be
extracted automatically from the KG (with the observed
triple set
as input), by using modern rule mining systems
like AMIE and AMIE+ (Gal´arraga et al. 2013; Gal´arraga et al. 2015).

x, y : (x, rs, y)

⇒

⇒

O

We then propositionalize these rules to get their ground-
ings. Here a grounding is the logical expression with all vari-
. For instance, a
ables instantiated with concrete entities in

E

.

E

∀

O

universally quantiﬁed rule

x, y : (x, BornInCountry, y)
(x, Nationality, y) could be instantiated with two en-
⇒
tities EmmanuelMacron and France, and gives a resultant
grounding (EmmanuelMacron, BornInCountry, France)
(EmmanuelMacron, Nationality, France). Ob-
⇒
there could be a huge number of groundings,
viously,
In this
especially given a large entity vocabulary
paper,
to maximize the utility for knowledge acquisi-
tion and inference, we take as valid groundings only
those where premise triples are observed in
while
conclusion triples are not. That means the aforemen-
tioned grounding will be considered as valid if the triple
(EmmanuelMacron, BornInCountry, France)
but
(EmmanuelMacron, Nationality, France)
/
. For
∈ O
Qp
each FOL rule fp, let
q=1 denote the set of
its valid groundings. All the premise triples of gpq are
contained in
, but the single conclusion triple is not.
These conclusion triples are further used to construct our
unlabeled triple set
. That means, our unlabeled triples are
those which are not directly observed in the KG but could
be inferred by the rules with high probabilities.
Modeling Triples and Rules. Given the labeled triples
unlabeled triples
{Gp}

,
L
, and the valid groundings of FOL rules
P
p=1, we discuss how to model these triples and
G
rules in the context of KG embedding. To model triples, we
follow ComplEx (Trouillon et al. 2016), a recently proposed
method which is simple and efﬁcient while achieving state-
of-the-art predictive performance. Speciﬁcally, we assume
entities and relations to have complex-valued vector embed-
dings. Given a triple (ei, rk, ej)
, we score it by
a multi-linear dot product:

∈ E ×R×E

Gp =

gpq}
{

∈ O

O

=

U

U

m

(cid:88)

) = Re(

ei, rk, ¯ej(cid:105)
[ei]m[rk]m[¯ej]m), (1)
ηijk = Re(
(cid:104)
Cd are the complex-valued vector em-
where ei, ej, rk ∈
beddings associated with ei, ej, and rk, respectively; ¯ej is
the conjugate of ej; [
]m is the m-th entry of a vector; and
·
) means taking the real part of a complex value. We fur-
Re(
·
ther introduce a mapping function φ :
(0, 1), so
as to map the score ηijk to a continuous truth value which
lies in the range of (0, 1), i.e.,

E×R×E →

,

(cid:0)

(cid:1)

Re(

φ(ei, rk, ej) = σ(ηijk) = σ

ei, rk, ¯ej(cid:105)
)
(2)
(cid:104)
where σ(x) = 1/(1 + exp(
x)) denotes the sigmoid func-
−
tion. Triples with higher truth values are more likely to hold.
To model propositionalized rules (i.e. groundings), we use
t-norm based fuzzy logics (H´ajek 1998). The key idea is to
model the truth value of a propositionalized rule as a com-
position of the truth values of its constituent triples, through
). For instance,
speciﬁc logical connectives (e.g.
⇒
the truth value of a grounded rule (eu, rs, ev)
(eu, rt, ev)
will be determined by the truth values of the two triples
(eu, rs, ev) and (eu, rt, ev), via a composition deﬁned by
logical implication. We follow (Guo et al. 2016) and deﬁne
the compositions associated with logical conjunction (
),
disjunction (

) as:

and

⇒

∧

∧

), and negation (
¬
b) = π(a)
π(b),
b) = π(a) + π(b)
a) = 1

π(a).

¬
π(a
π(a
π(

·

∧
∨
¬

−

π(a)

π(b),

−

·

(3)
(4)
(5)

Here, a and b are two logical expressions, which can either
be single triples or be constructed by combining triples with
logical connectives; and π(a) is the truth value of a, indi-
cating to what degree the logical expression is true. If a is a
single triple, say (ei, rk, ej), we have π(a) = φ(ei, rk, ej),
as deﬁned in Eq. (2). Given these compositions, the truth
value of any logical expression can be calculated recursively
(Guo et al. 2016), e.g.,

π(a

b) = π(

a

b) = π(a)

π(b)

π(a) + 1.

(6)

⇒

¬

∨

·

−

r
}r
∈E ∪ {

Logical expressions with higher truth values have greater de-
e
grees to be true. Let Θ =
denote the set of
}e
{
all entity and relation embeddings. The proposed approach,
RUGE, then aims to learn these embeddings by using the
labeled triples
, and valid groundings
P
p=1 in an iterative manner, where each iteration alter-
{Gp}
nates between a soft label prediction stage and an embedding
rectiﬁcation stage.

, unlabeled triples

∈R

L

U

Soft Label Prediction
This stage is to use currently learned embeddings and propo-
sitionalized rules to predict soft labels for unlabeled triples.
Speciﬁcally, let n be the iteration index, and Θ(n
1) the set
of current embeddings learned from the previous iteration.
Recall that we are given a set of P FOL rules with their con-
P
(fp, λp)
p=1, and each FOL rule fp has
ﬁdence levels
}
{
Qp
Qp valid groundings
gpq}
Gp =
q=1. Our aim is to predict a
{
soft label s(xu)
, by
∈
1) and all the groundings
using the current embeddings Θ(n

[0, 1] for each unlabeled triple xu ∈ U

=

F

−

−

P
p=1.

G

{Gp}

=
To do so, we solve a rule-constrained optimization prob-
lem, which projects truth values of unlabeled triples com-
puted by the current embeddings into a subspace constrained
by the rules. The key idea here is to ﬁnd optimal soft la-
bels that stay close to these truth values, while at the same
time ﬁtting the rules. For the ﬁrst property, given each un-
, we calculate its truth value φ(xu)
labeled triple xu ∈ U
using the current embeddings via Eq. (2), and require the
soft label s(xu) to stay close to this truth value. We measure
the closeness between s(xu) and φ(xu) with a squared loss,
and try to minimize it. For the second property, we further
s(xu)
.
impose rule constraints onto the soft labels
}
Speciﬁcally, for each FOL rule fp and each of its ground-
ings gpq, we expect gpq to be true, i.e., π(gpq|S
) = 1 with
) is the conditional truth value
conﬁdence λp. Here, π(gpq|S
of gpq given the soft labels, which can be calculated recur-
sively with the logical compositions deﬁned in Eq. (3) to
Eq. (5). Take gpq := (eu, rs, ev)
(eu, rt, ev) as an ex-
ample, where the premise (eu, rs, ev) is directly observed in
, and the conclusion (eu, rt, ev) is an unlabeled triple in-
. The conditional truth value of gpq can then be

O
cluded in
calculated as:
π(gpq|S
where φ(eu, rs, ev) is a truth value deﬁned by Eq. (2) with
the current embeddings; and s(eu, rt, ev) is a soft label to be
predicted. Comparing Eq. (7) with Eq. (6), we can see that
), for any unlabeled triple,
during the calculation of π(gpq|S

s(eu, rt, ev)
) = φ(eu, rs, ev)
·

φ(eu, rs, ev)+1, (7)

⇒

=

−

U

S

{

we use the soft label s(
) rather than the truth value φ(
·
·
as to better impose rule constraints onto the soft labels
S

Combining the two properties together and further allow-
ing slackness for rule constraints, we ﬁnally get the follow-
ing optimization problem:

), so
.

, Qp, p = 1,

, P,

· · ·

1
2

(s(xu)

min
−
,ξ
S
(cid:88)xu∈U
s.t. λp (1
π(gpq|S
−
0, q = 1,
ξpq ≥
· · ·
1,
0
∀
≤

s(xu)

≤

))

φ(xu))2 + C

ξpq,

p,q
(cid:88)

· · ·
, P,

ξpq, q = 1,

≤
, Qp, p = 1,
· · ·
,
s(xu)

∈ S

(8)
where ξpq is a slack variable and C the penalty coefﬁcient.
Note that conﬁdence levels of rules (i.e. λp’s) are encoded
in the constraints, making our approach capable of handling
soft rules. Rules with higher conﬁdence levels show less tol-
erance for violating the constraints. This optimization prob-
lem is convex, and can be solved efﬁciently with its closed-
form solution:

s(xu) =

φ(xu) + C

λp∇s(xu)π(gpq|S

)

p,q

(9)

1

0

(cid:104)

. Here,

(cid:105)
(cid:88)
) means the gradient
∇s(xu)π(gpq|S
for each xu ∈ U
,1 and
) w.r.t s(xu), which is a constant w.r.t.
of π(gpq|S
[x]1
0 = min(max(x, 0), 1) is a truncation function enforcing
the solutions to stay within [0, 1]. We provide the proof of
convexity and detailed derivation as supplementary materi-
als. Soft labels obtained in this way shall 1) stay close to the
predictions made by the current embedding model, and 2) ﬁt
the rules as well as possible.

S

Embedding Rectiﬁcation
This stage is to integrate both labeled and unlabeled triples
(with hard and soft labels respectively) to update current em-
beddings. Speciﬁcally, we are given a set of labeled triples
(x(cid:96), y(cid:96))
with their hard labels speciﬁed in
,
}
{
and also a set of unlabeled triples encoded in propositional-
. Each unlabeled triple xu has a
ized rules, i.e.,
U
soft label s(xu)
[0, 1], predicted by Eq. (9). We would
like to use these labeled and unlabeled triples to learn the
updated embeddings Θ(n). Here n is the iteration index.

, i.e.,
}

xu}
{

0, 1
{

=

=

L

∈

To this end, we minimize a global loss over

, so
as to ﬁnd embeddings which can predict the true hard labels
, while imitating the soft labels for
for triples contained in
those contained in

. The optimization problem is:

and

L

L

U

(cid:96)(φ(x(cid:96)), y(cid:96)) +

(cid:96)(φ(xu), s(xu)), (10)

U

1

min
Θ

|L| (cid:88)L

−

y log x

x) is the cross
y) log(1
where (cid:96)(x, y) =
−
entropy; and φ(
) is a function w.r.t. Θ deﬁned by Eq. (2).
·
We further impose L2 regularization on the parameters Θ to
avoid overﬁtting. Gradient descent algorithms can be used
to solve this problem. Embeddings learned in this way will
1) be compatible with all the labeled triples, and 2) absorb
rule knowledge carried by the unlabeled triples.

−

1Note that each gpq contains only a single unlabeled triple, i.e.,
the conclusion triple. Take π(gpq|S) deﬁned in Eq. (7) for example.
In this case, s(xu) = s(eu, rt, ev) is the soft label to be predicted
and ∇s(xu)π(gpq|S) = φ(eu, rs, ev) is a constant w.r.t. S.

1

|U| (cid:88)U
(1
−

Whole Procedure

G

Lb,
U

Gb from the labeled triples

Algorithm 1 summarizes the iterative learning procedure of
our approach. To enable efﬁcient learning, we use an online
scheme in mini-batch mode. At each iteration, we sample a
Ub, and
mini-batch
, unla-
L
beled triples
, respectively
, and propositionalized rules
(line 3).2 Soft label prediction and embedding rectiﬁcation
are then conducted locally on these mini-batches (line 4 and
line 5 respectively). This iterative procedure captures the in-
teractive nature between embedding learning and logical in-
ference: given current embeddings, logic rules can be used
to perform approximate inference and predict soft labels for
unlabeled triples; these newly labeled triples carry rich rule
knowledge and will in turn help to learn better embeddings.
In this way, knowledge contained in logic rules can be fully
transferred into the learned embeddings. Note also that our
approach is ﬂexible enough to handle soft rules with various
conﬁdence levels extracted automatically from the KG.

Discussions

We further analyze the space and time complexity, and dis-
cuss possible extensions of our approach.

Complexity. RUGE follows ComplEx to represent entities
and relations as complex-valued vectors, hence has a space
complexity of O(ned + nrd) which scales linearly w.r.t. ne,
nr, and d. Here, ne is the number of entities, nr the num-
ber of relations, and d the dimensionality of the embedding
space. During the learning procedure, each iteration requires
a time complexity of O(τ (n(cid:96)d + nud)), where n(cid:96)/nu is the
average number of labeled/unlabeled triples in a mini-batch,
and τ the number of inner epochs used for embedding recti-
n(cid:96)
ﬁcation (cf. Eq. (10)). In practice, we usually have nu (cid:28)
(see Table 2 for the number of labeled and unlabeled triples
used on our datasets), and we can also set τ to a very small
value, e.g., τ = 1. That means, RUGE has almost the same
time complexity as those most efﬁcient KG embedding tech-
niques (e.g. ComplEx) which require O(n(cid:96)d) per iteration
during training.3 In addition, RUGE further requires prepro-
cessing steps before training, i.e., rule mining and proposi-
tionalization. But these steps are performed only once, and
not required during the iterations.

Extensions. Our approach is quite generic and ﬂexible. 1)
The idea of iteratively injecting logic rules can be applied to
enhance a wide variety of embedding models, as long as an
appropriate scoring function is accordingly designed, e.g.,
the one deﬁned in Eq. (1) by ComplEx. 2) Various types of
rules can be incorporated as long as they can be modeled by
the logical compositions deﬁned in Eq. (3) to Eq. (5), and
we can even use other types of t-norm fuzzy logics to deﬁne
such compositions. 3) Rules with different conﬁdence levels
can be handled in a uniﬁed manner.

2We ﬁrst sample Lb from L. Gb is then constructed by those
whose premise triples are all contained in Lb but conclusion triples
are not. These conclusion triples are further used to construct Ub.

3Such techniques often use SGD in mini-batch mode for train-
ing, and sample a mini-batch of n(cid:96) labeled triples at each iteration.

Algorithm 1 Iterative Learning Procedure of RUGE
Require: Labeled triples L = {(x(cid:96), y(cid:96))}

Unlabeled triples U = {xu}
FOL rules F={(fp, λp)} and their groundings G={gpq}

1: Randomly initialize entity and relation embeddings Θ(0)
2: for n = 1 : N do
3:
4:

Sample a mini-batch Lb / Ub / Gb from L / U / G
1))
Sb ← SoftLabelPrediction (Ub, Gb, Θ(n
−

(cid:46) cf. Eq. (9)

Θ(n) ← EmbeddingRectiﬁcation (Lb, Ub, Sb) (cid:46) cf. Eq. (10)

5:
6: end for
Ensure: Θ(N )

Experiments
We evaluate RUGE in the link prediction task. This task is
to complete a triple (ei, rk, ej) with ei or ej missing, i.e., to
predict ei given (rk, ej) or ej given (ei, rk).
Datasets. We use two datasets: FB15K and YAGO37. The
former is a subgraph of Freebase containing 1,345 relations
and 14,951 entities, released by Bordes et al. (2013).4 The
latter is extracted from the core facts of YAGO3.5 During
the extraction, entities appearing less than 10 times are dis-
carded. The ﬁnal dataset consists of 37 relations and 123,189
entities. Triples on both datasets are split into training, vali-
dation, and test sets, used for model training, hyperparame-
ter tuning, and evaluation, respectively. We use the original
split for FB15K, and draw a split of 989,132/50,000/50,000
triples for YAGO37.

Note that on both datasets, the training sets contain only
positive triples. Negative triples are generated using the local
closed world assumption (Dong et al. 2014). This negative
sampling procedure is performed at runtime for each batch
of training positive triples. Such positive and negative triples
(along with their hard labels) form our labeled triple set.

We further employ AMIE+ (Gal´arraga et al. 2015)6 to au-
tomatically extract Horn clause rules from each dataset, with
the training set as input. To enable efﬁcient extraction, we
consider rules with length not longer than 2 and conﬁdence
levels not less than 0.8.7 The length of a Horn clause rule is
x, y :
the number of atoms appearing in its premise, e.g.,
(x, Nationality, y) has the
(x, BornInCountry, y)
length of 1. And the conﬁdence threshold of 0.8 leads to the
best performance on both datasets (detailed later). Using this
setting, we extract 454 (universally quantiﬁed) Horn clause
rules from FB15K, and 16 such rules from YAGO37. Table 1
shows some examples with their conﬁdence levels.

⇒

∀

Then, we instantiate these rules with concrete entities, i.e.,
propositionalization. Propositionalized rules whose premise
triples are all contained in the training set (while conclusion

4https://everest.hds.utc.fr/doku.php?id=en:smemlj12
5http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago/downloads/

6https://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/amie/

7AMIE+ provides two types of conﬁdence, i.e. standard conﬁ-

dence and PCA conﬁdence. This paper uses PCA conﬁdence.

Table 1: Horn clause rules with conﬁdence levels extracted
by AMIE+ from FB15K (top) and YAGO37 (bottom).

/location/people born here(x,y)
/director/ﬁlm(x,y)
⇒
/ﬁlm/directed by(x,y)

⇒
/ﬁlm/directed by(y,x)

/people/place of birth(y,x)

/person/language(y,z)

/ﬁlm/language(x,z)

∧

⇒

isMarriedTo(x,y)
hasChild(x,y)
playsFor(x,y)

∧
⇒

⇒

isMarriedTo(y,x)

isCitizenOf(y,z)
⇒
isAfﬁliatedTo(x,y)

isCitizenOf(x,z)

1.00
0.99
0.88

0.97
0.94
0.86

Table 2: Statistics of datasets, where ne/nr denotes the num-
ber of entities/relations, n(cid:96)/nu/ng is the number of labeled
triples/unlabeled triples/valid groundings used for training,
and nv/nt denotes the number of validation/test triples.

Train

Valid

Test

Dataset

ne

nr

n(cid:96)

nu

ng

nv

nt

FB15K
YAGO37

14,951
123,189

1,345
37

483,142
989,132

74,707
69,680

96,724
72,670

50,000
50,000

59,071
50,000

triples are not) are taken as valid groundings and used during
embedding learning. We obtain 96,724 valid groundings on
FB15K and 72,670 on YAGO37. Conclusion triples of these
valid groundings are further collected to form our unlabeled
triple set. We ﬁnally get 74,707 unlabeled triples on FB15K
and 69,680 on YAGO37. Table 2 provides some statistics of
the two datasets.
Evaluation Protocol. To evaluate the performance in link
prediction, we follow the standard protocol used in (Bordes
et al. 2013). For each test triple (ei, rk, ej), we replace the
head entity ei with each entity e(cid:48)i ∈ E
, and calculate the score
for (e(cid:48)i, rk, ej). Ranking these scores in descending order,
we get the rank of the correct entity ei. Similarly, we can get
another rank by replacing the tail entity. Aggregated over all
test triples, we report three metrics: 1) the mean reciprocal
rank (MRR), 2) the median of the ranks (MED), and 3) the
proportion of ranks no larger than n (HITS@N). During this
ranking process, we remove corrupted triples which already
exist in either the training, validation, or test set, since they
themselves are true triples. This corresponds to the “ﬁltered”
setting in (Bordes et al. 2013).
Comparison Settings. We compare RUGE with four state-
of-the-art basic embedding models, including TransE (Bor-
des et al. 2013), DistMult (Yang et al. 2015), HolE (Nickel,
Rosasco, and Poggio 2016), and ComplEx (Trouillon et al.
2016). These basic models rely only on triples observed in
a KG and use no rules. We further take PTransE (Lin et al.
2015a) and KALE (Guo et al. 2016) as additional baselines.
Both of them are extensions of TransE, with the former in-
tegrating relation paths (Horn clauses), and the latter FOL
rules (hard rules) in a one-time injection manner. In contrast,
RUGE incorporates soft rules and transfers rule knowledge
into KG embedding in an iterative manner.

We use the code provided by Trouillon et al. (2016)8 for

8https://github.com/ttrouill/complex

TransE, DistMult, and ComplEx, and reimplement HolE so
that all these four basic models share the identical mode of
optimization, i.e., SGD with AdaGrad (Duchi, Hazan, and
Singer 2011) and gradient normalization. As such, we repro-
duce the results of TransE, DistMult, and ComplEx reported
on FB15K (Trouillon et al. 2016), and improve the results of
HolE substantially compared to those reported in the origi-
nal paper (Nickel, Rosasco, and Poggio 2016).9 The code for
PTransE is provided by its authors.10 We implement KALE
and RUGE in Java, both using SGD with AdaGrad and gra-
dient normalization to facilitate a fair comparison.

There are two types of loss functions that could be used
for these baselines, i.e., the logistic loss or the pairwise rank-
ing loss (Nickel, Rosasco, and Poggio 2016). Trouillon et
al. (2016) have recently demonstrated that the logistic loss
generally performs better than the pairwise ranking loss, ex-
cept for TransE. So, for TransE and its extensions (PTransE
and KALE) we use the pairwise ranking loss, and for all the
other baselines we use the logistic loss. To extract relation
paths for PTransE, we follow the optimal conﬁguration re-
ported in (Lin et al. 2015a), where paths constituted by at
most 3 relations are included. For KALE and RUGE, we use
the same set of propositionalized rules to make it a fair com-
parison.11

0.001, 0.01, 0.1, 1

For all the methods, we create 100 mini-batches on each
dataset, and tune the embedding dimensionality d in
50,
{
, the number of negatives per positive triple α
100, 150, 200
}
0.01, 0.05, 0.1,
, the initial learning rate γ in
1, 2, 5, 10
in
{
}
{
0.5, 1.0
0.001,
, and the L2 regularization coefﬁcient λ in
{
}
0.003, 0.01, 0.03, 0.1
. For TransE and its extensions which
}
use the pairwise ranking loss, we further tune the margin δ in
. The slackness penalty C in RUGE
0.1, 0.2, 0.5, 1, 2, 5, 10
{
}
, and the
(cf. Eq. (8)) is selected from
}
{
number of inner iterations (cf. Eq. (10)) is ﬁxed to τ = 1.
Best models are selected by early stopping on the validation
set (monitoring MRR), with at most 1000 iterations over the
training set. The optimal conﬁgurations for RUGE are: d =
200, α = 10, γ = 0.5, λ = 0.01, C = 0.01 on FB15K; and
d = 150, α = 10, γ = 1.0, λ = 0.003, C = 0.01 on YAGO37.
Link Prediction Results. Table 3 shows the results of these
methods on the test sets of FB15K and YAGO37. The results
indicate that RUGE signiﬁcantly and consistently outper-
forms all the baselines on both datasets and in all metrics. It
beats not only the four basic models which use triples alone
(TransE, DistMult, HolE, and ComplEx), but also PTransE
and KALE which further incorporate logic rules (or relation
paths) in a one-time injection manner. This demonstrates the
superiority of injecting logic rules into KG embedding, par-
ticularly in an iterative manner. Compared to the best per-
forming baseline ComplEx (this is also the model based on
which RUGE is designed), RUGE achieves an improvement
of 11%/18% in MRR/HITS@1 on FB15K, and an improve-
ment of 3%/6% on YAGO37. The improvements on FB15K

9HolE in its original implementation uses SGD with AdaGrad,

but no gradient normalization.

10https://github.com/thunlp/KB2E
11KALE takes all these groundings as hard rules. This approxi-
mation works quite well with an appropriate conﬁdence threshold.

Table 3: Link prediction results on the test sets of FB15K and YAGO37. As baselines, rows 1-4 are the four basic models which
use triples alone, and rows 5-6 further integrate logic rules (or relation paths) in a one-time injection manner.

FB15K

HITS@N

YAGO37

HITS@N

Method

TransE
DistMult
HolE
ComplEx
PTransE
KALE

RUGE

MRR

0.400
0.644
0.600
0.690
0.679
0.523

0.768

MED

1

4.0
1.0
2.0
1.0
1.0
2.0

1.0

0.246
0.532
0.485
0.598
0.565
0.383

0.703

3

0.495
0.730
0.673
0.756
0.768
0.616

0.815

5

0.576
0.769
0.722
0.793
0.810
0.683

0.836

10

0.662
0.812
0.779
0.837
0.855
0.762

0.865

MRR

0.303
0.365
0.380
0.417
0.403
0.321

0.431

MED

1

13.0
6.0
7.0
4.0
9.0
9.0

4.0

0.218
0.262
0.288
0.320
0.339
0.215

0.340

3

0.336
0.411
0.420
0.471
0.444
0.372

0.482

5

0.387
0.493
0.479
0.533
0.473
0.438

0.541

10

0.475
0.575
0.551
0.603
0.506
0.522

0.603

Table 4: Runtime (in sec.) on FB15K and YAGO37. Extr. is
the time required for rule/path extraction, Prop. for proposi-
tionalization, and Lean. for training per iteration.

FB15K

YAGO37

Method

Extr. Prop. Learn.

Extr. Prop. Learn.

ComplEx
PTransE
KALE
RUGE

—
868.4
43.1
43.1

— 11.4
— 46.5
4.8
4.0
14.1
4.0

—
13939.5
337.5
337.5

— 49.5
— 23.8
27.5
55.2

13.8
13.8

we set d = 200 (embedding dimensionality) and α = 2 (num-
ber of negatives per positive triple) for all the methods. Other
hyperparameters are ﬁxed to their optimal conﬁgurations de-
termined in link prediction. We can see that RUGE is still
quite efﬁcient despite integrating additional rules. The aver-
age training time per iteration increases from 11.4 to 14.1
on FB15K, and from 49.5 to 55.2 on YAGO37. The prepro-
cessing steps, although performed only once, are also highly
efﬁcient, requiring much less time compared to PTransE.

Conclusion

This paper proposes a novel paradigm that learns entity and
relation embeddings with iterative guidance from soft rules,
referred to as RUGE. It enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. Each iteration alternates
between 1) a soft label prediction stage which predicts soft
labels for unlabeled triples using currently learned embed-
dings and soft rules, and 2) an embedding rectiﬁcation stage
which further integrates both labeled and unlabeled triples
to update current embeddings. This iterative procedure may
better transfer the knowledge contained in logic rules into
the learned embeddings. Link prediction results on Freebase
and YAGO show that RUGE achieves signiﬁcant and con-
sistent improvements over state-of-the-art baselines. More-
over, RUGE demonstrates the usefulness of automatically
extracted soft rules. Even those with moderate conﬁdence
levels can be highly beneﬁcial to KG embedding.

Figure 2: MRR achieved by RUGE with different conﬁdence
thresholds on the test set of FB15K.

are more substantial than those on YAGO37. The reason is
probably that FB15K contains more relations from which a
good range of rules can be extracted (454 universally quan-
tiﬁed rules from FB15K, and 16 from YAGO37).
Inﬂuence of Conﬁdence Levels. We further investigate the
inﬂuence of the threshold of rules’ conﬁdence levels used
in RUGE. To do so, we ﬁx all the hyperparameters to the
optimal conﬁgurations determined by the previous experi-
ment, and vary the conﬁdence threshold in [0.1, 1] with a
step 0.05. Fig. 2 shows MRR achieved by RUGE with var-
ious thresholds on the test set of FB15K. We can see that
the threshold of 0.8 is a good tradeoff and indeed performs
best. A threshold higher than that will reduce the number of
rules that can be extracted, while a one lower than that might
introduce too many less credible rules. Both hurt the perfor-
mance. However, even so, RUGE outperforms ComplEx by
a large margin, with the threshold set in a broad range of
[0.35, 0.9]. This observation indicates that soft rules, even
those with moderate conﬁdence levels, are highly beneﬁcial
to KG embedding despite their uncertainties.
Comparison of Runtime. Finally, we compare RUGE with
ComplEx, PTransE, and KALE in their runtime.12 ComplEx
is a basic model which only requires model training. RUGE
as well as the other two baselines further require preprocess-
ing of rule/path extraction and propositionalization. Table 4
lists the runtime of these methods required for each step on
FB15K and YAGO37. Here, to facilitate a fair comparison,

12The other three baselines are implemented in Python and much

slower. So they are not considered here.

Acknowledgments
The authors would like to thank all the reviewers for their
insightful and valuable suggestions, which signiﬁcantly im-
prove the quality of this paper. This work is supported by the
National Key Research and Development Program of China
(grants No. 2016YFB0801003 and No. 2016QY03D0503),
the Fundamental Theory and Cutting Edge Technology Re-
search Program of the Institute of Information Engineering,
Chinese Academy of Sciences (grant No. Y7Z0261101), and
the National Natural Science Foundation of China (grant No.
61402465).

References
[Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.;
Sturge, T.; and Taylor, J. 2008. Freebase: A collaboratively created
In SIGMOD,
graph database for structuring human knowledge.
1247–1250.

[Bordes et al. 2013] Bordes, A.; Usunier, N.; Garc´ıa-Dur´an, A.;
Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for
modeling multi-relational data. In NIPS, 2787–2795.

[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio,
Y. 2014. A semantic matching energy function for learning with
multi-relational data. MACH LEARN 94(2):233–259.

[Carlson et al. 2010] Carlson, A.; Betteridge, J.; Kisiel, B.; Settles,
B.; Hruschka Jr, E. R.; and Mitchell, T. M. 2010. Toward an archi-
tecture for never-ending language learning. In AAAI, 1306–1313.
T.;
Rockt¨aschel, T.; and Riedel, S. 2016. Lifted rule injection for
relation embeddings. In EMNLP, 1389–1399.

[Demeester, Rockt¨aschel, and Riedel 2016] Demeester,

[Dong et al. 2014] Dong, X.; Gabrilovich, E.; Heitz, G.; Horn, W.;
Lao, N.; Murphy, K.; Strohmann, T.; Sun, S.; and Zhang, W. 2014.
Knowledge vault: A web-scale approach to probabilistic knowl-
edge fusion. In SIGKDD, 601–610.

[Duchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and Singer,
Y. 2011. Adaptive subgradient methods for online learning and
stochastic optimization. J MACH LEARN RES 12(Jul):2121–2159.
[Faruqui et al. 2014] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer,
C.; Hovy, E.; and Smith, N. A. 2014. Retroﬁtting word vectors to
semantic lexicons. arXiv:1411.4166.

[Gal´arraga et al. 2013] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2013. AMIE: Association rule mining under
In WWW,
incomplete evidence in ontological knowledge bases.
413–422.

[Gal´arraga et al. 2015] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2015. Fast rule mining in ontological knowl-
edge bases with AMIE+. VLDB J 24(6):707–730.

[Gardner, Talukdar, and Mitchell 2015] Gardner, M.; Talukdar, P.;
and Mitchell, T. 2015. Combining vector space embeddings with
symbolic logical inference over open-domain text. In AAAI Spring
Symposium Series, 61–65.

[Guo et al. 2015] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2015. Semantically smooth knowledge graph embedding. In
ACL, 84–94.

[Guo et al. 2016] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2016. Jointly embedding knowledge graphs and logical rules.
In EMNLP, 192–202.

[Guu, Miller, and Liang 2015] Guu, K.; Miller, J.; and Liang, P.
2015. Traversing knowledge graphs in vector space. In EMNLP,
318–327.

[H´ajek 1998] H´ajek, P. 1998. The metamathematics of fuzzy logic.

Kluwer.

[Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E.
2016. Harnessing deep neural networks with logic rules. In ACL,
2410–2420.

[Lin et al. 2015a] Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and
Liu, S. 2015a. Modeling relation paths for representation learning
of knowledge bases. In EMNLP, 705–714.

[Lin et al. 2015b] Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X.
2015b. Learning entity and relation embeddings for knowledge
graph completion. In AAAI, 2181–2187.

[Liu et al. 2016] Liu, Q.; Jiang, H.; Evdokimov, A.; Ling, Z.-H.;
Zhu, X.; Wei, S.; and Hu, Y. 2016. Probabilistic reasoning via
deep learning: Neural association models.

[Miller 1995] Miller, G. A. 1995. WordNet: A lexical database for

English. COMMUN ACM 38(11):39–41.

[Neelakantan, Roth, and McCallum 2015] Neelakantan, A.; Roth,
B.; and McCallum, A. 2015. Compositional vector space mod-
els for knowledge base completion. In ACL, 156–166.

[Nickel, Rosasco, and Poggio 2016] Nickel, M.; Rosasco, L.; and
Poggio, T. 2016. Holographic embeddings of knowledge graphs.
In AAAI, 1955–1961.

[Nickel, Tresp, and Kriegel 2011] Nickel, M.; Tresp, V.;

and
Kriegel, H.-P. 2011. A three-way model for collective learning on
multi-relational data. In ICML, 809–816.

[Rockt¨aschel et al. 2014] Rockt¨aschel, T.; Boˇsnjak, M.; Singh, S.;
and Riedel, S. 2014. Low-dimensional embeddings of logic. In
ACL Workshop on Semantic Parsing, 45–49.

[Rockt¨aschel, Singh, and Riedel 2015] Rockt¨aschel, T.; Singh, S.;
and Riedel, S. 2015. Injecting logical background knowledge into
embeddings for relation extraction. In NAACL, 1119–1129.

[Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng,
A. Y. 2013. Reasoning with neural tensor networks for knowledge
base completion. In NIPS, 926–934.

[Suchanek, Kasneci, and Weikum 2007] Suchanek, F. M.; Kasneci,
G.; and Weikum, G. 2007. YAGO: A core of semantic knowledge.
In WWW, 697–706.

[Trouillon et al. 2016] Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,
E.; and Bouchard, G. 2016. Complex embeddings for simple link
prediction. In ICML, 2071–2080.

[Vendrov et al. 2015] Vendrov, I.; Kiros, R.; Fidler, S.; and Ur-
2015. Order-embeddings of images and language.

tasun, R.
arXiv:1511.06361.

[Wang and Cohen 2016] Wang, W. Y., and Cohen, W. W. 2016.
Learning ﬁrst-order logic embeddings via matrix factorization. In
IJCAI, 2132–2138.

[Wang et al. 2014] Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z.
2014. Knowledge graph embedding by translating on hyperplanes.
In AAAI, 1112–1119.

[Wang et al. 2017] Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017.
Knowledge graph embedding: A survey of approaches and appli-
cations. IEEE TRANS KNOWL DATA ENG 29(12):2724–2743.
[Wang, Wang, and Guo 2015] Wang, Q.; Wang, B.; and Guo, L.
2015. Knowledge base completion using embeddings and rules.
In IJCAI, 1859–1865.

[Wei et al. 2015] Wei, Z.; Zhao, J.; Liu, K.; Qi, Z.; Sun, Z.; and
Tian, G. 2015. Large-scale knowledge base completion: Infer-
ring via grounding network sampling over selected instances. In
CIKM, 1331–1340.

[Weston et al. 2013] Weston, J.; Bordes, A.; Yakhnenko, O.; and
Usunier, N. 2013. Connecting language and knowledge bases with
embedding models for relation extraction. In EMNLP, 1366–1371.
[Xiao, Huang, and Zhu 2017] Xiao, H.; Huang, M.; and Zhu, X.
2017. SSP: Semantic space projection for knowledge graph em-
bedding with text descriptions. In AAAI, 3104–3110.

[Xie et al. 2016] Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M.
2016. Representation learning of knowledge graphs with entity
descriptions. In AAAI, 2659–2665.

[Xie, Liu, and Sun 2016] Xie, R.; Liu, Z.; and Sun, M. 2016. Rep-
resentation learning of knowledge graphs with hierarchical types.
In IJCAI, 2965–2971.

[Xiong, Power, and Callan 2017] Xiong, C.; Power, R.; and Callan,
J. 2017. Explicit semantic ranking for academic search via knowl-
edge graph embedding. In WWW, 1271–1279.

[Yang et al. 2015] Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng,
L. 2015. Embedding entities and relations for learning and infer-
ence in knowledge bases. In ICLR.

[Zhang et al. 2016] Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and
Ma, W.-Y. 2016. Collaborative knowledge base embedding for
recommender systems. In SIGKDD, 353–362.

Knowledge Graph Embedding with Iterative Guidance from Soft Rules

Shu Guo1,2, Quan Wang1,2,3

∗, Lihong Wang4, Bin Wang1,2, Li Guo1,2

1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3State Key Laboratory of Information Security, Chinese Academy of Sciences
4National Computer Network Emergency Response Technical Team & Coordination Center of China

7
1
0
2
 
v
o
N
 
0
3
 
 
]
I

A
.
s
c
[
 
 
1
v
1
3
2
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Embedding knowledge graphs (KGs) into continuous vector
spaces is a focus of current research. Combining such an em-
bedding model with logic rules has recently attracted increas-
ing attention. Most previous attempts made a one-time injec-
tion of logic rules, ignoring the interactive nature between
embedding learning and logical inference. And they focused
only on hard rules, which always hold with no exception and
usually require extensive manual effort to create or validate.
In this paper, we propose Rule-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from 1) labeled triples that have been directly
observed in a given KG, 2) unlabeled triples whose labels are
going to be predicted iteratively, and 3) soft rules with vari-
ous conﬁdence levels extracted automatically from the KG. In
the learning process, RUGE iteratively queries rules to obtain
soft labels for unlabeled triples, and integrates such newly la-
beled triples to update the embedding model. Through this
iterative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings. We evalu-
ate RUGE in link prediction on Freebase and YAGO. Exper-
imental results show that: 1) with rule knowledge injected it-
eratively, RUGE achieves signiﬁcant and consistent improve-
ments over state-of-the-art baselines; and 2) despite their un-
certainties, automatically extracted soft rules are highly bene-
ﬁcial to KG embedding, even those with moderate conﬁdence
levels. The code and data used for this paper can be obtained
from https://github.com/iieir-km/RUGE.

Introduction
Knowledge graphs (KGs) such as WordNet (Miller 1995),
Freebase (Bollacker et al. 2008), YAGO (Suchanek, Kas-
neci, and Weikum 2007), and NELL (Carlson et al. 2010)
are extremely useful resources for many AI related applica-
tions. A KG is a multi-relational graph composed of entities
as nodes and relations as different types of edges. Each edge
is represented as a triple (head entity, relation, tail entity),
indicating that there is a speciﬁc relation between two enti-
ties, e.g., (Paris, CapitalOf, France). Although effective
in representing structured data, the underlying symbolic na-
ture of such triples often makes KGs hard to manipulate.

∗Corresponding author: Quan Wang (wangquan@iie.ac.cn).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recently, a new research direction termed as knowledge
graph embedding has been proposed and quickly received
massive attention (Nickel, Tresp, and Kriegel 2011; Bordes
et al. 2013; Wang et al. 2014; Lin et al. 2015b; Yang et al.
2015; Nickel, Rosasco, and Poggio 2016; Trouillon et al.
2016). The key idea is to embed entities and relations in a
KG into a low-dimensional continuous vector space, so as
to simplify the manipulation while preserving the inherent
structure of the KG. Such embeddings contain rich semantic
information, and can beneﬁt a broad range of downstream
applications (Weston et al. 2013; Bordes et al. 2014; Zhang
et al. 2016; Xiong, Power, and Callan 2017).

Traditional methods performed embedding based solely
on triples observed in a KG. But considering the power of
logic rules in knowledge acquisition and inference, combin-
ing embedding models with logic rules has become a focus
of current research (Rockt¨aschel et al. 2014; Vendrov et al.
2015; Wang and Cohen 2016; Hu et al. 2016). Wang et al.
(2015) and Wei et al. (2015) tried to use embedding models
and logic rules for KG completion. But in their work, rules
are modeled separately from embedding models, and would
not help to learn more predictive embeddings. Rockt¨aschel
et al. (2015) and Guo et al. (2016) then devised joint learning
paradigms which can inject ﬁrst-order logic (FOL) into KG
embedding. Demeester et al. (2016) further proposed lifted
rule injection to avoid the costly propositionalization of FOL
rules. Although these joint models are able to learn better
embeddings after integrating logic rules, they still have their
drawbacks and restrictions.

First of all, these joint models made a one-time injection
of logic rules, taking them as additional rule-based training
instances (Rockt¨aschel, Singh, and Riedel 2015) or regular-
ization terms (Demeester, Rockt¨aschel, and Riedel 2016).
We argue that rules can better enhance KG embedding, how-
ever, in an iterative manner. Given the learned embeddings
and their rough predictions, rules can be used to reﬁne the
predictions and infer new facts. The newly inferred facts, in
turn, will help to learn better embeddings and more accurate
logical inference. Previous methods fail to model such in-
teractions between embedding models and logic rules. Fur-
thermore, they focused only on hard rules which always hold
with no exception. Such rules usually require extensive man-
ual effort to create or validate. Actually, besides hard rules,
a signiﬁcant amount of background information can be en-

proach is quite generic and ﬂexible. It can integrate various
types of rules with different conﬁdence levels to enhance a
good variety of KG embedding models.

Related Work
Recent years have witnessed increasing interest in learning
distributed representations for entities and relations in KGs,
a.k.a. KG embedding. Various techniques have been devised
for this task, e.g., translation-based models which take re-
lations as translating operations between head and tail enti-
ties (Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015b),
simple compositional models which match compositions of
head-tail entity pairs with their relations (Nickel, Tresp, and
Kriegel 2011; Yang et al. 2015; Nickel, Rosasco, and Pog-
gio 2016; Trouillon et al. 2016), and neural networks which
further introduce non-linear layers and deep architectures
(Socher et al. 2013; Bordes et al. 2014; Dong et al. 2014;
Liu et al. 2016). Among these techniques, ComplEx (Trouil-
lon et al. 2016), a compositional model which represents
entities and relations as complex-valued vectors, achieves a
very good trade-off between accuracy and efﬁciency. Most
of the currently available techniques perform the embedding
task based solely on triples observed in a KG. Some recent
work further tried to use other information, e.g., entity types
(Guo et al. 2015; Xie, Liu, and Sun 2016) and textual de-
scriptions (Xie et al. 2016; Xiao, Huang, and Zhu 2017), to
learn more predictive embeddings. See (Wang et al. 2017)
for a thorough review of KG embedding techniques.

Given the power of logic rules in knowledge acquisition
and inference, combining KG embedding with logic rules
becomes a focus of current research. Wang et al. (2015) and
Wei et al. (2015) devised pipelined frameworks which use
logic rules to further reﬁne predictions made by embedding
models. In their work, rules will not help to learn better em-
beddings. Rockt¨aschel et al. (2015) and Guo et al. (2016)
then tried to learn KG embeddings jointly from triples and
propositionalized FOL rules. Demeester et al. (2016) further
proposed lifted rule injection to avoid the costly proposi-
tionalization. These joint models, however, made a one-time
injection of logic rules, ignoring the interactive nature be-
tween embedding learning and logical inference. Moreover,
they can only handle hard rules which are usually manually
created or validated.

Besides logic rules, relation paths which can be regarded
as Horn clauses and get a strong connection to logical infer-
ence (Gardner, Talukdar, and Mitchell 2015), have also been
studied in KG embedding (Neelakantan, Roth, and McCal-
lum 2015; Lin et al. 2015a; Guu, Miller, and Liang 2015).
But in these methods, relation paths are incorporated, again,
in a one-time manner. Our approach, in contrast, iteratively
injects knowledge contained in logic rules into KG embed-
ding, and is able to handle soft rules with various conﬁdence
levels extracted automatically from KGs.

Combining logic rules with distributed representations is
also an active research topic in other contexts outside KGs.
Faruqui et al. (2014) tried to inject ontological knowledge
from WordNet into word embeddings. Vendrov et al. (2015)
introduced order-embedding to model the partial order struc-
ture of hypernymy, textual entailment, and image caption-

Figure 1: Framework overview. RUGE enables an embed-
ding model to learn simultaneously from labeled triples, un-
labeled triples, and soft rules in an iterative manner, where
each iteration alternates between a soft label prediction stage
and an embedding rectiﬁcation stage.

coded as soft rules, e.g., “a person is very likely (but not nec-
essarily) to have a nationality of the country where he/she
was born”. Soft rules can be extracted automatically and ef-
ﬁciently via modern rule mining systems (Gal´arraga et al.
2013; Gal´arraga et al. 2015). Yet, despite this merit, soft
rules have not been well studied in previous methods.

This paper proposes RUle-Guided Embedding (RUGE), a
novel paradigm of KG embedding with iterative guidance
from soft rules. As sketched in Fig. 1, it enables an embed-
ding model to learn simultaneously from 1) labeled triples
that have been directly observed in a given KG, 2) unlabeled
triples whose labels are going to be predicted iteratively, and
3) soft rules with different conﬁdence levels extracted auto-
matically from the KG. During each iteration of the learning
process, the model alternates between a soft label prediction
stage and an embedding rectiﬁcation stage. The former uses
currently learned embeddings and soft rules to predict soft
labels for unlabeled triples, and the latter further integrates
both labeled and unlabeled triples (with hard and soft labels
respectively) to update current embeddings. Through this it-
erative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings.

We empirically evaluate RUGE on large scale public KGs,
namely Freebase and YAGO. Experimental results reveal
that: 1) by incorporating logic rules, RUGE signiﬁcantly and
consistently improves over state-of-the-art basic embedding
models (without rules); 2) compared to those one-time in-
jection schemes studied before, the iterative injection strat-
egy maximizes the utility of logic rules for KG embedding,
and indeed achieves substantially better performance; 3) de-
spite the uncertainties, automatically extracted soft rules are
highly beneﬁcial to KG embedding, even those with moder-
ate conﬁdence levels.

The contributions of this paper are threefold. 1) We devise
a novel paradigm of KG embedding which iteratively injects
logic rules into the learned embeddings. To our knowledge,
this is the ﬁrst work that models interactions between em-
bedding learning and logical inference in a principled frame-
work. 2) We demonstrate the usefulness of automatically ex-
tracted soft rules in KG embedding, thereby eliminating the
requirement of laborious manual rule creation. 3) Our ap-

ing. Hu et al. (2016) proposed to enhance various types of
neural networks with FOL rules. All these studies demon-
strate the capability of logic rules to enhance distributed rep-
resentation learning.

Rule-Guided Knowledge Graph Embedding
This section introduces RUle-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. During each iteration, the
model alternates between a soft label prediction stage and
an embedding rectiﬁcation stage. Fig. 1 sketches this overall
framework. In what follows, we ﬁrst describe our learning
resources, and then detail the two alternating stages.

O

and

, where

(ei, rk, ej)
}
{

Learning Resources
Suppose we are given a KG with a set of triples observed,
=
i.e.,
. Each triple is composed of two en-
and their relation rk ∈ R
tities ei, ej ∈ E
R
are the sets of entities and relations respectively. We obtain
our learning resources (i.e., labeled triples, unlabeled triples,
and soft rules) and model them as follows.
Labeled Triples. We take the triples observed in
as pos-
itive ones. For each positive triple (ei, rk, ej), we randomly
corrupt the head ei or the tail ej, to form a negative triple
and e(cid:48)j ∈
(e(cid:48)i, rk, ej) or (ei, rk, e(cid:48)j), where e(cid:48)i ∈ E \ {
. We denote a labeled triple as x(cid:96), and associate with
E \ {
it a label y(cid:96) = 1 if x(cid:96) is positive, and y(cid:96) = 0 otherwise. Let
=
denote the set of these labeled triples (along

ei}

O

E

ej}
(x(cid:96), y(cid:96))
}

{

{

U

F

=

=

O

(fp, λp)
}

L
with their labels).
Unlabeled Triples. Besides the labeled triples, we collect a
, where xu = (ei, rk, ej)
xu}
set of unlabeled triples
{
indicates an unlabeled triple. In fact, all the triples that have
not been observed in
can be taken as unlabeled ones. But
in this paper, we consider only those encoded in the conclu-
sion of a soft rule, as detailed below.
Soft Rules. We also consider a set of FOL rules with differ-
P
ent conﬁdence levels, denoted as
p=1. Here,
fp is the p-th logic rule deﬁned over the given KG, repre-
(x, rt, y),
sented, e.g., in the form of
∀
stating that two entities linked by relation rs might also
be linked by relation rt. The left-hand side of the im-
” is called the premise, and the right-hand
plication “
side the conclusion. In this paper, we restrict fp
to be
a Horn clause rule, where the conclusion contains only
a single atom and the premise is a conjunction of sev-
eral atoms. The conﬁdence level of rule fp is denoted
[0, 1]. Rules with higher conﬁdence levels are
as λp ∈
more likely to hold, and a conﬁdence level of λp = 1
indicates a hard rule which always holds with no excep-
tion. Such rules as well as their conﬁdence levels can be
extracted automatically from the KG (with the observed
triple set
as input), by using modern rule mining systems
like AMIE and AMIE+ (Gal´arraga et al. 2013; Gal´arraga et al. 2015).

x, y : (x, rs, y)

⇒

⇒

O

We then propositionalize these rules to get their ground-
ings. Here a grounding is the logical expression with all vari-
. For instance, a
ables instantiated with concrete entities in

E

.

E

∀

O

universally quantiﬁed rule

x, y : (x, BornInCountry, y)
(x, Nationality, y) could be instantiated with two en-
⇒
tities EmmanuelMacron and France, and gives a resultant
grounding (EmmanuelMacron, BornInCountry, France)
(EmmanuelMacron, Nationality, France). Ob-
⇒
there could be a huge number of groundings,
viously,
In this
especially given a large entity vocabulary
paper,
to maximize the utility for knowledge acquisi-
tion and inference, we take as valid groundings only
those where premise triples are observed in
while
conclusion triples are not. That means the aforemen-
tioned grounding will be considered as valid if the triple
(EmmanuelMacron, BornInCountry, France)
but
(EmmanuelMacron, Nationality, France)
/
. For
∈ O
Qp
each FOL rule fp, let
q=1 denote the set of
its valid groundings. All the premise triples of gpq are
contained in
, but the single conclusion triple is not.
These conclusion triples are further used to construct our
unlabeled triple set
. That means, our unlabeled triples are
those which are not directly observed in the KG but could
be inferred by the rules with high probabilities.
Modeling Triples and Rules. Given the labeled triples
unlabeled triples
{Gp}

,
L
, and the valid groundings of FOL rules
P
p=1, we discuss how to model these triples and
G
rules in the context of KG embedding. To model triples, we
follow ComplEx (Trouillon et al. 2016), a recently proposed
method which is simple and efﬁcient while achieving state-
of-the-art predictive performance. Speciﬁcally, we assume
entities and relations to have complex-valued vector embed-
dings. Given a triple (ei, rk, ej)
, we score it by
a multi-linear dot product:

∈ E ×R×E

Gp =

gpq}
{

∈ O

O

=

U

U

m

(cid:88)

) = Re(

ei, rk, ¯ej(cid:105)
[ei]m[rk]m[¯ej]m), (1)
ηijk = Re(
(cid:104)
Cd are the complex-valued vector em-
where ei, ej, rk ∈
beddings associated with ei, ej, and rk, respectively; ¯ej is
the conjugate of ej; [
]m is the m-th entry of a vector; and
·
) means taking the real part of a complex value. We fur-
Re(
·
ther introduce a mapping function φ :
(0, 1), so
as to map the score ηijk to a continuous truth value which
lies in the range of (0, 1), i.e.,

E×R×E →

,

(cid:0)

(cid:1)

Re(

φ(ei, rk, ej) = σ(ηijk) = σ

ei, rk, ¯ej(cid:105)
)
(2)
(cid:104)
where σ(x) = 1/(1 + exp(
x)) denotes the sigmoid func-
−
tion. Triples with higher truth values are more likely to hold.
To model propositionalized rules (i.e. groundings), we use
t-norm based fuzzy logics (H´ajek 1998). The key idea is to
model the truth value of a propositionalized rule as a com-
position of the truth values of its constituent triples, through
). For instance,
speciﬁc logical connectives (e.g.
⇒
the truth value of a grounded rule (eu, rs, ev)
(eu, rt, ev)
will be determined by the truth values of the two triples
(eu, rs, ev) and (eu, rt, ev), via a composition deﬁned by
logical implication. We follow (Guo et al. 2016) and deﬁne
the compositions associated with logical conjunction (
),
disjunction (

) as:

and

⇒

∧

∧

), and negation (
¬
b) = π(a)
π(b),
b) = π(a) + π(b)
a) = 1

π(a).

¬
π(a
π(a
π(

·

∧
∨
¬

−

π(a)

π(b),

−

·

(3)
(4)
(5)

Here, a and b are two logical expressions, which can either
be single triples or be constructed by combining triples with
logical connectives; and π(a) is the truth value of a, indi-
cating to what degree the logical expression is true. If a is a
single triple, say (ei, rk, ej), we have π(a) = φ(ei, rk, ej),
as deﬁned in Eq. (2). Given these compositions, the truth
value of any logical expression can be calculated recursively
(Guo et al. 2016), e.g.,

π(a

b) = π(

a

b) = π(a)

π(b)

π(a) + 1.

(6)

⇒

¬

∨

·

−

r
}r
∈E ∪ {

Logical expressions with higher truth values have greater de-
e
grees to be true. Let Θ =
denote the set of
}e
{
all entity and relation embeddings. The proposed approach,
RUGE, then aims to learn these embeddings by using the
labeled triples
, and valid groundings
P
p=1 in an iterative manner, where each iteration alter-
{Gp}
nates between a soft label prediction stage and an embedding
rectiﬁcation stage.

, unlabeled triples

∈R

L

U

Soft Label Prediction
This stage is to use currently learned embeddings and propo-
sitionalized rules to predict soft labels for unlabeled triples.
Speciﬁcally, let n be the iteration index, and Θ(n
1) the set
of current embeddings learned from the previous iteration.
Recall that we are given a set of P FOL rules with their con-
P
(fp, λp)
p=1, and each FOL rule fp has
ﬁdence levels
}
{
Qp
Qp valid groundings
gpq}
Gp =
q=1. Our aim is to predict a
{
soft label s(xu)
, by
∈
1) and all the groundings
using the current embeddings Θ(n

[0, 1] for each unlabeled triple xu ∈ U

=

F

−

−

P
p=1.

G

{Gp}

=
To do so, we solve a rule-constrained optimization prob-
lem, which projects truth values of unlabeled triples com-
puted by the current embeddings into a subspace constrained
by the rules. The key idea here is to ﬁnd optimal soft la-
bels that stay close to these truth values, while at the same
time ﬁtting the rules. For the ﬁrst property, given each un-
, we calculate its truth value φ(xu)
labeled triple xu ∈ U
using the current embeddings via Eq. (2), and require the
soft label s(xu) to stay close to this truth value. We measure
the closeness between s(xu) and φ(xu) with a squared loss,
and try to minimize it. For the second property, we further
s(xu)
.
impose rule constraints onto the soft labels
}
Speciﬁcally, for each FOL rule fp and each of its ground-
ings gpq, we expect gpq to be true, i.e., π(gpq|S
) = 1 with
) is the conditional truth value
conﬁdence λp. Here, π(gpq|S
of gpq given the soft labels, which can be calculated recur-
sively with the logical compositions deﬁned in Eq. (3) to
Eq. (5). Take gpq := (eu, rs, ev)
(eu, rt, ev) as an ex-
ample, where the premise (eu, rs, ev) is directly observed in
, and the conclusion (eu, rt, ev) is an unlabeled triple in-
. The conditional truth value of gpq can then be

O
cluded in
calculated as:
π(gpq|S
where φ(eu, rs, ev) is a truth value deﬁned by Eq. (2) with
the current embeddings; and s(eu, rt, ev) is a soft label to be
predicted. Comparing Eq. (7) with Eq. (6), we can see that
), for any unlabeled triple,
during the calculation of π(gpq|S

s(eu, rt, ev)
) = φ(eu, rs, ev)
·

φ(eu, rs, ev)+1, (7)

⇒

−

=

U

S

{

we use the soft label s(
) rather than the truth value φ(
·
·
as to better impose rule constraints onto the soft labels
S

Combining the two properties together and further allow-
ing slackness for rule constraints, we ﬁnally get the follow-
ing optimization problem:

), so
.

, Qp, p = 1,

, P,

· · ·

1
2

(s(xu)

min
−
,ξ
S
(cid:88)xu∈U
s.t. λp (1
π(gpq|S
−
0, q = 1,
ξpq ≥
· · ·
1,
0
∀
≤

s(xu)

≤

))

φ(xu))2 + C

ξpq,

p,q
(cid:88)

· · ·
, P,

ξpq, q = 1,

≤
, Qp, p = 1,
· · ·
,
s(xu)

∈ S

(8)
where ξpq is a slack variable and C the penalty coefﬁcient.
Note that conﬁdence levels of rules (i.e. λp’s) are encoded
in the constraints, making our approach capable of handling
soft rules. Rules with higher conﬁdence levels show less tol-
erance for violating the constraints. This optimization prob-
lem is convex, and can be solved efﬁciently with its closed-
form solution:

s(xu) =

φ(xu) + C

λp∇s(xu)π(gpq|S

)

p,q

(9)

1

0

(cid:104)

. Here,

(cid:105)
(cid:88)
) means the gradient
∇s(xu)π(gpq|S
for each xu ∈ U
,1 and
) w.r.t s(xu), which is a constant w.r.t.
of π(gpq|S
[x]1
0 = min(max(x, 0), 1) is a truncation function enforcing
the solutions to stay within [0, 1]. We provide the proof of
convexity and detailed derivation as supplementary materi-
als. Soft labels obtained in this way shall 1) stay close to the
predictions made by the current embedding model, and 2) ﬁt
the rules as well as possible.

S

Embedding Rectiﬁcation
This stage is to integrate both labeled and unlabeled triples
(with hard and soft labels respectively) to update current em-
beddings. Speciﬁcally, we are given a set of labeled triples
(x(cid:96), y(cid:96))
with their hard labels speciﬁed in
,
}
{
and also a set of unlabeled triples encoded in propositional-
. Each unlabeled triple xu has a
ized rules, i.e.,
U
soft label s(xu)
[0, 1], predicted by Eq. (9). We would
like to use these labeled and unlabeled triples to learn the
updated embeddings Θ(n). Here n is the iteration index.

, i.e.,
}

xu}
{

0, 1
{

=

=

L

∈

To this end, we minimize a global loss over

, so
as to ﬁnd embeddings which can predict the true hard labels
, while imitating the soft labels for
for triples contained in
those contained in

. The optimization problem is:

and

L

L

U

(cid:96)(φ(x(cid:96)), y(cid:96)) +

(cid:96)(φ(xu), s(xu)), (10)

U

1

min
Θ

|L| (cid:88)L

−

y log x

x) is the cross
y) log(1
where (cid:96)(x, y) =
−
entropy; and φ(
) is a function w.r.t. Θ deﬁned by Eq. (2).
·
We further impose L2 regularization on the parameters Θ to
avoid overﬁtting. Gradient descent algorithms can be used
to solve this problem. Embeddings learned in this way will
1) be compatible with all the labeled triples, and 2) absorb
rule knowledge carried by the unlabeled triples.

−

1Note that each gpq contains only a single unlabeled triple, i.e.,
the conclusion triple. Take π(gpq|S) deﬁned in Eq. (7) for example.
In this case, s(xu) = s(eu, rt, ev) is the soft label to be predicted
and ∇s(xu)π(gpq|S) = φ(eu, rs, ev) is a constant w.r.t. S.

1

|U| (cid:88)U
(1
−

Whole Procedure

G

Lb,
U

Gb from the labeled triples

Algorithm 1 summarizes the iterative learning procedure of
our approach. To enable efﬁcient learning, we use an online
scheme in mini-batch mode. At each iteration, we sample a
Ub, and
mini-batch
, unla-
L
beled triples
, respectively
, and propositionalized rules
(line 3).2 Soft label prediction and embedding rectiﬁcation
are then conducted locally on these mini-batches (line 4 and
line 5 respectively). This iterative procedure captures the in-
teractive nature between embedding learning and logical in-
ference: given current embeddings, logic rules can be used
to perform approximate inference and predict soft labels for
unlabeled triples; these newly labeled triples carry rich rule
knowledge and will in turn help to learn better embeddings.
In this way, knowledge contained in logic rules can be fully
transferred into the learned embeddings. Note also that our
approach is ﬂexible enough to handle soft rules with various
conﬁdence levels extracted automatically from the KG.

Discussions

We further analyze the space and time complexity, and dis-
cuss possible extensions of our approach.

Complexity. RUGE follows ComplEx to represent entities
and relations as complex-valued vectors, hence has a space
complexity of O(ned + nrd) which scales linearly w.r.t. ne,
nr, and d. Here, ne is the number of entities, nr the num-
ber of relations, and d the dimensionality of the embedding
space. During the learning procedure, each iteration requires
a time complexity of O(τ (n(cid:96)d + nud)), where n(cid:96)/nu is the
average number of labeled/unlabeled triples in a mini-batch,
and τ the number of inner epochs used for embedding recti-
n(cid:96)
ﬁcation (cf. Eq. (10)). In practice, we usually have nu (cid:28)
(see Table 2 for the number of labeled and unlabeled triples
used on our datasets), and we can also set τ to a very small
value, e.g., τ = 1. That means, RUGE has almost the same
time complexity as those most efﬁcient KG embedding tech-
niques (e.g. ComplEx) which require O(n(cid:96)d) per iteration
during training.3 In addition, RUGE further requires prepro-
cessing steps before training, i.e., rule mining and proposi-
tionalization. But these steps are performed only once, and
not required during the iterations.

Extensions. Our approach is quite generic and ﬂexible. 1)
The idea of iteratively injecting logic rules can be applied to
enhance a wide variety of embedding models, as long as an
appropriate scoring function is accordingly designed, e.g.,
the one deﬁned in Eq. (1) by ComplEx. 2) Various types of
rules can be incorporated as long as they can be modeled by
the logical compositions deﬁned in Eq. (3) to Eq. (5), and
we can even use other types of t-norm fuzzy logics to deﬁne
such compositions. 3) Rules with different conﬁdence levels
can be handled in a uniﬁed manner.

2We ﬁrst sample Lb from L. Gb is then constructed by those
whose premise triples are all contained in Lb but conclusion triples
are not. These conclusion triples are further used to construct Ub.

3Such techniques often use SGD in mini-batch mode for train-
ing, and sample a mini-batch of n(cid:96) labeled triples at each iteration.

Algorithm 1 Iterative Learning Procedure of RUGE
Require: Labeled triples L = {(x(cid:96), y(cid:96))}

Unlabeled triples U = {xu}
FOL rules F={(fp, λp)} and their groundings G={gpq}

1: Randomly initialize entity and relation embeddings Θ(0)
2: for n = 1 : N do
3:
4:

Sample a mini-batch Lb / Ub / Gb from L / U / G
1))
Sb ← SoftLabelPrediction (Ub, Gb, Θ(n
−

(cid:46) cf. Eq. (9)

Θ(n) ← EmbeddingRectiﬁcation (Lb, Ub, Sb) (cid:46) cf. Eq. (10)

5:
6: end for
Ensure: Θ(N )

Experiments
We evaluate RUGE in the link prediction task. This task is
to complete a triple (ei, rk, ej) with ei or ej missing, i.e., to
predict ei given (rk, ej) or ej given (ei, rk).
Datasets. We use two datasets: FB15K and YAGO37. The
former is a subgraph of Freebase containing 1,345 relations
and 14,951 entities, released by Bordes et al. (2013).4 The
latter is extracted from the core facts of YAGO3.5 During
the extraction, entities appearing less than 10 times are dis-
carded. The ﬁnal dataset consists of 37 relations and 123,189
entities. Triples on both datasets are split into training, vali-
dation, and test sets, used for model training, hyperparame-
ter tuning, and evaluation, respectively. We use the original
split for FB15K, and draw a split of 989,132/50,000/50,000
triples for YAGO37.

Note that on both datasets, the training sets contain only
positive triples. Negative triples are generated using the local
closed world assumption (Dong et al. 2014). This negative
sampling procedure is performed at runtime for each batch
of training positive triples. Such positive and negative triples
(along with their hard labels) form our labeled triple set.

We further employ AMIE+ (Gal´arraga et al. 2015)6 to au-
tomatically extract Horn clause rules from each dataset, with
the training set as input. To enable efﬁcient extraction, we
consider rules with length not longer than 2 and conﬁdence
levels not less than 0.8.7 The length of a Horn clause rule is
x, y :
the number of atoms appearing in its premise, e.g.,
(x, Nationality, y) has the
(x, BornInCountry, y)
length of 1. And the conﬁdence threshold of 0.8 leads to the
best performance on both datasets (detailed later). Using this
setting, we extract 454 (universally quantiﬁed) Horn clause
rules from FB15K, and 16 such rules from YAGO37. Table 1
shows some examples with their conﬁdence levels.

⇒

∀

Then, we instantiate these rules with concrete entities, i.e.,
propositionalization. Propositionalized rules whose premise
triples are all contained in the training set (while conclusion

4https://everest.hds.utc.fr/doku.php?id=en:smemlj12
5http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago/downloads/

6https://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/amie/

7AMIE+ provides two types of conﬁdence, i.e. standard conﬁ-

dence and PCA conﬁdence. This paper uses PCA conﬁdence.

Table 1: Horn clause rules with conﬁdence levels extracted
by AMIE+ from FB15K (top) and YAGO37 (bottom).

/location/people born here(x,y)
/director/ﬁlm(x,y)
⇒
/ﬁlm/directed by(x,y)

⇒
/ﬁlm/directed by(y,x)

/people/place of birth(y,x)

/person/language(y,z)

/ﬁlm/language(x,z)

∧

⇒

isMarriedTo(x,y)
hasChild(x,y)
playsFor(x,y)

∧
⇒

⇒

isMarriedTo(y,x)

isCitizenOf(y,z)
⇒
isAfﬁliatedTo(x,y)

isCitizenOf(x,z)

1.00
0.99
0.88

0.97
0.94
0.86

Table 2: Statistics of datasets, where ne/nr denotes the num-
ber of entities/relations, n(cid:96)/nu/ng is the number of labeled
triples/unlabeled triples/valid groundings used for training,
and nv/nt denotes the number of validation/test triples.

Train

Valid

Test

Dataset

ne

nr

n(cid:96)

nu

ng

nv

nt

FB15K
YAGO37

14,951
123,189

1,345
37

483,142
989,132

74,707
69,680

96,724
72,670

50,000
50,000

59,071
50,000

triples are not) are taken as valid groundings and used during
embedding learning. We obtain 96,724 valid groundings on
FB15K and 72,670 on YAGO37. Conclusion triples of these
valid groundings are further collected to form our unlabeled
triple set. We ﬁnally get 74,707 unlabeled triples on FB15K
and 69,680 on YAGO37. Table 2 provides some statistics of
the two datasets.
Evaluation Protocol. To evaluate the performance in link
prediction, we follow the standard protocol used in (Bordes
et al. 2013). For each test triple (ei, rk, ej), we replace the
head entity ei with each entity e(cid:48)i ∈ E
, and calculate the score
for (e(cid:48)i, rk, ej). Ranking these scores in descending order,
we get the rank of the correct entity ei. Similarly, we can get
another rank by replacing the tail entity. Aggregated over all
test triples, we report three metrics: 1) the mean reciprocal
rank (MRR), 2) the median of the ranks (MED), and 3) the
proportion of ranks no larger than n (HITS@N). During this
ranking process, we remove corrupted triples which already
exist in either the training, validation, or test set, since they
themselves are true triples. This corresponds to the “ﬁltered”
setting in (Bordes et al. 2013).
Comparison Settings. We compare RUGE with four state-
of-the-art basic embedding models, including TransE (Bor-
des et al. 2013), DistMult (Yang et al. 2015), HolE (Nickel,
Rosasco, and Poggio 2016), and ComplEx (Trouillon et al.
2016). These basic models rely only on triples observed in
a KG and use no rules. We further take PTransE (Lin et al.
2015a) and KALE (Guo et al. 2016) as additional baselines.
Both of them are extensions of TransE, with the former in-
tegrating relation paths (Horn clauses), and the latter FOL
rules (hard rules) in a one-time injection manner. In contrast,
RUGE incorporates soft rules and transfers rule knowledge
into KG embedding in an iterative manner.

We use the code provided by Trouillon et al. (2016)8 for

8https://github.com/ttrouill/complex

TransE, DistMult, and ComplEx, and reimplement HolE so
that all these four basic models share the identical mode of
optimization, i.e., SGD with AdaGrad (Duchi, Hazan, and
Singer 2011) and gradient normalization. As such, we repro-
duce the results of TransE, DistMult, and ComplEx reported
on FB15K (Trouillon et al. 2016), and improve the results of
HolE substantially compared to those reported in the origi-
nal paper (Nickel, Rosasco, and Poggio 2016).9 The code for
PTransE is provided by its authors.10 We implement KALE
and RUGE in Java, both using SGD with AdaGrad and gra-
dient normalization to facilitate a fair comparison.

There are two types of loss functions that could be used
for these baselines, i.e., the logistic loss or the pairwise rank-
ing loss (Nickel, Rosasco, and Poggio 2016). Trouillon et
al. (2016) have recently demonstrated that the logistic loss
generally performs better than the pairwise ranking loss, ex-
cept for TransE. So, for TransE and its extensions (PTransE
and KALE) we use the pairwise ranking loss, and for all the
other baselines we use the logistic loss. To extract relation
paths for PTransE, we follow the optimal conﬁguration re-
ported in (Lin et al. 2015a), where paths constituted by at
most 3 relations are included. For KALE and RUGE, we use
the same set of propositionalized rules to make it a fair com-
parison.11

0.001, 0.01, 0.1, 1

For all the methods, we create 100 mini-batches on each
dataset, and tune the embedding dimensionality d in
50,
{
, the number of negatives per positive triple α
100, 150, 200
}
0.01, 0.05, 0.1,
, the initial learning rate γ in
1, 2, 5, 10
in
{
}
{
0.5, 1.0
0.001,
, and the L2 regularization coefﬁcient λ in
{
}
0.003, 0.01, 0.03, 0.1
. For TransE and its extensions which
}
use the pairwise ranking loss, we further tune the margin δ in
. The slackness penalty C in RUGE
0.1, 0.2, 0.5, 1, 2, 5, 10
{
}
, and the
(cf. Eq. (8)) is selected from
}
{
number of inner iterations (cf. Eq. (10)) is ﬁxed to τ = 1.
Best models are selected by early stopping on the validation
set (monitoring MRR), with at most 1000 iterations over the
training set. The optimal conﬁgurations for RUGE are: d =
200, α = 10, γ = 0.5, λ = 0.01, C = 0.01 on FB15K; and
d = 150, α = 10, γ = 1.0, λ = 0.003, C = 0.01 on YAGO37.
Link Prediction Results. Table 3 shows the results of these
methods on the test sets of FB15K and YAGO37. The results
indicate that RUGE signiﬁcantly and consistently outper-
forms all the baselines on both datasets and in all metrics. It
beats not only the four basic models which use triples alone
(TransE, DistMult, HolE, and ComplEx), but also PTransE
and KALE which further incorporate logic rules (or relation
paths) in a one-time injection manner. This demonstrates the
superiority of injecting logic rules into KG embedding, par-
ticularly in an iterative manner. Compared to the best per-
forming baseline ComplEx (this is also the model based on
which RUGE is designed), RUGE achieves an improvement
of 11%/18% in MRR/HITS@1 on FB15K, and an improve-
ment of 3%/6% on YAGO37. The improvements on FB15K

9HolE in its original implementation uses SGD with AdaGrad,

but no gradient normalization.

10https://github.com/thunlp/KB2E
11KALE takes all these groundings as hard rules. This approxi-
mation works quite well with an appropriate conﬁdence threshold.

Table 3: Link prediction results on the test sets of FB15K and YAGO37. As baselines, rows 1-4 are the four basic models which
use triples alone, and rows 5-6 further integrate logic rules (or relation paths) in a one-time injection manner.

FB15K

HITS@N

YAGO37

HITS@N

Method

TransE
DistMult
HolE
ComplEx
PTransE
KALE

RUGE

MRR

0.400
0.644
0.600
0.690
0.679
0.523

0.768

MED

1

4.0
1.0
2.0
1.0
1.0
2.0

1.0

0.246
0.532
0.485
0.598
0.565
0.383

0.703

3

0.495
0.730
0.673
0.756
0.768
0.616

0.815

5

0.576
0.769
0.722
0.793
0.810
0.683

0.836

10

0.662
0.812
0.779
0.837
0.855
0.762

0.865

MRR

0.303
0.365
0.380
0.417
0.403
0.321

0.431

MED

1

13.0
6.0
7.0
4.0
9.0
9.0

4.0

0.218
0.262
0.288
0.320
0.339
0.215

0.340

3

0.336
0.411
0.420
0.471
0.444
0.372

0.482

5

0.387
0.493
0.479
0.533
0.473
0.438

0.541

10

0.475
0.575
0.551
0.603
0.506
0.522

0.603

Table 4: Runtime (in sec.) on FB15K and YAGO37. Extr. is
the time required for rule/path extraction, Prop. for proposi-
tionalization, and Lean. for training per iteration.

FB15K

YAGO37

Method

Extr. Prop. Learn.

Extr. Prop. Learn.

ComplEx
PTransE
KALE
RUGE

—
868.4
43.1
43.1

— 11.4
— 46.5
4.8
4.0
14.1
4.0

—
13939.5
337.5
337.5

— 49.5
— 23.8
27.5
55.2

13.8
13.8

we set d = 200 (embedding dimensionality) and α = 2 (num-
ber of negatives per positive triple) for all the methods. Other
hyperparameters are ﬁxed to their optimal conﬁgurations de-
termined in link prediction. We can see that RUGE is still
quite efﬁcient despite integrating additional rules. The aver-
age training time per iteration increases from 11.4 to 14.1
on FB15K, and from 49.5 to 55.2 on YAGO37. The prepro-
cessing steps, although performed only once, are also highly
efﬁcient, requiring much less time compared to PTransE.

Conclusion

This paper proposes a novel paradigm that learns entity and
relation embeddings with iterative guidance from soft rules,
referred to as RUGE. It enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. Each iteration alternates
between 1) a soft label prediction stage which predicts soft
labels for unlabeled triples using currently learned embed-
dings and soft rules, and 2) an embedding rectiﬁcation stage
which further integrates both labeled and unlabeled triples
to update current embeddings. This iterative procedure may
better transfer the knowledge contained in logic rules into
the learned embeddings. Link prediction results on Freebase
and YAGO show that RUGE achieves signiﬁcant and con-
sistent improvements over state-of-the-art baselines. More-
over, RUGE demonstrates the usefulness of automatically
extracted soft rules. Even those with moderate conﬁdence
levels can be highly beneﬁcial to KG embedding.

Figure 2: MRR achieved by RUGE with different conﬁdence
thresholds on the test set of FB15K.

are more substantial than those on YAGO37. The reason is
probably that FB15K contains more relations from which a
good range of rules can be extracted (454 universally quan-
tiﬁed rules from FB15K, and 16 from YAGO37).
Inﬂuence of Conﬁdence Levels. We further investigate the
inﬂuence of the threshold of rules’ conﬁdence levels used
in RUGE. To do so, we ﬁx all the hyperparameters to the
optimal conﬁgurations determined by the previous experi-
ment, and vary the conﬁdence threshold in [0.1, 1] with a
step 0.05. Fig. 2 shows MRR achieved by RUGE with var-
ious thresholds on the test set of FB15K. We can see that
the threshold of 0.8 is a good tradeoff and indeed performs
best. A threshold higher than that will reduce the number of
rules that can be extracted, while a one lower than that might
introduce too many less credible rules. Both hurt the perfor-
mance. However, even so, RUGE outperforms ComplEx by
a large margin, with the threshold set in a broad range of
[0.35, 0.9]. This observation indicates that soft rules, even
those with moderate conﬁdence levels, are highly beneﬁcial
to KG embedding despite their uncertainties.
Comparison of Runtime. Finally, we compare RUGE with
ComplEx, PTransE, and KALE in their runtime.12 ComplEx
is a basic model which only requires model training. RUGE
as well as the other two baselines further require preprocess-
ing of rule/path extraction and propositionalization. Table 4
lists the runtime of these methods required for each step on
FB15K and YAGO37. Here, to facilitate a fair comparison,

12The other three baselines are implemented in Python and much

slower. So they are not considered here.

Acknowledgments
The authors would like to thank all the reviewers for their
insightful and valuable suggestions, which signiﬁcantly im-
prove the quality of this paper. This work is supported by the
National Key Research and Development Program of China
(grants No. 2016YFB0801003 and No. 2016QY03D0503),
the Fundamental Theory and Cutting Edge Technology Re-
search Program of the Institute of Information Engineering,
Chinese Academy of Sciences (grant No. Y7Z0261101), and
the National Natural Science Foundation of China (grant No.
61402465).

References
[Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.;
Sturge, T.; and Taylor, J. 2008. Freebase: A collaboratively created
In SIGMOD,
graph database for structuring human knowledge.
1247–1250.

[Bordes et al. 2013] Bordes, A.; Usunier, N.; Garc´ıa-Dur´an, A.;
Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for
modeling multi-relational data. In NIPS, 2787–2795.

[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio,
Y. 2014. A semantic matching energy function for learning with
multi-relational data. MACH LEARN 94(2):233–259.

[Carlson et al. 2010] Carlson, A.; Betteridge, J.; Kisiel, B.; Settles,
B.; Hruschka Jr, E. R.; and Mitchell, T. M. 2010. Toward an archi-
tecture for never-ending language learning. In AAAI, 1306–1313.
T.;
Rockt¨aschel, T.; and Riedel, S. 2016. Lifted rule injection for
relation embeddings. In EMNLP, 1389–1399.

[Demeester, Rockt¨aschel, and Riedel 2016] Demeester,

[Dong et al. 2014] Dong, X.; Gabrilovich, E.; Heitz, G.; Horn, W.;
Lao, N.; Murphy, K.; Strohmann, T.; Sun, S.; and Zhang, W. 2014.
Knowledge vault: A web-scale approach to probabilistic knowl-
edge fusion. In SIGKDD, 601–610.

[Duchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and Singer,
Y. 2011. Adaptive subgradient methods for online learning and
stochastic optimization. J MACH LEARN RES 12(Jul):2121–2159.
[Faruqui et al. 2014] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer,
C.; Hovy, E.; and Smith, N. A. 2014. Retroﬁtting word vectors to
semantic lexicons. arXiv:1411.4166.

[Gal´arraga et al. 2013] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2013. AMIE: Association rule mining under
In WWW,
incomplete evidence in ontological knowledge bases.
413–422.

[Gal´arraga et al. 2015] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2015. Fast rule mining in ontological knowl-
edge bases with AMIE+. VLDB J 24(6):707–730.

[Gardner, Talukdar, and Mitchell 2015] Gardner, M.; Talukdar, P.;
and Mitchell, T. 2015. Combining vector space embeddings with
symbolic logical inference over open-domain text. In AAAI Spring
Symposium Series, 61–65.

[Guo et al. 2015] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2015. Semantically smooth knowledge graph embedding. In
ACL, 84–94.

[Guo et al. 2016] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2016. Jointly embedding knowledge graphs and logical rules.
In EMNLP, 192–202.

[Guu, Miller, and Liang 2015] Guu, K.; Miller, J.; and Liang, P.
2015. Traversing knowledge graphs in vector space. In EMNLP,
318–327.

[H´ajek 1998] H´ajek, P. 1998. The metamathematics of fuzzy logic.

Kluwer.

[Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E.
2016. Harnessing deep neural networks with logic rules. In ACL,
2410–2420.

[Lin et al. 2015a] Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and
Liu, S. 2015a. Modeling relation paths for representation learning
of knowledge bases. In EMNLP, 705–714.

[Lin et al. 2015b] Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X.
2015b. Learning entity and relation embeddings for knowledge
graph completion. In AAAI, 2181–2187.

[Liu et al. 2016] Liu, Q.; Jiang, H.; Evdokimov, A.; Ling, Z.-H.;
Zhu, X.; Wei, S.; and Hu, Y. 2016. Probabilistic reasoning via
deep learning: Neural association models.

[Miller 1995] Miller, G. A. 1995. WordNet: A lexical database for

English. COMMUN ACM 38(11):39–41.

[Neelakantan, Roth, and McCallum 2015] Neelakantan, A.; Roth,
B.; and McCallum, A. 2015. Compositional vector space mod-
els for knowledge base completion. In ACL, 156–166.

[Nickel, Rosasco, and Poggio 2016] Nickel, M.; Rosasco, L.; and
Poggio, T. 2016. Holographic embeddings of knowledge graphs.
In AAAI, 1955–1961.

[Nickel, Tresp, and Kriegel 2011] Nickel, M.; Tresp, V.;

and
Kriegel, H.-P. 2011. A three-way model for collective learning on
multi-relational data. In ICML, 809–816.

[Rockt¨aschel et al. 2014] Rockt¨aschel, T.; Boˇsnjak, M.; Singh, S.;
and Riedel, S. 2014. Low-dimensional embeddings of logic. In
ACL Workshop on Semantic Parsing, 45–49.

[Rockt¨aschel, Singh, and Riedel 2015] Rockt¨aschel, T.; Singh, S.;
and Riedel, S. 2015. Injecting logical background knowledge into
embeddings for relation extraction. In NAACL, 1119–1129.

[Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng,
A. Y. 2013. Reasoning with neural tensor networks for knowledge
base completion. In NIPS, 926–934.

[Suchanek, Kasneci, and Weikum 2007] Suchanek, F. M.; Kasneci,
G.; and Weikum, G. 2007. YAGO: A core of semantic knowledge.
In WWW, 697–706.

[Trouillon et al. 2016] Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,
E.; and Bouchard, G. 2016. Complex embeddings for simple link
prediction. In ICML, 2071–2080.

[Vendrov et al. 2015] Vendrov, I.; Kiros, R.; Fidler, S.; and Ur-
2015. Order-embeddings of images and language.

tasun, R.
arXiv:1511.06361.

[Wang and Cohen 2016] Wang, W. Y., and Cohen, W. W. 2016.
Learning ﬁrst-order logic embeddings via matrix factorization. In
IJCAI, 2132–2138.

[Wang et al. 2014] Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z.
2014. Knowledge graph embedding by translating on hyperplanes.
In AAAI, 1112–1119.

[Wang et al. 2017] Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017.
Knowledge graph embedding: A survey of approaches and appli-
cations. IEEE TRANS KNOWL DATA ENG 29(12):2724–2743.
[Wang, Wang, and Guo 2015] Wang, Q.; Wang, B.; and Guo, L.
2015. Knowledge base completion using embeddings and rules.
In IJCAI, 1859–1865.

[Wei et al. 2015] Wei, Z.; Zhao, J.; Liu, K.; Qi, Z.; Sun, Z.; and
Tian, G. 2015. Large-scale knowledge base completion: Infer-
ring via grounding network sampling over selected instances. In
CIKM, 1331–1340.

[Weston et al. 2013] Weston, J.; Bordes, A.; Yakhnenko, O.; and
Usunier, N. 2013. Connecting language and knowledge bases with
embedding models for relation extraction. In EMNLP, 1366–1371.
[Xiao, Huang, and Zhu 2017] Xiao, H.; Huang, M.; and Zhu, X.
2017. SSP: Semantic space projection for knowledge graph em-
bedding with text descriptions. In AAAI, 3104–3110.

[Xie et al. 2016] Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M.
2016. Representation learning of knowledge graphs with entity
descriptions. In AAAI, 2659–2665.

[Xie, Liu, and Sun 2016] Xie, R.; Liu, Z.; and Sun, M. 2016. Rep-
resentation learning of knowledge graphs with hierarchical types.
In IJCAI, 2965–2971.

[Xiong, Power, and Callan 2017] Xiong, C.; Power, R.; and Callan,
J. 2017. Explicit semantic ranking for academic search via knowl-
edge graph embedding. In WWW, 1271–1279.

[Yang et al. 2015] Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng,
L. 2015. Embedding entities and relations for learning and infer-
ence in knowledge bases. In ICLR.

[Zhang et al. 2016] Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and
Ma, W.-Y. 2016. Collaborative knowledge base embedding for
recommender systems. In SIGKDD, 353–362.

Knowledge Graph Embedding with Iterative Guidance from Soft Rules

Shu Guo1,2, Quan Wang1,2,3

∗, Lihong Wang4, Bin Wang1,2, Li Guo1,2

1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3State Key Laboratory of Information Security, Chinese Academy of Sciences
4National Computer Network Emergency Response Technical Team & Coordination Center of China

7
1
0
2
 
v
o
N
 
0
3
 
 
]
I

A
.
s
c
[
 
 
1
v
1
3
2
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Embedding knowledge graphs (KGs) into continuous vector
spaces is a focus of current research. Combining such an em-
bedding model with logic rules has recently attracted increas-
ing attention. Most previous attempts made a one-time injec-
tion of logic rules, ignoring the interactive nature between
embedding learning and logical inference. And they focused
only on hard rules, which always hold with no exception and
usually require extensive manual effort to create or validate.
In this paper, we propose Rule-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from 1) labeled triples that have been directly
observed in a given KG, 2) unlabeled triples whose labels are
going to be predicted iteratively, and 3) soft rules with vari-
ous conﬁdence levels extracted automatically from the KG. In
the learning process, RUGE iteratively queries rules to obtain
soft labels for unlabeled triples, and integrates such newly la-
beled triples to update the embedding model. Through this
iterative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings. We evalu-
ate RUGE in link prediction on Freebase and YAGO. Exper-
imental results show that: 1) with rule knowledge injected it-
eratively, RUGE achieves signiﬁcant and consistent improve-
ments over state-of-the-art baselines; and 2) despite their un-
certainties, automatically extracted soft rules are highly bene-
ﬁcial to KG embedding, even those with moderate conﬁdence
levels. The code and data used for this paper can be obtained
from https://github.com/iieir-km/RUGE.

Introduction
Knowledge graphs (KGs) such as WordNet (Miller 1995),
Freebase (Bollacker et al. 2008), YAGO (Suchanek, Kas-
neci, and Weikum 2007), and NELL (Carlson et al. 2010)
are extremely useful resources for many AI related applica-
tions. A KG is a multi-relational graph composed of entities
as nodes and relations as different types of edges. Each edge
is represented as a triple (head entity, relation, tail entity),
indicating that there is a speciﬁc relation between two enti-
ties, e.g., (Paris, CapitalOf, France). Although effective
in representing structured data, the underlying symbolic na-
ture of such triples often makes KGs hard to manipulate.

∗Corresponding author: Quan Wang (wangquan@iie.ac.cn).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recently, a new research direction termed as knowledge
graph embedding has been proposed and quickly received
massive attention (Nickel, Tresp, and Kriegel 2011; Bordes
et al. 2013; Wang et al. 2014; Lin et al. 2015b; Yang et al.
2015; Nickel, Rosasco, and Poggio 2016; Trouillon et al.
2016). The key idea is to embed entities and relations in a
KG into a low-dimensional continuous vector space, so as
to simplify the manipulation while preserving the inherent
structure of the KG. Such embeddings contain rich semantic
information, and can beneﬁt a broad range of downstream
applications (Weston et al. 2013; Bordes et al. 2014; Zhang
et al. 2016; Xiong, Power, and Callan 2017).

Traditional methods performed embedding based solely
on triples observed in a KG. But considering the power of
logic rules in knowledge acquisition and inference, combin-
ing embedding models with logic rules has become a focus
of current research (Rockt¨aschel et al. 2014; Vendrov et al.
2015; Wang and Cohen 2016; Hu et al. 2016). Wang et al.
(2015) and Wei et al. (2015) tried to use embedding models
and logic rules for KG completion. But in their work, rules
are modeled separately from embedding models, and would
not help to learn more predictive embeddings. Rockt¨aschel
et al. (2015) and Guo et al. (2016) then devised joint learning
paradigms which can inject ﬁrst-order logic (FOL) into KG
embedding. Demeester et al. (2016) further proposed lifted
rule injection to avoid the costly propositionalization of FOL
rules. Although these joint models are able to learn better
embeddings after integrating logic rules, they still have their
drawbacks and restrictions.

First of all, these joint models made a one-time injection
of logic rules, taking them as additional rule-based training
instances (Rockt¨aschel, Singh, and Riedel 2015) or regular-
ization terms (Demeester, Rockt¨aschel, and Riedel 2016).
We argue that rules can better enhance KG embedding, how-
ever, in an iterative manner. Given the learned embeddings
and their rough predictions, rules can be used to reﬁne the
predictions and infer new facts. The newly inferred facts, in
turn, will help to learn better embeddings and more accurate
logical inference. Previous methods fail to model such in-
teractions between embedding models and logic rules. Fur-
thermore, they focused only on hard rules which always hold
with no exception. Such rules usually require extensive man-
ual effort to create or validate. Actually, besides hard rules,
a signiﬁcant amount of background information can be en-

proach is quite generic and ﬂexible. It can integrate various
types of rules with different conﬁdence levels to enhance a
good variety of KG embedding models.

Related Work
Recent years have witnessed increasing interest in learning
distributed representations for entities and relations in KGs,
a.k.a. KG embedding. Various techniques have been devised
for this task, e.g., translation-based models which take re-
lations as translating operations between head and tail enti-
ties (Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015b),
simple compositional models which match compositions of
head-tail entity pairs with their relations (Nickel, Tresp, and
Kriegel 2011; Yang et al. 2015; Nickel, Rosasco, and Pog-
gio 2016; Trouillon et al. 2016), and neural networks which
further introduce non-linear layers and deep architectures
(Socher et al. 2013; Bordes et al. 2014; Dong et al. 2014;
Liu et al. 2016). Among these techniques, ComplEx (Trouil-
lon et al. 2016), a compositional model which represents
entities and relations as complex-valued vectors, achieves a
very good trade-off between accuracy and efﬁciency. Most
of the currently available techniques perform the embedding
task based solely on triples observed in a KG. Some recent
work further tried to use other information, e.g., entity types
(Guo et al. 2015; Xie, Liu, and Sun 2016) and textual de-
scriptions (Xie et al. 2016; Xiao, Huang, and Zhu 2017), to
learn more predictive embeddings. See (Wang et al. 2017)
for a thorough review of KG embedding techniques.

Given the power of logic rules in knowledge acquisition
and inference, combining KG embedding with logic rules
becomes a focus of current research. Wang et al. (2015) and
Wei et al. (2015) devised pipelined frameworks which use
logic rules to further reﬁne predictions made by embedding
models. In their work, rules will not help to learn better em-
beddings. Rockt¨aschel et al. (2015) and Guo et al. (2016)
then tried to learn KG embeddings jointly from triples and
propositionalized FOL rules. Demeester et al. (2016) further
proposed lifted rule injection to avoid the costly proposi-
tionalization. These joint models, however, made a one-time
injection of logic rules, ignoring the interactive nature be-
tween embedding learning and logical inference. Moreover,
they can only handle hard rules which are usually manually
created or validated.

Besides logic rules, relation paths which can be regarded
as Horn clauses and get a strong connection to logical infer-
ence (Gardner, Talukdar, and Mitchell 2015), have also been
studied in KG embedding (Neelakantan, Roth, and McCal-
lum 2015; Lin et al. 2015a; Guu, Miller, and Liang 2015).
But in these methods, relation paths are incorporated, again,
in a one-time manner. Our approach, in contrast, iteratively
injects knowledge contained in logic rules into KG embed-
ding, and is able to handle soft rules with various conﬁdence
levels extracted automatically from KGs.

Combining logic rules with distributed representations is
also an active research topic in other contexts outside KGs.
Faruqui et al. (2014) tried to inject ontological knowledge
from WordNet into word embeddings. Vendrov et al. (2015)
introduced order-embedding to model the partial order struc-
ture of hypernymy, textual entailment, and image caption-

Figure 1: Framework overview. RUGE enables an embed-
ding model to learn simultaneously from labeled triples, un-
labeled triples, and soft rules in an iterative manner, where
each iteration alternates between a soft label prediction stage
and an embedding rectiﬁcation stage.

coded as soft rules, e.g., “a person is very likely (but not nec-
essarily) to have a nationality of the country where he/she
was born”. Soft rules can be extracted automatically and ef-
ﬁciently via modern rule mining systems (Gal´arraga et al.
2013; Gal´arraga et al. 2015). Yet, despite this merit, soft
rules have not been well studied in previous methods.

This paper proposes RUle-Guided Embedding (RUGE), a
novel paradigm of KG embedding with iterative guidance
from soft rules. As sketched in Fig. 1, it enables an embed-
ding model to learn simultaneously from 1) labeled triples
that have been directly observed in a given KG, 2) unlabeled
triples whose labels are going to be predicted iteratively, and
3) soft rules with different conﬁdence levels extracted auto-
matically from the KG. During each iteration of the learning
process, the model alternates between a soft label prediction
stage and an embedding rectiﬁcation stage. The former uses
currently learned embeddings and soft rules to predict soft
labels for unlabeled triples, and the latter further integrates
both labeled and unlabeled triples (with hard and soft labels
respectively) to update current embeddings. Through this it-
erative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings.

We empirically evaluate RUGE on large scale public KGs,
namely Freebase and YAGO. Experimental results reveal
that: 1) by incorporating logic rules, RUGE signiﬁcantly and
consistently improves over state-of-the-art basic embedding
models (without rules); 2) compared to those one-time in-
jection schemes studied before, the iterative injection strat-
egy maximizes the utility of logic rules for KG embedding,
and indeed achieves substantially better performance; 3) de-
spite the uncertainties, automatically extracted soft rules are
highly beneﬁcial to KG embedding, even those with moder-
ate conﬁdence levels.

The contributions of this paper are threefold. 1) We devise
a novel paradigm of KG embedding which iteratively injects
logic rules into the learned embeddings. To our knowledge,
this is the ﬁrst work that models interactions between em-
bedding learning and logical inference in a principled frame-
work. 2) We demonstrate the usefulness of automatically ex-
tracted soft rules in KG embedding, thereby eliminating the
requirement of laborious manual rule creation. 3) Our ap-

ing. Hu et al. (2016) proposed to enhance various types of
neural networks with FOL rules. All these studies demon-
strate the capability of logic rules to enhance distributed rep-
resentation learning.

Rule-Guided Knowledge Graph Embedding
This section introduces RUle-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. During each iteration, the
model alternates between a soft label prediction stage and
an embedding rectiﬁcation stage. Fig. 1 sketches this overall
framework. In what follows, we ﬁrst describe our learning
resources, and then detail the two alternating stages.

O

and

, where

(ei, rk, ej)
}
{

Learning Resources
Suppose we are given a KG with a set of triples observed,
=
i.e.,
. Each triple is composed of two en-
and their relation rk ∈ R
tities ei, ej ∈ E
R
are the sets of entities and relations respectively. We obtain
our learning resources (i.e., labeled triples, unlabeled triples,
and soft rules) and model them as follows.
Labeled Triples. We take the triples observed in
as pos-
itive ones. For each positive triple (ei, rk, ej), we randomly
corrupt the head ei or the tail ej, to form a negative triple
and e(cid:48)j ∈
(e(cid:48)i, rk, ej) or (ei, rk, e(cid:48)j), where e(cid:48)i ∈ E \ {
. We denote a labeled triple as x(cid:96), and associate with
E \ {
it a label y(cid:96) = 1 if x(cid:96) is positive, and y(cid:96) = 0 otherwise. Let
=
denote the set of these labeled triples (along

ei}

O

E

ej}
(x(cid:96), y(cid:96))
}

{

{

U

F

=

=

O

(fp, λp)
}

L
with their labels).
Unlabeled Triples. Besides the labeled triples, we collect a
, where xu = (ei, rk, ej)
xu}
set of unlabeled triples
{
indicates an unlabeled triple. In fact, all the triples that have
not been observed in
can be taken as unlabeled ones. But
in this paper, we consider only those encoded in the conclu-
sion of a soft rule, as detailed below.
Soft Rules. We also consider a set of FOL rules with differ-
P
ent conﬁdence levels, denoted as
p=1. Here,
fp is the p-th logic rule deﬁned over the given KG, repre-
(x, rt, y),
sented, e.g., in the form of
∀
stating that two entities linked by relation rs might also
be linked by relation rt. The left-hand side of the im-
” is called the premise, and the right-hand
plication “
side the conclusion. In this paper, we restrict fp
to be
a Horn clause rule, where the conclusion contains only
a single atom and the premise is a conjunction of sev-
eral atoms. The conﬁdence level of rule fp is denoted
[0, 1]. Rules with higher conﬁdence levels are
as λp ∈
more likely to hold, and a conﬁdence level of λp = 1
indicates a hard rule which always holds with no excep-
tion. Such rules as well as their conﬁdence levels can be
extracted automatically from the KG (with the observed
triple set
as input), by using modern rule mining systems
like AMIE and AMIE+ (Gal´arraga et al. 2013; Gal´arraga et al. 2015).

x, y : (x, rs, y)

⇒

⇒

O

We then propositionalize these rules to get their ground-
ings. Here a grounding is the logical expression with all vari-
. For instance, a
ables instantiated with concrete entities in

E

.

E

∀

O

universally quantiﬁed rule

x, y : (x, BornInCountry, y)
(x, Nationality, y) could be instantiated with two en-
⇒
tities EmmanuelMacron and France, and gives a resultant
grounding (EmmanuelMacron, BornInCountry, France)
(EmmanuelMacron, Nationality, France). Ob-
⇒
there could be a huge number of groundings,
viously,
In this
especially given a large entity vocabulary
paper,
to maximize the utility for knowledge acquisi-
tion and inference, we take as valid groundings only
those where premise triples are observed in
while
conclusion triples are not. That means the aforemen-
tioned grounding will be considered as valid if the triple
(EmmanuelMacron, BornInCountry, France)
but
(EmmanuelMacron, Nationality, France)
/
. For
∈ O
Qp
each FOL rule fp, let
q=1 denote the set of
its valid groundings. All the premise triples of gpq are
contained in
, but the single conclusion triple is not.
These conclusion triples are further used to construct our
unlabeled triple set
. That means, our unlabeled triples are
those which are not directly observed in the KG but could
be inferred by the rules with high probabilities.
Modeling Triples and Rules. Given the labeled triples
unlabeled triples
{Gp}

,
L
, and the valid groundings of FOL rules
P
p=1, we discuss how to model these triples and
G
rules in the context of KG embedding. To model triples, we
follow ComplEx (Trouillon et al. 2016), a recently proposed
method which is simple and efﬁcient while achieving state-
of-the-art predictive performance. Speciﬁcally, we assume
entities and relations to have complex-valued vector embed-
dings. Given a triple (ei, rk, ej)
, we score it by
a multi-linear dot product:

∈ E ×R×E

Gp =

gpq}
{

∈ O

O

=

U

U

m

(cid:88)

) = Re(

ei, rk, ¯ej(cid:105)
[ei]m[rk]m[¯ej]m), (1)
ηijk = Re(
(cid:104)
Cd are the complex-valued vector em-
where ei, ej, rk ∈
beddings associated with ei, ej, and rk, respectively; ¯ej is
the conjugate of ej; [
]m is the m-th entry of a vector; and
·
) means taking the real part of a complex value. We fur-
Re(
·
ther introduce a mapping function φ :
(0, 1), so
as to map the score ηijk to a continuous truth value which
lies in the range of (0, 1), i.e.,

E×R×E →

,

(cid:0)

(cid:1)

Re(

φ(ei, rk, ej) = σ(ηijk) = σ

ei, rk, ¯ej(cid:105)
)
(2)
(cid:104)
where σ(x) = 1/(1 + exp(
x)) denotes the sigmoid func-
−
tion. Triples with higher truth values are more likely to hold.
To model propositionalized rules (i.e. groundings), we use
t-norm based fuzzy logics (H´ajek 1998). The key idea is to
model the truth value of a propositionalized rule as a com-
position of the truth values of its constituent triples, through
). For instance,
speciﬁc logical connectives (e.g.
⇒
the truth value of a grounded rule (eu, rs, ev)
(eu, rt, ev)
will be determined by the truth values of the two triples
(eu, rs, ev) and (eu, rt, ev), via a composition deﬁned by
logical implication. We follow (Guo et al. 2016) and deﬁne
the compositions associated with logical conjunction (
),
disjunction (

) as:

and

⇒

∧

∧

), and negation (
¬
b) = π(a)
π(b),
b) = π(a) + π(b)
a) = 1

π(a).

¬
π(a
π(a
π(

·

∧
∨
¬

−

π(a)

π(b),

−

·

(3)
(4)
(5)

Here, a and b are two logical expressions, which can either
be single triples or be constructed by combining triples with
logical connectives; and π(a) is the truth value of a, indi-
cating to what degree the logical expression is true. If a is a
single triple, say (ei, rk, ej), we have π(a) = φ(ei, rk, ej),
as deﬁned in Eq. (2). Given these compositions, the truth
value of any logical expression can be calculated recursively
(Guo et al. 2016), e.g.,

π(a

b) = π(

a

b) = π(a)

π(b)

π(a) + 1.

(6)

⇒

¬

∨

·

−

r
}r
∈E ∪ {

Logical expressions with higher truth values have greater de-
e
grees to be true. Let Θ =
denote the set of
}e
{
all entity and relation embeddings. The proposed approach,
RUGE, then aims to learn these embeddings by using the
labeled triples
, and valid groundings
P
p=1 in an iterative manner, where each iteration alter-
{Gp}
nates between a soft label prediction stage and an embedding
rectiﬁcation stage.

, unlabeled triples

∈R

L

U

Soft Label Prediction
This stage is to use currently learned embeddings and propo-
sitionalized rules to predict soft labels for unlabeled triples.
Speciﬁcally, let n be the iteration index, and Θ(n
1) the set
of current embeddings learned from the previous iteration.
Recall that we are given a set of P FOL rules with their con-
P
(fp, λp)
p=1, and each FOL rule fp has
ﬁdence levels
}
{
Qp
Qp valid groundings
gpq}
Gp =
q=1. Our aim is to predict a
{
soft label s(xu)
, by
∈
1) and all the groundings
using the current embeddings Θ(n

[0, 1] for each unlabeled triple xu ∈ U

=

F

−

−

P
p=1.

G

{Gp}

=
To do so, we solve a rule-constrained optimization prob-
lem, which projects truth values of unlabeled triples com-
puted by the current embeddings into a subspace constrained
by the rules. The key idea here is to ﬁnd optimal soft la-
bels that stay close to these truth values, while at the same
time ﬁtting the rules. For the ﬁrst property, given each un-
, we calculate its truth value φ(xu)
labeled triple xu ∈ U
using the current embeddings via Eq. (2), and require the
soft label s(xu) to stay close to this truth value. We measure
the closeness between s(xu) and φ(xu) with a squared loss,
and try to minimize it. For the second property, we further
s(xu)
.
impose rule constraints onto the soft labels
}
Speciﬁcally, for each FOL rule fp and each of its ground-
ings gpq, we expect gpq to be true, i.e., π(gpq|S
) = 1 with
) is the conditional truth value
conﬁdence λp. Here, π(gpq|S
of gpq given the soft labels, which can be calculated recur-
sively with the logical compositions deﬁned in Eq. (3) to
Eq. (5). Take gpq := (eu, rs, ev)
(eu, rt, ev) as an ex-
ample, where the premise (eu, rs, ev) is directly observed in
, and the conclusion (eu, rt, ev) is an unlabeled triple in-
. The conditional truth value of gpq can then be

O
cluded in
calculated as:
π(gpq|S
where φ(eu, rs, ev) is a truth value deﬁned by Eq. (2) with
the current embeddings; and s(eu, rt, ev) is a soft label to be
predicted. Comparing Eq. (7) with Eq. (6), we can see that
), for any unlabeled triple,
during the calculation of π(gpq|S

s(eu, rt, ev)
) = φ(eu, rs, ev)
·

φ(eu, rs, ev)+1, (7)

⇒

=

−

U

S

{

we use the soft label s(
) rather than the truth value φ(
·
·
as to better impose rule constraints onto the soft labels
S

Combining the two properties together and further allow-
ing slackness for rule constraints, we ﬁnally get the follow-
ing optimization problem:

), so
.

, Qp, p = 1,

, P,

· · ·

1
2

(s(xu)

min
−
,ξ
S
(cid:88)xu∈U
s.t. λp (1
π(gpq|S
−
0, q = 1,
ξpq ≥
· · ·
1,
0
∀
≤

s(xu)

≤

))

φ(xu))2 + C

ξpq,

p,q
(cid:88)

· · ·
, P,

ξpq, q = 1,

≤
, Qp, p = 1,
· · ·
,
s(xu)

∈ S

(8)
where ξpq is a slack variable and C the penalty coefﬁcient.
Note that conﬁdence levels of rules (i.e. λp’s) are encoded
in the constraints, making our approach capable of handling
soft rules. Rules with higher conﬁdence levels show less tol-
erance for violating the constraints. This optimization prob-
lem is convex, and can be solved efﬁciently with its closed-
form solution:

s(xu) =

φ(xu) + C

λp∇s(xu)π(gpq|S

)

p,q

(9)

1

0

(cid:104)

. Here,

(cid:105)
(cid:88)
) means the gradient
∇s(xu)π(gpq|S
for each xu ∈ U
,1 and
) w.r.t s(xu), which is a constant w.r.t.
of π(gpq|S
[x]1
0 = min(max(x, 0), 1) is a truncation function enforcing
the solutions to stay within [0, 1]. We provide the proof of
convexity and detailed derivation as supplementary materi-
als. Soft labels obtained in this way shall 1) stay close to the
predictions made by the current embedding model, and 2) ﬁt
the rules as well as possible.

S

Embedding Rectiﬁcation
This stage is to integrate both labeled and unlabeled triples
(with hard and soft labels respectively) to update current em-
beddings. Speciﬁcally, we are given a set of labeled triples
(x(cid:96), y(cid:96))
with their hard labels speciﬁed in
,
}
{
and also a set of unlabeled triples encoded in propositional-
. Each unlabeled triple xu has a
ized rules, i.e.,
U
soft label s(xu)
[0, 1], predicted by Eq. (9). We would
like to use these labeled and unlabeled triples to learn the
updated embeddings Θ(n). Here n is the iteration index.

, i.e.,
}

xu}
{

0, 1
{

=

=

L

∈

To this end, we minimize a global loss over

, so
as to ﬁnd embeddings which can predict the true hard labels
, while imitating the soft labels for
for triples contained in
those contained in

. The optimization problem is:

and

L

L

U

(cid:96)(φ(x(cid:96)), y(cid:96)) +

(cid:96)(φ(xu), s(xu)), (10)

U

1

min
Θ

|L| (cid:88)L

−

y log x

x) is the cross
y) log(1
where (cid:96)(x, y) =
−
entropy; and φ(
) is a function w.r.t. Θ deﬁned by Eq. (2).
·
We further impose L2 regularization on the parameters Θ to
avoid overﬁtting. Gradient descent algorithms can be used
to solve this problem. Embeddings learned in this way will
1) be compatible with all the labeled triples, and 2) absorb
rule knowledge carried by the unlabeled triples.

−

1Note that each gpq contains only a single unlabeled triple, i.e.,
the conclusion triple. Take π(gpq|S) deﬁned in Eq. (7) for example.
In this case, s(xu) = s(eu, rt, ev) is the soft label to be predicted
and ∇s(xu)π(gpq|S) = φ(eu, rs, ev) is a constant w.r.t. S.

1

|U| (cid:88)U
(1
−

Whole Procedure

G

Lb,
U

Gb from the labeled triples

Algorithm 1 summarizes the iterative learning procedure of
our approach. To enable efﬁcient learning, we use an online
scheme in mini-batch mode. At each iteration, we sample a
Ub, and
mini-batch
, unla-
L
beled triples
, respectively
, and propositionalized rules
(line 3).2 Soft label prediction and embedding rectiﬁcation
are then conducted locally on these mini-batches (line 4 and
line 5 respectively). This iterative procedure captures the in-
teractive nature between embedding learning and logical in-
ference: given current embeddings, logic rules can be used
to perform approximate inference and predict soft labels for
unlabeled triples; these newly labeled triples carry rich rule
knowledge and will in turn help to learn better embeddings.
In this way, knowledge contained in logic rules can be fully
transferred into the learned embeddings. Note also that our
approach is ﬂexible enough to handle soft rules with various
conﬁdence levels extracted automatically from the KG.

Discussions

We further analyze the space and time complexity, and dis-
cuss possible extensions of our approach.

Complexity. RUGE follows ComplEx to represent entities
and relations as complex-valued vectors, hence has a space
complexity of O(ned + nrd) which scales linearly w.r.t. ne,
nr, and d. Here, ne is the number of entities, nr the num-
ber of relations, and d the dimensionality of the embedding
space. During the learning procedure, each iteration requires
a time complexity of O(τ (n(cid:96)d + nud)), where n(cid:96)/nu is the
average number of labeled/unlabeled triples in a mini-batch,
and τ the number of inner epochs used for embedding recti-
n(cid:96)
ﬁcation (cf. Eq. (10)). In practice, we usually have nu (cid:28)
(see Table 2 for the number of labeled and unlabeled triples
used on our datasets), and we can also set τ to a very small
value, e.g., τ = 1. That means, RUGE has almost the same
time complexity as those most efﬁcient KG embedding tech-
niques (e.g. ComplEx) which require O(n(cid:96)d) per iteration
during training.3 In addition, RUGE further requires prepro-
cessing steps before training, i.e., rule mining and proposi-
tionalization. But these steps are performed only once, and
not required during the iterations.

Extensions. Our approach is quite generic and ﬂexible. 1)
The idea of iteratively injecting logic rules can be applied to
enhance a wide variety of embedding models, as long as an
appropriate scoring function is accordingly designed, e.g.,
the one deﬁned in Eq. (1) by ComplEx. 2) Various types of
rules can be incorporated as long as they can be modeled by
the logical compositions deﬁned in Eq. (3) to Eq. (5), and
we can even use other types of t-norm fuzzy logics to deﬁne
such compositions. 3) Rules with different conﬁdence levels
can be handled in a uniﬁed manner.

2We ﬁrst sample Lb from L. Gb is then constructed by those
whose premise triples are all contained in Lb but conclusion triples
are not. These conclusion triples are further used to construct Ub.

3Such techniques often use SGD in mini-batch mode for train-
ing, and sample a mini-batch of n(cid:96) labeled triples at each iteration.

Algorithm 1 Iterative Learning Procedure of RUGE
Require: Labeled triples L = {(x(cid:96), y(cid:96))}

Unlabeled triples U = {xu}
FOL rules F={(fp, λp)} and their groundings G={gpq}

1: Randomly initialize entity and relation embeddings Θ(0)
2: for n = 1 : N do
3:
4:

Sample a mini-batch Lb / Ub / Gb from L / U / G
1))
Sb ← SoftLabelPrediction (Ub, Gb, Θ(n
−

(cid:46) cf. Eq. (9)

Θ(n) ← EmbeddingRectiﬁcation (Lb, Ub, Sb) (cid:46) cf. Eq. (10)

5:
6: end for
Ensure: Θ(N )

Experiments
We evaluate RUGE in the link prediction task. This task is
to complete a triple (ei, rk, ej) with ei or ej missing, i.e., to
predict ei given (rk, ej) or ej given (ei, rk).
Datasets. We use two datasets: FB15K and YAGO37. The
former is a subgraph of Freebase containing 1,345 relations
and 14,951 entities, released by Bordes et al. (2013).4 The
latter is extracted from the core facts of YAGO3.5 During
the extraction, entities appearing less than 10 times are dis-
carded. The ﬁnal dataset consists of 37 relations and 123,189
entities. Triples on both datasets are split into training, vali-
dation, and test sets, used for model training, hyperparame-
ter tuning, and evaluation, respectively. We use the original
split for FB15K, and draw a split of 989,132/50,000/50,000
triples for YAGO37.

Note that on both datasets, the training sets contain only
positive triples. Negative triples are generated using the local
closed world assumption (Dong et al. 2014). This negative
sampling procedure is performed at runtime for each batch
of training positive triples. Such positive and negative triples
(along with their hard labels) form our labeled triple set.

We further employ AMIE+ (Gal´arraga et al. 2015)6 to au-
tomatically extract Horn clause rules from each dataset, with
the training set as input. To enable efﬁcient extraction, we
consider rules with length not longer than 2 and conﬁdence
levels not less than 0.8.7 The length of a Horn clause rule is
x, y :
the number of atoms appearing in its premise, e.g.,
(x, Nationality, y) has the
(x, BornInCountry, y)
length of 1. And the conﬁdence threshold of 0.8 leads to the
best performance on both datasets (detailed later). Using this
setting, we extract 454 (universally quantiﬁed) Horn clause
rules from FB15K, and 16 such rules from YAGO37. Table 1
shows some examples with their conﬁdence levels.

⇒

∀

Then, we instantiate these rules with concrete entities, i.e.,
propositionalization. Propositionalized rules whose premise
triples are all contained in the training set (while conclusion

4https://everest.hds.utc.fr/doku.php?id=en:smemlj12
5http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago/downloads/

6https://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/amie/

7AMIE+ provides two types of conﬁdence, i.e. standard conﬁ-

dence and PCA conﬁdence. This paper uses PCA conﬁdence.

Table 1: Horn clause rules with conﬁdence levels extracted
by AMIE+ from FB15K (top) and YAGO37 (bottom).

/location/people born here(x,y)
/director/ﬁlm(x,y)
⇒
/ﬁlm/directed by(x,y)

⇒
/ﬁlm/directed by(y,x)

/people/place of birth(y,x)

/person/language(y,z)

/ﬁlm/language(x,z)

∧

⇒

isMarriedTo(x,y)
hasChild(x,y)
playsFor(x,y)

∧
⇒

⇒

isMarriedTo(y,x)

isCitizenOf(y,z)
⇒
isAfﬁliatedTo(x,y)

isCitizenOf(x,z)

1.00
0.99
0.88

0.97
0.94
0.86

Table 2: Statistics of datasets, where ne/nr denotes the num-
ber of entities/relations, n(cid:96)/nu/ng is the number of labeled
triples/unlabeled triples/valid groundings used for training,
and nv/nt denotes the number of validation/test triples.

Train

Valid

Test

Dataset

ne

nr

n(cid:96)

nu

ng

nv

nt

FB15K
YAGO37

14,951
123,189

1,345
37

483,142
989,132

74,707
69,680

96,724
72,670

50,000
50,000

59,071
50,000

triples are not) are taken as valid groundings and used during
embedding learning. We obtain 96,724 valid groundings on
FB15K and 72,670 on YAGO37. Conclusion triples of these
valid groundings are further collected to form our unlabeled
triple set. We ﬁnally get 74,707 unlabeled triples on FB15K
and 69,680 on YAGO37. Table 2 provides some statistics of
the two datasets.
Evaluation Protocol. To evaluate the performance in link
prediction, we follow the standard protocol used in (Bordes
et al. 2013). For each test triple (ei, rk, ej), we replace the
head entity ei with each entity e(cid:48)i ∈ E
, and calculate the score
for (e(cid:48)i, rk, ej). Ranking these scores in descending order,
we get the rank of the correct entity ei. Similarly, we can get
another rank by replacing the tail entity. Aggregated over all
test triples, we report three metrics: 1) the mean reciprocal
rank (MRR), 2) the median of the ranks (MED), and 3) the
proportion of ranks no larger than n (HITS@N). During this
ranking process, we remove corrupted triples which already
exist in either the training, validation, or test set, since they
themselves are true triples. This corresponds to the “ﬁltered”
setting in (Bordes et al. 2013).
Comparison Settings. We compare RUGE with four state-
of-the-art basic embedding models, including TransE (Bor-
des et al. 2013), DistMult (Yang et al. 2015), HolE (Nickel,
Rosasco, and Poggio 2016), and ComplEx (Trouillon et al.
2016). These basic models rely only on triples observed in
a KG and use no rules. We further take PTransE (Lin et al.
2015a) and KALE (Guo et al. 2016) as additional baselines.
Both of them are extensions of TransE, with the former in-
tegrating relation paths (Horn clauses), and the latter FOL
rules (hard rules) in a one-time injection manner. In contrast,
RUGE incorporates soft rules and transfers rule knowledge
into KG embedding in an iterative manner.

We use the code provided by Trouillon et al. (2016)8 for

8https://github.com/ttrouill/complex

TransE, DistMult, and ComplEx, and reimplement HolE so
that all these four basic models share the identical mode of
optimization, i.e., SGD with AdaGrad (Duchi, Hazan, and
Singer 2011) and gradient normalization. As such, we repro-
duce the results of TransE, DistMult, and ComplEx reported
on FB15K (Trouillon et al. 2016), and improve the results of
HolE substantially compared to those reported in the origi-
nal paper (Nickel, Rosasco, and Poggio 2016).9 The code for
PTransE is provided by its authors.10 We implement KALE
and RUGE in Java, both using SGD with AdaGrad and gra-
dient normalization to facilitate a fair comparison.

There are two types of loss functions that could be used
for these baselines, i.e., the logistic loss or the pairwise rank-
ing loss (Nickel, Rosasco, and Poggio 2016). Trouillon et
al. (2016) have recently demonstrated that the logistic loss
generally performs better than the pairwise ranking loss, ex-
cept for TransE. So, for TransE and its extensions (PTransE
and KALE) we use the pairwise ranking loss, and for all the
other baselines we use the logistic loss. To extract relation
paths for PTransE, we follow the optimal conﬁguration re-
ported in (Lin et al. 2015a), where paths constituted by at
most 3 relations are included. For KALE and RUGE, we use
the same set of propositionalized rules to make it a fair com-
parison.11

0.001, 0.01, 0.1, 1

For all the methods, we create 100 mini-batches on each
dataset, and tune the embedding dimensionality d in
50,
{
, the number of negatives per positive triple α
100, 150, 200
}
0.01, 0.05, 0.1,
, the initial learning rate γ in
1, 2, 5, 10
in
{
}
{
0.5, 1.0
0.001,
, and the L2 regularization coefﬁcient λ in
{
}
0.003, 0.01, 0.03, 0.1
. For TransE and its extensions which
}
use the pairwise ranking loss, we further tune the margin δ in
. The slackness penalty C in RUGE
0.1, 0.2, 0.5, 1, 2, 5, 10
{
}
, and the
(cf. Eq. (8)) is selected from
}
{
number of inner iterations (cf. Eq. (10)) is ﬁxed to τ = 1.
Best models are selected by early stopping on the validation
set (monitoring MRR), with at most 1000 iterations over the
training set. The optimal conﬁgurations for RUGE are: d =
200, α = 10, γ = 0.5, λ = 0.01, C = 0.01 on FB15K; and
d = 150, α = 10, γ = 1.0, λ = 0.003, C = 0.01 on YAGO37.
Link Prediction Results. Table 3 shows the results of these
methods on the test sets of FB15K and YAGO37. The results
indicate that RUGE signiﬁcantly and consistently outper-
forms all the baselines on both datasets and in all metrics. It
beats not only the four basic models which use triples alone
(TransE, DistMult, HolE, and ComplEx), but also PTransE
and KALE which further incorporate logic rules (or relation
paths) in a one-time injection manner. This demonstrates the
superiority of injecting logic rules into KG embedding, par-
ticularly in an iterative manner. Compared to the best per-
forming baseline ComplEx (this is also the model based on
which RUGE is designed), RUGE achieves an improvement
of 11%/18% in MRR/HITS@1 on FB15K, and an improve-
ment of 3%/6% on YAGO37. The improvements on FB15K

9HolE in its original implementation uses SGD with AdaGrad,

but no gradient normalization.

10https://github.com/thunlp/KB2E
11KALE takes all these groundings as hard rules. This approxi-
mation works quite well with an appropriate conﬁdence threshold.

Table 3: Link prediction results on the test sets of FB15K and YAGO37. As baselines, rows 1-4 are the four basic models which
use triples alone, and rows 5-6 further integrate logic rules (or relation paths) in a one-time injection manner.

FB15K

HITS@N

YAGO37

HITS@N

Method

TransE
DistMult
HolE
ComplEx
PTransE
KALE

RUGE

MRR

0.400
0.644
0.600
0.690
0.679
0.523

0.768

MED

1

4.0
1.0
2.0
1.0
1.0
2.0

1.0

0.246
0.532
0.485
0.598
0.565
0.383

0.703

3

0.495
0.730
0.673
0.756
0.768
0.616

0.815

5

0.576
0.769
0.722
0.793
0.810
0.683

0.836

10

0.662
0.812
0.779
0.837
0.855
0.762

0.865

MRR

0.303
0.365
0.380
0.417
0.403
0.321

0.431

MED

1

13.0
6.0
7.0
4.0
9.0
9.0

4.0

0.218
0.262
0.288
0.320
0.339
0.215

0.340

3

0.336
0.411
0.420
0.471
0.444
0.372

0.482

5

0.387
0.493
0.479
0.533
0.473
0.438

0.541

10

0.475
0.575
0.551
0.603
0.506
0.522

0.603

Table 4: Runtime (in sec.) on FB15K and YAGO37. Extr. is
the time required for rule/path extraction, Prop. for proposi-
tionalization, and Lean. for training per iteration.

FB15K

YAGO37

Method

Extr. Prop. Learn.

Extr. Prop. Learn.

ComplEx
PTransE
KALE
RUGE

—
868.4
43.1
43.1

— 11.4
— 46.5
4.8
4.0
14.1
4.0

—
13939.5
337.5
337.5

— 49.5
— 23.8
27.5
55.2

13.8
13.8

we set d = 200 (embedding dimensionality) and α = 2 (num-
ber of negatives per positive triple) for all the methods. Other
hyperparameters are ﬁxed to their optimal conﬁgurations de-
termined in link prediction. We can see that RUGE is still
quite efﬁcient despite integrating additional rules. The aver-
age training time per iteration increases from 11.4 to 14.1
on FB15K, and from 49.5 to 55.2 on YAGO37. The prepro-
cessing steps, although performed only once, are also highly
efﬁcient, requiring much less time compared to PTransE.

Conclusion

This paper proposes a novel paradigm that learns entity and
relation embeddings with iterative guidance from soft rules,
referred to as RUGE. It enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. Each iteration alternates
between 1) a soft label prediction stage which predicts soft
labels for unlabeled triples using currently learned embed-
dings and soft rules, and 2) an embedding rectiﬁcation stage
which further integrates both labeled and unlabeled triples
to update current embeddings. This iterative procedure may
better transfer the knowledge contained in logic rules into
the learned embeddings. Link prediction results on Freebase
and YAGO show that RUGE achieves signiﬁcant and con-
sistent improvements over state-of-the-art baselines. More-
over, RUGE demonstrates the usefulness of automatically
extracted soft rules. Even those with moderate conﬁdence
levels can be highly beneﬁcial to KG embedding.

Figure 2: MRR achieved by RUGE with different conﬁdence
thresholds on the test set of FB15K.

are more substantial than those on YAGO37. The reason is
probably that FB15K contains more relations from which a
good range of rules can be extracted (454 universally quan-
tiﬁed rules from FB15K, and 16 from YAGO37).
Inﬂuence of Conﬁdence Levels. We further investigate the
inﬂuence of the threshold of rules’ conﬁdence levels used
in RUGE. To do so, we ﬁx all the hyperparameters to the
optimal conﬁgurations determined by the previous experi-
ment, and vary the conﬁdence threshold in [0.1, 1] with a
step 0.05. Fig. 2 shows MRR achieved by RUGE with var-
ious thresholds on the test set of FB15K. We can see that
the threshold of 0.8 is a good tradeoff and indeed performs
best. A threshold higher than that will reduce the number of
rules that can be extracted, while a one lower than that might
introduce too many less credible rules. Both hurt the perfor-
mance. However, even so, RUGE outperforms ComplEx by
a large margin, with the threshold set in a broad range of
[0.35, 0.9]. This observation indicates that soft rules, even
those with moderate conﬁdence levels, are highly beneﬁcial
to KG embedding despite their uncertainties.
Comparison of Runtime. Finally, we compare RUGE with
ComplEx, PTransE, and KALE in their runtime.12 ComplEx
is a basic model which only requires model training. RUGE
as well as the other two baselines further require preprocess-
ing of rule/path extraction and propositionalization. Table 4
lists the runtime of these methods required for each step on
FB15K and YAGO37. Here, to facilitate a fair comparison,

12The other three baselines are implemented in Python and much

slower. So they are not considered here.

Acknowledgments
The authors would like to thank all the reviewers for their
insightful and valuable suggestions, which signiﬁcantly im-
prove the quality of this paper. This work is supported by the
National Key Research and Development Program of China
(grants No. 2016YFB0801003 and No. 2016QY03D0503),
the Fundamental Theory and Cutting Edge Technology Re-
search Program of the Institute of Information Engineering,
Chinese Academy of Sciences (grant No. Y7Z0261101), and
the National Natural Science Foundation of China (grant No.
61402465).

References
[Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.;
Sturge, T.; and Taylor, J. 2008. Freebase: A collaboratively created
In SIGMOD,
graph database for structuring human knowledge.
1247–1250.

[Bordes et al. 2013] Bordes, A.; Usunier, N.; Garc´ıa-Dur´an, A.;
Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for
modeling multi-relational data. In NIPS, 2787–2795.

[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio,
Y. 2014. A semantic matching energy function for learning with
multi-relational data. MACH LEARN 94(2):233–259.

[Carlson et al. 2010] Carlson, A.; Betteridge, J.; Kisiel, B.; Settles,
B.; Hruschka Jr, E. R.; and Mitchell, T. M. 2010. Toward an archi-
tecture for never-ending language learning. In AAAI, 1306–1313.
T.;
Rockt¨aschel, T.; and Riedel, S. 2016. Lifted rule injection for
relation embeddings. In EMNLP, 1389–1399.

[Demeester, Rockt¨aschel, and Riedel 2016] Demeester,

[Dong et al. 2014] Dong, X.; Gabrilovich, E.; Heitz, G.; Horn, W.;
Lao, N.; Murphy, K.; Strohmann, T.; Sun, S.; and Zhang, W. 2014.
Knowledge vault: A web-scale approach to probabilistic knowl-
edge fusion. In SIGKDD, 601–610.

[Duchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and Singer,
Y. 2011. Adaptive subgradient methods for online learning and
stochastic optimization. J MACH LEARN RES 12(Jul):2121–2159.
[Faruqui et al. 2014] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer,
C.; Hovy, E.; and Smith, N. A. 2014. Retroﬁtting word vectors to
semantic lexicons. arXiv:1411.4166.

[Gal´arraga et al. 2013] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2013. AMIE: Association rule mining under
In WWW,
incomplete evidence in ontological knowledge bases.
413–422.

[Gal´arraga et al. 2015] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2015. Fast rule mining in ontological knowl-
edge bases with AMIE+. VLDB J 24(6):707–730.

[Gardner, Talukdar, and Mitchell 2015] Gardner, M.; Talukdar, P.;
and Mitchell, T. 2015. Combining vector space embeddings with
symbolic logical inference over open-domain text. In AAAI Spring
Symposium Series, 61–65.

[Guo et al. 2015] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2015. Semantically smooth knowledge graph embedding. In
ACL, 84–94.

[Guo et al. 2016] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2016. Jointly embedding knowledge graphs and logical rules.
In EMNLP, 192–202.

[Guu, Miller, and Liang 2015] Guu, K.; Miller, J.; and Liang, P.
2015. Traversing knowledge graphs in vector space. In EMNLP,
318–327.

[H´ajek 1998] H´ajek, P. 1998. The metamathematics of fuzzy logic.

Kluwer.

[Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E.
2016. Harnessing deep neural networks with logic rules. In ACL,
2410–2420.

[Lin et al. 2015a] Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and
Liu, S. 2015a. Modeling relation paths for representation learning
of knowledge bases. In EMNLP, 705–714.

[Lin et al. 2015b] Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X.
2015b. Learning entity and relation embeddings for knowledge
graph completion. In AAAI, 2181–2187.

[Liu et al. 2016] Liu, Q.; Jiang, H.; Evdokimov, A.; Ling, Z.-H.;
Zhu, X.; Wei, S.; and Hu, Y. 2016. Probabilistic reasoning via
deep learning: Neural association models.

[Miller 1995] Miller, G. A. 1995. WordNet: A lexical database for

English. COMMUN ACM 38(11):39–41.

[Neelakantan, Roth, and McCallum 2015] Neelakantan, A.; Roth,
B.; and McCallum, A. 2015. Compositional vector space mod-
els for knowledge base completion. In ACL, 156–166.

[Nickel, Rosasco, and Poggio 2016] Nickel, M.; Rosasco, L.; and
Poggio, T. 2016. Holographic embeddings of knowledge graphs.
In AAAI, 1955–1961.

[Nickel, Tresp, and Kriegel 2011] Nickel, M.; Tresp, V.;

and
Kriegel, H.-P. 2011. A three-way model for collective learning on
multi-relational data. In ICML, 809–816.

[Rockt¨aschel et al. 2014] Rockt¨aschel, T.; Boˇsnjak, M.; Singh, S.;
and Riedel, S. 2014. Low-dimensional embeddings of logic. In
ACL Workshop on Semantic Parsing, 45–49.

[Rockt¨aschel, Singh, and Riedel 2015] Rockt¨aschel, T.; Singh, S.;
and Riedel, S. 2015. Injecting logical background knowledge into
embeddings for relation extraction. In NAACL, 1119–1129.

[Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng,
A. Y. 2013. Reasoning with neural tensor networks for knowledge
base completion. In NIPS, 926–934.

[Suchanek, Kasneci, and Weikum 2007] Suchanek, F. M.; Kasneci,
G.; and Weikum, G. 2007. YAGO: A core of semantic knowledge.
In WWW, 697–706.

[Trouillon et al. 2016] Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,
E.; and Bouchard, G. 2016. Complex embeddings for simple link
prediction. In ICML, 2071–2080.

[Vendrov et al. 2015] Vendrov, I.; Kiros, R.; Fidler, S.; and Ur-
2015. Order-embeddings of images and language.

tasun, R.
arXiv:1511.06361.

[Wang and Cohen 2016] Wang, W. Y., and Cohen, W. W. 2016.
Learning ﬁrst-order logic embeddings via matrix factorization. In
IJCAI, 2132–2138.

[Wang et al. 2014] Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z.
2014. Knowledge graph embedding by translating on hyperplanes.
In AAAI, 1112–1119.

[Wang et al. 2017] Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017.
Knowledge graph embedding: A survey of approaches and appli-
cations. IEEE TRANS KNOWL DATA ENG 29(12):2724–2743.
[Wang, Wang, and Guo 2015] Wang, Q.; Wang, B.; and Guo, L.
2015. Knowledge base completion using embeddings and rules.
In IJCAI, 1859–1865.

[Wei et al. 2015] Wei, Z.; Zhao, J.; Liu, K.; Qi, Z.; Sun, Z.; and
Tian, G. 2015. Large-scale knowledge base completion: Infer-
ring via grounding network sampling over selected instances. In
CIKM, 1331–1340.

[Weston et al. 2013] Weston, J.; Bordes, A.; Yakhnenko, O.; and
Usunier, N. 2013. Connecting language and knowledge bases with
embedding models for relation extraction. In EMNLP, 1366–1371.
[Xiao, Huang, and Zhu 2017] Xiao, H.; Huang, M.; and Zhu, X.
2017. SSP: Semantic space projection for knowledge graph em-
bedding with text descriptions. In AAAI, 3104–3110.

[Xie et al. 2016] Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M.
2016. Representation learning of knowledge graphs with entity
descriptions. In AAAI, 2659–2665.

[Xie, Liu, and Sun 2016] Xie, R.; Liu, Z.; and Sun, M. 2016. Rep-
resentation learning of knowledge graphs with hierarchical types.
In IJCAI, 2965–2971.

[Xiong, Power, and Callan 2017] Xiong, C.; Power, R.; and Callan,
J. 2017. Explicit semantic ranking for academic search via knowl-
edge graph embedding. In WWW, 1271–1279.

[Yang et al. 2015] Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng,
L. 2015. Embedding entities and relations for learning and infer-
ence in knowledge bases. In ICLR.

[Zhang et al. 2016] Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and
Ma, W.-Y. 2016. Collaborative knowledge base embedding for
recommender systems. In SIGKDD, 353–362.

Knowledge Graph Embedding with Iterative Guidance from Soft Rules

Shu Guo1,2, Quan Wang1,2,3

∗, Lihong Wang4, Bin Wang1,2, Li Guo1,2

1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3State Key Laboratory of Information Security, Chinese Academy of Sciences
4National Computer Network Emergency Response Technical Team & Coordination Center of China

7
1
0
2
 
v
o
N
 
0
3
 
 
]
I

A
.
s
c
[
 
 
1
v
1
3
2
1
1
.
1
1
7
1
:
v
i
X
r
a

Abstract

Embedding knowledge graphs (KGs) into continuous vector
spaces is a focus of current research. Combining such an em-
bedding model with logic rules has recently attracted increas-
ing attention. Most previous attempts made a one-time injec-
tion of logic rules, ignoring the interactive nature between
embedding learning and logical inference. And they focused
only on hard rules, which always hold with no exception and
usually require extensive manual effort to create or validate.
In this paper, we propose Rule-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from 1) labeled triples that have been directly
observed in a given KG, 2) unlabeled triples whose labels are
going to be predicted iteratively, and 3) soft rules with vari-
ous conﬁdence levels extracted automatically from the KG. In
the learning process, RUGE iteratively queries rules to obtain
soft labels for unlabeled triples, and integrates such newly la-
beled triples to update the embedding model. Through this
iterative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings. We evalu-
ate RUGE in link prediction on Freebase and YAGO. Exper-
imental results show that: 1) with rule knowledge injected it-
eratively, RUGE achieves signiﬁcant and consistent improve-
ments over state-of-the-art baselines; and 2) despite their un-
certainties, automatically extracted soft rules are highly bene-
ﬁcial to KG embedding, even those with moderate conﬁdence
levels. The code and data used for this paper can be obtained
from https://github.com/iieir-km/RUGE.

Introduction
Knowledge graphs (KGs) such as WordNet (Miller 1995),
Freebase (Bollacker et al. 2008), YAGO (Suchanek, Kas-
neci, and Weikum 2007), and NELL (Carlson et al. 2010)
are extremely useful resources for many AI related applica-
tions. A KG is a multi-relational graph composed of entities
as nodes and relations as different types of edges. Each edge
is represented as a triple (head entity, relation, tail entity),
indicating that there is a speciﬁc relation between two enti-
ties, e.g., (Paris, CapitalOf, France). Although effective
in representing structured data, the underlying symbolic na-
ture of such triples often makes KGs hard to manipulate.

∗Corresponding author: Quan Wang (wangquan@iie.ac.cn).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Recently, a new research direction termed as knowledge
graph embedding has been proposed and quickly received
massive attention (Nickel, Tresp, and Kriegel 2011; Bordes
et al. 2013; Wang et al. 2014; Lin et al. 2015b; Yang et al.
2015; Nickel, Rosasco, and Poggio 2016; Trouillon et al.
2016). The key idea is to embed entities and relations in a
KG into a low-dimensional continuous vector space, so as
to simplify the manipulation while preserving the inherent
structure of the KG. Such embeddings contain rich semantic
information, and can beneﬁt a broad range of downstream
applications (Weston et al. 2013; Bordes et al. 2014; Zhang
et al. 2016; Xiong, Power, and Callan 2017).

Traditional methods performed embedding based solely
on triples observed in a KG. But considering the power of
logic rules in knowledge acquisition and inference, combin-
ing embedding models with logic rules has become a focus
of current research (Rockt¨aschel et al. 2014; Vendrov et al.
2015; Wang and Cohen 2016; Hu et al. 2016). Wang et al.
(2015) and Wei et al. (2015) tried to use embedding models
and logic rules for KG completion. But in their work, rules
are modeled separately from embedding models, and would
not help to learn more predictive embeddings. Rockt¨aschel
et al. (2015) and Guo et al. (2016) then devised joint learning
paradigms which can inject ﬁrst-order logic (FOL) into KG
embedding. Demeester et al. (2016) further proposed lifted
rule injection to avoid the costly propositionalization of FOL
rules. Although these joint models are able to learn better
embeddings after integrating logic rules, they still have their
drawbacks and restrictions.

First of all, these joint models made a one-time injection
of logic rules, taking them as additional rule-based training
instances (Rockt¨aschel, Singh, and Riedel 2015) or regular-
ization terms (Demeester, Rockt¨aschel, and Riedel 2016).
We argue that rules can better enhance KG embedding, how-
ever, in an iterative manner. Given the learned embeddings
and their rough predictions, rules can be used to reﬁne the
predictions and infer new facts. The newly inferred facts, in
turn, will help to learn better embeddings and more accurate
logical inference. Previous methods fail to model such in-
teractions between embedding models and logic rules. Fur-
thermore, they focused only on hard rules which always hold
with no exception. Such rules usually require extensive man-
ual effort to create or validate. Actually, besides hard rules,
a signiﬁcant amount of background information can be en-

proach is quite generic and ﬂexible. It can integrate various
types of rules with different conﬁdence levels to enhance a
good variety of KG embedding models.

Related Work
Recent years have witnessed increasing interest in learning
distributed representations for entities and relations in KGs,
a.k.a. KG embedding. Various techniques have been devised
for this task, e.g., translation-based models which take re-
lations as translating operations between head and tail enti-
ties (Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015b),
simple compositional models which match compositions of
head-tail entity pairs with their relations (Nickel, Tresp, and
Kriegel 2011; Yang et al. 2015; Nickel, Rosasco, and Pog-
gio 2016; Trouillon et al. 2016), and neural networks which
further introduce non-linear layers and deep architectures
(Socher et al. 2013; Bordes et al. 2014; Dong et al. 2014;
Liu et al. 2016). Among these techniques, ComplEx (Trouil-
lon et al. 2016), a compositional model which represents
entities and relations as complex-valued vectors, achieves a
very good trade-off between accuracy and efﬁciency. Most
of the currently available techniques perform the embedding
task based solely on triples observed in a KG. Some recent
work further tried to use other information, e.g., entity types
(Guo et al. 2015; Xie, Liu, and Sun 2016) and textual de-
scriptions (Xie et al. 2016; Xiao, Huang, and Zhu 2017), to
learn more predictive embeddings. See (Wang et al. 2017)
for a thorough review of KG embedding techniques.

Given the power of logic rules in knowledge acquisition
and inference, combining KG embedding with logic rules
becomes a focus of current research. Wang et al. (2015) and
Wei et al. (2015) devised pipelined frameworks which use
logic rules to further reﬁne predictions made by embedding
models. In their work, rules will not help to learn better em-
beddings. Rockt¨aschel et al. (2015) and Guo et al. (2016)
then tried to learn KG embeddings jointly from triples and
propositionalized FOL rules. Demeester et al. (2016) further
proposed lifted rule injection to avoid the costly proposi-
tionalization. These joint models, however, made a one-time
injection of logic rules, ignoring the interactive nature be-
tween embedding learning and logical inference. Moreover,
they can only handle hard rules which are usually manually
created or validated.

Besides logic rules, relation paths which can be regarded
as Horn clauses and get a strong connection to logical infer-
ence (Gardner, Talukdar, and Mitchell 2015), have also been
studied in KG embedding (Neelakantan, Roth, and McCal-
lum 2015; Lin et al. 2015a; Guu, Miller, and Liang 2015).
But in these methods, relation paths are incorporated, again,
in a one-time manner. Our approach, in contrast, iteratively
injects knowledge contained in logic rules into KG embed-
ding, and is able to handle soft rules with various conﬁdence
levels extracted automatically from KGs.

Combining logic rules with distributed representations is
also an active research topic in other contexts outside KGs.
Faruqui et al. (2014) tried to inject ontological knowledge
from WordNet into word embeddings. Vendrov et al. (2015)
introduced order-embedding to model the partial order struc-
ture of hypernymy, textual entailment, and image caption-

Figure 1: Framework overview. RUGE enables an embed-
ding model to learn simultaneously from labeled triples, un-
labeled triples, and soft rules in an iterative manner, where
each iteration alternates between a soft label prediction stage
and an embedding rectiﬁcation stage.

coded as soft rules, e.g., “a person is very likely (but not nec-
essarily) to have a nationality of the country where he/she
was born”. Soft rules can be extracted automatically and ef-
ﬁciently via modern rule mining systems (Gal´arraga et al.
2013; Gal´arraga et al. 2015). Yet, despite this merit, soft
rules have not been well studied in previous methods.

This paper proposes RUle-Guided Embedding (RUGE), a
novel paradigm of KG embedding with iterative guidance
from soft rules. As sketched in Fig. 1, it enables an embed-
ding model to learn simultaneously from 1) labeled triples
that have been directly observed in a given KG, 2) unlabeled
triples whose labels are going to be predicted iteratively, and
3) soft rules with different conﬁdence levels extracted auto-
matically from the KG. During each iteration of the learning
process, the model alternates between a soft label prediction
stage and an embedding rectiﬁcation stage. The former uses
currently learned embeddings and soft rules to predict soft
labels for unlabeled triples, and the latter further integrates
both labeled and unlabeled triples (with hard and soft labels
respectively) to update current embeddings. Through this it-
erative procedure, knowledge embodied in logic rules may
be better transferred into the learned embeddings.

We empirically evaluate RUGE on large scale public KGs,
namely Freebase and YAGO. Experimental results reveal
that: 1) by incorporating logic rules, RUGE signiﬁcantly and
consistently improves over state-of-the-art basic embedding
models (without rules); 2) compared to those one-time in-
jection schemes studied before, the iterative injection strat-
egy maximizes the utility of logic rules for KG embedding,
and indeed achieves substantially better performance; 3) de-
spite the uncertainties, automatically extracted soft rules are
highly beneﬁcial to KG embedding, even those with moder-
ate conﬁdence levels.

The contributions of this paper are threefold. 1) We devise
a novel paradigm of KG embedding which iteratively injects
logic rules into the learned embeddings. To our knowledge,
this is the ﬁrst work that models interactions between em-
bedding learning and logical inference in a principled frame-
work. 2) We demonstrate the usefulness of automatically ex-
tracted soft rules in KG embedding, thereby eliminating the
requirement of laborious manual rule creation. 3) Our ap-

ing. Hu et al. (2016) proposed to enhance various types of
neural networks with FOL rules. All these studies demon-
strate the capability of logic rules to enhance distributed rep-
resentation learning.

Rule-Guided Knowledge Graph Embedding
This section introduces RUle-Guided Embedding (RUGE),
a novel paradigm of KG embedding with iterative guidance
from soft rules. RUGE enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. During each iteration, the
model alternates between a soft label prediction stage and
an embedding rectiﬁcation stage. Fig. 1 sketches this overall
framework. In what follows, we ﬁrst describe our learning
resources, and then detail the two alternating stages.

O

and

, where

(ei, rk, ej)
}
{

Learning Resources
Suppose we are given a KG with a set of triples observed,
=
i.e.,
. Each triple is composed of two en-
and their relation rk ∈ R
tities ei, ej ∈ E
R
are the sets of entities and relations respectively. We obtain
our learning resources (i.e., labeled triples, unlabeled triples,
and soft rules) and model them as follows.
Labeled Triples. We take the triples observed in
as pos-
itive ones. For each positive triple (ei, rk, ej), we randomly
corrupt the head ei or the tail ej, to form a negative triple
and e(cid:48)j ∈
(e(cid:48)i, rk, ej) or (ei, rk, e(cid:48)j), where e(cid:48)i ∈ E \ {
. We denote a labeled triple as x(cid:96), and associate with
E \ {
it a label y(cid:96) = 1 if x(cid:96) is positive, and y(cid:96) = 0 otherwise. Let
=
denote the set of these labeled triples (along

ei}

O

E

ej}
(x(cid:96), y(cid:96))
}

{

{

U

F

=

=

O

(fp, λp)
}

L
with their labels).
Unlabeled Triples. Besides the labeled triples, we collect a
, where xu = (ei, rk, ej)
xu}
set of unlabeled triples
{
indicates an unlabeled triple. In fact, all the triples that have
not been observed in
can be taken as unlabeled ones. But
in this paper, we consider only those encoded in the conclu-
sion of a soft rule, as detailed below.
Soft Rules. We also consider a set of FOL rules with differ-
P
ent conﬁdence levels, denoted as
p=1. Here,
fp is the p-th logic rule deﬁned over the given KG, repre-
(x, rt, y),
sented, e.g., in the form of
∀
stating that two entities linked by relation rs might also
be linked by relation rt. The left-hand side of the im-
” is called the premise, and the right-hand
plication “
side the conclusion. In this paper, we restrict fp
to be
a Horn clause rule, where the conclusion contains only
a single atom and the premise is a conjunction of sev-
eral atoms. The conﬁdence level of rule fp is denoted
[0, 1]. Rules with higher conﬁdence levels are
as λp ∈
more likely to hold, and a conﬁdence level of λp = 1
indicates a hard rule which always holds with no excep-
tion. Such rules as well as their conﬁdence levels can be
extracted automatically from the KG (with the observed
triple set
as input), by using modern rule mining systems
like AMIE and AMIE+ (Gal´arraga et al. 2013; Gal´arraga et al. 2015).

x, y : (x, rs, y)

⇒

⇒

O

We then propositionalize these rules to get their ground-
ings. Here a grounding is the logical expression with all vari-
. For instance, a
ables instantiated with concrete entities in

E

.

E

∀

O

universally quantiﬁed rule

x, y : (x, BornInCountry, y)
(x, Nationality, y) could be instantiated with two en-
⇒
tities EmmanuelMacron and France, and gives a resultant
grounding (EmmanuelMacron, BornInCountry, France)
(EmmanuelMacron, Nationality, France). Ob-
⇒
there could be a huge number of groundings,
viously,
In this
especially given a large entity vocabulary
paper,
to maximize the utility for knowledge acquisi-
tion and inference, we take as valid groundings only
those where premise triples are observed in
while
conclusion triples are not. That means the aforemen-
tioned grounding will be considered as valid if the triple
(EmmanuelMacron, BornInCountry, France)
but
(EmmanuelMacron, Nationality, France)
/
. For
∈ O
Qp
each FOL rule fp, let
q=1 denote the set of
its valid groundings. All the premise triples of gpq are
contained in
, but the single conclusion triple is not.
These conclusion triples are further used to construct our
unlabeled triple set
. That means, our unlabeled triples are
those which are not directly observed in the KG but could
be inferred by the rules with high probabilities.
Modeling Triples and Rules. Given the labeled triples
unlabeled triples
{Gp}

,
L
, and the valid groundings of FOL rules
P
p=1, we discuss how to model these triples and
G
rules in the context of KG embedding. To model triples, we
follow ComplEx (Trouillon et al. 2016), a recently proposed
method which is simple and efﬁcient while achieving state-
of-the-art predictive performance. Speciﬁcally, we assume
entities and relations to have complex-valued vector embed-
dings. Given a triple (ei, rk, ej)
, we score it by
a multi-linear dot product:

∈ E ×R×E

Gp =

gpq}
{

∈ O

O

=

U

U

m

(cid:88)

) = Re(

ei, rk, ¯ej(cid:105)
[ei]m[rk]m[¯ej]m), (1)
ηijk = Re(
(cid:104)
Cd are the complex-valued vector em-
where ei, ej, rk ∈
beddings associated with ei, ej, and rk, respectively; ¯ej is
the conjugate of ej; [
]m is the m-th entry of a vector; and
·
) means taking the real part of a complex value. We fur-
Re(
·
ther introduce a mapping function φ :
(0, 1), so
as to map the score ηijk to a continuous truth value which
lies in the range of (0, 1), i.e.,

E×R×E →

,

(cid:0)

(cid:1)

Re(

φ(ei, rk, ej) = σ(ηijk) = σ

ei, rk, ¯ej(cid:105)
)
(2)
(cid:104)
where σ(x) = 1/(1 + exp(
x)) denotes the sigmoid func-
−
tion. Triples with higher truth values are more likely to hold.
To model propositionalized rules (i.e. groundings), we use
t-norm based fuzzy logics (H´ajek 1998). The key idea is to
model the truth value of a propositionalized rule as a com-
position of the truth values of its constituent triples, through
). For instance,
speciﬁc logical connectives (e.g.
⇒
the truth value of a grounded rule (eu, rs, ev)
(eu, rt, ev)
will be determined by the truth values of the two triples
(eu, rs, ev) and (eu, rt, ev), via a composition deﬁned by
logical implication. We follow (Guo et al. 2016) and deﬁne
the compositions associated with logical conjunction (
),
disjunction (

) as:

and

⇒

∧

∧

), and negation (
¬
b) = π(a)
π(b),
b) = π(a) + π(b)
a) = 1

π(a).

¬
π(a
π(a
π(

·

∧
∨
¬

−

π(a)

π(b),

−

·

(3)
(4)
(5)

Here, a and b are two logical expressions, which can either
be single triples or be constructed by combining triples with
logical connectives; and π(a) is the truth value of a, indi-
cating to what degree the logical expression is true. If a is a
single triple, say (ei, rk, ej), we have π(a) = φ(ei, rk, ej),
as deﬁned in Eq. (2). Given these compositions, the truth
value of any logical expression can be calculated recursively
(Guo et al. 2016), e.g.,

π(a

b) = π(

a

b) = π(a)

π(b)

π(a) + 1.

(6)

⇒

¬

∨

·

−

r
}r
∈E ∪ {

Logical expressions with higher truth values have greater de-
e
grees to be true. Let Θ =
denote the set of
}e
{
all entity and relation embeddings. The proposed approach,
RUGE, then aims to learn these embeddings by using the
labeled triples
, and valid groundings
P
p=1 in an iterative manner, where each iteration alter-
{Gp}
nates between a soft label prediction stage and an embedding
rectiﬁcation stage.

, unlabeled triples

∈R

L

U

Soft Label Prediction
This stage is to use currently learned embeddings and propo-
sitionalized rules to predict soft labels for unlabeled triples.
Speciﬁcally, let n be the iteration index, and Θ(n
1) the set
of current embeddings learned from the previous iteration.
Recall that we are given a set of P FOL rules with their con-
P
(fp, λp)
p=1, and each FOL rule fp has
ﬁdence levels
}
{
Qp
Qp valid groundings
gpq}
Gp =
q=1. Our aim is to predict a
{
soft label s(xu)
, by
∈
1) and all the groundings
using the current embeddings Θ(n

[0, 1] for each unlabeled triple xu ∈ U

=

F

−

−

P
p=1.

G

{Gp}

=
To do so, we solve a rule-constrained optimization prob-
lem, which projects truth values of unlabeled triples com-
puted by the current embeddings into a subspace constrained
by the rules. The key idea here is to ﬁnd optimal soft la-
bels that stay close to these truth values, while at the same
time ﬁtting the rules. For the ﬁrst property, given each un-
, we calculate its truth value φ(xu)
labeled triple xu ∈ U
using the current embeddings via Eq. (2), and require the
soft label s(xu) to stay close to this truth value. We measure
the closeness between s(xu) and φ(xu) with a squared loss,
and try to minimize it. For the second property, we further
s(xu)
.
impose rule constraints onto the soft labels
}
Speciﬁcally, for each FOL rule fp and each of its ground-
ings gpq, we expect gpq to be true, i.e., π(gpq|S
) = 1 with
) is the conditional truth value
conﬁdence λp. Here, π(gpq|S
of gpq given the soft labels, which can be calculated recur-
sively with the logical compositions deﬁned in Eq. (3) to
Eq. (5). Take gpq := (eu, rs, ev)
(eu, rt, ev) as an ex-
ample, where the premise (eu, rs, ev) is directly observed in
, and the conclusion (eu, rt, ev) is an unlabeled triple in-
. The conditional truth value of gpq can then be

O
cluded in
calculated as:
π(gpq|S
where φ(eu, rs, ev) is a truth value deﬁned by Eq. (2) with
the current embeddings; and s(eu, rt, ev) is a soft label to be
predicted. Comparing Eq. (7) with Eq. (6), we can see that
), for any unlabeled triple,
during the calculation of π(gpq|S

s(eu, rt, ev)
) = φ(eu, rs, ev)
·

φ(eu, rs, ev)+1, (7)

⇒

=

−

U

S

{

we use the soft label s(
) rather than the truth value φ(
·
·
as to better impose rule constraints onto the soft labels
S

Combining the two properties together and further allow-
ing slackness for rule constraints, we ﬁnally get the follow-
ing optimization problem:

), so
.

, Qp, p = 1,

, P,

· · ·

1
2

(s(xu)

min
−
,ξ
S
(cid:88)xu∈U
s.t. λp (1
π(gpq|S
−
0, q = 1,
ξpq ≥
· · ·
1,
0
∀
≤

s(xu)

≤

))

φ(xu))2 + C

ξpq,

p,q
(cid:88)

· · ·
, P,

ξpq, q = 1,

≤
, Qp, p = 1,
· · ·
,
s(xu)

∈ S

(8)
where ξpq is a slack variable and C the penalty coefﬁcient.
Note that conﬁdence levels of rules (i.e. λp’s) are encoded
in the constraints, making our approach capable of handling
soft rules. Rules with higher conﬁdence levels show less tol-
erance for violating the constraints. This optimization prob-
lem is convex, and can be solved efﬁciently with its closed-
form solution:

s(xu) =

φ(xu) + C

λp∇s(xu)π(gpq|S

)

p,q

(9)

1

0

(cid:104)

. Here,

(cid:105)
(cid:88)
) means the gradient
∇s(xu)π(gpq|S
for each xu ∈ U
,1 and
) w.r.t s(xu), which is a constant w.r.t.
of π(gpq|S
[x]1
0 = min(max(x, 0), 1) is a truncation function enforcing
the solutions to stay within [0, 1]. We provide the proof of
convexity and detailed derivation as supplementary materi-
als. Soft labels obtained in this way shall 1) stay close to the
predictions made by the current embedding model, and 2) ﬁt
the rules as well as possible.

S

Embedding Rectiﬁcation
This stage is to integrate both labeled and unlabeled triples
(with hard and soft labels respectively) to update current em-
beddings. Speciﬁcally, we are given a set of labeled triples
(x(cid:96), y(cid:96))
with their hard labels speciﬁed in
,
}
{
and also a set of unlabeled triples encoded in propositional-
. Each unlabeled triple xu has a
ized rules, i.e.,
U
soft label s(xu)
[0, 1], predicted by Eq. (9). We would
like to use these labeled and unlabeled triples to learn the
updated embeddings Θ(n). Here n is the iteration index.

, i.e.,
}

xu}
{

0, 1
{

=

=

L

∈

To this end, we minimize a global loss over

, so
as to ﬁnd embeddings which can predict the true hard labels
, while imitating the soft labels for
for triples contained in
those contained in

. The optimization problem is:

and

L

L

U

(cid:96)(φ(x(cid:96)), y(cid:96)) +

(cid:96)(φ(xu), s(xu)), (10)

U

1

min
Θ

|L| (cid:88)L

−

y log x

x) is the cross
y) log(1
where (cid:96)(x, y) =
−
entropy; and φ(
) is a function w.r.t. Θ deﬁned by Eq. (2).
·
We further impose L2 regularization on the parameters Θ to
avoid overﬁtting. Gradient descent algorithms can be used
to solve this problem. Embeddings learned in this way will
1) be compatible with all the labeled triples, and 2) absorb
rule knowledge carried by the unlabeled triples.

−

1Note that each gpq contains only a single unlabeled triple, i.e.,
the conclusion triple. Take π(gpq|S) deﬁned in Eq. (7) for example.
In this case, s(xu) = s(eu, rt, ev) is the soft label to be predicted
and ∇s(xu)π(gpq|S) = φ(eu, rs, ev) is a constant w.r.t. S.

1

|U| (cid:88)U
(1
−

Whole Procedure

G

Lb,
U

Gb from the labeled triples

Algorithm 1 summarizes the iterative learning procedure of
our approach. To enable efﬁcient learning, we use an online
scheme in mini-batch mode. At each iteration, we sample a
Ub, and
mini-batch
, unla-
L
beled triples
, respectively
, and propositionalized rules
(line 3).2 Soft label prediction and embedding rectiﬁcation
are then conducted locally on these mini-batches (line 4 and
line 5 respectively). This iterative procedure captures the in-
teractive nature between embedding learning and logical in-
ference: given current embeddings, logic rules can be used
to perform approximate inference and predict soft labels for
unlabeled triples; these newly labeled triples carry rich rule
knowledge and will in turn help to learn better embeddings.
In this way, knowledge contained in logic rules can be fully
transferred into the learned embeddings. Note also that our
approach is ﬂexible enough to handle soft rules with various
conﬁdence levels extracted automatically from the KG.

Discussions

We further analyze the space and time complexity, and dis-
cuss possible extensions of our approach.

Complexity. RUGE follows ComplEx to represent entities
and relations as complex-valued vectors, hence has a space
complexity of O(ned + nrd) which scales linearly w.r.t. ne,
nr, and d. Here, ne is the number of entities, nr the num-
ber of relations, and d the dimensionality of the embedding
space. During the learning procedure, each iteration requires
a time complexity of O(τ (n(cid:96)d + nud)), where n(cid:96)/nu is the
average number of labeled/unlabeled triples in a mini-batch,
and τ the number of inner epochs used for embedding recti-
n(cid:96)
ﬁcation (cf. Eq. (10)). In practice, we usually have nu (cid:28)
(see Table 2 for the number of labeled and unlabeled triples
used on our datasets), and we can also set τ to a very small
value, e.g., τ = 1. That means, RUGE has almost the same
time complexity as those most efﬁcient KG embedding tech-
niques (e.g. ComplEx) which require O(n(cid:96)d) per iteration
during training.3 In addition, RUGE further requires prepro-
cessing steps before training, i.e., rule mining and proposi-
tionalization. But these steps are performed only once, and
not required during the iterations.

Extensions. Our approach is quite generic and ﬂexible. 1)
The idea of iteratively injecting logic rules can be applied to
enhance a wide variety of embedding models, as long as an
appropriate scoring function is accordingly designed, e.g.,
the one deﬁned in Eq. (1) by ComplEx. 2) Various types of
rules can be incorporated as long as they can be modeled by
the logical compositions deﬁned in Eq. (3) to Eq. (5), and
we can even use other types of t-norm fuzzy logics to deﬁne
such compositions. 3) Rules with different conﬁdence levels
can be handled in a uniﬁed manner.

2We ﬁrst sample Lb from L. Gb is then constructed by those
whose premise triples are all contained in Lb but conclusion triples
are not. These conclusion triples are further used to construct Ub.

3Such techniques often use SGD in mini-batch mode for train-
ing, and sample a mini-batch of n(cid:96) labeled triples at each iteration.

Algorithm 1 Iterative Learning Procedure of RUGE
Require: Labeled triples L = {(x(cid:96), y(cid:96))}

Unlabeled triples U = {xu}
FOL rules F={(fp, λp)} and their groundings G={gpq}

1: Randomly initialize entity and relation embeddings Θ(0)
2: for n = 1 : N do
3:
4:

Sample a mini-batch Lb / Ub / Gb from L / U / G
1))
Sb ← SoftLabelPrediction (Ub, Gb, Θ(n
−

(cid:46) cf. Eq. (9)

Θ(n) ← EmbeddingRectiﬁcation (Lb, Ub, Sb) (cid:46) cf. Eq. (10)

5:
6: end for
Ensure: Θ(N )

Experiments
We evaluate RUGE in the link prediction task. This task is
to complete a triple (ei, rk, ej) with ei or ej missing, i.e., to
predict ei given (rk, ej) or ej given (ei, rk).
Datasets. We use two datasets: FB15K and YAGO37. The
former is a subgraph of Freebase containing 1,345 relations
and 14,951 entities, released by Bordes et al. (2013).4 The
latter is extracted from the core facts of YAGO3.5 During
the extraction, entities appearing less than 10 times are dis-
carded. The ﬁnal dataset consists of 37 relations and 123,189
entities. Triples on both datasets are split into training, vali-
dation, and test sets, used for model training, hyperparame-
ter tuning, and evaluation, respectively. We use the original
split for FB15K, and draw a split of 989,132/50,000/50,000
triples for YAGO37.

Note that on both datasets, the training sets contain only
positive triples. Negative triples are generated using the local
closed world assumption (Dong et al. 2014). This negative
sampling procedure is performed at runtime for each batch
of training positive triples. Such positive and negative triples
(along with their hard labels) form our labeled triple set.

We further employ AMIE+ (Gal´arraga et al. 2015)6 to au-
tomatically extract Horn clause rules from each dataset, with
the training set as input. To enable efﬁcient extraction, we
consider rules with length not longer than 2 and conﬁdence
levels not less than 0.8.7 The length of a Horn clause rule is
x, y :
the number of atoms appearing in its premise, e.g.,
(x, Nationality, y) has the
(x, BornInCountry, y)
length of 1. And the conﬁdence threshold of 0.8 leads to the
best performance on both datasets (detailed later). Using this
setting, we extract 454 (universally quantiﬁed) Horn clause
rules from FB15K, and 16 such rules from YAGO37. Table 1
shows some examples with their conﬁdence levels.

⇒

∀

Then, we instantiate these rules with concrete entities, i.e.,
propositionalization. Propositionalized rules whose premise
triples are all contained in the training set (while conclusion

4https://everest.hds.utc.fr/doku.php?id=en:smemlj12
5http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago/downloads/

6https://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/amie/

7AMIE+ provides two types of conﬁdence, i.e. standard conﬁ-

dence and PCA conﬁdence. This paper uses PCA conﬁdence.

Table 1: Horn clause rules with conﬁdence levels extracted
by AMIE+ from FB15K (top) and YAGO37 (bottom).

/location/people born here(x,y)
/director/ﬁlm(x,y)
⇒
/ﬁlm/directed by(x,y)

⇒
/ﬁlm/directed by(y,x)

/people/place of birth(y,x)

/person/language(y,z)

/ﬁlm/language(x,z)

∧

⇒

isMarriedTo(x,y)
hasChild(x,y)
playsFor(x,y)

∧
⇒

⇒

isMarriedTo(y,x)

isCitizenOf(y,z)
⇒
isAfﬁliatedTo(x,y)

isCitizenOf(x,z)

1.00
0.99
0.88

0.97
0.94
0.86

Table 2: Statistics of datasets, where ne/nr denotes the num-
ber of entities/relations, n(cid:96)/nu/ng is the number of labeled
triples/unlabeled triples/valid groundings used for training,
and nv/nt denotes the number of validation/test triples.

Train

Valid

Test

Dataset

ne

nr

n(cid:96)

nu

ng

nv

nt

FB15K
YAGO37

14,951
123,189

1,345
37

483,142
989,132

74,707
69,680

96,724
72,670

50,000
50,000

59,071
50,000

triples are not) are taken as valid groundings and used during
embedding learning. We obtain 96,724 valid groundings on
FB15K and 72,670 on YAGO37. Conclusion triples of these
valid groundings are further collected to form our unlabeled
triple set. We ﬁnally get 74,707 unlabeled triples on FB15K
and 69,680 on YAGO37. Table 2 provides some statistics of
the two datasets.
Evaluation Protocol. To evaluate the performance in link
prediction, we follow the standard protocol used in (Bordes
et al. 2013). For each test triple (ei, rk, ej), we replace the
head entity ei with each entity e(cid:48)i ∈ E
, and calculate the score
for (e(cid:48)i, rk, ej). Ranking these scores in descending order,
we get the rank of the correct entity ei. Similarly, we can get
another rank by replacing the tail entity. Aggregated over all
test triples, we report three metrics: 1) the mean reciprocal
rank (MRR), 2) the median of the ranks (MED), and 3) the
proportion of ranks no larger than n (HITS@N). During this
ranking process, we remove corrupted triples which already
exist in either the training, validation, or test set, since they
themselves are true triples. This corresponds to the “ﬁltered”
setting in (Bordes et al. 2013).
Comparison Settings. We compare RUGE with four state-
of-the-art basic embedding models, including TransE (Bor-
des et al. 2013), DistMult (Yang et al. 2015), HolE (Nickel,
Rosasco, and Poggio 2016), and ComplEx (Trouillon et al.
2016). These basic models rely only on triples observed in
a KG and use no rules. We further take PTransE (Lin et al.
2015a) and KALE (Guo et al. 2016) as additional baselines.
Both of them are extensions of TransE, with the former in-
tegrating relation paths (Horn clauses), and the latter FOL
rules (hard rules) in a one-time injection manner. In contrast,
RUGE incorporates soft rules and transfers rule knowledge
into KG embedding in an iterative manner.

We use the code provided by Trouillon et al. (2016)8 for

8https://github.com/ttrouill/complex

TransE, DistMult, and ComplEx, and reimplement HolE so
that all these four basic models share the identical mode of
optimization, i.e., SGD with AdaGrad (Duchi, Hazan, and
Singer 2011) and gradient normalization. As such, we repro-
duce the results of TransE, DistMult, and ComplEx reported
on FB15K (Trouillon et al. 2016), and improve the results of
HolE substantially compared to those reported in the origi-
nal paper (Nickel, Rosasco, and Poggio 2016).9 The code for
PTransE is provided by its authors.10 We implement KALE
and RUGE in Java, both using SGD with AdaGrad and gra-
dient normalization to facilitate a fair comparison.

There are two types of loss functions that could be used
for these baselines, i.e., the logistic loss or the pairwise rank-
ing loss (Nickel, Rosasco, and Poggio 2016). Trouillon et
al. (2016) have recently demonstrated that the logistic loss
generally performs better than the pairwise ranking loss, ex-
cept for TransE. So, for TransE and its extensions (PTransE
and KALE) we use the pairwise ranking loss, and for all the
other baselines we use the logistic loss. To extract relation
paths for PTransE, we follow the optimal conﬁguration re-
ported in (Lin et al. 2015a), where paths constituted by at
most 3 relations are included. For KALE and RUGE, we use
the same set of propositionalized rules to make it a fair com-
parison.11

0.001, 0.01, 0.1, 1

For all the methods, we create 100 mini-batches on each
dataset, and tune the embedding dimensionality d in
50,
{
, the number of negatives per positive triple α
100, 150, 200
}
0.01, 0.05, 0.1,
, the initial learning rate γ in
1, 2, 5, 10
in
{
}
{
0.5, 1.0
0.001,
, and the L2 regularization coefﬁcient λ in
{
}
0.003, 0.01, 0.03, 0.1
. For TransE and its extensions which
}
use the pairwise ranking loss, we further tune the margin δ in
. The slackness penalty C in RUGE
0.1, 0.2, 0.5, 1, 2, 5, 10
{
}
, and the
(cf. Eq. (8)) is selected from
}
{
number of inner iterations (cf. Eq. (10)) is ﬁxed to τ = 1.
Best models are selected by early stopping on the validation
set (monitoring MRR), with at most 1000 iterations over the
training set. The optimal conﬁgurations for RUGE are: d =
200, α = 10, γ = 0.5, λ = 0.01, C = 0.01 on FB15K; and
d = 150, α = 10, γ = 1.0, λ = 0.003, C = 0.01 on YAGO37.
Link Prediction Results. Table 3 shows the results of these
methods on the test sets of FB15K and YAGO37. The results
indicate that RUGE signiﬁcantly and consistently outper-
forms all the baselines on both datasets and in all metrics. It
beats not only the four basic models which use triples alone
(TransE, DistMult, HolE, and ComplEx), but also PTransE
and KALE which further incorporate logic rules (or relation
paths) in a one-time injection manner. This demonstrates the
superiority of injecting logic rules into KG embedding, par-
ticularly in an iterative manner. Compared to the best per-
forming baseline ComplEx (this is also the model based on
which RUGE is designed), RUGE achieves an improvement
of 11%/18% in MRR/HITS@1 on FB15K, and an improve-
ment of 3%/6% on YAGO37. The improvements on FB15K

9HolE in its original implementation uses SGD with AdaGrad,

but no gradient normalization.

10https://github.com/thunlp/KB2E
11KALE takes all these groundings as hard rules. This approxi-
mation works quite well with an appropriate conﬁdence threshold.

Table 3: Link prediction results on the test sets of FB15K and YAGO37. As baselines, rows 1-4 are the four basic models which
use triples alone, and rows 5-6 further integrate logic rules (or relation paths) in a one-time injection manner.

FB15K

HITS@N

YAGO37

HITS@N

Method

TransE
DistMult
HolE
ComplEx
PTransE
KALE

RUGE

MRR

0.400
0.644
0.600
0.690
0.679
0.523

0.768

MED

1

4.0
1.0
2.0
1.0
1.0
2.0

1.0

0.246
0.532
0.485
0.598
0.565
0.383

0.703

3

0.495
0.730
0.673
0.756
0.768
0.616

0.815

5

0.576
0.769
0.722
0.793
0.810
0.683

0.836

10

0.662
0.812
0.779
0.837
0.855
0.762

0.865

MRR

0.303
0.365
0.380
0.417
0.403
0.321

0.431

MED

1

13.0
6.0
7.0
4.0
9.0
9.0

4.0

0.218
0.262
0.288
0.320
0.339
0.215

0.340

3

0.336
0.411
0.420
0.471
0.444
0.372

0.482

5

0.387
0.493
0.479
0.533
0.473
0.438

0.541

10

0.475
0.575
0.551
0.603
0.506
0.522

0.603

Table 4: Runtime (in sec.) on FB15K and YAGO37. Extr. is
the time required for rule/path extraction, Prop. for proposi-
tionalization, and Lean. for training per iteration.

FB15K

YAGO37

Method

Extr. Prop. Learn.

Extr. Prop. Learn.

ComplEx
PTransE
KALE
RUGE

—
868.4
43.1
43.1

— 11.4
— 46.5
4.8
4.0
14.1
4.0

—
13939.5
337.5
337.5

— 49.5
— 23.8
27.5
55.2

13.8
13.8

we set d = 200 (embedding dimensionality) and α = 2 (num-
ber of negatives per positive triple) for all the methods. Other
hyperparameters are ﬁxed to their optimal conﬁgurations de-
termined in link prediction. We can see that RUGE is still
quite efﬁcient despite integrating additional rules. The aver-
age training time per iteration increases from 11.4 to 14.1
on FB15K, and from 49.5 to 55.2 on YAGO37. The prepro-
cessing steps, although performed only once, are also highly
efﬁcient, requiring much less time compared to PTransE.

Conclusion

This paper proposes a novel paradigm that learns entity and
relation embeddings with iterative guidance from soft rules,
referred to as RUGE. It enables an embedding model to learn
simultaneously from labeled triples, unlabeled triples, and
soft rules in an iterative manner. Each iteration alternates
between 1) a soft label prediction stage which predicts soft
labels for unlabeled triples using currently learned embed-
dings and soft rules, and 2) an embedding rectiﬁcation stage
which further integrates both labeled and unlabeled triples
to update current embeddings. This iterative procedure may
better transfer the knowledge contained in logic rules into
the learned embeddings. Link prediction results on Freebase
and YAGO show that RUGE achieves signiﬁcant and con-
sistent improvements over state-of-the-art baselines. More-
over, RUGE demonstrates the usefulness of automatically
extracted soft rules. Even those with moderate conﬁdence
levels can be highly beneﬁcial to KG embedding.

Figure 2: MRR achieved by RUGE with different conﬁdence
thresholds on the test set of FB15K.

are more substantial than those on YAGO37. The reason is
probably that FB15K contains more relations from which a
good range of rules can be extracted (454 universally quan-
tiﬁed rules from FB15K, and 16 from YAGO37).
Inﬂuence of Conﬁdence Levels. We further investigate the
inﬂuence of the threshold of rules’ conﬁdence levels used
in RUGE. To do so, we ﬁx all the hyperparameters to the
optimal conﬁgurations determined by the previous experi-
ment, and vary the conﬁdence threshold in [0.1, 1] with a
step 0.05. Fig. 2 shows MRR achieved by RUGE with var-
ious thresholds on the test set of FB15K. We can see that
the threshold of 0.8 is a good tradeoff and indeed performs
best. A threshold higher than that will reduce the number of
rules that can be extracted, while a one lower than that might
introduce too many less credible rules. Both hurt the perfor-
mance. However, even so, RUGE outperforms ComplEx by
a large margin, with the threshold set in a broad range of
[0.35, 0.9]. This observation indicates that soft rules, even
those with moderate conﬁdence levels, are highly beneﬁcial
to KG embedding despite their uncertainties.
Comparison of Runtime. Finally, we compare RUGE with
ComplEx, PTransE, and KALE in their runtime.12 ComplEx
is a basic model which only requires model training. RUGE
as well as the other two baselines further require preprocess-
ing of rule/path extraction and propositionalization. Table 4
lists the runtime of these methods required for each step on
FB15K and YAGO37. Here, to facilitate a fair comparison,

12The other three baselines are implemented in Python and much

slower. So they are not considered here.

Acknowledgments
The authors would like to thank all the reviewers for their
insightful and valuable suggestions, which signiﬁcantly im-
prove the quality of this paper. This work is supported by the
National Key Research and Development Program of China
(grants No. 2016YFB0801003 and No. 2016QY03D0503),
the Fundamental Theory and Cutting Edge Technology Re-
search Program of the Institute of Information Engineering,
Chinese Academy of Sciences (grant No. Y7Z0261101), and
the National Natural Science Foundation of China (grant No.
61402465).

References
[Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.;
Sturge, T.; and Taylor, J. 2008. Freebase: A collaboratively created
In SIGMOD,
graph database for structuring human knowledge.
1247–1250.

[Bordes et al. 2013] Bordes, A.; Usunier, N.; Garc´ıa-Dur´an, A.;
Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for
modeling multi-relational data. In NIPS, 2787–2795.

[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio,
Y. 2014. A semantic matching energy function for learning with
multi-relational data. MACH LEARN 94(2):233–259.

[Carlson et al. 2010] Carlson, A.; Betteridge, J.; Kisiel, B.; Settles,
B.; Hruschka Jr, E. R.; and Mitchell, T. M. 2010. Toward an archi-
tecture for never-ending language learning. In AAAI, 1306–1313.
T.;
Rockt¨aschel, T.; and Riedel, S. 2016. Lifted rule injection for
relation embeddings. In EMNLP, 1389–1399.

[Demeester, Rockt¨aschel, and Riedel 2016] Demeester,

[Dong et al. 2014] Dong, X.; Gabrilovich, E.; Heitz, G.; Horn, W.;
Lao, N.; Murphy, K.; Strohmann, T.; Sun, S.; and Zhang, W. 2014.
Knowledge vault: A web-scale approach to probabilistic knowl-
edge fusion. In SIGKDD, 601–610.

[Duchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and Singer,
Y. 2011. Adaptive subgradient methods for online learning and
stochastic optimization. J MACH LEARN RES 12(Jul):2121–2159.
[Faruqui et al. 2014] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer,
C.; Hovy, E.; and Smith, N. A. 2014. Retroﬁtting word vectors to
semantic lexicons. arXiv:1411.4166.

[Gal´arraga et al. 2013] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2013. AMIE: Association rule mining under
In WWW,
incomplete evidence in ontological knowledge bases.
413–422.

[Gal´arraga et al. 2015] Gal´arraga, L. A.; Teﬂioudi, C.; Hose, K.;
and Suchanek, F. M. 2015. Fast rule mining in ontological knowl-
edge bases with AMIE+. VLDB J 24(6):707–730.

[Gardner, Talukdar, and Mitchell 2015] Gardner, M.; Talukdar, P.;
and Mitchell, T. 2015. Combining vector space embeddings with
symbolic logical inference over open-domain text. In AAAI Spring
Symposium Series, 61–65.

[Guo et al. 2015] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2015. Semantically smooth knowledge graph embedding. In
ACL, 84–94.

[Guo et al. 2016] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo,
L. 2016. Jointly embedding knowledge graphs and logical rules.
In EMNLP, 192–202.

[Guu, Miller, and Liang 2015] Guu, K.; Miller, J.; and Liang, P.
2015. Traversing knowledge graphs in vector space. In EMNLP,
318–327.

[H´ajek 1998] H´ajek, P. 1998. The metamathematics of fuzzy logic.

Kluwer.

[Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E.
2016. Harnessing deep neural networks with logic rules. In ACL,
2410–2420.

[Lin et al. 2015a] Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and
Liu, S. 2015a. Modeling relation paths for representation learning
of knowledge bases. In EMNLP, 705–714.

[Lin et al. 2015b] Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X.
2015b. Learning entity and relation embeddings for knowledge
graph completion. In AAAI, 2181–2187.

[Liu et al. 2016] Liu, Q.; Jiang, H.; Evdokimov, A.; Ling, Z.-H.;
Zhu, X.; Wei, S.; and Hu, Y. 2016. Probabilistic reasoning via
deep learning: Neural association models.

[Miller 1995] Miller, G. A. 1995. WordNet: A lexical database for

English. COMMUN ACM 38(11):39–41.

[Neelakantan, Roth, and McCallum 2015] Neelakantan, A.; Roth,
B.; and McCallum, A. 2015. Compositional vector space mod-
els for knowledge base completion. In ACL, 156–166.

[Nickel, Rosasco, and Poggio 2016] Nickel, M.; Rosasco, L.; and
Poggio, T. 2016. Holographic embeddings of knowledge graphs.
In AAAI, 1955–1961.

[Nickel, Tresp, and Kriegel 2011] Nickel, M.; Tresp, V.;

and
Kriegel, H.-P. 2011. A three-way model for collective learning on
multi-relational data. In ICML, 809–816.

[Rockt¨aschel et al. 2014] Rockt¨aschel, T.; Boˇsnjak, M.; Singh, S.;
and Riedel, S. 2014. Low-dimensional embeddings of logic. In
ACL Workshop on Semantic Parsing, 45–49.

[Rockt¨aschel, Singh, and Riedel 2015] Rockt¨aschel, T.; Singh, S.;
and Riedel, S. 2015. Injecting logical background knowledge into
embeddings for relation extraction. In NAACL, 1119–1129.

[Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng,
A. Y. 2013. Reasoning with neural tensor networks for knowledge
base completion. In NIPS, 926–934.

[Suchanek, Kasneci, and Weikum 2007] Suchanek, F. M.; Kasneci,
G.; and Weikum, G. 2007. YAGO: A core of semantic knowledge.
In WWW, 697–706.

[Trouillon et al. 2016] Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,
E.; and Bouchard, G. 2016. Complex embeddings for simple link
prediction. In ICML, 2071–2080.

[Vendrov et al. 2015] Vendrov, I.; Kiros, R.; Fidler, S.; and Ur-
2015. Order-embeddings of images and language.

tasun, R.
arXiv:1511.06361.

[Wang and Cohen 2016] Wang, W. Y., and Cohen, W. W. 2016.
Learning ﬁrst-order logic embeddings via matrix factorization. In
IJCAI, 2132–2138.

[Wang et al. 2014] Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z.
2014. Knowledge graph embedding by translating on hyperplanes.
In AAAI, 1112–1119.

[Wang et al. 2017] Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017.
Knowledge graph embedding: A survey of approaches and appli-
cations. IEEE TRANS KNOWL DATA ENG 29(12):2724–2743.
[Wang, Wang, and Guo 2015] Wang, Q.; Wang, B.; and Guo, L.
2015. Knowledge base completion using embeddings and rules.
In IJCAI, 1859–1865.

[Wei et al. 2015] Wei, Z.; Zhao, J.; Liu, K.; Qi, Z.; Sun, Z.; and
Tian, G. 2015. Large-scale knowledge base completion: Infer-
ring via grounding network sampling over selected instances. In
CIKM, 1331–1340.

[Weston et al. 2013] Weston, J.; Bordes, A.; Yakhnenko, O.; and
Usunier, N. 2013. Connecting language and knowledge bases with
embedding models for relation extraction. In EMNLP, 1366–1371.
[Xiao, Huang, and Zhu 2017] Xiao, H.; Huang, M.; and Zhu, X.
2017. SSP: Semantic space projection for knowledge graph em-
bedding with text descriptions. In AAAI, 3104–3110.

[Xie et al. 2016] Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M.
2016. Representation learning of knowledge graphs with entity
descriptions. In AAAI, 2659–2665.

[Xie, Liu, and Sun 2016] Xie, R.; Liu, Z.; and Sun, M. 2016. Rep-
resentation learning of knowledge graphs with hierarchical types.
In IJCAI, 2965–2971.

[Xiong, Power, and Callan 2017] Xiong, C.; Power, R.; and Callan,
J. 2017. Explicit semantic ranking for academic search via knowl-
edge graph embedding. In WWW, 1271–1279.

[Yang et al. 2015] Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng,
L. 2015. Embedding entities and relations for learning and infer-
ence in knowledge bases. In ICLR.

[Zhang et al. 2016] Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and
Ma, W.-Y. 2016. Collaborative knowledge base embedding for
recommender systems. In SIGKDD, 353–362.

