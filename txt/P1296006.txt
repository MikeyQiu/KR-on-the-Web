An Interpretable Knowledge Transfer Model
for Knowledge Base Completion

Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{qzxie, xuezhem, dzihang, hovy}@cs.cmu.edu

7
1
0
2
 
y
a
M
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
8
0
9
5
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

Knowledge bases are important resources
for a variety of natural language process-
ing tasks but suffer from incompleteness.
We propose a novel embedding model,
ITransF, to perform knowledge base com-
pletion. Equipped with a sparse atten-
tion mechanism, ITransF discovers hidden
concepts of relations and transfer statisti-
cal strength through the sharing of con-
cepts. Moreover, the learned associations
between relations and concepts, which
are represented by sparse attention vec-
tors, can be interpreted easily. We evalu-
ate ITransF on two benchmark datasets—
WN18 and FB15k for knowledge base
completion and obtains improvements on
both the mean rank and Hits@10 metrics,
over all baselines that do not use additional
information.

1

Introduction

Knowledge bases (KB), such as WordNet (Fell-
baum, 1998), Freebase (Bollacker et al., 2008),
YAGO (Suchanek et al., 2007) and DBpe-
dia (Lehmann et al., 2015), are useful resources
for many applications such as question answer-
ing (Berant et al., 2013; Yih et al., 2015; Dai
et al., 2016) and information extraction (Mintz
et al., 2009). However, knowledge bases suf-
fer from incompleteness despite their formidable
sizes (Socher et al., 2013; West et al., 2014), lead-
ing to a number of studies on automatic knowl-
edge base completion (KBC) (Nickel et al., 2015)
or link prediction.

The fundamental motivation behind these stud-
ies is that there exist some statistical regularities
under the intertwined facts stored in the multi-
relational knowledge base. By discovering gener-

alizable regularities in known facts, missing ones
may be recovered in a faithful way. Due to its ex-
cellent generalization capability, distributed repre-
sentations, a.k.a. embeddings, have been popular-
ized to address the KBC task (Nickel et al., 2011;
Bordes et al., 2011, 2014, 2013; Socher et al.,
2013; Wang et al., 2014; Guu et al., 2015; Nguyen
et al., 2016b).

As a seminal work, Bordes et al. (2013) pro-
poses the TransE, which models the statistical
regularities with linear translations between en-
tity embeddings operated by a relation embed-
ding. Implicitly, TransE assumes both entity em-
beddings and relation embeddings dwell in the
same vector space, posing an unnecessarily strong
prior. To relax this requirement, a variety of mod-
els ﬁrst project the entity embeddings to a relation-
dependent space (Bordes et al., 2014; Ji et al.,
2015; Lin et al., 2015b; Nguyen et al., 2016b),
and then model the translation property in the pro-
jected space. Typically, these relation-dependent
spaces are characterized by the projection matri-
ces unique to each relation. As a beneﬁt, differ-
ent aspects of the same entity can be temporarily
emphasized or depressed as an effect of the projec-
tion. For instance, STransE (Nguyen et al., 2016b)
utilizes two projection matrices per relation, one
for the head entity and the other for the tail entity.
Despite the superior performance of STransE
compared to TransE, it is more prone to the data
sparsity problem. Concretely, since the projection
spaces are unique to each relation, projection ma-
trices associated with rare relations can only be ex-
posed to very few facts during training, resulting in
poor generalization. For common relations, a sim-
ilar issue exists. Without any restrictions on the
number of projection matrices, logically related or
conceptually similar relations may have distinct
projection spaces, hindering the discovery, shar-
ing, and generalization of statistical regularities.

Previously, a line of research makes use of ex-
ternal information such as textual relations from
web-scale corpus or node features (Toutanova
et al., 2015; Toutanova and Chen, 2015; Nguyen
et al., 2016a), alleviating the sparsity problem. In
parallel, recent work has proposed to model reg-
ularities beyond local facts by considering multi-
relation paths (Garc´ıa-Dur´an et al., 2015; Lin
et al., 2015a; Shen et al., 2016). Since the number
of paths grows exponentially with its length, as a
side effect, path-based models enjoy much more
training cases, suffering less from the problem.

In this paper, we propose an interpretable
knowledge transfer model (ITransF), which en-
courages the sharing of statistic regularities be-
tween the projection matrices of relations and al-
leviates the data sparsity problem. At the core of
ITransF is a sparse attention mechanism, which
learns to compose shared concept matrices into
relation-speciﬁc projection matrices, leading to a
better generalization property. Without any ex-
ternal resources, ITransF improves mean rank and
Hits@10 on two benchmark datasets, over all pre-
vious approaches of the same kind. In addition,
the parameter sharing is clearly indicated by the
learned sparse attention vectors, enabling us to in-
terpret how knowledge transfer is carried out. To
induce the desired sparsity during optimization,
we further introduce a block iterative optimization
algorithm.

In summary,

the contributions of this work
are: (i) proposing a novel knowledge embedding
model which enables knowledge transfer by learn-
ing to discover shared regularities; (ii) introducing
a learning algorithm to directly optimize a sparse
representation from which the knowledge transfer-
ring procedure is interpretable; (iii) showing the
effectiveness of our model by outperforming base-
lines on two benchmark datasets for knowledge
base completion task.

2 Notation and Previous Models

Let E denote the set of entities and R denote the
set of relations.
In knowledge base completion,
given a training set P of triples (h, r, t) where
h, t ∈ E are the head and tail entities having a
relation r ∈ R, e.g., (Steve Jobs, FounderOf,
Apple), we want to predict missing facts such as
(Steve Jobs, Profession, Businessperson).

Most of the embedding models for knowledge
base completion deﬁne an energy function fr(h, t)

according to the fact’s plausibility (Bordes et al.,
2011, 2014, 2013; Socher et al., 2013; Wang et al.,
2014; Yang et al., 2015; Guu et al., 2015; Nguyen
et al., 2016b). The models are learned to minimize
energy fr(h, t) of a plausible triple (h, r, t) and to
maximize energy fr(h(cid:48), t(cid:48)) of an implausible triple
(h(cid:48), r, t(cid:48)).

linear

Motivated by the

translation phe-
nomenon observed in well trained word embed-
dings (Mikolov et al., 2013), TransE (Bordes et al.,
2013) represents the head entity h, the relation r
and the tail entity t with vectors h, r and t ∈ Rn
respectively, which were trained so that h + r ≈ t.
They deﬁne the energy function as

fr(h, t) = (cid:107)h + r − t(cid:107)(cid:96)

where (cid:96) = 1 or 2, which means either the (cid:96)1 or
the (cid:96)2 norm of the vector h + r − t will be used
depending on the performance on the validation
set.

To better model relation-speciﬁc aspects of
the same entity, TransR (Lin et al., 2015b) uses
projection matrices and projects the head entity
and the tail entity to a relation-dependent space.
STransE (Nguyen et al., 2016b) extends TransR
by employing different matrices for mapping the
head and the tail entity. The energy function is

fr(h, t) = (cid:107)Wr,1h + r − Wr,2t(cid:107)(cid:96)

However, not all relations have abundant data
to estimate the relation speciﬁc matrices as most
of the training samples are associated with only a
few relations, leading to the data sparsity problem
for rare relations.

3

Interpretable Knowledge Transfer

3.1 Model

As discussed above, a fundamental weakness in
TransR and STransE is that they equip each re-
lation with a set of unique projection matrices,
which not only introduces more parameters but
also hinders knowledge sharing. Intuitively, many
relations share some concepts with each other, al-
though they are stored as independent symbols in
KB. For example, the relation “(somebody) won
award for (some work)” and “(somebody) was
nominated for (some work)” both describe a per-
son’s high-quality work which wins an award or
a nomination respectively. This phenomenon sug-
gests that one relation actually represents a col-
lection of real-world concepts, and one concept

can be shared by several relations. Inspired by the
existence of such lower-level concepts, instead of
deﬁning a unique set of projection matrices for ev-
ery relation, we can alternatively deﬁne a small set
of concept projection matrices and then compose
them into customized projection matrices. Effec-
tively, the relation-dependent translation space is
then reduced to the smaller concept spaces.

However,

in general, we do not have prior
knowledge about what concepts exist out there and
how they are composed to form relations. There-
fore, in ITransF, we propose to learn this informa-
tion simultaneously from data, together with all
knowledge embeddings. Following this idea, we
ﬁrst present the model details, then discuss the op-
timization techniques for training.

Energy function Speciﬁcally, we stack all the
concept projection matrices to a 3-dimensional
tensor D ∈ Rm×n×n, where m is the pre-speciﬁed
number of concept projection matrices and n is the
dimensionality of entity embeddings and relation
embeddings. We let each relation select the most
useful projection matrices from the tensor, where
the selection is represented by an attention vector.
The energy function of ITransF is deﬁned as:

(1)

i αααH

r , αααT

r · D · t(cid:107)(cid:96)

fr(h, t) = (cid:107)αααH

where αααH
(cid:80)
i αααT

r · D · h + r − αααT
r ∈ [0, 1]m, satisfying (cid:80)
r,i =
r,i = 1, are normalized attention vectors
used to compose all concept projection matrices
in D by a convex combination. It is obvious that
STransE can be expressed as a special case of our
model when we use m = 2|R| concept matrices
and set attention vectors to disjoint one-hot vec-
tors. Hence our model space is a generalization of
STransE. Note that we can safely use fewer con-
cept matrices in ITransF and obtain better perfor-
mance (see section 4.3), though STransE always
requires 2|R| projection matrices.

We follow previous work to minimize the fol-

lowing hinge loss function:

(cid:88)

L =

(cid:2)γ + fr(h, t) − fr(h(cid:48), t(cid:48))(cid:3)

+ (2)

(h,r,t)∼P,
(h(cid:48),r,t(cid:48))∼N

where P is the training set consisting of correct
triples, N is the distribution of corrupted triples
deﬁned in section 3.3, and [·]+ = max(·, 0). Note
that we have omitted the dependence of N on
(h, r, t) to avoid clutter. We normalize the en-
tity vectors h, t, and the projected entity vectors

αααH
r · D · h and αααT
r · D · t to have unit length after
each update, which is an effective regularization
method that beneﬁts all models.

Sparse attention vectors
In Eq. (1), we have
r , αααT
deﬁned αααH
to be some normalized vectors
r
used for composition. With a dense attention vec-
tor, it is computationally expensive to perform the
convex combination of m matrices in each itera-
tion. Moreover, a relation usually does not consist
of all existing concepts in practice. Furthermore,
when the attention vectors are sparse, it is often
easier to interpret their behaviors and understand
how concepts are shared by different relations.

Motivated by these potential beneﬁts, we fur-
ther hope to learn sparse attention vectors in
ITransF. However, directly posing (cid:96)1 regulariza-
tion (Tibshirani, 1996) on the attention vectors
fails to produce sparse representations in our pre-
liminary experiment, which motivates us to en-
force (cid:96)0 constraints on αααT

r , αααH
r .
In order to satisfy both the normalization condi-
tion and the (cid:96)0 constraints, we reparameterize the
attention vectors in the following way:

r = SparseSoftmax(vH
αααH
r = SparseSoftmax(vT
αααT

r , IH
r )
r , IT
r )

r , vT

r ∈ Rm are the pre-softmax scores,
where vH
IH
r ∈ {0, 1}m are the sparse assignment vec-
r , IT
tors, indicating the non-zero entries of attention
vectors, and the SparseSoftmax is deﬁned as

SparseSoftmax(v, I)i =

exp(vi/τ )Ii
j exp(vj/τ )Ij

(cid:80)

r , αααH

r replace αααT

with τ being the temperature of Softmax.
reparameterization, vH

and
With this
IH
r , IT
r to become the real param-
eters of the model. Also, note that it is equiva-
lent to pose the (cid:96)0 constraints on IH
r instead of
αααT
r , αααH
r . Putting these modiﬁcations together, we
can rewrite the optimization problem as

r , vT
r

r , IT

minimize L
subject to (cid:107)IH

r (cid:107)0 ≤ k, (cid:107)IT

r (cid:107)0 ≤ k

(3)

where L is the loss function deﬁned in Eq. (2).

3.2 Block Iterative Optimization

Though sparseness is favorable in practice, it is
generally NP-hard to ﬁnd the optimal solution un-
der (cid:96)0 constraints. Thus, we resort to an approxi-
mated algorithm in this work.

For convenience, we refer to the parameters
with and without the sparse constraints as the
sparse partition and the dense partition, respec-
tively. Based on this notion, the high-level idea
of the approximated algorithm is to iteratively op-
timize one of the two partitions while holding the
other one ﬁxed. Since all parameters in the dense
partition, including the embeddings, the projection
matrices, and the pre-softmax scores, are fully dif-
ferentiable with the sparse partition ﬁxed, we can
simply utilize SGD to optimize the dense partition.
Then, the core difﬁculty lies in the step of optimiz-
ing the sparse partition (i.e. the sparse assignment
vectors), during which we want the following two
properties to hold

1. the sparsity required by the (cid:96)0 constaint is

maintained, and

2. the cost deﬁne by Eq. (2) is decreased.

r(cid:48) , IT

r(cid:48) for any r(cid:48)

r is independent of IH

Satisfying the two criterion seems to highly re-
semble the original problem deﬁned in Eq. (3).
However, the dramatic difference here is that with
parameters in the dense partition regarded as con-
stant, the cost function is decoupled w.r.t. each
relation r. In other words, the optimal choice of
IH
r , IT
(cid:54)= r.
Therefore, we only need to consider the optimiza-
tion for a single relation r, which is essentially an
assignment problem. Note that, however, IH
r and
IT
r are still coupled, without which we basically
reach the situation in a backpack problem. In prin-
ciple, one can explore combinatorial optimization
techniques to optimize IH
r(cid:48) jointly, which usu-
ally involve some iterative procedure. To avoid
adding another inner loop to our algorithm, we
turn to a simple but fast approximation method
based on the following single-matrix cost.

r(cid:48) , IT

Speciﬁcally, for each relation r, we consider the
r,i where only a single projection

induced cost LH
matrix i is used for the head entity:

LH

r,i =

(cid:88)

(cid:2)γ + f H

r,i(h, t) − f H

r,i(h(cid:48), t(cid:48))(cid:3)

+

(h,r,t)∼Pr,
(h(cid:48),r,t(cid:48))∼Nr

r,i(h, t) = (cid:107)Di · h + r − αααT

where f H
r · D · t(cid:107) is
the corresponding energy function, and the sub-
script in Pr and Nr denotes the subsets with rela-
tion r. Intuitively, LH
r,i measures, given the current
tail attention vector αααT
r , if only one project matrix
could be chosen for the head entity, how implausi-
ble Di would be. Hence, i∗ = arg mini LH
r,i gives

us the best single projection matrix on the head
side given αααT
r .

Now, in order to choose the best k matrices, we
basically ignore the interaction among projection
matrices, and update IH

r in the following way:

(cid:40)

IH
r,i ←

i ∈ argpartitioni(LH

r,i, k)

1,
0, otherwise

where the function argpartitioni(xi, k) produces
the index set of the lowest-k values of xi.

r,i and the energy function f T

Analogously, we can deﬁne the single-matrix
cost LT
r,i(h, t) on the
tail side in a symmetric way. Then, the update
rule for IH
follows the same derivation. Admit-
r
tedly, the approximation described here is rela-
tively crude. But as we will show in section 4,
the proposed algorithm yields good performance
empirically. We leave the further improvement of
the optimization method as future work.

3.3 Corrupted Sample Generating Method

Recall that we need to sample a negative triple
(h(cid:48), r, t(cid:48)) to compute hinge loss shown in Eq. 2,
given a positive triple (h, r, t) ∈ P . The distri-
bution of negative triple is denoted by N (h, r, t).
Previous work (Bordes et al., 2013; Lin et al.,
2015b; Yang et al., 2015; Nguyen et al., 2016b)
generally constructs a set of corrupted triples by
replacing the head entity or tail entity with a ran-
dom entity uniformly sampled from the KB.

However, uniformly sampling corrupted entities
may not be optimal. Often, the head and tail en-
tities associated a relation can only belong to a
speciﬁc domain. When the corrupted entity comes
from other domains, it is very easy for the model
to induce a large energy gap between true triple
and corrupted one. As the energy gap exceeds
γ, there will be no training signal from this cor-
rupted triple. In comparison, if the corrupted en-
tity comes from the same domain, the task be-
comes harder for the model, leading to more con-
sistent training signal.

Motivated by this observation, we propose to
sample corrupted head or tail from entities in
the same domain with a probability pr and from
the whole entity set with probability 1 − pr.
The choice of relation-dependent probability pr is
speciﬁed in Appendix A.1. In the rest of the paper,
we refer to the new proposed sampling method as
”domain sampling”.

4 Experiments

4.1 Setup

To evaluate link prediction, we conduct experi-
ments on the WN18 (WordNet) and FB15k (Free-
base) introduced by Bordes et al. (2013) and use
the same training/validation/test split as in (Bordes
et al., 2013). The information of the two datasets
is given in Table 1.

Dataset
WN18
FB15k

#E
40,943
14,951

#R
18
1,345

#Train
141,442
483,142

#Valid
5,000
50,000

#Test
5,000
59,071

Table 1: Statistics of FB15k and WN18 used in
experiments. #E, #R denote the number of enti-
ties and relation types respectively. #Train, #Valid
and #Test are the numbers of triples in the training,
validation and test sets respectively.

In knowledge base completion task, we evaluate
model’s performance of predicting the head entity
or the tail entity given the relation and the other en-
tity. For example, to predict head given relation r
and tail t in triple (h, r, t), we compute the energy
function fr(h(cid:48), t) for each entity h(cid:48) in the knowl-
edge base and rank all the entities according to the
energy. We follow Bordes et al. (2013) to report
the ﬁlter results, i.e., removing all other correct
candidates h(cid:48) in ranking. The rank of the correct
entity is then obtained and we report the mean rank
(mean of the predicted ranks) and Hits@10 (top 10
accuracy). Lower mean rank or higher Hits@10
mean better performance.

4.2

Implementation Details

We initialize the projection matrices with iden-
tity matrices added with a small noise sampled
from normal distribution N (0, 0.0052). The en-
tity and relation vectors of ITransF are initialized
by TransE (Bordes et al., 2013), following Lin
et al. (2015b); Ji et al. (2015); Garc´ıa-Dur´an et al.
(2016, 2015); Lin et al. (2015a). We ran mini-
batch SGD until convergence. We employ the
“Bernoulli” sampling method to generate incor-
rect triples as used in Wang et al. (2014), Lin et al.
(2015b), He et al. (2015), Ji et al. (2015) and Lin
et al. (2015a).

STransE (Nguyen et al., 2016b) is the most sim-
ilar knowledge embedding model to ours except
that they use distinct projection matrices for each
relation. We use the same hyperparameters as used
in STransE and no signiﬁcant improvement is ob-

served when we alter hyperparameters. We set the
margin γ to 5 and dimension of embedding n to
50 for WN18, and γ = 1, n = 100 for FB15k.
We set the batch size to 20 for WN18 and 1000 for
FB15k. The learning rate is 0.01 on WN18 and
0.1 on FB15k. We use 30 matrices on WN18 and
300 matrices on FB15k. All the models are imple-
mented with Theano (Bergstra et al., 2010). The
Softmax temperature is set to 1/4.

4.3 Results & Analysis
The overall link prediction results1 are reported
in Table 2. Our model consistently outperforms
previous models without external information on
both the metrics of WN18 and FB15k. On WN18,
we even achieve a much better mean rank with
comparable Hits@10 than current state-of-the-art
model IRN employing external information.

We can see that path information is very help-
ful on FB15k and models taking advantage of path
information outperform intrinsic models by a sig-
niﬁcant margin. Indeed, a lot of facts are easier
to recover with the help of multi-step inference.
For example, if we know Barack Obama is born in
Honolulu, a city in the United States, then we eas-
ily know the nationality of Obama is the United
States. An straightforward way of extending our
proposed model to k-step path P = {ri}k
i=1 is
to deﬁne a path energy function (cid:107)αααH
P · D · h +
(cid:80)
P is a concept asso-
ciation related to the path. We plan to extend our
model to multi-step path in the future.

P · D · t(cid:107)(cid:96), αααH

ri∈P ri − αααT

To provide a detailed understanding why the
proposed model achieves better performance, we
present some further analysis in the sequel.

Performance on Rare Relations
In the pro-
posed ITransF, we design an attention mecha-
nism to encourage knowledge sharing across dif-
ferent relations. Naturally, facts associated with
rare relations should beneﬁt most from such shar-
ing, boosting the overall performance. To verify
this hypothesis, we investigate our model’s perfor-
mance on relations with different frequency.

The overall distribution of relation frequencies
resembles that of word frequencies, subject to the
zipf’s law. Since the frequencies of relations ap-
proximately follow a power distribution, their log

1Note that although IRN (Shen et al., 2016) does not ex-
plicitly exploit path information, it performs multi-step infer-
ence through the multiple usages of external memory. When
IRN is allowed to access memory once for each prediction, its
Hits@10 is 80.7, similar to models without path information.

Model

Additional Information

SE (Bordes et al., 2011)
Unstructured (Bordes et al., 2014)
TransE (Bordes et al., 2013)
TransH (Wang et al., 2014)
TransR (Lin et al., 2015b)
CTransR (Lin et al., 2015b)
KG2E (He et al., 2015)
TransD (Ji et al., 2015)
TATEC (Garc´ıa-Dur´an et al., 2016)
NTN (Socher et al., 2013)
DISTMULT (Yang et al., 2015)
STransE (Nguyen et al., 2016b)
ITransF
ITransF (domain sampling)
RTransE (Garc´ıa-Dur´an et al., 2015)
PTransE (Lin et al., 2015a)
NLFeat (Toutanova and Chen, 2015)
Random Walk (Wei et al., 2016)
IRN (Shen et al., 2016)

No
No
No
No
No
No
No
No
No
No
No
No
No
No
Path
Path
Node + Link Features
Path
External Memory

WN18

FB15k

Mean Rank
985
304
251
303
225
218
348
212
-
-
-
206 (244)
205
223
-
-
-
-
249

Hits@10 Mean Rank Hits@10
162
979
125
87
77
75
59
91
58
-
-
69
65
77
50
58
-
-
38

80.5
38.2
89.2
86.7
92.0
92.3
93.2
92.2
-
66.1
94.2
93.4 (94.7)
94.2
95.2
-
-
94.3
94.8
95.3

39.8
6.3
47.1
64.4
68.7
70.2
74.0
77.3
76.7
41.4
57.7
79.7
81.0
81.4
76.2
84.6
87.0
74.7
92.7

Table 2: Link prediction results on two datasets. Higher Hits@10 or lower Mean Rank indicates better
performance. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two
groups. The ﬁrst group contains intrinsic models without using extra information. The second group
make use of additional information. Results in the brackets are another set of results STransE reported.

frequencies are linear. The statistics of relations
on FB15k and WN18 are shown in Figure 1. We
can clearly see that the distributions exhibit long
tails, just like the Zipf’s law for word frequency.

In order to study the performance of relations
with different frequencies, we sort all relations by
their frequency in the training set, and split them
into 3 buckets evenly so that each bucket has a
similar interval length of log frequency.

Within each bucket, we compare our model
with STransE, as shown in Figure 2.2 As we can
see, on WN18, ITransF outperforms STransE by
In partic-
a signiﬁcant margin on rare relations.
ular, in the last bin (rarest relations), the aver-
age Hits@10 increases from 74.4 to 92.0, showing
the great beneﬁts of transferring statistical strength
from common relations to rare ones. The compar-
ison on each relation is shown in Appendix A.2
where we can observe tha. On FB15k, we can also
observe a similar pattern, although the degree of
improvement is less signiﬁcant. We conjecture the
difference roots in the fact that many rare relations
on FB15k have disjoint domains, knowledge trans-
fer through common concepts is harder.

Interpretability In addition to the quantitative
evidence supporting the effectiveness of knowl-
edge sharing, we provide some intuitive examples
to show how knowledge is shared in our model. As

2Domain sampling is not employed.

we mentioned earlier, the sparse attention vectors
fully capture the association between relations and
concepts and hence the knowledge transfer among
relations. Thus, we visualize the attention vectors
for several relations on both WN18 and FB15K in
Figure 3.

For WN18, the words “hyponym” and “hyper-
nym” refer to words with more speciﬁc or gen-
eral meaning respectively. For example, PhD is
a hyponym of student and student is a hypernym
of PhD. As we can see, concepts associated with
the head entities in one relation are also associated
with the tail entities in its reverse relation. Further,
“instance hypernym” is a special hypernym with
the head entity being an instance, and the tail en-
tity being an abstract notion. A typical example is
(New York, instance hypernym, city). This
connection has also been discovered by our model,
indicated by the fact that “instance hypernym(T)”
and “hypernym(T)” share a common concept ma-
trix. Finally, for symmetric relations like “simi-
lar to”, we see the head attention is identical to the
tail attention, which well matches our intuition.

On FB15k, we also see the sharing be-
tween reverse relations,
in “(somebody)
won award for (some work)” and “(some work)
What’s
award winning work (somebody)”.
more, although relation “won award for” and
“was nominated for” share the same concepts,

as

(a) WN18

(b) FB15k

Figure 1: Frequencies and log frequencies of relations on two datasets. The X-axis are relations sorted
by frequency.

(a) WN18

(b) FB15k

Figure 2: Hits@10 on relations with different amount of data. We give each relation the equal weight
and report the average Hits@10 of each relation in a bin instead of reporting the average Hits@10 of
each sample in a bin. Bins with smaller index corresponding to high-frequency relations.

their attention distributions are different, suggest-
ing distinct emphasis. Finally, symmetric relations
like spouse behave similarly as mentioned before.

Model Compression A byproduct of parame-
ter sharing mechanism employed by ITransF is
a much more compact model with equal perfor-
mance. Figure 5 plots the average performance
of ITransF against the number of projection ma-
trices m, together with two baseline models. On
FB15k, when we reduce the number of matri-
ces from 2200 to 30 (∼ 90× compression), our
model performance decreases by only 0.09% on
Hits@10, still outperforming STransE. Similarly,
on WN18, ITransF continues to achieve the best
performance when we reduce the number of con-
cept project matrices to 18.

5 Analysis on Sparseness

Sparseness is desirable since it contribute to in-
terpretability and computational efﬁciency of our
model. We investigate whether enforcing sparse-
ness would deteriorate the model performance and
compare our method with another sparse encoding
methods in this section.

Dense Attention w/o (cid:96)1 regularization Al-
though (cid:96)0 constrained model usually enjoys many
practical advantages, it may deteriorate the model
performance when applied improperly. Here, we
show that our model employing sparse attention
can achieve similar results with dense attention
with a signiﬁcantly less computational burden. We
also compare dense attention with (cid:96)1 regulariza-
tion. We set the (cid:96)1 coefﬁcient to 0.001 in our ex-
periments and does not apply Softmax since the (cid:96)1
of a vector after Softmax is always 1. We compare
models in a setting where the computation time of

(a) WN18

(b) FB15k

Figure 3: Heatmap visualization of attention vectors for ITransF on WN18 and FB15k. Each row is an
attention vector αααH

r for a relation’s head or tail concepts.

r or αααT

(a) WN18

(b) FB15k

Figure 4: Heatmap visualization of (cid:96)1 regularized dense attention vectors, which are not sparse. Note
that the colorscale is not from 0 to 1 since Softmax is not applied.

(a) FB15k

(b) WN18

Figure 5: Performance with different number of projection matrices. Note that the X-axis denoting the
number of matrices is not linearly scaled.

dense attention model is acceptable3. We use 22
weight matrices on WN18 and 15 weight matri-
ces on FB15k and train both the models for 2000
epochs.

The results are reported in Table 3. Generally,
ITransF with sparse attention has slightly better or
comparable performance comparing to dense at-
tention. Further, we show the attention vectors of

3With 300 projection matrices, it takes 1h1m to run one

epoch for a model with dense attention.

model with (cid:96)1 regularized dense attention in Fig-
ure 4. We see that (cid:96)1 regularization does not pro-
duce a sparse attention, especially on FB15k.

Nonnegative Sparse Encoding In the proposed
model, we induce the sparsity by a carefully de-
signed iterative optimization procedure. Apart
from this approach, one may utilize sparse en-
coding techniques to obtain sparseness based on
the pretrained projection matrices from STransE.
Concretely, stacking |2R| pretrained projection

Method

Dense
Dense + (cid:96)1
Sparse

WN18

FB15k

MR H10
199
94.0
94.2
228
94.1
207

Time MR H10
79.4
69
4m34s
78.9
131
4m25s
79.6
67
2m32s

Time
4m30s
5m47s
1m52s

Table 3: Performance of model with dense atten-
tion vectors or sparse attention vectors. MR, H10
and Time denotes mean rank, Hits@10 and train-
ing time per epoch respectively

tensor X ∈
matrices into a 3-dimensional
R2|R|×n×n, similar sparsity can be induced by
solving an (cid:96)1-regularized tensor completion prob-
lem minA,D ||X − DA||2
2 + λ(cid:107)A(cid:107)(cid:96)1. Basically,
A plays the same role as the attention vectors in
our model. For more details, we refer readers to
(Faruqui et al., 2015).

For completeness, we compare our model with
the aforementioned approach4. The comparison
is summarized in table 4. On both benchmarks,
ITransF achieves signiﬁcant improvement against
sparse encoding on pretrained model. This perfor-
mance gap should be expected since the objective
function of sparse encoding methods is to mini-
mize the reconstruction loss rather than optimize
the criterion for link prediction.

Method

Sparse Encoding
ITransF

WN18

FB15k

MR H10 MR H10
79.1
211
81.0
205

86.6
94.2

66
65

Table 4: Different methods to obtain sparse repre-
sentations

6 Related Work

In KBC, CTransR (Lin et al., 2015b) enables re-
lation embedding sharing across similar relations,
but they cluster relations before training rather
than learning it in a principled way. Further, they
do not solve the data sparsity problem because
there is no sharing of projection matrices which
have a lot more parameters. Learning the asso-
ciation between semantic relations has been used
in related problems such as relational similarity
measurement (Turney, 2012) and relation adapta-
tion (Bollegala et al., 2015).

Data sparsity is a common problem in many
ﬁelds. Transfer learning (Pan and Yang, 2010)
has been shown to be promising to transfer knowl-

4We use the toolkit provided by (Faruqui et al., 2015).

edge and statistical strengths across similar mod-
els or languages. For example, Bharadwaj et al.
(2016) transfers models on resource-rich lan-
guages to low resource languages by parameter
sharing through common phonological features in
name entity recognition. Zoph et al. (2016) ini-
tialize from models trained by resource-rich lan-
guages to translate low-resource languages.

Several works on obtaining a sparse atten-
tion (Martins and Astudillo, 2016; Makhzani and
Frey, 2014; Shazeer et al., 2017) share a similar
idea of sorting the values before softmax and only
keeping the K largest values. However, the sorting
operation in these works is not GPU-friendly.

The block iterative optimization algorithm in
our work is inspired by LightRNN (Li et al., 2016).
They allocate every word in the vocabulary in a
table. A word is represented by a row vector and
a column vector depending on its position in the
table. They iteratively optimize embeddings and
allocation of words in tables.

7 Conclusion and Future Work

In summary, we propose a knowledge embedding
model which can discover shared hidden concepts,
and design a learning algorithm to induce the in-
terpretable sparse representation. Empirically, we
show our model can improve the performance
on two benchmark datasets without external re-
sources, over all previous models of the same kind.
In the future, we plan to enable ITransF to per-
form multi-step inference, and extend the sharing
mechanism to entity and relation embeddings, fur-
ther enhancing the statistical binding across pa-
rameters. In addition, our framework can also be
applied to multi-task learning, promoting a ﬁner
sharing among different tasks.

Acknowledgments

We thank anonymous reviewers and Graham Neu-
big for valuable comments. We thank Yulun Du,
Paul Mitchell, Abhilasha Ravichander, Pengcheng
Yin and Chunting Zhou for suggestions on the
draft. We are also appreciative for the great work-
ing environment provided by staff in LTI.

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.

References

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Seattle, Washington, USA, pages 1533–
1544.

James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a cpu and gpu math
expression compiler. In Proceedings of the Python
for scientiﬁc computing conference (SciPy). Austin,
TX, volume 4, page 3.

Akash Bharadwaj, David Mortensen, Chris Dyer, and
Jaime Carbonell. 2016. Phonologically aware neu-
ral model for named entity recognition in low re-
source transfer settings. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pages 1462–1472.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database for Structur-
ing Human Knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data. pages 1247–1250.

Danushka Bollegala, Takanori Maehara, and Ken-ichi
Kawarabayashi. 2015. Embedding semantic rela-
In Proceedings of
tions into word representations.
the Twenty-Fourth International Joint Conference
on Artiﬁcial Intelligence.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A Semantic Matching Energy
Function for Learning with Multi-relational Data.
Machine Learning 94(2):233–259.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating Embeddings for Modeling Multi-
relational Data. In Advances in Neural Information
Processing Systems 26, pages 2787–2795.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning Structured Embed-
In Proceedings of the
dings of Knowledge Bases.
Twenty-Fifth AAAI Conference on Artiﬁcial Intelli-
gence. pages 301–306.

Zihang Dai, Lei Li, and Wei Xu. 2016. Cfo: Condi-
tional focused neural question answering with large-
scale knowledge bases. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics, Berlin, Ger-
many, pages 800–810.

Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris
Dyer, and Noah A. Smith. 2015. Sparse overcom-
In Proceedings
plete word vector representations.
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1491–
1500.

Christiane D. Fellbaum. 1998. WordNet: An Electronic

Lexical Database. MIT Press.

Alberto Garc´ıa-Dur´an, Antoine Bordes, and Nico-
las Usunier. 2015. Composing Relationships with
In Proceedings of the 2015 Confer-
Translations.
ence on Empirical Methods in Natural Language
Processing. pages 286–290.

Alberto Garc´ıa-Dur´an, Antoine Bordes, Nicolas
Usunier, and Yves Grandvalet. 2016. Combining
Two and Three-Way Embedding Models for Link
Prediction in Knowledge Bases. Journal of Artiﬁ-
cial Intelligence Research 55:715–742.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing Knowledge Graphs in Vector Space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. pages
318–327.

Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao.
2015. Learning to Represent Knowledge Graphs
In Proceedings of the
with Gaussian Embedding.
24th ACM International on Conference on Informa-
tion and Knowledge Management. pages 623–632.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and
Jun Zhao. 2015. Knowledge Graph Embedding via
In Proceedings of the
Dynamic Mapping Matrix.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). pages 687–696.

Jens Lehmann, Robert

Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, and Christian Bizer. 2015. DB-
pedia - A Large-scale, Multilingual Knowledge Base
Extracted from Wikipedia. Semantic Web 6(2):167–
195.

Xiang Li, Tao Qin, Jian Yang, and Tieyan Liu. 2016.
LightRNN: Memory and Computation-Efﬁcient Re-
current Neural Networks. In Advances in Neural In-
formation Processing Systems 29.

Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun,
Siwei Rao, and Song Liu. 2015a. Modeling Rela-
tion Paths for Representation Learning of Knowl-
In Proceedings of the 2015 Confer-
edge Bases.
ence on Empirical Methods in Natural Language
Processing. pages 705–714.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015b. Learning Entity and Re-
lation Embeddings for Knowledge Graph Comple-
tion. In Proceedings of the Twenty-Ninth AAAI Con-
ference on Artiﬁcial Intelligence Learning, pages
2181–2187.

Alireza Makhzani and Brendan Frey. 2014. K-sparse
In Proceedings of the International

autoencoders.
Conference on Learning Representations.

Andr´e FT Martins and Ram´on Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
In Pro-
of attention and multi-label classiﬁcation.
ceedings of the 33th International Conference on
Machine Learning.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Asso-
ciation for Computational Linguistics, Suntec, Sin-
gapore, pages 1003–1011.

Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark
Johnson. 2016a. Neighborhood mixture model for
knowledge base completion. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning (CoNLL). Association for Com-
putational Linguistics, page 4050.

Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark
Johnson. 2016b.
STransE: a novel embedding
model of entities and relationships in knowledge
In Proceedings of the 2016 Conference of
bases.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. pages 460–466.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2015. A Review of Relational
Machine Learning for Knowledge Graphs. Proceed-
ings of the IEEE, to appear .

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A Three-Way Model for Collective
Learning on Multi-Relational Data. In Proceedings
of the 28th International Conference on Machine
Learning. pages 809–816.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on knowledge
and data engineering 22(10):1345–1359.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:

The sparsely-gated mixture-of-experts layer. In Pro-
ceedings of the International Conference on Learn-
ing Representations.

Yelong Shen, Po-Sen Huang, Ming-Wei Chang, and
Jianfeng Gao. 2016.
Implicit reasonet: Model-
ing large-scale structured relationships with shared
memory. arXiv preprint arXiv:1611.04642 .

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning With Neural Ten-
sor Networks for Knowledge Base Completion. In
Advances in Neural Information Processing Systems
26, pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web. pages 697–706.

Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological) pages 267–288.

Kristina Toutanova and Danqi Chen. 2015. Observed
Versus Latent Features for Knowledge Base and
Text Inference. In Proceedings of the 3rd Workshop
on Continuous Vector Space Models and their Com-
positionality. pages 57–66.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing Text for Joint Embedding of
Text and Knowledge Bases. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1499–1509.

Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artiﬁcial Intelligence Research 44:533–
585.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge Graph Embedding by
Translating on Hyperplanes. In Proceedings of the
Twenty-Eighth AAAI Conference on Artiﬁcial Intel-
ligence, pages 1112–1119.

Zhuoyu Wei, Jun Zhao, and Kang Liu. 2016. Mining
inference formulas by goal-directed random walks.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, Austin, Texas,
pages 1379–1388.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014. Knowledge Base Completion via Search-
In Proceedings of the
based Question Answering.
23rd International Conference on World Wide Web.
pages 515–526.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding Entities and
Relations for Learning and Inference in Knowledge
Bases. In Proceedings of the International Confer-
ence on Learning Representations.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguistics,
Beijing, China, pages 1321–1331.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
In Proceedings of the
neural machine translation.
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1568–1575.

A Appendix

A.1 Domain Sampling Probability

In this section, we deﬁne the probability pr to
generate a negative sample from the same domain
mentioned in Section 3.3. The probability cannot
be too high to avoid generating negative samples
that are actually correct, since there are generally
a lot of facts missing in KBs.

Speciﬁcally, let MH

r = {h | ∃t(h, r, t) ∈ P }
and MT
r = {t | ∃h(h, r, t) ∈ P } denote the
head or tail domain of relation r. Suppose Nr =
{(h, r, t) ∈ P } is the induced set of edges with
relation r. We deﬁne the probability pr as

pr = min(

, 0.5)

(4)

λ|MT

r ||MH
r |
|Nr|

Our motivation of such a formulation is as
follows: Suppose Or is the set that contains all
truthful fact triples on relation r, i.e., all triples
in training set and all other missing correct
triples.
If we assume all fact triples within the
domain has uniform probability of being true, the
probability of a random triple being correct is
P r((h, r, t) ∈ Or | h ∈ MH

r ) = |Or|
r ||MT
r |
Assume that all facts are missing with a proba-
bility λ, then |Nr| = λ|Or| and the above prob-
ability can be approximated by
r | . We
want the probability of generating a negative sam-
ple from the domain to be inversely proportional
to the probability of the sample being true, so we
deﬁne the probability as Eq. 4. The results in sec-
tion 4 are obtained with λ set to 0.001.

r , t ∈ MT

|Nr|
r ||MT

λ|MH

|MH

We compare how different value of λ would in-
ﬂuence our model’s performance in Table. 5. With
large λ and higher domain sampling probability,
our model’s Hits@10 increases while mean rank
also increases. The rise of mean rank is due to
higher probability of generating a valid triple as
a negative sample causing the energy of a valid
triple to increase, which leads to a higher over-
all rank of a correct entity. However, the reason-
ing capability is boosted with higher Hits@10 as
shown in the table.

A.2 Performance on individual relations of

WN18

We plot the performance of ITransF and STransE
on each relation. We see that the improvement is
greater on rare relations.

Method

λ = 0.0003
λ = 0.001
λ = 0.003

WN18

FB15k

MR H10 MR H10
217
80.4
80.6
223
80.9
239

95.0
95.2
95.2

68
73
82

Table 5: Different λ’s effect on our model perfor-
mance. The compared models are trained for 2000
epochs

Figure 6: Hits@10 on each relation in WN18. The
relations are sorted according to their frequency.

