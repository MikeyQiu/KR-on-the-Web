TimeBankPT: A TimeML Annotated Corpus of Portuguese

Francisco Costa, Ant´onio Branco

University of Lisbon
fcosta@di.fc.ul.pt, Antonio.Branco@di.fc.ul.pt

Abstract
In this paper, we introduce TimeBankPT, a TimeML annotated corpus of Portuguese. It has been produced by adapting an existing
resource for English, namely the data used in the ﬁrst TempEval challenge. TimeBankPT is the ﬁrst corpus of Portuguese with rich
temporal annotations (i.e. it includes annotations not only of temporal expressions but also about events and temporal relations). In
addition, it was subjected to an automated error mining procedure that checks the consistency of the annotated temporal relations based
on their logical properties. This procedure allowed for the detection of some errors in the annotations, that also affect the original
English corpus. The Portuguese language is currently undergoing a spelling reform, and several countries where Portuguese is ofﬁcial
are in a transitional period where old and new orthographies are valid. TimeBankPT adopts the recent spelling reform. This decision is
to preserve its usefulness in the future. TimeBankPT is freely available for download.

Keywords: Corpora, Temporal Information Extraction, Portuguese

1.

Introduction

There has been recent interest in the extraction of temporal
information from natural language text. The TERN 2004
(http://timex2.mitre.org/tern.html) evalua-
tion campaign focused on the automated identiﬁcation of
dates and times in text, and their normalization (i.e. con-
verting natural language expressions like yesterday or last
year into a number-based representation).
The recent TempEval
(Verhagen et al., 2007) and
TempEval-2 (Verhagen et al., 2010) challenges, part of
SemEval-2007 (Agirre et al., 2007) and SemEval-2010
(Erk and Strapparava, 2010) respectively, went farther and
explored the automated classiﬁcation of temporal relations
holding between the events and the times and dates men-
tioned in a text. An annotation scheme called TimeML
(Pustejovsky et al., 2003a) has gained prominence, and
several corpora have been released with TimeML annota-
tions, mostly in association with the TempEval campaigns.
TimeML is a rich annotation scheme in so far as it allows
for the annotation of several phenomena related to time: the
times, dates and periods denoted by temporal expressions,
events, temporal relations, etc.
TempEval-2 released TimeML annotated data for Chinese,
English, French, Italian, Korean and Spanish. The previ-
ous TempEval had made available data for English. Both
English data sets (that used in TempEval and that used in
TempEval-2) are based on a previous English corpus an-
notated with TimeML, the TimeBank (Pustejovsky et al.,
2003b). These are all the languages so far with data featur-
ing rich annotations about time.
The TimeBank and the data used in the two TempEval chal-
lenges are important, as they have annotations describing
not just dates and times, but also events and temporal re-
lations between these entities. The TempEval challenges
also changed the focus of temporal information processing
to the temporal relations (previously it had been mostly on
temporal expressions). This is an important problem, as
the task of automatically classifying temporal relations (e.g.
identifying whether the temporal relation holding between
two speciﬁc events mentioned in a text is one of temporal

overlap or temporal precedence) is hard, and there is still
much improvement to be made.
In this paper, we describe TimeBankPT, a corpus of Por-
tuguese annotated with TimeML. The corpus has around
70,000 words, a size similar to what is available for other
languages with this sort of annotation. TimeBankPT is
based on the English data used in the ﬁrst TempEval.

2. Methodology
In order to create TimeBankPT, we adapted the existing En-
glish data that were used in the ﬁrst TempEval, which are
annotated with TimeML.
In a ﬁrst step, all TimeML markup in the English TempEval
data was removed. The result was then given as input to
the Google Translator Toolkit,1 which combines machine
translation with a translation memory. A human translator
corrected the proposed translations manually.
After that, there are three collections of documents (the
original TimeML annotated data, the English unannotated
data and the Portuguese unannotated data). These three col-
lections are aligned by paragraphs: the line breaks in the the
original collection are simply maintained in the other col-
lections. Therefore, for each paragraph in the Portuguese
data all the corresponding TimeML tags in the original En-
glish paragraph are known.
A small script was developed to place all relevant TimeML
markup at the end of each paragraph in the Portuguese text.
Each TimeML markup element was then manually placed
in the correct place in that paragraph. At this point some
necessary changes to the annotations were also done man-
ually. These are motivated by language differences (e.g.
the inventory of verb tenses is language speciﬁc; see Sec-
tion 3). This approach involving manual steps is feasible
because the original TempEval corpus is not very large. It
must be noted that the TimeML annotations that describe
the temporal relations (the <TLINK> elements; see Sec-
tion 3) always occur at the end of each document, each in a
separate line, separate from the text: therefore they do not
need to be repositioned.

1http://translate.google.com/toolkit

3727

This methodology is also described in (Costa and Branco,
2010; Costa, to appear). The main difference between the
data obtained by the methodology that is reported there and
the data being described in the present paper is the applica-
tion of automated error detection, which is explained below
in Section 4. The cited work also contains a more detailed
description of the points where the annotations employed in
TimeBankPT differ from the ones in the English TempEval
data due to language differences, and the motivation for
some of the annotation decisions that had to be made be-
cause of such differences.

3. Annotations
Figure 1 contains a fragment of the data in TimeBankPT, to-
gether with the corresponding original English data, which
is part of the annotated corpus used in the ﬁrst TempEval.
In the annotation scheme that is employed, words that de-
note events are enclosed in <EVENT> elements. The at-
tributes that are appropriate for these elements are tense,
aspect, class, polarity, pos, stem. The stem is
the term’s lemma, and pos is its part-of-speech. Grammat-
ical tense and aspect are encoded in the features tense
and aspect. The attribute polarity takes the value
NEG if the event term is in a negative syntactic context,
and POS otherwise. The attribute class contains sev-
eral levels of information. It makes a distinction between
terms that denote actions of speaking, which take the value
REPORTING and those that do not. For these, it dis-
tinguishes between states (value STATE) and non-states
(value OCCURRENCE), and it also encodes whether they
create an intensional context (value I STATE for states and
value I ACTION for non-states).
Temporal expressions (timexes) are inside <TIMEX3> el-
ements. The most important features for these elements
are value, type and mod. The timex’s value en-
codes a normalized representation of this temporal entity,
its type can be e.g. DATE, TIME or DURATION. The
mod attribute is optional.
It is used for expressions like
early this year, which are annotated with mod="START".
As can be seen in Figure 1 there are other attributes for
timexes that encode whether it is the document’s creation
time (functionInDocument) and whether its value
can be determined from the expression alone or requires
other sources of information (temporalFunction and
anchorTimeID). It is important to note that the corpus
is divided in several documents, and every document con-
tains the annotation of one special time—the document’s
creation time—that is the time when the document was cre-
ated.
The <TLINK> elements encode temporal relations. The
attribute relType of these elements represents the type of
relation. The attribute eventID is a reference to the ﬁrst
argument of that relation. The second argument is given by
the attribute relatedToTime (if it is a time, a date or a
duration) or relatedToEvent (if it is another event).
The temporal relations are divided in three groups: in group
A one ﬁnds the relations holding between an event and
a temporal expression occurring in the same sentence; in
group B, temporal relations between events and the doc-
ument’s creation time; and in group C there are relations

holding between the main events of two adjacent sentences.
These groups reﬂect the three tasks of the ﬁrst TempEval.
The task attribute of TLINK elements is the name of the
TempEval task to which this temporal relation pertains (A,
B or C).

4. Automated Error Mining

These data have been used extensively for over a year now
for experimental work. During this time, a few errors, al-
ready present in the English source, have been detected and
corrected.
It is possible to automatically detect errors in temporal an-
notation. For instance, if an event A is annotated as tem-
porally preceding another event B, and B is annotated as
preceding C, A must precede C as well, because temporal
precedence is a transitive relation. If we then ﬁnd an anno-
tation according to which C precedes A, we have a tempo-
ral loop, and something is wrong. We have run a temporal
reasoning system on the adapted data, which enabled us to
detect this kind of error.
The original TempEval data had been similarly checked
for consistency (Verhagen, 2005). However, our reasoning
component performs one extra step, that allowed us to iden-
tify more errors: before applying any temporal reasoning
rules, it ﬁrst orders annotated temporal expressions accord-
ing to their normalized value (e.g. the date 1989-09-29
is ordered as preceding 1989-10-02). That is, we ex-
ploit the TIMEX3 annotations in order to enrich the set of
temporal relations that we work with, and more speciﬁcally
we make use of the value attribute of TIMEX3 elements.
In this way, we end up working with many more temporal
relations than those explicitly annotated. All temporal rela-
tions that are explicitly annotated are binary and involve at
least one event. Our approach further adds a large number
of temporal relations between dates or times.
The corpus distribution contains a ﬁle where each error
that was discovered with the help of temporal reasoning
is described. This ﬁle serves as documentation about the
changes introduced during the adaptation process, but from
these descriptions it is also easy to identify the correspond-
ing data in the original English corpus.
The inference procedure allowed for the detection of
around 80 problems in the corpus (affecting both the train
and the test sets),
that were then manually corrected.
These corrections result in some differences between Time-
BankPT and the original TempEval English data. Since
they affect the type of the annotated temporal relations, they
cause differences in the distribution of temporal relations.
In Section 6, we quantify the effect of these corrections on
the data, by comparing the distribution of temporal rela-
tions in TimeBankPT with that in the English TempEval
data set.
Several authors have used reasoning as a means to aid tem-
poral annotation. Katz and Arosio (2001) used a temporal
reasoning system to compare the temporal annotations of
two annotators. In a similar spirit, Setzer and Gaizauskas
(2001) ﬁrst compute the deductive closure of annotated
temporal relations so that they can then assess annotator
agreement with standard precision and recall measures.

3728

<s><TIMEX3 tid="t18" type="DATE" value="1998-01-11" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t14">Hoje</TIMEX3> h´a helic´opteros a <EVENT
eid="e7" class="OCCURRENCE" stem="sobrevoar" aspect="NONE" tense="INF"
polarity="POS" pos="VERB">sobrevoar</EVENT> o norte de Nova Iorque a <EVENT eid="e8"
class="I ACTION" stem="tentar" aspect="NONE" tense="INF" polarity="POS"
pos="VERB">tentar</EVENT> <EVENT eid="e9" class="OCCURRENCE" stem="localizar"
aspect="NONE" tense="INF" polarity="POS" pos="VERB">localizar</EVENT> pessoas <EVENT
eid="e10" class="OCCURRENCE" stem="isolar" aspect="NONE" tense="PPA"
polarity="POS" pos="VERB">isoladas</EVENT> sem alimentos, aquecimento ou medicamentos.</s>
<TLINK lid="l3" relType="OVERLAP" eventID="e8" relatedToTime="t18" task="A"/>

<s>Helicopters are <EVENT eid="e7" class="OCCURRENCE" stem="fly" aspect="PROGRESSIVE"
tense="PRESENT" polarity="POS" pos="VERB">ﬂying</EVENT> over northern New York <TIMEX3
tid="t18" type="DATE" value="1998-01-11" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t14">today</TIMEX3> <EVENT eid="e8"
class="I ACTION" stem="try" aspect="NONE" tense="PRESPART" polarity="POS"
pos="VERB">trying</EVENT> to <EVENT eid="e9" class="OCCURRENCE" stem="locate"
aspect="NONE" tense="INFINITIVE" polarity="POS" pos="VERB">locate</EVENT> people
<EVENT eid="e10" class="OCCURRENCE" stem="strand" aspect="NONE" tense="PASTPART"
polarity="POS" pos="VERB">stranded</EVENT> without food, heat or medicine.</s>
<TLINK lid="l3" relType="OVERLAP" eventID="e8" relatedToTime="t18" task="A"/>

Figure 1: Example fragment taken from TimeBankPT, in the upper box. The original English annotation is shown in the
lower box. The raw text is Helicopters are ﬂying over northern New York today trying to locate people stranded without
food, heat or medicine. The event e8, denoted by the term tentar “trying”, overlaps the date t18, denoted by the term hoje
“today”.

Verhagen (2005) uses temporal closure as a means to aid
TimeML annotation. He reports that closing a set of manu-
ally annotated temporal relations more than quadruples the
number of temporal relations in TimeBank (Pustejovsky et
al., 2003b), a corpus that is the source of the data used for
the TempEval challenges.
A considerable amount of work in the area of tempo-
ral information processing—for which this sort of data is
useful—has used reasoning components in the proposed so-
lutions. One recent example is the work of Ha et al. (2010),
a participant of the second TempEval, but there are several
others.

4.1. Ordering of Dates and Times

As mentioned already, temporal expressions are ordered
according to their normalized value. For instance, the
date 2000-01-03 is ordered as preceding the date
2010-03-04. Since all temporal expressions are normal-
ized in the annotated data, we order temporal expressions
before applying any temporal reasoning. This increases the
number of temporal relations we start with, and the poten-
tial number of relations we end up with after applying tem-
poral reasoning.
end, we used Joda-Time 2.0 (http://
To this
joda-time.sourceforge.net). Each normalized
date or time is converted to an interval.
In many cases it is possible to specify the start and end
points of this interval. For instance, the date 2000-01-03
is represented internally by an interval with its start

point at 2000-01-03T00:00:00.000 and ending at
2000-01-03T23:59:59.999. Many different kinds
of normalized expressions require many rules. For instance,
an expression like last Winter could be annotated in the data
as 2010-WI, and dedicated rules are used to get its start
and end points.
Some time expressions are normalized as PRESENT REF
(e.g. now), PAST REF (the past) or FUTURE REF (the fu-
ture). These cases are not represented by any Joda-Time
object.
Instead we need to account for them in a spe-
cial way. They can be temporally ordered among them-
selves (e.g. PRESENT REF precedes FUTURE REF), but
not with other temporal expressions. We further stipu-
late that PRESENT REF includes each document’s creation
time (which therefore precedes FUTURE REF, etc.). So, in
additional to the representation of times and dates as time
intervals, we employ a layer of ad-hoc rules.
The variety of temporal expressions makes it impossible
to provide a full account of the implemented rules in this
paper; more details are in (Costa, to appear).
Chambers and Jurafsky (2008) also order dates symboli-
cally before applying reasoning to increase the number of
explicit temporal relations. Their work is, however, more
limited: they only order dates (we also order times); when
doing so they only look at the year, month and day of the
month (the normalized value of temporal expressions can
be represented by resorting to other ﬁelds, such as the sea-
son of the year, which we explore). In addition, our work
uses a richer set of temporal relations (we allow for inclu-

3729

sion relations between dates/times) and a richer set of rea-
soning rules.

4.2. Deduction Procedure
The rules implemented in our reasoning component are:

• Temporal precedence is transitive, irreﬂexive and an-

tisymmetric;

• Temporal overlap is reﬂexive and symmetric;

• If A does not precede B, then either B precedes A or

A and B overlap;

precede A.

• If A overlaps B and B precedes C, then C does not

Because we also consider temporal relations between times
and dates, we also deal with temporal inclusion, a type of
temporal relation that is not part of the annotations used
in the TempEval data but that is still useful for reasoning.
We make use of the following additional rules, dealing with
temporal inclusion:

• Temporal inclusion is transitive, reﬂexive and anti-

symmetric;

• If A includes B, then A and B overlap;

• If A includes B and C overlaps B, then C overlaps A;

• If A includes B and C precedes A, then C precedes B;

• If A includes B and A precedes C, then B precedes C;

• If A includes B and C precedes B, then either C pre-
cedes A or A and C overlap (A cannot precede C).

• If A includes B and B precedes C, then either A pre-
cedes C or A and C overlap (C cannot precede A).

5. Description of the Corpus
The original English data for TempEval are organized in
two data sets: one for training and development and another
one for evaluation. The full data are organized in 182 docu-
ments (162 documents in the training data and another 20 in
the test data). Each document is a news report from televi-
sion broadcasts, newswire or newspapers. A large amount
of the documents (123 in the training set and 12 in the test
data) are taken from several issues of the Wall Street Jour-
nal dating from 1989. These texts are usually smaller than
the other ones, and contain a large amount of jargon and
stock market data. Therefore, the corpus is mostly quite
domain speciﬁc.
TimeBankPT contains the same translated documents.

5.1. Qualitative Description
As mentioned before, TimeBankPT has annotations sim-
ilar to those of the data prepared for the ﬁrst TempEval,
which follow an annotation scheme similar to TimeML.
Event terms are identiﬁed and marked with <EVENT> tags.
Temporal expressions are annotated inside <TIMEX3>
elements.
Temporal relations holding between these
events and the dates, times or durations denoted by the

Sentences
Words

Annotations
Events
Temporal expressions
Temporal relations

Task A
Task B
Task C

Words/events
Words/temporal expressions

Train

Test

2,281
60,782

351
8,920

6,790
1,244
5,781
1490
2556
1735

8.95
48.86

1,097
165
758
169
331
258

8.13
54.06

Table 1: Size of the corpus and number of annotations

temporal expressions are represented with TLINK ele-
ments. These elements have attributes that refer to the
two arguments of the relation, and the relType at-
tribute encodes the relation type.
Its possible values are
BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE, but the last three val-
ues occur rarely.

5.2. Quantitative Description
The major difference between the original TempEval cor-
pus in English and TimeBankPT is the number of words,
which is due to language differences, with Portuguese be-
ing more verbose than English (the English data consist of
52,740 words for training and 8,107 words for evaluation).
Table 1 presents some quantitative information about Time-
BankPT.

6. Evaluation
We ran several off-the-shelf machine learning algorithms
on the adapted data and compared the results with those
published for the English data, using the same classiﬁers.
the results of training
(Hepple et al., 2007) present
Weka (Witten and Frank, 2005) classiﬁers on the English
TempEval data. The problem here is to guess the relation
type (the value of the relType attribute mentioned ear-
lier) of the annotated temporal relations: BEFORE, AFTER,
OVERLAP, etc. The temporal relations themselves are al-
ready identiﬁed, as well as their arguments, and all other
annotations (about events, times, etc.) are given. Many of
these annotations are used as classiﬁer attributes.
All classiﬁer features are based on the textual string and on
the remaining TimeML annotations. We used the same set
of features and the same algorithms as the ones that were
used for English by Hepple et al. (2007). Table 2 shows the
classiﬁer features employed by Hepple et al. (2007) and
also by us for this experiment.
In this table,
the features grouped under EVENT are
based on the attributes of TimeML EVENT elements
with the same name. The ones grouped under TIMEX3
are taken from TIMEX3 elements.
The ORDER at-
tributes are computed by simple string manipulation of
the TimeML annotated documents: event-first en-
codes whether the event term appears in the document

3730

Type

Attribute

EVENT

aspect
polarity
POS
stem
string
class
tense

ORDER

TIMEX3

X N/A
adjacent
X N/A
event-first
event-between × N/A
timex-between × N/A
X ×
X ×

mod
type

TLINK

relType

X X

Task
B

A

X X
X X
X X
X ×
×
×
× X
× X

C

X
×
X
×
×
X
X

N/A
N/A
N/A
N/A

N/A
N/A

X

Table 2: Features used by Hepple et al.
TempEval.

(2007) in

F-Measure

Group Algorithm

English

Portuguese

A

B

C

DecisionTable
Baseline

KStar
Baseline

SMO
Baseline

0.59
0.57

0.73
0.56

0.54
0.47

0.58
0.59

0.77
0.56

0.54
0.47

Table 3: Performance of some classiﬁers on the test data

before the timex; event-between whether there is an
annotated event term in the text between the two enti-
ties; timex-between is similar, but considers tempo-
ral expressions; and adjacent is true if and only if both
event-between and timex-between are false (even
if some textual material actually occurs between the two
annotated elements). The last feature is the class attribute
(what the classiﬁers are supposed to guess).
We also used the same implementation of the algorithms—
namely Weka’s (Witten and Frank, 2005): KStar, a near-
est neighbors algorithm for task A (Cleary and Trigg,
1995); a decision table for task B (Kohavi, 1995); and SMO,
a support vector algorithm for task C (Platt, 1998). These
classiﬁer and feature combinations are optimized for En-
glish, but they serve our purpose of comparing the two data
sets.
Table 3 shows their results alongside ours. The results pre-
sented are for classiﬁers trained on the entire training set
and evaluated on the test set.
The results in Table 3 show that, despite language differ-
ences and the additional corrections performed on the Por-
tuguese data, the results on the two data sets are neverthe-
less quite comparable. From these results we conclude that
the adaptation was not lossy.
The most salient difference, when it comes to classiﬁer per-

formance, is for task B, with a 4% difference between the
English data and the Portuguese data. After inspecting the
models produced by classiﬁcation algorithms that produce
human readable models (such as the RIPPER algorithm
(Cohen, 1995) or decision trees (Quinlan, 1993)) for this
task, we see that verb tense is the most important feature
used by them. Because verb tense is language speciﬁc, we
hypothesize that it is the differences in the tense system of
the two languages that are behind the differences in the re-
sults for task B (i.e. they are due to language differences).
The other tasks do not seem to be so sensitive to tense. It
makes sense that it is precisely task B that is affected the
most by it, as task B is about temporal relations holding
between events and the document’s creation time, and verb
tense is primarily an indicator of the temporal relation be-
tween the event denoted by the verb and the speech time.
In Table 3 we also present the majority class baselines for
each task. The differences in the baselines between Time-
BankPT and the TempEval corpus of English are due to the
corrections to the data resulting from the automated error
mining procedure described in Section 4. Table 4 shows the
class distributions for the three TempEval tasks, both for
the English data used in TempEval and for TimeBankPT, in
full detail. As can be seen from that table, the differences
are very small.

6.1. Size of the Corpus

A corpus of approximately 70,000 words is small for
many natural language processing tasks. In order to check
whether the size of TimeBankPT is adequate for the tasks
that it is meant to address (automatic temporal relation clas-
siﬁcation), one can measure the effect of the size of the data
on classiﬁer performance.
Figure 2 shows the performance of classiﬁers similar to the
ones in Table 3 but trained with subsets of the training data.
They were evaluated on the whole test set.
The machine learning algorithms employed to get the val-
ues shown there are the same as the ones in Table 3. The
models were produced using the same feature set, too. Each
value used to plot that graph is the average of ten samplings
of the training data that differ only in as much as they use
different seeds for the random number generator involved
in the sampling process.
The performance of the classiﬁers for the three sorts of tem-
poral relations appears quite stable across many sizes of
training data. Classiﬁer performance does go up with more
training data, but it does so very slowly. Therefore, more
data would likely not increase classiﬁer performance very
quickly.
Figure 3 shows similar data, this time using subsets of the
test data. That is, the classiﬁers trained with the full train-
ing set were tested with subsets of the test data of different
sizes. Each data point is also the average of ten runs that
used the same amount of test data but different seeds to the
random number generator used to sample the data. Once
again, it can be seen that the curves are rather stable after
an initial range of very short test data sizes, where, precisely
because of the small size of the test data, the curves are a bit
erratic and variation is high (not visible in that graph). This
problem is more obvious in Figure 3 than in Figure 2 be-

3731

Set

Class

Train BEFORE

AFTER
OVERLAP
BEFORE-OR-OVERLAP
OVERLAP-OR-AFTER
VAGUE

BEFORE
AFTER
OVERLAP
BEFORE-OR-OVERLAP
OVERLAP-OR-AFTER
VAGUE

Task A

Task B

Task C

EN

PT

EN

PT

EN

PT

19% 19% 62% 62% 25% 25%
25% 25% 14% 14% 18% 17%
50% 49% 19% 19% 42% 42%
4%
3%
9%

2%
1%
2%

4%
3%
9%

2%
2%
2%

2%
2%
2%

2%
1%
1%

12% 11% 56% 56% 23% 23%
18% 18% 15% 15% 16% 16%
57% 59% 24% 25% 47% 47%
5%
3%
6%

2%
3%
7%

2%
1%
2%

5%
3%
6%

1%
3%
8%

3%
0%
2%

Table 4: Class distributions for the three tasks, the two data sets in each corpus (train and test) and the two corpora (English,
EN, and Portuguese, PT).

Task A
Task B
Task C

Test

)
e
r
u
s
a
e
m
-
F
(

a
t
a
d

t
s
e
t
n
o
e
r
o
c
s

r
e
ﬁ
i
s
s
a
l
C

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

40

70
50
30
Training instances (% of total)

60

80

90

100

Figure 2: Classiﬁer performance by size of training data

cause the test data set is considerably smaller than the train
data set (see Table 1).
From these two results we conclude that it appears that in-
creasing the size of the corpus would not rapidly increase
classiﬁer performance.

7. A Note on Spelling

effect is quite recent.2 It uniﬁes the two ofﬁcial orthogra-
the Brazilian spelling,
phies that existed for Portuguese:
followed by Brazil, and the European spelling, followed by
the remaining Portuguese speaking countries.
The new orthography has already been ratiﬁed in ﬁve coun-
tries (Brazil, Cape Verde, East Timor, Guinea-Bissau, Por-
tugal and S˜ao Tom´e and Pr´ıncipe). Only two countries

The spelling of the Portuguese language is currently in the
middle of a reform. The new spelling (Houaiss, 1991) is
known as the 1990 spelling agreement but its coming into

2An ofﬁcial document with the spelling agreement can
at http://www.dre.pt/pdf1s/1991/08/
found

be
193A00/43704388.pdf.

3732

Task A
Task B
Task C

)
e
r
u
s
a
e
m
-
F
(

a
t
a
d

t
s
e
t
n
o
e
r
o
c
s

r
e
ﬁ
i
s
s
a
l
C

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50
Test instances (% of total)

60

70

80

90

100

Figure 3: Classiﬁer performance by size of test data

where Portuguese is ofﬁcial (Angola and Mozambique)
have yet to ratify it. In 2009, several countries, including
Brazil and Portugal, initiated a transitional period in which
the old spellings are still acceptable, in parallel with the
new ones.

The most noticeable change to the spelling is, from the
Brazilian point of view, the deletion of diacritic marks in
some words. In many cases the European spelling did not
use them already. So for instance, ideia (“idea”) is now
written like that by all speakers, whereas the old Brazil-
ian spelling is id´eia, and similarly for the word frequente
(“frequent”), with the older spelling freq¨uente. The most
striking change to the European orthography is the removal
of silent consonants (consonants that were written solely
because of etymology but had no phonological basis), that
had already been abandoned in the Brazilian spelling. One
example is the word ´otimo (“great”), which has the old Eu-
ropean spelling ´optimo, with a silent p.

TimeBankPT features the uniﬁed orthography, so that the
corpus remains useful for future research on the long run.
This decision has, however, negative short term conse-
quences, as the typical existing natural language processing
tools, developed for the old spellings, may not have been
updated yet. Error rates may be higher currently when pro-
cessing data with the new spelling, as some frequent words
are now out-of-vocabulary (because they have a different
spelling) for the natural language processing tools not yet
updated.

8. Concluding Remarks
In this paper we presented TimeBankPT, a corpus of Por-
tuguese with rich temporal annotations that is available for
free.
This is a novel resource for Portuguese. Although there
is data for this language containing annotated temporal ex-
pressions, full temporal annotation—with events and tem-
poral relations—had not been released before. In addition,
by increasing the set of languages for which this kind of an-
notated data are available, we hope to stimulate research on
temporal information processing, where a lot of progress
can still be made.
Furthermore, we reported on a sophisticated method to au-
tomatically detect errors in the corpus. It is based on ex-
isting methodology that was also employed in the creation
of the original data on which TimeBankPT is based, i.e.
the English data used in the ﬁrst TempEval. However, we
expanded this automated error mining procedure in such a
way that more errors were detected.
Finally, we also tried to check whether the resulting data
would be useful, by replicating the results obtained for En-
glish for the problem of temporal information extraction
(more speciﬁcally the classiﬁcation of temporal relations),
and whether the size of TimeBankPT, which is a small cor-
pus, is adequate for this task, which it is intended to serve.
TimeBankPT has already been used to develop a temporal
annotation tool for Portuguese (Costa and Branco, 2012a;
Costa and Branco, 2012b).
The data are freely available at http://nlx.di.fc.
ul.pt/˜fcosta/TimeBankPT.

3733

European Conference on Machine Learning, pages 174–
189.

John Platt. 1998. Fast training of support vector ma-
chines using sequential minimal optimization. In Bern-
hard Sch¨olkopf, Chris Burges, and Alexander J. Smola,
editors, Advances in Kernel Methods—Support Vector
Learning.

James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser
Saur´ı, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003a. TimeML: Robust speciﬁcation of event
and temporal expressions in text. In IWCS-5, Fifth In-
ternational Workshop on Computational Semantics.
James Pustejovsky, Patrick Hanks, Roser Saur´ı, An-
drew See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and Mar-
cia Lazo. 2003b. The TIMEBANK corpus. In Proceed-
ings of Corpus Linguistics 2003, pages 647–656.

John Ross Quinlan. 1993. C4.5: Programs for Machine

Learning. Morgan Kaufmann, San Mateo, CA.

Andreas Setzer and Robert Gaizauskas. 2001. A pilot
study on annotating temporal relations in text. In ACL
2001 Workshop on Temporal and Spatial Information
Processing.

Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark
Hepple, and James Pustejovsky. 2007. SemEval-2007
Task 15: TempEval temporal relation identiﬁcation. In
Proceedings of SemEval-2007.

Marc Verhagen, Roser Saur´ı, Tommaso Caselli, and James
Pustejovsky. 2010. SemEval-2010 task 13: TempEval-
In Katrin Erk and Carlo Strapparava, editors, Se-
2.
mEval 2010—5th International Workshop on Semantic
Evaluation—Proceedings of the Workshop, pages 51–62,
Uppsala, Sweden. Uppsala University.

Marc Verhagen. 2005. Temporal closure in an annotation
environment. In Language Resources and Evaluation,
number 39, pages 211–241.

Ian H. Witten and Eibe Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann, San Francisco.
second edition.

9. References

Eneko Agirre, Llu´ıs M´arquez, and Richard Wicentowski,
editors. 2007. Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
Association for Computational Linguistics, Prague,
Czech Republic, June.

Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal order-
ing. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
698–706, Honolulu, Hawaii. Association for Computa-
tional Linguistics.

John G. Cleary and Leonard E. Trigg. 1995. K*: An
instance-based learner using an entropic distance mea-
In 12th International Conference on Machine
sure.
Learning, pages 108–114.

William W. Cohen. 1995. Fast effective rule induction. In
Proceedings of the Twelfth International Conference on
Machine Learning, pages 115–123.

Francisco Costa and Ant´onio Branco. 2010. Temporal in-
formation processing of a new language: Fast porting
with minimal resources. In ACL2010—Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics.

Francisco Costa and Ant´onio Branco. 2012a. Extracting
temporal information from Portuguese texts. In Proceed-
ings of PROPOR2012.

Francisco Costa and Ant´onio Branco.

2012b. LX-
TimeAnalyzer: A temporal information processing sys-
tem for Portuguese. Technical report, Universidade de
Lisboa, Faculdade de Ciˆencias, Departamento de In-
form´atica.

Francisco Costa. to appear. Processing Temporal Informa-
tion in Unstructured Documents. Ph.D. thesis, Universi-
dade de Lisboa, Lisbon.

Katrin Erk and Carlo Strapparava, editors. 2010. Se-
mEval 2010—5th International Workshop on Seman-
tic Evaluation—Proceedings of the Workshop. Uppsala
University, Uppsala, Sweden, July.

Eun Young Ha, Alok Baikadi, Carlyle Licata, and James C.
Lester. 2010. NCSU: Modeling temporal relations with
Markov logic and lexical ontology. In Katrin Erk and
Carlo Strapparava, editors, SemEval 2010—5th Interna-
tional Workshop on Semantic Evaluation—Proceedings
of the Workshop, pages 341–344, Uppsala, Sweden. Up-
psala University.

Mark Hepple, Andrea Setzer, and Rob Gaizauskas. 2007.
USFD: Preliminary exploration of features and classi-
ﬁers for the TempEval-2007 tasks. In Proceedings of
SemEval-2007, pages 484–487, Prague, Czech Republic.
Association for Computational Linguistics.

Antˆonio Houaiss. 1991. A Nova Ortograﬁa da L´ıngua Por-

tuguesa. ´Atica, S˜ao Paulo.

Graham Katz and Fabrizio Arosio. 2001. The annotation
of temporal information in natural language sentences.
In Proceedings of the 2001 ACL Workshop on Temporal
and Spatial Information Processing, Toulouse, France.
Association for Computational Linguistics.

Ron Kohavi. 1995. The power of decision tables. In 8th

3734

