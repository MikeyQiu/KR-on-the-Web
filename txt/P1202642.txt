7
1
0
2
 
g
u
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
3
v
8
6
6
6
0
.
3
0
6
1
:
v
i
X
r
a

Learning Representations for Automatic Colorization

Gustav Larsson1, Michael Maire2, and Gregory Shakhnarovich2

1University of Chicago

2Toyota Technological Institute at Chicago

larsson@cs.uchicago.edu, {mmaire,greg}@ttic.edu

Abstract. We develop a fully automatic image colorization system. Our
approach leverages recent advances in deep networks, exploiting both
low-level and semantic representations. As many scene elements natu-
rally appear according to multimodal color distributions, we train our
model to predict per-pixel color histograms. This intermediate output
can be used to automatically generate a color image, or further manip-
ulated prior to image formation. On both fully and partially automatic
colorization tasks, we outperform existing methods. We also explore col-
orization as a vehicle for self-supervised visual representation learning.

Fig. 1: Our automatic colorization of grayscale input; more examples in Figs. 3 and 4.

1 Introduction

Colorization of grayscale images is a simple task for the human imagination. A
human need only recall that sky is blue and grass is green; for many objects, the
mind is free to hallucinate several plausible colors. The high-level comprehension
required for this process is precisely why the development of fully automatic col-
orization algorithms remains a challenge. Colorization is thus intriguing beyond
its immediate practical utility in graphics applications. Automatic colorization
serves as a proxy measure for visual understanding. Our work makes this con-
nection explicit; we unify a colorization pipeline with the type of deep neural
architectures driving advances in image classiﬁcation and object detection.

Both our technical approach and focus on fully automatic results depart
from past work. Given colorization’s importance across multiple applications
(e.g. historical photographs and videos [41], artist assistance [32, 38]), much re-
search strives to make it cheaper and less time-consuming [3,5–7,14,20,22,27,42].
However, most methods still require some level of user input [3, 6, 14, 20, 22, 34].
Our work joins the relatively few recent eﬀorts on fully automatic coloriza-
tion [5, 7, 27]. Some [5, 7] show promising results on typical scenes (e.g. land-
scapes), but their success is limited on complex images with foreground objects.

2

Larsson, Maire, Shakhnarovich

VGG-16-Gray

Hypercolumn

Hue

Ground-truth

(fc7) conv7

(fc6) conv6
conv5 3

conv1 1

p

h fc1

Chroma

Lightness

Input: Grayscale Image

Output: Color Image

Fig. 2: System overview. We process a grayscale image through a deep convolu-
tional architecture (VGG) [37] and take spatially localized multilayer slices (hyper-
columns) [15, 26, 28], as per-pixel descriptors. We train our system end-to-end for the
task of predicting hue and chroma distributions for each pixel p given its hypercolumn
descriptor. These predicted distributions determine color assignment at test time.

At a technical level, existing automatic colorization methods often employ
a strategy of ﬁnding suitable reference images and transferring their color onto
a target grayscale image [7, 27]. This works well if suﬃciently similar reference
images can be found, but is diﬃcult for unique grayscale input images. Such a
strategy also requires processing a large repository of reference images at test
time. In contrast, our approach is free of database search and fast at test time.
Section 2 provides a complete view of prior methods, highlighting diﬀerences.

Our approach to automatic colorization converts two intuitive observations
into design principles. First, semantic information matters. In order to colorize
arbitrary images, a system must interpret the semantic composition of the scene
(what is in the image: faces, cars, plants, . . . ) as well as localize objects (where
things are). Deep convolutional neural networks (CNNs) can serve as tools to
incorporate semantic parsing and localization into a colorization system.

Our second observation is that while some scene elements can be assigned
a single color with high conﬁdence, others (e.g. clothes or cars) may draw from
many suitable colors. Thus, we design our system to predict a color histogram,
instead of a single color, at every image location. Figure 2 sketches the CNN
architecture we use to connect semantics with color distributions by exploiting
features across multiple abstraction levels. Section 3 provides details.

Section 4 experimentally validates our algorithm against competing meth-
ods [7, 42] in two settings: fully (grayscale input only) and partially (grayscale
input with reference global color histogram) automatic colorization. Across ev-
ery metric and dataset [31, 33, 43], our method achieves the best performance.
Our system’s fully automatic output is superior to that of prior methods relying
on additional information such as reference images or ground-truth color his-

Learning Representations for Automatic Colorization

3

tograms. To ease the comparison burden for future research, we propose a new
colorization benchmark on ImageNet [33]. We also experiment with colorization
itself as an objective for learning visual representations from scratch, thereby
replacing use of ImageNet pretraining in a traditional semantic labeling task.

Section 5 summarizes our contributions: (1) a novel technical approach to
colorization, bringing semantic knowledge to bear using CNNs, and modeling
color distributions; (2) state-of-the-art performance across fully and partially
automatic colorization tasks; (3) a new ImageNet colorization benchmark; (4)
proof of concept on colorization for self-supervised representation learning.

2 Related work

Previous colorization methods broadly fall into three categories: scribble-based [16,
22,25,32,45], transfer [3,6,14,20,27,39,42], and automatic direct prediction [5,7].
Scribble-based methods, introduced by Levin et al. [22], require manually
specifying desired colors of certain regions. These scribble colors are propagated
under the assumption that adjacent pixels with similar luminance should have
similar color, with the optimization relying on Normalized Cuts [36]. Users can
interactively reﬁne results via additional scribbles. Further advances extend sim-
ilarity to texture [25, 32], and exploit edges to reduce color bleeding [16].

Transfer-based methods rely on availability of related reference image(s),
from which color is transferred to the target grayscale image. Mapping between
source and target is established automatically, using correspondences between
local descriptors [3, 27, 42], or in combination with manual intervention [6, 20].
Excepting [27], reference image selection is at least partially manual.

In contrast to these method families, our goal is fully automatic colorization.
We are aware of two recent eﬀorts in this direction. Deshpande et al. [7] colorize
an entire image by solving a linear system. This can be seen as an extension
of patch-matching techniques [42], adding interaction terms for spatial consis-
tency. Regression trees address the high-dimensionality of the system. Inference
requires an iterative algorithm. Most of the experiments are focused on a dataset
(SUN-6) limited to images of a few scene classes, and best results are obtained
when the scene class is known at test time. They also examine another partially
automatic task, in which a desired global color histogram is provided.

The work of Cheng et al. [5] is perhaps most related to ours. It combines three
levels of features with increasing receptive ﬁeld: the raw image patch, DAISY
features [40], and semantic features [24]. These features are concatenated and
fed into a three-layer fully connected neural network trained with an L2 loss.
Only this last component is optimized; the feature representations are ﬁxed.

Unlike [5,7], our system does not rely on hand-crafted features, is trained end-
to-end, and treats color prediction as a histogram estimation task rather than
as regression. Experiments in Section 4 justify these principles by demonstrating
performance superior to the best reported by [5, 7] across all regimes.

Two concurrent eﬀorts also present feed-forward networks trained end-to-end
for colorization. Iizuka & Simo-Serra et al. [17] propose a network that concate-

4

Larsson, Maire, Shakhnarovich

nates two separate paths, specializing in global and local features, respectively.
This concatenation can be seen as a two-tiered hypercolumn; in comparison, our
16-layer hypercolumn creates a continuum between low- and high-level features.
Their network is trained jointly for classiﬁcation (cross-entropy) and colorization
(L2 loss in Lab). We initialize, but do not anchor, our system to a classiﬁcation-
based network, allowing for ﬁne-tuning of colorization on unlabeled datasets.

Zhang et al. [46] similarly propose predicting color histograms to handle
multi-modality. Some key diﬀerences include their usage of up-convolutional lay-
ers, deep supervision, and dense training. In comparison, we use a fully convolu-
tional approach, with deep supervision implicit in the hypercolumn design, and,
as Section 3 describes, memory-eﬃcient training via spatially sparse samples.

3 Method

We frame the colorization problem as learning a function f : X → Y. Given
a grayscale image patch x ∈ X = [0, 1]S×S, f predicts the color y ∈ Y of its
center pixel. The patch size S × S is the receptive ﬁeld of the colorizer. The
output space Y depends on the choice of color parameterization. We implement
f according to the neural network architecture diagrammed in Figure 2.

Motivating this strategy is the success of similar architectures for semantic
segmentation [4, 11, 15, 24, 28] and edge detection [1, 12, 26, 35, 44]. Together with
colorization, these tasks can all be viewed as image-to-image prediction problems,
in which a value is predicted for each input pixel. Leading methods commonly
adapt deep convolutional neural networks pretrained for image classiﬁcation [33,
37]. Such classiﬁcation networks can be converted to fully convolutional networks
that produce output of the same spatial size as the input, e.g. using the shift-
and-stitch method [24] or the more eﬃcient `a trous algorithm [4]. Subsequent
training with a task-speciﬁc loss ﬁne-tunes the converted network.

Skip-layer connections, which directly link low- and mid-level features to pre-
diction layers, are an architectural addition beneﬁcial for many image-to-image
problems. Some methods implement skip connections directly through concate-
nation layers [4, 24], while others equivalently extract per-pixel descriptors by
reading localized slices of multiple layers [15, 26, 28]. We use this latter strategy
and adopt the recently coined hypercolumn terminology [15] for such slices.

Though we build upon these ideas, our technical approach innovates on two
fronts. First, we integrate domain knowledge for colorization, experimenting with
output spaces and loss functions. We design the network output to serve as an
intermediate representation, appropriate for direct or biased sampling. We intro-
duce an energy minimization procedure for optionally biasing sampling towards a
reference image. Second, we develop a novel and eﬃcient computational strategy
for network training that is widely applicable to hypercolumn architectures.

3.1 Color spaces

We generate training data by converting color images to grayscale according to
L = R+G+B
. This is only one of many desaturation options and chosen primarily

3

Learning Representations for Automatic Colorization

5

to facilitate comparison with Deshpande et al. [7]. For the representation of
color predictions, using RGB is overdetermined, as lightness L is already known.
We instead consider output color spaces with L (or a closely related quantity)
conveniently appearing as a separate pass-through channel:

– Hue/chroma. Hue-based spaces, such as HSL, can be thought of as a color
cylinder, with angular coordinate H (hue), radial distance S (saturation),
and height L (lightness). The values of S and H are unstable at the bot-
tom (black) and top (white) of the cylinder. HSV describes a similar color
cylinder which is only unstable at the bottom. However, L is no longer one
of the channels. We wish to avoid both instabilities and still retain L as a
channel. The solution is a color bicone, where chroma (C) takes the place of
saturation. Conversion to HSV is given by V = L + C

2 , S = C
V .

– Lab and αβ. Lab (or L*a*b) is designed to be perceptually linear. The
color vector (a, b) deﬁnes a Euclidean space where the distance to the origin
determines chroma. Deshpande et al. [7] use a color space somewhat similar
to Lab, denoted “ab”. To diﬀerentiate, we call their color space αβ.

3.2 Loss

For any output color representation, we require a loss function for measuring
prediction errors. A ﬁrst consideration, also used in [5], is L2 regression in Lab:

Lreg(x, y) = (cid:107)f (x) − y(cid:107)2

(1)

(2)

where Y = R2 describes the (a, b) vector space. However, regression targets
do not handle multimodal color distributions well. To address this, we instead
predict distributions over a set of color bins, a technique also used in [3]:

Lhist(x, y) = DKL(y(cid:107)f (x))

where Y = [0, 1]K describes a histogram over K bins, and DKL is the KL-
divergence. The ground-truth histogram y is set as the empirical distribution in
a rectangular region of size R around the center pixel. Somewhat surprisingly,
our experiments see no beneﬁt to predicting smoothed histograms, so we simply
set R = 1. This makes y a one-hot vector and Equation (2) the log loss. For
histogram predictions, the last layer of neural network f is always a softmax.

There are several choices of how to bin color space. We bin the Lab axes
by evenly spaced Gaussian quantiles (µ = 0, σ = 25). They can be encoded
separately for a and b (as marginal distributions), in which case our loss becomes
the sum of two separate terms deﬁned by Equation (2). They can also be encoded
as a joint distribution over a and b, in which case we let the quantiles form a 2D
grid of bins. In our experiments, we set K = 32 for marginal distributions and
K = 16 × 16 for joint. We determined these numbers, along with σ, to oﬀer a
good compromise of output ﬁdelity and output complexity.

For hue/chroma, we only consider marginal distributions and bin axes uni-
formly in [0, 1]. Since hue becomes unstable as chroma approaches zero, we add

6

Larsson, Maire, Shakhnarovich

a sample weight to the hue based on the chroma:

Lhue/chroma(x, y) = DKL(yC(cid:107)fC(x)) + λH yCDKL(yH(cid:107)fH(x))

(3)

where Y = [0, 1]2×K and yC ∈ [0, 1] is the sample pixel’s chroma. We set λH = 5,
roughly the inverse expectation of yC, thus equally weighting hue and chroma.

3.3

Inference

Given network f trained according to a loss function in the previous section, we
evaluate it at every pixel n in a test image: ˆyn = f (xn). For the L2 loss, all that
remains is to combine each ˆyn with the respective lightness and convert to RGB.
With histogram predictions, we consider options for inferring a ﬁnal color:

– Sample Draw a sample from the histogram. If done per pixel, this may
create high-frequency color changes in areas of high-entropy histograms.
– Mode Take the arg maxk ˆyn,k as the color. This can create jarring transitions

between colors, and is prone to vote splitting for proximal centroids.

– Median Compute cumulative sum of ˆyn and use linear interpolation to ﬁnd
the value at the middle bin. Undeﬁned for circular histograms, such as hue.
– Expectation Sum over the color bin centroids weighted by the histogram.

For Lab output, we achieve the best qualitative and quantitative results using
expectations. For hue/chroma, the best results are achieved by taking the median
of the chroma. Many objects can appear both with and without chroma, which
means C = 0 is a particularly common bin. This mode draws the expectation
closer to zero, producing less saturated images. As for hue, since it is circular,
we ﬁrst compute the complex expectation:

z = EH∼fh(x)[H] (cid:44) 1
K

(cid:88)

k

[fh(x)]keiθk ,

θk = 2π

(4)

k + 0.5
K

We then set hue to the argument of z remapped to lie in [0, 1).

In cases where the estimate of the chroma is
high and z is close to zero, the instability of the
hue can create artifacts. A simple, yet eﬀective,
ﬁx is chromatic fading: downweight the chroma if
the absolute value of z is too small. We thus re-
deﬁne the predicted chroma by multiplying it by
a factor of max(η−1|z|, 1). In our experiments, we
set η = 0.03 (obtained via cross-validation).

3.4 Histogram transfer from ground-truth

So far, we have only considered the fully automatic color inference task. Desh-
pande et al. [7], test a separate task where the ground-truth histogram in the

Learning Representations for Automatic Colorization

7

two non-lightness color channels of the original color image is made available.1
In order to compare, we propose two histogram transfer methods. We refer to
the predicted image as the source and the ground-truth image as the target.

Lightness-normalized quantile matching. Divide the RGB representation
of both source and target by their respective lightness. Compute marginal his-
tograms over the resulting three color channels. Alter each source histogram to
ﬁt the corresponding target histogram by quantile matching, and multiply by
lightness. Though it does not exploit our richer color distribution predictions,
quantile matching beats the cluster correspondence method of [7] (see Table 3).

Energy minimization. We phrase histogram matching as minimizing energy:

E =

1
N

(cid:88)

n

DKL(ˆy∗

n(cid:107)ˆyn) + λDχ2((cid:104) ˆy∗(cid:105), t)

(5)

where N is the number of pixels, ˆy, ˆy∗ ∈ [0, 1]N ×K are the predicted and poste-
rior distributions, respectively. The target histogram is denoted by t ∈ [0, 1]K.
The ﬁrst term contains unary potentials that anchor the posteriors to the pre-
dictions. The second term is a symmetric χ2 distance to promote proximity
between source and target histograms. Weight λ deﬁnes relative importance of
histogram matching. We estimate the source histogram as (cid:104)ˆy∗(cid:105) = 1
n. We
N
parameterize the posterior for all pixels n as: ˆy∗
n = softmax(log ˆyn + b), where
the vector b ∈ RK can be seen as a global bias for each bin. It is also possible
to solve for the posteriors directly; this does not perform better quantitatively
and is more prone to introducing artifacts. We solve for b using gradient descent
on E and use the resulting posteriors in place of the predictions. In the case of
marginal histograms, the optimization is run twice, once for each color channel.

n ˆy∗

(cid:80)

3.5 Neural network architecture and training

Our base network is a fully convolutional version of VGG-16 [37] with two
changes: (1) the classiﬁcation layer (fc8) is discarded, and (2) the ﬁrst ﬁlter
layer (conv1 1) operates on a single intensity channel instead of mean-subtracted
RGB. We extract a hypercolumn descriptor for a pixel by concatenating the fea-
tures at its spatial location in all layers, from data to conv7 (fc7), resulting in a
12, 417 channel descriptor. We feed this hypercolumn into a fully connected layer
with 1024 channels (h fc1 in Figure 2), to which we connect output predictors.
Processing each pixel separately in such manner is quite costly. We instead
run an entire image through a single forward pass of VGG-16 and approximate
hypercolumns using bilinear interpolation. Even with such sharing, densely ex-
tracting hypercolumns requires signiﬁcant memory (1.7 GB for 256 × 256 input).
To ﬁt image batches in memory during training, we instead extract hyper-
columns at only a sparse set of locations, implementing a custom Caﬀe [21] layer

1 Note that if the histogram of the L channel were available, it would be possible to
match lightness to lightness exactly and thus greatly narrow down color placement.

8

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 3: Fully automatic colorization results on ImageNet/ctest10k. Our sys-
tem reproduces known object color properties (e.g. faces, sky, grass, fruit, wood), and
coherently picks colors for objects without such properties (e.g. clothing).

to directly compute them.2 Extracting batches of only 128 hypercolumn descrip-
tors per input image, sampled at random locations, provides suﬃcient training
signal. In the backward pass of stochastic gradient descent, an interpolated hy-
percolumn propagates its gradients to the four closest spatial cells in each layer.
Locks ensure atomicity of gradient updates, without incurring any performance
penalty. This drops training memory for hypercolumns to only 13 MB per image.
We initialize with a version of VGG-16 pretrained on ImageNet, adapting
it to grayscale by averaging over color channels in the ﬁrst layer and rescaling
appropriately. Prior to training for colorization, we further ﬁne-tune the network
for one epoch on the ImageNet classiﬁcation task with grayscale input. As the
original VGG-16 was trained without batch normalization [18], scale of responses
in internal layers can vary dramatically, presenting a problem for learning atop
their hypercolumn concatenation. Liu et al. [23] compensate for such variability
by applying layer-wise L2 normalization. We use the alternative of balancing
hypercolumns so that each layer has roughly unit second moment (E[X 2] ≈ 1);
Appendix (Section A.1) provides additional details.

2 https://github.com/gustavla/autocolorize

Learning Representations for Automatic Colorization

9

Fig. 4: Additional results. Top: Our automatic colorizations of these ImageNet ex-
amples are diﬃcult to distinguish from real color images. Bottom: B&W photographs.

4 Experiments

Starting from pretrained VGG-16-Gray, described in the previous section, we
attach h fc1 and output prediction layers with Xavier initialization [13], and
ﬁne-tune the entire system for colorization. We consider multiple prediction layer
variants: Lab output with L2 loss, and both Lab and hue/chroma marginal or
joint histogram output with losses according to Equations (2) and (3). We train
each system variant end-to-end for one epoch on the 1.2 million images of the
ImageNet training set, each resized to at most 256 pixels in smaller dimension.
A single epoch takes approximately 17 hours on a GTX Titan X GPU. At test
time, colorizing a single 512 × 512 pixel image takes 0.5 seconds.

We setup two disjoint subsets of the ImageNet validation data for our own
use: 1000 validation images (cval1k) and 10000 test images (ctest10k). Each
set has a balanced representation for ImageNet categories, and excludes any
images encoded as grayscale, but may include images that are naturally grayscale

10

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 5: SUN-6. GT Scene: test image scene class is available. GT Hist: test image color
histogram is available. We obtain colorizations with visual quality better than those
from prior work, even though we do not exploit reference images or known scene class.
Our energy minimization method (Section 3.4) for GT Hist further improves results.
In either mode, our method appears less dependent on spatial priors: note splitting of
the sky in the ﬁrst row and correlation of green with actual grass in the last row.

Model\Metric

RMSE

PSNR

Model\Metric

RMSE

PSNR

No colorization
Lab, L2
Lab, K = 32
Lab, K = 16 × 16
Hue/chroma, K = 32
+ chromatic fading

0.343
0.318
0.321
0.328
0.342
0.299

22.98
24.25
24.33
24.30
23.77
24.45

data..fc7
data..conv5 3
conv4 1..fc7
conv5 1..fc7
fc6..fc7
fc7

0.299
0.306
0.302
0.307
0.323
0.324

24.45
24.13
24.45
24.38
24.22
24.19

Table 1:
ImageNet/cval1k. Validation
performance of system variants. Hue/chroma
is best, but only with chromatic fading.

Table 2: ImageNet/cval1k. Abla-
tion study of hypercolumn compo-
nents.

(e.g. closeup of nuts and bolts), where an algorithm should know not to add color.
Category labels are discarded; only images are available at test time. We propose
ctest10k as a standard benchmark with the following metrics:

– RMSE: root mean square error in αβ averaged over all pixels [7].
– PSNR: peak signal-to-noise ratio in RGB calculated per image [5]. We use
the arithmetic mean of PSNR over images, instead of the geometric mean
as in Cheng et al. [5]; geometric mean is overly sensitive to outliers.

By virtue of comparing to ground-truth color images, quantitative colorization
metrics can penalize reasonable, but incorrect, color guesses for many objects
(e.g. red car instead of blue car) more than jarring artifacts. This makes quali-
tative results for colorization as important as quantitative; we report both.

Figures 1, 3, and 4 show example test results of our best system variant,
selected according to performance on the validation set and trained for a total

Learning Representations for Automatic Colorization

11

Method

RMSE

Method

Grayscale (no colorization) 0.285
0.353
Welsh et al. [42]
0.262
Deshpande et al. [7]
0.254
0.211

Our Method

+ GT Scene

Deshpande et al. (C) [7]
Deshpande et al. (Q)
Our Method (Q)
Our Method (E)

RMSE

0.236
0.211
0.178
0.165

Table 3: SUN-6. Comparison
with competing methods.

Table 4: SUN-6 (GT Hist). Comparison us-
ing ground-truth histograms. Results for Desh-
pande et al. [7] use GT Scene.

of 10 epochs. This variant predicts hue and chroma and uses chromatic fading
during image generation. Table 1 provides validation benchmarks for all sys-
tem variants, including the trivial baseline of no colorization. On ImageNet test
(ctest10k), our selected model obtains 0.293 (RMSE, αβ, avg/px) and 24.94 dB
(PSNR, RGB, avg/im), compared to 0.333 and 23.27 dB for the baseline.

Table 2 examines the importance of diﬀerent neural network layers to col-
orization; it reports validation performance of ablated systems that include only
the speciﬁed subsets of layers in the hypercolumn used to predict hue and
chroma. Some lower layers may be discarded without much performance loss,
yet higher layers alone (fc6..fc7) are insuﬃcient for good colorization.

Our ImageNet colorization benchmark is new to a ﬁeld lacking an estab-
lished evaluation protocol. We therefore focus on comparisons with two recent
papers [5, 7], using their self-deﬁned evaluation criteria. To do so, we run our
ImageNet-trained hue and chroma model on two additional datasets:

– SUN-A [31] is a subset of the SUN dataset [43] containing 47 object cate-
gories. Cheng et al. [5] train a colorization system on 2688 images and report
results on 1344 test images. We were unable to obtain the list of test im-
ages, and therefore report results averaged over ﬁve random subsets of 1344
SUN-A images. We do not use any SUN-A images for training.

– SUN-6, another SUN subset, used by Deshpande et al. [7], includes images
from 6 scene categories (beach, castle, outdoor, kitchen, living room, bed-
room). We compare our results on 240 test images to those reported in [7]
for their method as well as for Welsh et al. [42] with automatically matched
reference images as in [27]. Following [7], we consider another evaluation
regime in which ground-truth target color histograms are available.

Figure 5 shows a comparison of results on SUN-6. Forgoing usage of ground-truth
global histograms, our fully automatic system produces output qualitatively su-
perior to methods relying on such side information. Tables 3 and 4 report quan-
titative performance corroborating this view. The partially automatic systems
in Table 4 adapt output to ﬁt global histograms using either: (C) cluster corre-
spondences [7], (Q) quantile matching, or (E) our energy minimization described
in Section 3.4. Our quantile matching results are superior to those of [7] and our
new energy minimization procedure oﬀers further improvement.

12

Larsson, Maire, Shakhnarovich

Fig. 6: SUN-6. Cumulative histogram
of per pixel error (higher=more pix-
els with lower error). Results for Desh-
pande et al. [7] use GT Scene.

Fig. 7: SUN-A. Histogram of per-image
PSNR for [5] and our method. The highest
geometric mean PSNR reported for experi-
ments in [5] is 24.2, vs. 32.7±2.0 for us.

Figures 6 and 7 compare error distributions on SUN-6 and SUN-A. As in
Table 3, our fully automatic method dominates all competing approaches, even
those which use auxiliary information. It is only outperformed by the version
of itself augmented with ground-truth global histograms. On SUN-A, Figure 7
shows clear separation between our method and [5] on per-image PSNR.

The Appendix (Figures 14 and 15) provides anecdotal comparisons to one
additional method, that of Charpiat et al. [2], which can be considered an auto-
matic system if reference images are available. Unfortunately, source code of [2]
is not available and reported time cost is prohibitive for large-scale evaluation
(30 minutes per image). We were thus unable to benchmark [2] on large datasets.
With regard to concurrent work, Zhang et al. [46] include a comparison of our
results to their own. The two systems are competitive in terms of quantitative
measures of colorization accuracy. Their system, set to produce more vibrant
colors, has an advantage in terms of human-measured preferences. In contrast,
an oﬀ-the-shelf VGG-16 network for image classiﬁcation, consuming our system’s
color output, more often produces correct labels, suggesting a realism advantage.
We refer interested readers to [46] for the full details of this comparison.

Though we achieve signiﬁcant improvements over prior state-of-the-art, our
results are not perfect. Figure 8 shows examples of signiﬁcant failures. Minor im-
perfections are also present in some of the results in Figures 3 and 4. We believe
a common failure mode correlates with gaps in semantic interpretation: incor-
rectly identiﬁed or unfamiliar objects and incorrect segmentation. In addition,
there are “mistakes” due to natural uncertainty of color – e.g. the graduation
robe at the bottom right of Figure 3 is red, but could as well be purple.

Since our method produces histograms, we can provide interactive means of
biasing colorizations according to user preferences. Rather than output a single
color per pixel, we can sample color for image regions and evaluate color uncer-
tainty. Speciﬁcally, solving our energy minimization formulation (Equation (5))
with global biases b that are not optimized based on a reference image, but sim-
ply “rotated” through color space, induces changed color preferences throughout
the image. The uncertainty in the predicted histogram modulates this eﬀect.

Learning Representations for Automatic Colorization

13

Fig. 8: Failure modes. Top row, left-to-right: texture confusion, too homogeneous,
color bleeding, unnatural color shifts (×2). Bottom row: inconsistent background, in-
consistent chromaticity, not enough color, object not recognized (upside down face
partly gray), context confusion (sky).

Fig. 9: Sampling colorizations. Left: Image & 3 samples; Right: Uncertainty map.

Figure 9 shows multiple sampled colorizations, together with a visualization
of uncertainty. Here, uncertainty is the entropy of the predicted hue multiplied
by the chroma. Our distributional output and energy minimization framework
open the path for future investigation of human-in-the-loop colorization tools.

4.1 Representation learning

High-level visual understanding is essential for the colorization of grayscale im-
ages, motivating our use of an ImageNet pretrained network as a starting point.
But with enough training data, perhaps we can turn this around and use col-
orization as means of learning networks for capturing high-level visual represen-
tations. Table 5 shows that a colorization network, trained from scratch using
only unlabeled color images, is surprisingly competitive. It converges slower, but
requires not more than twice the number of epochs.

Our preliminary work shows that the networks learned via training coloriza-
tion from scratch generalize well to other visual tasks. This is signiﬁcant because
such training requires no human annotation eﬀort. It follows a recent trend
of learning representations through self-supervision (e.g. context prediction [8],
solving jigsaw puzzles [29], inpainting [30], adversarial feature learning [9, 10]).
We examine self-supervised colorization as a replacement for supervised Im-
ageNet pretraining on the Pascal VOC 2012 semantic segmentation task, with
results on grayscale validation set images. We train colorization from scratch on
ImageNet (Table 5) and ﬁne-tune for Pascal semantic segmentation. We make

14

Larsson, Maire, Shakhnarovich

Initialization RMSE PSNR

Initialization

Architecture X Y C mIU (%)

Classiﬁer
Random

0.299
0.311

24.45
24.25

of

Table 5: ImageNet/cval1k.
ini-
Compares methods
tialization before colorization
training. Hue/chroma with
chromatic fading is used in
both cases (see in Tab. 1).

Classiﬁer
Colorizer
Random

VGG-16
VGG-16
VGG-16

Classiﬁer [9, 30] AlexNet
AlexNet
BiGAN [9]
AlexNet
Inpainter [30]
AlexNet
Random [30]

(cid:51) (cid:51)
(cid:51)

(cid:51) (cid:51) (cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

64.0
50.2
32.5

48.0
34.9
29.7
19.8

Table 6: VOC 2012 segmentation validation
set. Pretraining uses ImageNet images (X), labels
(Y ). VOC 2012 images are in color (C).

the one adjustment of employing cross-validated early stopping to avoid overﬁt-
ting. Table 6 shows this strategy to be promising as a drop-in replacement for
supervised ImageNet pretraining. Self-supervised colorization more than halfway
bridges the gap between random initialization and supervised pretraining.

As VGG-16 is a more performant architecture, comparison with prior work
is not straightforward. Yet, Table 6 still indicates that colorization is a front-
runner among the self-supervision methods, leading to an 18-point improvement
in mIU over the baseline. To our knowledge, 50.2% is the highest reported result
that does not supplement training with additional annotated data [19].

5 Conclusion

We present a system that demonstrates state-of-the-art ability to automatically
colorize grayscale images. Two novel contributions enable this progress: a deep
neural architecture that is trained end-to-end to incorporate semantically mean-
ingful features of varying complexity into colorization, and a color histogram pre-
diction framework that handles uncertainty and ambiguities inherent in coloriza-
tion while preventing jarring artifacts. Our fully automatic colorizer produces
strong results, improving upon previously leading methods by large margins on
all datasets tested; we also propose a new large-scale benchmark for automatic
image colorization, and establish a strong baseline with our method to facilitate
future comparisons. Our colorization results are visually appealing even on com-
plex scenes, and allow for eﬀective post-processing with creative control via color
histogram transfer and intelligent, uncertainty-driven color sampling. We further
reveal colorization as a promising avenue for self-supervised visual learning.

Acknowledgements. We thank Ayan Chakrabarti for suggesting lightness-
normalized quantile matching and for useful discussions, and Aditya Deshpande
and Jason Rock for discussions on their work. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of GPUs for this research.

Learning Representations for Automatic Colorization

15

References

1. Bertasius, G., Shi, J., Torresani, L.: Deepedge: A multi-scale bifurcated deep net-

work for top-down contour detection. In: CVPR (2015)

2. Charpiat, G., Bezrukov, I., Altun, Y., Hofmann, M., Sch¨olkopf, B.: Machine learn-
ing methods for automatic image colorization. In: Computational Photography:
Methods and Applications. CRC Press (2010)

3. Charpiat, G., Hofmann, M., Sch¨olkopf, B.: Automatic image colorization via mul-

timodal predictions. In: ECCV (2008)

4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. In:
ICLR (2015)

5. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: ICCV (2015)
6. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Se-
mantic colorization with internet images. ACM Transactions on Graphics (TOG)
30(6) (2011)

7. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-

orization. In: ICCV (2015)

8. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning
by context prediction. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 1422–1430 (2015)

9. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR

(2017)

10. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,
Courville, A.: Adversarially learned inference. arXiv preprint arXiv:1606.00704
(2016)

11. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013)

12. Ganin, Y., Lempitsky, V.S.: N4-ﬁelds: Neural network nearest neighbor ﬁelds for

13. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward

image transforms. In: ACCV (2014)

neural networks. In: AISTATS (2010)

14. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization
using similar images. In: ACM international conference on Multimedia (2012)
15. Hariharan, B., an R. Girshick, P.A., Malik, J.: Hypercolumns for object segmen-

tation and ﬁne-grained localization. CVPR (2015)

16. Huang, Y.C., Tung, Y.S., Chen, J.C., Wang, S.W., Wu, J.L.: An adaptive edge
detection based colorization algorithm and its applications. In: ACM international
conference on Multimedia (2005)

17. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Let there be Color!: Joint End-to-end
Learning of Global and Local Image Priors for Automatic Image Colorization with
Simultaneous Classiﬁcation. ACM Transactions on Graphics (Proc. of SIGGRAPH
2016) 35(4) (2016)

18. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

19. Ion, A., Carreira, J., Sminchisescu, C.: Probabilistic joint image segmentation and
labeling by ﬁgure-ground composition. International journal of computer vision
107(1), 40–57 (2014)

16

Larsson, Maire, Shakhnarovich

20. Irony, R., Cohen-Or, D., Lischinski, D.: Colorization by example. In: Eurographics

Symp. on Rendering (2005)

21. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

22. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. ACM Trans-

actions on Graphics (TOG) 23(3) (2004)

23. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better. arXiv

24. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

preprint arXiv:1506.04579 (2015)

segmentation. In: CVPR (2015)

25. Luan, Q., Wen, F., Cohen-Or, D., Liang, L., Xu, Y.Q., Shum, H.Y.: Natural image

colorization. In: Eurographics conference on Rendering Techniques (2007)

26. Maire, M., Yu, S.X., Perona, P.: Reconstructive sparse code transfer for contour

detection and semantic labeling. In: ACCV (2014)

27. Morimoto, Y., Taguchi, Y., Naemura, T.: Automatic colorization of grayscale im-

ages using multiple images on the web. In: SIGGRAPH: Posters (2009)

28. Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg-

mentation with zoom-out features. In: CVPR (2015)

29. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving

jigsaw puzzles. In: ECCV (2016)

30. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR (2016)

31. Patterson, G., Xu, C., Su, H., Hays, J.: The sun attribute database: Beyond cat-
egories for deeper scene understanding. International Journal of Computer Vision
108(1-2) (2014)

32. Qu, Y., Wong, T.T., Heng, P.A.: Manga colorization. ACM Transactions on Graph-

ics (TOG) 25(3) (2006)

33. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3) (2015)

34. Sapiro, G.: Inpainting the colors. In: ICIP (2005)
35. Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z.: Deepcontour: A deep convo-
lutional feature learned by positive-sharing loss for contour detection. In: CVPR
(2015)

36. Shi, J., Malik, J.: Normalized cuts and image segmentation. Pattern Analysis and

Machine Intelligence, IEEE Transactions on 22(8) (2000)

37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015)

38. S`ykora, D., Buri´anek, J., ˇZ´ara, J.: Unsupervised colorization of black-and-white
cartoons. In: International symposium on Non-photorealistic animation and ren-
dering (2004)

39. Tai, Y.W., Jia, J., Tang, C.K.: Local color transfer via probabilistic segmentation

by expectation-maximization. In: CVPR (2005)

40. Tola, E., Lepetit, V., Fua, P.: A fast local descriptor for dense matching. In: CVPR

(2008)

41. Tsaftaris, S.A., Casadio, F., Andral, J.L., Katsaggelos, A.K.: A novel visualization
tool for art history and conservation: Automated colorization of black and white
archival photographs of works of art. Studies in Conservation 59(3) (2014)

Learning Representations for Automatic Colorization

17

42. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.

ACM Transactions on Graphics (TOG) 21(3) (2002)

43. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale

scene recognition from abbey to zoo. In: CVPR (2010)

44. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)
45. Yatziv, L., Sapiro, G.: Fast image and video colorization using chrominance blend-

ing. Image Processing, IEEE Transactions on 15(5) (2006)

46. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV (2016)

18

Larsson, Maire, Shakhnarovich

Hue

Chroma

Hue

Chroma

Output: Color Image

Ground-truth

Hue

Chroma

Fig. 10: Histogram predictions. Example of predicted hue/chroma his-
tograms.

Appendix A provides additional training and evaluation details. This is followed
by more results and examples in Appendix B.

A Supplementary details

A.1 Re-balancing

To adjust the scale of the activations of layer l by factor m, without changing
any other layer’s activation, the weights W and the bias b are updated according
to:

Wl ← mWl

bl ← mbl

Wl+1 ←

Wl+1

(6)

1
m

The activation of xl+1 becomes:

xl+1 =

Wl+1ReLU(mWlxl + mbl) + bl+1

(7)

1
m

The m inside the ReLU will not aﬀect whether or not a value is rectiﬁed, so the
two cases remain the same: (1) negative: the activation will be the corresponding
feature in bl+1 regardless of m, and (2) positive: the ReLU becomes the identity
function and m and 1

m cancel to get back the original activation.

We set m =

1√

, estimated for each layer separately.

ˆE[X 2]

A.2 Color space αβ

The color channels αβ (“ab” in [7]) are calculated as

α =

B − 1

2 (R + G)
L + (cid:15)

β =

R − G
L + (cid:15)

(8)

where (cid:15) = 0.0001, R, G, B ∈ [0, 1] and L = R+G+B

.3

3

3 We know that this is how Deshpande et al. [7] calculate it based on their code release.

Learning Representations for Automatic Colorization

19

Output: Color Image

Hue

Chroma

Hue

Chroma

Ground-truth

Hue

Chroma

Fig. 11: Histogram predictions. Example of predicted hue/chroma his-
tograms.

A.3 Error metrics

For M images, each image m with Nm pixels, we calculate the error metrics as:

RMSE =

(cid:80)M

1
m=1 Nm
Nm(cid:88)
M
(cid:88)

1
M

m=1

n=1

M
(cid:88)

Nm(cid:88)

m=1

n=1

(cid:114)(cid:13)
(cid:104)
y(m)
(cid:13)
(cid:13)
αβ

(cid:105)

n

(cid:105)

(cid:104)
ˆy(m)
αβ

−

n

(cid:13)
2
(cid:13)
(cid:13)

(cid:33)

(cid:32)

(cid:107)y(m)

RGB − ˆy(m)
3Nm

RGB(cid:107)2

PSNR =

−10 · log10

(9)

(10)

Where y(m)

αβ ∈ [−3, 3]Nm×2 and y(m)

RGB ∈ [0, 1]Nm×3 for all m.

A.4 Lightness correction

Ideally the lightness L is an unaltered pass-through channel. However, due to
subtle diﬀerences in how L is deﬁned, it is possible that the lightness of the
predicted image, ˆL, does not agree with the input, L. To compensate for this,
we add L− ˆL to all color channels in the predicted RGB image as a ﬁnal corrective
step.

B Supplementary results

B.1 Validation

A more detailed list of validation results for hue/chroma inference methods is
seen in Table 7.

20

Larsson, Maire, Shakhnarovich

Hue

Chroma

CF RMSE PSNR

Sample
Mode

0.426
Sample
0.304
Mode
Expectation Expectation
0.374
Expectation Expectation (cid:51) 0.307
0.342
Expectation Median
Expectation Median

21.41
23.90
23.13
24.35
23.77
(cid:51) 0.299 24.45

7:

Table
ImageNet/cval1k.
Comparison of various histogram
inference methods for hue/chroma.
Mode/mode does fairly well but
has severe visual artifacts. (CF =
Chromatic fading)

B.2 Examples

We provide additional samples for global biasing (Figure 12) and SUN-6 (Fig-
ure 13). Comparisons with Charpiat et al. [2] appear in Figures 14 and 15.
Examples of how our algorithm can bring old photographs to life in Figure 16.
More examples on ImageNet (ctest10k) in Figures 17 to 20 and Figure 21 (failure
cases). Examples of histogram predictions in Figures 10 and 11.

C Document changelog

Overview of document revisions:

v1 Initial release.

v2 ECCV 2016 camera-ready version. Includes discussion about concurrent work
and new experiments using colorization to learn visual representations (Sec-
tion 4.1).

v3 Added overlooked reference.

Learning Representations for Automatic Colorization

21

Fig. 12: Sampling multiple colorizations. From left: graylevel input; three coloriza-
tions sampled from our model; color uncertainty map according to our model.

22

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 13: SUN-6. Additional qualitative comparisons.

Reference Image

Input

Charpiat et al. [2]

Our Method
(Energy Minimization)

Fig. 14: Transfer. Comparison with Charpiat et al. [2] with reference image.
Their method works fairly well when the reference image closely matches (com-
pare with Figure 15). However, they still present sharp unnatural color edges. We
apply our histogram transfer method (Energy Minimization) using the reference
image.

Learning Representations for Automatic Colorization

23

Input

Charpiat et al. [2]

Our Method

Ground-truth

Fig. 15: Portraits. Comparison with Charpiat et al. [2], a transfer-based method
using 53 reference portrait paintings. Note that their method works signiﬁcantly
worse when the reference images are not hand-picked for each grayscale input
(compare with Figure 14). Our model was not trained speciﬁcally for this task
and we used no reference images.

24

Larsson, Maire, Shakhnarovich

Input

Our Method

Input

Our Method

Fig. 16: B&W photographs. Old photographs that were automatically col-
orized. (Source: Library of Congress, www.loc.gov)

Learning Representations for Automatic Colorization

25

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 17: Fully automatic colorization results on ImageNet/ctest10k.

26

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 18: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

27

Fig. 19: Fully automatic colorization results on ImageNet/ctest10k.

28

Larsson, Maire, Shakhnarovich

Fig. 20: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

29

Too Desaturated

Inconsistent Chroma

Inconsistent Hue

Edge Pollution

Color Bleeding

Fig. 21: Failure cases. Examples of the ﬁve most common failure cases: the whole
image lacks saturation (Too Desaturated); inconsistent chroma in objects or regions,
causing parts to be gray (Inconsistent Chroma); inconsistent hue, causing unnatural
color shifts that are particularly typical between red and blue (Inconsistent Hue);
inconsistent hue and chroma around the edge, commonly occurring for closeups where
background context is unclear (Edge Pollution); color boundary is not clearly separated,
causing color bleeding (Color Bleeding).

7
1
0
2
 
g
u
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
3
v
8
6
6
6
0
.
3
0
6
1
:
v
i
X
r
a

Learning Representations for Automatic Colorization

Gustav Larsson1, Michael Maire2, and Gregory Shakhnarovich2

1University of Chicago

2Toyota Technological Institute at Chicago

larsson@cs.uchicago.edu, {mmaire,greg}@ttic.edu

Abstract. We develop a fully automatic image colorization system. Our
approach leverages recent advances in deep networks, exploiting both
low-level and semantic representations. As many scene elements natu-
rally appear according to multimodal color distributions, we train our
model to predict per-pixel color histograms. This intermediate output
can be used to automatically generate a color image, or further manip-
ulated prior to image formation. On both fully and partially automatic
colorization tasks, we outperform existing methods. We also explore col-
orization as a vehicle for self-supervised visual representation learning.

Fig. 1: Our automatic colorization of grayscale input; more examples in Figs. 3 and 4.

1 Introduction

Colorization of grayscale images is a simple task for the human imagination. A
human need only recall that sky is blue and grass is green; for many objects, the
mind is free to hallucinate several plausible colors. The high-level comprehension
required for this process is precisely why the development of fully automatic col-
orization algorithms remains a challenge. Colorization is thus intriguing beyond
its immediate practical utility in graphics applications. Automatic colorization
serves as a proxy measure for visual understanding. Our work makes this con-
nection explicit; we unify a colorization pipeline with the type of deep neural
architectures driving advances in image classiﬁcation and object detection.

Both our technical approach and focus on fully automatic results depart
from past work. Given colorization’s importance across multiple applications
(e.g. historical photographs and videos [41], artist assistance [32, 38]), much re-
search strives to make it cheaper and less time-consuming [3,5–7,14,20,22,27,42].
However, most methods still require some level of user input [3, 6, 14, 20, 22, 34].
Our work joins the relatively few recent eﬀorts on fully automatic coloriza-
tion [5, 7, 27]. Some [5, 7] show promising results on typical scenes (e.g. land-
scapes), but their success is limited on complex images with foreground objects.

2

Larsson, Maire, Shakhnarovich

VGG-16-Gray

Hypercolumn

Hue

Ground-truth

(fc7) conv7

(fc6) conv6
conv5 3

conv1 1

p

h fc1

Chroma

Lightness

Input: Grayscale Image

Output: Color Image

Fig. 2: System overview. We process a grayscale image through a deep convolu-
tional architecture (VGG) [37] and take spatially localized multilayer slices (hyper-
columns) [15, 26, 28], as per-pixel descriptors. We train our system end-to-end for the
task of predicting hue and chroma distributions for each pixel p given its hypercolumn
descriptor. These predicted distributions determine color assignment at test time.

At a technical level, existing automatic colorization methods often employ
a strategy of ﬁnding suitable reference images and transferring their color onto
a target grayscale image [7, 27]. This works well if suﬃciently similar reference
images can be found, but is diﬃcult for unique grayscale input images. Such a
strategy also requires processing a large repository of reference images at test
time. In contrast, our approach is free of database search and fast at test time.
Section 2 provides a complete view of prior methods, highlighting diﬀerences.

Our approach to automatic colorization converts two intuitive observations
into design principles. First, semantic information matters. In order to colorize
arbitrary images, a system must interpret the semantic composition of the scene
(what is in the image: faces, cars, plants, . . . ) as well as localize objects (where
things are). Deep convolutional neural networks (CNNs) can serve as tools to
incorporate semantic parsing and localization into a colorization system.

Our second observation is that while some scene elements can be assigned
a single color with high conﬁdence, others (e.g. clothes or cars) may draw from
many suitable colors. Thus, we design our system to predict a color histogram,
instead of a single color, at every image location. Figure 2 sketches the CNN
architecture we use to connect semantics with color distributions by exploiting
features across multiple abstraction levels. Section 3 provides details.

Section 4 experimentally validates our algorithm against competing meth-
ods [7, 42] in two settings: fully (grayscale input only) and partially (grayscale
input with reference global color histogram) automatic colorization. Across ev-
ery metric and dataset [31, 33, 43], our method achieves the best performance.
Our system’s fully automatic output is superior to that of prior methods relying
on additional information such as reference images or ground-truth color his-

Learning Representations for Automatic Colorization

3

tograms. To ease the comparison burden for future research, we propose a new
colorization benchmark on ImageNet [33]. We also experiment with colorization
itself as an objective for learning visual representations from scratch, thereby
replacing use of ImageNet pretraining in a traditional semantic labeling task.

Section 5 summarizes our contributions: (1) a novel technical approach to
colorization, bringing semantic knowledge to bear using CNNs, and modeling
color distributions; (2) state-of-the-art performance across fully and partially
automatic colorization tasks; (3) a new ImageNet colorization benchmark; (4)
proof of concept on colorization for self-supervised representation learning.

2 Related work

Previous colorization methods broadly fall into three categories: scribble-based [16,
22,25,32,45], transfer [3,6,14,20,27,39,42], and automatic direct prediction [5,7].
Scribble-based methods, introduced by Levin et al. [22], require manually
specifying desired colors of certain regions. These scribble colors are propagated
under the assumption that adjacent pixels with similar luminance should have
similar color, with the optimization relying on Normalized Cuts [36]. Users can
interactively reﬁne results via additional scribbles. Further advances extend sim-
ilarity to texture [25, 32], and exploit edges to reduce color bleeding [16].

Transfer-based methods rely on availability of related reference image(s),
from which color is transferred to the target grayscale image. Mapping between
source and target is established automatically, using correspondences between
local descriptors [3, 27, 42], or in combination with manual intervention [6, 20].
Excepting [27], reference image selection is at least partially manual.

In contrast to these method families, our goal is fully automatic colorization.
We are aware of two recent eﬀorts in this direction. Deshpande et al. [7] colorize
an entire image by solving a linear system. This can be seen as an extension
of patch-matching techniques [42], adding interaction terms for spatial consis-
tency. Regression trees address the high-dimensionality of the system. Inference
requires an iterative algorithm. Most of the experiments are focused on a dataset
(SUN-6) limited to images of a few scene classes, and best results are obtained
when the scene class is known at test time. They also examine another partially
automatic task, in which a desired global color histogram is provided.

The work of Cheng et al. [5] is perhaps most related to ours. It combines three
levels of features with increasing receptive ﬁeld: the raw image patch, DAISY
features [40], and semantic features [24]. These features are concatenated and
fed into a three-layer fully connected neural network trained with an L2 loss.
Only this last component is optimized; the feature representations are ﬁxed.

Unlike [5,7], our system does not rely on hand-crafted features, is trained end-
to-end, and treats color prediction as a histogram estimation task rather than
as regression. Experiments in Section 4 justify these principles by demonstrating
performance superior to the best reported by [5, 7] across all regimes.

Two concurrent eﬀorts also present feed-forward networks trained end-to-end
for colorization. Iizuka & Simo-Serra et al. [17] propose a network that concate-

4

Larsson, Maire, Shakhnarovich

nates two separate paths, specializing in global and local features, respectively.
This concatenation can be seen as a two-tiered hypercolumn; in comparison, our
16-layer hypercolumn creates a continuum between low- and high-level features.
Their network is trained jointly for classiﬁcation (cross-entropy) and colorization
(L2 loss in Lab). We initialize, but do not anchor, our system to a classiﬁcation-
based network, allowing for ﬁne-tuning of colorization on unlabeled datasets.

Zhang et al. [46] similarly propose predicting color histograms to handle
multi-modality. Some key diﬀerences include their usage of up-convolutional lay-
ers, deep supervision, and dense training. In comparison, we use a fully convolu-
tional approach, with deep supervision implicit in the hypercolumn design, and,
as Section 3 describes, memory-eﬃcient training via spatially sparse samples.

3 Method

We frame the colorization problem as learning a function f : X → Y. Given
a grayscale image patch x ∈ X = [0, 1]S×S, f predicts the color y ∈ Y of its
center pixel. The patch size S × S is the receptive ﬁeld of the colorizer. The
output space Y depends on the choice of color parameterization. We implement
f according to the neural network architecture diagrammed in Figure 2.

Motivating this strategy is the success of similar architectures for semantic
segmentation [4, 11, 15, 24, 28] and edge detection [1, 12, 26, 35, 44]. Together with
colorization, these tasks can all be viewed as image-to-image prediction problems,
in which a value is predicted for each input pixel. Leading methods commonly
adapt deep convolutional neural networks pretrained for image classiﬁcation [33,
37]. Such classiﬁcation networks can be converted to fully convolutional networks
that produce output of the same spatial size as the input, e.g. using the shift-
and-stitch method [24] or the more eﬃcient `a trous algorithm [4]. Subsequent
training with a task-speciﬁc loss ﬁne-tunes the converted network.

Skip-layer connections, which directly link low- and mid-level features to pre-
diction layers, are an architectural addition beneﬁcial for many image-to-image
problems. Some methods implement skip connections directly through concate-
nation layers [4, 24], while others equivalently extract per-pixel descriptors by
reading localized slices of multiple layers [15, 26, 28]. We use this latter strategy
and adopt the recently coined hypercolumn terminology [15] for such slices.

Though we build upon these ideas, our technical approach innovates on two
fronts. First, we integrate domain knowledge for colorization, experimenting with
output spaces and loss functions. We design the network output to serve as an
intermediate representation, appropriate for direct or biased sampling. We intro-
duce an energy minimization procedure for optionally biasing sampling towards a
reference image. Second, we develop a novel and eﬃcient computational strategy
for network training that is widely applicable to hypercolumn architectures.

3.1 Color spaces

We generate training data by converting color images to grayscale according to
L = R+G+B
. This is only one of many desaturation options and chosen primarily

3

Learning Representations for Automatic Colorization

5

to facilitate comparison with Deshpande et al. [7]. For the representation of
color predictions, using RGB is overdetermined, as lightness L is already known.
We instead consider output color spaces with L (or a closely related quantity)
conveniently appearing as a separate pass-through channel:

– Hue/chroma. Hue-based spaces, such as HSL, can be thought of as a color
cylinder, with angular coordinate H (hue), radial distance S (saturation),
and height L (lightness). The values of S and H are unstable at the bot-
tom (black) and top (white) of the cylinder. HSV describes a similar color
cylinder which is only unstable at the bottom. However, L is no longer one
of the channels. We wish to avoid both instabilities and still retain L as a
channel. The solution is a color bicone, where chroma (C) takes the place of
saturation. Conversion to HSV is given by V = L + C

2 , S = C
V .

– Lab and αβ. Lab (or L*a*b) is designed to be perceptually linear. The
color vector (a, b) deﬁnes a Euclidean space where the distance to the origin
determines chroma. Deshpande et al. [7] use a color space somewhat similar
to Lab, denoted “ab”. To diﬀerentiate, we call their color space αβ.

3.2 Loss

For any output color representation, we require a loss function for measuring
prediction errors. A ﬁrst consideration, also used in [5], is L2 regression in Lab:

Lreg(x, y) = (cid:107)f (x) − y(cid:107)2

(1)

(2)

where Y = R2 describes the (a, b) vector space. However, regression targets
do not handle multimodal color distributions well. To address this, we instead
predict distributions over a set of color bins, a technique also used in [3]:

Lhist(x, y) = DKL(y(cid:107)f (x))

where Y = [0, 1]K describes a histogram over K bins, and DKL is the KL-
divergence. The ground-truth histogram y is set as the empirical distribution in
a rectangular region of size R around the center pixel. Somewhat surprisingly,
our experiments see no beneﬁt to predicting smoothed histograms, so we simply
set R = 1. This makes y a one-hot vector and Equation (2) the log loss. For
histogram predictions, the last layer of neural network f is always a softmax.

There are several choices of how to bin color space. We bin the Lab axes
by evenly spaced Gaussian quantiles (µ = 0, σ = 25). They can be encoded
separately for a and b (as marginal distributions), in which case our loss becomes
the sum of two separate terms deﬁned by Equation (2). They can also be encoded
as a joint distribution over a and b, in which case we let the quantiles form a 2D
grid of bins. In our experiments, we set K = 32 for marginal distributions and
K = 16 × 16 for joint. We determined these numbers, along with σ, to oﬀer a
good compromise of output ﬁdelity and output complexity.

For hue/chroma, we only consider marginal distributions and bin axes uni-
formly in [0, 1]. Since hue becomes unstable as chroma approaches zero, we add

6

Larsson, Maire, Shakhnarovich

a sample weight to the hue based on the chroma:

Lhue/chroma(x, y) = DKL(yC(cid:107)fC(x)) + λH yCDKL(yH(cid:107)fH(x))

(3)

where Y = [0, 1]2×K and yC ∈ [0, 1] is the sample pixel’s chroma. We set λH = 5,
roughly the inverse expectation of yC, thus equally weighting hue and chroma.

3.3

Inference

Given network f trained according to a loss function in the previous section, we
evaluate it at every pixel n in a test image: ˆyn = f (xn). For the L2 loss, all that
remains is to combine each ˆyn with the respective lightness and convert to RGB.
With histogram predictions, we consider options for inferring a ﬁnal color:

– Sample Draw a sample from the histogram. If done per pixel, this may
create high-frequency color changes in areas of high-entropy histograms.
– Mode Take the arg maxk ˆyn,k as the color. This can create jarring transitions

between colors, and is prone to vote splitting for proximal centroids.

– Median Compute cumulative sum of ˆyn and use linear interpolation to ﬁnd
the value at the middle bin. Undeﬁned for circular histograms, such as hue.
– Expectation Sum over the color bin centroids weighted by the histogram.

For Lab output, we achieve the best qualitative and quantitative results using
expectations. For hue/chroma, the best results are achieved by taking the median
of the chroma. Many objects can appear both with and without chroma, which
means C = 0 is a particularly common bin. This mode draws the expectation
closer to zero, producing less saturated images. As for hue, since it is circular,
we ﬁrst compute the complex expectation:

z = EH∼fh(x)[H] (cid:44) 1
K

(cid:88)

k

[fh(x)]keiθk ,

θk = 2π

(4)

k + 0.5
K

We then set hue to the argument of z remapped to lie in [0, 1).

In cases where the estimate of the chroma is
high and z is close to zero, the instability of the
hue can create artifacts. A simple, yet eﬀective,
ﬁx is chromatic fading: downweight the chroma if
the absolute value of z is too small. We thus re-
deﬁne the predicted chroma by multiplying it by
a factor of max(η−1|z|, 1). In our experiments, we
set η = 0.03 (obtained via cross-validation).

3.4 Histogram transfer from ground-truth

So far, we have only considered the fully automatic color inference task. Desh-
pande et al. [7], test a separate task where the ground-truth histogram in the

Learning Representations for Automatic Colorization

7

two non-lightness color channels of the original color image is made available.1
In order to compare, we propose two histogram transfer methods. We refer to
the predicted image as the source and the ground-truth image as the target.

Lightness-normalized quantile matching. Divide the RGB representation
of both source and target by their respective lightness. Compute marginal his-
tograms over the resulting three color channels. Alter each source histogram to
ﬁt the corresponding target histogram by quantile matching, and multiply by
lightness. Though it does not exploit our richer color distribution predictions,
quantile matching beats the cluster correspondence method of [7] (see Table 3).

Energy minimization. We phrase histogram matching as minimizing energy:

E =

1
N

(cid:88)

n

DKL(ˆy∗

n(cid:107)ˆyn) + λDχ2((cid:104) ˆy∗(cid:105), t)

(5)

where N is the number of pixels, ˆy, ˆy∗ ∈ [0, 1]N ×K are the predicted and poste-
rior distributions, respectively. The target histogram is denoted by t ∈ [0, 1]K.
The ﬁrst term contains unary potentials that anchor the posteriors to the pre-
dictions. The second term is a symmetric χ2 distance to promote proximity
between source and target histograms. Weight λ deﬁnes relative importance of
histogram matching. We estimate the source histogram as (cid:104)ˆy∗(cid:105) = 1
n. We
N
parameterize the posterior for all pixels n as: ˆy∗
n = softmax(log ˆyn + b), where
the vector b ∈ RK can be seen as a global bias for each bin. It is also possible
to solve for the posteriors directly; this does not perform better quantitatively
and is more prone to introducing artifacts. We solve for b using gradient descent
on E and use the resulting posteriors in place of the predictions. In the case of
marginal histograms, the optimization is run twice, once for each color channel.

n ˆy∗

(cid:80)

3.5 Neural network architecture and training

Our base network is a fully convolutional version of VGG-16 [37] with two
changes: (1) the classiﬁcation layer (fc8) is discarded, and (2) the ﬁrst ﬁlter
layer (conv1 1) operates on a single intensity channel instead of mean-subtracted
RGB. We extract a hypercolumn descriptor for a pixel by concatenating the fea-
tures at its spatial location in all layers, from data to conv7 (fc7), resulting in a
12, 417 channel descriptor. We feed this hypercolumn into a fully connected layer
with 1024 channels (h fc1 in Figure 2), to which we connect output predictors.
Processing each pixel separately in such manner is quite costly. We instead
run an entire image through a single forward pass of VGG-16 and approximate
hypercolumns using bilinear interpolation. Even with such sharing, densely ex-
tracting hypercolumns requires signiﬁcant memory (1.7 GB for 256 × 256 input).
To ﬁt image batches in memory during training, we instead extract hyper-
columns at only a sparse set of locations, implementing a custom Caﬀe [21] layer

1 Note that if the histogram of the L channel were available, it would be possible to
match lightness to lightness exactly and thus greatly narrow down color placement.

8

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 3: Fully automatic colorization results on ImageNet/ctest10k. Our sys-
tem reproduces known object color properties (e.g. faces, sky, grass, fruit, wood), and
coherently picks colors for objects without such properties (e.g. clothing).

to directly compute them.2 Extracting batches of only 128 hypercolumn descrip-
tors per input image, sampled at random locations, provides suﬃcient training
signal. In the backward pass of stochastic gradient descent, an interpolated hy-
percolumn propagates its gradients to the four closest spatial cells in each layer.
Locks ensure atomicity of gradient updates, without incurring any performance
penalty. This drops training memory for hypercolumns to only 13 MB per image.
We initialize with a version of VGG-16 pretrained on ImageNet, adapting
it to grayscale by averaging over color channels in the ﬁrst layer and rescaling
appropriately. Prior to training for colorization, we further ﬁne-tune the network
for one epoch on the ImageNet classiﬁcation task with grayscale input. As the
original VGG-16 was trained without batch normalization [18], scale of responses
in internal layers can vary dramatically, presenting a problem for learning atop
their hypercolumn concatenation. Liu et al. [23] compensate for such variability
by applying layer-wise L2 normalization. We use the alternative of balancing
hypercolumns so that each layer has roughly unit second moment (E[X 2] ≈ 1);
Appendix (Section A.1) provides additional details.

2 https://github.com/gustavla/autocolorize

Learning Representations for Automatic Colorization

9

Fig. 4: Additional results. Top: Our automatic colorizations of these ImageNet ex-
amples are diﬃcult to distinguish from real color images. Bottom: B&W photographs.

4 Experiments

Starting from pretrained VGG-16-Gray, described in the previous section, we
attach h fc1 and output prediction layers with Xavier initialization [13], and
ﬁne-tune the entire system for colorization. We consider multiple prediction layer
variants: Lab output with L2 loss, and both Lab and hue/chroma marginal or
joint histogram output with losses according to Equations (2) and (3). We train
each system variant end-to-end for one epoch on the 1.2 million images of the
ImageNet training set, each resized to at most 256 pixels in smaller dimension.
A single epoch takes approximately 17 hours on a GTX Titan X GPU. At test
time, colorizing a single 512 × 512 pixel image takes 0.5 seconds.

We setup two disjoint subsets of the ImageNet validation data for our own
use: 1000 validation images (cval1k) and 10000 test images (ctest10k). Each
set has a balanced representation for ImageNet categories, and excludes any
images encoded as grayscale, but may include images that are naturally grayscale

10

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 5: SUN-6. GT Scene: test image scene class is available. GT Hist: test image color
histogram is available. We obtain colorizations with visual quality better than those
from prior work, even though we do not exploit reference images or known scene class.
Our energy minimization method (Section 3.4) for GT Hist further improves results.
In either mode, our method appears less dependent on spatial priors: note splitting of
the sky in the ﬁrst row and correlation of green with actual grass in the last row.

Model\Metric

RMSE

PSNR

Model\Metric

RMSE

PSNR

No colorization
Lab, L2
Lab, K = 32
Lab, K = 16 × 16
Hue/chroma, K = 32
+ chromatic fading

0.343
0.318
0.321
0.328
0.342
0.299

22.98
24.25
24.33
24.30
23.77
24.45

data..fc7
data..conv5 3
conv4 1..fc7
conv5 1..fc7
fc6..fc7
fc7

0.299
0.306
0.302
0.307
0.323
0.324

24.45
24.13
24.45
24.38
24.22
24.19

Table 1:
ImageNet/cval1k. Validation
performance of system variants. Hue/chroma
is best, but only with chromatic fading.

Table 2: ImageNet/cval1k. Abla-
tion study of hypercolumn compo-
nents.

(e.g. closeup of nuts and bolts), where an algorithm should know not to add color.
Category labels are discarded; only images are available at test time. We propose
ctest10k as a standard benchmark with the following metrics:

– RMSE: root mean square error in αβ averaged over all pixels [7].
– PSNR: peak signal-to-noise ratio in RGB calculated per image [5]. We use
the arithmetic mean of PSNR over images, instead of the geometric mean
as in Cheng et al. [5]; geometric mean is overly sensitive to outliers.

By virtue of comparing to ground-truth color images, quantitative colorization
metrics can penalize reasonable, but incorrect, color guesses for many objects
(e.g. red car instead of blue car) more than jarring artifacts. This makes quali-
tative results for colorization as important as quantitative; we report both.

Figures 1, 3, and 4 show example test results of our best system variant,
selected according to performance on the validation set and trained for a total

Learning Representations for Automatic Colorization

11

Method

RMSE

Method

Grayscale (no colorization) 0.285
0.353
Welsh et al. [42]
0.262
Deshpande et al. [7]
0.254
0.211

Our Method

+ GT Scene

Deshpande et al. (C) [7]
Deshpande et al. (Q)
Our Method (Q)
Our Method (E)

RMSE

0.236
0.211
0.178
0.165

Table 3: SUN-6. Comparison
with competing methods.

Table 4: SUN-6 (GT Hist). Comparison us-
ing ground-truth histograms. Results for Desh-
pande et al. [7] use GT Scene.

of 10 epochs. This variant predicts hue and chroma and uses chromatic fading
during image generation. Table 1 provides validation benchmarks for all sys-
tem variants, including the trivial baseline of no colorization. On ImageNet test
(ctest10k), our selected model obtains 0.293 (RMSE, αβ, avg/px) and 24.94 dB
(PSNR, RGB, avg/im), compared to 0.333 and 23.27 dB for the baseline.

Table 2 examines the importance of diﬀerent neural network layers to col-
orization; it reports validation performance of ablated systems that include only
the speciﬁed subsets of layers in the hypercolumn used to predict hue and
chroma. Some lower layers may be discarded without much performance loss,
yet higher layers alone (fc6..fc7) are insuﬃcient for good colorization.

Our ImageNet colorization benchmark is new to a ﬁeld lacking an estab-
lished evaluation protocol. We therefore focus on comparisons with two recent
papers [5, 7], using their self-deﬁned evaluation criteria. To do so, we run our
ImageNet-trained hue and chroma model on two additional datasets:

– SUN-A [31] is a subset of the SUN dataset [43] containing 47 object cate-
gories. Cheng et al. [5] train a colorization system on 2688 images and report
results on 1344 test images. We were unable to obtain the list of test im-
ages, and therefore report results averaged over ﬁve random subsets of 1344
SUN-A images. We do not use any SUN-A images for training.

– SUN-6, another SUN subset, used by Deshpande et al. [7], includes images
from 6 scene categories (beach, castle, outdoor, kitchen, living room, bed-
room). We compare our results on 240 test images to those reported in [7]
for their method as well as for Welsh et al. [42] with automatically matched
reference images as in [27]. Following [7], we consider another evaluation
regime in which ground-truth target color histograms are available.

Figure 5 shows a comparison of results on SUN-6. Forgoing usage of ground-truth
global histograms, our fully automatic system produces output qualitatively su-
perior to methods relying on such side information. Tables 3 and 4 report quan-
titative performance corroborating this view. The partially automatic systems
in Table 4 adapt output to ﬁt global histograms using either: (C) cluster corre-
spondences [7], (Q) quantile matching, or (E) our energy minimization described
in Section 3.4. Our quantile matching results are superior to those of [7] and our
new energy minimization procedure oﬀers further improvement.

12

Larsson, Maire, Shakhnarovich

Fig. 6: SUN-6. Cumulative histogram
of per pixel error (higher=more pix-
els with lower error). Results for Desh-
pande et al. [7] use GT Scene.

Fig. 7: SUN-A. Histogram of per-image
PSNR for [5] and our method. The highest
geometric mean PSNR reported for experi-
ments in [5] is 24.2, vs. 32.7±2.0 for us.

Figures 6 and 7 compare error distributions on SUN-6 and SUN-A. As in
Table 3, our fully automatic method dominates all competing approaches, even
those which use auxiliary information. It is only outperformed by the version
of itself augmented with ground-truth global histograms. On SUN-A, Figure 7
shows clear separation between our method and [5] on per-image PSNR.

The Appendix (Figures 14 and 15) provides anecdotal comparisons to one
additional method, that of Charpiat et al. [2], which can be considered an auto-
matic system if reference images are available. Unfortunately, source code of [2]
is not available and reported time cost is prohibitive for large-scale evaluation
(30 minutes per image). We were thus unable to benchmark [2] on large datasets.
With regard to concurrent work, Zhang et al. [46] include a comparison of our
results to their own. The two systems are competitive in terms of quantitative
measures of colorization accuracy. Their system, set to produce more vibrant
colors, has an advantage in terms of human-measured preferences. In contrast,
an oﬀ-the-shelf VGG-16 network for image classiﬁcation, consuming our system’s
color output, more often produces correct labels, suggesting a realism advantage.
We refer interested readers to [46] for the full details of this comparison.

Though we achieve signiﬁcant improvements over prior state-of-the-art, our
results are not perfect. Figure 8 shows examples of signiﬁcant failures. Minor im-
perfections are also present in some of the results in Figures 3 and 4. We believe
a common failure mode correlates with gaps in semantic interpretation: incor-
rectly identiﬁed or unfamiliar objects and incorrect segmentation. In addition,
there are “mistakes” due to natural uncertainty of color – e.g. the graduation
robe at the bottom right of Figure 3 is red, but could as well be purple.

Since our method produces histograms, we can provide interactive means of
biasing colorizations according to user preferences. Rather than output a single
color per pixel, we can sample color for image regions and evaluate color uncer-
tainty. Speciﬁcally, solving our energy minimization formulation (Equation (5))
with global biases b that are not optimized based on a reference image, but sim-
ply “rotated” through color space, induces changed color preferences throughout
the image. The uncertainty in the predicted histogram modulates this eﬀect.

Learning Representations for Automatic Colorization

13

Fig. 8: Failure modes. Top row, left-to-right: texture confusion, too homogeneous,
color bleeding, unnatural color shifts (×2). Bottom row: inconsistent background, in-
consistent chromaticity, not enough color, object not recognized (upside down face
partly gray), context confusion (sky).

Fig. 9: Sampling colorizations. Left: Image & 3 samples; Right: Uncertainty map.

Figure 9 shows multiple sampled colorizations, together with a visualization
of uncertainty. Here, uncertainty is the entropy of the predicted hue multiplied
by the chroma. Our distributional output and energy minimization framework
open the path for future investigation of human-in-the-loop colorization tools.

4.1 Representation learning

High-level visual understanding is essential for the colorization of grayscale im-
ages, motivating our use of an ImageNet pretrained network as a starting point.
But with enough training data, perhaps we can turn this around and use col-
orization as means of learning networks for capturing high-level visual represen-
tations. Table 5 shows that a colorization network, trained from scratch using
only unlabeled color images, is surprisingly competitive. It converges slower, but
requires not more than twice the number of epochs.

Our preliminary work shows that the networks learned via training coloriza-
tion from scratch generalize well to other visual tasks. This is signiﬁcant because
such training requires no human annotation eﬀort. It follows a recent trend
of learning representations through self-supervision (e.g. context prediction [8],
solving jigsaw puzzles [29], inpainting [30], adversarial feature learning [9, 10]).
We examine self-supervised colorization as a replacement for supervised Im-
ageNet pretraining on the Pascal VOC 2012 semantic segmentation task, with
results on grayscale validation set images. We train colorization from scratch on
ImageNet (Table 5) and ﬁne-tune for Pascal semantic segmentation. We make

14

Larsson, Maire, Shakhnarovich

Initialization RMSE PSNR

Initialization

Architecture X Y C mIU (%)

Classiﬁer
Random

0.299
0.311

24.45
24.25

of

Table 5: ImageNet/cval1k.
ini-
Compares methods
tialization before colorization
training. Hue/chroma with
chromatic fading is used in
both cases (see in Tab. 1).

Classiﬁer
Colorizer
Random

VGG-16
VGG-16
VGG-16

Classiﬁer [9, 30] AlexNet
AlexNet
BiGAN [9]
AlexNet
Inpainter [30]
AlexNet
Random [30]

(cid:51) (cid:51)
(cid:51)

(cid:51) (cid:51) (cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

64.0
50.2
32.5

48.0
34.9
29.7
19.8

Table 6: VOC 2012 segmentation validation
set. Pretraining uses ImageNet images (X), labels
(Y ). VOC 2012 images are in color (C).

the one adjustment of employing cross-validated early stopping to avoid overﬁt-
ting. Table 6 shows this strategy to be promising as a drop-in replacement for
supervised ImageNet pretraining. Self-supervised colorization more than halfway
bridges the gap between random initialization and supervised pretraining.

As VGG-16 is a more performant architecture, comparison with prior work
is not straightforward. Yet, Table 6 still indicates that colorization is a front-
runner among the self-supervision methods, leading to an 18-point improvement
in mIU over the baseline. To our knowledge, 50.2% is the highest reported result
that does not supplement training with additional annotated data [19].

5 Conclusion

We present a system that demonstrates state-of-the-art ability to automatically
colorize grayscale images. Two novel contributions enable this progress: a deep
neural architecture that is trained end-to-end to incorporate semantically mean-
ingful features of varying complexity into colorization, and a color histogram pre-
diction framework that handles uncertainty and ambiguities inherent in coloriza-
tion while preventing jarring artifacts. Our fully automatic colorizer produces
strong results, improving upon previously leading methods by large margins on
all datasets tested; we also propose a new large-scale benchmark for automatic
image colorization, and establish a strong baseline with our method to facilitate
future comparisons. Our colorization results are visually appealing even on com-
plex scenes, and allow for eﬀective post-processing with creative control via color
histogram transfer and intelligent, uncertainty-driven color sampling. We further
reveal colorization as a promising avenue for self-supervised visual learning.

Acknowledgements. We thank Ayan Chakrabarti for suggesting lightness-
normalized quantile matching and for useful discussions, and Aditya Deshpande
and Jason Rock for discussions on their work. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of GPUs for this research.

Learning Representations for Automatic Colorization

15

References

1. Bertasius, G., Shi, J., Torresani, L.: Deepedge: A multi-scale bifurcated deep net-

work for top-down contour detection. In: CVPR (2015)

2. Charpiat, G., Bezrukov, I., Altun, Y., Hofmann, M., Sch¨olkopf, B.: Machine learn-
ing methods for automatic image colorization. In: Computational Photography:
Methods and Applications. CRC Press (2010)

3. Charpiat, G., Hofmann, M., Sch¨olkopf, B.: Automatic image colorization via mul-

timodal predictions. In: ECCV (2008)

4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. In:
ICLR (2015)

5. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: ICCV (2015)
6. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Se-
mantic colorization with internet images. ACM Transactions on Graphics (TOG)
30(6) (2011)

7. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-

orization. In: ICCV (2015)

8. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning
by context prediction. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 1422–1430 (2015)

9. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR

(2017)

10. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,
Courville, A.: Adversarially learned inference. arXiv preprint arXiv:1606.00704
(2016)

11. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013)

12. Ganin, Y., Lempitsky, V.S.: N4-ﬁelds: Neural network nearest neighbor ﬁelds for

13. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward

image transforms. In: ACCV (2014)

neural networks. In: AISTATS (2010)

14. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization
using similar images. In: ACM international conference on Multimedia (2012)
15. Hariharan, B., an R. Girshick, P.A., Malik, J.: Hypercolumns for object segmen-

tation and ﬁne-grained localization. CVPR (2015)

16. Huang, Y.C., Tung, Y.S., Chen, J.C., Wang, S.W., Wu, J.L.: An adaptive edge
detection based colorization algorithm and its applications. In: ACM international
conference on Multimedia (2005)

17. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Let there be Color!: Joint End-to-end
Learning of Global and Local Image Priors for Automatic Image Colorization with
Simultaneous Classiﬁcation. ACM Transactions on Graphics (Proc. of SIGGRAPH
2016) 35(4) (2016)

18. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

19. Ion, A., Carreira, J., Sminchisescu, C.: Probabilistic joint image segmentation and
labeling by ﬁgure-ground composition. International journal of computer vision
107(1), 40–57 (2014)

16

Larsson, Maire, Shakhnarovich

20. Irony, R., Cohen-Or, D., Lischinski, D.: Colorization by example. In: Eurographics

Symp. on Rendering (2005)

21. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

22. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. ACM Trans-

actions on Graphics (TOG) 23(3) (2004)

23. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better. arXiv

24. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

preprint arXiv:1506.04579 (2015)

segmentation. In: CVPR (2015)

25. Luan, Q., Wen, F., Cohen-Or, D., Liang, L., Xu, Y.Q., Shum, H.Y.: Natural image

colorization. In: Eurographics conference on Rendering Techniques (2007)

26. Maire, M., Yu, S.X., Perona, P.: Reconstructive sparse code transfer for contour

detection and semantic labeling. In: ACCV (2014)

27. Morimoto, Y., Taguchi, Y., Naemura, T.: Automatic colorization of grayscale im-

ages using multiple images on the web. In: SIGGRAPH: Posters (2009)

28. Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg-

mentation with zoom-out features. In: CVPR (2015)

29. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving

jigsaw puzzles. In: ECCV (2016)

30. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR (2016)

31. Patterson, G., Xu, C., Su, H., Hays, J.: The sun attribute database: Beyond cat-
egories for deeper scene understanding. International Journal of Computer Vision
108(1-2) (2014)

32. Qu, Y., Wong, T.T., Heng, P.A.: Manga colorization. ACM Transactions on Graph-

ics (TOG) 25(3) (2006)

33. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3) (2015)

34. Sapiro, G.: Inpainting the colors. In: ICIP (2005)
35. Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z.: Deepcontour: A deep convo-
lutional feature learned by positive-sharing loss for contour detection. In: CVPR
(2015)

36. Shi, J., Malik, J.: Normalized cuts and image segmentation. Pattern Analysis and

Machine Intelligence, IEEE Transactions on 22(8) (2000)

37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015)

38. S`ykora, D., Buri´anek, J., ˇZ´ara, J.: Unsupervised colorization of black-and-white
cartoons. In: International symposium on Non-photorealistic animation and ren-
dering (2004)

39. Tai, Y.W., Jia, J., Tang, C.K.: Local color transfer via probabilistic segmentation

by expectation-maximization. In: CVPR (2005)

40. Tola, E., Lepetit, V., Fua, P.: A fast local descriptor for dense matching. In: CVPR

(2008)

41. Tsaftaris, S.A., Casadio, F., Andral, J.L., Katsaggelos, A.K.: A novel visualization
tool for art history and conservation: Automated colorization of black and white
archival photographs of works of art. Studies in Conservation 59(3) (2014)

Learning Representations for Automatic Colorization

17

42. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.

ACM Transactions on Graphics (TOG) 21(3) (2002)

43. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale

scene recognition from abbey to zoo. In: CVPR (2010)

44. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)
45. Yatziv, L., Sapiro, G.: Fast image and video colorization using chrominance blend-

ing. Image Processing, IEEE Transactions on 15(5) (2006)

46. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV (2016)

18

Larsson, Maire, Shakhnarovich

Hue

Chroma

Hue

Chroma

Output: Color Image

Ground-truth

Hue

Chroma

Fig. 10: Histogram predictions. Example of predicted hue/chroma his-
tograms.

Appendix A provides additional training and evaluation details. This is followed
by more results and examples in Appendix B.

A Supplementary details

A.1 Re-balancing

To adjust the scale of the activations of layer l by factor m, without changing
any other layer’s activation, the weights W and the bias b are updated according
to:

Wl ← mWl

bl ← mbl

Wl+1 ←

Wl+1

(6)

1
m

The activation of xl+1 becomes:

xl+1 =

Wl+1ReLU(mWlxl + mbl) + bl+1

(7)

1
m

The m inside the ReLU will not aﬀect whether or not a value is rectiﬁed, so the
two cases remain the same: (1) negative: the activation will be the corresponding
feature in bl+1 regardless of m, and (2) positive: the ReLU becomes the identity
function and m and 1

m cancel to get back the original activation.

We set m =

1√

, estimated for each layer separately.

ˆE[X 2]

A.2 Color space αβ

The color channels αβ (“ab” in [7]) are calculated as

α =

B − 1

2 (R + G)
L + (cid:15)

β =

R − G
L + (cid:15)

(8)

where (cid:15) = 0.0001, R, G, B ∈ [0, 1] and L = R+G+B

.3

3

3 We know that this is how Deshpande et al. [7] calculate it based on their code release.

Learning Representations for Automatic Colorization

19

Output: Color Image

Hue

Chroma

Hue

Chroma

Ground-truth

Hue

Chroma

Fig. 11: Histogram predictions. Example of predicted hue/chroma his-
tograms.

A.3 Error metrics

For M images, each image m with Nm pixels, we calculate the error metrics as:

RMSE =

(cid:80)M

1
m=1 Nm
Nm(cid:88)
M
(cid:88)

1
M

m=1

n=1

M
(cid:88)

Nm(cid:88)

m=1

n=1

(cid:114)(cid:13)
(cid:104)
y(m)
(cid:13)
(cid:13)
αβ

(cid:105)

n

(cid:105)

(cid:104)
ˆy(m)
αβ

−

n

(cid:13)
2
(cid:13)
(cid:13)

(cid:33)

(cid:32)

(cid:107)y(m)

RGB − ˆy(m)
3Nm

RGB(cid:107)2

PSNR =

−10 · log10

(9)

(10)

Where y(m)

αβ ∈ [−3, 3]Nm×2 and y(m)

RGB ∈ [0, 1]Nm×3 for all m.

A.4 Lightness correction

Ideally the lightness L is an unaltered pass-through channel. However, due to
subtle diﬀerences in how L is deﬁned, it is possible that the lightness of the
predicted image, ˆL, does not agree with the input, L. To compensate for this,
we add L− ˆL to all color channels in the predicted RGB image as a ﬁnal corrective
step.

B Supplementary results

B.1 Validation

A more detailed list of validation results for hue/chroma inference methods is
seen in Table 7.

20

Larsson, Maire, Shakhnarovich

Hue

Chroma

CF RMSE PSNR

Sample
Mode

0.426
Sample
0.304
Mode
Expectation Expectation
0.374
Expectation Expectation (cid:51) 0.307
0.342
Expectation Median
Expectation Median

21.41
23.90
23.13
24.35
23.77
(cid:51) 0.299 24.45

7:

Table
ImageNet/cval1k.
Comparison of various histogram
inference methods for hue/chroma.
Mode/mode does fairly well but
has severe visual artifacts. (CF =
Chromatic fading)

B.2 Examples

We provide additional samples for global biasing (Figure 12) and SUN-6 (Fig-
ure 13). Comparisons with Charpiat et al. [2] appear in Figures 14 and 15.
Examples of how our algorithm can bring old photographs to life in Figure 16.
More examples on ImageNet (ctest10k) in Figures 17 to 20 and Figure 21 (failure
cases). Examples of histogram predictions in Figures 10 and 11.

C Document changelog

Overview of document revisions:

v1 Initial release.

v2 ECCV 2016 camera-ready version. Includes discussion about concurrent work
and new experiments using colorization to learn visual representations (Sec-
tion 4.1).

v3 Added overlooked reference.

Learning Representations for Automatic Colorization

21

Fig. 12: Sampling multiple colorizations. From left: graylevel input; three coloriza-
tions sampled from our model; color uncertainty map according to our model.

22

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 13: SUN-6. Additional qualitative comparisons.

Reference Image

Input

Charpiat et al. [2]

Our Method
(Energy Minimization)

Fig. 14: Transfer. Comparison with Charpiat et al. [2] with reference image.
Their method works fairly well when the reference image closely matches (com-
pare with Figure 15). However, they still present sharp unnatural color edges. We
apply our histogram transfer method (Energy Minimization) using the reference
image.

Learning Representations for Automatic Colorization

23

Input

Charpiat et al. [2]

Our Method

Ground-truth

Fig. 15: Portraits. Comparison with Charpiat et al. [2], a transfer-based method
using 53 reference portrait paintings. Note that their method works signiﬁcantly
worse when the reference images are not hand-picked for each grayscale input
(compare with Figure 14). Our model was not trained speciﬁcally for this task
and we used no reference images.

24

Larsson, Maire, Shakhnarovich

Input

Our Method

Input

Our Method

Fig. 16: B&W photographs. Old photographs that were automatically col-
orized. (Source: Library of Congress, www.loc.gov)

Learning Representations for Automatic Colorization

25

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 17: Fully automatic colorization results on ImageNet/ctest10k.

26

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 18: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

27

Fig. 19: Fully automatic colorization results on ImageNet/ctest10k.

28

Larsson, Maire, Shakhnarovich

Fig. 20: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

29

Too Desaturated

Inconsistent Chroma

Inconsistent Hue

Edge Pollution

Color Bleeding

Fig. 21: Failure cases. Examples of the ﬁve most common failure cases: the whole
image lacks saturation (Too Desaturated); inconsistent chroma in objects or regions,
causing parts to be gray (Inconsistent Chroma); inconsistent hue, causing unnatural
color shifts that are particularly typical between red and blue (Inconsistent Hue);
inconsistent hue and chroma around the edge, commonly occurring for closeups where
background context is unclear (Edge Pollution); color boundary is not clearly separated,
causing color bleeding (Color Bleeding).

7
1
0
2
 
g
u
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
3
v
8
6
6
6
0
.
3
0
6
1
:
v
i
X
r
a

Learning Representations for Automatic Colorization

Gustav Larsson1, Michael Maire2, and Gregory Shakhnarovich2

1University of Chicago

2Toyota Technological Institute at Chicago

larsson@cs.uchicago.edu, {mmaire,greg}@ttic.edu

Abstract. We develop a fully automatic image colorization system. Our
approach leverages recent advances in deep networks, exploiting both
low-level and semantic representations. As many scene elements natu-
rally appear according to multimodal color distributions, we train our
model to predict per-pixel color histograms. This intermediate output
can be used to automatically generate a color image, or further manip-
ulated prior to image formation. On both fully and partially automatic
colorization tasks, we outperform existing methods. We also explore col-
orization as a vehicle for self-supervised visual representation learning.

Fig. 1: Our automatic colorization of grayscale input; more examples in Figs. 3 and 4.

1 Introduction

Colorization of grayscale images is a simple task for the human imagination. A
human need only recall that sky is blue and grass is green; for many objects, the
mind is free to hallucinate several plausible colors. The high-level comprehension
required for this process is precisely why the development of fully automatic col-
orization algorithms remains a challenge. Colorization is thus intriguing beyond
its immediate practical utility in graphics applications. Automatic colorization
serves as a proxy measure for visual understanding. Our work makes this con-
nection explicit; we unify a colorization pipeline with the type of deep neural
architectures driving advances in image classiﬁcation and object detection.

Both our technical approach and focus on fully automatic results depart
from past work. Given colorization’s importance across multiple applications
(e.g. historical photographs and videos [41], artist assistance [32, 38]), much re-
search strives to make it cheaper and less time-consuming [3,5–7,14,20,22,27,42].
However, most methods still require some level of user input [3, 6, 14, 20, 22, 34].
Our work joins the relatively few recent eﬀorts on fully automatic coloriza-
tion [5, 7, 27]. Some [5, 7] show promising results on typical scenes (e.g. land-
scapes), but their success is limited on complex images with foreground objects.

2

Larsson, Maire, Shakhnarovich

VGG-16-Gray

Hypercolumn

Hue

Ground-truth

(fc7) conv7

(fc6) conv6
conv5 3

conv1 1

p

h fc1

Chroma

Lightness

Input: Grayscale Image

Output: Color Image

Fig. 2: System overview. We process a grayscale image through a deep convolu-
tional architecture (VGG) [37] and take spatially localized multilayer slices (hyper-
columns) [15, 26, 28], as per-pixel descriptors. We train our system end-to-end for the
task of predicting hue and chroma distributions for each pixel p given its hypercolumn
descriptor. These predicted distributions determine color assignment at test time.

At a technical level, existing automatic colorization methods often employ
a strategy of ﬁnding suitable reference images and transferring their color onto
a target grayscale image [7, 27]. This works well if suﬃciently similar reference
images can be found, but is diﬃcult for unique grayscale input images. Such a
strategy also requires processing a large repository of reference images at test
time. In contrast, our approach is free of database search and fast at test time.
Section 2 provides a complete view of prior methods, highlighting diﬀerences.

Our approach to automatic colorization converts two intuitive observations
into design principles. First, semantic information matters. In order to colorize
arbitrary images, a system must interpret the semantic composition of the scene
(what is in the image: faces, cars, plants, . . . ) as well as localize objects (where
things are). Deep convolutional neural networks (CNNs) can serve as tools to
incorporate semantic parsing and localization into a colorization system.

Our second observation is that while some scene elements can be assigned
a single color with high conﬁdence, others (e.g. clothes or cars) may draw from
many suitable colors. Thus, we design our system to predict a color histogram,
instead of a single color, at every image location. Figure 2 sketches the CNN
architecture we use to connect semantics with color distributions by exploiting
features across multiple abstraction levels. Section 3 provides details.

Section 4 experimentally validates our algorithm against competing meth-
ods [7, 42] in two settings: fully (grayscale input only) and partially (grayscale
input with reference global color histogram) automatic colorization. Across ev-
ery metric and dataset [31, 33, 43], our method achieves the best performance.
Our system’s fully automatic output is superior to that of prior methods relying
on additional information such as reference images or ground-truth color his-

Learning Representations for Automatic Colorization

3

tograms. To ease the comparison burden for future research, we propose a new
colorization benchmark on ImageNet [33]. We also experiment with colorization
itself as an objective for learning visual representations from scratch, thereby
replacing use of ImageNet pretraining in a traditional semantic labeling task.

Section 5 summarizes our contributions: (1) a novel technical approach to
colorization, bringing semantic knowledge to bear using CNNs, and modeling
color distributions; (2) state-of-the-art performance across fully and partially
automatic colorization tasks; (3) a new ImageNet colorization benchmark; (4)
proof of concept on colorization for self-supervised representation learning.

2 Related work

Previous colorization methods broadly fall into three categories: scribble-based [16,
22,25,32,45], transfer [3,6,14,20,27,39,42], and automatic direct prediction [5,7].
Scribble-based methods, introduced by Levin et al. [22], require manually
specifying desired colors of certain regions. These scribble colors are propagated
under the assumption that adjacent pixels with similar luminance should have
similar color, with the optimization relying on Normalized Cuts [36]. Users can
interactively reﬁne results via additional scribbles. Further advances extend sim-
ilarity to texture [25, 32], and exploit edges to reduce color bleeding [16].

Transfer-based methods rely on availability of related reference image(s),
from which color is transferred to the target grayscale image. Mapping between
source and target is established automatically, using correspondences between
local descriptors [3, 27, 42], or in combination with manual intervention [6, 20].
Excepting [27], reference image selection is at least partially manual.

In contrast to these method families, our goal is fully automatic colorization.
We are aware of two recent eﬀorts in this direction. Deshpande et al. [7] colorize
an entire image by solving a linear system. This can be seen as an extension
of patch-matching techniques [42], adding interaction terms for spatial consis-
tency. Regression trees address the high-dimensionality of the system. Inference
requires an iterative algorithm. Most of the experiments are focused on a dataset
(SUN-6) limited to images of a few scene classes, and best results are obtained
when the scene class is known at test time. They also examine another partially
automatic task, in which a desired global color histogram is provided.

The work of Cheng et al. [5] is perhaps most related to ours. It combines three
levels of features with increasing receptive ﬁeld: the raw image patch, DAISY
features [40], and semantic features [24]. These features are concatenated and
fed into a three-layer fully connected neural network trained with an L2 loss.
Only this last component is optimized; the feature representations are ﬁxed.

Unlike [5,7], our system does not rely on hand-crafted features, is trained end-
to-end, and treats color prediction as a histogram estimation task rather than
as regression. Experiments in Section 4 justify these principles by demonstrating
performance superior to the best reported by [5, 7] across all regimes.

Two concurrent eﬀorts also present feed-forward networks trained end-to-end
for colorization. Iizuka & Simo-Serra et al. [17] propose a network that concate-

4

Larsson, Maire, Shakhnarovich

nates two separate paths, specializing in global and local features, respectively.
This concatenation can be seen as a two-tiered hypercolumn; in comparison, our
16-layer hypercolumn creates a continuum between low- and high-level features.
Their network is trained jointly for classiﬁcation (cross-entropy) and colorization
(L2 loss in Lab). We initialize, but do not anchor, our system to a classiﬁcation-
based network, allowing for ﬁne-tuning of colorization on unlabeled datasets.

Zhang et al. [46] similarly propose predicting color histograms to handle
multi-modality. Some key diﬀerences include their usage of up-convolutional lay-
ers, deep supervision, and dense training. In comparison, we use a fully convolu-
tional approach, with deep supervision implicit in the hypercolumn design, and,
as Section 3 describes, memory-eﬃcient training via spatially sparse samples.

3 Method

We frame the colorization problem as learning a function f : X → Y. Given
a grayscale image patch x ∈ X = [0, 1]S×S, f predicts the color y ∈ Y of its
center pixel. The patch size S × S is the receptive ﬁeld of the colorizer. The
output space Y depends on the choice of color parameterization. We implement
f according to the neural network architecture diagrammed in Figure 2.

Motivating this strategy is the success of similar architectures for semantic
segmentation [4, 11, 15, 24, 28] and edge detection [1, 12, 26, 35, 44]. Together with
colorization, these tasks can all be viewed as image-to-image prediction problems,
in which a value is predicted for each input pixel. Leading methods commonly
adapt deep convolutional neural networks pretrained for image classiﬁcation [33,
37]. Such classiﬁcation networks can be converted to fully convolutional networks
that produce output of the same spatial size as the input, e.g. using the shift-
and-stitch method [24] or the more eﬃcient `a trous algorithm [4]. Subsequent
training with a task-speciﬁc loss ﬁne-tunes the converted network.

Skip-layer connections, which directly link low- and mid-level features to pre-
diction layers, are an architectural addition beneﬁcial for many image-to-image
problems. Some methods implement skip connections directly through concate-
nation layers [4, 24], while others equivalently extract per-pixel descriptors by
reading localized slices of multiple layers [15, 26, 28]. We use this latter strategy
and adopt the recently coined hypercolumn terminology [15] for such slices.

Though we build upon these ideas, our technical approach innovates on two
fronts. First, we integrate domain knowledge for colorization, experimenting with
output spaces and loss functions. We design the network output to serve as an
intermediate representation, appropriate for direct or biased sampling. We intro-
duce an energy minimization procedure for optionally biasing sampling towards a
reference image. Second, we develop a novel and eﬃcient computational strategy
for network training that is widely applicable to hypercolumn architectures.

3.1 Color spaces

We generate training data by converting color images to grayscale according to
L = R+G+B
. This is only one of many desaturation options and chosen primarily

3

Learning Representations for Automatic Colorization

5

to facilitate comparison with Deshpande et al. [7]. For the representation of
color predictions, using RGB is overdetermined, as lightness L is already known.
We instead consider output color spaces with L (or a closely related quantity)
conveniently appearing as a separate pass-through channel:

– Hue/chroma. Hue-based spaces, such as HSL, can be thought of as a color
cylinder, with angular coordinate H (hue), radial distance S (saturation),
and height L (lightness). The values of S and H are unstable at the bot-
tom (black) and top (white) of the cylinder. HSV describes a similar color
cylinder which is only unstable at the bottom. However, L is no longer one
of the channels. We wish to avoid both instabilities and still retain L as a
channel. The solution is a color bicone, where chroma (C) takes the place of
saturation. Conversion to HSV is given by V = L + C

2 , S = C
V .

– Lab and αβ. Lab (or L*a*b) is designed to be perceptually linear. The
color vector (a, b) deﬁnes a Euclidean space where the distance to the origin
determines chroma. Deshpande et al. [7] use a color space somewhat similar
to Lab, denoted “ab”. To diﬀerentiate, we call their color space αβ.

3.2 Loss

For any output color representation, we require a loss function for measuring
prediction errors. A ﬁrst consideration, also used in [5], is L2 regression in Lab:

Lreg(x, y) = (cid:107)f (x) − y(cid:107)2

(1)

(2)

where Y = R2 describes the (a, b) vector space. However, regression targets
do not handle multimodal color distributions well. To address this, we instead
predict distributions over a set of color bins, a technique also used in [3]:

Lhist(x, y) = DKL(y(cid:107)f (x))

where Y = [0, 1]K describes a histogram over K bins, and DKL is the KL-
divergence. The ground-truth histogram y is set as the empirical distribution in
a rectangular region of size R around the center pixel. Somewhat surprisingly,
our experiments see no beneﬁt to predicting smoothed histograms, so we simply
set R = 1. This makes y a one-hot vector and Equation (2) the log loss. For
histogram predictions, the last layer of neural network f is always a softmax.

There are several choices of how to bin color space. We bin the Lab axes
by evenly spaced Gaussian quantiles (µ = 0, σ = 25). They can be encoded
separately for a and b (as marginal distributions), in which case our loss becomes
the sum of two separate terms deﬁned by Equation (2). They can also be encoded
as a joint distribution over a and b, in which case we let the quantiles form a 2D
grid of bins. In our experiments, we set K = 32 for marginal distributions and
K = 16 × 16 for joint. We determined these numbers, along with σ, to oﬀer a
good compromise of output ﬁdelity and output complexity.

For hue/chroma, we only consider marginal distributions and bin axes uni-
formly in [0, 1]. Since hue becomes unstable as chroma approaches zero, we add

6

Larsson, Maire, Shakhnarovich

a sample weight to the hue based on the chroma:

Lhue/chroma(x, y) = DKL(yC(cid:107)fC(x)) + λH yCDKL(yH(cid:107)fH(x))

(3)

where Y = [0, 1]2×K and yC ∈ [0, 1] is the sample pixel’s chroma. We set λH = 5,
roughly the inverse expectation of yC, thus equally weighting hue and chroma.

3.3

Inference

Given network f trained according to a loss function in the previous section, we
evaluate it at every pixel n in a test image: ˆyn = f (xn). For the L2 loss, all that
remains is to combine each ˆyn with the respective lightness and convert to RGB.
With histogram predictions, we consider options for inferring a ﬁnal color:

– Sample Draw a sample from the histogram. If done per pixel, this may
create high-frequency color changes in areas of high-entropy histograms.
– Mode Take the arg maxk ˆyn,k as the color. This can create jarring transitions

between colors, and is prone to vote splitting for proximal centroids.

– Median Compute cumulative sum of ˆyn and use linear interpolation to ﬁnd
the value at the middle bin. Undeﬁned for circular histograms, such as hue.
– Expectation Sum over the color bin centroids weighted by the histogram.

For Lab output, we achieve the best qualitative and quantitative results using
expectations. For hue/chroma, the best results are achieved by taking the median
of the chroma. Many objects can appear both with and without chroma, which
means C = 0 is a particularly common bin. This mode draws the expectation
closer to zero, producing less saturated images. As for hue, since it is circular,
we ﬁrst compute the complex expectation:

z = EH∼fh(x)[H] (cid:44) 1
K

(cid:88)

k

[fh(x)]keiθk ,

θk = 2π

(4)

k + 0.5
K

We then set hue to the argument of z remapped to lie in [0, 1).

In cases where the estimate of the chroma is
high and z is close to zero, the instability of the
hue can create artifacts. A simple, yet eﬀective,
ﬁx is chromatic fading: downweight the chroma if
the absolute value of z is too small. We thus re-
deﬁne the predicted chroma by multiplying it by
a factor of max(η−1|z|, 1). In our experiments, we
set η = 0.03 (obtained via cross-validation).

3.4 Histogram transfer from ground-truth

So far, we have only considered the fully automatic color inference task. Desh-
pande et al. [7], test a separate task where the ground-truth histogram in the

Learning Representations for Automatic Colorization

7

two non-lightness color channels of the original color image is made available.1
In order to compare, we propose two histogram transfer methods. We refer to
the predicted image as the source and the ground-truth image as the target.

Lightness-normalized quantile matching. Divide the RGB representation
of both source and target by their respective lightness. Compute marginal his-
tograms over the resulting three color channels. Alter each source histogram to
ﬁt the corresponding target histogram by quantile matching, and multiply by
lightness. Though it does not exploit our richer color distribution predictions,
quantile matching beats the cluster correspondence method of [7] (see Table 3).

Energy minimization. We phrase histogram matching as minimizing energy:

E =

1
N

(cid:88)

n

DKL(ˆy∗

n(cid:107)ˆyn) + λDχ2((cid:104) ˆy∗(cid:105), t)

(5)

where N is the number of pixels, ˆy, ˆy∗ ∈ [0, 1]N ×K are the predicted and poste-
rior distributions, respectively. The target histogram is denoted by t ∈ [0, 1]K.
The ﬁrst term contains unary potentials that anchor the posteriors to the pre-
dictions. The second term is a symmetric χ2 distance to promote proximity
between source and target histograms. Weight λ deﬁnes relative importance of
histogram matching. We estimate the source histogram as (cid:104)ˆy∗(cid:105) = 1
n. We
N
parameterize the posterior for all pixels n as: ˆy∗
n = softmax(log ˆyn + b), where
the vector b ∈ RK can be seen as a global bias for each bin. It is also possible
to solve for the posteriors directly; this does not perform better quantitatively
and is more prone to introducing artifacts. We solve for b using gradient descent
on E and use the resulting posteriors in place of the predictions. In the case of
marginal histograms, the optimization is run twice, once for each color channel.

n ˆy∗

(cid:80)

3.5 Neural network architecture and training

Our base network is a fully convolutional version of VGG-16 [37] with two
changes: (1) the classiﬁcation layer (fc8) is discarded, and (2) the ﬁrst ﬁlter
layer (conv1 1) operates on a single intensity channel instead of mean-subtracted
RGB. We extract a hypercolumn descriptor for a pixel by concatenating the fea-
tures at its spatial location in all layers, from data to conv7 (fc7), resulting in a
12, 417 channel descriptor. We feed this hypercolumn into a fully connected layer
with 1024 channels (h fc1 in Figure 2), to which we connect output predictors.
Processing each pixel separately in such manner is quite costly. We instead
run an entire image through a single forward pass of VGG-16 and approximate
hypercolumns using bilinear interpolation. Even with such sharing, densely ex-
tracting hypercolumns requires signiﬁcant memory (1.7 GB for 256 × 256 input).
To ﬁt image batches in memory during training, we instead extract hyper-
columns at only a sparse set of locations, implementing a custom Caﬀe [21] layer

1 Note that if the histogram of the L channel were available, it would be possible to
match lightness to lightness exactly and thus greatly narrow down color placement.

8

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 3: Fully automatic colorization results on ImageNet/ctest10k. Our sys-
tem reproduces known object color properties (e.g. faces, sky, grass, fruit, wood), and
coherently picks colors for objects without such properties (e.g. clothing).

to directly compute them.2 Extracting batches of only 128 hypercolumn descrip-
tors per input image, sampled at random locations, provides suﬃcient training
signal. In the backward pass of stochastic gradient descent, an interpolated hy-
percolumn propagates its gradients to the four closest spatial cells in each layer.
Locks ensure atomicity of gradient updates, without incurring any performance
penalty. This drops training memory for hypercolumns to only 13 MB per image.
We initialize with a version of VGG-16 pretrained on ImageNet, adapting
it to grayscale by averaging over color channels in the ﬁrst layer and rescaling
appropriately. Prior to training for colorization, we further ﬁne-tune the network
for one epoch on the ImageNet classiﬁcation task with grayscale input. As the
original VGG-16 was trained without batch normalization [18], scale of responses
in internal layers can vary dramatically, presenting a problem for learning atop
their hypercolumn concatenation. Liu et al. [23] compensate for such variability
by applying layer-wise L2 normalization. We use the alternative of balancing
hypercolumns so that each layer has roughly unit second moment (E[X 2] ≈ 1);
Appendix (Section A.1) provides additional details.

2 https://github.com/gustavla/autocolorize

Learning Representations for Automatic Colorization

9

Fig. 4: Additional results. Top: Our automatic colorizations of these ImageNet ex-
amples are diﬃcult to distinguish from real color images. Bottom: B&W photographs.

4 Experiments

Starting from pretrained VGG-16-Gray, described in the previous section, we
attach h fc1 and output prediction layers with Xavier initialization [13], and
ﬁne-tune the entire system for colorization. We consider multiple prediction layer
variants: Lab output with L2 loss, and both Lab and hue/chroma marginal or
joint histogram output with losses according to Equations (2) and (3). We train
each system variant end-to-end for one epoch on the 1.2 million images of the
ImageNet training set, each resized to at most 256 pixels in smaller dimension.
A single epoch takes approximately 17 hours on a GTX Titan X GPU. At test
time, colorizing a single 512 × 512 pixel image takes 0.5 seconds.

We setup two disjoint subsets of the ImageNet validation data for our own
use: 1000 validation images (cval1k) and 10000 test images (ctest10k). Each
set has a balanced representation for ImageNet categories, and excludes any
images encoded as grayscale, but may include images that are naturally grayscale

10

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 5: SUN-6. GT Scene: test image scene class is available. GT Hist: test image color
histogram is available. We obtain colorizations with visual quality better than those
from prior work, even though we do not exploit reference images or known scene class.
Our energy minimization method (Section 3.4) for GT Hist further improves results.
In either mode, our method appears less dependent on spatial priors: note splitting of
the sky in the ﬁrst row and correlation of green with actual grass in the last row.

Model\Metric

RMSE

PSNR

Model\Metric

RMSE

PSNR

No colorization
Lab, L2
Lab, K = 32
Lab, K = 16 × 16
Hue/chroma, K = 32
+ chromatic fading

0.343
0.318
0.321
0.328
0.342
0.299

22.98
24.25
24.33
24.30
23.77
24.45

data..fc7
data..conv5 3
conv4 1..fc7
conv5 1..fc7
fc6..fc7
fc7

0.299
0.306
0.302
0.307
0.323
0.324

24.45
24.13
24.45
24.38
24.22
24.19

Table 1:
ImageNet/cval1k. Validation
performance of system variants. Hue/chroma
is best, but only with chromatic fading.

Table 2: ImageNet/cval1k. Abla-
tion study of hypercolumn compo-
nents.

(e.g. closeup of nuts and bolts), where an algorithm should know not to add color.
Category labels are discarded; only images are available at test time. We propose
ctest10k as a standard benchmark with the following metrics:

– RMSE: root mean square error in αβ averaged over all pixels [7].
– PSNR: peak signal-to-noise ratio in RGB calculated per image [5]. We use
the arithmetic mean of PSNR over images, instead of the geometric mean
as in Cheng et al. [5]; geometric mean is overly sensitive to outliers.

By virtue of comparing to ground-truth color images, quantitative colorization
metrics can penalize reasonable, but incorrect, color guesses for many objects
(e.g. red car instead of blue car) more than jarring artifacts. This makes quali-
tative results for colorization as important as quantitative; we report both.

Figures 1, 3, and 4 show example test results of our best system variant,
selected according to performance on the validation set and trained for a total

Learning Representations for Automatic Colorization

11

Method

RMSE

Method

Grayscale (no colorization) 0.285
0.353
Welsh et al. [42]
0.262
Deshpande et al. [7]
0.254
0.211

Our Method

+ GT Scene

Deshpande et al. (C) [7]
Deshpande et al. (Q)
Our Method (Q)
Our Method (E)

RMSE

0.236
0.211
0.178
0.165

Table 3: SUN-6. Comparison
with competing methods.

Table 4: SUN-6 (GT Hist). Comparison us-
ing ground-truth histograms. Results for Desh-
pande et al. [7] use GT Scene.

of 10 epochs. This variant predicts hue and chroma and uses chromatic fading
during image generation. Table 1 provides validation benchmarks for all sys-
tem variants, including the trivial baseline of no colorization. On ImageNet test
(ctest10k), our selected model obtains 0.293 (RMSE, αβ, avg/px) and 24.94 dB
(PSNR, RGB, avg/im), compared to 0.333 and 23.27 dB for the baseline.

Table 2 examines the importance of diﬀerent neural network layers to col-
orization; it reports validation performance of ablated systems that include only
the speciﬁed subsets of layers in the hypercolumn used to predict hue and
chroma. Some lower layers may be discarded without much performance loss,
yet higher layers alone (fc6..fc7) are insuﬃcient for good colorization.

Our ImageNet colorization benchmark is new to a ﬁeld lacking an estab-
lished evaluation protocol. We therefore focus on comparisons with two recent
papers [5, 7], using their self-deﬁned evaluation criteria. To do so, we run our
ImageNet-trained hue and chroma model on two additional datasets:

– SUN-A [31] is a subset of the SUN dataset [43] containing 47 object cate-
gories. Cheng et al. [5] train a colorization system on 2688 images and report
results on 1344 test images. We were unable to obtain the list of test im-
ages, and therefore report results averaged over ﬁve random subsets of 1344
SUN-A images. We do not use any SUN-A images for training.

– SUN-6, another SUN subset, used by Deshpande et al. [7], includes images
from 6 scene categories (beach, castle, outdoor, kitchen, living room, bed-
room). We compare our results on 240 test images to those reported in [7]
for their method as well as for Welsh et al. [42] with automatically matched
reference images as in [27]. Following [7], we consider another evaluation
regime in which ground-truth target color histograms are available.

Figure 5 shows a comparison of results on SUN-6. Forgoing usage of ground-truth
global histograms, our fully automatic system produces output qualitatively su-
perior to methods relying on such side information. Tables 3 and 4 report quan-
titative performance corroborating this view. The partially automatic systems
in Table 4 adapt output to ﬁt global histograms using either: (C) cluster corre-
spondences [7], (Q) quantile matching, or (E) our energy minimization described
in Section 3.4. Our quantile matching results are superior to those of [7] and our
new energy minimization procedure oﬀers further improvement.

12

Larsson, Maire, Shakhnarovich

Fig. 6: SUN-6. Cumulative histogram
of per pixel error (higher=more pix-
els with lower error). Results for Desh-
pande et al. [7] use GT Scene.

Fig. 7: SUN-A. Histogram of per-image
PSNR for [5] and our method. The highest
geometric mean PSNR reported for experi-
ments in [5] is 24.2, vs. 32.7±2.0 for us.

Figures 6 and 7 compare error distributions on SUN-6 and SUN-A. As in
Table 3, our fully automatic method dominates all competing approaches, even
those which use auxiliary information. It is only outperformed by the version
of itself augmented with ground-truth global histograms. On SUN-A, Figure 7
shows clear separation between our method and [5] on per-image PSNR.

The Appendix (Figures 14 and 15) provides anecdotal comparisons to one
additional method, that of Charpiat et al. [2], which can be considered an auto-
matic system if reference images are available. Unfortunately, source code of [2]
is not available and reported time cost is prohibitive for large-scale evaluation
(30 minutes per image). We were thus unable to benchmark [2] on large datasets.
With regard to concurrent work, Zhang et al. [46] include a comparison of our
results to their own. The two systems are competitive in terms of quantitative
measures of colorization accuracy. Their system, set to produce more vibrant
colors, has an advantage in terms of human-measured preferences. In contrast,
an oﬀ-the-shelf VGG-16 network for image classiﬁcation, consuming our system’s
color output, more often produces correct labels, suggesting a realism advantage.
We refer interested readers to [46] for the full details of this comparison.

Though we achieve signiﬁcant improvements over prior state-of-the-art, our
results are not perfect. Figure 8 shows examples of signiﬁcant failures. Minor im-
perfections are also present in some of the results in Figures 3 and 4. We believe
a common failure mode correlates with gaps in semantic interpretation: incor-
rectly identiﬁed or unfamiliar objects and incorrect segmentation. In addition,
there are “mistakes” due to natural uncertainty of color – e.g. the graduation
robe at the bottom right of Figure 3 is red, but could as well be purple.

Since our method produces histograms, we can provide interactive means of
biasing colorizations according to user preferences. Rather than output a single
color per pixel, we can sample color for image regions and evaluate color uncer-
tainty. Speciﬁcally, solving our energy minimization formulation (Equation (5))
with global biases b that are not optimized based on a reference image, but sim-
ply “rotated” through color space, induces changed color preferences throughout
the image. The uncertainty in the predicted histogram modulates this eﬀect.

Learning Representations for Automatic Colorization

13

Fig. 8: Failure modes. Top row, left-to-right: texture confusion, too homogeneous,
color bleeding, unnatural color shifts (×2). Bottom row: inconsistent background, in-
consistent chromaticity, not enough color, object not recognized (upside down face
partly gray), context confusion (sky).

Fig. 9: Sampling colorizations. Left: Image & 3 samples; Right: Uncertainty map.

Figure 9 shows multiple sampled colorizations, together with a visualization
of uncertainty. Here, uncertainty is the entropy of the predicted hue multiplied
by the chroma. Our distributional output and energy minimization framework
open the path for future investigation of human-in-the-loop colorization tools.

4.1 Representation learning

High-level visual understanding is essential for the colorization of grayscale im-
ages, motivating our use of an ImageNet pretrained network as a starting point.
But with enough training data, perhaps we can turn this around and use col-
orization as means of learning networks for capturing high-level visual represen-
tations. Table 5 shows that a colorization network, trained from scratch using
only unlabeled color images, is surprisingly competitive. It converges slower, but
requires not more than twice the number of epochs.

Our preliminary work shows that the networks learned via training coloriza-
tion from scratch generalize well to other visual tasks. This is signiﬁcant because
such training requires no human annotation eﬀort. It follows a recent trend
of learning representations through self-supervision (e.g. context prediction [8],
solving jigsaw puzzles [29], inpainting [30], adversarial feature learning [9, 10]).
We examine self-supervised colorization as a replacement for supervised Im-
ageNet pretraining on the Pascal VOC 2012 semantic segmentation task, with
results on grayscale validation set images. We train colorization from scratch on
ImageNet (Table 5) and ﬁne-tune for Pascal semantic segmentation. We make

14

Larsson, Maire, Shakhnarovich

Initialization RMSE PSNR

Initialization

Architecture X Y C mIU (%)

Classiﬁer
Random

0.299
0.311

24.45
24.25

of

Table 5: ImageNet/cval1k.
ini-
Compares methods
tialization before colorization
training. Hue/chroma with
chromatic fading is used in
both cases (see in Tab. 1).

Classiﬁer
Colorizer
Random

VGG-16
VGG-16
VGG-16

Classiﬁer [9, 30] AlexNet
AlexNet
BiGAN [9]
AlexNet
Inpainter [30]
AlexNet
Random [30]

(cid:51) (cid:51)
(cid:51)

(cid:51) (cid:51) (cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

64.0
50.2
32.5

48.0
34.9
29.7
19.8

Table 6: VOC 2012 segmentation validation
set. Pretraining uses ImageNet images (X), labels
(Y ). VOC 2012 images are in color (C).

the one adjustment of employing cross-validated early stopping to avoid overﬁt-
ting. Table 6 shows this strategy to be promising as a drop-in replacement for
supervised ImageNet pretraining. Self-supervised colorization more than halfway
bridges the gap between random initialization and supervised pretraining.

As VGG-16 is a more performant architecture, comparison with prior work
is not straightforward. Yet, Table 6 still indicates that colorization is a front-
runner among the self-supervision methods, leading to an 18-point improvement
in mIU over the baseline. To our knowledge, 50.2% is the highest reported result
that does not supplement training with additional annotated data [19].

5 Conclusion

We present a system that demonstrates state-of-the-art ability to automatically
colorize grayscale images. Two novel contributions enable this progress: a deep
neural architecture that is trained end-to-end to incorporate semantically mean-
ingful features of varying complexity into colorization, and a color histogram pre-
diction framework that handles uncertainty and ambiguities inherent in coloriza-
tion while preventing jarring artifacts. Our fully automatic colorizer produces
strong results, improving upon previously leading methods by large margins on
all datasets tested; we also propose a new large-scale benchmark for automatic
image colorization, and establish a strong baseline with our method to facilitate
future comparisons. Our colorization results are visually appealing even on com-
plex scenes, and allow for eﬀective post-processing with creative control via color
histogram transfer and intelligent, uncertainty-driven color sampling. We further
reveal colorization as a promising avenue for self-supervised visual learning.

Acknowledgements. We thank Ayan Chakrabarti for suggesting lightness-
normalized quantile matching and for useful discussions, and Aditya Deshpande
and Jason Rock for discussions on their work. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of GPUs for this research.

Learning Representations for Automatic Colorization

15

References

1. Bertasius, G., Shi, J., Torresani, L.: Deepedge: A multi-scale bifurcated deep net-

work for top-down contour detection. In: CVPR (2015)

2. Charpiat, G., Bezrukov, I., Altun, Y., Hofmann, M., Sch¨olkopf, B.: Machine learn-
ing methods for automatic image colorization. In: Computational Photography:
Methods and Applications. CRC Press (2010)

3. Charpiat, G., Hofmann, M., Sch¨olkopf, B.: Automatic image colorization via mul-

timodal predictions. In: ECCV (2008)

4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. In:
ICLR (2015)

5. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: ICCV (2015)
6. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Se-
mantic colorization with internet images. ACM Transactions on Graphics (TOG)
30(6) (2011)

7. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-

orization. In: ICCV (2015)

8. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning
by context prediction. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 1422–1430 (2015)

9. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR

(2017)

10. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,
Courville, A.: Adversarially learned inference. arXiv preprint arXiv:1606.00704
(2016)

11. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013)

12. Ganin, Y., Lempitsky, V.S.: N4-ﬁelds: Neural network nearest neighbor ﬁelds for

13. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward

image transforms. In: ACCV (2014)

neural networks. In: AISTATS (2010)

14. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization
using similar images. In: ACM international conference on Multimedia (2012)
15. Hariharan, B., an R. Girshick, P.A., Malik, J.: Hypercolumns for object segmen-

tation and ﬁne-grained localization. CVPR (2015)

16. Huang, Y.C., Tung, Y.S., Chen, J.C., Wang, S.W., Wu, J.L.: An adaptive edge
detection based colorization algorithm and its applications. In: ACM international
conference on Multimedia (2005)

17. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Let there be Color!: Joint End-to-end
Learning of Global and Local Image Priors for Automatic Image Colorization with
Simultaneous Classiﬁcation. ACM Transactions on Graphics (Proc. of SIGGRAPH
2016) 35(4) (2016)

18. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

19. Ion, A., Carreira, J., Sminchisescu, C.: Probabilistic joint image segmentation and
labeling by ﬁgure-ground composition. International journal of computer vision
107(1), 40–57 (2014)

16

Larsson, Maire, Shakhnarovich

20. Irony, R., Cohen-Or, D., Lischinski, D.: Colorization by example. In: Eurographics

Symp. on Rendering (2005)

21. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

22. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. ACM Trans-

actions on Graphics (TOG) 23(3) (2004)

23. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better. arXiv

24. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

preprint arXiv:1506.04579 (2015)

segmentation. In: CVPR (2015)

25. Luan, Q., Wen, F., Cohen-Or, D., Liang, L., Xu, Y.Q., Shum, H.Y.: Natural image

colorization. In: Eurographics conference on Rendering Techniques (2007)

26. Maire, M., Yu, S.X., Perona, P.: Reconstructive sparse code transfer for contour

detection and semantic labeling. In: ACCV (2014)

27. Morimoto, Y., Taguchi, Y., Naemura, T.: Automatic colorization of grayscale im-

ages using multiple images on the web. In: SIGGRAPH: Posters (2009)

28. Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg-

mentation with zoom-out features. In: CVPR (2015)

29. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving

jigsaw puzzles. In: ECCV (2016)

30. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR (2016)

31. Patterson, G., Xu, C., Su, H., Hays, J.: The sun attribute database: Beyond cat-
egories for deeper scene understanding. International Journal of Computer Vision
108(1-2) (2014)

32. Qu, Y., Wong, T.T., Heng, P.A.: Manga colorization. ACM Transactions on Graph-

ics (TOG) 25(3) (2006)

33. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3) (2015)

34. Sapiro, G.: Inpainting the colors. In: ICIP (2005)
35. Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z.: Deepcontour: A deep convo-
lutional feature learned by positive-sharing loss for contour detection. In: CVPR
(2015)

36. Shi, J., Malik, J.: Normalized cuts and image segmentation. Pattern Analysis and

Machine Intelligence, IEEE Transactions on 22(8) (2000)

37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015)

38. S`ykora, D., Buri´anek, J., ˇZ´ara, J.: Unsupervised colorization of black-and-white
cartoons. In: International symposium on Non-photorealistic animation and ren-
dering (2004)

39. Tai, Y.W., Jia, J., Tang, C.K.: Local color transfer via probabilistic segmentation

by expectation-maximization. In: CVPR (2005)

40. Tola, E., Lepetit, V., Fua, P.: A fast local descriptor for dense matching. In: CVPR

(2008)

41. Tsaftaris, S.A., Casadio, F., Andral, J.L., Katsaggelos, A.K.: A novel visualization
tool for art history and conservation: Automated colorization of black and white
archival photographs of works of art. Studies in Conservation 59(3) (2014)

Learning Representations for Automatic Colorization

17

42. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.

ACM Transactions on Graphics (TOG) 21(3) (2002)

43. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale

scene recognition from abbey to zoo. In: CVPR (2010)

44. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)
45. Yatziv, L., Sapiro, G.: Fast image and video colorization using chrominance blend-

ing. Image Processing, IEEE Transactions on 15(5) (2006)

46. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV (2016)

18

Larsson, Maire, Shakhnarovich

Hue

Chroma

Hue

Chroma

Output: Color Image

Ground-truth

Hue

Chroma

Fig. 10: Histogram predictions. Example of predicted hue/chroma his-
tograms.

Appendix A provides additional training and evaluation details. This is followed
by more results and examples in Appendix B.

A Supplementary details

A.1 Re-balancing

To adjust the scale of the activations of layer l by factor m, without changing
any other layer’s activation, the weights W and the bias b are updated according
to:

Wl ← mWl

bl ← mbl

Wl+1 ←

Wl+1

(6)

1
m

The activation of xl+1 becomes:

xl+1 =

Wl+1ReLU(mWlxl + mbl) + bl+1

(7)

1
m

The m inside the ReLU will not aﬀect whether or not a value is rectiﬁed, so the
two cases remain the same: (1) negative: the activation will be the corresponding
feature in bl+1 regardless of m, and (2) positive: the ReLU becomes the identity
function and m and 1

m cancel to get back the original activation.

We set m =

1√

, estimated for each layer separately.

ˆE[X 2]

A.2 Color space αβ

The color channels αβ (“ab” in [7]) are calculated as

α =

B − 1

2 (R + G)
L + (cid:15)

β =

R − G
L + (cid:15)

(8)

where (cid:15) = 0.0001, R, G, B ∈ [0, 1] and L = R+G+B

.3

3

3 We know that this is how Deshpande et al. [7] calculate it based on their code release.

Learning Representations for Automatic Colorization

19

Output: Color Image

Hue

Chroma

Hue

Chroma

Ground-truth

Hue

Chroma

Fig. 11: Histogram predictions. Example of predicted hue/chroma his-
tograms.

A.3 Error metrics

For M images, each image m with Nm pixels, we calculate the error metrics as:

RMSE =

(cid:80)M

1
m=1 Nm
Nm(cid:88)
M
(cid:88)

1
M

m=1

n=1

M
(cid:88)

Nm(cid:88)

m=1

n=1

(cid:114)(cid:13)
(cid:104)
y(m)
(cid:13)
(cid:13)
αβ

(cid:105)

n

(cid:105)

(cid:104)
ˆy(m)
αβ

−

n

(cid:13)
2
(cid:13)
(cid:13)

(cid:33)

(cid:32)

(cid:107)y(m)

RGB − ˆy(m)
3Nm

RGB(cid:107)2

PSNR =

−10 · log10

(9)

(10)

Where y(m)

αβ ∈ [−3, 3]Nm×2 and y(m)

RGB ∈ [0, 1]Nm×3 for all m.

A.4 Lightness correction

Ideally the lightness L is an unaltered pass-through channel. However, due to
subtle diﬀerences in how L is deﬁned, it is possible that the lightness of the
predicted image, ˆL, does not agree with the input, L. To compensate for this,
we add L− ˆL to all color channels in the predicted RGB image as a ﬁnal corrective
step.

B Supplementary results

B.1 Validation

A more detailed list of validation results for hue/chroma inference methods is
seen in Table 7.

20

Larsson, Maire, Shakhnarovich

Hue

Chroma

CF RMSE PSNR

Sample
Mode

0.426
Sample
0.304
Mode
Expectation Expectation
0.374
Expectation Expectation (cid:51) 0.307
0.342
Expectation Median
Expectation Median

21.41
23.90
23.13
24.35
23.77
(cid:51) 0.299 24.45

7:

Table
ImageNet/cval1k.
Comparison of various histogram
inference methods for hue/chroma.
Mode/mode does fairly well but
has severe visual artifacts. (CF =
Chromatic fading)

B.2 Examples

We provide additional samples for global biasing (Figure 12) and SUN-6 (Fig-
ure 13). Comparisons with Charpiat et al. [2] appear in Figures 14 and 15.
Examples of how our algorithm can bring old photographs to life in Figure 16.
More examples on ImageNet (ctest10k) in Figures 17 to 20 and Figure 21 (failure
cases). Examples of histogram predictions in Figures 10 and 11.

C Document changelog

Overview of document revisions:

v1 Initial release.

v2 ECCV 2016 camera-ready version. Includes discussion about concurrent work
and new experiments using colorization to learn visual representations (Sec-
tion 4.1).

v3 Added overlooked reference.

Learning Representations for Automatic Colorization

21

Fig. 12: Sampling multiple colorizations. From left: graylevel input; three coloriza-
tions sampled from our model; color uncertainty map according to our model.

22

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 13: SUN-6. Additional qualitative comparisons.

Reference Image

Input

Charpiat et al. [2]

Our Method
(Energy Minimization)

Fig. 14: Transfer. Comparison with Charpiat et al. [2] with reference image.
Their method works fairly well when the reference image closely matches (com-
pare with Figure 15). However, they still present sharp unnatural color edges. We
apply our histogram transfer method (Energy Minimization) using the reference
image.

Learning Representations for Automatic Colorization

23

Input

Charpiat et al. [2]

Our Method

Ground-truth

Fig. 15: Portraits. Comparison with Charpiat et al. [2], a transfer-based method
using 53 reference portrait paintings. Note that their method works signiﬁcantly
worse when the reference images are not hand-picked for each grayscale input
(compare with Figure 14). Our model was not trained speciﬁcally for this task
and we used no reference images.

24

Larsson, Maire, Shakhnarovich

Input

Our Method

Input

Our Method

Fig. 16: B&W photographs. Old photographs that were automatically col-
orized. (Source: Library of Congress, www.loc.gov)

Learning Representations for Automatic Colorization

25

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 17: Fully automatic colorization results on ImageNet/ctest10k.

26

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 18: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

27

Fig. 19: Fully automatic colorization results on ImageNet/ctest10k.

28

Larsson, Maire, Shakhnarovich

Fig. 20: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

29

Too Desaturated

Inconsistent Chroma

Inconsistent Hue

Edge Pollution

Color Bleeding

Fig. 21: Failure cases. Examples of the ﬁve most common failure cases: the whole
image lacks saturation (Too Desaturated); inconsistent chroma in objects or regions,
causing parts to be gray (Inconsistent Chroma); inconsistent hue, causing unnatural
color shifts that are particularly typical between red and blue (Inconsistent Hue);
inconsistent hue and chroma around the edge, commonly occurring for closeups where
background context is unclear (Edge Pollution); color boundary is not clearly separated,
causing color bleeding (Color Bleeding).

7
1
0
2
 
g
u
A
 
3
1
 
 
]

V
C
.
s
c
[
 
 
3
v
8
6
6
6
0
.
3
0
6
1
:
v
i
X
r
a

Learning Representations for Automatic Colorization

Gustav Larsson1, Michael Maire2, and Gregory Shakhnarovich2

1University of Chicago

2Toyota Technological Institute at Chicago

larsson@cs.uchicago.edu, {mmaire,greg}@ttic.edu

Abstract. We develop a fully automatic image colorization system. Our
approach leverages recent advances in deep networks, exploiting both
low-level and semantic representations. As many scene elements natu-
rally appear according to multimodal color distributions, we train our
model to predict per-pixel color histograms. This intermediate output
can be used to automatically generate a color image, or further manip-
ulated prior to image formation. On both fully and partially automatic
colorization tasks, we outperform existing methods. We also explore col-
orization as a vehicle for self-supervised visual representation learning.

Fig. 1: Our automatic colorization of grayscale input; more examples in Figs. 3 and 4.

1 Introduction

Colorization of grayscale images is a simple task for the human imagination. A
human need only recall that sky is blue and grass is green; for many objects, the
mind is free to hallucinate several plausible colors. The high-level comprehension
required for this process is precisely why the development of fully automatic col-
orization algorithms remains a challenge. Colorization is thus intriguing beyond
its immediate practical utility in graphics applications. Automatic colorization
serves as a proxy measure for visual understanding. Our work makes this con-
nection explicit; we unify a colorization pipeline with the type of deep neural
architectures driving advances in image classiﬁcation and object detection.

Both our technical approach and focus on fully automatic results depart
from past work. Given colorization’s importance across multiple applications
(e.g. historical photographs and videos [41], artist assistance [32, 38]), much re-
search strives to make it cheaper and less time-consuming [3,5–7,14,20,22,27,42].
However, most methods still require some level of user input [3, 6, 14, 20, 22, 34].
Our work joins the relatively few recent eﬀorts on fully automatic coloriza-
tion [5, 7, 27]. Some [5, 7] show promising results on typical scenes (e.g. land-
scapes), but their success is limited on complex images with foreground objects.

2

Larsson, Maire, Shakhnarovich

VGG-16-Gray

Hypercolumn

Hue

Ground-truth

(fc7) conv7

(fc6) conv6
conv5 3

conv1 1

p

h fc1

Chroma

Lightness

Input: Grayscale Image

Output: Color Image

Fig. 2: System overview. We process a grayscale image through a deep convolu-
tional architecture (VGG) [37] and take spatially localized multilayer slices (hyper-
columns) [15, 26, 28], as per-pixel descriptors. We train our system end-to-end for the
task of predicting hue and chroma distributions for each pixel p given its hypercolumn
descriptor. These predicted distributions determine color assignment at test time.

At a technical level, existing automatic colorization methods often employ
a strategy of ﬁnding suitable reference images and transferring their color onto
a target grayscale image [7, 27]. This works well if suﬃciently similar reference
images can be found, but is diﬃcult for unique grayscale input images. Such a
strategy also requires processing a large repository of reference images at test
time. In contrast, our approach is free of database search and fast at test time.
Section 2 provides a complete view of prior methods, highlighting diﬀerences.

Our approach to automatic colorization converts two intuitive observations
into design principles. First, semantic information matters. In order to colorize
arbitrary images, a system must interpret the semantic composition of the scene
(what is in the image: faces, cars, plants, . . . ) as well as localize objects (where
things are). Deep convolutional neural networks (CNNs) can serve as tools to
incorporate semantic parsing and localization into a colorization system.

Our second observation is that while some scene elements can be assigned
a single color with high conﬁdence, others (e.g. clothes or cars) may draw from
many suitable colors. Thus, we design our system to predict a color histogram,
instead of a single color, at every image location. Figure 2 sketches the CNN
architecture we use to connect semantics with color distributions by exploiting
features across multiple abstraction levels. Section 3 provides details.

Section 4 experimentally validates our algorithm against competing meth-
ods [7, 42] in two settings: fully (grayscale input only) and partially (grayscale
input with reference global color histogram) automatic colorization. Across ev-
ery metric and dataset [31, 33, 43], our method achieves the best performance.
Our system’s fully automatic output is superior to that of prior methods relying
on additional information such as reference images or ground-truth color his-

Learning Representations for Automatic Colorization

3

tograms. To ease the comparison burden for future research, we propose a new
colorization benchmark on ImageNet [33]. We also experiment with colorization
itself as an objective for learning visual representations from scratch, thereby
replacing use of ImageNet pretraining in a traditional semantic labeling task.

Section 5 summarizes our contributions: (1) a novel technical approach to
colorization, bringing semantic knowledge to bear using CNNs, and modeling
color distributions; (2) state-of-the-art performance across fully and partially
automatic colorization tasks; (3) a new ImageNet colorization benchmark; (4)
proof of concept on colorization for self-supervised representation learning.

2 Related work

Previous colorization methods broadly fall into three categories: scribble-based [16,
22,25,32,45], transfer [3,6,14,20,27,39,42], and automatic direct prediction [5,7].
Scribble-based methods, introduced by Levin et al. [22], require manually
specifying desired colors of certain regions. These scribble colors are propagated
under the assumption that adjacent pixels with similar luminance should have
similar color, with the optimization relying on Normalized Cuts [36]. Users can
interactively reﬁne results via additional scribbles. Further advances extend sim-
ilarity to texture [25, 32], and exploit edges to reduce color bleeding [16].

Transfer-based methods rely on availability of related reference image(s),
from which color is transferred to the target grayscale image. Mapping between
source and target is established automatically, using correspondences between
local descriptors [3, 27, 42], or in combination with manual intervention [6, 20].
Excepting [27], reference image selection is at least partially manual.

In contrast to these method families, our goal is fully automatic colorization.
We are aware of two recent eﬀorts in this direction. Deshpande et al. [7] colorize
an entire image by solving a linear system. This can be seen as an extension
of patch-matching techniques [42], adding interaction terms for spatial consis-
tency. Regression trees address the high-dimensionality of the system. Inference
requires an iterative algorithm. Most of the experiments are focused on a dataset
(SUN-6) limited to images of a few scene classes, and best results are obtained
when the scene class is known at test time. They also examine another partially
automatic task, in which a desired global color histogram is provided.

The work of Cheng et al. [5] is perhaps most related to ours. It combines three
levels of features with increasing receptive ﬁeld: the raw image patch, DAISY
features [40], and semantic features [24]. These features are concatenated and
fed into a three-layer fully connected neural network trained with an L2 loss.
Only this last component is optimized; the feature representations are ﬁxed.

Unlike [5,7], our system does not rely on hand-crafted features, is trained end-
to-end, and treats color prediction as a histogram estimation task rather than
as regression. Experiments in Section 4 justify these principles by demonstrating
performance superior to the best reported by [5, 7] across all regimes.

Two concurrent eﬀorts also present feed-forward networks trained end-to-end
for colorization. Iizuka & Simo-Serra et al. [17] propose a network that concate-

4

Larsson, Maire, Shakhnarovich

nates two separate paths, specializing in global and local features, respectively.
This concatenation can be seen as a two-tiered hypercolumn; in comparison, our
16-layer hypercolumn creates a continuum between low- and high-level features.
Their network is trained jointly for classiﬁcation (cross-entropy) and colorization
(L2 loss in Lab). We initialize, but do not anchor, our system to a classiﬁcation-
based network, allowing for ﬁne-tuning of colorization on unlabeled datasets.

Zhang et al. [46] similarly propose predicting color histograms to handle
multi-modality. Some key diﬀerences include their usage of up-convolutional lay-
ers, deep supervision, and dense training. In comparison, we use a fully convolu-
tional approach, with deep supervision implicit in the hypercolumn design, and,
as Section 3 describes, memory-eﬃcient training via spatially sparse samples.

3 Method

We frame the colorization problem as learning a function f : X → Y. Given
a grayscale image patch x ∈ X = [0, 1]S×S, f predicts the color y ∈ Y of its
center pixel. The patch size S × S is the receptive ﬁeld of the colorizer. The
output space Y depends on the choice of color parameterization. We implement
f according to the neural network architecture diagrammed in Figure 2.

Motivating this strategy is the success of similar architectures for semantic
segmentation [4, 11, 15, 24, 28] and edge detection [1, 12, 26, 35, 44]. Together with
colorization, these tasks can all be viewed as image-to-image prediction problems,
in which a value is predicted for each input pixel. Leading methods commonly
adapt deep convolutional neural networks pretrained for image classiﬁcation [33,
37]. Such classiﬁcation networks can be converted to fully convolutional networks
that produce output of the same spatial size as the input, e.g. using the shift-
and-stitch method [24] or the more eﬃcient `a trous algorithm [4]. Subsequent
training with a task-speciﬁc loss ﬁne-tunes the converted network.

Skip-layer connections, which directly link low- and mid-level features to pre-
diction layers, are an architectural addition beneﬁcial for many image-to-image
problems. Some methods implement skip connections directly through concate-
nation layers [4, 24], while others equivalently extract per-pixel descriptors by
reading localized slices of multiple layers [15, 26, 28]. We use this latter strategy
and adopt the recently coined hypercolumn terminology [15] for such slices.

Though we build upon these ideas, our technical approach innovates on two
fronts. First, we integrate domain knowledge for colorization, experimenting with
output spaces and loss functions. We design the network output to serve as an
intermediate representation, appropriate for direct or biased sampling. We intro-
duce an energy minimization procedure for optionally biasing sampling towards a
reference image. Second, we develop a novel and eﬃcient computational strategy
for network training that is widely applicable to hypercolumn architectures.

3.1 Color spaces

We generate training data by converting color images to grayscale according to
L = R+G+B
. This is only one of many desaturation options and chosen primarily

3

Learning Representations for Automatic Colorization

5

to facilitate comparison with Deshpande et al. [7]. For the representation of
color predictions, using RGB is overdetermined, as lightness L is already known.
We instead consider output color spaces with L (or a closely related quantity)
conveniently appearing as a separate pass-through channel:

– Hue/chroma. Hue-based spaces, such as HSL, can be thought of as a color
cylinder, with angular coordinate H (hue), radial distance S (saturation),
and height L (lightness). The values of S and H are unstable at the bot-
tom (black) and top (white) of the cylinder. HSV describes a similar color
cylinder which is only unstable at the bottom. However, L is no longer one
of the channels. We wish to avoid both instabilities and still retain L as a
channel. The solution is a color bicone, where chroma (C) takes the place of
saturation. Conversion to HSV is given by V = L + C

2 , S = C
V .

– Lab and αβ. Lab (or L*a*b) is designed to be perceptually linear. The
color vector (a, b) deﬁnes a Euclidean space where the distance to the origin
determines chroma. Deshpande et al. [7] use a color space somewhat similar
to Lab, denoted “ab”. To diﬀerentiate, we call their color space αβ.

3.2 Loss

For any output color representation, we require a loss function for measuring
prediction errors. A ﬁrst consideration, also used in [5], is L2 regression in Lab:

Lreg(x, y) = (cid:107)f (x) − y(cid:107)2

(1)

(2)

where Y = R2 describes the (a, b) vector space. However, regression targets
do not handle multimodal color distributions well. To address this, we instead
predict distributions over a set of color bins, a technique also used in [3]:

Lhist(x, y) = DKL(y(cid:107)f (x))

where Y = [0, 1]K describes a histogram over K bins, and DKL is the KL-
divergence. The ground-truth histogram y is set as the empirical distribution in
a rectangular region of size R around the center pixel. Somewhat surprisingly,
our experiments see no beneﬁt to predicting smoothed histograms, so we simply
set R = 1. This makes y a one-hot vector and Equation (2) the log loss. For
histogram predictions, the last layer of neural network f is always a softmax.

There are several choices of how to bin color space. We bin the Lab axes
by evenly spaced Gaussian quantiles (µ = 0, σ = 25). They can be encoded
separately for a and b (as marginal distributions), in which case our loss becomes
the sum of two separate terms deﬁned by Equation (2). They can also be encoded
as a joint distribution over a and b, in which case we let the quantiles form a 2D
grid of bins. In our experiments, we set K = 32 for marginal distributions and
K = 16 × 16 for joint. We determined these numbers, along with σ, to oﬀer a
good compromise of output ﬁdelity and output complexity.

For hue/chroma, we only consider marginal distributions and bin axes uni-
formly in [0, 1]. Since hue becomes unstable as chroma approaches zero, we add

6

Larsson, Maire, Shakhnarovich

a sample weight to the hue based on the chroma:

Lhue/chroma(x, y) = DKL(yC(cid:107)fC(x)) + λH yCDKL(yH(cid:107)fH(x))

(3)

where Y = [0, 1]2×K and yC ∈ [0, 1] is the sample pixel’s chroma. We set λH = 5,
roughly the inverse expectation of yC, thus equally weighting hue and chroma.

3.3

Inference

Given network f trained according to a loss function in the previous section, we
evaluate it at every pixel n in a test image: ˆyn = f (xn). For the L2 loss, all that
remains is to combine each ˆyn with the respective lightness and convert to RGB.
With histogram predictions, we consider options for inferring a ﬁnal color:

– Sample Draw a sample from the histogram. If done per pixel, this may
create high-frequency color changes in areas of high-entropy histograms.
– Mode Take the arg maxk ˆyn,k as the color. This can create jarring transitions

between colors, and is prone to vote splitting for proximal centroids.

– Median Compute cumulative sum of ˆyn and use linear interpolation to ﬁnd
the value at the middle bin. Undeﬁned for circular histograms, such as hue.
– Expectation Sum over the color bin centroids weighted by the histogram.

For Lab output, we achieve the best qualitative and quantitative results using
expectations. For hue/chroma, the best results are achieved by taking the median
of the chroma. Many objects can appear both with and without chroma, which
means C = 0 is a particularly common bin. This mode draws the expectation
closer to zero, producing less saturated images. As for hue, since it is circular,
we ﬁrst compute the complex expectation:

z = EH∼fh(x)[H] (cid:44) 1
K

(cid:88)

k

[fh(x)]keiθk ,

θk = 2π

(4)

k + 0.5
K

We then set hue to the argument of z remapped to lie in [0, 1).

In cases where the estimate of the chroma is
high and z is close to zero, the instability of the
hue can create artifacts. A simple, yet eﬀective,
ﬁx is chromatic fading: downweight the chroma if
the absolute value of z is too small. We thus re-
deﬁne the predicted chroma by multiplying it by
a factor of max(η−1|z|, 1). In our experiments, we
set η = 0.03 (obtained via cross-validation).

3.4 Histogram transfer from ground-truth

So far, we have only considered the fully automatic color inference task. Desh-
pande et al. [7], test a separate task where the ground-truth histogram in the

Learning Representations for Automatic Colorization

7

two non-lightness color channels of the original color image is made available.1
In order to compare, we propose two histogram transfer methods. We refer to
the predicted image as the source and the ground-truth image as the target.

Lightness-normalized quantile matching. Divide the RGB representation
of both source and target by their respective lightness. Compute marginal his-
tograms over the resulting three color channels. Alter each source histogram to
ﬁt the corresponding target histogram by quantile matching, and multiply by
lightness. Though it does not exploit our richer color distribution predictions,
quantile matching beats the cluster correspondence method of [7] (see Table 3).

Energy minimization. We phrase histogram matching as minimizing energy:

E =

1
N

(cid:88)

n

DKL(ˆy∗

n(cid:107)ˆyn) + λDχ2((cid:104) ˆy∗(cid:105), t)

(5)

where N is the number of pixels, ˆy, ˆy∗ ∈ [0, 1]N ×K are the predicted and poste-
rior distributions, respectively. The target histogram is denoted by t ∈ [0, 1]K.
The ﬁrst term contains unary potentials that anchor the posteriors to the pre-
dictions. The second term is a symmetric χ2 distance to promote proximity
between source and target histograms. Weight λ deﬁnes relative importance of
histogram matching. We estimate the source histogram as (cid:104)ˆy∗(cid:105) = 1
n. We
N
parameterize the posterior for all pixels n as: ˆy∗
n = softmax(log ˆyn + b), where
the vector b ∈ RK can be seen as a global bias for each bin. It is also possible
to solve for the posteriors directly; this does not perform better quantitatively
and is more prone to introducing artifacts. We solve for b using gradient descent
on E and use the resulting posteriors in place of the predictions. In the case of
marginal histograms, the optimization is run twice, once for each color channel.

n ˆy∗

(cid:80)

3.5 Neural network architecture and training

Our base network is a fully convolutional version of VGG-16 [37] with two
changes: (1) the classiﬁcation layer (fc8) is discarded, and (2) the ﬁrst ﬁlter
layer (conv1 1) operates on a single intensity channel instead of mean-subtracted
RGB. We extract a hypercolumn descriptor for a pixel by concatenating the fea-
tures at its spatial location in all layers, from data to conv7 (fc7), resulting in a
12, 417 channel descriptor. We feed this hypercolumn into a fully connected layer
with 1024 channels (h fc1 in Figure 2), to which we connect output predictors.
Processing each pixel separately in such manner is quite costly. We instead
run an entire image through a single forward pass of VGG-16 and approximate
hypercolumns using bilinear interpolation. Even with such sharing, densely ex-
tracting hypercolumns requires signiﬁcant memory (1.7 GB for 256 × 256 input).
To ﬁt image batches in memory during training, we instead extract hyper-
columns at only a sparse set of locations, implementing a custom Caﬀe [21] layer

1 Note that if the histogram of the L channel were available, it would be possible to
match lightness to lightness exactly and thus greatly narrow down color placement.

8

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 3: Fully automatic colorization results on ImageNet/ctest10k. Our sys-
tem reproduces known object color properties (e.g. faces, sky, grass, fruit, wood), and
coherently picks colors for objects without such properties (e.g. clothing).

to directly compute them.2 Extracting batches of only 128 hypercolumn descrip-
tors per input image, sampled at random locations, provides suﬃcient training
signal. In the backward pass of stochastic gradient descent, an interpolated hy-
percolumn propagates its gradients to the four closest spatial cells in each layer.
Locks ensure atomicity of gradient updates, without incurring any performance
penalty. This drops training memory for hypercolumns to only 13 MB per image.
We initialize with a version of VGG-16 pretrained on ImageNet, adapting
it to grayscale by averaging over color channels in the ﬁrst layer and rescaling
appropriately. Prior to training for colorization, we further ﬁne-tune the network
for one epoch on the ImageNet classiﬁcation task with grayscale input. As the
original VGG-16 was trained without batch normalization [18], scale of responses
in internal layers can vary dramatically, presenting a problem for learning atop
their hypercolumn concatenation. Liu et al. [23] compensate for such variability
by applying layer-wise L2 normalization. We use the alternative of balancing
hypercolumns so that each layer has roughly unit second moment (E[X 2] ≈ 1);
Appendix (Section A.1) provides additional details.

2 https://github.com/gustavla/autocolorize

Learning Representations for Automatic Colorization

9

Fig. 4: Additional results. Top: Our automatic colorizations of these ImageNet ex-
amples are diﬃcult to distinguish from real color images. Bottom: B&W photographs.

4 Experiments

Starting from pretrained VGG-16-Gray, described in the previous section, we
attach h fc1 and output prediction layers with Xavier initialization [13], and
ﬁne-tune the entire system for colorization. We consider multiple prediction layer
variants: Lab output with L2 loss, and both Lab and hue/chroma marginal or
joint histogram output with losses according to Equations (2) and (3). We train
each system variant end-to-end for one epoch on the 1.2 million images of the
ImageNet training set, each resized to at most 256 pixels in smaller dimension.
A single epoch takes approximately 17 hours on a GTX Titan X GPU. At test
time, colorizing a single 512 × 512 pixel image takes 0.5 seconds.

We setup two disjoint subsets of the ImageNet validation data for our own
use: 1000 validation images (cval1k) and 10000 test images (ctest10k). Each
set has a balanced representation for ImageNet categories, and excludes any
images encoded as grayscale, but may include images that are naturally grayscale

10

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 5: SUN-6. GT Scene: test image scene class is available. GT Hist: test image color
histogram is available. We obtain colorizations with visual quality better than those
from prior work, even though we do not exploit reference images or known scene class.
Our energy minimization method (Section 3.4) for GT Hist further improves results.
In either mode, our method appears less dependent on spatial priors: note splitting of
the sky in the ﬁrst row and correlation of green with actual grass in the last row.

Model\Metric

RMSE

PSNR

Model\Metric

RMSE

PSNR

No colorization
Lab, L2
Lab, K = 32
Lab, K = 16 × 16
Hue/chroma, K = 32
+ chromatic fading

0.343
0.318
0.321
0.328
0.342
0.299

22.98
24.25
24.33
24.30
23.77
24.45

data..fc7
data..conv5 3
conv4 1..fc7
conv5 1..fc7
fc6..fc7
fc7

0.299
0.306
0.302
0.307
0.323
0.324

24.45
24.13
24.45
24.38
24.22
24.19

Table 1:
ImageNet/cval1k. Validation
performance of system variants. Hue/chroma
is best, but only with chromatic fading.

Table 2: ImageNet/cval1k. Abla-
tion study of hypercolumn compo-
nents.

(e.g. closeup of nuts and bolts), where an algorithm should know not to add color.
Category labels are discarded; only images are available at test time. We propose
ctest10k as a standard benchmark with the following metrics:

– RMSE: root mean square error in αβ averaged over all pixels [7].
– PSNR: peak signal-to-noise ratio in RGB calculated per image [5]. We use
the arithmetic mean of PSNR over images, instead of the geometric mean
as in Cheng et al. [5]; geometric mean is overly sensitive to outliers.

By virtue of comparing to ground-truth color images, quantitative colorization
metrics can penalize reasonable, but incorrect, color guesses for many objects
(e.g. red car instead of blue car) more than jarring artifacts. This makes quali-
tative results for colorization as important as quantitative; we report both.

Figures 1, 3, and 4 show example test results of our best system variant,
selected according to performance on the validation set and trained for a total

Learning Representations for Automatic Colorization

11

Method

RMSE

Method

Grayscale (no colorization) 0.285
0.353
Welsh et al. [42]
0.262
Deshpande et al. [7]
0.254
0.211

Our Method

+ GT Scene

Deshpande et al. (C) [7]
Deshpande et al. (Q)
Our Method (Q)
Our Method (E)

RMSE

0.236
0.211
0.178
0.165

Table 3: SUN-6. Comparison
with competing methods.

Table 4: SUN-6 (GT Hist). Comparison us-
ing ground-truth histograms. Results for Desh-
pande et al. [7] use GT Scene.

of 10 epochs. This variant predicts hue and chroma and uses chromatic fading
during image generation. Table 1 provides validation benchmarks for all sys-
tem variants, including the trivial baseline of no colorization. On ImageNet test
(ctest10k), our selected model obtains 0.293 (RMSE, αβ, avg/px) and 24.94 dB
(PSNR, RGB, avg/im), compared to 0.333 and 23.27 dB for the baseline.

Table 2 examines the importance of diﬀerent neural network layers to col-
orization; it reports validation performance of ablated systems that include only
the speciﬁed subsets of layers in the hypercolumn used to predict hue and
chroma. Some lower layers may be discarded without much performance loss,
yet higher layers alone (fc6..fc7) are insuﬃcient for good colorization.

Our ImageNet colorization benchmark is new to a ﬁeld lacking an estab-
lished evaluation protocol. We therefore focus on comparisons with two recent
papers [5, 7], using their self-deﬁned evaluation criteria. To do so, we run our
ImageNet-trained hue and chroma model on two additional datasets:

– SUN-A [31] is a subset of the SUN dataset [43] containing 47 object cate-
gories. Cheng et al. [5] train a colorization system on 2688 images and report
results on 1344 test images. We were unable to obtain the list of test im-
ages, and therefore report results averaged over ﬁve random subsets of 1344
SUN-A images. We do not use any SUN-A images for training.

– SUN-6, another SUN subset, used by Deshpande et al. [7], includes images
from 6 scene categories (beach, castle, outdoor, kitchen, living room, bed-
room). We compare our results on 240 test images to those reported in [7]
for their method as well as for Welsh et al. [42] with automatically matched
reference images as in [27]. Following [7], we consider another evaluation
regime in which ground-truth target color histograms are available.

Figure 5 shows a comparison of results on SUN-6. Forgoing usage of ground-truth
global histograms, our fully automatic system produces output qualitatively su-
perior to methods relying on such side information. Tables 3 and 4 report quan-
titative performance corroborating this view. The partially automatic systems
in Table 4 adapt output to ﬁt global histograms using either: (C) cluster corre-
spondences [7], (Q) quantile matching, or (E) our energy minimization described
in Section 3.4. Our quantile matching results are superior to those of [7] and our
new energy minimization procedure oﬀers further improvement.

12

Larsson, Maire, Shakhnarovich

Fig. 6: SUN-6. Cumulative histogram
of per pixel error (higher=more pix-
els with lower error). Results for Desh-
pande et al. [7] use GT Scene.

Fig. 7: SUN-A. Histogram of per-image
PSNR for [5] and our method. The highest
geometric mean PSNR reported for experi-
ments in [5] is 24.2, vs. 32.7±2.0 for us.

Figures 6 and 7 compare error distributions on SUN-6 and SUN-A. As in
Table 3, our fully automatic method dominates all competing approaches, even
those which use auxiliary information. It is only outperformed by the version
of itself augmented with ground-truth global histograms. On SUN-A, Figure 7
shows clear separation between our method and [5] on per-image PSNR.

The Appendix (Figures 14 and 15) provides anecdotal comparisons to one
additional method, that of Charpiat et al. [2], which can be considered an auto-
matic system if reference images are available. Unfortunately, source code of [2]
is not available and reported time cost is prohibitive for large-scale evaluation
(30 minutes per image). We were thus unable to benchmark [2] on large datasets.
With regard to concurrent work, Zhang et al. [46] include a comparison of our
results to their own. The two systems are competitive in terms of quantitative
measures of colorization accuracy. Their system, set to produce more vibrant
colors, has an advantage in terms of human-measured preferences. In contrast,
an oﬀ-the-shelf VGG-16 network for image classiﬁcation, consuming our system’s
color output, more often produces correct labels, suggesting a realism advantage.
We refer interested readers to [46] for the full details of this comparison.

Though we achieve signiﬁcant improvements over prior state-of-the-art, our
results are not perfect. Figure 8 shows examples of signiﬁcant failures. Minor im-
perfections are also present in some of the results in Figures 3 and 4. We believe
a common failure mode correlates with gaps in semantic interpretation: incor-
rectly identiﬁed or unfamiliar objects and incorrect segmentation. In addition,
there are “mistakes” due to natural uncertainty of color – e.g. the graduation
robe at the bottom right of Figure 3 is red, but could as well be purple.

Since our method produces histograms, we can provide interactive means of
biasing colorizations according to user preferences. Rather than output a single
color per pixel, we can sample color for image regions and evaluate color uncer-
tainty. Speciﬁcally, solving our energy minimization formulation (Equation (5))
with global biases b that are not optimized based on a reference image, but sim-
ply “rotated” through color space, induces changed color preferences throughout
the image. The uncertainty in the predicted histogram modulates this eﬀect.

Learning Representations for Automatic Colorization

13

Fig. 8: Failure modes. Top row, left-to-right: texture confusion, too homogeneous,
color bleeding, unnatural color shifts (×2). Bottom row: inconsistent background, in-
consistent chromaticity, not enough color, object not recognized (upside down face
partly gray), context confusion (sky).

Fig. 9: Sampling colorizations. Left: Image & 3 samples; Right: Uncertainty map.

Figure 9 shows multiple sampled colorizations, together with a visualization
of uncertainty. Here, uncertainty is the entropy of the predicted hue multiplied
by the chroma. Our distributional output and energy minimization framework
open the path for future investigation of human-in-the-loop colorization tools.

4.1 Representation learning

High-level visual understanding is essential for the colorization of grayscale im-
ages, motivating our use of an ImageNet pretrained network as a starting point.
But with enough training data, perhaps we can turn this around and use col-
orization as means of learning networks for capturing high-level visual represen-
tations. Table 5 shows that a colorization network, trained from scratch using
only unlabeled color images, is surprisingly competitive. It converges slower, but
requires not more than twice the number of epochs.

Our preliminary work shows that the networks learned via training coloriza-
tion from scratch generalize well to other visual tasks. This is signiﬁcant because
such training requires no human annotation eﬀort. It follows a recent trend
of learning representations through self-supervision (e.g. context prediction [8],
solving jigsaw puzzles [29], inpainting [30], adversarial feature learning [9, 10]).
We examine self-supervised colorization as a replacement for supervised Im-
ageNet pretraining on the Pascal VOC 2012 semantic segmentation task, with
results on grayscale validation set images. We train colorization from scratch on
ImageNet (Table 5) and ﬁne-tune for Pascal semantic segmentation. We make

14

Larsson, Maire, Shakhnarovich

Initialization RMSE PSNR

Initialization

Architecture X Y C mIU (%)

Classiﬁer
Random

0.299
0.311

24.45
24.25

of

Table 5: ImageNet/cval1k.
ini-
Compares methods
tialization before colorization
training. Hue/chroma with
chromatic fading is used in
both cases (see in Tab. 1).

Classiﬁer
Colorizer
Random

VGG-16
VGG-16
VGG-16

Classiﬁer [9, 30] AlexNet
AlexNet
BiGAN [9]
AlexNet
Inpainter [30]
AlexNet
Random [30]

(cid:51) (cid:51)
(cid:51)

(cid:51) (cid:51) (cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

64.0
50.2
32.5

48.0
34.9
29.7
19.8

Table 6: VOC 2012 segmentation validation
set. Pretraining uses ImageNet images (X), labels
(Y ). VOC 2012 images are in color (C).

the one adjustment of employing cross-validated early stopping to avoid overﬁt-
ting. Table 6 shows this strategy to be promising as a drop-in replacement for
supervised ImageNet pretraining. Self-supervised colorization more than halfway
bridges the gap between random initialization and supervised pretraining.

As VGG-16 is a more performant architecture, comparison with prior work
is not straightforward. Yet, Table 6 still indicates that colorization is a front-
runner among the self-supervision methods, leading to an 18-point improvement
in mIU over the baseline. To our knowledge, 50.2% is the highest reported result
that does not supplement training with additional annotated data [19].

5 Conclusion

We present a system that demonstrates state-of-the-art ability to automatically
colorize grayscale images. Two novel contributions enable this progress: a deep
neural architecture that is trained end-to-end to incorporate semantically mean-
ingful features of varying complexity into colorization, and a color histogram pre-
diction framework that handles uncertainty and ambiguities inherent in coloriza-
tion while preventing jarring artifacts. Our fully automatic colorizer produces
strong results, improving upon previously leading methods by large margins on
all datasets tested; we also propose a new large-scale benchmark for automatic
image colorization, and establish a strong baseline with our method to facilitate
future comparisons. Our colorization results are visually appealing even on com-
plex scenes, and allow for eﬀective post-processing with creative control via color
histogram transfer and intelligent, uncertainty-driven color sampling. We further
reveal colorization as a promising avenue for self-supervised visual learning.

Acknowledgements. We thank Ayan Chakrabarti for suggesting lightness-
normalized quantile matching and for useful discussions, and Aditya Deshpande
and Jason Rock for discussions on their work. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of GPUs for this research.

Learning Representations for Automatic Colorization

15

References

1. Bertasius, G., Shi, J., Torresani, L.: Deepedge: A multi-scale bifurcated deep net-

work for top-down contour detection. In: CVPR (2015)

2. Charpiat, G., Bezrukov, I., Altun, Y., Hofmann, M., Sch¨olkopf, B.: Machine learn-
ing methods for automatic image colorization. In: Computational Photography:
Methods and Applications. CRC Press (2010)

3. Charpiat, G., Hofmann, M., Sch¨olkopf, B.: Automatic image colorization via mul-

timodal predictions. In: ECCV (2008)

4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. In:
ICLR (2015)

5. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: ICCV (2015)
6. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Se-
mantic colorization with internet images. ACM Transactions on Graphics (TOG)
30(6) (2011)

7. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-

orization. In: ICCV (2015)

8. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning
by context prediction. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 1422–1430 (2015)

9. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR

(2017)

10. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,
Courville, A.: Adversarially learned inference. arXiv preprint arXiv:1606.00704
(2016)

11. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013)

12. Ganin, Y., Lempitsky, V.S.: N4-ﬁelds: Neural network nearest neighbor ﬁelds for

13. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward

image transforms. In: ACCV (2014)

neural networks. In: AISTATS (2010)

14. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization
using similar images. In: ACM international conference on Multimedia (2012)
15. Hariharan, B., an R. Girshick, P.A., Malik, J.: Hypercolumns for object segmen-

tation and ﬁne-grained localization. CVPR (2015)

16. Huang, Y.C., Tung, Y.S., Chen, J.C., Wang, S.W., Wu, J.L.: An adaptive edge
detection based colorization algorithm and its applications. In: ACM international
conference on Multimedia (2005)

17. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Let there be Color!: Joint End-to-end
Learning of Global and Local Image Priors for Automatic Image Colorization with
Simultaneous Classiﬁcation. ACM Transactions on Graphics (Proc. of SIGGRAPH
2016) 35(4) (2016)

18. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

19. Ion, A., Carreira, J., Sminchisescu, C.: Probabilistic joint image segmentation and
labeling by ﬁgure-ground composition. International journal of computer vision
107(1), 40–57 (2014)

16

Larsson, Maire, Shakhnarovich

20. Irony, R., Cohen-Or, D., Lischinski, D.: Colorization by example. In: Eurographics

Symp. on Rendering (2005)

21. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

22. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. ACM Trans-

actions on Graphics (TOG) 23(3) (2004)

23. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better. arXiv

24. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

preprint arXiv:1506.04579 (2015)

segmentation. In: CVPR (2015)

25. Luan, Q., Wen, F., Cohen-Or, D., Liang, L., Xu, Y.Q., Shum, H.Y.: Natural image

colorization. In: Eurographics conference on Rendering Techniques (2007)

26. Maire, M., Yu, S.X., Perona, P.: Reconstructive sparse code transfer for contour

detection and semantic labeling. In: ACCV (2014)

27. Morimoto, Y., Taguchi, Y., Naemura, T.: Automatic colorization of grayscale im-

ages using multiple images on the web. In: SIGGRAPH: Posters (2009)

28. Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg-

mentation with zoom-out features. In: CVPR (2015)

29. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving

jigsaw puzzles. In: ECCV (2016)

30. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR (2016)

31. Patterson, G., Xu, C., Su, H., Hays, J.: The sun attribute database: Beyond cat-
egories for deeper scene understanding. International Journal of Computer Vision
108(1-2) (2014)

32. Qu, Y., Wong, T.T., Heng, P.A.: Manga colorization. ACM Transactions on Graph-

ics (TOG) 25(3) (2006)

33. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3) (2015)

34. Sapiro, G.: Inpainting the colors. In: ICIP (2005)
35. Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z.: Deepcontour: A deep convo-
lutional feature learned by positive-sharing loss for contour detection. In: CVPR
(2015)

36. Shi, J., Malik, J.: Normalized cuts and image segmentation. Pattern Analysis and

Machine Intelligence, IEEE Transactions on 22(8) (2000)

37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015)

38. S`ykora, D., Buri´anek, J., ˇZ´ara, J.: Unsupervised colorization of black-and-white
cartoons. In: International symposium on Non-photorealistic animation and ren-
dering (2004)

39. Tai, Y.W., Jia, J., Tang, C.K.: Local color transfer via probabilistic segmentation

by expectation-maximization. In: CVPR (2005)

40. Tola, E., Lepetit, V., Fua, P.: A fast local descriptor for dense matching. In: CVPR

(2008)

41. Tsaftaris, S.A., Casadio, F., Andral, J.L., Katsaggelos, A.K.: A novel visualization
tool for art history and conservation: Automated colorization of black and white
archival photographs of works of art. Studies in Conservation 59(3) (2014)

Learning Representations for Automatic Colorization

17

42. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.

ACM Transactions on Graphics (TOG) 21(3) (2002)

43. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale

scene recognition from abbey to zoo. In: CVPR (2010)

44. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)
45. Yatziv, L., Sapiro, G.: Fast image and video colorization using chrominance blend-

ing. Image Processing, IEEE Transactions on 15(5) (2006)

46. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV (2016)

18

Larsson, Maire, Shakhnarovich

Hue

Chroma

Hue

Chroma

Output: Color Image

Ground-truth

Hue

Chroma

Fig. 10: Histogram predictions. Example of predicted hue/chroma his-
tograms.

Appendix A provides additional training and evaluation details. This is followed
by more results and examples in Appendix B.

A Supplementary details

A.1 Re-balancing

To adjust the scale of the activations of layer l by factor m, without changing
any other layer’s activation, the weights W and the bias b are updated according
to:

Wl ← mWl

bl ← mbl

Wl+1 ←

Wl+1

(6)

1
m

The activation of xl+1 becomes:

xl+1 =

Wl+1ReLU(mWlxl + mbl) + bl+1

(7)

1
m

The m inside the ReLU will not aﬀect whether or not a value is rectiﬁed, so the
two cases remain the same: (1) negative: the activation will be the corresponding
feature in bl+1 regardless of m, and (2) positive: the ReLU becomes the identity
function and m and 1

m cancel to get back the original activation.

We set m =

1√

, estimated for each layer separately.

ˆE[X 2]

A.2 Color space αβ

The color channels αβ (“ab” in [7]) are calculated as

α =

B − 1

2 (R + G)
L + (cid:15)

β =

R − G
L + (cid:15)

(8)

where (cid:15) = 0.0001, R, G, B ∈ [0, 1] and L = R+G+B

.3

3

3 We know that this is how Deshpande et al. [7] calculate it based on their code release.

Learning Representations for Automatic Colorization

19

Output: Color Image

Hue

Chroma

Hue

Chroma

Ground-truth

Hue

Chroma

Fig. 11: Histogram predictions. Example of predicted hue/chroma his-
tograms.

A.3 Error metrics

For M images, each image m with Nm pixels, we calculate the error metrics as:

RMSE =

(cid:80)M

1
m=1 Nm
Nm(cid:88)
M
(cid:88)

1
M

m=1

n=1

M
(cid:88)

Nm(cid:88)

m=1

n=1

(cid:114)(cid:13)
(cid:104)
y(m)
(cid:13)
(cid:13)
αβ

(cid:105)

n

(cid:105)

(cid:104)
ˆy(m)
αβ

−

n

(cid:13)
2
(cid:13)
(cid:13)

(cid:33)

(cid:32)

(cid:107)y(m)

RGB − ˆy(m)
3Nm

RGB(cid:107)2

PSNR =

−10 · log10

(9)

(10)

Where y(m)

αβ ∈ [−3, 3]Nm×2 and y(m)

RGB ∈ [0, 1]Nm×3 for all m.

A.4 Lightness correction

Ideally the lightness L is an unaltered pass-through channel. However, due to
subtle diﬀerences in how L is deﬁned, it is possible that the lightness of the
predicted image, ˆL, does not agree with the input, L. To compensate for this,
we add L− ˆL to all color channels in the predicted RGB image as a ﬁnal corrective
step.

B Supplementary results

B.1 Validation

A more detailed list of validation results for hue/chroma inference methods is
seen in Table 7.

20

Larsson, Maire, Shakhnarovich

Hue

Chroma

CF RMSE PSNR

Sample
Mode

0.426
Sample
0.304
Mode
Expectation Expectation
0.374
Expectation Expectation (cid:51) 0.307
0.342
Expectation Median
Expectation Median

21.41
23.90
23.13
24.35
23.77
(cid:51) 0.299 24.45

7:

Table
ImageNet/cval1k.
Comparison of various histogram
inference methods for hue/chroma.
Mode/mode does fairly well but
has severe visual artifacts. (CF =
Chromatic fading)

B.2 Examples

We provide additional samples for global biasing (Figure 12) and SUN-6 (Fig-
ure 13). Comparisons with Charpiat et al. [2] appear in Figures 14 and 15.
Examples of how our algorithm can bring old photographs to life in Figure 16.
More examples on ImageNet (ctest10k) in Figures 17 to 20 and Figure 21 (failure
cases). Examples of histogram predictions in Figures 10 and 11.

C Document changelog

Overview of document revisions:

v1 Initial release.

v2 ECCV 2016 camera-ready version. Includes discussion about concurrent work
and new experiments using colorization to learn visual representations (Sec-
tion 4.1).

v3 Added overlooked reference.

Learning Representations for Automatic Colorization

21

Fig. 12: Sampling multiple colorizations. From left: graylevel input; three coloriza-
tions sampled from our model; color uncertainty map according to our model.

22

Larsson, Maire, Shakhnarovich

Grayscale only

yGT Sceney

GT Scene & Hist

Grayscale only

GT Histogram

Ground-truth

Welsh et al. [42]

Deshpande et al. [7]

Our Method

Fig. 13: SUN-6. Additional qualitative comparisons.

Reference Image

Input

Charpiat et al. [2]

Our Method
(Energy Minimization)

Fig. 14: Transfer. Comparison with Charpiat et al. [2] with reference image.
Their method works fairly well when the reference image closely matches (com-
pare with Figure 15). However, they still present sharp unnatural color edges. We
apply our histogram transfer method (Energy Minimization) using the reference
image.

Learning Representations for Automatic Colorization

23

Input

Charpiat et al. [2]

Our Method

Ground-truth

Fig. 15: Portraits. Comparison with Charpiat et al. [2], a transfer-based method
using 53 reference portrait paintings. Note that their method works signiﬁcantly
worse when the reference images are not hand-picked for each grayscale input
(compare with Figure 14). Our model was not trained speciﬁcally for this task
and we used no reference images.

24

Larsson, Maire, Shakhnarovich

Input

Our Method

Input

Our Method

Fig. 16: B&W photographs. Old photographs that were automatically col-
orized. (Source: Library of Congress, www.loc.gov)

Learning Representations for Automatic Colorization

25

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 17: Fully automatic colorization results on ImageNet/ctest10k.

26

Larsson, Maire, Shakhnarovich

Input

Our Method

Ground-truth

Input

Our Method

Ground-truth

Fig. 18: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

27

Fig. 19: Fully automatic colorization results on ImageNet/ctest10k.

28

Larsson, Maire, Shakhnarovich

Fig. 20: Fully automatic colorization results on ImageNet/ctest10k.

Learning Representations for Automatic Colorization

29

Too Desaturated

Inconsistent Chroma

Inconsistent Hue

Edge Pollution

Color Bleeding

Fig. 21: Failure cases. Examples of the ﬁve most common failure cases: the whole
image lacks saturation (Too Desaturated); inconsistent chroma in objects or regions,
causing parts to be gray (Inconsistent Chroma); inconsistent hue, causing unnatural
color shifts that are particularly typical between red and blue (Inconsistent Hue);
inconsistent hue and chroma around the edge, commonly occurring for closeups where
background context is unclear (Edge Pollution); color boundary is not clearly separated,
causing color bleeding (Color Bleeding).

