9
1
0
2
 
r
p
A
 
7
2
 
 
]

G
L
.
s
c
[
 
 
5
v
7
9
6
1
0
.
7
0
8
1
:
v
i
X
r
a

Benchmarking Neural Network Robustness to
Common Corruptions and Surface Variations

Dan Hendrycks
University of California, Berkeley
hendrycks@berkeley.edu

Thomas G. Dietterich
Oregon State University
tgd@oregonstate.edu

Abstract

In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our
ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness
topic, while showing which classiﬁers are preferable in safety-critical applications.
Unlike recent robustness research, this benchmark evaluates performance on com-
monplace corruptions not worst-case adversarial corruptions. We ﬁnd that there are
negligible changes in relative corruption robustness from AlexNet to ResNet classi-
ﬁers, and we discover ways to enhance corruption robustness. Then we propose a
new dataset called ICONS-50 which opens research on a new kind of robustness,
surface variation robustness. With this dataset we evaluate the frailty of classiﬁers
on new styles of known objects and unexpected instances of known classes. We
also demonstrate two methods that improve surface variation robustness. Together
our benchmarks may aid future work toward networks that learn fundamental class
structure and also robustly generalize.
This document is superseded by https://arxiv.org/abs/1903.12261

1

Introduction

The human vision system is robust in ways that existing computer vision systems are not [2, 50].
Unlike current deep learning classiﬁers [37, 19, 56], the human vision system is not fooled by small
changes in query images. Humans are also not confused by many forms of corruption such as snow,
blur, pixelation, and novel combinations of these. Humans can even deal with abstract changes in
structure and style. Achieving these kinds of robustness is an important goal for computer vision
and machine learning. It is also essential for creating deep learning systems that can be deployed in
safety-critical applications.

Most work on robustness in deep learning methods for vision has focused on the important challenges
of robustness to adversarial examples [53, 6, 7], unknown unknowns [20, 41, 22], and model or data
poisoning [4, 52, 23]. In contrast, we develop and validate datasets for two other forms of robustness.
Speciﬁcally, we introduce the IMAGETNET-C dataset for input corruption robustness [55] and the
ICONS-50 dataset for surface variation robustness. These challenges can be overcome by future
networks which do not rely on spurious correlations or cues inessential to the object’s class.

To create IMAGENET-C, we introduce a set of 75 common visual corruptions and apply them to the
ImageNet object recognition challenge [10]. We hope that this will serve as a general dataset for
benchmarking robustness to image corruptions and prevent methodological problems such as moving
goal posts and result cherry picking. We evaluate the performance of current deep learning systems
and show that there is wide room for improvement on IMAGENET-C. We also introduce methods and
architectures that improve robustness on IMAGENET-C without losing accuracy.

We then benchmark surface variation robustness. In this setting the fundamental class structure is
unchanged, but the object’s surface-level statistics and superﬁcial aspects vary but are not corrupted.
To benchmark surface variation robustness, we introduce the ICONS-50 dataset. This dataset is
intended to support research on robustness to surface variations such as the introduction of new styles
and novel animal species. We then describe two methods that improve surface variation robustness.
By deﬁning and benchmarking surface variation robustness and corruption robustness, we facilitate

research that leads to classiﬁers that track the fundamental structure of the classes and robustly
generalize to unexpected inputs.

2 Related Work

Adversarial Examples. An adversarial image is a clean image perturbed by a small corruption so
as to confuse a classiﬁer. These deceptive corruptions can occasionally fool black-box classiﬁers [38].
Algorithms have been developed that search for the smallest corruption in RGB space that is sufﬁcient
to confuse a classiﬁer [9]. Thus adversarial corruptions serve as type of worst-case analysis for
network robustness. Its popularity has often led “adversarial robustness” to become interchangeable
with “robustness” in the literature [3, 49]. In efforts to ground adversarial robustness research, several
competing robustness measures have been introduced [3, 8, 9, 44, 33]. New defenses [43, 47, 45, 21]
quickly succumb to new attacks [14, 6, 7].

Robustness in Speech. Speech recognition research emphasizes robustness to common corruptions
over worst-case, adversarial corruptions [39, 46]. Common acoustic corruptions (e.g., street noise,
background chatter, wind) receive greater focus than adversarial audio, because common corruptions
are ever-present and unsolved. There are several popular datasets containing noisy test audio [25, 24].
Robustness in noisy environments requires robust architectures, and some research ﬁnds convolutional
networks more robust than fully connected networks [1]. Additional robustness has been achieved
through pre-processing techniques such as standardizing the statistics of the input [40, 54, 18, 35].

ConvNet Fragility Studies. Several studies demonstrate the fragility of convolutional networks on
simple corruptions. For example, Hosseini, Xiao, and Poovendran [2017] use impulse noise to break
Google’s Cloud Vision API. Using Gaussian noise and blur, Dodge and Karam [2017] demonstrate
the superior robustness of human vision to convolutional networks, even after networks are ﬁne-
tuned on Gaussian noise or blur. Geirhos et al. [2017] compare networks to humans on noisy and
elastically deformed images. They ﬁnd that ﬁne-tuning on speciﬁc corruptions does not generalize,
and classiﬁcation error patterns underlying network and human predictions are not similar. Others
show that networks fail to generalize to images deformed by slight geometric transformations [2].

Robustness Enhancements.
In an effort to reduce classiﬁer fragility, Vasiljevic, Chakrabarti, and
Shakhnarovich [2016] ﬁne-tune on blurred images. They ﬁnd it is not enough to ﬁne-tune on one
type of blur to generalize to other blurs. Furthermore, ﬁne-tuning on several blurs can marginally
decrease performance. Zheng et al. [2016] also ﬁnd that ﬁne-tuning on noisy images can cause
underﬁtting, so they encourage the noisy image softmax distribution to match the clean image softmax.
Dodge and Karam [2017] address underﬁtting via an ensemble. They ﬁne-tune each network on
one corruption and classify with an mixture of these corruption-speciﬁc experts, though they do not
assess combinations of known corruptions.

3 The IMAGENET-C Corruption Robustness Benchmark

3.1 The IMAGENET-C Dataset

IMAGENET-C Design. Our corruption robustness benchmark consists of 15 diverse corruption
types, exempliﬁed in Figure 1. The benchmark covers noise, blur, weather, and digital categories.
Research that improves performance on this benchmark should indicate general robustness gains, as
the corruptions are varied and great in number. These 15 corruption types each have ﬁve different
levels of severity, since corruptions can manifest themselves at varying intensities. The Supple-
mentary Materials gives an example of a corruption type’s ﬁve different severities. Real-world
corruptions also have variation even at a ﬁxed intensity. To simulate these, we introduce variation
for each corruption when possible. For example, each fog cloud is unique to each image. These
algorithmically generated corruptions are applied to the ImageNet [10] validation images to pro-
duce our corruption robustness dataset IMAGENET-C. The dataset can be downloaded or re-created
by visiting https://github.com/hendrycks/robustness. Our benchmark tests networks with
IMAGENET-C images, but networks do not train on these images. Networks are trained with datasets
such as ImageNet but not IMAGENET-C. To enable further experimentation, we also designed an
extra corruption for each noise, blur, weather, or digital category. Extra corruptions are depicted
and explicated in the Supplementary Materials and also available at the aforementioned URL. Even
more experimentation is possible by applying these distortions to other images, such as those from

2

Figure 1: Our IMAGENET-C dataset consists of 15 types of algorithmically generated corruptions
from noise, blur, weather, and digital categories. Each type of corruption has ﬁve levels of severity,
resulting in 75 distinct corruptions. See different severity levels in the Supplementary Materials.

Places365. We ﬁnd that applying these corruptions to 32×32 images and benchmarking on such small
images is not predictive of corruption robustness on larger images. Overall, the dataset IMAGENET-C
consists of 15 corruption types, each with ﬁve different severities, all applied to ImageNet validation
images for testing a pre-existing network.

Common Corruptions. The ﬁrst IMAGENET-C corruption is Gaussian noise. This corruption can
appear in low-lighting conditions. Shot noise, also called Poisson noise, is electronic noise caused by
the discrete nature of light itself. Impulse noise is a color analogue of salt-and-pepper noise and can be
caused by bit errors. Defocus blur occurs when an image is out of focus. Frosted Glass Blur appears
with “frosted glass” windows or panels. Motion blur appears when a camera is moving quickly.
Zoom blur occurs when a camera moves toward an object rapidly. Snow is a visually obstructive
form of precipitation. Frost forms when lenses or windows are coated with ice crystals. Fog shrouds
objects and is rendered with the diamond-square algorithm. Brightness varies with daylight intensity.
Contrast can be high or low depending on lighting conditions and the photographed object’s color.
Elastic transformations stretch or contract small image regions. Pixelation occurs when upsampling a
low-resolution image. JPEG is a lossy image compression format that increases image pixelation and
introduces artifacts. Each corruption type is tested with depth due to its ﬁve severity levels, and this
broad range of corruptions allows us to test model corruption robustness with breadth.

3.2 Metric and Setup

Mean and Relative Corruption Error. Common corruptions such as Gaussian noise can be
benign or destructive depending on their severity. In order to comprehensively evaluate a classiﬁer’s
robustness to a given type of corruption, we score the classiﬁer’s performance across ﬁve corruption
severity levels and aggregate its scores. The ﬁrst evaluation step is to take a pre-existing classiﬁer
here notated “Network,” which has not and will not train on IMAGENET-C, and then compute the
clean dataset top-1 error rate. Denote this error rate ENetwork
. This same classiﬁer will then test on an
IMAGENET-C corruption type notated “Corruption.” Let top-1 error rate for the Network classiﬁer
on Corruption with severity level s (1 ≤ s ≤ 5) be written ENetwork
s,Corruption. The classiﬁer’s aggregate

Clean

3

performance across the ﬁve severities of the corruption type Corruption is the Corruption Error,
computed with the formula

CENetwork

Corruption =

ENetwork

s,Corruption

EAlexNet

s,Corruption.

5
(cid:88)

s=1

(cid:30) 5

(cid:88)

s=1

Different corruptions pose different levels of difﬁculty. For example, fog corruptions often obscure
an object’s class more than Brightness corruptions. Thus to make Corruption Errors comparable
across corruption types, we adjust for the difﬁculty by dividing by AlexNet’s errors. Now with
commensurate Corruption Errors, we can summarize model corruption robustness by averaging the
(cid:1). This results in the mean CE
15 Corruption Error values (cid:0)CENetwork
or mCE for short.

Shot Noise, . . . , CENetwork

Gaussian Noise, CENetwork

JPEG

We now introduce a more nuanced corruption robustness measure. Consider a classiﬁer that withstands
most corruptions, so that the gap between the mCE and the clean data error is minuscule. Contrast
this with a classiﬁer with a low clean error rate that does not cope well with corruptions, which
corresponds to a large gap between the mCE and clean data error. It is possible the former classiﬁer
has a larger mCE than the latter, despite the former degrading more gracefully in the presence of
corruptions. The amount that the classiﬁer declines on corrupted inputs is given by the formula

Relative CENetwork

Corruption =

ENetwork

s,Corruption − ENetwork

Clean

EAlexNet

s,Corruption − EAlexNet

Clean

(cid:19)

.

(cid:19)(cid:30)(cid:18) 5

(cid:88)

s=1

(cid:18) 5

(cid:88)

s=1

Averaging these 15 Relative Corruption Errors results in the Relative mCE. In short, the Relative mCE
measures the relative robustness or the performance degradation when encountering corruptions.

Preserving Metric Validity. The goal of
ImageNet-C is to evaluate the robustness of ma-
chine learning algorithms to novel forms of cor-
ruption. Humans are able to generalize to novel
corruptions quite well. For example, they can
easily deal with new Instagram ﬁlters. Hence,
we propose the following protocol. The image
recognition network should be trained on the
ImageNet training set and on whatever other
training sets the investigator wishes to include.
However, the network should not be trained on
any of the 75 corruptions that were used to gen-
erate IMAGENET-C. Then the resulting trained
model should be evaluated on IMAGENET-C
and the above metrics computed. We provide
a separate set of additional corruptions that can
be employed for training or validation prior to
testing on IMAGENET-C (see Supplementary
Materials).

3.3 Architecture Robustness

Figure 2: Robustness (mCE) and Relative mCE
IMAGENET-C values. Relative mCE values sug-
gest robustness in itself declined from AlexNet to
ResNet. “BN” abbreviates Batch Normalization.

How robust are current methods and has progress in computer vision been achieved at the expense of
robustness? As seen in Figure 2, as architectures improve, so too does the mean Corruption Error
(mCE). By this measure, architectures have become progressively more successful at generalizing to
corrupted distributions. All Corruption Error values are in 1. Note that models with similar clean error
rates have fairly similar CEs, and there are no large shifts in a corruption type’s CE. Consequently, it
would seem that architectures have slowly and consistently improved their representations over time.
However, it appears that robustness improvements are mostly explained by accuracy improvements.
Recall that the Relative mCE tracks a classiﬁer’s accuracy decline in the presence of corruptions.
Figure 2 shows that the Relative mCE is worse than that of AlexNet [37]. In consequence, from
AlexNet to ResNet [19], robustness in itself has barely changed. Relative robustness remains near
AlexNet-levels and therefore below human-level, which shows that our “superhuman” classiﬁers are
decidedly subhuman. We now apply IMAGENET-C to evaluate several methods for attempting to
improve robustness to image corruption.

4

Noise

Blur

Weather

Digital

Error mCE Gauss. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG
Network
100 100 100
43.5 100.0 100 100 100
AlexNet
106 109 134
41.8 104.4 107 106 105
SqueezeNet
31.0 93.5
VGG-11
99
97 107 100
100
98
97 102 94
95
27.6 88.9
VGG-19
94
85 83
94
88
VGG-19+BN 25.8 81.6
80 85
90
91
91
30.2 84.7
ResNet-18
ResNet-50
23.9 76.7
77 77
85
89
83
Table 1: Corruption Error and mCE values of different corruptions and architectures on IMAGENET-C.
The mCE value is the mean Corruption Error of the corruptions in Noise, Blur, Weather, and Digital
columns. All models are trained on clean ImageNet images, not IMAGENET-C images. Here “BN”
abbreviates Batch Normalization [31].

100 100 100 100 100 100 100 100
97
100 103 101 100 101 103 97
75
92 91 84
92
68
89 86 75
89
61
80 78 69
82
69
86 84 78
84
57
78 75 66
75

100
98
86
80
74
78
71

97
89
82
87
80

93
90
84
87
78

91
90
86
89
80

97
91
83
88
82

3.4

Informative Robustness Enhancement Attempts

Stability Training. Stability training is a technique to improve the robustness of deep networks [57].
The method’s creators found that training on images corrupted with noise can lead to underﬁtting, so
they instead propose minimizing the cross-entropy from the noisy image’s softmax distribution to the
softmax of the clean image. The authors evaluated performance on images with subtle differences
and suggested that the method provides additional robustness to JPEG corruptions. We ﬁne-tune a
ResNet-50 with stability training for ﬁve epochs. For train time corruptions, we corrupt images with
uniform noise, where the maximum and minimum of the uniform noise is tuned over {0.01, 0.05, 0.1},
and the stability weight is tuned over {0.01, 0.05, 0.1}. Across all noise strengths and stability weight
combinations, the models with stability training tested on IMAGENET-C had a larger mCEs than
the baseline ResNet-50’s mCE. Even on unseen noise corruptions, stability training did not increase
robustness. An upshot of this failure is that benchmarking robustness-enhancing techniques requires
a diverse test set.

Image Denoising. An approach orthogonal to modifying model representations is to improve the
inputs using image restoration techniques. Although general image restoration techniques are not
yet mature, denoising restoration techniques are not. We thus attempt restore an image with the
denoising technique called non-local means [5]. The amount of denoising applied is determined by
the noise estimation technique of Donoho and Johnstone [1993]. Therefore clean images receive
nearly no modiﬁcations from the restoration method, while noisy images should undergo considerable
restoration. We found that denoising increased the mCE from 76.7% to 82.1%. A plausible account
is that the non-local means algorithm slightly smoothed images even when images lacked noise,
despite having the non-local means algorithm governed by the noise estimate. Therefore, the gains in
noise robustness were wiped away by subtle blurs to images with other types of corruptions, showing
that targeted image restoration can prove harmful for robustness.

Smaller Models. All else equal, “simpler” models often generalize better, and “simplicity” frequently
translates to model size. Accordingly, smaller models may be more robust. We test this hypothesis
with CondenseNets [27]. A CondenseNet attains its small size via sparse convolutions and pruned
ﬁlter weights. An off-the-shelf CondenseNet (C = G = 4) obtains a 26.3% error rate and a 80.8%
mCE. On the whole, this CondenseNet is slightly less robust than larger models of similar accuracy.
Even more pruning and sparsiﬁcation yields a CondenseNet (C = G = 8) with both deteriorated
performance (28.9% error rate) and robustness (84.6% mCE). Here again robustness is worse than
larger model robustness. Though models fashioned for mobile devices are smaller and in some sense
simpler, this does not improve robustness.

3.5 Successful Corruption Robustness Enhancements

Histogram Equalization. Histogram equalization successfully standardizes speech data for robust
speech recognition [54, 18]. For images, we ﬁnd that preprocessing with Contrast Limited Adaptive
Histogram Equalization [48] is quite effective. Unlike our previous image denoising attempt, CLAHE
reduces the effect of some corruptions while not worsening performance on most others, thereby
improving the mCE. We demonstrate CLAHE’s net improvement by taking a pre-trained ResNet-50
and ﬁne-tuning the whole model for ﬁve epochs on images processed with CLAHE. The ResNet-
50 has a 23.87% error rate, but ResNet-50 with CLAHE has an error rate of 23.55%. On nearly

5

Figure 3: Architectures like Multigrid networks
and DenseNets resist noise corruptions more ef-
fectively than ResNets.

Figure 4: Larger feature aggregating networks
achieve robustness gains that substantially out-
pace their accuracy gains.

all corruptions, CLAHE slightly decreases the Corruption Error. The ResNet-50 without CLAHE
preprocessing has an mCE of 76.7%, while with CLAHE the ResNet-50’s mCE decreases to 74.5%.

Multiscale, Feature Aggregating, and Larger Networks. Multiscale architectures achieve
greater robustness by propagating features across scales at each layer rather than slowly gaining
a global representation of the input as in typical convolutional neural networks. Some multiscale
architectures are called Multigrid Networks [34]. Multigrid networks each have a pyramid of grids in
each layer which enables the subsequent layer to operate across scales. Along similar lines, Multi-
Scale Dense Networks (MSDNets) [29] use information across scales. MSDNets bind network layers
with DenseNet-like [28] skip connections. These two different multiscale networks both enhance
robustness. Before comparing mCE values, we ﬁrst note the Multigrid network has 24.6% top-1
error, as does the MSDNet, while the ResNet-50 has 23.9% top-1 error. On noisy inputs, Multigrid
networks noticably surpass ResNets and MSDNets, as shown in Figure 3. Since multiscale architec-
tures have high-level representations processed in tandem with ﬁne details, the architectures appear
better equipped to suppress otherwise distracting pixel noise. When all corruptions are evaluated,
ResNet-50 has an mCE of 76.7%, the MSDNet has an mCE of 73.6%, and the Multigrid network has
an mCE of 73.3%.

Some recent models enhance the ResNet architecture by increasing what is called feature aggregation.
Of these, DenseNets [28] and ResNeXts [56] are most prominent. Each purports to have stronger
representations than ResNets, and the evidence is largely a hard-won ImageNet error-rate downtick.
Interestingly, the IMAGENET-C mCE clearly indicates that DenseNets and ResNeXts have superior
representations. Accordingly, a switch from a ResNet-50 (23.9% top-1 error) to a DenseNet-121
(25.6% error) decreases the mCE from 76.7% to 73.4%. More starkly, switching from a ResNet-50
to a ResNeXt-50 (22.9% top-1) drops the mCE from 76.7% to 68.2%. Results are summarized in
Figure 4. This shows that corruption robustness may be a better way to measure future progress in
object recognition than the clean dataset top-1 error rate.

Some of the greatest and simplest robustness gains sometimes emerge from making recent models
more monolithic. Apparently more layers, more connections, and more capacity allow these massive
models to operate more stably on corrupted inputs. We saw earlier that making models smaller does
the opposite. Swapping a DenseNet-121 (25.6% top-1) with the larger DenseNet-161 (22.9% top-1)
decreases the mCE from 73.4% to 66.4%. In a similar fashion, a ResNeXt-50 (22.9% top-1) is less
robust than the a giant ResNeXt-101 (21.0% top-1); the mCEs are 68.2% and 62.2% respectively.
Both model size and feature aggregation results are summarized in Figure 4. Consequently, future
models with more depth, width, and feature aggregation may attain further robustness.

4 Surface Variation Robustness

An important goal for machine learning is to learn the essential structural elements of a class while
being robust to unimportant surface variation. Whereas corruptions degrade the entirety of an image,

6

Figure 5: Image samples are from 12 of the 50 ICONS-50 classes. These images showcase the
dataset’s high image quality, interclass and intraclass diversity, and its many styles.

surface variations cleanly alter the surface of an object and preserve the fundamental structure of the
class. We developed the ICONS-50 dataset to test robustness to two forms of surface variation: style
variation and subtype variation. Style variation is variation in artistic style that leaves the fundamental
structure unchanged. Subtype variation is variation within a broad class (e.g., types of cats within the
broad class of “cat”) where the subtypes all share essential characteristics of the broad class.

4.1 The ICONS-50 Dataset

The ICONS-50 dataset consists of 10,000 images belonging to 50 classes of icons (e.g., peo-
technology
ple, food, activities, places, objects, symbols, etc.)
companies and platforms (e.g., Apple, Samsung, Google, Facebook, etc.). A full list of
ICONS-50 classes is in the Supplementary Materials, and the dataset can be downloaded at
https://github.com/hendrycks/robustness. A subset of ICONS-50 classes is shown in Fig-
ure 5. Each class has icons with different styles. For instance, icons with the thick, black outlines (like
the bottom right “Drink” icon) are created by Microsoft. Other styles in the ICONS-50 dataset are
from Apple, Samsung, Google, Facebook, and other platforms. The ICONS-50 dataset provides more
icons per class (mean of 200) and more classes (50) than the logo datasets of Romberg et al. [2011]
(32 classes, 70 logos per class) and of Joly and Buisson [2009] (37 classes and 53 logos per class).

collected from different

We propose two protocols for evaluating surface variation robustness using the ICONS-50 dataset.
For style robustness, the icons from one source (e.g., Microsoft) should be held out and the network
trained on the remaining icons. Then the ability to generalize to the held out source can be measured
by the classiﬁcation accuracy of the held out icon source. For subtype robustness, we construct a
training set by holding out 50 subtypes from ICONS-50. For example, we hold out the “duck” icons
from the broad “Bird” class. Entire broad classes are not withheld, only a fraction of the subtypes
within ICON-50’s 50 broad classes are withheld. The network is trained on the remaining icons and
learns to classify broad classes. We evaluate the classiﬁer’s subtype robustness by computing the
broad class accuracy on the held out subtypes. These experiments are in the following section, and
auxiliary experiments are in the Supplementary Materials.

4.2 Surface Variation Robustness Experiments with ICONS-50

Style Robustness. The ICONS-50 dataset has several styles like Microsoft’s ﬂat vector graphics
style and Apple’s realistic style. We aim to test the model’s robustness to an unseen style. To that end,
we train a network on ICONS-50 while holding out Microsoft-styled icons. Microsoft-styled icons
appear in all 50 classes, so each class is tested. Here, the metric for style robustness is simply the
classiﬁer’s accuracy on Microsoft-styled icons after the classiﬁer trains on all other styles. That said,

7

we train a 20 layer ResNet, a 40 layer DenseNet (L = 40, k = 24), and a 29 layer ResNeXt (8 × 32d)
for 50 epochs with the cosine learning rate schedule [42] and Nesterov momentum. Icons are resized
to 32 × 32 images. Like others, we use image mirroring and image cropping data augmentation.
What we ﬁnd is that the networks lacks style robustness—the ResNet only obtains 58.3% accuracy on
the held out Microsoft-styled icons, and the DenseNet and ResNeXt do worse with 48.5% and 51.8%
accuracy, respectively. Clearly DenseNets and ResNeXts do not necessarily outperform ResNets as
they did in the corruption robustness benchmark.

These results are not symptomatic of a training data shortage, rather a lack of style robustness. The
networks perform well if we instead use ICONS-50 for traditional classiﬁcation rather than style
robustness. To do this, we hold out one version or rendition of icons that have multiple versions. For
example, we hold out one version of the many Google-stylized “football” icons which have appeared
across different versions of Android. Then we test on these held out versions. By holding out and
testing on one version for icons with many versions, the ResNeXt achieves 98.0% accuracy, the
DenseNet obtains 97.3% accuracy, and the ResNet 97.2% accuracy. Thus the networks have enough
data. Thus we can conclude that with sub-60% accuracy on held out Microsoft-styled icons, the
networks demonstrate a clear lacking in style robustness.

Subtype Robustness. Each ICONS-50 class has many subtypes, so we can treat each class as a
broad class. We hold out 50 subtypes, train the classiﬁer on the remaining subtypes to predict the
broad class, and test the classiﬁer’s accuracy on the held out subtypes. Then, we train a ResNet,
DenseNet, and ResNeXt with the same training scheme from the style robustness experiment. After
training, we ﬁnd that classiﬁer accuracies on these held out subtypes are again meager and that style
robustness and subtype robustness indeed test different forms of robustness. This time the ResNet
is worst, as it obtains only 57.5% accuracy on the held out subtypes. Meanwhile the DenseNet has
58.7% accuracy, and the ResNeXt has 60.0% accuracy. This indicates that the classiﬁers have wide
room for subtype robustness improvement.

Enhancing Surface Variation Robustness. Multiscale networks can improve surface variation
robustness not just corruption robustness. MSDNets [29] can process the whole image structure early
in the forward pass, so that when the style changes while the fundamental structure is preserved,
MSDNets can persevere better than other networks. In comparison to DenseNets, the network type
most similar to MSDNets as both use dense connections, the MSDNet obtains 65.1% accuracy on
the style robustness task while the DenseNet obtains only 48.5% accuracy. However, for subtype
robustness, both networks differ by only a fraction of a percent in performance.

A method to improve both aspects of surface variation robustness is Shake-Shake regularization [15].
Shake-Shake regularization stochastically modulates the inﬂuence of each ResNeXt branch, so that
the network can gain fault tolerance and endure unusual representations. Note we could not test
shake-shake regularization’s effect on corruption robustness since contemporary GPUs do not have
enough memory for shake-shake regularization applied to networks processing large-scale images.
Now, the vanilla ResNeXt has 51.8% style robustness accuracy, but a ResNeXt with shake-shake
regularization jumps to 76.0% accuracy. Subtype robustness also improves appreciably; the ResNeXt
has 60.0% accuracy while the ResNeXt with shake-shake regularization has 63.6% accuracy. Clearly
subtype robustness can be harder to improve than style robustness, but shake-shake regularization
can improve both.

5 Conclusion

In this paper, we introduced what are to our knowledge the ﬁrst benchmarks for corruption robust-
ness and surface variation robustness. This was made possible by introducing two new datasets,
IMAGENET-C and ICONS-50, the ﬁrst of which showed that many years of architectural advance-
ments corresponded to minuscule changes in relative robustness. Therefore benchmarking and
improving robustness deserves attention, especially as top-1 clean ImageNet accuracy nears its
ceiling. We found that some methods harm corruption robustness, while methods such as histogram
equalization, multiscale architectures, and larger models improve corruption robustness. Afterward,
we opened research in surface variation robustness by deﬁning style and subtype robustness. Here we
found modern models to be fragile, but we also found that multiscale networks and highly regularized
networks can noticeably enhance surface variation robustness. In this work, we had several ﬁndings,
introduced novel experiments, and created new datasets for the rigorous study of model robustness, a
pressing necessity as models are unleashed into safety-critical real-world settings.

8

References

[1] Ossama Abdel-Hamid et al. “Applying Convolutional Neural Networks concepts to hybrid

NN-HMM model for speech recognition.” In: ICASSP (2013).

[2] Aharon Azulay and Yair Weiss. “Why do deep convolutional networks generalize so poorly to

small image transformations?” In: arXiv preprint (2018).

[3] Osbert Bastani et al. “Measuring Neural Net Robustness with Constraints”. In: Advances in

Neural Information Processing Systems 29. Ed. by D. D. Lee et al. 2016.

[4] Peva Blanchard et al. Machine Learning with Adversaries: Byzantine Tolerant Gradient

[5] Antoni Buades and Bartomeu Coll. “A non-local algorithm for image denoising”. In: CVPR.

[6] Nicholas Carlini and David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing

Ten Detection Methods. 2017.

[7] Nicholas Carlini and David Wagner. Defensive Distillation is Not Robust to Adversarial

[8] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks.

Descent. 2017.

2005.

Examples. 2016.

2016.

[9] Nicholas Carlini et al. Ground-Truth Adversarial Examples. 2017.
[10]
[11] Samuel Dodge and Lina Karam. A Study and Comparison of Human and Deep Learning

Jia Deng et al. “ImageNet: A Large-Scale Hierarchical Image Database”. In: CVPR (2009).

Recognition Performance Under Visual Distortions. 2017.

[12] Samuel Dodge and Lina Karam. Quality Resilient Deep Neural Networks. 2017.
[13] David Donoho and Iain Johnstone. “Ideal Spatial Adaptation by Wavelet Shrinkage”. In:

Biometrika (1993).
Ivan Evtimov et al. Robust Physical-World Attacks on Deep Learning Models. 2017.

[14]
[15] Xavier Gastaldi. “Shake-Shake Regularization”. In: ICLR Workshop (2017).
[16] Leon Gatys, Alexander Ecker, and Matthias Bethge. “Image Style Transfer Using Convolu-

tional Neural Networks”. In: CVPR (2016).

[17] Robert Geirhos et al. Comparing deep neural networks against humans: object recognition

when the signal gets weaker. 2017.

[18] Mark Harvilla and Richard Stern. Histogram-based subband powerwarping and spectral
averaging for robust speech recognition under matched and multistyle training. 2012.

[19] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: CVPR (2015).
[20] Dan Hendrycks and Kevin Gimpel. “A Baseline for Detecting Misclassiﬁed and Out-of-
Distribution Examples in Neural Networks”. In: International Conference on Machine Learn-
ing. 2017.

[21] Dan Hendrycks and Kevin Gimpel. Early Methods for Detecting Adversarial Images. 2017.
[22] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. “Deep Anomaly Detection with

Outlier Exposure”. In: arXiv preprint (2018).

[23] Dan Hendrycks et al. “Using Trusted Data to Train Deep Networks on Labels Corrupted by

Severe Noise”. In: arXiv preprint arXiv:1802.05300 (2018).

[24] Hans-Günter Hirsch. Aurora-5 Experimental Framework for the Performance Evaluation of
Speech Recognition in Case of a Hands-free Speech Input in Noisy Environments. 2007.
[25] Hans-Günter Hirsch and David Pearce. “The Aurora Experimental Framework for the Perfor-
mance Evaluation of Speech recognition Systems Under Noisy Conditions”. In: ISCA ITRW
ASR2000 (2000).

[26] Hossein Hosseini, Baicen Xiao, and Radha Poovendran. Google’s Cloud Vision API Is Not

Robust To Noise. 2017.

In: arXiv preprint (2017).

[27] Gao Huang et al. “CondenseNet: An Efﬁcient DenseNet using Learned Group Convolutions”.

[28] Gao Huang et al. “Densely connected convolutional networks”. In: Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition. 2017.

[29] Gao Huang et al. “Multi-Scale Dense Networks for Resource Efﬁcient Image Classiﬁcation”.

In: ICLR (2018).

9

[30] Xun Huang and Serge Belongie. “Arbitrary Style Transfer in Real-time with Adaptive Instance

Normalization”. In: ICCV (2017).

[31] Sergey Ioffe and Christian Szegedy. “Batch Normalization: Accelerating Deep Network

Training by Reducing Internal Covariate Shift”. In: JMLR (2015).

[32] Alexis Joly and Olivier Buisson. “Logo Retrieval with A Contrario Visual Query Expansion”.

In: ACM International Conference on Multimedia Retrieval (2009).

[33] Guy Katz et al. “Towards Proving the Adversarial Robustness of Deep Neural Networks”. In:

arXiv preprint (2017).

[34] Tsung-Wei Ke, Michael Maire, and Stella X. Yu. Multigrid Neural Architectures. 2017.
[35] Chanwoo Kim and Richard M. Stern. “Power-normalized Cepstral Coefﬁcients (PNCC) for
Robust Speech Recognition”. In: IEEE/ACM Trans. Audio, Speech and Lang. Proc. 24.7 (July
2016), pp. 1315–1329. ISSN: 2329-9290.

[36] Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech. rep. 2009.
[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classiﬁcation with Deep

Convolutional Neural Networks”. In: NIPS (2012).

[38] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. “Adversarial Machine Learning at Scale”.

In: ICLR (2017).
Jinyu Li et al. “An Overview of Noise-Robust Automatic Speech Recognition”. In: 2014.

[39]
[40] Fu-Hua Liu et al. “Efﬁcient Cepstral Normalization for Robust Speech Recognition”. In: Proc.

of DARPA Speech and Natural Language Workshop. 1993.

[41] Si Liu et al. “Open Category Detection with PAC Guarantees”. In: Proceedings of International

[42]

[43]

Conference on Machine Learning. 2018.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts.
2017.
Jiajun Lu et al. Standard detectors aren’t (currently) fooled by physical adversarial stop signs.
2017.

[44] Aleksander Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”.

In: ICLR (2018).
Jan Hendrik Metzen et al. On Detecting Adversarial Perturbations. 2017.

[45]
[46] Vikramjit Mitra et al. Robust features in Deep Learning based Speech Recognition. 2017.
[47] Nicolas Papernot et al. Distillation as a Defense to Adversarial Perturbations against Deep

Neural Networks. 2017.

[48] Stephen M. Pizer et al. “Adaptive histogram equalization and its variations”. In: Computer

[49]

Vision, Graphics, and Image Processing (1987).
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0.8.0: A Python toolbox to
benchmark the robustness of machine learning models. 2017.

[50] Benjamin Recht et al. “Do CIFAR-10 Classiﬁers Generalize to CIFAR-10?” In: arXiv preprint

(2018).

[51] Stefan Romberg et al. “Scalable Logo Recognition in Real-World Images”. In: ACM Interna-

[52]

tional Conference on Multimedia Retrieval 2011 (2011).
Jacob Steinhardt, Pang Wei Koh, and Percy Liang. “Certiﬁed Defenses for Data Poisoning
Attacks”. In: NIPS (2017).

[53] Christian Szegedy et al. Intriguing properties of neural networks. 2014.
[54] Ángel de la Torre et al. “Histogram equalization of speech representation for robust speech

[55]

recognition”. In: IEEE Signal Processing Society (2005).
Igor Vasiljevic, Ayan Chakrabarti, and Gregory Shakhnarovich. Examining the Impact of Blur
on Recognition by Convolutional Networks. 2016.

[56] Saining Xie et al. “Aggregated Residual Transformations for Deep Neural Networks”. In:

[57] Stephan Zheng et al. Improving the Robustness of Deep Neural Networks via Stability Training.

CVPR (2016).

2016.

10

A Example of IMAGENET-C Severities

Figure 6: Pixelation modestly to markedly corrupts a ﬁsh, showing our benchmark’s varying severities.

In Figure 6, we show the Pixelation corruption type in its ﬁve different severities. Clearly, IMAGENET-
C corruptions can range from negligible to pulverizing. Because of this range, the benchmark
comprehensively assesses each corruption type.

B Extra IMAGENET-C Corruptions

Figure 7: Extra IMAGENET-C corruption examples are available for model validation and sounder
experimentation.

Directly ﬁtting the types of IMAGENET-C corruptions is worth avoiding, as it would cause re-
searchers to overestimate a model’s robustness. Therefore, it is incumbent on us to simplify
model validation. For this reason, we provide extra corruptions that are available for download
at https://github.com/hendrycks/robustness. There is one corruption type for each noise,
blur, weather, and digital category. The ﬁrst corruption type is speckle noise, an additive noise where
the noise added to a pixel tends to be larger if the original pixel intensity is larger. Gaussian blur is a
low-pass ﬁlter where a blurred pixel is a result of a weighted average of its neighbors, and farther
pixels have decreasing weight. Spatter can occlude a lens in the form of rain or mud. Finally, saturate
is common in edited images where images are made more or less colorful. See Figure 7 for instances
of each corruption type.

C Full Corruption Robustness Results

IMAGENET-C corruption relative robustness results are in Table 2. Since we use AlexNet errors to
normalize Corruption Error values, we now specify the value 1
s,Corruption for each corruption
5
type. Gaussian Noise: 88.6%, Shot Noise: 89.4%,
Impulse Noise: 92.3%, Defocus Blur:
82.0%, Glass Blur: 82.6%, Motion Blur: 78.6%, Zoom Blur: 79.8%, Snow: 86.7%, Frost:
82.7%, Fog: 81.9%, Brightness: 56.5%, Contrast: 85.3%, Elastic Transformation: 64.6%,
Pixelate: 71.8%,
JPEG: 60.7%, Speckle Noise: 84.5%, Gaussian Blur: 78.7%, Spatter: 71.8%,
Saturate: 65.8%.

s=1 EAlexNet

(cid:80)5

D 10-Crop Classiﬁcation Fails to Enhance Robustness

Viewing an object at several different locations may give way to a more stable prediction. Having this
intuition in mind, we perform 10-crop classiﬁcation. 10-crop classiﬁcation is executed by cropping
all four corners and cropping the center of an image. These crops and their horizontal mirrors are

11

Noise

Blur

Weather

Digital

Error Rel. mCE Gauss. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG
Network
100 100 100
43.5
AlexNet
126 129 229
41.8
SqueezeNet
31.0
VGG-11
151 161 174
165 161 172
27.6
VGG-19
165 125 144
VGG-19+BN 25.8
133 97 126
30.2
ResNet-18
23.9
ResNet-50
146 111 132

100 100 100 100 100 100 100 100
104 110 106 105 106 110 98 101
116 129 121 115 114 113 99 86
118 136 123 122 114 111 88 82
108 132 114 119 102 100 79 68
100 116 108 112 103 101 89 67
126 107 110 101 97 79 62
97

100 100 100
118 116 114
122 121 125
114 117 122
104 105 114
104 106 111
104 107 107

100.0
117.9
123.3
122.9
111.1
103.9
105.0

100
100
102
98
89
87
89

Table 2: Relative Corruption Errors and Relative mCE values of different corruptions and architectures
on IMAGENET-C. All models are trained on clean ImageNet images, not IMAGENET-C images. Here
“BN” abbreviates Batch Normalization.

processed through a network to produce 10 predicted class probability distributions. We average
these distributions to compute the ﬁnal prediction. Of course, a prediction informed by 10-crops
rather than a single central crop is more accurate. Ideally, this revised prediction should be more
robust too. However, the gains in mCE do not outpace the gains in accuracy on a ResNet-50. In
all, 10-crop classiﬁcation is a computationally expensive option which contributes to classiﬁcation
accuracy but not noticeably to robustness.

E Auxiliary Surface Variation Robustness Experiments

We repurpose the CIFAR-100 [36] and ImageNet-22K datasets for a closer investigation into subtype
robustness. Both datasets have hierarchical taxonomies, so they have subtypes. Then we consider an
inferior but informative synthetic test for style robustness.

CIFAR-100. The CIFAR-100 dataset has 20 broad classes, each with ﬁve subtypes. For this
experiment, we hold out 1, 2, 3, or 4 subtypes from each broad class, train the classiﬁer on the
remaining subtypes while predicting the broad class, then test the classiﬁer’s broad class accuracy
on the held out subtypes. The classiﬁer is a 40-2 Wide ResNet. We ﬁnd the classiﬁer broad class
prediction error rate when we hold out 1, 2, 3, or 4 subtypes, and then we average these error rates.
The average broad class prediction error rate with held out subtypes as inputs is 59.7%. In sharp
contrast, the average broad class prediction error rate on subtypes known to the classiﬁer is 13.2%.
Like before there is a dearth of subtype robustness.

natural

ImageNet-22K. Another
image
dataset with many classes but more data is
ImageNet-22K, an ImageNet-1K superset. To
deﬁne this subtype robustness experiment,
we manually select 25 broad classes from
ImageNet-22K, listed with their WordNet IDs in
the Supplementary Materials. Each broad class
has many subtypes. We call a subtype “seen” if
and only if it is in ImageNet-1K and a subtype
of one of the 25 broad classes. The subtype
is “unseen” if and only if it is a subtype of the
25 broad classes and is from ImageNet-22K
but not ImageNet-1K. Fortunately, pre-trained
ImageNet-1K classiﬁers are readily available
and have not trained on subtypes which should
remain unseen. Therefore, we test subtype
robustness by ﬁne-tuning several pre-trained
ImageNet-1K classiﬁers on seen subtypes so
that they predict one of 25 broad classes. Their
“seen” and “unseen” accuracies are shown in
Figure 8, while the ImageNet-1K classiﬁcation accuracy before ﬁne-tuning is on the horizontal
axis. Despite only having 25 classes and having trained on millions of images, these classiﬁers
demonstrate a subtype robustness performance gap that should be far less pronounced.

Figure 8: ImageNet classiﬁers and their robustness
to unseen subtypes. Unseen subtypes of known
broad classes are noticeably harder for classiﬁers.

12

For completeness, we list the 25 broad classes which we selected from ImageNet. Amphib-
ian (n01627424), Appliance (n02729837), Aquatic Mammal (n02062017), Bird (n01503061),
Bear (n02131653), Beverage (n07881800), Big cat (n02127808), Building (n02913152), Cat
(n02121620), Clothing (n03051540), Dog (n02084071), Electronic Equipment (n03278248),
Fish (n02512053), Footwear (n03380867), Fruit (n13134947), Fungus (n12992868), Geolog-
ical Formation (n09287968), Hoofed Animal (n02370806), Insect (n02159955), Musical Instru-
ment (n03800933), Primate (n02469914), Reptile (n01661091), Utensil (n04516672), Vegetable
(n07707451), Vehicle (n04576211).

Style Transfer. Now we supplement our style robustness results with a lower quality, synthetic,
yet informative experiment. Style transfer [16] provides a synthetic way to test style robustness
by algorithmically and artiﬁcially recomposing the image in the style of another image. We test
robustness to style transfer in the same way we tested robustness in our IMAGENET-C corruption
benchmark. Therefore we corrupt ImageNet validation images with the style transfer method of
Huang and Belongie [30]. Images are corrupted at ﬁve different severities where higher severities
preserve less content and more strictly adhere to a randomly chosen target style. Improving style
transfer robustness proves to be more difﬁcult than improving IMAGENET-C performance. From
AlexNet to ResNet-50, the mCE of IMAGENET-C distortions went from 100% to 76.7%, but the
Corruption Error for style transfer only decreased from 100% to 92%. In fact, the ResNet-50 style
transfer Corruption Error is higher than all IMAGENET-C Corruption Errors, and this holds for nearly
all architectures tested. This again shows that current classiﬁers are not yet suited to surface variations
such as stylization.

F Classes in ICONS-50

The 50 classes of ICONS-50 are as follows: Airplane, Arrow Directions, Ball, Biking, Bird, Blade,
Boat, Books, Building, Bunny Ears, Cartwheeling, Clock, Cloud, Disk, Drink, Emotion Face,
Envelope, Family, Fast Train, Feline, Flag, Flower, Footwear, Golﬁng, Hand, Hat, Heart, Holding
Hands, Japanese Ideograph, Kiss, Lock, Mailbox, Marine Animal, Medal, Money, Monkey, Moon,
Mountain, Numbers, Phone, Prohibit Sign, Star, Surﬁng, Tree, Umbrella, Vehicle, Water Polo, Worker,
Wrestling, Writing Utensil.

13

