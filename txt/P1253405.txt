Cross-layer Optimization for High Speed Adders:
A Pareto Driven Machine Learning Approach

Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, and Bei Yu

8
1
0
2
 
t
c
O
 
6
1
 
 
]

R
A
.
s
c
[
 
 
2
v
3
2
0
7
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—In spite of maturity to the modern electronic design
automation (EDA) tools, optimized designs at architectural stage
may become sub-optimal after going through physical design
ﬂow. Adder design has been such a long studied fundamental
problem in VLSI industry yet designers cannot achieve optimal
solutions by running EDA tools on the set of available preﬁx
adder architectures. In this paper, we enhance a state-of-the-
art preﬁx adder synthesis algorithm to obtain a much wider
solution space in architectural domain. On top of
that, a
machine learning-based design space exploration methodology
the adders in
is applied to predict
physical domain, which is infeasible by exhaustively running
EDA tools for innumerable architectural solutions. Considering
the high cost of obtaining the true values for learning, an active
learning algorithm is proposed to select the representative data
during learning process, which uses less labeled data while
achieving better quality of Pareto frontier. Experimental results
demonstrate that our framework can achieve Pareto frontier of
high quality over a wide design space, bridging the gap between
architectural and physical designs. Source code and data are
available at https://github.com/yuzhe630/adder-DSE.

the Pareto frontier of

I. INTRODUCTION

I N the last decades, the industrial EDA tools have advanced

towards optimality, especially at the individual stages of
VLSI design cycle. Nevertheless, with growing design com-
plexity and aggressive technology scaling, physical design
issues have become more and more complex. As a result,
the constraints and the objectives of higher layers, such as
the system or logic level, are very difﬁcult to be mapped
into those of lower layers, such as physical design, and vice-
versa, thereby creating a gap between the optimality at the
logic stage and the physical design stage. This necessitates
the innovation of data-driven methodologies, such as machine
learning [1]–[5], to bridge this gap.

Adder design is one of the fundamental problems in digital
semiconductor industry, and its main bottleneck (in terms
of both delay and area) is the carry-propagation unit. This
unit can be realized by hundreds of thousands of parallel
preﬁx structures, but it is hard to evaluate the ﬁnal metrics
without running through physical design tools. Historically,
regular adders [6]–[9] have been proposed for achieving the
corner points in terms of various metrics as shown in Fig. 1

The preliminary version has been presented at the IEEE International
Symposium on Low Power Electronics and Design (ISLPED) in 2017. This
work is supported in part by The Research Grants Council of Hong Kong SAR
(Project No. CUHK24209017) and CUHK Undergraduate Summer Research
Internship 2017.

Y. Ma, J. Chen and B. Yu are with the Department of Computer Science

and Engineering, The Chinese University of Hong Kong, NT, Hong Kong.

S. Roy is with Intel Corporation, San Jose, CA, USA.
J. Miao is with the Cadence Design Systems, San Jose, CA, USA.

Fig. 1: Regular adders (picture taken from [12]).

in architectural stage. The main motivation for structural
regularity was the ease of manual layout, but EDA tools now
taking care of all physical design aspects, the regularity is
no longer essential. Moreover, the extreme corners do not
map well to the physical design metrics after synthesis, place-
ment and routing. To address this gap between preﬁx adder
synthesis and actual physical design of the adders, custom
adders are typically designed by tuning parameters, such as
gate-sizing, buffering etc., targeting at the optimization of
power/performance metrics for a speciﬁc technology library
[10], [11]. However, this custom approach (i) needs signiﬁcant
engineering effort, (ii) is not ﬂexible to Engineering Change
Order (ECO), and (iii) does not guarantee the optimality.

The algorithmic synthesis approach resolves the ﬁrst two
issues of the custom approach, by adding more ﬂexibility to
the late ECO changes and reducing the engineering effort.
Based on the number of solutions, the existing adder synthesis
algorithms can be broadly classiﬁed into two categories. The
ﬁrst and the most common approach is to generate a single
preﬁx network for a set of structural constraints, such as the
logic level, fan-out etc. Several algorithms have been proposed
to minimize the size of the preﬁx graph (s) under given bit-
width (n) and logic-level (L) constraints [13]–[16]. Closed
form theoretical bounds for size-optimality are provided by
[17] for L ≥ 2 log2 n − 2. [18] has given more general bound
for preﬁx graph size, but when L is reduced to log2 n, a pre-
requisite for high-performance adders, there is no closed form
bound for s. [19] presents a polynomial-time algorithm for

generating preﬁx graph structures by restricting both logic-
level and fan-out. The limitations in these approaches are
two-fold, (i) this restricted set of structures is not capable
of exploring the large solution space, and (ii) since it is very
hard to analytically model the physical design complexities,
such as wire-length and congestion issues, the physical design
metrics, such as the area, power, delay etc., may not be
mapped well to the preﬁx structure metrics, such as the size,
max-fan-out (mf o) etc. This motivates the second category
of algorithms where thousands of preﬁx adder solutions can
be generated and explored for synthesis and physical design
in the commercial EDA tools.

One such approach is [20] which presents an exhaus-
tive bottom-up enumeration technique with several pruning
strategies to generate innumerable preﬁx structure solutions.
However, it has two issues, (i) this approach cannot provide
solutions in several cases for restricted fan-out, which can
control the congestion and load-distribution during physical
design [19]. As a result, it may still miss the good solution
space to a large extent, and (ii) it is computationally very
intensive to run all solutions through synthesis, placement and
routing.

In this paper, we enhance the algorithm in [20], [21] to
generate adders under any arbitrary mf o constraint, which
enables a wider adder solution space in logical form. To
tackle the high computational effort during the physical design
ﬂow, we further propose to use machine learning to perform
the design space exploration in physical solution space. We
develop Pareto frontier driven machine learning methodolo-
gies to achieve rich adder solutions with trade-offs among
power, area, and delay. As a passive supervised learning, the
proposed quasi-random sampling approach is able to select
representative preﬁx adders out of the hundreds of thousands
of preﬁx structures.

It should be noted that various machine learning algorithms
have been investigated to explore design space in different
design scenarios. Palermo et al. [22] deploy both linear
regression and artiﬁcial neural network for multiprocessor
systems-on-chip design. Lin et al. [23] present a random
forest-based learning model in high level synthesis, which
can ﬁnd an approximate Pareto-optimal designs effectively.
Meng et al. [24] propose a random forest-based method for
Pareto frontier exploration, where non-Pareto-optimal designs
are carefully eliminated through an adaptive strategy. Multiple
predictions can be obtained through random forest, which can
be used for estimating the uncertainty. Superior to the random
forest, in this paper we further propose an active learning
approach based on Gaussian Process (GP), which by nature
can estimate the prediction uncertainty efﬁciently.

Our main contributions are summarized as follows:
• A comprehensive framework for optimal adder search
by machine learning methodology bridging the preﬁx
architecture synthesis to the ﬁnal physical design;

• An enhancement to a state-of-the-art preﬁx adder algo-
rithm [20] to optimize the preﬁx graph size for restricted
fan-out and explore a wider solution space;

• A machine learning model for preﬁx adders, guided by
quasi-random data sampling with features considering

architectural attributes and EDA tool settings;

• A design space exploration method to generate the Pareto
frontier for delay vs. power/area over a wide design
space;

• An active learning approach for the design space explo-
ration, which uses less labeled data and achieves better
quality of Pareto frontier.

The rest of the paper is organized as follows. Section II
presents the background of preﬁx adder synthesis, while
Section III discusses our preﬁx graph generation algorithm.
Next, two machine learning approaches of design space explo-
ration for high-performance adders are described. Section IV
presents the passive supervised learning, while Section V
introduces a Pareto frontier driven active learning approach.
Section VI lists the experimental results, followed by conclu-
sion in Section VII.

II. PREFIX ADDER SYNTHESIS

In this section, we ﬁrst provide the background of the preﬁx
adder synthesis problem. Then we present a brief discussion
on the algorithm presented in [20], which we enhance to our
Preﬁx Graph Generation (PGG) algorithm to synthesize the
preﬁx adder network.

A. Preliminaries

An n bit adder accepts two n bit addends A = an−1..a1a0
and B = bn−1..b1b0 as input, and computes the output
sum S = sn−1..s1s0 and carry out Cout = cn−1, where
si = ai ⊕ bi ⊕ ci−1 and ci = aibi + aici−1 + bici−1. The
simplest realization for the adder network is the ripple-carry-
adder, but with logic level n − 1, which is too slow. For faster
implementation, carry-lookahead principle is used to compute
the carry bits. Mathematically, this can be represented with
bitwise (group) generate function g (G) and propagate func-
tion p (P ) by the Weinberger’s recurrence equations as follows
[25]:

• Pre-processing (inputs): Bitwise generation of g, p

gi = ai · bi and pi = ai ⊕ bi.

(1)

• Preﬁx processing: This part is the main carry-propagation
component where the concept of generate/propagate is
extended to multiple bits and G[i:j], P[i:j] (i ≥ j) are
deﬁned as

(cid:26) pi,

(cid:26) gi,

P[i:j] =

G[i:j] =

if i = j,

P[i:k] · P[k−1:j], otherwise,

G[i:k] + P[i:k] · G[k−1:j], otherwise.

if i = j,

(2)

(3)

The associative operation ◦ is deﬁned for (G, P ) as:

(G, P )[i:j] = (G, P )[i:k] ◦ (G, P )[k−1:j]

= (G[i:k] + P[i:k] · G[k−1:j], P[i:k] · P[k−1:j]).
(4)

• Post-processing (outputs): Sum/Carry-out generation

si = pi ⊕ ci−1,

ci = G[i:0], and Cout = cn−1.

(5)

The ‘Preﬁx processing’ or carry propagation network can be
mapped to a preﬁx graph problem with inputs ik = (pk, gk)
and outputs ok = ck, such that ok depends on all previous
inputs ij (j ≤ k). Any node except the input nodes is called
a preﬁx node. Size of the preﬁx graph is deﬁned as the number
of preﬁx nodes in the graph. Fig. 2 shows an example of such
preﬁx graph of 6 bit and we can see that Cout = c5 = o5 is
given by

o5 = (i5 ◦ i4) ◦ ((i3 ◦ i2) ◦ (i1 ◦ i0)).

(6)

Size (s), logic level (L) and maximum-fan-out (mf o) for
this network are respectively 8, 3 and 2. Note that here the
number of fan-ins for each of the associative operation o is
two, thus this is called radix-2 implementation of the preﬁx
graph. However, there exist other options such as radix-3 or
radix-4, but the complexity is very high and not beneﬁcial in
static CMOS circuits [26]. In this work, the logic levels for
all output bits are log2 n, i.e., the minimum possible, to target
high performance adders.

B. Discussion on [20]

Our PGG algorithm to generate the preﬁx graph structures
for physical solution space exploration is based on [20]. So
it is imperative to ﬁrst discuss about [20]. However, we omit
the details and only mention the key points of [20] due to
space constraint.

[20] is an exhaustive bottom-up and pruning based enu-
meration technique for preﬁx adder synthesis. This work
presented an algorithm to generate all possible n + 1 bit
preﬁx graph structures from any n bit preﬁx graph. Then
this algorithm is employed in a bottom-up fashion (from 1
bit adder to 2 bit adders, then from all 2 bit adders to 3
bit adders, and so on) to synthesize preﬁx graphs of any
bit-width. As a result, scalability issue arises due to the
exhaustive nature of the algorithm, which is then tackled by
adopting various pruning strategies to scale the approach.
However, the pruning strategies are not sufﬁcient to scale
the algorithm well for different fan-out constraints. So when
it intends to ﬁnd the solutions for higher bit adders, the
intermediate adder solutions that need to be generated are
often huge. Consequently, it fails to get fan-out restricted
(e.g. when mf o = 8, 10, 12 etc. for 64 bit adders) solutions
even with 72GB RAM due to the generation of innumerable
intermediate solutions [19]. Pruning strategies, such as size-
bucketing [20], help to achieve solutions in some cases, but
with sub-optimality. So design space-exploration based on
this algorithm can miss a signiﬁcant spectrum of the adder
solutions.

C. Our PGG Algorithm

To better explore the wide design space of adders, in this
paper we have enhanced [20] for different fan-out constraints
by incorporating more pruning techniques.

Fig. 2: 6 bit preﬁx adder
network.

Fig. 3: Imposing semi-regularity.

1) Semi-Regularity in Preﬁx Graph Structure: The ﬁrst
strategy is to enforce a sort of regularity in the preﬁx graphs.
For instance, regular adders, such as Sklansky, Brent-Kung,
have the inherent property that the consecutive input nodes
(even and odd) are combined to create the preﬁx nodes at the
logic level 1. In our approach, we constrain this regularity
for those preﬁx nodes (logic level 1). To explain this, let
us consider Fig. 3. We can see that preﬁx nodes r1, r2, r3
and r4 are constructed by consecutive even-odd nodes. For
instance, i0 and i1 are used to construct r1. But with this
structural constraint, we are not allowed to construct any node
by combining i1 and i2 as done in Kogge-Stone adders. Note
that the sub-structure, as shown in Fig. 3, is a part of some
regular adders like Sklansky adder, and is imposed in our
preﬁx structure enumeration.

We have run experiments with 16 bit adders, and observed
that this pruning strategy (i) does not degrade the solution
quality (or size of the preﬁx graph under same L and mf o),
but (ii) able to reduce the search space signiﬁcantly,
in
comparison to not using this pruning strategy.

2) Level Restriction in Non-trivial Fan-in: Each of the
preﬁx node N (a:b), where a is the most-signiﬁcant-bit (MSB)
and b is the least-signiﬁcant-bit (LSB),
is constructed by
connecting the trivial fan-in Ntr (a:c) having same MSB as
N , and the non-trivial fan-in Nnon−tr (c − 1:b). For instance,
in Fig. 2, o3 and b2 are respectively the non-trivial fan-in
and the trivial fan-in node for the preﬁx node o5. In the
bottom-up enumeration technique, we put another additional
restriction that the level of the trivial fan-in node is always
i.e.,
less or equal
level(Ntr) ≤ level(Nnon−tr). Note that this sort of structural
restriction is also inherent in regular adders, such as Sklansky
or Brent-Kung adders.

to that of the non-trivial fan-in node,

In a nutshell, our PGG algorithm is a blend of regular
adders and [20]. We borrow some properties of regular adders
to enforce in [20] for reducing its huge search space without
hampering the solution quality. To illustrate this, we have
obtained the binary for [20] from the authors, and ﬁrst
compared our result for lower bit adders, such as n = 16, 32.
We got the solutions with same minimum size, which proves
that our structural constraints have not degraded the solution
quality. However, for higher bit adders, we get better solution
quality than [20] as shown in TABLE I.

Column 1 presents the mf o constraint, while columns
2 and 3 respectively show the size and run-time for our
enhanced algorithm, and the corresponding entries for [20]
are respectively represented in columns 4 and 5. In general,

TABLE I: Comparison with [20] for 64 bit adders

mf o

4
6
8
12
16
32

Our Approach

Approach in [20]

size
244
233
222
201
191
185

Run-time (s)
302
264
423
193
73
0.04

size
252
238
-
-
192
185

Run-time (s)
241
212
-
-
149
0.04

when fan-out is relaxed or mf o is higher, the run-time is less
due to relaxed size-pruning as explained in [20]. Note that [20]
cannot generate solutions for mf o = 8, 12 due to generation
of innumerable intermediate solutions as explained in [19]. On
the contrary, our structural constraints can do a pre-ﬁltering
of the potentially futile solutions, thereby allowing relaxed
size-pruning and size-bucketing to search for more effective
solution space. In terms of run-time, it is slightly worse in
a few cases, but importantly, this generation is a one-time
process, and this run-time is negligible in comparison to the
design space exploration by the physical design tools. So our
imposed structural restrictions (i) do not degrade the solution
quality, (ii) achieve better solution sizes for all mf o than [20]
for higher bit adders which could not even generate solutions
in all cases, and (iii) help to obtain wider physical solution
space to be demonstrated in Section II-E.

D. Quasi-random Sampling

We have mainly focused on 64 bit adders in this work as
this is mostly used in today’s microprocessors. From all preﬁx
adder solutions, we sample a set of solutions for building
the learning model via the quasi-random approach which
is conducted by a two-level binning (mf o, s) followed by
random selection, This approach aims to evenly sample the
preﬁx adders covering different architectural bins. The pri-
mary level of binning is determined by mf o of the solutions.
However, there may be thousands of architectures sharing the
same mf o, so the secondary level of binning is based on s.
Afterwards, adders are picked randomly from those secondary
bins.

We illustrate the quasi-random sampling with the following
example: given 5000 solutions with mf o = 4, we want to
pick 50 solutions from them. Suppose these 5000 solutions
have the size distribution from 244 to 258. First a random
solution is picked from the bucket of the solutions (mf o = 4,
s = 244). Then we pick a solution randomly from (mf o = 4,
s = 245), and so on. After picking 15 solutions from each of
those buckets with mf o = 4, we again start from the bucket
(mf o = 4, s = 244). This process is repeated until we get 50
solutions. Similar procedure is done with other mf o values.

E. Physical Solution Space Comparison with [20]

In this subsection, we show the usefulness of our algorithm
for obtaining wider solution space in physical design domain
in comparison to [20]. Among the preﬁx adders generated by
[20], we randomly sampled 7000 preﬁx adders. Those preﬁx
adders are fed into the full EDA ﬂow (synthesis, placement
and routing) to get their real delay, power and area values

(a)

(c)

(b)

(d)

Fig. 4: Quasi-random sampled adders vs. adders from [20].
(a) Solution space in area vs. delay domain from [20]; (b)
Solution space in area vs. delay domain from ours; (c)
Solution space in power vs. delay domain from [20]; (d)
Solution space in power vs. delay domain from ours.

(takes around 700 hours). We plot these adders by [20] and
our representative 3000 adders in Fig. 4. It can be seen that,
although the numbers of adders by [20] is more than 2 times of
our representative adders, our adders still cover wider solution
space in physical domain, demonstrating the effectiveness of
our enhanced algorithm PGG. This is in accordance with
the solutions missed by [20] as mentioned in TABLE I.
Those availabilities eventually offer more opportunities for
our machine learning methodology to identify close to ground
truth Pareto frontier solutions.

III. BRIDGING ARCHITECTURAL SOLUTION SPACE TO
PHYSICAL SOLUTION SPACE

In most EDA problems, the metrics of the solution quality
are typically conﬂicting. For instance, if we optimize the tim-
ing of the design, then the power/area may be compromised
and vice versa. So one imperative job of EDA engineers
is to ﬁnd the Pareto-optimal points of the design enabling
the designers to select among those. In this section, we ﬁrst
provide the preliminaries about Pareto optimality, and the
error metrics of Pareto optimal solutions. Then we discuss
the gap between the preﬁx architectural solution space and
physical solution space in adders, which motivates the need of
the machine learning-based approach for optimal adder explo-
ration. Finally, a domain knowledge-based feature selection
details are presented along with training data sampling for
the learning models.

architecture stage are preﬁx node size s and max fan-out mf o.
These two metrics are conﬂicting, i.e., if we reduce mf o,
s increases and vice-versa. Similar competing relationship
exists between delay and power/area after physical design. It
should be stressed that power and s are correlated, and mf o
indirectly controls the timing as more restricted fan-out can
mitigate congestion and load-distribution, thereby improving
the delay of the adder. However, this relationship between
architectural synthesis and physical design is approximate,
and not a very high-ﬁdelity one.

To demonstrate this, we plot node size s vs. mf o and power
vs. delay in Fig. 6 for several 64 bit adder solutions. In this
experiment, we have generated the preﬁx architecture solu-
tions by PGG, and the ﬁnal power/delay numbers are obtained
by running those solutions through EDA tools as explained
later in Section VI. An example of the preﬁx architecture and
the corresponding physical solution is presented in Fig. 7. In
Fig. 6(a), we broadly categorize the solutions into 2 groups,
(i) G1 with higher node size and lower mf o, and (ii) G2
with lower node size and higher mf o. In Fig. 6(b), the same
designs as Fig. 6(a) are projected into the physical solution
space, restoring the group information. Design Compiler [28]
(version F-2011.09-SP3) is used for logical synthesis, and
IC Compiler [29] (version J-2014.09-SP5-3) is used for the
placement and routing. Non linear delay model (NLDM) in
32nm SAED cell-library [30] is used for technology mapping.
The key observations here are ﬁrstly, there is a correlation
between architectural solution space and physical design
solution space. For instance, the solutions from G1 are mostly
on the upper side, and those of G2 are mostly on the lower
side in Fig. 6(b), thereby indicating a correspondence between
s and power. Nevertheless, it is not completely reliable. For
example, (i) the delay numbers for G1 and G2 are very much
spread, (ii) a cluster can be observed where the solutions
from G1 and G2 are mixed up in Fig. 6(b), and (iii) several
solutions of G1 are better than several solutions of G2 in
power, which is not in accordance with the metrics at the
preﬁx adder architecture stage. So we can not utterly rely on
architectural solution space to achieve the optimal output in
physical solution space.

However, since our algorithm generates hundreds of thou-
sands of preﬁx graph structures, it is intractable to run synthe-
sis and physical design ﬂows for even a small percentage of
all available preﬁx adder architectures. To address this ﬁdelity
gap between the two design stages and the high computational
cost together, we come up with a novel machine learning
guided design space exploration as replacement of exhaustive
search.

C. Feature Selection

The feature is a representation which is extracted from the
original input representation, and it plays an important role
in machine learning tasks. We now discuss the features to be
used for the learning model. Features are considered from both
preﬁx adder structure and tool settings, with a focus on the
former. We select node size and maximum-fan-out (mf o) of
a preﬁx adder as two main features for our learning model.

Fig. 5: Hypervolume with two objectives in objective space.

(a)

(b)

Fig. 6: Gap between preﬁx structure and physical design of
adders: (a) Architectural solution space; (b) Physical solution
space.

A. Preliminaries
Deﬁnition 1 (Pareto Optimality). An objective vector f (x) is
said to dominate f (x(cid:48)) if:

∀i ∈ [1, n], fi(x) ≤ fi(x(cid:48))
and ∃j ∈ [1, n], fj(x) < fj(x(cid:48)).
A point x is Pareto-optimal if there is no other x(cid:48) in design

(7)

space such that f (x(cid:48)) dominates f (x).

As in this paper for adder design, a Pareto-optimal design
is where none of the objective metrics, such as area, power
or delay, can be improved without worsening at least one of
the others. The Pareto Frontier is the set of all the Pareto-
optimal designs in the objective space. Therefore, the goal is
to identify the Pareto-optimal set P for all the Pareto-optimal
designs.

Deﬁnition 2 (Hypervolume). The hypervolume computes the
volume enclosed by the Pareto frontier and the reference point
in the objective space [27].

In Fig. 5, the shaded area is an example of the hypervolume
of a Pareto set with two objectives. Then the hypervolume
error for a predicted Pareto set ˆP is deﬁned as

η =

V (P ) − V ( ˆP )
V (P )

,

(8)

where P is the true Pareto-optimal set, and V (P ) is the
hypervolume of the Pareto set P . Note that a prediction ˆP
which contains the whole design space has an error of 0.
Thus the predicted set ˆP with less points is desired.

B. Gap Between Logic and Physical Design

Since we focus on high performance adders and explore
the preﬁx adders of logic level L = log2 n, the metrics at this

(a)

(b)

Fig. 7: (a) An example of architectural solution: Bit-width = 64, size = 201, Max. level
= 6, Max. fanout = 12; (b) Corresponding physical solution.

Fig. 8: Deﬁning spf o of a
node.

However, for any given mf o and node size, there will be
hundreds or even thousands of different preﬁx architectures.
Therefore, additional features are required to better distinguish
individual preﬁx adder attributes. We deﬁne a parameter sum-
path-fan-out (spf o) for this. Let a and b are the fan-in nodes
of a node n, then spf o(n) is deﬁned recursively as:

spf o(n) =

0,
sum(f o(a) + spf o(a),

if n ∈ input,

(9)

f o(b) + spf o(b)), otherwise.






Here f o(n) denotes the fan-out of any node n. Consider the
preﬁx adder structure in Fig. 8, and according to the deﬁnition
we have:

spf o(o1) = sum(f o(i0) + spf o(i0), f o(i1) + spf o(i1))

spf o(b1) = sum(f o(i2) + spf o(i2), f o(i3) + spf o(i3))

spf o(b2) = sum(f o(i4) + spf o(i4), f o(i5) + spf o(i5))

= sum(1, 1) = 2,

= sum(2, 1) = 3,

= sum(2, 1) = 3.

Therefore, we can use the recursive deﬁnition to calculate

spf o(o3) = sum(f o(o1) + spf o(o1), f o(b1) + spf o(b1))

= sum(3 + 2, 2 + 3) = 10,

spf o(o5) = sum(f o(o3) + spf o(o3), f o(b2) + spf o(b2))

= sum(3 + 10, 3 + 3) = 19.

In our methodology, we use the spf o of the output nodes
which are at log2 n level (there are 32 nodes at level 6 for 64
bit adder) as the features to characterize the preﬁx structures,
in addition to mf o, size and target delay. The basic intuition
for selecting spf o of the output nodes as the features is that
the critical path delay of the adder is the longest path delay
from input to output. So it depends on the (i) path-lengths,
which can be represented at the preﬁx graph stage by the
logic level of the node, and (ii) the number of fan-outs driven
at every node on the path. Note that we have skipped the
spf o of the output nodes which are not at log2 n level as for
those nodes, the path length is smaller, and those would not
potentially dictate the critical path delay.

Apart from these preﬁx graph structural features, we also
consider tool settings from synthesis stage and physical design
stage as other features. We have synthesized the adder struc-
tures using industry-standard EDA synthesis tool [28], where

we can specify the target-delay for the adder. The tool then
adopts different strategies internally to meet that target-delay
which we can hardly take into account during preﬁx graph
synthesis. Consequently, changing the target-delay can lead
to different power/timing/area metrics. So we have considered
target-delay as a feature in our learning approach.

In physical design, utilization is an important parameter,
which deﬁnes the area occupied by standard cell, macros and
blockages. Different utilization values can lead to different
layouts after physical design. Therefore, we take utilization
as another feature in the learning model.

In addition to the target delay and utilization, other tool
settings have also been explored. The optimization level
setting in logical synthesis has a potential impact on the per-
formance of adders, which can be adjusted by compile and
compile_ultra commands with different options. After
synthesizing, it is observed that the solutions generated with
compile_ultra can signiﬁcantly dominate the solutions
generated by compile. Therefore, this setting is ﬁxed to
compile_ultra level as we are aiming at superior designs.
In this work, the technology node is not used as a feature.
From the machine learning perspective, there is a common
assumption for conventional machine learning applications
that
the training and test data are drawn from the same
feature space and the same distribution [31]. The values of
area/power/delay may vary a lot under different technology
nodes, which results in different underlying data distributions.
Therefore, the technology node for synthesis should be con-
sistent. The proposed approach for feature extraction can also
be applied to other technology nodes as long as the technology
node is consistent during the design ﬂow. If the technology
node of the testing data switches to another one, the machine
learning model should be re-trained using the data from that
technology node to ensure the accuracy of the model.

D. Data Sampling

Since we can not afford to run the physical design ﬂow
for too many architectures, and too few training data may
degrade the model accuracy signiﬁcantly, a set of adders need
to be selected to represent the entire design solution space.
However, ﬁnding a succinct set of representative training data
for the traditional supervised learning is difﬁcult. In order
to tackle this difﬁculty, we come up with two learning ap-
proaches in the next two sections. The ﬁrst one is the passive

for delay with only primary features. Best model ﬁtting for
delay is achieved with SVR (RBF kernel) with these 4 primary
and 32 secondary features. Since SVR with RBF kernel give
good MSE (mean-squared-error) scores for all metrics, delay,
area and power, we have used this model throughout for
design space exploration.

The model experiments give us the following key insights:
(i) tool setting can play an important role in building the
learning models in EDA. For instance, MSE scores for area
and power improve from 0.021 to 0.003, and 0.228 to 0.027
respectively when we add the ‘target delay’ feature in our
model building, (ii) secondary features play an important
role in improving the model accuracy. For instance, when
we include spf o features in model building, MSE score for
delay improves from 0.200 to 0.170. (iii) linear models are
not sufﬁcient for modeling delay. For instance, MSE scores of
delay improve from 0.214 to 0.170 when we go from linear
models to SVR with RBF kernel, with the same set of features.
The problem of exploring the Pareto frontier of rich preﬁx
adder space can be approached by ﬁrst sampling a subset
of preﬁx adder architectures, and generating the power, area,
delay numbers of each preﬁx adder by running through the
logic synthesis and physical design ﬂow. Those known data
set will be used as the training and testing data for supervised
machine learning guided model ﬁtting. Once the model is
ﬁtted, we can apply the exhaustive preﬁx adder architectures
to this model and get the predicted Pareto frontier solution
set. This is due to the merit of much faster runtime for a
machine learning model in prediction stage than running the
entire VLSI CAD ﬂow.

However, conventional machine learning problem aims at
maximizing the prediction accuracy rather than exploring a
Pareto frontier out of a solution set. Improving the model
accuracy does not necessarily improve the Pareto frontier and
the direct use of the ﬁtted model for Pareto frontier exploration
can even miss up to 60% Pareto frontier points [3]. We
therefore need a machine learning integrated Pareto frontier
exploration methodology, where the Pareto frontier selection
does not rely only on the model accuracy. So we develop a fast
yet effective algorithmic methodology, enabled by regression
model to explore the Pareto frontier of preﬁx adder solutions.
First we consider two spaces for Pareto frontier exploration:
the delay vs. area as well as the delay vs. power. For either
space, there exists a strong trade-off between the two metrics.
For delay vs. power space, we propose to use a joint output
Power-Delay function (P D) as the regression output rather
than using any single output.

P D = α · P ower + Delay.

(10)

The rationale of using scalarization [32] or the linear
summation of the power and delay metrics is that such a linear
relation provides a weighted bonding between the power and
the delay so that by changing the α value, the regression
model will try to minimize the prediction error on the more
weighted axis hence leads to more accuracy on that direction.
In contrast, the other metric direction will be predicted with
less accuracy hence introducing some level of relaxations. It
can be foreseen that changing the α value can lead to different

Fig. 9: Overall ﬂow of α-sweep learning.

supervised learning where a quasi-random data sampling is
performed to obtain the training data, followed by multi-
objective scalarization to achieve the Pareto optimal solutions.
The second one is the active learning approach where model
training is integrated to ﬁnding Pareto-optimal frontiers of the
design space.

IV. α-SWEEP LEARNING
In this section we propose a pareto-frontier exploration
ﬂow which is based on support vector machine. The overall
ﬂow of our α-sweep supervised learning-based Pareto-frontier
exploration is presented in Fig. 9.

A. Scalarization to the Single-Objective

In this work, supervised learning is preferred over unsu-
pervised learning since supervised learning has a substantial
advantage over unsupervised learning for our problem. In
particular, supervised learning allows to take advantage of
the golden result, i.e., the true area/power/delay, generated
by the synthesis tools for each design, instead of just letting
the algorithm work out for itself what the classes should be.
In general, supervised learning usually outperforms the unsu-
pervised learning for this kind of regression and classiﬁcation
tasks.

Before applying machine learning for exploring Pareto
frontier, we ﬁrst validate the effectiveness of the features we
extract by building regression models for single metric predic-
tion. For learning models, we explored (i) several supervised
learning techniques, such as linear regression, Lasso/Ridge,
Bayesian ridge model and support vector regression (SVR)
with linear, polynomial and radial-basis-function (RBF) ker-
nel, and (ii) 36 features, including 4 primary features, size,
mf o,
target delay and utilization (tool settings), and 32
secondary features for spf o. We observed that we could get
an R2 score above 0.95 for area and power even with primary
features and linear models. However, we don’t get good scores

ﬁtting accuracies of the regression model. By sweeping α
over a wide range from 0 to large positive values, each time
the regression model will be ﬁtted to predict different best
solutions which altogether form the Pareto frontier. We call
this approach α-sweep. Note that, the P ower and Delay
values in Equation (10) are normalized and scaled to the range
between 0 and 1 by Equation (11).
x − min(X)
max(X) − min(X)

, x ∈ X.

(11)

x =

Similarly, we have a joint output Area-Delay (AD) function

for Pareto frontier exploration on Area and Delay space.

AD = α · Area + Delay.

(12)

This α-sweep technique can be extended to simultaneously
consider power, performance or delay, and area (PPA), using
two scalars (α1 and α2) instead of one scalar factor α. The
joint output function for Pareto frontier exploration on area –
power – delay space can be formulated as:

P P A = α1 · Area + α2 · P ower + Delay.
The results of α-sweep for both two-dimensional space and

(13)

three-dimensional space are shown in the Section VI.

V. PARETO ACTIVE LEARNING
In our adder design problem, obtaining the true area/pow-
er/delay values or the labeled data for each adder requires
running logic synthesis and physical design ﬂow, which is
often time-consuming if the amount of data is huge. Active
learning is an iterative supervised learning which is able to
interactively query the data pool to obtain the desired outputs
at new data points. Since the samples are selected by the
learning algorithm, the number of samples to ﬁt a model can
often be much lower than the number required in traditional
supervised learning. Since an active sampling strategy is
required in active learning, an “uncertainty estimation” of
the prediction is needed. Gaussian Process (GP) can make
predictions and, more importantly, provide the uncertainty
estimation of its predictions by nature. Therefore, in this paper
we further propose a Pareto active learning algorithm based
on Gaussian Process regression.

A. Overall Flow

The overall ﬂow of the Pareto active learning (PAL) is
shown in Fig. 10. Given all the preﬁx adder structures, ﬁrst
we extract the feature vector for each adder as introduced
in Section III-C. The active learning starts with Gaussian
Process regression which will be illustrated later. Unlike the
passive supervised learning in which all
the features and
the corresponding labels are prepared in advance, the active
learning derives the labels of each training data during the
learning process on-demand. To be speciﬁc, the algorithm in-
crementally identiﬁes the most representative instances along
with their features which are later fed into EDA synthesis ﬂow
(synthesis, placement and routing) for true area/power/delay
numbers. Namely, the EDA synthesis ﬂow and the learning
process are interleaving. As more and more designs being
selected, the model gets more and more accurate till conver-
gence.

Fig. 10: Overall ﬂow of Pareto active learning.

B. Gaussian Process Prediction

A Gaussian process is speciﬁed by its mean function and
covariance function. A Pareto active learning scheme based
on Gaussian process regression is proposed in [33]. The prior
information is important to train the Gaussian Process model,
which is a parameterized mean and covariance functions.
Conventionally, the training process selects the parameters in
the light of training data such that the marginal likelihood
is maximized. Then the Gaussian Process model can be
obtained and the regression can be proceeded with supervised
input [34]. The ability of GP indicating prediction uncer-
tainty reﬂects in GP learner providing a Gaussian distribution
N (m(x), σ(x)) of the values predicted for any test input x
by computing
m(x) = k(x, X)(cid:62)(k(X, X) + σ2I)−1Y,
σ2(x) = k(x, x) − k(x, X)(cid:62)(k(X, X) + σ2I)−1k(x, X),

(14)
where X is the training set, Y is the supervised information
of trained set X. For Gaussian Process regression, a prediction
of a design objective consists of a mean and a variance.
The mean value m(x) represents the predicted value and the
variance σ(x) represents the uncertainty of the prediction.

C. Active Learning Algorithm

The ability of GP learners in quantifying prediction un-
certainty enables a suitable application for active learning.
Basically, three sets are maintained during the PAL process,
including a set of Pareto-optimal designs (P ), non-Pareto-
optimal designs (N ) and ‘unclassiﬁed’ designs (U ).

The GP models with discrepant prior are applied to learn
the objective functions farea(x), fpower(x), fdelay(x). PAL

calls GP inference to predict the mean vector m(x) and the
standard deviation vector σ(x) of all unsampled x in the
design space based on Equation (14). Unlike other regression
models such as linear regression and support vector regres-
sion, whose outputs are in form of numerical or categorical
result, the output of GP is a distribution where uncertainties
are involved. To capture the prediction uncertainty for a design
x, a hyper-rectangle is deﬁned as

HR(x) = {y : mi(x) − β

2 σi(x) ≤ yi ≤ mi(x) + β

2 σi(x)},

1

1

where i ∈ {1, 2, 3}, corresponding to area, power and delay
metrics in physical space. β is a user-deﬁned parameter
which determines the impact of σi(x) on the region. In our
implementation, β is set to 16 based on the analysis in [33],
[35].

As shown in Fig. 10, the PAL algorithm is an iterative
process. A few new points are selected in each iteration, and
the GP model is retrained with new training set. Note that
the model is supposed to be more and more accurate as more
data being sampled. Therefore, the uncertainty region should
be smaller and smaller. In order to ensure the non-increasing
monotonicity of the uncertainty region while sampling and
incorporating the previous evaluations, the uncertainty region
of x in the (t + 1)-th iteration is deﬁned as

Rt+1(x) = Rt(x) ∩ HR(x),
(15)
where the initial R0 = Rn which is the entire objective space.
The numbers of designs in Pareto-optimal set P and
non-Pareto-optimal set N are non decreasing as iteration t
increments. Thus, at iteration t, the points in P and N keep
their classiﬁcation. Intuitively, if one wants to compare the
predicted performance of two designs, two extreme cases,
i.e., optimistic prediction min(Rt(x)) and the pessimistic
prediction max(Rt(x)) of each design, can be applied. If
the optimistic prediction of design x is dominated by the
pessimistic prediction of other design x(cid:48), then x is classiﬁed
as non-Pareto-optimal; And if the pessimistic prediction of
design x is not dominated by optimistic prediction of any
other design x(cid:48), then x is classiﬁed as Pareto-optimal; A
design will remain unclassiﬁed if neither condition holds.
Fig. 11 is presented here as an example.

In the implementation, an error tolerance δ with value 0.001
is applied during classiﬁcation. The rules for classiﬁcation can
be represented as follows.

P,

if max(Rt(x)) ≤ min(Rt(x(cid:48))) + δ,
if max(Rt(x(cid:48))) ≤ min(Rt(x)) + δ,

(16)






x ∈

N,
U, otherwise.

After classiﬁcation in each iteration, a new adder design
with the largest length of the diagonal of its uncertainty region
R(x) is selected for sampling. The value is attached to x as

wt(x) = max

y,y(cid:48)∈Rt(x)

||y − y(cid:48)||2.

(17)

Intuitively, Equation (17) picks the points which are most
worthy exploring. Afterwards, these designs are going through
EDA ﬂow to get the real area, power and delay numbers, and

Fig. 11: An example of classiﬁcation.

the GP model will hence be improved with those feedback
results.

Algorithm 1 Active Learning for Pareto-frontier Exploration

Require: Adder architectural design space E, GP prior, max-

imum iteration number Tmax;
Ensure: predicted Pareto-optimal set ˆP ;
1: P ← ∅, N ← ∅, U ← E;
2: Randomly select a small subset X = {xi} of E;
3: Get true values Y = {yi|yi = EDAFlow(xi)};
4: S ← X;
5: R0(x) ← Rn, ∀x ∈ E;
6: t ← 0;
7: while U (cid:54)= ∅ and t < Tmax do
8:
9:
10:
11:

Building GP model with {(xi, yi) : ∀xi ∈ S};
Obtain Rt(x), ∀x ∈ E;
for all x ∈ U do

if x is Pareto-optimal based on Equation (16)

P.add(x), U.delete(x);

else if x is non-Pareto-optimal based on Equa-

then

12:
13:

14:
15:

tion (16) then

end if

16:
17:
18:
19:
20:
21:
22: end while
23: ˆP ← P ;

N.add(x), U.delete(x);

end for
Obtain wt(x), ∀x ∈ (U ∪ P ) \ S;
Choose x(cid:48) ← argmax{wt(x)};
S ← S ∪ x(cid:48);
t ← t + 1;
Obtain new data (x(cid:48), y(cid:48)) by running EDA ﬂow;

The entire process is presented in Algorithm 1. It starts with
the initialization (lines 1–6). In each iteration, the GP model
is trained with the current training set S, and the uncertainty
region for each design is obtained (lines 8–9). Then the
designs in the U set are classiﬁed based on uncertainty regions
and classiﬁcation rules (lines 10–16). After that, the design
with the largest uncertainty is sampled and the sampling set
S is updated (lines 17–19). The newly sampled design is fed
into synthesis tools to get the label which is used for training
GP model in the next iteration (line 21). The learning process
stops after all adder designs in architectural design space are
classiﬁed. The prediction is ˆP = P (line 23). Suppose Tmax
is the maximum number of iterations, and |E| is the size of

solution set, then the complexity of Algorithm 1 is at most
O(Tmax|E|), as maximum size of U can be |E|. However, it
should be stressed that although there are Tmax|E| operations
for PAL algorithm, the cost of each operation (which is a
simple inference based on the Gaussian Process Regression
model) is negligible in comparison to EDA synthesis ﬂow
run-time, and we will demonstrate later in TABLE IV that
the total run-time of different approaches are dictated by the
number of EDA synthesis ﬂow runs needed in the respective
approaches.

VI. EXPERIMENTAL RESULTS

In this section we show the effectiveness of the proposed
algorithms and methodologies. First we compare the physical
solution space before/after applying PGG algorithm. Then the
Pareto frontier obtained by α-sweep is presented. Next, we
demonstrate the Pareto frontier obtained by active learning,
and compare the quality of Pareto frontiers generated by two
approaches. Finally, we compare our explored optimal adders
against legacy adders.

Since high performance adders are commonly used in CPU
architectures which are typically 64 bit, we have mainly
presented the results for 64 bit adders to demonstrate the
methodology. However, the approach is very general to be
used for adders of arbitrary bit-width. The ﬂow is imple-
mented in C++ and Python on Linux machine with 72GB
RAM and 2.8GHz CPU. We use Design Compiler [28]
(version F-2011.09-SP3) for logical synthesis, and IC Com-
piler [29] (version J-2014.09-SP5-3) for the placement and
routing. "tt1p05v125c" corner and Non Linear Delay
Model (NLDM) in 32nm SAED cell-library for LVT class
[30] (available by University Program) is used for technology
mapping. Primary input activity of 0.1 is used along with
1GHz operating frequency for power estimation. Regarding
the tool settings, target delays of 0.1ns, 0.2ns, 0.3ns and
0.4ns are used. Utilization values are set to 0.5, 0.6, 0.7 and
0.8. We used Python based machine learning package scikit-
learn [36] for the predictions. Throughout our all experiments,
the run time for machine learning predictions is less than a
minute.

We relied more on the ﬁdelity of the SAED library rather
than accuracy considering that SAED library may not be
very realistic as that used in industry. For instance, the FO4
delay for a unit sized inverter for this library in the operating
corner is 36ps [20], [37]. So 11 FO4 delay, typically being
presented to be the delay for 64-bit adders in literatures [38], is
approximately 400ps which is close to the reported delays for
64-bit adders in our work. To further demonstrate the ﬁdelity
of this library, we run the Kogge-Stone adders with bit-widths
of 8, 16, 32, 64, 128 and 256 through the synthesis ﬂow
using this library. Then we normalize the measured delay in
terms of FO4 delay, and plot it with bit-width (n) as shown
in Fig. 12. It can be seen that the delay is linear with log2 n,
which is expected for a logarithmic tree adder such as Kogge-
Stone adder. So we believe if this algorithmic methodology
is applied to more realistic industrial libraries, it can show
similar beneﬁt as demonstrated with SAED 32nm library.

y
a
l
e
D

16

12

8

4

0

8

64 128 256

32

16
Adder bit-width

Fig. 12: Delay values (× FO4 delay) of Kogge-Stone adders
with various bit-width.

(a)

(b)

Fig. 13: (a) Pareto Frontier: area vs. delay; (b) Pareto Frontier:
power vs. delay.

To validate the optimality and the hypervolume error of
the two learning approaches against the real world solution
space, we need to run the logical/physical EDA ﬂow on a
large set of adder solutions. Our machine and tool set takes
about 5.5 minutes to complete this full ﬂow of a single
preﬁx adder. Therefore, we select a reasonable number (3000)
of preﬁx adder solutions, which eventually took about 300
hours to complete, but still a comparatively larger data set
in comparison to our training data set. Crucially, those 3000
adders are also sampled in a Quasi-random manner in order
to represent the entire solution space.

A. Pareto Frontier Predicted by α-sweep Learning

In

this

learning

100 , 50, 1

experiment, we

show the
approach. We

15
20 , 10, 1
50 , 20, 1

apply
different α values
8 , 2, 1

effectiveness
the
of
our α-sweep
of
α-sweep method with
(1000, 0, 100, 1
and
collect the best 150 solutions for delay-area and delay-power
spaces where for each α value, the best 10 architectures with
lowest P D or AD values are fed into the logical/physical
EDA ﬂow to generate similar Pareto points. Note that
15 + 15 = 30 learning models have been derived for this for
all, but it is very fast as the same training data have been
used, and the models are regression based.

10 , 8, 1

2 , 1),

Fig. 13(a) and Fig. 13(b) respectively show the correspond-
ing Pareto frontiers of the α-sweep approach and the ground
truth Pareto frontiers for the 3000 representative adders. Each
dot
in the delay-area or delay-power space indicates one
adder solution after going through the logical/physical EDA
ﬂow. We can see that generally the predicted Pareto frontier
solutions are fairly close to the real Pareto frontier, with some
exceptions. Overall, the proposed approach can effectively

TABLE II: Comparison of different model accuracies

Model

Original
Noisy

Area
0.003
0.024

MSE
Power
0.027
0.951

Delay
0.170
0.711

Area-Delay
0.139
0.168

Hypervolume error
Power-Delay
0.122
0.148

Area-Power-Delay
0.154
0.162

TABLE III: Pareto frontiers for PAL vs. α-sweep [39]

Objective Hypervolume error

PAL

α-sweep [39]

in predicting Pareto frontier in all design spaces, including
area-delay, power-delay, and area-power-delay spaces.

Area-Delay

Power-Delay

Area-Power-Delay

average
best
average
best
average
best

0.100
0.044
0.109
0.075
0.056
0.039

0.139
0.093
0.122
0.076
0.154
0.125

Notes: All hypervolume error above are collected from 1000
repeated experiments.

achieve near optimal Pareto frontier without affording to
spend expensive runtime on every adder. So this learning
based methodology can be readily adopted to achieve Pareto
frontiers for much larger solution space which is intractable
for exhaustive exploration by conventional design ﬂow.

We have conducted additional experiments to show the
impacts of the low accuracy of the machine learning model.
The basic idea is to inject random noise in the prediction stage,
i.e., additional Gaussian noise is added into the predicted
value. The accuracy will be lower than original results. Then
we explore the Pareto frontier based on the noisy prediction.
Generally, the quality of the ﬁnal Pareto frontier is worse than
original model. The comparison of Pareto frontier quality is
presented in TABLE II.

B. Comparison of the Quality of Pareto-Frontier between PAL
and α-sweep

We implement PAL to predict Pareto-optimal designs in
both two-dimensional design spaces which are area-delay
space and power-delay space, as well as three-dimensional
space which is area-power-delay space. The results are com-
pared with those of [39]. The initial input set for both area-
delay and power-delay is of size 250, which are randomly se-
lected from the exhaustive design space. The curves of Pareto
frontiers for two-dimensional spaces are shown in Fig. 13.
The hypervolume of area-delay Pareto frontiers are calculated
with reference point (max(delay), max(area)). Similarly, The
hypervolume of power-delay Pareto frontiers are calculated
with reference point (max(delay), max(power)). Note that the
unit for delay is nanosecond (ns) when calculating the hyper-
volume. It should be stressed that there is a sort of randomness
in both α-sweep and PAL algorithm. For α-sweep, the training
set is selected randomly. On the contrary, the initial set in PAL
is randomly selected (line 2), thereby may result in different
outputs. So the experiments are conducted for 1000 times such
that the general performance is reﬂected. The comparison
between two approaches are shown in TABLE III. Comparing
the hypervolume error of Pareto frontier obtained by PAL and
α-sweep, it can be seen that PAL achieves better performance

C. Runtime Comparisons among Exhaustive Approach, α-
sweep and PAL

There are three factors that will affect the runtime: (i) the
total number of EDA synthesis runs required; (ii) Among all
these required EDA synthesis runs how many of them can
be parallelized; (3) The runtime of the training process in
machine learning model. All these details are recorded in
TABLE IV. The ‘INIT’ represents the set of training data
in the α-sweep and the initial set in PAL, which can be par-
allelized because all the points are obtained in advance. The
‘AS’ represents the set of designs which are actively sampled
during the learning process, which cannot be parallelized. The
α-sweep approach does not involve active sampling, so the
‘AS’ set is none here. The ‘VERI’ represents the set of designs
which are predicted to be Pareto-optimal. We should run EDA
synthesis ﬂow to get the real PPA values of these designs to
extract the Pareto-frontier. This set of designs are obtained
after the learning process stops, so the EDA synthesis runs
on these designs are also conducted ofﬂine, which can be
parallelized. Each EDA synthesis run takes about 5.5 minutes.
Then we can compare the total runtime of different ex-
ploration methodologies. For exhaustive exploration, all the
preﬁx adders should be fed into EDA tools for synthesiz-
ing to obtain the value of each metric, which is extremely
time-consuming. There is no training, additional sampling,
veriﬁcation. The total runtime cost involves EDA ﬂows of all
the designs in the design space. The Pareto frontier can be
extracted from the results, whose runtime is much less than
synthesizing and can be neglected. The total runtime is

Texh =

5.5 × #INIT
#Machines

.

(18)

Since the entire solution space is so huge that one can hardly
run all of them, in our experiment, we sample representative
10K designs by random sampling. The total runtime of
synthesizing is about 55000 minutes with single machine. It
should be noted that the entire solution space is much more
than 10K.

In the exploration by α-sweep, not all adders in the design

space are needed for synthesizing. The total runtime is

Tα =

5.5 × (#INIT + #VERI)
#Machines

+ Modeling time.

(19)

In our experiment, we select 2500 of the designs out of those
10K designs by random sampling to build the model, includ-
ing training and testing phases. It takes about 1.5 minutes
to build the model and make predictions. When exploring in
area-power-delay design space, 150 designs on average in the

TABLE IV: Comparison of runtime with single machine among different approaches

Method

#INIT

#AS

#VERI

#Total

Exhaustive
α-sweep
PAL

10000
2500
700

-
-
10

-
150
290

10000
2650
1000

Runtime (mins)
Modeling

EDA

55000.0
14575.0
5500.0

-
20.0
2.0

Total

55000.0
14595.0
5502.0

Notes: The designs in “#INIT” and “#VERI” can be synthesized in parallel. The number of designs in each category is collected from
1000 repeated experiments.

α-sweep
PAL

)
s
n
i
m

3
0
1
×

(

e
m

i
t
n
u
R

15

10

5

0

1 2 3 4 5 6 7 8 9 10
Number of machines

Fig. 14: Comparison of runtime with different number of
machines.

design space are predicted to be Pareto-optimal. So on average
2650 designs are needed. The runtime for synthesizing is
14575 minutes. Note that in terms of learning models, the
α-sweep method needs to build 15 × 15 = 225 models since
α1 and α2 both have 15 values to choose from.

Similarly, the runtime of PAL can be calculated by

TP AL =

5.5 × (#INIT + #VERI)
#Machines

+ 5.5 × #AS

+ Modeling time.

(20)

The size of initial set is ﬁxed, which is 700. It takes about 4
minutes to build the model and make predictions during the
PAL process. When exploring the Pareto-optimal designs in
area-power-delay space, 10 designs on average are sampled
during PAL. 290 designs on average in the design space are
predicted to be Pareto-optimal. In total, 1000 designs are
needed on average. The runtime is 5500 minutes with single
machine. PAL algorithm needs to build N models where N
is the number of iterations in PAL. In our implementation, the
maximum iteration is set to 20. It can be observed that the
active learning approach outperforms the α-sweep learning in
terms of both the quality of Pareto frontier and the number
of EDA ﬂow runs.

Note that all the runtime calculations are based on single
machine. However, the EDA synthesis runs in all three ﬂows
can be distributed to multiple machines if available, except the
adders sampled during active learning, which (10 on average
in our experiments) is very less in comparison to the total
number of the synthesis runs. So PAL can get a signiﬁcant
speedup over α-sweep and exhaustive approach with single
machine and multiple machines.

D. Comparison on Different Sampling Strategies in PAL

In the sampling stage of PAL, the number of instances to
be sampled has impact on the runtime since the EDA ﬂow is
required to obtain the real value for area, power and delay.
The less instances we sample in each iteration, the more
iterations are needed to ensure the PAL process converge,
which is more likely to result in less samples in total. The
more instances we sample in each iteration, the less iterations
are needed. However, the total number of sampled instances
would be large. In practice, the runtime cost of running EDA
ﬂow can be reduced by parallel execution if there are multiple
licenses available. In this section, we explore the effect of
different sampling strategies in terms of the total runtime and
the quality of Pareto frontier in practical scenarios.

The results for different sampling strategies are listed in
Fig. 15. Since the EDA ﬂow for synthesis, placement and
routing takes up the most signiﬁcant part of the total runtime
cost, the key factor is the number of adders which needs to
be through EDA tool ﬂow. If we have multiple machines
available for the EDA tool ﬂow, the runtime is determined
by the total number of iterations as long as the number of
samples does not exceed the number of machines. From the
result, it can be seen that we can obtain the Pareto-frontier
with comparable quality, using less runtime.

Note that when the sample size increases from 1 to 5,
the average hypervolume error increases from 0.056 to about
0.070, which is still less than 0.154 (average hypervolume
error achieved in α-sweep approach). Therefore, batch sam-
pling can not only take care of parallel synthesizing but also
achieve better quality for Pareto frontier than α-sweep, which
can also show the advantages of the PAL.

E. Adder Performance Comparison

Finally, we compare our explored adders against Design-
Ware adders, legacy adders, such as Kogge-Stone, Sklansky,
as well as a state-of-the-art adder synthesis algorithm in TA-
BLE V. Since our approach generates numerous solutions, it
is not feasible to perform a one-to-one comparison. Instead for
each of the solution points in regular adders and [19], we have
picked the Pareto points from our solution set which are able
to excel them in all metrics. For instance, P1 could provide
around 8ps better delay with respectively 14% and 12% lesser
area and power over Kogge-Stone adder. The DesignWare
adders are synthesized from behavioral description of adder
(Y = A + B) with the 16 conﬁgurations of tool settings
(Combination of 4 target delay and 4 utilization values) that
are used in generating the Fig. 4. We pick the one with best

s
n
o
i
t
a
r
e
t
i
#

e
g
a
r
e
v
A

10
8
6
4
2
0

s
r
e
d
d
a
#

e
g
a
r
e
v
A

1,000

990
980
970
960
950

r
o
r
r
e

v
h

e
g
a
r
e
v
A

0.080

0.060

0.040

0.020

0.000

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

(a)

(b)

(c)

Fig. 15: Comparison among different number of samples per iteration.

TABLE V: Comparison with other approaches for 64 bit
adders

Method
DesignWare
Ours (P1)
Kogge-Stone
Ours (P1)
Sklansky
Ours (P2)
[19]
Ours (P3)

Delay (ps)
346.5
339.0
347.9
339.0
356.1
353.0
348.7
343.0

Area (µm2)
2531.3
2180.8
2563.7
2180.8
1792.5
1753.0
1971.4
1912.6

Energy (f J/op)
8160
6930
8780
6930
6100
5900
6980
6390

delay, denoted by “DesignWare” in TABLE V. The same
pareto point P1 dominates that solution by providing around
7.5ps better delay, 14% lesser area, and 15% lesser energy.
For [19] we pick the best delay solution. Note for a ﬁxed
mf o, [19] can give preﬁx network with smaller size, but
this approach only provides a limited set of preﬁx structures.
As a result, it is hard for [19] to explore the full physical
design space of adders by machine learning. It should be
stressed that [19] beats the custom adders implemented in
an industrial design, and our methodology is able to excel the
adders generated by the algorithm presented in [19].

VII. CONCLUSION

This paper presents a novel methodology of machine
learning guided design space exploration for power efﬁcient
high-performance preﬁx adders. We have successfully demon-
strated the effectiveness of our learning models, developed
by training with quasi-random sampled data and features
encapsulating architectural and tool attributes. In addition,
an active learning approach is applied to ease the demand
of labeled data and achieves even better Pareto frontier.
Our adder synthesis algorithm is able to generate a wider
solution space in comparison to a state-of-the-art algorithm,
and when integrated with the learning model, could provide
a remarkable performance vs. power vs. area Pareto frontier
over a large representative solution space. To the best of our
knowledge, this is the ﬁrst work to bridge the gap between
architectural and physical solution space for parallel preﬁx
adders.

REFERENCES

[1] B. Yu, D. Z. Pan, T. Matsunawa, and X. Zeng, “Machine learning and
pattern matching in physical design,” in Proc. ASPDAC, 2015, pp. 286–
293.

[2] W.-T. J. Chan, K. Y. Chung, A. B. Kahng, N. D. MacDonald, and
S. Nath, “Learning-based prediction of embedded memory timing
failures during initial ﬂoorplan design,” in Proc. ASPDAC, 2016, pp.
178–185.

[3] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[4] W.-H. Chang, L.-D. Chen, C.-H. Lin, S.-P. Mu, M. C.-T. Chao, C.-
H. Tsai, and Y.-C. Chiu, “Generating routing-driven power distribution
networks with machine-learning technique,” in Proc. ISPD, 2016, pp.
145–152.

[5] R. Samanta, J. Hu, and P. Li, “Discrete buffer and wire sizing for link-
based non-tree clock networks,” in Proc. ISPD, 2008, pp. 175–181.
[6] R. P. Brent and H. T. Kung, “A regular layout for parallel adders,” IEEE
Transactions on Computers, vol. C-31, no. 3, pp. 260–264, 1982.
[7] P. M. Kogge and H. S. Stone, “A parallel algorithm for the efﬁcient
solution of a general class of recurrence equations,” IEEE Transactions
on Computers, vol. 100, no. 8, pp. 786–793, 1973.

[8] T. Han and D. Carlson, “Fast area-efﬁcient VLSI adders,” Proc. ARITH,

pp. 49–56, 1987.

[9] J. Sklansky, “Conditional sum addition logic,” IRE Trans. on Electronic

Computers, vol. EC-9, no. 2, pp. 226–231, 1960.

[10] C. Zhou, B. M. Fleischer, M. Gschwind, and R. Puri, “64-bit preﬁx
adders: Power-efﬁcient topologies and design solutions,” Proc. CICC,
pp. 179–182, 2009.

[11] J. Liu, Y. Zhu, H. Zhu, C.-K. Cheng, and J. Lillis, “Optimum preﬁx
adders in a comprehensive area, timing and power design space,” in
Proc. ASPDAC, 2007, pp. 609–615.

[12] N. H. Weste and D. Harris, CMOS VLSI Design: A Circuits and Systems

Perspective. Addison Wesley, 2004.

[13] T. Matsunaga and Y. Matsunaga, “Area minimization algorithm for par-
allel preﬁx adders under bitwise delay constraints,” in Proc. GLSVLSI,
2007, pp. 435–440.

[14] J. Liu, S. Zhou, H. Zhu, and C.-K. Cheng, “An algorithmic approach
for generic parallel adders,” in Proc. ICCAD, 2003, pp. 734–740.
[15] J. P. Fishburn, “A depth-decreasing heuristic for combinational logic;
or how to convert a ripple-carry adder into a carry-lookahead adder or
anything in-between,” in Proc. DAC, 1990, pp. 361–364.

[16] R. Zimmermann, “Non-heuristic optimization and synthesis of parallel

preﬁx adders,” Proc. IWLAS, pp. 123–132, 1996.

[17] M. Snir, “Depth-size trade-offs for parallel preﬁx computation,” Journal

of Algorithms, vol. 7, no. 2, pp. 185–201, 1986.

[18] H. Zhu, C.-K. Cheng, and R. Graham, “Constructing zero-deﬁciency
parallel preﬁx adder of minimum depth,” in Proc. ASPDAC, 2005, pp.
883–888.

[19] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” in Proc. ASPDAC, 2015, pp. 249–254.

[20] ——, “Towards optimal performance-area trade-off in adders by syn-
thesis of parallel preﬁx structures,” IEEE TCAD, vol. 33, no. 10, pp.
1517–1530, 2014.

[21] M. Choudhury, R. Puri, S. Roy, and S. C. Sundararajan, “Automated
synthesis of high performance two operand binary parallel preﬁx adder,”
Patent US 8,683,398, Mar, 2014.

[22] G. Palermo, C. Silvano, and V. Zaccaria, “ReSPIR: a response surface-
based pareto iterative reﬁnement for application-speciﬁc design space
exploration,” IEEE TCAD, vol. 28, no. 12, pp. 1816–1829, 2009.
[23] H.-Y. Liu and L. P. Carloni, “On learning-based methods for design-
space exploration with high-level synthesis,” in Proc. DAC, 2013, pp.
50:1–50:7.

[24] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[25] B. R. Zeydel, T. T. J. H. Kluter, and V. G. Oklobdzija, “Efﬁcient
mapping of addition recurrence algorithms in CMOS,” Proc. ARITH,
pp. 107–113, 2005.

[26] M. Ketter, D. M. Harris, A. Macrae, R. Glick, M. Ong, and J. Schauer,
“Implementation of 32-bit Ling and Jackson adders,” Proc. Asilomar,
pp. 170–175, 2011.

[27] E. Zitzler, D. Brockhoff, and L. Thiele, “The hypervolume indicator
revisited: On the design of pareto-compliant indicators via weighted
integration,” in Evolutionary multi-criterion optimization, 2007, pp.
862–876.

[28] “Synopsys Design Compiler,” http://www.synopsys.com.
[29] “Synopsys IC Compiler,” http://www.synopsys.com.
[30] “Synopsys SAED Library,” http://www.synopsys.com/Community/
UniversityProgram/Pages/32-28nm-generic-library.aspx, accessed 23-
April-2016.

[31] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-
tions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.

[32] K. Tumer and J. Ghosh, “Estimating the bayes error rate through
classiﬁer combining,” in Proc. ICPR, vol. 2, 1996, pp. 695–699.
[33] M. Zuluaga, A. Krause, G. Sergent, and M. P¨uschel, “Active learning
for multi-objective optimization,” in Proc. ICML, 2013, pp. 462–470.
[34] C. E. Rasmussen and C. K. I. Williams, Gaussian Process for Machine

Learning. The MIT Press, 2006.

[35] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Gaussian process
optimization in the bandit setting: no regret and experimental design,”
in Proc. ICML, 2010, pp. 1015–1022.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in Python,” Journal of Machine Learn-
ing Research, vol. 12, no. Oct., pp. 2825–2830, 2011.

[37] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” IEEE TCAD, vol. 35, no. 5, pp. 820–831, 2016.

[38] T. McAuley, W. Koven, A. Carter, P. Ning, and D. M. Harris, “Imple-
mentation of a 64-bit Jackson adders,” Proc. Asilomar, pp. 1149–1154,
2013.

[39] S. Roy, Y. Ma, J. Miao, and B. Yu, “A learning bridge from architec-
tural synthesis to physical design for exploring power efﬁcient high-
performance adders,” in Proc. ISLPED, 2017, pp. 1–6.

Cross-layer Optimization for High Speed Adders:
A Pareto Driven Machine Learning Approach

Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, and Bei Yu

8
1
0
2
 
t
c
O
 
6
1
 
 
]

R
A
.
s
c
[
 
 
2
v
3
2
0
7
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—In spite of maturity to the modern electronic design
automation (EDA) tools, optimized designs at architectural stage
may become sub-optimal after going through physical design
ﬂow. Adder design has been such a long studied fundamental
problem in VLSI industry yet designers cannot achieve optimal
solutions by running EDA tools on the set of available preﬁx
adder architectures. In this paper, we enhance a state-of-the-
art preﬁx adder synthesis algorithm to obtain a much wider
solution space in architectural domain. On top of
that, a
machine learning-based design space exploration methodology
the adders in
is applied to predict
physical domain, which is infeasible by exhaustively running
EDA tools for innumerable architectural solutions. Considering
the high cost of obtaining the true values for learning, an active
learning algorithm is proposed to select the representative data
during learning process, which uses less labeled data while
achieving better quality of Pareto frontier. Experimental results
demonstrate that our framework can achieve Pareto frontier of
high quality over a wide design space, bridging the gap between
architectural and physical designs. Source code and data are
available at https://github.com/yuzhe630/adder-DSE.

the Pareto frontier of

I. INTRODUCTION

I N the last decades, the industrial EDA tools have advanced

towards optimality, especially at the individual stages of
VLSI design cycle. Nevertheless, with growing design com-
plexity and aggressive technology scaling, physical design
issues have become more and more complex. As a result,
the constraints and the objectives of higher layers, such as
the system or logic level, are very difﬁcult to be mapped
into those of lower layers, such as physical design, and vice-
versa, thereby creating a gap between the optimality at the
logic stage and the physical design stage. This necessitates
the innovation of data-driven methodologies, such as machine
learning [1]–[5], to bridge this gap.

Adder design is one of the fundamental problems in digital
semiconductor industry, and its main bottleneck (in terms
of both delay and area) is the carry-propagation unit. This
unit can be realized by hundreds of thousands of parallel
preﬁx structures, but it is hard to evaluate the ﬁnal metrics
without running through physical design tools. Historically,
regular adders [6]–[9] have been proposed for achieving the
corner points in terms of various metrics as shown in Fig. 1

The preliminary version has been presented at the IEEE International
Symposium on Low Power Electronics and Design (ISLPED) in 2017. This
work is supported in part by The Research Grants Council of Hong Kong SAR
(Project No. CUHK24209017) and CUHK Undergraduate Summer Research
Internship 2017.

Y. Ma, J. Chen and B. Yu are with the Department of Computer Science

and Engineering, The Chinese University of Hong Kong, NT, Hong Kong.

S. Roy is with Intel Corporation, San Jose, CA, USA.
J. Miao is with the Cadence Design Systems, San Jose, CA, USA.

Fig. 1: Regular adders (picture taken from [12]).

in architectural stage. The main motivation for structural
regularity was the ease of manual layout, but EDA tools now
taking care of all physical design aspects, the regularity is
no longer essential. Moreover, the extreme corners do not
map well to the physical design metrics after synthesis, place-
ment and routing. To address this gap between preﬁx adder
synthesis and actual physical design of the adders, custom
adders are typically designed by tuning parameters, such as
gate-sizing, buffering etc., targeting at the optimization of
power/performance metrics for a speciﬁc technology library
[10], [11]. However, this custom approach (i) needs signiﬁcant
engineering effort, (ii) is not ﬂexible to Engineering Change
Order (ECO), and (iii) does not guarantee the optimality.

The algorithmic synthesis approach resolves the ﬁrst two
issues of the custom approach, by adding more ﬂexibility to
the late ECO changes and reducing the engineering effort.
Based on the number of solutions, the existing adder synthesis
algorithms can be broadly classiﬁed into two categories. The
ﬁrst and the most common approach is to generate a single
preﬁx network for a set of structural constraints, such as the
logic level, fan-out etc. Several algorithms have been proposed
to minimize the size of the preﬁx graph (s) under given bit-
width (n) and logic-level (L) constraints [13]–[16]. Closed
form theoretical bounds for size-optimality are provided by
[17] for L ≥ 2 log2 n − 2. [18] has given more general bound
for preﬁx graph size, but when L is reduced to log2 n, a pre-
requisite for high-performance adders, there is no closed form
bound for s. [19] presents a polynomial-time algorithm for

generating preﬁx graph structures by restricting both logic-
level and fan-out. The limitations in these approaches are
two-fold, (i) this restricted set of structures is not capable
of exploring the large solution space, and (ii) since it is very
hard to analytically model the physical design complexities,
such as wire-length and congestion issues, the physical design
metrics, such as the area, power, delay etc., may not be
mapped well to the preﬁx structure metrics, such as the size,
max-fan-out (mf o) etc. This motivates the second category
of algorithms where thousands of preﬁx adder solutions can
be generated and explored for synthesis and physical design
in the commercial EDA tools.

One such approach is [20] which presents an exhaus-
tive bottom-up enumeration technique with several pruning
strategies to generate innumerable preﬁx structure solutions.
However, it has two issues, (i) this approach cannot provide
solutions in several cases for restricted fan-out, which can
control the congestion and load-distribution during physical
design [19]. As a result, it may still miss the good solution
space to a large extent, and (ii) it is computationally very
intensive to run all solutions through synthesis, placement and
routing.

In this paper, we enhance the algorithm in [20], [21] to
generate adders under any arbitrary mf o constraint, which
enables a wider adder solution space in logical form. To
tackle the high computational effort during the physical design
ﬂow, we further propose to use machine learning to perform
the design space exploration in physical solution space. We
develop Pareto frontier driven machine learning methodolo-
gies to achieve rich adder solutions with trade-offs among
power, area, and delay. As a passive supervised learning, the
proposed quasi-random sampling approach is able to select
representative preﬁx adders out of the hundreds of thousands
of preﬁx structures.

It should be noted that various machine learning algorithms
have been investigated to explore design space in different
design scenarios. Palermo et al. [22] deploy both linear
regression and artiﬁcial neural network for multiprocessor
systems-on-chip design. Lin et al. [23] present a random
forest-based learning model in high level synthesis, which
can ﬁnd an approximate Pareto-optimal designs effectively.
Meng et al. [24] propose a random forest-based method for
Pareto frontier exploration, where non-Pareto-optimal designs
are carefully eliminated through an adaptive strategy. Multiple
predictions can be obtained through random forest, which can
be used for estimating the uncertainty. Superior to the random
forest, in this paper we further propose an active learning
approach based on Gaussian Process (GP), which by nature
can estimate the prediction uncertainty efﬁciently.

Our main contributions are summarized as follows:
• A comprehensive framework for optimal adder search
by machine learning methodology bridging the preﬁx
architecture synthesis to the ﬁnal physical design;

• An enhancement to a state-of-the-art preﬁx adder algo-
rithm [20] to optimize the preﬁx graph size for restricted
fan-out and explore a wider solution space;

• A machine learning model for preﬁx adders, guided by
quasi-random data sampling with features considering

architectural attributes and EDA tool settings;

• A design space exploration method to generate the Pareto
frontier for delay vs. power/area over a wide design
space;

• An active learning approach for the design space explo-
ration, which uses less labeled data and achieves better
quality of Pareto frontier.

The rest of the paper is organized as follows. Section II
presents the background of preﬁx adder synthesis, while
Section III discusses our preﬁx graph generation algorithm.
Next, two machine learning approaches of design space explo-
ration for high-performance adders are described. Section IV
presents the passive supervised learning, while Section V
introduces a Pareto frontier driven active learning approach.
Section VI lists the experimental results, followed by conclu-
sion in Section VII.

II. PREFIX ADDER SYNTHESIS

In this section, we ﬁrst provide the background of the preﬁx
adder synthesis problem. Then we present a brief discussion
on the algorithm presented in [20], which we enhance to our
Preﬁx Graph Generation (PGG) algorithm to synthesize the
preﬁx adder network.

A. Preliminaries

An n bit adder accepts two n bit addends A = an−1..a1a0
and B = bn−1..b1b0 as input, and computes the output
sum S = sn−1..s1s0 and carry out Cout = cn−1, where
si = ai ⊕ bi ⊕ ci−1 and ci = aibi + aici−1 + bici−1. The
simplest realization for the adder network is the ripple-carry-
adder, but with logic level n − 1, which is too slow. For faster
implementation, carry-lookahead principle is used to compute
the carry bits. Mathematically, this can be represented with
bitwise (group) generate function g (G) and propagate func-
tion p (P ) by the Weinberger’s recurrence equations as follows
[25]:

• Pre-processing (inputs): Bitwise generation of g, p

gi = ai · bi and pi = ai ⊕ bi.

(1)

• Preﬁx processing: This part is the main carry-propagation
component where the concept of generate/propagate is
extended to multiple bits and G[i:j], P[i:j] (i ≥ j) are
deﬁned as

(cid:26) pi,

(cid:26) gi,

P[i:j] =

G[i:j] =

if i = j,

P[i:k] · P[k−1:j], otherwise,

G[i:k] + P[i:k] · G[k−1:j], otherwise.

if i = j,

(2)

(3)

The associative operation ◦ is deﬁned for (G, P ) as:

(G, P )[i:j] = (G, P )[i:k] ◦ (G, P )[k−1:j]

= (G[i:k] + P[i:k] · G[k−1:j], P[i:k] · P[k−1:j]).
(4)

• Post-processing (outputs): Sum/Carry-out generation

si = pi ⊕ ci−1,

ci = G[i:0], and Cout = cn−1.

(5)

The ‘Preﬁx processing’ or carry propagation network can be
mapped to a preﬁx graph problem with inputs ik = (pk, gk)
and outputs ok = ck, such that ok depends on all previous
inputs ij (j ≤ k). Any node except the input nodes is called
a preﬁx node. Size of the preﬁx graph is deﬁned as the number
of preﬁx nodes in the graph. Fig. 2 shows an example of such
preﬁx graph of 6 bit and we can see that Cout = c5 = o5 is
given by

o5 = (i5 ◦ i4) ◦ ((i3 ◦ i2) ◦ (i1 ◦ i0)).

(6)

Size (s), logic level (L) and maximum-fan-out (mf o) for
this network are respectively 8, 3 and 2. Note that here the
number of fan-ins for each of the associative operation o is
two, thus this is called radix-2 implementation of the preﬁx
graph. However, there exist other options such as radix-3 or
radix-4, but the complexity is very high and not beneﬁcial in
static CMOS circuits [26]. In this work, the logic levels for
all output bits are log2 n, i.e., the minimum possible, to target
high performance adders.

B. Discussion on [20]

Our PGG algorithm to generate the preﬁx graph structures
for physical solution space exploration is based on [20]. So
it is imperative to ﬁrst discuss about [20]. However, we omit
the details and only mention the key points of [20] due to
space constraint.

[20] is an exhaustive bottom-up and pruning based enu-
meration technique for preﬁx adder synthesis. This work
presented an algorithm to generate all possible n + 1 bit
preﬁx graph structures from any n bit preﬁx graph. Then
this algorithm is employed in a bottom-up fashion (from 1
bit adder to 2 bit adders, then from all 2 bit adders to 3
bit adders, and so on) to synthesize preﬁx graphs of any
bit-width. As a result, scalability issue arises due to the
exhaustive nature of the algorithm, which is then tackled by
adopting various pruning strategies to scale the approach.
However, the pruning strategies are not sufﬁcient to scale
the algorithm well for different fan-out constraints. So when
it intends to ﬁnd the solutions for higher bit adders, the
intermediate adder solutions that need to be generated are
often huge. Consequently, it fails to get fan-out restricted
(e.g. when mf o = 8, 10, 12 etc. for 64 bit adders) solutions
even with 72GB RAM due to the generation of innumerable
intermediate solutions [19]. Pruning strategies, such as size-
bucketing [20], help to achieve solutions in some cases, but
with sub-optimality. So design space-exploration based on
this algorithm can miss a signiﬁcant spectrum of the adder
solutions.

C. Our PGG Algorithm

To better explore the wide design space of adders, in this
paper we have enhanced [20] for different fan-out constraints
by incorporating more pruning techniques.

Fig. 2: 6 bit preﬁx adder
network.

Fig. 3: Imposing semi-regularity.

1) Semi-Regularity in Preﬁx Graph Structure: The ﬁrst
strategy is to enforce a sort of regularity in the preﬁx graphs.
For instance, regular adders, such as Sklansky, Brent-Kung,
have the inherent property that the consecutive input nodes
(even and odd) are combined to create the preﬁx nodes at the
logic level 1. In our approach, we constrain this regularity
for those preﬁx nodes (logic level 1). To explain this, let
us consider Fig. 3. We can see that preﬁx nodes r1, r2, r3
and r4 are constructed by consecutive even-odd nodes. For
instance, i0 and i1 are used to construct r1. But with this
structural constraint, we are not allowed to construct any node
by combining i1 and i2 as done in Kogge-Stone adders. Note
that the sub-structure, as shown in Fig. 3, is a part of some
regular adders like Sklansky adder, and is imposed in our
preﬁx structure enumeration.

We have run experiments with 16 bit adders, and observed
that this pruning strategy (i) does not degrade the solution
quality (or size of the preﬁx graph under same L and mf o),
but (ii) able to reduce the search space signiﬁcantly,
in
comparison to not using this pruning strategy.

2) Level Restriction in Non-trivial Fan-in: Each of the
preﬁx node N (a:b), where a is the most-signiﬁcant-bit (MSB)
and b is the least-signiﬁcant-bit (LSB),
is constructed by
connecting the trivial fan-in Ntr (a:c) having same MSB as
N , and the non-trivial fan-in Nnon−tr (c − 1:b). For instance,
in Fig. 2, o3 and b2 are respectively the non-trivial fan-in
and the trivial fan-in node for the preﬁx node o5. In the
bottom-up enumeration technique, we put another additional
restriction that the level of the trivial fan-in node is always
i.e.,
less or equal
level(Ntr) ≤ level(Nnon−tr). Note that this sort of structural
restriction is also inherent in regular adders, such as Sklansky
or Brent-Kung adders.

to that of the non-trivial fan-in node,

In a nutshell, our PGG algorithm is a blend of regular
adders and [20]. We borrow some properties of regular adders
to enforce in [20] for reducing its huge search space without
hampering the solution quality. To illustrate this, we have
obtained the binary for [20] from the authors, and ﬁrst
compared our result for lower bit adders, such as n = 16, 32.
We got the solutions with same minimum size, which proves
that our structural constraints have not degraded the solution
quality. However, for higher bit adders, we get better solution
quality than [20] as shown in TABLE I.

Column 1 presents the mf o constraint, while columns
2 and 3 respectively show the size and run-time for our
enhanced algorithm, and the corresponding entries for [20]
are respectively represented in columns 4 and 5. In general,

TABLE I: Comparison with [20] for 64 bit adders

mf o

4
6
8
12
16
32

Our Approach

Approach in [20]

size
244
233
222
201
191
185

Run-time (s)
302
264
423
193
73
0.04

size
252
238
-
-
192
185

Run-time (s)
241
212
-
-
149
0.04

when fan-out is relaxed or mf o is higher, the run-time is less
due to relaxed size-pruning as explained in [20]. Note that [20]
cannot generate solutions for mf o = 8, 12 due to generation
of innumerable intermediate solutions as explained in [19]. On
the contrary, our structural constraints can do a pre-ﬁltering
of the potentially futile solutions, thereby allowing relaxed
size-pruning and size-bucketing to search for more effective
solution space. In terms of run-time, it is slightly worse in
a few cases, but importantly, this generation is a one-time
process, and this run-time is negligible in comparison to the
design space exploration by the physical design tools. So our
imposed structural restrictions (i) do not degrade the solution
quality, (ii) achieve better solution sizes for all mf o than [20]
for higher bit adders which could not even generate solutions
in all cases, and (iii) help to obtain wider physical solution
space to be demonstrated in Section II-E.

D. Quasi-random Sampling

We have mainly focused on 64 bit adders in this work as
this is mostly used in today’s microprocessors. From all preﬁx
adder solutions, we sample a set of solutions for building
the learning model via the quasi-random approach which
is conducted by a two-level binning (mf o, s) followed by
random selection, This approach aims to evenly sample the
preﬁx adders covering different architectural bins. The pri-
mary level of binning is determined by mf o of the solutions.
However, there may be thousands of architectures sharing the
same mf o, so the secondary level of binning is based on s.
Afterwards, adders are picked randomly from those secondary
bins.

We illustrate the quasi-random sampling with the following
example: given 5000 solutions with mf o = 4, we want to
pick 50 solutions from them. Suppose these 5000 solutions
have the size distribution from 244 to 258. First a random
solution is picked from the bucket of the solutions (mf o = 4,
s = 244). Then we pick a solution randomly from (mf o = 4,
s = 245), and so on. After picking 15 solutions from each of
those buckets with mf o = 4, we again start from the bucket
(mf o = 4, s = 244). This process is repeated until we get 50
solutions. Similar procedure is done with other mf o values.

E. Physical Solution Space Comparison with [20]

In this subsection, we show the usefulness of our algorithm
for obtaining wider solution space in physical design domain
in comparison to [20]. Among the preﬁx adders generated by
[20], we randomly sampled 7000 preﬁx adders. Those preﬁx
adders are fed into the full EDA ﬂow (synthesis, placement
and routing) to get their real delay, power and area values

(a)

(c)

(b)

(d)

Fig. 4: Quasi-random sampled adders vs. adders from [20].
(a) Solution space in area vs. delay domain from [20]; (b)
Solution space in area vs. delay domain from ours; (c)
Solution space in power vs. delay domain from [20]; (d)
Solution space in power vs. delay domain from ours.

(takes around 700 hours). We plot these adders by [20] and
our representative 3000 adders in Fig. 4. It can be seen that,
although the numbers of adders by [20] is more than 2 times of
our representative adders, our adders still cover wider solution
space in physical domain, demonstrating the effectiveness of
our enhanced algorithm PGG. This is in accordance with
the solutions missed by [20] as mentioned in TABLE I.
Those availabilities eventually offer more opportunities for
our machine learning methodology to identify close to ground
truth Pareto frontier solutions.

III. BRIDGING ARCHITECTURAL SOLUTION SPACE TO
PHYSICAL SOLUTION SPACE

In most EDA problems, the metrics of the solution quality
are typically conﬂicting. For instance, if we optimize the tim-
ing of the design, then the power/area may be compromised
and vice versa. So one imperative job of EDA engineers
is to ﬁnd the Pareto-optimal points of the design enabling
the designers to select among those. In this section, we ﬁrst
provide the preliminaries about Pareto optimality, and the
error metrics of Pareto optimal solutions. Then we discuss
the gap between the preﬁx architectural solution space and
physical solution space in adders, which motivates the need of
the machine learning-based approach for optimal adder explo-
ration. Finally, a domain knowledge-based feature selection
details are presented along with training data sampling for
the learning models.

architecture stage are preﬁx node size s and max fan-out mf o.
These two metrics are conﬂicting, i.e., if we reduce mf o,
s increases and vice-versa. Similar competing relationship
exists between delay and power/area after physical design. It
should be stressed that power and s are correlated, and mf o
indirectly controls the timing as more restricted fan-out can
mitigate congestion and load-distribution, thereby improving
the delay of the adder. However, this relationship between
architectural synthesis and physical design is approximate,
and not a very high-ﬁdelity one.

To demonstrate this, we plot node size s vs. mf o and power
vs. delay in Fig. 6 for several 64 bit adder solutions. In this
experiment, we have generated the preﬁx architecture solu-
tions by PGG, and the ﬁnal power/delay numbers are obtained
by running those solutions through EDA tools as explained
later in Section VI. An example of the preﬁx architecture and
the corresponding physical solution is presented in Fig. 7. In
Fig. 6(a), we broadly categorize the solutions into 2 groups,
(i) G1 with higher node size and lower mf o, and (ii) G2
with lower node size and higher mf o. In Fig. 6(b), the same
designs as Fig. 6(a) are projected into the physical solution
space, restoring the group information. Design Compiler [28]
(version F-2011.09-SP3) is used for logical synthesis, and
IC Compiler [29] (version J-2014.09-SP5-3) is used for the
placement and routing. Non linear delay model (NLDM) in
32nm SAED cell-library [30] is used for technology mapping.
The key observations here are ﬁrstly, there is a correlation
between architectural solution space and physical design
solution space. For instance, the solutions from G1 are mostly
on the upper side, and those of G2 are mostly on the lower
side in Fig. 6(b), thereby indicating a correspondence between
s and power. Nevertheless, it is not completely reliable. For
example, (i) the delay numbers for G1 and G2 are very much
spread, (ii) a cluster can be observed where the solutions
from G1 and G2 are mixed up in Fig. 6(b), and (iii) several
solutions of G1 are better than several solutions of G2 in
power, which is not in accordance with the metrics at the
preﬁx adder architecture stage. So we can not utterly rely on
architectural solution space to achieve the optimal output in
physical solution space.

However, since our algorithm generates hundreds of thou-
sands of preﬁx graph structures, it is intractable to run synthe-
sis and physical design ﬂows for even a small percentage of
all available preﬁx adder architectures. To address this ﬁdelity
gap between the two design stages and the high computational
cost together, we come up with a novel machine learning
guided design space exploration as replacement of exhaustive
search.

C. Feature Selection

The feature is a representation which is extracted from the
original input representation, and it plays an important role
in machine learning tasks. We now discuss the features to be
used for the learning model. Features are considered from both
preﬁx adder structure and tool settings, with a focus on the
former. We select node size and maximum-fan-out (mf o) of
a preﬁx adder as two main features for our learning model.

Fig. 5: Hypervolume with two objectives in objective space.

(a)

(b)

Fig. 6: Gap between preﬁx structure and physical design of
adders: (a) Architectural solution space; (b) Physical solution
space.

A. Preliminaries
Deﬁnition 1 (Pareto Optimality). An objective vector f (x) is
said to dominate f (x(cid:48)) if:

∀i ∈ [1, n], fi(x) ≤ fi(x(cid:48))
and ∃j ∈ [1, n], fj(x) < fj(x(cid:48)).
A point x is Pareto-optimal if there is no other x(cid:48) in design

(7)

space such that f (x(cid:48)) dominates f (x).

As in this paper for adder design, a Pareto-optimal design
is where none of the objective metrics, such as area, power
or delay, can be improved without worsening at least one of
the others. The Pareto Frontier is the set of all the Pareto-
optimal designs in the objective space. Therefore, the goal is
to identify the Pareto-optimal set P for all the Pareto-optimal
designs.

Deﬁnition 2 (Hypervolume). The hypervolume computes the
volume enclosed by the Pareto frontier and the reference point
in the objective space [27].

In Fig. 5, the shaded area is an example of the hypervolume
of a Pareto set with two objectives. Then the hypervolume
error for a predicted Pareto set ˆP is deﬁned as

η =

V (P ) − V ( ˆP )
V (P )

,

(8)

where P is the true Pareto-optimal set, and V (P ) is the
hypervolume of the Pareto set P . Note that a prediction ˆP
which contains the whole design space has an error of 0.
Thus the predicted set ˆP with less points is desired.

B. Gap Between Logic and Physical Design

Since we focus on high performance adders and explore
the preﬁx adders of logic level L = log2 n, the metrics at this

(a)

(b)

Fig. 7: (a) An example of architectural solution: Bit-width = 64, size = 201, Max. level
= 6, Max. fanout = 12; (b) Corresponding physical solution.

Fig. 8: Deﬁning spf o of a
node.

However, for any given mf o and node size, there will be
hundreds or even thousands of different preﬁx architectures.
Therefore, additional features are required to better distinguish
individual preﬁx adder attributes. We deﬁne a parameter sum-
path-fan-out (spf o) for this. Let a and b are the fan-in nodes
of a node n, then spf o(n) is deﬁned recursively as:

spf o(n) =

0,
sum(f o(a) + spf o(a),

if n ∈ input,

(9)

f o(b) + spf o(b)), otherwise.






Here f o(n) denotes the fan-out of any node n. Consider the
preﬁx adder structure in Fig. 8, and according to the deﬁnition
we have:

spf o(o1) = sum(f o(i0) + spf o(i0), f o(i1) + spf o(i1))

spf o(b1) = sum(f o(i2) + spf o(i2), f o(i3) + spf o(i3))

spf o(b2) = sum(f o(i4) + spf o(i4), f o(i5) + spf o(i5))

= sum(1, 1) = 2,

= sum(2, 1) = 3,

= sum(2, 1) = 3.

Therefore, we can use the recursive deﬁnition to calculate

spf o(o3) = sum(f o(o1) + spf o(o1), f o(b1) + spf o(b1))

= sum(3 + 2, 2 + 3) = 10,

spf o(o5) = sum(f o(o3) + spf o(o3), f o(b2) + spf o(b2))

= sum(3 + 10, 3 + 3) = 19.

In our methodology, we use the spf o of the output nodes
which are at log2 n level (there are 32 nodes at level 6 for 64
bit adder) as the features to characterize the preﬁx structures,
in addition to mf o, size and target delay. The basic intuition
for selecting spf o of the output nodes as the features is that
the critical path delay of the adder is the longest path delay
from input to output. So it depends on the (i) path-lengths,
which can be represented at the preﬁx graph stage by the
logic level of the node, and (ii) the number of fan-outs driven
at every node on the path. Note that we have skipped the
spf o of the output nodes which are not at log2 n level as for
those nodes, the path length is smaller, and those would not
potentially dictate the critical path delay.

Apart from these preﬁx graph structural features, we also
consider tool settings from synthesis stage and physical design
stage as other features. We have synthesized the adder struc-
tures using industry-standard EDA synthesis tool [28], where

we can specify the target-delay for the adder. The tool then
adopts different strategies internally to meet that target-delay
which we can hardly take into account during preﬁx graph
synthesis. Consequently, changing the target-delay can lead
to different power/timing/area metrics. So we have considered
target-delay as a feature in our learning approach.

In physical design, utilization is an important parameter,
which deﬁnes the area occupied by standard cell, macros and
blockages. Different utilization values can lead to different
layouts after physical design. Therefore, we take utilization
as another feature in the learning model.

In addition to the target delay and utilization, other tool
settings have also been explored. The optimization level
setting in logical synthesis has a potential impact on the per-
formance of adders, which can be adjusted by compile and
compile_ultra commands with different options. After
synthesizing, it is observed that the solutions generated with
compile_ultra can signiﬁcantly dominate the solutions
generated by compile. Therefore, this setting is ﬁxed to
compile_ultra level as we are aiming at superior designs.
In this work, the technology node is not used as a feature.
From the machine learning perspective, there is a common
assumption for conventional machine learning applications
that
the training and test data are drawn from the same
feature space and the same distribution [31]. The values of
area/power/delay may vary a lot under different technology
nodes, which results in different underlying data distributions.
Therefore, the technology node for synthesis should be con-
sistent. The proposed approach for feature extraction can also
be applied to other technology nodes as long as the technology
node is consistent during the design ﬂow. If the technology
node of the testing data switches to another one, the machine
learning model should be re-trained using the data from that
technology node to ensure the accuracy of the model.

D. Data Sampling

Since we can not afford to run the physical design ﬂow
for too many architectures, and too few training data may
degrade the model accuracy signiﬁcantly, a set of adders need
to be selected to represent the entire design solution space.
However, ﬁnding a succinct set of representative training data
for the traditional supervised learning is difﬁcult. In order
to tackle this difﬁculty, we come up with two learning ap-
proaches in the next two sections. The ﬁrst one is the passive

for delay with only primary features. Best model ﬁtting for
delay is achieved with SVR (RBF kernel) with these 4 primary
and 32 secondary features. Since SVR with RBF kernel give
good MSE (mean-squared-error) scores for all metrics, delay,
area and power, we have used this model throughout for
design space exploration.

The model experiments give us the following key insights:
(i) tool setting can play an important role in building the
learning models in EDA. For instance, MSE scores for area
and power improve from 0.021 to 0.003, and 0.228 to 0.027
respectively when we add the ‘target delay’ feature in our
model building, (ii) secondary features play an important
role in improving the model accuracy. For instance, when
we include spf o features in model building, MSE score for
delay improves from 0.200 to 0.170. (iii) linear models are
not sufﬁcient for modeling delay. For instance, MSE scores of
delay improve from 0.214 to 0.170 when we go from linear
models to SVR with RBF kernel, with the same set of features.
The problem of exploring the Pareto frontier of rich preﬁx
adder space can be approached by ﬁrst sampling a subset
of preﬁx adder architectures, and generating the power, area,
delay numbers of each preﬁx adder by running through the
logic synthesis and physical design ﬂow. Those known data
set will be used as the training and testing data for supervised
machine learning guided model ﬁtting. Once the model is
ﬁtted, we can apply the exhaustive preﬁx adder architectures
to this model and get the predicted Pareto frontier solution
set. This is due to the merit of much faster runtime for a
machine learning model in prediction stage than running the
entire VLSI CAD ﬂow.

However, conventional machine learning problem aims at
maximizing the prediction accuracy rather than exploring a
Pareto frontier out of a solution set. Improving the model
accuracy does not necessarily improve the Pareto frontier and
the direct use of the ﬁtted model for Pareto frontier exploration
can even miss up to 60% Pareto frontier points [3]. We
therefore need a machine learning integrated Pareto frontier
exploration methodology, where the Pareto frontier selection
does not rely only on the model accuracy. So we develop a fast
yet effective algorithmic methodology, enabled by regression
model to explore the Pareto frontier of preﬁx adder solutions.
First we consider two spaces for Pareto frontier exploration:
the delay vs. area as well as the delay vs. power. For either
space, there exists a strong trade-off between the two metrics.
For delay vs. power space, we propose to use a joint output
Power-Delay function (P D) as the regression output rather
than using any single output.

P D = α · P ower + Delay.

(10)

The rationale of using scalarization [32] or the linear
summation of the power and delay metrics is that such a linear
relation provides a weighted bonding between the power and
the delay so that by changing the α value, the regression
model will try to minimize the prediction error on the more
weighted axis hence leads to more accuracy on that direction.
In contrast, the other metric direction will be predicted with
less accuracy hence introducing some level of relaxations. It
can be foreseen that changing the α value can lead to different

Fig. 9: Overall ﬂow of α-sweep learning.

supervised learning where a quasi-random data sampling is
performed to obtain the training data, followed by multi-
objective scalarization to achieve the Pareto optimal solutions.
The second one is the active learning approach where model
training is integrated to ﬁnding Pareto-optimal frontiers of the
design space.

IV. α-SWEEP LEARNING
In this section we propose a pareto-frontier exploration
ﬂow which is based on support vector machine. The overall
ﬂow of our α-sweep supervised learning-based Pareto-frontier
exploration is presented in Fig. 9.

A. Scalarization to the Single-Objective

In this work, supervised learning is preferred over unsu-
pervised learning since supervised learning has a substantial
advantage over unsupervised learning for our problem. In
particular, supervised learning allows to take advantage of
the golden result, i.e., the true area/power/delay, generated
by the synthesis tools for each design, instead of just letting
the algorithm work out for itself what the classes should be.
In general, supervised learning usually outperforms the unsu-
pervised learning for this kind of regression and classiﬁcation
tasks.

Before applying machine learning for exploring Pareto
frontier, we ﬁrst validate the effectiveness of the features we
extract by building regression models for single metric predic-
tion. For learning models, we explored (i) several supervised
learning techniques, such as linear regression, Lasso/Ridge,
Bayesian ridge model and support vector regression (SVR)
with linear, polynomial and radial-basis-function (RBF) ker-
nel, and (ii) 36 features, including 4 primary features, size,
mf o,
target delay and utilization (tool settings), and 32
secondary features for spf o. We observed that we could get
an R2 score above 0.95 for area and power even with primary
features and linear models. However, we don’t get good scores

ﬁtting accuracies of the regression model. By sweeping α
over a wide range from 0 to large positive values, each time
the regression model will be ﬁtted to predict different best
solutions which altogether form the Pareto frontier. We call
this approach α-sweep. Note that, the P ower and Delay
values in Equation (10) are normalized and scaled to the range
between 0 and 1 by Equation (11).
x − min(X)
max(X) − min(X)

, x ∈ X.

(11)

x =

Similarly, we have a joint output Area-Delay (AD) function

for Pareto frontier exploration on Area and Delay space.

AD = α · Area + Delay.

(12)

This α-sweep technique can be extended to simultaneously
consider power, performance or delay, and area (PPA), using
two scalars (α1 and α2) instead of one scalar factor α. The
joint output function for Pareto frontier exploration on area –
power – delay space can be formulated as:

P P A = α1 · Area + α2 · P ower + Delay.
The results of α-sweep for both two-dimensional space and

(13)

three-dimensional space are shown in the Section VI.

V. PARETO ACTIVE LEARNING
In our adder design problem, obtaining the true area/pow-
er/delay values or the labeled data for each adder requires
running logic synthesis and physical design ﬂow, which is
often time-consuming if the amount of data is huge. Active
learning is an iterative supervised learning which is able to
interactively query the data pool to obtain the desired outputs
at new data points. Since the samples are selected by the
learning algorithm, the number of samples to ﬁt a model can
often be much lower than the number required in traditional
supervised learning. Since an active sampling strategy is
required in active learning, an “uncertainty estimation” of
the prediction is needed. Gaussian Process (GP) can make
predictions and, more importantly, provide the uncertainty
estimation of its predictions by nature. Therefore, in this paper
we further propose a Pareto active learning algorithm based
on Gaussian Process regression.

A. Overall Flow

The overall ﬂow of the Pareto active learning (PAL) is
shown in Fig. 10. Given all the preﬁx adder structures, ﬁrst
we extract the feature vector for each adder as introduced
in Section III-C. The active learning starts with Gaussian
Process regression which will be illustrated later. Unlike the
passive supervised learning in which all
the features and
the corresponding labels are prepared in advance, the active
learning derives the labels of each training data during the
learning process on-demand. To be speciﬁc, the algorithm in-
crementally identiﬁes the most representative instances along
with their features which are later fed into EDA synthesis ﬂow
(synthesis, placement and routing) for true area/power/delay
numbers. Namely, the EDA synthesis ﬂow and the learning
process are interleaving. As more and more designs being
selected, the model gets more and more accurate till conver-
gence.

Fig. 10: Overall ﬂow of Pareto active learning.

B. Gaussian Process Prediction

A Gaussian process is speciﬁed by its mean function and
covariance function. A Pareto active learning scheme based
on Gaussian process regression is proposed in [33]. The prior
information is important to train the Gaussian Process model,
which is a parameterized mean and covariance functions.
Conventionally, the training process selects the parameters in
the light of training data such that the marginal likelihood
is maximized. Then the Gaussian Process model can be
obtained and the regression can be proceeded with supervised
input [34]. The ability of GP indicating prediction uncer-
tainty reﬂects in GP learner providing a Gaussian distribution
N (m(x), σ(x)) of the values predicted for any test input x
by computing
m(x) = k(x, X)(cid:62)(k(X, X) + σ2I)−1Y,
σ2(x) = k(x, x) − k(x, X)(cid:62)(k(X, X) + σ2I)−1k(x, X),

(14)
where X is the training set, Y is the supervised information
of trained set X. For Gaussian Process regression, a prediction
of a design objective consists of a mean and a variance.
The mean value m(x) represents the predicted value and the
variance σ(x) represents the uncertainty of the prediction.

C. Active Learning Algorithm

The ability of GP learners in quantifying prediction un-
certainty enables a suitable application for active learning.
Basically, three sets are maintained during the PAL process,
including a set of Pareto-optimal designs (P ), non-Pareto-
optimal designs (N ) and ‘unclassiﬁed’ designs (U ).

The GP models with discrepant prior are applied to learn
the objective functions farea(x), fpower(x), fdelay(x). PAL

calls GP inference to predict the mean vector m(x) and the
standard deviation vector σ(x) of all unsampled x in the
design space based on Equation (14). Unlike other regression
models such as linear regression and support vector regres-
sion, whose outputs are in form of numerical or categorical
result, the output of GP is a distribution where uncertainties
are involved. To capture the prediction uncertainty for a design
x, a hyper-rectangle is deﬁned as

HR(x) = {y : mi(x) − β

2 σi(x) ≤ yi ≤ mi(x) + β

2 σi(x)},

1

1

where i ∈ {1, 2, 3}, corresponding to area, power and delay
metrics in physical space. β is a user-deﬁned parameter
which determines the impact of σi(x) on the region. In our
implementation, β is set to 16 based on the analysis in [33],
[35].

As shown in Fig. 10, the PAL algorithm is an iterative
process. A few new points are selected in each iteration, and
the GP model is retrained with new training set. Note that
the model is supposed to be more and more accurate as more
data being sampled. Therefore, the uncertainty region should
be smaller and smaller. In order to ensure the non-increasing
monotonicity of the uncertainty region while sampling and
incorporating the previous evaluations, the uncertainty region
of x in the (t + 1)-th iteration is deﬁned as

Rt+1(x) = Rt(x) ∩ HR(x),
(15)
where the initial R0 = Rn which is the entire objective space.
The numbers of designs in Pareto-optimal set P and
non-Pareto-optimal set N are non decreasing as iteration t
increments. Thus, at iteration t, the points in P and N keep
their classiﬁcation. Intuitively, if one wants to compare the
predicted performance of two designs, two extreme cases,
i.e., optimistic prediction min(Rt(x)) and the pessimistic
prediction max(Rt(x)) of each design, can be applied. If
the optimistic prediction of design x is dominated by the
pessimistic prediction of other design x(cid:48), then x is classiﬁed
as non-Pareto-optimal; And if the pessimistic prediction of
design x is not dominated by optimistic prediction of any
other design x(cid:48), then x is classiﬁed as Pareto-optimal; A
design will remain unclassiﬁed if neither condition holds.
Fig. 11 is presented here as an example.

In the implementation, an error tolerance δ with value 0.001
is applied during classiﬁcation. The rules for classiﬁcation can
be represented as follows.

P,

if max(Rt(x)) ≤ min(Rt(x(cid:48))) + δ,
if max(Rt(x(cid:48))) ≤ min(Rt(x)) + δ,

(16)






x ∈

N,
U, otherwise.

After classiﬁcation in each iteration, a new adder design
with the largest length of the diagonal of its uncertainty region
R(x) is selected for sampling. The value is attached to x as

wt(x) = max

y,y(cid:48)∈Rt(x)

||y − y(cid:48)||2.

(17)

Intuitively, Equation (17) picks the points which are most
worthy exploring. Afterwards, these designs are going through
EDA ﬂow to get the real area, power and delay numbers, and

Fig. 11: An example of classiﬁcation.

the GP model will hence be improved with those feedback
results.

Algorithm 1 Active Learning for Pareto-frontier Exploration

Require: Adder architectural design space E, GP prior, max-

imum iteration number Tmax;
Ensure: predicted Pareto-optimal set ˆP ;
1: P ← ∅, N ← ∅, U ← E;
2: Randomly select a small subset X = {xi} of E;
3: Get true values Y = {yi|yi = EDAFlow(xi)};
4: S ← X;
5: R0(x) ← Rn, ∀x ∈ E;
6: t ← 0;
7: while U (cid:54)= ∅ and t < Tmax do
8:
9:
10:
11:

Building GP model with {(xi, yi) : ∀xi ∈ S};
Obtain Rt(x), ∀x ∈ E;
for all x ∈ U do

if x is Pareto-optimal based on Equation (16)

P.add(x), U.delete(x);

else if x is non-Pareto-optimal based on Equa-

then

12:
13:

14:
15:

tion (16) then

end if

16:
17:
18:
19:
20:
21:
22: end while
23: ˆP ← P ;

N.add(x), U.delete(x);

end for
Obtain wt(x), ∀x ∈ (U ∪ P ) \ S;
Choose x(cid:48) ← argmax{wt(x)};
S ← S ∪ x(cid:48);
t ← t + 1;
Obtain new data (x(cid:48), y(cid:48)) by running EDA ﬂow;

The entire process is presented in Algorithm 1. It starts with
the initialization (lines 1–6). In each iteration, the GP model
is trained with the current training set S, and the uncertainty
region for each design is obtained (lines 8–9). Then the
designs in the U set are classiﬁed based on uncertainty regions
and classiﬁcation rules (lines 10–16). After that, the design
with the largest uncertainty is sampled and the sampling set
S is updated (lines 17–19). The newly sampled design is fed
into synthesis tools to get the label which is used for training
GP model in the next iteration (line 21). The learning process
stops after all adder designs in architectural design space are
classiﬁed. The prediction is ˆP = P (line 23). Suppose Tmax
is the maximum number of iterations, and |E| is the size of

solution set, then the complexity of Algorithm 1 is at most
O(Tmax|E|), as maximum size of U can be |E|. However, it
should be stressed that although there are Tmax|E| operations
for PAL algorithm, the cost of each operation (which is a
simple inference based on the Gaussian Process Regression
model) is negligible in comparison to EDA synthesis ﬂow
run-time, and we will demonstrate later in TABLE IV that
the total run-time of different approaches are dictated by the
number of EDA synthesis ﬂow runs needed in the respective
approaches.

VI. EXPERIMENTAL RESULTS

In this section we show the effectiveness of the proposed
algorithms and methodologies. First we compare the physical
solution space before/after applying PGG algorithm. Then the
Pareto frontier obtained by α-sweep is presented. Next, we
demonstrate the Pareto frontier obtained by active learning,
and compare the quality of Pareto frontiers generated by two
approaches. Finally, we compare our explored optimal adders
against legacy adders.

Since high performance adders are commonly used in CPU
architectures which are typically 64 bit, we have mainly
presented the results for 64 bit adders to demonstrate the
methodology. However, the approach is very general to be
used for adders of arbitrary bit-width. The ﬂow is imple-
mented in C++ and Python on Linux machine with 72GB
RAM and 2.8GHz CPU. We use Design Compiler [28]
(version F-2011.09-SP3) for logical synthesis, and IC Com-
piler [29] (version J-2014.09-SP5-3) for the placement and
routing. "tt1p05v125c" corner and Non Linear Delay
Model (NLDM) in 32nm SAED cell-library for LVT class
[30] (available by University Program) is used for technology
mapping. Primary input activity of 0.1 is used along with
1GHz operating frequency for power estimation. Regarding
the tool settings, target delays of 0.1ns, 0.2ns, 0.3ns and
0.4ns are used. Utilization values are set to 0.5, 0.6, 0.7 and
0.8. We used Python based machine learning package scikit-
learn [36] for the predictions. Throughout our all experiments,
the run time for machine learning predictions is less than a
minute.

We relied more on the ﬁdelity of the SAED library rather
than accuracy considering that SAED library may not be
very realistic as that used in industry. For instance, the FO4
delay for a unit sized inverter for this library in the operating
corner is 36ps [20], [37]. So 11 FO4 delay, typically being
presented to be the delay for 64-bit adders in literatures [38], is
approximately 400ps which is close to the reported delays for
64-bit adders in our work. To further demonstrate the ﬁdelity
of this library, we run the Kogge-Stone adders with bit-widths
of 8, 16, 32, 64, 128 and 256 through the synthesis ﬂow
using this library. Then we normalize the measured delay in
terms of FO4 delay, and plot it with bit-width (n) as shown
in Fig. 12. It can be seen that the delay is linear with log2 n,
which is expected for a logarithmic tree adder such as Kogge-
Stone adder. So we believe if this algorithmic methodology
is applied to more realistic industrial libraries, it can show
similar beneﬁt as demonstrated with SAED 32nm library.

y
a
l
e
D

16

12

8

4

0

8

64 128 256

32

16
Adder bit-width

Fig. 12: Delay values (× FO4 delay) of Kogge-Stone adders
with various bit-width.

(a)

(b)

Fig. 13: (a) Pareto Frontier: area vs. delay; (b) Pareto Frontier:
power vs. delay.

To validate the optimality and the hypervolume error of
the two learning approaches against the real world solution
space, we need to run the logical/physical EDA ﬂow on a
large set of adder solutions. Our machine and tool set takes
about 5.5 minutes to complete this full ﬂow of a single
preﬁx adder. Therefore, we select a reasonable number (3000)
of preﬁx adder solutions, which eventually took about 300
hours to complete, but still a comparatively larger data set
in comparison to our training data set. Crucially, those 3000
adders are also sampled in a Quasi-random manner in order
to represent the entire solution space.

A. Pareto Frontier Predicted by α-sweep Learning

In

this

learning

100 , 50, 1

experiment, we

show the
approach. We

15
20 , 10, 1
50 , 20, 1

apply
different α values
8 , 2, 1

effectiveness
the
of
our α-sweep
of
α-sweep method with
(1000, 0, 100, 1
and
collect the best 150 solutions for delay-area and delay-power
spaces where for each α value, the best 10 architectures with
lowest P D or AD values are fed into the logical/physical
EDA ﬂow to generate similar Pareto points. Note that
15 + 15 = 30 learning models have been derived for this for
all, but it is very fast as the same training data have been
used, and the models are regression based.

10 , 8, 1

2 , 1),

Fig. 13(a) and Fig. 13(b) respectively show the correspond-
ing Pareto frontiers of the α-sweep approach and the ground
truth Pareto frontiers for the 3000 representative adders. Each
dot
in the delay-area or delay-power space indicates one
adder solution after going through the logical/physical EDA
ﬂow. We can see that generally the predicted Pareto frontier
solutions are fairly close to the real Pareto frontier, with some
exceptions. Overall, the proposed approach can effectively

TABLE II: Comparison of different model accuracies

Model

Original
Noisy

Area
0.003
0.024

MSE
Power
0.027
0.951

Delay
0.170
0.711

Area-Delay
0.139
0.168

Hypervolume error
Power-Delay
0.122
0.148

Area-Power-Delay
0.154
0.162

TABLE III: Pareto frontiers for PAL vs. α-sweep [39]

Objective Hypervolume error

PAL

α-sweep [39]

in predicting Pareto frontier in all design spaces, including
area-delay, power-delay, and area-power-delay spaces.

Area-Delay

Power-Delay

Area-Power-Delay

average
best
average
best
average
best

0.100
0.044
0.109
0.075
0.056
0.039

0.139
0.093
0.122
0.076
0.154
0.125

Notes: All hypervolume error above are collected from 1000
repeated experiments.

achieve near optimal Pareto frontier without affording to
spend expensive runtime on every adder. So this learning
based methodology can be readily adopted to achieve Pareto
frontiers for much larger solution space which is intractable
for exhaustive exploration by conventional design ﬂow.

We have conducted additional experiments to show the
impacts of the low accuracy of the machine learning model.
The basic idea is to inject random noise in the prediction stage,
i.e., additional Gaussian noise is added into the predicted
value. The accuracy will be lower than original results. Then
we explore the Pareto frontier based on the noisy prediction.
Generally, the quality of the ﬁnal Pareto frontier is worse than
original model. The comparison of Pareto frontier quality is
presented in TABLE II.

B. Comparison of the Quality of Pareto-Frontier between PAL
and α-sweep

We implement PAL to predict Pareto-optimal designs in
both two-dimensional design spaces which are area-delay
space and power-delay space, as well as three-dimensional
space which is area-power-delay space. The results are com-
pared with those of [39]. The initial input set for both area-
delay and power-delay is of size 250, which are randomly se-
lected from the exhaustive design space. The curves of Pareto
frontiers for two-dimensional spaces are shown in Fig. 13.
The hypervolume of area-delay Pareto frontiers are calculated
with reference point (max(delay), max(area)). Similarly, The
hypervolume of power-delay Pareto frontiers are calculated
with reference point (max(delay), max(power)). Note that the
unit for delay is nanosecond (ns) when calculating the hyper-
volume. It should be stressed that there is a sort of randomness
in both α-sweep and PAL algorithm. For α-sweep, the training
set is selected randomly. On the contrary, the initial set in PAL
is randomly selected (line 2), thereby may result in different
outputs. So the experiments are conducted for 1000 times such
that the general performance is reﬂected. The comparison
between two approaches are shown in TABLE III. Comparing
the hypervolume error of Pareto frontier obtained by PAL and
α-sweep, it can be seen that PAL achieves better performance

C. Runtime Comparisons among Exhaustive Approach, α-
sweep and PAL

There are three factors that will affect the runtime: (i) the
total number of EDA synthesis runs required; (ii) Among all
these required EDA synthesis runs how many of them can
be parallelized; (3) The runtime of the training process in
machine learning model. All these details are recorded in
TABLE IV. The ‘INIT’ represents the set of training data
in the α-sweep and the initial set in PAL, which can be par-
allelized because all the points are obtained in advance. The
‘AS’ represents the set of designs which are actively sampled
during the learning process, which cannot be parallelized. The
α-sweep approach does not involve active sampling, so the
‘AS’ set is none here. The ‘VERI’ represents the set of designs
which are predicted to be Pareto-optimal. We should run EDA
synthesis ﬂow to get the real PPA values of these designs to
extract the Pareto-frontier. This set of designs are obtained
after the learning process stops, so the EDA synthesis runs
on these designs are also conducted ofﬂine, which can be
parallelized. Each EDA synthesis run takes about 5.5 minutes.
Then we can compare the total runtime of different ex-
ploration methodologies. For exhaustive exploration, all the
preﬁx adders should be fed into EDA tools for synthesiz-
ing to obtain the value of each metric, which is extremely
time-consuming. There is no training, additional sampling,
veriﬁcation. The total runtime cost involves EDA ﬂows of all
the designs in the design space. The Pareto frontier can be
extracted from the results, whose runtime is much less than
synthesizing and can be neglected. The total runtime is

Texh =

5.5 × #INIT
#Machines

.

(18)

Since the entire solution space is so huge that one can hardly
run all of them, in our experiment, we sample representative
10K designs by random sampling. The total runtime of
synthesizing is about 55000 minutes with single machine. It
should be noted that the entire solution space is much more
than 10K.

In the exploration by α-sweep, not all adders in the design

space are needed for synthesizing. The total runtime is

Tα =

5.5 × (#INIT + #VERI)
#Machines

+ Modeling time.

(19)

In our experiment, we select 2500 of the designs out of those
10K designs by random sampling to build the model, includ-
ing training and testing phases. It takes about 1.5 minutes
to build the model and make predictions. When exploring in
area-power-delay design space, 150 designs on average in the

TABLE IV: Comparison of runtime with single machine among different approaches

Method

#INIT

#AS

#VERI

#Total

Exhaustive
α-sweep
PAL

10000
2500
700

-
-
10

-
150
290

10000
2650
1000

Runtime (mins)
Modeling

EDA

55000.0
14575.0
5500.0

-
20.0
2.0

Total

55000.0
14595.0
5502.0

Notes: The designs in “#INIT” and “#VERI” can be synthesized in parallel. The number of designs in each category is collected from
1000 repeated experiments.

α-sweep
PAL

)
s
n
i
m

3
0
1
×

(

e
m

i
t
n
u
R

15

10

5

0

1 2 3 4 5 6 7 8 9 10
Number of machines

Fig. 14: Comparison of runtime with different number of
machines.

design space are predicted to be Pareto-optimal. So on average
2650 designs are needed. The runtime for synthesizing is
14575 minutes. Note that in terms of learning models, the
α-sweep method needs to build 15 × 15 = 225 models since
α1 and α2 both have 15 values to choose from.

Similarly, the runtime of PAL can be calculated by

TP AL =

5.5 × (#INIT + #VERI)
#Machines

+ 5.5 × #AS

+ Modeling time.

(20)

The size of initial set is ﬁxed, which is 700. It takes about 4
minutes to build the model and make predictions during the
PAL process. When exploring the Pareto-optimal designs in
area-power-delay space, 10 designs on average are sampled
during PAL. 290 designs on average in the design space are
predicted to be Pareto-optimal. In total, 1000 designs are
needed on average. The runtime is 5500 minutes with single
machine. PAL algorithm needs to build N models where N
is the number of iterations in PAL. In our implementation, the
maximum iteration is set to 20. It can be observed that the
active learning approach outperforms the α-sweep learning in
terms of both the quality of Pareto frontier and the number
of EDA ﬂow runs.

Note that all the runtime calculations are based on single
machine. However, the EDA synthesis runs in all three ﬂows
can be distributed to multiple machines if available, except the
adders sampled during active learning, which (10 on average
in our experiments) is very less in comparison to the total
number of the synthesis runs. So PAL can get a signiﬁcant
speedup over α-sweep and exhaustive approach with single
machine and multiple machines.

D. Comparison on Different Sampling Strategies in PAL

In the sampling stage of PAL, the number of instances to
be sampled has impact on the runtime since the EDA ﬂow is
required to obtain the real value for area, power and delay.
The less instances we sample in each iteration, the more
iterations are needed to ensure the PAL process converge,
which is more likely to result in less samples in total. The
more instances we sample in each iteration, the less iterations
are needed. However, the total number of sampled instances
would be large. In practice, the runtime cost of running EDA
ﬂow can be reduced by parallel execution if there are multiple
licenses available. In this section, we explore the effect of
different sampling strategies in terms of the total runtime and
the quality of Pareto frontier in practical scenarios.

The results for different sampling strategies are listed in
Fig. 15. Since the EDA ﬂow for synthesis, placement and
routing takes up the most signiﬁcant part of the total runtime
cost, the key factor is the number of adders which needs to
be through EDA tool ﬂow. If we have multiple machines
available for the EDA tool ﬂow, the runtime is determined
by the total number of iterations as long as the number of
samples does not exceed the number of machines. From the
result, it can be seen that we can obtain the Pareto-frontier
with comparable quality, using less runtime.

Note that when the sample size increases from 1 to 5,
the average hypervolume error increases from 0.056 to about
0.070, which is still less than 0.154 (average hypervolume
error achieved in α-sweep approach). Therefore, batch sam-
pling can not only take care of parallel synthesizing but also
achieve better quality for Pareto frontier than α-sweep, which
can also show the advantages of the PAL.

E. Adder Performance Comparison

Finally, we compare our explored adders against Design-
Ware adders, legacy adders, such as Kogge-Stone, Sklansky,
as well as a state-of-the-art adder synthesis algorithm in TA-
BLE V. Since our approach generates numerous solutions, it
is not feasible to perform a one-to-one comparison. Instead for
each of the solution points in regular adders and [19], we have
picked the Pareto points from our solution set which are able
to excel them in all metrics. For instance, P1 could provide
around 8ps better delay with respectively 14% and 12% lesser
area and power over Kogge-Stone adder. The DesignWare
adders are synthesized from behavioral description of adder
(Y = A + B) with the 16 conﬁgurations of tool settings
(Combination of 4 target delay and 4 utilization values) that
are used in generating the Fig. 4. We pick the one with best

s
n
o
i
t
a
r
e
t
i
#

e
g
a
r
e
v
A

10
8
6
4
2
0

s
r
e
d
d
a
#

e
g
a
r
e
v
A

1,000

990
980
970
960
950

r
o
r
r
e

v
h

e
g
a
r
e
v
A

0.080

0.060

0.040

0.020

0.000

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

(a)

(b)

(c)

Fig. 15: Comparison among different number of samples per iteration.

TABLE V: Comparison with other approaches for 64 bit
adders

Method
DesignWare
Ours (P1)
Kogge-Stone
Ours (P1)
Sklansky
Ours (P2)
[19]
Ours (P3)

Delay (ps)
346.5
339.0
347.9
339.0
356.1
353.0
348.7
343.0

Area (µm2)
2531.3
2180.8
2563.7
2180.8
1792.5
1753.0
1971.4
1912.6

Energy (f J/op)
8160
6930
8780
6930
6100
5900
6980
6390

delay, denoted by “DesignWare” in TABLE V. The same
pareto point P1 dominates that solution by providing around
7.5ps better delay, 14% lesser area, and 15% lesser energy.
For [19] we pick the best delay solution. Note for a ﬁxed
mf o, [19] can give preﬁx network with smaller size, but
this approach only provides a limited set of preﬁx structures.
As a result, it is hard for [19] to explore the full physical
design space of adders by machine learning. It should be
stressed that [19] beats the custom adders implemented in
an industrial design, and our methodology is able to excel the
adders generated by the algorithm presented in [19].

VII. CONCLUSION

This paper presents a novel methodology of machine
learning guided design space exploration for power efﬁcient
high-performance preﬁx adders. We have successfully demon-
strated the effectiveness of our learning models, developed
by training with quasi-random sampled data and features
encapsulating architectural and tool attributes. In addition,
an active learning approach is applied to ease the demand
of labeled data and achieves even better Pareto frontier.
Our adder synthesis algorithm is able to generate a wider
solution space in comparison to a state-of-the-art algorithm,
and when integrated with the learning model, could provide
a remarkable performance vs. power vs. area Pareto frontier
over a large representative solution space. To the best of our
knowledge, this is the ﬁrst work to bridge the gap between
architectural and physical solution space for parallel preﬁx
adders.

REFERENCES

[1] B. Yu, D. Z. Pan, T. Matsunawa, and X. Zeng, “Machine learning and
pattern matching in physical design,” in Proc. ASPDAC, 2015, pp. 286–
293.

[2] W.-T. J. Chan, K. Y. Chung, A. B. Kahng, N. D. MacDonald, and
S. Nath, “Learning-based prediction of embedded memory timing
failures during initial ﬂoorplan design,” in Proc. ASPDAC, 2016, pp.
178–185.

[3] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[4] W.-H. Chang, L.-D. Chen, C.-H. Lin, S.-P. Mu, M. C.-T. Chao, C.-
H. Tsai, and Y.-C. Chiu, “Generating routing-driven power distribution
networks with machine-learning technique,” in Proc. ISPD, 2016, pp.
145–152.

[5] R. Samanta, J. Hu, and P. Li, “Discrete buffer and wire sizing for link-
based non-tree clock networks,” in Proc. ISPD, 2008, pp. 175–181.
[6] R. P. Brent and H. T. Kung, “A regular layout for parallel adders,” IEEE
Transactions on Computers, vol. C-31, no. 3, pp. 260–264, 1982.
[7] P. M. Kogge and H. S. Stone, “A parallel algorithm for the efﬁcient
solution of a general class of recurrence equations,” IEEE Transactions
on Computers, vol. 100, no. 8, pp. 786–793, 1973.

[8] T. Han and D. Carlson, “Fast area-efﬁcient VLSI adders,” Proc. ARITH,

pp. 49–56, 1987.

[9] J. Sklansky, “Conditional sum addition logic,” IRE Trans. on Electronic

Computers, vol. EC-9, no. 2, pp. 226–231, 1960.

[10] C. Zhou, B. M. Fleischer, M. Gschwind, and R. Puri, “64-bit preﬁx
adders: Power-efﬁcient topologies and design solutions,” Proc. CICC,
pp. 179–182, 2009.

[11] J. Liu, Y. Zhu, H. Zhu, C.-K. Cheng, and J. Lillis, “Optimum preﬁx
adders in a comprehensive area, timing and power design space,” in
Proc. ASPDAC, 2007, pp. 609–615.

[12] N. H. Weste and D. Harris, CMOS VLSI Design: A Circuits and Systems

Perspective. Addison Wesley, 2004.

[13] T. Matsunaga and Y. Matsunaga, “Area minimization algorithm for par-
allel preﬁx adders under bitwise delay constraints,” in Proc. GLSVLSI,
2007, pp. 435–440.

[14] J. Liu, S. Zhou, H. Zhu, and C.-K. Cheng, “An algorithmic approach
for generic parallel adders,” in Proc. ICCAD, 2003, pp. 734–740.
[15] J. P. Fishburn, “A depth-decreasing heuristic for combinational logic;
or how to convert a ripple-carry adder into a carry-lookahead adder or
anything in-between,” in Proc. DAC, 1990, pp. 361–364.

[16] R. Zimmermann, “Non-heuristic optimization and synthesis of parallel

preﬁx adders,” Proc. IWLAS, pp. 123–132, 1996.

[17] M. Snir, “Depth-size trade-offs for parallel preﬁx computation,” Journal

of Algorithms, vol. 7, no. 2, pp. 185–201, 1986.

[18] H. Zhu, C.-K. Cheng, and R. Graham, “Constructing zero-deﬁciency
parallel preﬁx adder of minimum depth,” in Proc. ASPDAC, 2005, pp.
883–888.

[19] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” in Proc. ASPDAC, 2015, pp. 249–254.

[20] ——, “Towards optimal performance-area trade-off in adders by syn-
thesis of parallel preﬁx structures,” IEEE TCAD, vol. 33, no. 10, pp.
1517–1530, 2014.

[21] M. Choudhury, R. Puri, S. Roy, and S. C. Sundararajan, “Automated
synthesis of high performance two operand binary parallel preﬁx adder,”
Patent US 8,683,398, Mar, 2014.

[22] G. Palermo, C. Silvano, and V. Zaccaria, “ReSPIR: a response surface-
based pareto iterative reﬁnement for application-speciﬁc design space
exploration,” IEEE TCAD, vol. 28, no. 12, pp. 1816–1829, 2009.
[23] H.-Y. Liu and L. P. Carloni, “On learning-based methods for design-
space exploration with high-level synthesis,” in Proc. DAC, 2013, pp.
50:1–50:7.

[24] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[25] B. R. Zeydel, T. T. J. H. Kluter, and V. G. Oklobdzija, “Efﬁcient
mapping of addition recurrence algorithms in CMOS,” Proc. ARITH,
pp. 107–113, 2005.

[26] M. Ketter, D. M. Harris, A. Macrae, R. Glick, M. Ong, and J. Schauer,
“Implementation of 32-bit Ling and Jackson adders,” Proc. Asilomar,
pp. 170–175, 2011.

[27] E. Zitzler, D. Brockhoff, and L. Thiele, “The hypervolume indicator
revisited: On the design of pareto-compliant indicators via weighted
integration,” in Evolutionary multi-criterion optimization, 2007, pp.
862–876.

[28] “Synopsys Design Compiler,” http://www.synopsys.com.
[29] “Synopsys IC Compiler,” http://www.synopsys.com.
[30] “Synopsys SAED Library,” http://www.synopsys.com/Community/
UniversityProgram/Pages/32-28nm-generic-library.aspx, accessed 23-
April-2016.

[31] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-
tions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.

[32] K. Tumer and J. Ghosh, “Estimating the bayes error rate through
classiﬁer combining,” in Proc. ICPR, vol. 2, 1996, pp. 695–699.
[33] M. Zuluaga, A. Krause, G. Sergent, and M. P¨uschel, “Active learning
for multi-objective optimization,” in Proc. ICML, 2013, pp. 462–470.
[34] C. E. Rasmussen and C. K. I. Williams, Gaussian Process for Machine

Learning. The MIT Press, 2006.

[35] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Gaussian process
optimization in the bandit setting: no regret and experimental design,”
in Proc. ICML, 2010, pp. 1015–1022.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in Python,” Journal of Machine Learn-
ing Research, vol. 12, no. Oct., pp. 2825–2830, 2011.

[37] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” IEEE TCAD, vol. 35, no. 5, pp. 820–831, 2016.

[38] T. McAuley, W. Koven, A. Carter, P. Ning, and D. M. Harris, “Imple-
mentation of a 64-bit Jackson adders,” Proc. Asilomar, pp. 1149–1154,
2013.

[39] S. Roy, Y. Ma, J. Miao, and B. Yu, “A learning bridge from architec-
tural synthesis to physical design for exploring power efﬁcient high-
performance adders,” in Proc. ISLPED, 2017, pp. 1–6.

Cross-layer Optimization for High Speed Adders:
A Pareto Driven Machine Learning Approach

Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, and Bei Yu

8
1
0
2
 
t
c
O
 
6
1
 
 
]

R
A
.
s
c
[
 
 
2
v
3
2
0
7
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—In spite of maturity to the modern electronic design
automation (EDA) tools, optimized designs at architectural stage
may become sub-optimal after going through physical design
ﬂow. Adder design has been such a long studied fundamental
problem in VLSI industry yet designers cannot achieve optimal
solutions by running EDA tools on the set of available preﬁx
adder architectures. In this paper, we enhance a state-of-the-
art preﬁx adder synthesis algorithm to obtain a much wider
solution space in architectural domain. On top of
that, a
machine learning-based design space exploration methodology
the adders in
is applied to predict
physical domain, which is infeasible by exhaustively running
EDA tools for innumerable architectural solutions. Considering
the high cost of obtaining the true values for learning, an active
learning algorithm is proposed to select the representative data
during learning process, which uses less labeled data while
achieving better quality of Pareto frontier. Experimental results
demonstrate that our framework can achieve Pareto frontier of
high quality over a wide design space, bridging the gap between
architectural and physical designs. Source code and data are
available at https://github.com/yuzhe630/adder-DSE.

the Pareto frontier of

I. INTRODUCTION

I N the last decades, the industrial EDA tools have advanced

towards optimality, especially at the individual stages of
VLSI design cycle. Nevertheless, with growing design com-
plexity and aggressive technology scaling, physical design
issues have become more and more complex. As a result,
the constraints and the objectives of higher layers, such as
the system or logic level, are very difﬁcult to be mapped
into those of lower layers, such as physical design, and vice-
versa, thereby creating a gap between the optimality at the
logic stage and the physical design stage. This necessitates
the innovation of data-driven methodologies, such as machine
learning [1]–[5], to bridge this gap.

Adder design is one of the fundamental problems in digital
semiconductor industry, and its main bottleneck (in terms
of both delay and area) is the carry-propagation unit. This
unit can be realized by hundreds of thousands of parallel
preﬁx structures, but it is hard to evaluate the ﬁnal metrics
without running through physical design tools. Historically,
regular adders [6]–[9] have been proposed for achieving the
corner points in terms of various metrics as shown in Fig. 1

The preliminary version has been presented at the IEEE International
Symposium on Low Power Electronics and Design (ISLPED) in 2017. This
work is supported in part by The Research Grants Council of Hong Kong SAR
(Project No. CUHK24209017) and CUHK Undergraduate Summer Research
Internship 2017.

Y. Ma, J. Chen and B. Yu are with the Department of Computer Science

and Engineering, The Chinese University of Hong Kong, NT, Hong Kong.

S. Roy is with Intel Corporation, San Jose, CA, USA.
J. Miao is with the Cadence Design Systems, San Jose, CA, USA.

Fig. 1: Regular adders (picture taken from [12]).

in architectural stage. The main motivation for structural
regularity was the ease of manual layout, but EDA tools now
taking care of all physical design aspects, the regularity is
no longer essential. Moreover, the extreme corners do not
map well to the physical design metrics after synthesis, place-
ment and routing. To address this gap between preﬁx adder
synthesis and actual physical design of the adders, custom
adders are typically designed by tuning parameters, such as
gate-sizing, buffering etc., targeting at the optimization of
power/performance metrics for a speciﬁc technology library
[10], [11]. However, this custom approach (i) needs signiﬁcant
engineering effort, (ii) is not ﬂexible to Engineering Change
Order (ECO), and (iii) does not guarantee the optimality.

The algorithmic synthesis approach resolves the ﬁrst two
issues of the custom approach, by adding more ﬂexibility to
the late ECO changes and reducing the engineering effort.
Based on the number of solutions, the existing adder synthesis
algorithms can be broadly classiﬁed into two categories. The
ﬁrst and the most common approach is to generate a single
preﬁx network for a set of structural constraints, such as the
logic level, fan-out etc. Several algorithms have been proposed
to minimize the size of the preﬁx graph (s) under given bit-
width (n) and logic-level (L) constraints [13]–[16]. Closed
form theoretical bounds for size-optimality are provided by
[17] for L ≥ 2 log2 n − 2. [18] has given more general bound
for preﬁx graph size, but when L is reduced to log2 n, a pre-
requisite for high-performance adders, there is no closed form
bound for s. [19] presents a polynomial-time algorithm for

generating preﬁx graph structures by restricting both logic-
level and fan-out. The limitations in these approaches are
two-fold, (i) this restricted set of structures is not capable
of exploring the large solution space, and (ii) since it is very
hard to analytically model the physical design complexities,
such as wire-length and congestion issues, the physical design
metrics, such as the area, power, delay etc., may not be
mapped well to the preﬁx structure metrics, such as the size,
max-fan-out (mf o) etc. This motivates the second category
of algorithms where thousands of preﬁx adder solutions can
be generated and explored for synthesis and physical design
in the commercial EDA tools.

One such approach is [20] which presents an exhaus-
tive bottom-up enumeration technique with several pruning
strategies to generate innumerable preﬁx structure solutions.
However, it has two issues, (i) this approach cannot provide
solutions in several cases for restricted fan-out, which can
control the congestion and load-distribution during physical
design [19]. As a result, it may still miss the good solution
space to a large extent, and (ii) it is computationally very
intensive to run all solutions through synthesis, placement and
routing.

In this paper, we enhance the algorithm in [20], [21] to
generate adders under any arbitrary mf o constraint, which
enables a wider adder solution space in logical form. To
tackle the high computational effort during the physical design
ﬂow, we further propose to use machine learning to perform
the design space exploration in physical solution space. We
develop Pareto frontier driven machine learning methodolo-
gies to achieve rich adder solutions with trade-offs among
power, area, and delay. As a passive supervised learning, the
proposed quasi-random sampling approach is able to select
representative preﬁx adders out of the hundreds of thousands
of preﬁx structures.

It should be noted that various machine learning algorithms
have been investigated to explore design space in different
design scenarios. Palermo et al. [22] deploy both linear
regression and artiﬁcial neural network for multiprocessor
systems-on-chip design. Lin et al. [23] present a random
forest-based learning model in high level synthesis, which
can ﬁnd an approximate Pareto-optimal designs effectively.
Meng et al. [24] propose a random forest-based method for
Pareto frontier exploration, where non-Pareto-optimal designs
are carefully eliminated through an adaptive strategy. Multiple
predictions can be obtained through random forest, which can
be used for estimating the uncertainty. Superior to the random
forest, in this paper we further propose an active learning
approach based on Gaussian Process (GP), which by nature
can estimate the prediction uncertainty efﬁciently.

Our main contributions are summarized as follows:
• A comprehensive framework for optimal adder search
by machine learning methodology bridging the preﬁx
architecture synthesis to the ﬁnal physical design;

• An enhancement to a state-of-the-art preﬁx adder algo-
rithm [20] to optimize the preﬁx graph size for restricted
fan-out and explore a wider solution space;

• A machine learning model for preﬁx adders, guided by
quasi-random data sampling with features considering

architectural attributes and EDA tool settings;

• A design space exploration method to generate the Pareto
frontier for delay vs. power/area over a wide design
space;

• An active learning approach for the design space explo-
ration, which uses less labeled data and achieves better
quality of Pareto frontier.

The rest of the paper is organized as follows. Section II
presents the background of preﬁx adder synthesis, while
Section III discusses our preﬁx graph generation algorithm.
Next, two machine learning approaches of design space explo-
ration for high-performance adders are described. Section IV
presents the passive supervised learning, while Section V
introduces a Pareto frontier driven active learning approach.
Section VI lists the experimental results, followed by conclu-
sion in Section VII.

II. PREFIX ADDER SYNTHESIS

In this section, we ﬁrst provide the background of the preﬁx
adder synthesis problem. Then we present a brief discussion
on the algorithm presented in [20], which we enhance to our
Preﬁx Graph Generation (PGG) algorithm to synthesize the
preﬁx adder network.

A. Preliminaries

An n bit adder accepts two n bit addends A = an−1..a1a0
and B = bn−1..b1b0 as input, and computes the output
sum S = sn−1..s1s0 and carry out Cout = cn−1, where
si = ai ⊕ bi ⊕ ci−1 and ci = aibi + aici−1 + bici−1. The
simplest realization for the adder network is the ripple-carry-
adder, but with logic level n − 1, which is too slow. For faster
implementation, carry-lookahead principle is used to compute
the carry bits. Mathematically, this can be represented with
bitwise (group) generate function g (G) and propagate func-
tion p (P ) by the Weinberger’s recurrence equations as follows
[25]:

• Pre-processing (inputs): Bitwise generation of g, p

gi = ai · bi and pi = ai ⊕ bi.

(1)

• Preﬁx processing: This part is the main carry-propagation
component where the concept of generate/propagate is
extended to multiple bits and G[i:j], P[i:j] (i ≥ j) are
deﬁned as

(cid:26) pi,

(cid:26) gi,

P[i:j] =

G[i:j] =

if i = j,

P[i:k] · P[k−1:j], otherwise,

G[i:k] + P[i:k] · G[k−1:j], otherwise.

if i = j,

(2)

(3)

The associative operation ◦ is deﬁned for (G, P ) as:

(G, P )[i:j] = (G, P )[i:k] ◦ (G, P )[k−1:j]

= (G[i:k] + P[i:k] · G[k−1:j], P[i:k] · P[k−1:j]).
(4)

• Post-processing (outputs): Sum/Carry-out generation

si = pi ⊕ ci−1,

ci = G[i:0], and Cout = cn−1.

(5)

The ‘Preﬁx processing’ or carry propagation network can be
mapped to a preﬁx graph problem with inputs ik = (pk, gk)
and outputs ok = ck, such that ok depends on all previous
inputs ij (j ≤ k). Any node except the input nodes is called
a preﬁx node. Size of the preﬁx graph is deﬁned as the number
of preﬁx nodes in the graph. Fig. 2 shows an example of such
preﬁx graph of 6 bit and we can see that Cout = c5 = o5 is
given by

o5 = (i5 ◦ i4) ◦ ((i3 ◦ i2) ◦ (i1 ◦ i0)).

(6)

Size (s), logic level (L) and maximum-fan-out (mf o) for
this network are respectively 8, 3 and 2. Note that here the
number of fan-ins for each of the associative operation o is
two, thus this is called radix-2 implementation of the preﬁx
graph. However, there exist other options such as radix-3 or
radix-4, but the complexity is very high and not beneﬁcial in
static CMOS circuits [26]. In this work, the logic levels for
all output bits are log2 n, i.e., the minimum possible, to target
high performance adders.

B. Discussion on [20]

Our PGG algorithm to generate the preﬁx graph structures
for physical solution space exploration is based on [20]. So
it is imperative to ﬁrst discuss about [20]. However, we omit
the details and only mention the key points of [20] due to
space constraint.

[20] is an exhaustive bottom-up and pruning based enu-
meration technique for preﬁx adder synthesis. This work
presented an algorithm to generate all possible n + 1 bit
preﬁx graph structures from any n bit preﬁx graph. Then
this algorithm is employed in a bottom-up fashion (from 1
bit adder to 2 bit adders, then from all 2 bit adders to 3
bit adders, and so on) to synthesize preﬁx graphs of any
bit-width. As a result, scalability issue arises due to the
exhaustive nature of the algorithm, which is then tackled by
adopting various pruning strategies to scale the approach.
However, the pruning strategies are not sufﬁcient to scale
the algorithm well for different fan-out constraints. So when
it intends to ﬁnd the solutions for higher bit adders, the
intermediate adder solutions that need to be generated are
often huge. Consequently, it fails to get fan-out restricted
(e.g. when mf o = 8, 10, 12 etc. for 64 bit adders) solutions
even with 72GB RAM due to the generation of innumerable
intermediate solutions [19]. Pruning strategies, such as size-
bucketing [20], help to achieve solutions in some cases, but
with sub-optimality. So design space-exploration based on
this algorithm can miss a signiﬁcant spectrum of the adder
solutions.

C. Our PGG Algorithm

To better explore the wide design space of adders, in this
paper we have enhanced [20] for different fan-out constraints
by incorporating more pruning techniques.

Fig. 2: 6 bit preﬁx adder
network.

Fig. 3: Imposing semi-regularity.

1) Semi-Regularity in Preﬁx Graph Structure: The ﬁrst
strategy is to enforce a sort of regularity in the preﬁx graphs.
For instance, regular adders, such as Sklansky, Brent-Kung,
have the inherent property that the consecutive input nodes
(even and odd) are combined to create the preﬁx nodes at the
logic level 1. In our approach, we constrain this regularity
for those preﬁx nodes (logic level 1). To explain this, let
us consider Fig. 3. We can see that preﬁx nodes r1, r2, r3
and r4 are constructed by consecutive even-odd nodes. For
instance, i0 and i1 are used to construct r1. But with this
structural constraint, we are not allowed to construct any node
by combining i1 and i2 as done in Kogge-Stone adders. Note
that the sub-structure, as shown in Fig. 3, is a part of some
regular adders like Sklansky adder, and is imposed in our
preﬁx structure enumeration.

We have run experiments with 16 bit adders, and observed
that this pruning strategy (i) does not degrade the solution
quality (or size of the preﬁx graph under same L and mf o),
but (ii) able to reduce the search space signiﬁcantly,
in
comparison to not using this pruning strategy.

2) Level Restriction in Non-trivial Fan-in: Each of the
preﬁx node N (a:b), where a is the most-signiﬁcant-bit (MSB)
and b is the least-signiﬁcant-bit (LSB),
is constructed by
connecting the trivial fan-in Ntr (a:c) having same MSB as
N , and the non-trivial fan-in Nnon−tr (c − 1:b). For instance,
in Fig. 2, o3 and b2 are respectively the non-trivial fan-in
and the trivial fan-in node for the preﬁx node o5. In the
bottom-up enumeration technique, we put another additional
restriction that the level of the trivial fan-in node is always
i.e.,
less or equal
level(Ntr) ≤ level(Nnon−tr). Note that this sort of structural
restriction is also inherent in regular adders, such as Sklansky
or Brent-Kung adders.

to that of the non-trivial fan-in node,

In a nutshell, our PGG algorithm is a blend of regular
adders and [20]. We borrow some properties of regular adders
to enforce in [20] for reducing its huge search space without
hampering the solution quality. To illustrate this, we have
obtained the binary for [20] from the authors, and ﬁrst
compared our result for lower bit adders, such as n = 16, 32.
We got the solutions with same minimum size, which proves
that our structural constraints have not degraded the solution
quality. However, for higher bit adders, we get better solution
quality than [20] as shown in TABLE I.

Column 1 presents the mf o constraint, while columns
2 and 3 respectively show the size and run-time for our
enhanced algorithm, and the corresponding entries for [20]
are respectively represented in columns 4 and 5. In general,

TABLE I: Comparison with [20] for 64 bit adders

mf o

4
6
8
12
16
32

Our Approach

Approach in [20]

size
244
233
222
201
191
185

Run-time (s)
302
264
423
193
73
0.04

size
252
238
-
-
192
185

Run-time (s)
241
212
-
-
149
0.04

when fan-out is relaxed or mf o is higher, the run-time is less
due to relaxed size-pruning as explained in [20]. Note that [20]
cannot generate solutions for mf o = 8, 12 due to generation
of innumerable intermediate solutions as explained in [19]. On
the contrary, our structural constraints can do a pre-ﬁltering
of the potentially futile solutions, thereby allowing relaxed
size-pruning and size-bucketing to search for more effective
solution space. In terms of run-time, it is slightly worse in
a few cases, but importantly, this generation is a one-time
process, and this run-time is negligible in comparison to the
design space exploration by the physical design tools. So our
imposed structural restrictions (i) do not degrade the solution
quality, (ii) achieve better solution sizes for all mf o than [20]
for higher bit adders which could not even generate solutions
in all cases, and (iii) help to obtain wider physical solution
space to be demonstrated in Section II-E.

D. Quasi-random Sampling

We have mainly focused on 64 bit adders in this work as
this is mostly used in today’s microprocessors. From all preﬁx
adder solutions, we sample a set of solutions for building
the learning model via the quasi-random approach which
is conducted by a two-level binning (mf o, s) followed by
random selection, This approach aims to evenly sample the
preﬁx adders covering different architectural bins. The pri-
mary level of binning is determined by mf o of the solutions.
However, there may be thousands of architectures sharing the
same mf o, so the secondary level of binning is based on s.
Afterwards, adders are picked randomly from those secondary
bins.

We illustrate the quasi-random sampling with the following
example: given 5000 solutions with mf o = 4, we want to
pick 50 solutions from them. Suppose these 5000 solutions
have the size distribution from 244 to 258. First a random
solution is picked from the bucket of the solutions (mf o = 4,
s = 244). Then we pick a solution randomly from (mf o = 4,
s = 245), and so on. After picking 15 solutions from each of
those buckets with mf o = 4, we again start from the bucket
(mf o = 4, s = 244). This process is repeated until we get 50
solutions. Similar procedure is done with other mf o values.

E. Physical Solution Space Comparison with [20]

In this subsection, we show the usefulness of our algorithm
for obtaining wider solution space in physical design domain
in comparison to [20]. Among the preﬁx adders generated by
[20], we randomly sampled 7000 preﬁx adders. Those preﬁx
adders are fed into the full EDA ﬂow (synthesis, placement
and routing) to get their real delay, power and area values

(a)

(c)

(b)

(d)

Fig. 4: Quasi-random sampled adders vs. adders from [20].
(a) Solution space in area vs. delay domain from [20]; (b)
Solution space in area vs. delay domain from ours; (c)
Solution space in power vs. delay domain from [20]; (d)
Solution space in power vs. delay domain from ours.

(takes around 700 hours). We plot these adders by [20] and
our representative 3000 adders in Fig. 4. It can be seen that,
although the numbers of adders by [20] is more than 2 times of
our representative adders, our adders still cover wider solution
space in physical domain, demonstrating the effectiveness of
our enhanced algorithm PGG. This is in accordance with
the solutions missed by [20] as mentioned in TABLE I.
Those availabilities eventually offer more opportunities for
our machine learning methodology to identify close to ground
truth Pareto frontier solutions.

III. BRIDGING ARCHITECTURAL SOLUTION SPACE TO
PHYSICAL SOLUTION SPACE

In most EDA problems, the metrics of the solution quality
are typically conﬂicting. For instance, if we optimize the tim-
ing of the design, then the power/area may be compromised
and vice versa. So one imperative job of EDA engineers
is to ﬁnd the Pareto-optimal points of the design enabling
the designers to select among those. In this section, we ﬁrst
provide the preliminaries about Pareto optimality, and the
error metrics of Pareto optimal solutions. Then we discuss
the gap between the preﬁx architectural solution space and
physical solution space in adders, which motivates the need of
the machine learning-based approach for optimal adder explo-
ration. Finally, a domain knowledge-based feature selection
details are presented along with training data sampling for
the learning models.

architecture stage are preﬁx node size s and max fan-out mf o.
These two metrics are conﬂicting, i.e., if we reduce mf o,
s increases and vice-versa. Similar competing relationship
exists between delay and power/area after physical design. It
should be stressed that power and s are correlated, and mf o
indirectly controls the timing as more restricted fan-out can
mitigate congestion and load-distribution, thereby improving
the delay of the adder. However, this relationship between
architectural synthesis and physical design is approximate,
and not a very high-ﬁdelity one.

To demonstrate this, we plot node size s vs. mf o and power
vs. delay in Fig. 6 for several 64 bit adder solutions. In this
experiment, we have generated the preﬁx architecture solu-
tions by PGG, and the ﬁnal power/delay numbers are obtained
by running those solutions through EDA tools as explained
later in Section VI. An example of the preﬁx architecture and
the corresponding physical solution is presented in Fig. 7. In
Fig. 6(a), we broadly categorize the solutions into 2 groups,
(i) G1 with higher node size and lower mf o, and (ii) G2
with lower node size and higher mf o. In Fig. 6(b), the same
designs as Fig. 6(a) are projected into the physical solution
space, restoring the group information. Design Compiler [28]
(version F-2011.09-SP3) is used for logical synthesis, and
IC Compiler [29] (version J-2014.09-SP5-3) is used for the
placement and routing. Non linear delay model (NLDM) in
32nm SAED cell-library [30] is used for technology mapping.
The key observations here are ﬁrstly, there is a correlation
between architectural solution space and physical design
solution space. For instance, the solutions from G1 are mostly
on the upper side, and those of G2 are mostly on the lower
side in Fig. 6(b), thereby indicating a correspondence between
s and power. Nevertheless, it is not completely reliable. For
example, (i) the delay numbers for G1 and G2 are very much
spread, (ii) a cluster can be observed where the solutions
from G1 and G2 are mixed up in Fig. 6(b), and (iii) several
solutions of G1 are better than several solutions of G2 in
power, which is not in accordance with the metrics at the
preﬁx adder architecture stage. So we can not utterly rely on
architectural solution space to achieve the optimal output in
physical solution space.

However, since our algorithm generates hundreds of thou-
sands of preﬁx graph structures, it is intractable to run synthe-
sis and physical design ﬂows for even a small percentage of
all available preﬁx adder architectures. To address this ﬁdelity
gap between the two design stages and the high computational
cost together, we come up with a novel machine learning
guided design space exploration as replacement of exhaustive
search.

C. Feature Selection

The feature is a representation which is extracted from the
original input representation, and it plays an important role
in machine learning tasks. We now discuss the features to be
used for the learning model. Features are considered from both
preﬁx adder structure and tool settings, with a focus on the
former. We select node size and maximum-fan-out (mf o) of
a preﬁx adder as two main features for our learning model.

Fig. 5: Hypervolume with two objectives in objective space.

(a)

(b)

Fig. 6: Gap between preﬁx structure and physical design of
adders: (a) Architectural solution space; (b) Physical solution
space.

A. Preliminaries
Deﬁnition 1 (Pareto Optimality). An objective vector f (x) is
said to dominate f (x(cid:48)) if:

∀i ∈ [1, n], fi(x) ≤ fi(x(cid:48))
and ∃j ∈ [1, n], fj(x) < fj(x(cid:48)).
A point x is Pareto-optimal if there is no other x(cid:48) in design

(7)

space such that f (x(cid:48)) dominates f (x).

As in this paper for adder design, a Pareto-optimal design
is where none of the objective metrics, such as area, power
or delay, can be improved without worsening at least one of
the others. The Pareto Frontier is the set of all the Pareto-
optimal designs in the objective space. Therefore, the goal is
to identify the Pareto-optimal set P for all the Pareto-optimal
designs.

Deﬁnition 2 (Hypervolume). The hypervolume computes the
volume enclosed by the Pareto frontier and the reference point
in the objective space [27].

In Fig. 5, the shaded area is an example of the hypervolume
of a Pareto set with two objectives. Then the hypervolume
error for a predicted Pareto set ˆP is deﬁned as

η =

V (P ) − V ( ˆP )
V (P )

,

(8)

where P is the true Pareto-optimal set, and V (P ) is the
hypervolume of the Pareto set P . Note that a prediction ˆP
which contains the whole design space has an error of 0.
Thus the predicted set ˆP with less points is desired.

B. Gap Between Logic and Physical Design

Since we focus on high performance adders and explore
the preﬁx adders of logic level L = log2 n, the metrics at this

(a)

(b)

Fig. 7: (a) An example of architectural solution: Bit-width = 64, size = 201, Max. level
= 6, Max. fanout = 12; (b) Corresponding physical solution.

Fig. 8: Deﬁning spf o of a
node.

However, for any given mf o and node size, there will be
hundreds or even thousands of different preﬁx architectures.
Therefore, additional features are required to better distinguish
individual preﬁx adder attributes. We deﬁne a parameter sum-
path-fan-out (spf o) for this. Let a and b are the fan-in nodes
of a node n, then spf o(n) is deﬁned recursively as:

spf o(n) =

0,
sum(f o(a) + spf o(a),

if n ∈ input,

(9)

f o(b) + spf o(b)), otherwise.






Here f o(n) denotes the fan-out of any node n. Consider the
preﬁx adder structure in Fig. 8, and according to the deﬁnition
we have:

spf o(o1) = sum(f o(i0) + spf o(i0), f o(i1) + spf o(i1))

spf o(b1) = sum(f o(i2) + spf o(i2), f o(i3) + spf o(i3))

spf o(b2) = sum(f o(i4) + spf o(i4), f o(i5) + spf o(i5))

= sum(1, 1) = 2,

= sum(2, 1) = 3,

= sum(2, 1) = 3.

Therefore, we can use the recursive deﬁnition to calculate

spf o(o3) = sum(f o(o1) + spf o(o1), f o(b1) + spf o(b1))

= sum(3 + 2, 2 + 3) = 10,

spf o(o5) = sum(f o(o3) + spf o(o3), f o(b2) + spf o(b2))

= sum(3 + 10, 3 + 3) = 19.

In our methodology, we use the spf o of the output nodes
which are at log2 n level (there are 32 nodes at level 6 for 64
bit adder) as the features to characterize the preﬁx structures,
in addition to mf o, size and target delay. The basic intuition
for selecting spf o of the output nodes as the features is that
the critical path delay of the adder is the longest path delay
from input to output. So it depends on the (i) path-lengths,
which can be represented at the preﬁx graph stage by the
logic level of the node, and (ii) the number of fan-outs driven
at every node on the path. Note that we have skipped the
spf o of the output nodes which are not at log2 n level as for
those nodes, the path length is smaller, and those would not
potentially dictate the critical path delay.

Apart from these preﬁx graph structural features, we also
consider tool settings from synthesis stage and physical design
stage as other features. We have synthesized the adder struc-
tures using industry-standard EDA synthesis tool [28], where

we can specify the target-delay for the adder. The tool then
adopts different strategies internally to meet that target-delay
which we can hardly take into account during preﬁx graph
synthesis. Consequently, changing the target-delay can lead
to different power/timing/area metrics. So we have considered
target-delay as a feature in our learning approach.

In physical design, utilization is an important parameter,
which deﬁnes the area occupied by standard cell, macros and
blockages. Different utilization values can lead to different
layouts after physical design. Therefore, we take utilization
as another feature in the learning model.

In addition to the target delay and utilization, other tool
settings have also been explored. The optimization level
setting in logical synthesis has a potential impact on the per-
formance of adders, which can be adjusted by compile and
compile_ultra commands with different options. After
synthesizing, it is observed that the solutions generated with
compile_ultra can signiﬁcantly dominate the solutions
generated by compile. Therefore, this setting is ﬁxed to
compile_ultra level as we are aiming at superior designs.
In this work, the technology node is not used as a feature.
From the machine learning perspective, there is a common
assumption for conventional machine learning applications
that
the training and test data are drawn from the same
feature space and the same distribution [31]. The values of
area/power/delay may vary a lot under different technology
nodes, which results in different underlying data distributions.
Therefore, the technology node for synthesis should be con-
sistent. The proposed approach for feature extraction can also
be applied to other technology nodes as long as the technology
node is consistent during the design ﬂow. If the technology
node of the testing data switches to another one, the machine
learning model should be re-trained using the data from that
technology node to ensure the accuracy of the model.

D. Data Sampling

Since we can not afford to run the physical design ﬂow
for too many architectures, and too few training data may
degrade the model accuracy signiﬁcantly, a set of adders need
to be selected to represent the entire design solution space.
However, ﬁnding a succinct set of representative training data
for the traditional supervised learning is difﬁcult. In order
to tackle this difﬁculty, we come up with two learning ap-
proaches in the next two sections. The ﬁrst one is the passive

for delay with only primary features. Best model ﬁtting for
delay is achieved with SVR (RBF kernel) with these 4 primary
and 32 secondary features. Since SVR with RBF kernel give
good MSE (mean-squared-error) scores for all metrics, delay,
area and power, we have used this model throughout for
design space exploration.

The model experiments give us the following key insights:
(i) tool setting can play an important role in building the
learning models in EDA. For instance, MSE scores for area
and power improve from 0.021 to 0.003, and 0.228 to 0.027
respectively when we add the ‘target delay’ feature in our
model building, (ii) secondary features play an important
role in improving the model accuracy. For instance, when
we include spf o features in model building, MSE score for
delay improves from 0.200 to 0.170. (iii) linear models are
not sufﬁcient for modeling delay. For instance, MSE scores of
delay improve from 0.214 to 0.170 when we go from linear
models to SVR with RBF kernel, with the same set of features.
The problem of exploring the Pareto frontier of rich preﬁx
adder space can be approached by ﬁrst sampling a subset
of preﬁx adder architectures, and generating the power, area,
delay numbers of each preﬁx adder by running through the
logic synthesis and physical design ﬂow. Those known data
set will be used as the training and testing data for supervised
machine learning guided model ﬁtting. Once the model is
ﬁtted, we can apply the exhaustive preﬁx adder architectures
to this model and get the predicted Pareto frontier solution
set. This is due to the merit of much faster runtime for a
machine learning model in prediction stage than running the
entire VLSI CAD ﬂow.

However, conventional machine learning problem aims at
maximizing the prediction accuracy rather than exploring a
Pareto frontier out of a solution set. Improving the model
accuracy does not necessarily improve the Pareto frontier and
the direct use of the ﬁtted model for Pareto frontier exploration
can even miss up to 60% Pareto frontier points [3]. We
therefore need a machine learning integrated Pareto frontier
exploration methodology, where the Pareto frontier selection
does not rely only on the model accuracy. So we develop a fast
yet effective algorithmic methodology, enabled by regression
model to explore the Pareto frontier of preﬁx adder solutions.
First we consider two spaces for Pareto frontier exploration:
the delay vs. area as well as the delay vs. power. For either
space, there exists a strong trade-off between the two metrics.
For delay vs. power space, we propose to use a joint output
Power-Delay function (P D) as the regression output rather
than using any single output.

P D = α · P ower + Delay.

(10)

The rationale of using scalarization [32] or the linear
summation of the power and delay metrics is that such a linear
relation provides a weighted bonding between the power and
the delay so that by changing the α value, the regression
model will try to minimize the prediction error on the more
weighted axis hence leads to more accuracy on that direction.
In contrast, the other metric direction will be predicted with
less accuracy hence introducing some level of relaxations. It
can be foreseen that changing the α value can lead to different

Fig. 9: Overall ﬂow of α-sweep learning.

supervised learning where a quasi-random data sampling is
performed to obtain the training data, followed by multi-
objective scalarization to achieve the Pareto optimal solutions.
The second one is the active learning approach where model
training is integrated to ﬁnding Pareto-optimal frontiers of the
design space.

IV. α-SWEEP LEARNING
In this section we propose a pareto-frontier exploration
ﬂow which is based on support vector machine. The overall
ﬂow of our α-sweep supervised learning-based Pareto-frontier
exploration is presented in Fig. 9.

A. Scalarization to the Single-Objective

In this work, supervised learning is preferred over unsu-
pervised learning since supervised learning has a substantial
advantage over unsupervised learning for our problem. In
particular, supervised learning allows to take advantage of
the golden result, i.e., the true area/power/delay, generated
by the synthesis tools for each design, instead of just letting
the algorithm work out for itself what the classes should be.
In general, supervised learning usually outperforms the unsu-
pervised learning for this kind of regression and classiﬁcation
tasks.

Before applying machine learning for exploring Pareto
frontier, we ﬁrst validate the effectiveness of the features we
extract by building regression models for single metric predic-
tion. For learning models, we explored (i) several supervised
learning techniques, such as linear regression, Lasso/Ridge,
Bayesian ridge model and support vector regression (SVR)
with linear, polynomial and radial-basis-function (RBF) ker-
nel, and (ii) 36 features, including 4 primary features, size,
mf o,
target delay and utilization (tool settings), and 32
secondary features for spf o. We observed that we could get
an R2 score above 0.95 for area and power even with primary
features and linear models. However, we don’t get good scores

ﬁtting accuracies of the regression model. By sweeping α
over a wide range from 0 to large positive values, each time
the regression model will be ﬁtted to predict different best
solutions which altogether form the Pareto frontier. We call
this approach α-sweep. Note that, the P ower and Delay
values in Equation (10) are normalized and scaled to the range
between 0 and 1 by Equation (11).
x − min(X)
max(X) − min(X)

, x ∈ X.

(11)

x =

Similarly, we have a joint output Area-Delay (AD) function

for Pareto frontier exploration on Area and Delay space.

AD = α · Area + Delay.

(12)

This α-sweep technique can be extended to simultaneously
consider power, performance or delay, and area (PPA), using
two scalars (α1 and α2) instead of one scalar factor α. The
joint output function for Pareto frontier exploration on area –
power – delay space can be formulated as:

P P A = α1 · Area + α2 · P ower + Delay.
The results of α-sweep for both two-dimensional space and

(13)

three-dimensional space are shown in the Section VI.

V. PARETO ACTIVE LEARNING
In our adder design problem, obtaining the true area/pow-
er/delay values or the labeled data for each adder requires
running logic synthesis and physical design ﬂow, which is
often time-consuming if the amount of data is huge. Active
learning is an iterative supervised learning which is able to
interactively query the data pool to obtain the desired outputs
at new data points. Since the samples are selected by the
learning algorithm, the number of samples to ﬁt a model can
often be much lower than the number required in traditional
supervised learning. Since an active sampling strategy is
required in active learning, an “uncertainty estimation” of
the prediction is needed. Gaussian Process (GP) can make
predictions and, more importantly, provide the uncertainty
estimation of its predictions by nature. Therefore, in this paper
we further propose a Pareto active learning algorithm based
on Gaussian Process regression.

A. Overall Flow

The overall ﬂow of the Pareto active learning (PAL) is
shown in Fig. 10. Given all the preﬁx adder structures, ﬁrst
we extract the feature vector for each adder as introduced
in Section III-C. The active learning starts with Gaussian
Process regression which will be illustrated later. Unlike the
passive supervised learning in which all
the features and
the corresponding labels are prepared in advance, the active
learning derives the labels of each training data during the
learning process on-demand. To be speciﬁc, the algorithm in-
crementally identiﬁes the most representative instances along
with their features which are later fed into EDA synthesis ﬂow
(synthesis, placement and routing) for true area/power/delay
numbers. Namely, the EDA synthesis ﬂow and the learning
process are interleaving. As more and more designs being
selected, the model gets more and more accurate till conver-
gence.

Fig. 10: Overall ﬂow of Pareto active learning.

B. Gaussian Process Prediction

A Gaussian process is speciﬁed by its mean function and
covariance function. A Pareto active learning scheme based
on Gaussian process regression is proposed in [33]. The prior
information is important to train the Gaussian Process model,
which is a parameterized mean and covariance functions.
Conventionally, the training process selects the parameters in
the light of training data such that the marginal likelihood
is maximized. Then the Gaussian Process model can be
obtained and the regression can be proceeded with supervised
input [34]. The ability of GP indicating prediction uncer-
tainty reﬂects in GP learner providing a Gaussian distribution
N (m(x), σ(x)) of the values predicted for any test input x
by computing
m(x) = k(x, X)(cid:62)(k(X, X) + σ2I)−1Y,
σ2(x) = k(x, x) − k(x, X)(cid:62)(k(X, X) + σ2I)−1k(x, X),

(14)
where X is the training set, Y is the supervised information
of trained set X. For Gaussian Process regression, a prediction
of a design objective consists of a mean and a variance.
The mean value m(x) represents the predicted value and the
variance σ(x) represents the uncertainty of the prediction.

C. Active Learning Algorithm

The ability of GP learners in quantifying prediction un-
certainty enables a suitable application for active learning.
Basically, three sets are maintained during the PAL process,
including a set of Pareto-optimal designs (P ), non-Pareto-
optimal designs (N ) and ‘unclassiﬁed’ designs (U ).

The GP models with discrepant prior are applied to learn
the objective functions farea(x), fpower(x), fdelay(x). PAL

calls GP inference to predict the mean vector m(x) and the
standard deviation vector σ(x) of all unsampled x in the
design space based on Equation (14). Unlike other regression
models such as linear regression and support vector regres-
sion, whose outputs are in form of numerical or categorical
result, the output of GP is a distribution where uncertainties
are involved. To capture the prediction uncertainty for a design
x, a hyper-rectangle is deﬁned as

HR(x) = {y : mi(x) − β

2 σi(x) ≤ yi ≤ mi(x) + β

2 σi(x)},

1

1

where i ∈ {1, 2, 3}, corresponding to area, power and delay
metrics in physical space. β is a user-deﬁned parameter
which determines the impact of σi(x) on the region. In our
implementation, β is set to 16 based on the analysis in [33],
[35].

As shown in Fig. 10, the PAL algorithm is an iterative
process. A few new points are selected in each iteration, and
the GP model is retrained with new training set. Note that
the model is supposed to be more and more accurate as more
data being sampled. Therefore, the uncertainty region should
be smaller and smaller. In order to ensure the non-increasing
monotonicity of the uncertainty region while sampling and
incorporating the previous evaluations, the uncertainty region
of x in the (t + 1)-th iteration is deﬁned as

Rt+1(x) = Rt(x) ∩ HR(x),
(15)
where the initial R0 = Rn which is the entire objective space.
The numbers of designs in Pareto-optimal set P and
non-Pareto-optimal set N are non decreasing as iteration t
increments. Thus, at iteration t, the points in P and N keep
their classiﬁcation. Intuitively, if one wants to compare the
predicted performance of two designs, two extreme cases,
i.e., optimistic prediction min(Rt(x)) and the pessimistic
prediction max(Rt(x)) of each design, can be applied. If
the optimistic prediction of design x is dominated by the
pessimistic prediction of other design x(cid:48), then x is classiﬁed
as non-Pareto-optimal; And if the pessimistic prediction of
design x is not dominated by optimistic prediction of any
other design x(cid:48), then x is classiﬁed as Pareto-optimal; A
design will remain unclassiﬁed if neither condition holds.
Fig. 11 is presented here as an example.

In the implementation, an error tolerance δ with value 0.001
is applied during classiﬁcation. The rules for classiﬁcation can
be represented as follows.

P,

if max(Rt(x)) ≤ min(Rt(x(cid:48))) + δ,
if max(Rt(x(cid:48))) ≤ min(Rt(x)) + δ,

(16)






x ∈

N,
U, otherwise.

After classiﬁcation in each iteration, a new adder design
with the largest length of the diagonal of its uncertainty region
R(x) is selected for sampling. The value is attached to x as

wt(x) = max

y,y(cid:48)∈Rt(x)

||y − y(cid:48)||2.

(17)

Intuitively, Equation (17) picks the points which are most
worthy exploring. Afterwards, these designs are going through
EDA ﬂow to get the real area, power and delay numbers, and

Fig. 11: An example of classiﬁcation.

the GP model will hence be improved with those feedback
results.

Algorithm 1 Active Learning for Pareto-frontier Exploration

Require: Adder architectural design space E, GP prior, max-

imum iteration number Tmax;
Ensure: predicted Pareto-optimal set ˆP ;
1: P ← ∅, N ← ∅, U ← E;
2: Randomly select a small subset X = {xi} of E;
3: Get true values Y = {yi|yi = EDAFlow(xi)};
4: S ← X;
5: R0(x) ← Rn, ∀x ∈ E;
6: t ← 0;
7: while U (cid:54)= ∅ and t < Tmax do
8:
9:
10:
11:

Building GP model with {(xi, yi) : ∀xi ∈ S};
Obtain Rt(x), ∀x ∈ E;
for all x ∈ U do

if x is Pareto-optimal based on Equation (16)

P.add(x), U.delete(x);

else if x is non-Pareto-optimal based on Equa-

then

12:
13:

14:
15:

tion (16) then

end if

16:
17:
18:
19:
20:
21:
22: end while
23: ˆP ← P ;

N.add(x), U.delete(x);

end for
Obtain wt(x), ∀x ∈ (U ∪ P ) \ S;
Choose x(cid:48) ← argmax{wt(x)};
S ← S ∪ x(cid:48);
t ← t + 1;
Obtain new data (x(cid:48), y(cid:48)) by running EDA ﬂow;

The entire process is presented in Algorithm 1. It starts with
the initialization (lines 1–6). In each iteration, the GP model
is trained with the current training set S, and the uncertainty
region for each design is obtained (lines 8–9). Then the
designs in the U set are classiﬁed based on uncertainty regions
and classiﬁcation rules (lines 10–16). After that, the design
with the largest uncertainty is sampled and the sampling set
S is updated (lines 17–19). The newly sampled design is fed
into synthesis tools to get the label which is used for training
GP model in the next iteration (line 21). The learning process
stops after all adder designs in architectural design space are
classiﬁed. The prediction is ˆP = P (line 23). Suppose Tmax
is the maximum number of iterations, and |E| is the size of

solution set, then the complexity of Algorithm 1 is at most
O(Tmax|E|), as maximum size of U can be |E|. However, it
should be stressed that although there are Tmax|E| operations
for PAL algorithm, the cost of each operation (which is a
simple inference based on the Gaussian Process Regression
model) is negligible in comparison to EDA synthesis ﬂow
run-time, and we will demonstrate later in TABLE IV that
the total run-time of different approaches are dictated by the
number of EDA synthesis ﬂow runs needed in the respective
approaches.

VI. EXPERIMENTAL RESULTS

In this section we show the effectiveness of the proposed
algorithms and methodologies. First we compare the physical
solution space before/after applying PGG algorithm. Then the
Pareto frontier obtained by α-sweep is presented. Next, we
demonstrate the Pareto frontier obtained by active learning,
and compare the quality of Pareto frontiers generated by two
approaches. Finally, we compare our explored optimal adders
against legacy adders.

Since high performance adders are commonly used in CPU
architectures which are typically 64 bit, we have mainly
presented the results for 64 bit adders to demonstrate the
methodology. However, the approach is very general to be
used for adders of arbitrary bit-width. The ﬂow is imple-
mented in C++ and Python on Linux machine with 72GB
RAM and 2.8GHz CPU. We use Design Compiler [28]
(version F-2011.09-SP3) for logical synthesis, and IC Com-
piler [29] (version J-2014.09-SP5-3) for the placement and
routing. "tt1p05v125c" corner and Non Linear Delay
Model (NLDM) in 32nm SAED cell-library for LVT class
[30] (available by University Program) is used for technology
mapping. Primary input activity of 0.1 is used along with
1GHz operating frequency for power estimation. Regarding
the tool settings, target delays of 0.1ns, 0.2ns, 0.3ns and
0.4ns are used. Utilization values are set to 0.5, 0.6, 0.7 and
0.8. We used Python based machine learning package scikit-
learn [36] for the predictions. Throughout our all experiments,
the run time for machine learning predictions is less than a
minute.

We relied more on the ﬁdelity of the SAED library rather
than accuracy considering that SAED library may not be
very realistic as that used in industry. For instance, the FO4
delay for a unit sized inverter for this library in the operating
corner is 36ps [20], [37]. So 11 FO4 delay, typically being
presented to be the delay for 64-bit adders in literatures [38], is
approximately 400ps which is close to the reported delays for
64-bit adders in our work. To further demonstrate the ﬁdelity
of this library, we run the Kogge-Stone adders with bit-widths
of 8, 16, 32, 64, 128 and 256 through the synthesis ﬂow
using this library. Then we normalize the measured delay in
terms of FO4 delay, and plot it with bit-width (n) as shown
in Fig. 12. It can be seen that the delay is linear with log2 n,
which is expected for a logarithmic tree adder such as Kogge-
Stone adder. So we believe if this algorithmic methodology
is applied to more realistic industrial libraries, it can show
similar beneﬁt as demonstrated with SAED 32nm library.

y
a
l
e
D

16

12

8

4

0

8

64 128 256

32

16
Adder bit-width

Fig. 12: Delay values (× FO4 delay) of Kogge-Stone adders
with various bit-width.

(a)

(b)

Fig. 13: (a) Pareto Frontier: area vs. delay; (b) Pareto Frontier:
power vs. delay.

To validate the optimality and the hypervolume error of
the two learning approaches against the real world solution
space, we need to run the logical/physical EDA ﬂow on a
large set of adder solutions. Our machine and tool set takes
about 5.5 minutes to complete this full ﬂow of a single
preﬁx adder. Therefore, we select a reasonable number (3000)
of preﬁx adder solutions, which eventually took about 300
hours to complete, but still a comparatively larger data set
in comparison to our training data set. Crucially, those 3000
adders are also sampled in a Quasi-random manner in order
to represent the entire solution space.

A. Pareto Frontier Predicted by α-sweep Learning

In

this

learning

100 , 50, 1

experiment, we

show the
approach. We

15
20 , 10, 1
50 , 20, 1

apply
different α values
8 , 2, 1

effectiveness
the
of
our α-sweep
of
α-sweep method with
(1000, 0, 100, 1
and
collect the best 150 solutions for delay-area and delay-power
spaces where for each α value, the best 10 architectures with
lowest P D or AD values are fed into the logical/physical
EDA ﬂow to generate similar Pareto points. Note that
15 + 15 = 30 learning models have been derived for this for
all, but it is very fast as the same training data have been
used, and the models are regression based.

10 , 8, 1

2 , 1),

Fig. 13(a) and Fig. 13(b) respectively show the correspond-
ing Pareto frontiers of the α-sweep approach and the ground
truth Pareto frontiers for the 3000 representative adders. Each
dot
in the delay-area or delay-power space indicates one
adder solution after going through the logical/physical EDA
ﬂow. We can see that generally the predicted Pareto frontier
solutions are fairly close to the real Pareto frontier, with some
exceptions. Overall, the proposed approach can effectively

TABLE II: Comparison of different model accuracies

Model

Original
Noisy

Area
0.003
0.024

MSE
Power
0.027
0.951

Delay
0.170
0.711

Area-Delay
0.139
0.168

Hypervolume error
Power-Delay
0.122
0.148

Area-Power-Delay
0.154
0.162

TABLE III: Pareto frontiers for PAL vs. α-sweep [39]

Objective Hypervolume error

PAL

α-sweep [39]

in predicting Pareto frontier in all design spaces, including
area-delay, power-delay, and area-power-delay spaces.

Area-Delay

Power-Delay

Area-Power-Delay

average
best
average
best
average
best

0.100
0.044
0.109
0.075
0.056
0.039

0.139
0.093
0.122
0.076
0.154
0.125

Notes: All hypervolume error above are collected from 1000
repeated experiments.

achieve near optimal Pareto frontier without affording to
spend expensive runtime on every adder. So this learning
based methodology can be readily adopted to achieve Pareto
frontiers for much larger solution space which is intractable
for exhaustive exploration by conventional design ﬂow.

We have conducted additional experiments to show the
impacts of the low accuracy of the machine learning model.
The basic idea is to inject random noise in the prediction stage,
i.e., additional Gaussian noise is added into the predicted
value. The accuracy will be lower than original results. Then
we explore the Pareto frontier based on the noisy prediction.
Generally, the quality of the ﬁnal Pareto frontier is worse than
original model. The comparison of Pareto frontier quality is
presented in TABLE II.

B. Comparison of the Quality of Pareto-Frontier between PAL
and α-sweep

We implement PAL to predict Pareto-optimal designs in
both two-dimensional design spaces which are area-delay
space and power-delay space, as well as three-dimensional
space which is area-power-delay space. The results are com-
pared with those of [39]. The initial input set for both area-
delay and power-delay is of size 250, which are randomly se-
lected from the exhaustive design space. The curves of Pareto
frontiers for two-dimensional spaces are shown in Fig. 13.
The hypervolume of area-delay Pareto frontiers are calculated
with reference point (max(delay), max(area)). Similarly, The
hypervolume of power-delay Pareto frontiers are calculated
with reference point (max(delay), max(power)). Note that the
unit for delay is nanosecond (ns) when calculating the hyper-
volume. It should be stressed that there is a sort of randomness
in both α-sweep and PAL algorithm. For α-sweep, the training
set is selected randomly. On the contrary, the initial set in PAL
is randomly selected (line 2), thereby may result in different
outputs. So the experiments are conducted for 1000 times such
that the general performance is reﬂected. The comparison
between two approaches are shown in TABLE III. Comparing
the hypervolume error of Pareto frontier obtained by PAL and
α-sweep, it can be seen that PAL achieves better performance

C. Runtime Comparisons among Exhaustive Approach, α-
sweep and PAL

There are three factors that will affect the runtime: (i) the
total number of EDA synthesis runs required; (ii) Among all
these required EDA synthesis runs how many of them can
be parallelized; (3) The runtime of the training process in
machine learning model. All these details are recorded in
TABLE IV. The ‘INIT’ represents the set of training data
in the α-sweep and the initial set in PAL, which can be par-
allelized because all the points are obtained in advance. The
‘AS’ represents the set of designs which are actively sampled
during the learning process, which cannot be parallelized. The
α-sweep approach does not involve active sampling, so the
‘AS’ set is none here. The ‘VERI’ represents the set of designs
which are predicted to be Pareto-optimal. We should run EDA
synthesis ﬂow to get the real PPA values of these designs to
extract the Pareto-frontier. This set of designs are obtained
after the learning process stops, so the EDA synthesis runs
on these designs are also conducted ofﬂine, which can be
parallelized. Each EDA synthesis run takes about 5.5 minutes.
Then we can compare the total runtime of different ex-
ploration methodologies. For exhaustive exploration, all the
preﬁx adders should be fed into EDA tools for synthesiz-
ing to obtain the value of each metric, which is extremely
time-consuming. There is no training, additional sampling,
veriﬁcation. The total runtime cost involves EDA ﬂows of all
the designs in the design space. The Pareto frontier can be
extracted from the results, whose runtime is much less than
synthesizing and can be neglected. The total runtime is

Texh =

5.5 × #INIT
#Machines

.

(18)

Since the entire solution space is so huge that one can hardly
run all of them, in our experiment, we sample representative
10K designs by random sampling. The total runtime of
synthesizing is about 55000 minutes with single machine. It
should be noted that the entire solution space is much more
than 10K.

In the exploration by α-sweep, not all adders in the design

space are needed for synthesizing. The total runtime is

Tα =

5.5 × (#INIT + #VERI)
#Machines

+ Modeling time.

(19)

In our experiment, we select 2500 of the designs out of those
10K designs by random sampling to build the model, includ-
ing training and testing phases. It takes about 1.5 minutes
to build the model and make predictions. When exploring in
area-power-delay design space, 150 designs on average in the

TABLE IV: Comparison of runtime with single machine among different approaches

Method

#INIT

#AS

#VERI

#Total

Exhaustive
α-sweep
PAL

10000
2500
700

-
-
10

-
150
290

10000
2650
1000

Runtime (mins)
Modeling

EDA

55000.0
14575.0
5500.0

-
20.0
2.0

Total

55000.0
14595.0
5502.0

Notes: The designs in “#INIT” and “#VERI” can be synthesized in parallel. The number of designs in each category is collected from
1000 repeated experiments.

α-sweep
PAL

)
s
n
i
m

3
0
1
×

(

e
m

i
t
n
u
R

15

10

5

0

1 2 3 4 5 6 7 8 9 10
Number of machines

Fig. 14: Comparison of runtime with different number of
machines.

design space are predicted to be Pareto-optimal. So on average
2650 designs are needed. The runtime for synthesizing is
14575 minutes. Note that in terms of learning models, the
α-sweep method needs to build 15 × 15 = 225 models since
α1 and α2 both have 15 values to choose from.

Similarly, the runtime of PAL can be calculated by

TP AL =

5.5 × (#INIT + #VERI)
#Machines

+ 5.5 × #AS

+ Modeling time.

(20)

The size of initial set is ﬁxed, which is 700. It takes about 4
minutes to build the model and make predictions during the
PAL process. When exploring the Pareto-optimal designs in
area-power-delay space, 10 designs on average are sampled
during PAL. 290 designs on average in the design space are
predicted to be Pareto-optimal. In total, 1000 designs are
needed on average. The runtime is 5500 minutes with single
machine. PAL algorithm needs to build N models where N
is the number of iterations in PAL. In our implementation, the
maximum iteration is set to 20. It can be observed that the
active learning approach outperforms the α-sweep learning in
terms of both the quality of Pareto frontier and the number
of EDA ﬂow runs.

Note that all the runtime calculations are based on single
machine. However, the EDA synthesis runs in all three ﬂows
can be distributed to multiple machines if available, except the
adders sampled during active learning, which (10 on average
in our experiments) is very less in comparison to the total
number of the synthesis runs. So PAL can get a signiﬁcant
speedup over α-sweep and exhaustive approach with single
machine and multiple machines.

D. Comparison on Different Sampling Strategies in PAL

In the sampling stage of PAL, the number of instances to
be sampled has impact on the runtime since the EDA ﬂow is
required to obtain the real value for area, power and delay.
The less instances we sample in each iteration, the more
iterations are needed to ensure the PAL process converge,
which is more likely to result in less samples in total. The
more instances we sample in each iteration, the less iterations
are needed. However, the total number of sampled instances
would be large. In practice, the runtime cost of running EDA
ﬂow can be reduced by parallel execution if there are multiple
licenses available. In this section, we explore the effect of
different sampling strategies in terms of the total runtime and
the quality of Pareto frontier in practical scenarios.

The results for different sampling strategies are listed in
Fig. 15. Since the EDA ﬂow for synthesis, placement and
routing takes up the most signiﬁcant part of the total runtime
cost, the key factor is the number of adders which needs to
be through EDA tool ﬂow. If we have multiple machines
available for the EDA tool ﬂow, the runtime is determined
by the total number of iterations as long as the number of
samples does not exceed the number of machines. From the
result, it can be seen that we can obtain the Pareto-frontier
with comparable quality, using less runtime.

Note that when the sample size increases from 1 to 5,
the average hypervolume error increases from 0.056 to about
0.070, which is still less than 0.154 (average hypervolume
error achieved in α-sweep approach). Therefore, batch sam-
pling can not only take care of parallel synthesizing but also
achieve better quality for Pareto frontier than α-sweep, which
can also show the advantages of the PAL.

E. Adder Performance Comparison

Finally, we compare our explored adders against Design-
Ware adders, legacy adders, such as Kogge-Stone, Sklansky,
as well as a state-of-the-art adder synthesis algorithm in TA-
BLE V. Since our approach generates numerous solutions, it
is not feasible to perform a one-to-one comparison. Instead for
each of the solution points in regular adders and [19], we have
picked the Pareto points from our solution set which are able
to excel them in all metrics. For instance, P1 could provide
around 8ps better delay with respectively 14% and 12% lesser
area and power over Kogge-Stone adder. The DesignWare
adders are synthesized from behavioral description of adder
(Y = A + B) with the 16 conﬁgurations of tool settings
(Combination of 4 target delay and 4 utilization values) that
are used in generating the Fig. 4. We pick the one with best

s
n
o
i
t
a
r
e
t
i
#

e
g
a
r
e
v
A

10
8
6
4
2
0

s
r
e
d
d
a
#

e
g
a
r
e
v
A

1,000

990
980
970
960
950

r
o
r
r
e

v
h

e
g
a
r
e
v
A

0.080

0.060

0.040

0.020

0.000

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

(a)

(b)

(c)

Fig. 15: Comparison among different number of samples per iteration.

TABLE V: Comparison with other approaches for 64 bit
adders

Method
DesignWare
Ours (P1)
Kogge-Stone
Ours (P1)
Sklansky
Ours (P2)
[19]
Ours (P3)

Delay (ps)
346.5
339.0
347.9
339.0
356.1
353.0
348.7
343.0

Area (µm2)
2531.3
2180.8
2563.7
2180.8
1792.5
1753.0
1971.4
1912.6

Energy (f J/op)
8160
6930
8780
6930
6100
5900
6980
6390

delay, denoted by “DesignWare” in TABLE V. The same
pareto point P1 dominates that solution by providing around
7.5ps better delay, 14% lesser area, and 15% lesser energy.
For [19] we pick the best delay solution. Note for a ﬁxed
mf o, [19] can give preﬁx network with smaller size, but
this approach only provides a limited set of preﬁx structures.
As a result, it is hard for [19] to explore the full physical
design space of adders by machine learning. It should be
stressed that [19] beats the custom adders implemented in
an industrial design, and our methodology is able to excel the
adders generated by the algorithm presented in [19].

VII. CONCLUSION

This paper presents a novel methodology of machine
learning guided design space exploration for power efﬁcient
high-performance preﬁx adders. We have successfully demon-
strated the effectiveness of our learning models, developed
by training with quasi-random sampled data and features
encapsulating architectural and tool attributes. In addition,
an active learning approach is applied to ease the demand
of labeled data and achieves even better Pareto frontier.
Our adder synthesis algorithm is able to generate a wider
solution space in comparison to a state-of-the-art algorithm,
and when integrated with the learning model, could provide
a remarkable performance vs. power vs. area Pareto frontier
over a large representative solution space. To the best of our
knowledge, this is the ﬁrst work to bridge the gap between
architectural and physical solution space for parallel preﬁx
adders.

REFERENCES

[1] B. Yu, D. Z. Pan, T. Matsunawa, and X. Zeng, “Machine learning and
pattern matching in physical design,” in Proc. ASPDAC, 2015, pp. 286–
293.

[2] W.-T. J. Chan, K. Y. Chung, A. B. Kahng, N. D. MacDonald, and
S. Nath, “Learning-based prediction of embedded memory timing
failures during initial ﬂoorplan design,” in Proc. ASPDAC, 2016, pp.
178–185.

[3] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[4] W.-H. Chang, L.-D. Chen, C.-H. Lin, S.-P. Mu, M. C.-T. Chao, C.-
H. Tsai, and Y.-C. Chiu, “Generating routing-driven power distribution
networks with machine-learning technique,” in Proc. ISPD, 2016, pp.
145–152.

[5] R. Samanta, J. Hu, and P. Li, “Discrete buffer and wire sizing for link-
based non-tree clock networks,” in Proc. ISPD, 2008, pp. 175–181.
[6] R. P. Brent and H. T. Kung, “A regular layout for parallel adders,” IEEE
Transactions on Computers, vol. C-31, no. 3, pp. 260–264, 1982.
[7] P. M. Kogge and H. S. Stone, “A parallel algorithm for the efﬁcient
solution of a general class of recurrence equations,” IEEE Transactions
on Computers, vol. 100, no. 8, pp. 786–793, 1973.

[8] T. Han and D. Carlson, “Fast area-efﬁcient VLSI adders,” Proc. ARITH,

pp. 49–56, 1987.

[9] J. Sklansky, “Conditional sum addition logic,” IRE Trans. on Electronic

Computers, vol. EC-9, no. 2, pp. 226–231, 1960.

[10] C. Zhou, B. M. Fleischer, M. Gschwind, and R. Puri, “64-bit preﬁx
adders: Power-efﬁcient topologies and design solutions,” Proc. CICC,
pp. 179–182, 2009.

[11] J. Liu, Y. Zhu, H. Zhu, C.-K. Cheng, and J. Lillis, “Optimum preﬁx
adders in a comprehensive area, timing and power design space,” in
Proc. ASPDAC, 2007, pp. 609–615.

[12] N. H. Weste and D. Harris, CMOS VLSI Design: A Circuits and Systems

Perspective. Addison Wesley, 2004.

[13] T. Matsunaga and Y. Matsunaga, “Area minimization algorithm for par-
allel preﬁx adders under bitwise delay constraints,” in Proc. GLSVLSI,
2007, pp. 435–440.

[14] J. Liu, S. Zhou, H. Zhu, and C.-K. Cheng, “An algorithmic approach
for generic parallel adders,” in Proc. ICCAD, 2003, pp. 734–740.
[15] J. P. Fishburn, “A depth-decreasing heuristic for combinational logic;
or how to convert a ripple-carry adder into a carry-lookahead adder or
anything in-between,” in Proc. DAC, 1990, pp. 361–364.

[16] R. Zimmermann, “Non-heuristic optimization and synthesis of parallel

preﬁx adders,” Proc. IWLAS, pp. 123–132, 1996.

[17] M. Snir, “Depth-size trade-offs for parallel preﬁx computation,” Journal

of Algorithms, vol. 7, no. 2, pp. 185–201, 1986.

[18] H. Zhu, C.-K. Cheng, and R. Graham, “Constructing zero-deﬁciency
parallel preﬁx adder of minimum depth,” in Proc. ASPDAC, 2005, pp.
883–888.

[19] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” in Proc. ASPDAC, 2015, pp. 249–254.

[20] ——, “Towards optimal performance-area trade-off in adders by syn-
thesis of parallel preﬁx structures,” IEEE TCAD, vol. 33, no. 10, pp.
1517–1530, 2014.

[21] M. Choudhury, R. Puri, S. Roy, and S. C. Sundararajan, “Automated
synthesis of high performance two operand binary parallel preﬁx adder,”
Patent US 8,683,398, Mar, 2014.

[22] G. Palermo, C. Silvano, and V. Zaccaria, “ReSPIR: a response surface-
based pareto iterative reﬁnement for application-speciﬁc design space
exploration,” IEEE TCAD, vol. 28, no. 12, pp. 1816–1829, 2009.
[23] H.-Y. Liu and L. P. Carloni, “On learning-based methods for design-
space exploration with high-level synthesis,” in Proc. DAC, 2013, pp.
50:1–50:7.

[24] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[25] B. R. Zeydel, T. T. J. H. Kluter, and V. G. Oklobdzija, “Efﬁcient
mapping of addition recurrence algorithms in CMOS,” Proc. ARITH,
pp. 107–113, 2005.

[26] M. Ketter, D. M. Harris, A. Macrae, R. Glick, M. Ong, and J. Schauer,
“Implementation of 32-bit Ling and Jackson adders,” Proc. Asilomar,
pp. 170–175, 2011.

[27] E. Zitzler, D. Brockhoff, and L. Thiele, “The hypervolume indicator
revisited: On the design of pareto-compliant indicators via weighted
integration,” in Evolutionary multi-criterion optimization, 2007, pp.
862–876.

[28] “Synopsys Design Compiler,” http://www.synopsys.com.
[29] “Synopsys IC Compiler,” http://www.synopsys.com.
[30] “Synopsys SAED Library,” http://www.synopsys.com/Community/
UniversityProgram/Pages/32-28nm-generic-library.aspx, accessed 23-
April-2016.

[31] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-
tions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.

[32] K. Tumer and J. Ghosh, “Estimating the bayes error rate through
classiﬁer combining,” in Proc. ICPR, vol. 2, 1996, pp. 695–699.
[33] M. Zuluaga, A. Krause, G. Sergent, and M. P¨uschel, “Active learning
for multi-objective optimization,” in Proc. ICML, 2013, pp. 462–470.
[34] C. E. Rasmussen and C. K. I. Williams, Gaussian Process for Machine

Learning. The MIT Press, 2006.

[35] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Gaussian process
optimization in the bandit setting: no regret and experimental design,”
in Proc. ICML, 2010, pp. 1015–1022.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in Python,” Journal of Machine Learn-
ing Research, vol. 12, no. Oct., pp. 2825–2830, 2011.

[37] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” IEEE TCAD, vol. 35, no. 5, pp. 820–831, 2016.

[38] T. McAuley, W. Koven, A. Carter, P. Ning, and D. M. Harris, “Imple-
mentation of a 64-bit Jackson adders,” Proc. Asilomar, pp. 1149–1154,
2013.

[39] S. Roy, Y. Ma, J. Miao, and B. Yu, “A learning bridge from architec-
tural synthesis to physical design for exploring power efﬁcient high-
performance adders,” in Proc. ISLPED, 2017, pp. 1–6.

Cross-layer Optimization for High Speed Adders:
A Pareto Driven Machine Learning Approach

Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, and Bei Yu

8
1
0
2
 
t
c
O
 
6
1
 
 
]

R
A
.
s
c
[
 
 
2
v
3
2
0
7
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—In spite of maturity to the modern electronic design
automation (EDA) tools, optimized designs at architectural stage
may become sub-optimal after going through physical design
ﬂow. Adder design has been such a long studied fundamental
problem in VLSI industry yet designers cannot achieve optimal
solutions by running EDA tools on the set of available preﬁx
adder architectures. In this paper, we enhance a state-of-the-
art preﬁx adder synthesis algorithm to obtain a much wider
solution space in architectural domain. On top of
that, a
machine learning-based design space exploration methodology
the adders in
is applied to predict
physical domain, which is infeasible by exhaustively running
EDA tools for innumerable architectural solutions. Considering
the high cost of obtaining the true values for learning, an active
learning algorithm is proposed to select the representative data
during learning process, which uses less labeled data while
achieving better quality of Pareto frontier. Experimental results
demonstrate that our framework can achieve Pareto frontier of
high quality over a wide design space, bridging the gap between
architectural and physical designs. Source code and data are
available at https://github.com/yuzhe630/adder-DSE.

the Pareto frontier of

I. INTRODUCTION

I N the last decades, the industrial EDA tools have advanced

towards optimality, especially at the individual stages of
VLSI design cycle. Nevertheless, with growing design com-
plexity and aggressive technology scaling, physical design
issues have become more and more complex. As a result,
the constraints and the objectives of higher layers, such as
the system or logic level, are very difﬁcult to be mapped
into those of lower layers, such as physical design, and vice-
versa, thereby creating a gap between the optimality at the
logic stage and the physical design stage. This necessitates
the innovation of data-driven methodologies, such as machine
learning [1]–[5], to bridge this gap.

Adder design is one of the fundamental problems in digital
semiconductor industry, and its main bottleneck (in terms
of both delay and area) is the carry-propagation unit. This
unit can be realized by hundreds of thousands of parallel
preﬁx structures, but it is hard to evaluate the ﬁnal metrics
without running through physical design tools. Historically,
regular adders [6]–[9] have been proposed for achieving the
corner points in terms of various metrics as shown in Fig. 1

The preliminary version has been presented at the IEEE International
Symposium on Low Power Electronics and Design (ISLPED) in 2017. This
work is supported in part by The Research Grants Council of Hong Kong SAR
(Project No. CUHK24209017) and CUHK Undergraduate Summer Research
Internship 2017.

Y. Ma, J. Chen and B. Yu are with the Department of Computer Science

and Engineering, The Chinese University of Hong Kong, NT, Hong Kong.

S. Roy is with Intel Corporation, San Jose, CA, USA.
J. Miao is with the Cadence Design Systems, San Jose, CA, USA.

Fig. 1: Regular adders (picture taken from [12]).

in architectural stage. The main motivation for structural
regularity was the ease of manual layout, but EDA tools now
taking care of all physical design aspects, the regularity is
no longer essential. Moreover, the extreme corners do not
map well to the physical design metrics after synthesis, place-
ment and routing. To address this gap between preﬁx adder
synthesis and actual physical design of the adders, custom
adders are typically designed by tuning parameters, such as
gate-sizing, buffering etc., targeting at the optimization of
power/performance metrics for a speciﬁc technology library
[10], [11]. However, this custom approach (i) needs signiﬁcant
engineering effort, (ii) is not ﬂexible to Engineering Change
Order (ECO), and (iii) does not guarantee the optimality.

The algorithmic synthesis approach resolves the ﬁrst two
issues of the custom approach, by adding more ﬂexibility to
the late ECO changes and reducing the engineering effort.
Based on the number of solutions, the existing adder synthesis
algorithms can be broadly classiﬁed into two categories. The
ﬁrst and the most common approach is to generate a single
preﬁx network for a set of structural constraints, such as the
logic level, fan-out etc. Several algorithms have been proposed
to minimize the size of the preﬁx graph (s) under given bit-
width (n) and logic-level (L) constraints [13]–[16]. Closed
form theoretical bounds for size-optimality are provided by
[17] for L ≥ 2 log2 n − 2. [18] has given more general bound
for preﬁx graph size, but when L is reduced to log2 n, a pre-
requisite for high-performance adders, there is no closed form
bound for s. [19] presents a polynomial-time algorithm for

generating preﬁx graph structures by restricting both logic-
level and fan-out. The limitations in these approaches are
two-fold, (i) this restricted set of structures is not capable
of exploring the large solution space, and (ii) since it is very
hard to analytically model the physical design complexities,
such as wire-length and congestion issues, the physical design
metrics, such as the area, power, delay etc., may not be
mapped well to the preﬁx structure metrics, such as the size,
max-fan-out (mf o) etc. This motivates the second category
of algorithms where thousands of preﬁx adder solutions can
be generated and explored for synthesis and physical design
in the commercial EDA tools.

One such approach is [20] which presents an exhaus-
tive bottom-up enumeration technique with several pruning
strategies to generate innumerable preﬁx structure solutions.
However, it has two issues, (i) this approach cannot provide
solutions in several cases for restricted fan-out, which can
control the congestion and load-distribution during physical
design [19]. As a result, it may still miss the good solution
space to a large extent, and (ii) it is computationally very
intensive to run all solutions through synthesis, placement and
routing.

In this paper, we enhance the algorithm in [20], [21] to
generate adders under any arbitrary mf o constraint, which
enables a wider adder solution space in logical form. To
tackle the high computational effort during the physical design
ﬂow, we further propose to use machine learning to perform
the design space exploration in physical solution space. We
develop Pareto frontier driven machine learning methodolo-
gies to achieve rich adder solutions with trade-offs among
power, area, and delay. As a passive supervised learning, the
proposed quasi-random sampling approach is able to select
representative preﬁx adders out of the hundreds of thousands
of preﬁx structures.

It should be noted that various machine learning algorithms
have been investigated to explore design space in different
design scenarios. Palermo et al. [22] deploy both linear
regression and artiﬁcial neural network for multiprocessor
systems-on-chip design. Lin et al. [23] present a random
forest-based learning model in high level synthesis, which
can ﬁnd an approximate Pareto-optimal designs effectively.
Meng et al. [24] propose a random forest-based method for
Pareto frontier exploration, where non-Pareto-optimal designs
are carefully eliminated through an adaptive strategy. Multiple
predictions can be obtained through random forest, which can
be used for estimating the uncertainty. Superior to the random
forest, in this paper we further propose an active learning
approach based on Gaussian Process (GP), which by nature
can estimate the prediction uncertainty efﬁciently.

Our main contributions are summarized as follows:
• A comprehensive framework for optimal adder search
by machine learning methodology bridging the preﬁx
architecture synthesis to the ﬁnal physical design;

• An enhancement to a state-of-the-art preﬁx adder algo-
rithm [20] to optimize the preﬁx graph size for restricted
fan-out and explore a wider solution space;

• A machine learning model for preﬁx adders, guided by
quasi-random data sampling with features considering

architectural attributes and EDA tool settings;

• A design space exploration method to generate the Pareto
frontier for delay vs. power/area over a wide design
space;

• An active learning approach for the design space explo-
ration, which uses less labeled data and achieves better
quality of Pareto frontier.

The rest of the paper is organized as follows. Section II
presents the background of preﬁx adder synthesis, while
Section III discusses our preﬁx graph generation algorithm.
Next, two machine learning approaches of design space explo-
ration for high-performance adders are described. Section IV
presents the passive supervised learning, while Section V
introduces a Pareto frontier driven active learning approach.
Section VI lists the experimental results, followed by conclu-
sion in Section VII.

II. PREFIX ADDER SYNTHESIS

In this section, we ﬁrst provide the background of the preﬁx
adder synthesis problem. Then we present a brief discussion
on the algorithm presented in [20], which we enhance to our
Preﬁx Graph Generation (PGG) algorithm to synthesize the
preﬁx adder network.

A. Preliminaries

An n bit adder accepts two n bit addends A = an−1..a1a0
and B = bn−1..b1b0 as input, and computes the output
sum S = sn−1..s1s0 and carry out Cout = cn−1, where
si = ai ⊕ bi ⊕ ci−1 and ci = aibi + aici−1 + bici−1. The
simplest realization for the adder network is the ripple-carry-
adder, but with logic level n − 1, which is too slow. For faster
implementation, carry-lookahead principle is used to compute
the carry bits. Mathematically, this can be represented with
bitwise (group) generate function g (G) and propagate func-
tion p (P ) by the Weinberger’s recurrence equations as follows
[25]:

• Pre-processing (inputs): Bitwise generation of g, p

gi = ai · bi and pi = ai ⊕ bi.

(1)

• Preﬁx processing: This part is the main carry-propagation
component where the concept of generate/propagate is
extended to multiple bits and G[i:j], P[i:j] (i ≥ j) are
deﬁned as

(cid:26) pi,

(cid:26) gi,

P[i:j] =

G[i:j] =

if i = j,

P[i:k] · P[k−1:j], otherwise,

G[i:k] + P[i:k] · G[k−1:j], otherwise.

if i = j,

(2)

(3)

The associative operation ◦ is deﬁned for (G, P ) as:

(G, P )[i:j] = (G, P )[i:k] ◦ (G, P )[k−1:j]

= (G[i:k] + P[i:k] · G[k−1:j], P[i:k] · P[k−1:j]).
(4)

• Post-processing (outputs): Sum/Carry-out generation

si = pi ⊕ ci−1,

ci = G[i:0], and Cout = cn−1.

(5)

The ‘Preﬁx processing’ or carry propagation network can be
mapped to a preﬁx graph problem with inputs ik = (pk, gk)
and outputs ok = ck, such that ok depends on all previous
inputs ij (j ≤ k). Any node except the input nodes is called
a preﬁx node. Size of the preﬁx graph is deﬁned as the number
of preﬁx nodes in the graph. Fig. 2 shows an example of such
preﬁx graph of 6 bit and we can see that Cout = c5 = o5 is
given by

o5 = (i5 ◦ i4) ◦ ((i3 ◦ i2) ◦ (i1 ◦ i0)).

(6)

Size (s), logic level (L) and maximum-fan-out (mf o) for
this network are respectively 8, 3 and 2. Note that here the
number of fan-ins for each of the associative operation o is
two, thus this is called radix-2 implementation of the preﬁx
graph. However, there exist other options such as radix-3 or
radix-4, but the complexity is very high and not beneﬁcial in
static CMOS circuits [26]. In this work, the logic levels for
all output bits are log2 n, i.e., the minimum possible, to target
high performance adders.

B. Discussion on [20]

Our PGG algorithm to generate the preﬁx graph structures
for physical solution space exploration is based on [20]. So
it is imperative to ﬁrst discuss about [20]. However, we omit
the details and only mention the key points of [20] due to
space constraint.

[20] is an exhaustive bottom-up and pruning based enu-
meration technique for preﬁx adder synthesis. This work
presented an algorithm to generate all possible n + 1 bit
preﬁx graph structures from any n bit preﬁx graph. Then
this algorithm is employed in a bottom-up fashion (from 1
bit adder to 2 bit adders, then from all 2 bit adders to 3
bit adders, and so on) to synthesize preﬁx graphs of any
bit-width. As a result, scalability issue arises due to the
exhaustive nature of the algorithm, which is then tackled by
adopting various pruning strategies to scale the approach.
However, the pruning strategies are not sufﬁcient to scale
the algorithm well for different fan-out constraints. So when
it intends to ﬁnd the solutions for higher bit adders, the
intermediate adder solutions that need to be generated are
often huge. Consequently, it fails to get fan-out restricted
(e.g. when mf o = 8, 10, 12 etc. for 64 bit adders) solutions
even with 72GB RAM due to the generation of innumerable
intermediate solutions [19]. Pruning strategies, such as size-
bucketing [20], help to achieve solutions in some cases, but
with sub-optimality. So design space-exploration based on
this algorithm can miss a signiﬁcant spectrum of the adder
solutions.

C. Our PGG Algorithm

To better explore the wide design space of adders, in this
paper we have enhanced [20] for different fan-out constraints
by incorporating more pruning techniques.

Fig. 2: 6 bit preﬁx adder
network.

Fig. 3: Imposing semi-regularity.

1) Semi-Regularity in Preﬁx Graph Structure: The ﬁrst
strategy is to enforce a sort of regularity in the preﬁx graphs.
For instance, regular adders, such as Sklansky, Brent-Kung,
have the inherent property that the consecutive input nodes
(even and odd) are combined to create the preﬁx nodes at the
logic level 1. In our approach, we constrain this regularity
for those preﬁx nodes (logic level 1). To explain this, let
us consider Fig. 3. We can see that preﬁx nodes r1, r2, r3
and r4 are constructed by consecutive even-odd nodes. For
instance, i0 and i1 are used to construct r1. But with this
structural constraint, we are not allowed to construct any node
by combining i1 and i2 as done in Kogge-Stone adders. Note
that the sub-structure, as shown in Fig. 3, is a part of some
regular adders like Sklansky adder, and is imposed in our
preﬁx structure enumeration.

We have run experiments with 16 bit adders, and observed
that this pruning strategy (i) does not degrade the solution
quality (or size of the preﬁx graph under same L and mf o),
but (ii) able to reduce the search space signiﬁcantly,
in
comparison to not using this pruning strategy.

2) Level Restriction in Non-trivial Fan-in: Each of the
preﬁx node N (a:b), where a is the most-signiﬁcant-bit (MSB)
and b is the least-signiﬁcant-bit (LSB),
is constructed by
connecting the trivial fan-in Ntr (a:c) having same MSB as
N , and the non-trivial fan-in Nnon−tr (c − 1:b). For instance,
in Fig. 2, o3 and b2 are respectively the non-trivial fan-in
and the trivial fan-in node for the preﬁx node o5. In the
bottom-up enumeration technique, we put another additional
restriction that the level of the trivial fan-in node is always
i.e.,
less or equal
level(Ntr) ≤ level(Nnon−tr). Note that this sort of structural
restriction is also inherent in regular adders, such as Sklansky
or Brent-Kung adders.

to that of the non-trivial fan-in node,

In a nutshell, our PGG algorithm is a blend of regular
adders and [20]. We borrow some properties of regular adders
to enforce in [20] for reducing its huge search space without
hampering the solution quality. To illustrate this, we have
obtained the binary for [20] from the authors, and ﬁrst
compared our result for lower bit adders, such as n = 16, 32.
We got the solutions with same minimum size, which proves
that our structural constraints have not degraded the solution
quality. However, for higher bit adders, we get better solution
quality than [20] as shown in TABLE I.

Column 1 presents the mf o constraint, while columns
2 and 3 respectively show the size and run-time for our
enhanced algorithm, and the corresponding entries for [20]
are respectively represented in columns 4 and 5. In general,

TABLE I: Comparison with [20] for 64 bit adders

mf o

4
6
8
12
16
32

Our Approach

Approach in [20]

size
244
233
222
201
191
185

Run-time (s)
302
264
423
193
73
0.04

size
252
238
-
-
192
185

Run-time (s)
241
212
-
-
149
0.04

when fan-out is relaxed or mf o is higher, the run-time is less
due to relaxed size-pruning as explained in [20]. Note that [20]
cannot generate solutions for mf o = 8, 12 due to generation
of innumerable intermediate solutions as explained in [19]. On
the contrary, our structural constraints can do a pre-ﬁltering
of the potentially futile solutions, thereby allowing relaxed
size-pruning and size-bucketing to search for more effective
solution space. In terms of run-time, it is slightly worse in
a few cases, but importantly, this generation is a one-time
process, and this run-time is negligible in comparison to the
design space exploration by the physical design tools. So our
imposed structural restrictions (i) do not degrade the solution
quality, (ii) achieve better solution sizes for all mf o than [20]
for higher bit adders which could not even generate solutions
in all cases, and (iii) help to obtain wider physical solution
space to be demonstrated in Section II-E.

D. Quasi-random Sampling

We have mainly focused on 64 bit adders in this work as
this is mostly used in today’s microprocessors. From all preﬁx
adder solutions, we sample a set of solutions for building
the learning model via the quasi-random approach which
is conducted by a two-level binning (mf o, s) followed by
random selection, This approach aims to evenly sample the
preﬁx adders covering different architectural bins. The pri-
mary level of binning is determined by mf o of the solutions.
However, there may be thousands of architectures sharing the
same mf o, so the secondary level of binning is based on s.
Afterwards, adders are picked randomly from those secondary
bins.

We illustrate the quasi-random sampling with the following
example: given 5000 solutions with mf o = 4, we want to
pick 50 solutions from them. Suppose these 5000 solutions
have the size distribution from 244 to 258. First a random
solution is picked from the bucket of the solutions (mf o = 4,
s = 244). Then we pick a solution randomly from (mf o = 4,
s = 245), and so on. After picking 15 solutions from each of
those buckets with mf o = 4, we again start from the bucket
(mf o = 4, s = 244). This process is repeated until we get 50
solutions. Similar procedure is done with other mf o values.

E. Physical Solution Space Comparison with [20]

In this subsection, we show the usefulness of our algorithm
for obtaining wider solution space in physical design domain
in comparison to [20]. Among the preﬁx adders generated by
[20], we randomly sampled 7000 preﬁx adders. Those preﬁx
adders are fed into the full EDA ﬂow (synthesis, placement
and routing) to get their real delay, power and area values

(a)

(c)

(b)

(d)

Fig. 4: Quasi-random sampled adders vs. adders from [20].
(a) Solution space in area vs. delay domain from [20]; (b)
Solution space in area vs. delay domain from ours; (c)
Solution space in power vs. delay domain from [20]; (d)
Solution space in power vs. delay domain from ours.

(takes around 700 hours). We plot these adders by [20] and
our representative 3000 adders in Fig. 4. It can be seen that,
although the numbers of adders by [20] is more than 2 times of
our representative adders, our adders still cover wider solution
space in physical domain, demonstrating the effectiveness of
our enhanced algorithm PGG. This is in accordance with
the solutions missed by [20] as mentioned in TABLE I.
Those availabilities eventually offer more opportunities for
our machine learning methodology to identify close to ground
truth Pareto frontier solutions.

III. BRIDGING ARCHITECTURAL SOLUTION SPACE TO
PHYSICAL SOLUTION SPACE

In most EDA problems, the metrics of the solution quality
are typically conﬂicting. For instance, if we optimize the tim-
ing of the design, then the power/area may be compromised
and vice versa. So one imperative job of EDA engineers
is to ﬁnd the Pareto-optimal points of the design enabling
the designers to select among those. In this section, we ﬁrst
provide the preliminaries about Pareto optimality, and the
error metrics of Pareto optimal solutions. Then we discuss
the gap between the preﬁx architectural solution space and
physical solution space in adders, which motivates the need of
the machine learning-based approach for optimal adder explo-
ration. Finally, a domain knowledge-based feature selection
details are presented along with training data sampling for
the learning models.

architecture stage are preﬁx node size s and max fan-out mf o.
These two metrics are conﬂicting, i.e., if we reduce mf o,
s increases and vice-versa. Similar competing relationship
exists between delay and power/area after physical design. It
should be stressed that power and s are correlated, and mf o
indirectly controls the timing as more restricted fan-out can
mitigate congestion and load-distribution, thereby improving
the delay of the adder. However, this relationship between
architectural synthesis and physical design is approximate,
and not a very high-ﬁdelity one.

To demonstrate this, we plot node size s vs. mf o and power
vs. delay in Fig. 6 for several 64 bit adder solutions. In this
experiment, we have generated the preﬁx architecture solu-
tions by PGG, and the ﬁnal power/delay numbers are obtained
by running those solutions through EDA tools as explained
later in Section VI. An example of the preﬁx architecture and
the corresponding physical solution is presented in Fig. 7. In
Fig. 6(a), we broadly categorize the solutions into 2 groups,
(i) G1 with higher node size and lower mf o, and (ii) G2
with lower node size and higher mf o. In Fig. 6(b), the same
designs as Fig. 6(a) are projected into the physical solution
space, restoring the group information. Design Compiler [28]
(version F-2011.09-SP3) is used for logical synthesis, and
IC Compiler [29] (version J-2014.09-SP5-3) is used for the
placement and routing. Non linear delay model (NLDM) in
32nm SAED cell-library [30] is used for technology mapping.
The key observations here are ﬁrstly, there is a correlation
between architectural solution space and physical design
solution space. For instance, the solutions from G1 are mostly
on the upper side, and those of G2 are mostly on the lower
side in Fig. 6(b), thereby indicating a correspondence between
s and power. Nevertheless, it is not completely reliable. For
example, (i) the delay numbers for G1 and G2 are very much
spread, (ii) a cluster can be observed where the solutions
from G1 and G2 are mixed up in Fig. 6(b), and (iii) several
solutions of G1 are better than several solutions of G2 in
power, which is not in accordance with the metrics at the
preﬁx adder architecture stage. So we can not utterly rely on
architectural solution space to achieve the optimal output in
physical solution space.

However, since our algorithm generates hundreds of thou-
sands of preﬁx graph structures, it is intractable to run synthe-
sis and physical design ﬂows for even a small percentage of
all available preﬁx adder architectures. To address this ﬁdelity
gap between the two design stages and the high computational
cost together, we come up with a novel machine learning
guided design space exploration as replacement of exhaustive
search.

C. Feature Selection

The feature is a representation which is extracted from the
original input representation, and it plays an important role
in machine learning tasks. We now discuss the features to be
used for the learning model. Features are considered from both
preﬁx adder structure and tool settings, with a focus on the
former. We select node size and maximum-fan-out (mf o) of
a preﬁx adder as two main features for our learning model.

Fig. 5: Hypervolume with two objectives in objective space.

(a)

(b)

Fig. 6: Gap between preﬁx structure and physical design of
adders: (a) Architectural solution space; (b) Physical solution
space.

A. Preliminaries
Deﬁnition 1 (Pareto Optimality). An objective vector f (x) is
said to dominate f (x(cid:48)) if:

∀i ∈ [1, n], fi(x) ≤ fi(x(cid:48))
and ∃j ∈ [1, n], fj(x) < fj(x(cid:48)).
A point x is Pareto-optimal if there is no other x(cid:48) in design

(7)

space such that f (x(cid:48)) dominates f (x).

As in this paper for adder design, a Pareto-optimal design
is where none of the objective metrics, such as area, power
or delay, can be improved without worsening at least one of
the others. The Pareto Frontier is the set of all the Pareto-
optimal designs in the objective space. Therefore, the goal is
to identify the Pareto-optimal set P for all the Pareto-optimal
designs.

Deﬁnition 2 (Hypervolume). The hypervolume computes the
volume enclosed by the Pareto frontier and the reference point
in the objective space [27].

In Fig. 5, the shaded area is an example of the hypervolume
of a Pareto set with two objectives. Then the hypervolume
error for a predicted Pareto set ˆP is deﬁned as

η =

V (P ) − V ( ˆP )
V (P )

,

(8)

where P is the true Pareto-optimal set, and V (P ) is the
hypervolume of the Pareto set P . Note that a prediction ˆP
which contains the whole design space has an error of 0.
Thus the predicted set ˆP with less points is desired.

B. Gap Between Logic and Physical Design

Since we focus on high performance adders and explore
the preﬁx adders of logic level L = log2 n, the metrics at this

(a)

(b)

Fig. 7: (a) An example of architectural solution: Bit-width = 64, size = 201, Max. level
= 6, Max. fanout = 12; (b) Corresponding physical solution.

Fig. 8: Deﬁning spf o of a
node.

However, for any given mf o and node size, there will be
hundreds or even thousands of different preﬁx architectures.
Therefore, additional features are required to better distinguish
individual preﬁx adder attributes. We deﬁne a parameter sum-
path-fan-out (spf o) for this. Let a and b are the fan-in nodes
of a node n, then spf o(n) is deﬁned recursively as:

spf o(n) =

0,
sum(f o(a) + spf o(a),

if n ∈ input,

(9)

f o(b) + spf o(b)), otherwise.






Here f o(n) denotes the fan-out of any node n. Consider the
preﬁx adder structure in Fig. 8, and according to the deﬁnition
we have:

spf o(o1) = sum(f o(i0) + spf o(i0), f o(i1) + spf o(i1))

spf o(b1) = sum(f o(i2) + spf o(i2), f o(i3) + spf o(i3))

spf o(b2) = sum(f o(i4) + spf o(i4), f o(i5) + spf o(i5))

= sum(1, 1) = 2,

= sum(2, 1) = 3,

= sum(2, 1) = 3.

Therefore, we can use the recursive deﬁnition to calculate

spf o(o3) = sum(f o(o1) + spf o(o1), f o(b1) + spf o(b1))

= sum(3 + 2, 2 + 3) = 10,

spf o(o5) = sum(f o(o3) + spf o(o3), f o(b2) + spf o(b2))

= sum(3 + 10, 3 + 3) = 19.

In our methodology, we use the spf o of the output nodes
which are at log2 n level (there are 32 nodes at level 6 for 64
bit adder) as the features to characterize the preﬁx structures,
in addition to mf o, size and target delay. The basic intuition
for selecting spf o of the output nodes as the features is that
the critical path delay of the adder is the longest path delay
from input to output. So it depends on the (i) path-lengths,
which can be represented at the preﬁx graph stage by the
logic level of the node, and (ii) the number of fan-outs driven
at every node on the path. Note that we have skipped the
spf o of the output nodes which are not at log2 n level as for
those nodes, the path length is smaller, and those would not
potentially dictate the critical path delay.

Apart from these preﬁx graph structural features, we also
consider tool settings from synthesis stage and physical design
stage as other features. We have synthesized the adder struc-
tures using industry-standard EDA synthesis tool [28], where

we can specify the target-delay for the adder. The tool then
adopts different strategies internally to meet that target-delay
which we can hardly take into account during preﬁx graph
synthesis. Consequently, changing the target-delay can lead
to different power/timing/area metrics. So we have considered
target-delay as a feature in our learning approach.

In physical design, utilization is an important parameter,
which deﬁnes the area occupied by standard cell, macros and
blockages. Different utilization values can lead to different
layouts after physical design. Therefore, we take utilization
as another feature in the learning model.

In addition to the target delay and utilization, other tool
settings have also been explored. The optimization level
setting in logical synthesis has a potential impact on the per-
formance of adders, which can be adjusted by compile and
compile_ultra commands with different options. After
synthesizing, it is observed that the solutions generated with
compile_ultra can signiﬁcantly dominate the solutions
generated by compile. Therefore, this setting is ﬁxed to
compile_ultra level as we are aiming at superior designs.
In this work, the technology node is not used as a feature.
From the machine learning perspective, there is a common
assumption for conventional machine learning applications
that
the training and test data are drawn from the same
feature space and the same distribution [31]. The values of
area/power/delay may vary a lot under different technology
nodes, which results in different underlying data distributions.
Therefore, the technology node for synthesis should be con-
sistent. The proposed approach for feature extraction can also
be applied to other technology nodes as long as the technology
node is consistent during the design ﬂow. If the technology
node of the testing data switches to another one, the machine
learning model should be re-trained using the data from that
technology node to ensure the accuracy of the model.

D. Data Sampling

Since we can not afford to run the physical design ﬂow
for too many architectures, and too few training data may
degrade the model accuracy signiﬁcantly, a set of adders need
to be selected to represent the entire design solution space.
However, ﬁnding a succinct set of representative training data
for the traditional supervised learning is difﬁcult. In order
to tackle this difﬁculty, we come up with two learning ap-
proaches in the next two sections. The ﬁrst one is the passive

for delay with only primary features. Best model ﬁtting for
delay is achieved with SVR (RBF kernel) with these 4 primary
and 32 secondary features. Since SVR with RBF kernel give
good MSE (mean-squared-error) scores for all metrics, delay,
area and power, we have used this model throughout for
design space exploration.

The model experiments give us the following key insights:
(i) tool setting can play an important role in building the
learning models in EDA. For instance, MSE scores for area
and power improve from 0.021 to 0.003, and 0.228 to 0.027
respectively when we add the ‘target delay’ feature in our
model building, (ii) secondary features play an important
role in improving the model accuracy. For instance, when
we include spf o features in model building, MSE score for
delay improves from 0.200 to 0.170. (iii) linear models are
not sufﬁcient for modeling delay. For instance, MSE scores of
delay improve from 0.214 to 0.170 when we go from linear
models to SVR with RBF kernel, with the same set of features.
The problem of exploring the Pareto frontier of rich preﬁx
adder space can be approached by ﬁrst sampling a subset
of preﬁx adder architectures, and generating the power, area,
delay numbers of each preﬁx adder by running through the
logic synthesis and physical design ﬂow. Those known data
set will be used as the training and testing data for supervised
machine learning guided model ﬁtting. Once the model is
ﬁtted, we can apply the exhaustive preﬁx adder architectures
to this model and get the predicted Pareto frontier solution
set. This is due to the merit of much faster runtime for a
machine learning model in prediction stage than running the
entire VLSI CAD ﬂow.

However, conventional machine learning problem aims at
maximizing the prediction accuracy rather than exploring a
Pareto frontier out of a solution set. Improving the model
accuracy does not necessarily improve the Pareto frontier and
the direct use of the ﬁtted model for Pareto frontier exploration
can even miss up to 60% Pareto frontier points [3]. We
therefore need a machine learning integrated Pareto frontier
exploration methodology, where the Pareto frontier selection
does not rely only on the model accuracy. So we develop a fast
yet effective algorithmic methodology, enabled by regression
model to explore the Pareto frontier of preﬁx adder solutions.
First we consider two spaces for Pareto frontier exploration:
the delay vs. area as well as the delay vs. power. For either
space, there exists a strong trade-off between the two metrics.
For delay vs. power space, we propose to use a joint output
Power-Delay function (P D) as the regression output rather
than using any single output.

P D = α · P ower + Delay.

(10)

The rationale of using scalarization [32] or the linear
summation of the power and delay metrics is that such a linear
relation provides a weighted bonding between the power and
the delay so that by changing the α value, the regression
model will try to minimize the prediction error on the more
weighted axis hence leads to more accuracy on that direction.
In contrast, the other metric direction will be predicted with
less accuracy hence introducing some level of relaxations. It
can be foreseen that changing the α value can lead to different

Fig. 9: Overall ﬂow of α-sweep learning.

supervised learning where a quasi-random data sampling is
performed to obtain the training data, followed by multi-
objective scalarization to achieve the Pareto optimal solutions.
The second one is the active learning approach where model
training is integrated to ﬁnding Pareto-optimal frontiers of the
design space.

IV. α-SWEEP LEARNING
In this section we propose a pareto-frontier exploration
ﬂow which is based on support vector machine. The overall
ﬂow of our α-sweep supervised learning-based Pareto-frontier
exploration is presented in Fig. 9.

A. Scalarization to the Single-Objective

In this work, supervised learning is preferred over unsu-
pervised learning since supervised learning has a substantial
advantage over unsupervised learning for our problem. In
particular, supervised learning allows to take advantage of
the golden result, i.e., the true area/power/delay, generated
by the synthesis tools for each design, instead of just letting
the algorithm work out for itself what the classes should be.
In general, supervised learning usually outperforms the unsu-
pervised learning for this kind of regression and classiﬁcation
tasks.

Before applying machine learning for exploring Pareto
frontier, we ﬁrst validate the effectiveness of the features we
extract by building regression models for single metric predic-
tion. For learning models, we explored (i) several supervised
learning techniques, such as linear regression, Lasso/Ridge,
Bayesian ridge model and support vector regression (SVR)
with linear, polynomial and radial-basis-function (RBF) ker-
nel, and (ii) 36 features, including 4 primary features, size,
mf o,
target delay and utilization (tool settings), and 32
secondary features for spf o. We observed that we could get
an R2 score above 0.95 for area and power even with primary
features and linear models. However, we don’t get good scores

ﬁtting accuracies of the regression model. By sweeping α
over a wide range from 0 to large positive values, each time
the regression model will be ﬁtted to predict different best
solutions which altogether form the Pareto frontier. We call
this approach α-sweep. Note that, the P ower and Delay
values in Equation (10) are normalized and scaled to the range
between 0 and 1 by Equation (11).
x − min(X)
max(X) − min(X)

, x ∈ X.

(11)

x =

Similarly, we have a joint output Area-Delay (AD) function

for Pareto frontier exploration on Area and Delay space.

AD = α · Area + Delay.

(12)

This α-sweep technique can be extended to simultaneously
consider power, performance or delay, and area (PPA), using
two scalars (α1 and α2) instead of one scalar factor α. The
joint output function for Pareto frontier exploration on area –
power – delay space can be formulated as:

P P A = α1 · Area + α2 · P ower + Delay.
The results of α-sweep for both two-dimensional space and

(13)

three-dimensional space are shown in the Section VI.

V. PARETO ACTIVE LEARNING
In our adder design problem, obtaining the true area/pow-
er/delay values or the labeled data for each adder requires
running logic synthesis and physical design ﬂow, which is
often time-consuming if the amount of data is huge. Active
learning is an iterative supervised learning which is able to
interactively query the data pool to obtain the desired outputs
at new data points. Since the samples are selected by the
learning algorithm, the number of samples to ﬁt a model can
often be much lower than the number required in traditional
supervised learning. Since an active sampling strategy is
required in active learning, an “uncertainty estimation” of
the prediction is needed. Gaussian Process (GP) can make
predictions and, more importantly, provide the uncertainty
estimation of its predictions by nature. Therefore, in this paper
we further propose a Pareto active learning algorithm based
on Gaussian Process regression.

A. Overall Flow

The overall ﬂow of the Pareto active learning (PAL) is
shown in Fig. 10. Given all the preﬁx adder structures, ﬁrst
we extract the feature vector for each adder as introduced
in Section III-C. The active learning starts with Gaussian
Process regression which will be illustrated later. Unlike the
passive supervised learning in which all
the features and
the corresponding labels are prepared in advance, the active
learning derives the labels of each training data during the
learning process on-demand. To be speciﬁc, the algorithm in-
crementally identiﬁes the most representative instances along
with their features which are later fed into EDA synthesis ﬂow
(synthesis, placement and routing) for true area/power/delay
numbers. Namely, the EDA synthesis ﬂow and the learning
process are interleaving. As more and more designs being
selected, the model gets more and more accurate till conver-
gence.

Fig. 10: Overall ﬂow of Pareto active learning.

B. Gaussian Process Prediction

A Gaussian process is speciﬁed by its mean function and
covariance function. A Pareto active learning scheme based
on Gaussian process regression is proposed in [33]. The prior
information is important to train the Gaussian Process model,
which is a parameterized mean and covariance functions.
Conventionally, the training process selects the parameters in
the light of training data such that the marginal likelihood
is maximized. Then the Gaussian Process model can be
obtained and the regression can be proceeded with supervised
input [34]. The ability of GP indicating prediction uncer-
tainty reﬂects in GP learner providing a Gaussian distribution
N (m(x), σ(x)) of the values predicted for any test input x
by computing
m(x) = k(x, X)(cid:62)(k(X, X) + σ2I)−1Y,
σ2(x) = k(x, x) − k(x, X)(cid:62)(k(X, X) + σ2I)−1k(x, X),

(14)
where X is the training set, Y is the supervised information
of trained set X. For Gaussian Process regression, a prediction
of a design objective consists of a mean and a variance.
The mean value m(x) represents the predicted value and the
variance σ(x) represents the uncertainty of the prediction.

C. Active Learning Algorithm

The ability of GP learners in quantifying prediction un-
certainty enables a suitable application for active learning.
Basically, three sets are maintained during the PAL process,
including a set of Pareto-optimal designs (P ), non-Pareto-
optimal designs (N ) and ‘unclassiﬁed’ designs (U ).

The GP models with discrepant prior are applied to learn
the objective functions farea(x), fpower(x), fdelay(x). PAL

calls GP inference to predict the mean vector m(x) and the
standard deviation vector σ(x) of all unsampled x in the
design space based on Equation (14). Unlike other regression
models such as linear regression and support vector regres-
sion, whose outputs are in form of numerical or categorical
result, the output of GP is a distribution where uncertainties
are involved. To capture the prediction uncertainty for a design
x, a hyper-rectangle is deﬁned as

HR(x) = {y : mi(x) − β

2 σi(x) ≤ yi ≤ mi(x) + β

2 σi(x)},

1

1

where i ∈ {1, 2, 3}, corresponding to area, power and delay
metrics in physical space. β is a user-deﬁned parameter
which determines the impact of σi(x) on the region. In our
implementation, β is set to 16 based on the analysis in [33],
[35].

As shown in Fig. 10, the PAL algorithm is an iterative
process. A few new points are selected in each iteration, and
the GP model is retrained with new training set. Note that
the model is supposed to be more and more accurate as more
data being sampled. Therefore, the uncertainty region should
be smaller and smaller. In order to ensure the non-increasing
monotonicity of the uncertainty region while sampling and
incorporating the previous evaluations, the uncertainty region
of x in the (t + 1)-th iteration is deﬁned as

Rt+1(x) = Rt(x) ∩ HR(x),
(15)
where the initial R0 = Rn which is the entire objective space.
The numbers of designs in Pareto-optimal set P and
non-Pareto-optimal set N are non decreasing as iteration t
increments. Thus, at iteration t, the points in P and N keep
their classiﬁcation. Intuitively, if one wants to compare the
predicted performance of two designs, two extreme cases,
i.e., optimistic prediction min(Rt(x)) and the pessimistic
prediction max(Rt(x)) of each design, can be applied. If
the optimistic prediction of design x is dominated by the
pessimistic prediction of other design x(cid:48), then x is classiﬁed
as non-Pareto-optimal; And if the pessimistic prediction of
design x is not dominated by optimistic prediction of any
other design x(cid:48), then x is classiﬁed as Pareto-optimal; A
design will remain unclassiﬁed if neither condition holds.
Fig. 11 is presented here as an example.

In the implementation, an error tolerance δ with value 0.001
is applied during classiﬁcation. The rules for classiﬁcation can
be represented as follows.

P,

if max(Rt(x)) ≤ min(Rt(x(cid:48))) + δ,
if max(Rt(x(cid:48))) ≤ min(Rt(x)) + δ,

(16)






x ∈

N,
U, otherwise.

After classiﬁcation in each iteration, a new adder design
with the largest length of the diagonal of its uncertainty region
R(x) is selected for sampling. The value is attached to x as

wt(x) = max

y,y(cid:48)∈Rt(x)

||y − y(cid:48)||2.

(17)

Intuitively, Equation (17) picks the points which are most
worthy exploring. Afterwards, these designs are going through
EDA ﬂow to get the real area, power and delay numbers, and

Fig. 11: An example of classiﬁcation.

the GP model will hence be improved with those feedback
results.

Algorithm 1 Active Learning for Pareto-frontier Exploration

Require: Adder architectural design space E, GP prior, max-

imum iteration number Tmax;
Ensure: predicted Pareto-optimal set ˆP ;
1: P ← ∅, N ← ∅, U ← E;
2: Randomly select a small subset X = {xi} of E;
3: Get true values Y = {yi|yi = EDAFlow(xi)};
4: S ← X;
5: R0(x) ← Rn, ∀x ∈ E;
6: t ← 0;
7: while U (cid:54)= ∅ and t < Tmax do
8:
9:
10:
11:

Building GP model with {(xi, yi) : ∀xi ∈ S};
Obtain Rt(x), ∀x ∈ E;
for all x ∈ U do

if x is Pareto-optimal based on Equation (16)

P.add(x), U.delete(x);

else if x is non-Pareto-optimal based on Equa-

then

12:
13:

14:
15:

tion (16) then

end if

16:
17:
18:
19:
20:
21:
22: end while
23: ˆP ← P ;

N.add(x), U.delete(x);

end for
Obtain wt(x), ∀x ∈ (U ∪ P ) \ S;
Choose x(cid:48) ← argmax{wt(x)};
S ← S ∪ x(cid:48);
t ← t + 1;
Obtain new data (x(cid:48), y(cid:48)) by running EDA ﬂow;

The entire process is presented in Algorithm 1. It starts with
the initialization (lines 1–6). In each iteration, the GP model
is trained with the current training set S, and the uncertainty
region for each design is obtained (lines 8–9). Then the
designs in the U set are classiﬁed based on uncertainty regions
and classiﬁcation rules (lines 10–16). After that, the design
with the largest uncertainty is sampled and the sampling set
S is updated (lines 17–19). The newly sampled design is fed
into synthesis tools to get the label which is used for training
GP model in the next iteration (line 21). The learning process
stops after all adder designs in architectural design space are
classiﬁed. The prediction is ˆP = P (line 23). Suppose Tmax
is the maximum number of iterations, and |E| is the size of

solution set, then the complexity of Algorithm 1 is at most
O(Tmax|E|), as maximum size of U can be |E|. However, it
should be stressed that although there are Tmax|E| operations
for PAL algorithm, the cost of each operation (which is a
simple inference based on the Gaussian Process Regression
model) is negligible in comparison to EDA synthesis ﬂow
run-time, and we will demonstrate later in TABLE IV that
the total run-time of different approaches are dictated by the
number of EDA synthesis ﬂow runs needed in the respective
approaches.

VI. EXPERIMENTAL RESULTS

In this section we show the effectiveness of the proposed
algorithms and methodologies. First we compare the physical
solution space before/after applying PGG algorithm. Then the
Pareto frontier obtained by α-sweep is presented. Next, we
demonstrate the Pareto frontier obtained by active learning,
and compare the quality of Pareto frontiers generated by two
approaches. Finally, we compare our explored optimal adders
against legacy adders.

Since high performance adders are commonly used in CPU
architectures which are typically 64 bit, we have mainly
presented the results for 64 bit adders to demonstrate the
methodology. However, the approach is very general to be
used for adders of arbitrary bit-width. The ﬂow is imple-
mented in C++ and Python on Linux machine with 72GB
RAM and 2.8GHz CPU. We use Design Compiler [28]
(version F-2011.09-SP3) for logical synthesis, and IC Com-
piler [29] (version J-2014.09-SP5-3) for the placement and
routing. "tt1p05v125c" corner and Non Linear Delay
Model (NLDM) in 32nm SAED cell-library for LVT class
[30] (available by University Program) is used for technology
mapping. Primary input activity of 0.1 is used along with
1GHz operating frequency for power estimation. Regarding
the tool settings, target delays of 0.1ns, 0.2ns, 0.3ns and
0.4ns are used. Utilization values are set to 0.5, 0.6, 0.7 and
0.8. We used Python based machine learning package scikit-
learn [36] for the predictions. Throughout our all experiments,
the run time for machine learning predictions is less than a
minute.

We relied more on the ﬁdelity of the SAED library rather
than accuracy considering that SAED library may not be
very realistic as that used in industry. For instance, the FO4
delay for a unit sized inverter for this library in the operating
corner is 36ps [20], [37]. So 11 FO4 delay, typically being
presented to be the delay for 64-bit adders in literatures [38], is
approximately 400ps which is close to the reported delays for
64-bit adders in our work. To further demonstrate the ﬁdelity
of this library, we run the Kogge-Stone adders with bit-widths
of 8, 16, 32, 64, 128 and 256 through the synthesis ﬂow
using this library. Then we normalize the measured delay in
terms of FO4 delay, and plot it with bit-width (n) as shown
in Fig. 12. It can be seen that the delay is linear with log2 n,
which is expected for a logarithmic tree adder such as Kogge-
Stone adder. So we believe if this algorithmic methodology
is applied to more realistic industrial libraries, it can show
similar beneﬁt as demonstrated with SAED 32nm library.

y
a
l
e
D

16

12

8

4

0

8

64 128 256

32

16
Adder bit-width

Fig. 12: Delay values (× FO4 delay) of Kogge-Stone adders
with various bit-width.

(a)

(b)

Fig. 13: (a) Pareto Frontier: area vs. delay; (b) Pareto Frontier:
power vs. delay.

To validate the optimality and the hypervolume error of
the two learning approaches against the real world solution
space, we need to run the logical/physical EDA ﬂow on a
large set of adder solutions. Our machine and tool set takes
about 5.5 minutes to complete this full ﬂow of a single
preﬁx adder. Therefore, we select a reasonable number (3000)
of preﬁx adder solutions, which eventually took about 300
hours to complete, but still a comparatively larger data set
in comparison to our training data set. Crucially, those 3000
adders are also sampled in a Quasi-random manner in order
to represent the entire solution space.

A. Pareto Frontier Predicted by α-sweep Learning

In

this

learning

100 , 50, 1

experiment, we

show the
approach. We

15
20 , 10, 1
50 , 20, 1

apply
different α values
8 , 2, 1

effectiveness
the
of
our α-sweep
of
α-sweep method with
(1000, 0, 100, 1
and
collect the best 150 solutions for delay-area and delay-power
spaces where for each α value, the best 10 architectures with
lowest P D or AD values are fed into the logical/physical
EDA ﬂow to generate similar Pareto points. Note that
15 + 15 = 30 learning models have been derived for this for
all, but it is very fast as the same training data have been
used, and the models are regression based.

10 , 8, 1

2 , 1),

Fig. 13(a) and Fig. 13(b) respectively show the correspond-
ing Pareto frontiers of the α-sweep approach and the ground
truth Pareto frontiers for the 3000 representative adders. Each
dot
in the delay-area or delay-power space indicates one
adder solution after going through the logical/physical EDA
ﬂow. We can see that generally the predicted Pareto frontier
solutions are fairly close to the real Pareto frontier, with some
exceptions. Overall, the proposed approach can effectively

TABLE II: Comparison of different model accuracies

Model

Original
Noisy

Area
0.003
0.024

MSE
Power
0.027
0.951

Delay
0.170
0.711

Area-Delay
0.139
0.168

Hypervolume error
Power-Delay
0.122
0.148

Area-Power-Delay
0.154
0.162

TABLE III: Pareto frontiers for PAL vs. α-sweep [39]

Objective Hypervolume error

PAL

α-sweep [39]

in predicting Pareto frontier in all design spaces, including
area-delay, power-delay, and area-power-delay spaces.

Area-Delay

Power-Delay

Area-Power-Delay

average
best
average
best
average
best

0.100
0.044
0.109
0.075
0.056
0.039

0.139
0.093
0.122
0.076
0.154
0.125

Notes: All hypervolume error above are collected from 1000
repeated experiments.

achieve near optimal Pareto frontier without affording to
spend expensive runtime on every adder. So this learning
based methodology can be readily adopted to achieve Pareto
frontiers for much larger solution space which is intractable
for exhaustive exploration by conventional design ﬂow.

We have conducted additional experiments to show the
impacts of the low accuracy of the machine learning model.
The basic idea is to inject random noise in the prediction stage,
i.e., additional Gaussian noise is added into the predicted
value. The accuracy will be lower than original results. Then
we explore the Pareto frontier based on the noisy prediction.
Generally, the quality of the ﬁnal Pareto frontier is worse than
original model. The comparison of Pareto frontier quality is
presented in TABLE II.

B. Comparison of the Quality of Pareto-Frontier between PAL
and α-sweep

We implement PAL to predict Pareto-optimal designs in
both two-dimensional design spaces which are area-delay
space and power-delay space, as well as three-dimensional
space which is area-power-delay space. The results are com-
pared with those of [39]. The initial input set for both area-
delay and power-delay is of size 250, which are randomly se-
lected from the exhaustive design space. The curves of Pareto
frontiers for two-dimensional spaces are shown in Fig. 13.
The hypervolume of area-delay Pareto frontiers are calculated
with reference point (max(delay), max(area)). Similarly, The
hypervolume of power-delay Pareto frontiers are calculated
with reference point (max(delay), max(power)). Note that the
unit for delay is nanosecond (ns) when calculating the hyper-
volume. It should be stressed that there is a sort of randomness
in both α-sweep and PAL algorithm. For α-sweep, the training
set is selected randomly. On the contrary, the initial set in PAL
is randomly selected (line 2), thereby may result in different
outputs. So the experiments are conducted for 1000 times such
that the general performance is reﬂected. The comparison
between two approaches are shown in TABLE III. Comparing
the hypervolume error of Pareto frontier obtained by PAL and
α-sweep, it can be seen that PAL achieves better performance

C. Runtime Comparisons among Exhaustive Approach, α-
sweep and PAL

There are three factors that will affect the runtime: (i) the
total number of EDA synthesis runs required; (ii) Among all
these required EDA synthesis runs how many of them can
be parallelized; (3) The runtime of the training process in
machine learning model. All these details are recorded in
TABLE IV. The ‘INIT’ represents the set of training data
in the α-sweep and the initial set in PAL, which can be par-
allelized because all the points are obtained in advance. The
‘AS’ represents the set of designs which are actively sampled
during the learning process, which cannot be parallelized. The
α-sweep approach does not involve active sampling, so the
‘AS’ set is none here. The ‘VERI’ represents the set of designs
which are predicted to be Pareto-optimal. We should run EDA
synthesis ﬂow to get the real PPA values of these designs to
extract the Pareto-frontier. This set of designs are obtained
after the learning process stops, so the EDA synthesis runs
on these designs are also conducted ofﬂine, which can be
parallelized. Each EDA synthesis run takes about 5.5 minutes.
Then we can compare the total runtime of different ex-
ploration methodologies. For exhaustive exploration, all the
preﬁx adders should be fed into EDA tools for synthesiz-
ing to obtain the value of each metric, which is extremely
time-consuming. There is no training, additional sampling,
veriﬁcation. The total runtime cost involves EDA ﬂows of all
the designs in the design space. The Pareto frontier can be
extracted from the results, whose runtime is much less than
synthesizing and can be neglected. The total runtime is

Texh =

5.5 × #INIT
#Machines

.

(18)

Since the entire solution space is so huge that one can hardly
run all of them, in our experiment, we sample representative
10K designs by random sampling. The total runtime of
synthesizing is about 55000 minutes with single machine. It
should be noted that the entire solution space is much more
than 10K.

In the exploration by α-sweep, not all adders in the design

space are needed for synthesizing. The total runtime is

Tα =

5.5 × (#INIT + #VERI)
#Machines

+ Modeling time.

(19)

In our experiment, we select 2500 of the designs out of those
10K designs by random sampling to build the model, includ-
ing training and testing phases. It takes about 1.5 minutes
to build the model and make predictions. When exploring in
area-power-delay design space, 150 designs on average in the

TABLE IV: Comparison of runtime with single machine among different approaches

Method

#INIT

#AS

#VERI

#Total

Exhaustive
α-sweep
PAL

10000
2500
700

-
-
10

-
150
290

10000
2650
1000

Runtime (mins)
Modeling

EDA

55000.0
14575.0
5500.0

-
20.0
2.0

Total

55000.0
14595.0
5502.0

Notes: The designs in “#INIT” and “#VERI” can be synthesized in parallel. The number of designs in each category is collected from
1000 repeated experiments.

α-sweep
PAL

)
s
n
i
m

3
0
1
×

(

e
m

i
t
n
u
R

15

10

5

0

1 2 3 4 5 6 7 8 9 10
Number of machines

Fig. 14: Comparison of runtime with different number of
machines.

design space are predicted to be Pareto-optimal. So on average
2650 designs are needed. The runtime for synthesizing is
14575 minutes. Note that in terms of learning models, the
α-sweep method needs to build 15 × 15 = 225 models since
α1 and α2 both have 15 values to choose from.

Similarly, the runtime of PAL can be calculated by

TP AL =

5.5 × (#INIT + #VERI)
#Machines

+ 5.5 × #AS

+ Modeling time.

(20)

The size of initial set is ﬁxed, which is 700. It takes about 4
minutes to build the model and make predictions during the
PAL process. When exploring the Pareto-optimal designs in
area-power-delay space, 10 designs on average are sampled
during PAL. 290 designs on average in the design space are
predicted to be Pareto-optimal. In total, 1000 designs are
needed on average. The runtime is 5500 minutes with single
machine. PAL algorithm needs to build N models where N
is the number of iterations in PAL. In our implementation, the
maximum iteration is set to 20. It can be observed that the
active learning approach outperforms the α-sweep learning in
terms of both the quality of Pareto frontier and the number
of EDA ﬂow runs.

Note that all the runtime calculations are based on single
machine. However, the EDA synthesis runs in all three ﬂows
can be distributed to multiple machines if available, except the
adders sampled during active learning, which (10 on average
in our experiments) is very less in comparison to the total
number of the synthesis runs. So PAL can get a signiﬁcant
speedup over α-sweep and exhaustive approach with single
machine and multiple machines.

D. Comparison on Different Sampling Strategies in PAL

In the sampling stage of PAL, the number of instances to
be sampled has impact on the runtime since the EDA ﬂow is
required to obtain the real value for area, power and delay.
The less instances we sample in each iteration, the more
iterations are needed to ensure the PAL process converge,
which is more likely to result in less samples in total. The
more instances we sample in each iteration, the less iterations
are needed. However, the total number of sampled instances
would be large. In practice, the runtime cost of running EDA
ﬂow can be reduced by parallel execution if there are multiple
licenses available. In this section, we explore the effect of
different sampling strategies in terms of the total runtime and
the quality of Pareto frontier in practical scenarios.

The results for different sampling strategies are listed in
Fig. 15. Since the EDA ﬂow for synthesis, placement and
routing takes up the most signiﬁcant part of the total runtime
cost, the key factor is the number of adders which needs to
be through EDA tool ﬂow. If we have multiple machines
available for the EDA tool ﬂow, the runtime is determined
by the total number of iterations as long as the number of
samples does not exceed the number of machines. From the
result, it can be seen that we can obtain the Pareto-frontier
with comparable quality, using less runtime.

Note that when the sample size increases from 1 to 5,
the average hypervolume error increases from 0.056 to about
0.070, which is still less than 0.154 (average hypervolume
error achieved in α-sweep approach). Therefore, batch sam-
pling can not only take care of parallel synthesizing but also
achieve better quality for Pareto frontier than α-sweep, which
can also show the advantages of the PAL.

E. Adder Performance Comparison

Finally, we compare our explored adders against Design-
Ware adders, legacy adders, such as Kogge-Stone, Sklansky,
as well as a state-of-the-art adder synthesis algorithm in TA-
BLE V. Since our approach generates numerous solutions, it
is not feasible to perform a one-to-one comparison. Instead for
each of the solution points in regular adders and [19], we have
picked the Pareto points from our solution set which are able
to excel them in all metrics. For instance, P1 could provide
around 8ps better delay with respectively 14% and 12% lesser
area and power over Kogge-Stone adder. The DesignWare
adders are synthesized from behavioral description of adder
(Y = A + B) with the 16 conﬁgurations of tool settings
(Combination of 4 target delay and 4 utilization values) that
are used in generating the Fig. 4. We pick the one with best

s
n
o
i
t
a
r
e
t
i
#

e
g
a
r
e
v
A

10
8
6
4
2
0

s
r
e
d
d
a
#

e
g
a
r
e
v
A

1,000

990
980
970
960
950

r
o
r
r
e

v
h

e
g
a
r
e
v
A

0.080

0.060

0.040

0.020

0.000

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

0

2

1

3
#Samples per iteration

4

5

(a)

(b)

(c)

Fig. 15: Comparison among different number of samples per iteration.

TABLE V: Comparison with other approaches for 64 bit
adders

Method
DesignWare
Ours (P1)
Kogge-Stone
Ours (P1)
Sklansky
Ours (P2)
[19]
Ours (P3)

Delay (ps)
346.5
339.0
347.9
339.0
356.1
353.0
348.7
343.0

Area (µm2)
2531.3
2180.8
2563.7
2180.8
1792.5
1753.0
1971.4
1912.6

Energy (f J/op)
8160
6930
8780
6930
6100
5900
6980
6390

delay, denoted by “DesignWare” in TABLE V. The same
pareto point P1 dominates that solution by providing around
7.5ps better delay, 14% lesser area, and 15% lesser energy.
For [19] we pick the best delay solution. Note for a ﬁxed
mf o, [19] can give preﬁx network with smaller size, but
this approach only provides a limited set of preﬁx structures.
As a result, it is hard for [19] to explore the full physical
design space of adders by machine learning. It should be
stressed that [19] beats the custom adders implemented in
an industrial design, and our methodology is able to excel the
adders generated by the algorithm presented in [19].

VII. CONCLUSION

This paper presents a novel methodology of machine
learning guided design space exploration for power efﬁcient
high-performance preﬁx adders. We have successfully demon-
strated the effectiveness of our learning models, developed
by training with quasi-random sampled data and features
encapsulating architectural and tool attributes. In addition,
an active learning approach is applied to ease the demand
of labeled data and achieves even better Pareto frontier.
Our adder synthesis algorithm is able to generate a wider
solution space in comparison to a state-of-the-art algorithm,
and when integrated with the learning model, could provide
a remarkable performance vs. power vs. area Pareto frontier
over a large representative solution space. To the best of our
knowledge, this is the ﬁrst work to bridge the gap between
architectural and physical solution space for parallel preﬁx
adders.

REFERENCES

[1] B. Yu, D. Z. Pan, T. Matsunawa, and X. Zeng, “Machine learning and
pattern matching in physical design,” in Proc. ASPDAC, 2015, pp. 286–
293.

[2] W.-T. J. Chan, K. Y. Chung, A. B. Kahng, N. D. MacDonald, and
S. Nath, “Learning-based prediction of embedded memory timing
failures during initial ﬂoorplan design,” in Proc. ASPDAC, 2016, pp.
178–185.

[3] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[4] W.-H. Chang, L.-D. Chen, C.-H. Lin, S.-P. Mu, M. C.-T. Chao, C.-
H. Tsai, and Y.-C. Chiu, “Generating routing-driven power distribution
networks with machine-learning technique,” in Proc. ISPD, 2016, pp.
145–152.

[5] R. Samanta, J. Hu, and P. Li, “Discrete buffer and wire sizing for link-
based non-tree clock networks,” in Proc. ISPD, 2008, pp. 175–181.
[6] R. P. Brent and H. T. Kung, “A regular layout for parallel adders,” IEEE
Transactions on Computers, vol. C-31, no. 3, pp. 260–264, 1982.
[7] P. M. Kogge and H. S. Stone, “A parallel algorithm for the efﬁcient
solution of a general class of recurrence equations,” IEEE Transactions
on Computers, vol. 100, no. 8, pp. 786–793, 1973.

[8] T. Han and D. Carlson, “Fast area-efﬁcient VLSI adders,” Proc. ARITH,

pp. 49–56, 1987.

[9] J. Sklansky, “Conditional sum addition logic,” IRE Trans. on Electronic

Computers, vol. EC-9, no. 2, pp. 226–231, 1960.

[10] C. Zhou, B. M. Fleischer, M. Gschwind, and R. Puri, “64-bit preﬁx
adders: Power-efﬁcient topologies and design solutions,” Proc. CICC,
pp. 179–182, 2009.

[11] J. Liu, Y. Zhu, H. Zhu, C.-K. Cheng, and J. Lillis, “Optimum preﬁx
adders in a comprehensive area, timing and power design space,” in
Proc. ASPDAC, 2007, pp. 609–615.

[12] N. H. Weste and D. Harris, CMOS VLSI Design: A Circuits and Systems

Perspective. Addison Wesley, 2004.

[13] T. Matsunaga and Y. Matsunaga, “Area minimization algorithm for par-
allel preﬁx adders under bitwise delay constraints,” in Proc. GLSVLSI,
2007, pp. 435–440.

[14] J. Liu, S. Zhou, H. Zhu, and C.-K. Cheng, “An algorithmic approach
for generic parallel adders,” in Proc. ICCAD, 2003, pp. 734–740.
[15] J. P. Fishburn, “A depth-decreasing heuristic for combinational logic;
or how to convert a ripple-carry adder into a carry-lookahead adder or
anything in-between,” in Proc. DAC, 1990, pp. 361–364.

[16] R. Zimmermann, “Non-heuristic optimization and synthesis of parallel

preﬁx adders,” Proc. IWLAS, pp. 123–132, 1996.

[17] M. Snir, “Depth-size trade-offs for parallel preﬁx computation,” Journal

of Algorithms, vol. 7, no. 2, pp. 185–201, 1986.

[18] H. Zhu, C.-K. Cheng, and R. Graham, “Constructing zero-deﬁciency
parallel preﬁx adder of minimum depth,” in Proc. ASPDAC, 2005, pp.
883–888.

[19] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” in Proc. ASPDAC, 2015, pp. 249–254.

[20] ——, “Towards optimal performance-area trade-off in adders by syn-
thesis of parallel preﬁx structures,” IEEE TCAD, vol. 33, no. 10, pp.
1517–1530, 2014.

[21] M. Choudhury, R. Puri, S. Roy, and S. C. Sundararajan, “Automated
synthesis of high performance two operand binary parallel preﬁx adder,”
Patent US 8,683,398, Mar, 2014.

[22] G. Palermo, C. Silvano, and V. Zaccaria, “ReSPIR: a response surface-
based pareto iterative reﬁnement for application-speciﬁc design space
exploration,” IEEE TCAD, vol. 28, no. 12, pp. 1816–1829, 2009.
[23] H.-Y. Liu and L. P. Carloni, “On learning-based methods for design-
space exploration with high-level synthesis,” in Proc. DAC, 2013, pp.
50:1–50:7.

[24] P. Meng, A. Althoff, Q. Gautier, and R. Kastner, “Adaptive threshold
non-pareto elimination: Re-thinking machine learning for system level
design space exploration on FPGAs,” in Proc. DATE, 2016, pp. 918–
923.

[25] B. R. Zeydel, T. T. J. H. Kluter, and V. G. Oklobdzija, “Efﬁcient
mapping of addition recurrence algorithms in CMOS,” Proc. ARITH,
pp. 107–113, 2005.

[26] M. Ketter, D. M. Harris, A. Macrae, R. Glick, M. Ong, and J. Schauer,
“Implementation of 32-bit Ling and Jackson adders,” Proc. Asilomar,
pp. 170–175, 2011.

[27] E. Zitzler, D. Brockhoff, and L. Thiele, “The hypervolume indicator
revisited: On the design of pareto-compliant indicators via weighted
integration,” in Evolutionary multi-criterion optimization, 2007, pp.
862–876.

[28] “Synopsys Design Compiler,” http://www.synopsys.com.
[29] “Synopsys IC Compiler,” http://www.synopsys.com.
[30] “Synopsys SAED Library,” http://www.synopsys.com/Community/
UniversityProgram/Pages/32-28nm-generic-library.aspx, accessed 23-
April-2016.

[31] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-
tions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.

[32] K. Tumer and J. Ghosh, “Estimating the bayes error rate through
classiﬁer combining,” in Proc. ICPR, vol. 2, 1996, pp. 695–699.
[33] M. Zuluaga, A. Krause, G. Sergent, and M. P¨uschel, “Active learning
for multi-objective optimization,” in Proc. ICML, 2013, pp. 462–470.
[34] C. E. Rasmussen and C. K. I. Williams, Gaussian Process for Machine

Learning. The MIT Press, 2006.

[35] N. Srinivas, A. Krause, S. Kakade, and M. Seeger, “Gaussian process
optimization in the bandit setting: no regret and experimental design,”
in Proc. ICML, 2010, pp. 1015–1022.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in Python,” Journal of Machine Learn-
ing Research, vol. 12, no. Oct., pp. 2825–2830, 2011.

[37] S. Roy, M. Choudhury, R. Puri, and D. Z. Pan, “Polynomial time algo-
rithm for area and power efﬁcient adder synthesis in high-performance
designs,” IEEE TCAD, vol. 35, no. 5, pp. 820–831, 2016.

[38] T. McAuley, W. Koven, A. Carter, P. Ning, and D. M. Harris, “Imple-
mentation of a 64-bit Jackson adders,” Proc. Asilomar, pp. 1149–1154,
2013.

[39] S. Roy, Y. Ma, J. Miao, and B. Yu, “A learning bridge from architec-
tural synthesis to physical design for exploring power efﬁcient high-
performance adders,” in Proc. ISLPED, 2017, pp. 1–6.

