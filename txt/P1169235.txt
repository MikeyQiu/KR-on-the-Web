Semi-Supervised First-Person Activity Recognition
in Body-Worn Video

Honglin Chen, Hao Li, Alexander Song, Matt Haberland, Osman Akar, Adam Dhillon, Tiankuang Zhou,
Andrea L. Bertozzi, and P. Jeffrey Brantingham

1

9
1
0
2
 
r
p
A
 
9
1
 
 
]

V

I
.
s
s
e
e
[
 
 
1
v
2
6
0
9
0
.
4
0
9
1
:
v
i
X
r
a

Abstract—Body-worn cameras are now commonly used for log-
ging daily life, sports, and law enforcement activities, creating a
large volume of archived footage. This paper studies the problem
of classifying frames of footage according to the activity of the
camera-wearer with an emphasis on application to real-world
police body-worn video. Real-world datasets pose a different
set of challenges from existing egocentric vision datasets: the
amount of footage of different activities is unbalanced, the data
contains personally identiﬁable information, and in practice it is
difﬁcult to provide substantial training footage for a supervised
approach. We address these challenges by extracting features
based exclusively on motion information then segmenting the
video footage using a semi-supervised classiﬁcation algorithm.
On publicly available datasets, our method achieves results
comparable to, if not better than, supervised and/or deep learning
methods using a fraction of the training data. It also shows
promising results on real-world police body-worn video.

Index Terms—Egocentric vision, semi-supervised learning.

I. INTRODUCTION

it

ogy,

is now possible and convenient

W ITH the development of body-worn camera technol-

to record
continuously for a long period of time, enabling video capture
of entire days. This technology has been been adopted by law
enforcement agencies to log all police activity. The footage
serves as a source of evidence and a way to promote trans-
parency and accountability in law enforcement [1]. However,
this generates a massive amount of ﬁrst-person footage on
a daily basis that is impossible for humans for review. It is
hence necessary to develop algorithms to aid in summarizing
and indexing such video footage. In this paper, we are con-
cerned with recognizing what the camera-wearer is doing and
indexing the footage according to this “ego-activity”.

Classifying ego-activities in body-worn video footage has
often been studied in the context of sports videos [2] and
life-log videos [3]–[6]. Other than [7], no experiments have
been conducted on real-world police body-worn video. In the

Manuscript created April 22, 2019. This work was supported by NIJ grant
2014-R2-CX-0101, NSF grant DMS-1737770, and NSF grant DMS-1417674.
Tiankuang Zhou was supported by the UCLA-CSST program. The ﬁrst two
authors contributed equally to this work.

H. Chen (chenhonglin@g.ucla.edu), H. Li (lihao0809@math.ucla.edu), O.
Akar (osmanakar1123@hotmail.com ), A. Bertozzi (bertozzi@math.ucla.edu),
and J. Brantinghan (branting@ucla.edu) are with the University of California,
Los Angeles.

A. Song (alexandersong@umail.ucsb.edu) is with the University of Cali-

fornia, Santa Barbara.

University.

Technology of China.

M. Haberland (mhaberla@calpoly.edu) is with California Polytechnic State

T. Zhou (tiankuangzhou@gmail.com) is with the University of Science and

present paper, we ﬁll this gap by studying a police body-worn
video dataset provided by the Los Angeles Police Department
(LAPD) and designing an ego-activity recognition algorithm
to handle the unique challenges present in the ﬁeld dataset. As
pointed out in [7], the real-world human behaviors captured
in the footage are diverse; we also observe that the amount of
footage of each activity is distributed unevenly. In the provided
dataset, the majority of the footage is recorded when the police
ofﬁcers are standing still, walking, or inside a vehicle, but we
have also noticed a variety of relatively rare activities such
as riding a bike or motorcycle. One reason that footage of
an activity is scarce is that the activity is not recorded very
often. For instance, we have only found police ofﬁcers riding
a motorcycle in two of the hundred videos we have studied.
On the other hand, some activities occur frequently, but each
occurrence lasts a very short period of time, e.g. a burst of
running in chase of a suspect. It is important for us to correctly
recognize these rare activities because often they are the more
signiﬁcant ones to identify.

Most ego-activity recognition methods require a substantial
amount of training footage; supervision is either used to
determine the importance of extracted low-level features in a
bottom-up system (e.g. [8]) or used to learn features in a deep-
learning approach (e.g. [5]). In the present paper, we consider a
semi-supervised approach, in which we utilize a much smaller
amount of labeled training data than a typical supervised
method. Although body-worn video footage is abundant, the
majority of the footage is recording routine activities. This
makes it tedious and difﬁcult to locate and annotate a large
amount footage of rare but signiﬁcant activities. In the appli-
cation to police body-worn video, which is highly sensitive
because it contains personally identiﬁable information, the
annotation process is even more time-consuming due to strict
data security protocols.

The proposed semi-supervised approach is based on simi-
larity graphs. It ﬁrst quantiﬁes the similarities between pairs of
data points, i.e. short pieces of video, according to handcrafted,
motion-based features adapted from [9]. Then, it spreads the
label information from a small set of manually labeled ﬁdelity
points to unlabeled data. We propose the use of handcrafted
features instead of deep-learning features so that we can ensure
that the features do not compromise personally identiﬁable
information in police body-worn videos. For the same data
security reason, we employ features based exclusively on
motion cues without object detection and tracking. With the
aid of the Nystr¨om extension, the graph-based semi-supervised
classiﬁcation method is scalable to handle the enormous size

of body-worn video datasets.

Besides the police body-worn video dataset, we also bench-
mark the proposed method on publicly available datasets
and demonstrate its comparable performance to supervised
methods although it only uses a fraction of training data.

The paper is organized as follows. In section II we survey
related work on analyzing egocentric vision and activity recog-
nition. In section III, we introduce our feature extraction and
semi-supervised learning method. We report our experimental
setup and results in section IV. Finally, the conclusions and
future work follow in section V.

II. RELATED WORKS

Research in summarizing and segmenting egocentric videos
recorded by body-worn cameras dates back to the early 2000s
[10]. Since then, this has been an active research area due to
the advancement in computer vision, machine learning, and
deep learning [11]; here we review work most relevant to our
own.

The task of activity recognition in body-worn video can
be categorized into three lines of research: (1) one relies on
object-hand interactions and video content (i.e. what objects
and people are in the video), (2) one uses the motion of
the camera, and (3) ones uses a combination of the previous
two. Typically, neither object-hand interactions nor the motion
of the camera is directly available as metadata in egocentric
vision datasets, so all three lines of research start with inferring
respective pieces of information from raw video footage.

Works following the ﬁrst approach rely on object detection
and tracking to classify the camera-wearer’s activities, for
instance, [4], [6], [12]–[14]. Popular benchmark datasets used
to validate methods focusing on hand-object interactions are
the GTEA and GTEA Gaze+ datasets, provided by [6], and
ADL-short
in [3] and ADL-long in [4]. The GTEA and
GTEA Gaze+ datasets are recorded by Tobii eye-tracking
glasses when wearers are cooking in a natural setting, so
these two datasets contain eye-gaze direction information not
typically available in other body-worn video datasets. Both
ADL dataset are recorded with a chest-mounted camera when
the wearers are performing various daily tasks indoors. The
aforementioned datasets are different from our police body-
worn video dataset, which was recorded outdoor and usually
did not capture police ofﬁcers’ hand movement, so we do not
pursue this thread of research.

The second line of research is to recognize activities based
on motion analysis. A wide variety of motion features have
been proposed in the literature. [2] uses a histogram-based
motion feature to classify sports activities in videos recorded
by head-mounted GoPro cameras. [9] proposes a motion
descriptor that
inspired our feature selection method. [7]
uses inferred camera movement signals and their dominant
frequencies. Many ways of incorporating temporal information
in motion analysis are proposed; for instance, [15] proposes
to apply multiple temporal pooling operators to any per-frame
motion descriptor. Deep convolutional neural networks are also
used to extract motion features; for instance, [16] learns a
motion representation by using 2D convolution neural network

2

on stacked spectrograms and a Long Short-Term Memory
(LSTM) network. With multiple available features extracted,
[17] proposes a multiple kernel learning method to combine
local and global motion features. A benchmark dataset for
this line of research is the HUJI EgoSeg dataset provided
by [8], which is recorded when the wearer is performing a
variety of activities in both indoor and outdoor settings. As
in the HUJI EgoSeg dataset, we observe that many activities
of interest
induce
distinctive camera movement patterns, and so we focus on a
motion-based approach. The proposed approach differs from
the aforementioned methods in that
is semi-supervised;
we demonstrate in section IV-C that it achieves comparable
performance to the supervised methods on the HUJI EgoSeg
dataset using a fraction of training data.

in our police body-worn video dataset

it

For the third line of research, methods that utilize both
appearance (i.e. object recognition and tracking) and motion
cues are often combined with deep learning. Both [5] and
[18] use a two-stream deep convolution neural network, one
stream for images and another stream for optical ﬂow ﬁelds,
to discover long-term activities in body-worn video. Both [19]
and [20] use an auto-encoder network to extract motion and
appearance features in an unsupervised fashion. We note that
features extracted from appearance cues using a convolutional
neural network may be used to reconstruct the original frame,
which can potentially be used to recover personally identiﬁable
information, so we do not pursue this line of approach.

In [7], the authors also study ego-activity recognition in
police body-worn video. We improve upon their work by
choosing a more sophisticated feature than theirs to handle
the increased diversity of activities in our much larger dataset.
We demonstrate the improved performance of the proposed
method in section IV. To the best of our knowledge, no other
experiment results on real-world police body-worn video have
been reported in the literature.

III. METHOD

We start with extracting features based on motion cues from
the video. The extracted motion features are potentially high-
dimensional, so they are compressed to a lower-dimensional
representation to alleviate computational burden. Finally, we
classify the video footage with the low-dimensional represen-
tation using a graph-based semi-supervised learning method
that only requires 10% training data from each class of activity.
The ﬂowchart ﬁg. 1 summarizes the proposed system, which
we detail below.

A. Motion Descriptor

Our motion descriptor is similar to the one presented in
[9] except for the ﬁnal dimension reduction step: [9] uses the
principle component analysis (PCA) whereas we choose the
non-negative matrix factorization (NMF) because the features
are inherently non-negative. Before we compute any feature,
we resize all video frames to have a resolution of 576 × 1024
and hence an aspect ratio of 16 : 9, allowing us to choose a
uniform set of video parameters across all datasets.

3

Fig. 1. A summary of the proposed method. First, we compute a dense optical ﬂow ﬁeld for each pair of consecutive frames. We then divide each optical ﬂow
ﬁeld into sx × sy spatial regions, where each region consists of dx × dy pixels, and divide the video into st temporal segments, where each segment consists
of dt frames. For each dx × dy × dt cuboid, we count the number of ﬂow vectors with direction lying within in each octant, yielding a sx × sy histogram
for each segment of video. We reshape and concatenate each histogram into a single feature vector of dimension sx × sy × 8 describing the motion that
occurs within the video segment. The dimension of the feature vectors is reduced with NMF and we smooth them with a moving-window average operator.
Finally, we classify the smoothed features with a semi-supervised MBO scheme.

1) Dense Optical Flow Fields: Dense optical ﬂow ﬁelds
[21]–[24], which describe relative motion between objects
in the scene and the camera, form the basis of our motion
analysis. Optical ﬂow ﬁelds are ﬁelds of two-dimensional
vectors deﬁned on the two-dimensional domain of images.
In the discrete setting, an optical ﬂow ﬁeld associates each
pixel in an image with an optical ﬂow vector which consists
of a horizontal and vertical component. An optical ﬂow ﬁeld
is calculated from a pair of consecutive frames under the
assumption that pixels displaced according to the optical ﬂow
ﬁeld should preserve their intensities after the displacement.
Assuming that the objects recorded in a pair of frames are
static, the optical ﬂow ﬁeld encodes the movement of the
camera and hence the movement of the camera-wearer. Al-
though this assumption does not necessarily hold perfectly for
real-world body-worn video footage, static background objects
often cover the majority of frames, and thus we can use optical
ﬂow ﬁelds to estimate the movement of the camera-wearer.
Even when this assumption is not true, we have found that
optical ﬂow ﬁelds induced by the movement of objects instead
of the camera-wearer are still helpful in certain situations. For
instance, they characterize driving a car by the static interior
of the vehicle and the movement in the windshield region.
This is also observed in the experiments conducted by [5]; the
authors ﬁnd distinctive patterns of optical ﬂow ﬁelds in the
windshield region that correspond well to the camera-wearer
driving a car.

Algorithm 1 Global Motion Descriptor
1: Input: Optical ﬂow ﬁelds matrix O ∈ Rnf ×nx×ny×2
2: Output: Matrix X ∈ Rst×(sx·sy·8)
3: Initialize dt = 60, dx = dy = 64, sx = nx

dx , sy = ny
dy ,
dt (cid:99), histogram count matrix C ∈ Rst×sx×sy×8

st = (cid:98) nf

for j = 0 : sx do

4: for i = 0 : st do
5:
6:
7:
8:

for k = 0 : sy do

% Step 1. Partition:
cuboid = O[idt : (i+1)dt, jdx : (j+1)dx,
kdy : (k+1)dy, :]
% reshape: Rdt×dx×dy×2 (cid:55)→ R(dt·dx·dy)×2
cuboid = reshape(cuboid)
% Step 2. Histogram count:
for l = 0, 1, · · · , (dt · dx · dy) do

9:
10:
11:
12:
13:

v = cuboid[l, :]
θ = phase(v)
bin = (cid:98)θ/ π
4 (cid:99)
C[i, j, k, bin]++

14:
15:
16:
17:
18:
19:
20: end for
21: % reshape: Rst×sx×sy×8 (cid:55)→ R(sx·sy·8)×st
22: X = reshape(C)

end for

end for

end for

2) Histograms on Dense Optical Flow Fields: Using op-
tical ﬂow ﬁelds is common in classifying ego-activities.
Different motion features are effectively different ways of
aggregating them. For instance, authors of [2], [9], [15] bin
optical ﬂow vectors to construct features in the form of
concatenated histograms, [5], [19], [20] aggregate them via
convolution kernels, and [7], [8] infer camera movement using
unaggregated optical ﬂow ﬁelds as input. In our case, we
compute the motion descriptors, proposed in [9], as histograms
of extracted dense optical ﬂow vectors. We bin the vectors
according to their locations in the frames and orientations,
and then count
the number of vectors in each bin. Note
that we lose magnitude information in this process because
the bins only correspond with locations and orientations.
The features proposed in [2] retain magnitude information
by further grouping optical ﬂow vectors according to their
magnitudes, but in our experiments we observe comparable
performance using the simpliﬁed features.

To compute the motion descriptors from the optical ﬂow
ﬁelds, we consider a video as a 3D volume with frames (optical
ﬂow ﬁelds) stacked along the time axis. We spatially divide
each frame into sx by sy rectangular regions of ﬁxed width
dx and height dy pixels; the choice of dx and dy determines
the spatial resolution of the ﬁnal features. We have found that
choosing dx and dy that are divisible by the total number of
pixels in length and height, respectively — yielding sx = 16
and sy = 9 — gives good performance on all datasets tested.
We also divide the video into st video segments, each with a
ﬁxed time duration ∆T , that is, dt frames. We choose ∆T
depending on the time scale of the ego-activities that we
wish to classify. For instance, we choose ∆T = 0.2 second
for videos containing a mix of long term and short term
ego-activities, whereas we choose ∆T = 4 seconds if we
wish to classify relatively long-term activities. The choice of
∆T also determines the computation cost of the subsequent
analysis. A ﬁner time resolution, i.e. a smaller ∆T , yields
more video segments for a given video and hence results in
more computations.

Consider the optical ﬂow vectors in each dx × dy × dt
volume. We place each of them into one of the pre-deﬁned
eight histogram bins based on its orientation. Formally, a
(cid:5).
vector with a directional angle θ is placed in bin (cid:4)θ/ π
Repeating the above steps for every dx × dy × dt volumes
in each video segment of duration ∆T , we obtain a feature
vector with a dimension of sx × sy × 8 for each segment,
which we reshape into a single column vector. By repeating the
above procedures for every video segments of length ∆T and
stacking obtained feature vectors, we obtain a data matrix X
with the number of columns equal to the number of segments
in the video.

4

3) Non-negative Matrix Factorization: The concatenated
histograms for each video segment can have 9×16×8 = 1152
entries, which can potentially be expensive to compute with.
To alleviate this problem, we employ dimension reduction
techniques. In [9], the authors use the principal component
analysis (PCA) to perform dimension reduction. However, we
use non-negative matrix factorization (NMF) [25] because the
concatenated histograms are inherently non-negative. NMF is

4

widely used in the context of topic modeling, where users
want to learn topics, a collection of words that often co-
occur in textual documents, each of which is represented by
a histogram of words. In our case, each video segment is
represented by a histogram of “motion words”; each motion
word is the movement of a speciﬁc orientation in a speciﬁc
region of the frame. Analogously, a topic — a collection of
motion words — describes a global movement pattern. We
then model the concatenated histogram of motion words of
each video segment as a non-negative linear combination of
the topics.

NMF factorizes a non-negative m×n matrix X (in our case,
m = sx × sy × 8 and n = st) into the product of two low
rank non-negative m × ˆk and ˆk × n matrices V and H. The
number ˆk is chosen by the users according to their computation
resources and tuned based on the resulting performance. We
have found that ˆk = 50 works well for all considered datasets.
Formally, this is achieved by solving the following constrained
minimization problem,

min
V,H

(cid:107)X − V H(cid:107)2

F , subject to V ≥ 0, H ≥ 0,

(1)

where (cid:107) · (cid:107)F denotes the Frobenius norm. Each column in V
represents a basis vector (a topic), and each entry in H rep-
resents the non-negative linear combination coefﬁcients. Each
column in the matrix H is the feature vector for a single video
segment, which will be passed into our classiﬁcation algorithm
after a post-processing step (detailed in section III-A4).

We also note that we do not necessarily need to perform
NMF every time we obtain a new video. We may choose to
ﬁx V which we obtain by applying NMF to the initial dataset.
Then we only need to compute the combination coefﬁcients
H new for the new videos X new by solving a non-negative least
squares problem

H new = arg min
H

(cid:107)X new − V H(cid:107)2
F ,

(2)

which can be solved very efﬁciently using methods such as
ones proposed in [26].

The dimension reduction step also helps to secure personally
identiﬁable information. Since we do not make use of V in the
classiﬁcation algorithm, there is no need to save it. Without
the basis, it is impossible to reconstruct the data matrix X and
let alone the content of the videos.

4) Post-processing: We assume a certain degree of tem-
poral regularity of the extracted features:
the duration of
activities is typically much longer than transitions between
them, and so transitions are relatively rare. We note that none
of our previous feature extraction procedure takes advantage
of this temporal regularity. Each optical ﬂow ﬁeld is com-
puted from only two adjacent frames, motion descriptors are
aggregated within non-overlapping video segments, and NMF
treats columns in the data matrix X (motion descriptors of
video segments) independently. Methods exploiting temporal
regularity have been proposed before. In [15], for instance,
the authors apply multiple temporal pooling operators to the
extracted per-frame motion and visual features and use the
outputs as additional features. We choose a simpler approach,
in which we apply a single moving-window average operator

wij = exp

−

(cid:18)

(cid:107)Hi − Hj(cid:107)2
2
τij

(cid:19)

,

(3)

objective function

on each row of H and then pass these averaged features to
the classiﬁcation method. We determine the window size of
the moving-window average operator experimentally for each
dataset. Choosing a large window size may eliminate distinct
features of short-term activities, so the choice depends on the
types of activities in the dataset as well as the chosen value
of ∆T .

B. Classiﬁcation Method

1) Graph-based semi-supervised classiﬁcation method:
Recently, graph-based semi-supervised and unsupervised
learning methods have been successfully applied to image
processing [27] and classiﬁcation of high-dimensional data
such as hyperspectral images [28]–[31] and body-worn videos
[7]. In this section, we outline one of these methods based on
minimizing the graph Total Variation, which has been studied
in [32]–[34]. We consider each data point (i.e. video segment)
as a node in a weighted graph. The edge weight between a
pair of nodes i and j is given by the similarity

where (cid:107) · (cid:107)2 denotes the 2-norm of a vector and τij’s are
scaling constants. Here Hi is the ith column of matrix H
obtained from NMF. The scaling constants can either be the
same chosen τ for all pairs of i and j, or chosen locally
for each individual pair [35]. We choose the local scaling
constants τij = τiτj where τi is the distance between i and
its Kth nearest neighbor.

We aim to partition n nodes into c classes (i.e. ego-

activities) such that

1) similar nodes between which edge weights are large (i.e.
wij’s are close to 1 should be in the same class, and
2) ﬁdelity nodes (i.e. manually labeled nodes) should be

classiﬁed according to their labels.

To achieve 1), we optimize the graph Total Variation (TV)
deﬁned as follows. Let u be an {0, 1}c-valued assignment
function on the set of nodes, that is u(cid:96)(i) = 1 meaning we
assign the ith data point to class (cid:96). We can then deﬁne the
graph Total Variation

|u|T V =

wij(cid:107)u(i) − u(j)(cid:107)1

(4)

We observe that (4) admits a trivial minimizer that is con-
stant across all nodes. To avoid this problem and to incorporate
the ﬁdelity data, we introduce a quadratic data ﬁdelity term

F (u) =

M (i)(cid:107)u(i) − f (i)(cid:107)2
2 ,

(5)

where M (i) = 1 if node i is chosen as ﬁdelity and 0 otherwise,
and f (i) ∈ {0, 1}c encodes the known label of node i. We
weight the ﬁdelity term by a positive parameter η to balance
the graph TV term and the ﬁdelity term in the objective
function,

|u|T V + ηF (u).

(6)

1
2

n
(cid:88)

i,j=1

1
2

n
(cid:88)

i=1

1
2

5

Instead of minimizing (6) directly, which is discrete and
combinatorial, we solve the Ginzburg-Landau relaxation [32]
for u(i) ∈ Rc. Namely, we replace the graph Total Variation
|u|T V with

GL(cid:15)(u) =

wij(cid:107)u(i) − u(j)(cid:107)2

2 +

P (u(i)) , (7)

1
4

n
(cid:88)

i,j=1

1
(cid:15)

n
(cid:88)

i=1

where (cid:15) is a small positive constant, and P is a multi-well
potential with minima at the corners of the unit simplex, for
instance

P (u(i)) =

1
4
where e(cid:96) is the unit vector in Rc in the (cid:96)th direction. The
authors of [36] prove the following Γ-convergence
(cid:40)

(cid:107)u(i) − e(cid:96)(cid:107)2
2,

(8)

(cid:96)=1

c
(cid:89)

GL(cid:15)(u) Γ−→

if u is binary

|u|T V
+∞ otherwise

(9)

as (cid:15) → 0 in the case of c = 2. The Γ-convergence ensures that
the minimizers of GL(cid:15)(u) approach the minimizers of |u|T V .
the
After the Ginzburg-Landau relaxation, we arrive at

GL(cid:15)(u) + ηF (u),

(10)

which we minimize with respect to u.

To formulate (10) in terms of matrices, we ﬁrst identify u
and f by a n × c matrix where ui(cid:96) = u(cid:96)(i) and fi(cid:96) = f(cid:96)(i).
We let W be the matrix of wij’s, and D be an n × n diagonal
matrix with the ith entry di being the strength of node i, i.e.
di = (cid:80)n
j=1 wij, and then deﬁne the graph Laplacian
L = D − W.

(11)

We also let M be an n × n diagonal matrix of which the ith
entry is M (i) indicating whether node i is chosen as ﬁdelity. If
we deﬁne L and M this way, we can write (10) in the matrix
form
1
2

trace (cid:0)uT Lu(cid:1) +

(cid:107)M (u − f )(cid:107)2
2.

P (ui) +

n
(cid:88)

(12)

η
2

1
(cid:15)

i=1

In graph clustering, unsupervised learning, and community
detection literature, the graph Laplacian is often normalized
to guarantee convergence to a continuum differential operator
with a large number of data points (see, for instance, [32]).
One popular version of normalized graph Laplacian is the
symmetric Laplacian

Ls = I − D− 1

2 W D− 1
2 .

If we substitute L for Ls, the ﬁrst quadratic term of (12)
becomes

trace (cid:0)uT Lsu(cid:1) =

1
2

n
(cid:88)

i,j=1

wij

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

u(i)
√
di

−

u(j)
(cid:112)dj

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

The methods described in the remainder of this paper carry
over regardless of which graph Laplacian is used, and the
notation L is a placeholder for any choice of graph Laplacian.
In our experiments, we choose to use the symmetric graph
Laplacian Ls because it permits the use of efﬁcient and
simple computation routines to approximate its eigenvalues
and eigenvectors.

2) Optimization scheme: Minimizing (12) using the stan-

dard gradient descent method yields

∂u
∂t

1
(cid:15)

= −Lu −

∇ ˆP (u) − ηM (u − f ),

(13)

where ˆP (u) = (cid:80)n
i=1 P (ui). The steady-state solution of (13)
is a stationary point of (12). This is known as the graph Allen-
Cahn equation, and we refer readers to [37] for a convergence
analysis of the graph Allen-Cahn scheme. We follow [38] to
use a variant of the MBO (Merriman-Bence-Osher) scheme
to approximate and solve (13). In short, we ﬁrst randomly
initialize u0, which we use as the initial condition for (13).
We then alternate between the following two steps:
1. Diffusion: for given uk, we obtain uk+ 1

2 by solving a

force-driven heat equation

∂u
∂t

= −Lu − ηM (u − f ),

(14)

for tk ≤ t ≤ tk + 1

2. Threshold: we threshold uk+ 1

2 ∆t, where ∆t is a parameter.
2 to obtain uk+1, i.e.

uk+1(i) = e(cid:96) , where (cid:96) = arg max

uk+ 1
iˆ(cid:96)

2

.

ˆ(cid:96)

For a small (cid:15), this approximates solving

(15)

(16)

∂u
∂t

= −

∇ ˆP (u)

1
(cid:15)

for tk + 1

2 ∆t ≤ t ≤ tk+1 = tk + ∆t.

Choosing ∆t is delicate. If it is too small, uk+1 = uk after
thresholding, whereas if it is too large, u converges to the
steady-state solution of (14),

(L + ηM )−1M f,

in one diffusion step,
independent of the initial condition
uk. Either way, extreme ∆t results in a “freezing” scheme.
In [39], the authors give guidance on how to choose ∆t
in the case of unnormalized graph Laplacian, c = 2 (i.e.
binary classiﬁcation), and η = 0. Currently,
there is no
analogous result for a symmetric graph Laplacian, multi-class
classiﬁcation, and nonzero η. We have found, however, that
∆t = 0.1 gives nontrivial dynamics (i.e. convergent and not
“freezing”) on all datasets used in testing.

3) Numerical methods: We follow [32], [33] to employ
a semi-implicit ordinary differential equation solver to solve
(14), and use a pseudo-spectral method coupled with the
Nystr¨om extension to make the ordinary differential equation
solver efﬁcient. We note that the graph Laplacian matrix L is
large, with n2 entries where n is the number of data points;
it is also not inherently sparse, which makes approximation
techniques such as the Nystr¨om extension necessary.

For the ordinary differential equation solver, we take Nstep
time steps to reach uk+ 1
2 from uk, where Nstep is a parameter
to choose. Formally, we let uk,s, s = 0, 1, · · · , Nsteps denote
the numerical solutions of (14) at intermediate time tk + sδt,
where δt = ∆t/2Nstep. We solve

uk,s+1 − uk,s
δt

= −Luk,s+1 − ηM (uk,s − f )

(17)

6

for uk,s+1. We use Nstep = 10 to ensure convergence of the
ordinary differential equation solver when η < 500 and ∆t =
0.1.

We use a pseudo-spectral method to solve Equation (17).
We project the solution u onto an orthonormal eigenbasis of
the graph Laplacian L, or an eigen-subbasis that consists of
Neig eigenvectors corresponding to the smallest Neig eigen-
values. We detail how we compute the spectrum of L with
the Nystr¨om extension in section III-C. Choosing a modest
Neig (cid:28) n will greatly improve the efﬁciency of the algorithm
because solving (17) only requires O(nNeig) operations if the
eigenvectors and eigenvalues of L are provided. Suppose Φ
is an n × Neig eigenvector matrix, of which the jth column
φj is the eigenvector of L corresponding to the jth smallest
eigenvalue λj, and Λ is the diagonal matrix containing all Neig
smallest eigenvalues λj’s. We let a denote the coordinates we
obtain by projecting columns of u onto the eigen-subspace
spanned by columns of Φ, i.e. a = ΦT u. Solving (17) in the
eigen-subspace is simply

ak,s+1 = (I + δtΛ)−1ak,s − δt · ηΦT M (uk,s − f ) ,
uk,s+1 = Φak,s+1 .

Algorithm 2 Graph MBO scheme [32]

1: Input: Φ, Λ, M, f, η, and initial guess u0.
2: Output: u.
3: Initialize u0,0 = u0, a0,0 = ΦT u0.
4: for k = 1, 2, · · · , MaxIter or uk has converged do
5:

a. Diffusion:
for s = 0, 1, · · · , Nstep − 1 do

ak,s+1 = (I + δtΛ)−1ak,s − δt · ηΦT M (uk,s − f ).
uk,s+1 = Φak,s+1.

end for
b. Threshold uk+1/2 := uk,Nstep :
for i = 1, 2, · · · , n do

uk+1,0(i) = e(cid:96), where (cid:96) = arg maxˆ(cid:96) uk,Nstep

iˆ(cid:96)

6:
7:
8:
9:
10:
11:

12:
13:
14: end for

end for

C. Nystr¨om extension

We employ the Nystr¨om extension [40], which approxi-
(cid:1)

mates the eigenvectors and eigenvalues of L with O (cid:0)nN 3
eig
computation complexity and O(nNeig) memory requirement.
With Neig (cid:28) n, the computation complexity and memory
scales linearly with respect to the number of data points.
The idea of the Nystr¨om extension is to uniformly randomly
sample a smaller set of data points A ⊂ {1, 2, · · · , n} with
|A| = Nsample (cid:28) n, perform spectral decomposition on
an Nsample × Nsample system calculated from the set of
data points A, and then interpolate the result to obtain an
approximation to the spectral decomposition of the entirety of
L. Let B be the complement of A, i.e. A ∪ B = {1, 2, · · · , n}
and A ∩ B = ∅. Let WAA denote the weights associated with
nodes in set A, and similarly, let WAB = W T
BA denote weights
between nodes in set A and B. If we reorder the nodes so that

A = {1, 2, · · · , Nsample} and B = {Nsample + 1, Nsample +
2, · · · , n}, we can rewrite

and recall

W =

(cid:20)WAA WAB
WBA WBB

(cid:21)

.

(18)

It can be shown [40] that the matrix WBB can be approximated
by WBB ≈ WBAW −1
AAWAB in the context of approximating
the spectral decomposition. The Nystr¨om extension uses this
property to approximate the spectrum of W , and henceforth
L. We summarize the Nystr¨om extension algorithm to ap-
proximate the spectrum of symmetric graph Laplacian in
Algorithm 3. An analogous algorithm for unnormalized graph
Laplacian can be found in [32]. In Algorithm 3, 1 denotes
a vector of one’s that is used to compute the strength of
each nodes, i.e. the sum of weights, and let X./Y denote
component-wise division between two matrices X and Y of
X denote the non-negative square root
the same size. We let
of each component of any non-negative matrix X, and if X is
positive deﬁnite with the spectral decomposition X = QΓQT ,
we let X 1/2 = QΓ1/2QT and similarly X −1/2 = QΓ−1/2QT .

√

Algorithm 3 Nystr¨om Extension for symmetric graph Lapla-
cian [32] [40]
i=1 and {τij}n
1: Input: {Hi}n
2: Output: Φ, {λj}Neig
j=1 .
3: Randomly sample A ⊂ {1, 2, · · · , n} with |A| =
Nsample ≥ Neig and B such that A ∪ B = {1, 2, · · · , n}.

ij=1.

4: Compute WAA and WAB using (3).
5: Compute the strength of nodes in A, dA = WAA1.
6: Approximate the strength of nodes in B, dB = WBA1 +

WBAW −1

AAWAB1.
7: Normalize WAA = WAA./

(cid:113)

(cid:113)

dAdT
A.
dAdT
B.
decomposition

8: Normalize WAB = WAB./
9: Perform spectral
W −1/2
AA WABW T
eigenvalues {ξi}Neig
{ψi}Neig
and Ξ be a diagonal matrix with ξi’s on the diagonal.

on WAA +
ABW −1/2
to obtain the Neig
largest
i=1 and the corresponding eigenvectors
i=1 . We let Ψ denote the matrix of the eigenvectors

AA

10: Output λi = 1 − ξi, and Φ =

(cid:35)

(cid:34)

W 1/2
AA
WBAW −1/2

AA

ΨΞ−1/2.

IV. EXPERIMENTS

We apply the proposed method on two publicly available
datasets, the QUAD dataset [2] and the HUJI EgoSeg dataset
[8], and compare our results to those reported in [2], [5],
[8], [15], [41]. We also apply both our method and the one
proposed in [7]1 on a police body-worn video dataset provided
by the LAPD. Our experimental procedures and parameters are
summarized in TABLE I. The measures of success we use are
precision

True Positive
True Positive + False Positive

7

True Positive
True Positive + False Negative

,

within each class, mean precision and recall directly averaged
over all classes, and the overall accuracy, i.e. the percentage
of correctly classiﬁed data points.

The feature extraction is done on an ofﬂine machine to
ensure the security of the LAPD video. Subsequent analysis,
including the Nystr¨om extension and the graph MBO scheme,
is performed on a 2.3GHz machine with Intel Core i7 and 4
GB of memory. Both experiments on the QUAD dataset and
the HUJI EgoSeg dataset can be ﬁnished within a minute after
extracting features; each batch of the LAPD body-worn video
dataset (see section IV-B for details) takes around two minutes.

A. QUAD dataset

The authors of [2] choreographed and made public the
QUAD dataset, which is about four minutes long and ﬁlmed
at 60 frames per second. The footage was recorded with a
head-mounted Go-Pro Camera while the camera-wearer was
undergoing nine ego-activities (reported in TABLE II), such
as walking, jumping, and climbing up stairs2. The authors of
[2] and [7] tested their ego-activity classiﬁcation methods on
this dataset; we follow the same experimental protocol as [7].
Each video “segment” is chosen to be an individual frame and
we uniformly sample 10% segments within each category as
ﬁdelity in agreement with the protocol employed in [7]. Such
choice of one frame per segment yields 14,399 segments.

In TABLE II, we report precision within each category
and the mean precision, directly averaged over nine classes;
the authors of [2] have also reported the mean precision
and the authors of [7] provided detailed precision per class.
Both our method and the method in [7] use 10% of the
video, sampled uniformly, as ﬁdelity. The method in [2] is
unsupervised and the reported mean precision is calculated
after matching the discovered ego-activity categories to the
ground-truth categories in a way that the best match gives the
highest harmonic mean of the precision and recall (i.e. the best
F-measure). Our result is overall an improvement upon [7] in
terms of precision.

The QUAD dataset only consists of a short choreographed
video, in which activities of interest have a relatively balanced
proportion, and the challenges we observe in the ﬁeld datasets
are absent. However, the experiment on the choreographed
dataset validates the baseline ability of our method in rec-
ognizing ego-activities in body-worn videos. We further test
our method and showcase the applicability of our method to
datasets consist of multiple videos of different lengths that are
not choreographed and recorded in a variety conditions.

B. LAPD BWV dataset

The LAPD body-worn video dataset consists of 100 videos
with a total length of 15.25 hours recorded at 30 frames per
second. The video footage is recorded by cameras mounted

2The reported categories of ego-activities are the same ones used in [7] and

1with the implementation kindly provided by the authors of [7]

are different from [2].

8

TABLE I
EXPERIMENTAL SETUP

∆T
(sec)

1/60
1/5
1/5
4

FPS

60
30
30
15

QUAD
LAPD
LAPD [7]
HUJI

Motion feature
Number of
segments

Window size
(segment)

NMF

Spectrum of the Graph Laplacian

MBO

ˆk Neig

τij

Nsample

Batch size
(segment)

η ∆t Nstep

14,399
274,443
274,443
36,421

-
5
-
20

50
50
-
50

τ = 1

500
2000 K = 100
2000 K = 100
400 K = 40

1000
2000
2000
400

-
30000
30000
-

300
400
400
300

0.1
0.1
0.1
0.1

10
10
10
10

TABLE II
CLASS PROPORTION AND PRECISION OF THE QUAD DATASET

TABLE III
CLASS PROPORTION, PRECISION, AND RECALL OF THE SELECTED NINE
CLASSES IN THE LAPD BODY-WORN VIDEO DATASET

Class

Proportion

[2]

[7]

Ours

Jump
Stand
Walk
Step
Turn Left
Turn Right
Run
Look Up
Look Down

14.54%
13.74%
12.75%
12.65%
11.25%
10.16%
9.00%
8.85%
7.06%

Precision

92.51% 99.07%
87.90% 87.11%
84.52% 98.37%
93.98% 98.54%
89.43% 96.96%
92.80% 96.21%
92.38% 96.17%
80.36% 90.02%
84.59% 89.00%

-
-
-
-
-
-
-
-
-

Mean

11.11%

95% 88.74% 94.49%

Class

Proportion

[7]

Ours

[7]

Ours

Precision

Recall

Stand still
In stationary car
Walk
In moving car
At car window
At car trunk
Run
Bike
Motorcycle

62.57%
16.84%
9.04%
5.76%
0.64%
0.58%
0.33%
0.33%
0.08%

73.10% 89.44% 85.42% 95.24%
41.83% 93.69% 43.18% 89.73%
38.36% 70.53% 19.54% 59.41%
70.71% 91.03% 25.08% 84.40%
17.23% 71.45% 10.94% 45.28%
73.78% 71.79% 11.09% 51.78%
96.15% 75.94% 11.03% 53.35%
85.71% 86.49% 14.37% 75.44%
92.49% 10.76% 71.75%
100%

Mean

10.68%

66.32% 82.54% 25.71% 69.60%

on police ofﬁcers’ chests when they are performing a variety
of law enforcement activities. The dataset consists of videos
recorded both inside vehicles and outdoors and under a variety
of illumination conditions. We manually annotated each frame
of all 100 videos with one of 14 class labels. Although we
train on and classify video footage in all 14 categories, we
exclude ﬁve insigniﬁcant classes, such as “exiting car” and
“obscured camera”, from performance evaluations of the ego-
activity recognition algorithms. We report activity proportions
of the selected classes in TABLE III and, for completeness,
all 14 classes in TABLE V of the Appendix.

We apply the method in [7] with the provided implemen-
tation on the LAPD body-worn video dataset. [7] computes a
feature vector per frame instead of per short video segment,
which consists of 6 frames (0.2 seconds). The average of the
frame-wise features over a segment is used as the feature
vector of the segment. By doing so, the numbers of video
segments to classify in both methods are the same. We apply
a moving window average operator with a window size of
one second (ﬁve segments) to our features. The features of [7]
inherently incorporate temporal information, so we use the
aggregated segment-wise features as they are without further
smoothing.

We divide the 274,443 segments into 9 disjoint batches, each
of which consists of approximately 30,000 segments. As each
segment has a duration of 0.2 seconds, each batch therefore
consists of 100-minutes of footage spanning multiple videos.
We perform the classiﬁcation on each batch independently
and concatenate the classiﬁcation results. We note that both
our method and the method proposed in [7] make use of
the Nystr¨om extension and the MBO scheme described in
section III-C and section III-B respectively, so they share the

same set of parameters. We choose Nsample = 2000 and
Neig = 2000 to be the same for both methods for each batch
so that they share the same computation cost and both give
good performance relatively to other choices of parameters.
We have tuned parameters η ranging from 0.01 to 1000 and
found that η = 400 and τ selected automatically according to
[35] with K = 100 work well for both methods.

With regards to sampling ﬁdelity points, we use the same
protocol as the one used in [7] where we uniformly sample
10% segments within each class. Consequently, we have many
more samples of common activities than rare activities.

In TABLE III, we report the precision and recall within each
class and their respective means averaged over the selected
nine classes. We refer readers to TABLE V in the Appendix
for a full
table of all 14 classes as well as the overall
accuracy, which is the proportion of video segments that are
correctly classiﬁed. We also present a sample of the color-
coded classiﬁcation results in ﬁg. 2 and the confusion matrices
in ﬁg. 3.

Our method outperforms [7] in most of the categories in
terms of precision and is a major improvement according
to recall. We theorize that the features proposed in [7] are
too simple to distinguish among the increased variety of
ego-activities in the larger LAPD body-worn video dataset.
The features they propose do not make use of the locality
of motion within each frame, which we consider crucial in
order to differentiate, for instance, driving a car and walking
forward. Both activities feature forward motion, but the motion
is localized within the windshield region only in the former
case. We also note that frequency is a signiﬁcant component
of the features proposed in [7]; however, we do not observe
much periodic motion in many ego-activities.

9

C. HUJI EgoSeg dataset

We also evaluate the performance of our method on the
HUJI EgoSeg dataset [8] [5]. This dataset contains 65 hours
of egocentric videos including 44 videos shot using a head-
mounted GoPro Hero3+, the Disney dataset [42] and other
YouTube videos3. The dataset contains 7 ego-action categories:
Walking, Driving, Riding Bus, Biking, Standing, Sitting, and
Static. We normalize the frame rate of each video to 15 frames
per second to match with the normalized frame rate in [5].
We divide each video sequence into segments of 4 seconds
(∆T = 4 seconds, 60 frames), which also matches the length
of each video segment in [5]. The activities present in the HUJI
EgoSeg dataset are all relatively long-term activities compared
to the LAPD Body-worn video dataset, so using longer video
segments reduces the number of data points without the risk
of missing short-term activities. With such choice of ∆T ,
we have 36,421 segments. For the Nystr¨om extension and
the MBO scheme, we have found that the combination of
Nsample = 400, Neig = 400, η = 300, and K = 40 gives
satisfactory results.

We follow the same experimental protocol of [5], [8] to
divide the entire dataset into a training set and a testing set. We
randomly pick video sequences until we have 1300 segments
(approximately 90 minutes of video) per class as the training
set, and we uniformly sample 10% of the training set as ﬁdelity
points, which is about 10% of the training data used in [5]4.
In this experiment, we use recall to evaluate the performance
since it is the common measure of success in [5], [8], [15],
[41]. TABLE IV details the classiﬁcation results on the testing
set. The classiﬁcation performance of methods other than ours
are reported in [5]. We also report the confusion matrix in ﬁg. 4
and a color-coded sample of the classiﬁcation result in ﬁg. 5.
We observe that the recalls of Sitting, Standing, and Riding
Bus are typically lower than other activities across all ﬁve
methods, so we believe that these activities are inherently
difﬁcult to recognize with motion-based features. According
to TABLE IV, our method outperforms — using recall as a
measure of success — other methods that use handcrafted
motion and/or appearance features with or without deep
convolution neural networks, with the exception of [5]. We
emphasize that our method uses a fraction of the training
data of the supervised methods and still achieves comparable
results. When we use the entire training set as ﬁdelity, the
mean recall only sees a slight increase.

V. CONCLUSION AND FUTURE WORK

In this paper, we study ego-activity recognition in ﬁrst-
person video with an emphasis on the application to real-world
police body-worn video. We propose a system for classifying
ego-activities in body-worn video footage using handcrafted
features and a graph-based semi-supervised learning method.
These features based on motion cues do not identify people

3The HUJI EgoSeg dataset can be downloaded at http://www.vision.huji.

ac.il/egoseg/videos/dataset.html.

4The authors of [5] do not explicitly mention the ﬁdelity percentage; we
estimate the percentage according to their released code at http://www.vision.
huji.ac.il/egoseg/.

Fig. 2. Classiﬁcation results on a contiguous sample of 4000 segments
(approximately 13 minutes) from the LAPD body-worn video dataset. The
results are obtained by running both methods with the parameters described
in section IV-B.

(a) Method proposed in [7]

(b) Ours

Fig. 3. Confusion matrices for the LAPD Body-worn video dataset. The
background intensity in cell (k, (cid:96)) corresponds to the number of data points
in class k that are classiﬁed as class (cid:96) by the algorithm.

10

those in prior works, which include both classical and deep-
learning methodologies. The proposed system also demon-
strates promising results on ﬁeld data from body-worn cameras
used by the Los Angeles Police Department.

We note that the MBO-based classiﬁcation method can be
used with any feature design, not only the global motion
descriptor as presented here. The general graphical setting of
the classiﬁcation method even allows features that cannot be
represented by a vector in the Euclidean space so long there
exists a way to measure similarity between the features of two
data points. Also, the Nystr¨om extension is still applicable with
new features or similarity measures to efﬁciently approximate
the spectrum of the graph Laplacian.

the recovery of personal

Recent developments in unsupervised convolution neural
networks [19], [20] might be used to improve and extend
the current feature selection method, although caution must
be taken to prevent
identiﬁable
information from the learned features. Better incorporating
temporal information is another way to move forward. We
observe that the police body-worn video dataset contains a
mix of long-term and short-term activities, making it difﬁcult
to select a single time scale to design the features around. In
the present experiment, we chose the length of each segment to
be 0.2 seconds in order to capture short-term activities, but this
was redundant for recognizing long-term activities. We chose
∆T to be four seconds for the HUJI EgoSeg dataset, which
signiﬁcantly reduced the computation cost without sacriﬁcing
accuracy, but this was only possible because all activities
in the HUJI EgoSeg dataset have long durations. Designing
features that efﬁciently handle a mix of long-term and short-
term activities is another challenge to be addressed.

Future work will also be directed towards improving the
propose classiﬁcation method. For instance, [43] recently
proposed to incorporate the knowledge of the proportions of
classes as an extra input in the semi-supervised classiﬁcation
method described in section III-B. Considering the hetero-
geneity in the class distribution that we observe in the police
body-worn video data set (see table III), we expect to see an
improvement in the classiﬁcation performance with the class
proportion information.

Despite our best effort to develop an accurate system for the
classiﬁcation of police body-worn videos, the variability of the
data leads to imperfect classiﬁcation. Our classiﬁcation method
is naturally paired with uncertainty quantiﬁcation (UQ) [44].
Besides giving a video segment an ego-activity label, we
may use this technology to estimate a measure of uncertainty,
which identiﬁes hard-to-classify video segments that require
further investigation. Moreover, the measure of uncertainty
can suggest footage for police analysts to label
to train
classiﬁcation algorithms making an efﬁcient use of human
labeling effort. We expect that further development of the
feature selection, classiﬁcation, and uncertainty quantiﬁcation
methodologies will facilitate an implementation of the propose
system to be used by law enforcement agencies to summarize
a large volume of body-worn video footage.

Fig. 4. Confusion matrix for the HUJI EgoSeg dataset. The background
intensity in cell (k, (cid:96)) corresponds to the number of data points in class k
that are classiﬁed as class (cid:96) by the algorithm.

Fig. 5. Classiﬁcation results on a contiguous sample of 4000 segments
(approximately 4 hours) from the testing set of HUJI EgoSeg dataset. The
recall of the same experiment is reported in TABLE IV.

or objects in the scene and hence secure any personally
identiﬁable information within the video. Our experiments also
illustrate that the features are able to differentiate a variety
of ego-activities and yield better classiﬁcation results than an
earlier work [7] with the same graph-based semi-supervised
learning method. The semi-supervised classiﬁcation method
addresses the challenge of insufﬁcient training data; it achieves
comparable performance to supervised methods on two pub-
licly available benchmark datasets using only a fraction of
training data. Despite using a smaller fraction of training
data, our classiﬁcation results are comparable to or better than

TABLE IV
CLASS PROPORTION AND RECALL OF THE HUJI EGOSEG DATASET

Class

Proportion

[5]

Ours

Walking
Sitting
Standing
Biking
Driving
Static
Riding Bus

Mean
Training

34%
25%
21%
8%
5%
4%
4%

14%

[8]

83%
62%
47%
86%
74%
97%
43%

[15]

91%
70%
44%
34%
82%
61%
37%

Recall
[41]

79%
62%
62%
36%
92%
100%
58%

89%
84%
79%
91%
100%
98%
82%

89%
70%
∼60% ∼60% ∼60% ∼60%

70%

60%

91%
71%
47%
88%
95%
96%
84%

82%
6%

VI. ACKNOWLEDGEMENT

This work used computational and storage services associ-
ated with the Hoffman2 Shared Cluster provided by UCLA
Institute for Digital Research and Educations Research Tech-
nology Group. We thank Zhaoyi Meng, Xiyang Luo, and Matt
Jacobs for helpful discussion and sharing their code. We also
thank the Los Angeles Police Department for providing the
body-worn video dataset.

REFERENCES

[1] “Body worn video procedures - established,” 2015. [Online]. Available:
http://assets.lapdonline.org/assets/pdf/body%20worn%20camera.pdf
[2] K. M. Kitani, T. Okabe, Y. Sato, and A. Sugimoto, “Fast unsupervised
ego-action learning for ﬁrst-person sports videos,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2011, pp. 3241–3248.

[3] S. Singh, C. Arora, and C. Jawahar, “Trajectory aligned features for
ﬁrst person action recognition,” Pattern Recognition, vol. 62, pp. 45–55,
2017.

[4] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in
ﬁrst-person camera views,” in Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on.

IEEE, 2012, pp. 2847–2854.

[5] Y. Poleg, A. Ephrat, S. Peleg, and C. Arora, “Compact CNN for indexing
egocentric videos,” in Applications of Computer Vision (WACV), 2016
IEEE Winter Conference on.

IEEE, 2016, pp. 1–9.

[6] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
Springer,

using gaze,” in European Conference on Computer Vision.
2012, pp. 314–327.

[7] Z. Meng, J. S´anchez, J.-M. Morel, A. L. Bertozzi, and P. J. Brantingham,
“Ego-motion classiﬁcation for body-worn videos,” in Imaging, Vision
and Learning Based on Optimization and PDEs, X.-C. Tai, E. Bae, and
M. Lysaker, Eds. Cham: Springer International Publishing, 2018, pp.
221–239.

[8] Y. Poleg, C. Arora, and S. Peleg, “Temporal segmentation of egocentric
videos,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2014, pp. 2537–2544.

[9] M. S. Ryoo and L. Matthies, “First-person activity recognition: What are
they doing to me?” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2013, pp. 2730–2737.

[10] K. Aizawa, K. Ishijima, and M. Shiina, “Summarizing wearable video,”
in Proceedings to 2001 International Conference on Image Processing,
vol. 3.

IEEE, 2001, pp. 398–401.

[11] A. G. del Molino, C. Tan, J.-H. Lim, and A.-H. Tan, “Summarization
of egocentric videos: A comprehensive survey,” IEEE Transactions on
Human-Machine Systems, vol. 47, no. 1, pp. 65–76, 2017.

[12] A. Fathi, A. Farhadi, and J. M. Rehg, “Understanding egocentric activi-
ties,” in Computer Vision (ICCV), 2011 IEEE International Conference
on.

IEEE, 2011, pp. 407–414.

[13] Y. Li, Z. Ye, and J. M. Rehg, “Delving into egocentric actions,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015, pp. 287–295.

[14] E. H. Spriggs, F. De La Torre, and M. Hebert, “Temporal segmentation
and activity classiﬁcation from ﬁrst-person sensing,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2009, pp. 17–24.

[15] M. S. Ryoo, B. Rothrock, and L. Matthies, “Pooled motion features
for ﬁrst-person videos,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015, pp. 896–904.
[16] G. Abebe and A. Cavallaro, “A long short-term memory convolutional
neural network for ﬁrst-person vision activity recognition,” in Inter-
national Conference on Computer Vision Workshops (ICCVW), Venice,
Italy, vol. 1, no. 2, 2017, p. 3.

[17] F. ¨Ozkan, M. A. Arabaci, E. Surer, and A. Temizel, “Boosted multiple
kernel learning for ﬁrst-person activity recognition,” in Signal Process-
ing Conference (EUSIPCO), 2017 25th European.
IEEE, 2017, pp.
1050–1054.

[18] M. Ma, H. Fan, and K. M. Kitani, “Going deeper into ﬁrst-person activity

recognition,” arXiv preprint arXiv:1605.03688, 2016.

[19] C. A. C. J. Bharat Lal Bhatnagar, Suriya Singh, “Unsupervised learning
of deep feature representation for clustering egocentric actions,”
in Proceedings of
the Twenty-Sixth International Joint Conference
on Artiﬁcial Intelligence, IJCAI-17, 2017, pp. 1447–1453. [Online].
Available: https://doi.org/10.24963/ijcai.2017/200

11

[20] X. Wang, L. Gao, J. Song, X. Zhen, N. Sebe, and H. T. Shen, “Deep
appearance and motion learning for egocentric activity recognition,”
Neurocomputing, vol. 275, pp. 438–447, 2018.

[21] B. K. Horn and B. G. Schunck, “Determining optical ﬂow,” Artiﬁcial

intelligence, vol. 17, no. 1-3, pp. 185–203, 1981.

[22] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in Proc. DARPA Image Under-
standing Workshop, 1981.

[23] J. L. Barron, D. J. Fleet, and S. S. Beauchemin, “Performance of optical
ﬂow techniques,” International journal of computer vision, vol. 12, no. 1,
pp. 43–77, 1994.

[24] G. Farneb¨ack, “Two-frame motion estimation based on polynomial
expansion,” in Scandinavian conference on Image analysis. Springer,
2003, pp. 363–370.

[25] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix
factorization,” in Advances in neural information processing systems,
2001, pp. 556–562.

[26] C. L. Lawson and R. J. Hanson, Solving least squares problems. SIAM,

1995.

[27] E. Merkurjev, T. Kostic, and A. Bertozzi, “An MBO scheme on graphs
for segmentation and image processing. to appear in siam j,” Imag. Proc,
2013.

[28] E. Merkurjev, J. Sunu, and A. L. Bertozzi, “Graph MBO method for
multiclass segmentation of hyperspectral stand-off detection video,”
in Image Processing (ICIP), 2014 IEEE International Conference on.
IEEE, 2014, pp. 689–693.

[29] G. Iyer, J. Chanussot, and A. L. Bertozzi, “A graph-based approach
for feature extraction and segmentation of multimodal images,” 2017.
[Online]. Available: http://sigport.org/2185

[30] W. Zhu, V. Chayes, A. Tiard, S. Sanchez, D. Dahlberg, A. L. Bertozzi,
S. Osher, D. Zosso, and D. Kuang, “Unsupervised classiﬁcation in
hyperspectral
total variation and primal-dual
hybrid gradient algorithm,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 55, no. 5, pp. 2786–2798, 2017.

imagery with nonlocal

[31] Z. Meng, E. Merkurjev, A. Koniges, and A. L. Bertozzi, “Hyperspectral
image classiﬁcation using graph clustering methods,” Image Processing
On Line, vol. 7, pp. 218–245, 2017.

[32] A. L. Bertozzi and A. Flenner, “Diffuse interface models on graphs for
classiﬁcation of high dimensional data,” SIAM Review, vol. 58, no. 2,
pp. 293–328, 2016.

[33] C. Garcia-Cardona, E. Merkurjev, A. L. Bertozzi, A. Flenner, and A. G.
Percus, “Multiclass data segmentation using diffuse interface methods on
graphs,” IEEE transactions on pattern analysis and machine intelligence,
vol. 36, no. 8, pp. 1600–1613, 2014.

[34] E. Merkurjev, C. Garcia-Cardona, A. L. Bertozzi, A. Flenner, and A. G.
Percus, “Diffuse interface methods for multiclass segmentation of high-
dimensional data,” Applied Mathematics Letters, vol. 33, pp. 29–34,
2014.

[35] L. Zelnik-Manor and P. Perona, “Self-tuning spectral clustering,” in
Advances in neural information processing systems, 2005, pp. 1601–
1608.

[36] Y. Van Gennip, A. L. Bertozzi et al., “Γ-convergence of graph Ginzburg-
Landau functionals,” Advances in Differential Equations, vol. 17, no.
11/12, pp. 1115–1180, 2012.

[37] X. Luo and A. L. Bertozzi, “Convergence of the graph Allen–Cahn
scheme,” Journal of Statistical Physics, vol. 167, no. 3-4, pp. 934–958,
2017.

[38] Z. Meng, A. Koniges, Y. H. He, S. Williams, T. Kurth, B. Cook,
J. Deslippe, and A. L. Bertozzi, “OpenMP parallelization and opti-
mization of graph-based machine learning algorithms,” in International
Workshop on OpenMP. Springer, 2016, pp. 17–31.

[39] Y. Van Gennip, N. Guillen, B. Osting, and A. L. Bertozzi, “Mean
curvature, threshold dynamics, and phase ﬁeld theory on ﬁnite graphs,”
Milan Journal of Mathematics, vol. 82, no. 1, pp. 3–65, 2014.

[40] C. Fowlkes, S. Belongie, F. Chung, and J. Malik, “Spectral grouping
using the Nystr¨om method,” IEEE transactions on pattern analysis and
machine intelligence, vol. 26, no. 2, pp. 214–225, 2004.

[41] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
spatiotemporal features with 3D convolutional networks,” in Computer
Vision (ICCV), 2015 IEEE International Conference on.
IEEE, 2015,
pp. 4489–4497.

[42] A. Fathi, J. K. Hodgins, and J. M. Rehg, “Social

interactions: A
ﬁrst-person perspective,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1226–
1233.

12

TABLE V
CLASS PROPORTION, PRECISION , RECALL, AND ACCURACY ON THE
LAPD BODY-WORN VIDEO DATASET

Class

Proportion

[7]

Ours

[7]

Ours

Precision

Recall

Stand still
In stationary car
Walk
In moving car
Obscured camera
At car window
At car trunk
Exit driver
Exit passenger
Run
Bike
Enter passenger
Enter driver
Motorcycle

Mean
Accuracy

62.57%
16.84%
9.04%
5.76%
2.80%
0.64%
0.58%
0.35%
0.34%
0.33%
0.33%
0.20%
0.12%
0.08%

7.14%

73.10% 89.44% 85.42 % 95.24%
89.73%
41.83% 93.69%
59.41%
38.36% 70.53%
84.40%
70.71% 91.03%
70.46%
51.65% 80.82%
45.28%
17.23% 71.45%
51.78%
73.78% 71.79%
21.12%
50.25%
6.68%
26.29%
79.69% 48.08%
53.35%
96.15% 75.94%
75.44%
85.71% 86.49%
24.51%
45.82%
5.97%
20.91%
34.33%
5.72%
71.75%
92.49%
100%

43.18%
19.54%
25.08%
15.93%
10.94%
11.09%
11.82%
11.59%
11.03%
14.37%
13.27%
12.3%
10.76%

53.33% 71.58%
65.03% 88.15%

21.17%

56.41%

Honglin Chen Honglin Chen received the B.S.
degree in Mathematics of Computation from the
University of California, Los Angeles, CA, USA in
2018.

From 2015 to 2018, he was an undergraduate re-
search assistant with the Computer Vision & Graph-
ics Laboratory at UCLA and the Center for Brains,
Minds and machines (CBMM) at the Massachusetts
Institute of Technology under the supervision of
Dr. Demetri Terzopoulos and Dr. Tomaso Poggio
respectively. His research interest includes human
vision, computer vision, computer graphics, machine learning, deep learning,
and computational neuroscience.

Hao Li Hao Li received the B.S. degree in the
Mathematics of Computation and the M.A. degree in
Applied Mathematics at the University of California,
Los Angeles, CA, USA in 2016. He is currently
a third-year graduate student in the Department of
Mathematics at UCLA working toward his Ph.D.
degree.

He is a Research Assistant with UCLA since
2016 and graduate student mentor in the UCLA
Applied Math REU (Research Experience for Un-
dergraduates) in the summers from 2016 to 2018.
His research interests lie in numerical optimization and Bayesian uncertainty
quantiﬁcation with their applications to data analysis and machine learning.

(a) Method proposed in [7]

(b) Ours

Fig. 6. Confusion matrices for the LAPD police Body-worn video dataset.
The background intensity of cell (k, (cid:96)) corresponds to the number of data
points in class k that are classiﬁed as class (cid:96) by the algorithm.

[43] M. Jacobs, E. Merkurjev, and S. Esedolu, “Auction dynamics: A volume
constrained MBO scheme,” Journal of Computational Physics, vol. 354,
pp. 288–310, 2018.

[44] A. L. Bertozzi, X. Luo, A. M. Stuart, and K. C. Zygalakis, “Uncertainty
quantiﬁcation in graph-based classiﬁcation of high dimensional data,”
SIAM/ASA Journal on Uncertainty Quantiﬁcation, vol. 6, no. 2, pp. 568–
595, 2018.

VII. APPENDIX

We report the classiﬁcation results of the entire 14 ego-
activity categories in the LAPD body-worn video dataset in
TABLE V as well as the full confusion matrices in ﬁg. 6.

Alexander Song Alexander Song was born in Stock-
ton, CA, USA in 1991. He received a B.A. in phi-
losophy from the University of California, Berkeley,
CA, USA in 2013 and a B.S. in mathematics from
the University of California, Santa Barbara, CA,
USA in 2018.

In the summer of 2011, he worked as a Mathemat-
ics Tutor and Teaching Assistant for UC Berkeleys
Summer Bridge Program, a residential academic
program for incoming freshmen. He received a
2014-15 Fulbright grant to teach English and rep-
resent the United States as a cultural ambassador in South Korea. He was
an Undergraduate Researcher at UC Santa Barbaras 2016 Math Summer
Research Program, where he investigated structured linearizations of struc-
tured matrix polynomials in collaboration with two other undergraduates and
under the supervision of faculty mentors. The results of this collaboration
were published in the article Explicit Block-Structures for Block-Symmetric
Fiedler-like Pencils in the Electronic Journal of Linear Algebra in 2018. In the
summer of 2017, he was an Undergraduate Researcher at the UCLA Applied
Math REU (Research Experience for Undergraduates), conducting research
in semi-supervised learning and its application to ego-action classiﬁcation in
body-worn video. He is currently seeking a position in industry as a data
scientist.

He received a Cal Alumni Association Leadership Award, highest honors
in general scholarship upon graduation from UC Berkeley, and membership
in Phi Beta Kappa.

Matt Haberland Matt Haberland received the B.S.
and M.Eng. degrees in mechanical engineering from
Cornell University, Ithaca, NY in 2007 and the
Ph.D. degree in mechanical engineering from the
Massachusetts Institute of Technology, Cambridge,
MA in 2014.

From 2007 to 2009, he was an engineer at the
Jet Propulsion Laboratory in Pasadena, CA, where
he created the rock drill contact sensor/stabilizer for
the Mars rover Curiosity. From 2014 to 2018, he was
an Assistant Adjunct Professor in the Department of
Mathematics at the University of California, Los Angeles. Currently, he is
an Assistant Professor in the BioResource and Agricultural Engineering De-
partment at the California Polytechnic State University, San Luis Obispo. He
has published research in journals including IEEE Robotics and Automation
Letters, Bioinspiration and Biomimetics, and Robotica. His research interests
include agricultural automation, applications of machine learning, control of
legged robots, and linear programming.

Adam Dhillon Adam Dhillon is a senior undergrad-
uate student at Harvey Mudd College, Claremont,
CA, USA. He expects to receive his B.S. in mathe-
matics in May 2019. He will attend a Ph.D. program
in mathematics in Fall 2019.

He is interested in methods of optimizing travel
under uncertainty of conditions and approximating
optimal trajectories under randomly changing con-
ditions. He has also investigated the effectiveness
of statistics on circular data. He recently worked
on a notion of complexity based on the number of

integrators required to generate a function with an analog computer.

13

Tiankuang Zhou Tiankuang Zhou received the
B.Eng. in Electronic Information Engineering from
University of Science and Technology of China,
Hefei, China in 2018. He is currently a Ph.D.
student in Department of Automation in Tsinghua
University, Beijing, China. His research interests
include optical computing, computational imaging
and machine learning.

Osman Akar Osman Akar was born in Izmir,
Turkey in 1996. He is currently undergraduate stu-
dent at University of California, Los Angeles. He
expects to have his B.S. degree in Mathematics of
Computation and M.A. degree in Mathematics by
July 2019. He is also prospective Applied Mathe-
matics PhD student of UCLA. This is his second
paper, and he expects to study numerical PDEs and
image processing in his graduate studies.

He is a recipient of Mathematics Undergraduate
Merit Scholarship of UCLA Mathematics depart-
ment. His awards and honors include IMO Gold Medal and William L. Putnam
Competition Honorable Mention.

Andrea L. Bertozzi Andrea L. Bertozzi received the
B.A., M.A., and Ph.D. degrees in mathematics from
Princeton University, Princeton, NJ, USA, in 1987,
1988, and 1991, respectively.

She was on the faculty of the University of
Chicago, Chicago, IL, USA, from 1991 to 1995
and Duke University, Durham, NC, USA, from
1995 to 2004. During 19951996, she was the
Maria Goeppert-Mayer Distinguished Scholar with
Argonne National Laboratory. Since 2003, she has
been a Professor of mathematics with the University
of California, Los Angeles, where he currently serves as the Director of
Applied Mathematics.

In 2012, she was appointed the Betsy Wood Knapp Chair for Innovation and
Creativity. Her research interests include machine learning, image processing,
cooperative control of robotic vehicles, and swarming. Dr. Bertozzi is a
Fellow of the Society for Industrial and Applied Mathematics, the American
Mathematical Society, and the American Physical Society. She is a member of
the US National Academy of Sciences and a fellow of the American Academy
of Arts and Sciences.

P. Jeffrey Brantingham P. Jeffrey Brantingham re-
ceived the B.A. in Anthropology from the University
of British Columnbia, Vancouver, Canada in 1992,
the M.A. and Ph.D. in Anthropology from University
of Arizona, Tucson, AZ, USA, in 1996 and 1999
respectively.

During 2000-2002, he was a postdoctoral scholar
at Santa Fe Institute, NM, USA. Since 2002, he
has been a Professor of Anthropology with the
University of California, Los Angeles, CA, USA.

During 2010-2012, he Served on the LAPD
Community-Police Advisory Boards for Counter-Terrorism. He is a co-
founder of PredPol, a company that delivers real-time predictive policing to
law enforce agencies.

