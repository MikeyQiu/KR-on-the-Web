Understanding Back-Translation at Scale

Sergey Edunov△ Myle Ott△ Michael Auli△ David Grangier ▽∗
△Facebook AI Research, Menlo Park, CA & New York, NY.
▽Google Brain, Mountain View, CA.

8
1
0
2
 
t
c
O
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
1
8
3
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

An effective method to improve neural ma-
chine translation with monolingual data is
to augment the parallel training corpus with
back-translations of target language sentences.
This work broadens the understanding of
back-translation and investigates a number
of methods to generate synthetic source sen-
tences. We ﬁnd that in all but resource poor
settings back-translations obtained via sam-
pling or noised beam outputs are most effec-
tive. Our analysis shows that sampling or
noisy synthetic data gives a much stronger
training signal than data generated by beam or
greedy search. We also compare how synthetic
data compares to genuine bitext and study var-
ious domain effects. Finally, we scale to hun-
dreds of millions of monolingual sentences
and achieve a new state of the art of 35 BLEU
on the WMT’14 English-German test set.

1 Introduction

Machine translation relies on the statistics of large
parallel corpora, i.e. datasets of paired sentences
in both the source and target language. However,
bitext is limited and there is a much larger amount
of monolingual data available. Monolingual data
has been traditionally used to train language mod-
els which improved the ﬂuency of statistical ma-
chine translation (Koehn, 2010).

In the context of neural machine translation
(NMT; Bahdanau et al. 2015; Gehring et al. 2017;
Vaswani et al. 2017),
there has been extensive
work to improve models with monolingual data,
including language model fusion (Gulcehre et al.,
2015, 2017), back-translation (Sennrich et al.,
2016a) and dual
learning (Cheng et al., 2016;
He et al., 2016a). These methods have different
advantages and can be combined to reach high ac-
curacy (Hassan et al., 2018).

*Work done while at Facebook AI Research.

We focus on back-translation (BT) which
operates in a semi-supervised setup where both
bilingual and monolingual data in the target lan-
guage are available. Back-translation ﬁrst trains
an intermediate system on the parallel data which
is used to translate the target monolingual data into
the source language. The result is a parallel corpus
where the source side is synthetic machine transla-
tion output while the target is genuine text written
by humans. The synthetic parallel corpus is then
simply added to the real bitext in order to train
a ﬁnal system that will translate from the source
to the target language. Although simple,
this
method has been shown to be helpful for phrase-
based translation (Bojar and Tamchyna, 2011),
NMT (Sennrich et al., 2016a; Poncelas et al.,
2018) as well as unsupervised MT (Lample et al.,
2018a).

In this paper, we investigate back-translation
for neural machine translation at a large scale
by adding hundreds of millions of back-translated
sentences to the bitext. Our experiments are based
on strong baseline models trained on the public bi-
text of the WMT competition. We extend previous
analysis (Sennrich et al., 2016a; Poncelas et al.,
2018) of back-translation in several ways. We pro-
vide a comprehensive analysis of different meth-
ods to generate synthetic source sentences and we
show that this choice matters: sampling from the
model distribution or noising beam outputs out-
performs pure beam search, which is typically
used, by 1.7 BLEU on average across several test
sets. Our analysis shows that synthetic data based
on sampling and noised beam search provides a
stronger training signal than synthetic data based
on argmax inference. We also study how adding
synthetic data compares to adding real bitext in
a controlled setup with the surprising ﬁnding that
synthetic data can sometimes match the accuracy
of real bitext. Our best setup achieves 35 BLEU

on the WMT’14 English-German test set by rely-
ing only on public WMT bitext as well as 226M
monolingual sentences. This outperforms the sys-
tem of DeepL by 1.7 BLEU who train on large
amounts of high quality non-benchmark data. On
WMT’14 English-French we achieve 45.6 BLEU.

2 Related work

This section describes prior work in machine
translation with neural networks as well as semi-
supervised machine translation.

2.1 Neural machine translation

neural

Different

architectures

We build upon recent work on neural machine
translation which is typically a neural network
with an encoder/decoder architecture. The en-
coder infers a continuous space representation
of the source sentence, while the decoder is a
neural language model conditioned on the en-
coder output. The parameters of both models are
learned jointly to maximize the likelihood of the
target sentences given the corresponding source
sentences from a parallel corpus (Sutskever et al.,
2014; Cho et al., 2014). At inference, a target sen-
tence is generated by left-to-right decoding.
have

been
improving efﬁ-
proposed with the goal of
includes
ciency and/or effectiveness.
2014;
recurrent
Bahdanau et al., 2015; Luong et al., 2015), con-
volutional networks (Kalchbrenner et al., 2016;
Gehring et al., 2017; Kaiser et al., 2017) and
(Vaswani et al., 2017).
transformer
Recent work relies on attention mechanisms
where the encoder produces a sequence of
vectors and, for each target token, the decoder
attends to the most relevant part of the source
through a context-dependent weighted-sum of
the
(Bahdanau et al., 2015;
Luong et al., 2015). Attention has been reﬁned
with multi-hop attention (Gehring et al., 2017),
self-attention (Vaswani et al., 2017; Paulus et al.,
2018) and multi-head attention (Vaswani et al.,
2017). We use a transformer architecture
(Vaswani et al., 2017).

encoder vectors

(Sutskever et al.,

networks

networks

This

2.2 Semi-supervised NMT

Monolingual
target data has been used to im-
prove the ﬂuency of machine translations since the
early IBM models (Brown et al., 1990). In phrase-
based systems, language models (LM) in the tar-

get language increase the score of ﬂuent outputs
during decoding (Koehn et al., 2003; Brants et al.,
2007). A similar strategy can be applied to
NMT (He et al., 2016b). Besides improving ac-
curacy during decoding, neural LM and NMT can
beneﬁt from deeper integration, e.g. by combining
the hidden states of both models (Gulcehre et al.,
2017). Neural architecture also allows multi-task
learning and parameter sharing between MT and
target-side LM (Domhan and Hieber, 2017).

Back-translation (BT) is an alternative to lever-
age monolingual data. BT is simple and easy to
apply as it does not require modiﬁcation to the
MT training algorithms.
It requires training a
target-to-source system in order to generate ad-
ditional synthetic parallel data from the mono-
lingual target data. This data complements hu-
man bitext to train the desired source-to-target
system. BT has been applied earlier to phrase-
base systems (Bojar and Tamchyna, 2011). For
these systems, BT has also been successful in
leveraging monolingual data for domain adapta-
tion (Bertoldi and Federico, 2009; Lambert et al.,
2011). Recently, BT has been shown beneﬁcial
for NMT (Sennrich et al., 2016a; Poncelas et al.,
2018).
It has been found to be particularly use-
ful when parallel data is scarce (Karakanta et al.,
2017).

Currey et al. (2017) show that low resource lan-
guage pairs can also be improved with synthetic
data where the source is simply a copy of the
monolingual
target data. Concurrently to our
work, Imamura et al. (2018) show that sampling
synthetic sources is more effective than beam
search. Speciﬁcally, they sample multiple sources
for each target whereas we draw only a sin-
gle sample, opting to train on a larger number
of target sentences instead. Hoang et al. (2018)
and Cotterell and Kreutzer (2018) suggest an iter-
ative procedure which continuously improves the
quality of the back-translation and ﬁnal systems.
Niu et al. (2018) experiment with a multilingual
model that does both the forward and backward
translation which is continuously trained with new
synthetic data.

There has also been work using source-side
monolingual data (Zhang and Zong, 2016). Fur-
thermore, Cheng et al. (2016); He et al. (2016a);
text
Xia et al.
from both languages can be leveraged by ex-
tending back-translation to dual learning: when

(2017) show how monolingual

training both source-to-target and target-to-source
models jointly, one can use back-translation in
both directions and perform multiple rounds of
BT. A similar idea is applied in unsupervised
NMT (Lample et al., 2018a,b). Besides mono-
lingual data, various approaches have been in-
troduced to beneﬁt from parallel data in other
language pairs (Johnson et al., 2017; Firat et al.,
2016a,b; Ha et al., 2016; Gu et al., 2018).

Data augmentation is an established technique
in computer vision where a labeled dataset is sup-
plemented with cropped or rotated input images.
Recently, generative adversarial networks (GANs)
have been successfully used to the same end
(Antoniou et al., 2017; Perez and Wang, 2017) as
well as models that learn distributions over image
transformations (Hauberg et al., 2016).

3 Generating synthetic sources

Back-translation typically uses beam search
(Sennrich et al., 2016a) or
just greedy search
to generate synthetic
(Lample et al., 2018a,b)
Both are approximate al-
source sentences.
gorithms to identify the maximum a-posteriori
(MAP) output, i.e.
the sentence with the largest
estimated probability given an input. Beam is gen-
erally successful in ﬁnding high probability out-
puts (Ott et al., 2018a).

However, MAP prediction can lead to less rich
translations (Ott et al., 2018a) since it always fa-
vors the most likely alternative in case of ambi-
guity. This is particularly problematic in tasks
where there is a high level of uncertainty such
as dialog (Serban et al., 2016) and story genera-
tion (Fan et al., 2018). We argue that this is also
problematic for a data augmentation scheme such
as back-translation. Beam and greedy focus on
the head of the model distribution which results
in very regular synthetic source sentences that do
not properly cover the true data distribution.

As alternative, we consider sampling from the
model distribution as well as adding noise to beam
search outputs. First, we explore unrestricted sam-
pling which generates outputs that are very di-
verse but sometimes highly unlikely. Second, we
investigate sampling restricted to the most likely
words (Graves, 2013; Ott et al., 2018a; Fan et al.,
2018). At each time step, we select the k most
re-
likely tokens from the output distribution,
normalize and then sample from this restricted set.
This is a middle ground between MAP and unre-

stricted sampling.

(2018a)

As a third alternative, we apply noising
to beam search out-
Lample et al.
Adding noise to input sentences has
puts.
the autoencoder se-
for
been very beneﬁcial
tups of (Lample et al., 2018a; Hill et al., 2016)
which is inspired by denoising autoencoders
(Vincent et al., 2008). In particular, we transform
source sentences with three types of noise:
deleting words with probability 0.1, replacing
words by a ﬁller token with probability 0.1,
and swapping words which is implemented as a
random permutation over the tokens, drawn from
the uniform distribution but restricted to swapping
words no further than three positions apart.

4 Experimental setup

4.1 Datasets

The majority of our experiments are based on data
from the WMT’18 English-German news transla-
tion task. We train on all available bitext exclud-
ing the ParaCrawl corpus and remove sentences
longer than 250 words as well as sentence-pairs
with a source/target length ratio exceeding 1.5.
This results in 5.18M sentence pairs. For the back-
translation experiments we use the German mono-
lingual newscrawl data distributed with WMT’18
comprising 226M sentences after removing dupli-
cates. We tokenize all data with the Moses tok-
enizer (Koehn et al., 2007) and learn a joint source
and target Byte-Pair-Encoding (BPE; Sennrich et
al., 2016) with 35K types. We develop on new-
stest2012 and report ﬁnal results on newstest2013-
2017; additionally we consider a held-out set from
the training data of 52K sentence-pairs.

We also experiment on the larger WMT’14
English-French task which we ﬁlter in the same
way as WMT’18 English-German. This results in
35.7M sentence-pairs for training and we learn a
joint BPE vocabulary of 44K types. As monolin-
gual data we use newscrawl2010-2014, compris-
ing 31M sentences after language identiﬁcation
(Lui and Baldwin, 2012). We use newstest2012
as development set and report ﬁnal results on
newstest2013-2015.

The majority of results in this paper are in terms
of case-sensitive tokenized BLEU (Papineni et al.,
2002) but we also report test accuracy with de-
tokenized BLEU using sacreBLEU (Post, 2018).

4.2 Model and hyperparameters

We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. Dropout is
set to 0.3 for En-De and 0.1 for En-Fr, we use
16 attention heads, and we average the check-
points of the last ten epochs. Models are opti-
mized with Adam (Kingma and Ba, 2015) using
β1 = 0.9, β2 = 0.98, and ǫ = 1e − 8 and we use
the same learning rate schedule as Vaswani et al.
(2017). All models use label smoothing with a
uniform prior distribution over the vocabulary ǫ =
0.1 (Szegedy et al., 2015; Pereyra et al., 2017).
We run experiments on DGX-1 machines with 8
Nvidia V100 GPUs and machines are intercon-
nected by Inﬁniband. Experiments are run on 16
machines and we perform 30K synchronous up-
dates. We also use the NCCL2 library and the
torch distributed package for inter-GPU communi-
cation. We train models with 16-bit ﬂoating point
operations, following Ott et al. (2018b). For ﬁnal
evaluation, we generate translations with a beam
of size 5 and with no length penalty.

5 Results

Our evaluation ﬁrst compares the accuracy of
back-translation generation methods (§5.1) and
analyzes the results (§5.2). Next, we simulate a
low-resource setup to experiment further with dif-
ferent generation methods (§5.3). We also com-
pare synthetic bitext to genuine parallel data and
examine domain effects arising in back-translation
(§5.4). We also measure the effect of upsampling
bitext during training (§5.5). Finally, we scale to a
very large setup of up to 226M monolingual sen-
tences and compare to previous research (§5.6).

5.1 Synthetic data generation methods

We ﬁrst investigate different methods to gener-
ate synthetic source translations given a back-
translation model, i.e., a model trained in the re-
verse language direction (Section 3). We con-
sider two types of MAP prediction: greedy search
(greedy) and beam search with beam size 5
(beam). Non-MAP methods include unrestricted

1Code available at

https://github.com/pytorch/fairseq

25.5

25

24.5

24

23.5

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

greedy
top10
beam+noise

beam
sampling

5M

8M

11M
Total training data

17M

29M

Figure 1: Accuracy of models trained on dif-
ferent amounts of back-translated data obtained
with greedy search, beam search (k = 5), ran-
domly sampling from the model distribution, re-
stricting sampling over the ten most likely words
(top10), and by adding noise to the beam outputs
(beam+noise). Results based on newstest2012 of
WMT English-German translation.

sampling from the model distribution (sampling),
restricting sampling to the k highest scoring out-
puts at every time step with k = 10 (top10) as well
as adding noise to the beam outputs (beam+noise).
Restricted sampling is a middle-ground between
beam search and unrestricted sampling, it is less
likely to pick very low scoring outputs but still
preserves some randomness. Preliminary experi-
ments with top5, top20, top50 gave similar results
to top10.

We also vary the amount of synthetic data and
perform 30K updates during training for the bi-
text only, 50K updates when adding 3M synthetic
sentences, 75K updates for 6M and 12M sen-
tences and 100K updates for 24M sentences. For
each setting, this corresponds to enough updates to
reach convergence in terms of held-out loss. In our
128 GPU setup, training of the ﬁnal models takes
3h 20min for the bitext only model, 7h 30min for
6M and 12M synthetic sentences, and 10h 15min
for 24M sentences. During training we also sam-
ple the bitext more frequently than the synthetic
data and we analyze the effect of this in more de-
tail in §5.5.

Figure 1 shows that sampling and beam+noise
outperform the MAP methods (pure beam search
and greedy) by 0.8-1.1 BLEU. Sampling and
beam+noise improve over bitext-only (5M) by be-

news2013 news2014 news2015

news2016 news2017 Average

bitext

+ beam
+ greedy
+ top10
+ sampling
+ beam+noise

27.84

27.82
27.67
28.25
28.81
29.28

30.88

32.33
32.55
33.94
34.46
33.53

31.82

32.20
32.57
34.00
34.87
33.79

34.98

35.43
35.74
36.45
37.08
37.89

29.46

31.11
31.25
32.08
32.35
32.66

31.00

31.78
31.96
32.94
33.51
33.43

Table 1: Tokenized BLEU on various test sets of WMT English-German when adding 24M synthetic
sentence pairs obtained by various generation methods to a 5.2M sentence-pair bitext (cf. Figure 1).

6

5

4

3

2

y
t
i
x
e
l
p
r
e
p

g
n
i
n
i
a
r
T

greedy
top10
beam+noise

beam
sampling
bitext

1

20

40

60

80

100

epoch

Figure 2: Training perplexity (PPL) per epoch for
different synthetic data. We separately report PPL
on the synthetic data and the bitext. Bitext PPL is
averaged over all generation methods.

tween 1.7-2 BLEU in the largest data setting.
Restricted sampling (top10) performs better than
beam and greedy but is not as effective as unre-
stricted sampling (sampling) or beam+noise.

Table 1 shows results on a wider range of
Sampling and
test sets (newstest2013-2017).
beam+noise perform roughly equal and we adopt
sampling for the remaining experiments.

5.2 Analysis of generation methods

The previous experiment showed that synthetic
source sentences generated via sampling and beam
with noise perform signiﬁcantly better than those
obtained by pure MAP methods. Why is this?

Beam search focuses on very likely outputs
which reduces the diversity and richness of the
generated source translations. Adding noise to
beam outputs and sampling do not have this
problem: Noisy source sentences make it harder

human data
beam
sampling
top10
beam+noise

Perplexity

75.34
72.42
500.17
87.15
2823.73

Table 2: Perplexity of source data as assigned by a
language model (5-gram Kneser–Ney). Data gen-
erated by beam search is most predictable.

the target

to predict
translations which may
help learning, similar to denoising autoencoders
(Vincent et al., 2008). Sampling is known to better
approximate the data distribution which is richer
than the argmax model outputs (Ott et al., 2018a).
Therefore, sampling is also more likely to provide
a richer training signal than argmax sequences.

To get a better sense of the training signal pro-
vided by each method, we compare the loss on
the training data for each method. We report the
cross entropy loss averaged over all tokens and
separate the loss over the synthetic data and the
real bitext data. Speciﬁcally, we choose the setup
with 24M synthetic sentences. At the end of each
epoch we measure the loss over 500K sentence
pairs sub-sampled from the synthetic data as well
as an equally sized subset of the bitext. For each
generation method we choose the same sentences
except for the bitext which is disjoint from the syn-
thetic data. This means that losses over the syn-
thetic data are measured over the same target to-
kens because the generation methods only differ
in the source sentences. We found it helpful to up-
sample the frequency with which we observe the
bitext compared to the synthetic data (§5.5) but we
do not upsample for this experiment to keep condi-

source

reference
beam
sample

top10

beam+noise

Diese gegenstzlichen Auffassungen von Fairness liegen nicht nur der politischen Debatte
zugrunde.
These competing principles of fairness underlie not only the political debate.
These conﬂicting interpretations of fairness are not solely based on the political debate.
Mr President, these contradictory interpretations of fairness are not based solely on the
political debate.
Those conﬂicting interpretations of fairness are not solely at the heart of the political
debate.
conﬂicting BLANK interpretations BLANK are of not BLANK based on the political
debate.

Table 3: Example where sampling produces inadequate outputs. ”Mr President,” is not in the source.
BLANK means that a word has been replaced by a ﬁller token.

tions as similar as possible. We assume that when
the training loss is low, then the model can easily
ﬁt the training data without extracting much learn-
ing signal compared to data which is harder to ﬁt.
Figure 2 shows that synthetic data based on
greedy or beam is much easier to ﬁt compared to
data from sampling, top10, beam+noise and the
bitext. In fact, the perplexity on beam data falls
below 2 after only 5 epochs. Except for sampling,
we ﬁnd that the perplexity on the training data is
somewhat correlated to the end-model accuracy
(cf. Figure 1) and that all methods except sam-
pling have a lower loss than real bitext.

These results suggest that synthetic data ob-
tained with argmax inference does not provide
as rich a training signal as sampling or adding
noise. We conjecture that the regularity of syn-
thetic data obtained with argmax inference is not
optimal. Sampling and noised argmax both expose
the model to a wider range of source sentences
which makes the model more robust to reorder-
ing and substitutions that happen naturally, even if
the model of reordering and substitution through
noising is not very realistic.

Next we analyze the richness of synthetic out-
puts and train a language model on real human text
and score synthetic source sentences generated by
beam search, sampling, top10 and beam+noise.
We hypothesize that data that
is very regular
should be more predictable by the language model
and therefore receive low perplexity. We elimi-
nate a possible domain mismatch effect between
the language model training data and the synthetic
data by splitting the parallel corpus into three non-
overlapping parts:

1. On 640K sentences pairs, we train a back-

translation model,

2. On 4.1M sentence pairs, we take the source
side and train a 5-gram Kneser-Ney language
model (Heaﬁeld et al., 2013),

3. On the remaining 450K sentences, we apply
the back-translation system using beam, sam-
pling and top10 generation.

For the last set, we have genuine source sen-
tences as well as synthetic sources from different
generation techniques. We report the perplexity of
our language model on all versions of the source
data in Table 2. The results show that beam out-
puts receive higher probability by the language
model compared to sampling, beam+noise and
real source sentences. This indicates that beam
search outputs are not as rich as sampling outputs
or beam+noise. This lack of variability probably
explains in part why back-translations from pure
beam search provide a weaker training signal than
alternatives.

Closer inspection of the synthetic sources (Ta-
ble 3) reveals that sampled and noised beam out-
puts are sometimes not very adequate, much more
so than MAP outputs, e.g., sampling often in-
troduces target words which have no counterpart
in the source. This happens because sampling
sometimes picks highly unlikely outputs which are
harder to ﬁt (cf. Figure 2).

5.3 Low resource vs. high resource setup

The experiments so far are based on a setup with a
large bilingual corpus. However, in resource poor
settings the back-translation model is of much
lower quality. Are non-MAP methods still more

26

24

22

20

18

16

14

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

beam 80K
sampling 80K
beam 640K
sampling 640K
beam 5M
sampling 5M

80 K

160 K

320 K

640 K

1.2 M

2.6 M

5 M 8 M

11 M

17 M

29 M

Total training data

Figure 3: BLEU when adding synthetic data from
beam and sampling to bitext systems with 80K,
640K and 5M sentence pairs.

effective in such a setup? To answer this ques-
tion, we simulate such setups by sub-sampling
the training data to either 80K sentence-pairs or
640K sentence-pairs and then add synthetic data
from sampling and beam search. We compare
these smaller setups to our original 5.2M sen-
tence bitext conﬁguration. The accuracy of the
German-English back-translation systems steadily
increases with more training data: On new-
stest2012 we measure 13.5 BLEU for 80K bitext,
24.3 BLEU for 640K and 28.3 BLEU for 5M.

Figure 3 shows that sampling is more effective
than beam for larger setups (640K and 5.2M bi-
texts) while the opposite is true for resource poor
settings (80K bitext). This is likely because the
back-translations in the 80K setup are of very poor
quality and the noise of sampling and beam+noise
is too detrimental for this brittle low-resource set-
ting. When the setup is very small the very regu-
lar MAP outputs still provide useful training signal
while the noise from sampling becomes harmful.

5.4 Domain of synthetic data

Next, we turn to two different questions: How
does real human bitext compare to synthetic data
in terms of ﬁnal model accuracy? And how does
the domain of the monolingual data affect results?
To answer these questions, we subsample 640K
sentence-pairs of the bitext and train a back-
translation system on this set. To train a forward
model, we consider three alternative types of data

to add to this 640K training set. We either add:

• the remaining parallel data (bitext),

• the back-translated target side of the remain-

ing parallel data (BT-bitext),

• back-translated newscrawl data (BT-news).

The back-translated data is generated via sam-
pling. This setup allows us to compare synthetic
data to genuine data since BT-bitext and bitext
It also allows us to
share the same target side.
estimate the value of BT data for domain adap-
tation since the newscrawl corpus (BT-news) is
pure news whereas the bitext is a mixture of eu-
roparl and commoncrawl with only a small news-
commentary portion. To assess domain adaptation
effects, we measure accuracy on two held-out sets:

• newstest2012, i.e. pure newswire data.

• a held-out set of the WMT training data
(valid-mixed), which is a mixture of eu-
roparl, commoncrawl and the small news-
commentary portion.

Figure 4 shows the results on both validation
sets. Most strikingly, BT-news performs almost
as well as bitext on newstest2012 (Figure 4a) and
improves the baseline (640K) by 2.6 BLEU. BT-
bitext improves by 2.2 BLEU, achieving 83% of
the improvement with real bitext. This shows that
synthetic data can be nearly as effective as real hu-
man translated data when the domains match.

Figure 4b shows the accuracy on valid-mixed,
the mixed domain valid set. The accuracy of BT-
news is not as good as before since the domain of
the BT data and the test set do not match. How-
ever, BT-news still improves the baseline by up to
1.2 BLEU. On the other hand, BT-bitext matches
the domain of valid-mixed and improves by 2.7
BLEU. This trails the real bitext by only 1.3 BLEU
and corresponds to 67% of the gain achieved with
real human bitext.

In summary, synthetic data performs remark-
ably well, coming close to the improvements
achieved with real bitext for newswire test data,
or trailing real bitext by only 1.3 BLEU for valid-
mixed. In absence of a large parallel corpus for
news, back-translation therefore offers a simple,
yet very effective domain adaptation technique.

U
E
L
B

33

32

31

30

29

28

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

25

24

23

22

U
E
L
B

23

22

21

20

bitext
BT-bitext
BT-news

bitext
BT-bitext
BT-news

640K

1.28M

2.56M

5.19M

640K

1.28M

2.56M

5.19M

Amount of data

(a) newstest2012

Amount of data

(b) valid-mixed

Figure 4: Accuracy on (a) newstest2012 and (b) a mixed domain valid set when growing a 640K bitext
corpus with (i) real parallel data (bitext), (ii) a back-translated version of the target side of the bitext
(BT-bitext), (iii) or back-translated newscrawl data (BT-news).

5.5 Upsampling the bitext

We found it beneﬁcial to adjust the ratio of bitext
to synthetic data observed during training. In par-
ticular, we tuned the rate at which we sample data
from the bitext compared to synthetic data. For
example, in a setup of 5M bitext sentences and
10M synthetic sentences, an upsampling rate of 2
means that we double the frequency at which we
visit bitext, i.e. training batches contain on aver-
age an equal amount of bitext and synthetic data
as opposed to 1/3 bitext and 2/3 synthetic data.

Figure 5 shows the accuracy of various upsam-
pling rates for different generation methods in a
setup with 5M bitext sentences and 24M synthetic
sentences. Beam and greedy beneﬁt a lot from
higher rates which results in training more on the
bitext data. This is likely because synthetic beam
and greedy data does not provide as much training
signal as the bitext which has more variation and
is harder to ﬁt. On the other hand, sampling and
beam+noise require no upsampling of the bitext,
which is likely because the synthetic data is al-
ready hard enough to ﬁt and thus provides a strong
training signal (§5.2).

5.6 Large scale results

To conﬁrm our ﬁndings we experiment on
WMT’14 English-French translation where we
show results on newstest2013-2015. We augment
the large bitext of 35.7M sentence pairs by 31M
newscrawl sentences generated by sampling. To

greedy
beam
top10
sampling
beam+noise

1

2

4

8

bitext upsample rate

Figure 5: Accuracy when changing the rate at
which the bitext is upsampled during training.
Rates larger than one mean that the bitext is ob-
served more often than actually present in the
combined bitext and synthetic training corpus.

train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs; we do not up-
sample the bitext for this experiment. Table 4
shows tokenized BLEU and Table 5 shows deto-
kenized BLEU.2 To our knowledge, our baseline
is the best reported result in the literature for new-
stest2014, and back-translation further improves
upon this by 2.6 BLEU (tokenized).

2sacreBLEU signatures:

BLEU+case.mixed+lang.en-

fr+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.7

news13 news14 news15

detok. sacreBLEU3

news13 news14 news15

bitext
+sampling

36.97
37.85

42.90
45.60

39.92
43.95

Table 4: Tokenized BLEU on various test sets for
WMT English-French translation.

bitext
+sampling

35.30
36.13

41.03
43.84

38.31
40.91

Table 5: De-tokenized BLEU (sacreBLEU) on var-
ious test sets for WMT English-French.

Finally, for WMT English-German we train
on all 226M available monolingual training sen-
tences and perform 250K updates in 22.5 hours
on 128 GPUs. We upsample the bitext with a
rate of 16 so that we observe every bitext sentence
16 times more often than each monolingual sen-
tence. This results in a new state of the art of
35 BLEU on newstest2014 by using only WMT
benchmark data. For comparison, DeepL, a com-
mercial translation engine relying on high qual-
ity bilingual training data, achieves 33.3 tokenized
BLEU .4 Table 6 summarizes our results and com-
pares to other work in the literature. This shows
that back-translation with sampling can result in
high-quality translation models based on bench-
mark data only.

a. Gehring et al. (2017)
b. Vaswani et al. (2017)
c. Ahmed et al. (2017)
d. Shaw et al. (2018)

DeepL
Our result

En–De En–Fr
40.5
41.0
41.4
41.5

25.2
28.4
28.9
29.2

33.3
35.0
33.8

45.9
45.6
43.8

Table 6: BLEU on newstest2014 for WMT
English-German (En–De) and English-French
(En–Fr). The ﬁrst four results use only WMT
bitext (WMT’14, except for b, c, d in En–De
which train on WMT’16). DeepL uses propri-
etary high-quality bitext and our result relies on
back-translation with 226M newscrawl sentences
for En–De and 31M for En–Fr. We also show deto-
kenized BLEU (SacreBLEU).

baseline
+BT
+ensemble
+ﬁlter copies

news17 news18

29.36
32.66
33.31
33.35

42.38
44.94
46.39
46.53

% of source copies

0.56% 0.53%

Table 7: De-tokenized case-insensitive sacreBLEU
on WMT English-German newstest17 and new-
stest18.

6 Submission to WMT’18

described in §4.

This section describes our entry to the WMT’18
English-German news translation task which was
ranked #1 in the human evaluation (Bojar et al.,
2018). Our entry is based on the WMT English-
German models described in the previous section
(§5.6).
In particular, we ensembled six back-
translation models trained on all available bitext
plus 226M newscrawl sentences or 5.8B German
tokens. Four models used bitext upsample ratio
16, one model upsample ratio 32, and another one
upsample ratio 8. Upsample ratios differed be-
cause we reused models previously trained to tune
the upsample ratio. We did not use checkpoint av-
eraging. More details of our setup and data are

Ott et al. (2018a) showed that beam search
sometimes outputs source copies rather than tar-
get language translations. We replaced source
copies by the output of a model trained only on the
news-commentary portion of the WMT’18 task
(nc model). This model produced far fewer copies
since this dataset is less noisy. Outputs are deemed
to be a source copy if the Jaccard similarity be-
tween the source and the target unigrams exceeds
0.5. About 0.5% of outputs are identiﬁed as source
copies. We used newstest17 as a development set
to ﬁne tune ensemble size and model parameters.
Table 7 summarizes the effect of back-translation
data, ensembling and source copy ﬁltering.5

with SET ∈ {wmt13, wmt14/full, wmt15}

3sacreBLEU signatures:

BLEU+case.mixed+lang.en-

4https://www.deepl.com/press.html
5BLEU+case.lc+lang.en-

LANG+numrefs.1+smooth.exp+test.wmt14/full+
tok.13a+version.1.2.7 with LANG ∈ {de,fr}

de+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.11
with SET ∈ {wmt17, wmt18}

7 Conclusions and future work

Back-translation is a very effective data augmen-
tation technique for neural machine translation.
Generating synthetic sources by sampling or by
adding noise to beam outputs leads to higher ac-
curacy than argmax inference which is typically
used.
In particular, sampling and noised beam
outperforms pure beam by 1.7 BLEU on average
on newstest2013-2017 for WMT English-German
translation. Both methods provide a richer train-
ing signal for all but resource poor setups. We
also ﬁnd that synthetic data can achieve up to 83%
of the performance attainable with real bitext. Fi-
nally, we achieve a new state of the art result of 35
BLEU on the WMT’14 English-German test set
by using publicly available benchmark data only.
In future work, we would like to investigate
an end-to-end approach where the back-translation
model is optimized to output synthetic sources that
are most helpful to the ﬁnal forward model.

References

Karim Ahmed, Nitish Shirish Keskar, and Richard
Socher. 2017. Weighted transformer network for
machine translation. arxiv, 1711.02132.

Antreas Antoniou, Amos J. Storkey, and Harrison Ed-
wards. 2017. Data augmentation generative adver-
sarial networks. arXiv, abs/1711.04340.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation with
monolingual resources. In Workshop on Statistical
Machine Translation (WMT).

Ondrej Bojar and Ales Tamchyna. 2011.

Improving
translation model by monolingual data. In Workshop
on Statistical Machine Translation (WMT).

Ondˇrej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(WMT18). In Proceedings of the Third Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, Brussels, Belgium. Association for Computa-
tional Linguistics.

Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990.
A statistical approach to machine translation. Com-
putational Linguistics, 16:79–85.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Conference of the Association for Computational
Linguistics (ACL).

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Ryan Cotterell and Julia Kreutzer. 2018. Explain-
ing and generalizing back-translation through wake-
sleep. arXiv preprint arXiv:1806.04402.

Anna Currey, Antonio Valerio Miceli Barone, and Ken-
neth Heaﬁeld. 2017. Copied Monolingual Data Im-
proves Low-Resource Neural Machine Translation.
In Proc. of WMT.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine transla-
tion through multi-task learning. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Angela Fan, Yann Dauphin, and Mike Lewis. 2018.
In Confer-
Hierarchical neural story generation.
ence of the Association for Computational Linguis-
tics (ACL).

Orhan Firat, Kyunghyun Cho, and Yoshua Ben-
gio. 2016a. Multi-way, multilingual neural ma-
chine translation with a shared attention mecha-
nism. In Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).

Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman-Vural, and Kyunghyun Cho. 2016b.
Zero-resource translation with multi-lingual neu-
In Conference on Em-
ral machine translation.
pirical Methods in Natural Language Processing
(EMNLP).

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
In International
sequence to sequence learning.
Conference of Machine Learning (ICML).

Alex Graves. 2013. Generating sequences with recur-

rent neural networks. arXiv, 1308.0850.

Thorsten Brants, Ashok C. Popat, Peng Xu, Franz Josef
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Conference on Natural
Language Learning (CoNLL).

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor
O. K. Li. 2018. Universal neural machine transla-
tion for extremely low resource languages. arXiv,
1802.05368.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv, 1503.03535.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, and Yoshua Bengio. 2017. On integrating
a language model into neural machine translation.
Computer Speech & Language, 45:137–148.

Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. arXiv,
1611.04798.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary,
Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv, 1803.05567.

Soren Hauberg, Oren Freifeld, Anders Boesen Lindbo
Larsen, John W. Fisher, and Lars Kai Hansen. 2016.
Dreaming more data: Class-dependent distributions
over diffeomorphisms for learned data augmenta-
tion. In AISTATS.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016a. Dual learning
for machine translation. In Conference on Advances
in Neural Information Processing Systems (NIPS).

Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016b.
Improved neural machine translation with
smt features. In Conference of the Association for
the Advancement of Artiﬁcial Intelligence (AAAI),
pages 151–157.

Kenneth Heaﬁeld,

Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modiﬁed
Kneser-Ney Language Model Estimation. In Con-
ference of the Association for Computational Lin-
guistics (ACL).

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
In Conference of the North
from unlabelled data.
American Chapter of the Association for Computa-
tional Linguistics (NAACL).

Vu Cong Duy Hoang, Philipp Koehn, Gholamreza
Haffari, and Trevor Cohn. 2018.
Iterative back-
translation for neural machine translation. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, pages 18–24.

Kenji Imamura, Atsushi Fujita, and Eiichiro Sumita.
2018. Enhancement of encoder and attention using
target monolingual corpora in neural machine trans-
lation. In Proceedings of the 2nd Workshop on Neu-
ral Machine Translation and Generation, pages 55–
63.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Vi´egas, Martin Wattenberg, Gre-
gory S. Corrado, Macduff Hughes, and Jeffrey Dean.
2017. Google’s multilingual neural machine transla-
tion system: Enabling zero-shot translation. Trans-
actions of the Association for Computational Lin-
guistics (TACL), 5:339–351.

Lukasz Kaiser, Aidan N. Gomez, and Franc¸ois Chollet.
2017. Depthwise separable convolutions for neural
machine translation. CoRR, abs/1706.03059.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
A¨aron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. CoRR, abs/1610.10099.

Alina Karakanta, Jon Dehdari, and Josef van Genabith.
2017. Neural machine translation for low-resource
languages without parallel corpora. Machine Trans-
lation, pages 1–23.

Diederik P. Kingma and Jimmy Ba. 2015. Adam:
In Inter-
A Method for Stochastic Optimization.
national Conference on Learning Representations
(ICLR).

Philipp Koehn. 2010. Statistical machine translation.

Cambridge University Press.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL Demo Session.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011.
Investigations
on translation model adaptation using monolingual
data. In Workshop on Statistical Machine Transla-
tion (WMT).

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In International Conference on Learning Represen-
tations (ICLR).

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine trans-
lation. arXiv, 1803.05567.

Marco Lui and Timothy Baldwin. 2012. langid. py: An
In Pro-
off-the-shelf language identiﬁcation tool.
ceedings of the ACL 2012 system demonstrations,
pages 25–30. Association for Computational Lin-
guistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proc. of NAACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
In Conference on Advances in Neural In-
works.
formation Processing Systems (NIPS).

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2015. Re-
thinking the Inception Architecture for Computer
Vision. arXiv preprint arXiv:1512.00567.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Conference on Advances in Neural In-
formation Processing Systems (NIPS).

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,

,
and Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
In International Conference on Machine
coders.
Learning (ICML).

Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai
Yu, and Tie-Yan Liu. 2017. Dual supervised learn-
ing. In International Conference on Machine Learn-
ing (ICML).

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Xing Niu, Michael Denkowski, and Marine Carpuat.
Bi-directional neural machine transla-
arXiv preprint

2018.
tion with synthetic parallel data.
arXiv:1805.11213.

Myle Ott, Michael Auli, David Grangier,

and
Marc’Aurelio Ranzato. 2018a. Analyzing uncer-
In Proceed-
tainty in neural machine translation.
ings of the 35th International Conference on Ma-
chine Learning, volume 80, pages 3956–3965.

Myle Ott, Sergey Edunov, David Grangier, and
Michael Auli. 2018b. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation: Research Papers.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
In Conference
evaluation of machine translation.
of the Association for Computational Linguistics
(ACL).

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In International Conference on Learn-
ing Representations (ICLR).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Lukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-
ularizing neural networks by penalizing conﬁdent
output distributions. In International Conference on
Learning Representations (ICLR) Workshop.

Luis Perez and Jason Wang. 2017. The effectiveness of
data augmentation in image classiﬁcation using deep
learning. arxiv, 1712.04621.

Alberto Poncelas, Dimitar Sht. Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Peyman
Passban. 2018. Investigating backtranslation in neu-
ral machine translation. arXiv, 1804.06189.

Matt Post. 2018. A call for clarity in reporting bleu

scores. arXiv, 1804.08771.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. Conference of the Asso-
ciation for Computational Linguistics (ACL).

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Conference of the Associa-
tion for Computational Linguistics (ACL).

Iulian Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron C. Courville, and Joelle Pineau. 2016. Build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. In Conference of
the Association for the Advancement of Artiﬁcial In-
telligence (AAAI).

Understanding Back-Translation at Scale

Sergey Edunov△ Myle Ott△ Michael Auli△ David Grangier ▽∗
△Facebook AI Research, Menlo Park, CA & New York, NY.
▽Google Brain, Mountain View, CA.

8
1
0
2
 
t
c
O
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
1
8
3
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

An effective method to improve neural ma-
chine translation with monolingual data is
to augment the parallel training corpus with
back-translations of target language sentences.
This work broadens the understanding of
back-translation and investigates a number
of methods to generate synthetic source sen-
tences. We ﬁnd that in all but resource poor
settings back-translations obtained via sam-
pling or noised beam outputs are most effec-
tive. Our analysis shows that sampling or
noisy synthetic data gives a much stronger
training signal than data generated by beam or
greedy search. We also compare how synthetic
data compares to genuine bitext and study var-
ious domain effects. Finally, we scale to hun-
dreds of millions of monolingual sentences
and achieve a new state of the art of 35 BLEU
on the WMT’14 English-German test set.

1 Introduction

Machine translation relies on the statistics of large
parallel corpora, i.e. datasets of paired sentences
in both the source and target language. However,
bitext is limited and there is a much larger amount
of monolingual data available. Monolingual data
has been traditionally used to train language mod-
els which improved the ﬂuency of statistical ma-
chine translation (Koehn, 2010).

In the context of neural machine translation
(NMT; Bahdanau et al. 2015; Gehring et al. 2017;
Vaswani et al. 2017),
there has been extensive
work to improve models with monolingual data,
including language model fusion (Gulcehre et al.,
2015, 2017), back-translation (Sennrich et al.,
2016a) and dual
learning (Cheng et al., 2016;
He et al., 2016a). These methods have different
advantages and can be combined to reach high ac-
curacy (Hassan et al., 2018).

*Work done while at Facebook AI Research.

We focus on back-translation (BT) which
operates in a semi-supervised setup where both
bilingual and monolingual data in the target lan-
guage are available. Back-translation ﬁrst trains
an intermediate system on the parallel data which
is used to translate the target monolingual data into
the source language. The result is a parallel corpus
where the source side is synthetic machine transla-
tion output while the target is genuine text written
by humans. The synthetic parallel corpus is then
simply added to the real bitext in order to train
a ﬁnal system that will translate from the source
to the target language. Although simple,
this
method has been shown to be helpful for phrase-
based translation (Bojar and Tamchyna, 2011),
NMT (Sennrich et al., 2016a; Poncelas et al.,
2018) as well as unsupervised MT (Lample et al.,
2018a).

In this paper, we investigate back-translation
for neural machine translation at a large scale
by adding hundreds of millions of back-translated
sentences to the bitext. Our experiments are based
on strong baseline models trained on the public bi-
text of the WMT competition. We extend previous
analysis (Sennrich et al., 2016a; Poncelas et al.,
2018) of back-translation in several ways. We pro-
vide a comprehensive analysis of different meth-
ods to generate synthetic source sentences and we
show that this choice matters: sampling from the
model distribution or noising beam outputs out-
performs pure beam search, which is typically
used, by 1.7 BLEU on average across several test
sets. Our analysis shows that synthetic data based
on sampling and noised beam search provides a
stronger training signal than synthetic data based
on argmax inference. We also study how adding
synthetic data compares to adding real bitext in
a controlled setup with the surprising ﬁnding that
synthetic data can sometimes match the accuracy
of real bitext. Our best setup achieves 35 BLEU

on the WMT’14 English-German test set by rely-
ing only on public WMT bitext as well as 226M
monolingual sentences. This outperforms the sys-
tem of DeepL by 1.7 BLEU who train on large
amounts of high quality non-benchmark data. On
WMT’14 English-French we achieve 45.6 BLEU.

2 Related work

This section describes prior work in machine
translation with neural networks as well as semi-
supervised machine translation.

2.1 Neural machine translation

neural

Different

architectures

We build upon recent work on neural machine
translation which is typically a neural network
with an encoder/decoder architecture. The en-
coder infers a continuous space representation
of the source sentence, while the decoder is a
neural language model conditioned on the en-
coder output. The parameters of both models are
learned jointly to maximize the likelihood of the
target sentences given the corresponding source
sentences from a parallel corpus (Sutskever et al.,
2014; Cho et al., 2014). At inference, a target sen-
tence is generated by left-to-right decoding.
have

been
improving efﬁ-
proposed with the goal of
includes
ciency and/or effectiveness.
2014;
recurrent
Bahdanau et al., 2015; Luong et al., 2015), con-
volutional networks (Kalchbrenner et al., 2016;
Gehring et al., 2017; Kaiser et al., 2017) and
(Vaswani et al., 2017).
transformer
Recent work relies on attention mechanisms
where the encoder produces a sequence of
vectors and, for each target token, the decoder
attends to the most relevant part of the source
through a context-dependent weighted-sum of
the
(Bahdanau et al., 2015;
Luong et al., 2015). Attention has been reﬁned
with multi-hop attention (Gehring et al., 2017),
self-attention (Vaswani et al., 2017; Paulus et al.,
2018) and multi-head attention (Vaswani et al.,
2017). We use a transformer architecture
(Vaswani et al., 2017).

encoder vectors

(Sutskever et al.,

networks

networks

This

2.2 Semi-supervised NMT

Monolingual
target data has been used to im-
prove the ﬂuency of machine translations since the
early IBM models (Brown et al., 1990). In phrase-
based systems, language models (LM) in the tar-

get language increase the score of ﬂuent outputs
during decoding (Koehn et al., 2003; Brants et al.,
2007). A similar strategy can be applied to
NMT (He et al., 2016b). Besides improving ac-
curacy during decoding, neural LM and NMT can
beneﬁt from deeper integration, e.g. by combining
the hidden states of both models (Gulcehre et al.,
2017). Neural architecture also allows multi-task
learning and parameter sharing between MT and
target-side LM (Domhan and Hieber, 2017).

Back-translation (BT) is an alternative to lever-
age monolingual data. BT is simple and easy to
apply as it does not require modiﬁcation to the
MT training algorithms.
It requires training a
target-to-source system in order to generate ad-
ditional synthetic parallel data from the mono-
lingual target data. This data complements hu-
man bitext to train the desired source-to-target
system. BT has been applied earlier to phrase-
base systems (Bojar and Tamchyna, 2011). For
these systems, BT has also been successful in
leveraging monolingual data for domain adapta-
tion (Bertoldi and Federico, 2009; Lambert et al.,
2011). Recently, BT has been shown beneﬁcial
for NMT (Sennrich et al., 2016a; Poncelas et al.,
2018).
It has been found to be particularly use-
ful when parallel data is scarce (Karakanta et al.,
2017).

Currey et al. (2017) show that low resource lan-
guage pairs can also be improved with synthetic
data where the source is simply a copy of the
monolingual
target data. Concurrently to our
work, Imamura et al. (2018) show that sampling
synthetic sources is more effective than beam
search. Speciﬁcally, they sample multiple sources
for each target whereas we draw only a sin-
gle sample, opting to train on a larger number
of target sentences instead. Hoang et al. (2018)
and Cotterell and Kreutzer (2018) suggest an iter-
ative procedure which continuously improves the
quality of the back-translation and ﬁnal systems.
Niu et al. (2018) experiment with a multilingual
model that does both the forward and backward
translation which is continuously trained with new
synthetic data.

There has also been work using source-side
monolingual data (Zhang and Zong, 2016). Fur-
thermore, Cheng et al. (2016); He et al. (2016a);
text
Xia et al.
from both languages can be leveraged by ex-
tending back-translation to dual learning: when

(2017) show how monolingual

training both source-to-target and target-to-source
models jointly, one can use back-translation in
both directions and perform multiple rounds of
BT. A similar idea is applied in unsupervised
NMT (Lample et al., 2018a,b). Besides mono-
lingual data, various approaches have been in-
troduced to beneﬁt from parallel data in other
language pairs (Johnson et al., 2017; Firat et al.,
2016a,b; Ha et al., 2016; Gu et al., 2018).

Data augmentation is an established technique
in computer vision where a labeled dataset is sup-
plemented with cropped or rotated input images.
Recently, generative adversarial networks (GANs)
have been successfully used to the same end
(Antoniou et al., 2017; Perez and Wang, 2017) as
well as models that learn distributions over image
transformations (Hauberg et al., 2016).

3 Generating synthetic sources

Back-translation typically uses beam search
(Sennrich et al., 2016a) or
just greedy search
to generate synthetic
(Lample et al., 2018a,b)
Both are approximate al-
source sentences.
gorithms to identify the maximum a-posteriori
(MAP) output, i.e.
the sentence with the largest
estimated probability given an input. Beam is gen-
erally successful in ﬁnding high probability out-
puts (Ott et al., 2018a).

However, MAP prediction can lead to less rich
translations (Ott et al., 2018a) since it always fa-
vors the most likely alternative in case of ambi-
guity. This is particularly problematic in tasks
where there is a high level of uncertainty such
as dialog (Serban et al., 2016) and story genera-
tion (Fan et al., 2018). We argue that this is also
problematic for a data augmentation scheme such
as back-translation. Beam and greedy focus on
the head of the model distribution which results
in very regular synthetic source sentences that do
not properly cover the true data distribution.

As alternative, we consider sampling from the
model distribution as well as adding noise to beam
search outputs. First, we explore unrestricted sam-
pling which generates outputs that are very di-
verse but sometimes highly unlikely. Second, we
investigate sampling restricted to the most likely
words (Graves, 2013; Ott et al., 2018a; Fan et al.,
2018). At each time step, we select the k most
re-
likely tokens from the output distribution,
normalize and then sample from this restricted set.
This is a middle ground between MAP and unre-

stricted sampling.

(2018a)

As a third alternative, we apply noising
to beam search out-
Lample et al.
Adding noise to input sentences has
puts.
the autoencoder se-
for
been very beneﬁcial
tups of (Lample et al., 2018a; Hill et al., 2016)
which is inspired by denoising autoencoders
(Vincent et al., 2008). In particular, we transform
source sentences with three types of noise:
deleting words with probability 0.1, replacing
words by a ﬁller token with probability 0.1,
and swapping words which is implemented as a
random permutation over the tokens, drawn from
the uniform distribution but restricted to swapping
words no further than three positions apart.

4 Experimental setup

4.1 Datasets

The majority of our experiments are based on data
from the WMT’18 English-German news transla-
tion task. We train on all available bitext exclud-
ing the ParaCrawl corpus and remove sentences
longer than 250 words as well as sentence-pairs
with a source/target length ratio exceeding 1.5.
This results in 5.18M sentence pairs. For the back-
translation experiments we use the German mono-
lingual newscrawl data distributed with WMT’18
comprising 226M sentences after removing dupli-
cates. We tokenize all data with the Moses tok-
enizer (Koehn et al., 2007) and learn a joint source
and target Byte-Pair-Encoding (BPE; Sennrich et
al., 2016) with 35K types. We develop on new-
stest2012 and report ﬁnal results on newstest2013-
2017; additionally we consider a held-out set from
the training data of 52K sentence-pairs.

We also experiment on the larger WMT’14
English-French task which we ﬁlter in the same
way as WMT’18 English-German. This results in
35.7M sentence-pairs for training and we learn a
joint BPE vocabulary of 44K types. As monolin-
gual data we use newscrawl2010-2014, compris-
ing 31M sentences after language identiﬁcation
(Lui and Baldwin, 2012). We use newstest2012
as development set and report ﬁnal results on
newstest2013-2015.

The majority of results in this paper are in terms
of case-sensitive tokenized BLEU (Papineni et al.,
2002) but we also report test accuracy with de-
tokenized BLEU using sacreBLEU (Post, 2018).

4.2 Model and hyperparameters

We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. Dropout is
set to 0.3 for En-De and 0.1 for En-Fr, we use
16 attention heads, and we average the check-
points of the last ten epochs. Models are opti-
mized with Adam (Kingma and Ba, 2015) using
β1 = 0.9, β2 = 0.98, and ǫ = 1e − 8 and we use
the same learning rate schedule as Vaswani et al.
(2017). All models use label smoothing with a
uniform prior distribution over the vocabulary ǫ =
0.1 (Szegedy et al., 2015; Pereyra et al., 2017).
We run experiments on DGX-1 machines with 8
Nvidia V100 GPUs and machines are intercon-
nected by Inﬁniband. Experiments are run on 16
machines and we perform 30K synchronous up-
dates. We also use the NCCL2 library and the
torch distributed package for inter-GPU communi-
cation. We train models with 16-bit ﬂoating point
operations, following Ott et al. (2018b). For ﬁnal
evaluation, we generate translations with a beam
of size 5 and with no length penalty.

5 Results

Our evaluation ﬁrst compares the accuracy of
back-translation generation methods (§5.1) and
analyzes the results (§5.2). Next, we simulate a
low-resource setup to experiment further with dif-
ferent generation methods (§5.3). We also com-
pare synthetic bitext to genuine parallel data and
examine domain effects arising in back-translation
(§5.4). We also measure the effect of upsampling
bitext during training (§5.5). Finally, we scale to a
very large setup of up to 226M monolingual sen-
tences and compare to previous research (§5.6).

5.1 Synthetic data generation methods

We ﬁrst investigate different methods to gener-
ate synthetic source translations given a back-
translation model, i.e., a model trained in the re-
verse language direction (Section 3). We con-
sider two types of MAP prediction: greedy search
(greedy) and beam search with beam size 5
(beam). Non-MAP methods include unrestricted

1Code available at

https://github.com/pytorch/fairseq

25.5

25

24.5

24

23.5

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

greedy
top10
beam+noise

beam
sampling

5M

8M

11M
Total training data

17M

29M

Figure 1: Accuracy of models trained on dif-
ferent amounts of back-translated data obtained
with greedy search, beam search (k = 5), ran-
domly sampling from the model distribution, re-
stricting sampling over the ten most likely words
(top10), and by adding noise to the beam outputs
(beam+noise). Results based on newstest2012 of
WMT English-German translation.

sampling from the model distribution (sampling),
restricting sampling to the k highest scoring out-
puts at every time step with k = 10 (top10) as well
as adding noise to the beam outputs (beam+noise).
Restricted sampling is a middle-ground between
beam search and unrestricted sampling, it is less
likely to pick very low scoring outputs but still
preserves some randomness. Preliminary experi-
ments with top5, top20, top50 gave similar results
to top10.

We also vary the amount of synthetic data and
perform 30K updates during training for the bi-
text only, 50K updates when adding 3M synthetic
sentences, 75K updates for 6M and 12M sen-
tences and 100K updates for 24M sentences. For
each setting, this corresponds to enough updates to
reach convergence in terms of held-out loss. In our
128 GPU setup, training of the ﬁnal models takes
3h 20min for the bitext only model, 7h 30min for
6M and 12M synthetic sentences, and 10h 15min
for 24M sentences. During training we also sam-
ple the bitext more frequently than the synthetic
data and we analyze the effect of this in more de-
tail in §5.5.

Figure 1 shows that sampling and beam+noise
outperform the MAP methods (pure beam search
and greedy) by 0.8-1.1 BLEU. Sampling and
beam+noise improve over bitext-only (5M) by be-

news2013 news2014 news2015

news2016 news2017 Average

bitext

+ beam
+ greedy
+ top10
+ sampling
+ beam+noise

27.84

27.82
27.67
28.25
28.81
29.28

30.88

32.33
32.55
33.94
34.46
33.53

31.82

32.20
32.57
34.00
34.87
33.79

34.98

35.43
35.74
36.45
37.08
37.89

29.46

31.11
31.25
32.08
32.35
32.66

31.00

31.78
31.96
32.94
33.51
33.43

Table 1: Tokenized BLEU on various test sets of WMT English-German when adding 24M synthetic
sentence pairs obtained by various generation methods to a 5.2M sentence-pair bitext (cf. Figure 1).

6

5

4

3

2

y
t
i
x
e
l
p
r
e
p

g
n
i
n
i
a
r
T

greedy
top10
beam+noise

beam
sampling
bitext

1

20

40

60

80

100

epoch

Figure 2: Training perplexity (PPL) per epoch for
different synthetic data. We separately report PPL
on the synthetic data and the bitext. Bitext PPL is
averaged over all generation methods.

tween 1.7-2 BLEU in the largest data setting.
Restricted sampling (top10) performs better than
beam and greedy but is not as effective as unre-
stricted sampling (sampling) or beam+noise.

Table 1 shows results on a wider range of
Sampling and
test sets (newstest2013-2017).
beam+noise perform roughly equal and we adopt
sampling for the remaining experiments.

5.2 Analysis of generation methods

The previous experiment showed that synthetic
source sentences generated via sampling and beam
with noise perform signiﬁcantly better than those
obtained by pure MAP methods. Why is this?

Beam search focuses on very likely outputs
which reduces the diversity and richness of the
generated source translations. Adding noise to
beam outputs and sampling do not have this
problem: Noisy source sentences make it harder

human data
beam
sampling
top10
beam+noise

Perplexity

75.34
72.42
500.17
87.15
2823.73

Table 2: Perplexity of source data as assigned by a
language model (5-gram Kneser–Ney). Data gen-
erated by beam search is most predictable.

the target

to predict
translations which may
help learning, similar to denoising autoencoders
(Vincent et al., 2008). Sampling is known to better
approximate the data distribution which is richer
than the argmax model outputs (Ott et al., 2018a).
Therefore, sampling is also more likely to provide
a richer training signal than argmax sequences.

To get a better sense of the training signal pro-
vided by each method, we compare the loss on
the training data for each method. We report the
cross entropy loss averaged over all tokens and
separate the loss over the synthetic data and the
real bitext data. Speciﬁcally, we choose the setup
with 24M synthetic sentences. At the end of each
epoch we measure the loss over 500K sentence
pairs sub-sampled from the synthetic data as well
as an equally sized subset of the bitext. For each
generation method we choose the same sentences
except for the bitext which is disjoint from the syn-
thetic data. This means that losses over the syn-
thetic data are measured over the same target to-
kens because the generation methods only differ
in the source sentences. We found it helpful to up-
sample the frequency with which we observe the
bitext compared to the synthetic data (§5.5) but we
do not upsample for this experiment to keep condi-

source

reference
beam
sample

top10

beam+noise

Diese gegenstzlichen Auffassungen von Fairness liegen nicht nur der politischen Debatte
zugrunde.
These competing principles of fairness underlie not only the political debate.
These conﬂicting interpretations of fairness are not solely based on the political debate.
Mr President, these contradictory interpretations of fairness are not based solely on the
political debate.
Those conﬂicting interpretations of fairness are not solely at the heart of the political
debate.
conﬂicting BLANK interpretations BLANK are of not BLANK based on the political
debate.

Table 3: Example where sampling produces inadequate outputs. ”Mr President,” is not in the source.
BLANK means that a word has been replaced by a ﬁller token.

tions as similar as possible. We assume that when
the training loss is low, then the model can easily
ﬁt the training data without extracting much learn-
ing signal compared to data which is harder to ﬁt.
Figure 2 shows that synthetic data based on
greedy or beam is much easier to ﬁt compared to
data from sampling, top10, beam+noise and the
bitext. In fact, the perplexity on beam data falls
below 2 after only 5 epochs. Except for sampling,
we ﬁnd that the perplexity on the training data is
somewhat correlated to the end-model accuracy
(cf. Figure 1) and that all methods except sam-
pling have a lower loss than real bitext.

These results suggest that synthetic data ob-
tained with argmax inference does not provide
as rich a training signal as sampling or adding
noise. We conjecture that the regularity of syn-
thetic data obtained with argmax inference is not
optimal. Sampling and noised argmax both expose
the model to a wider range of source sentences
which makes the model more robust to reorder-
ing and substitutions that happen naturally, even if
the model of reordering and substitution through
noising is not very realistic.

Next we analyze the richness of synthetic out-
puts and train a language model on real human text
and score synthetic source sentences generated by
beam search, sampling, top10 and beam+noise.
We hypothesize that data that
is very regular
should be more predictable by the language model
and therefore receive low perplexity. We elimi-
nate a possible domain mismatch effect between
the language model training data and the synthetic
data by splitting the parallel corpus into three non-
overlapping parts:

1. On 640K sentences pairs, we train a back-

translation model,

2. On 4.1M sentence pairs, we take the source
side and train a 5-gram Kneser-Ney language
model (Heaﬁeld et al., 2013),

3. On the remaining 450K sentences, we apply
the back-translation system using beam, sam-
pling and top10 generation.

For the last set, we have genuine source sen-
tences as well as synthetic sources from different
generation techniques. We report the perplexity of
our language model on all versions of the source
data in Table 2. The results show that beam out-
puts receive higher probability by the language
model compared to sampling, beam+noise and
real source sentences. This indicates that beam
search outputs are not as rich as sampling outputs
or beam+noise. This lack of variability probably
explains in part why back-translations from pure
beam search provide a weaker training signal than
alternatives.

Closer inspection of the synthetic sources (Ta-
ble 3) reveals that sampled and noised beam out-
puts are sometimes not very adequate, much more
so than MAP outputs, e.g., sampling often in-
troduces target words which have no counterpart
in the source. This happens because sampling
sometimes picks highly unlikely outputs which are
harder to ﬁt (cf. Figure 2).

5.3 Low resource vs. high resource setup

The experiments so far are based on a setup with a
large bilingual corpus. However, in resource poor
settings the back-translation model is of much
lower quality. Are non-MAP methods still more

26

24

22

20

18

16

14

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

beam 80K
sampling 80K
beam 640K
sampling 640K
beam 5M
sampling 5M

80 K

160 K

320 K

640 K

1.2 M

2.6 M

5 M 8 M

11 M

17 M

29 M

Total training data

Figure 3: BLEU when adding synthetic data from
beam and sampling to bitext systems with 80K,
640K and 5M sentence pairs.

effective in such a setup? To answer this ques-
tion, we simulate such setups by sub-sampling
the training data to either 80K sentence-pairs or
640K sentence-pairs and then add synthetic data
from sampling and beam search. We compare
these smaller setups to our original 5.2M sen-
tence bitext conﬁguration. The accuracy of the
German-English back-translation systems steadily
increases with more training data: On new-
stest2012 we measure 13.5 BLEU for 80K bitext,
24.3 BLEU for 640K and 28.3 BLEU for 5M.

Figure 3 shows that sampling is more effective
than beam for larger setups (640K and 5.2M bi-
texts) while the opposite is true for resource poor
settings (80K bitext). This is likely because the
back-translations in the 80K setup are of very poor
quality and the noise of sampling and beam+noise
is too detrimental for this brittle low-resource set-
ting. When the setup is very small the very regu-
lar MAP outputs still provide useful training signal
while the noise from sampling becomes harmful.

5.4 Domain of synthetic data

Next, we turn to two different questions: How
does real human bitext compare to synthetic data
in terms of ﬁnal model accuracy? And how does
the domain of the monolingual data affect results?
To answer these questions, we subsample 640K
sentence-pairs of the bitext and train a back-
translation system on this set. To train a forward
model, we consider three alternative types of data

to add to this 640K training set. We either add:

• the remaining parallel data (bitext),

• the back-translated target side of the remain-

ing parallel data (BT-bitext),

• back-translated newscrawl data (BT-news).

The back-translated data is generated via sam-
pling. This setup allows us to compare synthetic
data to genuine data since BT-bitext and bitext
It also allows us to
share the same target side.
estimate the value of BT data for domain adap-
tation since the newscrawl corpus (BT-news) is
pure news whereas the bitext is a mixture of eu-
roparl and commoncrawl with only a small news-
commentary portion. To assess domain adaptation
effects, we measure accuracy on two held-out sets:

• newstest2012, i.e. pure newswire data.

• a held-out set of the WMT training data
(valid-mixed), which is a mixture of eu-
roparl, commoncrawl and the small news-
commentary portion.

Figure 4 shows the results on both validation
sets. Most strikingly, BT-news performs almost
as well as bitext on newstest2012 (Figure 4a) and
improves the baseline (640K) by 2.6 BLEU. BT-
bitext improves by 2.2 BLEU, achieving 83% of
the improvement with real bitext. This shows that
synthetic data can be nearly as effective as real hu-
man translated data when the domains match.

Figure 4b shows the accuracy on valid-mixed,
the mixed domain valid set. The accuracy of BT-
news is not as good as before since the domain of
the BT data and the test set do not match. How-
ever, BT-news still improves the baseline by up to
1.2 BLEU. On the other hand, BT-bitext matches
the domain of valid-mixed and improves by 2.7
BLEU. This trails the real bitext by only 1.3 BLEU
and corresponds to 67% of the gain achieved with
real human bitext.

In summary, synthetic data performs remark-
ably well, coming close to the improvements
achieved with real bitext for newswire test data,
or trailing real bitext by only 1.3 BLEU for valid-
mixed. In absence of a large parallel corpus for
news, back-translation therefore offers a simple,
yet very effective domain adaptation technique.

U
E
L
B

33

32

31

30

29

28

)
2
1
0
2
t
s
e
t
s
w
e
n
(

U
E
L
B

25

24

23

22

U
E
L
B

23

22

21

20

bitext
BT-bitext
BT-news

bitext
BT-bitext
BT-news

640K

1.28M

2.56M

5.19M

640K

1.28M

2.56M

5.19M

Amount of data

(a) newstest2012

Amount of data

(b) valid-mixed

Figure 4: Accuracy on (a) newstest2012 and (b) a mixed domain valid set when growing a 640K bitext
corpus with (i) real parallel data (bitext), (ii) a back-translated version of the target side of the bitext
(BT-bitext), (iii) or back-translated newscrawl data (BT-news).

5.5 Upsampling the bitext

We found it beneﬁcial to adjust the ratio of bitext
to synthetic data observed during training. In par-
ticular, we tuned the rate at which we sample data
from the bitext compared to synthetic data. For
example, in a setup of 5M bitext sentences and
10M synthetic sentences, an upsampling rate of 2
means that we double the frequency at which we
visit bitext, i.e. training batches contain on aver-
age an equal amount of bitext and synthetic data
as opposed to 1/3 bitext and 2/3 synthetic data.

Figure 5 shows the accuracy of various upsam-
pling rates for different generation methods in a
setup with 5M bitext sentences and 24M synthetic
sentences. Beam and greedy beneﬁt a lot from
higher rates which results in training more on the
bitext data. This is likely because synthetic beam
and greedy data does not provide as much training
signal as the bitext which has more variation and
is harder to ﬁt. On the other hand, sampling and
beam+noise require no upsampling of the bitext,
which is likely because the synthetic data is al-
ready hard enough to ﬁt and thus provides a strong
training signal (§5.2).

5.6 Large scale results

To conﬁrm our ﬁndings we experiment on
WMT’14 English-French translation where we
show results on newstest2013-2015. We augment
the large bitext of 35.7M sentence pairs by 31M
newscrawl sentences generated by sampling. To

greedy
beam
top10
sampling
beam+noise

1

2

4

8

bitext upsample rate

Figure 5: Accuracy when changing the rate at
which the bitext is upsampled during training.
Rates larger than one mean that the bitext is ob-
served more often than actually present in the
combined bitext and synthetic training corpus.

train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs; we do not up-
sample the bitext for this experiment. Table 4
shows tokenized BLEU and Table 5 shows deto-
kenized BLEU.2 To our knowledge, our baseline
is the best reported result in the literature for new-
stest2014, and back-translation further improves
upon this by 2.6 BLEU (tokenized).

2sacreBLEU signatures:

BLEU+case.mixed+lang.en-

fr+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.7

news13 news14 news15

detok. sacreBLEU3

news13 news14 news15

bitext
+sampling

36.97
37.85

42.90
45.60

39.92
43.95

Table 4: Tokenized BLEU on various test sets for
WMT English-French translation.

bitext
+sampling

35.30
36.13

41.03
43.84

38.31
40.91

Table 5: De-tokenized BLEU (sacreBLEU) on var-
ious test sets for WMT English-French.

Finally, for WMT English-German we train
on all 226M available monolingual training sen-
tences and perform 250K updates in 22.5 hours
on 128 GPUs. We upsample the bitext with a
rate of 16 so that we observe every bitext sentence
16 times more often than each monolingual sen-
tence. This results in a new state of the art of
35 BLEU on newstest2014 by using only WMT
benchmark data. For comparison, DeepL, a com-
mercial translation engine relying on high qual-
ity bilingual training data, achieves 33.3 tokenized
BLEU .4 Table 6 summarizes our results and com-
pares to other work in the literature. This shows
that back-translation with sampling can result in
high-quality translation models based on bench-
mark data only.

a. Gehring et al. (2017)
b. Vaswani et al. (2017)
c. Ahmed et al. (2017)
d. Shaw et al. (2018)

DeepL
Our result

En–De En–Fr
40.5
41.0
41.4
41.5

25.2
28.4
28.9
29.2

33.3
35.0
33.8

45.9
45.6
43.8

Table 6: BLEU on newstest2014 for WMT
English-German (En–De) and English-French
(En–Fr). The ﬁrst four results use only WMT
bitext (WMT’14, except for b, c, d in En–De
which train on WMT’16). DeepL uses propri-
etary high-quality bitext and our result relies on
back-translation with 226M newscrawl sentences
for En–De and 31M for En–Fr. We also show deto-
kenized BLEU (SacreBLEU).

baseline
+BT
+ensemble
+ﬁlter copies

news17 news18

29.36
32.66
33.31
33.35

42.38
44.94
46.39
46.53

% of source copies

0.56% 0.53%

Table 7: De-tokenized case-insensitive sacreBLEU
on WMT English-German newstest17 and new-
stest18.

6 Submission to WMT’18

described in §4.

This section describes our entry to the WMT’18
English-German news translation task which was
ranked #1 in the human evaluation (Bojar et al.,
2018). Our entry is based on the WMT English-
German models described in the previous section
(§5.6).
In particular, we ensembled six back-
translation models trained on all available bitext
plus 226M newscrawl sentences or 5.8B German
tokens. Four models used bitext upsample ratio
16, one model upsample ratio 32, and another one
upsample ratio 8. Upsample ratios differed be-
cause we reused models previously trained to tune
the upsample ratio. We did not use checkpoint av-
eraging. More details of our setup and data are

Ott et al. (2018a) showed that beam search
sometimes outputs source copies rather than tar-
get language translations. We replaced source
copies by the output of a model trained only on the
news-commentary portion of the WMT’18 task
(nc model). This model produced far fewer copies
since this dataset is less noisy. Outputs are deemed
to be a source copy if the Jaccard similarity be-
tween the source and the target unigrams exceeds
0.5. About 0.5% of outputs are identiﬁed as source
copies. We used newstest17 as a development set
to ﬁne tune ensemble size and model parameters.
Table 7 summarizes the effect of back-translation
data, ensembling and source copy ﬁltering.5

with SET ∈ {wmt13, wmt14/full, wmt15}

3sacreBLEU signatures:

BLEU+case.mixed+lang.en-

4https://www.deepl.com/press.html
5BLEU+case.lc+lang.en-

LANG+numrefs.1+smooth.exp+test.wmt14/full+
tok.13a+version.1.2.7 with LANG ∈ {de,fr}

de+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.11
with SET ∈ {wmt17, wmt18}

7 Conclusions and future work

Back-translation is a very effective data augmen-
tation technique for neural machine translation.
Generating synthetic sources by sampling or by
adding noise to beam outputs leads to higher ac-
curacy than argmax inference which is typically
used.
In particular, sampling and noised beam
outperforms pure beam by 1.7 BLEU on average
on newstest2013-2017 for WMT English-German
translation. Both methods provide a richer train-
ing signal for all but resource poor setups. We
also ﬁnd that synthetic data can achieve up to 83%
of the performance attainable with real bitext. Fi-
nally, we achieve a new state of the art result of 35
BLEU on the WMT’14 English-German test set
by using publicly available benchmark data only.
In future work, we would like to investigate
an end-to-end approach where the back-translation
model is optimized to output synthetic sources that
are most helpful to the ﬁnal forward model.

References

Karim Ahmed, Nitish Shirish Keskar, and Richard
Socher. 2017. Weighted transformer network for
machine translation. arxiv, 1711.02132.

Antreas Antoniou, Amos J. Storkey, and Harrison Ed-
wards. 2017. Data augmentation generative adver-
sarial networks. arXiv, abs/1711.04340.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation with
monolingual resources. In Workshop on Statistical
Machine Translation (WMT).

Ondrej Bojar and Ales Tamchyna. 2011.

Improving
translation model by monolingual data. In Workshop
on Statistical Machine Translation (WMT).

Ondˇrej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(WMT18). In Proceedings of the Third Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, Brussels, Belgium. Association for Computa-
tional Linguistics.

Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990.
A statistical approach to machine translation. Com-
putational Linguistics, 16:79–85.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
In Conference of the Association for Computational
Linguistics (ACL).

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Ryan Cotterell and Julia Kreutzer. 2018. Explain-
ing and generalizing back-translation through wake-
sleep. arXiv preprint arXiv:1806.04402.

Anna Currey, Antonio Valerio Miceli Barone, and Ken-
neth Heaﬁeld. 2017. Copied Monolingual Data Im-
proves Low-Resource Neural Machine Translation.
In Proc. of WMT.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine transla-
tion through multi-task learning. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Angela Fan, Yann Dauphin, and Mike Lewis. 2018.
In Confer-
Hierarchical neural story generation.
ence of the Association for Computational Linguis-
tics (ACL).

Orhan Firat, Kyunghyun Cho, and Yoshua Ben-
gio. 2016a. Multi-way, multilingual neural ma-
chine translation with a shared attention mecha-
nism. In Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).

Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman-Vural, and Kyunghyun Cho. 2016b.
Zero-resource translation with multi-lingual neu-
In Conference on Em-
ral machine translation.
pirical Methods in Natural Language Processing
(EMNLP).

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
In International
sequence to sequence learning.
Conference of Machine Learning (ICML).

Alex Graves. 2013. Generating sequences with recur-

rent neural networks. arXiv, 1308.0850.

Thorsten Brants, Ashok C. Popat, Peng Xu, Franz Josef
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Conference on Natural
Language Learning (CoNLL).

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor
O. K. Li. 2018. Universal neural machine transla-
tion for extremely low resource languages. arXiv,
1802.05368.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv, 1503.03535.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, and Yoshua Bengio. 2017. On integrating
a language model into neural machine translation.
Computer Speech & Language, 45:137–148.

Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. arXiv,
1611.04798.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary,
Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv, 1803.05567.

Soren Hauberg, Oren Freifeld, Anders Boesen Lindbo
Larsen, John W. Fisher, and Lars Kai Hansen. 2016.
Dreaming more data: Class-dependent distributions
over diffeomorphisms for learned data augmenta-
tion. In AISTATS.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016a. Dual learning
for machine translation. In Conference on Advances
in Neural Information Processing Systems (NIPS).

Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016b.
Improved neural machine translation with
smt features. In Conference of the Association for
the Advancement of Artiﬁcial Intelligence (AAAI),
pages 151–157.

Kenneth Heaﬁeld,

Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modiﬁed
Kneser-Ney Language Model Estimation. In Con-
ference of the Association for Computational Lin-
guistics (ACL).

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
In Conference of the North
from unlabelled data.
American Chapter of the Association for Computa-
tional Linguistics (NAACL).

Vu Cong Duy Hoang, Philipp Koehn, Gholamreza
Haffari, and Trevor Cohn. 2018.
Iterative back-
translation for neural machine translation. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, pages 18–24.

Kenji Imamura, Atsushi Fujita, and Eiichiro Sumita.
2018. Enhancement of encoder and attention using
target monolingual corpora in neural machine trans-
lation. In Proceedings of the 2nd Workshop on Neu-
ral Machine Translation and Generation, pages 55–
63.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Vi´egas, Martin Wattenberg, Gre-
gory S. Corrado, Macduff Hughes, and Jeffrey Dean.
2017. Google’s multilingual neural machine transla-
tion system: Enabling zero-shot translation. Trans-
actions of the Association for Computational Lin-
guistics (TACL), 5:339–351.

Lukasz Kaiser, Aidan N. Gomez, and Franc¸ois Chollet.
2017. Depthwise separable convolutions for neural
machine translation. CoRR, abs/1706.03059.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
A¨aron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. CoRR, abs/1610.10099.

Alina Karakanta, Jon Dehdari, and Josef van Genabith.
2017. Neural machine translation for low-resource
languages without parallel corpora. Machine Trans-
lation, pages 1–23.

Diederik P. Kingma and Jimmy Ba. 2015. Adam:
In Inter-
A Method for Stochastic Optimization.
national Conference on Learning Representations
(ICLR).

Philipp Koehn. 2010. Statistical machine translation.

Cambridge University Press.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL Demo Session.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011.
Investigations
on translation model adaptation using monolingual
data. In Workshop on Statistical Machine Transla-
tion (WMT).

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In International Conference on Learning Represen-
tations (ICLR).

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine trans-
lation. arXiv, 1803.05567.

Marco Lui and Timothy Baldwin. 2012. langid. py: An
In Pro-
off-the-shelf language identiﬁcation tool.
ceedings of the ACL 2012 system demonstrations,
pages 25–30. Association for Computational Lin-
guistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
tations. In Proc. of NAACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
In Conference on Advances in Neural In-
works.
formation Processing Systems (NIPS).

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2015. Re-
thinking the Inception Architecture for Computer
Vision. arXiv preprint arXiv:1512.00567.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Conference on Advances in Neural In-
formation Processing Systems (NIPS).

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,

,
and Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
In International Conference on Machine
coders.
Learning (ICML).

Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai
Yu, and Tie-Yan Liu. 2017. Dual supervised learn-
ing. In International Conference on Machine Learn-
ing (ICML).

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Xing Niu, Michael Denkowski, and Marine Carpuat.
Bi-directional neural machine transla-
arXiv preprint

2018.
tion with synthetic parallel data.
arXiv:1805.11213.

Myle Ott, Michael Auli, David Grangier,

and
Marc’Aurelio Ranzato. 2018a. Analyzing uncer-
In Proceed-
tainty in neural machine translation.
ings of the 35th International Conference on Ma-
chine Learning, volume 80, pages 3956–3965.

Myle Ott, Sergey Edunov, David Grangier, and
Michael Auli. 2018b. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation: Research Papers.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
In Conference
evaluation of machine translation.
of the Association for Computational Linguistics
(ACL).

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In International Conference on Learn-
ing Representations (ICLR).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Lukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-
ularizing neural networks by penalizing conﬁdent
output distributions. In International Conference on
Learning Representations (ICLR) Workshop.

Luis Perez and Jason Wang. 2017. The effectiveness of
data augmentation in image classiﬁcation using deep
learning. arxiv, 1712.04621.

Alberto Poncelas, Dimitar Sht. Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Peyman
Passban. 2018. Investigating backtranslation in neu-
ral machine translation. arXiv, 1804.06189.

Matt Post. 2018. A call for clarity in reporting bleu

scores. arXiv, 1804.08771.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. Conference of the Asso-
ciation for Computational Linguistics (ACL).

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Conference of the Associa-
tion for Computational Linguistics (ACL).

Iulian Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron C. Courville, and Joelle Pineau. 2016. Build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. In Conference of
the Association for the Advancement of Artiﬁcial In-
telligence (AAAI).

