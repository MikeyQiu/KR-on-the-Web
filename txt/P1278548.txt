EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION

Hideyuki Tachibana, Katsuya Uenoyama

Shunsuke Aihara

PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.com

Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jp

7
1
0
2
 
t
c
O
 
4
2
 
 
]

D
S
.
s
c
[
 
 
1
v
9
6
9
8
0
.
0
1
7
1
:
v
i
X
r
a

ABSTRACT

This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without any recur-
rent units. Recurrent neural network (RNN) has been a standard
technique to model sequential data recently, and this technique has
been used in some cutting-edge neural TTS techniques. However,
training RNN component often requires a very powerful computer,
or very long time typically several days or weeks. Recent other stud-
ies, on the other hand, have shown that CNN-based sequence syn-
thesis can be much faster than RNN-based techniques, because of
high parallelizability. The objective of this paper is to show an al-
ternative neural TTS system, based only on CNN, that can alleviate
these economic costs of training. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Index Terms— Text-to-speech, deep learning, convolutional

neural network, attention, sequence-to-sequence learning.

1. INTRODUCTION

Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
encourage further use of TTS in various systems, it is signiﬁcant
to develop a handy, maintainable, extensible TTS component that
is accessible to speech non-specialists, enterprising individuals and
small teams who do not have massive computers.

Traditional TTS systems, however, are not necessarily friendly
for them, as these systems are typically composed of many domain-
speciﬁc modules. For example, a typical parametric TTS system is
an elaborate integration of many modules e.g. a text analyzer, an
F0 generator, a spectrum generator, a pause estimator, and a vocoder
that synthesize a waveform from these data, etc.

Deep learning [1] sometimes can unite these internal build-
ing blocks into a single model, and directly connects the input
and the output; this type of technique is sometimes called ‘end-to-
end’ learning. Although such a technique is sometimes criticized
as ‘a black box,’ nevertheless, an end-to-end TTS system named
Tacotron [2], which directly estimates a spectrogram from an in-
put text, has achieved promising performance recently, without
intensively-engineered parametric models based on domain-speciﬁc
knowledge. Tacotron, however, has a drawback that it exploits many
recurrent units, which are quite costly to train, making it almost
infeasible for ordinary labs without luxurious machines to study
and extend it further. Indeed, some people tried to implement open

clones of Tacotron [3, 4, 5, 6], but they are struggling to reproduce
the speech of satisfactory quality as clear as the original work.

The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel, handy neural TTS, which is fully convolutional.
The architecture is largely similar to Tacotron [2], but is based on
a fully convolutional sequence-to-sequence learning model similar
to the literature [7]. We show this handy TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention, which we call ‘guided attention,’ is also shown.

1.1. Related Work

1.1.1. Deep Learning and TTS

Recently, deep learning-based TTS systems have been intensively
studied, and some of recent studies are achieving surprisingly clear
results. The TTS systems based on deep neural networks include
Zen’s work in 2013 [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepVoice1&2 [15, 16], and Tacotron [2].

Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.

Most of the existing methods above use RNN, a natural tech-
nique for time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.

1.1.2. Sequence to Sequence (seq2seq) Learning

Recently, recurrent neural network (RNN) has been a standard tech-
nique to map a sequence into another sequence, especially in the ﬁeld
of natural language processing, e.g. machine translation [17, 18], di-
alogue system [19, 20], etc. See also [1, sec. 10.4].

RNN-based seq2seq, however, has some disadvantages. One
is that a vanilla encode-decoder model cannot encode too long se-
quence into a ﬁxed-length vector effectively. This problem has been

resolved by a mechanism called ‘attention’ [21], and the attention
mechanism now has become a standard idea in seq2seq learning
techniques; see also [1, sec. 12.4.5.1].

Another problem is that RNN typically requires much time to
train, since it is less suitable for parallel computation using GPUs.
In order to overcome this problem, several people proposed the use
of CNN, instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have
shown that CNN-based alternative networks can be trained much
faster, and can even outperform the RNN-based techniques.

Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea how to use attention mech-
anism in a CNN-based seq2seq learning model, and showed that
the method is quite effective for machine translation. Our proposed
method, indeed, is based on the similar idea to the literature [7].

2. PRELIMINARY

2.1. Basic Knowledge of the Audio Spectrograms

An audio waveform can be mutually converted to a complex spec-
trogram Z = {Zf,t} ∈ CF (cid:48)×T (cid:48)
by linear maps called STFT and
inverse STFT, where F (cid:48) and T (cid:48) denote the number of frequency
bins and temporal bins, respectively. It is common to consider only
the magnitude |Z| = {|Zf,t|}, since it still has useful information
for many purposes, and that |Z| is almost identical to Z in a sense
that there exist many phase estimation (∼ waveform synthesis) tech-
niques from magnitude spectrograms, e.g. the famous Grifﬁn&Lim
algorithm [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online G&L, to synthesize a waveform.
In this paper, we al-
ways normalize STFT spectrograms as |Z| ← (|Z|/ max(|Z|))γ,
and convert back |Z| ← |Z|η/γ when we ﬁnally need to synthesize
the waveform, where γ, η are pre- and post-emphasis factors.

It is also common to consider a mel spectrogram S ∈ RF ×T (cid:48)
,
(F (cid:28) F (cid:48)), by applying a mel ﬁlter-bank to |Z|. This is a standard
dimension reduction technique in speech processing. In this paper,
we also reduce the temporal dimensionality from T (cid:48) to (cid:100)T (cid:48)/4(cid:101) =: T
by picking up a time frame every four time frames, to accelerate the
training of Text2Mel shown below. We also normalize mel spectro-
grams as S ← (S/ max(S))γ.

2.2. Notation: Convolution and Highway Activation

In this paper, we denote 1D convolution layer [30] by a space sav-
ing notation Co←i
k(cid:63)δ (X), where i is the sizes of input channel, o is the
sizes of output channel, k is the size of kernel, δ is the dilation fac-
tor, and an argument X is a tensor having three dimensions (batch,
channel, temporal). The stride of convolution is always 1. Convolu-
tion layers are preceded by appropriately-sized zero padding, whose
size is suitably determined by a simple arithmetic so that the length
of the sequence is kept constant. Let us also denote the 1D deconvo-
lution layer as Do←i
k(cid:63)δ (X). The stride of deconvolution is always 2 in
this paper. Let us write a layer composition operator as · (cid:47) ·, and let
us write networks like F (cid:47) ReLU (cid:47) G(X) := F(ReLU(G(X))), and
(F (cid:47) G)2(X) := F (cid:47) G (cid:47) F (cid:47) G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU(x) = max(x, 0).

Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway(X; L) = σ(H1) (cid:12) H2 + (1 − σ(H1)) (cid:12) X,
where H1, H2 are properly-sized two matrices, output by a layer L
as [H1, H2] = L(X). The operator (cid:12) is the element-wise multipli-
cation, and σ is the element-wise sigmoid function. Hereafter let us
denote HCd←d

k(cid:63)δ (X) := Highway(X; C2d←d

k(cid:63)δ

).

Fig. 1. Network architecture.

TextEnc(L) := (HC2d←2d
HC2d←2d
3(cid:63)3

(cid:47)HC2d←2d
3(cid:63)1

1(cid:63)1
)2(cid:47)C2d←2d
1(cid:63)1

)2 (cid:47) (HC2d←2d

3(cid:63)1

(cid:47)ReLU(cid:47)C2d←e

)2 (cid:47) (HC2d←2d
(cid:47)
1(cid:63)1 (cid:47)CharEmbede-dim(L).

(cid:47) HC2d←2d
3(cid:63)9

3(cid:63)27

AudioEnc(S) := (HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←d
Cd←d

3(cid:63)3 )2 (cid:47)(HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←F

3(cid:63)27 (cid:47)HCd←d
1(cid:63)1 (S).

3(cid:63)9 (cid:47)HCd←d

3(cid:63)3 (cid:47)HCd←d

3(cid:63)1 )2 (cid:47)

AudioDec(R(cid:48)) := σ(cid:47)CF ←d
3(cid:63)3 (cid:47) HCd←d
HCd←d

3(cid:63)9 (cid:47) HCd←d

SSRN(Y ) := σ(cid:47)CF (cid:48)←F (cid:48)
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c
1(cid:63)1 (cid:47) (HCc←c
C2c←c

3(cid:63)1 )2 (cid:47)(HCd←d
3(cid:63)27 (cid:47)

1(cid:63)1 (cid:47)(ReLU(cid:47)Cd←d
1(cid:63)1 )3 (cid:47)(HCd←d
(R(cid:48)).
3(cid:63)1 ) (cid:47) Cd←2d
1(cid:63)1
(cid:47)(ReLU(cid:47)CF (cid:48)←F (cid:48)

3(cid:63)1 (cid:47) Dc←c

1(cid:63)1
2(cid:63)1 )2 (cid:47) (HCc←c

)2(cid:47)CF (cid:48)←2c
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c

(cid:47)(HC2c←2c
)2(cid:47)
3(cid:63)1
3(cid:63)1 ) (cid:47) Cc←F
1(cid:63)1 (Y ).

Fig. 2. Details of each component. For notation, see section 2.2.

3. PROPOSED NETWORK

Our DCTTS model consists of two networks: (1) Text2Mel, which
synthesize a mel spectrogram from an input text, and (2) Spectro-
gram Super-resolution Network (SSRN), which convert a coarse mel
spectrogram to the full STFT spectrogram. Fig. 1 shows the overall
architecture of the proposed method.

3.1. Text2Mel: Text to Mel Spectrogram Network

We ﬁrst consider to synthesize a coarse mel spectrogram from a
text. This is the main part of the proposed method. This module
consists of four submodules: Text Encoder, Audio Encoder, At-
tention, and Audio Decoder. The network TextEnc ﬁrst encodes
the input sentence L = [l1, . . . , lN ] ∈ CharN consisting of N
characters, into the two matrices K, V ∈ Rd×N . On the other
hand, the network AudioEnc encodes the coarse mel spectrogram
S(= S1:F,1:T ) ∈ RF ×T , of previously spoken speech, whose length
is T , into a matrix Q ∈ Rd×T .

(K, V ) = TextEnc(L).

Q = AudioEnc(S1:F,1:T ).

An attention matrix A ∈ RN ×T , deﬁned as follows, evaluates how
strongly the n-th character ln and t-th time frame S1:F,t are related,

A = softmaxn-axis(K TQ/

d).

√

Ant ∼ 1 implies that the module is looking at n-th character ln at
the time frame t, and it will look at ln or ln+1 or characters around
them, at the subsequent time frame t + 1. Whatever, let us expect
those are encoded in the n-th column of V . Thus a seed R ∈ Rd×T ,
decoded to the subsequent frames S1:F,2:T +1, is obtained as

R = Att(Q, K, V ) := V A. (Note: matrix product.)

(4)

(1)
(2)

(3)

The resultant R is concatenated with the encoded audio Q, as R(cid:48) =
[R, Q], because we found it beneﬁcial in our pilot study. Then, the
concatenated matrix R(cid:48) ∈ R2d×T is decoded by the Audio Decoder
module to synthesize a coarse mel spectrogram,

Y1:F,2:T +1 = AudioDec(R(cid:48)).

(5)

The result Y1:F,2:T +1 is compared with the temporally-shifted
ground truth S1:F,2:T +1, by a loss function Lspec(Y1:F,2:T +1|S1:F,2:T +1),
and the error is back-propagated to the network parameters. The loss
function was the sum of L1 loss and the binary divergence Dbin,

Dbin(Y |S) := Ef t[−Sf t log Yf t − (1 − Sf t) log(1 − Yf t)]
= Ef t[−Sf t ˆYf t + log(1 + exp ˆYf t)],

(6)

where ˆYf t = logit(Yf t). Since the binary divergence gives a non-
vanishing gradient to the network, ∂Dbin(Y |S)/∂ ˆYf t ∝ Yf t − Sf t,
it is advantageous in gradient-based training. It is easily veriﬁed that
the spectrogram error is non-negative, Lspec(Y |S) = Dbin(Y |S) +
E[|Yf t − Sf t|] ≥ 0, and the equality holds iff Y = S.

3.1.1. Details of TextEnc, AudioEnc, and AudioDec

Our networks are fully convolutional, and are not dependent on any
recurrent units. Instead of RNN, we sometimes take advantages of
dilated convolution [32, 13, 24] to take long contextual information
into account. The top equation of Fig. 2 is the content of TextEnc. It
consists of the character embedding and the stacked 1D non-causal
convolution. A previous literature [2] used a heavier RNN-based
component named ‘CBHG,’ but we found this simpler network also
works well. AudioEnc and AudioDec, shown in Fig. 2, are com-
posed of 1D causal convolution layers with Highway activation.
These convolution should be causal because the output of AudioDec
is feedbacked to the input of AudioEnc in the synthesis stage.

3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram |Z| ∈ RF (cid:48)×4T , from the
obtained coarse mel spectrogram Y ∈ RF ×T , by a spectrogram
super-resolution network (SSRN). Upsampling frequency from F to
F (cid:48) is rather straightforward. We can achieve that by increasing the
convolution channels of 1D convolutional network. Upsampling in
temporal direction is not similarly done, but by twice applying de-
convolution layers of stride size 2, we can quadruple the length of
sequence from T to 4T = T (cid:48). The bottom equation of Fig. 2 shows
SSRN. In this paper, as we do not consider online processing, all
convolutions can be non-causal. The loss function was the same as
Text2Mel: sum of binary divergence and L1 distance between the
synthesized spectrogram SSRN(S) and the ground truth |Z|.

4. GUIDED ATTENTION

4.1. Guided Attention Loss: Motivation, Method and Effects

In general, an attention module is quite costly to train. Therefore, if
there is some prior knowledge, it may be a help incorporating them
into the model to alleviate the heavy training. We show that the
simple measure below is helpful to train the attention module.

In TTS, the possible attention matrix A lies in the very small
subspace of RN ×T . This is because of the rough correspondence of
the order of the characters and the audio segments. That is, if one
reads a text, it is natural to assume that the text position n progresses
nearly linearly to the time t, i.e., n ∼ at, where a ∼ N/T . This is

Comparison of the attention matrix A, trained with
Fig. 3.
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is “icassp stands
for the international conference on acoustics, speech, and signal
processing.” We did not use the heuristics described in section 4.2.

the prominent difference of TTS from other seq2seq learning tech-
niques such as machine translation, in which an attention module
should resolve the word alignment between two languages that have
very different syntax, e.g. English and Japanese.

Based on this idea, we introduce another constraint on the at-
tention matrix A to prompt it to be ‘nearly diagonal,’ Latt(A) =
Ent[AntWnt], where Wnt = 1 − exp{−(n/N − t/T )2/2g2}. In
this paper, we set g = 0.2. If A is far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main loss Lspec with equal weight.

Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after ∼100 iterations. After ∼5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after ∼10K it-
erations, and it required ∼50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.

4.2. Forcibly Incremental Attention in Synthesis Stage

In the synthesis stage, the attention matrix A sometimes fails to look
at the correct characters. Typical errors we observed were (1) it occa-
sionally skipped several characters, and (2) it repeatedly read a same
word twice or more. In order to make the system more robust, we
heuristically modify the matrix A to be ‘nearly diagonal,’ by a simple
rule as follows. We observed this device sometimes alleviated such
misattentions. Let nt be the position of the character to be read at t-
th time frame; nt = argmaxnAn,t. Comparing the current position
nt and the previous position nt−1, unless the difference nt − nt−1
is within the range −1 ≤ nt − nt−1 ≤ 3, the current attention
position forcibly set to An,t = δn,nt−1+1 (Kronecker’s delta), to
forcibly make the attention target incremental, i.e., nt − nt−1 = 1.

5. EXPERIMENT

5.1. Experimental Setup

To train the networks, we used LJ Speech Dataset [33], a public do-
main speech dataset consisting of ∼13K pairs of text&speech, ∼24
hours in total, without phoneme-level alignment. These speech are
a little reverbed. We preprocessed the texts by spelling out some of

Table 1. Parameter Settings.

Sampling rate of audio signals
STFT window function
STFT window length and shift
STFT spectrogram size F (cid:48) × 4T
Mel spectrogram size F × T
Dimension e, d and c
ADAM parameters (α, β1, β2, ε)
Minibatch size
Emphasis factors (γ, η)
RTISI-LA window and iteration
Character set, Char

22050 Hz
Hanning
1024 (∼46.4 [ms]), 256 (∼11.6[ms])
513 × 4T (T depends on audio clip)
80 × T (T depends on audio clip)
128, 256, 512
(2 × 10−4, 0.5, 0.9, 10−6)
16
(0.6, 1.3)
100, 10
a-z,.’- and Space and NULL

Table 2. Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].

Method
Open Tacotron [5]
DCTTS
DCTTS
DCTTS
DCTTS

Iteration
12 days∗
877K∗
∼2 hours
20K/ 40K
∼7 hours
90K/150K
200K/340K ∼15 hours
540K/900K ∼40 hours

Time MOS (95% CI)
2.07 ± 0.62
1.74 ± 0.52
2.63 ± 0.75
2.71 ± 0.66
2.61 ± 0.62

abbreviations and numeric expressions, decapitalizing the capitals,
and removing less frequent characters not shown in Table 1, where
NULL is a dummy character for zero-padding.

We implemented our neural networks using Chainer 2.0 [34].
The models are trained on a household gamine PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.

For simplicity, we trained Text2Mel and SSRN independently
and asynchronously, using different GPUs. All network parameters
are initialized using He’s Gaussian initializer [35]. Both networks
were trained by ADAM optimizer [36]. When training SSRN, we
randomly extracted short sequences of length T = 64 for each it-
eration, to save the memory usage. To reduce the frequency of disc
access, we only took the snapshot of the parameters, every 5K itera-
tions. Other parameters are shown in Table 1.

As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds among the
open implementations.
It is reported that this model was trained
using LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti,
newer GPU than ours. Note, this iteration is still much less than the
original Tacotron, which was trained for more than 2M iterations.

We evaluated mean opinion scores (MOS) for both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [37]. We used 20 sentences from Harvard Sentences List
1&2. We synthesized the audio using 5 methods shown in Table 2.
The crowdworkers evaluated these 100 clips, rating them from 1
(Bad) to 5 (Excellent). Each worker is supposed to rate at least
10 clips. To obtain more responses with higher quality, we set a
few incentives shown in the literature. The results were statistically
processed using the method shown in the literature [37].

Fig. 4.
(Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input is “icassp stands for the international confer-
ence on acoustics, speech and signal processing.”

shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly look at the correct characters, and can synthesize quite
clear spectrograms. More samples will be available at the author’s
web page.1

In our crowdsourcing experiment, 31 subjective evaluated our
data. After the automatic screening by the toolkit [37], 560 scores
by 6 subjective were selected for ﬁnal statistics calculation. Table 2
compares the performance of our proposed method (DCTTS) and an
open Tacotron. Our MOS (95% conﬁdence interval) was 2.71±0.66
(15 hours training) while the Tacotron’s was 2.07 ± 0.62. Although
it is not a strict comparison since the frameworks and machines are
different, it would be still concluded that our proposed method is
quite rapidly trained to the satisfactory level compared to Tacotron.

6. SUMMARY AND FUTURE WORK

This paper described a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), as well as a technique
to train the attention module rapidly. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(∼15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in deep learning community. We
believe this handy method encourages further development of the
applications based on speech synthesis. We can expect this sim-
ple neural TTS may be extended to other versatile purposes, such
as emotional/non-linguistic/personalized speech synthesis, singing
voice synthesis, music synthesis, etc., by further studies. In addi-
tion, since a neural TTS has become this lightweight, the studies on
more integrated speech systems e.g. some multimodal systems, si-
multaneous training of TTS+ASR, and speech translation, etc., may
have become more feasible. These issues should be worked out in
the future.

7. ACKNOWLEDGEMENT

5.2. Result and Discussion

In our setting, the training throughput was ∼3.8 minibatch/s (Text2Mel)
and ∼6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel, 200K times only in 15 hours. Fig. 4

The authors would like to thank the OSS contributors, and the mem-
bers and contributors of LibriVox public domain audiobook project,
and @keithito who created the speech dataset.

1https://github.com/tachi-hi/tts_samples

8. REFERENCES

[21] D Bahdanau et al.,

learning to align and translate,”
arXiv:1409.0473, 2014.

“Neural machine translation by jointly
in Proc. ICLR 2015,

[22] Y. Kim,

classiﬁcation,”
arXiv:1408.5882.

“Convolutional neural networks for sentence
in Proc. EMNLP, 2014, pp. 1746–1752,

[23] X. Zhang et al., “Character-level convolutional networks for

text classiﬁcation,” in Proc. NIPS, 2015, arXiv:1509.01626.

[24] N. Kalchbrenner et al., “Neural machine translation in linear

time,” arXiv:1610.10099, 2016.

[25] Y. N. Dauphin et al., “Language modeling with gated convolu-

tional networks,” arXiv:1612.08083, 2016.

[26] J. Bradbury et al., “Quasi-recurrent neural networks,” in Proc.

ICLR 2017, 2016.

[27] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Trans. ASSP, vol. 32, no. 2, pp.
236–243, 1984.

[28] P. Mowlee et al., Phase-Aware Signal Processing in Speech

Communication: Theory and Practice, Wiley, 2016.

[29] X. Zhu et al.,

“Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra,” IEEE Trans.
ASLP, vol. 15, no. 5, 2007.

[30] Y. LeCun and Y. Bengio, “The handbook of brain theory and
neural networks,” chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255–258. MIT Press, 1998.

[31] R. K. Srivastava et al., “Training very deep networks,” in Proc.

NIPS, 2015, pp. 2377–2385.

[32] F. Yu and V. Koltun, “Multi-scale context aggregation by di-

lated convolutions,” in Proc. ICLR, 2016.

[33] K. Ito, “The LJ speech dataset,” 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).

[34] S. Tokui et al.,

“Chainer: A next-generation open source
framework for deep learning,” in Proc. Workshop LearningSys,
NIPS, 2015.

[35] K. He et al., “Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation,” in Proc. ICCV,
2015, pp. 1026–1034, arXiv:1502.01852.

[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” in Proc. ICLR 2015, 2014, arXiv:1412.6980.

[37] F. Ribeiro et al., “CrowdMOS: An approach for crowdsourcing
mean opinion score studies,” in Proc ICASSP, 2011, pp. 2416–
2419.

[1] I. Goodfellow et al., Deep Learning, MIT Press, 2016, http:

//www.deeplearningbook.org.

[2] Y. Wang et al., “Tacotron: Towards end-to-end speech synthe-

sis,” in Proc. Interspeech, 2017, arXiv:1703.10135.

[3] A. Barron,

“Implementation of Google’s Tacotron in Ten-
sorFlow,” 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).

[4] K. Park,

“A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model,” 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).

[5] K. Ito, “Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model,” 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).

[6] R. Yamamoto, “PyTorch implementation of Tacotron speech
synthesis model,” 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).

[7] J. Gehring et al., “Convolutional sequence to sequence learn-
ing,” in Proc. ICML, 2017, pp. 1243–1252, arXiv:1705.03122.

[8] H. Zen et al., “Statistical parametric speech synthesis using
deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.

[9] Y. Fan et al., “TTS synthesis with bidirectional LSTM based
recurrent neural networks,” in Proc. Interspeech, 2014, pp.
1964–1968.

[10] H. Zen and H. Sak, “Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis,” in Proc. ICASSP, 2015, pp. 4470–
4474.

[11] S. Achanta et al., “An investigation of recurrent neural network
architectures for statistical parametric speech synthesis.,” in
Proc. Interspeech, 2015, pp. 859–863.

[12] Z. Wu and S. King, “Investigating gated recurrent networks for
speech synthesis,” in Proc. ICASSP, 2016, pp. 5140–5144.

[13] A. van den Oord et al., “WaveNet: A generative model for raw

audio,” arXiv:1609.03499, 2016.

[14] J. Sotelo et al., “Char2wav: End-to-end speech synthesis,” in

Proc. ICLR, 2017.

[15] S. Arik et al., “Deep voice: Real-time neural text-to-speech,”
in Proc. ICML, 2017, pp. 195–204, arXiv:1702.07825.

[16] S. Arik et al., “Deep voice 2: Multi-speaker neural text-to-

speech,” in Proc. NIPS, 2017, arXiv:1705.08947.

[17] K. Cho et al., “Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in Proc.
EMNLP, 2014, pp. 1724–1734.

[18] I. Sutskever et al., “Sequence to sequence learning with neural

networks,” in Proc. NIPS, 2014, pp. 3104–3112.

[19] O. Vinyals and Q. Le, “A neural conversational model,” in

Proc. Deep Learning Workshop, ICML, 2015.

[20] I. V. Serban et al., “Building end-to-end dialogue systems us-
ing generative hierarchical neural network models.,” in Proc.
AAAI, 2016, pp. 3776–3784.

EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION

Hideyuki Tachibana, Katsuya Uenoyama

Shunsuke Aihara

PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.com

Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jp

7
1
0
2
 
t
c
O
 
4
2
 
 
]

D
S
.
s
c
[
 
 
1
v
9
6
9
8
0
.
0
1
7
1
:
v
i
X
r
a

ABSTRACT

This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without any recur-
rent units. Recurrent neural network (RNN) has been a standard
technique to model sequential data recently, and this technique has
been used in some cutting-edge neural TTS techniques. However,
training RNN component often requires a very powerful computer,
or very long time typically several days or weeks. Recent other stud-
ies, on the other hand, have shown that CNN-based sequence syn-
thesis can be much faster than RNN-based techniques, because of
high parallelizability. The objective of this paper is to show an al-
ternative neural TTS system, based only on CNN, that can alleviate
these economic costs of training. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Index Terms— Text-to-speech, deep learning, convolutional

neural network, attention, sequence-to-sequence learning.

1. INTRODUCTION

Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
encourage further use of TTS in various systems, it is signiﬁcant
to develop a handy, maintainable, extensible TTS component that
is accessible to speech non-specialists, enterprising individuals and
small teams who do not have massive computers.

Traditional TTS systems, however, are not necessarily friendly
for them, as these systems are typically composed of many domain-
speciﬁc modules. For example, a typical parametric TTS system is
an elaborate integration of many modules e.g. a text analyzer, an
F0 generator, a spectrum generator, a pause estimator, and a vocoder
that synthesize a waveform from these data, etc.

Deep learning [1] sometimes can unite these internal build-
ing blocks into a single model, and directly connects the input
and the output; this type of technique is sometimes called ‘end-to-
end’ learning. Although such a technique is sometimes criticized
as ‘a black box,’ nevertheless, an end-to-end TTS system named
Tacotron [2], which directly estimates a spectrogram from an in-
put text, has achieved promising performance recently, without
intensively-engineered parametric models based on domain-speciﬁc
knowledge. Tacotron, however, has a drawback that it exploits many
recurrent units, which are quite costly to train, making it almost
infeasible for ordinary labs without luxurious machines to study
and extend it further. Indeed, some people tried to implement open

clones of Tacotron [3, 4, 5, 6], but they are struggling to reproduce
the speech of satisfactory quality as clear as the original work.

The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel, handy neural TTS, which is fully convolutional.
The architecture is largely similar to Tacotron [2], but is based on
a fully convolutional sequence-to-sequence learning model similar
to the literature [7]. We show this handy TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention, which we call ‘guided attention,’ is also shown.

1.1. Related Work

1.1.1. Deep Learning and TTS

Recently, deep learning-based TTS systems have been intensively
studied, and some of recent studies are achieving surprisingly clear
results. The TTS systems based on deep neural networks include
Zen’s work in 2013 [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepVoice1&2 [15, 16], and Tacotron [2].

Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.

Most of the existing methods above use RNN, a natural tech-
nique for time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.

1.1.2. Sequence to Sequence (seq2seq) Learning

Recently, recurrent neural network (RNN) has been a standard tech-
nique to map a sequence into another sequence, especially in the ﬁeld
of natural language processing, e.g. machine translation [17, 18], di-
alogue system [19, 20], etc. See also [1, sec. 10.4].

RNN-based seq2seq, however, has some disadvantages. One
is that a vanilla encode-decoder model cannot encode too long se-
quence into a ﬁxed-length vector effectively. This problem has been

resolved by a mechanism called ‘attention’ [21], and the attention
mechanism now has become a standard idea in seq2seq learning
techniques; see also [1, sec. 12.4.5.1].

Another problem is that RNN typically requires much time to
train, since it is less suitable for parallel computation using GPUs.
In order to overcome this problem, several people proposed the use
of CNN, instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have
shown that CNN-based alternative networks can be trained much
faster, and can even outperform the RNN-based techniques.

Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea how to use attention mech-
anism in a CNN-based seq2seq learning model, and showed that
the method is quite effective for machine translation. Our proposed
method, indeed, is based on the similar idea to the literature [7].

2. PRELIMINARY

2.1. Basic Knowledge of the Audio Spectrograms

An audio waveform can be mutually converted to a complex spec-
trogram Z = {Zf,t} ∈ CF (cid:48)×T (cid:48)
by linear maps called STFT and
inverse STFT, where F (cid:48) and T (cid:48) denote the number of frequency
bins and temporal bins, respectively. It is common to consider only
the magnitude |Z| = {|Zf,t|}, since it still has useful information
for many purposes, and that |Z| is almost identical to Z in a sense
that there exist many phase estimation (∼ waveform synthesis) tech-
niques from magnitude spectrograms, e.g. the famous Grifﬁn&Lim
algorithm [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online G&L, to synthesize a waveform.
In this paper, we al-
ways normalize STFT spectrograms as |Z| ← (|Z|/ max(|Z|))γ,
and convert back |Z| ← |Z|η/γ when we ﬁnally need to synthesize
the waveform, where γ, η are pre- and post-emphasis factors.

It is also common to consider a mel spectrogram S ∈ RF ×T (cid:48)
,
(F (cid:28) F (cid:48)), by applying a mel ﬁlter-bank to |Z|. This is a standard
dimension reduction technique in speech processing. In this paper,
we also reduce the temporal dimensionality from T (cid:48) to (cid:100)T (cid:48)/4(cid:101) =: T
by picking up a time frame every four time frames, to accelerate the
training of Text2Mel shown below. We also normalize mel spectro-
grams as S ← (S/ max(S))γ.

2.2. Notation: Convolution and Highway Activation

In this paper, we denote 1D convolution layer [30] by a space sav-
ing notation Co←i
k(cid:63)δ (X), where i is the sizes of input channel, o is the
sizes of output channel, k is the size of kernel, δ is the dilation fac-
tor, and an argument X is a tensor having three dimensions (batch,
channel, temporal). The stride of convolution is always 1. Convolu-
tion layers are preceded by appropriately-sized zero padding, whose
size is suitably determined by a simple arithmetic so that the length
of the sequence is kept constant. Let us also denote the 1D deconvo-
lution layer as Do←i
k(cid:63)δ (X). The stride of deconvolution is always 2 in
this paper. Let us write a layer composition operator as · (cid:47) ·, and let
us write networks like F (cid:47) ReLU (cid:47) G(X) := F(ReLU(G(X))), and
(F (cid:47) G)2(X) := F (cid:47) G (cid:47) F (cid:47) G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU(x) = max(x, 0).

Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway(X; L) = σ(H1) (cid:12) H2 + (1 − σ(H1)) (cid:12) X,
where H1, H2 are properly-sized two matrices, output by a layer L
as [H1, H2] = L(X). The operator (cid:12) is the element-wise multipli-
cation, and σ is the element-wise sigmoid function. Hereafter let us
denote HCd←d

k(cid:63)δ (X) := Highway(X; C2d←d

k(cid:63)δ

).

Fig. 1. Network architecture.

TextEnc(L) := (HC2d←2d
HC2d←2d
3(cid:63)3

(cid:47)HC2d←2d
3(cid:63)1

1(cid:63)1
)2(cid:47)C2d←2d
1(cid:63)1

)2 (cid:47) (HC2d←2d

3(cid:63)1

(cid:47)ReLU(cid:47)C2d←e

)2 (cid:47) (HC2d←2d
(cid:47)
1(cid:63)1 (cid:47)CharEmbede-dim(L).

(cid:47) HC2d←2d
3(cid:63)9

3(cid:63)27

AudioEnc(S) := (HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←d
Cd←d

3(cid:63)3 )2 (cid:47)(HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←F

3(cid:63)27 (cid:47)HCd←d
1(cid:63)1 (S).

3(cid:63)9 (cid:47)HCd←d

3(cid:63)3 (cid:47)HCd←d

3(cid:63)1 )2 (cid:47)

AudioDec(R(cid:48)) := σ(cid:47)CF ←d
3(cid:63)3 (cid:47) HCd←d
HCd←d

3(cid:63)9 (cid:47) HCd←d

SSRN(Y ) := σ(cid:47)CF (cid:48)←F (cid:48)
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c
1(cid:63)1 (cid:47) (HCc←c
C2c←c

3(cid:63)1 )2 (cid:47)(HCd←d
3(cid:63)27 (cid:47)

1(cid:63)1 (cid:47)(ReLU(cid:47)Cd←d
1(cid:63)1 )3 (cid:47)(HCd←d
(R(cid:48)).
3(cid:63)1 ) (cid:47) Cd←2d
1(cid:63)1
(cid:47)(ReLU(cid:47)CF (cid:48)←F (cid:48)

3(cid:63)1 (cid:47) Dc←c

1(cid:63)1
2(cid:63)1 )2 (cid:47) (HCc←c

)2(cid:47)CF (cid:48)←2c
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c

(cid:47)(HC2c←2c
)2(cid:47)
3(cid:63)1
3(cid:63)1 ) (cid:47) Cc←F
1(cid:63)1 (Y ).

Fig. 2. Details of each component. For notation, see section 2.2.

3. PROPOSED NETWORK

Our DCTTS model consists of two networks: (1) Text2Mel, which
synthesize a mel spectrogram from an input text, and (2) Spectro-
gram Super-resolution Network (SSRN), which convert a coarse mel
spectrogram to the full STFT spectrogram. Fig. 1 shows the overall
architecture of the proposed method.

3.1. Text2Mel: Text to Mel Spectrogram Network

We ﬁrst consider to synthesize a coarse mel spectrogram from a
text. This is the main part of the proposed method. This module
consists of four submodules: Text Encoder, Audio Encoder, At-
tention, and Audio Decoder. The network TextEnc ﬁrst encodes
the input sentence L = [l1, . . . , lN ] ∈ CharN consisting of N
characters, into the two matrices K, V ∈ Rd×N . On the other
hand, the network AudioEnc encodes the coarse mel spectrogram
S(= S1:F,1:T ) ∈ RF ×T , of previously spoken speech, whose length
is T , into a matrix Q ∈ Rd×T .

(K, V ) = TextEnc(L).

Q = AudioEnc(S1:F,1:T ).

An attention matrix A ∈ RN ×T , deﬁned as follows, evaluates how
strongly the n-th character ln and t-th time frame S1:F,t are related,

A = softmaxn-axis(K TQ/

d).

√

Ant ∼ 1 implies that the module is looking at n-th character ln at
the time frame t, and it will look at ln or ln+1 or characters around
them, at the subsequent time frame t + 1. Whatever, let us expect
those are encoded in the n-th column of V . Thus a seed R ∈ Rd×T ,
decoded to the subsequent frames S1:F,2:T +1, is obtained as

R = Att(Q, K, V ) := V A. (Note: matrix product.)

(4)

(1)
(2)

(3)

The resultant R is concatenated with the encoded audio Q, as R(cid:48) =
[R, Q], because we found it beneﬁcial in our pilot study. Then, the
concatenated matrix R(cid:48) ∈ R2d×T is decoded by the Audio Decoder
module to synthesize a coarse mel spectrogram,

Y1:F,2:T +1 = AudioDec(R(cid:48)).

(5)

The result Y1:F,2:T +1 is compared with the temporally-shifted
ground truth S1:F,2:T +1, by a loss function Lspec(Y1:F,2:T +1|S1:F,2:T +1),
and the error is back-propagated to the network parameters. The loss
function was the sum of L1 loss and the binary divergence Dbin,

Dbin(Y |S) := Ef t[−Sf t log Yf t − (1 − Sf t) log(1 − Yf t)]
= Ef t[−Sf t ˆYf t + log(1 + exp ˆYf t)],

(6)

where ˆYf t = logit(Yf t). Since the binary divergence gives a non-
vanishing gradient to the network, ∂Dbin(Y |S)/∂ ˆYf t ∝ Yf t − Sf t,
it is advantageous in gradient-based training. It is easily veriﬁed that
the spectrogram error is non-negative, Lspec(Y |S) = Dbin(Y |S) +
E[|Yf t − Sf t|] ≥ 0, and the equality holds iff Y = S.

3.1.1. Details of TextEnc, AudioEnc, and AudioDec

Our networks are fully convolutional, and are not dependent on any
recurrent units. Instead of RNN, we sometimes take advantages of
dilated convolution [32, 13, 24] to take long contextual information
into account. The top equation of Fig. 2 is the content of TextEnc. It
consists of the character embedding and the stacked 1D non-causal
convolution. A previous literature [2] used a heavier RNN-based
component named ‘CBHG,’ but we found this simpler network also
works well. AudioEnc and AudioDec, shown in Fig. 2, are com-
posed of 1D causal convolution layers with Highway activation.
These convolution should be causal because the output of AudioDec
is feedbacked to the input of AudioEnc in the synthesis stage.

3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram |Z| ∈ RF (cid:48)×4T , from the
obtained coarse mel spectrogram Y ∈ RF ×T , by a spectrogram
super-resolution network (SSRN). Upsampling frequency from F to
F (cid:48) is rather straightforward. We can achieve that by increasing the
convolution channels of 1D convolutional network. Upsampling in
temporal direction is not similarly done, but by twice applying de-
convolution layers of stride size 2, we can quadruple the length of
sequence from T to 4T = T (cid:48). The bottom equation of Fig. 2 shows
SSRN. In this paper, as we do not consider online processing, all
convolutions can be non-causal. The loss function was the same as
Text2Mel: sum of binary divergence and L1 distance between the
synthesized spectrogram SSRN(S) and the ground truth |Z|.

4. GUIDED ATTENTION

4.1. Guided Attention Loss: Motivation, Method and Effects

In general, an attention module is quite costly to train. Therefore, if
there is some prior knowledge, it may be a help incorporating them
into the model to alleviate the heavy training. We show that the
simple measure below is helpful to train the attention module.

In TTS, the possible attention matrix A lies in the very small
subspace of RN ×T . This is because of the rough correspondence of
the order of the characters and the audio segments. That is, if one
reads a text, it is natural to assume that the text position n progresses
nearly linearly to the time t, i.e., n ∼ at, where a ∼ N/T . This is

Comparison of the attention matrix A, trained with
Fig. 3.
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is “icassp stands
for the international conference on acoustics, speech, and signal
processing.” We did not use the heuristics described in section 4.2.

the prominent difference of TTS from other seq2seq learning tech-
niques such as machine translation, in which an attention module
should resolve the word alignment between two languages that have
very different syntax, e.g. English and Japanese.

Based on this idea, we introduce another constraint on the at-
tention matrix A to prompt it to be ‘nearly diagonal,’ Latt(A) =
Ent[AntWnt], where Wnt = 1 − exp{−(n/N − t/T )2/2g2}. In
this paper, we set g = 0.2. If A is far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main loss Lspec with equal weight.

Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after ∼100 iterations. After ∼5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after ∼10K it-
erations, and it required ∼50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.

4.2. Forcibly Incremental Attention in Synthesis Stage

In the synthesis stage, the attention matrix A sometimes fails to look
at the correct characters. Typical errors we observed were (1) it occa-
sionally skipped several characters, and (2) it repeatedly read a same
word twice or more. In order to make the system more robust, we
heuristically modify the matrix A to be ‘nearly diagonal,’ by a simple
rule as follows. We observed this device sometimes alleviated such
misattentions. Let nt be the position of the character to be read at t-
th time frame; nt = argmaxnAn,t. Comparing the current position
nt and the previous position nt−1, unless the difference nt − nt−1
is within the range −1 ≤ nt − nt−1 ≤ 3, the current attention
position forcibly set to An,t = δn,nt−1+1 (Kronecker’s delta), to
forcibly make the attention target incremental, i.e., nt − nt−1 = 1.

5. EXPERIMENT

5.1. Experimental Setup

To train the networks, we used LJ Speech Dataset [33], a public do-
main speech dataset consisting of ∼13K pairs of text&speech, ∼24
hours in total, without phoneme-level alignment. These speech are
a little reverbed. We preprocessed the texts by spelling out some of

Table 1. Parameter Settings.

Sampling rate of audio signals
STFT window function
STFT window length and shift
STFT spectrogram size F (cid:48) × 4T
Mel spectrogram size F × T
Dimension e, d and c
ADAM parameters (α, β1, β2, ε)
Minibatch size
Emphasis factors (γ, η)
RTISI-LA window and iteration
Character set, Char

22050 Hz
Hanning
1024 (∼46.4 [ms]), 256 (∼11.6[ms])
513 × 4T (T depends on audio clip)
80 × T (T depends on audio clip)
128, 256, 512
(2 × 10−4, 0.5, 0.9, 10−6)
16
(0.6, 1.3)
100, 10
a-z,.’- and Space and NULL

Table 2. Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].

Method
Open Tacotron [5]
DCTTS
DCTTS
DCTTS
DCTTS

Iteration
12 days∗
877K∗
∼2 hours
20K/ 40K
∼7 hours
90K/150K
200K/340K ∼15 hours
540K/900K ∼40 hours

Time MOS (95% CI)
2.07 ± 0.62
1.74 ± 0.52
2.63 ± 0.75
2.71 ± 0.66
2.61 ± 0.62

abbreviations and numeric expressions, decapitalizing the capitals,
and removing less frequent characters not shown in Table 1, where
NULL is a dummy character for zero-padding.

We implemented our neural networks using Chainer 2.0 [34].
The models are trained on a household gamine PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.

For simplicity, we trained Text2Mel and SSRN independently
and asynchronously, using different GPUs. All network parameters
are initialized using He’s Gaussian initializer [35]. Both networks
were trained by ADAM optimizer [36]. When training SSRN, we
randomly extracted short sequences of length T = 64 for each it-
eration, to save the memory usage. To reduce the frequency of disc
access, we only took the snapshot of the parameters, every 5K itera-
tions. Other parameters are shown in Table 1.

As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds among the
open implementations.
It is reported that this model was trained
using LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti,
newer GPU than ours. Note, this iteration is still much less than the
original Tacotron, which was trained for more than 2M iterations.

We evaluated mean opinion scores (MOS) for both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [37]. We used 20 sentences from Harvard Sentences List
1&2. We synthesized the audio using 5 methods shown in Table 2.
The crowdworkers evaluated these 100 clips, rating them from 1
(Bad) to 5 (Excellent). Each worker is supposed to rate at least
10 clips. To obtain more responses with higher quality, we set a
few incentives shown in the literature. The results were statistically
processed using the method shown in the literature [37].

Fig. 4.
(Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input is “icassp stands for the international confer-
ence on acoustics, speech and signal processing.”

shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly look at the correct characters, and can synthesize quite
clear spectrograms. More samples will be available at the author’s
web page.1

In our crowdsourcing experiment, 31 subjective evaluated our
data. After the automatic screening by the toolkit [37], 560 scores
by 6 subjective were selected for ﬁnal statistics calculation. Table 2
compares the performance of our proposed method (DCTTS) and an
open Tacotron. Our MOS (95% conﬁdence interval) was 2.71±0.66
(15 hours training) while the Tacotron’s was 2.07 ± 0.62. Although
it is not a strict comparison since the frameworks and machines are
different, it would be still concluded that our proposed method is
quite rapidly trained to the satisfactory level compared to Tacotron.

6. SUMMARY AND FUTURE WORK

This paper described a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), as well as a technique
to train the attention module rapidly. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(∼15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in deep learning community. We
believe this handy method encourages further development of the
applications based on speech synthesis. We can expect this sim-
ple neural TTS may be extended to other versatile purposes, such
as emotional/non-linguistic/personalized speech synthesis, singing
voice synthesis, music synthesis, etc., by further studies. In addi-
tion, since a neural TTS has become this lightweight, the studies on
more integrated speech systems e.g. some multimodal systems, si-
multaneous training of TTS+ASR, and speech translation, etc., may
have become more feasible. These issues should be worked out in
the future.

7. ACKNOWLEDGEMENT

5.2. Result and Discussion

In our setting, the training throughput was ∼3.8 minibatch/s (Text2Mel)
and ∼6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel, 200K times only in 15 hours. Fig. 4

The authors would like to thank the OSS contributors, and the mem-
bers and contributors of LibriVox public domain audiobook project,
and @keithito who created the speech dataset.

1https://github.com/tachi-hi/tts_samples

8. REFERENCES

[21] D Bahdanau et al.,

learning to align and translate,”
arXiv:1409.0473, 2014.

“Neural machine translation by jointly
in Proc. ICLR 2015,

[22] Y. Kim,

classiﬁcation,”
arXiv:1408.5882.

“Convolutional neural networks for sentence
in Proc. EMNLP, 2014, pp. 1746–1752,

[23] X. Zhang et al., “Character-level convolutional networks for

text classiﬁcation,” in Proc. NIPS, 2015, arXiv:1509.01626.

[24] N. Kalchbrenner et al., “Neural machine translation in linear

time,” arXiv:1610.10099, 2016.

[25] Y. N. Dauphin et al., “Language modeling with gated convolu-

tional networks,” arXiv:1612.08083, 2016.

[26] J. Bradbury et al., “Quasi-recurrent neural networks,” in Proc.

ICLR 2017, 2016.

[27] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Trans. ASSP, vol. 32, no. 2, pp.
236–243, 1984.

[28] P. Mowlee et al., Phase-Aware Signal Processing in Speech

Communication: Theory and Practice, Wiley, 2016.

[29] X. Zhu et al.,

“Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra,” IEEE Trans.
ASLP, vol. 15, no. 5, 2007.

[30] Y. LeCun and Y. Bengio, “The handbook of brain theory and
neural networks,” chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255–258. MIT Press, 1998.

[31] R. K. Srivastava et al., “Training very deep networks,” in Proc.

NIPS, 2015, pp. 2377–2385.

[32] F. Yu and V. Koltun, “Multi-scale context aggregation by di-

lated convolutions,” in Proc. ICLR, 2016.

[33] K. Ito, “The LJ speech dataset,” 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).

[34] S. Tokui et al.,

“Chainer: A next-generation open source
framework for deep learning,” in Proc. Workshop LearningSys,
NIPS, 2015.

[35] K. He et al., “Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation,” in Proc. ICCV,
2015, pp. 1026–1034, arXiv:1502.01852.

[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” in Proc. ICLR 2015, 2014, arXiv:1412.6980.

[37] F. Ribeiro et al., “CrowdMOS: An approach for crowdsourcing
mean opinion score studies,” in Proc ICASSP, 2011, pp. 2416–
2419.

[1] I. Goodfellow et al., Deep Learning, MIT Press, 2016, http:

//www.deeplearningbook.org.

[2] Y. Wang et al., “Tacotron: Towards end-to-end speech synthe-

sis,” in Proc. Interspeech, 2017, arXiv:1703.10135.

[3] A. Barron,

“Implementation of Google’s Tacotron in Ten-
sorFlow,” 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).

[4] K. Park,

“A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model,” 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).

[5] K. Ito, “Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model,” 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).

[6] R. Yamamoto, “PyTorch implementation of Tacotron speech
synthesis model,” 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).

[7] J. Gehring et al., “Convolutional sequence to sequence learn-
ing,” in Proc. ICML, 2017, pp. 1243–1252, arXiv:1705.03122.

[8] H. Zen et al., “Statistical parametric speech synthesis using
deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.

[9] Y. Fan et al., “TTS synthesis with bidirectional LSTM based
recurrent neural networks,” in Proc. Interspeech, 2014, pp.
1964–1968.

[10] H. Zen and H. Sak, “Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis,” in Proc. ICASSP, 2015, pp. 4470–
4474.

[11] S. Achanta et al., “An investigation of recurrent neural network
architectures for statistical parametric speech synthesis.,” in
Proc. Interspeech, 2015, pp. 859–863.

[12] Z. Wu and S. King, “Investigating gated recurrent networks for
speech synthesis,” in Proc. ICASSP, 2016, pp. 5140–5144.

[13] A. van den Oord et al., “WaveNet: A generative model for raw

audio,” arXiv:1609.03499, 2016.

[14] J. Sotelo et al., “Char2wav: End-to-end speech synthesis,” in

Proc. ICLR, 2017.

[15] S. Arik et al., “Deep voice: Real-time neural text-to-speech,”
in Proc. ICML, 2017, pp. 195–204, arXiv:1702.07825.

[16] S. Arik et al., “Deep voice 2: Multi-speaker neural text-to-

speech,” in Proc. NIPS, 2017, arXiv:1705.08947.

[17] K. Cho et al., “Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in Proc.
EMNLP, 2014, pp. 1724–1734.

[18] I. Sutskever et al., “Sequence to sequence learning with neural

networks,” in Proc. NIPS, 2014, pp. 3104–3112.

[19] O. Vinyals and Q. Le, “A neural conversational model,” in

Proc. Deep Learning Workshop, ICML, 2015.

[20] I. V. Serban et al., “Building end-to-end dialogue systems us-
ing generative hierarchical neural network models.,” in Proc.
AAAI, 2016, pp. 3776–3784.

EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION

Hideyuki Tachibana, Katsuya Uenoyama

Shunsuke Aihara

PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.com

Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jp

7
1
0
2
 
t
c
O
 
4
2
 
 
]

D
S
.
s
c
[
 
 
1
v
9
6
9
8
0
.
0
1
7
1
:
v
i
X
r
a

ABSTRACT

This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without any recur-
rent units. Recurrent neural network (RNN) has been a standard
technique to model sequential data recently, and this technique has
been used in some cutting-edge neural TTS techniques. However,
training RNN component often requires a very powerful computer,
or very long time typically several days or weeks. Recent other stud-
ies, on the other hand, have shown that CNN-based sequence syn-
thesis can be much faster than RNN-based techniques, because of
high parallelizability. The objective of this paper is to show an al-
ternative neural TTS system, based only on CNN, that can alleviate
these economic costs of training. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Index Terms— Text-to-speech, deep learning, convolutional

neural network, attention, sequence-to-sequence learning.

1. INTRODUCTION

Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
encourage further use of TTS in various systems, it is signiﬁcant
to develop a handy, maintainable, extensible TTS component that
is accessible to speech non-specialists, enterprising individuals and
small teams who do not have massive computers.

Traditional TTS systems, however, are not necessarily friendly
for them, as these systems are typically composed of many domain-
speciﬁc modules. For example, a typical parametric TTS system is
an elaborate integration of many modules e.g. a text analyzer, an
F0 generator, a spectrum generator, a pause estimator, and a vocoder
that synthesize a waveform from these data, etc.

Deep learning [1] sometimes can unite these internal build-
ing blocks into a single model, and directly connects the input
and the output; this type of technique is sometimes called ‘end-to-
end’ learning. Although such a technique is sometimes criticized
as ‘a black box,’ nevertheless, an end-to-end TTS system named
Tacotron [2], which directly estimates a spectrogram from an in-
put text, has achieved promising performance recently, without
intensively-engineered parametric models based on domain-speciﬁc
knowledge. Tacotron, however, has a drawback that it exploits many
recurrent units, which are quite costly to train, making it almost
infeasible for ordinary labs without luxurious machines to study
and extend it further. Indeed, some people tried to implement open

clones of Tacotron [3, 4, 5, 6], but they are struggling to reproduce
the speech of satisfactory quality as clear as the original work.

The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel, handy neural TTS, which is fully convolutional.
The architecture is largely similar to Tacotron [2], but is based on
a fully convolutional sequence-to-sequence learning model similar
to the literature [7]. We show this handy TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention, which we call ‘guided attention,’ is also shown.

1.1. Related Work

1.1.1. Deep Learning and TTS

Recently, deep learning-based TTS systems have been intensively
studied, and some of recent studies are achieving surprisingly clear
results. The TTS systems based on deep neural networks include
Zen’s work in 2013 [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepVoice1&2 [15, 16], and Tacotron [2].

Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.

Most of the existing methods above use RNN, a natural tech-
nique for time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.

1.1.2. Sequence to Sequence (seq2seq) Learning

Recently, recurrent neural network (RNN) has been a standard tech-
nique to map a sequence into another sequence, especially in the ﬁeld
of natural language processing, e.g. machine translation [17, 18], di-
alogue system [19, 20], etc. See also [1, sec. 10.4].

RNN-based seq2seq, however, has some disadvantages. One
is that a vanilla encode-decoder model cannot encode too long se-
quence into a ﬁxed-length vector effectively. This problem has been

resolved by a mechanism called ‘attention’ [21], and the attention
mechanism now has become a standard idea in seq2seq learning
techniques; see also [1, sec. 12.4.5.1].

Another problem is that RNN typically requires much time to
train, since it is less suitable for parallel computation using GPUs.
In order to overcome this problem, several people proposed the use
of CNN, instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have
shown that CNN-based alternative networks can be trained much
faster, and can even outperform the RNN-based techniques.

Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea how to use attention mech-
anism in a CNN-based seq2seq learning model, and showed that
the method is quite effective for machine translation. Our proposed
method, indeed, is based on the similar idea to the literature [7].

2. PRELIMINARY

2.1. Basic Knowledge of the Audio Spectrograms

An audio waveform can be mutually converted to a complex spec-
trogram Z = {Zf,t} ∈ CF (cid:48)×T (cid:48)
by linear maps called STFT and
inverse STFT, where F (cid:48) and T (cid:48) denote the number of frequency
bins and temporal bins, respectively. It is common to consider only
the magnitude |Z| = {|Zf,t|}, since it still has useful information
for many purposes, and that |Z| is almost identical to Z in a sense
that there exist many phase estimation (∼ waveform synthesis) tech-
niques from magnitude spectrograms, e.g. the famous Grifﬁn&Lim
algorithm [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online G&L, to synthesize a waveform.
In this paper, we al-
ways normalize STFT spectrograms as |Z| ← (|Z|/ max(|Z|))γ,
and convert back |Z| ← |Z|η/γ when we ﬁnally need to synthesize
the waveform, where γ, η are pre- and post-emphasis factors.

It is also common to consider a mel spectrogram S ∈ RF ×T (cid:48)
,
(F (cid:28) F (cid:48)), by applying a mel ﬁlter-bank to |Z|. This is a standard
dimension reduction technique in speech processing. In this paper,
we also reduce the temporal dimensionality from T (cid:48) to (cid:100)T (cid:48)/4(cid:101) =: T
by picking up a time frame every four time frames, to accelerate the
training of Text2Mel shown below. We also normalize mel spectro-
grams as S ← (S/ max(S))γ.

2.2. Notation: Convolution and Highway Activation

In this paper, we denote 1D convolution layer [30] by a space sav-
ing notation Co←i
k(cid:63)δ (X), where i is the sizes of input channel, o is the
sizes of output channel, k is the size of kernel, δ is the dilation fac-
tor, and an argument X is a tensor having three dimensions (batch,
channel, temporal). The stride of convolution is always 1. Convolu-
tion layers are preceded by appropriately-sized zero padding, whose
size is suitably determined by a simple arithmetic so that the length
of the sequence is kept constant. Let us also denote the 1D deconvo-
lution layer as Do←i
k(cid:63)δ (X). The stride of deconvolution is always 2 in
this paper. Let us write a layer composition operator as · (cid:47) ·, and let
us write networks like F (cid:47) ReLU (cid:47) G(X) := F(ReLU(G(X))), and
(F (cid:47) G)2(X) := F (cid:47) G (cid:47) F (cid:47) G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU(x) = max(x, 0).

Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway(X; L) = σ(H1) (cid:12) H2 + (1 − σ(H1)) (cid:12) X,
where H1, H2 are properly-sized two matrices, output by a layer L
as [H1, H2] = L(X). The operator (cid:12) is the element-wise multipli-
cation, and σ is the element-wise sigmoid function. Hereafter let us
denote HCd←d

k(cid:63)δ (X) := Highway(X; C2d←d

k(cid:63)δ

).

Fig. 1. Network architecture.

TextEnc(L) := (HC2d←2d
HC2d←2d
3(cid:63)3

(cid:47)HC2d←2d
3(cid:63)1

1(cid:63)1
)2(cid:47)C2d←2d
1(cid:63)1

)2 (cid:47) (HC2d←2d

3(cid:63)1

(cid:47)ReLU(cid:47)C2d←e

)2 (cid:47) (HC2d←2d
(cid:47)
1(cid:63)1 (cid:47)CharEmbede-dim(L).

(cid:47) HC2d←2d
3(cid:63)9

3(cid:63)27

AudioEnc(S) := (HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←d
Cd←d

3(cid:63)3 )2 (cid:47)(HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←F

3(cid:63)27 (cid:47)HCd←d
1(cid:63)1 (S).

3(cid:63)9 (cid:47)HCd←d

3(cid:63)3 (cid:47)HCd←d

3(cid:63)1 )2 (cid:47)

AudioDec(R(cid:48)) := σ(cid:47)CF ←d
3(cid:63)3 (cid:47) HCd←d
HCd←d

3(cid:63)9 (cid:47) HCd←d

SSRN(Y ) := σ(cid:47)CF (cid:48)←F (cid:48)
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c
1(cid:63)1 (cid:47) (HCc←c
C2c←c

3(cid:63)1 )2 (cid:47)(HCd←d
3(cid:63)27 (cid:47)

1(cid:63)1 (cid:47)(ReLU(cid:47)Cd←d
1(cid:63)1 )3 (cid:47)(HCd←d
(R(cid:48)).
3(cid:63)1 ) (cid:47) Cd←2d
1(cid:63)1
(cid:47)(ReLU(cid:47)CF (cid:48)←F (cid:48)

3(cid:63)1 (cid:47) Dc←c

1(cid:63)1
2(cid:63)1 )2 (cid:47) (HCc←c

)2(cid:47)CF (cid:48)←2c
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c

(cid:47)(HC2c←2c
)2(cid:47)
3(cid:63)1
3(cid:63)1 ) (cid:47) Cc←F
1(cid:63)1 (Y ).

Fig. 2. Details of each component. For notation, see section 2.2.

3. PROPOSED NETWORK

Our DCTTS model consists of two networks: (1) Text2Mel, which
synthesize a mel spectrogram from an input text, and (2) Spectro-
gram Super-resolution Network (SSRN), which convert a coarse mel
spectrogram to the full STFT spectrogram. Fig. 1 shows the overall
architecture of the proposed method.

3.1. Text2Mel: Text to Mel Spectrogram Network

We ﬁrst consider to synthesize a coarse mel spectrogram from a
text. This is the main part of the proposed method. This module
consists of four submodules: Text Encoder, Audio Encoder, At-
tention, and Audio Decoder. The network TextEnc ﬁrst encodes
the input sentence L = [l1, . . . , lN ] ∈ CharN consisting of N
characters, into the two matrices K, V ∈ Rd×N . On the other
hand, the network AudioEnc encodes the coarse mel spectrogram
S(= S1:F,1:T ) ∈ RF ×T , of previously spoken speech, whose length
is T , into a matrix Q ∈ Rd×T .

(K, V ) = TextEnc(L).

Q = AudioEnc(S1:F,1:T ).

An attention matrix A ∈ RN ×T , deﬁned as follows, evaluates how
strongly the n-th character ln and t-th time frame S1:F,t are related,

A = softmaxn-axis(K TQ/

d).

√

Ant ∼ 1 implies that the module is looking at n-th character ln at
the time frame t, and it will look at ln or ln+1 or characters around
them, at the subsequent time frame t + 1. Whatever, let us expect
those are encoded in the n-th column of V . Thus a seed R ∈ Rd×T ,
decoded to the subsequent frames S1:F,2:T +1, is obtained as

R = Att(Q, K, V ) := V A. (Note: matrix product.)

(4)

(1)
(2)

(3)

The resultant R is concatenated with the encoded audio Q, as R(cid:48) =
[R, Q], because we found it beneﬁcial in our pilot study. Then, the
concatenated matrix R(cid:48) ∈ R2d×T is decoded by the Audio Decoder
module to synthesize a coarse mel spectrogram,

Y1:F,2:T +1 = AudioDec(R(cid:48)).

(5)

The result Y1:F,2:T +1 is compared with the temporally-shifted
ground truth S1:F,2:T +1, by a loss function Lspec(Y1:F,2:T +1|S1:F,2:T +1),
and the error is back-propagated to the network parameters. The loss
function was the sum of L1 loss and the binary divergence Dbin,

Dbin(Y |S) := Ef t[−Sf t log Yf t − (1 − Sf t) log(1 − Yf t)]
= Ef t[−Sf t ˆYf t + log(1 + exp ˆYf t)],

(6)

where ˆYf t = logit(Yf t). Since the binary divergence gives a non-
vanishing gradient to the network, ∂Dbin(Y |S)/∂ ˆYf t ∝ Yf t − Sf t,
it is advantageous in gradient-based training. It is easily veriﬁed that
the spectrogram error is non-negative, Lspec(Y |S) = Dbin(Y |S) +
E[|Yf t − Sf t|] ≥ 0, and the equality holds iff Y = S.

3.1.1. Details of TextEnc, AudioEnc, and AudioDec

Our networks are fully convolutional, and are not dependent on any
recurrent units. Instead of RNN, we sometimes take advantages of
dilated convolution [32, 13, 24] to take long contextual information
into account. The top equation of Fig. 2 is the content of TextEnc. It
consists of the character embedding and the stacked 1D non-causal
convolution. A previous literature [2] used a heavier RNN-based
component named ‘CBHG,’ but we found this simpler network also
works well. AudioEnc and AudioDec, shown in Fig. 2, are com-
posed of 1D causal convolution layers with Highway activation.
These convolution should be causal because the output of AudioDec
is feedbacked to the input of AudioEnc in the synthesis stage.

3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram |Z| ∈ RF (cid:48)×4T , from the
obtained coarse mel spectrogram Y ∈ RF ×T , by a spectrogram
super-resolution network (SSRN). Upsampling frequency from F to
F (cid:48) is rather straightforward. We can achieve that by increasing the
convolution channels of 1D convolutional network. Upsampling in
temporal direction is not similarly done, but by twice applying de-
convolution layers of stride size 2, we can quadruple the length of
sequence from T to 4T = T (cid:48). The bottom equation of Fig. 2 shows
SSRN. In this paper, as we do not consider online processing, all
convolutions can be non-causal. The loss function was the same as
Text2Mel: sum of binary divergence and L1 distance between the
synthesized spectrogram SSRN(S) and the ground truth |Z|.

4. GUIDED ATTENTION

4.1. Guided Attention Loss: Motivation, Method and Effects

In general, an attention module is quite costly to train. Therefore, if
there is some prior knowledge, it may be a help incorporating them
into the model to alleviate the heavy training. We show that the
simple measure below is helpful to train the attention module.

In TTS, the possible attention matrix A lies in the very small
subspace of RN ×T . This is because of the rough correspondence of
the order of the characters and the audio segments. That is, if one
reads a text, it is natural to assume that the text position n progresses
nearly linearly to the time t, i.e., n ∼ at, where a ∼ N/T . This is

Comparison of the attention matrix A, trained with
Fig. 3.
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is “icassp stands
for the international conference on acoustics, speech, and signal
processing.” We did not use the heuristics described in section 4.2.

the prominent difference of TTS from other seq2seq learning tech-
niques such as machine translation, in which an attention module
should resolve the word alignment between two languages that have
very different syntax, e.g. English and Japanese.

Based on this idea, we introduce another constraint on the at-
tention matrix A to prompt it to be ‘nearly diagonal,’ Latt(A) =
Ent[AntWnt], where Wnt = 1 − exp{−(n/N − t/T )2/2g2}. In
this paper, we set g = 0.2. If A is far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main loss Lspec with equal weight.

Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after ∼100 iterations. After ∼5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after ∼10K it-
erations, and it required ∼50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.

4.2. Forcibly Incremental Attention in Synthesis Stage

In the synthesis stage, the attention matrix A sometimes fails to look
at the correct characters. Typical errors we observed were (1) it occa-
sionally skipped several characters, and (2) it repeatedly read a same
word twice or more. In order to make the system more robust, we
heuristically modify the matrix A to be ‘nearly diagonal,’ by a simple
rule as follows. We observed this device sometimes alleviated such
misattentions. Let nt be the position of the character to be read at t-
th time frame; nt = argmaxnAn,t. Comparing the current position
nt and the previous position nt−1, unless the difference nt − nt−1
is within the range −1 ≤ nt − nt−1 ≤ 3, the current attention
position forcibly set to An,t = δn,nt−1+1 (Kronecker’s delta), to
forcibly make the attention target incremental, i.e., nt − nt−1 = 1.

5. EXPERIMENT

5.1. Experimental Setup

To train the networks, we used LJ Speech Dataset [33], a public do-
main speech dataset consisting of ∼13K pairs of text&speech, ∼24
hours in total, without phoneme-level alignment. These speech are
a little reverbed. We preprocessed the texts by spelling out some of

Table 1. Parameter Settings.

Sampling rate of audio signals
STFT window function
STFT window length and shift
STFT spectrogram size F (cid:48) × 4T
Mel spectrogram size F × T
Dimension e, d and c
ADAM parameters (α, β1, β2, ε)
Minibatch size
Emphasis factors (γ, η)
RTISI-LA window and iteration
Character set, Char

22050 Hz
Hanning
1024 (∼46.4 [ms]), 256 (∼11.6[ms])
513 × 4T (T depends on audio clip)
80 × T (T depends on audio clip)
128, 256, 512
(2 × 10−4, 0.5, 0.9, 10−6)
16
(0.6, 1.3)
100, 10
a-z,.’- and Space and NULL

Table 2. Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].

Method
Open Tacotron [5]
DCTTS
DCTTS
DCTTS
DCTTS

Iteration
12 days∗
877K∗
∼2 hours
20K/ 40K
∼7 hours
90K/150K
200K/340K ∼15 hours
540K/900K ∼40 hours

Time MOS (95% CI)
2.07 ± 0.62
1.74 ± 0.52
2.63 ± 0.75
2.71 ± 0.66
2.61 ± 0.62

abbreviations and numeric expressions, decapitalizing the capitals,
and removing less frequent characters not shown in Table 1, where
NULL is a dummy character for zero-padding.

We implemented our neural networks using Chainer 2.0 [34].
The models are trained on a household gamine PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.

For simplicity, we trained Text2Mel and SSRN independently
and asynchronously, using different GPUs. All network parameters
are initialized using He’s Gaussian initializer [35]. Both networks
were trained by ADAM optimizer [36]. When training SSRN, we
randomly extracted short sequences of length T = 64 for each it-
eration, to save the memory usage. To reduce the frequency of disc
access, we only took the snapshot of the parameters, every 5K itera-
tions. Other parameters are shown in Table 1.

As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds among the
open implementations.
It is reported that this model was trained
using LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti,
newer GPU than ours. Note, this iteration is still much less than the
original Tacotron, which was trained for more than 2M iterations.

We evaluated mean opinion scores (MOS) for both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [37]. We used 20 sentences from Harvard Sentences List
1&2. We synthesized the audio using 5 methods shown in Table 2.
The crowdworkers evaluated these 100 clips, rating them from 1
(Bad) to 5 (Excellent). Each worker is supposed to rate at least
10 clips. To obtain more responses with higher quality, we set a
few incentives shown in the literature. The results were statistically
processed using the method shown in the literature [37].

Fig. 4.
(Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input is “icassp stands for the international confer-
ence on acoustics, speech and signal processing.”

shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly look at the correct characters, and can synthesize quite
clear spectrograms. More samples will be available at the author’s
web page.1

In our crowdsourcing experiment, 31 subjective evaluated our
data. After the automatic screening by the toolkit [37], 560 scores
by 6 subjective were selected for ﬁnal statistics calculation. Table 2
compares the performance of our proposed method (DCTTS) and an
open Tacotron. Our MOS (95% conﬁdence interval) was 2.71±0.66
(15 hours training) while the Tacotron’s was 2.07 ± 0.62. Although
it is not a strict comparison since the frameworks and machines are
different, it would be still concluded that our proposed method is
quite rapidly trained to the satisfactory level compared to Tacotron.

6. SUMMARY AND FUTURE WORK

This paper described a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), as well as a technique
to train the attention module rapidly. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(∼15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in deep learning community. We
believe this handy method encourages further development of the
applications based on speech synthesis. We can expect this sim-
ple neural TTS may be extended to other versatile purposes, such
as emotional/non-linguistic/personalized speech synthesis, singing
voice synthesis, music synthesis, etc., by further studies. In addi-
tion, since a neural TTS has become this lightweight, the studies on
more integrated speech systems e.g. some multimodal systems, si-
multaneous training of TTS+ASR, and speech translation, etc., may
have become more feasible. These issues should be worked out in
the future.

7. ACKNOWLEDGEMENT

5.2. Result and Discussion

In our setting, the training throughput was ∼3.8 minibatch/s (Text2Mel)
and ∼6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel, 200K times only in 15 hours. Fig. 4

The authors would like to thank the OSS contributors, and the mem-
bers and contributors of LibriVox public domain audiobook project,
and @keithito who created the speech dataset.

1https://github.com/tachi-hi/tts_samples

8. REFERENCES

[21] D Bahdanau et al.,

learning to align and translate,”
arXiv:1409.0473, 2014.

“Neural machine translation by jointly
in Proc. ICLR 2015,

[22] Y. Kim,

classiﬁcation,”
arXiv:1408.5882.

“Convolutional neural networks for sentence
in Proc. EMNLP, 2014, pp. 1746–1752,

[23] X. Zhang et al., “Character-level convolutional networks for

text classiﬁcation,” in Proc. NIPS, 2015, arXiv:1509.01626.

[24] N. Kalchbrenner et al., “Neural machine translation in linear

time,” arXiv:1610.10099, 2016.

[25] Y. N. Dauphin et al., “Language modeling with gated convolu-

tional networks,” arXiv:1612.08083, 2016.

[26] J. Bradbury et al., “Quasi-recurrent neural networks,” in Proc.

ICLR 2017, 2016.

[27] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Trans. ASSP, vol. 32, no. 2, pp.
236–243, 1984.

[28] P. Mowlee et al., Phase-Aware Signal Processing in Speech

Communication: Theory and Practice, Wiley, 2016.

[29] X. Zhu et al.,

“Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra,” IEEE Trans.
ASLP, vol. 15, no. 5, 2007.

[30] Y. LeCun and Y. Bengio, “The handbook of brain theory and
neural networks,” chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255–258. MIT Press, 1998.

[31] R. K. Srivastava et al., “Training very deep networks,” in Proc.

NIPS, 2015, pp. 2377–2385.

[32] F. Yu and V. Koltun, “Multi-scale context aggregation by di-

lated convolutions,” in Proc. ICLR, 2016.

[33] K. Ito, “The LJ speech dataset,” 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).

[34] S. Tokui et al.,

“Chainer: A next-generation open source
framework for deep learning,” in Proc. Workshop LearningSys,
NIPS, 2015.

[35] K. He et al., “Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation,” in Proc. ICCV,
2015, pp. 1026–1034, arXiv:1502.01852.

[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” in Proc. ICLR 2015, 2014, arXiv:1412.6980.

[37] F. Ribeiro et al., “CrowdMOS: An approach for crowdsourcing
mean opinion score studies,” in Proc ICASSP, 2011, pp. 2416–
2419.

[1] I. Goodfellow et al., Deep Learning, MIT Press, 2016, http:

//www.deeplearningbook.org.

[2] Y. Wang et al., “Tacotron: Towards end-to-end speech synthe-

sis,” in Proc. Interspeech, 2017, arXiv:1703.10135.

[3] A. Barron,

“Implementation of Google’s Tacotron in Ten-
sorFlow,” 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).

[4] K. Park,

“A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model,” 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).

[5] K. Ito, “Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model,” 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).

[6] R. Yamamoto, “PyTorch implementation of Tacotron speech
synthesis model,” 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).

[7] J. Gehring et al., “Convolutional sequence to sequence learn-
ing,” in Proc. ICML, 2017, pp. 1243–1252, arXiv:1705.03122.

[8] H. Zen et al., “Statistical parametric speech synthesis using
deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.

[9] Y. Fan et al., “TTS synthesis with bidirectional LSTM based
recurrent neural networks,” in Proc. Interspeech, 2014, pp.
1964–1968.

[10] H. Zen and H. Sak, “Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis,” in Proc. ICASSP, 2015, pp. 4470–
4474.

[11] S. Achanta et al., “An investigation of recurrent neural network
architectures for statistical parametric speech synthesis.,” in
Proc. Interspeech, 2015, pp. 859–863.

[12] Z. Wu and S. King, “Investigating gated recurrent networks for
speech synthesis,” in Proc. ICASSP, 2016, pp. 5140–5144.

[13] A. van den Oord et al., “WaveNet: A generative model for raw

audio,” arXiv:1609.03499, 2016.

[14] J. Sotelo et al., “Char2wav: End-to-end speech synthesis,” in

Proc. ICLR, 2017.

[15] S. Arik et al., “Deep voice: Real-time neural text-to-speech,”
in Proc. ICML, 2017, pp. 195–204, arXiv:1702.07825.

[16] S. Arik et al., “Deep voice 2: Multi-speaker neural text-to-

speech,” in Proc. NIPS, 2017, arXiv:1705.08947.

[17] K. Cho et al., “Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in Proc.
EMNLP, 2014, pp. 1724–1734.

[18] I. Sutskever et al., “Sequence to sequence learning with neural

networks,” in Proc. NIPS, 2014, pp. 3104–3112.

[19] O. Vinyals and Q. Le, “A neural conversational model,” in

Proc. Deep Learning Workshop, ICML, 2015.

[20] I. V. Serban et al., “Building end-to-end dialogue systems us-
ing generative hierarchical neural network models.,” in Proc.
AAAI, 2016, pp. 3776–3784.

EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ON
DEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION

Hideyuki Tachibana, Katsuya Uenoyama

Shunsuke Aihara

PKSHA Technology, Inc.
Bunkyo, Tokyo, Japan
h_tachibana@pkshatech.com

Independent Researcher
Shinjuku, Tokyo, Japan
aihara@argmax.jp

7
1
0
2
 
t
c
O
 
4
2
 
 
]

D
S
.
s
c
[
 
 
1
v
9
6
9
8
0
.
0
1
7
1
:
v
i
X
r
a

ABSTRACT

This paper describes a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), without any recur-
rent units. Recurrent neural network (RNN) has been a standard
technique to model sequential data recently, and this technique has
been used in some cutting-edge neural TTS techniques. However,
training RNN component often requires a very powerful computer,
or very long time typically several days or weeks. Recent other stud-
ies, on the other hand, have shown that CNN-based sequence syn-
thesis can be much faster than RNN-based techniques, because of
high parallelizability. The objective of this paper is to show an al-
ternative neural TTS system, based only on CNN, that can alleviate
these economic costs of training. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Index Terms— Text-to-speech, deep learning, convolutional

neural network, attention, sequence-to-sequence learning.

1. INTRODUCTION

Text-to-speech (TTS) is getting more and more common recently,
and is getting to be a basic user interface for many systems. To
encourage further use of TTS in various systems, it is signiﬁcant
to develop a handy, maintainable, extensible TTS component that
is accessible to speech non-specialists, enterprising individuals and
small teams who do not have massive computers.

Traditional TTS systems, however, are not necessarily friendly
for them, as these systems are typically composed of many domain-
speciﬁc modules. For example, a typical parametric TTS system is
an elaborate integration of many modules e.g. a text analyzer, an
F0 generator, a spectrum generator, a pause estimator, and a vocoder
that synthesize a waveform from these data, etc.

Deep learning [1] sometimes can unite these internal build-
ing blocks into a single model, and directly connects the input
and the output; this type of technique is sometimes called ‘end-to-
end’ learning. Although such a technique is sometimes criticized
as ‘a black box,’ nevertheless, an end-to-end TTS system named
Tacotron [2], which directly estimates a spectrogram from an in-
put text, has achieved promising performance recently, without
intensively-engineered parametric models based on domain-speciﬁc
knowledge. Tacotron, however, has a drawback that it exploits many
recurrent units, which are quite costly to train, making it almost
infeasible for ordinary labs without luxurious machines to study
and extend it further. Indeed, some people tried to implement open

clones of Tacotron [3, 4, 5, 6], but they are struggling to reproduce
the speech of satisfactory quality as clear as the original work.

The purpose of this paper is to show Deep Convolutional TTS
(DCTTS), a novel, handy neural TTS, which is fully convolutional.
The architecture is largely similar to Tacotron [2], but is based on
a fully convolutional sequence-to-sequence learning model similar
to the literature [7]. We show this handy TTS actually works in a
reasonable setting. The contribution of this article is twofold: (1)
Propose a fully CNN-based TTS system which can be trained much
faster than an RNN-based state-of-the-art neural TTS system, while
the sound quality is still acceptable. (2) An idea to rapidly train the
attention, which we call ‘guided attention,’ is also shown.

1.1. Related Work

1.1.1. Deep Learning and TTS

Recently, deep learning-based TTS systems have been intensively
studied, and some of recent studies are achieving surprisingly clear
results. The TTS systems based on deep neural networks include
Zen’s work in 2013 [8], the studies based on RNN e.g. [9, 10, 11,
12], and recently proposed techniques e.g. WaveNet [13, sec. 3.2],
Char2Wav [14], DeepVoice1&2 [15, 16], and Tacotron [2].

Some of them tried to reduce the dependency on hand-engineered
internal modules. The most extreme technique in this trend would
be Tacotron [2], which depends only on mel and linear spectro-
grams, and not on any other speech features e.g. F0. Our method is
close to Tacotron in a sense that it depends only on these spectral
representations of audio signals.

Most of the existing methods above use RNN, a natural tech-
nique for time series prediction. An exception is WaveNet, which
is fully convolutional. Our method is also based only on CNN but
our usage of CNN would be different from WaveNet, as WaveNet is
a kind of a vocoder, or a back-end, which synthesizes a waveform
from some conditioning information that is given by front-end com-
ponents. On the other hand, ours is rather a front-end (and most of
back-end processing). We use CNN to synthesize a spectrogram,
from which a simple vocoder can synthesize a waveform.

1.1.2. Sequence to Sequence (seq2seq) Learning

Recently, recurrent neural network (RNN) has been a standard tech-
nique to map a sequence into another sequence, especially in the ﬁeld
of natural language processing, e.g. machine translation [17, 18], di-
alogue system [19, 20], etc. See also [1, sec. 10.4].

RNN-based seq2seq, however, has some disadvantages. One
is that a vanilla encode-decoder model cannot encode too long se-
quence into a ﬁxed-length vector effectively. This problem has been

resolved by a mechanism called ‘attention’ [21], and the attention
mechanism now has become a standard idea in seq2seq learning
techniques; see also [1, sec. 12.4.5.1].

Another problem is that RNN typically requires much time to
train, since it is less suitable for parallel computation using GPUs.
In order to overcome this problem, several people proposed the use
of CNN, instead of RNN, e.g. [22, 23, 24, 25, 26]. Some studies have
shown that CNN-based alternative networks can be trained much
faster, and can even outperform the RNN-based techniques.

Gehring et al. [7] recently united these two improvements of
seq2seq learning. They proposed an idea how to use attention mech-
anism in a CNN-based seq2seq learning model, and showed that
the method is quite effective for machine translation. Our proposed
method, indeed, is based on the similar idea to the literature [7].

2. PRELIMINARY

2.1. Basic Knowledge of the Audio Spectrograms

An audio waveform can be mutually converted to a complex spec-
trogram Z = {Zf,t} ∈ CF (cid:48)×T (cid:48)
by linear maps called STFT and
inverse STFT, where F (cid:48) and T (cid:48) denote the number of frequency
bins and temporal bins, respectively. It is common to consider only
the magnitude |Z| = {|Zf,t|}, since it still has useful information
for many purposes, and that |Z| is almost identical to Z in a sense
that there exist many phase estimation (∼ waveform synthesis) tech-
niques from magnitude spectrograms, e.g. the famous Grifﬁn&Lim
algorithm [27]; see also e.g. [28]. We always use RTISI-LA [29],
an online G&L, to synthesize a waveform.
In this paper, we al-
ways normalize STFT spectrograms as |Z| ← (|Z|/ max(|Z|))γ,
and convert back |Z| ← |Z|η/γ when we ﬁnally need to synthesize
the waveform, where γ, η are pre- and post-emphasis factors.

It is also common to consider a mel spectrogram S ∈ RF ×T (cid:48)
,
(F (cid:28) F (cid:48)), by applying a mel ﬁlter-bank to |Z|. This is a standard
dimension reduction technique in speech processing. In this paper,
we also reduce the temporal dimensionality from T (cid:48) to (cid:100)T (cid:48)/4(cid:101) =: T
by picking up a time frame every four time frames, to accelerate the
training of Text2Mel shown below. We also normalize mel spectro-
grams as S ← (S/ max(S))γ.

2.2. Notation: Convolution and Highway Activation

In this paper, we denote 1D convolution layer [30] by a space sav-
ing notation Co←i
k(cid:63)δ (X), where i is the sizes of input channel, o is the
sizes of output channel, k is the size of kernel, δ is the dilation fac-
tor, and an argument X is a tensor having three dimensions (batch,
channel, temporal). The stride of convolution is always 1. Convolu-
tion layers are preceded by appropriately-sized zero padding, whose
size is suitably determined by a simple arithmetic so that the length
of the sequence is kept constant. Let us also denote the 1D deconvo-
lution layer as Do←i
k(cid:63)δ (X). The stride of deconvolution is always 2 in
this paper. Let us write a layer composition operator as · (cid:47) ·, and let
us write networks like F (cid:47) ReLU (cid:47) G(X) := F(ReLU(G(X))), and
(F (cid:47) G)2(X) := F (cid:47) G (cid:47) F (cid:47) G(X), etc. ReLU is an element-wise
activation function deﬁned by ReLU(x) = max(x, 0).

Convolution layers are sometimes followed by a Highway Net-
work [31]-like gated activation, which is advantageous in very deep
networks: Highway(X; L) = σ(H1) (cid:12) H2 + (1 − σ(H1)) (cid:12) X,
where H1, H2 are properly-sized two matrices, output by a layer L
as [H1, H2] = L(X). The operator (cid:12) is the element-wise multipli-
cation, and σ is the element-wise sigmoid function. Hereafter let us
denote HCd←d

k(cid:63)δ (X) := Highway(X; C2d←d

k(cid:63)δ

).

Fig. 1. Network architecture.

TextEnc(L) := (HC2d←2d
HC2d←2d
3(cid:63)3

(cid:47)HC2d←2d
3(cid:63)1

1(cid:63)1
)2(cid:47)C2d←2d
1(cid:63)1

)2 (cid:47) (HC2d←2d

3(cid:63)1

(cid:47)ReLU(cid:47)C2d←e

)2 (cid:47) (HC2d←2d
(cid:47)
1(cid:63)1 (cid:47)CharEmbede-dim(L).

(cid:47) HC2d←2d
3(cid:63)9

3(cid:63)27

AudioEnc(S) := (HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←d
Cd←d

3(cid:63)3 )2 (cid:47)(HCd←d
1(cid:63)1 (cid:47) ReLU (cid:47) Cd←F

3(cid:63)27 (cid:47)HCd←d
1(cid:63)1 (S).

3(cid:63)9 (cid:47)HCd←d

3(cid:63)3 (cid:47)HCd←d

3(cid:63)1 )2 (cid:47)

AudioDec(R(cid:48)) := σ(cid:47)CF ←d
3(cid:63)3 (cid:47) HCd←d
HCd←d

3(cid:63)9 (cid:47) HCd←d

SSRN(Y ) := σ(cid:47)CF (cid:48)←F (cid:48)
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c
1(cid:63)1 (cid:47) (HCc←c
C2c←c

3(cid:63)1 )2 (cid:47)(HCd←d
3(cid:63)27 (cid:47)

1(cid:63)1 (cid:47)(ReLU(cid:47)Cd←d
1(cid:63)1 )3 (cid:47)(HCd←d
(R(cid:48)).
3(cid:63)1 ) (cid:47) Cd←2d
1(cid:63)1
(cid:47)(ReLU(cid:47)CF (cid:48)←F (cid:48)

3(cid:63)1 (cid:47) Dc←c

1(cid:63)1
2(cid:63)1 )2 (cid:47) (HCc←c

)2(cid:47)CF (cid:48)←2c
1(cid:63)1
3(cid:63)3 (cid:47) HCc←c

(cid:47)(HC2c←2c
)2(cid:47)
3(cid:63)1
3(cid:63)1 ) (cid:47) Cc←F
1(cid:63)1 (Y ).

Fig. 2. Details of each component. For notation, see section 2.2.

3. PROPOSED NETWORK

Our DCTTS model consists of two networks: (1) Text2Mel, which
synthesize a mel spectrogram from an input text, and (2) Spectro-
gram Super-resolution Network (SSRN), which convert a coarse mel
spectrogram to the full STFT spectrogram. Fig. 1 shows the overall
architecture of the proposed method.

3.1. Text2Mel: Text to Mel Spectrogram Network

We ﬁrst consider to synthesize a coarse mel spectrogram from a
text. This is the main part of the proposed method. This module
consists of four submodules: Text Encoder, Audio Encoder, At-
tention, and Audio Decoder. The network TextEnc ﬁrst encodes
the input sentence L = [l1, . . . , lN ] ∈ CharN consisting of N
characters, into the two matrices K, V ∈ Rd×N . On the other
hand, the network AudioEnc encodes the coarse mel spectrogram
S(= S1:F,1:T ) ∈ RF ×T , of previously spoken speech, whose length
is T , into a matrix Q ∈ Rd×T .

(K, V ) = TextEnc(L).

Q = AudioEnc(S1:F,1:T ).

An attention matrix A ∈ RN ×T , deﬁned as follows, evaluates how
strongly the n-th character ln and t-th time frame S1:F,t are related,

A = softmaxn-axis(K TQ/

d).

√

Ant ∼ 1 implies that the module is looking at n-th character ln at
the time frame t, and it will look at ln or ln+1 or characters around
them, at the subsequent time frame t + 1. Whatever, let us expect
those are encoded in the n-th column of V . Thus a seed R ∈ Rd×T ,
decoded to the subsequent frames S1:F,2:T +1, is obtained as

R = Att(Q, K, V ) := V A. (Note: matrix product.)

(4)

(1)
(2)

(3)

The resultant R is concatenated with the encoded audio Q, as R(cid:48) =
[R, Q], because we found it beneﬁcial in our pilot study. Then, the
concatenated matrix R(cid:48) ∈ R2d×T is decoded by the Audio Decoder
module to synthesize a coarse mel spectrogram,

Y1:F,2:T +1 = AudioDec(R(cid:48)).

(5)

The result Y1:F,2:T +1 is compared with the temporally-shifted
ground truth S1:F,2:T +1, by a loss function Lspec(Y1:F,2:T +1|S1:F,2:T +1),
and the error is back-propagated to the network parameters. The loss
function was the sum of L1 loss and the binary divergence Dbin,

Dbin(Y |S) := Ef t[−Sf t log Yf t − (1 − Sf t) log(1 − Yf t)]
= Ef t[−Sf t ˆYf t + log(1 + exp ˆYf t)],

(6)

where ˆYf t = logit(Yf t). Since the binary divergence gives a non-
vanishing gradient to the network, ∂Dbin(Y |S)/∂ ˆYf t ∝ Yf t − Sf t,
it is advantageous in gradient-based training. It is easily veriﬁed that
the spectrogram error is non-negative, Lspec(Y |S) = Dbin(Y |S) +
E[|Yf t − Sf t|] ≥ 0, and the equality holds iff Y = S.

3.1.1. Details of TextEnc, AudioEnc, and AudioDec

Our networks are fully convolutional, and are not dependent on any
recurrent units. Instead of RNN, we sometimes take advantages of
dilated convolution [32, 13, 24] to take long contextual information
into account. The top equation of Fig. 2 is the content of TextEnc. It
consists of the character embedding and the stacked 1D non-causal
convolution. A previous literature [2] used a heavier RNN-based
component named ‘CBHG,’ but we found this simpler network also
works well. AudioEnc and AudioDec, shown in Fig. 2, are com-
posed of 1D causal convolution layers with Highway activation.
These convolution should be causal because the output of AudioDec
is feedbacked to the input of AudioEnc in the synthesis stage.

3.2. Spectrogram Super-resolution Network (SSRN)
We ﬁnally synthesize a full spectrogram |Z| ∈ RF (cid:48)×4T , from the
obtained coarse mel spectrogram Y ∈ RF ×T , by a spectrogram
super-resolution network (SSRN). Upsampling frequency from F to
F (cid:48) is rather straightforward. We can achieve that by increasing the
convolution channels of 1D convolutional network. Upsampling in
temporal direction is not similarly done, but by twice applying de-
convolution layers of stride size 2, we can quadruple the length of
sequence from T to 4T = T (cid:48). The bottom equation of Fig. 2 shows
SSRN. In this paper, as we do not consider online processing, all
convolutions can be non-causal. The loss function was the same as
Text2Mel: sum of binary divergence and L1 distance between the
synthesized spectrogram SSRN(S) and the ground truth |Z|.

4. GUIDED ATTENTION

4.1. Guided Attention Loss: Motivation, Method and Effects

In general, an attention module is quite costly to train. Therefore, if
there is some prior knowledge, it may be a help incorporating them
into the model to alleviate the heavy training. We show that the
simple measure below is helpful to train the attention module.

In TTS, the possible attention matrix A lies in the very small
subspace of RN ×T . This is because of the rough correspondence of
the order of the characters and the audio segments. That is, if one
reads a text, it is natural to assume that the text position n progresses
nearly linearly to the time t, i.e., n ∼ at, where a ∼ N/T . This is

Comparison of the attention matrix A, trained with
Fig. 3.
and without the guided attention loss Latt(A). (Left) Without, and
(Right) with the guided attention. The test text is “icassp stands
for the international conference on acoustics, speech, and signal
processing.” We did not use the heuristics described in section 4.2.

the prominent difference of TTS from other seq2seq learning tech-
niques such as machine translation, in which an attention module
should resolve the word alignment between two languages that have
very different syntax, e.g. English and Japanese.

Based on this idea, we introduce another constraint on the at-
tention matrix A to prompt it to be ‘nearly diagonal,’ Latt(A) =
Ent[AntWnt], where Wnt = 1 − exp{−(n/N − t/T )2/2g2}. In
this paper, we set g = 0.2. If A is far from diagonal (e.g., reading the
characters in the random order), it is strongly penalized by the loss
function. This subsidiary loss function is simultaneously optimized
with the main loss Lspec with equal weight.

Although this measure is based on quite a rough assumption,
it improved the training efﬁciency. In our experiment, if we added
the guided attention loss to the objective, the term began decreas-
ing only after ∼100 iterations. After ∼5K iterations, the attention
became roughly correct, not only for training data, but also for new
input texts. On the other hand, without the guided attention loss,
it required much more iterations. It began learning after ∼10K it-
erations, and it required ∼50K iterations to look at roughly correct
positions, but the attention matrix was still vague. Fig. 3 compares
the attention matrix, trained with and without guided attention loss.

4.2. Forcibly Incremental Attention in Synthesis Stage

In the synthesis stage, the attention matrix A sometimes fails to look
at the correct characters. Typical errors we observed were (1) it occa-
sionally skipped several characters, and (2) it repeatedly read a same
word twice or more. In order to make the system more robust, we
heuristically modify the matrix A to be ‘nearly diagonal,’ by a simple
rule as follows. We observed this device sometimes alleviated such
misattentions. Let nt be the position of the character to be read at t-
th time frame; nt = argmaxnAn,t. Comparing the current position
nt and the previous position nt−1, unless the difference nt − nt−1
is within the range −1 ≤ nt − nt−1 ≤ 3, the current attention
position forcibly set to An,t = δn,nt−1+1 (Kronecker’s delta), to
forcibly make the attention target incremental, i.e., nt − nt−1 = 1.

5. EXPERIMENT

5.1. Experimental Setup

To train the networks, we used LJ Speech Dataset [33], a public do-
main speech dataset consisting of ∼13K pairs of text&speech, ∼24
hours in total, without phoneme-level alignment. These speech are
a little reverbed. We preprocessed the texts by spelling out some of

Table 1. Parameter Settings.

Sampling rate of audio signals
STFT window function
STFT window length and shift
STFT spectrogram size F (cid:48) × 4T
Mel spectrogram size F × T
Dimension e, d and c
ADAM parameters (α, β1, β2, ε)
Minibatch size
Emphasis factors (γ, η)
RTISI-LA window and iteration
Character set, Char

22050 Hz
Hanning
1024 (∼46.4 [ms]), 256 (∼11.6[ms])
513 × 4T (T depends on audio clip)
80 × T (T depends on audio clip)
128, 256, 512
(2 × 10−4, 0.5, 0.9, 10−6)
16
(0.6, 1.3)
100, 10
a-z,.’- and Space and NULL

Table 2. Comparison of MOS (95% conﬁdence interval), training
time, and iterations (Text2Mel/SSRN), of an open Tacotron [5] and
the proposed method (DCTTS). The digits with * were excerpted
from the repository [5].

Method
Open Tacotron [5]
DCTTS
DCTTS
DCTTS
DCTTS

Iteration
12 days∗
877K∗
∼2 hours
20K/ 40K
∼7 hours
90K/150K
200K/340K ∼15 hours
540K/900K ∼40 hours

Time MOS (95% CI)
2.07 ± 0.62
1.74 ± 0.52
2.63 ± 0.75
2.71 ± 0.66
2.61 ± 0.62

abbreviations and numeric expressions, decapitalizing the capitals,
and removing less frequent characters not shown in Table 1, where
NULL is a dummy character for zero-padding.

We implemented our neural networks using Chainer 2.0 [34].
The models are trained on a household gamine PC equipped with
two GPUs. The main memory of the machine was 62GB, which
is much larger than the audio dataset. Both GPUs were NVIDIA
GeForce GTX 980 Ti, with 6 GB memories.

For simplicity, we trained Text2Mel and SSRN independently
and asynchronously, using different GPUs. All network parameters
are initialized using He’s Gaussian initializer [35]. Both networks
were trained by ADAM optimizer [36]. When training SSRN, we
randomly extracted short sequences of length T = 64 for each it-
eration, to save the memory usage. To reduce the frequency of disc
access, we only took the snapshot of the parameters, every 5K itera-
tions. Other parameters are shown in Table 1.

As it is not easy for us to reproduce the original results of
Tacotron, we instead used a ready-to-use model [5] for comparison,
which seemed to produce the most reasonable sounds among the
open implementations.
It is reported that this model was trained
using LJ Dataset for 12 days (877K iterations) on a GTX 1080 Ti,
newer GPU than ours. Note, this iteration is still much less than the
original Tacotron, which was trained for more than 2M iterations.

We evaluated mean opinion scores (MOS) for both methods
by crowdsourcing on Amazon Mechanical Turk using crowdMOS
toolkit [37]. We used 20 sentences from Harvard Sentences List
1&2. We synthesized the audio using 5 methods shown in Table 2.
The crowdworkers evaluated these 100 clips, rating them from 1
(Bad) to 5 (Excellent). Each worker is supposed to rate at least
10 clips. To obtain more responses with higher quality, we set a
few incentives shown in the literature. The results were statistically
processed using the method shown in the literature [37].

Fig. 4.
(Top) Attention, (middle) mel, and (bottom) linear STFT
spectrogram, synthesized by the proposed method, after 15 hours
training. The input is “icassp stands for the international confer-
ence on acoustics, speech and signal processing.”

shows an example of attention, synthesized mel and full spectro-
grams, after 15 hours training. It shows that the method can almost
correctly look at the correct characters, and can synthesize quite
clear spectrograms. More samples will be available at the author’s
web page.1

In our crowdsourcing experiment, 31 subjective evaluated our
data. After the automatic screening by the toolkit [37], 560 scores
by 6 subjective were selected for ﬁnal statistics calculation. Table 2
compares the performance of our proposed method (DCTTS) and an
open Tacotron. Our MOS (95% conﬁdence interval) was 2.71±0.66
(15 hours training) while the Tacotron’s was 2.07 ± 0.62. Although
it is not a strict comparison since the frameworks and machines are
different, it would be still concluded that our proposed method is
quite rapidly trained to the satisfactory level compared to Tacotron.

6. SUMMARY AND FUTURE WORK

This paper described a novel text-to-speech (TTS) technique based
on deep convolutional neural networks (CNN), as well as a technique
to train the attention module rapidly. In our experiment, the proposed
Deep Convolutional TTS can be sufﬁciently trained only in a night
(∼15 hours), using an ordinary gaming PC equipped with two GPUs,
while the quality of the synthesized speech was almost acceptable.

Although the audio quality is far from perfect yet, it may be
improved by tuning some hyper-parameters thoroughly, and by ap-
plying some techniques developed in deep learning community. We
believe this handy method encourages further development of the
applications based on speech synthesis. We can expect this sim-
ple neural TTS may be extended to other versatile purposes, such
as emotional/non-linguistic/personalized speech synthesis, singing
voice synthesis, music synthesis, etc., by further studies. In addi-
tion, since a neural TTS has become this lightweight, the studies on
more integrated speech systems e.g. some multimodal systems, si-
multaneous training of TTS+ASR, and speech translation, etc., may
have become more feasible. These issues should be worked out in
the future.

7. ACKNOWLEDGEMENT

5.2. Result and Discussion

In our setting, the training throughput was ∼3.8 minibatch/s (Text2Mel)
and ∼6.4 minibatch/s (SSRN). This implies that we can iterate the
updating formulae of Text2Mel, 200K times only in 15 hours. Fig. 4

The authors would like to thank the OSS contributors, and the mem-
bers and contributors of LibriVox public domain audiobook project,
and @keithito who created the speech dataset.

1https://github.com/tachi-hi/tts_samples

8. REFERENCES

[21] D Bahdanau et al.,

learning to align and translate,”
arXiv:1409.0473, 2014.

“Neural machine translation by jointly
in Proc. ICLR 2015,

[22] Y. Kim,

classiﬁcation,”
arXiv:1408.5882.

“Convolutional neural networks for sentence
in Proc. EMNLP, 2014, pp. 1746–1752,

[23] X. Zhang et al., “Character-level convolutional networks for

text classiﬁcation,” in Proc. NIPS, 2015, arXiv:1509.01626.

[24] N. Kalchbrenner et al., “Neural machine translation in linear

time,” arXiv:1610.10099, 2016.

[25] Y. N. Dauphin et al., “Language modeling with gated convolu-

tional networks,” arXiv:1612.08083, 2016.

[26] J. Bradbury et al., “Quasi-recurrent neural networks,” in Proc.

ICLR 2017, 2016.

[27] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Trans. ASSP, vol. 32, no. 2, pp.
236–243, 1984.

[28] P. Mowlee et al., Phase-Aware Signal Processing in Speech

Communication: Theory and Practice, Wiley, 2016.

[29] X. Zhu et al.,

“Real-time signal estimation from modiﬁed
short-time fourier transform magnitude spectra,” IEEE Trans.
ASLP, vol. 15, no. 5, 2007.

[30] Y. LeCun and Y. Bengio, “The handbook of brain theory and
neural networks,” chapter Convolutional Networks for Images,
Speech, and Time Series, pp. 255–258. MIT Press, 1998.

[31] R. K. Srivastava et al., “Training very deep networks,” in Proc.

NIPS, 2015, pp. 2377–2385.

[32] F. Yu and V. Koltun, “Multi-scale context aggregation by di-

lated convolutions,” in Proc. ICLR, 2016.

[33] K. Ito, “The LJ speech dataset,” 2017, Available at https://
keithito.com/LJ-Speech-Dataset/ (visited Sep.
2017).

[34] S. Tokui et al.,

“Chainer: A next-generation open source
framework for deep learning,” in Proc. Workshop LearningSys,
NIPS, 2015.

[35] K. He et al., “Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation,” in Proc. ICCV,
2015, pp. 1026–1034, arXiv:1502.01852.

[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-
mization,” in Proc. ICLR 2015, 2014, arXiv:1412.6980.

[37] F. Ribeiro et al., “CrowdMOS: An approach for crowdsourcing
mean opinion score studies,” in Proc ICASSP, 2011, pp. 2416–
2419.

[1] I. Goodfellow et al., Deep Learning, MIT Press, 2016, http:

//www.deeplearningbook.org.

[2] Y. Wang et al., “Tacotron: Towards end-to-end speech synthe-

sis,” in Proc. Interspeech, 2017, arXiv:1703.10135.

[3] A. Barron,

“Implementation of Google’s Tacotron in Ten-
sorFlow,” 2017, Available at GitHub, https://github.
com/barronalex/Tacotron (visited Oct. 2017).

[4] K. Park,

“A TensorFlow implementation of Tacotron: A
fully end-to-end text-to-speech synthesis model,” 2017, Avail-
able at GitHub, https://github.com/Kyubyong/
tacotron (visited Oct. 2017).

[5] K. Ito, “Tacotron speech synthesis implemented in Tensor-
Flow, with samples and a pre-trained model,” 2017, Avail-
able at GitHub, https://github.com/keithito/
tacotron (visited Oct. 2017).

[6] R. Yamamoto, “PyTorch implementation of Tacotron speech
synthesis model,” 2017, Available at GitHub, https://
github.com/r9y9/tacotron_pytorch (visited Oct.
2017).

[7] J. Gehring et al., “Convolutional sequence to sequence learn-
ing,” in Proc. ICML, 2017, pp. 1243–1252, arXiv:1705.03122.

[8] H. Zen et al., “Statistical parametric speech synthesis using
deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.

[9] Y. Fan et al., “TTS synthesis with bidirectional LSTM based
recurrent neural networks,” in Proc. Interspeech, 2014, pp.
1964–1968.

[10] H. Zen and H. Sak, “Unidirectional long short-term memory
recurrent neural network with recurrent output layer for low-
latency speech synthesis,” in Proc. ICASSP, 2015, pp. 4470–
4474.

[11] S. Achanta et al., “An investigation of recurrent neural network
architectures for statistical parametric speech synthesis.,” in
Proc. Interspeech, 2015, pp. 859–863.

[12] Z. Wu and S. King, “Investigating gated recurrent networks for
speech synthesis,” in Proc. ICASSP, 2016, pp. 5140–5144.

[13] A. van den Oord et al., “WaveNet: A generative model for raw

audio,” arXiv:1609.03499, 2016.

[14] J. Sotelo et al., “Char2wav: End-to-end speech synthesis,” in

Proc. ICLR, 2017.

[15] S. Arik et al., “Deep voice: Real-time neural text-to-speech,”
in Proc. ICML, 2017, pp. 195–204, arXiv:1702.07825.

[16] S. Arik et al., “Deep voice 2: Multi-speaker neural text-to-

speech,” in Proc. NIPS, 2017, arXiv:1705.08947.

[17] K. Cho et al., “Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in Proc.
EMNLP, 2014, pp. 1724–1734.

[18] I. Sutskever et al., “Sequence to sequence learning with neural

networks,” in Proc. NIPS, 2014, pp. 3104–3112.

[19] O. Vinyals and Q. Le, “A neural conversational model,” in

Proc. Deep Learning Workshop, ICML, 2015.

[20] I. V. Serban et al., “Building end-to-end dialogue systems us-
ing generative hierarchical neural network models.,” in Proc.
AAAI, 2016, pp. 3776–3784.

