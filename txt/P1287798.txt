HPILN: A feature learning framework for cross-modality person
re-identiﬁcation

Jian-Wu Lin1, Hao Li2 ∗†

9
1
0
2
 
g
u
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
4
1
3
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

Most video surveillance systems use both RGB and
infrared cameras, making it a vital technique to re-
identify a person cross the RGB and infrared modal-
ities. This task can be challenging due to both the
cross-modality variations caused by heterogeneous im-
ages in RGB and infrared, and the intra-modality vari-
ations caused by the heterogeneous human poses, cam-
era views, light brightness, etc. To meet these chal-
lenges a novel feature learning framework, HPILN, is
proposed. In the framework existing single-modality re-
identiﬁcation models are modiﬁed to ﬁt for the cross-
modality scenario, following which speciﬁcally designed
hard pentaplet loss and identity loss are used to im-
prove the performance of the modiﬁed cross-modality
re-identiﬁcation models. Based on the benchmark of the
SYSU-MM01 dataset, extensive experiments have been
conducted, which show that the proposed method out-
performs all existing methods in terms of Cumulative
Match Characteristic curve (CMC) and Mean Average
Precision (MAP).

1. Introduction

Person re-identiﬁcation (Re-ID) is the technique
of identifying an individual from a surveillance cam-
era who has previously shown up from other non-
overlapping cameras [1], which has recently become a
research hotspot in the ﬁeld of computer vision due
to its practical importance. Typical Re-ID uses only
RGB cameras, i.e., identifying an individual from RGB
cameras based on previously recorded RGB camera
videos/images, and hence the name RGB-RGB Re-ID
[2–6]. However, in many cases both RGB and infrared
cameras are used, and consequently it is necessary to
develop Re-ID methods capable of cross RGB and in-
frared modalities, that is, either identifying an individ-
ual from RGB cameras based on previously recorded in-

∗∗Corresponding author. E-mail: jianwu.lin@foxmail.com
†The authors are with College of Information Engineering,

Zhejiang University of Technology, Hangzhou 310023, China.

Figure 1: RGB and infrared images in SYSU-MM01
dataset. The 1st, 2st, 4st, 5st and 3st, 6st rows are cap-
tured by RGB and infrared cameras, respectively

frared camera videos/images, or identifying an individ-
ual from infrared cameras based on previously recorded
RGB cameral videos/images, both being referred to as
RGB-IR Re-ID [7–11].

RGB-IR Re-ID has not been well studied to date,
with few literature being reported. To name just a few,
in [7], a deep zero-padding network is proposed to auto-
matically learn the common features of the two modali-
ties. In [8] a dual-path network with top-ranking loss is
proposed which considers both the cross-modality and
intra-modality variations.
In [9] a cmGAN approach
with cross-modality triplet loss is proposed to learn
the discriminative feature. In [10] a single image input
method is proposed to simplify the convolutional neu-
ral network structure. In [11] a dual-level discrepancy
reduction learning (D2RL) scheme is proposed to de-
compose the mixed modality and appearance discrep-
ancies. A dedicated dataset for RGB-IR Re-ID called
SYSU-MM01 has been collected [7], as shown in Fig.
1.

RGB-IR Re-ID is challenging mainly due to the
great cross-modality and intra-modality variations as
illustrated in Fig.2. By ”cross-modality variation” we
mean that RGB and infrared images are essentially het-
erogeneous as the former consists of three channels of

1

(a) cross-modality variations

(b) intra-modality variations

Figure 2: Cross-modality and intra-modality variations
in RGB-IR Re-ID. The solid and the dotted lines are
for the RGB and infrared domains, respectively.

color information while the latter only one. By ”intra-
modality variation” we mean that the image quality
including the camera view, resolution, light brightness,
human body pose, etc. can still be signiﬁcantly diﬀer-
ent even within the same RGB or infrared modality, as
long as multiple heterogeneous cameras and diﬀerent
monitoring scenarios are involved.

To meet the above challenges, a novel feature learn-
ing framework based on hard pentaplet and identity
loss network (HPILN) is proposed in this work. Specif-
ically, we select existing RGB-RGB Re-ID models as
the feature extraction module in our framework [2–6],
and then design the hard pentaplet loss to compen-
sate for the deﬁciencies of the RGB-RGB Re-ID model
in the cross-modality Re-ID task. The hard pentaplet
loss considers the following two aspects: 1) a pentaplet
loss, consisting of the global and cross-modality triplet
loss where the former can simultaneously handle cross-
modality and intra-modality variations, and the latter
can increase the ability to handle cross-modality vari-
ations. 2) an improved hard mining sampling method
by selecting the hardest global triplet and the hard-
est cross-modality triplet to form the hardest penta-
plet pair and to contribute to the convergence of the
convolutional neural networks.

The main contributions of this paper can be sum-

marized as follows.

• An end-to-end feature learning framework is pro-
posed yielding the state-of-the-art performance on
the RGB-IR Re-ID dataset SYSU-MM01.

• The proposed RGB-RGB Re-ID model migration
to RGB-IR Re-ID task provides a superior feature
extraction method for future improvements.

• A novel loss function called hard pentaplet loss is
proposed which is capable of simultaneously han-

dling the cross-modality and intra-modality varia-
tions in RGB-IR Re-ID.

The remainder of the paper is organized as follows.
Section 2 provides some preliminaries on Re-ID. The
proposed method is detailed in Section 3, which is then
veriﬁed experimentally in Section 4. Section 5 con-
cludes the paper.

2. Related Work

In this section we discuss related works on single-

modality and multi-modality Re-ID.

2.1. Single-modality person re-identiﬁcation

In the single-modality person re-identiﬁcation study,
most attentions have been paid to RGB-RGB Re-ID.
For RGB-RGB Re-ID, hand-designed descriptors
are often used to extract pedestrian features such as
color and texture information.
In [12], pedestrian
body is segmented from the background, and then the
weighted color histogram and the maximally stable
color regions are calculated for the pedestrian body
part. In recent years, the mainstream of Re-ID is to
design the loss function and convolutional neural net-
works based on deep learning methods. The design of
the loss function may depend on either metric learn-
ing or representation learning. Metric learning aims to
learn the similarity of two pedestrian images through a
deep CNN network, where the similarity is usually rep-
resented by the Euclidean distance. Frequently used
metric learning methods include contrastive loss [13],
triplet loss [14], hard triplet loss [15] and quadruplet
loss [16]. Representation learning uses identity tags
to automatically extract pedestrian representation fea-
tures, including identity loss [17] and veriﬁcation loss
[18]. In addition, three types of special networks have
been designed for Re-ID, i.e., either global-based, or
part-based, or attention-based. Global-based networks
aggregate global-level features into a global vector [2,4].
Part-based networks divide the pedestrian image into
diﬀerent parts, and the local feature vectors of diﬀerent
parts is merged into a vector [3, 5, 6]. Attention-based
networks focus on automatically ﬁnding local salient
regions for computing deep features [19, 20]. These
existing single-modality re-identiﬁcation models have
rarely been applied to RGB-IR Re-ID to date and ef-
forts need to be taken for such a migration.

2.2. Multi-modality person re-identiﬁcation

Existing multi-modal fusion person re-identiﬁcation
focus on RGB-D modules [21,22], visible-thermal mod-
ules [8, 23] and RGB-IR modules [7]. RGB-D Re-ID
combines human RGB image and depth information,

and depth information is used to provide more invari-
ant body shape and skeleton information to reduce the
impact of changed clothes or extreme illumination on
re-identiﬁcation. RGB-IR and visible-thermal (VT)
Re-ID is based on the principle of infrared imaging,
enabling re-identiﬁcation to take place at night. The
diﬀerence is that the RGB-IR Re-ID transmits and col-
lects infrared light through the infrared camera to ob-
tain infrared images, while the VT Re-ID capturing
the heat emitted by the human body to obtain infrared
images. However, depth cameras and thermal cameras
are rare in surveillance systems. In contrast, infrared
cameras have been widely deployed. Most surveillance
cameras in the real world are visible light cameras dur-
ing the day and become infrared cameras at night.
Therefore, from the perspective of practical applica-
tions, RGB-IR Re-ID can be of more value.

3. The Proposed Method

This paper addresses RGB-IR Re-ID by a feature
learning framework based on hard pentaplet loss and
identity loss as shown in Fig.4. The framework con-
sists of three parts: 1) Re-ID neural network for fea-
ture extraction; 2) the hard mining sampling method
to ﬁnd hardest pentaplet pair sets after getting feature
embedding; 3) HPI loss for feature learning. Speciﬁ-
cally, the Re-ID neural network is taken from existing
RGB-RGB Re-ID convolutional neural network which
can also extract the representation feature of infrared
person images. By calculating the Euclidean distance
of the feature embedding, the hard mining sampling
method maximizes training and ensures model conver-
gence. The hard pentaplet loss enables the network
to handle cross-modality and intra-modality variations
simultaneously, and the hard pentaplet loss and the
identity loss are integrated into multiple losses to facil-
itate the process of feature learning.

Figure 3: A typical RGB-RGB Re-ID CNN model. The
green and the red represent the unchanged and changed
parts, respectively

3.1. Re-ID neural network

We use RGB-RGB Re-ID neural network as fea-
ture extraction modules to extract common features of
two heterogeneous modalities, since these RGB-RGB
Re-ID-speciﬁc models can outperform image classiﬁ-
cation models, despite the heterogeneous images from
two modalities.

In our framework, we slightly adjust the structure of
the RGB-RGB Re-ID model. A typical RGB-RGB Re-
ID model based convolutional neural network is shown
in Fig.3. Most Re-ID models have at least two fully
connected layers, where the last layer (FC-2) is for iden-
tity loss, and the output of the penultimate layer (FC-
1) is used as feature embedding supervised by ranking
loss based on metric learning. In our method we change
the dimension of the last fully connected layer (FC-
2) to the number of person class in the SYSU-MM01
training set. The front convolutional neural network
(CNN) part is used as feature extractor to obtain fea-
ture embedding.

3.2. Hard pentaplet loss

Our approach is inspired by the combination of hard
triplet loss and triplet loss. We discuss in turn the
triplet loss, the hard triplet loss, and ﬁnally the pro-
posed hard pentaplet loss.

3.2.1 The triplet loss

i , xn

i , xp

The triplet loss is widely used in image retrieval tasks
such as face recognition, person re-identiﬁcation, and
vehicle retrieval. In the person re-identiﬁcation task,
for the anchor image xa in the candidate triplet set
i }, i ∈ [1, N ], xp is a positive sample image of
{xa
the same identity, and xn is a negative sample image of
a diﬀerent identity. Using the convolutional neural net-
work as the feature extractor, the image x is mapped
into the d-dimensional Euclidean space. The feature
embedding vector can be expressed as f (x) ∈ Rd. The
Euclidean distance between feature embedding mea-
sures the similarity of two images, which can be ex-
pressed as follows,

d(xi, xj) = (cid:107)f (xi) − f (xj)(cid:107)2

(1)

The triplet loss is obtained as follows,

Ltrp =

[d(xa

i , xp

i )2 − d(xa

i , xn

i )2 + α]+

(2)

N
(cid:88)

i

i , xn
where [z]+ = max(z, 0). For {xa
i }, the i-th pair
i , xp
of triplets, d(xa
i ) represents the Euclidean distance
i ), and d(xa
between positive samples (xa
i ) repre-
sents the Euclidean distance between negative samples

i , xp

i , xn

i , xp

Figure 4: The proposed feature learning framework based hard pentaplet and identity (HPI) loss for RGB-IR Re-
ID. The framework consists of three main components: 1) The Re-ID neural network which extracts the common
features of RGB and infrared images; 2) The hard mining sampling method which obtains the hardest pentaplet
pair sets; 3) The HPI loss for feature learning which consists of pentaplet loss and identity loss. 2P K is the training
batch size. In each training batch, P individuals are randomly selected, and each person randomly selects K RGB
images and K infrared images. The rectangles of diﬀerent colors below the image represent the diﬀerent elements
in the pentaplet pair.

i , xn

(xa
i ). α is a hyperparameter that forces the positive
and negative sample pairs to separate in the Euclidean
space.

Under the supervision of triple loss, the CNN can
learn discriminative feature embedding in Euclidean
space.
It can be seen from Equation (2) that if the
positive sample becomes larger or the negative sam-
ple becomes smaller, the loss value will increase, and
the adjustment of the weight and bias of the CNN will
be larger during the back propagation. Intuitively, the
triplet loss reduces the distance between positive sam-
ples, i.e., the intra-class distance, increases the distance
between negative samples, i.e., the inter-class distance,
and ﬁnally distinguishes diﬀerent person in the Eu-
clidean space.

The training goal is that for any triplet {xa

i },
the positive and negative sample pairs in the Euclidean
space meet the following inequality,

i , xn

i , xp

d(xa

i , xp

i )2 + α < d(xa

i , xn

i )2

(3)

3.2.2 The hard triplet loss

In order to ensure the network convergence, it is neces-
sary to choose triplets that violate (3). Let the triplet
that already satisﬁes (3) be named by “easy triplet”. It
is then not wise to randomly choose a triplet set since it

would contain many such easy triplets and hence harm
the convergence of the model.

Alexander Hermans et al. designed the hard triplet
loss [15], which improves the training speed and accu-
racy in many retrieval tasks by improving the triplet
sampling method. Using this loss, each batch randomly
samples P -identity person, and each person randomly
samples K images, thus P K images for each batch. For
each sample in the batch, select the hardest positive
and negative samples to form the hardest triplet. The
hardest positive sample represents the positive sample
with the largest Euclidean distance from the anchor,
and the hardest negative sample represents the nega-
tive sample with the smallest Euclidean distance from
the anchor. The hard triplet loss can be expressed as
follows,

all anchor
(cid:122) (cid:125)(cid:124) (cid:123)
K
P
(cid:88)
(cid:88)

i=1

a=1

Lhtrp =

hardest positive
(cid:125)(cid:124)
(cid:123)
(cid:122)
i , xp
d(xa
i )

max
p=1...K

[α +

d(xa

i , xn
j )

]+

(4)

− min
j=1...P
n=1...K
j(cid:54)=i

(cid:125)
(cid:123)(cid:122)
(cid:124)
hardest negative

(a) Hard triplet loss

(b) Hard pentaplet loss

Figure 5: Geometry interpretation of triplet loss and
pentaplet loss in Euclidean space. (a) The triplet loss
minimizes the distance between the anchor xa
i and a
positive xp
i , and maximizes the distance between the
anchor xa
i and a negative xn
i . (b) In addition to the
function of triplet loss, pentaplet loss can minimize the
distance between an anchor xa
i and a cross-modality
positive xcp
i , and maximizes the distance between the
i and a cross-modality negative xcn
anchor xa
i

.

3.2.3 The hard pentaplet loss

As shown in Fig.5(a), the hard triplet loss focuse on re-
ducing the intra-class distance and increasing the inter-
class distance, which is eﬀective in the conventional re-
trieval task. However, the hard triplet loss does not
perform very well in RGB-IR person re-identiﬁcation
task. As shown in Fig.2(a), the same person in diﬀer-
ent modalities can be dissimilar. The hard triplet loss
does not consider cross-modality factors, and hence the
training model does not deal well with cross-modality
and intra-modality variations at the same time.

To address the huge cross-modality and intra-
modality variations in cross-class or intra-class, we pro-
pose a hard global triplet loss based on a cross-modality
batch (cm-batch) structure. Speciﬁcally, in each cm-
batch, P individuals are randomly selected, each per-
son randomly selects K RGB images and K infrared
images. For an anchor image xa
i , the sum of cross-
modality negative set xcn and intra-modality negative
set xin constitutes the global negative set xn, and
the sum of cross-modality positive set xcp and intra-
modality positive set xip constitutes the global posi-
tive set xp. The hard global triplet loss is computed as

follows,

all anchor
(cid:122) (cid:125)(cid:124) (cid:123)
P
2K
(cid:88)
(cid:88)

i=1

a=1

Lhgt =

[α +

hardest global positive
(cid:125)(cid:124)
d(xa

(cid:123)
i , xp
i )

(cid:122)

max
p=1...2K
p(cid:54)=a

d(xa

i , xn
j )

]+

(5)

− min
n=1...2K
j=1...P
j(cid:54)=i

(cid:124)

(cid:123)(cid:122)
hardest global negative

(cid:125)

i ∈ xn, xj

where α is a hyperparameter, and xa
i ∈
xp, xn
i represents the i-th image of the j-th
person in the corresponding set of anchor. For any xa
i
in the cm-batch, the hardest global positive or negative
may be the same or diﬀerent modality.

i ∈ xa, xp

Although hard global triplet loss can handle cross-
modality and intra-modality variations at the same
time, usually cross-modality variations are much larger
than intra-modality variations. We thus design a hard
cross-modality loss to handle cross-modality variations.
The hard cross-modality triplet loss is computed as fol-
lows,

Lhct =

all anchor
(cid:122) (cid:125)(cid:124) (cid:123)
2K
P
(cid:88)
(cid:88)

i=1

a=1

[α +

−

hardest cross−modality positive
(cid:125)(cid:124)
d(xa

(cid:123)
i , xcp
i )

(cid:122)
max
cp∈A

d(xa

i , xcn
k )

]+

min
cn∈A
k=1...K
k(cid:54)=i

(cid:123)(cid:122)
hardest cross−modality negative

(cid:125)

(cid:124)

(6)

where A = {1, 2, . . . , K} when a ≥ K, and otherwise
A = {K + 1, K + 2, . . . , 2K}. Consistent with Equa-
tion (5), xj
i represents the i-th image of the j-th person
in the corresponding set of anchor.

Our proposed hard pentaplet loss consists of hard
global and cross-modality loss. For an anchor im-
age xa
in cm-batch, the hardest global triplet pair
i
i , xp
j , xn
{xa
k } and the hardest cross-modality triplet pair
i , xcp
{xa
h , xcn
t } can be obtained by hard sampling meth-
ods, i.e., combining the hardest triplet pairs above to
k , xcp
obtain a hardest pentaplet pair {xa
t }.
Note that xp
j and xcp
k and xcn
t may be the same
image. The hard pentaplet loss can be expressed as
follows,

h , xn

h , xcn

i , xp

j , xn

LHP =

1
2 × P × K

(Lhgt + Lhct)

(7)

As shown in Fig.5(b), after the training of hard pen-
taplet loss, the distribution of human images in Eu-
clidean space is more discriminative. The hard penta-

plet loss has two main advantages: 1) The hard penta-
plet loss can handle intra-modality and deeper cross-
modality variations simultaneously. 2) The hard penta-
plet sampling method uses a limited number of images
to generate suﬃcient hardest pentaplet pairs, which en-
riches the training samples and speeds up model con-
vergence.

3.3. Hard pentaplet with identity loss

We use the identity loss to handle intra-class varia-
tions. As shown in Fig.2(a), 2(b), there may be large
variations in person images of the same identity. Given
the success of identity loss in cross-modality Re-ID
task, identity loss enables the CNN framework to ex-
tract the identity-speciﬁc information to reduce intra-
class variations. We regard the same person in the het-
erogeneous modality as the same class, and the identity
loss is then expressed by softmax loss, as follows,

Lid =

1
2 × P × K

2P K
(cid:88)

i=1

− log(

efyi
j efj

(cid:80)

)

(8)

where 2P K is the number of training samples in cm-
batch, f is designed as the output vector of the last
fully connected layer in CNN, fj denotes the j-th ele-
ment of class score vector f , j ∈ [1, T ], T is the number
of class, yi is the class label of the input image xi, and
fyi is the class score of xi.

We add identity loss to our framework to learn a
more robust feature representation. HPI loss are com-
bined by hard pentaplet loss and identity loss, which
can be expressed as follows,

LHP I = Lhp + Lid

(9)

4. Experimental Results

In this section, we conduct a series of experiments
to evaluate the eﬀectiveness of the proposed method.

4.1. Datasets and settings

The publicly available SYSU-MM01 dataset are
adopted for evaluation, which is the ﬁrst benchmark for
RGB-IR Re-ID. As shown in Fig.1, the SYSU-MM01
dataset contains 491 identities with 287628 RGB im-
ages and 15792 infrared images in total, captured by
four RGB cameras and two IR cameras. RGB cameras
work in bright environments while IR cameras work
in dark. Camera 1, 2, 3 capture indoor images, and
camera 4, 5, 6 capture outdoor images.

4.2. Evaluation protocol

The SYSU-MM01 dataset is divided into training
set and test set, where the former contains 395 persons

with 22258 RGB images and 11909 infrared images,
and the latter contains 96 persons. Note that a person
does not appear in the two sets simultaneously.

In the training stage, all images in the training set
can be used for training. In the test stage, the RGB
images are for the gallery set and the infrared images
are for the probe set. There are two veriﬁcation modes:
all-search mode and indoor-search mode. For the all-
search mode, the RGB images from RGB cameras 1, 2,
4 and 5 are for the gallery set and the infrared images
from IR cameras 3 and 6 are for the probe set. For
the indoor-search mode, the RGB images from RGB
cameras 1 and 2 are for the gallery set and the infrared
images from IR cameras 3 are for the probe set. For
each mode, there are multi-shot and single-shot set-
tings. For every identity in gallery set, we randomly
select 1/10 images from the RGB camera as single-
shot/multi-shot setting respectively. For the probe set,
all infrared images are used.

For a given probe image, we match it by calculating
the similarity between the probe image and gallery im-
ages. The matching of the Re-ID is performed between
cameras at diﬀerent positions, so the probe images of
camera 3 skips the gallery images of camera 2 because
camera 2 and camera 3 are located at the same po-
sition. After calculating the similarity, we can obtain
the ranking list according to the descending order of
similarity. To indicate the performance, we use Cu-
mulative Match Characteristic curve (CMC) [24] and
average accuracy (mAP).

4.3. Implementation details

We use NVIDIA GeForce 1080Ti graphics cards with
Pytorch computing framework to implement our algo-
rithm. Five RGB-RGB Re-ID neural networks were
used to verify the superiority of our algorithms: Res-
Mid, MGN, PCB, BFE, MLFN, which are described in
Section 3.1. As shown in Table 2, the input image size
and the output embedding feature dimension are dif-
ferent due to the diﬀerence of the model. The infrared
image is padding to three channels, which copies the
information of one channel. We use the Adam [25] op-
timizer to train 10k iterations, and the initial learning
rate is set to 3 ∗ 10−4.

Since our hard pentaplet loss requires slightly diﬀer-
ent cm-batches, we sample a 2P K batch by randomly
sampling P identities, and each person randomly sam-
ples K RGB images and K infrared images.
In our
experiment, P is set to 8, K is set to 4, and the batch
size is calculated to be 64. For input images, the meth-
ods of random horizontal ﬂip and random cropping is
used to expand the amount of data. We set margin α in
hard pentaplet loss in the range [0.3, 0.6, 0.9, 1.2, 1.5,

All-search

Indoor-search

Method

HOG+Euclidean
HOG+CRAFT
HOG+CCA
HOG+LFDA
LOMO+CCA
LOMO+CRAFT
LOMO+CDFE
LOMO+LFDA
One-stream
Two-stream
zero-padding
cmGAN
BDTR
IPVT-1+MSR
D2RL
Res-Mid+HPI
MGN+HPI
PCB+HPI
MLFN+HPI
BFE+HPI

r1
2.76
2.59
2.74
2.33
2.42
2.34
3.64
2.98
12.04
11.65
14.80
26.97
17.01
23.18
28.9
40.49
39.77
33.29
33.34
41.36

Single-shot
r20
r10
31.91
18.25
31.50
17.93
32.51
18.91
33.38
18.58
32.45
18.22
32.93
18.70
37.28
23.18
35.36
21.11
66.74
49.68
65.50
47.99
71.33
54.12
80.56
67.51
71.96
55.43
61.73
51.21
82.4
70.6
93.13
83.61
90.14
79.78
91.42
80.66
89.66
78.54
94.51
84.78

mAP
4.24
4.24
4.28
4.35
4.19
4.22
4.53
4.81
13.67
12.85
15.95
27.80
19.66
22.49
29.2
41.64
41.12
35.15
36.13
42.95

r1
3.82
3.58
3.25
3.82
2.63
3.03
4.70
3.86
16.26
16.33
19.13
31.49
/
/
/
47.70
44.86
38.55
39.45
47.56

Multi-shot
r20
r10
37.63
22.77
38.59
22.90
36.51
21.82
35.84
20.48
34.82
19.68
37.05
21.70
43.05
28.23
40.54
24.01
75.05
58.14
74.46
58.35
78.41
61.40
85.01
72.74
/
/
/
/
/
/
95.34
87.99
91.61
82.54
92.82
82.86
92.45
83.21
95.98
88.13

mAP
2.16
2.06
2.04
2.20
2.15
2.13
2.28
2.61
8.59
8.03
10.89
22.27
/
/
/
35.15
34.88
28.16
29.52
36.08

r1
3.22
3.03
4.38
2.44
4.11
3.89
5.75
4.81
16.94
15.60
20.58
31.63
/
/
/
45.65
44.06
39.70
36.25
45.77

Single-shot
r20
r10
44.52
24.68
42.89
24.07
50.43
29.96
45.50
24.13
52.54
30.60
48.16
27.55
54.90
34.35
52.50
32.16
82.10
63.55
81.02
61.18
85.79
68.38
89.18
77.23
/
/
/
/
/
/
97.77
90.76
95.59
87.77
96.68
88.26
94.51
85.07
98.46
91.82

mAP
7.25
7.07
8.70
6.87
8.83
8.37
10.19
9.56
22.95
21.49
26.92
42.19
/
/
/
56.19
54.52
50.49
47.99
56.52

r1
4.75
4.16
4.62
3.42
4.86
2.45
7.36
6.27
22.62
22.49
24.43
37.00
/
/
/
50.79
50.55
46.86
41.99
53.05

Multi-shot
r20
r10
49.38
29.06
47.16
27.75
56.28
34.22
45.11
25.27
57.30
34.40
38.15
20.20
60.33
40.38
58.11
36.29
87.82
71.74
88.61
72.22
91.32
75.86
92.11
80.94
/
/
/
/
/
/
97.86
93.03
96.06
89.99
96.85
90.31
95.20
86.34
98.93
93.71

mAP
3.51
3.17
3.87
3.19
4.47
2.69
5.64
5.15
15.04
13.92
18.64
32.76
/
/
/
46.21
44.90
40.93
38.43
47.48

Table 1: Comparison with the state-of-the-arts on the SYSU-MM01 dataset. r1, r10, r20 denote rank-1, 10, 20
accuracies(%).

Model
Res-Mid
MGN
PCB
BFE
MLFN

W*H
224*224
128*384
224*224
128*256
224*224

Dim Batch Size
3072
2048
12288
1024
1024

64
64
64
64
64

Lr
3 ∗ 10−4
3 ∗ 10−4
3 ∗ 10−4
3 ∗ 10−4
3 ∗ 10−4

comparison contains four state-of-the-art methods:

• Zero-padding [7]. A deep zero-padding method
for training one-stream network towards automat-
ically capturing domain-speciﬁc information for
cross-modality matching.

Table 2: Settings for diﬀerent model training:
in-
put image width and height (W*H), feature dimension
(Dim), training batch size (Batch Size) and learning
rate (Lr).

• BDTR [8].

A dual-path network with bi-
directional dual-constrained top-ranking loss to
learn discriminative feature representations from
two modalities.

DukeMTMC-reID

Method
Res-Mid
MGN
PCB
BFE
MLFN

Market1501
mAP
r1
75.55
89.87
86.9
95.7
77.3
92.4
85
94.4
74.3
90

CUHK03
r1
43.51
66.8
61.3
72.1
52.8

mAP
47.14
66
54.2
67.9
47.8

r1
63.88
88.7
81.9
88.7
81.0

mAP
80.43
78.4
65.3
75.8
62.8

Table 3: Performance of RGB-RGB Re-ID models on
Market1501, CUHK03, DukeMTMC-reID datasets.

1.8] and evaluate our method by experimenting with
other hyper-parameters.

4.4. Comparison with the state-of-the-arts

We evaluated our HPILN method against 15 exist-
ing methods on the SYSU-MM01 dataset in Table 1.
For performance measure, the rank-1, 10, 20 accura-
cies of Cumulative Match Characteristic curve (CMC)
and mean average precision (mAP) are used to show
the clear performance superiority of our method. The

• CmGAN [9]. A cross-modality generative adver-
sarial network using a cutting-edge generative ad-
versarial training based discriminator and cross-
modality triplet loss to learn discriminative feature
representation from two modalities.

• IPVT-1+MSR [10].

IPVT-1 combining RGB
image and infrared as a single input to reduce com-
putational complexity. Moreover, the accuracy of
Re-ID is improved by multi-scale Retinex (MSR)-
ﬁltered input images.

• D2RL [11]. Dual-level discrepancy reduction
learning (D2RL) to decompose and handle the
mixed modality and appearance discrepancies.
Images from diﬀerent modalities are mapped to
a uniﬁed space, and then a cascaded sub-network
is used to obtain discriminative features.

In addition, other existing methods are used for
comparison,
including handcrafted features such as
HOG [26] and LOMO [27], cross-domain models such

as CDFE [28] and CRAFT [29], CCA [30], one-
stream and two-stream networks [7], and metric learn-
ing method LFDA [31]. Most of the results were ob-
tained from the references [7–11].

We use Res-Mid [2], MGN [6], PCB [3], BFE [5],
MLFN [4] as feature extractors in our HPILN method.
To our best knowledge, these models are the state-of-
the-art methods in RGB-RGB Re-ID in the past two
years, and Table 3 shows their performance on the Mar-
ket1501 [32], CUHK03 [33] and DukeMTMC-reID [34]
datasets.

In Table 1, the results of ﬁve rows on the bottom
show the performance of HPILN method which applies
It is clear that our HPILN
HPI loss to ﬁve models.
method is signiﬁcantly better than all existing methods
in the SYSU-MM01 benchmark, where the ﬁve mod-
els based on HPI loss have higher rank-1, 10, 20 and
mAP in all veriﬁcation modes and setting than exist-
ing methods. Speciﬁcally, the BFE model based HPI
loss performs the best in most of the indicators, which
outperforms the 2nd best method (D2RL) on all-search
single-shot setting in terms of the rank1 and mAP met-
ric 12.46% (41.36-28.9) and 13.75% (42.95-29.2), re-
spectively.

4.5. Effectiveness of fusion loss

To verify the eﬀectiveness of fusion identity loss and
hard pentaplet loss, we compared the rank-1 precision
of identity loss, hard pentaplet (HP) loss and hard pen-
taplet with identity (HPI) loss on the SYSU-MM01
dataset. We report the results with ﬁve models in Ta-
ble 4. As can be seen from Table 4, the combination of
identity loss is eﬀective. It is clear that the RGB-RGB
Re-ID models based on identity loss can also achieve
excellent precision, even the Res-Mid based identity
loss performance is better than the 2nd best method

(a) Identity Loss

(b) HP Loss

(c) HPI Loss

Figure 6: Comparison among identity loss, HP loss and
HPI loss. In this toy experiment, we modiﬁed Res-Mid
to learn a 2-D feature on a subset of the SYSU-MM01
dataset. Speciﬁcally, we set the output of dimension
of the last fully connected layer as 2 and visualize the
learned features. The ﬁve color points represent ﬁve
identity classes, the circular and star shapes represent
RGB modality and IR modality respectively.

All-search
Single-shot Multi-shot

Indoor-search
Single-shot Multi-shot

Mode
Setting
Res-Mid
Identity Loss
HP Loss
HPI Loss
MGN
Identity Loss
HP Loss
HPI Loss
PCB
Identity Loss
HP Loss
HPI Loss
MLFN
Identity Loss
HP Loss
HPI Loss
BFE
Identity Loss
HP Loss
HPI Loss

32.68
36.06
40.49

27.29
36.68
39.77

11.22
26.51
33.29

28.44
30.62
33.34

25.69
38.89
41.36

38.58
42.32
47.70

31.05
41.62
44.86

15.67
32.40
38.55

33.23
35.43
39.45

32.02
45.69
47.56

37.41
40.46
45.65

33.47
41.95
44.06

8.7
33.61
39.70

30.19
31.28
36.25

29.65
44.51
45.77

45.23
44.08
50.79

38.19
48.37
50.55

12.76
40.42
46.86

34.82
36.69
41.99

36.68
52.51
53.05

Table 4: Eﬀectiveness of fusion loss on the SYSU-
MM01 dataset. Rank-1 accuracies (%) in all/indoor-
search mode and single/multi-shot setting.

(cmGAN) in rank-1 accuracies. In addition, although
HP loss has shown excellent performance, HPI loss
which integrates identity loss and HP loss further im-
proves the accuracy. We speculate that the fusion of
identity loss further enhances the feature discrimina-
tion of HP loss.

In order to verify the above speculation, we con-
ducted a toy experiment to illustrate the diﬀerences
of features in 2-D Euclidean space learned by iden-
tity loss, HP loss, and HPI loss respectively, shown
in Fig.6. Under the supervision of identity loss, the
learned features are slightly separable which are not
discriminative enough, since Fig.6(a) still shows large
cross-modality variations and small inter-class discrim-
ination. Fig.6(b) shows that there is a large mar-
gin between the dot clusters, which means HP loss
learned discriminative large-margin features. For HPI
loss combined with HP loss and identity loss, Fig.6(c)
shows that the same classes are clustered together and
there is signiﬁcant separation between the diﬀerent
classes. The reason why the HPI loss performance su-
perior is that HP loss handle the cross-modality and
intra-modality variations to learn the distinguishing
large-margin features, and identity loss assists HP loss
to further reduce intra-class distance.

4.6. Comparison with other advanced loss

To demonstrate the superiority of our methods, we
compare our HP loss and HPI loss with other advanced
loss functions in Re-ID, including HT (hard triplet)

(a) Rank-1

Table 5: The rank-1 of classiﬁcation models and RGB-
RGB Re-ID models in HPILN framework.

All-search
Single-shot Multi-shot

Type

Global-based

Part-based

Attention-based

Classiﬁcation-based

Method
Res-Mid
MLFN
PCB
MGN
BFE
Hacnn
Mudeep
Resnet50
Densenet121

40.49
33.34
33.29
39.77
41.36
1.07
9.35
5.36
13.59

47.70
39.45
38.55
44.86
47.56
1.53
11.78
5.69
16.10

to balance the number of input images so that the
model does not focus on a certain modality images.

4.7. Analysis of model selection

In HPILN framework, the RGB-RGB Re-ID models
was ﬁrst adopted as the feature extractor for RGB-IR
Re-ID task. We tested the performance of the clas-
siﬁcation models and some RGB-RGB Re-ID models
in HPILN framework. The RGB-RGB Re-ID models
include diﬀerent types: global-based networks, part-
based networks and attention-based networks. The re-
sults are shown in Table.5.

In the HPILN framework, the RGB-RGB Re-ID
model is more suitable for RGB-IR Re-ID tasks than
classiﬁcation models. We chose Resnet50 [36] and
Densenet121 [37] as classiﬁcation models, which per-
form well on ImageNet dataset. From Table 5, we ob-
served that classiﬁcation models do not achieve good
accuracy in the HPILN framework compared to the
RGB-RGB Re-ID model. The reason is that the RGB-
RGB Re-ID model is designed for person images. Al-
though infrared images and RGB images are very dif-
ferent, heterogeneous images also have certain com-
mon features, such as body shape and clothing shape.
Therefore, the RGB-RGB Re-ID model performs well
in RGB-IR Re-ID tasks.

However, not all RGB-RGB Re-ID models perform
well in RGB-IR Re-ID tasks. We tested two attention-
based RGB-RGB Re-ID models: mudeep [19] and
hacnn [20]. From Table 5, mudeep and hacnn have
lower precision on SYSU-MM01. Both mudeep and
hacnn use the attention mechanism which automati-
cally focus on local salient areas for computing deep
features. Attention mechanism is not robust in cross-
modality training because there are few similar local
regions of heterogeneous images.

(b) mAP

Figure 7: Performance of diﬀerent loss functions. We
tested rank-1 and mAP with all-search multi-shot set-
ting on ﬁve models.

loss [15], HTI (hard triplet with identity) loss, center
loss [35], and identity loss [17]. The performance of the
contrast methods was reported in Fig.7 and Fig.8.

Results shown in Fig.7 illustrate that HP loss and
HPI loss have better performance on rank-1 and mAP
than other loss functions. We tested ﬁve models, and
the rank-1 and mAP of the PCB based HPI loss were
22.88% and 16.69% higher than the second loss (except
HP loss), respectively.

Fig.8 shows the Cumulative Match Characteristic
(CMC) curves of diﬀerent models under diﬀerent loss
in the SYSU-MM01 dataset. The CMC curve can more
fully reﬂect the performance of the model. We tested
ﬁve models under the all-search single-shot setting. In
all tested models, HP loss and HPI loss performed bet-
ter than other losses, and our method is not only higher
than the existing method in rank-1, but also maintains
a lead in rank 1-50.

The reason we are better than other methods is that
we consider cross-modality variations to better extract
common features in heterogeneous modality. In addi-
tion, we use a more reasonable image sampling method

(a) Res-Mid

(b) MGN

(c) BFE

(d) MLFN

(e) PCB

Figure 8: Cumulative Match Characteristic curve of diﬀerent models under diﬀerent loss functions

5. Conclusion

A novel feature learning framework based on hard
pentaplet and identity loss network (HPILN) is pro-
posed for RGB-IR person re-identiﬁcation.
In the
framework, existing RGB-RGB Re-ID model is used
as the feature extractor, hard pentaplet (HP) loss is
used to learn the discriminative large-margin features
in order to handle cross-modality and intra-modality
variations, and the identity loss is combined to ex-
tract identity-speciﬁc information to learn the sepa-
ration features. The experimental results show that
our method achieves state-of-the-art performance on
SYSU-MM01 dataset.

References

[1] L. Zheng, Y. Yang, and A. G. Hauptmann, “Per-
son re-identiﬁcation: Past, present and future,”
arXiv preprint arXiv:1610.02984, 2016. 1

[2] Q. Yu, X. Chang, Y.-Z. Song, T. Xiang, and
T. M. Hospedales, “The devil
is in the mid-
for
Exploiting mid-level
dle:
cross-domain instance matching,” arXiv preprint
arXiv:1711.08106, 2017. 1, 2, 8

representations

[3] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang,
“Beyond part models: Person retrieval with re-
ﬁned part pooling (and a strong convolutional
baseline),” in Proceedings of the European Con-
ference on Computer Vision (ECCV), 2018, pp.
480–496. 1, 2, 8

[4] X. Chang, T. M. Hospedales, and T. Xi-
ang, “Multi-level factorisation net for person re-
identiﬁcation,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, 2018, pp. 2109–2118. 1, 2, 8

[5] Z. Dai, M. Chen, S. Zhu, and P. Tan, “Batch fea-
ture erasing for person re-identiﬁcation and be-
yond,” arXiv preprint arXiv:1811.07130, 2018. 1,
2, 8

[6] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou,
“Learning discriminative features with multiple
granularities for person re-identiﬁcation,” in 2018
ACM Multimedia Conference on Multimedia Con-
ference. ACM, 2018, pp. 274–282. 1, 2, 8

[7] A. Wu, W.-S. Zheng, H.-X. Yu, S. Gong, and
J. Lai, “Rgb-infrared cross-modality person re-
identiﬁcation,” in Proceedings of the IEEE Inter-

national Conference on Computer Vision, 2017,
pp. 5380–5389. 1, 2, 7, 8

[8] M. Ye, Z. Wang, X. Lan, and P. C. Yuen,
“Visible thermal person re-identiﬁcation via dual-
constrained top-ranking.” in IJCAI, 2018, pp.
1092–1099. 1, 2, 7, 8

[9] P. Dai, R. Ji, H. Wang, Q. Wu, and Y. Huang,
“Cross-modality person re-identiﬁcation with gen-
erative adversarial training.” in IJCAI, 2018, pp.
677–683. 1, 7, 8

[10] J. K. Kang, T. M. Hoang, and K. R. Park, “Person
re-identiﬁcation between visible and thermal cam-
era images based on deep residual cnn using sin-
gle input,” IEEE Access, vol. 7, pp. 57 972–57 984,
2019. 1, 7, 8

[11] Z. Wang, Z. Wang, Y. Zheng, Y.-Y. Chuang, and
S. Satoh, “Learning to reduce dual-level discrep-
ancy for infrared-visible person re-identiﬁcation,”
in The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019. 1, 7, 8

[12] M. Farenzena, L. Bazzani, A. Perina, V. Murino,
and M. Cristani, “Person re-identiﬁcation by
symmetry-driven accumulation of local features,”
in 2010 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition. IEEE,
2010, pp. 2360–2367. 2

[13] R. R. Varior, M. Haloi, and G. Wang, “Gated
siamese convolutional neural network architecture
for human re-identiﬁcation,” in European Confer-
ence on Computer Vision.
Springer, 2016, pp.
791–808. 2

[14] F. Schroﬀ, D. Kalenichenko, and J. Philbin,
“Facenet: A uniﬁed embedding for face recogni-
tion and clustering,” in Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, 2015, pp. 815–823. 2

in Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2016, pp.
1249–1258. 2, 8

[18] H. Chen, Y. Wang, Y. Shi, K. Yan, M. Geng,
Y. Tian, and T. Xiang, “Deep transfer learning
for person re-identiﬁcation,” in 2018 IEEE Fourth
International Conference on Multimedia Big Data
IEEE, 2018, pp. 1–5. 2
(BigMM).

[19] X. Qian, Y. Fu, Y.-G. Jiang, T. Xiang, and
X. Xue, “Multi-scale deep learning architectures
for person re-identiﬁcation,” in 2017 IEEE Inter-
national Conference on Computer Vision (ICCV).
IEEE, 2017, pp. 5409–5418. 2, 9

[20] W. Li, X. Zhu, and S. Gong, “Harmonious at-
tention network for person re-identiﬁcation,” in
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018, pp. 2285–
2294. 2, 9

[21] A. Wu, W.-S. Zheng, and J.-H. Lai, “Ro-
bust depth-based person re-identiﬁcation,” IEEE
Transactions on Image Processing, vol. 26, no. 6,
pp. 2588–2603, 2017. 2

[22] I. B. Barbosa, M. Cristani, A. Del Bue, L. Baz-
zani, and V. Murino, “Re-identiﬁcation with rgb-
d sensors,” in European Conference on Computer
Vision. Springer, 2012, pp. 433–442. 2

[23] V. V. Kniaz, V. A. Knyaz, J. Hladuvka, W. G.
Kropatsch, and V. Mizginov, “Thermalgan: Mul-
timodal color-to-thermal
image translation for
person re-identiﬁcation in multispectral dataset,”
in Proceedings of the European Conference on
Computer Vision (ECCV), 2018, pp. 0–0. 2

[24] H. Moon and P. J. Phillips, “Computational and
performance aspects of pca-based face-recognition
algorithms,” Perception, vol. 30, no. 3, pp. 303–
321, 2001. 6

[15] A. Hermans, L. Beyer, and B. Leibe, “In defense of
the triplet loss for person re-identiﬁcation,” arXiv
preprint arXiv:1703.07737, 2017. 2, 4, 8

[25] D. P. Kingma and J. Ba, “Adam: A method
arXiv preprint

stochastic optimization,”

for
arXiv:1412.6980, 2014. 6

[16] W. Chen, X. Chen, J. Zhang, and K. Huang,
“Beyond triplet loss: a deep quadruplet network
for person re-identiﬁcation,” in Proceedings of the
IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017, pp. 403–412. 2

[26] N. Dalal and B. Triggs, “Histograms of oriented
gradients for human detection,” in international
Conference on computer vision & Pattern Recog-
nition (CVPR’05), vol. 1.
IEEE Computer Soci-
ety, 2005, pp. 886–893. 7

[17] T. Xiao, H. Li, W. Ouyang, and X. Wang,
“Learning deep feature representations with do-
main guided dropout for person re-identiﬁcation,”

[27] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person
re-identiﬁcation by local maximal occurrence rep-
resentation and metric learning,” in Proceedings

of the IEEE conference on computer vision and
pattern recognition, 2015, pp. 2197–2206. 7

ference on computer vision and pattern recogni-
tion, 2017, pp. 4700–4708. 9

[28] D. Lin and X. Tang, “Inter-modality face recogni-
tion,” in European conference on computer vision.
Springer, 2006, pp. 13–26. 7

[29] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H.
Lai, “Person re-identiﬁcation by camera correla-
tion aware feature augmentation,” IEEE transac-
tions on pattern analysis and machine intelligence,
vol. 40, no. 2, pp. 392–408, 2018. 7

[30] N. Rasiwasia, J. Costa Pereira, E. Coviello,
G. Doyle, G. R. Lanckriet, R. Levy, and N. Vas-
concelos, “A new approach to cross-modal multi-
media retrieval,” in Proceedings of the 18th ACM
international conference on Multimedia. ACM,
2010, pp. 251–260. 7

[31] S. Pedagadi, J. Orwell, S. Velastin, and B. Boghos-
sian, “Local ﬁsher discriminant analysis for pedes-
trian re-identiﬁcation,” in Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, 2013, pp. 3318–3325. 8

[32] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang,
and Q. Tian, “Scalable person re-identiﬁcation: A
benchmark,” in Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2015, pp.
1116–1124. 8

[33] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deep-
reid: Deep ﬁlter pairing neural network for person
re-identiﬁcation,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, 2014, pp. 152–159. 8

[34] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and
C. Tomasi, “Performance measures and a data set
for multi-target, multi-camera tracking,” in Euro-
pean Conference on Computer Vision. Springer,
2016, pp. 17–35. 8

[35] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A dis-
criminative feature learning approach for deep face
recognition,” in European conference on computer
vision. Springer, 2016, pp. 499–515. 8

[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep resid-
ual learning for image recognition,” in Proceedings
of the IEEE conference on computer vision and
pattern recognition, 2016, pp. 770–778. 9

[37] G. Huang, Z. Liu, L. Van Der Maaten, and
K. Q. Weinberger, “Densely connected convolu-
tional networks,” in Proceedings of the IEEE con-

