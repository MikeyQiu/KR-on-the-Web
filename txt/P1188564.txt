9
1
0
2
 
v
o
N
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
7
4
8
9
0
.
5
0
7
1
:
v
i
X
r
a

LIFELONG GENERATIVE MODELING

Jason Ramapuram ∗ †
Jason.Ramapuram@etu.unige.ch

Magda Gregorova ∗ †
magda.gregorova@hesge.ch

Alexandros Kalousis ∗ †
Alexandros.Kalousis@hesge.ch

ABSTRACT

Lifelong learning is the problem of learning multiple consecutive tasks in a se-
quential manner, where knowledge gained from previous tasks is retained and
used to aid future learning over the lifetime of the learner. It is essential towards
the development of intelligent machines that can adapt to their surroundings. In
this work we focus on a lifelong learning approach to unsupervised generative
modeling, where we continuously incorporate newly observed distributions into a
learned model. We do so through a student-teacher Variational Autoencoder ar-
chitecture which allows us to learn and preserve all the distributions seen so far,
without the need to retain the past data nor the past models. Through the intro-
duction of a novel cross-model regularizer, inspired by a Bayesian update rule,
the student model leverages the information learned by the teacher, which acts
as a probabilistic knowledge store. The regularizer reduces the effect of catas-
trophic interference that appears when we learn over sequences of distributions.
We validate our model’s performance on sequential variants of MNIST, Fashion-
MNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model
mitigates the effects of catastrophic interference faced by neural networks in se-
quential learning scenarios.

1

INTRODUCTION

Machine learning is the process of approximating unknown functions through the observation of typ-
ically noisy data samples. Supervised learning approximates these functions by learning a mapping
from inputs to a predeﬁned set of outputs such as categorical class labels (classiﬁcation) or contin-
uous targets (regression). Unsupervised learning seeks to uncover structure and patterns from the
input data without any supervision. Examples of this learning paradigm include density estimation
and clustering methods. Both learning paradigms make assumptions that restrict the set of plausible
solutions. These assumptions are referred to as hypothesis spaces, biases or priors and aid the model
in favoring one solution over another [82, 128]. For example, the use of convolutions [70] to process
images favors local structure; recurrent models [54, 46] exploit sequential dependencies and graph
neural networks [109, 64] assume that the underlying data can be modeled accurately as a graph.

Current state of the art machine-learning models typically focus on learning a single model for a
single task, such as image classiﬁcation [72, 121, 43, 118, 66], image generation [100, 13, 63, 39],
natural language question answering [25, 96] or single game playing [129, 113]. In contrast, humans
experience a sequence of learning tasks over their lifetimes, and are able to leverage previous learn-
ing experiences to rapidly learn new tasks. Consider learning how to ride a motorbike after learning
to ride a bicycle: the task is drastically simpliﬁed through the use of prior learning experience. Stud-
ies [2, 3, 67] in psychology have shown that humans are able to generalize to new concepts in a rapid
manner, given only a handful of samples. [67] demonstrates that humans can classify and generate
new concepts of two wheel vehicles given just a single related sample. This contrasts the state of the
art machine learning models described above which use hundreds of thousands of samples and fail
to generalize to slight variations of the original task [22].

∗University of Geneva, Switzerland
†Haute cole de gestion de Genve, HES-SO, Switzerland

1

Lifelong learning [126, 125] argues for the need to consider learning over task sequences, where
learned task representations and models are stored over the entire lifetime of the learner and can be
used to aid current and future learning. This form of learning allows for the transfer of previously
learned models and representations and can reduce the sample complexity of the current learning
problem [126]. In this work we restrict ourselves to a subset of the broad lifelong learning paradigm;
rather than focus on the supervised lifelong learning scenario as most state of the art methods, our
work is one of the ﬁrst to tackle the more challenging problem of deep lifelong unsupervised learn-
ing. We also identity and relax crucial limitations of prior work in life-long learning that requires
the storage of previous models and training data, allowing us to operate in a more realistic learning
scenario.

2 RELATED WORK
The idea of learning in a continual manner has been explored extensively in machine learning,
seeded by the seminal works of lifelong-learning [126, 125, 117], online-learning [32, 9, 11, 12]
and sequential linear gaussian models [105, 36] such as the Kalman Filter [56] and its non-linear
counterpart, the Particle Filter [24]. Lifelong learning bears some similarities to online learning
in that both learning paradigms observe data in a sequential manner. Online learning differs from
lifelong learning in that the central objective of a typical online learner [11, 12] is to best solve/ﬁt the
current learning problem, without preserving previous learning. In contrast, lifelong learners seek to
retain, and reuse, the learned behavior acquired over past tasks, and aim to maximize performance
across all tasks. Consider the example of forecasting click through rate: the objective of the online
learner is to evolve over time, such that it best represents current user preferences. This contrasts
lifelong learners which enforce a constraint between tasks to ensure that previous learning is not
lost.

Lifelong Learning [126] was initially proposed in a supervised learning framework for concept
learning, where each task seeks to learn a particular concept/class using binary classiﬁcation. The
original framework used a task speciﬁc model, such as a K Nearest Neighbors (KNN) 1, coupled with
a representation learning network that used training data from all past learning tasks (support sets),
to learn a common, global representation. This supervised approach was later improved through the
use of dynamic learning rates [115], core-sets [114] and multi-head classiﬁers [31].
In parallel,
lifelong learning was extended to independent multi-task learning [107, 31], reinforcement learning
[125, 122, 102], topic modeling [19, 130] and semi-supervised language learning [83, 81]. For a
more detailed review see [20].

More recently, lifelong learning has seen a resurgence within the framework of deep learning. As
mentioned earlier, one of the central tenets of lifelong learning is that that the learner should perform
well over all observed tasks. Neural networks, and more generally, models that learn using stochastic
gradient descent [103], typically cannot persist past task learning without directly preserving past
models or data. This problem of catastrophic forgetting [78] is well known in the neural network
community and is the central obstacle that needs to be resolved to build an effective neural lifelong
learner. Catastrophic forgetting is the phenomenon where model parameters of a neural network
trained in a sequential manner become biased towards the distribution of the latest observations,
forgetting previously learned representations, over data no longer accessible for training. In order to
mitigate catastrophic forgetting current research in lifelong learning employs four major strategies:
transfer learning, replay mechanisms, parameter regularization and distribution regularization. In
table 1 we classify the different lifelong learning methods that we will discuss in the following
paragraphs into these strategies.

EWC [65] VCL [91] LwF [71] ALTM [33]
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

PNN [94] DGR [112, 57] DBMNN [123]
(cid:88)
(cid:55)

(cid:88)
(cid:55)

(cid:55)
(cid:88)

(cid:88)

Transfer learning
Replay mechanisms
Parameter
regularization
Functional
regularization
Table 1: Catastropic interference mitigation strategies of state of the art models. Rows highlighted in gray
represent desirable mitigation strategies.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

SI [136] VASE [1] LGM (us)
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:88)

1These models were known as memory based learning in [126].

2

Transfer learning: These approaches mitigate catastrophic forgetting by freezing previous task
models and relaying a latent representation of the previous task to the current model. Research in
transfer learning for the mitigation of catastrophic forgetting include Progressive Neural Networks
(PNN) [94] and Deep Block-Modular Neural Networks (DBMNN) [123] to name a few. These
approaches allow the current model to adapt its parameters to the (new) joint representation in an
efﬁcient manner and prevent forgetting through the direct preservation of all previous task mod-
els. Deploying such a transfer learning mechanism in a lifelong learning setting would necessitate
training a new model with every new task, considerably increasing the memory footprint of the life-
long learner. In addition, since transfer learning approaches freeze previous models, it negates the
possibility of improving previous task performance using knowledge gathered from new tasks.

Replay mechanisms: The original formulation of lifelong learning [126] required the preservation
of all previous task data. This requirement was later relaxed in the form of core-sets [116, 114, 91],
which represent small weighted subsets of inputs that approximate the full dataset. Recently, within
the classiﬁcation setting, there have been replay approaches that try to lift the requirement of storing
past training data by relying on generative modeling [112, 57]; we will call such methods Deep gen-
erative replay (DGR) methods. DGR methods methods use a student-teacher network architecture,
where the teacher (generative) model augments the student (classiﬁer) model with synthetic samples
from previous tasks. These synthetic task samples are used in conjunction with real samples from
the current task to learn a new joint model across all tasks. While strongly motivated by biologi-
cal rehearsal processes [119, 53, 58, 110], these generative replay strategies fail to efﬁciently use
previous learning and simply re-learn each new joint task from scratch.

Parameter regularization: Most work that mitigates catastrophic forgetting falls under the um-
brella of parameter regularization. There are two approaches within this mitigation strategy: con-
straining the parameters of the new task to be close to the previous task through a predeﬁned metric,
and enforcing task-speciﬁc parameter sparsity. The two approaches are related as task-speciﬁc pa-
rameter sparsity can be perceived as a reﬁnement of the parameter constraining approach. Parameter
constraining approaches typically share the same model/parameters, but encourage new tasks from
altering important learned parameters from previous tasks. Task speciﬁc parameter sparsity relaxes
this, by enforcing that each task use a different subset of parameters from a global model, through
the use of an attention mechanism.

Models such as Laplace Propagation [29], Elastic Weight Consolidation (EWC) [65], Synaptic In-
telligence (SI) [136] and Variational Continual Learning (VCL) [91] fall under the parameter con-
straining approach. EWC for example, uses the Fisher Information matrix (FIM) to control the
change of model parameters between two learning tasks. Intuitively, important parameters should
not have their values changed, while non-important parameters are left unconstrained. The FIM is
used as a weighting in a quadratic parameter difference regularizer under a Gaussianity assump-
tion of the parameter posterior. However, this Gaussian parameter posterior assumption has been
demonstrated [86, 10] to be sub-optimal for learned neural network parameters. VCL improves
upon EWC, by generalizing the local assumption of the FIM to a KL-Divergence between the (vari-
ational) parameter posterior and prior. This generalization derives from the fact that the FIM can
be cast as a KL divergence between the posterior and an epsilon perturbation of the same random
variable [51]. VCL actually spans a number of different mitigation strategies as it uses parameter
regularization (described above) , transfer learning (it keeps a separate head network per task) and
replay (it persists a core-set of true data per task).

Models such as Hard Attention to the Task (HAT) [111] and the Variational Autoencoder with Shared
Embeddings (VASE) [1] fall under the task-speciﬁc parameter sparsity strategy. This mitigation
strategy enforces that different tasks use different components of a single model, typically through
the use of attention vectors [6] that are learned given supervised task labels. Multiplying the attention
vectors with the model outputs prevents gradient descent updates for different subsets of the model’s
parameters, allowing them to be used for future task learning. Task speciﬁc parameter sparsity allows
a model to hold-out a subset of its parameters for future learning and typically works well in practice
[111], with its strongest disadvantage being the requirement of supervised information.

Functional regularization: Parameter regularization methods attempt to preserve the learned be-
havior of the past models by controlling how the model parameters change between tasks. However,
the model parameters are only a proxy for the way a model actually behaves. Models with very
different parameters can have exactly the same behavior with respect to input-output relations (non-

3

uniqueness [131]). Functional regularization concerns itself with preserving the actual object of
interest:
the input-output relations. This strategy allows the model to ﬂexibly adapt its internal
parameter representation between tasks, while still preserving past learning.

Methods such as distillation [45], ALTM [33] and Learning Without Forgetting (LwF) [71] impose
similarity constraints on the classiﬁcation outputs of models learned over different tasks. This can
be interpreted as functional regularization by generalizing the constraining metric (or semi-metric)
to be a divergence on the output conditional distribution. In contrast to parameter regularization, no
assumptions are made on the parametric form of the parameter posterior distribution. This allows
models to ﬂexibly adapt their internal representation as needed, making functional regularization a
desirable mitigation strategy. One of the pitfalls of current functional regularization approaches is
that they necessitate the preservation of all previously data.

2.1 LIMITATIONS OF EXISTING APPROACHES.

A simple solution to the problem of lifelong learning is to store all data and re-learn a new joint
multi-task representation [17] at each newly observed task. Alternatively, it is possible to retain
all previous model parameters and select the model that presents the best performance on new test
task data. Existing solutions typically relax one these requirements. [33, 71, 91] relaxes the need
for model persistence, but requires preservation of all data [33, 71], or a growing core-set of data
[116, 114, 91]. Conversely, [94, 123, 91, 136] relaxes the need to store data, but persists all previous
models [94, 123] or a subset of model parameters [91, 136].

Unlike these approaches, we draw inspiration from how humans learn over time and remove the
requirement of storing past training data and models. Consider the human visual system; research
has shown [23, 7] that the human eye is capable of capturing 576 megapixels of content per image
frame. If stored naively on a traditional computer, this corresponds to approximately 6.9 gigabytes of
information per sample. Given that we perceive trillions of frames over our lifetimes, it is infeasible
to store this information in its base, uncompressed representation. Research in neuroscience has
validated [132, 5] that the associative human mind, compresses, merges and reconstructs information
content in a dynamic way. Motivated by this, we believe that a lifelong learner should not store past
training data or models. Instead, it should retain a latent representation that is common over all tasks
and evolve it as more tasks are observed.

2.2 OUR SOLUTION AT A HIGH LEVEL.

While most research in lifelong learning focuses on supervised learning [94, 33, 71, 123, 136, 65],
we focus on the more challenging task of deep unsupervised latent variable generative modeling.
These models have wide ranging applications such as clustering [77, 52, 85] and pre-training [68,
95].

Central to our lifelong learning method are a pair of generative models, aptly named the teacher and
student, which we train by exploiting the replay and functional regularization strategies described
above. After training a single generative model over the ﬁrst task, we use it is used as the teacher for
a newly instantiated student model. The student model receives data from the current task, as well as
replayed data from the teacher, which acts as a probabilistic storage container of past tasks. In order
to preserve previous learning, we make use of functional regularization, which aids in preserving
input-output relations over past tasks.

Unlike EWC or VCL, we make no assumptions on the form of the parameter posterior and allow
the generative models to use available parameters as appropriate, to best accommodate current and
past learning. The use of generative replay, coupled with functional regularization, renders the
preservation of the past models and past data unnecessary. It also signiﬁcantly improves the sample
complexity on future task learning, which we empirically demonstrate in our experiments. Finally
we should note that it is straightforward to adapt our approach to the supervised learning setting, as
we have done in [69].

3 BACKGROUND

In this section we describe the main concepts that we use throughout this work. We begin by de-
scribing the base-level generative modeling approach in Section 3.1, followed by how it extends to

4

the lifelong setting in Section 3.2. Finally, in Section 3.3, we describe the Variational Autoencoder
over which we instantiate our lifelong generative model.

3.1 LATENT VARIABLE GENERATIVE MODELING

We consider a scenario where we observe a dataset, D, consisting of N variates, D = {xj}N
j=1,
of a continuous or discrete variable x. We assume that the data is generated by a random process
involving a non-observed random variable, z. The data generation process involves ﬁrst sampling
zj ∼ P (z) and then producing a variate from the conditional, xj ∼ Pθ(x|z). We visualize this
form of latent generative model in the graphical model in Figure 14.

Figure 1: Typical latent variable graphical model. Gray nodes represent observed variables while white nodes
represent unobserved variables.

Typically, latent variables models are solved through maximum likelihood estimation which can be
formalized as:

max
θ

log Pθ(x) = max

log

Pθ(x|z)P (z)dz = max

log Ez[Pθ(x|z)]

(1)

θ

(cid:90)

θ

In many cases, the expectation from Equation 1 does not have a closed form solution (eg: non-
conjugate distributions) and quadrature is not computationally tractable due to large dimensional
spaces [63, 101] (eg: images). To overcome these intractabilities we use a Variational Autoencoder
(VAE), which we summarize in Section 3.3. The VAE allows us to infer our latent variables and
jointly estimate the parameters of our model. However, before describing the VAE, it is important
to understand how this generative setting can be perceived in a lifelong learning scenario.

3.2 LIFELONG GENERATIVE MODELING

Lifelong generative modeling extends the single-distribution estimation task from Section 3.1 to a
set of i = {1...L} sequentially observed learning tasks. The i-th learning task has variates that are
realized from the task speciﬁc conditional, Pi(x) = P (x|zd = i), where zd acts as a categorical
indicator variable of the current task. We visualize a simpliﬁed form of this in Figure 2 below.

Figure 2: Simpliﬁed lifetime of a lifelong learner. Given a true (unknown) distribution, P (x) =
(cid:82) P (x|zd)P (zd)δzd, we observe partial information in the form of L sequential tasks, {D1 (cid:55)→ D2, ... (cid:55)→
DL}. Observing more tasks, reduces the uncertainty of the model until convergence, PθL (x) ≈ P (x).

Crucially, when observing task, Di, the model has no access to any of the previous task datasets,
D<i. As the lifelong learner observes more tasks, {D1 (cid:55)→ D2 (cid:55)→ ... (cid:55)→ DL}, it should improve its
estimate of the true distribution, Pθ(x) ≈ P (x) = (cid:82) P (x|zd)P (zd)δzd, which is unknown at the
start of training.

5

3.3 THE VARIATIONAL AUTOENCODER

As eluded to in Section 3.1, we would like to infer the latent variables from the data. This
can be realized as an alternative form of Equation 1 in the form of Bayes rule: Pφ(z|x) =
Pθ(x|z)P (z)/Pθ(x), where Pφ(z|x) is referred to as the latent variable posterior and Pθ(x|z)
as the likelihood. One method of approximating the posterior, Pφ(z|x), is through MCMC sam-
pling methods such as Gibbs sampling [34] or Hamiltonian MCMC [87]. MCMC methods have
the advantage that they provide asymptotic guarantees [88] of convergence to the true posterior,
Pφ(z|x). However in practice it is not possible to know when convergence has been achieved. In
addition, due to their Markovian nature, they possess an inner loop, which makes it challenging to
scale for large scale datasets.

In contrast, Variational Inference (VI) [55] side-steps the intractability of the posterior by approxi-
mating it with a tractable distribution family, Qφ(z|x). VI rephrases the objective of determining the
posterior as an optimization problem by minimizing the KL divergence between the known distri-
butional family, Qφ(z|x), and the unknown true posterior, Pθ(z|x). Applying VI to the intractable
integral from Equation 1 results in the evidence lower bound (ELBO) or variational free energy,
which can easily be derived from ﬁrst principles:

log Pθ(x) = log

Pθ(x|z)P (z)dz

(cid:90)

= log

(cid:90) Qφ(z|x)
Qφ(z|x)

Pθ(x|z)P (z)dz

≥ EQ[log Pθ(x|z)] − DKL[Qφ(z|x)||P (z)]
(cid:123)(cid:122)
(cid:125)
ELBO

(cid:124)

(2)

(3)

(4)

where we used Jensen’s inequality to transition from Equation 3 to Equation 4. The objective intro-
duced in Equation 4 induces the graphical model shown below in Figure 3.

Figure 3: Standard VAE graphical model. Gray nodes represent observed variables while white nodes represent
unobserved variables; dashed lines represent inferred variables.

VAEs typically use deep neural networks to model the approximate inference network, Qφ(z|x)
and conditional, Pθ(x|z), which are also known as the encoder and decoder networks (respec-
tively). To optimize for the parameters of these networks, VAEs maximize the ELBO (Equation
4) using Stochastic Gradient Descent [103]. By sharing the variational parameters of the encoder,
φ, across the data points (amortized inference [35]), variational autoencoders avoid per-data inner
loops typically needed by MCMC approaches.

Optimizing the ELBO in Equation 4 requires computing the gradient of an expectation over the ap-
proximate posterior, Qφ(z|x). This typically takes place through the use of the path-wise estimator
[101, 63] (originally called “push-out” [106]). The path-wise reparameterizer uses the Law of the
Unconscious Statistician (LOTUS) [42], which enables us to compute the expectation of a function
of a random variable (without knowing its distribution) if we know its corresponding sampling path
and base distribution [84]. For the typical isotropic gaussian approximate posterior, Qφ(z|x), used
in standard VAEs this can be aptly summarized by:

z ∼ Qφ(z|x) ⇔ µφ(x) + σφ(x)(cid:15), (cid:15) ∼ N (0, 1)

∇φEQφ(z|x)[log Pθ(x|z)] ⇔ E
N ((cid:15)|0, 1)
(cid:125)
(cid:123)(cid:122)
(cid:124)

base distribution

)]
[∇φ log Pθ(x| µφ(x) + σφ(x)(cid:15)
(cid:125)
(cid:124)

(cid:123)(cid:122)
sampling path

(5)
(6)

6

where Equation 5 deﬁnes the sampling procedure of our latent variable through the location-scale
transformation and Equation 6 deﬁnes the path-wise Monte Carlo gradient estimator applied on the
decoder (ﬁrst term in Equation 4). This Monte Carlo estimator enables differentiating through the
sampling process of the distribution Qφ(z|x). Note that computing the gradient of the second term
in Equation 4, ∇φDKL[Qφ(z|x)||P (z)], is possible through a closed form analytical solution for
the case of isotropic gaussian distributions.

While it is possible to extend any latent variable generative model to the lifelong setting, we choose
to build our lifelong generative models using variational autoencoders (VAEs) [63] as they provide
a mechanism for stable training; this contrasts other state of the art unsupervised models such as
Generative Adversarial Networks (GANs) [39, 61]. Furthermore, latent-variable posterior approxi-
mations are a requirement in many learning scenarios such as clustering [93], compression [92] and
unsupervised representation learning [30]. Finally, GANs can suffer from low sample diversity [28]
which can lead to compounding errors in a lifelong generative setting.

4 LIFELONG LEARNING MODEL

Algorithm 1 Data Flow

Teacher:
Sample Prior: zj ∼ P (z)
Decode: ˆxj ∼ PΘ(x|z)

Student:
Sample : xj ∼ P (ω)P (x|ω)
Encode : zj ∼ Qφ(z|x)
Decode: ˆxj ∼ Pθ(x|z)

Figure 4: Student training procedure. Left: graphical model for student-teacher model. Data generated from
the teacher model (top row) is used to augment the current training data observed by the student model (bottom
row). A posterior regularizer is also applied between Qφ(z|x) and QΦ(z|x) to enable functional regularization
(not shown, but discussed in detail in Section 4.1.1). Right: data ﬂow algorithm.

fMRI studies of the rodent [119, 53, 58] and human [110] brains have shown that previously experi-
enced sequences of events are replayed in the hippocampus during rest. These replays are necessary
for better planning [53] and memory consolidation [16]. We take inspiration from the memory con-
solidation of biological learners and introduce our model of Lifelong Generative Modeling (LGM).
We visualize the LGM student-teacher architecture in Figure 4.

The student and the teacher are both instantiations of the same base-level generative model, but
have different roles throughout the learning process. The teacher’s role is to act as a probabilistic
knowledge store of previously learned distributions, which it transfers to the student in the form
of replay and functional regularization. The student’s role is to learn the distribution over the new
task, while accommodating the learned representation of the teacher over old tasks. In the following
sections we provide detailed descriptions of the student-teacher architecture, as well as the base-
level generative model that each of them use. The base-level model uses a variant of VAEs, which
we tailor for lifelong learning and is learned by maximizing a variant of the standard VAE ELBO
from Equation 4 ; we describe this objective at end of this section.

4.1 STUDENT-TEACHER ARCHITECTURE

The top row of Figure 4 represents the teacher model. At any given time, the teacher contains a
summary of all previous distributions within the learned parameters, Φ, of the encoder QΦ(z|x),
and the learned parameters, Θ, of the decoder PΘ(x|z). We use the teacher to generate synthetic
variates, ˆxj, from these past distributions by decoding variates from the prior, zj ∼ P (z) (cid:55)→
PΘ(x|z = zj). We pass the generated (synthetic) variates, ˆxj, to the student model as a form of
knowledge transfer about the past distributions. Information transfer in this manner is known as
generative replay and our work is the ﬁrst to explore it in a VAE setting.

7

The bottom row of Figure 4 represents the student. The student is responsible for updating the
parameters, φ, of its encoder, Qφ(z|x), and θ, of its decoder Pθ(x|z). Importantly, the student re-
ceives data from both the currently observed task, as well as synthetic data generated by the teacher.
This can be formalized as xj ∼ P (ω)P (x|ω), ω ∼ Ber(π), as shown in Equation 7:

P (ω)P (x|ω) =

(cid:26)PΘ(x|z) ω = 0
ω = 1

Pi(x)

(7)

The mean, π, of the Bernoulli distribution, controls the sampling proportion of the previously learned
distributions to the current one and is set based on the number of assimilated distributions. Thus,
given i observed distributions: π = 1
i+1 . This ensures that the samples observed by the student
are representative of both the current and past distributions. Note that this does not correspond
to varying sample sizes in datasets, but merely our assumption to model each distribution with
equivalent weighting.

Once a new task is observed, the old teacher is dropped, the student model is frozen and becomes the
new teacher (φ → Φ, θ → Θ). A new student is then instantiated with the latest weights φ and θ
from the previous student (the new teacher). Due to the cyclic nature of this process, no new models
are added. This contrasts many existing state of the art deep lifelong learning methods which add an
entire new model or head-network per task (eg: [91, 94, 123]).

A crucial aspect in the lifelong learning process is to ensure that previous learning is successfully
exploited to bias current learning [126]. While the replay mechanism that we put in place ensures
that the student will observe data from all tasks, it does not ensure that previous knowledge from
the teacher is efﬁciently exploited to improve current student learning. The student model will re-
learn (from scratch) a completely new representation, which might be different than the teacher. In
order to successfully transfer knowledge between both VAE models, we rely on functional regular-
ization, which we enforce through a Bayesian update regularizer of the posteriors of both models.
Intuitively, we would like the student model’s latent outputs, zj ∼ Qφ(z|x) to be similar to la-
tent outputs of teacher model, zj ∼ QΦ(z|x), over synthetic variates generated by the teacher,
xj ∼ P (ω)P (x|ω = 0) = PΘ(x|z). In the following section, we describe the exact functional
form of this regularizer and demonstrate how it can be perceived as a natural extension of the VAE
learning objective to a sequential setting.

4.1.1 KNOWLEDGE TRANSFER VIA BAYESIAN UPDATE.

While both the student and teacher are instantiations of VAE variants, tailored for the particularities
of the lifelong setting, for the purpose of this exposition we use the standard VAE formulation. Our
objective is to learn the set of parameters [φ, θ] of the student, such that it can generate variates
from the complete distribution, P (x), described in Section 3.2. Subsuming the deﬁnition of the
augmented input data, x ∼ P (ω)P (x|ω), from Equation 7, we can deﬁne the student ELBO as:

Lθ,φ(x) = EQφ(z|x)

(cid:20)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)],

(8)

x ∼ P (ω)P (x|ω), ω ∼ Ber(π).

Rather than naively shrinking the full posterior to the prior via the KL divergence in Equation 8, we
rely on one of the core tenets of the Bayesian paradigm which states that we can always update our
posterior when given new information (yesterdays posterior is todays prior) [79]. Given this tenet,
we introduce our posterior regularizer 2:

KL[Qφ(z|x)||QΦ(z|x)], x ∼ P (x|ω = 0)
(9)
which distills the teacher’s learnt representation into the student over the generated data only. Com-
bining Equations 8 and 9, yields the objective that we can use to train the student and is described
below in Equation 10:

Lθ,φ(x) = EQφ(z|x)

log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(10)

(cid:20)

(cid:21)

2While it

is also possible to apply a similar

regularizer

to the reconstruction term,

i.e:

KL[Pθ(x|z) || PΘ(x|z)], we observed that doing so hurts performance (Appendix 10.2).

+ (1 − ω)KL[Qφ(z|x)||QΦ(z|x)],
x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

8

Note that this is not the ﬁnal objective, due to the fact that we have yet to present the VAE variant
tailored to the particularities of the lifelong setting. We will now show how the posterior regularizer
can be perceived as a natural extension of the VAE learning objective, through the lens of a Bayesian
update of the student posterior.

Lemma 1 For random variables x and z with conditionals QΦ(z|x) and Qφ(z|x), both distributed
as a categorical or gaussian and parameterized by Φ and φ respectively, the KL divergence between
the distributions is:

KL[Qφ(z|x)||QΦ(z|x)] = KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(11)

where ˆφ = f (φ, Φ) depends on the parametric form of Q, and C is only a function of the parameters,
Φ.

We prove Lemma 1 for the relevant distributions (under some mild assumptions) in Appendix 10.1.
Using Lemma 1 allows us to rewrite Equation 10 as shown below in Equation 12:
(cid:20)

Lθ,φ(x) = EQφ(z|x)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(12)

(cid:20)

+ (1 − ω)

KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(cid:21)
,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

This rewrite makes it easy to see that our posterior regularizer from Equation 10 is a standard VAE
ELBO (Equation 4) under a reparameterization of the student parameters, ˆφ = f (φ, Φ). Note that
C(Φ) is constant with respect to the student parameters, φ, and thus not used during optimization.
While the change seems minor, it omits the introduction of f (φ, Φ) which allows for a transfer of
information between models. In practice, we simply analytically evaluate KL[Qφ(z|x) ||QΦ(z|x)],
the KL divergence between the teacher and the student posteriors, instead of deriving the functional
form of f (φ, Φ) for each different distribution pair. We present Equation 12 simply as a means to
provide a more intuitive understanding of our functional regularizer.

4.2 BASE-LEVEL GENERATIVE MODEL.

While it is theoretically possible to use the vanilla VAE from Section 3.3 for the teacher and student
models, doing so brings to light a number of limitations that render it problematic for use in the con-
text of lifelong learning (visualized in Figure 5-Right). Speciﬁcally, using a standard VAE decoder,
Pθ(x|z), to generate synthetic replay data for the student is problematic due to two reasons:

1. Mixed Distributions: Sampling the continuous standard normal prior, N (0, 1), can select
a point in latent space that is in between two separate distributions, causing generation of
unrealistic synthetic data and eventually leading to loss of previously learnt distributions.
2. Undersampling: Data points mapped to the isotropic-gaussian posterior that are further
away from the prior mean will be sampled less frequently, resulting in an undersampling
of some of the constituent distributions.

To address these sampling limitations we decompose the latent variable, z, into an independent
continuous, zc ∼ Qφ(zc|x), and a discrete component, zd ∼ Qφ(zd|x), as shown in Equation 13
and visually in Figure 5-Left:

Qφ(zc, zd|x) = Qφ(zc|x)Qφ(zd|x).

(13)

The objective of the discrete component is to summarize the discriminative information of the indi-
vidual generative distributions. The continuous component on the other hand, caters for the remain-
ing sample variability (a nuisance variable [73]). Given that the discrete component can accurately
summarize the discriminative information, we can then explicitly sample from any of the past dis-
tributions, allowing us to balance the student model’s synthetic inputs with samples from all of the
previous learned distributions. We describe this beneﬁcial generative sampling property in more
detail in Section 4.2.1.

9

Figure 5: Left: Graphical model for VAE with independent discrete and continuous posterior, Qφ(zc, zd|x) =
Qφ(zc|x)Qφ(zd|x). Right: Two dimensional test variates, zj ∼ Qφ(z|x), zj ∈ R2, of a vanilla VAE
trained on MNIST. We depict the two generative shortcomings visually: 1) mixing of distributions which
causes aliasing in a lifelong setting and 2) undersampling of distributions in a standard isotropic-gaussian VAE
posterior.

Naively introducing the discrete component, zd, does not guarantee that the decoder will use it to
represent the most discriminative aspects of the modeled distribution. In preliminary experiments,
we observed that that the decoder typically learns to ignore the discrete component and simply
relies on the continuous variable, zc. This is similar to the posterior collapse phenomenon which
has received a lot of recent interest within the VAE community [99, 40]. Posterior collapse occurs
when training a VAE with a powerful decoder model such as a PixelCNN++ [127] or RNN [21, 40].
The output of the decoder, xj ∼ Pθ(x|z) can become almost independent of the posterior sample,
zj ∼ Qφ(z|x), but is still able to reconstruct the original sample by relying on its auto-regressive
property [40]. In Section 4.2.2, we introduce a mutual information regulariser which ensures that
the discrete component of the latent variable is not ignored.

4.2.1 CONTROLLED GENERATIONS.

Desired Task Conditional
P1(x) = P (x|zd = 1)
P2(x) = P (x|zd = 2)
P3(x) = P (x|zd = 3)

zc
∼ N (0, 1)
∼ N (0, 1)
∼ N (0, 1)

zd
[0, 0, 1]
[0, 1, 0]
[1, 0, 0]

Figure 6 & Table 2: FashionMNIST with L = 3 tasks: t-shirts, sandals and bag. To generate samples from
the i-th task conditional, Pi(x) = P (x|zd = i), we set zd = i, randomly sample zc ∼ N (0, 1), and run
[zc, zd] through the decoder, Pθ(x|zc, zd). Resampling zc, while keeping zd ﬁxed, enables generation of
varied samples from the task conditional. Left: Desired task conditionals. Right: Desired decoder behavior.

Given the importance of generative replay for knowledge transfer in LGM, synthetic sample gen-
eration by the teacher model needs to be representative of all the previously observed distributions
in order to prevent catastrophic forgetting. Under the assumption that zd accurately captures the
underlying discriminativeness of the individual distributions and through the deﬁnition of the LGM
generative process, shown in Equation 14:

PΘ(x|zd, zc), zc ∼ N (0, 1), zd ∼ Cat(1/L),

we can control generations by setting a ﬁxed value, zd = i, and randomly sampling the continuous
prior, zc ∼ N (0, 1). This is possible because the trained decoder approximates the task conditional
from Section 3.2:

(14)

(15)

P (x|zd = i) = Pi(x) ≈

PΘ(x|zc, zd = i)P (zc)
QΦ(zc|x)

10

where sampling the true task conditional, Pi(x), can be approximated by sampling zc ∼ P (zc) =
N (0, 1), keeping zd ﬁxed, and decoding the variates as shown in Equation 16 below:

ˆx ∼ PΘ(x|zc ∼ N (0, 1), zd = i).

(16)

We provide a simple example of our desired behavior for three generative tasks, L = 3, using Fash-
ion MNIST in Figure 6 and Table 2 above. The assumption made up till now is that zd accurately
captures the discriminative aspects of each distribution. However, there is no theoretical reason for
the model to impose this constraint on the latent variables. In practice, we often observe that the
decoder Pθ(x|zc, zd) ignores zd due to the much richer representation of the continuous variable,
zc. In the following section we introduce a mutual information constraint that encourages the model
to fully utilize zd.

4.2.2

INFORMATION RESTRICTING REGULARIZER

As eluded to in the previous section, the synthetic samples observed by the student model need to be
representative of all previous distributions. In order to control sampling via the process described in
Section 4.2.1, we need to enforce that the discrete variable, zd, carries the discriminative informa-
tion about each distribution. Given our graphical model from Figure 4-Left, we observe that there
are two ways to accomplish this: maximize the information content between the discrete random
variable, zd and the decoded ˆx, or minimize the information content between the continuous vari-
able, zc and the decoded ˆx. Since our graphical model and underlying network does not contain skip
connections, information from the input, x, has to ﬂow through the latent variables z = [zc, zd] to
reach the decoder. While both formulations can theoretically achieve the same objective, we ob-
served that in practice, minimizing I(ˆx, zc) provided better results. We believe the reason for this
is that minimizing I(ˆx, zc) provides the model with more subtle gradient information in contrast to
maximizing I(ˆx, zd) which receives no gradient information when the value of the k-th element of
the categorical sample is 1. We now formalize our mutual information regularizer, which we derive
from ﬁrst principles in Equation 17:

Ex∼Pi(x)[I(ˆx, zc)] = Ex∼Pi(x)[H(zc) − H(zc|ˆx)]

(17)

= Ex∼Pi(x)Ezc∼Qφ(zc|x)
(cid:123)(cid:122)
Ex∼Pi(x)[H(zc)]
+

(cid:124)

(cid:20)

(cid:21)
− log Qφ(zc|x)

(cid:125)

Ex∼Pi(x)E(zd,zc)∼Qφ(zc,zd|x)
(cid:124)

(cid:21)
(cid:20)
Eˆx∼Pθ (ˆx|zc,zd) log Qφ(zc|ˆx)
,
(cid:123)(cid:122)
Ex∼Pi(x)[−H(zc|ˆx)]

(cid:125)

where we use the independence assumption of our posterior from Equation 13 and the fact that the
expectation of a constant is the constant. This regularizer has parallels to the regularizer in InfoGAN
[18]. In contrast to InfoGAN, VAEs already estimate the posterior Qφ(zc|x) and thus do not need
the introduction of any extra parameters φ for the approximation. In addition [47] demonstrated that
InfoGAN uses the variational bound (twice) on the mutual information, making its interpretation
unclear from a theoretical point of view. In contrast, our regularizer has a clear interpretation: it
restricts information through a speciﬁc latent variable within the computational graph. We observe
that this constraint is essential for empirical performance of our model and empirically validate this
in our ablation study in Experiment 7.2.

4.3 LEARNING OBJECTIVE

The ﬁnal learning objective for each of the student models is the maximization of the sequential
VAE ELBO (Equation 10), coupled with generative replay (Equation 7)and the mutual information
regularizer, I(ˆx, zc), (Equation 17):

11

Lθ,φ(x) = EQφ(zc,zd|x)
(cid:124)

(cid:20)

(cid:21)
log Pθ(x|zc, zd)

− KL[Qφ(zc, zd|x)||P (zc, zd)]

(18)

(cid:125)

(cid:124)

(cid:123)(cid:122)
VAE ELBO
+ (1 − ω)KL[Qφ(zc, zd|x)||QΦ(zc, zd|x)]
(cid:125)
(cid:123)(cid:122)
Posterior Consistency Regularizer
− λI(ˆx, zc)
(cid:125)

(cid:123)(cid:122)
Mutual Information

(cid:124)

,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

The λ hyper-parameter controls the importance of the information gain regularizer. Too large a
value for λ causes a lack of sample diversity, while too small a value causes the model to not use
the discrete latent distribution. We did a random hyperparameter search and determined λ = 0.01
to be a reasonable choice for all of our experiments. This is in line with the λ used in InfoGAN
[18] for continuous latent variables. We empirically validate the necessity of both terms proposed in
Equation 18 in our ablation study in Experiment 7.2. We also validate the beneﬁt of the latent vari-
able factorization in Experiment 7.1. Before delving into the experiments, we provide a theoretical
analysis of computational complexity induced by our model and objective (Equation 18) in Section
4.4 below.

4.4 COMPUTATIONAL COMPLEXITY

We deﬁne the computational complexity of a typical VAE encoder and decoder as O(E)
and O(D) correspondingly;
internally these are dominated by the matrix-vector products
which take approximately LO(n2) for L layers. We also deﬁne the cost of applying the loss
function as O(K) + O(R), where O(K) is the cost of evaluating the KL divergence from
the ELBO (Equation 4) and O(R) the cost for evaluating the reconstruction term. Given
these deﬁnitions, we can summarize LGM’s computation complexity as follows in Equation 19:

O(D)
(cid:124) (cid:123)(cid:122) (cid:125)
teacher
generations

+ O(E) + O(D)
(cid:123)(cid:122)
(cid:125)
student encode
+ decode

(cid:124)

+ O(K) + O(R)
(cid:125)
(cid:123)(cid:122)
vae loss

(cid:124)

+ O(K)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
regularizer

+ O(K) + O(E)
(cid:123)(cid:122)
(cid:125)
mutual info

(cid:124)

= 2[O(D) + O(E)] + 3[O(K)] + O(R),

(19)

where we introduce increased computational complexity due to teacher generations, the cost of the
posterior regularizer, and the mutual information terms; the latter of which necessitates an extra
encode operation, O(E). The computational complexity is still dominated by the matrix-vector
product from evaluating forward functionals of the neural network. These operations can easily be
amortized through parallelization on modern GPUs and typical experiments do not directly scale
as per Equation 19. In our most demanding experiment (Experiment 6.6), we observe an average
empirical increase of 13.53 seconds per training epoch and 6.3 seconds per test epoch.

5 REVISITING STATE OF THE ART METHODS.

In this section we revisit some of the state of the art methods from Section 2. We begin by providing
a mathematical description of the differences between EWC [65], VCL [91] and LGM and follow it
up with a discussion of VASE [1] and their extensions of our work.

EWC and VCL: Our posterior regularizer, KL[Qφ(z|x)||QΦ(z|x)], affects the same parameters,
φ, as parameter regularizer methods such as EWC and VCL. However, rather than assuming a func-
tional form for the parameter posterior, P (φ|x), our method regularizes the output latent distribu-
tion Qφ(z|x). EWC and VCL, both make the assumption that P (φ|x) is distributed as an isotropic
gaussian3. This allows the use of the Fisher Information Matrix (FIM) in a quadratic parameter reg-
ularizer in EWC, and an analytical KL divergence of the posterior in VCL. This is a very stringent

3VCL assumes an isotropic gaussian variational form vs. EWC which directly assumes the parametric form

on P (φ|x).

12

requirement for the parameters of a neural network and there is active research in Bayesian neural
networks that attempts to relax this constraint [74, 75, 80].

EWC minφ d[P (φ|x)||P (Φ|x)]
2 (φ − Φ)T F (φ − Φ)

≈ γ

LGM ( Isotropic Gaussian Posterior ) minφ d[Qφ(z|x)||QΦ(z|x)]
|ΣΦ|
|Σφ|

Φ Σφ) + (µΦ − µφ)T Σ−1

Φ (µΦ − µφ) − C + log

(cid:20)
tr(Σ−1

(cid:18)

= 0.5

(cid:19)(cid:21)

In the above table we examine the distance metric d, used to minimize the effects of catastrophic
inference in both EWC and LGM. While our method can operate over any distribution that has
a tractable KL-divergence, for the purposes of demonstration we examine the simple case of an
isotropic gaussian latent-variable posterior. EWC directly enforces a quadratic constraint on the
model parameters φ, while our method indirectly affects the same parameters through a regulariza-
tion of the posterior distribution Qφ(z|x). For any given input variate, xj, LGM allows to model to
freely change its internal parameters, φ; it does so in a non-linear4 way such that the analytical KL
shown above is minimized.

VASE : The recent work of Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies (VASE) [1] extend upon our work [1, p. 7], but take a more empirical route by
incorporating a classiﬁcation-based heuristic for their posterior distribution. In contrast, we show
(Section 4.1.1) that our objective naturally emerges in a sequential learning setting for VAEs, allow-
ing us to infer the discrete posterior, Qφ(zd|x) in an unsupervised manner. Due to the incorporation
of direct supervised class information [1] also observe that regularizing the decoding distribution
Pθ(x|z) aids in the learning process, something that we observe to fail in a purely unsupervised
generative setting (Appendix Section 10.2). Finally, in contrast to [1], we include an information
restricting regularizer (Section 4.2.2) which allows us to directly control the interpretation and ﬂow
of information of the learnt latent variables.

6 EXPERIMENTS

We evaluate our model and the baselines over standard datasets used in other state of the art life-
long / continual learning literature [91, 136, 112, 57, 65, 94]. While these datasets are simple in
a traditional classiﬁcation setting, transitioning to a lifelong-generative setting scales the problem
complexity substantially. We evaluate LGM on a set of progressively more complicated tasks (Sec-
tion 6.2) and provide comparisons against baselines [91, 136, 65, 29, 63] using a set of standard
metrics (Section 6.1). All network architectures and other optimization details for our LGM model
are provided in Appendix Section 10.3 as well our open-source git repository [98].

6.1 PERFORMANCE METRICS

To validate the beneﬁt of LGM in a lifelong setting we explore three main performance dimensions:
the ability for the model to reconstruct and generate samples from all previous tasks and the ability
to learn a common representation over time, thus reducing learning sample complexity. We use three
main quantitative performance metrics for our experiments: the log-likelihood importance sample
estimate [15, 91], the negative test ELBO, and the Frechet distance metric [44]. In addition, we also
provide two auxiliary metrics to validate the beneﬁts of LGM in a lifelong setting: training sample
complexity and wall clock time per training and test epoch.

To fairly compare models with varying latent variable conﬁgurations, one solution is to marginal-
ize out the latents, z, during model evaluation / test time: (cid:82)
k=1 Pθ(x|z = zk).
This is realized in practice by using a Monte Carlo approximation (typically K=5000) and is com-
monly known as the importance sample (IS) log-likelihood estimate [15, 91]. As latent variable
and model complexity grows, this estimate tends to become noisier and intractable to compute. For
our experiments we use this metric only for the FashionMNIST and MNIST datasets as computing
one estimate over 10,000 test samples for a complex model takes approximately 35 hours on a K80
GPU.

z Pθ(x|z)dz ≈ (cid:80)K

In contrast to the IS log-likelihood estimate, the negative test ELBO (Equation 4) is only applicable
when comparing models with the same latent variable conﬁgurations; it is however much faster to

4This is because the parameters of the distribution are modeled by a deep neural network.

13

compute. The negative test ELBO provides a lower bound to the test log-likelihood of the true data
distribution under the assumed latent variable conﬁguration. One crucial aspect missing from both
these metrics is an evaluation of generation quality. We resolve this by using the Frechet distance
metric [44] and qualitative image samples.

The Frechet distance metric allows us to quantify the quality and diversity of generated samples by
using a pre-trained classiﬁer model to compare the feature statistics (generally under a Gaussian-
ity assumption) between synthetic generated samples and samples drawn from the test set. If the
Frechet distance between these two distributions is small, then the generative model is said to be
generating realistic images. The Frechet distance between two gaussians (produced by evaluating
latent embeddings of a classiﬁer model) with means mtest, mgen with corresponding covariances
Ctest, Cgen is:

||mtest − mgen||2

2 + T r(Ctest + Cgen − 2[CtestCgen]0.5).

(20)

While the Frechet distance, negative ELBO and IS log-likelihood estimate provide a glimpse into
model performance, there exists no conclusive metric that captures the quality of unsupervised gen-
erative models [124, 108] and active research suggests a direct trade-off between perceptual quality
and model representation [8]. Thus, in addition to the metrics described above, we also provide
qualitative metrics in the form of test image reconstructions and image generations. We summarize
all used performance metrics in Table 3 below:

Negative ELBO

Deﬁnition

Equation 4.

Purpoose
Quantitative metric on
likelihood / reconstructions.
Quantitative metric on
density estimate.
Quantitative metric on generations.

Lower is better?

yes

Negative Log-Likelihood

Frechet Distance
Test Reconstructions
Generations
#Training Samples

5000 (latent) sample Monte
Carlo estimate of Equation 4.
Equation 20.
yes
Pθ(x|zc ∼ Qφ(zc|x), zd ∼ Qφ(zd|x)) Qualitative view of reconstructions. N/A
Pθ(x|zd ∼ Cat(1/L), zc ∼ N (0, 1)).
N/A
# real training samples used for task i.
yes

Qualitative view of generations.
Sample Complexity.

yes

Table 3: Summary of different performance metrics.

6.2 DATA FLOW

Figure 7: Visual examples of training and test task sequences (top to bottom) for the datasets used to validate
LGM. The training set only consists of samples from the current task while the test set is a cumulative union
of the current task, coupled with all previous tasks. The permuted MNIST tasks uses {G1, ...GL−1} different
ﬁxed permutation matrices to create 4 auxiliary datasets.

14

In Figure 7 we list train and test variates depicting the data ﬂow for each of the problems that we
model. Due to the relaxing the need to preserve data in a lifelong setting, the train task sequence ob-
serves a single dataset, Dtr
i , at a time, without access to any previous, Dtr
<i. The corresponding test
dataset consists of a union (∪ operator) of the current test dataset, Dte
i , merged with all previously
observed test datasets, ˆD
i ∪ Dte

i−1 ∪ ... ∪ Dte
1 .

te
i = Dte

MNIST / Fashion MNIST: For the MNIST and Fashion MNIST problems, we observe a single
MNIST digit or fashion object (such as shirts) at a time. Each training set consists of 6000 training
samples and 1000 test samples. These samples are originally extracted from the full training and
test datasets which consist of 60,000 training and 10,000 test samples.

Permuted MNIST: this problem differs from the MNIST problem described above in that we use
the entire MNIST dataset at each task. After observing the ﬁrst task, which is the standard MNIST
dataset, each subsequent task differs through the application of a ﬁxed permutation matrix Gi on
the entire MNIST dataset. The test task sequence differs from the training task sequence in that we
simply use the corresponding full train and test MNIST datasets (with the appropriate application of
Gi).

Celeb-A: We split the CelebA dataset into four individual distributions using the features: bald,
male, young and eye-glasses. As with the previous problems, we treat each subset of data as an
individual distribution, and present our model samples from a single distribution at a time. This
presents a real world scenario as the samples per distribution varies drastically from only 3,713
samples for the bald distribution, to 126,788 samples for young. In addition speciﬁc samples can
span one or more of these distributions.

SVHN to MNIST: in this problem, we transition from fully observing the centered SVHN [89]
dataset to observing the MNIST dataset. We treat all samples from SVHN as being generated by
one distribution P1(x) and all the MNIST 5 samples as generated by another distribution P2(x)
(irrespective of the speciﬁc digit). At inference, the model is required to reconstruct and generate
from both datasets.

6.3 SITUATING AGAINST STATE OF THE ART LIFELONG LEARNING MODELS.
To situate LGM against other state of the art methods in lifelong learning we use the sequential
FashionMNIST and MNIST datasets described earlier in Section 6.2 and the data ﬂow diagram in
Figure 7. We contrast our LGM model against VCL [91], VCL without a task speciﬁc head network,
SI [136], EWC [65], Laplace propagation [29], a full batch VAE trained jointly on all data and a
standard naive sequential VAE without any catastrophic forgetting prevention strategy in Figures 8
and 9 below. The full batch VAE presents the upper-bound performance and all lifelong learning
models typically under-perform this model by the ﬁnal learning task. For the baselines, we use
the generously open sourced code [90] by the VCL authors, using the optimal hyper-parameters
speciﬁed for each model. We begin by evaluating the 5000 sample Monte Carlo estimate of the
log-likelihood of all compared models in Figure 8 below:

Figure 8: IS log-likelihood (mean ± std) × 5. Left: Fashion MNIST. Right: MNIST.

5MNIST was resized to 32x32 and converted to RBG to make it consistent with the dimensions of SVHN.

15

Even though each trial was repeated ﬁve times (each), we observe large increases in the estimates
at a few critical points. After further inspection, we determined the large magnitude increases were
due to the model observing a drastically different distribution at that point. We overlay the graphs
with an example variate for of the magnitude spikes. In the case of FashionMNIST for example,
the model observes its ﬁrst shoe distribution at i = 6; this contrasts the previously observed items
which were mainly clothing related objects. Interestingly we observe that LGM has much smoother
performance across tasks. We posit this is because LGM does not constrain its parameters, and
instead enforces the same input-output mapping through functional regularization.

Figure 9: Final model, i = 10, generation and reconstructions for MNIST and FashionMNIST. The LGM
model presents competitive performance for both generations and reconstructions, while not preserving any
past data nor past models.

16

Since one of the core tenets of lifelong learning is to reduce sample complexity over time, we use
this experiment to validate if LGM does in fact achieve this objective. Since all LGM models are
trained with an early-stopping criterion, we can directly calculate the number of samples used for
each learning task using the stopping epoch and mean, π of the Bernoulli sampling distribution of
the student model. In Figure 10 we plot the number of true samples and the number of synthetic
samples used by a model until it satisﬁed its early-stopping criterion. We observe a steady decrease
in the number of real samples used over time, validating LGMs advantage in a lifelong setting.

Figure 10: FashionMNIST sample complexity. Left: Synthetic training samples used till early-stopping. Right:
Real samples used till early-stopping.

6.4 DIVING DEEPER INTO THE SEQUENCE.
Rather than only visualizing the ﬁnal model’s qualitative results as in Figure 9, we provide quali-
tative results for model performance over time for the PermutedMNIST experiment in Figure 11.
This allows us to visually observe lifelong model performance over time. In this experiment, we
focus our efforts on EWC and LGM and visualize model (test) reconstructions starting from the
second learning task, G1D, till the ﬁnal G4D. The EWC-VAE variant that we use as a baseline
has the same latent variable conﬁguration as our model, enabling the usage of the test ELBO as a
quantitative metric for comparison. We use an unpermuted version of the MNIST dataset, D, as our
ﬁrst distribution, P1(x), as it allows us to visually asses the degradation of reconstructions. This is a
common setup utilized in continual learning [65, 136] and we extend it here to the density estimation
setting.

Figure 11: Top row: test-samples; bottom row: reconstructions. We visualize an increasing number of accu-
mulated distributions from left to right. (a) Lifelong VAE model (b) EWC VAE model.

17

Both models exhibit a different form of degradation: EWC experiences a more destructive form of
degradation as exempliﬁed by the salt-and-pepper noise observed in the ﬁnal dataset reconstruction
at G4D. LGM on the hand experiences a form of Gaussian noise as visible in the corresponding
ﬁnal dataset reconstruction. In order to numerically quantify this performance we analyze the log-
Frechet distance and negative ELBO below in Figure 12, where we contrast the LGM to EWC, a
batch VAE (full-vae in graph), an upto-VAE that observes all training data up to the current distri-
bution and a vanilla sequential VAE (vanilla). We examine a variety of different convolutional and
dense architectures and present the top performing models below. We observe that LGM drastically
outperforms EWC and the baseline naive sequential VAE in both metrics.

Figure 12: PermutedMNIST (a) negative Test ELBO and (b) log-Frechet distance.

6.5 LEARNING ACROSS COMPLEX DISTRIBUTIONS

Figure 13: (a) Reconstructions of test samples from SVHN[left] and MNIST[right]; (b) Decoded samples
ˆx ∼ Pθ(x|zd, zc) based on linear interpolation of zc ∈ R2 with zd = [0, 1]; (c) Same as (b) but with
zd = [1, 0].

The typical assumption in lifelong learning is that the sequence of observed distributions are related
[125] in some manner. In this experiment we relax this constraint by learning a common model
between the colored SVHN dataset and the binary MNIST dataset. While semantically similar to
humans, these datasets are vastly different, as one is based on RGB images of real world house
numbers and the other of synthetically hand-drawn digits. We visualize examples of the true test
inputs, x, and their respective reconstructions, ˆx, from the ﬁnal lifelong model in ﬁgure 13(a). Even
though the only true data the ﬁnal model received for training was the MNIST dataset, it is still able
to reconstruct the SVHN data observed previously. This demonstrates the ability of our architecture
to transition between complex distributions while still preserving the knowledge learned from the
previously observed distributions.

Finally, in ﬁgure 13(b) and 13(c) we illustrate the data generated from an interpolation of a 2-
dimensional continuous latent space, zc ∈ R2. To generate variates, we set the discrete categorical,
zd, to one of the possible values {[0, 1], [1, 0]} and linearly interpolate the continuous zc over the
range [−3, 3]. We then decode these to obtain the samples, ˆx ∼ Pθ(x|zd, zc). The model learns

18

a common continuous structure for the two distributions which can be followed by observing the
development in the generated samples from top left to bottom right on both ﬁgure 13(b) and 13(c).

6.6 VALIDATING EMPIRICAL SAMPLE COMPLEXITY USING CELEB-A

We iterate the Celeb-A dataset as described in the data ﬂow diagram (Figure 7) and use this learn-
ing task to explore qualitative and quantitative generations, as well as empirical real world time
complexity (as described in Section 4.4) on modern GPU hardware. We train a lifelong model and
a typical VAE baseline without catastrophic forgetting mitigation strategies and evaluate the ﬁnal
model’s generations in Figure 14. As visually demonstrated in Figure 14-Left, the lifelong model is
able to generate instances from all of the previous distributions, however the baseline model catas-
trophically forgets (Figure 14-Right) and only generates samples from the eye-glasses distribution.
This is also reinforced by the log-Frechet distance shown in Figure 15.

Figure 14: Left: Sequential generations for Celeb-A from the ﬁnal lifelong model for bald, male, young and
eye-glasses (left to right). Right: (random) generations by the ﬁnal baseline VAE model.

We also evaluate the wall-clock time in seconds (Table 4) for the lifelong model and the baseline-vae
for the 44,218 samples of the male distribution. We observe that the lifelong model does not add a
signiﬁcant overhead, especially since the baseline-vae undergoes catastrophic forgetting (Figure 14
Right) and completely fails to generate samples from previous distributions. Note that we present
the number of parameters and other detailed model information in our code and Appendix 10.3.

44,218 male samples
training-epoch (s)
testing-epoch (s)

baseline-VAE
43.1 +/- 0.6
9.79 +/- 0.12

Lifelong
56.63 +/- 0.28
16.09 +/- 0.01

Table 4: Mean & standard deviation wall-clock for one epoch of
male distribution of Celeb-A.

Figure 15: Celeb-A log-Frechet distance of lifelong vs. naive baseline VAE model without catastrophic miti-
gation strategies over the four distributions. Listed on the right is the time per epoch (in seconds) for an epoch
of the corresponding models.

7 ABLATION STUDIES

In this section we independently validate the beneﬁt of each of the newly introduced components
to the learning objective proposed in Section 4.3. In Experiment 7.1 we demonstrate the beneﬁt
of the discrete-continuous posterior factorization introduced in Section 4.2.1. Then in Experiment

19

7.2, we validate the necessity of the information restricting regularizer (Section 4.2.2) and posterior
consistency regularizer (Section 4.1.1).

7.1 LINEAR SEPARABILITY OF DISCRETE AND CONTINUOUS POSTERIOR

Figure 16: Left: Graphical model depicting classiﬁcation using pretrained VAE, coupled with a linear classiﬁer,
fθlin : z (cid:55)→ y. Right: Linear classiﬁer accuracy on the Fashion MNIST test set for a varying range of latent
dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024] and distributions.

In order to validate that the (independent) discrete and continuous latent variable posterior,
Qφ(zd, zc|x), aids in learning a better representation, we classify the encoded posterior sample
using a simple linear classiﬁer fθlin : z (cid:55)→ y, where y corresponds to the categorical class predic-
tion. Higher (linear) classiﬁcation accuracies demonstrate that the the VAE is able to learn a more
linearly separable representation. Since the latent representation of VAEs are typically used in aux-
iliary tasks, learning such a representation is useful in downstream tasks. This is a standard method
to measure posterior separability and is used in methods such as Associative Compression Networks
[41].

We use the standard training set of FashionMNIST [135] (60,000 samples) to train a standard VAE
with a discrete only (disc) posterior, an isotropic-gaussian only (gauss) posterior, a bernoulli only
(bern) posterior and ﬁnally the proposed independent discrete and continuous (disc+gauss) posterior
presented in Section 4.2.1. For each different posterior reparameterization, we train a set of VAEs
with varying latent dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024]. In the case of the disc+gauss
model we ﬁx the discrete dimension, |zd| = 10 and vary the isotropic-gaussian dimension to match
the total required dimension. After training each VAE, we proceed to use the same training data to
train a linear classiﬁer on the encoded posterior sample, z ∼ Qφ(z|x).

In Figure 16 we present the mean and standard deviation linear test classiﬁcation accuracies of each
set of the different experiments. As expected, the discrete only (disc) posterior performs poorly due
to the strong restriction of mapping an entire input sample to a single one-hot vector. The isotropic-
gaussian (gauss) and bernoulli (bern) only models provide a strong baseline, but the combination
of isotropic-gaussian and discrete posteriors (disc+gauss) performs much better, reaching an upper-
bound (linear) test-classiﬁcation accuracy of 87.1%. This validates that the decoupling of latent
represention presented in Section 4.2.1 aids in learning a more meaningful, separable posterior.

7.2 VALIDATING THE MUTUAL INFORMATION AND POSTERIOR CONSISTENCY

REGULARIZERS.

In order to independently evaluate the beneﬁt of our proposed Bayesian update regularizer (Sec-
tion 4.1.1) and the mutual information regularizer proposed in (Section 4.2.1) we perform an ab-
lation study using the MNIST data ﬂow sequence from Figure 7. We evaluate three scenarios: 1)
with posterior consistency and mutual information regularizers, 2) only posterior consistency and
3) without both regularizers. We observe that both components are necessary in order to gener-
ate high quality samples as evidenced by the negative test ELBO in Figure 17-(a) and the corre-

20

Figure 17: MNIST Ablation: (a) negative test ELBO. (b) Sequentially generated samples by setting zd and
sampling zc ∼ N (0, 1) (Section 4.2.1) with consistency + mutual information (MI). (c) Sequentially generated
samples with no consistency + no mutual information (MI).

sponding generations in Figure 17-(b-c). The generations produced without the information gain
regularizer and consistency in Figure 17-(c) are blurry. We attribute this to: 1) uniformly sam-
pling the discrete component is not guaranteed to generate samples representative samples from
P<i(x) and 2) the decoder, PΘ(x|zd, zc), relays more information through the continuous compo-
nent, PΘ(x|zd, zc) = PΘ(x|zc), causing catastrophic forgetting and posterior collapse [4].

8 LIMITATIONS

While LGM presents strong performance, it fails to completely solve the problem of lifelong gen-
erative modeling and we see a slow degradation in model performance over time. We attribute this
mainly to the problem of poor VAE generations that compound upon each other (also discussed
below). In addition, there are a few poignant issues that need to be resolved in order to achieve
an optimal (in terms of non-degrading Frechet distance / -ELBO) unsupervised generative lifelong
learner:

Distribution Boundary Evaluation: The standard assumption in current lifelong / continual learn-
ing approaches [91, 136, 112, 57, 65, 94] is to use known, ﬁxed distributions instead of learning
the distribution transition boundaries. For the purposes of this work, we focus on the accumulation
of distributions (in an unsupervised way), rather than introduce an additional level of indirection
through the incorporation of anomaly detection methods that aid in detecting distributional bound-
aries.

Blurry VAE Generations: VAEs are known to generate images that are blurry in contrast to GAN
based methods. This has been attributed to the fact that VAEs don’t learn the true posterior and
make a simplistic assumption regarding the reconstruction distribution Pθ(x|z) [4, 97]. While there
exist methods such as ALI [27] and BiGAN [26], that learn a posterior distribution within the GAN
framework, recent work has shown that adversarial methods fail to accurately match posterior-prior
distribution ratios in large dimensions [104].

Memory: In order to scale to a truly lifelong setting, we posit that a learning algorithm needs a
global pool of memory that can be decoupled from the learning algorithm itself. This decoupling
would also allow for a principled mechanism for parameter transfer between sequentially learnt
models as well a centralized location for compressing non-essential historical data. Recent work

21

such as the Kanerva Machine [133] and its extensions [134] provide a principled way to do this in
the VAE setting.

9 CONCLUSION

In this work we propose a novel method for learning generative models over a lifelong setting. The
principal assumption for the data is that they are generated by multiple distributions and presented
to the learner in a sequential manner. A key limitation for the learning process is that the method
has no access to any of the old data and that it shall distill all the necessary information into a
single ﬁnal model. The proposed method is based on a dual student-teacher architecture where the
teacher’s role is to preserve the past knowledge and aid the student in future learning. We argue for
and augment the standard VAE’s ELBO objective by terms helping the teacher-student knowledge
transfer. We demonstrate the beneﬁts this augmented objective brings to the lifelong learning setting
using a series of experiments. The architecture, combined with the proposed regularizers, aid in
mitigating the effects of catastrophic interference by supporting the retention of previously learned
knowledge.

REFERENCES

[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. Life-
long disentangled representation learning with cross-domain latent homologies. In Advances
in Neural Information Processing Systems, pages 9895–9905, 2018.

[2] W.-K. Ahn and W. F. Brewer. Psychological studies of explanationbased learning. In Investi-

gating explanation-based learning, pages 295–316. Springer, 1993.

[3] W.-K. Ahn, R. J. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one ex-
ample: Psychological evidence for explanation-based learning. Technical report, Coordinated
Science Laboratory, University of Illinois at Urbana-Champaign, 1987.

[4] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken

elbo. In International Conference on Machine Learning, pages 159–168, 2018.

[5] J. R. Anderson and G. H. Bower. Human associative memory. Psychology press, 2014.
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align

and translate. ICLR, 2015.

[7] H. R. Blackwell. Contrast thresholds of the human eye. JOSA, 36(11):624–643, 1946.
[8] Y. Blau and T. Michaeli. The perception-distortion tradeoff.

In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018.

[9] A. Blum. On-line algorithms in machine learning. In Online algorithms, pages 306–325.

Springer, 1998.

[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning, pages 1613–1622, 2015.
[11] L. Bottou. Online learning and stochastic approximations. On-line learning in neural net-

works, 17(9):142, 1998.

[12] L. Bottou and Y. L. Cun. Large scale online learning. In Advances in neural information

processing systems, pages 217–224, 2004.

[13] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.

[14] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational
bayes. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 1727–1735, 2013.

[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. ICLR, 2016.
[16] M. F. Carr, S. P. Jadhav, and L. M. Frank. Hippocampal replay in the awake state: a potential
substrate for memory consolidation and retrieval. Nature neuroscience, 14(2):147, 2011.

[17] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[18] X. Chen, X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.

22

In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc., 2016.
[19] Z. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and

big data. In International Conference on Machine Learning, pages 703–711, 2014.

[20] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning, 10(3):1–145, 2016.

[21] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent
variable model for sequential data. In Advances in neural information processing systems,
pages 2980–2988, 2015.

[22] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International Conference on Machine Learning, pages 1282–1289,
2019.

[23] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson. Human photoreceptor

topography. Journal of comparative neurology, 292(4):497–523, 1990.

[24] P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and

related ﬁelds, 2(4):555–581, 1996.

[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

[26] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782, 2016.

[27] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and

A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
[28] E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances

in Neural Information Processing Systems, pages 708–718, 2018.

[29] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace propagation. In Advances in Neural

Information Processing Systems, pages 441–448, 2004.

[30] L. Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories.
In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141.
IEEE, 2003.

[31] G. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable.

In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1565–1574. ACM, 2016.

[32] A. Fiat and G. J. Woeginger. Online algorithms: The state of the art, volume 1442. Springer,

1998.

[33] T. Furlanello, J. Zhao, A. M. Saxe, L. Itti, and B. S. Tjan. Active long term memory networks.

arXiv preprint arXiv:1606.02355, 2016.

[34] A. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities.

Journal of the American statistical association, 85(410):398–409, 1990.

[35] S. Gershman and N. Goodman. Amortized inference in probabilistic reasoning. In Proceed-

ings of the Cognitive Science Society, volume 36, 2014.

[36] Z. Ghahramani and H. Attias. Online variational bayesian learning.

In Slides from talk

presented at NIPS workshop on Online Learning, 2000.

[37] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats, volume 9, pages 249–256, 2010.

[38] R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric bayesian mix-
ture models. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2008.

[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.

[40] A. G. A. P. Goyal, A. Sordoni, M.-A. Cˆot´e, N. R. Ke, and Y. Bengio. Z-forcing: Training
stochastic recurrent networks. In Advances in neural information processing systems, pages
6713–6723, 2017.

[41] A. Graves, J. Menick, and A. v. d. Oord. Associative compression networks. arXiv preprint

arXiv:1804.02476, 2018.

23

[42] G. Grimmett, G. R. Grimmett, D. Stirzaker, et al. Probability and random processes. Oxford

university press, 2001.

[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6629–6640, 2017.

[45] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. stat,

[46] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1050:9, 2015.

1780, 1997.

[47] F. Huszar. Infogan: using the variational bound on mutual information (twice), Aug 2016.
[48] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reduc-

ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[49] V. Jain and E. Learned-Miller. Online domain adaptation of a pre-trained cascade of classi-
ﬁers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
577–584. IEEE, 2011.

[50] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Interna-

tional Conference on Learning Representations, 2017.

[51] H. Jeffreys. An invariant form for the prior probability in estimation problems. In Proceedings
of the Royal Society of London a: mathematical, physical and engineering sciences, volume
186, pages 453–461. The Royal Society, 1946.

[52] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper-
vised and generative approach to clustering. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, pages 1965–1972. AAAI Press, 2017.

[53] A. Johnson and A. D. Redish. Neural ensembles in ca3 transiently encode paths forward of

the animal at a decision point. Journal of Neuroscience, 27(45):12176–12189, 2007.

[54] M. I. Jordan. Artiﬁcial neural networks. chapter Attractor Dynamics and Parallelism in a
Connectionist Sequential Machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
[55] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[56] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering, 82(1):35–45, 1960.

[57] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learn-

ing. arXiv preprint arXiv:1710.10368, 2017.

[58] M. P. Karlsson and L. M. Frank. Awake replay of remote experiences in the hippocampus.

Nature neuroscience, 12(7):913, 2009.

[59] M. Karpinski and A. Macintyre. Polynomial bounds for vc dimension of sigmoidal and
general pfafﬁan neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.

[60] I. Katakis, G. Tsoumakas, and I. Vlahavas. Incremental clustering for the classiﬁcation of

[61] H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine

concept-drifting data streams.

Learning, pages 2654–2663, 2018.

[62] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. 2015.
[63] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
[64] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

[65] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–
1105, 2012.

[67] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

24

[68] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning, pages
1558–1566, 2016.

[69] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis. Continual classiﬁcation learning

using generative models. CoRR, abs/1810.10612, 2018.

[70] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks, 3361(10):1995, 1995.

[71] Z. Li and D. Hoiem. Learning without forgetting.

In European Conference on Computer

Vision, pages 614–629. Springer, 2016.

[72] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang,
In Proceedings of the European

and K. Murphy. Progressive neural architecture search.
Conference on Computer Vision (ECCV), pages 19–34, 2018.

[73] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder.

ICLR, 2016.

[74] C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[75] C. Louizos and M. Welling. Multiplicative normalizing ﬂows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 2218–2227. JMLR. org, 2017.

[76] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation

of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[77] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders.

arXiv preprint arXiv:1511.05644, 2015.

[78] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.
[79] J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling
on streams. In Advances in Neural Information Processing Systems, pages 1153–1161, 2015.
[80] A. Mishkin, F. Kunstner, D. Nielsen, M. Schmidt, and M. E. Khan. Slang: Fast structured
covariance approximations for bayesian deep learning with natural gradient. In Advances in
Neural Information Processing Systems, pages 6245–6255, 2018.

[81] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the ACM, 61(5):103–
115, 2018.

[82] T. M. Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research, 1980.

[83] T. M. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. D. Mishra,
M. Gardner, B. Kisiel, J. Krishnamurthy, et al. Never-ending learning. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[84] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in

machine learning. CoRR, abs/1906.10652, 2019.

[85] E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In International Confer-

ence on Learning Representations (ICLR), 2017.

[86] R. M. Neal. Bayesian Learning For Neural Networks. PhD thesis, University of Toronto,

[87] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte

1995.

carlo, 2(11):2, 2011.

[88] W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically exact, embarrassingly parallel
In N. L. Zhang and J. Tian, editors, Proceedings of the Thirtieth Conference on
MCMC.
Uncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27,
2014, pages 623–632. AUAI Press, 2014.

[89] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
In NIPS workshop on deep learning and

ural images with unsupervised feature learning.
unsupervised feature learning, page 5, 2011.

[90] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. nvcuong/variational-continual-learning,

2018.

[91] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. ICLR, 2018.

25

[92] K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, R. A. Olshen, and K. L. Oehler. Bayes risk
weighted vector quantization with posterior estimation for image compression and classiﬁca-
tion. IEEE Transactions on Image Processing, 5(2):347–360, 1996.

[93] F. A. Quintana and P. L. Iglesias. Bayesian clustering and product partition models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):557–574, 2003.
[94] N. C. Rabinowitz, G. Desjardins, A.-A. Rusu, K. Kavukcuoglu, R. T. Hadsell, R. Pascanu,
J. Kirkpatrick, and H. J. Soyer. Progressive neural networks, Nov. 23 2017. US Patent App.
15/396,319.

[95] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep con-

volutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[96] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are

unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

[97] T. Rainforth, A. Kosiorek, T. A. Le, C. Maddison, M. Igl, F. Wood, and Y. W. Teh. Tighter
variational bounds are not necessarily better. In International Conference on Machine Learn-
ing, pages 4277–4285, 2018.

[98] J. Ramapuram. Lifelongvae pytorch repository., 2017.
[99] A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with delta-

vaes. ICLR, 2019.

[100] A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with

VQ-VAE-2. CoRR, abs/1906.00446, 2019.

[101] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
In International Conference on Machine Learning,

inference in deep generative models.
pages 1278–1286, 2014.

[102] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,

[103] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

1997.

statistics, pages 400–407, 1951.

[104] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational

inference. arXiv preprint arXiv:1802.06847, 2018.

[105] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Comput.,

11(2):305–345, Feb. 1999.

[106] R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method.

Annals of Operations Research, 39(1):229–250, 1992.

[107] P. Ruvolo and E. Eaton. Ella: An efﬁcient lifelong learning algorithm.

In International

Conference on Machine Learning, pages 507–515, 2013.

[108] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models
via precision and recall. In Advances in Neural Information Processing Systems, pages 5228–
5237, 2018.

[109] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[110] N. W. Schuck and Y. Niv. Sequential replay of nonspatial task states in the human hippocam-

pus. Science, 364(6447):eaaw5181, 2019.

[111] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning, pages 4555–
4564, 2018.

[112] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In

Advances in Neural Information Processing Systems, pages 2994–3003, 2017.

[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484, 2016.

[114] D. L. Silver, G. Mason, and L. Eljabu. Consolidation using sweep task rehearsal: overcoming
In Canadian Conference on Artiﬁcial Intelligence, pages

the stability-plasticity problem.
307–322. Springer, 2015.

[115] D. L. Silver and R. E. Mercer. The parallel transfer of task knowledge using dynamic learning
rates based on a measure of relatedness. In Learning to learn, pages 213–233. Springer, 1996.
[116] D. L. Silver and R. E. Mercer. The task rehearsal method of life-long learning: Overcoming
In Conference of the Canadian Society for Computational Studies of

impoverished data.
Intelligence, pages 90–101. Springer, 2002.

26

[117] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algo-

rithms. In 2013 AAAI spring symposium series, 2013.

[118] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[119] W. E. Skaggs and B. L. McNaughton. Replay of neuronal ﬁring sequences in rat hippocampus

during sleep following spatial experience. Science, 271(5257):1870–1873, 1996.

[120] E. D. Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems

Sciences, 168:69–96, 1998.

[121] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[122] F. Tanaka and M. Yamamura. An approach to lifelong reinforcement learning through multi-
ple environments. In 6th European Workshop on Learning Robots, pages 93–99, 1997.
[123] A. V. Terekhov, G. Montone, and J. K. O’Regan. Knowledge transfer in deep block-modular
In Proceedings of the 4th International Conference on Biomimetic and

neural networks.
Biohybrid Systems-Volume 9222, pages 268–279. Springer-Verlag New York, Inc., 2015.
[124] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pages 1–10, 2016.
[125] S. Thrun. Lifelong learning: A case study. Technical report, CARNEGIE-MELLON UNIV

PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1995.

[126] S. Thrun and T. M. Mitchell. Lifelong robot learning.

In The biology and technology of

intelligent autonomous agents, pages 165–196. Springer, 1995.

[127] J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artiﬁcial

Intelligence and Statistics, pages 1214–1223, 2018.

[128] V. Vapnik. Estimation of dependences based on empirical data. Springer Science & Business

Media, 2006.

[129] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, R. Powell, et al. Alphastar: Mastering the real-time strategy game
starcraft ii. DeepMind Blog, 2019.

[130] S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic
model. In Proceedings of the 25th international conference on world wide web, pages 167–
176. International World Wide Web Conferences Steering Committee, 2016.

[131] R. C. Williamson and U. Helmke. Existence and uniqueness results for neural network ap-

proximations. IEEE Transactions on Neural Networks, 6(1):2–13, 1995.

[132] M. C. Wittrock. Generative learning processes of the brain. Educational Psychologist,

[133] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed

27(4):531–541, 1992.

memory. ICLR, 2018.

[134] Y. Wu, G. Wayne, K. Gregor, and T. Lillicrap. Learning attractor dynamics for generative

memory. In Advances in Neural Information Processing Systems, pages 9401–9410, 2018.

[135] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[136] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence.

In

International Conference on Machine Learning, pages 3987–3995, 2017.

27

10 APPENDIX

10.1 UNDERSTANDING THE CONSISTENCY REGULARIZER

The analytical derivations of the consistency regularizer show that the regularizer can be interpreted
as an a transformation of the standard VAE regularizer. In the case of an isotropic gaussian posterior,
the proposed regularizer scales the mean and variance of the student posterior by the variance of the
teacher 1 and adds an extra ’volume’ term. This interpretation of the consistency regularizer shows
that the proposed regularizer preserves the same learning objective as that of the standard VAE.
Below we present the analytical form of the consistency regularizer with categorical and isotropic
gaussian posteriors:

Proof 1 We assume the learnt posterior of the teacher is parameterized by a centered, isotropic
gaussian with Φ = [µE = 0, ΣE = diag(σE2
)] and the posterior of our student by a non-centered
isotropic gaussian with φ = [µS, ΣS = diag(σS2)], then

(cid:20)
tr(ΣE−1
KL(Qφ(z|x)||QΦ(z|x)) = 0.5

ΣS) + (µE − µS)T ΣE−1

(µE − µS) − F + log

= 0.5

F
(cid:88)

(cid:20)

j=1

1
σE2(j)

= KL(Qφ∗(z|x)||N (0, I)) − log |ΣE|

(σS2(j) + µS2(j)) − 1 + log σE2(j) − log σS2(j)

Via a reparameterization of the student’s parameters:

φ∗ = [µS∗, σS∗2]

µS∗ =

; σS∗2 =

µS(j)
σE2(j)

σS2(j)
σE2(j)

(cid:19)(cid:21)

(cid:18) |ΣE|
|ΣS|
(cid:21)

(21)

(22)

It is also interesting to note that our posterior regularizer becomes the prior if:

limσE2 (cid:55)→1KL(Qφ(z|x)||QΦ(z|x)) = KL(Qφ(z|x)||N (0, I))

Proof 2 We parameterize the learnt posterior of the teacher by Φi =

exp(pE
i )
i=1 exp(pE
i )

(cid:80)J

and the pos-

terior of the student by φi =
i ) and cS = (cid:80)J
cE = (cid:80)J
i=1 exp(pE
The KL divergence from the ELBO can now be re-written as:

exp(pS
i )
i=1 exp(pS
i )
i=1 exp(pS

(cid:80)J

. We also redeﬁne the normalizing constants as

i ) for the teacher and student models respectively.

KL(Qφ(zd|x)||QΦ(zd|x)) =

J
(cid:88)

i=1

exp(pS
i )
cS

log

(cid:18) exp(pS
i )
cS

(cid:19)

cE
exp(pE
i )

= H(pS, pS − pE) = −H(ps) + H(pS, pE)

(23)

where H( ) is the entropy operator and H( , ) is the cross-entropy operator.

28

10.2 RECONSTRUCTION REGULARIZER

Figure 18: Fashion Negative Test ELBO

Figure 19: Fashion Log-Frechet Distance

While it is possible to constrain the reconstruction/decoder term of the VAE in a similar manner to
the consistency posterior-regularizer, i.e: KL[Pθ(ˆx|z)||PΘ(ˆx|z)], doing so diminishes model per-
formance. We hypothesize that this is due to the fact that this regularizer contradicts the objective of
the reconstruction term Pθ(x|z) in the ELBO which already aims to minimize some metric between
the input samples x and the reconstructed samples ˆx; eg: if Pθ(x|z) ∼ N (µ, diag[σ]), then the
loss is proportional to ||ˆx − x||2
2, the standard L2 loss. Without the addition of this reconstruction
cross-model regularizer, the model is also provided with more ﬂexibility in how it reconstructs the
output samples.

In order to quantify the this we duplicate the FashionMNIST Experiment listed in the data ﬂow
deﬁnition in Figure 7. We use a simpler model than the main experiments to validate this hypothesis.
We train two dense models (-D): one with just the posterior consistency regularizer (without-LL-
D) and one with the consistency and likelihood regularizer (with-LL-D). We observe the model
performance drops (with respect to the Frechet distance as well the test ELBO) in the case of the
with-LL-D as demonstrated in Figures 18 and 19.

10.3 MODEL ARCHITECTURE

We used two different architectures for our experiments. When we use a dense network (-D) we
used two layers of 512 units to map to the latent representation and two layers of 512 to map back
to the reconstruction for the decoder. We used batch norm [48] and ELU activations for all the
layers barring the layer projecting into the latent representation and the output layer. Note that
while we used the same architecture for EWC we observed a drastic negative effect when using
batch norm and thus dropped it’s usage. The convolution architectures (-C) used the architecture
described below for the encoder and the decoder (where the decoder used conv-transpose layers for
upsampling). The notation is [OutputChannels, (ﬁlterX, ﬁlterY), stride]:

Encoder: [32, (5, 5), 1] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 1] (cid:55)→

GN+ELU (cid:55)→ [256, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [512, (1, 1), 1] (cid:55)→
GN+ELU (cid:55)→ [512, (1, 1), 1]

Decoder: [256, (4, 4), 1] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 1]

(24)

(cid:55)→ GN+ELU (cid:55)→ [32, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [32, (5, 5), 1]
(cid:55)→ GN+ELU (cid:55)→ [chans, (1, 1), 1]

29

Method
EWC-D
naive-D
batch-D
batch-D
lifelong-D
EWC-C
naive-C
batch-C
batch-C
lifelong-C

Initial zd dimension
10
10
10
10
1
10
10
10
10
1

Final zd dimension
10
10
10
10
10
10
10
10
10
10

zc dimension
14
14
14
14
14
14
14
14
14
14

# initial parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,165,311
30,767,428
7,691,280
7,691,280
15,382,560
15,235,072

# ﬁnal parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,179,661
30,767,428
7,691,280
7,691,280
15,382,560
15,382,560

The table above lists the number of parameters for each model and architecture used in our ex-
periments. The lifelong models initially start with a zd of dimension 1 and at each step we grow
the representation by one dimension to accommodate the new distribution (more info in Section
10.7). In contrast, the baseline EWC models are provided with the full representation throughout
the learning process. EWC has double the number of parameters because the computed diagonal
ﬁsher information matrix which is the same dimensionality as the number of parameters. EWC also
neeeds the preservation of the teacher model [Φ, Θ] to use in it’s quadratic regularizer. Both the
naive and batch models have the fewest number of parameters as they do not use a student-teacher
framework and only use one model, however the vanilla model has no protection against catastrophic
interference and the full model is just used as an upper bound for performance.

We used Adam [62] to optimize all of our problems with a learning rate of 1e-4 or 1e-3. When we
used weight transfer we re-initialized the accumulated momentum vector of Adam as well as the
aggregated mean and variance of the batch norm layers. The full architecture can be examined in
our github repository [98] and is provided under an MIT license.

10.4 CONTRAST TO STREAMING / ONLINE METHODS

Our method has similarities to streaming methods such as Streaming Variational Bayes (SVB) [14]
and Incremental Bayesian Clustering methods [60, 38] in that we estimate and reﬁne posteriors
through time. In general this can be done through the following Bayesian update rule that states that
the lastest posterior is proportional to the current likelihood times the previous posterior:

P (z|X1, ..., Xt) ∝ P (Xt|z)P (z|X1, ..., Xt−1)
SVB computes the intractable posterior, P (z|X1, ..., Xt), utilizing an approximation, At, that ac-
cepts as input the current dataset, Xt, along with the previous posterior At−1 :

(25)

P (z|X1, ..., Xt) ≈ At(Xt, At−1)

(26)

The ﬁrst posterior input (At=0) to the approximating function is the prior P (z). The objective of
SVB and other streaming methods is to model the posterior of the currently observed data in the
best possible manner. Our setting differs from this in that we want to retain information from all
previously observed distributions (sometimes called a knowledge store [126]). This can be useful
in scenarios where a distribution is seen once, but only used much later down the road. Rather than
creating a posterior update rule, we recompute the posterior via Equation 25, leveraging the fact that
we can re-generate X<t ≈ ˆX<t through the generative process. This allows us to recompute a more
appropriate posterior re-using all of the (generated) data, rather than using the previously computed
(approximate) posterior At−1:

P (z|X1, X2, ..., Xt) ∝ P (Xt|z)P (z| ˆX1, ..., ˆXt−1)

(27)

Coupling this generative replay strategy with the Bayesian update regularizer introduced in Section
4.1.1, we demonstrate that not only do we learn an updated poster as in Equation 27, but also allow
for a natural transfer of information between sequentially learnt models: a fundamental tenant of
lifelong learning [126, 125].

Finally, another key difference between lifelong learning and online methods is that lifelong learning
aims to learn from a sequence of tentatively different [20] tasks while still retaining and accumulating

30

knowledge; online learning generally assumes that the true underlying distribution comes from a
single distribution [11]. There are some exceptions to this where online learning is applied to the
problem of domain adaptation, eg: [49, 60].

10.5 EWC BASELINES: COMPARING CONV & DENSE NETWORKS

We compared a whole range of EWC baselines and use the best performing models few in our
experiments. Listed in Figure 10.5 are the full range of EWC baselines run on the PermutedMNIST
and FashionMNIST experiments. Recall that C / D describes whether a model is convolutional or
dense and the the number following is the hyperparameter for the EWC or Lifelong VAE.

10.6 GUMBEL REPARAMETERIZATION

Since we model our latent variable as a combination of a discrete and a continuous distribution we
also use the Gumbel-Softmax reparameterization [76, 50]. The Gumbel-Softmax reparameterization
over logits [linear output of the last layer in the encoder] p ∈ RM and an annealed temperature
parameter τ ∈ R is deﬁned as:

z = sof tmax(

); g = −log(−log(u ∼ U nif (0, 1)))

(28)

log(p) + g
τ

u ∈ RM , g ∈ RM . As the temperature parameter τ (cid:55)→ 0, z converges to a categorical.

10.7 EXPANDABLE MODEL CAPACITY AND REPRESENTATIONS

Multilayer neural networks with sigmoidal activations have a VC dimension bounded between
O(ρ2)[120] and O(ρ4)[59] where ρ are the number of parameters. A model that is able to con-

31

sistently add new information should also be able to expand its VC dimension by adding new pa-
rameters over time. Our formulation imposes no restrictions on the model architecture: i.e. new
layers can be added freely to the new student model.
In addition we also allow the dimensionality of zd ∈ RJ , our discrete latent representation to grow
in order to accommodate new distributions. This is possible because the KL divergence between
two categorical distributions of different sizes can be evaluated by simply zero padding the teacher’s
smaller discrete distribution. Since we also transfer weights between the teacher and the student
model, we need to handle the case of expanding latent representations appropriately. In the event
that we add a new distribution we copy all the weights besides the ones immediately surrounding
the projection into and out of the latent distribution. These surrounding weights are reinitialized to
their standard Glorot initializations [37].

32

9
1
0
2
 
v
o
N
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
7
4
8
9
0
.
5
0
7
1
:
v
i
X
r
a

LIFELONG GENERATIVE MODELING

Jason Ramapuram ∗ †
Jason.Ramapuram@etu.unige.ch

Magda Gregorova ∗ †
magda.gregorova@hesge.ch

Alexandros Kalousis ∗ †
Alexandros.Kalousis@hesge.ch

ABSTRACT

Lifelong learning is the problem of learning multiple consecutive tasks in a se-
quential manner, where knowledge gained from previous tasks is retained and
used to aid future learning over the lifetime of the learner. It is essential towards
the development of intelligent machines that can adapt to their surroundings. In
this work we focus on a lifelong learning approach to unsupervised generative
modeling, where we continuously incorporate newly observed distributions into a
learned model. We do so through a student-teacher Variational Autoencoder ar-
chitecture which allows us to learn and preserve all the distributions seen so far,
without the need to retain the past data nor the past models. Through the intro-
duction of a novel cross-model regularizer, inspired by a Bayesian update rule,
the student model leverages the information learned by the teacher, which acts
as a probabilistic knowledge store. The regularizer reduces the effect of catas-
trophic interference that appears when we learn over sequences of distributions.
We validate our model’s performance on sequential variants of MNIST, Fashion-
MNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model
mitigates the effects of catastrophic interference faced by neural networks in se-
quential learning scenarios.

1

INTRODUCTION

Machine learning is the process of approximating unknown functions through the observation of typ-
ically noisy data samples. Supervised learning approximates these functions by learning a mapping
from inputs to a predeﬁned set of outputs such as categorical class labels (classiﬁcation) or contin-
uous targets (regression). Unsupervised learning seeks to uncover structure and patterns from the
input data without any supervision. Examples of this learning paradigm include density estimation
and clustering methods. Both learning paradigms make assumptions that restrict the set of plausible
solutions. These assumptions are referred to as hypothesis spaces, biases or priors and aid the model
in favoring one solution over another [82, 128]. For example, the use of convolutions [70] to process
images favors local structure; recurrent models [54, 46] exploit sequential dependencies and graph
neural networks [109, 64] assume that the underlying data can be modeled accurately as a graph.

Current state of the art machine-learning models typically focus on learning a single model for a
single task, such as image classiﬁcation [72, 121, 43, 118, 66], image generation [100, 13, 63, 39],
natural language question answering [25, 96] or single game playing [129, 113]. In contrast, humans
experience a sequence of learning tasks over their lifetimes, and are able to leverage previous learn-
ing experiences to rapidly learn new tasks. Consider learning how to ride a motorbike after learning
to ride a bicycle: the task is drastically simpliﬁed through the use of prior learning experience. Stud-
ies [2, 3, 67] in psychology have shown that humans are able to generalize to new concepts in a rapid
manner, given only a handful of samples. [67] demonstrates that humans can classify and generate
new concepts of two wheel vehicles given just a single related sample. This contrasts the state of the
art machine learning models described above which use hundreds of thousands of samples and fail
to generalize to slight variations of the original task [22].

∗University of Geneva, Switzerland
†Haute cole de gestion de Genve, HES-SO, Switzerland

1

Lifelong learning [126, 125] argues for the need to consider learning over task sequences, where
learned task representations and models are stored over the entire lifetime of the learner and can be
used to aid current and future learning. This form of learning allows for the transfer of previously
learned models and representations and can reduce the sample complexity of the current learning
problem [126]. In this work we restrict ourselves to a subset of the broad lifelong learning paradigm;
rather than focus on the supervised lifelong learning scenario as most state of the art methods, our
work is one of the ﬁrst to tackle the more challenging problem of deep lifelong unsupervised learn-
ing. We also identity and relax crucial limitations of prior work in life-long learning that requires
the storage of previous models and training data, allowing us to operate in a more realistic learning
scenario.

2 RELATED WORK
The idea of learning in a continual manner has been explored extensively in machine learning,
seeded by the seminal works of lifelong-learning [126, 125, 117], online-learning [32, 9, 11, 12]
and sequential linear gaussian models [105, 36] such as the Kalman Filter [56] and its non-linear
counterpart, the Particle Filter [24]. Lifelong learning bears some similarities to online learning
in that both learning paradigms observe data in a sequential manner. Online learning differs from
lifelong learning in that the central objective of a typical online learner [11, 12] is to best solve/ﬁt the
current learning problem, without preserving previous learning. In contrast, lifelong learners seek to
retain, and reuse, the learned behavior acquired over past tasks, and aim to maximize performance
across all tasks. Consider the example of forecasting click through rate: the objective of the online
learner is to evolve over time, such that it best represents current user preferences. This contrasts
lifelong learners which enforce a constraint between tasks to ensure that previous learning is not
lost.

Lifelong Learning [126] was initially proposed in a supervised learning framework for concept
learning, where each task seeks to learn a particular concept/class using binary classiﬁcation. The
original framework used a task speciﬁc model, such as a K Nearest Neighbors (KNN) 1, coupled with
a representation learning network that used training data from all past learning tasks (support sets),
to learn a common, global representation. This supervised approach was later improved through the
use of dynamic learning rates [115], core-sets [114] and multi-head classiﬁers [31].
In parallel,
lifelong learning was extended to independent multi-task learning [107, 31], reinforcement learning
[125, 122, 102], topic modeling [19, 130] and semi-supervised language learning [83, 81]. For a
more detailed review see [20].

More recently, lifelong learning has seen a resurgence within the framework of deep learning. As
mentioned earlier, one of the central tenets of lifelong learning is that that the learner should perform
well over all observed tasks. Neural networks, and more generally, models that learn using stochastic
gradient descent [103], typically cannot persist past task learning without directly preserving past
models or data. This problem of catastrophic forgetting [78] is well known in the neural network
community and is the central obstacle that needs to be resolved to build an effective neural lifelong
learner. Catastrophic forgetting is the phenomenon where model parameters of a neural network
trained in a sequential manner become biased towards the distribution of the latest observations,
forgetting previously learned representations, over data no longer accessible for training. In order to
mitigate catastrophic forgetting current research in lifelong learning employs four major strategies:
transfer learning, replay mechanisms, parameter regularization and distribution regularization. In
table 1 we classify the different lifelong learning methods that we will discuss in the following
paragraphs into these strategies.

EWC [65] VCL [91] LwF [71] ALTM [33]
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

PNN [94] DGR [112, 57] DBMNN [123]
(cid:88)
(cid:55)

(cid:88)
(cid:55)

(cid:55)
(cid:88)

(cid:88)

Transfer learning
Replay mechanisms
Parameter
regularization
Functional
regularization
Table 1: Catastropic interference mitigation strategies of state of the art models. Rows highlighted in gray
represent desirable mitigation strategies.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

SI [136] VASE [1] LGM (us)
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:88)

1These models were known as memory based learning in [126].

2

Transfer learning: These approaches mitigate catastrophic forgetting by freezing previous task
models and relaying a latent representation of the previous task to the current model. Research in
transfer learning for the mitigation of catastrophic forgetting include Progressive Neural Networks
(PNN) [94] and Deep Block-Modular Neural Networks (DBMNN) [123] to name a few. These
approaches allow the current model to adapt its parameters to the (new) joint representation in an
efﬁcient manner and prevent forgetting through the direct preservation of all previous task mod-
els. Deploying such a transfer learning mechanism in a lifelong learning setting would necessitate
training a new model with every new task, considerably increasing the memory footprint of the life-
long learner. In addition, since transfer learning approaches freeze previous models, it negates the
possibility of improving previous task performance using knowledge gathered from new tasks.

Replay mechanisms: The original formulation of lifelong learning [126] required the preservation
of all previous task data. This requirement was later relaxed in the form of core-sets [116, 114, 91],
which represent small weighted subsets of inputs that approximate the full dataset. Recently, within
the classiﬁcation setting, there have been replay approaches that try to lift the requirement of storing
past training data by relying on generative modeling [112, 57]; we will call such methods Deep gen-
erative replay (DGR) methods. DGR methods methods use a student-teacher network architecture,
where the teacher (generative) model augments the student (classiﬁer) model with synthetic samples
from previous tasks. These synthetic task samples are used in conjunction with real samples from
the current task to learn a new joint model across all tasks. While strongly motivated by biologi-
cal rehearsal processes [119, 53, 58, 110], these generative replay strategies fail to efﬁciently use
previous learning and simply re-learn each new joint task from scratch.

Parameter regularization: Most work that mitigates catastrophic forgetting falls under the um-
brella of parameter regularization. There are two approaches within this mitigation strategy: con-
straining the parameters of the new task to be close to the previous task through a predeﬁned metric,
and enforcing task-speciﬁc parameter sparsity. The two approaches are related as task-speciﬁc pa-
rameter sparsity can be perceived as a reﬁnement of the parameter constraining approach. Parameter
constraining approaches typically share the same model/parameters, but encourage new tasks from
altering important learned parameters from previous tasks. Task speciﬁc parameter sparsity relaxes
this, by enforcing that each task use a different subset of parameters from a global model, through
the use of an attention mechanism.

Models such as Laplace Propagation [29], Elastic Weight Consolidation (EWC) [65], Synaptic In-
telligence (SI) [136] and Variational Continual Learning (VCL) [91] fall under the parameter con-
straining approach. EWC for example, uses the Fisher Information matrix (FIM) to control the
change of model parameters between two learning tasks. Intuitively, important parameters should
not have their values changed, while non-important parameters are left unconstrained. The FIM is
used as a weighting in a quadratic parameter difference regularizer under a Gaussianity assump-
tion of the parameter posterior. However, this Gaussian parameter posterior assumption has been
demonstrated [86, 10] to be sub-optimal for learned neural network parameters. VCL improves
upon EWC, by generalizing the local assumption of the FIM to a KL-Divergence between the (vari-
ational) parameter posterior and prior. This generalization derives from the fact that the FIM can
be cast as a KL divergence between the posterior and an epsilon perturbation of the same random
variable [51]. VCL actually spans a number of different mitigation strategies as it uses parameter
regularization (described above) , transfer learning (it keeps a separate head network per task) and
replay (it persists a core-set of true data per task).

Models such as Hard Attention to the Task (HAT) [111] and the Variational Autoencoder with Shared
Embeddings (VASE) [1] fall under the task-speciﬁc parameter sparsity strategy. This mitigation
strategy enforces that different tasks use different components of a single model, typically through
the use of attention vectors [6] that are learned given supervised task labels. Multiplying the attention
vectors with the model outputs prevents gradient descent updates for different subsets of the model’s
parameters, allowing them to be used for future task learning. Task speciﬁc parameter sparsity allows
a model to hold-out a subset of its parameters for future learning and typically works well in practice
[111], with its strongest disadvantage being the requirement of supervised information.

Functional regularization: Parameter regularization methods attempt to preserve the learned be-
havior of the past models by controlling how the model parameters change between tasks. However,
the model parameters are only a proxy for the way a model actually behaves. Models with very
different parameters can have exactly the same behavior with respect to input-output relations (non-

3

uniqueness [131]). Functional regularization concerns itself with preserving the actual object of
interest:
the input-output relations. This strategy allows the model to ﬂexibly adapt its internal
parameter representation between tasks, while still preserving past learning.

Methods such as distillation [45], ALTM [33] and Learning Without Forgetting (LwF) [71] impose
similarity constraints on the classiﬁcation outputs of models learned over different tasks. This can
be interpreted as functional regularization by generalizing the constraining metric (or semi-metric)
to be a divergence on the output conditional distribution. In contrast to parameter regularization, no
assumptions are made on the parametric form of the parameter posterior distribution. This allows
models to ﬂexibly adapt their internal representation as needed, making functional regularization a
desirable mitigation strategy. One of the pitfalls of current functional regularization approaches is
that they necessitate the preservation of all previously data.

2.1 LIMITATIONS OF EXISTING APPROACHES.

A simple solution to the problem of lifelong learning is to store all data and re-learn a new joint
multi-task representation [17] at each newly observed task. Alternatively, it is possible to retain
all previous model parameters and select the model that presents the best performance on new test
task data. Existing solutions typically relax one these requirements. [33, 71, 91] relaxes the need
for model persistence, but requires preservation of all data [33, 71], or a growing core-set of data
[116, 114, 91]. Conversely, [94, 123, 91, 136] relaxes the need to store data, but persists all previous
models [94, 123] or a subset of model parameters [91, 136].

Unlike these approaches, we draw inspiration from how humans learn over time and remove the
requirement of storing past training data and models. Consider the human visual system; research
has shown [23, 7] that the human eye is capable of capturing 576 megapixels of content per image
frame. If stored naively on a traditional computer, this corresponds to approximately 6.9 gigabytes of
information per sample. Given that we perceive trillions of frames over our lifetimes, it is infeasible
to store this information in its base, uncompressed representation. Research in neuroscience has
validated [132, 5] that the associative human mind, compresses, merges and reconstructs information
content in a dynamic way. Motivated by this, we believe that a lifelong learner should not store past
training data or models. Instead, it should retain a latent representation that is common over all tasks
and evolve it as more tasks are observed.

2.2 OUR SOLUTION AT A HIGH LEVEL.

While most research in lifelong learning focuses on supervised learning [94, 33, 71, 123, 136, 65],
we focus on the more challenging task of deep unsupervised latent variable generative modeling.
These models have wide ranging applications such as clustering [77, 52, 85] and pre-training [68,
95].

Central to our lifelong learning method are a pair of generative models, aptly named the teacher and
student, which we train by exploiting the replay and functional regularization strategies described
above. After training a single generative model over the ﬁrst task, we use it is used as the teacher for
a newly instantiated student model. The student model receives data from the current task, as well as
replayed data from the teacher, which acts as a probabilistic storage container of past tasks. In order
to preserve previous learning, we make use of functional regularization, which aids in preserving
input-output relations over past tasks.

Unlike EWC or VCL, we make no assumptions on the form of the parameter posterior and allow
the generative models to use available parameters as appropriate, to best accommodate current and
past learning. The use of generative replay, coupled with functional regularization, renders the
preservation of the past models and past data unnecessary. It also signiﬁcantly improves the sample
complexity on future task learning, which we empirically demonstrate in our experiments. Finally
we should note that it is straightforward to adapt our approach to the supervised learning setting, as
we have done in [69].

3 BACKGROUND

In this section we describe the main concepts that we use throughout this work. We begin by de-
scribing the base-level generative modeling approach in Section 3.1, followed by how it extends to

4

the lifelong setting in Section 3.2. Finally, in Section 3.3, we describe the Variational Autoencoder
over which we instantiate our lifelong generative model.

3.1 LATENT VARIABLE GENERATIVE MODELING

We consider a scenario where we observe a dataset, D, consisting of N variates, D = {xj}N
j=1,
of a continuous or discrete variable x. We assume that the data is generated by a random process
involving a non-observed random variable, z. The data generation process involves ﬁrst sampling
zj ∼ P (z) and then producing a variate from the conditional, xj ∼ Pθ(x|z). We visualize this
form of latent generative model in the graphical model in Figure 14.

Figure 1: Typical latent variable graphical model. Gray nodes represent observed variables while white nodes
represent unobserved variables.

Typically, latent variables models are solved through maximum likelihood estimation which can be
formalized as:

max
θ

log Pθ(x) = max

log

Pθ(x|z)P (z)dz = max

log Ez[Pθ(x|z)]

(1)

θ

(cid:90)

θ

In many cases, the expectation from Equation 1 does not have a closed form solution (eg: non-
conjugate distributions) and quadrature is not computationally tractable due to large dimensional
spaces [63, 101] (eg: images). To overcome these intractabilities we use a Variational Autoencoder
(VAE), which we summarize in Section 3.3. The VAE allows us to infer our latent variables and
jointly estimate the parameters of our model. However, before describing the VAE, it is important
to understand how this generative setting can be perceived in a lifelong learning scenario.

3.2 LIFELONG GENERATIVE MODELING

Lifelong generative modeling extends the single-distribution estimation task from Section 3.1 to a
set of i = {1...L} sequentially observed learning tasks. The i-th learning task has variates that are
realized from the task speciﬁc conditional, Pi(x) = P (x|zd = i), where zd acts as a categorical
indicator variable of the current task. We visualize a simpliﬁed form of this in Figure 2 below.

Figure 2: Simpliﬁed lifetime of a lifelong learner. Given a true (unknown) distribution, P (x) =
(cid:82) P (x|zd)P (zd)δzd, we observe partial information in the form of L sequential tasks, {D1 (cid:55)→ D2, ... (cid:55)→
DL}. Observing more tasks, reduces the uncertainty of the model until convergence, PθL (x) ≈ P (x).

Crucially, when observing task, Di, the model has no access to any of the previous task datasets,
D<i. As the lifelong learner observes more tasks, {D1 (cid:55)→ D2 (cid:55)→ ... (cid:55)→ DL}, it should improve its
estimate of the true distribution, Pθ(x) ≈ P (x) = (cid:82) P (x|zd)P (zd)δzd, which is unknown at the
start of training.

5

3.3 THE VARIATIONAL AUTOENCODER

As eluded to in Section 3.1, we would like to infer the latent variables from the data. This
can be realized as an alternative form of Equation 1 in the form of Bayes rule: Pφ(z|x) =
Pθ(x|z)P (z)/Pθ(x), where Pφ(z|x) is referred to as the latent variable posterior and Pθ(x|z)
as the likelihood. One method of approximating the posterior, Pφ(z|x), is through MCMC sam-
pling methods such as Gibbs sampling [34] or Hamiltonian MCMC [87]. MCMC methods have
the advantage that they provide asymptotic guarantees [88] of convergence to the true posterior,
Pφ(z|x). However in practice it is not possible to know when convergence has been achieved. In
addition, due to their Markovian nature, they possess an inner loop, which makes it challenging to
scale for large scale datasets.

In contrast, Variational Inference (VI) [55] side-steps the intractability of the posterior by approxi-
mating it with a tractable distribution family, Qφ(z|x). VI rephrases the objective of determining the
posterior as an optimization problem by minimizing the KL divergence between the known distri-
butional family, Qφ(z|x), and the unknown true posterior, Pθ(z|x). Applying VI to the intractable
integral from Equation 1 results in the evidence lower bound (ELBO) or variational free energy,
which can easily be derived from ﬁrst principles:

log Pθ(x) = log

Pθ(x|z)P (z)dz

(cid:90)

= log

(cid:90) Qφ(z|x)
Qφ(z|x)

Pθ(x|z)P (z)dz

≥ EQ[log Pθ(x|z)] − DKL[Qφ(z|x)||P (z)]
(cid:123)(cid:122)
(cid:125)
ELBO

(cid:124)

(2)

(3)

(4)

where we used Jensen’s inequality to transition from Equation 3 to Equation 4. The objective intro-
duced in Equation 4 induces the graphical model shown below in Figure 3.

Figure 3: Standard VAE graphical model. Gray nodes represent observed variables while white nodes represent
unobserved variables; dashed lines represent inferred variables.

VAEs typically use deep neural networks to model the approximate inference network, Qφ(z|x)
and conditional, Pθ(x|z), which are also known as the encoder and decoder networks (respec-
tively). To optimize for the parameters of these networks, VAEs maximize the ELBO (Equation
4) using Stochastic Gradient Descent [103]. By sharing the variational parameters of the encoder,
φ, across the data points (amortized inference [35]), variational autoencoders avoid per-data inner
loops typically needed by MCMC approaches.

Optimizing the ELBO in Equation 4 requires computing the gradient of an expectation over the ap-
proximate posterior, Qφ(z|x). This typically takes place through the use of the path-wise estimator
[101, 63] (originally called “push-out” [106]). The path-wise reparameterizer uses the Law of the
Unconscious Statistician (LOTUS) [42], which enables us to compute the expectation of a function
of a random variable (without knowing its distribution) if we know its corresponding sampling path
and base distribution [84]. For the typical isotropic gaussian approximate posterior, Qφ(z|x), used
in standard VAEs this can be aptly summarized by:

z ∼ Qφ(z|x) ⇔ µφ(x) + σφ(x)(cid:15), (cid:15) ∼ N (0, 1)

∇φEQφ(z|x)[log Pθ(x|z)] ⇔ E
N ((cid:15)|0, 1)
(cid:125)
(cid:123)(cid:122)
(cid:124)

base distribution

)]
[∇φ log Pθ(x| µφ(x) + σφ(x)(cid:15)
(cid:125)
(cid:124)

(cid:123)(cid:122)
sampling path

(5)
(6)

6

where Equation 5 deﬁnes the sampling procedure of our latent variable through the location-scale
transformation and Equation 6 deﬁnes the path-wise Monte Carlo gradient estimator applied on the
decoder (ﬁrst term in Equation 4). This Monte Carlo estimator enables differentiating through the
sampling process of the distribution Qφ(z|x). Note that computing the gradient of the second term
in Equation 4, ∇φDKL[Qφ(z|x)||P (z)], is possible through a closed form analytical solution for
the case of isotropic gaussian distributions.

While it is possible to extend any latent variable generative model to the lifelong setting, we choose
to build our lifelong generative models using variational autoencoders (VAEs) [63] as they provide
a mechanism for stable training; this contrasts other state of the art unsupervised models such as
Generative Adversarial Networks (GANs) [39, 61]. Furthermore, latent-variable posterior approxi-
mations are a requirement in many learning scenarios such as clustering [93], compression [92] and
unsupervised representation learning [30]. Finally, GANs can suffer from low sample diversity [28]
which can lead to compounding errors in a lifelong generative setting.

4 LIFELONG LEARNING MODEL

Algorithm 1 Data Flow

Teacher:
Sample Prior: zj ∼ P (z)
Decode: ˆxj ∼ PΘ(x|z)

Student:
Sample : xj ∼ P (ω)P (x|ω)
Encode : zj ∼ Qφ(z|x)
Decode: ˆxj ∼ Pθ(x|z)

Figure 4: Student training procedure. Left: graphical model for student-teacher model. Data generated from
the teacher model (top row) is used to augment the current training data observed by the student model (bottom
row). A posterior regularizer is also applied between Qφ(z|x) and QΦ(z|x) to enable functional regularization
(not shown, but discussed in detail in Section 4.1.1). Right: data ﬂow algorithm.

fMRI studies of the rodent [119, 53, 58] and human [110] brains have shown that previously experi-
enced sequences of events are replayed in the hippocampus during rest. These replays are necessary
for better planning [53] and memory consolidation [16]. We take inspiration from the memory con-
solidation of biological learners and introduce our model of Lifelong Generative Modeling (LGM).
We visualize the LGM student-teacher architecture in Figure 4.

The student and the teacher are both instantiations of the same base-level generative model, but
have different roles throughout the learning process. The teacher’s role is to act as a probabilistic
knowledge store of previously learned distributions, which it transfers to the student in the form
of replay and functional regularization. The student’s role is to learn the distribution over the new
task, while accommodating the learned representation of the teacher over old tasks. In the following
sections we provide detailed descriptions of the student-teacher architecture, as well as the base-
level generative model that each of them use. The base-level model uses a variant of VAEs, which
we tailor for lifelong learning and is learned by maximizing a variant of the standard VAE ELBO
from Equation 4 ; we describe this objective at end of this section.

4.1 STUDENT-TEACHER ARCHITECTURE

The top row of Figure 4 represents the teacher model. At any given time, the teacher contains a
summary of all previous distributions within the learned parameters, Φ, of the encoder QΦ(z|x),
and the learned parameters, Θ, of the decoder PΘ(x|z). We use the teacher to generate synthetic
variates, ˆxj, from these past distributions by decoding variates from the prior, zj ∼ P (z) (cid:55)→
PΘ(x|z = zj). We pass the generated (synthetic) variates, ˆxj, to the student model as a form of
knowledge transfer about the past distributions. Information transfer in this manner is known as
generative replay and our work is the ﬁrst to explore it in a VAE setting.

7

The bottom row of Figure 4 represents the student. The student is responsible for updating the
parameters, φ, of its encoder, Qφ(z|x), and θ, of its decoder Pθ(x|z). Importantly, the student re-
ceives data from both the currently observed task, as well as synthetic data generated by the teacher.
This can be formalized as xj ∼ P (ω)P (x|ω), ω ∼ Ber(π), as shown in Equation 7:

P (ω)P (x|ω) =

(cid:26)PΘ(x|z) ω = 0
ω = 1

Pi(x)

(7)

The mean, π, of the Bernoulli distribution, controls the sampling proportion of the previously learned
distributions to the current one and is set based on the number of assimilated distributions. Thus,
given i observed distributions: π = 1
i+1 . This ensures that the samples observed by the student
are representative of both the current and past distributions. Note that this does not correspond
to varying sample sizes in datasets, but merely our assumption to model each distribution with
equivalent weighting.

Once a new task is observed, the old teacher is dropped, the student model is frozen and becomes the
new teacher (φ → Φ, θ → Θ). A new student is then instantiated with the latest weights φ and θ
from the previous student (the new teacher). Due to the cyclic nature of this process, no new models
are added. This contrasts many existing state of the art deep lifelong learning methods which add an
entire new model or head-network per task (eg: [91, 94, 123]).

A crucial aspect in the lifelong learning process is to ensure that previous learning is successfully
exploited to bias current learning [126]. While the replay mechanism that we put in place ensures
that the student will observe data from all tasks, it does not ensure that previous knowledge from
the teacher is efﬁciently exploited to improve current student learning. The student model will re-
learn (from scratch) a completely new representation, which might be different than the teacher. In
order to successfully transfer knowledge between both VAE models, we rely on functional regular-
ization, which we enforce through a Bayesian update regularizer of the posteriors of both models.
Intuitively, we would like the student model’s latent outputs, zj ∼ Qφ(z|x) to be similar to la-
tent outputs of teacher model, zj ∼ QΦ(z|x), over synthetic variates generated by the teacher,
xj ∼ P (ω)P (x|ω = 0) = PΘ(x|z). In the following section, we describe the exact functional
form of this regularizer and demonstrate how it can be perceived as a natural extension of the VAE
learning objective to a sequential setting.

4.1.1 KNOWLEDGE TRANSFER VIA BAYESIAN UPDATE.

While both the student and teacher are instantiations of VAE variants, tailored for the particularities
of the lifelong setting, for the purpose of this exposition we use the standard VAE formulation. Our
objective is to learn the set of parameters [φ, θ] of the student, such that it can generate variates
from the complete distribution, P (x), described in Section 3.2. Subsuming the deﬁnition of the
augmented input data, x ∼ P (ω)P (x|ω), from Equation 7, we can deﬁne the student ELBO as:

Lθ,φ(x) = EQφ(z|x)

(cid:20)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)],

(8)

x ∼ P (ω)P (x|ω), ω ∼ Ber(π).

Rather than naively shrinking the full posterior to the prior via the KL divergence in Equation 8, we
rely on one of the core tenets of the Bayesian paradigm which states that we can always update our
posterior when given new information (yesterdays posterior is todays prior) [79]. Given this tenet,
we introduce our posterior regularizer 2:

KL[Qφ(z|x)||QΦ(z|x)], x ∼ P (x|ω = 0)
(9)
which distills the teacher’s learnt representation into the student over the generated data only. Com-
bining Equations 8 and 9, yields the objective that we can use to train the student and is described
below in Equation 10:

Lθ,φ(x) = EQφ(z|x)

log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(10)

(cid:20)

(cid:21)

2While it

is also possible to apply a similar

regularizer

to the reconstruction term,

i.e:

KL[Pθ(x|z) || PΘ(x|z)], we observed that doing so hurts performance (Appendix 10.2).

+ (1 − ω)KL[Qφ(z|x)||QΦ(z|x)],
x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

8

Note that this is not the ﬁnal objective, due to the fact that we have yet to present the VAE variant
tailored to the particularities of the lifelong setting. We will now show how the posterior regularizer
can be perceived as a natural extension of the VAE learning objective, through the lens of a Bayesian
update of the student posterior.

Lemma 1 For random variables x and z with conditionals QΦ(z|x) and Qφ(z|x), both distributed
as a categorical or gaussian and parameterized by Φ and φ respectively, the KL divergence between
the distributions is:

KL[Qφ(z|x)||QΦ(z|x)] = KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(11)

where ˆφ = f (φ, Φ) depends on the parametric form of Q, and C is only a function of the parameters,
Φ.

We prove Lemma 1 for the relevant distributions (under some mild assumptions) in Appendix 10.1.
Using Lemma 1 allows us to rewrite Equation 10 as shown below in Equation 12:
(cid:20)

Lθ,φ(x) = EQφ(z|x)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(12)

(cid:20)

+ (1 − ω)

KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(cid:21)
,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

This rewrite makes it easy to see that our posterior regularizer from Equation 10 is a standard VAE
ELBO (Equation 4) under a reparameterization of the student parameters, ˆφ = f (φ, Φ). Note that
C(Φ) is constant with respect to the student parameters, φ, and thus not used during optimization.
While the change seems minor, it omits the introduction of f (φ, Φ) which allows for a transfer of
information between models. In practice, we simply analytically evaluate KL[Qφ(z|x) ||QΦ(z|x)],
the KL divergence between the teacher and the student posteriors, instead of deriving the functional
form of f (φ, Φ) for each different distribution pair. We present Equation 12 simply as a means to
provide a more intuitive understanding of our functional regularizer.

4.2 BASE-LEVEL GENERATIVE MODEL.

While it is theoretically possible to use the vanilla VAE from Section 3.3 for the teacher and student
models, doing so brings to light a number of limitations that render it problematic for use in the con-
text of lifelong learning (visualized in Figure 5-Right). Speciﬁcally, using a standard VAE decoder,
Pθ(x|z), to generate synthetic replay data for the student is problematic due to two reasons:

1. Mixed Distributions: Sampling the continuous standard normal prior, N (0, 1), can select
a point in latent space that is in between two separate distributions, causing generation of
unrealistic synthetic data and eventually leading to loss of previously learnt distributions.
2. Undersampling: Data points mapped to the isotropic-gaussian posterior that are further
away from the prior mean will be sampled less frequently, resulting in an undersampling
of some of the constituent distributions.

To address these sampling limitations we decompose the latent variable, z, into an independent
continuous, zc ∼ Qφ(zc|x), and a discrete component, zd ∼ Qφ(zd|x), as shown in Equation 13
and visually in Figure 5-Left:

Qφ(zc, zd|x) = Qφ(zc|x)Qφ(zd|x).

(13)

The objective of the discrete component is to summarize the discriminative information of the indi-
vidual generative distributions. The continuous component on the other hand, caters for the remain-
ing sample variability (a nuisance variable [73]). Given that the discrete component can accurately
summarize the discriminative information, we can then explicitly sample from any of the past dis-
tributions, allowing us to balance the student model’s synthetic inputs with samples from all of the
previous learned distributions. We describe this beneﬁcial generative sampling property in more
detail in Section 4.2.1.

9

Figure 5: Left: Graphical model for VAE with independent discrete and continuous posterior, Qφ(zc, zd|x) =
Qφ(zc|x)Qφ(zd|x). Right: Two dimensional test variates, zj ∼ Qφ(z|x), zj ∈ R2, of a vanilla VAE
trained on MNIST. We depict the two generative shortcomings visually: 1) mixing of distributions which
causes aliasing in a lifelong setting and 2) undersampling of distributions in a standard isotropic-gaussian VAE
posterior.

Naively introducing the discrete component, zd, does not guarantee that the decoder will use it to
represent the most discriminative aspects of the modeled distribution. In preliminary experiments,
we observed that that the decoder typically learns to ignore the discrete component and simply
relies on the continuous variable, zc. This is similar to the posterior collapse phenomenon which
has received a lot of recent interest within the VAE community [99, 40]. Posterior collapse occurs
when training a VAE with a powerful decoder model such as a PixelCNN++ [127] or RNN [21, 40].
The output of the decoder, xj ∼ Pθ(x|z) can become almost independent of the posterior sample,
zj ∼ Qφ(z|x), but is still able to reconstruct the original sample by relying on its auto-regressive
property [40]. In Section 4.2.2, we introduce a mutual information regulariser which ensures that
the discrete component of the latent variable is not ignored.

4.2.1 CONTROLLED GENERATIONS.

Desired Task Conditional
P1(x) = P (x|zd = 1)
P2(x) = P (x|zd = 2)
P3(x) = P (x|zd = 3)

zc
∼ N (0, 1)
∼ N (0, 1)
∼ N (0, 1)

zd
[0, 0, 1]
[0, 1, 0]
[1, 0, 0]

Figure 6 & Table 2: FashionMNIST with L = 3 tasks: t-shirts, sandals and bag. To generate samples from
the i-th task conditional, Pi(x) = P (x|zd = i), we set zd = i, randomly sample zc ∼ N (0, 1), and run
[zc, zd] through the decoder, Pθ(x|zc, zd). Resampling zc, while keeping zd ﬁxed, enables generation of
varied samples from the task conditional. Left: Desired task conditionals. Right: Desired decoder behavior.

Given the importance of generative replay for knowledge transfer in LGM, synthetic sample gen-
eration by the teacher model needs to be representative of all the previously observed distributions
in order to prevent catastrophic forgetting. Under the assumption that zd accurately captures the
underlying discriminativeness of the individual distributions and through the deﬁnition of the LGM
generative process, shown in Equation 14:

PΘ(x|zd, zc), zc ∼ N (0, 1), zd ∼ Cat(1/L),

we can control generations by setting a ﬁxed value, zd = i, and randomly sampling the continuous
prior, zc ∼ N (0, 1). This is possible because the trained decoder approximates the task conditional
from Section 3.2:

(14)

(15)

P (x|zd = i) = Pi(x) ≈

PΘ(x|zc, zd = i)P (zc)
QΦ(zc|x)

10

where sampling the true task conditional, Pi(x), can be approximated by sampling zc ∼ P (zc) =
N (0, 1), keeping zd ﬁxed, and decoding the variates as shown in Equation 16 below:

ˆx ∼ PΘ(x|zc ∼ N (0, 1), zd = i).

(16)

We provide a simple example of our desired behavior for three generative tasks, L = 3, using Fash-
ion MNIST in Figure 6 and Table 2 above. The assumption made up till now is that zd accurately
captures the discriminative aspects of each distribution. However, there is no theoretical reason for
the model to impose this constraint on the latent variables. In practice, we often observe that the
decoder Pθ(x|zc, zd) ignores zd due to the much richer representation of the continuous variable,
zc. In the following section we introduce a mutual information constraint that encourages the model
to fully utilize zd.

4.2.2

INFORMATION RESTRICTING REGULARIZER

As eluded to in the previous section, the synthetic samples observed by the student model need to be
representative of all previous distributions. In order to control sampling via the process described in
Section 4.2.1, we need to enforce that the discrete variable, zd, carries the discriminative informa-
tion about each distribution. Given our graphical model from Figure 4-Left, we observe that there
are two ways to accomplish this: maximize the information content between the discrete random
variable, zd and the decoded ˆx, or minimize the information content between the continuous vari-
able, zc and the decoded ˆx. Since our graphical model and underlying network does not contain skip
connections, information from the input, x, has to ﬂow through the latent variables z = [zc, zd] to
reach the decoder. While both formulations can theoretically achieve the same objective, we ob-
served that in practice, minimizing I(ˆx, zc) provided better results. We believe the reason for this
is that minimizing I(ˆx, zc) provides the model with more subtle gradient information in contrast to
maximizing I(ˆx, zd) which receives no gradient information when the value of the k-th element of
the categorical sample is 1. We now formalize our mutual information regularizer, which we derive
from ﬁrst principles in Equation 17:

Ex∼Pi(x)[I(ˆx, zc)] = Ex∼Pi(x)[H(zc) − H(zc|ˆx)]

(17)

= Ex∼Pi(x)Ezc∼Qφ(zc|x)
(cid:123)(cid:122)
Ex∼Pi(x)[H(zc)]
+

(cid:124)

(cid:20)

(cid:21)
− log Qφ(zc|x)

(cid:125)

Ex∼Pi(x)E(zd,zc)∼Qφ(zc,zd|x)
(cid:124)

(cid:21)
(cid:20)
Eˆx∼Pθ (ˆx|zc,zd) log Qφ(zc|ˆx)
,
(cid:123)(cid:122)
Ex∼Pi(x)[−H(zc|ˆx)]

(cid:125)

where we use the independence assumption of our posterior from Equation 13 and the fact that the
expectation of a constant is the constant. This regularizer has parallels to the regularizer in InfoGAN
[18]. In contrast to InfoGAN, VAEs already estimate the posterior Qφ(zc|x) and thus do not need
the introduction of any extra parameters φ for the approximation. In addition [47] demonstrated that
InfoGAN uses the variational bound (twice) on the mutual information, making its interpretation
unclear from a theoretical point of view. In contrast, our regularizer has a clear interpretation: it
restricts information through a speciﬁc latent variable within the computational graph. We observe
that this constraint is essential for empirical performance of our model and empirically validate this
in our ablation study in Experiment 7.2.

4.3 LEARNING OBJECTIVE

The ﬁnal learning objective for each of the student models is the maximization of the sequential
VAE ELBO (Equation 10), coupled with generative replay (Equation 7)and the mutual information
regularizer, I(ˆx, zc), (Equation 17):

11

Lθ,φ(x) = EQφ(zc,zd|x)
(cid:124)

(cid:20)

(cid:21)
log Pθ(x|zc, zd)

− KL[Qφ(zc, zd|x)||P (zc, zd)]

(18)

(cid:125)

(cid:124)

(cid:123)(cid:122)
VAE ELBO
+ (1 − ω)KL[Qφ(zc, zd|x)||QΦ(zc, zd|x)]
(cid:125)
(cid:123)(cid:122)
Posterior Consistency Regularizer
− λI(ˆx, zc)
(cid:125)

(cid:123)(cid:122)
Mutual Information

(cid:124)

,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

The λ hyper-parameter controls the importance of the information gain regularizer. Too large a
value for λ causes a lack of sample diversity, while too small a value causes the model to not use
the discrete latent distribution. We did a random hyperparameter search and determined λ = 0.01
to be a reasonable choice for all of our experiments. This is in line with the λ used in InfoGAN
[18] for continuous latent variables. We empirically validate the necessity of both terms proposed in
Equation 18 in our ablation study in Experiment 7.2. We also validate the beneﬁt of the latent vari-
able factorization in Experiment 7.1. Before delving into the experiments, we provide a theoretical
analysis of computational complexity induced by our model and objective (Equation 18) in Section
4.4 below.

4.4 COMPUTATIONAL COMPLEXITY

We deﬁne the computational complexity of a typical VAE encoder and decoder as O(E)
and O(D) correspondingly;
internally these are dominated by the matrix-vector products
which take approximately LO(n2) for L layers. We also deﬁne the cost of applying the loss
function as O(K) + O(R), where O(K) is the cost of evaluating the KL divergence from
the ELBO (Equation 4) and O(R) the cost for evaluating the reconstruction term. Given
these deﬁnitions, we can summarize LGM’s computation complexity as follows in Equation 19:

O(D)
(cid:124) (cid:123)(cid:122) (cid:125)
teacher
generations

+ O(E) + O(D)
(cid:123)(cid:122)
(cid:125)
student encode
+ decode

(cid:124)

+ O(K) + O(R)
(cid:125)
(cid:123)(cid:122)
vae loss

(cid:124)

+ O(K)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
regularizer

+ O(K) + O(E)
(cid:123)(cid:122)
(cid:125)
mutual info

(cid:124)

= 2[O(D) + O(E)] + 3[O(K)] + O(R),

(19)

where we introduce increased computational complexity due to teacher generations, the cost of the
posterior regularizer, and the mutual information terms; the latter of which necessitates an extra
encode operation, O(E). The computational complexity is still dominated by the matrix-vector
product from evaluating forward functionals of the neural network. These operations can easily be
amortized through parallelization on modern GPUs and typical experiments do not directly scale
as per Equation 19. In our most demanding experiment (Experiment 6.6), we observe an average
empirical increase of 13.53 seconds per training epoch and 6.3 seconds per test epoch.

5 REVISITING STATE OF THE ART METHODS.

In this section we revisit some of the state of the art methods from Section 2. We begin by providing
a mathematical description of the differences between EWC [65], VCL [91] and LGM and follow it
up with a discussion of VASE [1] and their extensions of our work.

EWC and VCL: Our posterior regularizer, KL[Qφ(z|x)||QΦ(z|x)], affects the same parameters,
φ, as parameter regularizer methods such as EWC and VCL. However, rather than assuming a func-
tional form for the parameter posterior, P (φ|x), our method regularizes the output latent distribu-
tion Qφ(z|x). EWC and VCL, both make the assumption that P (φ|x) is distributed as an isotropic
gaussian3. This allows the use of the Fisher Information Matrix (FIM) in a quadratic parameter reg-
ularizer in EWC, and an analytical KL divergence of the posterior in VCL. This is a very stringent

3VCL assumes an isotropic gaussian variational form vs. EWC which directly assumes the parametric form

on P (φ|x).

12

requirement for the parameters of a neural network and there is active research in Bayesian neural
networks that attempts to relax this constraint [74, 75, 80].

EWC minφ d[P (φ|x)||P (Φ|x)]
2 (φ − Φ)T F (φ − Φ)

≈ γ

LGM ( Isotropic Gaussian Posterior ) minφ d[Qφ(z|x)||QΦ(z|x)]
|ΣΦ|
|Σφ|

Φ Σφ) + (µΦ − µφ)T Σ−1

Φ (µΦ − µφ) − C + log

(cid:20)
tr(Σ−1

(cid:18)

= 0.5

(cid:19)(cid:21)

In the above table we examine the distance metric d, used to minimize the effects of catastrophic
inference in both EWC and LGM. While our method can operate over any distribution that has
a tractable KL-divergence, for the purposes of demonstration we examine the simple case of an
isotropic gaussian latent-variable posterior. EWC directly enforces a quadratic constraint on the
model parameters φ, while our method indirectly affects the same parameters through a regulariza-
tion of the posterior distribution Qφ(z|x). For any given input variate, xj, LGM allows to model to
freely change its internal parameters, φ; it does so in a non-linear4 way such that the analytical KL
shown above is minimized.

VASE : The recent work of Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies (VASE) [1] extend upon our work [1, p. 7], but take a more empirical route by
incorporating a classiﬁcation-based heuristic for their posterior distribution. In contrast, we show
(Section 4.1.1) that our objective naturally emerges in a sequential learning setting for VAEs, allow-
ing us to infer the discrete posterior, Qφ(zd|x) in an unsupervised manner. Due to the incorporation
of direct supervised class information [1] also observe that regularizing the decoding distribution
Pθ(x|z) aids in the learning process, something that we observe to fail in a purely unsupervised
generative setting (Appendix Section 10.2). Finally, in contrast to [1], we include an information
restricting regularizer (Section 4.2.2) which allows us to directly control the interpretation and ﬂow
of information of the learnt latent variables.

6 EXPERIMENTS

We evaluate our model and the baselines over standard datasets used in other state of the art life-
long / continual learning literature [91, 136, 112, 57, 65, 94]. While these datasets are simple in
a traditional classiﬁcation setting, transitioning to a lifelong-generative setting scales the problem
complexity substantially. We evaluate LGM on a set of progressively more complicated tasks (Sec-
tion 6.2) and provide comparisons against baselines [91, 136, 65, 29, 63] using a set of standard
metrics (Section 6.1). All network architectures and other optimization details for our LGM model
are provided in Appendix Section 10.3 as well our open-source git repository [98].

6.1 PERFORMANCE METRICS

To validate the beneﬁt of LGM in a lifelong setting we explore three main performance dimensions:
the ability for the model to reconstruct and generate samples from all previous tasks and the ability
to learn a common representation over time, thus reducing learning sample complexity. We use three
main quantitative performance metrics for our experiments: the log-likelihood importance sample
estimate [15, 91], the negative test ELBO, and the Frechet distance metric [44]. In addition, we also
provide two auxiliary metrics to validate the beneﬁts of LGM in a lifelong setting: training sample
complexity and wall clock time per training and test epoch.

To fairly compare models with varying latent variable conﬁgurations, one solution is to marginal-
ize out the latents, z, during model evaluation / test time: (cid:82)
k=1 Pθ(x|z = zk).
This is realized in practice by using a Monte Carlo approximation (typically K=5000) and is com-
monly known as the importance sample (IS) log-likelihood estimate [15, 91]. As latent variable
and model complexity grows, this estimate tends to become noisier and intractable to compute. For
our experiments we use this metric only for the FashionMNIST and MNIST datasets as computing
one estimate over 10,000 test samples for a complex model takes approximately 35 hours on a K80
GPU.

z Pθ(x|z)dz ≈ (cid:80)K

In contrast to the IS log-likelihood estimate, the negative test ELBO (Equation 4) is only applicable
when comparing models with the same latent variable conﬁgurations; it is however much faster to

4This is because the parameters of the distribution are modeled by a deep neural network.

13

compute. The negative test ELBO provides a lower bound to the test log-likelihood of the true data
distribution under the assumed latent variable conﬁguration. One crucial aspect missing from both
these metrics is an evaluation of generation quality. We resolve this by using the Frechet distance
metric [44] and qualitative image samples.

The Frechet distance metric allows us to quantify the quality and diversity of generated samples by
using a pre-trained classiﬁer model to compare the feature statistics (generally under a Gaussian-
ity assumption) between synthetic generated samples and samples drawn from the test set. If the
Frechet distance between these two distributions is small, then the generative model is said to be
generating realistic images. The Frechet distance between two gaussians (produced by evaluating
latent embeddings of a classiﬁer model) with means mtest, mgen with corresponding covariances
Ctest, Cgen is:

||mtest − mgen||2

2 + T r(Ctest + Cgen − 2[CtestCgen]0.5).

(20)

While the Frechet distance, negative ELBO and IS log-likelihood estimate provide a glimpse into
model performance, there exists no conclusive metric that captures the quality of unsupervised gen-
erative models [124, 108] and active research suggests a direct trade-off between perceptual quality
and model representation [8]. Thus, in addition to the metrics described above, we also provide
qualitative metrics in the form of test image reconstructions and image generations. We summarize
all used performance metrics in Table 3 below:

Negative ELBO

Deﬁnition

Equation 4.

Purpoose
Quantitative metric on
likelihood / reconstructions.
Quantitative metric on
density estimate.
Quantitative metric on generations.

Lower is better?

yes

Negative Log-Likelihood

Frechet Distance
Test Reconstructions
Generations
#Training Samples

5000 (latent) sample Monte
Carlo estimate of Equation 4.
Equation 20.
yes
Pθ(x|zc ∼ Qφ(zc|x), zd ∼ Qφ(zd|x)) Qualitative view of reconstructions. N/A
Pθ(x|zd ∼ Cat(1/L), zc ∼ N (0, 1)).
N/A
# real training samples used for task i.
yes

Qualitative view of generations.
Sample Complexity.

yes

Table 3: Summary of different performance metrics.

6.2 DATA FLOW

Figure 7: Visual examples of training and test task sequences (top to bottom) for the datasets used to validate
LGM. The training set only consists of samples from the current task while the test set is a cumulative union
of the current task, coupled with all previous tasks. The permuted MNIST tasks uses {G1, ...GL−1} different
ﬁxed permutation matrices to create 4 auxiliary datasets.

14

In Figure 7 we list train and test variates depicting the data ﬂow for each of the problems that we
model. Due to the relaxing the need to preserve data in a lifelong setting, the train task sequence ob-
serves a single dataset, Dtr
i , at a time, without access to any previous, Dtr
<i. The corresponding test
dataset consists of a union (∪ operator) of the current test dataset, Dte
i , merged with all previously
observed test datasets, ˆD
i ∪ Dte

i−1 ∪ ... ∪ Dte
1 .

te
i = Dte

MNIST / Fashion MNIST: For the MNIST and Fashion MNIST problems, we observe a single
MNIST digit or fashion object (such as shirts) at a time. Each training set consists of 6000 training
samples and 1000 test samples. These samples are originally extracted from the full training and
test datasets which consist of 60,000 training and 10,000 test samples.

Permuted MNIST: this problem differs from the MNIST problem described above in that we use
the entire MNIST dataset at each task. After observing the ﬁrst task, which is the standard MNIST
dataset, each subsequent task differs through the application of a ﬁxed permutation matrix Gi on
the entire MNIST dataset. The test task sequence differs from the training task sequence in that we
simply use the corresponding full train and test MNIST datasets (with the appropriate application of
Gi).

Celeb-A: We split the CelebA dataset into four individual distributions using the features: bald,
male, young and eye-glasses. As with the previous problems, we treat each subset of data as an
individual distribution, and present our model samples from a single distribution at a time. This
presents a real world scenario as the samples per distribution varies drastically from only 3,713
samples for the bald distribution, to 126,788 samples for young. In addition speciﬁc samples can
span one or more of these distributions.

SVHN to MNIST: in this problem, we transition from fully observing the centered SVHN [89]
dataset to observing the MNIST dataset. We treat all samples from SVHN as being generated by
one distribution P1(x) and all the MNIST 5 samples as generated by another distribution P2(x)
(irrespective of the speciﬁc digit). At inference, the model is required to reconstruct and generate
from both datasets.

6.3 SITUATING AGAINST STATE OF THE ART LIFELONG LEARNING MODELS.
To situate LGM against other state of the art methods in lifelong learning we use the sequential
FashionMNIST and MNIST datasets described earlier in Section 6.2 and the data ﬂow diagram in
Figure 7. We contrast our LGM model against VCL [91], VCL without a task speciﬁc head network,
SI [136], EWC [65], Laplace propagation [29], a full batch VAE trained jointly on all data and a
standard naive sequential VAE without any catastrophic forgetting prevention strategy in Figures 8
and 9 below. The full batch VAE presents the upper-bound performance and all lifelong learning
models typically under-perform this model by the ﬁnal learning task. For the baselines, we use
the generously open sourced code [90] by the VCL authors, using the optimal hyper-parameters
speciﬁed for each model. We begin by evaluating the 5000 sample Monte Carlo estimate of the
log-likelihood of all compared models in Figure 8 below:

Figure 8: IS log-likelihood (mean ± std) × 5. Left: Fashion MNIST. Right: MNIST.

5MNIST was resized to 32x32 and converted to RBG to make it consistent with the dimensions of SVHN.

15

Even though each trial was repeated ﬁve times (each), we observe large increases in the estimates
at a few critical points. After further inspection, we determined the large magnitude increases were
due to the model observing a drastically different distribution at that point. We overlay the graphs
with an example variate for of the magnitude spikes. In the case of FashionMNIST for example,
the model observes its ﬁrst shoe distribution at i = 6; this contrasts the previously observed items
which were mainly clothing related objects. Interestingly we observe that LGM has much smoother
performance across tasks. We posit this is because LGM does not constrain its parameters, and
instead enforces the same input-output mapping through functional regularization.

Figure 9: Final model, i = 10, generation and reconstructions for MNIST and FashionMNIST. The LGM
model presents competitive performance for both generations and reconstructions, while not preserving any
past data nor past models.

16

Since one of the core tenets of lifelong learning is to reduce sample complexity over time, we use
this experiment to validate if LGM does in fact achieve this objective. Since all LGM models are
trained with an early-stopping criterion, we can directly calculate the number of samples used for
each learning task using the stopping epoch and mean, π of the Bernoulli sampling distribution of
the student model. In Figure 10 we plot the number of true samples and the number of synthetic
samples used by a model until it satisﬁed its early-stopping criterion. We observe a steady decrease
in the number of real samples used over time, validating LGMs advantage in a lifelong setting.

Figure 10: FashionMNIST sample complexity. Left: Synthetic training samples used till early-stopping. Right:
Real samples used till early-stopping.

6.4 DIVING DEEPER INTO THE SEQUENCE.
Rather than only visualizing the ﬁnal model’s qualitative results as in Figure 9, we provide quali-
tative results for model performance over time for the PermutedMNIST experiment in Figure 11.
This allows us to visually observe lifelong model performance over time. In this experiment, we
focus our efforts on EWC and LGM and visualize model (test) reconstructions starting from the
second learning task, G1D, till the ﬁnal G4D. The EWC-VAE variant that we use as a baseline
has the same latent variable conﬁguration as our model, enabling the usage of the test ELBO as a
quantitative metric for comparison. We use an unpermuted version of the MNIST dataset, D, as our
ﬁrst distribution, P1(x), as it allows us to visually asses the degradation of reconstructions. This is a
common setup utilized in continual learning [65, 136] and we extend it here to the density estimation
setting.

Figure 11: Top row: test-samples; bottom row: reconstructions. We visualize an increasing number of accu-
mulated distributions from left to right. (a) Lifelong VAE model (b) EWC VAE model.

17

Both models exhibit a different form of degradation: EWC experiences a more destructive form of
degradation as exempliﬁed by the salt-and-pepper noise observed in the ﬁnal dataset reconstruction
at G4D. LGM on the hand experiences a form of Gaussian noise as visible in the corresponding
ﬁnal dataset reconstruction. In order to numerically quantify this performance we analyze the log-
Frechet distance and negative ELBO below in Figure 12, where we contrast the LGM to EWC, a
batch VAE (full-vae in graph), an upto-VAE that observes all training data up to the current distri-
bution and a vanilla sequential VAE (vanilla). We examine a variety of different convolutional and
dense architectures and present the top performing models below. We observe that LGM drastically
outperforms EWC and the baseline naive sequential VAE in both metrics.

Figure 12: PermutedMNIST (a) negative Test ELBO and (b) log-Frechet distance.

6.5 LEARNING ACROSS COMPLEX DISTRIBUTIONS

Figure 13: (a) Reconstructions of test samples from SVHN[left] and MNIST[right]; (b) Decoded samples
ˆx ∼ Pθ(x|zd, zc) based on linear interpolation of zc ∈ R2 with zd = [0, 1]; (c) Same as (b) but with
zd = [1, 0].

The typical assumption in lifelong learning is that the sequence of observed distributions are related
[125] in some manner. In this experiment we relax this constraint by learning a common model
between the colored SVHN dataset and the binary MNIST dataset. While semantically similar to
humans, these datasets are vastly different, as one is based on RGB images of real world house
numbers and the other of synthetically hand-drawn digits. We visualize examples of the true test
inputs, x, and their respective reconstructions, ˆx, from the ﬁnal lifelong model in ﬁgure 13(a). Even
though the only true data the ﬁnal model received for training was the MNIST dataset, it is still able
to reconstruct the SVHN data observed previously. This demonstrates the ability of our architecture
to transition between complex distributions while still preserving the knowledge learned from the
previously observed distributions.

Finally, in ﬁgure 13(b) and 13(c) we illustrate the data generated from an interpolation of a 2-
dimensional continuous latent space, zc ∈ R2. To generate variates, we set the discrete categorical,
zd, to one of the possible values {[0, 1], [1, 0]} and linearly interpolate the continuous zc over the
range [−3, 3]. We then decode these to obtain the samples, ˆx ∼ Pθ(x|zd, zc). The model learns

18

a common continuous structure for the two distributions which can be followed by observing the
development in the generated samples from top left to bottom right on both ﬁgure 13(b) and 13(c).

6.6 VALIDATING EMPIRICAL SAMPLE COMPLEXITY USING CELEB-A

We iterate the Celeb-A dataset as described in the data ﬂow diagram (Figure 7) and use this learn-
ing task to explore qualitative and quantitative generations, as well as empirical real world time
complexity (as described in Section 4.4) on modern GPU hardware. We train a lifelong model and
a typical VAE baseline without catastrophic forgetting mitigation strategies and evaluate the ﬁnal
model’s generations in Figure 14. As visually demonstrated in Figure 14-Left, the lifelong model is
able to generate instances from all of the previous distributions, however the baseline model catas-
trophically forgets (Figure 14-Right) and only generates samples from the eye-glasses distribution.
This is also reinforced by the log-Frechet distance shown in Figure 15.

Figure 14: Left: Sequential generations for Celeb-A from the ﬁnal lifelong model for bald, male, young and
eye-glasses (left to right). Right: (random) generations by the ﬁnal baseline VAE model.

We also evaluate the wall-clock time in seconds (Table 4) for the lifelong model and the baseline-vae
for the 44,218 samples of the male distribution. We observe that the lifelong model does not add a
signiﬁcant overhead, especially since the baseline-vae undergoes catastrophic forgetting (Figure 14
Right) and completely fails to generate samples from previous distributions. Note that we present
the number of parameters and other detailed model information in our code and Appendix 10.3.

44,218 male samples
training-epoch (s)
testing-epoch (s)

baseline-VAE
43.1 +/- 0.6
9.79 +/- 0.12

Lifelong
56.63 +/- 0.28
16.09 +/- 0.01

Table 4: Mean & standard deviation wall-clock for one epoch of
male distribution of Celeb-A.

Figure 15: Celeb-A log-Frechet distance of lifelong vs. naive baseline VAE model without catastrophic miti-
gation strategies over the four distributions. Listed on the right is the time per epoch (in seconds) for an epoch
of the corresponding models.

7 ABLATION STUDIES

In this section we independently validate the beneﬁt of each of the newly introduced components
to the learning objective proposed in Section 4.3. In Experiment 7.1 we demonstrate the beneﬁt
of the discrete-continuous posterior factorization introduced in Section 4.2.1. Then in Experiment

19

7.2, we validate the necessity of the information restricting regularizer (Section 4.2.2) and posterior
consistency regularizer (Section 4.1.1).

7.1 LINEAR SEPARABILITY OF DISCRETE AND CONTINUOUS POSTERIOR

Figure 16: Left: Graphical model depicting classiﬁcation using pretrained VAE, coupled with a linear classiﬁer,
fθlin : z (cid:55)→ y. Right: Linear classiﬁer accuracy on the Fashion MNIST test set for a varying range of latent
dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024] and distributions.

In order to validate that the (independent) discrete and continuous latent variable posterior,
Qφ(zd, zc|x), aids in learning a better representation, we classify the encoded posterior sample
using a simple linear classiﬁer fθlin : z (cid:55)→ y, where y corresponds to the categorical class predic-
tion. Higher (linear) classiﬁcation accuracies demonstrate that the the VAE is able to learn a more
linearly separable representation. Since the latent representation of VAEs are typically used in aux-
iliary tasks, learning such a representation is useful in downstream tasks. This is a standard method
to measure posterior separability and is used in methods such as Associative Compression Networks
[41].

We use the standard training set of FashionMNIST [135] (60,000 samples) to train a standard VAE
with a discrete only (disc) posterior, an isotropic-gaussian only (gauss) posterior, a bernoulli only
(bern) posterior and ﬁnally the proposed independent discrete and continuous (disc+gauss) posterior
presented in Section 4.2.1. For each different posterior reparameterization, we train a set of VAEs
with varying latent dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024]. In the case of the disc+gauss
model we ﬁx the discrete dimension, |zd| = 10 and vary the isotropic-gaussian dimension to match
the total required dimension. After training each VAE, we proceed to use the same training data to
train a linear classiﬁer on the encoded posterior sample, z ∼ Qφ(z|x).

In Figure 16 we present the mean and standard deviation linear test classiﬁcation accuracies of each
set of the different experiments. As expected, the discrete only (disc) posterior performs poorly due
to the strong restriction of mapping an entire input sample to a single one-hot vector. The isotropic-
gaussian (gauss) and bernoulli (bern) only models provide a strong baseline, but the combination
of isotropic-gaussian and discrete posteriors (disc+gauss) performs much better, reaching an upper-
bound (linear) test-classiﬁcation accuracy of 87.1%. This validates that the decoupling of latent
represention presented in Section 4.2.1 aids in learning a more meaningful, separable posterior.

7.2 VALIDATING THE MUTUAL INFORMATION AND POSTERIOR CONSISTENCY

REGULARIZERS.

In order to independently evaluate the beneﬁt of our proposed Bayesian update regularizer (Sec-
tion 4.1.1) and the mutual information regularizer proposed in (Section 4.2.1) we perform an ab-
lation study using the MNIST data ﬂow sequence from Figure 7. We evaluate three scenarios: 1)
with posterior consistency and mutual information regularizers, 2) only posterior consistency and
3) without both regularizers. We observe that both components are necessary in order to gener-
ate high quality samples as evidenced by the negative test ELBO in Figure 17-(a) and the corre-

20

Figure 17: MNIST Ablation: (a) negative test ELBO. (b) Sequentially generated samples by setting zd and
sampling zc ∼ N (0, 1) (Section 4.2.1) with consistency + mutual information (MI). (c) Sequentially generated
samples with no consistency + no mutual information (MI).

sponding generations in Figure 17-(b-c). The generations produced without the information gain
regularizer and consistency in Figure 17-(c) are blurry. We attribute this to: 1) uniformly sam-
pling the discrete component is not guaranteed to generate samples representative samples from
P<i(x) and 2) the decoder, PΘ(x|zd, zc), relays more information through the continuous compo-
nent, PΘ(x|zd, zc) = PΘ(x|zc), causing catastrophic forgetting and posterior collapse [4].

8 LIMITATIONS

While LGM presents strong performance, it fails to completely solve the problem of lifelong gen-
erative modeling and we see a slow degradation in model performance over time. We attribute this
mainly to the problem of poor VAE generations that compound upon each other (also discussed
below). In addition, there are a few poignant issues that need to be resolved in order to achieve
an optimal (in terms of non-degrading Frechet distance / -ELBO) unsupervised generative lifelong
learner:

Distribution Boundary Evaluation: The standard assumption in current lifelong / continual learn-
ing approaches [91, 136, 112, 57, 65, 94] is to use known, ﬁxed distributions instead of learning
the distribution transition boundaries. For the purposes of this work, we focus on the accumulation
of distributions (in an unsupervised way), rather than introduce an additional level of indirection
through the incorporation of anomaly detection methods that aid in detecting distributional bound-
aries.

Blurry VAE Generations: VAEs are known to generate images that are blurry in contrast to GAN
based methods. This has been attributed to the fact that VAEs don’t learn the true posterior and
make a simplistic assumption regarding the reconstruction distribution Pθ(x|z) [4, 97]. While there
exist methods such as ALI [27] and BiGAN [26], that learn a posterior distribution within the GAN
framework, recent work has shown that adversarial methods fail to accurately match posterior-prior
distribution ratios in large dimensions [104].

Memory: In order to scale to a truly lifelong setting, we posit that a learning algorithm needs a
global pool of memory that can be decoupled from the learning algorithm itself. This decoupling
would also allow for a principled mechanism for parameter transfer between sequentially learnt
models as well a centralized location for compressing non-essential historical data. Recent work

21

such as the Kanerva Machine [133] and its extensions [134] provide a principled way to do this in
the VAE setting.

9 CONCLUSION

In this work we propose a novel method for learning generative models over a lifelong setting. The
principal assumption for the data is that they are generated by multiple distributions and presented
to the learner in a sequential manner. A key limitation for the learning process is that the method
has no access to any of the old data and that it shall distill all the necessary information into a
single ﬁnal model. The proposed method is based on a dual student-teacher architecture where the
teacher’s role is to preserve the past knowledge and aid the student in future learning. We argue for
and augment the standard VAE’s ELBO objective by terms helping the teacher-student knowledge
transfer. We demonstrate the beneﬁts this augmented objective brings to the lifelong learning setting
using a series of experiments. The architecture, combined with the proposed regularizers, aid in
mitigating the effects of catastrophic interference by supporting the retention of previously learned
knowledge.

REFERENCES

[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. Life-
long disentangled representation learning with cross-domain latent homologies. In Advances
in Neural Information Processing Systems, pages 9895–9905, 2018.

[2] W.-K. Ahn and W. F. Brewer. Psychological studies of explanationbased learning. In Investi-

gating explanation-based learning, pages 295–316. Springer, 1993.

[3] W.-K. Ahn, R. J. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one ex-
ample: Psychological evidence for explanation-based learning. Technical report, Coordinated
Science Laboratory, University of Illinois at Urbana-Champaign, 1987.

[4] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken

elbo. In International Conference on Machine Learning, pages 159–168, 2018.

[5] J. R. Anderson and G. H. Bower. Human associative memory. Psychology press, 2014.
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align

and translate. ICLR, 2015.

[7] H. R. Blackwell. Contrast thresholds of the human eye. JOSA, 36(11):624–643, 1946.
[8] Y. Blau and T. Michaeli. The perception-distortion tradeoff.

In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018.

[9] A. Blum. On-line algorithms in machine learning. In Online algorithms, pages 306–325.

Springer, 1998.

[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning, pages 1613–1622, 2015.
[11] L. Bottou. Online learning and stochastic approximations. On-line learning in neural net-

works, 17(9):142, 1998.

[12] L. Bottou and Y. L. Cun. Large scale online learning. In Advances in neural information

processing systems, pages 217–224, 2004.

[13] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.

[14] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational
bayes. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 1727–1735, 2013.

[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. ICLR, 2016.
[16] M. F. Carr, S. P. Jadhav, and L. M. Frank. Hippocampal replay in the awake state: a potential
substrate for memory consolidation and retrieval. Nature neuroscience, 14(2):147, 2011.

[17] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[18] X. Chen, X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.

22

In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc., 2016.
[19] Z. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and

big data. In International Conference on Machine Learning, pages 703–711, 2014.

[20] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning, 10(3):1–145, 2016.

[21] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent
variable model for sequential data. In Advances in neural information processing systems,
pages 2980–2988, 2015.

[22] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International Conference on Machine Learning, pages 1282–1289,
2019.

[23] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson. Human photoreceptor

topography. Journal of comparative neurology, 292(4):497–523, 1990.

[24] P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and

related ﬁelds, 2(4):555–581, 1996.

[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

[26] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782, 2016.

[27] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and

A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
[28] E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances

in Neural Information Processing Systems, pages 708–718, 2018.

[29] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace propagation. In Advances in Neural

Information Processing Systems, pages 441–448, 2004.

[30] L. Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories.
In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141.
IEEE, 2003.

[31] G. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable.

In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1565–1574. ACM, 2016.

[32] A. Fiat and G. J. Woeginger. Online algorithms: The state of the art, volume 1442. Springer,

1998.

[33] T. Furlanello, J. Zhao, A. M. Saxe, L. Itti, and B. S. Tjan. Active long term memory networks.

arXiv preprint arXiv:1606.02355, 2016.

[34] A. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities.

Journal of the American statistical association, 85(410):398–409, 1990.

[35] S. Gershman and N. Goodman. Amortized inference in probabilistic reasoning. In Proceed-

ings of the Cognitive Science Society, volume 36, 2014.

[36] Z. Ghahramani and H. Attias. Online variational bayesian learning.

In Slides from talk

presented at NIPS workshop on Online Learning, 2000.

[37] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats, volume 9, pages 249–256, 2010.

[38] R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric bayesian mix-
ture models. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2008.

[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.

[40] A. G. A. P. Goyal, A. Sordoni, M.-A. Cˆot´e, N. R. Ke, and Y. Bengio. Z-forcing: Training
stochastic recurrent networks. In Advances in neural information processing systems, pages
6713–6723, 2017.

[41] A. Graves, J. Menick, and A. v. d. Oord. Associative compression networks. arXiv preprint

arXiv:1804.02476, 2018.

23

[42] G. Grimmett, G. R. Grimmett, D. Stirzaker, et al. Probability and random processes. Oxford

university press, 2001.

[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6629–6640, 2017.

[45] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. stat,

[46] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1050:9, 2015.

1780, 1997.

[47] F. Huszar. Infogan: using the variational bound on mutual information (twice), Aug 2016.
[48] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reduc-

ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[49] V. Jain and E. Learned-Miller. Online domain adaptation of a pre-trained cascade of classi-
ﬁers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
577–584. IEEE, 2011.

[50] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Interna-

tional Conference on Learning Representations, 2017.

[51] H. Jeffreys. An invariant form for the prior probability in estimation problems. In Proceedings
of the Royal Society of London a: mathematical, physical and engineering sciences, volume
186, pages 453–461. The Royal Society, 1946.

[52] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper-
vised and generative approach to clustering. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, pages 1965–1972. AAAI Press, 2017.

[53] A. Johnson and A. D. Redish. Neural ensembles in ca3 transiently encode paths forward of

the animal at a decision point. Journal of Neuroscience, 27(45):12176–12189, 2007.

[54] M. I. Jordan. Artiﬁcial neural networks. chapter Attractor Dynamics and Parallelism in a
Connectionist Sequential Machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
[55] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[56] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering, 82(1):35–45, 1960.

[57] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learn-

ing. arXiv preprint arXiv:1710.10368, 2017.

[58] M. P. Karlsson and L. M. Frank. Awake replay of remote experiences in the hippocampus.

Nature neuroscience, 12(7):913, 2009.

[59] M. Karpinski and A. Macintyre. Polynomial bounds for vc dimension of sigmoidal and
general pfafﬁan neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.

[60] I. Katakis, G. Tsoumakas, and I. Vlahavas. Incremental clustering for the classiﬁcation of

[61] H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine

concept-drifting data streams.

Learning, pages 2654–2663, 2018.

[62] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. 2015.
[63] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
[64] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

[65] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–
1105, 2012.

[67] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

24

[68] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning, pages
1558–1566, 2016.

[69] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis. Continual classiﬁcation learning

using generative models. CoRR, abs/1810.10612, 2018.

[70] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks, 3361(10):1995, 1995.

[71] Z. Li and D. Hoiem. Learning without forgetting.

In European Conference on Computer

Vision, pages 614–629. Springer, 2016.

[72] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang,
In Proceedings of the European

and K. Murphy. Progressive neural architecture search.
Conference on Computer Vision (ECCV), pages 19–34, 2018.

[73] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder.

ICLR, 2016.

[74] C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[75] C. Louizos and M. Welling. Multiplicative normalizing ﬂows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 2218–2227. JMLR. org, 2017.

[76] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation

of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[77] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders.

arXiv preprint arXiv:1511.05644, 2015.

[78] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.
[79] J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling
on streams. In Advances in Neural Information Processing Systems, pages 1153–1161, 2015.
[80] A. Mishkin, F. Kunstner, D. Nielsen, M. Schmidt, and M. E. Khan. Slang: Fast structured
covariance approximations for bayesian deep learning with natural gradient. In Advances in
Neural Information Processing Systems, pages 6245–6255, 2018.

[81] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the ACM, 61(5):103–
115, 2018.

[82] T. M. Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research, 1980.

[83] T. M. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. D. Mishra,
M. Gardner, B. Kisiel, J. Krishnamurthy, et al. Never-ending learning. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[84] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in

machine learning. CoRR, abs/1906.10652, 2019.

[85] E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In International Confer-

ence on Learning Representations (ICLR), 2017.

[86] R. M. Neal. Bayesian Learning For Neural Networks. PhD thesis, University of Toronto,

[87] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte

1995.

carlo, 2(11):2, 2011.

[88] W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically exact, embarrassingly parallel
In N. L. Zhang and J. Tian, editors, Proceedings of the Thirtieth Conference on
MCMC.
Uncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27,
2014, pages 623–632. AUAI Press, 2014.

[89] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
In NIPS workshop on deep learning and

ural images with unsupervised feature learning.
unsupervised feature learning, page 5, 2011.

[90] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. nvcuong/variational-continual-learning,

2018.

[91] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. ICLR, 2018.

25

[92] K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, R. A. Olshen, and K. L. Oehler. Bayes risk
weighted vector quantization with posterior estimation for image compression and classiﬁca-
tion. IEEE Transactions on Image Processing, 5(2):347–360, 1996.

[93] F. A. Quintana and P. L. Iglesias. Bayesian clustering and product partition models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):557–574, 2003.
[94] N. C. Rabinowitz, G. Desjardins, A.-A. Rusu, K. Kavukcuoglu, R. T. Hadsell, R. Pascanu,
J. Kirkpatrick, and H. J. Soyer. Progressive neural networks, Nov. 23 2017. US Patent App.
15/396,319.

[95] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep con-

volutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[96] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are

unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

[97] T. Rainforth, A. Kosiorek, T. A. Le, C. Maddison, M. Igl, F. Wood, and Y. W. Teh. Tighter
variational bounds are not necessarily better. In International Conference on Machine Learn-
ing, pages 4277–4285, 2018.

[98] J. Ramapuram. Lifelongvae pytorch repository., 2017.
[99] A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with delta-

vaes. ICLR, 2019.

[100] A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with

VQ-VAE-2. CoRR, abs/1906.00446, 2019.

[101] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
In International Conference on Machine Learning,

inference in deep generative models.
pages 1278–1286, 2014.

[102] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,

[103] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

1997.

statistics, pages 400–407, 1951.

[104] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational

inference. arXiv preprint arXiv:1802.06847, 2018.

[105] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Comput.,

11(2):305–345, Feb. 1999.

[106] R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method.

Annals of Operations Research, 39(1):229–250, 1992.

[107] P. Ruvolo and E. Eaton. Ella: An efﬁcient lifelong learning algorithm.

In International

Conference on Machine Learning, pages 507–515, 2013.

[108] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models
via precision and recall. In Advances in Neural Information Processing Systems, pages 5228–
5237, 2018.

[109] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[110] N. W. Schuck and Y. Niv. Sequential replay of nonspatial task states in the human hippocam-

pus. Science, 364(6447):eaaw5181, 2019.

[111] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning, pages 4555–
4564, 2018.

[112] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In

Advances in Neural Information Processing Systems, pages 2994–3003, 2017.

[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484, 2016.

[114] D. L. Silver, G. Mason, and L. Eljabu. Consolidation using sweep task rehearsal: overcoming
In Canadian Conference on Artiﬁcial Intelligence, pages

the stability-plasticity problem.
307–322. Springer, 2015.

[115] D. L. Silver and R. E. Mercer. The parallel transfer of task knowledge using dynamic learning
rates based on a measure of relatedness. In Learning to learn, pages 213–233. Springer, 1996.
[116] D. L. Silver and R. E. Mercer. The task rehearsal method of life-long learning: Overcoming
In Conference of the Canadian Society for Computational Studies of

impoverished data.
Intelligence, pages 90–101. Springer, 2002.

26

[117] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algo-

rithms. In 2013 AAAI spring symposium series, 2013.

[118] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[119] W. E. Skaggs and B. L. McNaughton. Replay of neuronal ﬁring sequences in rat hippocampus

during sleep following spatial experience. Science, 271(5257):1870–1873, 1996.

[120] E. D. Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems

Sciences, 168:69–96, 1998.

[121] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[122] F. Tanaka and M. Yamamura. An approach to lifelong reinforcement learning through multi-
ple environments. In 6th European Workshop on Learning Robots, pages 93–99, 1997.
[123] A. V. Terekhov, G. Montone, and J. K. O’Regan. Knowledge transfer in deep block-modular
In Proceedings of the 4th International Conference on Biomimetic and

neural networks.
Biohybrid Systems-Volume 9222, pages 268–279. Springer-Verlag New York, Inc., 2015.
[124] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pages 1–10, 2016.
[125] S. Thrun. Lifelong learning: A case study. Technical report, CARNEGIE-MELLON UNIV

PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1995.

[126] S. Thrun and T. M. Mitchell. Lifelong robot learning.

In The biology and technology of

intelligent autonomous agents, pages 165–196. Springer, 1995.

[127] J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artiﬁcial

Intelligence and Statistics, pages 1214–1223, 2018.

[128] V. Vapnik. Estimation of dependences based on empirical data. Springer Science & Business

Media, 2006.

[129] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, R. Powell, et al. Alphastar: Mastering the real-time strategy game
starcraft ii. DeepMind Blog, 2019.

[130] S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic
model. In Proceedings of the 25th international conference on world wide web, pages 167–
176. International World Wide Web Conferences Steering Committee, 2016.

[131] R. C. Williamson and U. Helmke. Existence and uniqueness results for neural network ap-

proximations. IEEE Transactions on Neural Networks, 6(1):2–13, 1995.

[132] M. C. Wittrock. Generative learning processes of the brain. Educational Psychologist,

[133] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed

27(4):531–541, 1992.

memory. ICLR, 2018.

[134] Y. Wu, G. Wayne, K. Gregor, and T. Lillicrap. Learning attractor dynamics for generative

memory. In Advances in Neural Information Processing Systems, pages 9401–9410, 2018.

[135] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[136] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence.

In

International Conference on Machine Learning, pages 3987–3995, 2017.

27

10 APPENDIX

10.1 UNDERSTANDING THE CONSISTENCY REGULARIZER

The analytical derivations of the consistency regularizer show that the regularizer can be interpreted
as an a transformation of the standard VAE regularizer. In the case of an isotropic gaussian posterior,
the proposed regularizer scales the mean and variance of the student posterior by the variance of the
teacher 1 and adds an extra ’volume’ term. This interpretation of the consistency regularizer shows
that the proposed regularizer preserves the same learning objective as that of the standard VAE.
Below we present the analytical form of the consistency regularizer with categorical and isotropic
gaussian posteriors:

Proof 1 We assume the learnt posterior of the teacher is parameterized by a centered, isotropic
gaussian with Φ = [µE = 0, ΣE = diag(σE2
)] and the posterior of our student by a non-centered
isotropic gaussian with φ = [µS, ΣS = diag(σS2)], then

(cid:20)
tr(ΣE−1
KL(Qφ(z|x)||QΦ(z|x)) = 0.5

ΣS) + (µE − µS)T ΣE−1

(µE − µS) − F + log

= 0.5

F
(cid:88)

(cid:20)

j=1

1
σE2(j)

= KL(Qφ∗(z|x)||N (0, I)) − log |ΣE|

(σS2(j) + µS2(j)) − 1 + log σE2(j) − log σS2(j)

Via a reparameterization of the student’s parameters:

φ∗ = [µS∗, σS∗2]

µS∗ =

; σS∗2 =

µS(j)
σE2(j)

σS2(j)
σE2(j)

(cid:19)(cid:21)

(cid:18) |ΣE|
|ΣS|
(cid:21)

(21)

(22)

It is also interesting to note that our posterior regularizer becomes the prior if:

limσE2 (cid:55)→1KL(Qφ(z|x)||QΦ(z|x)) = KL(Qφ(z|x)||N (0, I))

Proof 2 We parameterize the learnt posterior of the teacher by Φi =

exp(pE
i )
i=1 exp(pE
i )

(cid:80)J

and the pos-

terior of the student by φi =
i ) and cS = (cid:80)J
cE = (cid:80)J
i=1 exp(pE
The KL divergence from the ELBO can now be re-written as:

exp(pS
i )
i=1 exp(pS
i )
i=1 exp(pS

(cid:80)J

. We also redeﬁne the normalizing constants as

i ) for the teacher and student models respectively.

KL(Qφ(zd|x)||QΦ(zd|x)) =

J
(cid:88)

i=1

exp(pS
i )
cS

log

(cid:18) exp(pS
i )
cS

(cid:19)

cE
exp(pE
i )

= H(pS, pS − pE) = −H(ps) + H(pS, pE)

(23)

where H( ) is the entropy operator and H( , ) is the cross-entropy operator.

28

10.2 RECONSTRUCTION REGULARIZER

Figure 18: Fashion Negative Test ELBO

Figure 19: Fashion Log-Frechet Distance

While it is possible to constrain the reconstruction/decoder term of the VAE in a similar manner to
the consistency posterior-regularizer, i.e: KL[Pθ(ˆx|z)||PΘ(ˆx|z)], doing so diminishes model per-
formance. We hypothesize that this is due to the fact that this regularizer contradicts the objective of
the reconstruction term Pθ(x|z) in the ELBO which already aims to minimize some metric between
the input samples x and the reconstructed samples ˆx; eg: if Pθ(x|z) ∼ N (µ, diag[σ]), then the
loss is proportional to ||ˆx − x||2
2, the standard L2 loss. Without the addition of this reconstruction
cross-model regularizer, the model is also provided with more ﬂexibility in how it reconstructs the
output samples.

In order to quantify the this we duplicate the FashionMNIST Experiment listed in the data ﬂow
deﬁnition in Figure 7. We use a simpler model than the main experiments to validate this hypothesis.
We train two dense models (-D): one with just the posterior consistency regularizer (without-LL-
D) and one with the consistency and likelihood regularizer (with-LL-D). We observe the model
performance drops (with respect to the Frechet distance as well the test ELBO) in the case of the
with-LL-D as demonstrated in Figures 18 and 19.

10.3 MODEL ARCHITECTURE

We used two different architectures for our experiments. When we use a dense network (-D) we
used two layers of 512 units to map to the latent representation and two layers of 512 to map back
to the reconstruction for the decoder. We used batch norm [48] and ELU activations for all the
layers barring the layer projecting into the latent representation and the output layer. Note that
while we used the same architecture for EWC we observed a drastic negative effect when using
batch norm and thus dropped it’s usage. The convolution architectures (-C) used the architecture
described below for the encoder and the decoder (where the decoder used conv-transpose layers for
upsampling). The notation is [OutputChannels, (ﬁlterX, ﬁlterY), stride]:

Encoder: [32, (5, 5), 1] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 1] (cid:55)→

GN+ELU (cid:55)→ [256, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [512, (1, 1), 1] (cid:55)→
GN+ELU (cid:55)→ [512, (1, 1), 1]

Decoder: [256, (4, 4), 1] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 1]

(24)

(cid:55)→ GN+ELU (cid:55)→ [32, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [32, (5, 5), 1]
(cid:55)→ GN+ELU (cid:55)→ [chans, (1, 1), 1]

29

Method
EWC-D
naive-D
batch-D
batch-D
lifelong-D
EWC-C
naive-C
batch-C
batch-C
lifelong-C

Initial zd dimension
10
10
10
10
1
10
10
10
10
1

Final zd dimension
10
10
10
10
10
10
10
10
10
10

zc dimension
14
14
14
14
14
14
14
14
14
14

# initial parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,165,311
30,767,428
7,691,280
7,691,280
15,382,560
15,235,072

# ﬁnal parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,179,661
30,767,428
7,691,280
7,691,280
15,382,560
15,382,560

The table above lists the number of parameters for each model and architecture used in our ex-
periments. The lifelong models initially start with a zd of dimension 1 and at each step we grow
the representation by one dimension to accommodate the new distribution (more info in Section
10.7). In contrast, the baseline EWC models are provided with the full representation throughout
the learning process. EWC has double the number of parameters because the computed diagonal
ﬁsher information matrix which is the same dimensionality as the number of parameters. EWC also
neeeds the preservation of the teacher model [Φ, Θ] to use in it’s quadratic regularizer. Both the
naive and batch models have the fewest number of parameters as they do not use a student-teacher
framework and only use one model, however the vanilla model has no protection against catastrophic
interference and the full model is just used as an upper bound for performance.

We used Adam [62] to optimize all of our problems with a learning rate of 1e-4 or 1e-3. When we
used weight transfer we re-initialized the accumulated momentum vector of Adam as well as the
aggregated mean and variance of the batch norm layers. The full architecture can be examined in
our github repository [98] and is provided under an MIT license.

10.4 CONTRAST TO STREAMING / ONLINE METHODS

Our method has similarities to streaming methods such as Streaming Variational Bayes (SVB) [14]
and Incremental Bayesian Clustering methods [60, 38] in that we estimate and reﬁne posteriors
through time. In general this can be done through the following Bayesian update rule that states that
the lastest posterior is proportional to the current likelihood times the previous posterior:

P (z|X1, ..., Xt) ∝ P (Xt|z)P (z|X1, ..., Xt−1)
SVB computes the intractable posterior, P (z|X1, ..., Xt), utilizing an approximation, At, that ac-
cepts as input the current dataset, Xt, along with the previous posterior At−1 :

(25)

P (z|X1, ..., Xt) ≈ At(Xt, At−1)

(26)

The ﬁrst posterior input (At=0) to the approximating function is the prior P (z). The objective of
SVB and other streaming methods is to model the posterior of the currently observed data in the
best possible manner. Our setting differs from this in that we want to retain information from all
previously observed distributions (sometimes called a knowledge store [126]). This can be useful
in scenarios where a distribution is seen once, but only used much later down the road. Rather than
creating a posterior update rule, we recompute the posterior via Equation 25, leveraging the fact that
we can re-generate X<t ≈ ˆX<t through the generative process. This allows us to recompute a more
appropriate posterior re-using all of the (generated) data, rather than using the previously computed
(approximate) posterior At−1:

P (z|X1, X2, ..., Xt) ∝ P (Xt|z)P (z| ˆX1, ..., ˆXt−1)

(27)

Coupling this generative replay strategy with the Bayesian update regularizer introduced in Section
4.1.1, we demonstrate that not only do we learn an updated poster as in Equation 27, but also allow
for a natural transfer of information between sequentially learnt models: a fundamental tenant of
lifelong learning [126, 125].

Finally, another key difference between lifelong learning and online methods is that lifelong learning
aims to learn from a sequence of tentatively different [20] tasks while still retaining and accumulating

30

knowledge; online learning generally assumes that the true underlying distribution comes from a
single distribution [11]. There are some exceptions to this where online learning is applied to the
problem of domain adaptation, eg: [49, 60].

10.5 EWC BASELINES: COMPARING CONV & DENSE NETWORKS

We compared a whole range of EWC baselines and use the best performing models few in our
experiments. Listed in Figure 10.5 are the full range of EWC baselines run on the PermutedMNIST
and FashionMNIST experiments. Recall that C / D describes whether a model is convolutional or
dense and the the number following is the hyperparameter for the EWC or Lifelong VAE.

10.6 GUMBEL REPARAMETERIZATION

Since we model our latent variable as a combination of a discrete and a continuous distribution we
also use the Gumbel-Softmax reparameterization [76, 50]. The Gumbel-Softmax reparameterization
over logits [linear output of the last layer in the encoder] p ∈ RM and an annealed temperature
parameter τ ∈ R is deﬁned as:

z = sof tmax(

); g = −log(−log(u ∼ U nif (0, 1)))

(28)

log(p) + g
τ

u ∈ RM , g ∈ RM . As the temperature parameter τ (cid:55)→ 0, z converges to a categorical.

10.7 EXPANDABLE MODEL CAPACITY AND REPRESENTATIONS

Multilayer neural networks with sigmoidal activations have a VC dimension bounded between
O(ρ2)[120] and O(ρ4)[59] where ρ are the number of parameters. A model that is able to con-

31

sistently add new information should also be able to expand its VC dimension by adding new pa-
rameters over time. Our formulation imposes no restrictions on the model architecture: i.e. new
layers can be added freely to the new student model.
In addition we also allow the dimensionality of zd ∈ RJ , our discrete latent representation to grow
in order to accommodate new distributions. This is possible because the KL divergence between
two categorical distributions of different sizes can be evaluated by simply zero padding the teacher’s
smaller discrete distribution. Since we also transfer weights between the teacher and the student
model, we need to handle the case of expanding latent representations appropriately. In the event
that we add a new distribution we copy all the weights besides the ones immediately surrounding
the projection into and out of the latent distribution. These surrounding weights are reinitialized to
their standard Glorot initializations [37].

32

9
1
0
2
 
v
o
N
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
7
4
8
9
0
.
5
0
7
1
:
v
i
X
r
a

LIFELONG GENERATIVE MODELING

Jason Ramapuram ∗ †
Jason.Ramapuram@etu.unige.ch

Magda Gregorova ∗ †
magda.gregorova@hesge.ch

Alexandros Kalousis ∗ †
Alexandros.Kalousis@hesge.ch

ABSTRACT

Lifelong learning is the problem of learning multiple consecutive tasks in a se-
quential manner, where knowledge gained from previous tasks is retained and
used to aid future learning over the lifetime of the learner. It is essential towards
the development of intelligent machines that can adapt to their surroundings. In
this work we focus on a lifelong learning approach to unsupervised generative
modeling, where we continuously incorporate newly observed distributions into a
learned model. We do so through a student-teacher Variational Autoencoder ar-
chitecture which allows us to learn and preserve all the distributions seen so far,
without the need to retain the past data nor the past models. Through the intro-
duction of a novel cross-model regularizer, inspired by a Bayesian update rule,
the student model leverages the information learned by the teacher, which acts
as a probabilistic knowledge store. The regularizer reduces the effect of catas-
trophic interference that appears when we learn over sequences of distributions.
We validate our model’s performance on sequential variants of MNIST, Fashion-
MNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model
mitigates the effects of catastrophic interference faced by neural networks in se-
quential learning scenarios.

1

INTRODUCTION

Machine learning is the process of approximating unknown functions through the observation of typ-
ically noisy data samples. Supervised learning approximates these functions by learning a mapping
from inputs to a predeﬁned set of outputs such as categorical class labels (classiﬁcation) or contin-
uous targets (regression). Unsupervised learning seeks to uncover structure and patterns from the
input data without any supervision. Examples of this learning paradigm include density estimation
and clustering methods. Both learning paradigms make assumptions that restrict the set of plausible
solutions. These assumptions are referred to as hypothesis spaces, biases or priors and aid the model
in favoring one solution over another [82, 128]. For example, the use of convolutions [70] to process
images favors local structure; recurrent models [54, 46] exploit sequential dependencies and graph
neural networks [109, 64] assume that the underlying data can be modeled accurately as a graph.

Current state of the art machine-learning models typically focus on learning a single model for a
single task, such as image classiﬁcation [72, 121, 43, 118, 66], image generation [100, 13, 63, 39],
natural language question answering [25, 96] or single game playing [129, 113]. In contrast, humans
experience a sequence of learning tasks over their lifetimes, and are able to leverage previous learn-
ing experiences to rapidly learn new tasks. Consider learning how to ride a motorbike after learning
to ride a bicycle: the task is drastically simpliﬁed through the use of prior learning experience. Stud-
ies [2, 3, 67] in psychology have shown that humans are able to generalize to new concepts in a rapid
manner, given only a handful of samples. [67] demonstrates that humans can classify and generate
new concepts of two wheel vehicles given just a single related sample. This contrasts the state of the
art machine learning models described above which use hundreds of thousands of samples and fail
to generalize to slight variations of the original task [22].

∗University of Geneva, Switzerland
†Haute cole de gestion de Genve, HES-SO, Switzerland

1

Lifelong learning [126, 125] argues for the need to consider learning over task sequences, where
learned task representations and models are stored over the entire lifetime of the learner and can be
used to aid current and future learning. This form of learning allows for the transfer of previously
learned models and representations and can reduce the sample complexity of the current learning
problem [126]. In this work we restrict ourselves to a subset of the broad lifelong learning paradigm;
rather than focus on the supervised lifelong learning scenario as most state of the art methods, our
work is one of the ﬁrst to tackle the more challenging problem of deep lifelong unsupervised learn-
ing. We also identity and relax crucial limitations of prior work in life-long learning that requires
the storage of previous models and training data, allowing us to operate in a more realistic learning
scenario.

2 RELATED WORK
The idea of learning in a continual manner has been explored extensively in machine learning,
seeded by the seminal works of lifelong-learning [126, 125, 117], online-learning [32, 9, 11, 12]
and sequential linear gaussian models [105, 36] such as the Kalman Filter [56] and its non-linear
counterpart, the Particle Filter [24]. Lifelong learning bears some similarities to online learning
in that both learning paradigms observe data in a sequential manner. Online learning differs from
lifelong learning in that the central objective of a typical online learner [11, 12] is to best solve/ﬁt the
current learning problem, without preserving previous learning. In contrast, lifelong learners seek to
retain, and reuse, the learned behavior acquired over past tasks, and aim to maximize performance
across all tasks. Consider the example of forecasting click through rate: the objective of the online
learner is to evolve over time, such that it best represents current user preferences. This contrasts
lifelong learners which enforce a constraint between tasks to ensure that previous learning is not
lost.

Lifelong Learning [126] was initially proposed in a supervised learning framework for concept
learning, where each task seeks to learn a particular concept/class using binary classiﬁcation. The
original framework used a task speciﬁc model, such as a K Nearest Neighbors (KNN) 1, coupled with
a representation learning network that used training data from all past learning tasks (support sets),
to learn a common, global representation. This supervised approach was later improved through the
use of dynamic learning rates [115], core-sets [114] and multi-head classiﬁers [31].
In parallel,
lifelong learning was extended to independent multi-task learning [107, 31], reinforcement learning
[125, 122, 102], topic modeling [19, 130] and semi-supervised language learning [83, 81]. For a
more detailed review see [20].

More recently, lifelong learning has seen a resurgence within the framework of deep learning. As
mentioned earlier, one of the central tenets of lifelong learning is that that the learner should perform
well over all observed tasks. Neural networks, and more generally, models that learn using stochastic
gradient descent [103], typically cannot persist past task learning without directly preserving past
models or data. This problem of catastrophic forgetting [78] is well known in the neural network
community and is the central obstacle that needs to be resolved to build an effective neural lifelong
learner. Catastrophic forgetting is the phenomenon where model parameters of a neural network
trained in a sequential manner become biased towards the distribution of the latest observations,
forgetting previously learned representations, over data no longer accessible for training. In order to
mitigate catastrophic forgetting current research in lifelong learning employs four major strategies:
transfer learning, replay mechanisms, parameter regularization and distribution regularization. In
table 1 we classify the different lifelong learning methods that we will discuss in the following
paragraphs into these strategies.

EWC [65] VCL [91] LwF [71] ALTM [33]
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

PNN [94] DGR [112, 57] DBMNN [123]
(cid:88)
(cid:55)

(cid:88)
(cid:55)

(cid:55)
(cid:88)

(cid:88)

Transfer learning
Replay mechanisms
Parameter
regularization
Functional
regularization
Table 1: Catastropic interference mitigation strategies of state of the art models. Rows highlighted in gray
represent desirable mitigation strategies.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

SI [136] VASE [1] LGM (us)
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:88)

1These models were known as memory based learning in [126].

2

Transfer learning: These approaches mitigate catastrophic forgetting by freezing previous task
models and relaying a latent representation of the previous task to the current model. Research in
transfer learning for the mitigation of catastrophic forgetting include Progressive Neural Networks
(PNN) [94] and Deep Block-Modular Neural Networks (DBMNN) [123] to name a few. These
approaches allow the current model to adapt its parameters to the (new) joint representation in an
efﬁcient manner and prevent forgetting through the direct preservation of all previous task mod-
els. Deploying such a transfer learning mechanism in a lifelong learning setting would necessitate
training a new model with every new task, considerably increasing the memory footprint of the life-
long learner. In addition, since transfer learning approaches freeze previous models, it negates the
possibility of improving previous task performance using knowledge gathered from new tasks.

Replay mechanisms: The original formulation of lifelong learning [126] required the preservation
of all previous task data. This requirement was later relaxed in the form of core-sets [116, 114, 91],
which represent small weighted subsets of inputs that approximate the full dataset. Recently, within
the classiﬁcation setting, there have been replay approaches that try to lift the requirement of storing
past training data by relying on generative modeling [112, 57]; we will call such methods Deep gen-
erative replay (DGR) methods. DGR methods methods use a student-teacher network architecture,
where the teacher (generative) model augments the student (classiﬁer) model with synthetic samples
from previous tasks. These synthetic task samples are used in conjunction with real samples from
the current task to learn a new joint model across all tasks. While strongly motivated by biologi-
cal rehearsal processes [119, 53, 58, 110], these generative replay strategies fail to efﬁciently use
previous learning and simply re-learn each new joint task from scratch.

Parameter regularization: Most work that mitigates catastrophic forgetting falls under the um-
brella of parameter regularization. There are two approaches within this mitigation strategy: con-
straining the parameters of the new task to be close to the previous task through a predeﬁned metric,
and enforcing task-speciﬁc parameter sparsity. The two approaches are related as task-speciﬁc pa-
rameter sparsity can be perceived as a reﬁnement of the parameter constraining approach. Parameter
constraining approaches typically share the same model/parameters, but encourage new tasks from
altering important learned parameters from previous tasks. Task speciﬁc parameter sparsity relaxes
this, by enforcing that each task use a different subset of parameters from a global model, through
the use of an attention mechanism.

Models such as Laplace Propagation [29], Elastic Weight Consolidation (EWC) [65], Synaptic In-
telligence (SI) [136] and Variational Continual Learning (VCL) [91] fall under the parameter con-
straining approach. EWC for example, uses the Fisher Information matrix (FIM) to control the
change of model parameters between two learning tasks. Intuitively, important parameters should
not have their values changed, while non-important parameters are left unconstrained. The FIM is
used as a weighting in a quadratic parameter difference regularizer under a Gaussianity assump-
tion of the parameter posterior. However, this Gaussian parameter posterior assumption has been
demonstrated [86, 10] to be sub-optimal for learned neural network parameters. VCL improves
upon EWC, by generalizing the local assumption of the FIM to a KL-Divergence between the (vari-
ational) parameter posterior and prior. This generalization derives from the fact that the FIM can
be cast as a KL divergence between the posterior and an epsilon perturbation of the same random
variable [51]. VCL actually spans a number of different mitigation strategies as it uses parameter
regularization (described above) , transfer learning (it keeps a separate head network per task) and
replay (it persists a core-set of true data per task).

Models such as Hard Attention to the Task (HAT) [111] and the Variational Autoencoder with Shared
Embeddings (VASE) [1] fall under the task-speciﬁc parameter sparsity strategy. This mitigation
strategy enforces that different tasks use different components of a single model, typically through
the use of attention vectors [6] that are learned given supervised task labels. Multiplying the attention
vectors with the model outputs prevents gradient descent updates for different subsets of the model’s
parameters, allowing them to be used for future task learning. Task speciﬁc parameter sparsity allows
a model to hold-out a subset of its parameters for future learning and typically works well in practice
[111], with its strongest disadvantage being the requirement of supervised information.

Functional regularization: Parameter regularization methods attempt to preserve the learned be-
havior of the past models by controlling how the model parameters change between tasks. However,
the model parameters are only a proxy for the way a model actually behaves. Models with very
different parameters can have exactly the same behavior with respect to input-output relations (non-

3

uniqueness [131]). Functional regularization concerns itself with preserving the actual object of
interest:
the input-output relations. This strategy allows the model to ﬂexibly adapt its internal
parameter representation between tasks, while still preserving past learning.

Methods such as distillation [45], ALTM [33] and Learning Without Forgetting (LwF) [71] impose
similarity constraints on the classiﬁcation outputs of models learned over different tasks. This can
be interpreted as functional regularization by generalizing the constraining metric (or semi-metric)
to be a divergence on the output conditional distribution. In contrast to parameter regularization, no
assumptions are made on the parametric form of the parameter posterior distribution. This allows
models to ﬂexibly adapt their internal representation as needed, making functional regularization a
desirable mitigation strategy. One of the pitfalls of current functional regularization approaches is
that they necessitate the preservation of all previously data.

2.1 LIMITATIONS OF EXISTING APPROACHES.

A simple solution to the problem of lifelong learning is to store all data and re-learn a new joint
multi-task representation [17] at each newly observed task. Alternatively, it is possible to retain
all previous model parameters and select the model that presents the best performance on new test
task data. Existing solutions typically relax one these requirements. [33, 71, 91] relaxes the need
for model persistence, but requires preservation of all data [33, 71], or a growing core-set of data
[116, 114, 91]. Conversely, [94, 123, 91, 136] relaxes the need to store data, but persists all previous
models [94, 123] or a subset of model parameters [91, 136].

Unlike these approaches, we draw inspiration from how humans learn over time and remove the
requirement of storing past training data and models. Consider the human visual system; research
has shown [23, 7] that the human eye is capable of capturing 576 megapixels of content per image
frame. If stored naively on a traditional computer, this corresponds to approximately 6.9 gigabytes of
information per sample. Given that we perceive trillions of frames over our lifetimes, it is infeasible
to store this information in its base, uncompressed representation. Research in neuroscience has
validated [132, 5] that the associative human mind, compresses, merges and reconstructs information
content in a dynamic way. Motivated by this, we believe that a lifelong learner should not store past
training data or models. Instead, it should retain a latent representation that is common over all tasks
and evolve it as more tasks are observed.

2.2 OUR SOLUTION AT A HIGH LEVEL.

While most research in lifelong learning focuses on supervised learning [94, 33, 71, 123, 136, 65],
we focus on the more challenging task of deep unsupervised latent variable generative modeling.
These models have wide ranging applications such as clustering [77, 52, 85] and pre-training [68,
95].

Central to our lifelong learning method are a pair of generative models, aptly named the teacher and
student, which we train by exploiting the replay and functional regularization strategies described
above. After training a single generative model over the ﬁrst task, we use it is used as the teacher for
a newly instantiated student model. The student model receives data from the current task, as well as
replayed data from the teacher, which acts as a probabilistic storage container of past tasks. In order
to preserve previous learning, we make use of functional regularization, which aids in preserving
input-output relations over past tasks.

Unlike EWC or VCL, we make no assumptions on the form of the parameter posterior and allow
the generative models to use available parameters as appropriate, to best accommodate current and
past learning. The use of generative replay, coupled with functional regularization, renders the
preservation of the past models and past data unnecessary. It also signiﬁcantly improves the sample
complexity on future task learning, which we empirically demonstrate in our experiments. Finally
we should note that it is straightforward to adapt our approach to the supervised learning setting, as
we have done in [69].

3 BACKGROUND

In this section we describe the main concepts that we use throughout this work. We begin by de-
scribing the base-level generative modeling approach in Section 3.1, followed by how it extends to

4

the lifelong setting in Section 3.2. Finally, in Section 3.3, we describe the Variational Autoencoder
over which we instantiate our lifelong generative model.

3.1 LATENT VARIABLE GENERATIVE MODELING

We consider a scenario where we observe a dataset, D, consisting of N variates, D = {xj}N
j=1,
of a continuous or discrete variable x. We assume that the data is generated by a random process
involving a non-observed random variable, z. The data generation process involves ﬁrst sampling
zj ∼ P (z) and then producing a variate from the conditional, xj ∼ Pθ(x|z). We visualize this
form of latent generative model in the graphical model in Figure 14.

Figure 1: Typical latent variable graphical model. Gray nodes represent observed variables while white nodes
represent unobserved variables.

Typically, latent variables models are solved through maximum likelihood estimation which can be
formalized as:

max
θ

log Pθ(x) = max

log

Pθ(x|z)P (z)dz = max

log Ez[Pθ(x|z)]

(1)

θ

(cid:90)

θ

In many cases, the expectation from Equation 1 does not have a closed form solution (eg: non-
conjugate distributions) and quadrature is not computationally tractable due to large dimensional
spaces [63, 101] (eg: images). To overcome these intractabilities we use a Variational Autoencoder
(VAE), which we summarize in Section 3.3. The VAE allows us to infer our latent variables and
jointly estimate the parameters of our model. However, before describing the VAE, it is important
to understand how this generative setting can be perceived in a lifelong learning scenario.

3.2 LIFELONG GENERATIVE MODELING

Lifelong generative modeling extends the single-distribution estimation task from Section 3.1 to a
set of i = {1...L} sequentially observed learning tasks. The i-th learning task has variates that are
realized from the task speciﬁc conditional, Pi(x) = P (x|zd = i), where zd acts as a categorical
indicator variable of the current task. We visualize a simpliﬁed form of this in Figure 2 below.

Figure 2: Simpliﬁed lifetime of a lifelong learner. Given a true (unknown) distribution, P (x) =
(cid:82) P (x|zd)P (zd)δzd, we observe partial information in the form of L sequential tasks, {D1 (cid:55)→ D2, ... (cid:55)→
DL}. Observing more tasks, reduces the uncertainty of the model until convergence, PθL (x) ≈ P (x).

Crucially, when observing task, Di, the model has no access to any of the previous task datasets,
D<i. As the lifelong learner observes more tasks, {D1 (cid:55)→ D2 (cid:55)→ ... (cid:55)→ DL}, it should improve its
estimate of the true distribution, Pθ(x) ≈ P (x) = (cid:82) P (x|zd)P (zd)δzd, which is unknown at the
start of training.

5

3.3 THE VARIATIONAL AUTOENCODER

As eluded to in Section 3.1, we would like to infer the latent variables from the data. This
can be realized as an alternative form of Equation 1 in the form of Bayes rule: Pφ(z|x) =
Pθ(x|z)P (z)/Pθ(x), where Pφ(z|x) is referred to as the latent variable posterior and Pθ(x|z)
as the likelihood. One method of approximating the posterior, Pφ(z|x), is through MCMC sam-
pling methods such as Gibbs sampling [34] or Hamiltonian MCMC [87]. MCMC methods have
the advantage that they provide asymptotic guarantees [88] of convergence to the true posterior,
Pφ(z|x). However in practice it is not possible to know when convergence has been achieved. In
addition, due to their Markovian nature, they possess an inner loop, which makes it challenging to
scale for large scale datasets.

In contrast, Variational Inference (VI) [55] side-steps the intractability of the posterior by approxi-
mating it with a tractable distribution family, Qφ(z|x). VI rephrases the objective of determining the
posterior as an optimization problem by minimizing the KL divergence between the known distri-
butional family, Qφ(z|x), and the unknown true posterior, Pθ(z|x). Applying VI to the intractable
integral from Equation 1 results in the evidence lower bound (ELBO) or variational free energy,
which can easily be derived from ﬁrst principles:

log Pθ(x) = log

Pθ(x|z)P (z)dz

(cid:90)

= log

(cid:90) Qφ(z|x)
Qφ(z|x)

Pθ(x|z)P (z)dz

≥ EQ[log Pθ(x|z)] − DKL[Qφ(z|x)||P (z)]
(cid:123)(cid:122)
(cid:125)
ELBO

(cid:124)

(2)

(3)

(4)

where we used Jensen’s inequality to transition from Equation 3 to Equation 4. The objective intro-
duced in Equation 4 induces the graphical model shown below in Figure 3.

Figure 3: Standard VAE graphical model. Gray nodes represent observed variables while white nodes represent
unobserved variables; dashed lines represent inferred variables.

VAEs typically use deep neural networks to model the approximate inference network, Qφ(z|x)
and conditional, Pθ(x|z), which are also known as the encoder and decoder networks (respec-
tively). To optimize for the parameters of these networks, VAEs maximize the ELBO (Equation
4) using Stochastic Gradient Descent [103]. By sharing the variational parameters of the encoder,
φ, across the data points (amortized inference [35]), variational autoencoders avoid per-data inner
loops typically needed by MCMC approaches.

Optimizing the ELBO in Equation 4 requires computing the gradient of an expectation over the ap-
proximate posterior, Qφ(z|x). This typically takes place through the use of the path-wise estimator
[101, 63] (originally called “push-out” [106]). The path-wise reparameterizer uses the Law of the
Unconscious Statistician (LOTUS) [42], which enables us to compute the expectation of a function
of a random variable (without knowing its distribution) if we know its corresponding sampling path
and base distribution [84]. For the typical isotropic gaussian approximate posterior, Qφ(z|x), used
in standard VAEs this can be aptly summarized by:

z ∼ Qφ(z|x) ⇔ µφ(x) + σφ(x)(cid:15), (cid:15) ∼ N (0, 1)

∇φEQφ(z|x)[log Pθ(x|z)] ⇔ E
N ((cid:15)|0, 1)
(cid:125)
(cid:123)(cid:122)
(cid:124)

base distribution

)]
[∇φ log Pθ(x| µφ(x) + σφ(x)(cid:15)
(cid:125)
(cid:124)

(cid:123)(cid:122)
sampling path

(5)
(6)

6

where Equation 5 deﬁnes the sampling procedure of our latent variable through the location-scale
transformation and Equation 6 deﬁnes the path-wise Monte Carlo gradient estimator applied on the
decoder (ﬁrst term in Equation 4). This Monte Carlo estimator enables differentiating through the
sampling process of the distribution Qφ(z|x). Note that computing the gradient of the second term
in Equation 4, ∇φDKL[Qφ(z|x)||P (z)], is possible through a closed form analytical solution for
the case of isotropic gaussian distributions.

While it is possible to extend any latent variable generative model to the lifelong setting, we choose
to build our lifelong generative models using variational autoencoders (VAEs) [63] as they provide
a mechanism for stable training; this contrasts other state of the art unsupervised models such as
Generative Adversarial Networks (GANs) [39, 61]. Furthermore, latent-variable posterior approxi-
mations are a requirement in many learning scenarios such as clustering [93], compression [92] and
unsupervised representation learning [30]. Finally, GANs can suffer from low sample diversity [28]
which can lead to compounding errors in a lifelong generative setting.

4 LIFELONG LEARNING MODEL

Algorithm 1 Data Flow

Teacher:
Sample Prior: zj ∼ P (z)
Decode: ˆxj ∼ PΘ(x|z)

Student:
Sample : xj ∼ P (ω)P (x|ω)
Encode : zj ∼ Qφ(z|x)
Decode: ˆxj ∼ Pθ(x|z)

Figure 4: Student training procedure. Left: graphical model for student-teacher model. Data generated from
the teacher model (top row) is used to augment the current training data observed by the student model (bottom
row). A posterior regularizer is also applied between Qφ(z|x) and QΦ(z|x) to enable functional regularization
(not shown, but discussed in detail in Section 4.1.1). Right: data ﬂow algorithm.

fMRI studies of the rodent [119, 53, 58] and human [110] brains have shown that previously experi-
enced sequences of events are replayed in the hippocampus during rest. These replays are necessary
for better planning [53] and memory consolidation [16]. We take inspiration from the memory con-
solidation of biological learners and introduce our model of Lifelong Generative Modeling (LGM).
We visualize the LGM student-teacher architecture in Figure 4.

The student and the teacher are both instantiations of the same base-level generative model, but
have different roles throughout the learning process. The teacher’s role is to act as a probabilistic
knowledge store of previously learned distributions, which it transfers to the student in the form
of replay and functional regularization. The student’s role is to learn the distribution over the new
task, while accommodating the learned representation of the teacher over old tasks. In the following
sections we provide detailed descriptions of the student-teacher architecture, as well as the base-
level generative model that each of them use. The base-level model uses a variant of VAEs, which
we tailor for lifelong learning and is learned by maximizing a variant of the standard VAE ELBO
from Equation 4 ; we describe this objective at end of this section.

4.1 STUDENT-TEACHER ARCHITECTURE

The top row of Figure 4 represents the teacher model. At any given time, the teacher contains a
summary of all previous distributions within the learned parameters, Φ, of the encoder QΦ(z|x),
and the learned parameters, Θ, of the decoder PΘ(x|z). We use the teacher to generate synthetic
variates, ˆxj, from these past distributions by decoding variates from the prior, zj ∼ P (z) (cid:55)→
PΘ(x|z = zj). We pass the generated (synthetic) variates, ˆxj, to the student model as a form of
knowledge transfer about the past distributions. Information transfer in this manner is known as
generative replay and our work is the ﬁrst to explore it in a VAE setting.

7

The bottom row of Figure 4 represents the student. The student is responsible for updating the
parameters, φ, of its encoder, Qφ(z|x), and θ, of its decoder Pθ(x|z). Importantly, the student re-
ceives data from both the currently observed task, as well as synthetic data generated by the teacher.
This can be formalized as xj ∼ P (ω)P (x|ω), ω ∼ Ber(π), as shown in Equation 7:

P (ω)P (x|ω) =

(cid:26)PΘ(x|z) ω = 0
ω = 1

Pi(x)

(7)

The mean, π, of the Bernoulli distribution, controls the sampling proportion of the previously learned
distributions to the current one and is set based on the number of assimilated distributions. Thus,
given i observed distributions: π = 1
i+1 . This ensures that the samples observed by the student
are representative of both the current and past distributions. Note that this does not correspond
to varying sample sizes in datasets, but merely our assumption to model each distribution with
equivalent weighting.

Once a new task is observed, the old teacher is dropped, the student model is frozen and becomes the
new teacher (φ → Φ, θ → Θ). A new student is then instantiated with the latest weights φ and θ
from the previous student (the new teacher). Due to the cyclic nature of this process, no new models
are added. This contrasts many existing state of the art deep lifelong learning methods which add an
entire new model or head-network per task (eg: [91, 94, 123]).

A crucial aspect in the lifelong learning process is to ensure that previous learning is successfully
exploited to bias current learning [126]. While the replay mechanism that we put in place ensures
that the student will observe data from all tasks, it does not ensure that previous knowledge from
the teacher is efﬁciently exploited to improve current student learning. The student model will re-
learn (from scratch) a completely new representation, which might be different than the teacher. In
order to successfully transfer knowledge between both VAE models, we rely on functional regular-
ization, which we enforce through a Bayesian update regularizer of the posteriors of both models.
Intuitively, we would like the student model’s latent outputs, zj ∼ Qφ(z|x) to be similar to la-
tent outputs of teacher model, zj ∼ QΦ(z|x), over synthetic variates generated by the teacher,
xj ∼ P (ω)P (x|ω = 0) = PΘ(x|z). In the following section, we describe the exact functional
form of this regularizer and demonstrate how it can be perceived as a natural extension of the VAE
learning objective to a sequential setting.

4.1.1 KNOWLEDGE TRANSFER VIA BAYESIAN UPDATE.

While both the student and teacher are instantiations of VAE variants, tailored for the particularities
of the lifelong setting, for the purpose of this exposition we use the standard VAE formulation. Our
objective is to learn the set of parameters [φ, θ] of the student, such that it can generate variates
from the complete distribution, P (x), described in Section 3.2. Subsuming the deﬁnition of the
augmented input data, x ∼ P (ω)P (x|ω), from Equation 7, we can deﬁne the student ELBO as:

Lθ,φ(x) = EQφ(z|x)

(cid:20)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)],

(8)

x ∼ P (ω)P (x|ω), ω ∼ Ber(π).

Rather than naively shrinking the full posterior to the prior via the KL divergence in Equation 8, we
rely on one of the core tenets of the Bayesian paradigm which states that we can always update our
posterior when given new information (yesterdays posterior is todays prior) [79]. Given this tenet,
we introduce our posterior regularizer 2:

KL[Qφ(z|x)||QΦ(z|x)], x ∼ P (x|ω = 0)
(9)
which distills the teacher’s learnt representation into the student over the generated data only. Com-
bining Equations 8 and 9, yields the objective that we can use to train the student and is described
below in Equation 10:

Lθ,φ(x) = EQφ(z|x)

log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(10)

(cid:20)

(cid:21)

2While it

is also possible to apply a similar

regularizer

to the reconstruction term,

i.e:

KL[Pθ(x|z) || PΘ(x|z)], we observed that doing so hurts performance (Appendix 10.2).

+ (1 − ω)KL[Qφ(z|x)||QΦ(z|x)],
x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

8

Note that this is not the ﬁnal objective, due to the fact that we have yet to present the VAE variant
tailored to the particularities of the lifelong setting. We will now show how the posterior regularizer
can be perceived as a natural extension of the VAE learning objective, through the lens of a Bayesian
update of the student posterior.

Lemma 1 For random variables x and z with conditionals QΦ(z|x) and Qφ(z|x), both distributed
as a categorical or gaussian and parameterized by Φ and φ respectively, the KL divergence between
the distributions is:

KL[Qφ(z|x)||QΦ(z|x)] = KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(11)

where ˆφ = f (φ, Φ) depends on the parametric form of Q, and C is only a function of the parameters,
Φ.

We prove Lemma 1 for the relevant distributions (under some mild assumptions) in Appendix 10.1.
Using Lemma 1 allows us to rewrite Equation 10 as shown below in Equation 12:
(cid:20)

Lθ,φ(x) = EQφ(z|x)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(12)

(cid:20)

+ (1 − ω)

KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(cid:21)
,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

This rewrite makes it easy to see that our posterior regularizer from Equation 10 is a standard VAE
ELBO (Equation 4) under a reparameterization of the student parameters, ˆφ = f (φ, Φ). Note that
C(Φ) is constant with respect to the student parameters, φ, and thus not used during optimization.
While the change seems minor, it omits the introduction of f (φ, Φ) which allows for a transfer of
information between models. In practice, we simply analytically evaluate KL[Qφ(z|x) ||QΦ(z|x)],
the KL divergence between the teacher and the student posteriors, instead of deriving the functional
form of f (φ, Φ) for each different distribution pair. We present Equation 12 simply as a means to
provide a more intuitive understanding of our functional regularizer.

4.2 BASE-LEVEL GENERATIVE MODEL.

While it is theoretically possible to use the vanilla VAE from Section 3.3 for the teacher and student
models, doing so brings to light a number of limitations that render it problematic for use in the con-
text of lifelong learning (visualized in Figure 5-Right). Speciﬁcally, using a standard VAE decoder,
Pθ(x|z), to generate synthetic replay data for the student is problematic due to two reasons:

1. Mixed Distributions: Sampling the continuous standard normal prior, N (0, 1), can select
a point in latent space that is in between two separate distributions, causing generation of
unrealistic synthetic data and eventually leading to loss of previously learnt distributions.
2. Undersampling: Data points mapped to the isotropic-gaussian posterior that are further
away from the prior mean will be sampled less frequently, resulting in an undersampling
of some of the constituent distributions.

To address these sampling limitations we decompose the latent variable, z, into an independent
continuous, zc ∼ Qφ(zc|x), and a discrete component, zd ∼ Qφ(zd|x), as shown in Equation 13
and visually in Figure 5-Left:

Qφ(zc, zd|x) = Qφ(zc|x)Qφ(zd|x).

(13)

The objective of the discrete component is to summarize the discriminative information of the indi-
vidual generative distributions. The continuous component on the other hand, caters for the remain-
ing sample variability (a nuisance variable [73]). Given that the discrete component can accurately
summarize the discriminative information, we can then explicitly sample from any of the past dis-
tributions, allowing us to balance the student model’s synthetic inputs with samples from all of the
previous learned distributions. We describe this beneﬁcial generative sampling property in more
detail in Section 4.2.1.

9

Figure 5: Left: Graphical model for VAE with independent discrete and continuous posterior, Qφ(zc, zd|x) =
Qφ(zc|x)Qφ(zd|x). Right: Two dimensional test variates, zj ∼ Qφ(z|x), zj ∈ R2, of a vanilla VAE
trained on MNIST. We depict the two generative shortcomings visually: 1) mixing of distributions which
causes aliasing in a lifelong setting and 2) undersampling of distributions in a standard isotropic-gaussian VAE
posterior.

Naively introducing the discrete component, zd, does not guarantee that the decoder will use it to
represent the most discriminative aspects of the modeled distribution. In preliminary experiments,
we observed that that the decoder typically learns to ignore the discrete component and simply
relies on the continuous variable, zc. This is similar to the posterior collapse phenomenon which
has received a lot of recent interest within the VAE community [99, 40]. Posterior collapse occurs
when training a VAE with a powerful decoder model such as a PixelCNN++ [127] or RNN [21, 40].
The output of the decoder, xj ∼ Pθ(x|z) can become almost independent of the posterior sample,
zj ∼ Qφ(z|x), but is still able to reconstruct the original sample by relying on its auto-regressive
property [40]. In Section 4.2.2, we introduce a mutual information regulariser which ensures that
the discrete component of the latent variable is not ignored.

4.2.1 CONTROLLED GENERATIONS.

Desired Task Conditional
P1(x) = P (x|zd = 1)
P2(x) = P (x|zd = 2)
P3(x) = P (x|zd = 3)

zc
∼ N (0, 1)
∼ N (0, 1)
∼ N (0, 1)

zd
[0, 0, 1]
[0, 1, 0]
[1, 0, 0]

Figure 6 & Table 2: FashionMNIST with L = 3 tasks: t-shirts, sandals and bag. To generate samples from
the i-th task conditional, Pi(x) = P (x|zd = i), we set zd = i, randomly sample zc ∼ N (0, 1), and run
[zc, zd] through the decoder, Pθ(x|zc, zd). Resampling zc, while keeping zd ﬁxed, enables generation of
varied samples from the task conditional. Left: Desired task conditionals. Right: Desired decoder behavior.

Given the importance of generative replay for knowledge transfer in LGM, synthetic sample gen-
eration by the teacher model needs to be representative of all the previously observed distributions
in order to prevent catastrophic forgetting. Under the assumption that zd accurately captures the
underlying discriminativeness of the individual distributions and through the deﬁnition of the LGM
generative process, shown in Equation 14:

PΘ(x|zd, zc), zc ∼ N (0, 1), zd ∼ Cat(1/L),

we can control generations by setting a ﬁxed value, zd = i, and randomly sampling the continuous
prior, zc ∼ N (0, 1). This is possible because the trained decoder approximates the task conditional
from Section 3.2:

(14)

(15)

P (x|zd = i) = Pi(x) ≈

PΘ(x|zc, zd = i)P (zc)
QΦ(zc|x)

10

where sampling the true task conditional, Pi(x), can be approximated by sampling zc ∼ P (zc) =
N (0, 1), keeping zd ﬁxed, and decoding the variates as shown in Equation 16 below:

ˆx ∼ PΘ(x|zc ∼ N (0, 1), zd = i).

(16)

We provide a simple example of our desired behavior for three generative tasks, L = 3, using Fash-
ion MNIST in Figure 6 and Table 2 above. The assumption made up till now is that zd accurately
captures the discriminative aspects of each distribution. However, there is no theoretical reason for
the model to impose this constraint on the latent variables. In practice, we often observe that the
decoder Pθ(x|zc, zd) ignores zd due to the much richer representation of the continuous variable,
zc. In the following section we introduce a mutual information constraint that encourages the model
to fully utilize zd.

4.2.2

INFORMATION RESTRICTING REGULARIZER

As eluded to in the previous section, the synthetic samples observed by the student model need to be
representative of all previous distributions. In order to control sampling via the process described in
Section 4.2.1, we need to enforce that the discrete variable, zd, carries the discriminative informa-
tion about each distribution. Given our graphical model from Figure 4-Left, we observe that there
are two ways to accomplish this: maximize the information content between the discrete random
variable, zd and the decoded ˆx, or minimize the information content between the continuous vari-
able, zc and the decoded ˆx. Since our graphical model and underlying network does not contain skip
connections, information from the input, x, has to ﬂow through the latent variables z = [zc, zd] to
reach the decoder. While both formulations can theoretically achieve the same objective, we ob-
served that in practice, minimizing I(ˆx, zc) provided better results. We believe the reason for this
is that minimizing I(ˆx, zc) provides the model with more subtle gradient information in contrast to
maximizing I(ˆx, zd) which receives no gradient information when the value of the k-th element of
the categorical sample is 1. We now formalize our mutual information regularizer, which we derive
from ﬁrst principles in Equation 17:

Ex∼Pi(x)[I(ˆx, zc)] = Ex∼Pi(x)[H(zc) − H(zc|ˆx)]

(17)

= Ex∼Pi(x)Ezc∼Qφ(zc|x)
(cid:123)(cid:122)
Ex∼Pi(x)[H(zc)]
+

(cid:124)

(cid:20)

(cid:21)
− log Qφ(zc|x)

(cid:125)

Ex∼Pi(x)E(zd,zc)∼Qφ(zc,zd|x)
(cid:124)

(cid:21)
(cid:20)
Eˆx∼Pθ (ˆx|zc,zd) log Qφ(zc|ˆx)
,
(cid:123)(cid:122)
Ex∼Pi(x)[−H(zc|ˆx)]

(cid:125)

where we use the independence assumption of our posterior from Equation 13 and the fact that the
expectation of a constant is the constant. This regularizer has parallels to the regularizer in InfoGAN
[18]. In contrast to InfoGAN, VAEs already estimate the posterior Qφ(zc|x) and thus do not need
the introduction of any extra parameters φ for the approximation. In addition [47] demonstrated that
InfoGAN uses the variational bound (twice) on the mutual information, making its interpretation
unclear from a theoretical point of view. In contrast, our regularizer has a clear interpretation: it
restricts information through a speciﬁc latent variable within the computational graph. We observe
that this constraint is essential for empirical performance of our model and empirically validate this
in our ablation study in Experiment 7.2.

4.3 LEARNING OBJECTIVE

The ﬁnal learning objective for each of the student models is the maximization of the sequential
VAE ELBO (Equation 10), coupled with generative replay (Equation 7)and the mutual information
regularizer, I(ˆx, zc), (Equation 17):

11

Lθ,φ(x) = EQφ(zc,zd|x)
(cid:124)

(cid:20)

(cid:21)
log Pθ(x|zc, zd)

− KL[Qφ(zc, zd|x)||P (zc, zd)]

(18)

(cid:125)

(cid:124)

(cid:123)(cid:122)
VAE ELBO
+ (1 − ω)KL[Qφ(zc, zd|x)||QΦ(zc, zd|x)]
(cid:125)
(cid:123)(cid:122)
Posterior Consistency Regularizer
− λI(ˆx, zc)
(cid:125)

(cid:123)(cid:122)
Mutual Information

(cid:124)

,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

The λ hyper-parameter controls the importance of the information gain regularizer. Too large a
value for λ causes a lack of sample diversity, while too small a value causes the model to not use
the discrete latent distribution. We did a random hyperparameter search and determined λ = 0.01
to be a reasonable choice for all of our experiments. This is in line with the λ used in InfoGAN
[18] for continuous latent variables. We empirically validate the necessity of both terms proposed in
Equation 18 in our ablation study in Experiment 7.2. We also validate the beneﬁt of the latent vari-
able factorization in Experiment 7.1. Before delving into the experiments, we provide a theoretical
analysis of computational complexity induced by our model and objective (Equation 18) in Section
4.4 below.

4.4 COMPUTATIONAL COMPLEXITY

We deﬁne the computational complexity of a typical VAE encoder and decoder as O(E)
and O(D) correspondingly;
internally these are dominated by the matrix-vector products
which take approximately LO(n2) for L layers. We also deﬁne the cost of applying the loss
function as O(K) + O(R), where O(K) is the cost of evaluating the KL divergence from
the ELBO (Equation 4) and O(R) the cost for evaluating the reconstruction term. Given
these deﬁnitions, we can summarize LGM’s computation complexity as follows in Equation 19:

O(D)
(cid:124) (cid:123)(cid:122) (cid:125)
teacher
generations

+ O(E) + O(D)
(cid:123)(cid:122)
(cid:125)
student encode
+ decode

(cid:124)

+ O(K) + O(R)
(cid:125)
(cid:123)(cid:122)
vae loss

(cid:124)

+ O(K)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
regularizer

+ O(K) + O(E)
(cid:123)(cid:122)
(cid:125)
mutual info

(cid:124)

= 2[O(D) + O(E)] + 3[O(K)] + O(R),

(19)

where we introduce increased computational complexity due to teacher generations, the cost of the
posterior regularizer, and the mutual information terms; the latter of which necessitates an extra
encode operation, O(E). The computational complexity is still dominated by the matrix-vector
product from evaluating forward functionals of the neural network. These operations can easily be
amortized through parallelization on modern GPUs and typical experiments do not directly scale
as per Equation 19. In our most demanding experiment (Experiment 6.6), we observe an average
empirical increase of 13.53 seconds per training epoch and 6.3 seconds per test epoch.

5 REVISITING STATE OF THE ART METHODS.

In this section we revisit some of the state of the art methods from Section 2. We begin by providing
a mathematical description of the differences between EWC [65], VCL [91] and LGM and follow it
up with a discussion of VASE [1] and their extensions of our work.

EWC and VCL: Our posterior regularizer, KL[Qφ(z|x)||QΦ(z|x)], affects the same parameters,
φ, as parameter regularizer methods such as EWC and VCL. However, rather than assuming a func-
tional form for the parameter posterior, P (φ|x), our method regularizes the output latent distribu-
tion Qφ(z|x). EWC and VCL, both make the assumption that P (φ|x) is distributed as an isotropic
gaussian3. This allows the use of the Fisher Information Matrix (FIM) in a quadratic parameter reg-
ularizer in EWC, and an analytical KL divergence of the posterior in VCL. This is a very stringent

3VCL assumes an isotropic gaussian variational form vs. EWC which directly assumes the parametric form

on P (φ|x).

12

requirement for the parameters of a neural network and there is active research in Bayesian neural
networks that attempts to relax this constraint [74, 75, 80].

EWC minφ d[P (φ|x)||P (Φ|x)]
2 (φ − Φ)T F (φ − Φ)

≈ γ

LGM ( Isotropic Gaussian Posterior ) minφ d[Qφ(z|x)||QΦ(z|x)]
|ΣΦ|
|Σφ|

Φ Σφ) + (µΦ − µφ)T Σ−1

Φ (µΦ − µφ) − C + log

(cid:20)
tr(Σ−1

(cid:18)

= 0.5

(cid:19)(cid:21)

In the above table we examine the distance metric d, used to minimize the effects of catastrophic
inference in both EWC and LGM. While our method can operate over any distribution that has
a tractable KL-divergence, for the purposes of demonstration we examine the simple case of an
isotropic gaussian latent-variable posterior. EWC directly enforces a quadratic constraint on the
model parameters φ, while our method indirectly affects the same parameters through a regulariza-
tion of the posterior distribution Qφ(z|x). For any given input variate, xj, LGM allows to model to
freely change its internal parameters, φ; it does so in a non-linear4 way such that the analytical KL
shown above is minimized.

VASE : The recent work of Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies (VASE) [1] extend upon our work [1, p. 7], but take a more empirical route by
incorporating a classiﬁcation-based heuristic for their posterior distribution. In contrast, we show
(Section 4.1.1) that our objective naturally emerges in a sequential learning setting for VAEs, allow-
ing us to infer the discrete posterior, Qφ(zd|x) in an unsupervised manner. Due to the incorporation
of direct supervised class information [1] also observe that regularizing the decoding distribution
Pθ(x|z) aids in the learning process, something that we observe to fail in a purely unsupervised
generative setting (Appendix Section 10.2). Finally, in contrast to [1], we include an information
restricting regularizer (Section 4.2.2) which allows us to directly control the interpretation and ﬂow
of information of the learnt latent variables.

6 EXPERIMENTS

We evaluate our model and the baselines over standard datasets used in other state of the art life-
long / continual learning literature [91, 136, 112, 57, 65, 94]. While these datasets are simple in
a traditional classiﬁcation setting, transitioning to a lifelong-generative setting scales the problem
complexity substantially. We evaluate LGM on a set of progressively more complicated tasks (Sec-
tion 6.2) and provide comparisons against baselines [91, 136, 65, 29, 63] using a set of standard
metrics (Section 6.1). All network architectures and other optimization details for our LGM model
are provided in Appendix Section 10.3 as well our open-source git repository [98].

6.1 PERFORMANCE METRICS

To validate the beneﬁt of LGM in a lifelong setting we explore three main performance dimensions:
the ability for the model to reconstruct and generate samples from all previous tasks and the ability
to learn a common representation over time, thus reducing learning sample complexity. We use three
main quantitative performance metrics for our experiments: the log-likelihood importance sample
estimate [15, 91], the negative test ELBO, and the Frechet distance metric [44]. In addition, we also
provide two auxiliary metrics to validate the beneﬁts of LGM in a lifelong setting: training sample
complexity and wall clock time per training and test epoch.

To fairly compare models with varying latent variable conﬁgurations, one solution is to marginal-
ize out the latents, z, during model evaluation / test time: (cid:82)
k=1 Pθ(x|z = zk).
This is realized in practice by using a Monte Carlo approximation (typically K=5000) and is com-
monly known as the importance sample (IS) log-likelihood estimate [15, 91]. As latent variable
and model complexity grows, this estimate tends to become noisier and intractable to compute. For
our experiments we use this metric only for the FashionMNIST and MNIST datasets as computing
one estimate over 10,000 test samples for a complex model takes approximately 35 hours on a K80
GPU.

z Pθ(x|z)dz ≈ (cid:80)K

In contrast to the IS log-likelihood estimate, the negative test ELBO (Equation 4) is only applicable
when comparing models with the same latent variable conﬁgurations; it is however much faster to

4This is because the parameters of the distribution are modeled by a deep neural network.

13

compute. The negative test ELBO provides a lower bound to the test log-likelihood of the true data
distribution under the assumed latent variable conﬁguration. One crucial aspect missing from both
these metrics is an evaluation of generation quality. We resolve this by using the Frechet distance
metric [44] and qualitative image samples.

The Frechet distance metric allows us to quantify the quality and diversity of generated samples by
using a pre-trained classiﬁer model to compare the feature statistics (generally under a Gaussian-
ity assumption) between synthetic generated samples and samples drawn from the test set. If the
Frechet distance between these two distributions is small, then the generative model is said to be
generating realistic images. The Frechet distance between two gaussians (produced by evaluating
latent embeddings of a classiﬁer model) with means mtest, mgen with corresponding covariances
Ctest, Cgen is:

||mtest − mgen||2

2 + T r(Ctest + Cgen − 2[CtestCgen]0.5).

(20)

While the Frechet distance, negative ELBO and IS log-likelihood estimate provide a glimpse into
model performance, there exists no conclusive metric that captures the quality of unsupervised gen-
erative models [124, 108] and active research suggests a direct trade-off between perceptual quality
and model representation [8]. Thus, in addition to the metrics described above, we also provide
qualitative metrics in the form of test image reconstructions and image generations. We summarize
all used performance metrics in Table 3 below:

Negative ELBO

Deﬁnition

Equation 4.

Purpoose
Quantitative metric on
likelihood / reconstructions.
Quantitative metric on
density estimate.
Quantitative metric on generations.

Lower is better?

yes

Negative Log-Likelihood

Frechet Distance
Test Reconstructions
Generations
#Training Samples

5000 (latent) sample Monte
Carlo estimate of Equation 4.
Equation 20.
yes
Pθ(x|zc ∼ Qφ(zc|x), zd ∼ Qφ(zd|x)) Qualitative view of reconstructions. N/A
Pθ(x|zd ∼ Cat(1/L), zc ∼ N (0, 1)).
N/A
# real training samples used for task i.
yes

Qualitative view of generations.
Sample Complexity.

yes

Table 3: Summary of different performance metrics.

6.2 DATA FLOW

Figure 7: Visual examples of training and test task sequences (top to bottom) for the datasets used to validate
LGM. The training set only consists of samples from the current task while the test set is a cumulative union
of the current task, coupled with all previous tasks. The permuted MNIST tasks uses {G1, ...GL−1} different
ﬁxed permutation matrices to create 4 auxiliary datasets.

14

In Figure 7 we list train and test variates depicting the data ﬂow for each of the problems that we
model. Due to the relaxing the need to preserve data in a lifelong setting, the train task sequence ob-
serves a single dataset, Dtr
i , at a time, without access to any previous, Dtr
<i. The corresponding test
dataset consists of a union (∪ operator) of the current test dataset, Dte
i , merged with all previously
observed test datasets, ˆD
i ∪ Dte

i−1 ∪ ... ∪ Dte
1 .

te
i = Dte

MNIST / Fashion MNIST: For the MNIST and Fashion MNIST problems, we observe a single
MNIST digit or fashion object (such as shirts) at a time. Each training set consists of 6000 training
samples and 1000 test samples. These samples are originally extracted from the full training and
test datasets which consist of 60,000 training and 10,000 test samples.

Permuted MNIST: this problem differs from the MNIST problem described above in that we use
the entire MNIST dataset at each task. After observing the ﬁrst task, which is the standard MNIST
dataset, each subsequent task differs through the application of a ﬁxed permutation matrix Gi on
the entire MNIST dataset. The test task sequence differs from the training task sequence in that we
simply use the corresponding full train and test MNIST datasets (with the appropriate application of
Gi).

Celeb-A: We split the CelebA dataset into four individual distributions using the features: bald,
male, young and eye-glasses. As with the previous problems, we treat each subset of data as an
individual distribution, and present our model samples from a single distribution at a time. This
presents a real world scenario as the samples per distribution varies drastically from only 3,713
samples for the bald distribution, to 126,788 samples for young. In addition speciﬁc samples can
span one or more of these distributions.

SVHN to MNIST: in this problem, we transition from fully observing the centered SVHN [89]
dataset to observing the MNIST dataset. We treat all samples from SVHN as being generated by
one distribution P1(x) and all the MNIST 5 samples as generated by another distribution P2(x)
(irrespective of the speciﬁc digit). At inference, the model is required to reconstruct and generate
from both datasets.

6.3 SITUATING AGAINST STATE OF THE ART LIFELONG LEARNING MODELS.
To situate LGM against other state of the art methods in lifelong learning we use the sequential
FashionMNIST and MNIST datasets described earlier in Section 6.2 and the data ﬂow diagram in
Figure 7. We contrast our LGM model against VCL [91], VCL without a task speciﬁc head network,
SI [136], EWC [65], Laplace propagation [29], a full batch VAE trained jointly on all data and a
standard naive sequential VAE without any catastrophic forgetting prevention strategy in Figures 8
and 9 below. The full batch VAE presents the upper-bound performance and all lifelong learning
models typically under-perform this model by the ﬁnal learning task. For the baselines, we use
the generously open sourced code [90] by the VCL authors, using the optimal hyper-parameters
speciﬁed for each model. We begin by evaluating the 5000 sample Monte Carlo estimate of the
log-likelihood of all compared models in Figure 8 below:

Figure 8: IS log-likelihood (mean ± std) × 5. Left: Fashion MNIST. Right: MNIST.

5MNIST was resized to 32x32 and converted to RBG to make it consistent with the dimensions of SVHN.

15

Even though each trial was repeated ﬁve times (each), we observe large increases in the estimates
at a few critical points. After further inspection, we determined the large magnitude increases were
due to the model observing a drastically different distribution at that point. We overlay the graphs
with an example variate for of the magnitude spikes. In the case of FashionMNIST for example,
the model observes its ﬁrst shoe distribution at i = 6; this contrasts the previously observed items
which were mainly clothing related objects. Interestingly we observe that LGM has much smoother
performance across tasks. We posit this is because LGM does not constrain its parameters, and
instead enforces the same input-output mapping through functional regularization.

Figure 9: Final model, i = 10, generation and reconstructions for MNIST and FashionMNIST. The LGM
model presents competitive performance for both generations and reconstructions, while not preserving any
past data nor past models.

16

Since one of the core tenets of lifelong learning is to reduce sample complexity over time, we use
this experiment to validate if LGM does in fact achieve this objective. Since all LGM models are
trained with an early-stopping criterion, we can directly calculate the number of samples used for
each learning task using the stopping epoch and mean, π of the Bernoulli sampling distribution of
the student model. In Figure 10 we plot the number of true samples and the number of synthetic
samples used by a model until it satisﬁed its early-stopping criterion. We observe a steady decrease
in the number of real samples used over time, validating LGMs advantage in a lifelong setting.

Figure 10: FashionMNIST sample complexity. Left: Synthetic training samples used till early-stopping. Right:
Real samples used till early-stopping.

6.4 DIVING DEEPER INTO THE SEQUENCE.
Rather than only visualizing the ﬁnal model’s qualitative results as in Figure 9, we provide quali-
tative results for model performance over time for the PermutedMNIST experiment in Figure 11.
This allows us to visually observe lifelong model performance over time. In this experiment, we
focus our efforts on EWC and LGM and visualize model (test) reconstructions starting from the
second learning task, G1D, till the ﬁnal G4D. The EWC-VAE variant that we use as a baseline
has the same latent variable conﬁguration as our model, enabling the usage of the test ELBO as a
quantitative metric for comparison. We use an unpermuted version of the MNIST dataset, D, as our
ﬁrst distribution, P1(x), as it allows us to visually asses the degradation of reconstructions. This is a
common setup utilized in continual learning [65, 136] and we extend it here to the density estimation
setting.

Figure 11: Top row: test-samples; bottom row: reconstructions. We visualize an increasing number of accu-
mulated distributions from left to right. (a) Lifelong VAE model (b) EWC VAE model.

17

Both models exhibit a different form of degradation: EWC experiences a more destructive form of
degradation as exempliﬁed by the salt-and-pepper noise observed in the ﬁnal dataset reconstruction
at G4D. LGM on the hand experiences a form of Gaussian noise as visible in the corresponding
ﬁnal dataset reconstruction. In order to numerically quantify this performance we analyze the log-
Frechet distance and negative ELBO below in Figure 12, where we contrast the LGM to EWC, a
batch VAE (full-vae in graph), an upto-VAE that observes all training data up to the current distri-
bution and a vanilla sequential VAE (vanilla). We examine a variety of different convolutional and
dense architectures and present the top performing models below. We observe that LGM drastically
outperforms EWC and the baseline naive sequential VAE in both metrics.

Figure 12: PermutedMNIST (a) negative Test ELBO and (b) log-Frechet distance.

6.5 LEARNING ACROSS COMPLEX DISTRIBUTIONS

Figure 13: (a) Reconstructions of test samples from SVHN[left] and MNIST[right]; (b) Decoded samples
ˆx ∼ Pθ(x|zd, zc) based on linear interpolation of zc ∈ R2 with zd = [0, 1]; (c) Same as (b) but with
zd = [1, 0].

The typical assumption in lifelong learning is that the sequence of observed distributions are related
[125] in some manner. In this experiment we relax this constraint by learning a common model
between the colored SVHN dataset and the binary MNIST dataset. While semantically similar to
humans, these datasets are vastly different, as one is based on RGB images of real world house
numbers and the other of synthetically hand-drawn digits. We visualize examples of the true test
inputs, x, and their respective reconstructions, ˆx, from the ﬁnal lifelong model in ﬁgure 13(a). Even
though the only true data the ﬁnal model received for training was the MNIST dataset, it is still able
to reconstruct the SVHN data observed previously. This demonstrates the ability of our architecture
to transition between complex distributions while still preserving the knowledge learned from the
previously observed distributions.

Finally, in ﬁgure 13(b) and 13(c) we illustrate the data generated from an interpolation of a 2-
dimensional continuous latent space, zc ∈ R2. To generate variates, we set the discrete categorical,
zd, to one of the possible values {[0, 1], [1, 0]} and linearly interpolate the continuous zc over the
range [−3, 3]. We then decode these to obtain the samples, ˆx ∼ Pθ(x|zd, zc). The model learns

18

a common continuous structure for the two distributions which can be followed by observing the
development in the generated samples from top left to bottom right on both ﬁgure 13(b) and 13(c).

6.6 VALIDATING EMPIRICAL SAMPLE COMPLEXITY USING CELEB-A

We iterate the Celeb-A dataset as described in the data ﬂow diagram (Figure 7) and use this learn-
ing task to explore qualitative and quantitative generations, as well as empirical real world time
complexity (as described in Section 4.4) on modern GPU hardware. We train a lifelong model and
a typical VAE baseline without catastrophic forgetting mitigation strategies and evaluate the ﬁnal
model’s generations in Figure 14. As visually demonstrated in Figure 14-Left, the lifelong model is
able to generate instances from all of the previous distributions, however the baseline model catas-
trophically forgets (Figure 14-Right) and only generates samples from the eye-glasses distribution.
This is also reinforced by the log-Frechet distance shown in Figure 15.

Figure 14: Left: Sequential generations for Celeb-A from the ﬁnal lifelong model for bald, male, young and
eye-glasses (left to right). Right: (random) generations by the ﬁnal baseline VAE model.

We also evaluate the wall-clock time in seconds (Table 4) for the lifelong model and the baseline-vae
for the 44,218 samples of the male distribution. We observe that the lifelong model does not add a
signiﬁcant overhead, especially since the baseline-vae undergoes catastrophic forgetting (Figure 14
Right) and completely fails to generate samples from previous distributions. Note that we present
the number of parameters and other detailed model information in our code and Appendix 10.3.

44,218 male samples
training-epoch (s)
testing-epoch (s)

baseline-VAE
43.1 +/- 0.6
9.79 +/- 0.12

Lifelong
56.63 +/- 0.28
16.09 +/- 0.01

Table 4: Mean & standard deviation wall-clock for one epoch of
male distribution of Celeb-A.

Figure 15: Celeb-A log-Frechet distance of lifelong vs. naive baseline VAE model without catastrophic miti-
gation strategies over the four distributions. Listed on the right is the time per epoch (in seconds) for an epoch
of the corresponding models.

7 ABLATION STUDIES

In this section we independently validate the beneﬁt of each of the newly introduced components
to the learning objective proposed in Section 4.3. In Experiment 7.1 we demonstrate the beneﬁt
of the discrete-continuous posterior factorization introduced in Section 4.2.1. Then in Experiment

19

7.2, we validate the necessity of the information restricting regularizer (Section 4.2.2) and posterior
consistency regularizer (Section 4.1.1).

7.1 LINEAR SEPARABILITY OF DISCRETE AND CONTINUOUS POSTERIOR

Figure 16: Left: Graphical model depicting classiﬁcation using pretrained VAE, coupled with a linear classiﬁer,
fθlin : z (cid:55)→ y. Right: Linear classiﬁer accuracy on the Fashion MNIST test set for a varying range of latent
dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024] and distributions.

In order to validate that the (independent) discrete and continuous latent variable posterior,
Qφ(zd, zc|x), aids in learning a better representation, we classify the encoded posterior sample
using a simple linear classiﬁer fθlin : z (cid:55)→ y, where y corresponds to the categorical class predic-
tion. Higher (linear) classiﬁcation accuracies demonstrate that the the VAE is able to learn a more
linearly separable representation. Since the latent representation of VAEs are typically used in aux-
iliary tasks, learning such a representation is useful in downstream tasks. This is a standard method
to measure posterior separability and is used in methods such as Associative Compression Networks
[41].

We use the standard training set of FashionMNIST [135] (60,000 samples) to train a standard VAE
with a discrete only (disc) posterior, an isotropic-gaussian only (gauss) posterior, a bernoulli only
(bern) posterior and ﬁnally the proposed independent discrete and continuous (disc+gauss) posterior
presented in Section 4.2.1. For each different posterior reparameterization, we train a set of VAEs
with varying latent dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024]. In the case of the disc+gauss
model we ﬁx the discrete dimension, |zd| = 10 and vary the isotropic-gaussian dimension to match
the total required dimension. After training each VAE, we proceed to use the same training data to
train a linear classiﬁer on the encoded posterior sample, z ∼ Qφ(z|x).

In Figure 16 we present the mean and standard deviation linear test classiﬁcation accuracies of each
set of the different experiments. As expected, the discrete only (disc) posterior performs poorly due
to the strong restriction of mapping an entire input sample to a single one-hot vector. The isotropic-
gaussian (gauss) and bernoulli (bern) only models provide a strong baseline, but the combination
of isotropic-gaussian and discrete posteriors (disc+gauss) performs much better, reaching an upper-
bound (linear) test-classiﬁcation accuracy of 87.1%. This validates that the decoupling of latent
represention presented in Section 4.2.1 aids in learning a more meaningful, separable posterior.

7.2 VALIDATING THE MUTUAL INFORMATION AND POSTERIOR CONSISTENCY

REGULARIZERS.

In order to independently evaluate the beneﬁt of our proposed Bayesian update regularizer (Sec-
tion 4.1.1) and the mutual information regularizer proposed in (Section 4.2.1) we perform an ab-
lation study using the MNIST data ﬂow sequence from Figure 7. We evaluate three scenarios: 1)
with posterior consistency and mutual information regularizers, 2) only posterior consistency and
3) without both regularizers. We observe that both components are necessary in order to gener-
ate high quality samples as evidenced by the negative test ELBO in Figure 17-(a) and the corre-

20

Figure 17: MNIST Ablation: (a) negative test ELBO. (b) Sequentially generated samples by setting zd and
sampling zc ∼ N (0, 1) (Section 4.2.1) with consistency + mutual information (MI). (c) Sequentially generated
samples with no consistency + no mutual information (MI).

sponding generations in Figure 17-(b-c). The generations produced without the information gain
regularizer and consistency in Figure 17-(c) are blurry. We attribute this to: 1) uniformly sam-
pling the discrete component is not guaranteed to generate samples representative samples from
P<i(x) and 2) the decoder, PΘ(x|zd, zc), relays more information through the continuous compo-
nent, PΘ(x|zd, zc) = PΘ(x|zc), causing catastrophic forgetting and posterior collapse [4].

8 LIMITATIONS

While LGM presents strong performance, it fails to completely solve the problem of lifelong gen-
erative modeling and we see a slow degradation in model performance over time. We attribute this
mainly to the problem of poor VAE generations that compound upon each other (also discussed
below). In addition, there are a few poignant issues that need to be resolved in order to achieve
an optimal (in terms of non-degrading Frechet distance / -ELBO) unsupervised generative lifelong
learner:

Distribution Boundary Evaluation: The standard assumption in current lifelong / continual learn-
ing approaches [91, 136, 112, 57, 65, 94] is to use known, ﬁxed distributions instead of learning
the distribution transition boundaries. For the purposes of this work, we focus on the accumulation
of distributions (in an unsupervised way), rather than introduce an additional level of indirection
through the incorporation of anomaly detection methods that aid in detecting distributional bound-
aries.

Blurry VAE Generations: VAEs are known to generate images that are blurry in contrast to GAN
based methods. This has been attributed to the fact that VAEs don’t learn the true posterior and
make a simplistic assumption regarding the reconstruction distribution Pθ(x|z) [4, 97]. While there
exist methods such as ALI [27] and BiGAN [26], that learn a posterior distribution within the GAN
framework, recent work has shown that adversarial methods fail to accurately match posterior-prior
distribution ratios in large dimensions [104].

Memory: In order to scale to a truly lifelong setting, we posit that a learning algorithm needs a
global pool of memory that can be decoupled from the learning algorithm itself. This decoupling
would also allow for a principled mechanism for parameter transfer between sequentially learnt
models as well a centralized location for compressing non-essential historical data. Recent work

21

such as the Kanerva Machine [133] and its extensions [134] provide a principled way to do this in
the VAE setting.

9 CONCLUSION

In this work we propose a novel method for learning generative models over a lifelong setting. The
principal assumption for the data is that they are generated by multiple distributions and presented
to the learner in a sequential manner. A key limitation for the learning process is that the method
has no access to any of the old data and that it shall distill all the necessary information into a
single ﬁnal model. The proposed method is based on a dual student-teacher architecture where the
teacher’s role is to preserve the past knowledge and aid the student in future learning. We argue for
and augment the standard VAE’s ELBO objective by terms helping the teacher-student knowledge
transfer. We demonstrate the beneﬁts this augmented objective brings to the lifelong learning setting
using a series of experiments. The architecture, combined with the proposed regularizers, aid in
mitigating the effects of catastrophic interference by supporting the retention of previously learned
knowledge.

REFERENCES

[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. Life-
long disentangled representation learning with cross-domain latent homologies. In Advances
in Neural Information Processing Systems, pages 9895–9905, 2018.

[2] W.-K. Ahn and W. F. Brewer. Psychological studies of explanationbased learning. In Investi-

gating explanation-based learning, pages 295–316. Springer, 1993.

[3] W.-K. Ahn, R. J. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one ex-
ample: Psychological evidence for explanation-based learning. Technical report, Coordinated
Science Laboratory, University of Illinois at Urbana-Champaign, 1987.

[4] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken

elbo. In International Conference on Machine Learning, pages 159–168, 2018.

[5] J. R. Anderson and G. H. Bower. Human associative memory. Psychology press, 2014.
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align

and translate. ICLR, 2015.

[7] H. R. Blackwell. Contrast thresholds of the human eye. JOSA, 36(11):624–643, 1946.
[8] Y. Blau and T. Michaeli. The perception-distortion tradeoff.

In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018.

[9] A. Blum. On-line algorithms in machine learning. In Online algorithms, pages 306–325.

Springer, 1998.

[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning, pages 1613–1622, 2015.
[11] L. Bottou. Online learning and stochastic approximations. On-line learning in neural net-

works, 17(9):142, 1998.

[12] L. Bottou and Y. L. Cun. Large scale online learning. In Advances in neural information

processing systems, pages 217–224, 2004.

[13] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.

[14] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational
bayes. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 1727–1735, 2013.

[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. ICLR, 2016.
[16] M. F. Carr, S. P. Jadhav, and L. M. Frank. Hippocampal replay in the awake state: a potential
substrate for memory consolidation and retrieval. Nature neuroscience, 14(2):147, 2011.

[17] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[18] X. Chen, X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.

22

In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc., 2016.
[19] Z. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and

big data. In International Conference on Machine Learning, pages 703–711, 2014.

[20] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning, 10(3):1–145, 2016.

[21] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent
variable model for sequential data. In Advances in neural information processing systems,
pages 2980–2988, 2015.

[22] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International Conference on Machine Learning, pages 1282–1289,
2019.

[23] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson. Human photoreceptor

topography. Journal of comparative neurology, 292(4):497–523, 1990.

[24] P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and

related ﬁelds, 2(4):555–581, 1996.

[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

[26] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782, 2016.

[27] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and

A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
[28] E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances

in Neural Information Processing Systems, pages 708–718, 2018.

[29] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace propagation. In Advances in Neural

Information Processing Systems, pages 441–448, 2004.

[30] L. Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories.
In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141.
IEEE, 2003.

[31] G. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable.

In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1565–1574. ACM, 2016.

[32] A. Fiat and G. J. Woeginger. Online algorithms: The state of the art, volume 1442. Springer,

1998.

[33] T. Furlanello, J. Zhao, A. M. Saxe, L. Itti, and B. S. Tjan. Active long term memory networks.

arXiv preprint arXiv:1606.02355, 2016.

[34] A. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities.

Journal of the American statistical association, 85(410):398–409, 1990.

[35] S. Gershman and N. Goodman. Amortized inference in probabilistic reasoning. In Proceed-

ings of the Cognitive Science Society, volume 36, 2014.

[36] Z. Ghahramani and H. Attias. Online variational bayesian learning.

In Slides from talk

presented at NIPS workshop on Online Learning, 2000.

[37] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats, volume 9, pages 249–256, 2010.

[38] R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric bayesian mix-
ture models. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2008.

[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.

[40] A. G. A. P. Goyal, A. Sordoni, M.-A. Cˆot´e, N. R. Ke, and Y. Bengio. Z-forcing: Training
stochastic recurrent networks. In Advances in neural information processing systems, pages
6713–6723, 2017.

[41] A. Graves, J. Menick, and A. v. d. Oord. Associative compression networks. arXiv preprint

arXiv:1804.02476, 2018.

23

[42] G. Grimmett, G. R. Grimmett, D. Stirzaker, et al. Probability and random processes. Oxford

university press, 2001.

[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6629–6640, 2017.

[45] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. stat,

[46] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1050:9, 2015.

1780, 1997.

[47] F. Huszar. Infogan: using the variational bound on mutual information (twice), Aug 2016.
[48] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reduc-

ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[49] V. Jain and E. Learned-Miller. Online domain adaptation of a pre-trained cascade of classi-
ﬁers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
577–584. IEEE, 2011.

[50] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Interna-

tional Conference on Learning Representations, 2017.

[51] H. Jeffreys. An invariant form for the prior probability in estimation problems. In Proceedings
of the Royal Society of London a: mathematical, physical and engineering sciences, volume
186, pages 453–461. The Royal Society, 1946.

[52] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper-
vised and generative approach to clustering. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, pages 1965–1972. AAAI Press, 2017.

[53] A. Johnson and A. D. Redish. Neural ensembles in ca3 transiently encode paths forward of

the animal at a decision point. Journal of Neuroscience, 27(45):12176–12189, 2007.

[54] M. I. Jordan. Artiﬁcial neural networks. chapter Attractor Dynamics and Parallelism in a
Connectionist Sequential Machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
[55] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[56] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering, 82(1):35–45, 1960.

[57] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learn-

ing. arXiv preprint arXiv:1710.10368, 2017.

[58] M. P. Karlsson and L. M. Frank. Awake replay of remote experiences in the hippocampus.

Nature neuroscience, 12(7):913, 2009.

[59] M. Karpinski and A. Macintyre. Polynomial bounds for vc dimension of sigmoidal and
general pfafﬁan neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.

[60] I. Katakis, G. Tsoumakas, and I. Vlahavas. Incremental clustering for the classiﬁcation of

[61] H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine

concept-drifting data streams.

Learning, pages 2654–2663, 2018.

[62] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. 2015.
[63] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
[64] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

[65] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–
1105, 2012.

[67] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

24

[68] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning, pages
1558–1566, 2016.

[69] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis. Continual classiﬁcation learning

using generative models. CoRR, abs/1810.10612, 2018.

[70] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks, 3361(10):1995, 1995.

[71] Z. Li and D. Hoiem. Learning without forgetting.

In European Conference on Computer

Vision, pages 614–629. Springer, 2016.

[72] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang,
In Proceedings of the European

and K. Murphy. Progressive neural architecture search.
Conference on Computer Vision (ECCV), pages 19–34, 2018.

[73] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder.

ICLR, 2016.

[74] C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[75] C. Louizos and M. Welling. Multiplicative normalizing ﬂows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 2218–2227. JMLR. org, 2017.

[76] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation

of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[77] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders.

arXiv preprint arXiv:1511.05644, 2015.

[78] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.
[79] J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling
on streams. In Advances in Neural Information Processing Systems, pages 1153–1161, 2015.
[80] A. Mishkin, F. Kunstner, D. Nielsen, M. Schmidt, and M. E. Khan. Slang: Fast structured
covariance approximations for bayesian deep learning with natural gradient. In Advances in
Neural Information Processing Systems, pages 6245–6255, 2018.

[81] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the ACM, 61(5):103–
115, 2018.

[82] T. M. Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research, 1980.

[83] T. M. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. D. Mishra,
M. Gardner, B. Kisiel, J. Krishnamurthy, et al. Never-ending learning. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[84] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in

machine learning. CoRR, abs/1906.10652, 2019.

[85] E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In International Confer-

ence on Learning Representations (ICLR), 2017.

[86] R. M. Neal. Bayesian Learning For Neural Networks. PhD thesis, University of Toronto,

[87] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte

1995.

carlo, 2(11):2, 2011.

[88] W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically exact, embarrassingly parallel
In N. L. Zhang and J. Tian, editors, Proceedings of the Thirtieth Conference on
MCMC.
Uncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27,
2014, pages 623–632. AUAI Press, 2014.

[89] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
In NIPS workshop on deep learning and

ural images with unsupervised feature learning.
unsupervised feature learning, page 5, 2011.

[90] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. nvcuong/variational-continual-learning,

2018.

[91] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. ICLR, 2018.

25

[92] K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, R. A. Olshen, and K. L. Oehler. Bayes risk
weighted vector quantization with posterior estimation for image compression and classiﬁca-
tion. IEEE Transactions on Image Processing, 5(2):347–360, 1996.

[93] F. A. Quintana and P. L. Iglesias. Bayesian clustering and product partition models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):557–574, 2003.
[94] N. C. Rabinowitz, G. Desjardins, A.-A. Rusu, K. Kavukcuoglu, R. T. Hadsell, R. Pascanu,
J. Kirkpatrick, and H. J. Soyer. Progressive neural networks, Nov. 23 2017. US Patent App.
15/396,319.

[95] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep con-

volutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[96] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are

unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

[97] T. Rainforth, A. Kosiorek, T. A. Le, C. Maddison, M. Igl, F. Wood, and Y. W. Teh. Tighter
variational bounds are not necessarily better. In International Conference on Machine Learn-
ing, pages 4277–4285, 2018.

[98] J. Ramapuram. Lifelongvae pytorch repository., 2017.
[99] A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with delta-

vaes. ICLR, 2019.

[100] A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with

VQ-VAE-2. CoRR, abs/1906.00446, 2019.

[101] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
In International Conference on Machine Learning,

inference in deep generative models.
pages 1278–1286, 2014.

[102] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,

[103] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

1997.

statistics, pages 400–407, 1951.

[104] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational

inference. arXiv preprint arXiv:1802.06847, 2018.

[105] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Comput.,

11(2):305–345, Feb. 1999.

[106] R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method.

Annals of Operations Research, 39(1):229–250, 1992.

[107] P. Ruvolo and E. Eaton. Ella: An efﬁcient lifelong learning algorithm.

In International

Conference on Machine Learning, pages 507–515, 2013.

[108] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models
via precision and recall. In Advances in Neural Information Processing Systems, pages 5228–
5237, 2018.

[109] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[110] N. W. Schuck and Y. Niv. Sequential replay of nonspatial task states in the human hippocam-

pus. Science, 364(6447):eaaw5181, 2019.

[111] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning, pages 4555–
4564, 2018.

[112] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In

Advances in Neural Information Processing Systems, pages 2994–3003, 2017.

[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484, 2016.

[114] D. L. Silver, G. Mason, and L. Eljabu. Consolidation using sweep task rehearsal: overcoming
In Canadian Conference on Artiﬁcial Intelligence, pages

the stability-plasticity problem.
307–322. Springer, 2015.

[115] D. L. Silver and R. E. Mercer. The parallel transfer of task knowledge using dynamic learning
rates based on a measure of relatedness. In Learning to learn, pages 213–233. Springer, 1996.
[116] D. L. Silver and R. E. Mercer. The task rehearsal method of life-long learning: Overcoming
In Conference of the Canadian Society for Computational Studies of

impoverished data.
Intelligence, pages 90–101. Springer, 2002.

26

[117] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algo-

rithms. In 2013 AAAI spring symposium series, 2013.

[118] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[119] W. E. Skaggs and B. L. McNaughton. Replay of neuronal ﬁring sequences in rat hippocampus

during sleep following spatial experience. Science, 271(5257):1870–1873, 1996.

[120] E. D. Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems

Sciences, 168:69–96, 1998.

[121] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[122] F. Tanaka and M. Yamamura. An approach to lifelong reinforcement learning through multi-
ple environments. In 6th European Workshop on Learning Robots, pages 93–99, 1997.
[123] A. V. Terekhov, G. Montone, and J. K. O’Regan. Knowledge transfer in deep block-modular
In Proceedings of the 4th International Conference on Biomimetic and

neural networks.
Biohybrid Systems-Volume 9222, pages 268–279. Springer-Verlag New York, Inc., 2015.
[124] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pages 1–10, 2016.
[125] S. Thrun. Lifelong learning: A case study. Technical report, CARNEGIE-MELLON UNIV

PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1995.

[126] S. Thrun and T. M. Mitchell. Lifelong robot learning.

In The biology and technology of

intelligent autonomous agents, pages 165–196. Springer, 1995.

[127] J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artiﬁcial

Intelligence and Statistics, pages 1214–1223, 2018.

[128] V. Vapnik. Estimation of dependences based on empirical data. Springer Science & Business

Media, 2006.

[129] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, R. Powell, et al. Alphastar: Mastering the real-time strategy game
starcraft ii. DeepMind Blog, 2019.

[130] S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic
model. In Proceedings of the 25th international conference on world wide web, pages 167–
176. International World Wide Web Conferences Steering Committee, 2016.

[131] R. C. Williamson and U. Helmke. Existence and uniqueness results for neural network ap-

proximations. IEEE Transactions on Neural Networks, 6(1):2–13, 1995.

[132] M. C. Wittrock. Generative learning processes of the brain. Educational Psychologist,

[133] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed

27(4):531–541, 1992.

memory. ICLR, 2018.

[134] Y. Wu, G. Wayne, K. Gregor, and T. Lillicrap. Learning attractor dynamics for generative

memory. In Advances in Neural Information Processing Systems, pages 9401–9410, 2018.

[135] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[136] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence.

In

International Conference on Machine Learning, pages 3987–3995, 2017.

27

10 APPENDIX

10.1 UNDERSTANDING THE CONSISTENCY REGULARIZER

The analytical derivations of the consistency regularizer show that the regularizer can be interpreted
as an a transformation of the standard VAE regularizer. In the case of an isotropic gaussian posterior,
the proposed regularizer scales the mean and variance of the student posterior by the variance of the
teacher 1 and adds an extra ’volume’ term. This interpretation of the consistency regularizer shows
that the proposed regularizer preserves the same learning objective as that of the standard VAE.
Below we present the analytical form of the consistency regularizer with categorical and isotropic
gaussian posteriors:

Proof 1 We assume the learnt posterior of the teacher is parameterized by a centered, isotropic
gaussian with Φ = [µE = 0, ΣE = diag(σE2
)] and the posterior of our student by a non-centered
isotropic gaussian with φ = [µS, ΣS = diag(σS2)], then

(cid:20)
tr(ΣE−1
KL(Qφ(z|x)||QΦ(z|x)) = 0.5

ΣS) + (µE − µS)T ΣE−1

(µE − µS) − F + log

= 0.5

F
(cid:88)

(cid:20)

j=1

1
σE2(j)

= KL(Qφ∗(z|x)||N (0, I)) − log |ΣE|

(σS2(j) + µS2(j)) − 1 + log σE2(j) − log σS2(j)

Via a reparameterization of the student’s parameters:

φ∗ = [µS∗, σS∗2]

µS∗ =

; σS∗2 =

µS(j)
σE2(j)

σS2(j)
σE2(j)

(cid:19)(cid:21)

(cid:18) |ΣE|
|ΣS|
(cid:21)

(21)

(22)

It is also interesting to note that our posterior regularizer becomes the prior if:

limσE2 (cid:55)→1KL(Qφ(z|x)||QΦ(z|x)) = KL(Qφ(z|x)||N (0, I))

Proof 2 We parameterize the learnt posterior of the teacher by Φi =

exp(pE
i )
i=1 exp(pE
i )

(cid:80)J

and the pos-

terior of the student by φi =
i ) and cS = (cid:80)J
cE = (cid:80)J
i=1 exp(pE
The KL divergence from the ELBO can now be re-written as:

exp(pS
i )
i=1 exp(pS
i )
i=1 exp(pS

(cid:80)J

. We also redeﬁne the normalizing constants as

i ) for the teacher and student models respectively.

KL(Qφ(zd|x)||QΦ(zd|x)) =

J
(cid:88)

i=1

exp(pS
i )
cS

log

(cid:18) exp(pS
i )
cS

(cid:19)

cE
exp(pE
i )

= H(pS, pS − pE) = −H(ps) + H(pS, pE)

(23)

where H( ) is the entropy operator and H( , ) is the cross-entropy operator.

28

10.2 RECONSTRUCTION REGULARIZER

Figure 18: Fashion Negative Test ELBO

Figure 19: Fashion Log-Frechet Distance

While it is possible to constrain the reconstruction/decoder term of the VAE in a similar manner to
the consistency posterior-regularizer, i.e: KL[Pθ(ˆx|z)||PΘ(ˆx|z)], doing so diminishes model per-
formance. We hypothesize that this is due to the fact that this regularizer contradicts the objective of
the reconstruction term Pθ(x|z) in the ELBO which already aims to minimize some metric between
the input samples x and the reconstructed samples ˆx; eg: if Pθ(x|z) ∼ N (µ, diag[σ]), then the
loss is proportional to ||ˆx − x||2
2, the standard L2 loss. Without the addition of this reconstruction
cross-model regularizer, the model is also provided with more ﬂexibility in how it reconstructs the
output samples.

In order to quantify the this we duplicate the FashionMNIST Experiment listed in the data ﬂow
deﬁnition in Figure 7. We use a simpler model than the main experiments to validate this hypothesis.
We train two dense models (-D): one with just the posterior consistency regularizer (without-LL-
D) and one with the consistency and likelihood regularizer (with-LL-D). We observe the model
performance drops (with respect to the Frechet distance as well the test ELBO) in the case of the
with-LL-D as demonstrated in Figures 18 and 19.

10.3 MODEL ARCHITECTURE

We used two different architectures for our experiments. When we use a dense network (-D) we
used two layers of 512 units to map to the latent representation and two layers of 512 to map back
to the reconstruction for the decoder. We used batch norm [48] and ELU activations for all the
layers barring the layer projecting into the latent representation and the output layer. Note that
while we used the same architecture for EWC we observed a drastic negative effect when using
batch norm and thus dropped it’s usage. The convolution architectures (-C) used the architecture
described below for the encoder and the decoder (where the decoder used conv-transpose layers for
upsampling). The notation is [OutputChannels, (ﬁlterX, ﬁlterY), stride]:

Encoder: [32, (5, 5), 1] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 1] (cid:55)→

GN+ELU (cid:55)→ [256, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [512, (1, 1), 1] (cid:55)→
GN+ELU (cid:55)→ [512, (1, 1), 1]

Decoder: [256, (4, 4), 1] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 1]

(24)

(cid:55)→ GN+ELU (cid:55)→ [32, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [32, (5, 5), 1]
(cid:55)→ GN+ELU (cid:55)→ [chans, (1, 1), 1]

29

Method
EWC-D
naive-D
batch-D
batch-D
lifelong-D
EWC-C
naive-C
batch-C
batch-C
lifelong-C

Initial zd dimension
10
10
10
10
1
10
10
10
10
1

Final zd dimension
10
10
10
10
10
10
10
10
10
10

zc dimension
14
14
14
14
14
14
14
14
14
14

# initial parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,165,311
30,767,428
7,691,280
7,691,280
15,382,560
15,235,072

# ﬁnal parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,179,661
30,767,428
7,691,280
7,691,280
15,382,560
15,382,560

The table above lists the number of parameters for each model and architecture used in our ex-
periments. The lifelong models initially start with a zd of dimension 1 and at each step we grow
the representation by one dimension to accommodate the new distribution (more info in Section
10.7). In contrast, the baseline EWC models are provided with the full representation throughout
the learning process. EWC has double the number of parameters because the computed diagonal
ﬁsher information matrix which is the same dimensionality as the number of parameters. EWC also
neeeds the preservation of the teacher model [Φ, Θ] to use in it’s quadratic regularizer. Both the
naive and batch models have the fewest number of parameters as they do not use a student-teacher
framework and only use one model, however the vanilla model has no protection against catastrophic
interference and the full model is just used as an upper bound for performance.

We used Adam [62] to optimize all of our problems with a learning rate of 1e-4 or 1e-3. When we
used weight transfer we re-initialized the accumulated momentum vector of Adam as well as the
aggregated mean and variance of the batch norm layers. The full architecture can be examined in
our github repository [98] and is provided under an MIT license.

10.4 CONTRAST TO STREAMING / ONLINE METHODS

Our method has similarities to streaming methods such as Streaming Variational Bayes (SVB) [14]
and Incremental Bayesian Clustering methods [60, 38] in that we estimate and reﬁne posteriors
through time. In general this can be done through the following Bayesian update rule that states that
the lastest posterior is proportional to the current likelihood times the previous posterior:

P (z|X1, ..., Xt) ∝ P (Xt|z)P (z|X1, ..., Xt−1)
SVB computes the intractable posterior, P (z|X1, ..., Xt), utilizing an approximation, At, that ac-
cepts as input the current dataset, Xt, along with the previous posterior At−1 :

(25)

P (z|X1, ..., Xt) ≈ At(Xt, At−1)

(26)

The ﬁrst posterior input (At=0) to the approximating function is the prior P (z). The objective of
SVB and other streaming methods is to model the posterior of the currently observed data in the
best possible manner. Our setting differs from this in that we want to retain information from all
previously observed distributions (sometimes called a knowledge store [126]). This can be useful
in scenarios where a distribution is seen once, but only used much later down the road. Rather than
creating a posterior update rule, we recompute the posterior via Equation 25, leveraging the fact that
we can re-generate X<t ≈ ˆX<t through the generative process. This allows us to recompute a more
appropriate posterior re-using all of the (generated) data, rather than using the previously computed
(approximate) posterior At−1:

P (z|X1, X2, ..., Xt) ∝ P (Xt|z)P (z| ˆX1, ..., ˆXt−1)

(27)

Coupling this generative replay strategy with the Bayesian update regularizer introduced in Section
4.1.1, we demonstrate that not only do we learn an updated poster as in Equation 27, but also allow
for a natural transfer of information between sequentially learnt models: a fundamental tenant of
lifelong learning [126, 125].

Finally, another key difference between lifelong learning and online methods is that lifelong learning
aims to learn from a sequence of tentatively different [20] tasks while still retaining and accumulating

30

knowledge; online learning generally assumes that the true underlying distribution comes from a
single distribution [11]. There are some exceptions to this where online learning is applied to the
problem of domain adaptation, eg: [49, 60].

10.5 EWC BASELINES: COMPARING CONV & DENSE NETWORKS

We compared a whole range of EWC baselines and use the best performing models few in our
experiments. Listed in Figure 10.5 are the full range of EWC baselines run on the PermutedMNIST
and FashionMNIST experiments. Recall that C / D describes whether a model is convolutional or
dense and the the number following is the hyperparameter for the EWC or Lifelong VAE.

10.6 GUMBEL REPARAMETERIZATION

Since we model our latent variable as a combination of a discrete and a continuous distribution we
also use the Gumbel-Softmax reparameterization [76, 50]. The Gumbel-Softmax reparameterization
over logits [linear output of the last layer in the encoder] p ∈ RM and an annealed temperature
parameter τ ∈ R is deﬁned as:

z = sof tmax(

); g = −log(−log(u ∼ U nif (0, 1)))

(28)

log(p) + g
τ

u ∈ RM , g ∈ RM . As the temperature parameter τ (cid:55)→ 0, z converges to a categorical.

10.7 EXPANDABLE MODEL CAPACITY AND REPRESENTATIONS

Multilayer neural networks with sigmoidal activations have a VC dimension bounded between
O(ρ2)[120] and O(ρ4)[59] where ρ are the number of parameters. A model that is able to con-

31

sistently add new information should also be able to expand its VC dimension by adding new pa-
rameters over time. Our formulation imposes no restrictions on the model architecture: i.e. new
layers can be added freely to the new student model.
In addition we also allow the dimensionality of zd ∈ RJ , our discrete latent representation to grow
in order to accommodate new distributions. This is possible because the KL divergence between
two categorical distributions of different sizes can be evaluated by simply zero padding the teacher’s
smaller discrete distribution. Since we also transfer weights between the teacher and the student
model, we need to handle the case of expanding latent representations appropriately. In the event
that we add a new distribution we copy all the weights besides the ones immediately surrounding
the projection into and out of the latent distribution. These surrounding weights are reinitialized to
their standard Glorot initializations [37].

32

9
1
0
2
 
v
o
N
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
7
4
8
9
0
.
5
0
7
1
:
v
i
X
r
a

LIFELONG GENERATIVE MODELING

Jason Ramapuram ∗ †
Jason.Ramapuram@etu.unige.ch

Magda Gregorova ∗ †
magda.gregorova@hesge.ch

Alexandros Kalousis ∗ †
Alexandros.Kalousis@hesge.ch

ABSTRACT

Lifelong learning is the problem of learning multiple consecutive tasks in a se-
quential manner, where knowledge gained from previous tasks is retained and
used to aid future learning over the lifetime of the learner. It is essential towards
the development of intelligent machines that can adapt to their surroundings. In
this work we focus on a lifelong learning approach to unsupervised generative
modeling, where we continuously incorporate newly observed distributions into a
learned model. We do so through a student-teacher Variational Autoencoder ar-
chitecture which allows us to learn and preserve all the distributions seen so far,
without the need to retain the past data nor the past models. Through the intro-
duction of a novel cross-model regularizer, inspired by a Bayesian update rule,
the student model leverages the information learned by the teacher, which acts
as a probabilistic knowledge store. The regularizer reduces the effect of catas-
trophic interference that appears when we learn over sequences of distributions.
We validate our model’s performance on sequential variants of MNIST, Fashion-
MNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model
mitigates the effects of catastrophic interference faced by neural networks in se-
quential learning scenarios.

1

INTRODUCTION

Machine learning is the process of approximating unknown functions through the observation of typ-
ically noisy data samples. Supervised learning approximates these functions by learning a mapping
from inputs to a predeﬁned set of outputs such as categorical class labels (classiﬁcation) or contin-
uous targets (regression). Unsupervised learning seeks to uncover structure and patterns from the
input data without any supervision. Examples of this learning paradigm include density estimation
and clustering methods. Both learning paradigms make assumptions that restrict the set of plausible
solutions. These assumptions are referred to as hypothesis spaces, biases or priors and aid the model
in favoring one solution over another [82, 128]. For example, the use of convolutions [70] to process
images favors local structure; recurrent models [54, 46] exploit sequential dependencies and graph
neural networks [109, 64] assume that the underlying data can be modeled accurately as a graph.

Current state of the art machine-learning models typically focus on learning a single model for a
single task, such as image classiﬁcation [72, 121, 43, 118, 66], image generation [100, 13, 63, 39],
natural language question answering [25, 96] or single game playing [129, 113]. In contrast, humans
experience a sequence of learning tasks over their lifetimes, and are able to leverage previous learn-
ing experiences to rapidly learn new tasks. Consider learning how to ride a motorbike after learning
to ride a bicycle: the task is drastically simpliﬁed through the use of prior learning experience. Stud-
ies [2, 3, 67] in psychology have shown that humans are able to generalize to new concepts in a rapid
manner, given only a handful of samples. [67] demonstrates that humans can classify and generate
new concepts of two wheel vehicles given just a single related sample. This contrasts the state of the
art machine learning models described above which use hundreds of thousands of samples and fail
to generalize to slight variations of the original task [22].

∗University of Geneva, Switzerland
†Haute cole de gestion de Genve, HES-SO, Switzerland

1

Lifelong learning [126, 125] argues for the need to consider learning over task sequences, where
learned task representations and models are stored over the entire lifetime of the learner and can be
used to aid current and future learning. This form of learning allows for the transfer of previously
learned models and representations and can reduce the sample complexity of the current learning
problem [126]. In this work we restrict ourselves to a subset of the broad lifelong learning paradigm;
rather than focus on the supervised lifelong learning scenario as most state of the art methods, our
work is one of the ﬁrst to tackle the more challenging problem of deep lifelong unsupervised learn-
ing. We also identity and relax crucial limitations of prior work in life-long learning that requires
the storage of previous models and training data, allowing us to operate in a more realistic learning
scenario.

2 RELATED WORK
The idea of learning in a continual manner has been explored extensively in machine learning,
seeded by the seminal works of lifelong-learning [126, 125, 117], online-learning [32, 9, 11, 12]
and sequential linear gaussian models [105, 36] such as the Kalman Filter [56] and its non-linear
counterpart, the Particle Filter [24]. Lifelong learning bears some similarities to online learning
in that both learning paradigms observe data in a sequential manner. Online learning differs from
lifelong learning in that the central objective of a typical online learner [11, 12] is to best solve/ﬁt the
current learning problem, without preserving previous learning. In contrast, lifelong learners seek to
retain, and reuse, the learned behavior acquired over past tasks, and aim to maximize performance
across all tasks. Consider the example of forecasting click through rate: the objective of the online
learner is to evolve over time, such that it best represents current user preferences. This contrasts
lifelong learners which enforce a constraint between tasks to ensure that previous learning is not
lost.

Lifelong Learning [126] was initially proposed in a supervised learning framework for concept
learning, where each task seeks to learn a particular concept/class using binary classiﬁcation. The
original framework used a task speciﬁc model, such as a K Nearest Neighbors (KNN) 1, coupled with
a representation learning network that used training data from all past learning tasks (support sets),
to learn a common, global representation. This supervised approach was later improved through the
use of dynamic learning rates [115], core-sets [114] and multi-head classiﬁers [31].
In parallel,
lifelong learning was extended to independent multi-task learning [107, 31], reinforcement learning
[125, 122, 102], topic modeling [19, 130] and semi-supervised language learning [83, 81]. For a
more detailed review see [20].

More recently, lifelong learning has seen a resurgence within the framework of deep learning. As
mentioned earlier, one of the central tenets of lifelong learning is that that the learner should perform
well over all observed tasks. Neural networks, and more generally, models that learn using stochastic
gradient descent [103], typically cannot persist past task learning without directly preserving past
models or data. This problem of catastrophic forgetting [78] is well known in the neural network
community and is the central obstacle that needs to be resolved to build an effective neural lifelong
learner. Catastrophic forgetting is the phenomenon where model parameters of a neural network
trained in a sequential manner become biased towards the distribution of the latest observations,
forgetting previously learned representations, over data no longer accessible for training. In order to
mitigate catastrophic forgetting current research in lifelong learning employs four major strategies:
transfer learning, replay mechanisms, parameter regularization and distribution regularization. In
table 1 we classify the different lifelong learning methods that we will discuss in the following
paragraphs into these strategies.

EWC [65] VCL [91] LwF [71] ALTM [33]
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

PNN [94] DGR [112, 57] DBMNN [123]
(cid:88)
(cid:55)

(cid:88)
(cid:55)

(cid:55)
(cid:88)

(cid:88)

Transfer learning
Replay mechanisms
Parameter
regularization
Functional
regularization
Table 1: Catastropic interference mitigation strategies of state of the art models. Rows highlighted in gray
represent desirable mitigation strategies.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

SI [136] VASE [1] LGM (us)
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:88)

1These models were known as memory based learning in [126].

2

Transfer learning: These approaches mitigate catastrophic forgetting by freezing previous task
models and relaying a latent representation of the previous task to the current model. Research in
transfer learning for the mitigation of catastrophic forgetting include Progressive Neural Networks
(PNN) [94] and Deep Block-Modular Neural Networks (DBMNN) [123] to name a few. These
approaches allow the current model to adapt its parameters to the (new) joint representation in an
efﬁcient manner and prevent forgetting through the direct preservation of all previous task mod-
els. Deploying such a transfer learning mechanism in a lifelong learning setting would necessitate
training a new model with every new task, considerably increasing the memory footprint of the life-
long learner. In addition, since transfer learning approaches freeze previous models, it negates the
possibility of improving previous task performance using knowledge gathered from new tasks.

Replay mechanisms: The original formulation of lifelong learning [126] required the preservation
of all previous task data. This requirement was later relaxed in the form of core-sets [116, 114, 91],
which represent small weighted subsets of inputs that approximate the full dataset. Recently, within
the classiﬁcation setting, there have been replay approaches that try to lift the requirement of storing
past training data by relying on generative modeling [112, 57]; we will call such methods Deep gen-
erative replay (DGR) methods. DGR methods methods use a student-teacher network architecture,
where the teacher (generative) model augments the student (classiﬁer) model with synthetic samples
from previous tasks. These synthetic task samples are used in conjunction with real samples from
the current task to learn a new joint model across all tasks. While strongly motivated by biologi-
cal rehearsal processes [119, 53, 58, 110], these generative replay strategies fail to efﬁciently use
previous learning and simply re-learn each new joint task from scratch.

Parameter regularization: Most work that mitigates catastrophic forgetting falls under the um-
brella of parameter regularization. There are two approaches within this mitigation strategy: con-
straining the parameters of the new task to be close to the previous task through a predeﬁned metric,
and enforcing task-speciﬁc parameter sparsity. The two approaches are related as task-speciﬁc pa-
rameter sparsity can be perceived as a reﬁnement of the parameter constraining approach. Parameter
constraining approaches typically share the same model/parameters, but encourage new tasks from
altering important learned parameters from previous tasks. Task speciﬁc parameter sparsity relaxes
this, by enforcing that each task use a different subset of parameters from a global model, through
the use of an attention mechanism.

Models such as Laplace Propagation [29], Elastic Weight Consolidation (EWC) [65], Synaptic In-
telligence (SI) [136] and Variational Continual Learning (VCL) [91] fall under the parameter con-
straining approach. EWC for example, uses the Fisher Information matrix (FIM) to control the
change of model parameters between two learning tasks. Intuitively, important parameters should
not have their values changed, while non-important parameters are left unconstrained. The FIM is
used as a weighting in a quadratic parameter difference regularizer under a Gaussianity assump-
tion of the parameter posterior. However, this Gaussian parameter posterior assumption has been
demonstrated [86, 10] to be sub-optimal for learned neural network parameters. VCL improves
upon EWC, by generalizing the local assumption of the FIM to a KL-Divergence between the (vari-
ational) parameter posterior and prior. This generalization derives from the fact that the FIM can
be cast as a KL divergence between the posterior and an epsilon perturbation of the same random
variable [51]. VCL actually spans a number of different mitigation strategies as it uses parameter
regularization (described above) , transfer learning (it keeps a separate head network per task) and
replay (it persists a core-set of true data per task).

Models such as Hard Attention to the Task (HAT) [111] and the Variational Autoencoder with Shared
Embeddings (VASE) [1] fall under the task-speciﬁc parameter sparsity strategy. This mitigation
strategy enforces that different tasks use different components of a single model, typically through
the use of attention vectors [6] that are learned given supervised task labels. Multiplying the attention
vectors with the model outputs prevents gradient descent updates for different subsets of the model’s
parameters, allowing them to be used for future task learning. Task speciﬁc parameter sparsity allows
a model to hold-out a subset of its parameters for future learning and typically works well in practice
[111], with its strongest disadvantage being the requirement of supervised information.

Functional regularization: Parameter regularization methods attempt to preserve the learned be-
havior of the past models by controlling how the model parameters change between tasks. However,
the model parameters are only a proxy for the way a model actually behaves. Models with very
different parameters can have exactly the same behavior with respect to input-output relations (non-

3

uniqueness [131]). Functional regularization concerns itself with preserving the actual object of
interest:
the input-output relations. This strategy allows the model to ﬂexibly adapt its internal
parameter representation between tasks, while still preserving past learning.

Methods such as distillation [45], ALTM [33] and Learning Without Forgetting (LwF) [71] impose
similarity constraints on the classiﬁcation outputs of models learned over different tasks. This can
be interpreted as functional regularization by generalizing the constraining metric (or semi-metric)
to be a divergence on the output conditional distribution. In contrast to parameter regularization, no
assumptions are made on the parametric form of the parameter posterior distribution. This allows
models to ﬂexibly adapt their internal representation as needed, making functional regularization a
desirable mitigation strategy. One of the pitfalls of current functional regularization approaches is
that they necessitate the preservation of all previously data.

2.1 LIMITATIONS OF EXISTING APPROACHES.

A simple solution to the problem of lifelong learning is to store all data and re-learn a new joint
multi-task representation [17] at each newly observed task. Alternatively, it is possible to retain
all previous model parameters and select the model that presents the best performance on new test
task data. Existing solutions typically relax one these requirements. [33, 71, 91] relaxes the need
for model persistence, but requires preservation of all data [33, 71], or a growing core-set of data
[116, 114, 91]. Conversely, [94, 123, 91, 136] relaxes the need to store data, but persists all previous
models [94, 123] or a subset of model parameters [91, 136].

Unlike these approaches, we draw inspiration from how humans learn over time and remove the
requirement of storing past training data and models. Consider the human visual system; research
has shown [23, 7] that the human eye is capable of capturing 576 megapixels of content per image
frame. If stored naively on a traditional computer, this corresponds to approximately 6.9 gigabytes of
information per sample. Given that we perceive trillions of frames over our lifetimes, it is infeasible
to store this information in its base, uncompressed representation. Research in neuroscience has
validated [132, 5] that the associative human mind, compresses, merges and reconstructs information
content in a dynamic way. Motivated by this, we believe that a lifelong learner should not store past
training data or models. Instead, it should retain a latent representation that is common over all tasks
and evolve it as more tasks are observed.

2.2 OUR SOLUTION AT A HIGH LEVEL.

While most research in lifelong learning focuses on supervised learning [94, 33, 71, 123, 136, 65],
we focus on the more challenging task of deep unsupervised latent variable generative modeling.
These models have wide ranging applications such as clustering [77, 52, 85] and pre-training [68,
95].

Central to our lifelong learning method are a pair of generative models, aptly named the teacher and
student, which we train by exploiting the replay and functional regularization strategies described
above. After training a single generative model over the ﬁrst task, we use it is used as the teacher for
a newly instantiated student model. The student model receives data from the current task, as well as
replayed data from the teacher, which acts as a probabilistic storage container of past tasks. In order
to preserve previous learning, we make use of functional regularization, which aids in preserving
input-output relations over past tasks.

Unlike EWC or VCL, we make no assumptions on the form of the parameter posterior and allow
the generative models to use available parameters as appropriate, to best accommodate current and
past learning. The use of generative replay, coupled with functional regularization, renders the
preservation of the past models and past data unnecessary. It also signiﬁcantly improves the sample
complexity on future task learning, which we empirically demonstrate in our experiments. Finally
we should note that it is straightforward to adapt our approach to the supervised learning setting, as
we have done in [69].

3 BACKGROUND

In this section we describe the main concepts that we use throughout this work. We begin by de-
scribing the base-level generative modeling approach in Section 3.1, followed by how it extends to

4

the lifelong setting in Section 3.2. Finally, in Section 3.3, we describe the Variational Autoencoder
over which we instantiate our lifelong generative model.

3.1 LATENT VARIABLE GENERATIVE MODELING

We consider a scenario where we observe a dataset, D, consisting of N variates, D = {xj}N
j=1,
of a continuous or discrete variable x. We assume that the data is generated by a random process
involving a non-observed random variable, z. The data generation process involves ﬁrst sampling
zj ∼ P (z) and then producing a variate from the conditional, xj ∼ Pθ(x|z). We visualize this
form of latent generative model in the graphical model in Figure 14.

Figure 1: Typical latent variable graphical model. Gray nodes represent observed variables while white nodes
represent unobserved variables.

Typically, latent variables models are solved through maximum likelihood estimation which can be
formalized as:

max
θ

log Pθ(x) = max

log

Pθ(x|z)P (z)dz = max

log Ez[Pθ(x|z)]

(1)

θ

(cid:90)

θ

In many cases, the expectation from Equation 1 does not have a closed form solution (eg: non-
conjugate distributions) and quadrature is not computationally tractable due to large dimensional
spaces [63, 101] (eg: images). To overcome these intractabilities we use a Variational Autoencoder
(VAE), which we summarize in Section 3.3. The VAE allows us to infer our latent variables and
jointly estimate the parameters of our model. However, before describing the VAE, it is important
to understand how this generative setting can be perceived in a lifelong learning scenario.

3.2 LIFELONG GENERATIVE MODELING

Lifelong generative modeling extends the single-distribution estimation task from Section 3.1 to a
set of i = {1...L} sequentially observed learning tasks. The i-th learning task has variates that are
realized from the task speciﬁc conditional, Pi(x) = P (x|zd = i), where zd acts as a categorical
indicator variable of the current task. We visualize a simpliﬁed form of this in Figure 2 below.

Figure 2: Simpliﬁed lifetime of a lifelong learner. Given a true (unknown) distribution, P (x) =
(cid:82) P (x|zd)P (zd)δzd, we observe partial information in the form of L sequential tasks, {D1 (cid:55)→ D2, ... (cid:55)→
DL}. Observing more tasks, reduces the uncertainty of the model until convergence, PθL (x) ≈ P (x).

Crucially, when observing task, Di, the model has no access to any of the previous task datasets,
D<i. As the lifelong learner observes more tasks, {D1 (cid:55)→ D2 (cid:55)→ ... (cid:55)→ DL}, it should improve its
estimate of the true distribution, Pθ(x) ≈ P (x) = (cid:82) P (x|zd)P (zd)δzd, which is unknown at the
start of training.

5

3.3 THE VARIATIONAL AUTOENCODER

As eluded to in Section 3.1, we would like to infer the latent variables from the data. This
can be realized as an alternative form of Equation 1 in the form of Bayes rule: Pφ(z|x) =
Pθ(x|z)P (z)/Pθ(x), where Pφ(z|x) is referred to as the latent variable posterior and Pθ(x|z)
as the likelihood. One method of approximating the posterior, Pφ(z|x), is through MCMC sam-
pling methods such as Gibbs sampling [34] or Hamiltonian MCMC [87]. MCMC methods have
the advantage that they provide asymptotic guarantees [88] of convergence to the true posterior,
Pφ(z|x). However in practice it is not possible to know when convergence has been achieved. In
addition, due to their Markovian nature, they possess an inner loop, which makes it challenging to
scale for large scale datasets.

In contrast, Variational Inference (VI) [55] side-steps the intractability of the posterior by approxi-
mating it with a tractable distribution family, Qφ(z|x). VI rephrases the objective of determining the
posterior as an optimization problem by minimizing the KL divergence between the known distri-
butional family, Qφ(z|x), and the unknown true posterior, Pθ(z|x). Applying VI to the intractable
integral from Equation 1 results in the evidence lower bound (ELBO) or variational free energy,
which can easily be derived from ﬁrst principles:

log Pθ(x) = log

Pθ(x|z)P (z)dz

(cid:90)

= log

(cid:90) Qφ(z|x)
Qφ(z|x)

Pθ(x|z)P (z)dz

≥ EQ[log Pθ(x|z)] − DKL[Qφ(z|x)||P (z)]
(cid:123)(cid:122)
(cid:125)
ELBO

(cid:124)

(2)

(3)

(4)

where we used Jensen’s inequality to transition from Equation 3 to Equation 4. The objective intro-
duced in Equation 4 induces the graphical model shown below in Figure 3.

Figure 3: Standard VAE graphical model. Gray nodes represent observed variables while white nodes represent
unobserved variables; dashed lines represent inferred variables.

VAEs typically use deep neural networks to model the approximate inference network, Qφ(z|x)
and conditional, Pθ(x|z), which are also known as the encoder and decoder networks (respec-
tively). To optimize for the parameters of these networks, VAEs maximize the ELBO (Equation
4) using Stochastic Gradient Descent [103]. By sharing the variational parameters of the encoder,
φ, across the data points (amortized inference [35]), variational autoencoders avoid per-data inner
loops typically needed by MCMC approaches.

Optimizing the ELBO in Equation 4 requires computing the gradient of an expectation over the ap-
proximate posterior, Qφ(z|x). This typically takes place through the use of the path-wise estimator
[101, 63] (originally called “push-out” [106]). The path-wise reparameterizer uses the Law of the
Unconscious Statistician (LOTUS) [42], which enables us to compute the expectation of a function
of a random variable (without knowing its distribution) if we know its corresponding sampling path
and base distribution [84]. For the typical isotropic gaussian approximate posterior, Qφ(z|x), used
in standard VAEs this can be aptly summarized by:

z ∼ Qφ(z|x) ⇔ µφ(x) + σφ(x)(cid:15), (cid:15) ∼ N (0, 1)

∇φEQφ(z|x)[log Pθ(x|z)] ⇔ E
N ((cid:15)|0, 1)
(cid:125)
(cid:123)(cid:122)
(cid:124)

base distribution

)]
[∇φ log Pθ(x| µφ(x) + σφ(x)(cid:15)
(cid:125)
(cid:124)

(cid:123)(cid:122)
sampling path

(5)
(6)

6

where Equation 5 deﬁnes the sampling procedure of our latent variable through the location-scale
transformation and Equation 6 deﬁnes the path-wise Monte Carlo gradient estimator applied on the
decoder (ﬁrst term in Equation 4). This Monte Carlo estimator enables differentiating through the
sampling process of the distribution Qφ(z|x). Note that computing the gradient of the second term
in Equation 4, ∇φDKL[Qφ(z|x)||P (z)], is possible through a closed form analytical solution for
the case of isotropic gaussian distributions.

While it is possible to extend any latent variable generative model to the lifelong setting, we choose
to build our lifelong generative models using variational autoencoders (VAEs) [63] as they provide
a mechanism for stable training; this contrasts other state of the art unsupervised models such as
Generative Adversarial Networks (GANs) [39, 61]. Furthermore, latent-variable posterior approxi-
mations are a requirement in many learning scenarios such as clustering [93], compression [92] and
unsupervised representation learning [30]. Finally, GANs can suffer from low sample diversity [28]
which can lead to compounding errors in a lifelong generative setting.

4 LIFELONG LEARNING MODEL

Algorithm 1 Data Flow

Teacher:
Sample Prior: zj ∼ P (z)
Decode: ˆxj ∼ PΘ(x|z)

Student:
Sample : xj ∼ P (ω)P (x|ω)
Encode : zj ∼ Qφ(z|x)
Decode: ˆxj ∼ Pθ(x|z)

Figure 4: Student training procedure. Left: graphical model for student-teacher model. Data generated from
the teacher model (top row) is used to augment the current training data observed by the student model (bottom
row). A posterior regularizer is also applied between Qφ(z|x) and QΦ(z|x) to enable functional regularization
(not shown, but discussed in detail in Section 4.1.1). Right: data ﬂow algorithm.

fMRI studies of the rodent [119, 53, 58] and human [110] brains have shown that previously experi-
enced sequences of events are replayed in the hippocampus during rest. These replays are necessary
for better planning [53] and memory consolidation [16]. We take inspiration from the memory con-
solidation of biological learners and introduce our model of Lifelong Generative Modeling (LGM).
We visualize the LGM student-teacher architecture in Figure 4.

The student and the teacher are both instantiations of the same base-level generative model, but
have different roles throughout the learning process. The teacher’s role is to act as a probabilistic
knowledge store of previously learned distributions, which it transfers to the student in the form
of replay and functional regularization. The student’s role is to learn the distribution over the new
task, while accommodating the learned representation of the teacher over old tasks. In the following
sections we provide detailed descriptions of the student-teacher architecture, as well as the base-
level generative model that each of them use. The base-level model uses a variant of VAEs, which
we tailor for lifelong learning and is learned by maximizing a variant of the standard VAE ELBO
from Equation 4 ; we describe this objective at end of this section.

4.1 STUDENT-TEACHER ARCHITECTURE

The top row of Figure 4 represents the teacher model. At any given time, the teacher contains a
summary of all previous distributions within the learned parameters, Φ, of the encoder QΦ(z|x),
and the learned parameters, Θ, of the decoder PΘ(x|z). We use the teacher to generate synthetic
variates, ˆxj, from these past distributions by decoding variates from the prior, zj ∼ P (z) (cid:55)→
PΘ(x|z = zj). We pass the generated (synthetic) variates, ˆxj, to the student model as a form of
knowledge transfer about the past distributions. Information transfer in this manner is known as
generative replay and our work is the ﬁrst to explore it in a VAE setting.

7

The bottom row of Figure 4 represents the student. The student is responsible for updating the
parameters, φ, of its encoder, Qφ(z|x), and θ, of its decoder Pθ(x|z). Importantly, the student re-
ceives data from both the currently observed task, as well as synthetic data generated by the teacher.
This can be formalized as xj ∼ P (ω)P (x|ω), ω ∼ Ber(π), as shown in Equation 7:

P (ω)P (x|ω) =

(cid:26)PΘ(x|z) ω = 0
ω = 1

Pi(x)

(7)

The mean, π, of the Bernoulli distribution, controls the sampling proportion of the previously learned
distributions to the current one and is set based on the number of assimilated distributions. Thus,
given i observed distributions: π = 1
i+1 . This ensures that the samples observed by the student
are representative of both the current and past distributions. Note that this does not correspond
to varying sample sizes in datasets, but merely our assumption to model each distribution with
equivalent weighting.

Once a new task is observed, the old teacher is dropped, the student model is frozen and becomes the
new teacher (φ → Φ, θ → Θ). A new student is then instantiated with the latest weights φ and θ
from the previous student (the new teacher). Due to the cyclic nature of this process, no new models
are added. This contrasts many existing state of the art deep lifelong learning methods which add an
entire new model or head-network per task (eg: [91, 94, 123]).

A crucial aspect in the lifelong learning process is to ensure that previous learning is successfully
exploited to bias current learning [126]. While the replay mechanism that we put in place ensures
that the student will observe data from all tasks, it does not ensure that previous knowledge from
the teacher is efﬁciently exploited to improve current student learning. The student model will re-
learn (from scratch) a completely new representation, which might be different than the teacher. In
order to successfully transfer knowledge between both VAE models, we rely on functional regular-
ization, which we enforce through a Bayesian update regularizer of the posteriors of both models.
Intuitively, we would like the student model’s latent outputs, zj ∼ Qφ(z|x) to be similar to la-
tent outputs of teacher model, zj ∼ QΦ(z|x), over synthetic variates generated by the teacher,
xj ∼ P (ω)P (x|ω = 0) = PΘ(x|z). In the following section, we describe the exact functional
form of this regularizer and demonstrate how it can be perceived as a natural extension of the VAE
learning objective to a sequential setting.

4.1.1 KNOWLEDGE TRANSFER VIA BAYESIAN UPDATE.

While both the student and teacher are instantiations of VAE variants, tailored for the particularities
of the lifelong setting, for the purpose of this exposition we use the standard VAE formulation. Our
objective is to learn the set of parameters [φ, θ] of the student, such that it can generate variates
from the complete distribution, P (x), described in Section 3.2. Subsuming the deﬁnition of the
augmented input data, x ∼ P (ω)P (x|ω), from Equation 7, we can deﬁne the student ELBO as:

Lθ,φ(x) = EQφ(z|x)

(cid:20)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)],

(8)

x ∼ P (ω)P (x|ω), ω ∼ Ber(π).

Rather than naively shrinking the full posterior to the prior via the KL divergence in Equation 8, we
rely on one of the core tenets of the Bayesian paradigm which states that we can always update our
posterior when given new information (yesterdays posterior is todays prior) [79]. Given this tenet,
we introduce our posterior regularizer 2:

KL[Qφ(z|x)||QΦ(z|x)], x ∼ P (x|ω = 0)
(9)
which distills the teacher’s learnt representation into the student over the generated data only. Com-
bining Equations 8 and 9, yields the objective that we can use to train the student and is described
below in Equation 10:

Lθ,φ(x) = EQφ(z|x)

log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(10)

(cid:20)

(cid:21)

2While it

is also possible to apply a similar

regularizer

to the reconstruction term,

i.e:

KL[Pθ(x|z) || PΘ(x|z)], we observed that doing so hurts performance (Appendix 10.2).

+ (1 − ω)KL[Qφ(z|x)||QΦ(z|x)],
x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

8

Note that this is not the ﬁnal objective, due to the fact that we have yet to present the VAE variant
tailored to the particularities of the lifelong setting. We will now show how the posterior regularizer
can be perceived as a natural extension of the VAE learning objective, through the lens of a Bayesian
update of the student posterior.

Lemma 1 For random variables x and z with conditionals QΦ(z|x) and Qφ(z|x), both distributed
as a categorical or gaussian and parameterized by Φ and φ respectively, the KL divergence between
the distributions is:

KL[Qφ(z|x)||QΦ(z|x)] = KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(11)

where ˆφ = f (φ, Φ) depends on the parametric form of Q, and C is only a function of the parameters,
Φ.

We prove Lemma 1 for the relevant distributions (under some mild assumptions) in Appendix 10.1.
Using Lemma 1 allows us to rewrite Equation 10 as shown below in Equation 12:
(cid:20)

Lθ,φ(x) = EQφ(z|x)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(12)

(cid:20)

+ (1 − ω)

KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(cid:21)
,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

This rewrite makes it easy to see that our posterior regularizer from Equation 10 is a standard VAE
ELBO (Equation 4) under a reparameterization of the student parameters, ˆφ = f (φ, Φ). Note that
C(Φ) is constant with respect to the student parameters, φ, and thus not used during optimization.
While the change seems minor, it omits the introduction of f (φ, Φ) which allows for a transfer of
information between models. In practice, we simply analytically evaluate KL[Qφ(z|x) ||QΦ(z|x)],
the KL divergence between the teacher and the student posteriors, instead of deriving the functional
form of f (φ, Φ) for each different distribution pair. We present Equation 12 simply as a means to
provide a more intuitive understanding of our functional regularizer.

4.2 BASE-LEVEL GENERATIVE MODEL.

While it is theoretically possible to use the vanilla VAE from Section 3.3 for the teacher and student
models, doing so brings to light a number of limitations that render it problematic for use in the con-
text of lifelong learning (visualized in Figure 5-Right). Speciﬁcally, using a standard VAE decoder,
Pθ(x|z), to generate synthetic replay data for the student is problematic due to two reasons:

1. Mixed Distributions: Sampling the continuous standard normal prior, N (0, 1), can select
a point in latent space that is in between two separate distributions, causing generation of
unrealistic synthetic data and eventually leading to loss of previously learnt distributions.
2. Undersampling: Data points mapped to the isotropic-gaussian posterior that are further
away from the prior mean will be sampled less frequently, resulting in an undersampling
of some of the constituent distributions.

To address these sampling limitations we decompose the latent variable, z, into an independent
continuous, zc ∼ Qφ(zc|x), and a discrete component, zd ∼ Qφ(zd|x), as shown in Equation 13
and visually in Figure 5-Left:

Qφ(zc, zd|x) = Qφ(zc|x)Qφ(zd|x).

(13)

The objective of the discrete component is to summarize the discriminative information of the indi-
vidual generative distributions. The continuous component on the other hand, caters for the remain-
ing sample variability (a nuisance variable [73]). Given that the discrete component can accurately
summarize the discriminative information, we can then explicitly sample from any of the past dis-
tributions, allowing us to balance the student model’s synthetic inputs with samples from all of the
previous learned distributions. We describe this beneﬁcial generative sampling property in more
detail in Section 4.2.1.

9

Figure 5: Left: Graphical model for VAE with independent discrete and continuous posterior, Qφ(zc, zd|x) =
Qφ(zc|x)Qφ(zd|x). Right: Two dimensional test variates, zj ∼ Qφ(z|x), zj ∈ R2, of a vanilla VAE
trained on MNIST. We depict the two generative shortcomings visually: 1) mixing of distributions which
causes aliasing in a lifelong setting and 2) undersampling of distributions in a standard isotropic-gaussian VAE
posterior.

Naively introducing the discrete component, zd, does not guarantee that the decoder will use it to
represent the most discriminative aspects of the modeled distribution. In preliminary experiments,
we observed that that the decoder typically learns to ignore the discrete component and simply
relies on the continuous variable, zc. This is similar to the posterior collapse phenomenon which
has received a lot of recent interest within the VAE community [99, 40]. Posterior collapse occurs
when training a VAE with a powerful decoder model such as a PixelCNN++ [127] or RNN [21, 40].
The output of the decoder, xj ∼ Pθ(x|z) can become almost independent of the posterior sample,
zj ∼ Qφ(z|x), but is still able to reconstruct the original sample by relying on its auto-regressive
property [40]. In Section 4.2.2, we introduce a mutual information regulariser which ensures that
the discrete component of the latent variable is not ignored.

4.2.1 CONTROLLED GENERATIONS.

Desired Task Conditional
P1(x) = P (x|zd = 1)
P2(x) = P (x|zd = 2)
P3(x) = P (x|zd = 3)

zc
∼ N (0, 1)
∼ N (0, 1)
∼ N (0, 1)

zd
[0, 0, 1]
[0, 1, 0]
[1, 0, 0]

Figure 6 & Table 2: FashionMNIST with L = 3 tasks: t-shirts, sandals and bag. To generate samples from
the i-th task conditional, Pi(x) = P (x|zd = i), we set zd = i, randomly sample zc ∼ N (0, 1), and run
[zc, zd] through the decoder, Pθ(x|zc, zd). Resampling zc, while keeping zd ﬁxed, enables generation of
varied samples from the task conditional. Left: Desired task conditionals. Right: Desired decoder behavior.

Given the importance of generative replay for knowledge transfer in LGM, synthetic sample gen-
eration by the teacher model needs to be representative of all the previously observed distributions
in order to prevent catastrophic forgetting. Under the assumption that zd accurately captures the
underlying discriminativeness of the individual distributions and through the deﬁnition of the LGM
generative process, shown in Equation 14:

PΘ(x|zd, zc), zc ∼ N (0, 1), zd ∼ Cat(1/L),

we can control generations by setting a ﬁxed value, zd = i, and randomly sampling the continuous
prior, zc ∼ N (0, 1). This is possible because the trained decoder approximates the task conditional
from Section 3.2:

(14)

(15)

P (x|zd = i) = Pi(x) ≈

PΘ(x|zc, zd = i)P (zc)
QΦ(zc|x)

10

where sampling the true task conditional, Pi(x), can be approximated by sampling zc ∼ P (zc) =
N (0, 1), keeping zd ﬁxed, and decoding the variates as shown in Equation 16 below:

ˆx ∼ PΘ(x|zc ∼ N (0, 1), zd = i).

(16)

We provide a simple example of our desired behavior for three generative tasks, L = 3, using Fash-
ion MNIST in Figure 6 and Table 2 above. The assumption made up till now is that zd accurately
captures the discriminative aspects of each distribution. However, there is no theoretical reason for
the model to impose this constraint on the latent variables. In practice, we often observe that the
decoder Pθ(x|zc, zd) ignores zd due to the much richer representation of the continuous variable,
zc. In the following section we introduce a mutual information constraint that encourages the model
to fully utilize zd.

4.2.2

INFORMATION RESTRICTING REGULARIZER

As eluded to in the previous section, the synthetic samples observed by the student model need to be
representative of all previous distributions. In order to control sampling via the process described in
Section 4.2.1, we need to enforce that the discrete variable, zd, carries the discriminative informa-
tion about each distribution. Given our graphical model from Figure 4-Left, we observe that there
are two ways to accomplish this: maximize the information content between the discrete random
variable, zd and the decoded ˆx, or minimize the information content between the continuous vari-
able, zc and the decoded ˆx. Since our graphical model and underlying network does not contain skip
connections, information from the input, x, has to ﬂow through the latent variables z = [zc, zd] to
reach the decoder. While both formulations can theoretically achieve the same objective, we ob-
served that in practice, minimizing I(ˆx, zc) provided better results. We believe the reason for this
is that minimizing I(ˆx, zc) provides the model with more subtle gradient information in contrast to
maximizing I(ˆx, zd) which receives no gradient information when the value of the k-th element of
the categorical sample is 1. We now formalize our mutual information regularizer, which we derive
from ﬁrst principles in Equation 17:

Ex∼Pi(x)[I(ˆx, zc)] = Ex∼Pi(x)[H(zc) − H(zc|ˆx)]

(17)

= Ex∼Pi(x)Ezc∼Qφ(zc|x)
(cid:123)(cid:122)
Ex∼Pi(x)[H(zc)]
+

(cid:124)

(cid:20)

(cid:21)
− log Qφ(zc|x)

(cid:125)

Ex∼Pi(x)E(zd,zc)∼Qφ(zc,zd|x)
(cid:124)

(cid:21)
(cid:20)
Eˆx∼Pθ (ˆx|zc,zd) log Qφ(zc|ˆx)
,
(cid:123)(cid:122)
Ex∼Pi(x)[−H(zc|ˆx)]

(cid:125)

where we use the independence assumption of our posterior from Equation 13 and the fact that the
expectation of a constant is the constant. This regularizer has parallels to the regularizer in InfoGAN
[18]. In contrast to InfoGAN, VAEs already estimate the posterior Qφ(zc|x) and thus do not need
the introduction of any extra parameters φ for the approximation. In addition [47] demonstrated that
InfoGAN uses the variational bound (twice) on the mutual information, making its interpretation
unclear from a theoretical point of view. In contrast, our regularizer has a clear interpretation: it
restricts information through a speciﬁc latent variable within the computational graph. We observe
that this constraint is essential for empirical performance of our model and empirically validate this
in our ablation study in Experiment 7.2.

4.3 LEARNING OBJECTIVE

The ﬁnal learning objective for each of the student models is the maximization of the sequential
VAE ELBO (Equation 10), coupled with generative replay (Equation 7)and the mutual information
regularizer, I(ˆx, zc), (Equation 17):

11

Lθ,φ(x) = EQφ(zc,zd|x)
(cid:124)

(cid:20)

(cid:21)
log Pθ(x|zc, zd)

− KL[Qφ(zc, zd|x)||P (zc, zd)]

(18)

(cid:125)

(cid:124)

(cid:123)(cid:122)
VAE ELBO
+ (1 − ω)KL[Qφ(zc, zd|x)||QΦ(zc, zd|x)]
(cid:125)
(cid:123)(cid:122)
Posterior Consistency Regularizer
− λI(ˆx, zc)
(cid:125)

(cid:123)(cid:122)
Mutual Information

(cid:124)

,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

The λ hyper-parameter controls the importance of the information gain regularizer. Too large a
value for λ causes a lack of sample diversity, while too small a value causes the model to not use
the discrete latent distribution. We did a random hyperparameter search and determined λ = 0.01
to be a reasonable choice for all of our experiments. This is in line with the λ used in InfoGAN
[18] for continuous latent variables. We empirically validate the necessity of both terms proposed in
Equation 18 in our ablation study in Experiment 7.2. We also validate the beneﬁt of the latent vari-
able factorization in Experiment 7.1. Before delving into the experiments, we provide a theoretical
analysis of computational complexity induced by our model and objective (Equation 18) in Section
4.4 below.

4.4 COMPUTATIONAL COMPLEXITY

We deﬁne the computational complexity of a typical VAE encoder and decoder as O(E)
and O(D) correspondingly;
internally these are dominated by the matrix-vector products
which take approximately LO(n2) for L layers. We also deﬁne the cost of applying the loss
function as O(K) + O(R), where O(K) is the cost of evaluating the KL divergence from
the ELBO (Equation 4) and O(R) the cost for evaluating the reconstruction term. Given
these deﬁnitions, we can summarize LGM’s computation complexity as follows in Equation 19:

O(D)
(cid:124) (cid:123)(cid:122) (cid:125)
teacher
generations

+ O(E) + O(D)
(cid:123)(cid:122)
(cid:125)
student encode
+ decode

(cid:124)

+ O(K) + O(R)
(cid:125)
(cid:123)(cid:122)
vae loss

(cid:124)

+ O(K)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
regularizer

+ O(K) + O(E)
(cid:123)(cid:122)
(cid:125)
mutual info

(cid:124)

= 2[O(D) + O(E)] + 3[O(K)] + O(R),

(19)

where we introduce increased computational complexity due to teacher generations, the cost of the
posterior regularizer, and the mutual information terms; the latter of which necessitates an extra
encode operation, O(E). The computational complexity is still dominated by the matrix-vector
product from evaluating forward functionals of the neural network. These operations can easily be
amortized through parallelization on modern GPUs and typical experiments do not directly scale
as per Equation 19. In our most demanding experiment (Experiment 6.6), we observe an average
empirical increase of 13.53 seconds per training epoch and 6.3 seconds per test epoch.

5 REVISITING STATE OF THE ART METHODS.

In this section we revisit some of the state of the art methods from Section 2. We begin by providing
a mathematical description of the differences between EWC [65], VCL [91] and LGM and follow it
up with a discussion of VASE [1] and their extensions of our work.

EWC and VCL: Our posterior regularizer, KL[Qφ(z|x)||QΦ(z|x)], affects the same parameters,
φ, as parameter regularizer methods such as EWC and VCL. However, rather than assuming a func-
tional form for the parameter posterior, P (φ|x), our method regularizes the output latent distribu-
tion Qφ(z|x). EWC and VCL, both make the assumption that P (φ|x) is distributed as an isotropic
gaussian3. This allows the use of the Fisher Information Matrix (FIM) in a quadratic parameter reg-
ularizer in EWC, and an analytical KL divergence of the posterior in VCL. This is a very stringent

3VCL assumes an isotropic gaussian variational form vs. EWC which directly assumes the parametric form

on P (φ|x).

12

requirement for the parameters of a neural network and there is active research in Bayesian neural
networks that attempts to relax this constraint [74, 75, 80].

EWC minφ d[P (φ|x)||P (Φ|x)]
2 (φ − Φ)T F (φ − Φ)

≈ γ

LGM ( Isotropic Gaussian Posterior ) minφ d[Qφ(z|x)||QΦ(z|x)]
|ΣΦ|
|Σφ|

Φ Σφ) + (µΦ − µφ)T Σ−1

Φ (µΦ − µφ) − C + log

(cid:20)
tr(Σ−1

(cid:18)

= 0.5

(cid:19)(cid:21)

In the above table we examine the distance metric d, used to minimize the effects of catastrophic
inference in both EWC and LGM. While our method can operate over any distribution that has
a tractable KL-divergence, for the purposes of demonstration we examine the simple case of an
isotropic gaussian latent-variable posterior. EWC directly enforces a quadratic constraint on the
model parameters φ, while our method indirectly affects the same parameters through a regulariza-
tion of the posterior distribution Qφ(z|x). For any given input variate, xj, LGM allows to model to
freely change its internal parameters, φ; it does so in a non-linear4 way such that the analytical KL
shown above is minimized.

VASE : The recent work of Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies (VASE) [1] extend upon our work [1, p. 7], but take a more empirical route by
incorporating a classiﬁcation-based heuristic for their posterior distribution. In contrast, we show
(Section 4.1.1) that our objective naturally emerges in a sequential learning setting for VAEs, allow-
ing us to infer the discrete posterior, Qφ(zd|x) in an unsupervised manner. Due to the incorporation
of direct supervised class information [1] also observe that regularizing the decoding distribution
Pθ(x|z) aids in the learning process, something that we observe to fail in a purely unsupervised
generative setting (Appendix Section 10.2). Finally, in contrast to [1], we include an information
restricting regularizer (Section 4.2.2) which allows us to directly control the interpretation and ﬂow
of information of the learnt latent variables.

6 EXPERIMENTS

We evaluate our model and the baselines over standard datasets used in other state of the art life-
long / continual learning literature [91, 136, 112, 57, 65, 94]. While these datasets are simple in
a traditional classiﬁcation setting, transitioning to a lifelong-generative setting scales the problem
complexity substantially. We evaluate LGM on a set of progressively more complicated tasks (Sec-
tion 6.2) and provide comparisons against baselines [91, 136, 65, 29, 63] using a set of standard
metrics (Section 6.1). All network architectures and other optimization details for our LGM model
are provided in Appendix Section 10.3 as well our open-source git repository [98].

6.1 PERFORMANCE METRICS

To validate the beneﬁt of LGM in a lifelong setting we explore three main performance dimensions:
the ability for the model to reconstruct and generate samples from all previous tasks and the ability
to learn a common representation over time, thus reducing learning sample complexity. We use three
main quantitative performance metrics for our experiments: the log-likelihood importance sample
estimate [15, 91], the negative test ELBO, and the Frechet distance metric [44]. In addition, we also
provide two auxiliary metrics to validate the beneﬁts of LGM in a lifelong setting: training sample
complexity and wall clock time per training and test epoch.

To fairly compare models with varying latent variable conﬁgurations, one solution is to marginal-
ize out the latents, z, during model evaluation / test time: (cid:82)
k=1 Pθ(x|z = zk).
This is realized in practice by using a Monte Carlo approximation (typically K=5000) and is com-
monly known as the importance sample (IS) log-likelihood estimate [15, 91]. As latent variable
and model complexity grows, this estimate tends to become noisier and intractable to compute. For
our experiments we use this metric only for the FashionMNIST and MNIST datasets as computing
one estimate over 10,000 test samples for a complex model takes approximately 35 hours on a K80
GPU.

z Pθ(x|z)dz ≈ (cid:80)K

In contrast to the IS log-likelihood estimate, the negative test ELBO (Equation 4) is only applicable
when comparing models with the same latent variable conﬁgurations; it is however much faster to

4This is because the parameters of the distribution are modeled by a deep neural network.

13

compute. The negative test ELBO provides a lower bound to the test log-likelihood of the true data
distribution under the assumed latent variable conﬁguration. One crucial aspect missing from both
these metrics is an evaluation of generation quality. We resolve this by using the Frechet distance
metric [44] and qualitative image samples.

The Frechet distance metric allows us to quantify the quality and diversity of generated samples by
using a pre-trained classiﬁer model to compare the feature statistics (generally under a Gaussian-
ity assumption) between synthetic generated samples and samples drawn from the test set. If the
Frechet distance between these two distributions is small, then the generative model is said to be
generating realistic images. The Frechet distance between two gaussians (produced by evaluating
latent embeddings of a classiﬁer model) with means mtest, mgen with corresponding covariances
Ctest, Cgen is:

||mtest − mgen||2

2 + T r(Ctest + Cgen − 2[CtestCgen]0.5).

(20)

While the Frechet distance, negative ELBO and IS log-likelihood estimate provide a glimpse into
model performance, there exists no conclusive metric that captures the quality of unsupervised gen-
erative models [124, 108] and active research suggests a direct trade-off between perceptual quality
and model representation [8]. Thus, in addition to the metrics described above, we also provide
qualitative metrics in the form of test image reconstructions and image generations. We summarize
all used performance metrics in Table 3 below:

Negative ELBO

Deﬁnition

Equation 4.

Purpoose
Quantitative metric on
likelihood / reconstructions.
Quantitative metric on
density estimate.
Quantitative metric on generations.

Lower is better?

yes

Negative Log-Likelihood

Frechet Distance
Test Reconstructions
Generations
#Training Samples

5000 (latent) sample Monte
Carlo estimate of Equation 4.
Equation 20.
yes
Pθ(x|zc ∼ Qφ(zc|x), zd ∼ Qφ(zd|x)) Qualitative view of reconstructions. N/A
Pθ(x|zd ∼ Cat(1/L), zc ∼ N (0, 1)).
N/A
# real training samples used for task i.
yes

Qualitative view of generations.
Sample Complexity.

yes

Table 3: Summary of different performance metrics.

6.2 DATA FLOW

Figure 7: Visual examples of training and test task sequences (top to bottom) for the datasets used to validate
LGM. The training set only consists of samples from the current task while the test set is a cumulative union
of the current task, coupled with all previous tasks. The permuted MNIST tasks uses {G1, ...GL−1} different
ﬁxed permutation matrices to create 4 auxiliary datasets.

14

In Figure 7 we list train and test variates depicting the data ﬂow for each of the problems that we
model. Due to the relaxing the need to preserve data in a lifelong setting, the train task sequence ob-
serves a single dataset, Dtr
i , at a time, without access to any previous, Dtr
<i. The corresponding test
dataset consists of a union (∪ operator) of the current test dataset, Dte
i , merged with all previously
observed test datasets, ˆD
i ∪ Dte

i−1 ∪ ... ∪ Dte
1 .

te
i = Dte

MNIST / Fashion MNIST: For the MNIST and Fashion MNIST problems, we observe a single
MNIST digit or fashion object (such as shirts) at a time. Each training set consists of 6000 training
samples and 1000 test samples. These samples are originally extracted from the full training and
test datasets which consist of 60,000 training and 10,000 test samples.

Permuted MNIST: this problem differs from the MNIST problem described above in that we use
the entire MNIST dataset at each task. After observing the ﬁrst task, which is the standard MNIST
dataset, each subsequent task differs through the application of a ﬁxed permutation matrix Gi on
the entire MNIST dataset. The test task sequence differs from the training task sequence in that we
simply use the corresponding full train and test MNIST datasets (with the appropriate application of
Gi).

Celeb-A: We split the CelebA dataset into four individual distributions using the features: bald,
male, young and eye-glasses. As with the previous problems, we treat each subset of data as an
individual distribution, and present our model samples from a single distribution at a time. This
presents a real world scenario as the samples per distribution varies drastically from only 3,713
samples for the bald distribution, to 126,788 samples for young. In addition speciﬁc samples can
span one or more of these distributions.

SVHN to MNIST: in this problem, we transition from fully observing the centered SVHN [89]
dataset to observing the MNIST dataset. We treat all samples from SVHN as being generated by
one distribution P1(x) and all the MNIST 5 samples as generated by another distribution P2(x)
(irrespective of the speciﬁc digit). At inference, the model is required to reconstruct and generate
from both datasets.

6.3 SITUATING AGAINST STATE OF THE ART LIFELONG LEARNING MODELS.
To situate LGM against other state of the art methods in lifelong learning we use the sequential
FashionMNIST and MNIST datasets described earlier in Section 6.2 and the data ﬂow diagram in
Figure 7. We contrast our LGM model against VCL [91], VCL without a task speciﬁc head network,
SI [136], EWC [65], Laplace propagation [29], a full batch VAE trained jointly on all data and a
standard naive sequential VAE without any catastrophic forgetting prevention strategy in Figures 8
and 9 below. The full batch VAE presents the upper-bound performance and all lifelong learning
models typically under-perform this model by the ﬁnal learning task. For the baselines, we use
the generously open sourced code [90] by the VCL authors, using the optimal hyper-parameters
speciﬁed for each model. We begin by evaluating the 5000 sample Monte Carlo estimate of the
log-likelihood of all compared models in Figure 8 below:

Figure 8: IS log-likelihood (mean ± std) × 5. Left: Fashion MNIST. Right: MNIST.

5MNIST was resized to 32x32 and converted to RBG to make it consistent with the dimensions of SVHN.

15

Even though each trial was repeated ﬁve times (each), we observe large increases in the estimates
at a few critical points. After further inspection, we determined the large magnitude increases were
due to the model observing a drastically different distribution at that point. We overlay the graphs
with an example variate for of the magnitude spikes. In the case of FashionMNIST for example,
the model observes its ﬁrst shoe distribution at i = 6; this contrasts the previously observed items
which were mainly clothing related objects. Interestingly we observe that LGM has much smoother
performance across tasks. We posit this is because LGM does not constrain its parameters, and
instead enforces the same input-output mapping through functional regularization.

Figure 9: Final model, i = 10, generation and reconstructions for MNIST and FashionMNIST. The LGM
model presents competitive performance for both generations and reconstructions, while not preserving any
past data nor past models.

16

Since one of the core tenets of lifelong learning is to reduce sample complexity over time, we use
this experiment to validate if LGM does in fact achieve this objective. Since all LGM models are
trained with an early-stopping criterion, we can directly calculate the number of samples used for
each learning task using the stopping epoch and mean, π of the Bernoulli sampling distribution of
the student model. In Figure 10 we plot the number of true samples and the number of synthetic
samples used by a model until it satisﬁed its early-stopping criterion. We observe a steady decrease
in the number of real samples used over time, validating LGMs advantage in a lifelong setting.

Figure 10: FashionMNIST sample complexity. Left: Synthetic training samples used till early-stopping. Right:
Real samples used till early-stopping.

6.4 DIVING DEEPER INTO THE SEQUENCE.
Rather than only visualizing the ﬁnal model’s qualitative results as in Figure 9, we provide quali-
tative results for model performance over time for the PermutedMNIST experiment in Figure 11.
This allows us to visually observe lifelong model performance over time. In this experiment, we
focus our efforts on EWC and LGM and visualize model (test) reconstructions starting from the
second learning task, G1D, till the ﬁnal G4D. The EWC-VAE variant that we use as a baseline
has the same latent variable conﬁguration as our model, enabling the usage of the test ELBO as a
quantitative metric for comparison. We use an unpermuted version of the MNIST dataset, D, as our
ﬁrst distribution, P1(x), as it allows us to visually asses the degradation of reconstructions. This is a
common setup utilized in continual learning [65, 136] and we extend it here to the density estimation
setting.

Figure 11: Top row: test-samples; bottom row: reconstructions. We visualize an increasing number of accu-
mulated distributions from left to right. (a) Lifelong VAE model (b) EWC VAE model.

17

Both models exhibit a different form of degradation: EWC experiences a more destructive form of
degradation as exempliﬁed by the salt-and-pepper noise observed in the ﬁnal dataset reconstruction
at G4D. LGM on the hand experiences a form of Gaussian noise as visible in the corresponding
ﬁnal dataset reconstruction. In order to numerically quantify this performance we analyze the log-
Frechet distance and negative ELBO below in Figure 12, where we contrast the LGM to EWC, a
batch VAE (full-vae in graph), an upto-VAE that observes all training data up to the current distri-
bution and a vanilla sequential VAE (vanilla). We examine a variety of different convolutional and
dense architectures and present the top performing models below. We observe that LGM drastically
outperforms EWC and the baseline naive sequential VAE in both metrics.

Figure 12: PermutedMNIST (a) negative Test ELBO and (b) log-Frechet distance.

6.5 LEARNING ACROSS COMPLEX DISTRIBUTIONS

Figure 13: (a) Reconstructions of test samples from SVHN[left] and MNIST[right]; (b) Decoded samples
ˆx ∼ Pθ(x|zd, zc) based on linear interpolation of zc ∈ R2 with zd = [0, 1]; (c) Same as (b) but with
zd = [1, 0].

The typical assumption in lifelong learning is that the sequence of observed distributions are related
[125] in some manner. In this experiment we relax this constraint by learning a common model
between the colored SVHN dataset and the binary MNIST dataset. While semantically similar to
humans, these datasets are vastly different, as one is based on RGB images of real world house
numbers and the other of synthetically hand-drawn digits. We visualize examples of the true test
inputs, x, and their respective reconstructions, ˆx, from the ﬁnal lifelong model in ﬁgure 13(a). Even
though the only true data the ﬁnal model received for training was the MNIST dataset, it is still able
to reconstruct the SVHN data observed previously. This demonstrates the ability of our architecture
to transition between complex distributions while still preserving the knowledge learned from the
previously observed distributions.

Finally, in ﬁgure 13(b) and 13(c) we illustrate the data generated from an interpolation of a 2-
dimensional continuous latent space, zc ∈ R2. To generate variates, we set the discrete categorical,
zd, to one of the possible values {[0, 1], [1, 0]} and linearly interpolate the continuous zc over the
range [−3, 3]. We then decode these to obtain the samples, ˆx ∼ Pθ(x|zd, zc). The model learns

18

a common continuous structure for the two distributions which can be followed by observing the
development in the generated samples from top left to bottom right on both ﬁgure 13(b) and 13(c).

6.6 VALIDATING EMPIRICAL SAMPLE COMPLEXITY USING CELEB-A

We iterate the Celeb-A dataset as described in the data ﬂow diagram (Figure 7) and use this learn-
ing task to explore qualitative and quantitative generations, as well as empirical real world time
complexity (as described in Section 4.4) on modern GPU hardware. We train a lifelong model and
a typical VAE baseline without catastrophic forgetting mitigation strategies and evaluate the ﬁnal
model’s generations in Figure 14. As visually demonstrated in Figure 14-Left, the lifelong model is
able to generate instances from all of the previous distributions, however the baseline model catas-
trophically forgets (Figure 14-Right) and only generates samples from the eye-glasses distribution.
This is also reinforced by the log-Frechet distance shown in Figure 15.

Figure 14: Left: Sequential generations for Celeb-A from the ﬁnal lifelong model for bald, male, young and
eye-glasses (left to right). Right: (random) generations by the ﬁnal baseline VAE model.

We also evaluate the wall-clock time in seconds (Table 4) for the lifelong model and the baseline-vae
for the 44,218 samples of the male distribution. We observe that the lifelong model does not add a
signiﬁcant overhead, especially since the baseline-vae undergoes catastrophic forgetting (Figure 14
Right) and completely fails to generate samples from previous distributions. Note that we present
the number of parameters and other detailed model information in our code and Appendix 10.3.

44,218 male samples
training-epoch (s)
testing-epoch (s)

baseline-VAE
43.1 +/- 0.6
9.79 +/- 0.12

Lifelong
56.63 +/- 0.28
16.09 +/- 0.01

Table 4: Mean & standard deviation wall-clock for one epoch of
male distribution of Celeb-A.

Figure 15: Celeb-A log-Frechet distance of lifelong vs. naive baseline VAE model without catastrophic miti-
gation strategies over the four distributions. Listed on the right is the time per epoch (in seconds) for an epoch
of the corresponding models.

7 ABLATION STUDIES

In this section we independently validate the beneﬁt of each of the newly introduced components
to the learning objective proposed in Section 4.3. In Experiment 7.1 we demonstrate the beneﬁt
of the discrete-continuous posterior factorization introduced in Section 4.2.1. Then in Experiment

19

7.2, we validate the necessity of the information restricting regularizer (Section 4.2.2) and posterior
consistency regularizer (Section 4.1.1).

7.1 LINEAR SEPARABILITY OF DISCRETE AND CONTINUOUS POSTERIOR

Figure 16: Left: Graphical model depicting classiﬁcation using pretrained VAE, coupled with a linear classiﬁer,
fθlin : z (cid:55)→ y. Right: Linear classiﬁer accuracy on the Fashion MNIST test set for a varying range of latent
dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024] and distributions.

In order to validate that the (independent) discrete and continuous latent variable posterior,
Qφ(zd, zc|x), aids in learning a better representation, we classify the encoded posterior sample
using a simple linear classiﬁer fθlin : z (cid:55)→ y, where y corresponds to the categorical class predic-
tion. Higher (linear) classiﬁcation accuracies demonstrate that the the VAE is able to learn a more
linearly separable representation. Since the latent representation of VAEs are typically used in aux-
iliary tasks, learning such a representation is useful in downstream tasks. This is a standard method
to measure posterior separability and is used in methods such as Associative Compression Networks
[41].

We use the standard training set of FashionMNIST [135] (60,000 samples) to train a standard VAE
with a discrete only (disc) posterior, an isotropic-gaussian only (gauss) posterior, a bernoulli only
(bern) posterior and ﬁnally the proposed independent discrete and continuous (disc+gauss) posterior
presented in Section 4.2.1. For each different posterior reparameterization, we train a set of VAEs
with varying latent dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024]. In the case of the disc+gauss
model we ﬁx the discrete dimension, |zd| = 10 and vary the isotropic-gaussian dimension to match
the total required dimension. After training each VAE, we proceed to use the same training data to
train a linear classiﬁer on the encoded posterior sample, z ∼ Qφ(z|x).

In Figure 16 we present the mean and standard deviation linear test classiﬁcation accuracies of each
set of the different experiments. As expected, the discrete only (disc) posterior performs poorly due
to the strong restriction of mapping an entire input sample to a single one-hot vector. The isotropic-
gaussian (gauss) and bernoulli (bern) only models provide a strong baseline, but the combination
of isotropic-gaussian and discrete posteriors (disc+gauss) performs much better, reaching an upper-
bound (linear) test-classiﬁcation accuracy of 87.1%. This validates that the decoupling of latent
represention presented in Section 4.2.1 aids in learning a more meaningful, separable posterior.

7.2 VALIDATING THE MUTUAL INFORMATION AND POSTERIOR CONSISTENCY

REGULARIZERS.

In order to independently evaluate the beneﬁt of our proposed Bayesian update regularizer (Sec-
tion 4.1.1) and the mutual information regularizer proposed in (Section 4.2.1) we perform an ab-
lation study using the MNIST data ﬂow sequence from Figure 7. We evaluate three scenarios: 1)
with posterior consistency and mutual information regularizers, 2) only posterior consistency and
3) without both regularizers. We observe that both components are necessary in order to gener-
ate high quality samples as evidenced by the negative test ELBO in Figure 17-(a) and the corre-

20

Figure 17: MNIST Ablation: (a) negative test ELBO. (b) Sequentially generated samples by setting zd and
sampling zc ∼ N (0, 1) (Section 4.2.1) with consistency + mutual information (MI). (c) Sequentially generated
samples with no consistency + no mutual information (MI).

sponding generations in Figure 17-(b-c). The generations produced without the information gain
regularizer and consistency in Figure 17-(c) are blurry. We attribute this to: 1) uniformly sam-
pling the discrete component is not guaranteed to generate samples representative samples from
P<i(x) and 2) the decoder, PΘ(x|zd, zc), relays more information through the continuous compo-
nent, PΘ(x|zd, zc) = PΘ(x|zc), causing catastrophic forgetting and posterior collapse [4].

8 LIMITATIONS

While LGM presents strong performance, it fails to completely solve the problem of lifelong gen-
erative modeling and we see a slow degradation in model performance over time. We attribute this
mainly to the problem of poor VAE generations that compound upon each other (also discussed
below). In addition, there are a few poignant issues that need to be resolved in order to achieve
an optimal (in terms of non-degrading Frechet distance / -ELBO) unsupervised generative lifelong
learner:

Distribution Boundary Evaluation: The standard assumption in current lifelong / continual learn-
ing approaches [91, 136, 112, 57, 65, 94] is to use known, ﬁxed distributions instead of learning
the distribution transition boundaries. For the purposes of this work, we focus on the accumulation
of distributions (in an unsupervised way), rather than introduce an additional level of indirection
through the incorporation of anomaly detection methods that aid in detecting distributional bound-
aries.

Blurry VAE Generations: VAEs are known to generate images that are blurry in contrast to GAN
based methods. This has been attributed to the fact that VAEs don’t learn the true posterior and
make a simplistic assumption regarding the reconstruction distribution Pθ(x|z) [4, 97]. While there
exist methods such as ALI [27] and BiGAN [26], that learn a posterior distribution within the GAN
framework, recent work has shown that adversarial methods fail to accurately match posterior-prior
distribution ratios in large dimensions [104].

Memory: In order to scale to a truly lifelong setting, we posit that a learning algorithm needs a
global pool of memory that can be decoupled from the learning algorithm itself. This decoupling
would also allow for a principled mechanism for parameter transfer between sequentially learnt
models as well a centralized location for compressing non-essential historical data. Recent work

21

such as the Kanerva Machine [133] and its extensions [134] provide a principled way to do this in
the VAE setting.

9 CONCLUSION

In this work we propose a novel method for learning generative models over a lifelong setting. The
principal assumption for the data is that they are generated by multiple distributions and presented
to the learner in a sequential manner. A key limitation for the learning process is that the method
has no access to any of the old data and that it shall distill all the necessary information into a
single ﬁnal model. The proposed method is based on a dual student-teacher architecture where the
teacher’s role is to preserve the past knowledge and aid the student in future learning. We argue for
and augment the standard VAE’s ELBO objective by terms helping the teacher-student knowledge
transfer. We demonstrate the beneﬁts this augmented objective brings to the lifelong learning setting
using a series of experiments. The architecture, combined with the proposed regularizers, aid in
mitigating the effects of catastrophic interference by supporting the retention of previously learned
knowledge.

REFERENCES

[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. Life-
long disentangled representation learning with cross-domain latent homologies. In Advances
in Neural Information Processing Systems, pages 9895–9905, 2018.

[2] W.-K. Ahn and W. F. Brewer. Psychological studies of explanationbased learning. In Investi-

gating explanation-based learning, pages 295–316. Springer, 1993.

[3] W.-K. Ahn, R. J. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one ex-
ample: Psychological evidence for explanation-based learning. Technical report, Coordinated
Science Laboratory, University of Illinois at Urbana-Champaign, 1987.

[4] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken

elbo. In International Conference on Machine Learning, pages 159–168, 2018.

[5] J. R. Anderson and G. H. Bower. Human associative memory. Psychology press, 2014.
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align

and translate. ICLR, 2015.

[7] H. R. Blackwell. Contrast thresholds of the human eye. JOSA, 36(11):624–643, 1946.
[8] Y. Blau and T. Michaeli. The perception-distortion tradeoff.

In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018.

[9] A. Blum. On-line algorithms in machine learning. In Online algorithms, pages 306–325.

Springer, 1998.

[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning, pages 1613–1622, 2015.
[11] L. Bottou. Online learning and stochastic approximations. On-line learning in neural net-

works, 17(9):142, 1998.

[12] L. Bottou and Y. L. Cun. Large scale online learning. In Advances in neural information

processing systems, pages 217–224, 2004.

[13] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.

[14] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational
bayes. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 1727–1735, 2013.

[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. ICLR, 2016.
[16] M. F. Carr, S. P. Jadhav, and L. M. Frank. Hippocampal replay in the awake state: a potential
substrate for memory consolidation and retrieval. Nature neuroscience, 14(2):147, 2011.

[17] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[18] X. Chen, X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.

22

In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc., 2016.
[19] Z. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and

big data. In International Conference on Machine Learning, pages 703–711, 2014.

[20] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning, 10(3):1–145, 2016.

[21] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent
variable model for sequential data. In Advances in neural information processing systems,
pages 2980–2988, 2015.

[22] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International Conference on Machine Learning, pages 1282–1289,
2019.

[23] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson. Human photoreceptor

topography. Journal of comparative neurology, 292(4):497–523, 1990.

[24] P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and

related ﬁelds, 2(4):555–581, 1996.

[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

[26] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782, 2016.

[27] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and

A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
[28] E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances

in Neural Information Processing Systems, pages 708–718, 2018.

[29] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace propagation. In Advances in Neural

Information Processing Systems, pages 441–448, 2004.

[30] L. Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories.
In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141.
IEEE, 2003.

[31] G. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable.

In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1565–1574. ACM, 2016.

[32] A. Fiat and G. J. Woeginger. Online algorithms: The state of the art, volume 1442. Springer,

1998.

[33] T. Furlanello, J. Zhao, A. M. Saxe, L. Itti, and B. S. Tjan. Active long term memory networks.

arXiv preprint arXiv:1606.02355, 2016.

[34] A. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities.

Journal of the American statistical association, 85(410):398–409, 1990.

[35] S. Gershman and N. Goodman. Amortized inference in probabilistic reasoning. In Proceed-

ings of the Cognitive Science Society, volume 36, 2014.

[36] Z. Ghahramani and H. Attias. Online variational bayesian learning.

In Slides from talk

presented at NIPS workshop on Online Learning, 2000.

[37] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats, volume 9, pages 249–256, 2010.

[38] R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric bayesian mix-
ture models. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2008.

[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.

[40] A. G. A. P. Goyal, A. Sordoni, M.-A. Cˆot´e, N. R. Ke, and Y. Bengio. Z-forcing: Training
stochastic recurrent networks. In Advances in neural information processing systems, pages
6713–6723, 2017.

[41] A. Graves, J. Menick, and A. v. d. Oord. Associative compression networks. arXiv preprint

arXiv:1804.02476, 2018.

23

[42] G. Grimmett, G. R. Grimmett, D. Stirzaker, et al. Probability and random processes. Oxford

university press, 2001.

[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6629–6640, 2017.

[45] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. stat,

[46] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1050:9, 2015.

1780, 1997.

[47] F. Huszar. Infogan: using the variational bound on mutual information (twice), Aug 2016.
[48] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reduc-

ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[49] V. Jain and E. Learned-Miller. Online domain adaptation of a pre-trained cascade of classi-
ﬁers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
577–584. IEEE, 2011.

[50] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Interna-

tional Conference on Learning Representations, 2017.

[51] H. Jeffreys. An invariant form for the prior probability in estimation problems. In Proceedings
of the Royal Society of London a: mathematical, physical and engineering sciences, volume
186, pages 453–461. The Royal Society, 1946.

[52] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper-
vised and generative approach to clustering. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, pages 1965–1972. AAAI Press, 2017.

[53] A. Johnson and A. D. Redish. Neural ensembles in ca3 transiently encode paths forward of

the animal at a decision point. Journal of Neuroscience, 27(45):12176–12189, 2007.

[54] M. I. Jordan. Artiﬁcial neural networks. chapter Attractor Dynamics and Parallelism in a
Connectionist Sequential Machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
[55] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[56] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering, 82(1):35–45, 1960.

[57] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learn-

ing. arXiv preprint arXiv:1710.10368, 2017.

[58] M. P. Karlsson and L. M. Frank. Awake replay of remote experiences in the hippocampus.

Nature neuroscience, 12(7):913, 2009.

[59] M. Karpinski and A. Macintyre. Polynomial bounds for vc dimension of sigmoidal and
general pfafﬁan neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.

[60] I. Katakis, G. Tsoumakas, and I. Vlahavas. Incremental clustering for the classiﬁcation of

[61] H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine

concept-drifting data streams.

Learning, pages 2654–2663, 2018.

[62] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. 2015.
[63] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
[64] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

[65] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–
1105, 2012.

[67] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

24

[68] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning, pages
1558–1566, 2016.

[69] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis. Continual classiﬁcation learning

using generative models. CoRR, abs/1810.10612, 2018.

[70] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks, 3361(10):1995, 1995.

[71] Z. Li and D. Hoiem. Learning without forgetting.

In European Conference on Computer

Vision, pages 614–629. Springer, 2016.

[72] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang,
In Proceedings of the European

and K. Murphy. Progressive neural architecture search.
Conference on Computer Vision (ECCV), pages 19–34, 2018.

[73] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder.

ICLR, 2016.

[74] C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[75] C. Louizos and M. Welling. Multiplicative normalizing ﬂows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 2218–2227. JMLR. org, 2017.

[76] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation

of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[77] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders.

arXiv preprint arXiv:1511.05644, 2015.

[78] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.
[79] J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling
on streams. In Advances in Neural Information Processing Systems, pages 1153–1161, 2015.
[80] A. Mishkin, F. Kunstner, D. Nielsen, M. Schmidt, and M. E. Khan. Slang: Fast structured
covariance approximations for bayesian deep learning with natural gradient. In Advances in
Neural Information Processing Systems, pages 6245–6255, 2018.

[81] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the ACM, 61(5):103–
115, 2018.

[82] T. M. Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research, 1980.

[83] T. M. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. D. Mishra,
M. Gardner, B. Kisiel, J. Krishnamurthy, et al. Never-ending learning. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[84] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in

machine learning. CoRR, abs/1906.10652, 2019.

[85] E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In International Confer-

ence on Learning Representations (ICLR), 2017.

[86] R. M. Neal. Bayesian Learning For Neural Networks. PhD thesis, University of Toronto,

[87] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte

1995.

carlo, 2(11):2, 2011.

[88] W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically exact, embarrassingly parallel
In N. L. Zhang and J. Tian, editors, Proceedings of the Thirtieth Conference on
MCMC.
Uncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27,
2014, pages 623–632. AUAI Press, 2014.

[89] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
In NIPS workshop on deep learning and

ural images with unsupervised feature learning.
unsupervised feature learning, page 5, 2011.

[90] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. nvcuong/variational-continual-learning,

2018.

[91] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. ICLR, 2018.

25

[92] K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, R. A. Olshen, and K. L. Oehler. Bayes risk
weighted vector quantization with posterior estimation for image compression and classiﬁca-
tion. IEEE Transactions on Image Processing, 5(2):347–360, 1996.

[93] F. A. Quintana and P. L. Iglesias. Bayesian clustering and product partition models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):557–574, 2003.
[94] N. C. Rabinowitz, G. Desjardins, A.-A. Rusu, K. Kavukcuoglu, R. T. Hadsell, R. Pascanu,
J. Kirkpatrick, and H. J. Soyer. Progressive neural networks, Nov. 23 2017. US Patent App.
15/396,319.

[95] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep con-

volutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[96] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are

unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

[97] T. Rainforth, A. Kosiorek, T. A. Le, C. Maddison, M. Igl, F. Wood, and Y. W. Teh. Tighter
variational bounds are not necessarily better. In International Conference on Machine Learn-
ing, pages 4277–4285, 2018.

[98] J. Ramapuram. Lifelongvae pytorch repository., 2017.
[99] A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with delta-

vaes. ICLR, 2019.

[100] A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with

VQ-VAE-2. CoRR, abs/1906.00446, 2019.

[101] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
In International Conference on Machine Learning,

inference in deep generative models.
pages 1278–1286, 2014.

[102] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,

[103] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

1997.

statistics, pages 400–407, 1951.

[104] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational

inference. arXiv preprint arXiv:1802.06847, 2018.

[105] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Comput.,

11(2):305–345, Feb. 1999.

[106] R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method.

Annals of Operations Research, 39(1):229–250, 1992.

[107] P. Ruvolo and E. Eaton. Ella: An efﬁcient lifelong learning algorithm.

In International

Conference on Machine Learning, pages 507–515, 2013.

[108] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models
via precision and recall. In Advances in Neural Information Processing Systems, pages 5228–
5237, 2018.

[109] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[110] N. W. Schuck and Y. Niv. Sequential replay of nonspatial task states in the human hippocam-

pus. Science, 364(6447):eaaw5181, 2019.

[111] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning, pages 4555–
4564, 2018.

[112] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In

Advances in Neural Information Processing Systems, pages 2994–3003, 2017.

[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484, 2016.

[114] D. L. Silver, G. Mason, and L. Eljabu. Consolidation using sweep task rehearsal: overcoming
In Canadian Conference on Artiﬁcial Intelligence, pages

the stability-plasticity problem.
307–322. Springer, 2015.

[115] D. L. Silver and R. E. Mercer. The parallel transfer of task knowledge using dynamic learning
rates based on a measure of relatedness. In Learning to learn, pages 213–233. Springer, 1996.
[116] D. L. Silver and R. E. Mercer. The task rehearsal method of life-long learning: Overcoming
In Conference of the Canadian Society for Computational Studies of

impoverished data.
Intelligence, pages 90–101. Springer, 2002.

26

[117] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algo-

rithms. In 2013 AAAI spring symposium series, 2013.

[118] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[119] W. E. Skaggs and B. L. McNaughton. Replay of neuronal ﬁring sequences in rat hippocampus

during sleep following spatial experience. Science, 271(5257):1870–1873, 1996.

[120] E. D. Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems

Sciences, 168:69–96, 1998.

[121] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[122] F. Tanaka and M. Yamamura. An approach to lifelong reinforcement learning through multi-
ple environments. In 6th European Workshop on Learning Robots, pages 93–99, 1997.
[123] A. V. Terekhov, G. Montone, and J. K. O’Regan. Knowledge transfer in deep block-modular
In Proceedings of the 4th International Conference on Biomimetic and

neural networks.
Biohybrid Systems-Volume 9222, pages 268–279. Springer-Verlag New York, Inc., 2015.
[124] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pages 1–10, 2016.
[125] S. Thrun. Lifelong learning: A case study. Technical report, CARNEGIE-MELLON UNIV

PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1995.

[126] S. Thrun and T. M. Mitchell. Lifelong robot learning.

In The biology and technology of

intelligent autonomous agents, pages 165–196. Springer, 1995.

[127] J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artiﬁcial

Intelligence and Statistics, pages 1214–1223, 2018.

[128] V. Vapnik. Estimation of dependences based on empirical data. Springer Science & Business

Media, 2006.

[129] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, R. Powell, et al. Alphastar: Mastering the real-time strategy game
starcraft ii. DeepMind Blog, 2019.

[130] S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic
model. In Proceedings of the 25th international conference on world wide web, pages 167–
176. International World Wide Web Conferences Steering Committee, 2016.

[131] R. C. Williamson and U. Helmke. Existence and uniqueness results for neural network ap-

proximations. IEEE Transactions on Neural Networks, 6(1):2–13, 1995.

[132] M. C. Wittrock. Generative learning processes of the brain. Educational Psychologist,

[133] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed

27(4):531–541, 1992.

memory. ICLR, 2018.

[134] Y. Wu, G. Wayne, K. Gregor, and T. Lillicrap. Learning attractor dynamics for generative

memory. In Advances in Neural Information Processing Systems, pages 9401–9410, 2018.

[135] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[136] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence.

In

International Conference on Machine Learning, pages 3987–3995, 2017.

27

10 APPENDIX

10.1 UNDERSTANDING THE CONSISTENCY REGULARIZER

The analytical derivations of the consistency regularizer show that the regularizer can be interpreted
as an a transformation of the standard VAE regularizer. In the case of an isotropic gaussian posterior,
the proposed regularizer scales the mean and variance of the student posterior by the variance of the
teacher 1 and adds an extra ’volume’ term. This interpretation of the consistency regularizer shows
that the proposed regularizer preserves the same learning objective as that of the standard VAE.
Below we present the analytical form of the consistency regularizer with categorical and isotropic
gaussian posteriors:

Proof 1 We assume the learnt posterior of the teacher is parameterized by a centered, isotropic
gaussian with Φ = [µE = 0, ΣE = diag(σE2
)] and the posterior of our student by a non-centered
isotropic gaussian with φ = [µS, ΣS = diag(σS2)], then

(cid:20)
tr(ΣE−1
KL(Qφ(z|x)||QΦ(z|x)) = 0.5

ΣS) + (µE − µS)T ΣE−1

(µE − µS) − F + log

= 0.5

F
(cid:88)

(cid:20)

j=1

1
σE2(j)

= KL(Qφ∗(z|x)||N (0, I)) − log |ΣE|

(σS2(j) + µS2(j)) − 1 + log σE2(j) − log σS2(j)

Via a reparameterization of the student’s parameters:

φ∗ = [µS∗, σS∗2]

µS∗ =

; σS∗2 =

µS(j)
σE2(j)

σS2(j)
σE2(j)

(cid:19)(cid:21)

(cid:18) |ΣE|
|ΣS|
(cid:21)

(21)

(22)

It is also interesting to note that our posterior regularizer becomes the prior if:

limσE2 (cid:55)→1KL(Qφ(z|x)||QΦ(z|x)) = KL(Qφ(z|x)||N (0, I))

Proof 2 We parameterize the learnt posterior of the teacher by Φi =

exp(pE
i )
i=1 exp(pE
i )

(cid:80)J

and the pos-

terior of the student by φi =
i ) and cS = (cid:80)J
cE = (cid:80)J
i=1 exp(pE
The KL divergence from the ELBO can now be re-written as:

exp(pS
i )
i=1 exp(pS
i )
i=1 exp(pS

(cid:80)J

. We also redeﬁne the normalizing constants as

i ) for the teacher and student models respectively.

KL(Qφ(zd|x)||QΦ(zd|x)) =

J
(cid:88)

i=1

exp(pS
i )
cS

log

(cid:18) exp(pS
i )
cS

(cid:19)

cE
exp(pE
i )

= H(pS, pS − pE) = −H(ps) + H(pS, pE)

(23)

where H( ) is the entropy operator and H( , ) is the cross-entropy operator.

28

10.2 RECONSTRUCTION REGULARIZER

Figure 18: Fashion Negative Test ELBO

Figure 19: Fashion Log-Frechet Distance

While it is possible to constrain the reconstruction/decoder term of the VAE in a similar manner to
the consistency posterior-regularizer, i.e: KL[Pθ(ˆx|z)||PΘ(ˆx|z)], doing so diminishes model per-
formance. We hypothesize that this is due to the fact that this regularizer contradicts the objective of
the reconstruction term Pθ(x|z) in the ELBO which already aims to minimize some metric between
the input samples x and the reconstructed samples ˆx; eg: if Pθ(x|z) ∼ N (µ, diag[σ]), then the
loss is proportional to ||ˆx − x||2
2, the standard L2 loss. Without the addition of this reconstruction
cross-model regularizer, the model is also provided with more ﬂexibility in how it reconstructs the
output samples.

In order to quantify the this we duplicate the FashionMNIST Experiment listed in the data ﬂow
deﬁnition in Figure 7. We use a simpler model than the main experiments to validate this hypothesis.
We train two dense models (-D): one with just the posterior consistency regularizer (without-LL-
D) and one with the consistency and likelihood regularizer (with-LL-D). We observe the model
performance drops (with respect to the Frechet distance as well the test ELBO) in the case of the
with-LL-D as demonstrated in Figures 18 and 19.

10.3 MODEL ARCHITECTURE

We used two different architectures for our experiments. When we use a dense network (-D) we
used two layers of 512 units to map to the latent representation and two layers of 512 to map back
to the reconstruction for the decoder. We used batch norm [48] and ELU activations for all the
layers barring the layer projecting into the latent representation and the output layer. Note that
while we used the same architecture for EWC we observed a drastic negative effect when using
batch norm and thus dropped it’s usage. The convolution architectures (-C) used the architecture
described below for the encoder and the decoder (where the decoder used conv-transpose layers for
upsampling). The notation is [OutputChannels, (ﬁlterX, ﬁlterY), stride]:

Encoder: [32, (5, 5), 1] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 1] (cid:55)→

GN+ELU (cid:55)→ [256, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [512, (1, 1), 1] (cid:55)→
GN+ELU (cid:55)→ [512, (1, 1), 1]

Decoder: [256, (4, 4), 1] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 1]

(24)

(cid:55)→ GN+ELU (cid:55)→ [32, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [32, (5, 5), 1]
(cid:55)→ GN+ELU (cid:55)→ [chans, (1, 1), 1]

29

Method
EWC-D
naive-D
batch-D
batch-D
lifelong-D
EWC-C
naive-C
batch-C
batch-C
lifelong-C

Initial zd dimension
10
10
10
10
1
10
10
10
10
1

Final zd dimension
10
10
10
10
10
10
10
10
10
10

zc dimension
14
14
14
14
14
14
14
14
14
14

# initial parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,165,311
30,767,428
7,691,280
7,691,280
15,382,560
15,235,072

# ﬁnal parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,179,661
30,767,428
7,691,280
7,691,280
15,382,560
15,382,560

The table above lists the number of parameters for each model and architecture used in our ex-
periments. The lifelong models initially start with a zd of dimension 1 and at each step we grow
the representation by one dimension to accommodate the new distribution (more info in Section
10.7). In contrast, the baseline EWC models are provided with the full representation throughout
the learning process. EWC has double the number of parameters because the computed diagonal
ﬁsher information matrix which is the same dimensionality as the number of parameters. EWC also
neeeds the preservation of the teacher model [Φ, Θ] to use in it’s quadratic regularizer. Both the
naive and batch models have the fewest number of parameters as they do not use a student-teacher
framework and only use one model, however the vanilla model has no protection against catastrophic
interference and the full model is just used as an upper bound for performance.

We used Adam [62] to optimize all of our problems with a learning rate of 1e-4 or 1e-3. When we
used weight transfer we re-initialized the accumulated momentum vector of Adam as well as the
aggregated mean and variance of the batch norm layers. The full architecture can be examined in
our github repository [98] and is provided under an MIT license.

10.4 CONTRAST TO STREAMING / ONLINE METHODS

Our method has similarities to streaming methods such as Streaming Variational Bayes (SVB) [14]
and Incremental Bayesian Clustering methods [60, 38] in that we estimate and reﬁne posteriors
through time. In general this can be done through the following Bayesian update rule that states that
the lastest posterior is proportional to the current likelihood times the previous posterior:

P (z|X1, ..., Xt) ∝ P (Xt|z)P (z|X1, ..., Xt−1)
SVB computes the intractable posterior, P (z|X1, ..., Xt), utilizing an approximation, At, that ac-
cepts as input the current dataset, Xt, along with the previous posterior At−1 :

(25)

P (z|X1, ..., Xt) ≈ At(Xt, At−1)

(26)

The ﬁrst posterior input (At=0) to the approximating function is the prior P (z). The objective of
SVB and other streaming methods is to model the posterior of the currently observed data in the
best possible manner. Our setting differs from this in that we want to retain information from all
previously observed distributions (sometimes called a knowledge store [126]). This can be useful
in scenarios where a distribution is seen once, but only used much later down the road. Rather than
creating a posterior update rule, we recompute the posterior via Equation 25, leveraging the fact that
we can re-generate X<t ≈ ˆX<t through the generative process. This allows us to recompute a more
appropriate posterior re-using all of the (generated) data, rather than using the previously computed
(approximate) posterior At−1:

P (z|X1, X2, ..., Xt) ∝ P (Xt|z)P (z| ˆX1, ..., ˆXt−1)

(27)

Coupling this generative replay strategy with the Bayesian update regularizer introduced in Section
4.1.1, we demonstrate that not only do we learn an updated poster as in Equation 27, but also allow
for a natural transfer of information between sequentially learnt models: a fundamental tenant of
lifelong learning [126, 125].

Finally, another key difference between lifelong learning and online methods is that lifelong learning
aims to learn from a sequence of tentatively different [20] tasks while still retaining and accumulating

30

knowledge; online learning generally assumes that the true underlying distribution comes from a
single distribution [11]. There are some exceptions to this where online learning is applied to the
problem of domain adaptation, eg: [49, 60].

10.5 EWC BASELINES: COMPARING CONV & DENSE NETWORKS

We compared a whole range of EWC baselines and use the best performing models few in our
experiments. Listed in Figure 10.5 are the full range of EWC baselines run on the PermutedMNIST
and FashionMNIST experiments. Recall that C / D describes whether a model is convolutional or
dense and the the number following is the hyperparameter for the EWC or Lifelong VAE.

10.6 GUMBEL REPARAMETERIZATION

Since we model our latent variable as a combination of a discrete and a continuous distribution we
also use the Gumbel-Softmax reparameterization [76, 50]. The Gumbel-Softmax reparameterization
over logits [linear output of the last layer in the encoder] p ∈ RM and an annealed temperature
parameter τ ∈ R is deﬁned as:

z = sof tmax(

); g = −log(−log(u ∼ U nif (0, 1)))

(28)

log(p) + g
τ

u ∈ RM , g ∈ RM . As the temperature parameter τ (cid:55)→ 0, z converges to a categorical.

10.7 EXPANDABLE MODEL CAPACITY AND REPRESENTATIONS

Multilayer neural networks with sigmoidal activations have a VC dimension bounded between
O(ρ2)[120] and O(ρ4)[59] where ρ are the number of parameters. A model that is able to con-

31

sistently add new information should also be able to expand its VC dimension by adding new pa-
rameters over time. Our formulation imposes no restrictions on the model architecture: i.e. new
layers can be added freely to the new student model.
In addition we also allow the dimensionality of zd ∈ RJ , our discrete latent representation to grow
in order to accommodate new distributions. This is possible because the KL divergence between
two categorical distributions of different sizes can be evaluated by simply zero padding the teacher’s
smaller discrete distribution. Since we also transfer weights between the teacher and the student
model, we need to handle the case of expanding latent representations appropriately. In the event
that we add a new distribution we copy all the weights besides the ones immediately surrounding
the projection into and out of the latent distribution. These surrounding weights are reinitialized to
their standard Glorot initializations [37].

32

9
1
0
2
 
v
o
N
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
7
4
8
9
0
.
5
0
7
1
:
v
i
X
r
a

LIFELONG GENERATIVE MODELING

Jason Ramapuram ∗ †
Jason.Ramapuram@etu.unige.ch

Magda Gregorova ∗ †
magda.gregorova@hesge.ch

Alexandros Kalousis ∗ †
Alexandros.Kalousis@hesge.ch

ABSTRACT

Lifelong learning is the problem of learning multiple consecutive tasks in a se-
quential manner, where knowledge gained from previous tasks is retained and
used to aid future learning over the lifetime of the learner. It is essential towards
the development of intelligent machines that can adapt to their surroundings. In
this work we focus on a lifelong learning approach to unsupervised generative
modeling, where we continuously incorporate newly observed distributions into a
learned model. We do so through a student-teacher Variational Autoencoder ar-
chitecture which allows us to learn and preserve all the distributions seen so far,
without the need to retain the past data nor the past models. Through the intro-
duction of a novel cross-model regularizer, inspired by a Bayesian update rule,
the student model leverages the information learned by the teacher, which acts
as a probabilistic knowledge store. The regularizer reduces the effect of catas-
trophic interference that appears when we learn over sequences of distributions.
We validate our model’s performance on sequential variants of MNIST, Fashion-
MNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model
mitigates the effects of catastrophic interference faced by neural networks in se-
quential learning scenarios.

1

INTRODUCTION

Machine learning is the process of approximating unknown functions through the observation of typ-
ically noisy data samples. Supervised learning approximates these functions by learning a mapping
from inputs to a predeﬁned set of outputs such as categorical class labels (classiﬁcation) or contin-
uous targets (regression). Unsupervised learning seeks to uncover structure and patterns from the
input data without any supervision. Examples of this learning paradigm include density estimation
and clustering methods. Both learning paradigms make assumptions that restrict the set of plausible
solutions. These assumptions are referred to as hypothesis spaces, biases or priors and aid the model
in favoring one solution over another [82, 128]. For example, the use of convolutions [70] to process
images favors local structure; recurrent models [54, 46] exploit sequential dependencies and graph
neural networks [109, 64] assume that the underlying data can be modeled accurately as a graph.

Current state of the art machine-learning models typically focus on learning a single model for a
single task, such as image classiﬁcation [72, 121, 43, 118, 66], image generation [100, 13, 63, 39],
natural language question answering [25, 96] or single game playing [129, 113]. In contrast, humans
experience a sequence of learning tasks over their lifetimes, and are able to leverage previous learn-
ing experiences to rapidly learn new tasks. Consider learning how to ride a motorbike after learning
to ride a bicycle: the task is drastically simpliﬁed through the use of prior learning experience. Stud-
ies [2, 3, 67] in psychology have shown that humans are able to generalize to new concepts in a rapid
manner, given only a handful of samples. [67] demonstrates that humans can classify and generate
new concepts of two wheel vehicles given just a single related sample. This contrasts the state of the
art machine learning models described above which use hundreds of thousands of samples and fail
to generalize to slight variations of the original task [22].

∗University of Geneva, Switzerland
†Haute cole de gestion de Genve, HES-SO, Switzerland

1

Lifelong learning [126, 125] argues for the need to consider learning over task sequences, where
learned task representations and models are stored over the entire lifetime of the learner and can be
used to aid current and future learning. This form of learning allows for the transfer of previously
learned models and representations and can reduce the sample complexity of the current learning
problem [126]. In this work we restrict ourselves to a subset of the broad lifelong learning paradigm;
rather than focus on the supervised lifelong learning scenario as most state of the art methods, our
work is one of the ﬁrst to tackle the more challenging problem of deep lifelong unsupervised learn-
ing. We also identity and relax crucial limitations of prior work in life-long learning that requires
the storage of previous models and training data, allowing us to operate in a more realistic learning
scenario.

2 RELATED WORK
The idea of learning in a continual manner has been explored extensively in machine learning,
seeded by the seminal works of lifelong-learning [126, 125, 117], online-learning [32, 9, 11, 12]
and sequential linear gaussian models [105, 36] such as the Kalman Filter [56] and its non-linear
counterpart, the Particle Filter [24]. Lifelong learning bears some similarities to online learning
in that both learning paradigms observe data in a sequential manner. Online learning differs from
lifelong learning in that the central objective of a typical online learner [11, 12] is to best solve/ﬁt the
current learning problem, without preserving previous learning. In contrast, lifelong learners seek to
retain, and reuse, the learned behavior acquired over past tasks, and aim to maximize performance
across all tasks. Consider the example of forecasting click through rate: the objective of the online
learner is to evolve over time, such that it best represents current user preferences. This contrasts
lifelong learners which enforce a constraint between tasks to ensure that previous learning is not
lost.

Lifelong Learning [126] was initially proposed in a supervised learning framework for concept
learning, where each task seeks to learn a particular concept/class using binary classiﬁcation. The
original framework used a task speciﬁc model, such as a K Nearest Neighbors (KNN) 1, coupled with
a representation learning network that used training data from all past learning tasks (support sets),
to learn a common, global representation. This supervised approach was later improved through the
use of dynamic learning rates [115], core-sets [114] and multi-head classiﬁers [31].
In parallel,
lifelong learning was extended to independent multi-task learning [107, 31], reinforcement learning
[125, 122, 102], topic modeling [19, 130] and semi-supervised language learning [83, 81]. For a
more detailed review see [20].

More recently, lifelong learning has seen a resurgence within the framework of deep learning. As
mentioned earlier, one of the central tenets of lifelong learning is that that the learner should perform
well over all observed tasks. Neural networks, and more generally, models that learn using stochastic
gradient descent [103], typically cannot persist past task learning without directly preserving past
models or data. This problem of catastrophic forgetting [78] is well known in the neural network
community and is the central obstacle that needs to be resolved to build an effective neural lifelong
learner. Catastrophic forgetting is the phenomenon where model parameters of a neural network
trained in a sequential manner become biased towards the distribution of the latest observations,
forgetting previously learned representations, over data no longer accessible for training. In order to
mitigate catastrophic forgetting current research in lifelong learning employs four major strategies:
transfer learning, replay mechanisms, parameter regularization and distribution regularization. In
table 1 we classify the different lifelong learning methods that we will discuss in the following
paragraphs into these strategies.

EWC [65] VCL [91] LwF [71] ALTM [33]
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

PNN [94] DGR [112, 57] DBMNN [123]
(cid:88)
(cid:55)

(cid:55)
(cid:88)

(cid:88)
(cid:55)

(cid:88)

Transfer learning
Replay mechanisms
Parameter
regularization
Functional
regularization
Table 1: Catastropic interference mitigation strategies of state of the art models. Rows highlighted in gray
represent desirable mitigation strategies.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

SI [136] VASE [1] LGM (us)
(cid:55)
(cid:55)

(cid:55)
(cid:88)

(cid:55)
(cid:88)

1These models were known as memory based learning in [126].

2

Transfer learning: These approaches mitigate catastrophic forgetting by freezing previous task
models and relaying a latent representation of the previous task to the current model. Research in
transfer learning for the mitigation of catastrophic forgetting include Progressive Neural Networks
(PNN) [94] and Deep Block-Modular Neural Networks (DBMNN) [123] to name a few. These
approaches allow the current model to adapt its parameters to the (new) joint representation in an
efﬁcient manner and prevent forgetting through the direct preservation of all previous task mod-
els. Deploying such a transfer learning mechanism in a lifelong learning setting would necessitate
training a new model with every new task, considerably increasing the memory footprint of the life-
long learner. In addition, since transfer learning approaches freeze previous models, it negates the
possibility of improving previous task performance using knowledge gathered from new tasks.

Replay mechanisms: The original formulation of lifelong learning [126] required the preservation
of all previous task data. This requirement was later relaxed in the form of core-sets [116, 114, 91],
which represent small weighted subsets of inputs that approximate the full dataset. Recently, within
the classiﬁcation setting, there have been replay approaches that try to lift the requirement of storing
past training data by relying on generative modeling [112, 57]; we will call such methods Deep gen-
erative replay (DGR) methods. DGR methods methods use a student-teacher network architecture,
where the teacher (generative) model augments the student (classiﬁer) model with synthetic samples
from previous tasks. These synthetic task samples are used in conjunction with real samples from
the current task to learn a new joint model across all tasks. While strongly motivated by biologi-
cal rehearsal processes [119, 53, 58, 110], these generative replay strategies fail to efﬁciently use
previous learning and simply re-learn each new joint task from scratch.

Parameter regularization: Most work that mitigates catastrophic forgetting falls under the um-
brella of parameter regularization. There are two approaches within this mitigation strategy: con-
straining the parameters of the new task to be close to the previous task through a predeﬁned metric,
and enforcing task-speciﬁc parameter sparsity. The two approaches are related as task-speciﬁc pa-
rameter sparsity can be perceived as a reﬁnement of the parameter constraining approach. Parameter
constraining approaches typically share the same model/parameters, but encourage new tasks from
altering important learned parameters from previous tasks. Task speciﬁc parameter sparsity relaxes
this, by enforcing that each task use a different subset of parameters from a global model, through
the use of an attention mechanism.

Models such as Laplace Propagation [29], Elastic Weight Consolidation (EWC) [65], Synaptic In-
telligence (SI) [136] and Variational Continual Learning (VCL) [91] fall under the parameter con-
straining approach. EWC for example, uses the Fisher Information matrix (FIM) to control the
change of model parameters between two learning tasks. Intuitively, important parameters should
not have their values changed, while non-important parameters are left unconstrained. The FIM is
used as a weighting in a quadratic parameter difference regularizer under a Gaussianity assump-
tion of the parameter posterior. However, this Gaussian parameter posterior assumption has been
demonstrated [86, 10] to be sub-optimal for learned neural network parameters. VCL improves
upon EWC, by generalizing the local assumption of the FIM to a KL-Divergence between the (vari-
ational) parameter posterior and prior. This generalization derives from the fact that the FIM can
be cast as a KL divergence between the posterior and an epsilon perturbation of the same random
variable [51]. VCL actually spans a number of different mitigation strategies as it uses parameter
regularization (described above) , transfer learning (it keeps a separate head network per task) and
replay (it persists a core-set of true data per task).

Models such as Hard Attention to the Task (HAT) [111] and the Variational Autoencoder with Shared
Embeddings (VASE) [1] fall under the task-speciﬁc parameter sparsity strategy. This mitigation
strategy enforces that different tasks use different components of a single model, typically through
the use of attention vectors [6] that are learned given supervised task labels. Multiplying the attention
vectors with the model outputs prevents gradient descent updates for different subsets of the model’s
parameters, allowing them to be used for future task learning. Task speciﬁc parameter sparsity allows
a model to hold-out a subset of its parameters for future learning and typically works well in practice
[111], with its strongest disadvantage being the requirement of supervised information.

Functional regularization: Parameter regularization methods attempt to preserve the learned be-
havior of the past models by controlling how the model parameters change between tasks. However,
the model parameters are only a proxy for the way a model actually behaves. Models with very
different parameters can have exactly the same behavior with respect to input-output relations (non-

3

uniqueness [131]). Functional regularization concerns itself with preserving the actual object of
interest:
the input-output relations. This strategy allows the model to ﬂexibly adapt its internal
parameter representation between tasks, while still preserving past learning.

Methods such as distillation [45], ALTM [33] and Learning Without Forgetting (LwF) [71] impose
similarity constraints on the classiﬁcation outputs of models learned over different tasks. This can
be interpreted as functional regularization by generalizing the constraining metric (or semi-metric)
to be a divergence on the output conditional distribution. In contrast to parameter regularization, no
assumptions are made on the parametric form of the parameter posterior distribution. This allows
models to ﬂexibly adapt their internal representation as needed, making functional regularization a
desirable mitigation strategy. One of the pitfalls of current functional regularization approaches is
that they necessitate the preservation of all previously data.

2.1 LIMITATIONS OF EXISTING APPROACHES.

A simple solution to the problem of lifelong learning is to store all data and re-learn a new joint
multi-task representation [17] at each newly observed task. Alternatively, it is possible to retain
all previous model parameters and select the model that presents the best performance on new test
task data. Existing solutions typically relax one these requirements. [33, 71, 91] relaxes the need
for model persistence, but requires preservation of all data [33, 71], or a growing core-set of data
[116, 114, 91]. Conversely, [94, 123, 91, 136] relaxes the need to store data, but persists all previous
models [94, 123] or a subset of model parameters [91, 136].

Unlike these approaches, we draw inspiration from how humans learn over time and remove the
requirement of storing past training data and models. Consider the human visual system; research
has shown [23, 7] that the human eye is capable of capturing 576 megapixels of content per image
frame. If stored naively on a traditional computer, this corresponds to approximately 6.9 gigabytes of
information per sample. Given that we perceive trillions of frames over our lifetimes, it is infeasible
to store this information in its base, uncompressed representation. Research in neuroscience has
validated [132, 5] that the associative human mind, compresses, merges and reconstructs information
content in a dynamic way. Motivated by this, we believe that a lifelong learner should not store past
training data or models. Instead, it should retain a latent representation that is common over all tasks
and evolve it as more tasks are observed.

2.2 OUR SOLUTION AT A HIGH LEVEL.

While most research in lifelong learning focuses on supervised learning [94, 33, 71, 123, 136, 65],
we focus on the more challenging task of deep unsupervised latent variable generative modeling.
These models have wide ranging applications such as clustering [77, 52, 85] and pre-training [68,
95].

Central to our lifelong learning method are a pair of generative models, aptly named the teacher and
student, which we train by exploiting the replay and functional regularization strategies described
above. After training a single generative model over the ﬁrst task, we use it is used as the teacher for
a newly instantiated student model. The student model receives data from the current task, as well as
replayed data from the teacher, which acts as a probabilistic storage container of past tasks. In order
to preserve previous learning, we make use of functional regularization, which aids in preserving
input-output relations over past tasks.

Unlike EWC or VCL, we make no assumptions on the form of the parameter posterior and allow
the generative models to use available parameters as appropriate, to best accommodate current and
past learning. The use of generative replay, coupled with functional regularization, renders the
preservation of the past models and past data unnecessary. It also signiﬁcantly improves the sample
complexity on future task learning, which we empirically demonstrate in our experiments. Finally
we should note that it is straightforward to adapt our approach to the supervised learning setting, as
we have done in [69].

3 BACKGROUND

In this section we describe the main concepts that we use throughout this work. We begin by de-
scribing the base-level generative modeling approach in Section 3.1, followed by how it extends to

4

the lifelong setting in Section 3.2. Finally, in Section 3.3, we describe the Variational Autoencoder
over which we instantiate our lifelong generative model.

3.1 LATENT VARIABLE GENERATIVE MODELING

We consider a scenario where we observe a dataset, D, consisting of N variates, D = {xj}N
j=1,
of a continuous or discrete variable x. We assume that the data is generated by a random process
involving a non-observed random variable, z. The data generation process involves ﬁrst sampling
zj ∼ P (z) and then producing a variate from the conditional, xj ∼ Pθ(x|z). We visualize this
form of latent generative model in the graphical model in Figure 14.

Figure 1: Typical latent variable graphical model. Gray nodes represent observed variables while white nodes
represent unobserved variables.

Typically, latent variables models are solved through maximum likelihood estimation which can be
formalized as:

max
θ

log Pθ(x) = max

log

Pθ(x|z)P (z)dz = max

log Ez[Pθ(x|z)]

(1)

θ

(cid:90)

θ

In many cases, the expectation from Equation 1 does not have a closed form solution (eg: non-
conjugate distributions) and quadrature is not computationally tractable due to large dimensional
spaces [63, 101] (eg: images). To overcome these intractabilities we use a Variational Autoencoder
(VAE), which we summarize in Section 3.3. The VAE allows us to infer our latent variables and
jointly estimate the parameters of our model. However, before describing the VAE, it is important
to understand how this generative setting can be perceived in a lifelong learning scenario.

3.2 LIFELONG GENERATIVE MODELING

Lifelong generative modeling extends the single-distribution estimation task from Section 3.1 to a
set of i = {1...L} sequentially observed learning tasks. The i-th learning task has variates that are
realized from the task speciﬁc conditional, Pi(x) = P (x|zd = i), where zd acts as a categorical
indicator variable of the current task. We visualize a simpliﬁed form of this in Figure 2 below.

Figure 2: Simpliﬁed lifetime of a lifelong learner. Given a true (unknown) distribution, P (x) =
(cid:82) P (x|zd)P (zd)δzd, we observe partial information in the form of L sequential tasks, {D1 (cid:55)→ D2, ... (cid:55)→
DL}. Observing more tasks, reduces the uncertainty of the model until convergence, PθL (x) ≈ P (x).

Crucially, when observing task, Di, the model has no access to any of the previous task datasets,
D<i. As the lifelong learner observes more tasks, {D1 (cid:55)→ D2 (cid:55)→ ... (cid:55)→ DL}, it should improve its
estimate of the true distribution, Pθ(x) ≈ P (x) = (cid:82) P (x|zd)P (zd)δzd, which is unknown at the
start of training.

5

3.3 THE VARIATIONAL AUTOENCODER

As eluded to in Section 3.1, we would like to infer the latent variables from the data. This
can be realized as an alternative form of Equation 1 in the form of Bayes rule: Pφ(z|x) =
Pθ(x|z)P (z)/Pθ(x), where Pφ(z|x) is referred to as the latent variable posterior and Pθ(x|z)
as the likelihood. One method of approximating the posterior, Pφ(z|x), is through MCMC sam-
pling methods such as Gibbs sampling [34] or Hamiltonian MCMC [87]. MCMC methods have
the advantage that they provide asymptotic guarantees [88] of convergence to the true posterior,
Pφ(z|x). However in practice it is not possible to know when convergence has been achieved. In
addition, due to their Markovian nature, they possess an inner loop, which makes it challenging to
scale for large scale datasets.

In contrast, Variational Inference (VI) [55] side-steps the intractability of the posterior by approxi-
mating it with a tractable distribution family, Qφ(z|x). VI rephrases the objective of determining the
posterior as an optimization problem by minimizing the KL divergence between the known distri-
butional family, Qφ(z|x), and the unknown true posterior, Pθ(z|x). Applying VI to the intractable
integral from Equation 1 results in the evidence lower bound (ELBO) or variational free energy,
which can easily be derived from ﬁrst principles:

log Pθ(x) = log

Pθ(x|z)P (z)dz

(cid:90)

= log

(cid:90) Qφ(z|x)
Qφ(z|x)

Pθ(x|z)P (z)dz

≥ EQ[log Pθ(x|z)] − DKL[Qφ(z|x)||P (z)]
(cid:123)(cid:122)
(cid:125)
ELBO

(cid:124)

(2)

(3)

(4)

where we used Jensen’s inequality to transition from Equation 3 to Equation 4. The objective intro-
duced in Equation 4 induces the graphical model shown below in Figure 3.

Figure 3: Standard VAE graphical model. Gray nodes represent observed variables while white nodes represent
unobserved variables; dashed lines represent inferred variables.

VAEs typically use deep neural networks to model the approximate inference network, Qφ(z|x)
and conditional, Pθ(x|z), which are also known as the encoder and decoder networks (respec-
tively). To optimize for the parameters of these networks, VAEs maximize the ELBO (Equation
4) using Stochastic Gradient Descent [103]. By sharing the variational parameters of the encoder,
φ, across the data points (amortized inference [35]), variational autoencoders avoid per-data inner
loops typically needed by MCMC approaches.

Optimizing the ELBO in Equation 4 requires computing the gradient of an expectation over the ap-
proximate posterior, Qφ(z|x). This typically takes place through the use of the path-wise estimator
[101, 63] (originally called “push-out” [106]). The path-wise reparameterizer uses the Law of the
Unconscious Statistician (LOTUS) [42], which enables us to compute the expectation of a function
of a random variable (without knowing its distribution) if we know its corresponding sampling path
and base distribution [84]. For the typical isotropic gaussian approximate posterior, Qφ(z|x), used
in standard VAEs this can be aptly summarized by:

z ∼ Qφ(z|x) ⇔ µφ(x) + σφ(x)(cid:15), (cid:15) ∼ N (0, 1)

∇φEQφ(z|x)[log Pθ(x|z)] ⇔ E
N ((cid:15)|0, 1)
(cid:125)
(cid:123)(cid:122)
(cid:124)

base distribution

)]
[∇φ log Pθ(x| µφ(x) + σφ(x)(cid:15)
(cid:125)
(cid:124)

(cid:123)(cid:122)
sampling path

(5)
(6)

6

where Equation 5 deﬁnes the sampling procedure of our latent variable through the location-scale
transformation and Equation 6 deﬁnes the path-wise Monte Carlo gradient estimator applied on the
decoder (ﬁrst term in Equation 4). This Monte Carlo estimator enables differentiating through the
sampling process of the distribution Qφ(z|x). Note that computing the gradient of the second term
in Equation 4, ∇φDKL[Qφ(z|x)||P (z)], is possible through a closed form analytical solution for
the case of isotropic gaussian distributions.

While it is possible to extend any latent variable generative model to the lifelong setting, we choose
to build our lifelong generative models using variational autoencoders (VAEs) [63] as they provide
a mechanism for stable training; this contrasts other state of the art unsupervised models such as
Generative Adversarial Networks (GANs) [39, 61]. Furthermore, latent-variable posterior approxi-
mations are a requirement in many learning scenarios such as clustering [93], compression [92] and
unsupervised representation learning [30]. Finally, GANs can suffer from low sample diversity [28]
which can lead to compounding errors in a lifelong generative setting.

4 LIFELONG LEARNING MODEL

Algorithm 1 Data Flow

Teacher:
Sample Prior: zj ∼ P (z)
Decode: ˆxj ∼ PΘ(x|z)

Student:
Sample : xj ∼ P (ω)P (x|ω)
Encode : zj ∼ Qφ(z|x)
Decode: ˆxj ∼ Pθ(x|z)

Figure 4: Student training procedure. Left: graphical model for student-teacher model. Data generated from
the teacher model (top row) is used to augment the current training data observed by the student model (bottom
row). A posterior regularizer is also applied between Qφ(z|x) and QΦ(z|x) to enable functional regularization
(not shown, but discussed in detail in Section 4.1.1). Right: data ﬂow algorithm.

fMRI studies of the rodent [119, 53, 58] and human [110] brains have shown that previously experi-
enced sequences of events are replayed in the hippocampus during rest. These replays are necessary
for better planning [53] and memory consolidation [16]. We take inspiration from the memory con-
solidation of biological learners and introduce our model of Lifelong Generative Modeling (LGM).
We visualize the LGM student-teacher architecture in Figure 4.

The student and the teacher are both instantiations of the same base-level generative model, but
have different roles throughout the learning process. The teacher’s role is to act as a probabilistic
knowledge store of previously learned distributions, which it transfers to the student in the form
of replay and functional regularization. The student’s role is to learn the distribution over the new
task, while accommodating the learned representation of the teacher over old tasks. In the following
sections we provide detailed descriptions of the student-teacher architecture, as well as the base-
level generative model that each of them use. The base-level model uses a variant of VAEs, which
we tailor for lifelong learning and is learned by maximizing a variant of the standard VAE ELBO
from Equation 4 ; we describe this objective at end of this section.

4.1 STUDENT-TEACHER ARCHITECTURE

The top row of Figure 4 represents the teacher model. At any given time, the teacher contains a
summary of all previous distributions within the learned parameters, Φ, of the encoder QΦ(z|x),
and the learned parameters, Θ, of the decoder PΘ(x|z). We use the teacher to generate synthetic
variates, ˆxj, from these past distributions by decoding variates from the prior, zj ∼ P (z) (cid:55)→
PΘ(x|z = zj). We pass the generated (synthetic) variates, ˆxj, to the student model as a form of
knowledge transfer about the past distributions. Information transfer in this manner is known as
generative replay and our work is the ﬁrst to explore it in a VAE setting.

7

The bottom row of Figure 4 represents the student. The student is responsible for updating the
parameters, φ, of its encoder, Qφ(z|x), and θ, of its decoder Pθ(x|z). Importantly, the student re-
ceives data from both the currently observed task, as well as synthetic data generated by the teacher.
This can be formalized as xj ∼ P (ω)P (x|ω), ω ∼ Ber(π), as shown in Equation 7:

P (ω)P (x|ω) =

(cid:26)PΘ(x|z) ω = 0
ω = 1

Pi(x)

(7)

The mean, π, of the Bernoulli distribution, controls the sampling proportion of the previously learned
distributions to the current one and is set based on the number of assimilated distributions. Thus,
given i observed distributions: π = 1
i+1 . This ensures that the samples observed by the student
are representative of both the current and past distributions. Note that this does not correspond
to varying sample sizes in datasets, but merely our assumption to model each distribution with
equivalent weighting.

Once a new task is observed, the old teacher is dropped, the student model is frozen and becomes the
new teacher (φ → Φ, θ → Θ). A new student is then instantiated with the latest weights φ and θ
from the previous student (the new teacher). Due to the cyclic nature of this process, no new models
are added. This contrasts many existing state of the art deep lifelong learning methods which add an
entire new model or head-network per task (eg: [91, 94, 123]).

A crucial aspect in the lifelong learning process is to ensure that previous learning is successfully
exploited to bias current learning [126]. While the replay mechanism that we put in place ensures
that the student will observe data from all tasks, it does not ensure that previous knowledge from
the teacher is efﬁciently exploited to improve current student learning. The student model will re-
learn (from scratch) a completely new representation, which might be different than the teacher. In
order to successfully transfer knowledge between both VAE models, we rely on functional regular-
ization, which we enforce through a Bayesian update regularizer of the posteriors of both models.
Intuitively, we would like the student model’s latent outputs, zj ∼ Qφ(z|x) to be similar to la-
tent outputs of teacher model, zj ∼ QΦ(z|x), over synthetic variates generated by the teacher,
xj ∼ P (ω)P (x|ω = 0) = PΘ(x|z). In the following section, we describe the exact functional
form of this regularizer and demonstrate how it can be perceived as a natural extension of the VAE
learning objective to a sequential setting.

4.1.1 KNOWLEDGE TRANSFER VIA BAYESIAN UPDATE.

While both the student and teacher are instantiations of VAE variants, tailored for the particularities
of the lifelong setting, for the purpose of this exposition we use the standard VAE formulation. Our
objective is to learn the set of parameters [φ, θ] of the student, such that it can generate variates
from the complete distribution, P (x), described in Section 3.2. Subsuming the deﬁnition of the
augmented input data, x ∼ P (ω)P (x|ω), from Equation 7, we can deﬁne the student ELBO as:

Lθ,φ(x) = EQφ(z|x)

(cid:20)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)],

(8)

x ∼ P (ω)P (x|ω), ω ∼ Ber(π).

Rather than naively shrinking the full posterior to the prior via the KL divergence in Equation 8, we
rely on one of the core tenets of the Bayesian paradigm which states that we can always update our
posterior when given new information (yesterdays posterior is todays prior) [79]. Given this tenet,
we introduce our posterior regularizer 2:

KL[Qφ(z|x)||QΦ(z|x)], x ∼ P (x|ω = 0)
(9)
which distills the teacher’s learnt representation into the student over the generated data only. Com-
bining Equations 8 and 9, yields the objective that we can use to train the student and is described
below in Equation 10:

Lθ,φ(x) = EQφ(z|x)

log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(10)

(cid:20)

(cid:21)

2While it

is also possible to apply a similar

regularizer

to the reconstruction term,

i.e:

KL[Pθ(x|z) || PΘ(x|z)], we observed that doing so hurts performance (Appendix 10.2).

+ (1 − ω)KL[Qφ(z|x)||QΦ(z|x)],
x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

8

Note that this is not the ﬁnal objective, due to the fact that we have yet to present the VAE variant
tailored to the particularities of the lifelong setting. We will now show how the posterior regularizer
can be perceived as a natural extension of the VAE learning objective, through the lens of a Bayesian
update of the student posterior.

Lemma 1 For random variables x and z with conditionals QΦ(z|x) and Qφ(z|x), both distributed
as a categorical or gaussian and parameterized by Φ and φ respectively, the KL divergence between
the distributions is:

KL[Qφ(z|x)||QΦ(z|x)] = KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(11)

where ˆφ = f (φ, Φ) depends on the parametric form of Q, and C is only a function of the parameters,
Φ.

We prove Lemma 1 for the relevant distributions (under some mild assumptions) in Appendix 10.1.
Using Lemma 1 allows us to rewrite Equation 10 as shown below in Equation 12:
(cid:20)

Lθ,φ(x) = EQφ(z|x)

(cid:21)
log Pθ(x|z)

− KL[Qφ(z|x)||P (z)]

(12)

(cid:20)

+ (1 − ω)

KL[Q ˆφ(z|x)||P (z)] + C(Φ)

(cid:21)
,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

This rewrite makes it easy to see that our posterior regularizer from Equation 10 is a standard VAE
ELBO (Equation 4) under a reparameterization of the student parameters, ˆφ = f (φ, Φ). Note that
C(Φ) is constant with respect to the student parameters, φ, and thus not used during optimization.
While the change seems minor, it omits the introduction of f (φ, Φ) which allows for a transfer of
information between models. In practice, we simply analytically evaluate KL[Qφ(z|x) ||QΦ(z|x)],
the KL divergence between the teacher and the student posteriors, instead of deriving the functional
form of f (φ, Φ) for each different distribution pair. We present Equation 12 simply as a means to
provide a more intuitive understanding of our functional regularizer.

4.2 BASE-LEVEL GENERATIVE MODEL.

While it is theoretically possible to use the vanilla VAE from Section 3.3 for the teacher and student
models, doing so brings to light a number of limitations that render it problematic for use in the con-
text of lifelong learning (visualized in Figure 5-Right). Speciﬁcally, using a standard VAE decoder,
Pθ(x|z), to generate synthetic replay data for the student is problematic due to two reasons:

1. Mixed Distributions: Sampling the continuous standard normal prior, N (0, 1), can select
a point in latent space that is in between two separate distributions, causing generation of
unrealistic synthetic data and eventually leading to loss of previously learnt distributions.
2. Undersampling: Data points mapped to the isotropic-gaussian posterior that are further
away from the prior mean will be sampled less frequently, resulting in an undersampling
of some of the constituent distributions.

To address these sampling limitations we decompose the latent variable, z, into an independent
continuous, zc ∼ Qφ(zc|x), and a discrete component, zd ∼ Qφ(zd|x), as shown in Equation 13
and visually in Figure 5-Left:

Qφ(zc, zd|x) = Qφ(zc|x)Qφ(zd|x).

(13)

The objective of the discrete component is to summarize the discriminative information of the indi-
vidual generative distributions. The continuous component on the other hand, caters for the remain-
ing sample variability (a nuisance variable [73]). Given that the discrete component can accurately
summarize the discriminative information, we can then explicitly sample from any of the past dis-
tributions, allowing us to balance the student model’s synthetic inputs with samples from all of the
previous learned distributions. We describe this beneﬁcial generative sampling property in more
detail in Section 4.2.1.

9

Figure 5: Left: Graphical model for VAE with independent discrete and continuous posterior, Qφ(zc, zd|x) =
Qφ(zc|x)Qφ(zd|x). Right: Two dimensional test variates, zj ∼ Qφ(z|x), zj ∈ R2, of a vanilla VAE
trained on MNIST. We depict the two generative shortcomings visually: 1) mixing of distributions which
causes aliasing in a lifelong setting and 2) undersampling of distributions in a standard isotropic-gaussian VAE
posterior.

Naively introducing the discrete component, zd, does not guarantee that the decoder will use it to
represent the most discriminative aspects of the modeled distribution. In preliminary experiments,
we observed that that the decoder typically learns to ignore the discrete component and simply
relies on the continuous variable, zc. This is similar to the posterior collapse phenomenon which
has received a lot of recent interest within the VAE community [99, 40]. Posterior collapse occurs
when training a VAE with a powerful decoder model such as a PixelCNN++ [127] or RNN [21, 40].
The output of the decoder, xj ∼ Pθ(x|z) can become almost independent of the posterior sample,
zj ∼ Qφ(z|x), but is still able to reconstruct the original sample by relying on its auto-regressive
property [40]. In Section 4.2.2, we introduce a mutual information regulariser which ensures that
the discrete component of the latent variable is not ignored.

4.2.1 CONTROLLED GENERATIONS.

Desired Task Conditional
P1(x) = P (x|zd = 1)
P2(x) = P (x|zd = 2)
P3(x) = P (x|zd = 3)

zc
∼ N (0, 1)
∼ N (0, 1)
∼ N (0, 1)

zd
[0, 0, 1]
[0, 1, 0]
[1, 0, 0]

Figure 6 & Table 2: FashionMNIST with L = 3 tasks: t-shirts, sandals and bag. To generate samples from
the i-th task conditional, Pi(x) = P (x|zd = i), we set zd = i, randomly sample zc ∼ N (0, 1), and run
[zc, zd] through the decoder, Pθ(x|zc, zd). Resampling zc, while keeping zd ﬁxed, enables generation of
varied samples from the task conditional. Left: Desired task conditionals. Right: Desired decoder behavior.

Given the importance of generative replay for knowledge transfer in LGM, synthetic sample gen-
eration by the teacher model needs to be representative of all the previously observed distributions
in order to prevent catastrophic forgetting. Under the assumption that zd accurately captures the
underlying discriminativeness of the individual distributions and through the deﬁnition of the LGM
generative process, shown in Equation 14:

PΘ(x|zd, zc), zc ∼ N (0, 1), zd ∼ Cat(1/L),

we can control generations by setting a ﬁxed value, zd = i, and randomly sampling the continuous
prior, zc ∼ N (0, 1). This is possible because the trained decoder approximates the task conditional
from Section 3.2:

(14)

(15)

P (x|zd = i) = Pi(x) ≈

PΘ(x|zc, zd = i)P (zc)
QΦ(zc|x)

10

where sampling the true task conditional, Pi(x), can be approximated by sampling zc ∼ P (zc) =
N (0, 1), keeping zd ﬁxed, and decoding the variates as shown in Equation 16 below:

ˆx ∼ PΘ(x|zc ∼ N (0, 1), zd = i).

(16)

We provide a simple example of our desired behavior for three generative tasks, L = 3, using Fash-
ion MNIST in Figure 6 and Table 2 above. The assumption made up till now is that zd accurately
captures the discriminative aspects of each distribution. However, there is no theoretical reason for
the model to impose this constraint on the latent variables. In practice, we often observe that the
decoder Pθ(x|zc, zd) ignores zd due to the much richer representation of the continuous variable,
zc. In the following section we introduce a mutual information constraint that encourages the model
to fully utilize zd.

4.2.2

INFORMATION RESTRICTING REGULARIZER

As eluded to in the previous section, the synthetic samples observed by the student model need to be
representative of all previous distributions. In order to control sampling via the process described in
Section 4.2.1, we need to enforce that the discrete variable, zd, carries the discriminative informa-
tion about each distribution. Given our graphical model from Figure 4-Left, we observe that there
are two ways to accomplish this: maximize the information content between the discrete random
variable, zd and the decoded ˆx, or minimize the information content between the continuous vari-
able, zc and the decoded ˆx. Since our graphical model and underlying network does not contain skip
connections, information from the input, x, has to ﬂow through the latent variables z = [zc, zd] to
reach the decoder. While both formulations can theoretically achieve the same objective, we ob-
served that in practice, minimizing I(ˆx, zc) provided better results. We believe the reason for this
is that minimizing I(ˆx, zc) provides the model with more subtle gradient information in contrast to
maximizing I(ˆx, zd) which receives no gradient information when the value of the k-th element of
the categorical sample is 1. We now formalize our mutual information regularizer, which we derive
from ﬁrst principles in Equation 17:

Ex∼Pi(x)[I(ˆx, zc)] = Ex∼Pi(x)[H(zc) − H(zc|ˆx)]

(17)

= Ex∼Pi(x)Ezc∼Qφ(zc|x)
(cid:123)(cid:122)
Ex∼Pi(x)[H(zc)]
+

(cid:124)

(cid:20)

(cid:21)
− log Qφ(zc|x)

(cid:125)

Ex∼Pi(x)E(zd,zc)∼Qφ(zc,zd|x)
(cid:124)

(cid:21)
(cid:20)
Eˆx∼Pθ (ˆx|zc,zd) log Qφ(zc|ˆx)
,
(cid:123)(cid:122)
Ex∼Pi(x)[−H(zc|ˆx)]

(cid:125)

where we use the independence assumption of our posterior from Equation 13 and the fact that the
expectation of a constant is the constant. This regularizer has parallels to the regularizer in InfoGAN
[18]. In contrast to InfoGAN, VAEs already estimate the posterior Qφ(zc|x) and thus do not need
the introduction of any extra parameters φ for the approximation. In addition [47] demonstrated that
InfoGAN uses the variational bound (twice) on the mutual information, making its interpretation
unclear from a theoretical point of view. In contrast, our regularizer has a clear interpretation: it
restricts information through a speciﬁc latent variable within the computational graph. We observe
that this constraint is essential for empirical performance of our model and empirically validate this
in our ablation study in Experiment 7.2.

4.3 LEARNING OBJECTIVE

The ﬁnal learning objective for each of the student models is the maximization of the sequential
VAE ELBO (Equation 10), coupled with generative replay (Equation 7)and the mutual information
regularizer, I(ˆx, zc), (Equation 17):

11

Lθ,φ(x) = EQφ(zc,zd|x)
(cid:124)

(cid:20)

(cid:21)
log Pθ(x|zc, zd)

− KL[Qφ(zc, zd|x)||P (zc, zd)]

(18)

(cid:125)

(cid:124)

(cid:123)(cid:122)
VAE ELBO
+ (1 − ω)KL[Qφ(zc, zd|x)||QΦ(zc, zd|x)]
(cid:125)
(cid:123)(cid:122)
Posterior Consistency Regularizer
− λI(ˆx, zc)
(cid:125)

(cid:123)(cid:122)
Mutual Information

(cid:124)

,

x ∼ P (ω)P (x|ω), ω ∼ Ber(π)

The λ hyper-parameter controls the importance of the information gain regularizer. Too large a
value for λ causes a lack of sample diversity, while too small a value causes the model to not use
the discrete latent distribution. We did a random hyperparameter search and determined λ = 0.01
to be a reasonable choice for all of our experiments. This is in line with the λ used in InfoGAN
[18] for continuous latent variables. We empirically validate the necessity of both terms proposed in
Equation 18 in our ablation study in Experiment 7.2. We also validate the beneﬁt of the latent vari-
able factorization in Experiment 7.1. Before delving into the experiments, we provide a theoretical
analysis of computational complexity induced by our model and objective (Equation 18) in Section
4.4 below.

4.4 COMPUTATIONAL COMPLEXITY

We deﬁne the computational complexity of a typical VAE encoder and decoder as O(E)
and O(D) correspondingly;
internally these are dominated by the matrix-vector products
which take approximately LO(n2) for L layers. We also deﬁne the cost of applying the loss
function as O(K) + O(R), where O(K) is the cost of evaluating the KL divergence from
the ELBO (Equation 4) and O(R) the cost for evaluating the reconstruction term. Given
these deﬁnitions, we can summarize LGM’s computation complexity as follows in Equation 19:

O(D)
(cid:124) (cid:123)(cid:122) (cid:125)
teacher
generations

+ O(E) + O(D)
(cid:123)(cid:122)
(cid:125)
student encode
+ decode

(cid:124)

+ O(K) + O(R)
(cid:125)
(cid:123)(cid:122)
vae loss

(cid:124)

+ O(K)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
regularizer

+ O(K) + O(E)
(cid:123)(cid:122)
(cid:125)
mutual info

(cid:124)

= 2[O(D) + O(E)] + 3[O(K)] + O(R),

(19)

where we introduce increased computational complexity due to teacher generations, the cost of the
posterior regularizer, and the mutual information terms; the latter of which necessitates an extra
encode operation, O(E). The computational complexity is still dominated by the matrix-vector
product from evaluating forward functionals of the neural network. These operations can easily be
amortized through parallelization on modern GPUs and typical experiments do not directly scale
as per Equation 19. In our most demanding experiment (Experiment 6.6), we observe an average
empirical increase of 13.53 seconds per training epoch and 6.3 seconds per test epoch.

5 REVISITING STATE OF THE ART METHODS.

In this section we revisit some of the state of the art methods from Section 2. We begin by providing
a mathematical description of the differences between EWC [65], VCL [91] and LGM and follow it
up with a discussion of VASE [1] and their extensions of our work.

EWC and VCL: Our posterior regularizer, KL[Qφ(z|x)||QΦ(z|x)], affects the same parameters,
φ, as parameter regularizer methods such as EWC and VCL. However, rather than assuming a func-
tional form for the parameter posterior, P (φ|x), our method regularizes the output latent distribu-
tion Qφ(z|x). EWC and VCL, both make the assumption that P (φ|x) is distributed as an isotropic
gaussian3. This allows the use of the Fisher Information Matrix (FIM) in a quadratic parameter reg-
ularizer in EWC, and an analytical KL divergence of the posterior in VCL. This is a very stringent

3VCL assumes an isotropic gaussian variational form vs. EWC which directly assumes the parametric form

on P (φ|x).

12

requirement for the parameters of a neural network and there is active research in Bayesian neural
networks that attempts to relax this constraint [74, 75, 80].

EWC minφ d[P (φ|x)||P (Φ|x)]
2 (φ − Φ)T F (φ − Φ)

≈ γ

LGM ( Isotropic Gaussian Posterior ) minφ d[Qφ(z|x)||QΦ(z|x)]
|ΣΦ|
|Σφ|

Φ Σφ) + (µΦ − µφ)T Σ−1

Φ (µΦ − µφ) − C + log

(cid:20)
tr(Σ−1

(cid:18)

= 0.5

(cid:19)(cid:21)

In the above table we examine the distance metric d, used to minimize the effects of catastrophic
inference in both EWC and LGM. While our method can operate over any distribution that has
a tractable KL-divergence, for the purposes of demonstration we examine the simple case of an
isotropic gaussian latent-variable posterior. EWC directly enforces a quadratic constraint on the
model parameters φ, while our method indirectly affects the same parameters through a regulariza-
tion of the posterior distribution Qφ(z|x). For any given input variate, xj, LGM allows to model to
freely change its internal parameters, φ; it does so in a non-linear4 way such that the analytical KL
shown above is minimized.

VASE : The recent work of Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies (VASE) [1] extend upon our work [1, p. 7], but take a more empirical route by
incorporating a classiﬁcation-based heuristic for their posterior distribution. In contrast, we show
(Section 4.1.1) that our objective naturally emerges in a sequential learning setting for VAEs, allow-
ing us to infer the discrete posterior, Qφ(zd|x) in an unsupervised manner. Due to the incorporation
of direct supervised class information [1] also observe that regularizing the decoding distribution
Pθ(x|z) aids in the learning process, something that we observe to fail in a purely unsupervised
generative setting (Appendix Section 10.2). Finally, in contrast to [1], we include an information
restricting regularizer (Section 4.2.2) which allows us to directly control the interpretation and ﬂow
of information of the learnt latent variables.

6 EXPERIMENTS

We evaluate our model and the baselines over standard datasets used in other state of the art life-
long / continual learning literature [91, 136, 112, 57, 65, 94]. While these datasets are simple in
a traditional classiﬁcation setting, transitioning to a lifelong-generative setting scales the problem
complexity substantially. We evaluate LGM on a set of progressively more complicated tasks (Sec-
tion 6.2) and provide comparisons against baselines [91, 136, 65, 29, 63] using a set of standard
metrics (Section 6.1). All network architectures and other optimization details for our LGM model
are provided in Appendix Section 10.3 as well our open-source git repository [98].

6.1 PERFORMANCE METRICS

To validate the beneﬁt of LGM in a lifelong setting we explore three main performance dimensions:
the ability for the model to reconstruct and generate samples from all previous tasks and the ability
to learn a common representation over time, thus reducing learning sample complexity. We use three
main quantitative performance metrics for our experiments: the log-likelihood importance sample
estimate [15, 91], the negative test ELBO, and the Frechet distance metric [44]. In addition, we also
provide two auxiliary metrics to validate the beneﬁts of LGM in a lifelong setting: training sample
complexity and wall clock time per training and test epoch.

To fairly compare models with varying latent variable conﬁgurations, one solution is to marginal-
ize out the latents, z, during model evaluation / test time: (cid:82)
k=1 Pθ(x|z = zk).
This is realized in practice by using a Monte Carlo approximation (typically K=5000) and is com-
monly known as the importance sample (IS) log-likelihood estimate [15, 91]. As latent variable
and model complexity grows, this estimate tends to become noisier and intractable to compute. For
our experiments we use this metric only for the FashionMNIST and MNIST datasets as computing
one estimate over 10,000 test samples for a complex model takes approximately 35 hours on a K80
GPU.

z Pθ(x|z)dz ≈ (cid:80)K

In contrast to the IS log-likelihood estimate, the negative test ELBO (Equation 4) is only applicable
when comparing models with the same latent variable conﬁgurations; it is however much faster to

4This is because the parameters of the distribution are modeled by a deep neural network.

13

compute. The negative test ELBO provides a lower bound to the test log-likelihood of the true data
distribution under the assumed latent variable conﬁguration. One crucial aspect missing from both
these metrics is an evaluation of generation quality. We resolve this by using the Frechet distance
metric [44] and qualitative image samples.

The Frechet distance metric allows us to quantify the quality and diversity of generated samples by
using a pre-trained classiﬁer model to compare the feature statistics (generally under a Gaussian-
ity assumption) between synthetic generated samples and samples drawn from the test set. If the
Frechet distance between these two distributions is small, then the generative model is said to be
generating realistic images. The Frechet distance between two gaussians (produced by evaluating
latent embeddings of a classiﬁer model) with means mtest, mgen with corresponding covariances
Ctest, Cgen is:

||mtest − mgen||2

2 + T r(Ctest + Cgen − 2[CtestCgen]0.5).

(20)

While the Frechet distance, negative ELBO and IS log-likelihood estimate provide a glimpse into
model performance, there exists no conclusive metric that captures the quality of unsupervised gen-
erative models [124, 108] and active research suggests a direct trade-off between perceptual quality
and model representation [8]. Thus, in addition to the metrics described above, we also provide
qualitative metrics in the form of test image reconstructions and image generations. We summarize
all used performance metrics in Table 3 below:

Negative ELBO

Deﬁnition

Equation 4.

Purpoose
Quantitative metric on
likelihood / reconstructions.
Quantitative metric on
density estimate.
Quantitative metric on generations.

Lower is better?

yes

Negative Log-Likelihood

Frechet Distance
Test Reconstructions
Generations
#Training Samples

5000 (latent) sample Monte
Carlo estimate of Equation 4.
Equation 20.
yes
Pθ(x|zc ∼ Qφ(zc|x), zd ∼ Qφ(zd|x)) Qualitative view of reconstructions. N/A
Pθ(x|zd ∼ Cat(1/L), zc ∼ N (0, 1)).
N/A
# real training samples used for task i.
yes

Qualitative view of generations.
Sample Complexity.

yes

Table 3: Summary of different performance metrics.

6.2 DATA FLOW

Figure 7: Visual examples of training and test task sequences (top to bottom) for the datasets used to validate
LGM. The training set only consists of samples from the current task while the test set is a cumulative union
of the current task, coupled with all previous tasks. The permuted MNIST tasks uses {G1, ...GL−1} different
ﬁxed permutation matrices to create 4 auxiliary datasets.

14

In Figure 7 we list train and test variates depicting the data ﬂow for each of the problems that we
model. Due to the relaxing the need to preserve data in a lifelong setting, the train task sequence ob-
serves a single dataset, Dtr
i , at a time, without access to any previous, Dtr
<i. The corresponding test
dataset consists of a union (∪ operator) of the current test dataset, Dte
i , merged with all previously
observed test datasets, ˆD
i ∪ Dte

i−1 ∪ ... ∪ Dte
1 .

te
i = Dte

MNIST / Fashion MNIST: For the MNIST and Fashion MNIST problems, we observe a single
MNIST digit or fashion object (such as shirts) at a time. Each training set consists of 6000 training
samples and 1000 test samples. These samples are originally extracted from the full training and
test datasets which consist of 60,000 training and 10,000 test samples.

Permuted MNIST: this problem differs from the MNIST problem described above in that we use
the entire MNIST dataset at each task. After observing the ﬁrst task, which is the standard MNIST
dataset, each subsequent task differs through the application of a ﬁxed permutation matrix Gi on
the entire MNIST dataset. The test task sequence differs from the training task sequence in that we
simply use the corresponding full train and test MNIST datasets (with the appropriate application of
Gi).

Celeb-A: We split the CelebA dataset into four individual distributions using the features: bald,
male, young and eye-glasses. As with the previous problems, we treat each subset of data as an
individual distribution, and present our model samples from a single distribution at a time. This
presents a real world scenario as the samples per distribution varies drastically from only 3,713
samples for the bald distribution, to 126,788 samples for young. In addition speciﬁc samples can
span one or more of these distributions.

SVHN to MNIST: in this problem, we transition from fully observing the centered SVHN [89]
dataset to observing the MNIST dataset. We treat all samples from SVHN as being generated by
one distribution P1(x) and all the MNIST 5 samples as generated by another distribution P2(x)
(irrespective of the speciﬁc digit). At inference, the model is required to reconstruct and generate
from both datasets.

6.3 SITUATING AGAINST STATE OF THE ART LIFELONG LEARNING MODELS.
To situate LGM against other state of the art methods in lifelong learning we use the sequential
FashionMNIST and MNIST datasets described earlier in Section 6.2 and the data ﬂow diagram in
Figure 7. We contrast our LGM model against VCL [91], VCL without a task speciﬁc head network,
SI [136], EWC [65], Laplace propagation [29], a full batch VAE trained jointly on all data and a
standard naive sequential VAE without any catastrophic forgetting prevention strategy in Figures 8
and 9 below. The full batch VAE presents the upper-bound performance and all lifelong learning
models typically under-perform this model by the ﬁnal learning task. For the baselines, we use
the generously open sourced code [90] by the VCL authors, using the optimal hyper-parameters
speciﬁed for each model. We begin by evaluating the 5000 sample Monte Carlo estimate of the
log-likelihood of all compared models in Figure 8 below:

Figure 8: IS log-likelihood (mean ± std) × 5. Left: Fashion MNIST. Right: MNIST.

5MNIST was resized to 32x32 and converted to RBG to make it consistent with the dimensions of SVHN.

15

Even though each trial was repeated ﬁve times (each), we observe large increases in the estimates
at a few critical points. After further inspection, we determined the large magnitude increases were
due to the model observing a drastically different distribution at that point. We overlay the graphs
with an example variate for of the magnitude spikes. In the case of FashionMNIST for example,
the model observes its ﬁrst shoe distribution at i = 6; this contrasts the previously observed items
which were mainly clothing related objects. Interestingly we observe that LGM has much smoother
performance across tasks. We posit this is because LGM does not constrain its parameters, and
instead enforces the same input-output mapping through functional regularization.

Figure 9: Final model, i = 10, generation and reconstructions for MNIST and FashionMNIST. The LGM
model presents competitive performance for both generations and reconstructions, while not preserving any
past data nor past models.

16

Since one of the core tenets of lifelong learning is to reduce sample complexity over time, we use
this experiment to validate if LGM does in fact achieve this objective. Since all LGM models are
trained with an early-stopping criterion, we can directly calculate the number of samples used for
each learning task using the stopping epoch and mean, π of the Bernoulli sampling distribution of
the student model. In Figure 10 we plot the number of true samples and the number of synthetic
samples used by a model until it satisﬁed its early-stopping criterion. We observe a steady decrease
in the number of real samples used over time, validating LGMs advantage in a lifelong setting.

Figure 10: FashionMNIST sample complexity. Left: Synthetic training samples used till early-stopping. Right:
Real samples used till early-stopping.

6.4 DIVING DEEPER INTO THE SEQUENCE.
Rather than only visualizing the ﬁnal model’s qualitative results as in Figure 9, we provide quali-
tative results for model performance over time for the PermutedMNIST experiment in Figure 11.
This allows us to visually observe lifelong model performance over time. In this experiment, we
focus our efforts on EWC and LGM and visualize model (test) reconstructions starting from the
second learning task, G1D, till the ﬁnal G4D. The EWC-VAE variant that we use as a baseline
has the same latent variable conﬁguration as our model, enabling the usage of the test ELBO as a
quantitative metric for comparison. We use an unpermuted version of the MNIST dataset, D, as our
ﬁrst distribution, P1(x), as it allows us to visually asses the degradation of reconstructions. This is a
common setup utilized in continual learning [65, 136] and we extend it here to the density estimation
setting.

Figure 11: Top row: test-samples; bottom row: reconstructions. We visualize an increasing number of accu-
mulated distributions from left to right. (a) Lifelong VAE model (b) EWC VAE model.

17

Both models exhibit a different form of degradation: EWC experiences a more destructive form of
degradation as exempliﬁed by the salt-and-pepper noise observed in the ﬁnal dataset reconstruction
at G4D. LGM on the hand experiences a form of Gaussian noise as visible in the corresponding
ﬁnal dataset reconstruction. In order to numerically quantify this performance we analyze the log-
Frechet distance and negative ELBO below in Figure 12, where we contrast the LGM to EWC, a
batch VAE (full-vae in graph), an upto-VAE that observes all training data up to the current distri-
bution and a vanilla sequential VAE (vanilla). We examine a variety of different convolutional and
dense architectures and present the top performing models below. We observe that LGM drastically
outperforms EWC and the baseline naive sequential VAE in both metrics.

Figure 12: PermutedMNIST (a) negative Test ELBO and (b) log-Frechet distance.

6.5 LEARNING ACROSS COMPLEX DISTRIBUTIONS

Figure 13: (a) Reconstructions of test samples from SVHN[left] and MNIST[right]; (b) Decoded samples
ˆx ∼ Pθ(x|zd, zc) based on linear interpolation of zc ∈ R2 with zd = [0, 1]; (c) Same as (b) but with
zd = [1, 0].

The typical assumption in lifelong learning is that the sequence of observed distributions are related
[125] in some manner. In this experiment we relax this constraint by learning a common model
between the colored SVHN dataset and the binary MNIST dataset. While semantically similar to
humans, these datasets are vastly different, as one is based on RGB images of real world house
numbers and the other of synthetically hand-drawn digits. We visualize examples of the true test
inputs, x, and their respective reconstructions, ˆx, from the ﬁnal lifelong model in ﬁgure 13(a). Even
though the only true data the ﬁnal model received for training was the MNIST dataset, it is still able
to reconstruct the SVHN data observed previously. This demonstrates the ability of our architecture
to transition between complex distributions while still preserving the knowledge learned from the
previously observed distributions.

Finally, in ﬁgure 13(b) and 13(c) we illustrate the data generated from an interpolation of a 2-
dimensional continuous latent space, zc ∈ R2. To generate variates, we set the discrete categorical,
zd, to one of the possible values {[0, 1], [1, 0]} and linearly interpolate the continuous zc over the
range [−3, 3]. We then decode these to obtain the samples, ˆx ∼ Pθ(x|zd, zc). The model learns

18

a common continuous structure for the two distributions which can be followed by observing the
development in the generated samples from top left to bottom right on both ﬁgure 13(b) and 13(c).

6.6 VALIDATING EMPIRICAL SAMPLE COMPLEXITY USING CELEB-A

We iterate the Celeb-A dataset as described in the data ﬂow diagram (Figure 7) and use this learn-
ing task to explore qualitative and quantitative generations, as well as empirical real world time
complexity (as described in Section 4.4) on modern GPU hardware. We train a lifelong model and
a typical VAE baseline without catastrophic forgetting mitigation strategies and evaluate the ﬁnal
model’s generations in Figure 14. As visually demonstrated in Figure 14-Left, the lifelong model is
able to generate instances from all of the previous distributions, however the baseline model catas-
trophically forgets (Figure 14-Right) and only generates samples from the eye-glasses distribution.
This is also reinforced by the log-Frechet distance shown in Figure 15.

Figure 14: Left: Sequential generations for Celeb-A from the ﬁnal lifelong model for bald, male, young and
eye-glasses (left to right). Right: (random) generations by the ﬁnal baseline VAE model.

We also evaluate the wall-clock time in seconds (Table 4) for the lifelong model and the baseline-vae
for the 44,218 samples of the male distribution. We observe that the lifelong model does not add a
signiﬁcant overhead, especially since the baseline-vae undergoes catastrophic forgetting (Figure 14
Right) and completely fails to generate samples from previous distributions. Note that we present
the number of parameters and other detailed model information in our code and Appendix 10.3.

44,218 male samples
training-epoch (s)
testing-epoch (s)

baseline-VAE
43.1 +/- 0.6
9.79 +/- 0.12

Lifelong
56.63 +/- 0.28
16.09 +/- 0.01

Table 4: Mean & standard deviation wall-clock for one epoch of
male distribution of Celeb-A.

Figure 15: Celeb-A log-Frechet distance of lifelong vs. naive baseline VAE model without catastrophic miti-
gation strategies over the four distributions. Listed on the right is the time per epoch (in seconds) for an epoch
of the corresponding models.

7 ABLATION STUDIES

In this section we independently validate the beneﬁt of each of the newly introduced components
to the learning objective proposed in Section 4.3. In Experiment 7.1 we demonstrate the beneﬁt
of the discrete-continuous posterior factorization introduced in Section 4.2.1. Then in Experiment

19

7.2, we validate the necessity of the information restricting regularizer (Section 4.2.2) and posterior
consistency regularizer (Section 4.1.1).

7.1 LINEAR SEPARABILITY OF DISCRETE AND CONTINUOUS POSTERIOR

Figure 16: Left: Graphical model depicting classiﬁcation using pretrained VAE, coupled with a linear classiﬁer,
fθlin : z (cid:55)→ y. Right: Linear classiﬁer accuracy on the Fashion MNIST test set for a varying range of latent
dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024] and distributions.

In order to validate that the (independent) discrete and continuous latent variable posterior,
Qφ(zd, zc|x), aids in learning a better representation, we classify the encoded posterior sample
using a simple linear classiﬁer fθlin : z (cid:55)→ y, where y corresponds to the categorical class predic-
tion. Higher (linear) classiﬁcation accuracies demonstrate that the the VAE is able to learn a more
linearly separable representation. Since the latent representation of VAEs are typically used in aux-
iliary tasks, learning such a representation is useful in downstream tasks. This is a standard method
to measure posterior separability and is used in methods such as Associative Compression Networks
[41].

We use the standard training set of FashionMNIST [135] (60,000 samples) to train a standard VAE
with a discrete only (disc) posterior, an isotropic-gaussian only (gauss) posterior, a bernoulli only
(bern) posterior and ﬁnally the proposed independent discrete and continuous (disc+gauss) posterior
presented in Section 4.2.1. For each different posterior reparameterization, we train a set of VAEs
with varying latent dimensions, |z| ∈ [32, 64, 128, 256, 512, 1024]. In the case of the disc+gauss
model we ﬁx the discrete dimension, |zd| = 10 and vary the isotropic-gaussian dimension to match
the total required dimension. After training each VAE, we proceed to use the same training data to
train a linear classiﬁer on the encoded posterior sample, z ∼ Qφ(z|x).

In Figure 16 we present the mean and standard deviation linear test classiﬁcation accuracies of each
set of the different experiments. As expected, the discrete only (disc) posterior performs poorly due
to the strong restriction of mapping an entire input sample to a single one-hot vector. The isotropic-
gaussian (gauss) and bernoulli (bern) only models provide a strong baseline, but the combination
of isotropic-gaussian and discrete posteriors (disc+gauss) performs much better, reaching an upper-
bound (linear) test-classiﬁcation accuracy of 87.1%. This validates that the decoupling of latent
represention presented in Section 4.2.1 aids in learning a more meaningful, separable posterior.

7.2 VALIDATING THE MUTUAL INFORMATION AND POSTERIOR CONSISTENCY

REGULARIZERS.

In order to independently evaluate the beneﬁt of our proposed Bayesian update regularizer (Sec-
tion 4.1.1) and the mutual information regularizer proposed in (Section 4.2.1) we perform an ab-
lation study using the MNIST data ﬂow sequence from Figure 7. We evaluate three scenarios: 1)
with posterior consistency and mutual information regularizers, 2) only posterior consistency and
3) without both regularizers. We observe that both components are necessary in order to gener-
ate high quality samples as evidenced by the negative test ELBO in Figure 17-(a) and the corre-

20

Figure 17: MNIST Ablation: (a) negative test ELBO. (b) Sequentially generated samples by setting zd and
sampling zc ∼ N (0, 1) (Section 4.2.1) with consistency + mutual information (MI). (c) Sequentially generated
samples with no consistency + no mutual information (MI).

sponding generations in Figure 17-(b-c). The generations produced without the information gain
regularizer and consistency in Figure 17-(c) are blurry. We attribute this to: 1) uniformly sam-
pling the discrete component is not guaranteed to generate samples representative samples from
P<i(x) and 2) the decoder, PΘ(x|zd, zc), relays more information through the continuous compo-
nent, PΘ(x|zd, zc) = PΘ(x|zc), causing catastrophic forgetting and posterior collapse [4].

8 LIMITATIONS

While LGM presents strong performance, it fails to completely solve the problem of lifelong gen-
erative modeling and we see a slow degradation in model performance over time. We attribute this
mainly to the problem of poor VAE generations that compound upon each other (also discussed
below). In addition, there are a few poignant issues that need to be resolved in order to achieve
an optimal (in terms of non-degrading Frechet distance / -ELBO) unsupervised generative lifelong
learner:

Distribution Boundary Evaluation: The standard assumption in current lifelong / continual learn-
ing approaches [91, 136, 112, 57, 65, 94] is to use known, ﬁxed distributions instead of learning
the distribution transition boundaries. For the purposes of this work, we focus on the accumulation
of distributions (in an unsupervised way), rather than introduce an additional level of indirection
through the incorporation of anomaly detection methods that aid in detecting distributional bound-
aries.

Blurry VAE Generations: VAEs are known to generate images that are blurry in contrast to GAN
based methods. This has been attributed to the fact that VAEs don’t learn the true posterior and
make a simplistic assumption regarding the reconstruction distribution Pθ(x|z) [4, 97]. While there
exist methods such as ALI [27] and BiGAN [26], that learn a posterior distribution within the GAN
framework, recent work has shown that adversarial methods fail to accurately match posterior-prior
distribution ratios in large dimensions [104].

Memory: In order to scale to a truly lifelong setting, we posit that a learning algorithm needs a
global pool of memory that can be decoupled from the learning algorithm itself. This decoupling
would also allow for a principled mechanism for parameter transfer between sequentially learnt
models as well a centralized location for compressing non-essential historical data. Recent work

21

such as the Kanerva Machine [133] and its extensions [134] provide a principled way to do this in
the VAE setting.

9 CONCLUSION

In this work we propose a novel method for learning generative models over a lifelong setting. The
principal assumption for the data is that they are generated by multiple distributions and presented
to the learner in a sequential manner. A key limitation for the learning process is that the method
has no access to any of the old data and that it shall distill all the necessary information into a
single ﬁnal model. The proposed method is based on a dual student-teacher architecture where the
teacher’s role is to preserve the past knowledge and aid the student in future learning. We argue for
and augment the standard VAE’s ELBO objective by terms helping the teacher-student knowledge
transfer. We demonstrate the beneﬁts this augmented objective brings to the lifelong learning setting
using a series of experiments. The architecture, combined with the proposed regularizers, aid in
mitigating the effects of catastrophic interference by supporting the retention of previously learned
knowledge.

REFERENCES

[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. Life-
long disentangled representation learning with cross-domain latent homologies. In Advances
in Neural Information Processing Systems, pages 9895–9905, 2018.

[2] W.-K. Ahn and W. F. Brewer. Psychological studies of explanationbased learning. In Investi-

gating explanation-based learning, pages 295–316. Springer, 1993.

[3] W.-K. Ahn, R. J. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one ex-
ample: Psychological evidence for explanation-based learning. Technical report, Coordinated
Science Laboratory, University of Illinois at Urbana-Champaign, 1987.

[4] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken

elbo. In International Conference on Machine Learning, pages 159–168, 2018.

[5] J. R. Anderson and G. H. Bower. Human associative memory. Psychology press, 2014.
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align

and translate. ICLR, 2015.

[7] H. R. Blackwell. Contrast thresholds of the human eye. JOSA, 36(11):624–643, 1946.
[8] Y. Blau and T. Michaeli. The perception-distortion tradeoff.

In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018.

[9] A. Blum. On-line algorithms in machine learning. In Online algorithms, pages 306–325.

Springer, 1998.

[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning, pages 1613–1622, 2015.
[11] L. Bottou. Online learning and stochastic approximations. On-line learning in neural net-

works, 17(9):142, 1998.

[12] L. Bottou and Y. L. Cun. Large scale online learning. In Advances in neural information

processing systems, pages 217–224, 2004.

[13] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural
image synthesis. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.

[14] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational
bayes. In Advances in Neural Information Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pages 1727–1735, 2013.

[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. ICLR, 2016.
[16] M. F. Carr, S. P. Jadhav, and L. M. Frank. Hippocampal replay in the awake state: a potential
substrate for memory consolidation and retrieval. Nature neuroscience, 14(2):147, 2011.

[17] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[18] X. Chen, X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.

22

In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
Neural Information Processing Systems 29, pages 2172–2180. Curran Associates, Inc., 2016.
[19] Z. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and

big data. In International Conference on Machine Learning, pages 703–711, 2014.

[20] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning, 10(3):1–145, 2016.

[21] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent
variable model for sequential data. In Advances in neural information processing systems,
pages 2980–2988, 2015.

[22] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International Conference on Machine Learning, pages 1282–1289,
2019.

[23] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson. Human photoreceptor

topography. Journal of comparative neurology, 292(4):497–523, 1990.

[24] P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and

related ﬁelds, 2(4):555–581, 1996.

[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.

[26] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782, 2016.

[27] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and

A. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
[28] E. Dupont. Learning disentangled joint continuous and discrete representations. In Advances

in Neural Information Processing Systems, pages 708–718, 2018.

[29] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace propagation. In Advances in Neural

Information Processing Systems, pages 441–448, 2004.

[30] L. Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories.
In Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141.
IEEE, 2003.

[31] G. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable.

In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1565–1574. ACM, 2016.

[32] A. Fiat and G. J. Woeginger. Online algorithms: The state of the art, volume 1442. Springer,

1998.

[33] T. Furlanello, J. Zhao, A. M. Saxe, L. Itti, and B. S. Tjan. Active long term memory networks.

arXiv preprint arXiv:1606.02355, 2016.

[34] A. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities.

Journal of the American statistical association, 85(410):398–409, 1990.

[35] S. Gershman and N. Goodman. Amortized inference in probabilistic reasoning. In Proceed-

ings of the Cognitive Science Society, volume 36, 2014.

[36] Z. Ghahramani and H. Attias. Online variational bayesian learning.

In Slides from talk

presented at NIPS workshop on Online Learning, 2000.

[37] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats, volume 9, pages 249–256, 2010.

[38] R. Gomes, M. Welling, and P. Perona. Incremental learning of nonparametric bayesian mix-
ture models. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2008.

[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.

[40] A. G. A. P. Goyal, A. Sordoni, M.-A. Cˆot´e, N. R. Ke, and Y. Bengio. Z-forcing: Training
stochastic recurrent networks. In Advances in neural information processing systems, pages
6713–6723, 2017.

[41] A. Graves, J. Menick, and A. v. d. Oord. Associative compression networks. arXiv preprint

arXiv:1804.02476, 2018.

23

[42] G. Grimmett, G. R. Grimmett, D. Stirzaker, et al. Probability and random processes. Oxford

university press, 2001.

[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6629–6640, 2017.

[45] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. stat,

[46] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–

1050:9, 2015.

1780, 1997.

[47] F. Huszar. Infogan: using the variational bound on mutual information (twice), Aug 2016.
[48] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reduc-

ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

[49] V. Jain and E. Learned-Miller. Online domain adaptation of a pre-trained cascade of classi-
ﬁers. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
577–584. IEEE, 2011.

[50] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. Interna-

tional Conference on Learning Representations, 2017.

[51] H. Jeffreys. An invariant form for the prior probability in estimation problems. In Proceedings
of the Royal Society of London a: mathematical, physical and engineering sciences, volume
186, pages 453–461. The Royal Society, 1946.

[52] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper-
vised and generative approach to clustering. In Proceedings of the 26th International Joint
Conference on Artiﬁcial Intelligence, pages 1965–1972. AAAI Press, 2017.

[53] A. Johnson and A. D. Redish. Neural ensembles in ca3 transiently encode paths forward of

the animal at a decision point. Journal of Neuroscience, 27(45):12176–12189, 2007.

[54] M. I. Jordan. Artiﬁcial neural networks. chapter Attractor Dynamics and Parallelism in a
Connectionist Sequential Machine, pages 112–127. IEEE Press, Piscataway, NJ, USA, 1990.
[55] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning, 37(2):183–233, 1999.

[56] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering, 82(1):35–45, 1960.

[57] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learn-

ing. arXiv preprint arXiv:1710.10368, 2017.

[58] M. P. Karlsson and L. M. Frank. Awake replay of remote experiences in the hippocampus.

Nature neuroscience, 12(7):913, 2009.

[59] M. Karpinski and A. Macintyre. Polynomial bounds for vc dimension of sigmoidal and
general pfafﬁan neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.

[60] I. Katakis, G. Tsoumakas, and I. Vlahavas. Incremental clustering for the classiﬁcation of

[61] H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine

concept-drifting data streams.

Learning, pages 2654–2663, 2018.

[62] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. 2015.
[63] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
[64] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907, 2016.

[65] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097–
1105, 2012.

[67] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

24

[68] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning, pages
1558–1566, 2016.

[69] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis. Continual classiﬁcation learning

using generative models. CoRR, abs/1810.10612, 2018.

[70] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks, 3361(10):1995, 1995.

[71] Z. Li and D. Hoiem. Learning without forgetting.

In European Conference on Computer

Vision, pages 614–629. Springer, 2016.

[72] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang,
In Proceedings of the European

and K. Murphy. Progressive neural architecture search.
Conference on Computer Vision (ECCV), pages 19–34, 2018.

[73] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder.

ICLR, 2016.

[74] C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix
gaussian posteriors. In International Conference on Machine Learning, pages 1708–1716,
2016.

[75] C. Louizos and M. Welling. Multiplicative normalizing ﬂows for variational bayesian neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 2218–2227. JMLR. org, 2017.

[76] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation

of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[77] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders.

arXiv preprint arXiv:1511.05644, 2015.

[78] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.
[79] J. McInerney, R. Ranganath, and D. Blei. The population posterior and bayesian modeling
on streams. In Advances in Neural Information Processing Systems, pages 1153–1161, 2015.
[80] A. Mishkin, F. Kunstner, D. Nielsen, M. Schmidt, and M. E. Khan. Slang: Fast structured
covariance approximations for bayesian deep learning with natural gradient. In Advances in
Neural Information Processing Systems, pages 6245–6255, 2018.

[81] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi,
M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the ACM, 61(5):103–
115, 2018.

[82] T. M. Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research, 1980.

[83] T. M. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. D. Mishra,
M. Gardner, B. Kisiel, J. Krishnamurthy, et al. Never-ending learning. In Twenty-Ninth AAAI
Conference on Artiﬁcial Intelligence, 2015.

[84] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte carlo gradient estimation in

machine learning. CoRR, abs/1906.10652, 2019.

[85] E. Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In International Confer-

ence on Learning Representations (ICLR), 2017.

[86] R. M. Neal. Bayesian Learning For Neural Networks. PhD thesis, University of Toronto,

[87] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte

1995.

carlo, 2(11):2, 2011.

[88] W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically exact, embarrassingly parallel
In N. L. Zhang and J. Tian, editors, Proceedings of the Thirtieth Conference on
MCMC.
Uncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27,
2014, pages 623–632. AUAI Press, 2014.

[89] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
In NIPS workshop on deep learning and

ural images with unsupervised feature learning.
unsupervised feature learning, page 5, 2011.

[90] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. nvcuong/variational-continual-learning,

2018.

[91] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. ICLR, 2018.

25

[92] K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, R. A. Olshen, and K. L. Oehler. Bayes risk
weighted vector quantization with posterior estimation for image compression and classiﬁca-
tion. IEEE Transactions on Image Processing, 5(2):347–360, 1996.

[93] F. A. Quintana and P. L. Iglesias. Bayesian clustering and product partition models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 65(2):557–574, 2003.
[94] N. C. Rabinowitz, G. Desjardins, A.-A. Rusu, K. Kavukcuoglu, R. T. Hadsell, R. Pascanu,
J. Kirkpatrick, and H. J. Soyer. Progressive neural networks, Nov. 23 2017. US Patent App.
15/396,319.

[95] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep con-

volutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[96] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are

unsupervised multitask learners. OpenAI Blog, 1(8), 2019.

[97] T. Rainforth, A. Kosiorek, T. A. Le, C. Maddison, M. Igl, F. Wood, and Y. W. Teh. Tighter
variational bounds are not necessarily better. In International Conference on Machine Learn-
ing, pages 4277–4285, 2018.

[98] J. Ramapuram. Lifelongvae pytorch repository., 2017.
[99] A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals. Preventing posterior collapse with delta-

vaes. ICLR, 2019.

[100] A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with

VQ-VAE-2. CoRR, abs/1906.00446, 2019.

[101] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
In International Conference on Machine Learning,

inference in deep generative models.
pages 1278–1286, 2014.

[102] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,

[103] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

1997.

statistics, pages 400–407, 1951.

[104] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational

inference. arXiv preprint arXiv:1802.06847, 2018.

[105] S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Comput.,

11(2):305–345, Feb. 1999.

[106] R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method.

Annals of Operations Research, 39(1):229–250, 1992.

[107] P. Ruvolo and E. Eaton. Ella: An efﬁcient lifelong learning algorithm.

In International

Conference on Machine Learning, pages 507–515, 2013.

[108] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models
via precision and recall. In Advances in Neural Information Processing Systems, pages 5228–
5237, 2018.

[109] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.

[110] N. W. Schuck and Y. Niv. Sequential replay of nonspatial task states in the human hippocam-

pus. Science, 364(6447):eaaw5181, 2019.

[111] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
hard attention to the task. In International Conference on Machine Learning, pages 4555–
4564, 2018.

[112] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In

Advances in Neural Information Processing Systems, pages 2994–3003, 2017.

[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484, 2016.

[114] D. L. Silver, G. Mason, and L. Eljabu. Consolidation using sweep task rehearsal: overcoming
In Canadian Conference on Artiﬁcial Intelligence, pages

the stability-plasticity problem.
307–322. Springer, 2015.

[115] D. L. Silver and R. E. Mercer. The parallel transfer of task knowledge using dynamic learning
rates based on a measure of relatedness. In Learning to learn, pages 213–233. Springer, 1996.
[116] D. L. Silver and R. E. Mercer. The task rehearsal method of life-long learning: Overcoming
In Conference of the Canadian Society for Computational Studies of

impoverished data.
Intelligence, pages 90–101. Springer, 2002.

26

[117] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algo-

rithms. In 2013 AAAI spring symposium series, 2013.

[118] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[119] W. E. Skaggs and B. L. McNaughton. Replay of neuronal ﬁring sequences in rat hippocampus

during sleep following spatial experience. Science, 271(5257):1870–1873, 1996.

[120] E. D. Sontag. Vc dimension of neural networks. NATO ASI Series F Computer and Systems

Sciences, 168:69–96, 1998.

[121] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[122] F. Tanaka and M. Yamamura. An approach to lifelong reinforcement learning through multi-
ple environments. In 6th European Workshop on Learning Robots, pages 93–99, 1997.
[123] A. V. Terekhov, G. Montone, and J. K. O’Regan. Knowledge transfer in deep block-modular
In Proceedings of the 4th International Conference on Biomimetic and

neural networks.
Biohybrid Systems-Volume 9222, pages 268–279. Springer-Verlag New York, Inc., 2015.
[124] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In
International Conference on Learning Representations (ICLR 2016), pages 1–10, 2016.
[125] S. Thrun. Lifelong learning: A case study. Technical report, CARNEGIE-MELLON UNIV

PITTSBURGH PA DEPT OF COMPUTER SCIENCE, 1995.

[126] S. Thrun and T. M. Mitchell. Lifelong robot learning.

In The biology and technology of

intelligent autonomous agents, pages 165–196. Springer, 1995.

[127] J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artiﬁcial

Intelligence and Statistics, pages 1214–1223, 2018.

[128] V. Vapnik. Estimation of dependences based on empirical data. Springer Science & Business

Media, 2006.

[129] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, R. Powell, et al. Alphastar: Mastering the real-time strategy game
starcraft ii. DeepMind Blog, 2019.

[130] S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic
model. In Proceedings of the 25th international conference on world wide web, pages 167–
176. International World Wide Web Conferences Steering Committee, 2016.

[131] R. C. Williamson and U. Helmke. Existence and uniqueness results for neural network ap-

proximations. IEEE Transactions on Neural Networks, 6(1):2–13, 1995.

[132] M. C. Wittrock. Generative learning processes of the brain. Educational Psychologist,

[133] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed

27(4):531–541, 1992.

memory. ICLR, 2018.

[134] Y. Wu, G. Wayne, K. Gregor, and T. Lillicrap. Learning attractor dynamics for generative

memory. In Advances in Neural Information Processing Systems, pages 9401–9410, 2018.

[135] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[136] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence.

In

International Conference on Machine Learning, pages 3987–3995, 2017.

27

10 APPENDIX

10.1 UNDERSTANDING THE CONSISTENCY REGULARIZER

The analytical derivations of the consistency regularizer show that the regularizer can be interpreted
as an a transformation of the standard VAE regularizer. In the case of an isotropic gaussian posterior,
the proposed regularizer scales the mean and variance of the student posterior by the variance of the
teacher 1 and adds an extra ’volume’ term. This interpretation of the consistency regularizer shows
that the proposed regularizer preserves the same learning objective as that of the standard VAE.
Below we present the analytical form of the consistency regularizer with categorical and isotropic
gaussian posteriors:

Proof 1 We assume the learnt posterior of the teacher is parameterized by a centered, isotropic
gaussian with Φ = [µE = 0, ΣE = diag(σE2
)] and the posterior of our student by a non-centered
isotropic gaussian with φ = [µS, ΣS = diag(σS2)], then

(cid:20)
tr(ΣE−1
KL(Qφ(z|x)||QΦ(z|x)) = 0.5

ΣS) + (µE − µS)T ΣE−1

(µE − µS) − F + log

= 0.5

F
(cid:88)

(cid:20)

j=1

1
σE2(j)

= KL(Qφ∗(z|x)||N (0, I)) − log |ΣE|

(σS2(j) + µS2(j)) − 1 + log σE2(j) − log σS2(j)

Via a reparameterization of the student’s parameters:

φ∗ = [µS∗, σS∗2]

µS∗ =

; σS∗2 =

µS(j)
σE2(j)

σS2(j)
σE2(j)

(cid:19)(cid:21)

(cid:18) |ΣE|
|ΣS|
(cid:21)

(21)

(22)

It is also interesting to note that our posterior regularizer becomes the prior if:

limσE2 (cid:55)→1KL(Qφ(z|x)||QΦ(z|x)) = KL(Qφ(z|x)||N (0, I))

Proof 2 We parameterize the learnt posterior of the teacher by Φi =

exp(pE
i )
i=1 exp(pE
i )

(cid:80)J

and the pos-

terior of the student by φi =
i ) and cS = (cid:80)J
cE = (cid:80)J
i=1 exp(pE
The KL divergence from the ELBO can now be re-written as:

exp(pS
i )
i=1 exp(pS
i )
i=1 exp(pS

(cid:80)J

. We also redeﬁne the normalizing constants as

i ) for the teacher and student models respectively.

KL(Qφ(zd|x)||QΦ(zd|x)) =

J
(cid:88)

i=1

exp(pS
i )
cS

log

(cid:18) exp(pS
i )
cS

(cid:19)

cE
exp(pE
i )

= H(pS, pS − pE) = −H(ps) + H(pS, pE)

(23)

where H( ) is the entropy operator and H( , ) is the cross-entropy operator.

28

10.2 RECONSTRUCTION REGULARIZER

Figure 18: Fashion Negative Test ELBO

Figure 19: Fashion Log-Frechet Distance

While it is possible to constrain the reconstruction/decoder term of the VAE in a similar manner to
the consistency posterior-regularizer, i.e: KL[Pθ(ˆx|z)||PΘ(ˆx|z)], doing so diminishes model per-
formance. We hypothesize that this is due to the fact that this regularizer contradicts the objective of
the reconstruction term Pθ(x|z) in the ELBO which already aims to minimize some metric between
the input samples x and the reconstructed samples ˆx; eg: if Pθ(x|z) ∼ N (µ, diag[σ]), then the
loss is proportional to ||ˆx − x||2
2, the standard L2 loss. Without the addition of this reconstruction
cross-model regularizer, the model is also provided with more ﬂexibility in how it reconstructs the
output samples.

In order to quantify the this we duplicate the FashionMNIST Experiment listed in the data ﬂow
deﬁnition in Figure 7. We use a simpler model than the main experiments to validate this hypothesis.
We train two dense models (-D): one with just the posterior consistency regularizer (without-LL-
D) and one with the consistency and likelihood regularizer (with-LL-D). We observe the model
performance drops (with respect to the Frechet distance as well the test ELBO) in the case of the
with-LL-D as demonstrated in Figures 18 and 19.

10.3 MODEL ARCHITECTURE

We used two different architectures for our experiments. When we use a dense network (-D) we
used two layers of 512 units to map to the latent representation and two layers of 512 to map back
to the reconstruction for the decoder. We used batch norm [48] and ELU activations for all the
layers barring the layer projecting into the latent representation and the output layer. Note that
while we used the same architecture for EWC we observed a drastic negative effect when using
batch norm and thus dropped it’s usage. The convolution architectures (-C) used the architecture
described below for the encoder and the decoder (where the decoder used conv-transpose layers for
upsampling). The notation is [OutputChannels, (ﬁlterX, ﬁlterY), stride]:

Encoder: [32, (5, 5), 1] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 1] (cid:55)→

GN+ELU (cid:55)→ [256, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [512, (1, 1), 1] (cid:55)→
GN+ELU (cid:55)→ [512, (1, 1), 1]

Decoder: [256, (4, 4), 1] (cid:55)→ GN+ELU (cid:55)→ [128, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [64, (4, 4), 1]

(24)

(cid:55)→ GN+ELU (cid:55)→ [32, (4, 4), 2] (cid:55)→ GN+ELU (cid:55)→ [32, (5, 5), 1]
(cid:55)→ GN+ELU (cid:55)→ [chans, (1, 1), 1]

29

Method
EWC-D
naive-D
batch-D
batch-D
lifelong-D
EWC-C
naive-C
batch-C
batch-C
lifelong-C

Initial zd dimension
10
10
10
10
1
10
10
10
10
1

Final zd dimension
10
10
10
10
10
10
10
10
10
10

zc dimension
14
14
14
14
14
14
14
14
14
14

# initial parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,165,311
30,767,428
7,691,280
7,691,280
15,382,560
15,235,072

# ﬁnal parameters
4,353,184
1,089,830
1,089,830
2,179,661
2,179,661
30,767,428
7,691,280
7,691,280
15,382,560
15,382,560

The table above lists the number of parameters for each model and architecture used in our ex-
periments. The lifelong models initially start with a zd of dimension 1 and at each step we grow
the representation by one dimension to accommodate the new distribution (more info in Section
10.7). In contrast, the baseline EWC models are provided with the full representation throughout
the learning process. EWC has double the number of parameters because the computed diagonal
ﬁsher information matrix which is the same dimensionality as the number of parameters. EWC also
neeeds the preservation of the teacher model [Φ, Θ] to use in it’s quadratic regularizer. Both the
naive and batch models have the fewest number of parameters as they do not use a student-teacher
framework and only use one model, however the vanilla model has no protection against catastrophic
interference and the full model is just used as an upper bound for performance.

We used Adam [62] to optimize all of our problems with a learning rate of 1e-4 or 1e-3. When we
used weight transfer we re-initialized the accumulated momentum vector of Adam as well as the
aggregated mean and variance of the batch norm layers. The full architecture can be examined in
our github repository [98] and is provided under an MIT license.

10.4 CONTRAST TO STREAMING / ONLINE METHODS

Our method has similarities to streaming methods such as Streaming Variational Bayes (SVB) [14]
and Incremental Bayesian Clustering methods [60, 38] in that we estimate and reﬁne posteriors
through time. In general this can be done through the following Bayesian update rule that states that
the lastest posterior is proportional to the current likelihood times the previous posterior:

P (z|X1, ..., Xt) ∝ P (Xt|z)P (z|X1, ..., Xt−1)
SVB computes the intractable posterior, P (z|X1, ..., Xt), utilizing an approximation, At, that ac-
cepts as input the current dataset, Xt, along with the previous posterior At−1 :

(25)

P (z|X1, ..., Xt) ≈ At(Xt, At−1)

(26)

The ﬁrst posterior input (At=0) to the approximating function is the prior P (z). The objective of
SVB and other streaming methods is to model the posterior of the currently observed data in the
best possible manner. Our setting differs from this in that we want to retain information from all
previously observed distributions (sometimes called a knowledge store [126]). This can be useful
in scenarios where a distribution is seen once, but only used much later down the road. Rather than
creating a posterior update rule, we recompute the posterior via Equation 25, leveraging the fact that
we can re-generate X<t ≈ ˆX<t through the generative process. This allows us to recompute a more
appropriate posterior re-using all of the (generated) data, rather than using the previously computed
(approximate) posterior At−1:

P (z|X1, X2, ..., Xt) ∝ P (Xt|z)P (z| ˆX1, ..., ˆXt−1)

(27)

Coupling this generative replay strategy with the Bayesian update regularizer introduced in Section
4.1.1, we demonstrate that not only do we learn an updated poster as in Equation 27, but also allow
for a natural transfer of information between sequentially learnt models: a fundamental tenant of
lifelong learning [126, 125].

Finally, another key difference between lifelong learning and online methods is that lifelong learning
aims to learn from a sequence of tentatively different [20] tasks while still retaining and accumulating

30

knowledge; online learning generally assumes that the true underlying distribution comes from a
single distribution [11]. There are some exceptions to this where online learning is applied to the
problem of domain adaptation, eg: [49, 60].

10.5 EWC BASELINES: COMPARING CONV & DENSE NETWORKS

We compared a whole range of EWC baselines and use the best performing models few in our
experiments. Listed in Figure 10.5 are the full range of EWC baselines run on the PermutedMNIST
and FashionMNIST experiments. Recall that C / D describes whether a model is convolutional or
dense and the the number following is the hyperparameter for the EWC or Lifelong VAE.

10.6 GUMBEL REPARAMETERIZATION

Since we model our latent variable as a combination of a discrete and a continuous distribution we
also use the Gumbel-Softmax reparameterization [76, 50]. The Gumbel-Softmax reparameterization
over logits [linear output of the last layer in the encoder] p ∈ RM and an annealed temperature
parameter τ ∈ R is deﬁned as:

z = sof tmax(

); g = −log(−log(u ∼ U nif (0, 1)))

(28)

log(p) + g
τ

u ∈ RM , g ∈ RM . As the temperature parameter τ (cid:55)→ 0, z converges to a categorical.

10.7 EXPANDABLE MODEL CAPACITY AND REPRESENTATIONS

Multilayer neural networks with sigmoidal activations have a VC dimension bounded between
O(ρ2)[120] and O(ρ4)[59] where ρ are the number of parameters. A model that is able to con-

31

sistently add new information should also be able to expand its VC dimension by adding new pa-
rameters over time. Our formulation imposes no restrictions on the model architecture: i.e. new
layers can be added freely to the new student model.
In addition we also allow the dimensionality of zd ∈ RJ , our discrete latent representation to grow
in order to accommodate new distributions. This is possible because the KL divergence between
two categorical distributions of different sizes can be evaluated by simply zero padding the teacher’s
smaller discrete distribution. Since we also transfer weights between the teacher and the student
model, we need to handle the case of expanding latent representations appropriately. In the event
that we add a new distribution we copy all the weights besides the ones immediately surrounding
the projection into and out of the latent distribution. These surrounding weights are reinitialized to
their standard Glorot initializations [37].

32

