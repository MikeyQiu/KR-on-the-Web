Relevance of Unsupervised Metrics in Task-Oriented Dialogue for
Evaluating Natural Language Generation

Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer
Microsoft Maluuba
first.last@microsoft.com

7
1
0
2
 
n
u
J
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
9
7
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Automated metrics such as BLEU are
widely used in the machine translation lit-
erature. They have also been used recently
in the dialogue community for evaluating
dialogue response generation. However,
previous work in dialogue response gener-
ation has shown that these metrics do not
correlate strongly with human judgment
in the non task-oriented dialogue setting.
Task-oriented dialogue responses are ex-
pressed on narrower domains and exhibit
lower diversity.
It is thus reasonable to
think that these automated metrics would
correlate well with human judgment in the
task-oriented setting where the generation
task consists of translating dialogue acts
into a sentence. We conduct an empirical
study to conﬁrm whether this is the case.
Our ﬁndings indicate that these automated
metrics have stronger correlation with hu-
man judgments in the task-oriented setting
compared to what has been observed in the
non task-oriented setting. We also observe
that these metrics correlate even better for
datasets which provide multiple ground
truth reference sentences. In addition, we
show that some of the currently available
corpora for task-oriented language genera-
tion can be solved with simple models and
advocate for more challenging datasets.

1

Introduction

Rule-based and template-based dialogue response
generation systems have been around for a long
time (Axelrod, 2000; Elhadad, 1992). Even today,
many task-oriented dialogue systems deployed
in production are rule-based and template-based.
These systems do not scale with increasing do-

main complexity and maintaining the increasing
number of templates becomes cumbersome.
In
the past, Oh and Rudnicky (2000) proposed a
corpus-based approach for Natural Language Gen-
eration (NLG) for task-oriented dialogue systems.
Other statistical approaches were proposed us-
ing tree-based models and reinforcement learning
(Walker et al., 2007; Rieser and Lemon, 2009).
Recently, deep-learning based approaches (Wen
et al., 2015b; Sharma et al., 2017; Lowe et al.,
2015; Serban et al., 2016) have shown promising
results for dialogue response generation.

The

automated

of machine-
evaluation
challenging and an
generated language
is
important problem for
language
the natural
processing community. The most widely used au-
tomated metrics currently are word-overlap based
metrics such as BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005) which
were proposed originally for machine translation.
While these metrics were shown to correlate
well with manual human evaluation in machine
translation tasks, previous studies showed that this
is not the case in non-task oriented dialogue (Liu
et al., 2016). This is explained by the fact that for
the same context (e.g. a user utterance), responses
in dialogue have more diversity. Word-overlap
metrics are unable to capture semantics and thus,
can lead to poor scores even for appropriate
responses. Human evaluation in this case is the
most reliable metric. However, human judgments
are expensive to obtain and not readily available
at all times.

Task-oriented dialogue systems are employed in
narrower domains (e.g. booking a restaurant) and
responses do not have as much diversity as in the
non-task oriented setting. Another important dif-
ference is that in the non-task oriented setting, re-
sponse generation is often performed end-to-end,
which means that the model takes as input the last

user utterance and potentially the dialogue history
and it outputs the next system answer. In the task-
oriented setting, on the other hand, the language
generation task is often seen as a translation step
from an abstract representation of a sentence to
the sentence itself. As a consequence, automated
metrics which compare a generated sentence to a
reference sentence might be more appropriate and
In this paper,
correlate with human judgments.
we:

• study the correlation between human judg-
ments and several unsupervised automated
metrics on two popular task-oriented dia-
logue datasets,

• introduce variants of existing models and
evaluate their performance on these metrics

We ﬁnd that the automated metrics have stronger
correlation with human judgments in the task-
oriented setting than what has been observed in
the non task-oriented setting. We also observe that
these metrics correlate even more in the presence
of multiple reference sentences.

2 Related Work

Liu et al. (2016) did an empirical study to eval-
uate the correlation between human scores and
several automated word-overlap metrics as well
as embedding-based metrics for dialogue response
generation. They observed that these metrics,
though widely used in the literature, had only
weak correlation with human judgments in the non
task-oriented dialogue NLG setting.

In terms of supervised NLG evaluation metrics,
Lowe et al. (2017) proposed the ADEM model
which trains a hierarchical recurrent neural net-
work in a supervised manner to predict human-like
scores. This learned score was shown to corre-
late better with human judgments than any other
automated metric. However, the drawback of this
approach is the requirement for expensive human
ratings.

Li et al. (2016) proposed to use reinforcement
learning to train an end-to-end dialogue system.
They simulate a dialogue between two agents and
use a policy gradient algorithm with a reward
function which evaluates speciﬁc properties of the
responses generated by the dialogue system.

In the adversarial setting, Kannan and Vinyals
(2016) train a recurrent neural network discrimi-
nator to differentiate human-generated responses

from model-generated responses. However, an ex-
tensive analysis of the viability and the ease of
standardization of this approach is yet to be con-
ducted. Li et al. (2017), apart from adversari-
ally training dialogue response models, propose an
independent adversarial evaluation metric Adver-
Suc and a measure of the model’s reliability called
evaluator reliability error. Drawbacks of these ap-
proaches are that they are model-dependent. Ad-
versarial methods might be promising for task-
oriented dialogue systems but more research needs
to be conducted on their account.

Most of the work described so far has been
done in the non task-oriented dialogue setting as
there has been prior work indicating that auto-
mated metrics do not correlate well with humans
in that setting. There has not yet been any empiri-
cal validation that these conclusions also apply to
the task oriented setting. Research in the task ori-
ented setting has mostly made use of automated
metrics such as BLEU and human evaluation (Wen
et al., 2015b; Sharma et al., 2017; Duˇsek and Jur-
cicek, 2016).

3 Metrics

This section describes the set of automatic met-
rics whose correlation with human evaluation is
studied. We consider ﬁrst word-overlap metrics
and then embedding-based metrics.
In all that
follows, when multiple references are provided,
we compute the similarity between the prediction
and all the references one-by-one, and then select
the maximum value. We then average the scores
across the entire corpus.

3.1 Word-overlap based metrics

3.1.1 BLEU
The BLEU metric (Papineni et al., 2002) compares
n-grams between the candidate utterance and the
reference utterance. The BLEU score is com-
puted at the corpus-level and relies on the follow-
ing modiﬁed precision:

pn =

(cid:88)

(cid:88)

Ctclip(n − gram)

C∈{Candidates}
(cid:88)

n−gram∈C
(cid:88)

C(cid:48)∈{Candidates(cid:48)}

n−gram(cid:48)∈C(cid:48)

Ctclip(n − gram(cid:48))

(1)

where {Candidates} are the candidate answers
generated by the model and Ctclip is the clipped

count for the n-gram which is the number of times
the n-gram is common to the candidate answer
and the reference answer clipped by the maximum
number of occurrences of the n-gram in the refer-
ence answer. The BLEU-N score is deﬁned as:

BLEU-N = BP exp(

ωn log(pn))

(2)

N
(cid:88)

n

where N is the maximum length of the n-grams
(in this paper, we compute BLEU-1 to BLEU-4),
ω is a weighting that is often uniform and BP is a
brevity penalty. In this paper we report the BLEU
score at the corpus level but we also compute this
score at the sentence level to analyze its correla-
tion with human evaluation.

3.1.2 METEOR
The METEOR metric (Banerjee and Lavie, 2005)
was proposed as a metric which correlates better
at the sentence level with human evaluation. To
compute the METEOR score, ﬁrst, an alignment
between the candidate and the reference sentences
is created by mapping each unigram in the can-
didate sentence to 0 or 1 unigram in the reference
sentence. The alignment is not only based on exact
matches but also stem, synonym, and paraphrase
matches. Based on this alignment, unigram pre-
cision and recall are computed and the METEOR
score is:

METEOR = Fmean(1 − p)

where Fmean is the harmonic mean between preci-
sion and recall with the weight for recall 9 times a
high as the weight for precision, and p is a penalty.

3.1.3 ROUGE
ROUGE (Lin, 2004) is a set of metrics that was
ﬁrst introduced for summarization. We compute
ROUGE-L which is an F-measure based on the
Longest Common Subsequence (LCS) between
the candidate and reference utterances.

3.2 Embedding based metrics

We consider another set of metrics which compute
the cosine similarity between the embeddings of
the predicted and the reference sentence instead of
relying on word overlaps.

3.2.1 Skip-Thought
The Skip-Thought model (Kiros et al., 2015) is
trained in an unsupervised fashion and uses a re-
current network to encode a given sentence into

an embedding and then decode it to predict the
preceding and following sentences. The model
was trained on the BookCorpus dataset (Zhu et al.,
2015). The embeddings produced by the encoder
have a robust performance on semantic relatedness
tasks. We use the pre-trained Skip-Thought en-
coder provided by the authors1.

We also compute other embedding-based meth-
ods which have been used as evaluation metrics
for measuring human correlation in recent litera-
ture (Liu et al., 2016) for non task-oriented dia-
logue in Sections 3.2.2, 3.2.3, and 3.2.4.

3.2.2 Embedding average
This metric computes a sentence-level embedding
by averaging the embeddings of the words com-
posing this sentence:

¯eC =

(cid:80)
| (cid:80)

w∈C ew
w(cid:48)∈C ew(cid:48)|

.

In this equation, the vectors ew are embeddings for
the words w in the candidate sentence C.

3.2.3 Vector extrema
Vector extrema (Forgues et al., 2014) computes a
sentence-level embedding by taking the most ex-
treme value of the embeddings of the words com-
posing the sentence for each dimension of the em-
bedding:

(cid:40)

(3)

erd =

maxw∈C ewd
minw∈C ewd

if ewd > | minw(cid:48)∈C ew(cid:48)d|
otherwise.

In this equation, d is an index over the dimensions
of the embedding and C is the candidate sentence.

3.2.4 Greedy matching
Greedy matching does not compute a sentence em-
bedding but directly a similarity score between a
candidate C and a reference r (Rus and Lintean,
2012). This similarity score is computed as fol-
lows:

G(C, r) =

(cid:80) w ∈ C max ˆw∈r cos sim(ew, w ˆw)
|C|

GM (C, r) =

G(C, r) + G(r, C)
2

.

(4)

In other words, each word in the candidate sen-
tence is greedily matched to a word in the ref-
erence sentence based on the cosine similarity of

1https://github.com/ryankiros/skip-thoughts

their embeddings. The score is an average of these
similarities over the number of words in the can-
didate sentence. The same score is computed by
reversing the roles of the candidate and reference
sentences and the average of the two scores gives
the ﬁnal similarity score.

4 Response Generation Models

This section presents the different natural lan-
guage generation models that we use in this study.
All of these models take as input a set of dialogue
acts (Austin, 1962) potentially with slot types and
slot values and translate this input into an utter-
ance. An example input is inform(food =
Chinese) and a corresponding output would be
“I am looking for a Chinese restaurant.”. In this
example, the dialogue act is inform, the slot type
is food, and the slot value is Chinese.

4.1 Random

Given a dialogue act with one or more slot types,
the random model ﬁnds all the examples in the
training set with the same dialogue act and slots
(while ignoring slot values) and it randomly se-
lects its output from this set of reference sen-
tences. The datasets that we experiment on have
some special slot values such as “yes”, “no”, and
“don’t care”. Since the model ignores all slot val-
ues, these special cases are not properly handled,
which results in slightly lower performance than
what we could get by spending more time hand-
engineering the model’s behavior for these values.

4.2 LSTM

This model consists of a recurrent LSTM (Hochre-
iter and Schmidhuber, 1997) decoder. The dia-
logue acts and slot types are ﬁrst encoded as a bi-
nary vector whose length is the number of possi-
ble combinations of dialogue acts and slot types
in the dataset. We refer to this binary vector as
the Dialogue Act (DA) vector. The DA vector
for a given set of dialogue acts is a binary vec-
tor over the fused dialogue act-slot types, e.g.,
INFORM-FOOD, INFORM-COUNT, etc.

This binary vector is given as input to the de-
coder at each time-step of the LSTM. The decoder
then outputs a delexicalized sentence. A delexi-
calized sentence contains placeholders for the slot
values. An example is “I am looking for a FOOD
restaurant.”. The values for the delexicalized slots
(the type of food in this example) are then directly

copied from the input.

4.3 delex-sc-LSTM

This model uses the same architecture as the
LSTM model presented in the previous section ex-
cept that it uses sc-LSTM (Wen et al., 2015b) units
in the decoder instead of LSTM units. We call this
model the “delex-sc-LSTM”2. As in the previous
model, the input DA vector only encodes acts and
delexicalized slots. It does not contain any infor-
mation about the slot value.

By providing this model the same DA vector
input as the one given to the LSTM model, we can
directly study if the additional complexity of the
sc-LSTM unit’s reading gate provides signiﬁcant
improvement over the small-sized task-oriented
dialogue datasets which are currently available.

4.4 hierarchical-lex-delex-sc-LSTM

is a variant of the “ld-sc-LSTM”
This model
model proposed by Sharma et al. (2017) which is
based on an encoder-decoder framework. We call
our model “hierarchical-lex-delex-sc-LSTM”3.

Figure 1: Encoder of the hld-scLSTM model

Figure 2: Decoder of the hld-scLSTM model

We present the encoder in Figure 1. The en-
coder consists of a hierarchical LSTM with Ne
time-steps, where Ne is the number of non-zero
entries in the DA vector. Each time-step of the en-
coder encodes one dialogue act’s delexicalized and
lexicalized slot-value pair (e.g. (INFORM-FOOD,

2We will also refer to it as “d-scLSTM”.
3We will also refer to it as“hld-scLSTM”.

Gold
Random
LSTM

B-1
1.00
0.875
0.900
d-scLSTM 0.880
hld-scLSTM 0.909

B-2
1.00
0.843
0.879
0.850
0.890

DSTC2
B-3
1.00
0.822
0.863
0.828
0.878

B-4
1.00
0.807
0.851
0.812
0.870

M
1.00
0.564
0.610
0.578
0.624

R L
1.00
0.852
0.888
0.874
0.899

B-1
1.00
0.872
0.982
0.980
0.985

B-2
1.00
0.813
0.966
0.964
0.978

Restaurants
B-4
B-3
1.00
1.00
0.721
0.765
0.932
0.949
0.931
0.948
0.962
0.970

M
1.00
0.504
0.652
0.654
0.704

R L
1.00
0.796
0.944
0.945
0.965

Table 1: Performance comparison across models on word-overlap based automated metrics

Skip
Thought
1.00
0.906
0.946
0.925
0.932

DSTC2
Embedding
Average
1.00
0.981
0.985
0.984
0.987

Restaurants

Greedy
Vector
Extrema Matching

1.00
0.910
0.935
0.926
0.942

1.00
0.947
0.962
0.957
0.964

Skip
Thought
1.00
0.843
0.945
0.948
0.968

Embedding
Average
1.00
0.957
0.997
0.997
0.997

Greedy
Vector
Extrema Matching

1.00
0.905
0.986
0.986
0.989

1.00
0.930
0.991
0.991
0.993

Gold
Random
LSTM
d-scLSTM
hld-scLSTM

Table 2: Performance comparison across models on sentence-embedding based automated metrics

‘Chinese’)). The delexicalized act-slot part is en-
coded as a one-hot vector which we refer to as
DAt. DAt is constructed by masking all except
the tth dialogue act in the DA vector4. The lexi-
calized value part is encoded by an LSTM encoder
which shares parameters across all time-steps and
operates over the word-embeddings of the lexical-
ized values vt,i. Our model differs from the “ld-sc-
LSTM” model in that we use an LSTM encoder
over the word-embeddings instead of computing
the mean of the word-embeddings. The ﬁnal hid-
den state of this LSTM is concatenated with DAt
and is given as input to the upper LSTM (see Fig-
ure 1). The ﬁnal hidden state of the upper LSTM
is then provided to the decoder as input. This is
another difference from the “ld-sc-LSTM” as that
uses the mean of all the hidden states of the en-
coder instead, which, in our experiments, did not
perform as well as using just the ﬁnal hidden state
E.

The decoder is described in Figure 2. It is the
same as in the “ld-sc-LSTM” model. At each
time-step, it takes as input the encoder output E,
the DA vector, and the word-embedding of the
word generated at the previous time-step. The
DA vector is also additionally provided to the sc-
LSTM cell in order for it to be regulated by its
reading gate as described in Wen et al. (2015b).

5 Experiments

5.1 Decoding

During training, at each time-step, we use the
ground truth word from the previous time-step.

4also referred to as DAt=1:Ne

The model thus learns to generate the next word
given the previous one. On the other hand, to
generate sentences during test time, we use beam
search. The ﬁrst word input to the generator is a
special token < bos > which indicates the begin-
ning of the sequence. Decoding is stopped if we
reach a speciﬁed maximum number of time-steps
or if the model outputs a special token < eos >
which indicates the end of the sequence. We also
use a slot error rate penalty, similarly to Wen et al.
(2015b), to re-rank the sentences generated with
beam search. We use this method for all three of
the LSTM, d-scLSTM, and hld-scLSTM models
for fairness.

Similarly to the LSTM model, the d-scLSTM
and hld-scLSTM generate delexicalized sen-
tences, i.e., they generate slot tokens instead of
slot values directly. These slot tokens are replaced
with slot values in a post-processing step which
is a fairly common step in task-oriented dialogue
NLG literature.

5.2 Evaluation

In NLG tasks, improvements in automated met-
ric scores are most commonly used to demonstrate
improvement in the generation task. However,
these metrics have been shown to only weakly
correlate with human evaluation in the non task-
oriented dialogue setting (Liu et al., 2016) and
hence are not considered reliable measures of im-
provement. Human evaluation is considered the
metric of choice, but human ratings are expensive
to obtain. The ease of computing these automated
metrics and their availability for rapid prototyping
has lead to their widespread adoption.

Metric
Bleu 1
Bleu 2
Bleu 3
Bleu 4
METEOR
ROUGE L
Skip Thought
Embedding Average
Vector Extrema
Greedy Matching
Human

Spearman
-0.317
-0.318
-0.318
-0.318
0.295
0.294
0.528
0.295
0.299
0.295
0.810

DSTC2
p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.583
0.526
0.500
0.461
0.582
0.448
0.086
0.485
0.624
0.572
0.984

p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
0.397
<0.005
<0.005
<0.005
<0.005

Restaurants

Spearman
0.069
0.091
0.109
0.105
0.353
0.346
0.284
0.423
0.446
0.446
0.653

p-value
0.494
0.366
0.280
0.296
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.277
0.166
0.223
0.255
0.489
0.382
0.364
0.260
0.287
0.325
0.857

p-value
0.005
0.099
0.026
0.010
<0.005
<0.005
<0.005
0.009
<0.005
<0.005
<0.005

Table 3: Correlation of automated metrics with human evaluations scores

We evaluate the models described in the pre-
vious section on the DSTC2 (Henderson et al.,
2014) and the Restaurants datasets (Wen et al.,
2015a) using these automated metrics. These
datasets are some of the only available resources
for studying NLG for task-oriented dialogue. The
DSTC2 dataset contains dialogues between human
users and a dialogue system in a restaurant do-
main. The dataset is annotated with dialogue acts,
slot type, and slot values. The NLG component
of the dialogue system used for data collection is
templated. The Restaurants dataset was speciﬁ-
cally proposed for NLG and provides, for a set of
dialogue acts with slot types and slot values, two
sentences generated by humans.

We present the results of our experiments in Ta-
ble 1 and Table 2. The code for our automated
metric evaluation pipeline is available at https:
//github.com/Maluuba/nlg-eval. The
scores of all the models on these automated met-
rics are very high. This indicates that there is sig-
niﬁcant word overlap between the generated and
the reference sentences and that the NLG task on
these datasets can be solved with a simple model
such as the LSTM model. In effect, table 1 shows
that the LSTM model performs comparably to the
d-scLSTM model based on the word-overlap met-
rics. This can be explained by the fact that the
d-scLSTM model has more parameters and might
suffer from overﬁtting issues on these relatively
small datasets.

The hld-scLSTM is considered to consistently
outperform the other models based on the word-
overlap metrics. As explained by Sharma et al.
(2017), this improvement results from the model’s
access to the lexicalized slot values, due to which
it can take into account the grammatical associa-
tions of the generated words near the output to-
kens, thereby generating higher quality sentences.

κ
>0.1
>0.2
>0.3
>0.4
>0.5
>0.6

# pairs % pairs
100.0 %
55/55
72.7 %
40/55
50.9 %
28/55
34.5 %
19/55
14.5 %
8/55
0.0 %
0/55

Table 4: Pairwise Cohen’s kappa scores for the 11
human users

However, Table 2 shows that sentence-embedding
based metrics judge all the models except the ran-
dom one to perform quite similarly with again,
very high performance scores.

In the next section, we add human evaluation

for these models on these datasets.

5.3 Human rating collection

We randomly selected 20 dialogue acts from the
test set of each dataset. For each of these contexts,
we presented 5 sentences to the evaluators:
the
gold response provided in the test set and the re-
sponses generated by the four models described in
Section 4. These sentences were randomly shuf-
ﬂed and not presented in the same order. We in-
vited 18 human users to score each of these 100
sentences on a Likert-type scale of 1 to 5. The
users were asked to rate the responses depending
on how appropriate they were for the speciﬁed di-
alogue acts. A score of 1 was the lowest score,
meaning that the response was not appropriate at
all whereas a score of 5 meant that the sentence
was highly appropriate.

We computed Cohen’s kappa scores (Cohen,
1960) between the human users in pairs of two.
We removed 7 users who had kappa scores less
than 0.1 and used the remaining 11 users for the
correlation study. The kappa scores are presented
in Table 4. Most of the user pairs have a Cohen’s
κ > 0.3 which indicates fair agreement between

(a) DSTC2

(b) Restaurants

Figure 3: Scatter plots for correlation of some automated metrics with human evaluation for (a) the
DSTC2 dataset, and (b) the Restaurants dataset. Random gaussian noise N (0, 0.1) has been added to
data points along the human score axis and N (0, 0.02) has been added to the automated metric score’s
axis to aid visualization of overlapping data points. Transparency has been added for the same effect.

users (Viera et al., 2005).

5.4 Correlation between automated metrics

and human scores

We present the correlation between the automated
metrics and our collected human ratings in Ta-
ble 3. We measure human v.s. human correla-
tion by randomly splitting the human users into
two groups. The results indicate that in most
cases, human scores correlate the best with other
human scores. Except in the case of the Spear-
man correlation for BLEU-N scores, we can see
that there is a positive correlation between the au-
tomated metrics and the human scores for these
task-oriented datasets, which contrasts with the
non task-oriented dialogue setting where Liu et al.
(2016) observed no strong correlation trends.

A likely explanation for the negative Spearman
correlation values for BLEU-N is that there is
only one gold reference per context in the DSTC2
dataset. The Restaurants dataset, on the other
hand, provides two gold references per context.
Having multiple gold references increases the like-
lihood that the generated response will have sig-
niﬁcant word-overlap with one of the reference re-
sponses.

We present scatter plots for some of the metrics
presented in Table 3 in Figure 3. We observe that
all the metrics correlate very well with humans
on high scoring examples. As it can be seen in
the scatter plots, most of the sentences are given
the maximal score of 5 by the human evaluators.
This conﬁrms our previous observation that the
available corpora for task-oriented dialogue NLG
task are not very challenging and a simple LSTM-
based model can output high-quality responses.

Overall, among the word overlap based auto-
mated metrics, METEOR consistently correlates
with human evaluation on both datasets. These
results conﬁrm the original ﬁndings by Banerjee
and Lavie (2005) who showed that METEOR had
good correlation with human evaluation in the ma-
chine translation task. The comparison with ma-
chine translation is highly relevant in the task-
oriented setting because the NLG model essen-
tially learns to translate the abstract representation
of a sentence into a sentence. It is a translation task
contrary to the non task-oriented setting where the
NLG model needs to decide and output a new sen-
tence based on the last sentence typed by a user
and dialogue history. Therefore, automated met-
rics coming from the machine translation literature

are more adequate in our case than in the non-task
oriented case as shown by Liu et al. (2016).

It is interesting to see that METEOR correlates
well with human evaluation consistently. This
can be explained by the fact that even though
METEOR does not rely on word embeddings, it
includes notions of synonymy and paraphrasing
when computing the alignment between the can-
didate and reference utterances.

6 Discussion

We evaluated several natural language generation
models trained on the DSTC2 and the Restaurants
datasets based on several automated metrics. We
also performed human evaluation on the model-
generated responses and our study shows that hu-
man evaluation is a much more reliable metric
compared to the others. Among the word-overlap
based automated metrics, we found that the ME-
TEOR score correlates the most with human judg-
ments and we suggest using METEOR for task-
oriented dialogue natural language generation in-
stead of BLEU. We also observe that these met-
rics are more reliable in the task-oriented dialogue
setting compared to the general, non task-oriented
one due to the limited possible diversity in the
task-oriented setting. Also, as observed by Gal-
ley et al. (2015), we can see that word-overlap
based metrics correlate better with human evalu-
ation when multiple references are provided, as in
the Restaurants dataset. Otherwise, as in the case
of DSTC2 which only provides one reference sen-
tence per example, we observe that all the BLEU-
N metrics negatively correlate with human evalu-
ation on Spearman correlation.

As has been observed in the machine translation
literature, using beam search improves the quality
of generated sentences signiﬁcantly compared to
stochastic sampling. For similar models, our re-
sults show improvement in the automated metrics’
scores compared to Wen et al. (2015b) who used
stochastic sampling for decoding instead of beam
search.

Wen et al. (2015b) did not use the slot er-
ror rate penalty with the vanilla LSTM model
in their experiments. After adding the penalty
in our case, we observe that the vanilla LSTM-
based model performs as well as the delexicalized
semantically-controlled LSTM model. This sug-
gests that the added complexity introduced by the
sc-LSTM unit does not offer a signiﬁcant advan-

tage for these two datasets.

High performance on automated metrics,
achieved by our models on the DSTC2 and the
Restaurants datasets lead us to conclude that these
datasets are not very challenging for the NLG task.
The task-oriented dialogue community should
move towards using larger and more complex
datasets, which have been recently announced,
such as the Frames dataset (El Asri et al., 2017) or
the E2E NLG Challenge dataset (Novikova et al.,
2016).

References

John Langshaw Austin. 1962. How to do things with

words. Oxford University Press.

information system.

Scott Axelrod. 2000. Natural language generation
In Pro-
in the ibm ﬂight
ceedings of the 2000 ANLP/NAACL Workshop on
Conversational Systems - Volume 3. Association
for Computational Linguistics, Stroudsburg, PA,
USA, ANLP/NAACL-ConvSyst ’00, pages 21–26.
https://doi.org/10.3115/1117562.1117567.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proc. ACL
correlation with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and psychological

nominal scales.
measurement 20(1):37–46.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-
to-sequence generation for spoken dialogue via
In Proceed-
deep syntax trees and strings.
ings of
the As-
the 54th Annual Meeting of
sociation for Computational Linguistics (Volume
2:
Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057 .

Michael Elhadad. 1992. Generating coherent argumen-
In In Proceedings of COLING

tative paragraphs.
’92, volume II. pages 638–644.

Gabriel

Forgues,

Jean-Marie
Larcheveque, and Real Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.

Pineau,

Joelle

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for generation

tasks with intrinsically diverse targets. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
http://www.aclweb.org/anthology/P15-
445–450.
2073.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state track-
In Proceedings of the 15th Annual
ing challenge.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL). Association for Compu-
tational Linguistics, Philadelphia, PA, U.S.A., pages
http://www.aclweb.org/anthology/W14-
263–272.
4337.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016.

Ad-
versarial evaluation of dialogue models.
In
NIPS 2016 Workshop on Adversarial Training.
https://arxiv.org/pdf/1701.08198.pdf.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdi-
nov, Richard Zemel, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. 2015.
Skip-
thought vectors.
In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, ed-
itors, Advances in Neural Information Process-
ing Systems 28, Curran Associates,
Inc., pages
3294–3302. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1192–1202.
https://aclweb.org/anthology/D16-
1127.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. CoRR abs/1701.06547.
http://arxiv.org/abs/1701.06547.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2122–2132.
http://aclweb.org/anthology/D16-1230.

Ryan Lowe, Michael Noseworthy,

Iulian V. Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic tur-
ing test: Learning to evaluate dialogue responses.
In Proceedings of
the 5th International Confer-
ence on Learning Representations (ICLR) Work-
shop. Toulon, France.

Ryan Lowe, Nissan Pow, IV Serban, Laurent Charlin,
and Joelle Pineau. 2015. Incorporating unstructured
textual knowledge sources into neural dialogue sys-
In Neural Information Processing Systems
tems.
Workshop on Machine Learning for Spoken Lan-
guage Understanding.

Jekaterina Novikova, Oliver Lemon, and Verena
Crowd-sourcing nlg data: Pic-
Rieser. 2016.
In Proceedings of
tures elicit better data.
the 9th International Natural Language Gen-
eration conference. Association for Computa-
tional Linguistics, Edinburgh, UK, pages 265–273.
http://anthology.aclweb.org/W16-6644.

Alice H. Oh and Alexander

In Proceedings of

I. Rudnicky. 2000.
Stochastic language generation for spoken di-
the 2000
alogue systems.
ANLP/NAACL Workshop
on Conversational
Systems - Volume 3. Association for Compu-
PA, USA,
tational Linguistics,
ANLP/NAACL-ConvSyst
27–32.
pages
https://doi.org/10.3115/1117562.1117568.

Stroudsburg,
’00,

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Verena Rieser and Oliver Lemon. 2009. Natural
language generation as planning under uncertainty
In Proceedings of
for spoken dialogue systems.
the 12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 683–691.
http://www.aclweb.org/anthology/E09-1078.

Vasile Rus and Mihai C. Lintean. 2012. A compari-
son of greedy and optimal assessment of natural lan-
guage student input using word-to-word similarity
metrics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP.

Iulian Vlad Serban, Alessandro Sordoni, Ryan
Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2016.
A hi-
erarchical
latent variable encoder-decoder model
for generating dialogues. CoRR abs/1605.06069.
http://arxiv.org/abs/1605.06069.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2017. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR) Workshop.

Anthony J Viera, Joanne M Garrett, et al. 2005. Under-
standing interobserver agreement: the kappa statis-
tic. Family Medicine 37(5):360–363.

Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007.
Individual and
domain adaptation in sentence planning for
Int. Res. 30(1):413–456.
dialogue.
http://dl.acm.org/citation.cfm?id=1622637.1622648.

J. Artif.

Tsung-Hsien Wen, Milica Gaˇsi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a.
Stochastic Language Generation
in Dialogue using Recurrent Neural Networks with
Convolutional Sentence Reranking. In Proceedings
of the 16th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL). As-
sociation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In The IEEE International Con-
ference on Computer Vision (ICCV).

Relevance of Unsupervised Metrics in Task-Oriented Dialogue for
Evaluating Natural Language Generation

Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer
Microsoft Maluuba
first.last@microsoft.com

7
1
0
2
 
n
u
J
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
9
7
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Automated metrics such as BLEU are
widely used in the machine translation lit-
erature. They have also been used recently
in the dialogue community for evaluating
dialogue response generation. However,
previous work in dialogue response gener-
ation has shown that these metrics do not
correlate strongly with human judgment
in the non task-oriented dialogue setting.
Task-oriented dialogue responses are ex-
pressed on narrower domains and exhibit
lower diversity.
It is thus reasonable to
think that these automated metrics would
correlate well with human judgment in the
task-oriented setting where the generation
task consists of translating dialogue acts
into a sentence. We conduct an empirical
study to conﬁrm whether this is the case.
Our ﬁndings indicate that these automated
metrics have stronger correlation with hu-
man judgments in the task-oriented setting
compared to what has been observed in the
non task-oriented setting. We also observe
that these metrics correlate even better for
datasets which provide multiple ground
truth reference sentences. In addition, we
show that some of the currently available
corpora for task-oriented language genera-
tion can be solved with simple models and
advocate for more challenging datasets.

1

Introduction

Rule-based and template-based dialogue response
generation systems have been around for a long
time (Axelrod, 2000; Elhadad, 1992). Even today,
many task-oriented dialogue systems deployed
in production are rule-based and template-based.
These systems do not scale with increasing do-

main complexity and maintaining the increasing
number of templates becomes cumbersome.
In
the past, Oh and Rudnicky (2000) proposed a
corpus-based approach for Natural Language Gen-
eration (NLG) for task-oriented dialogue systems.
Other statistical approaches were proposed us-
ing tree-based models and reinforcement learning
(Walker et al., 2007; Rieser and Lemon, 2009).
Recently, deep-learning based approaches (Wen
et al., 2015b; Sharma et al., 2017; Lowe et al.,
2015; Serban et al., 2016) have shown promising
results for dialogue response generation.

The

automated

of machine-
evaluation
challenging and an
generated language
is
important problem for
language
the natural
processing community. The most widely used au-
tomated metrics currently are word-overlap based
metrics such as BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005) which
were proposed originally for machine translation.
While these metrics were shown to correlate
well with manual human evaluation in machine
translation tasks, previous studies showed that this
is not the case in non-task oriented dialogue (Liu
et al., 2016). This is explained by the fact that for
the same context (e.g. a user utterance), responses
in dialogue have more diversity. Word-overlap
metrics are unable to capture semantics and thus,
can lead to poor scores even for appropriate
responses. Human evaluation in this case is the
most reliable metric. However, human judgments
are expensive to obtain and not readily available
at all times.

Task-oriented dialogue systems are employed in
narrower domains (e.g. booking a restaurant) and
responses do not have as much diversity as in the
non-task oriented setting. Another important dif-
ference is that in the non-task oriented setting, re-
sponse generation is often performed end-to-end,
which means that the model takes as input the last

user utterance and potentially the dialogue history
and it outputs the next system answer. In the task-
oriented setting, on the other hand, the language
generation task is often seen as a translation step
from an abstract representation of a sentence to
the sentence itself. As a consequence, automated
metrics which compare a generated sentence to a
reference sentence might be more appropriate and
In this paper,
correlate with human judgments.
we:

• study the correlation between human judg-
ments and several unsupervised automated
metrics on two popular task-oriented dia-
logue datasets,

• introduce variants of existing models and
evaluate their performance on these metrics

We ﬁnd that the automated metrics have stronger
correlation with human judgments in the task-
oriented setting than what has been observed in
the non task-oriented setting. We also observe that
these metrics correlate even more in the presence
of multiple reference sentences.

2 Related Work

Liu et al. (2016) did an empirical study to eval-
uate the correlation between human scores and
several automated word-overlap metrics as well
as embedding-based metrics for dialogue response
generation. They observed that these metrics,
though widely used in the literature, had only
weak correlation with human judgments in the non
task-oriented dialogue NLG setting.

In terms of supervised NLG evaluation metrics,
Lowe et al. (2017) proposed the ADEM model
which trains a hierarchical recurrent neural net-
work in a supervised manner to predict human-like
scores. This learned score was shown to corre-
late better with human judgments than any other
automated metric. However, the drawback of this
approach is the requirement for expensive human
ratings.

Li et al. (2016) proposed to use reinforcement
learning to train an end-to-end dialogue system.
They simulate a dialogue between two agents and
use a policy gradient algorithm with a reward
function which evaluates speciﬁc properties of the
responses generated by the dialogue system.

In the adversarial setting, Kannan and Vinyals
(2016) train a recurrent neural network discrimi-
nator to differentiate human-generated responses

from model-generated responses. However, an ex-
tensive analysis of the viability and the ease of
standardization of this approach is yet to be con-
ducted. Li et al. (2017), apart from adversari-
ally training dialogue response models, propose an
independent adversarial evaluation metric Adver-
Suc and a measure of the model’s reliability called
evaluator reliability error. Drawbacks of these ap-
proaches are that they are model-dependent. Ad-
versarial methods might be promising for task-
oriented dialogue systems but more research needs
to be conducted on their account.

Most of the work described so far has been
done in the non task-oriented dialogue setting as
there has been prior work indicating that auto-
mated metrics do not correlate well with humans
in that setting. There has not yet been any empiri-
cal validation that these conclusions also apply to
the task oriented setting. Research in the task ori-
ented setting has mostly made use of automated
metrics such as BLEU and human evaluation (Wen
et al., 2015b; Sharma et al., 2017; Duˇsek and Jur-
cicek, 2016).

3 Metrics

This section describes the set of automatic met-
rics whose correlation with human evaluation is
studied. We consider ﬁrst word-overlap metrics
and then embedding-based metrics.
In all that
follows, when multiple references are provided,
we compute the similarity between the prediction
and all the references one-by-one, and then select
the maximum value. We then average the scores
across the entire corpus.

3.1 Word-overlap based metrics

3.1.1 BLEU
The BLEU metric (Papineni et al., 2002) compares
n-grams between the candidate utterance and the
reference utterance. The BLEU score is com-
puted at the corpus-level and relies on the follow-
ing modiﬁed precision:

pn =

(cid:88)

(cid:88)

Ctclip(n − gram)

C∈{Candidates}
(cid:88)

n−gram∈C
(cid:88)

C(cid:48)∈{Candidates(cid:48)}

n−gram(cid:48)∈C(cid:48)

Ctclip(n − gram(cid:48))

(1)

where {Candidates} are the candidate answers
generated by the model and Ctclip is the clipped

count for the n-gram which is the number of times
the n-gram is common to the candidate answer
and the reference answer clipped by the maximum
number of occurrences of the n-gram in the refer-
ence answer. The BLEU-N score is deﬁned as:

BLEU-N = BP exp(

ωn log(pn))

(2)

N
(cid:88)

n

where N is the maximum length of the n-grams
(in this paper, we compute BLEU-1 to BLEU-4),
ω is a weighting that is often uniform and BP is a
brevity penalty. In this paper we report the BLEU
score at the corpus level but we also compute this
score at the sentence level to analyze its correla-
tion with human evaluation.

3.1.2 METEOR
The METEOR metric (Banerjee and Lavie, 2005)
was proposed as a metric which correlates better
at the sentence level with human evaluation. To
compute the METEOR score, ﬁrst, an alignment
between the candidate and the reference sentences
is created by mapping each unigram in the can-
didate sentence to 0 or 1 unigram in the reference
sentence. The alignment is not only based on exact
matches but also stem, synonym, and paraphrase
matches. Based on this alignment, unigram pre-
cision and recall are computed and the METEOR
score is:

METEOR = Fmean(1 − p)

where Fmean is the harmonic mean between preci-
sion and recall with the weight for recall 9 times a
high as the weight for precision, and p is a penalty.

3.1.3 ROUGE
ROUGE (Lin, 2004) is a set of metrics that was
ﬁrst introduced for summarization. We compute
ROUGE-L which is an F-measure based on the
Longest Common Subsequence (LCS) between
the candidate and reference utterances.

3.2 Embedding based metrics

We consider another set of metrics which compute
the cosine similarity between the embeddings of
the predicted and the reference sentence instead of
relying on word overlaps.

3.2.1 Skip-Thought
The Skip-Thought model (Kiros et al., 2015) is
trained in an unsupervised fashion and uses a re-
current network to encode a given sentence into

an embedding and then decode it to predict the
preceding and following sentences. The model
was trained on the BookCorpus dataset (Zhu et al.,
2015). The embeddings produced by the encoder
have a robust performance on semantic relatedness
tasks. We use the pre-trained Skip-Thought en-
coder provided by the authors1.

We also compute other embedding-based meth-
ods which have been used as evaluation metrics
for measuring human correlation in recent litera-
ture (Liu et al., 2016) for non task-oriented dia-
logue in Sections 3.2.2, 3.2.3, and 3.2.4.

3.2.2 Embedding average
This metric computes a sentence-level embedding
by averaging the embeddings of the words com-
posing this sentence:

¯eC =

(cid:80)
| (cid:80)

w∈C ew
w(cid:48)∈C ew(cid:48)|

.

In this equation, the vectors ew are embeddings for
the words w in the candidate sentence C.

3.2.3 Vector extrema
Vector extrema (Forgues et al., 2014) computes a
sentence-level embedding by taking the most ex-
treme value of the embeddings of the words com-
posing the sentence for each dimension of the em-
bedding:

(cid:40)

(3)

erd =

maxw∈C ewd
minw∈C ewd

if ewd > | minw(cid:48)∈C ew(cid:48)d|
otherwise.

In this equation, d is an index over the dimensions
of the embedding and C is the candidate sentence.

3.2.4 Greedy matching
Greedy matching does not compute a sentence em-
bedding but directly a similarity score between a
candidate C and a reference r (Rus and Lintean,
2012). This similarity score is computed as fol-
lows:

G(C, r) =

(cid:80) w ∈ C max ˆw∈r cos sim(ew, w ˆw)
|C|

GM (C, r) =

G(C, r) + G(r, C)
2

.

(4)

In other words, each word in the candidate sen-
tence is greedily matched to a word in the ref-
erence sentence based on the cosine similarity of

1https://github.com/ryankiros/skip-thoughts

their embeddings. The score is an average of these
similarities over the number of words in the can-
didate sentence. The same score is computed by
reversing the roles of the candidate and reference
sentences and the average of the two scores gives
the ﬁnal similarity score.

4 Response Generation Models

This section presents the different natural lan-
guage generation models that we use in this study.
All of these models take as input a set of dialogue
acts (Austin, 1962) potentially with slot types and
slot values and translate this input into an utter-
ance. An example input is inform(food =
Chinese) and a corresponding output would be
“I am looking for a Chinese restaurant.”. In this
example, the dialogue act is inform, the slot type
is food, and the slot value is Chinese.

4.1 Random

Given a dialogue act with one or more slot types,
the random model ﬁnds all the examples in the
training set with the same dialogue act and slots
(while ignoring slot values) and it randomly se-
lects its output from this set of reference sen-
tences. The datasets that we experiment on have
some special slot values such as “yes”, “no”, and
“don’t care”. Since the model ignores all slot val-
ues, these special cases are not properly handled,
which results in slightly lower performance than
what we could get by spending more time hand-
engineering the model’s behavior for these values.

4.2 LSTM

This model consists of a recurrent LSTM (Hochre-
iter and Schmidhuber, 1997) decoder. The dia-
logue acts and slot types are ﬁrst encoded as a bi-
nary vector whose length is the number of possi-
ble combinations of dialogue acts and slot types
in the dataset. We refer to this binary vector as
the Dialogue Act (DA) vector. The DA vector
for a given set of dialogue acts is a binary vec-
tor over the fused dialogue act-slot types, e.g.,
INFORM-FOOD, INFORM-COUNT, etc.

This binary vector is given as input to the de-
coder at each time-step of the LSTM. The decoder
then outputs a delexicalized sentence. A delexi-
calized sentence contains placeholders for the slot
values. An example is “I am looking for a FOOD
restaurant.”. The values for the delexicalized slots
(the type of food in this example) are then directly

copied from the input.

4.3 delex-sc-LSTM

This model uses the same architecture as the
LSTM model presented in the previous section ex-
cept that it uses sc-LSTM (Wen et al., 2015b) units
in the decoder instead of LSTM units. We call this
model the “delex-sc-LSTM”2. As in the previous
model, the input DA vector only encodes acts and
delexicalized slots. It does not contain any infor-
mation about the slot value.

By providing this model the same DA vector
input as the one given to the LSTM model, we can
directly study if the additional complexity of the
sc-LSTM unit’s reading gate provides signiﬁcant
improvement over the small-sized task-oriented
dialogue datasets which are currently available.

4.4 hierarchical-lex-delex-sc-LSTM

is a variant of the “ld-sc-LSTM”
This model
model proposed by Sharma et al. (2017) which is
based on an encoder-decoder framework. We call
our model “hierarchical-lex-delex-sc-LSTM”3.

Figure 1: Encoder of the hld-scLSTM model

Figure 2: Decoder of the hld-scLSTM model

We present the encoder in Figure 1. The en-
coder consists of a hierarchical LSTM with Ne
time-steps, where Ne is the number of non-zero
entries in the DA vector. Each time-step of the en-
coder encodes one dialogue act’s delexicalized and
lexicalized slot-value pair (e.g. (INFORM-FOOD,

2We will also refer to it as “d-scLSTM”.
3We will also refer to it as“hld-scLSTM”.

Gold
Random
LSTM

B-1
1.00
0.875
0.900
d-scLSTM 0.880
hld-scLSTM 0.909

B-2
1.00
0.843
0.879
0.850
0.890

DSTC2
B-3
1.00
0.822
0.863
0.828
0.878

B-4
1.00
0.807
0.851
0.812
0.870

M
1.00
0.564
0.610
0.578
0.624

R L
1.00
0.852
0.888
0.874
0.899

B-1
1.00
0.872
0.982
0.980
0.985

B-2
1.00
0.813
0.966
0.964
0.978

Restaurants
B-4
B-3
1.00
1.00
0.721
0.765
0.932
0.949
0.931
0.948
0.962
0.970

M
1.00
0.504
0.652
0.654
0.704

R L
1.00
0.796
0.944
0.945
0.965

Table 1: Performance comparison across models on word-overlap based automated metrics

Skip
Thought
1.00
0.906
0.946
0.925
0.932

DSTC2
Embedding
Average
1.00
0.981
0.985
0.984
0.987

Restaurants

Greedy
Vector
Extrema Matching

1.00
0.910
0.935
0.926
0.942

1.00
0.947
0.962
0.957
0.964

Skip
Thought
1.00
0.843
0.945
0.948
0.968

Embedding
Average
1.00
0.957
0.997
0.997
0.997

Greedy
Vector
Extrema Matching

1.00
0.905
0.986
0.986
0.989

1.00
0.930
0.991
0.991
0.993

Gold
Random
LSTM
d-scLSTM
hld-scLSTM

Table 2: Performance comparison across models on sentence-embedding based automated metrics

‘Chinese’)). The delexicalized act-slot part is en-
coded as a one-hot vector which we refer to as
DAt. DAt is constructed by masking all except
the tth dialogue act in the DA vector4. The lexi-
calized value part is encoded by an LSTM encoder
which shares parameters across all time-steps and
operates over the word-embeddings of the lexical-
ized values vt,i. Our model differs from the “ld-sc-
LSTM” model in that we use an LSTM encoder
over the word-embeddings instead of computing
the mean of the word-embeddings. The ﬁnal hid-
den state of this LSTM is concatenated with DAt
and is given as input to the upper LSTM (see Fig-
ure 1). The ﬁnal hidden state of the upper LSTM
is then provided to the decoder as input. This is
another difference from the “ld-sc-LSTM” as that
uses the mean of all the hidden states of the en-
coder instead, which, in our experiments, did not
perform as well as using just the ﬁnal hidden state
E.

The decoder is described in Figure 2. It is the
same as in the “ld-sc-LSTM” model. At each
time-step, it takes as input the encoder output E,
the DA vector, and the word-embedding of the
word generated at the previous time-step. The
DA vector is also additionally provided to the sc-
LSTM cell in order for it to be regulated by its
reading gate as described in Wen et al. (2015b).

5 Experiments

5.1 Decoding

During training, at each time-step, we use the
ground truth word from the previous time-step.

4also referred to as DAt=1:Ne

The model thus learns to generate the next word
given the previous one. On the other hand, to
generate sentences during test time, we use beam
search. The ﬁrst word input to the generator is a
special token < bos > which indicates the begin-
ning of the sequence. Decoding is stopped if we
reach a speciﬁed maximum number of time-steps
or if the model outputs a special token < eos >
which indicates the end of the sequence. We also
use a slot error rate penalty, similarly to Wen et al.
(2015b), to re-rank the sentences generated with
beam search. We use this method for all three of
the LSTM, d-scLSTM, and hld-scLSTM models
for fairness.

Similarly to the LSTM model, the d-scLSTM
and hld-scLSTM generate delexicalized sen-
tences, i.e., they generate slot tokens instead of
slot values directly. These slot tokens are replaced
with slot values in a post-processing step which
is a fairly common step in task-oriented dialogue
NLG literature.

5.2 Evaluation

In NLG tasks, improvements in automated met-
ric scores are most commonly used to demonstrate
improvement in the generation task. However,
these metrics have been shown to only weakly
correlate with human evaluation in the non task-
oriented dialogue setting (Liu et al., 2016) and
hence are not considered reliable measures of im-
provement. Human evaluation is considered the
metric of choice, but human ratings are expensive
to obtain. The ease of computing these automated
metrics and their availability for rapid prototyping
has lead to their widespread adoption.

Metric
Bleu 1
Bleu 2
Bleu 3
Bleu 4
METEOR
ROUGE L
Skip Thought
Embedding Average
Vector Extrema
Greedy Matching
Human

Spearman
-0.317
-0.318
-0.318
-0.318
0.295
0.294
0.528
0.295
0.299
0.295
0.810

DSTC2
p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.583
0.526
0.500
0.461
0.582
0.448
0.086
0.485
0.624
0.572
0.984

p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
0.397
<0.005
<0.005
<0.005
<0.005

Restaurants

Spearman
0.069
0.091
0.109
0.105
0.353
0.346
0.284
0.423
0.446
0.446
0.653

p-value
0.494
0.366
0.280
0.296
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.277
0.166
0.223
0.255
0.489
0.382
0.364
0.260
0.287
0.325
0.857

p-value
0.005
0.099
0.026
0.010
<0.005
<0.005
<0.005
0.009
<0.005
<0.005
<0.005

Table 3: Correlation of automated metrics with human evaluations scores

We evaluate the models described in the pre-
vious section on the DSTC2 (Henderson et al.,
2014) and the Restaurants datasets (Wen et al.,
2015a) using these automated metrics. These
datasets are some of the only available resources
for studying NLG for task-oriented dialogue. The
DSTC2 dataset contains dialogues between human
users and a dialogue system in a restaurant do-
main. The dataset is annotated with dialogue acts,
slot type, and slot values. The NLG component
of the dialogue system used for data collection is
templated. The Restaurants dataset was speciﬁ-
cally proposed for NLG and provides, for a set of
dialogue acts with slot types and slot values, two
sentences generated by humans.

We present the results of our experiments in Ta-
ble 1 and Table 2. The code for our automated
metric evaluation pipeline is available at https:
//github.com/Maluuba/nlg-eval. The
scores of all the models on these automated met-
rics are very high. This indicates that there is sig-
niﬁcant word overlap between the generated and
the reference sentences and that the NLG task on
these datasets can be solved with a simple model
such as the LSTM model. In effect, table 1 shows
that the LSTM model performs comparably to the
d-scLSTM model based on the word-overlap met-
rics. This can be explained by the fact that the
d-scLSTM model has more parameters and might
suffer from overﬁtting issues on these relatively
small datasets.

The hld-scLSTM is considered to consistently
outperform the other models based on the word-
overlap metrics. As explained by Sharma et al.
(2017), this improvement results from the model’s
access to the lexicalized slot values, due to which
it can take into account the grammatical associa-
tions of the generated words near the output to-
kens, thereby generating higher quality sentences.

κ
>0.1
>0.2
>0.3
>0.4
>0.5
>0.6

# pairs % pairs
100.0 %
55/55
72.7 %
40/55
50.9 %
28/55
34.5 %
19/55
14.5 %
8/55
0.0 %
0/55

Table 4: Pairwise Cohen’s kappa scores for the 11
human users

However, Table 2 shows that sentence-embedding
based metrics judge all the models except the ran-
dom one to perform quite similarly with again,
very high performance scores.

In the next section, we add human evaluation

for these models on these datasets.

5.3 Human rating collection

We randomly selected 20 dialogue acts from the
test set of each dataset. For each of these contexts,
we presented 5 sentences to the evaluators:
the
gold response provided in the test set and the re-
sponses generated by the four models described in
Section 4. These sentences were randomly shuf-
ﬂed and not presented in the same order. We in-
vited 18 human users to score each of these 100
sentences on a Likert-type scale of 1 to 5. The
users were asked to rate the responses depending
on how appropriate they were for the speciﬁed di-
alogue acts. A score of 1 was the lowest score,
meaning that the response was not appropriate at
all whereas a score of 5 meant that the sentence
was highly appropriate.

We computed Cohen’s kappa scores (Cohen,
1960) between the human users in pairs of two.
We removed 7 users who had kappa scores less
than 0.1 and used the remaining 11 users for the
correlation study. The kappa scores are presented
in Table 4. Most of the user pairs have a Cohen’s
κ > 0.3 which indicates fair agreement between

(a) DSTC2

(b) Restaurants

Figure 3: Scatter plots for correlation of some automated metrics with human evaluation for (a) the
DSTC2 dataset, and (b) the Restaurants dataset. Random gaussian noise N (0, 0.1) has been added to
data points along the human score axis and N (0, 0.02) has been added to the automated metric score’s
axis to aid visualization of overlapping data points. Transparency has been added for the same effect.

users (Viera et al., 2005).

5.4 Correlation between automated metrics

and human scores

We present the correlation between the automated
metrics and our collected human ratings in Ta-
ble 3. We measure human v.s. human correla-
tion by randomly splitting the human users into
two groups. The results indicate that in most
cases, human scores correlate the best with other
human scores. Except in the case of the Spear-
man correlation for BLEU-N scores, we can see
that there is a positive correlation between the au-
tomated metrics and the human scores for these
task-oriented datasets, which contrasts with the
non task-oriented dialogue setting where Liu et al.
(2016) observed no strong correlation trends.

A likely explanation for the negative Spearman
correlation values for BLEU-N is that there is
only one gold reference per context in the DSTC2
dataset. The Restaurants dataset, on the other
hand, provides two gold references per context.
Having multiple gold references increases the like-
lihood that the generated response will have sig-
niﬁcant word-overlap with one of the reference re-
sponses.

We present scatter plots for some of the metrics
presented in Table 3 in Figure 3. We observe that
all the metrics correlate very well with humans
on high scoring examples. As it can be seen in
the scatter plots, most of the sentences are given
the maximal score of 5 by the human evaluators.
This conﬁrms our previous observation that the
available corpora for task-oriented dialogue NLG
task are not very challenging and a simple LSTM-
based model can output high-quality responses.

Overall, among the word overlap based auto-
mated metrics, METEOR consistently correlates
with human evaluation on both datasets. These
results conﬁrm the original ﬁndings by Banerjee
and Lavie (2005) who showed that METEOR had
good correlation with human evaluation in the ma-
chine translation task. The comparison with ma-
chine translation is highly relevant in the task-
oriented setting because the NLG model essen-
tially learns to translate the abstract representation
of a sentence into a sentence. It is a translation task
contrary to the non task-oriented setting where the
NLG model needs to decide and output a new sen-
tence based on the last sentence typed by a user
and dialogue history. Therefore, automated met-
rics coming from the machine translation literature

are more adequate in our case than in the non-task
oriented case as shown by Liu et al. (2016).

It is interesting to see that METEOR correlates
well with human evaluation consistently. This
can be explained by the fact that even though
METEOR does not rely on word embeddings, it
includes notions of synonymy and paraphrasing
when computing the alignment between the can-
didate and reference utterances.

6 Discussion

We evaluated several natural language generation
models trained on the DSTC2 and the Restaurants
datasets based on several automated metrics. We
also performed human evaluation on the model-
generated responses and our study shows that hu-
man evaluation is a much more reliable metric
compared to the others. Among the word-overlap
based automated metrics, we found that the ME-
TEOR score correlates the most with human judg-
ments and we suggest using METEOR for task-
oriented dialogue natural language generation in-
stead of BLEU. We also observe that these met-
rics are more reliable in the task-oriented dialogue
setting compared to the general, non task-oriented
one due to the limited possible diversity in the
task-oriented setting. Also, as observed by Gal-
ley et al. (2015), we can see that word-overlap
based metrics correlate better with human evalu-
ation when multiple references are provided, as in
the Restaurants dataset. Otherwise, as in the case
of DSTC2 which only provides one reference sen-
tence per example, we observe that all the BLEU-
N metrics negatively correlate with human evalu-
ation on Spearman correlation.

As has been observed in the machine translation
literature, using beam search improves the quality
of generated sentences signiﬁcantly compared to
stochastic sampling. For similar models, our re-
sults show improvement in the automated metrics’
scores compared to Wen et al. (2015b) who used
stochastic sampling for decoding instead of beam
search.

Wen et al. (2015b) did not use the slot er-
ror rate penalty with the vanilla LSTM model
in their experiments. After adding the penalty
in our case, we observe that the vanilla LSTM-
based model performs as well as the delexicalized
semantically-controlled LSTM model. This sug-
gests that the added complexity introduced by the
sc-LSTM unit does not offer a signiﬁcant advan-

tage for these two datasets.

High performance on automated metrics,
achieved by our models on the DSTC2 and the
Restaurants datasets lead us to conclude that these
datasets are not very challenging for the NLG task.
The task-oriented dialogue community should
move towards using larger and more complex
datasets, which have been recently announced,
such as the Frames dataset (El Asri et al., 2017) or
the E2E NLG Challenge dataset (Novikova et al.,
2016).

References

John Langshaw Austin. 1962. How to do things with

words. Oxford University Press.

information system.

Scott Axelrod. 2000. Natural language generation
In Pro-
in the ibm ﬂight
ceedings of the 2000 ANLP/NAACL Workshop on
Conversational Systems - Volume 3. Association
for Computational Linguistics, Stroudsburg, PA,
USA, ANLP/NAACL-ConvSyst ’00, pages 21–26.
https://doi.org/10.3115/1117562.1117567.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proc. ACL
correlation with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and psychological

nominal scales.
measurement 20(1):37–46.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-
to-sequence generation for spoken dialogue via
In Proceed-
deep syntax trees and strings.
ings of
the As-
the 54th Annual Meeting of
sociation for Computational Linguistics (Volume
2:
Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057 .

Michael Elhadad. 1992. Generating coherent argumen-
In In Proceedings of COLING

tative paragraphs.
’92, volume II. pages 638–644.

Gabriel

Forgues,

Jean-Marie
Larcheveque, and Real Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.

Pineau,

Joelle

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for generation

tasks with intrinsically diverse targets. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
http://www.aclweb.org/anthology/P15-
445–450.
2073.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state track-
In Proceedings of the 15th Annual
ing challenge.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL). Association for Compu-
tational Linguistics, Philadelphia, PA, U.S.A., pages
http://www.aclweb.org/anthology/W14-
263–272.
4337.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016.

Ad-
versarial evaluation of dialogue models.
In
NIPS 2016 Workshop on Adversarial Training.
https://arxiv.org/pdf/1701.08198.pdf.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdi-
nov, Richard Zemel, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. 2015.
Skip-
thought vectors.
In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, ed-
itors, Advances in Neural Information Process-
ing Systems 28, Curran Associates,
Inc., pages
3294–3302. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1192–1202.
https://aclweb.org/anthology/D16-
1127.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. CoRR abs/1701.06547.
http://arxiv.org/abs/1701.06547.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2122–2132.
http://aclweb.org/anthology/D16-1230.

Ryan Lowe, Michael Noseworthy,

Iulian V. Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic tur-
ing test: Learning to evaluate dialogue responses.
In Proceedings of
the 5th International Confer-
ence on Learning Representations (ICLR) Work-
shop. Toulon, France.

Ryan Lowe, Nissan Pow, IV Serban, Laurent Charlin,
and Joelle Pineau. 2015. Incorporating unstructured
textual knowledge sources into neural dialogue sys-
In Neural Information Processing Systems
tems.
Workshop on Machine Learning for Spoken Lan-
guage Understanding.

Jekaterina Novikova, Oliver Lemon, and Verena
Crowd-sourcing nlg data: Pic-
Rieser. 2016.
In Proceedings of
tures elicit better data.
the 9th International Natural Language Gen-
eration conference. Association for Computa-
tional Linguistics, Edinburgh, UK, pages 265–273.
http://anthology.aclweb.org/W16-6644.

Alice H. Oh and Alexander

In Proceedings of

I. Rudnicky. 2000.
Stochastic language generation for spoken di-
the 2000
alogue systems.
ANLP/NAACL Workshop
on Conversational
Systems - Volume 3. Association for Compu-
PA, USA,
tational Linguistics,
ANLP/NAACL-ConvSyst
27–32.
pages
https://doi.org/10.3115/1117562.1117568.

Stroudsburg,
’00,

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Verena Rieser and Oliver Lemon. 2009. Natural
language generation as planning under uncertainty
In Proceedings of
for spoken dialogue systems.
the 12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 683–691.
http://www.aclweb.org/anthology/E09-1078.

Vasile Rus and Mihai C. Lintean. 2012. A compari-
son of greedy and optimal assessment of natural lan-
guage student input using word-to-word similarity
metrics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP.

Iulian Vlad Serban, Alessandro Sordoni, Ryan
Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2016.
A hi-
erarchical
latent variable encoder-decoder model
for generating dialogues. CoRR abs/1605.06069.
http://arxiv.org/abs/1605.06069.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2017. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR) Workshop.

Anthony J Viera, Joanne M Garrett, et al. 2005. Under-
standing interobserver agreement: the kappa statis-
tic. Family Medicine 37(5):360–363.

Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007.
Individual and
domain adaptation in sentence planning for
Int. Res. 30(1):413–456.
dialogue.
http://dl.acm.org/citation.cfm?id=1622637.1622648.

J. Artif.

Tsung-Hsien Wen, Milica Gaˇsi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a.
Stochastic Language Generation
in Dialogue using Recurrent Neural Networks with
Convolutional Sentence Reranking. In Proceedings
of the 16th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL). As-
sociation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In The IEEE International Con-
ference on Computer Vision (ICCV).

Relevance of Unsupervised Metrics in Task-Oriented Dialogue for
Evaluating Natural Language Generation

Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer
Microsoft Maluuba
first.last@microsoft.com

7
1
0
2
 
n
u
J
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
9
7
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Automated metrics such as BLEU are
widely used in the machine translation lit-
erature. They have also been used recently
in the dialogue community for evaluating
dialogue response generation. However,
previous work in dialogue response gener-
ation has shown that these metrics do not
correlate strongly with human judgment
in the non task-oriented dialogue setting.
Task-oriented dialogue responses are ex-
pressed on narrower domains and exhibit
lower diversity.
It is thus reasonable to
think that these automated metrics would
correlate well with human judgment in the
task-oriented setting where the generation
task consists of translating dialogue acts
into a sentence. We conduct an empirical
study to conﬁrm whether this is the case.
Our ﬁndings indicate that these automated
metrics have stronger correlation with hu-
man judgments in the task-oriented setting
compared to what has been observed in the
non task-oriented setting. We also observe
that these metrics correlate even better for
datasets which provide multiple ground
truth reference sentences. In addition, we
show that some of the currently available
corpora for task-oriented language genera-
tion can be solved with simple models and
advocate for more challenging datasets.

1

Introduction

Rule-based and template-based dialogue response
generation systems have been around for a long
time (Axelrod, 2000; Elhadad, 1992). Even today,
many task-oriented dialogue systems deployed
in production are rule-based and template-based.
These systems do not scale with increasing do-

main complexity and maintaining the increasing
number of templates becomes cumbersome.
In
the past, Oh and Rudnicky (2000) proposed a
corpus-based approach for Natural Language Gen-
eration (NLG) for task-oriented dialogue systems.
Other statistical approaches were proposed us-
ing tree-based models and reinforcement learning
(Walker et al., 2007; Rieser and Lemon, 2009).
Recently, deep-learning based approaches (Wen
et al., 2015b; Sharma et al., 2017; Lowe et al.,
2015; Serban et al., 2016) have shown promising
results for dialogue response generation.

The

automated

of machine-
evaluation
challenging and an
generated language
is
important problem for
language
the natural
processing community. The most widely used au-
tomated metrics currently are word-overlap based
metrics such as BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005) which
were proposed originally for machine translation.
While these metrics were shown to correlate
well with manual human evaluation in machine
translation tasks, previous studies showed that this
is not the case in non-task oriented dialogue (Liu
et al., 2016). This is explained by the fact that for
the same context (e.g. a user utterance), responses
in dialogue have more diversity. Word-overlap
metrics are unable to capture semantics and thus,
can lead to poor scores even for appropriate
responses. Human evaluation in this case is the
most reliable metric. However, human judgments
are expensive to obtain and not readily available
at all times.

Task-oriented dialogue systems are employed in
narrower domains (e.g. booking a restaurant) and
responses do not have as much diversity as in the
non-task oriented setting. Another important dif-
ference is that in the non-task oriented setting, re-
sponse generation is often performed end-to-end,
which means that the model takes as input the last

user utterance and potentially the dialogue history
and it outputs the next system answer. In the task-
oriented setting, on the other hand, the language
generation task is often seen as a translation step
from an abstract representation of a sentence to
the sentence itself. As a consequence, automated
metrics which compare a generated sentence to a
reference sentence might be more appropriate and
In this paper,
correlate with human judgments.
we:

• study the correlation between human judg-
ments and several unsupervised automated
metrics on two popular task-oriented dia-
logue datasets,

• introduce variants of existing models and
evaluate their performance on these metrics

We ﬁnd that the automated metrics have stronger
correlation with human judgments in the task-
oriented setting than what has been observed in
the non task-oriented setting. We also observe that
these metrics correlate even more in the presence
of multiple reference sentences.

2 Related Work

Liu et al. (2016) did an empirical study to eval-
uate the correlation between human scores and
several automated word-overlap metrics as well
as embedding-based metrics for dialogue response
generation. They observed that these metrics,
though widely used in the literature, had only
weak correlation with human judgments in the non
task-oriented dialogue NLG setting.

In terms of supervised NLG evaluation metrics,
Lowe et al. (2017) proposed the ADEM model
which trains a hierarchical recurrent neural net-
work in a supervised manner to predict human-like
scores. This learned score was shown to corre-
late better with human judgments than any other
automated metric. However, the drawback of this
approach is the requirement for expensive human
ratings.

Li et al. (2016) proposed to use reinforcement
learning to train an end-to-end dialogue system.
They simulate a dialogue between two agents and
use a policy gradient algorithm with a reward
function which evaluates speciﬁc properties of the
responses generated by the dialogue system.

In the adversarial setting, Kannan and Vinyals
(2016) train a recurrent neural network discrimi-
nator to differentiate human-generated responses

from model-generated responses. However, an ex-
tensive analysis of the viability and the ease of
standardization of this approach is yet to be con-
ducted. Li et al. (2017), apart from adversari-
ally training dialogue response models, propose an
independent adversarial evaluation metric Adver-
Suc and a measure of the model’s reliability called
evaluator reliability error. Drawbacks of these ap-
proaches are that they are model-dependent. Ad-
versarial methods might be promising for task-
oriented dialogue systems but more research needs
to be conducted on their account.

Most of the work described so far has been
done in the non task-oriented dialogue setting as
there has been prior work indicating that auto-
mated metrics do not correlate well with humans
in that setting. There has not yet been any empiri-
cal validation that these conclusions also apply to
the task oriented setting. Research in the task ori-
ented setting has mostly made use of automated
metrics such as BLEU and human evaluation (Wen
et al., 2015b; Sharma et al., 2017; Duˇsek and Jur-
cicek, 2016).

3 Metrics

This section describes the set of automatic met-
rics whose correlation with human evaluation is
studied. We consider ﬁrst word-overlap metrics
and then embedding-based metrics.
In all that
follows, when multiple references are provided,
we compute the similarity between the prediction
and all the references one-by-one, and then select
the maximum value. We then average the scores
across the entire corpus.

3.1 Word-overlap based metrics

3.1.1 BLEU
The BLEU metric (Papineni et al., 2002) compares
n-grams between the candidate utterance and the
reference utterance. The BLEU score is com-
puted at the corpus-level and relies on the follow-
ing modiﬁed precision:

pn =

(cid:88)

(cid:88)

Ctclip(n − gram)

C∈{Candidates}
(cid:88)

n−gram∈C
(cid:88)

C(cid:48)∈{Candidates(cid:48)}

n−gram(cid:48)∈C(cid:48)

Ctclip(n − gram(cid:48))

(1)

where {Candidates} are the candidate answers
generated by the model and Ctclip is the clipped

count for the n-gram which is the number of times
the n-gram is common to the candidate answer
and the reference answer clipped by the maximum
number of occurrences of the n-gram in the refer-
ence answer. The BLEU-N score is deﬁned as:

BLEU-N = BP exp(

ωn log(pn))

(2)

N
(cid:88)

n

where N is the maximum length of the n-grams
(in this paper, we compute BLEU-1 to BLEU-4),
ω is a weighting that is often uniform and BP is a
brevity penalty. In this paper we report the BLEU
score at the corpus level but we also compute this
score at the sentence level to analyze its correla-
tion with human evaluation.

3.1.2 METEOR
The METEOR metric (Banerjee and Lavie, 2005)
was proposed as a metric which correlates better
at the sentence level with human evaluation. To
compute the METEOR score, ﬁrst, an alignment
between the candidate and the reference sentences
is created by mapping each unigram in the can-
didate sentence to 0 or 1 unigram in the reference
sentence. The alignment is not only based on exact
matches but also stem, synonym, and paraphrase
matches. Based on this alignment, unigram pre-
cision and recall are computed and the METEOR
score is:

METEOR = Fmean(1 − p)

where Fmean is the harmonic mean between preci-
sion and recall with the weight for recall 9 times a
high as the weight for precision, and p is a penalty.

3.1.3 ROUGE
ROUGE (Lin, 2004) is a set of metrics that was
ﬁrst introduced for summarization. We compute
ROUGE-L which is an F-measure based on the
Longest Common Subsequence (LCS) between
the candidate and reference utterances.

3.2 Embedding based metrics

We consider another set of metrics which compute
the cosine similarity between the embeddings of
the predicted and the reference sentence instead of
relying on word overlaps.

3.2.1 Skip-Thought
The Skip-Thought model (Kiros et al., 2015) is
trained in an unsupervised fashion and uses a re-
current network to encode a given sentence into

an embedding and then decode it to predict the
preceding and following sentences. The model
was trained on the BookCorpus dataset (Zhu et al.,
2015). The embeddings produced by the encoder
have a robust performance on semantic relatedness
tasks. We use the pre-trained Skip-Thought en-
coder provided by the authors1.

We also compute other embedding-based meth-
ods which have been used as evaluation metrics
for measuring human correlation in recent litera-
ture (Liu et al., 2016) for non task-oriented dia-
logue in Sections 3.2.2, 3.2.3, and 3.2.4.

3.2.2 Embedding average
This metric computes a sentence-level embedding
by averaging the embeddings of the words com-
posing this sentence:

¯eC =

(cid:80)
| (cid:80)

w∈C ew
w(cid:48)∈C ew(cid:48)|

.

In this equation, the vectors ew are embeddings for
the words w in the candidate sentence C.

3.2.3 Vector extrema
Vector extrema (Forgues et al., 2014) computes a
sentence-level embedding by taking the most ex-
treme value of the embeddings of the words com-
posing the sentence for each dimension of the em-
bedding:

(cid:40)

(3)

erd =

maxw∈C ewd
minw∈C ewd

if ewd > | minw(cid:48)∈C ew(cid:48)d|
otherwise.

In this equation, d is an index over the dimensions
of the embedding and C is the candidate sentence.

3.2.4 Greedy matching
Greedy matching does not compute a sentence em-
bedding but directly a similarity score between a
candidate C and a reference r (Rus and Lintean,
2012). This similarity score is computed as fol-
lows:

G(C, r) =

(cid:80) w ∈ C max ˆw∈r cos sim(ew, w ˆw)
|C|

GM (C, r) =

G(C, r) + G(r, C)
2

.

(4)

In other words, each word in the candidate sen-
tence is greedily matched to a word in the ref-
erence sentence based on the cosine similarity of

1https://github.com/ryankiros/skip-thoughts

their embeddings. The score is an average of these
similarities over the number of words in the can-
didate sentence. The same score is computed by
reversing the roles of the candidate and reference
sentences and the average of the two scores gives
the ﬁnal similarity score.

4 Response Generation Models

This section presents the different natural lan-
guage generation models that we use in this study.
All of these models take as input a set of dialogue
acts (Austin, 1962) potentially with slot types and
slot values and translate this input into an utter-
ance. An example input is inform(food =
Chinese) and a corresponding output would be
“I am looking for a Chinese restaurant.”. In this
example, the dialogue act is inform, the slot type
is food, and the slot value is Chinese.

4.1 Random

Given a dialogue act with one or more slot types,
the random model ﬁnds all the examples in the
training set with the same dialogue act and slots
(while ignoring slot values) and it randomly se-
lects its output from this set of reference sen-
tences. The datasets that we experiment on have
some special slot values such as “yes”, “no”, and
“don’t care”. Since the model ignores all slot val-
ues, these special cases are not properly handled,
which results in slightly lower performance than
what we could get by spending more time hand-
engineering the model’s behavior for these values.

4.2 LSTM

This model consists of a recurrent LSTM (Hochre-
iter and Schmidhuber, 1997) decoder. The dia-
logue acts and slot types are ﬁrst encoded as a bi-
nary vector whose length is the number of possi-
ble combinations of dialogue acts and slot types
in the dataset. We refer to this binary vector as
the Dialogue Act (DA) vector. The DA vector
for a given set of dialogue acts is a binary vec-
tor over the fused dialogue act-slot types, e.g.,
INFORM-FOOD, INFORM-COUNT, etc.

This binary vector is given as input to the de-
coder at each time-step of the LSTM. The decoder
then outputs a delexicalized sentence. A delexi-
calized sentence contains placeholders for the slot
values. An example is “I am looking for a FOOD
restaurant.”. The values for the delexicalized slots
(the type of food in this example) are then directly

copied from the input.

4.3 delex-sc-LSTM

This model uses the same architecture as the
LSTM model presented in the previous section ex-
cept that it uses sc-LSTM (Wen et al., 2015b) units
in the decoder instead of LSTM units. We call this
model the “delex-sc-LSTM”2. As in the previous
model, the input DA vector only encodes acts and
delexicalized slots. It does not contain any infor-
mation about the slot value.

By providing this model the same DA vector
input as the one given to the LSTM model, we can
directly study if the additional complexity of the
sc-LSTM unit’s reading gate provides signiﬁcant
improvement over the small-sized task-oriented
dialogue datasets which are currently available.

4.4 hierarchical-lex-delex-sc-LSTM

is a variant of the “ld-sc-LSTM”
This model
model proposed by Sharma et al. (2017) which is
based on an encoder-decoder framework. We call
our model “hierarchical-lex-delex-sc-LSTM”3.

Figure 1: Encoder of the hld-scLSTM model

Figure 2: Decoder of the hld-scLSTM model

We present the encoder in Figure 1. The en-
coder consists of a hierarchical LSTM with Ne
time-steps, where Ne is the number of non-zero
entries in the DA vector. Each time-step of the en-
coder encodes one dialogue act’s delexicalized and
lexicalized slot-value pair (e.g. (INFORM-FOOD,

2We will also refer to it as “d-scLSTM”.
3We will also refer to it as“hld-scLSTM”.

Gold
Random
LSTM

B-1
1.00
0.875
0.900
d-scLSTM 0.880
hld-scLSTM 0.909

B-2
1.00
0.843
0.879
0.850
0.890

DSTC2
B-3
1.00
0.822
0.863
0.828
0.878

B-4
1.00
0.807
0.851
0.812
0.870

M
1.00
0.564
0.610
0.578
0.624

R L
1.00
0.852
0.888
0.874
0.899

B-1
1.00
0.872
0.982
0.980
0.985

B-2
1.00
0.813
0.966
0.964
0.978

Restaurants
B-4
B-3
1.00
1.00
0.721
0.765
0.932
0.949
0.931
0.948
0.962
0.970

M
1.00
0.504
0.652
0.654
0.704

R L
1.00
0.796
0.944
0.945
0.965

Table 1: Performance comparison across models on word-overlap based automated metrics

Skip
Thought
1.00
0.906
0.946
0.925
0.932

DSTC2
Embedding
Average
1.00
0.981
0.985
0.984
0.987

Restaurants

Greedy
Vector
Extrema Matching

1.00
0.910
0.935
0.926
0.942

1.00
0.947
0.962
0.957
0.964

Skip
Thought
1.00
0.843
0.945
0.948
0.968

Embedding
Average
1.00
0.957
0.997
0.997
0.997

Greedy
Vector
Extrema Matching

1.00
0.905
0.986
0.986
0.989

1.00
0.930
0.991
0.991
0.993

Gold
Random
LSTM
d-scLSTM
hld-scLSTM

Table 2: Performance comparison across models on sentence-embedding based automated metrics

‘Chinese’)). The delexicalized act-slot part is en-
coded as a one-hot vector which we refer to as
DAt. DAt is constructed by masking all except
the tth dialogue act in the DA vector4. The lexi-
calized value part is encoded by an LSTM encoder
which shares parameters across all time-steps and
operates over the word-embeddings of the lexical-
ized values vt,i. Our model differs from the “ld-sc-
LSTM” model in that we use an LSTM encoder
over the word-embeddings instead of computing
the mean of the word-embeddings. The ﬁnal hid-
den state of this LSTM is concatenated with DAt
and is given as input to the upper LSTM (see Fig-
ure 1). The ﬁnal hidden state of the upper LSTM
is then provided to the decoder as input. This is
another difference from the “ld-sc-LSTM” as that
uses the mean of all the hidden states of the en-
coder instead, which, in our experiments, did not
perform as well as using just the ﬁnal hidden state
E.

The decoder is described in Figure 2. It is the
same as in the “ld-sc-LSTM” model. At each
time-step, it takes as input the encoder output E,
the DA vector, and the word-embedding of the
word generated at the previous time-step. The
DA vector is also additionally provided to the sc-
LSTM cell in order for it to be regulated by its
reading gate as described in Wen et al. (2015b).

5 Experiments

5.1 Decoding

During training, at each time-step, we use the
ground truth word from the previous time-step.

4also referred to as DAt=1:Ne

The model thus learns to generate the next word
given the previous one. On the other hand, to
generate sentences during test time, we use beam
search. The ﬁrst word input to the generator is a
special token < bos > which indicates the begin-
ning of the sequence. Decoding is stopped if we
reach a speciﬁed maximum number of time-steps
or if the model outputs a special token < eos >
which indicates the end of the sequence. We also
use a slot error rate penalty, similarly to Wen et al.
(2015b), to re-rank the sentences generated with
beam search. We use this method for all three of
the LSTM, d-scLSTM, and hld-scLSTM models
for fairness.

Similarly to the LSTM model, the d-scLSTM
and hld-scLSTM generate delexicalized sen-
tences, i.e., they generate slot tokens instead of
slot values directly. These slot tokens are replaced
with slot values in a post-processing step which
is a fairly common step in task-oriented dialogue
NLG literature.

5.2 Evaluation

In NLG tasks, improvements in automated met-
ric scores are most commonly used to demonstrate
improvement in the generation task. However,
these metrics have been shown to only weakly
correlate with human evaluation in the non task-
oriented dialogue setting (Liu et al., 2016) and
hence are not considered reliable measures of im-
provement. Human evaluation is considered the
metric of choice, but human ratings are expensive
to obtain. The ease of computing these automated
metrics and their availability for rapid prototyping
has lead to their widespread adoption.

Metric
Bleu 1
Bleu 2
Bleu 3
Bleu 4
METEOR
ROUGE L
Skip Thought
Embedding Average
Vector Extrema
Greedy Matching
Human

Spearman
-0.317
-0.318
-0.318
-0.318
0.295
0.294
0.528
0.295
0.299
0.295
0.810

DSTC2
p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.583
0.526
0.500
0.461
0.582
0.448
0.086
0.485
0.624
0.572
0.984

p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
0.397
<0.005
<0.005
<0.005
<0.005

Restaurants

Spearman
0.069
0.091
0.109
0.105
0.353
0.346
0.284
0.423
0.446
0.446
0.653

p-value
0.494
0.366
0.280
0.296
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.277
0.166
0.223
0.255
0.489
0.382
0.364
0.260
0.287
0.325
0.857

p-value
0.005
0.099
0.026
0.010
<0.005
<0.005
<0.005
0.009
<0.005
<0.005
<0.005

Table 3: Correlation of automated metrics with human evaluations scores

We evaluate the models described in the pre-
vious section on the DSTC2 (Henderson et al.,
2014) and the Restaurants datasets (Wen et al.,
2015a) using these automated metrics. These
datasets are some of the only available resources
for studying NLG for task-oriented dialogue. The
DSTC2 dataset contains dialogues between human
users and a dialogue system in a restaurant do-
main. The dataset is annotated with dialogue acts,
slot type, and slot values. The NLG component
of the dialogue system used for data collection is
templated. The Restaurants dataset was speciﬁ-
cally proposed for NLG and provides, for a set of
dialogue acts with slot types and slot values, two
sentences generated by humans.

We present the results of our experiments in Ta-
ble 1 and Table 2. The code for our automated
metric evaluation pipeline is available at https:
//github.com/Maluuba/nlg-eval. The
scores of all the models on these automated met-
rics are very high. This indicates that there is sig-
niﬁcant word overlap between the generated and
the reference sentences and that the NLG task on
these datasets can be solved with a simple model
such as the LSTM model. In effect, table 1 shows
that the LSTM model performs comparably to the
d-scLSTM model based on the word-overlap met-
rics. This can be explained by the fact that the
d-scLSTM model has more parameters and might
suffer from overﬁtting issues on these relatively
small datasets.

The hld-scLSTM is considered to consistently
outperform the other models based on the word-
overlap metrics. As explained by Sharma et al.
(2017), this improvement results from the model’s
access to the lexicalized slot values, due to which
it can take into account the grammatical associa-
tions of the generated words near the output to-
kens, thereby generating higher quality sentences.

κ
>0.1
>0.2
>0.3
>0.4
>0.5
>0.6

# pairs % pairs
100.0 %
55/55
72.7 %
40/55
50.9 %
28/55
34.5 %
19/55
14.5 %
8/55
0.0 %
0/55

Table 4: Pairwise Cohen’s kappa scores for the 11
human users

However, Table 2 shows that sentence-embedding
based metrics judge all the models except the ran-
dom one to perform quite similarly with again,
very high performance scores.

In the next section, we add human evaluation

for these models on these datasets.

5.3 Human rating collection

We randomly selected 20 dialogue acts from the
test set of each dataset. For each of these contexts,
we presented 5 sentences to the evaluators:
the
gold response provided in the test set and the re-
sponses generated by the four models described in
Section 4. These sentences were randomly shuf-
ﬂed and not presented in the same order. We in-
vited 18 human users to score each of these 100
sentences on a Likert-type scale of 1 to 5. The
users were asked to rate the responses depending
on how appropriate they were for the speciﬁed di-
alogue acts. A score of 1 was the lowest score,
meaning that the response was not appropriate at
all whereas a score of 5 meant that the sentence
was highly appropriate.

We computed Cohen’s kappa scores (Cohen,
1960) between the human users in pairs of two.
We removed 7 users who had kappa scores less
than 0.1 and used the remaining 11 users for the
correlation study. The kappa scores are presented
in Table 4. Most of the user pairs have a Cohen’s
κ > 0.3 which indicates fair agreement between

(a) DSTC2

(b) Restaurants

Figure 3: Scatter plots for correlation of some automated metrics with human evaluation for (a) the
DSTC2 dataset, and (b) the Restaurants dataset. Random gaussian noise N (0, 0.1) has been added to
data points along the human score axis and N (0, 0.02) has been added to the automated metric score’s
axis to aid visualization of overlapping data points. Transparency has been added for the same effect.

users (Viera et al., 2005).

5.4 Correlation between automated metrics

and human scores

We present the correlation between the automated
metrics and our collected human ratings in Ta-
ble 3. We measure human v.s. human correla-
tion by randomly splitting the human users into
two groups. The results indicate that in most
cases, human scores correlate the best with other
human scores. Except in the case of the Spear-
man correlation for BLEU-N scores, we can see
that there is a positive correlation between the au-
tomated metrics and the human scores for these
task-oriented datasets, which contrasts with the
non task-oriented dialogue setting where Liu et al.
(2016) observed no strong correlation trends.

A likely explanation for the negative Spearman
correlation values for BLEU-N is that there is
only one gold reference per context in the DSTC2
dataset. The Restaurants dataset, on the other
hand, provides two gold references per context.
Having multiple gold references increases the like-
lihood that the generated response will have sig-
niﬁcant word-overlap with one of the reference re-
sponses.

We present scatter plots for some of the metrics
presented in Table 3 in Figure 3. We observe that
all the metrics correlate very well with humans
on high scoring examples. As it can be seen in
the scatter plots, most of the sentences are given
the maximal score of 5 by the human evaluators.
This conﬁrms our previous observation that the
available corpora for task-oriented dialogue NLG
task are not very challenging and a simple LSTM-
based model can output high-quality responses.

Overall, among the word overlap based auto-
mated metrics, METEOR consistently correlates
with human evaluation on both datasets. These
results conﬁrm the original ﬁndings by Banerjee
and Lavie (2005) who showed that METEOR had
good correlation with human evaluation in the ma-
chine translation task. The comparison with ma-
chine translation is highly relevant in the task-
oriented setting because the NLG model essen-
tially learns to translate the abstract representation
of a sentence into a sentence. It is a translation task
contrary to the non task-oriented setting where the
NLG model needs to decide and output a new sen-
tence based on the last sentence typed by a user
and dialogue history. Therefore, automated met-
rics coming from the machine translation literature

are more adequate in our case than in the non-task
oriented case as shown by Liu et al. (2016).

It is interesting to see that METEOR correlates
well with human evaluation consistently. This
can be explained by the fact that even though
METEOR does not rely on word embeddings, it
includes notions of synonymy and paraphrasing
when computing the alignment between the can-
didate and reference utterances.

6 Discussion

We evaluated several natural language generation
models trained on the DSTC2 and the Restaurants
datasets based on several automated metrics. We
also performed human evaluation on the model-
generated responses and our study shows that hu-
man evaluation is a much more reliable metric
compared to the others. Among the word-overlap
based automated metrics, we found that the ME-
TEOR score correlates the most with human judg-
ments and we suggest using METEOR for task-
oriented dialogue natural language generation in-
stead of BLEU. We also observe that these met-
rics are more reliable in the task-oriented dialogue
setting compared to the general, non task-oriented
one due to the limited possible diversity in the
task-oriented setting. Also, as observed by Gal-
ley et al. (2015), we can see that word-overlap
based metrics correlate better with human evalu-
ation when multiple references are provided, as in
the Restaurants dataset. Otherwise, as in the case
of DSTC2 which only provides one reference sen-
tence per example, we observe that all the BLEU-
N metrics negatively correlate with human evalu-
ation on Spearman correlation.

As has been observed in the machine translation
literature, using beam search improves the quality
of generated sentences signiﬁcantly compared to
stochastic sampling. For similar models, our re-
sults show improvement in the automated metrics’
scores compared to Wen et al. (2015b) who used
stochastic sampling for decoding instead of beam
search.

Wen et al. (2015b) did not use the slot er-
ror rate penalty with the vanilla LSTM model
in their experiments. After adding the penalty
in our case, we observe that the vanilla LSTM-
based model performs as well as the delexicalized
semantically-controlled LSTM model. This sug-
gests that the added complexity introduced by the
sc-LSTM unit does not offer a signiﬁcant advan-

tage for these two datasets.

High performance on automated metrics,
achieved by our models on the DSTC2 and the
Restaurants datasets lead us to conclude that these
datasets are not very challenging for the NLG task.
The task-oriented dialogue community should
move towards using larger and more complex
datasets, which have been recently announced,
such as the Frames dataset (El Asri et al., 2017) or
the E2E NLG Challenge dataset (Novikova et al.,
2016).

References

John Langshaw Austin. 1962. How to do things with

words. Oxford University Press.

information system.

Scott Axelrod. 2000. Natural language generation
In Pro-
in the ibm ﬂight
ceedings of the 2000 ANLP/NAACL Workshop on
Conversational Systems - Volume 3. Association
for Computational Linguistics, Stroudsburg, PA,
USA, ANLP/NAACL-ConvSyst ’00, pages 21–26.
https://doi.org/10.3115/1117562.1117567.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proc. ACL
correlation with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and psychological

nominal scales.
measurement 20(1):37–46.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-
to-sequence generation for spoken dialogue via
In Proceed-
deep syntax trees and strings.
ings of
the As-
the 54th Annual Meeting of
sociation for Computational Linguistics (Volume
2:
Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057 .

Michael Elhadad. 1992. Generating coherent argumen-
In In Proceedings of COLING

tative paragraphs.
’92, volume II. pages 638–644.

Gabriel

Forgues,

Jean-Marie
Larcheveque, and Real Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.

Pineau,

Joelle

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for generation

tasks with intrinsically diverse targets. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
http://www.aclweb.org/anthology/P15-
445–450.
2073.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state track-
In Proceedings of the 15th Annual
ing challenge.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL). Association for Compu-
tational Linguistics, Philadelphia, PA, U.S.A., pages
http://www.aclweb.org/anthology/W14-
263–272.
4337.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016.

Ad-
versarial evaluation of dialogue models.
In
NIPS 2016 Workshop on Adversarial Training.
https://arxiv.org/pdf/1701.08198.pdf.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdi-
nov, Richard Zemel, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. 2015.
Skip-
thought vectors.
In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, ed-
itors, Advances in Neural Information Process-
ing Systems 28, Curran Associates,
Inc., pages
3294–3302. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1192–1202.
https://aclweb.org/anthology/D16-
1127.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. CoRR abs/1701.06547.
http://arxiv.org/abs/1701.06547.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2122–2132.
http://aclweb.org/anthology/D16-1230.

Ryan Lowe, Michael Noseworthy,

Iulian V. Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic tur-
ing test: Learning to evaluate dialogue responses.
In Proceedings of
the 5th International Confer-
ence on Learning Representations (ICLR) Work-
shop. Toulon, France.

Ryan Lowe, Nissan Pow, IV Serban, Laurent Charlin,
and Joelle Pineau. 2015. Incorporating unstructured
textual knowledge sources into neural dialogue sys-
In Neural Information Processing Systems
tems.
Workshop on Machine Learning for Spoken Lan-
guage Understanding.

Jekaterina Novikova, Oliver Lemon, and Verena
Crowd-sourcing nlg data: Pic-
Rieser. 2016.
In Proceedings of
tures elicit better data.
the 9th International Natural Language Gen-
eration conference. Association for Computa-
tional Linguistics, Edinburgh, UK, pages 265–273.
http://anthology.aclweb.org/W16-6644.

Alice H. Oh and Alexander

In Proceedings of

I. Rudnicky. 2000.
Stochastic language generation for spoken di-
the 2000
alogue systems.
ANLP/NAACL Workshop
on Conversational
Systems - Volume 3. Association for Compu-
PA, USA,
tational Linguistics,
ANLP/NAACL-ConvSyst
27–32.
pages
https://doi.org/10.3115/1117562.1117568.

Stroudsburg,
’00,

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Verena Rieser and Oliver Lemon. 2009. Natural
language generation as planning under uncertainty
In Proceedings of
for spoken dialogue systems.
the 12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 683–691.
http://www.aclweb.org/anthology/E09-1078.

Vasile Rus and Mihai C. Lintean. 2012. A compari-
son of greedy and optimal assessment of natural lan-
guage student input using word-to-word similarity
metrics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP.

Iulian Vlad Serban, Alessandro Sordoni, Ryan
Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2016.
A hi-
erarchical
latent variable encoder-decoder model
for generating dialogues. CoRR abs/1605.06069.
http://arxiv.org/abs/1605.06069.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2017. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR) Workshop.

Anthony J Viera, Joanne M Garrett, et al. 2005. Under-
standing interobserver agreement: the kappa statis-
tic. Family Medicine 37(5):360–363.

Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007.
Individual and
domain adaptation in sentence planning for
Int. Res. 30(1):413–456.
dialogue.
http://dl.acm.org/citation.cfm?id=1622637.1622648.

J. Artif.

Tsung-Hsien Wen, Milica Gaˇsi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a.
Stochastic Language Generation
in Dialogue using Recurrent Neural Networks with
Convolutional Sentence Reranking. In Proceedings
of the 16th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL). As-
sociation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In The IEEE International Con-
ference on Computer Vision (ICCV).

Relevance of Unsupervised Metrics in Task-Oriented Dialogue for
Evaluating Natural Language Generation

Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer
Microsoft Maluuba
first.last@microsoft.com

7
1
0
2
 
n
u
J
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
9
7
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Automated metrics such as BLEU are
widely used in the machine translation lit-
erature. They have also been used recently
in the dialogue community for evaluating
dialogue response generation. However,
previous work in dialogue response gener-
ation has shown that these metrics do not
correlate strongly with human judgment
in the non task-oriented dialogue setting.
Task-oriented dialogue responses are ex-
pressed on narrower domains and exhibit
lower diversity.
It is thus reasonable to
think that these automated metrics would
correlate well with human judgment in the
task-oriented setting where the generation
task consists of translating dialogue acts
into a sentence. We conduct an empirical
study to conﬁrm whether this is the case.
Our ﬁndings indicate that these automated
metrics have stronger correlation with hu-
man judgments in the task-oriented setting
compared to what has been observed in the
non task-oriented setting. We also observe
that these metrics correlate even better for
datasets which provide multiple ground
truth reference sentences. In addition, we
show that some of the currently available
corpora for task-oriented language genera-
tion can be solved with simple models and
advocate for more challenging datasets.

1

Introduction

Rule-based and template-based dialogue response
generation systems have been around for a long
time (Axelrod, 2000; Elhadad, 1992). Even today,
many task-oriented dialogue systems deployed
in production are rule-based and template-based.
These systems do not scale with increasing do-

main complexity and maintaining the increasing
number of templates becomes cumbersome.
In
the past, Oh and Rudnicky (2000) proposed a
corpus-based approach for Natural Language Gen-
eration (NLG) for task-oriented dialogue systems.
Other statistical approaches were proposed us-
ing tree-based models and reinforcement learning
(Walker et al., 2007; Rieser and Lemon, 2009).
Recently, deep-learning based approaches (Wen
et al., 2015b; Sharma et al., 2017; Lowe et al.,
2015; Serban et al., 2016) have shown promising
results for dialogue response generation.

The

automated

of machine-
evaluation
challenging and an
generated language
is
important problem for
language
the natural
processing community. The most widely used au-
tomated metrics currently are word-overlap based
metrics such as BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005) which
were proposed originally for machine translation.
While these metrics were shown to correlate
well with manual human evaluation in machine
translation tasks, previous studies showed that this
is not the case in non-task oriented dialogue (Liu
et al., 2016). This is explained by the fact that for
the same context (e.g. a user utterance), responses
in dialogue have more diversity. Word-overlap
metrics are unable to capture semantics and thus,
can lead to poor scores even for appropriate
responses. Human evaluation in this case is the
most reliable metric. However, human judgments
are expensive to obtain and not readily available
at all times.

Task-oriented dialogue systems are employed in
narrower domains (e.g. booking a restaurant) and
responses do not have as much diversity as in the
non-task oriented setting. Another important dif-
ference is that in the non-task oriented setting, re-
sponse generation is often performed end-to-end,
which means that the model takes as input the last

user utterance and potentially the dialogue history
and it outputs the next system answer. In the task-
oriented setting, on the other hand, the language
generation task is often seen as a translation step
from an abstract representation of a sentence to
the sentence itself. As a consequence, automated
metrics which compare a generated sentence to a
reference sentence might be more appropriate and
In this paper,
correlate with human judgments.
we:

• study the correlation between human judg-
ments and several unsupervised automated
metrics on two popular task-oriented dia-
logue datasets,

• introduce variants of existing models and
evaluate their performance on these metrics

We ﬁnd that the automated metrics have stronger
correlation with human judgments in the task-
oriented setting than what has been observed in
the non task-oriented setting. We also observe that
these metrics correlate even more in the presence
of multiple reference sentences.

2 Related Work

Liu et al. (2016) did an empirical study to eval-
uate the correlation between human scores and
several automated word-overlap metrics as well
as embedding-based metrics for dialogue response
generation. They observed that these metrics,
though widely used in the literature, had only
weak correlation with human judgments in the non
task-oriented dialogue NLG setting.

In terms of supervised NLG evaluation metrics,
Lowe et al. (2017) proposed the ADEM model
which trains a hierarchical recurrent neural net-
work in a supervised manner to predict human-like
scores. This learned score was shown to corre-
late better with human judgments than any other
automated metric. However, the drawback of this
approach is the requirement for expensive human
ratings.

Li et al. (2016) proposed to use reinforcement
learning to train an end-to-end dialogue system.
They simulate a dialogue between two agents and
use a policy gradient algorithm with a reward
function which evaluates speciﬁc properties of the
responses generated by the dialogue system.

In the adversarial setting, Kannan and Vinyals
(2016) train a recurrent neural network discrimi-
nator to differentiate human-generated responses

from model-generated responses. However, an ex-
tensive analysis of the viability and the ease of
standardization of this approach is yet to be con-
ducted. Li et al. (2017), apart from adversari-
ally training dialogue response models, propose an
independent adversarial evaluation metric Adver-
Suc and a measure of the model’s reliability called
evaluator reliability error. Drawbacks of these ap-
proaches are that they are model-dependent. Ad-
versarial methods might be promising for task-
oriented dialogue systems but more research needs
to be conducted on their account.

Most of the work described so far has been
done in the non task-oriented dialogue setting as
there has been prior work indicating that auto-
mated metrics do not correlate well with humans
in that setting. There has not yet been any empiri-
cal validation that these conclusions also apply to
the task oriented setting. Research in the task ori-
ented setting has mostly made use of automated
metrics such as BLEU and human evaluation (Wen
et al., 2015b; Sharma et al., 2017; Duˇsek and Jur-
cicek, 2016).

3 Metrics

This section describes the set of automatic met-
rics whose correlation with human evaluation is
studied. We consider ﬁrst word-overlap metrics
and then embedding-based metrics.
In all that
follows, when multiple references are provided,
we compute the similarity between the prediction
and all the references one-by-one, and then select
the maximum value. We then average the scores
across the entire corpus.

3.1 Word-overlap based metrics

3.1.1 BLEU
The BLEU metric (Papineni et al., 2002) compares
n-grams between the candidate utterance and the
reference utterance. The BLEU score is com-
puted at the corpus-level and relies on the follow-
ing modiﬁed precision:

pn =

(cid:88)

(cid:88)

Ctclip(n − gram)

C∈{Candidates}
(cid:88)

n−gram∈C
(cid:88)

C(cid:48)∈{Candidates(cid:48)}

n−gram(cid:48)∈C(cid:48)

Ctclip(n − gram(cid:48))

(1)

where {Candidates} are the candidate answers
generated by the model and Ctclip is the clipped

count for the n-gram which is the number of times
the n-gram is common to the candidate answer
and the reference answer clipped by the maximum
number of occurrences of the n-gram in the refer-
ence answer. The BLEU-N score is deﬁned as:

BLEU-N = BP exp(

ωn log(pn))

(2)

N
(cid:88)

n

where N is the maximum length of the n-grams
(in this paper, we compute BLEU-1 to BLEU-4),
ω is a weighting that is often uniform and BP is a
brevity penalty. In this paper we report the BLEU
score at the corpus level but we also compute this
score at the sentence level to analyze its correla-
tion with human evaluation.

3.1.2 METEOR
The METEOR metric (Banerjee and Lavie, 2005)
was proposed as a metric which correlates better
at the sentence level with human evaluation. To
compute the METEOR score, ﬁrst, an alignment
between the candidate and the reference sentences
is created by mapping each unigram in the can-
didate sentence to 0 or 1 unigram in the reference
sentence. The alignment is not only based on exact
matches but also stem, synonym, and paraphrase
matches. Based on this alignment, unigram pre-
cision and recall are computed and the METEOR
score is:

METEOR = Fmean(1 − p)

where Fmean is the harmonic mean between preci-
sion and recall with the weight for recall 9 times a
high as the weight for precision, and p is a penalty.

3.1.3 ROUGE
ROUGE (Lin, 2004) is a set of metrics that was
ﬁrst introduced for summarization. We compute
ROUGE-L which is an F-measure based on the
Longest Common Subsequence (LCS) between
the candidate and reference utterances.

3.2 Embedding based metrics

We consider another set of metrics which compute
the cosine similarity between the embeddings of
the predicted and the reference sentence instead of
relying on word overlaps.

3.2.1 Skip-Thought
The Skip-Thought model (Kiros et al., 2015) is
trained in an unsupervised fashion and uses a re-
current network to encode a given sentence into

an embedding and then decode it to predict the
preceding and following sentences. The model
was trained on the BookCorpus dataset (Zhu et al.,
2015). The embeddings produced by the encoder
have a robust performance on semantic relatedness
tasks. We use the pre-trained Skip-Thought en-
coder provided by the authors1.

We also compute other embedding-based meth-
ods which have been used as evaluation metrics
for measuring human correlation in recent litera-
ture (Liu et al., 2016) for non task-oriented dia-
logue in Sections 3.2.2, 3.2.3, and 3.2.4.

3.2.2 Embedding average
This metric computes a sentence-level embedding
by averaging the embeddings of the words com-
posing this sentence:

¯eC =

(cid:80)
| (cid:80)

w∈C ew
w(cid:48)∈C ew(cid:48)|

.

In this equation, the vectors ew are embeddings for
the words w in the candidate sentence C.

3.2.3 Vector extrema
Vector extrema (Forgues et al., 2014) computes a
sentence-level embedding by taking the most ex-
treme value of the embeddings of the words com-
posing the sentence for each dimension of the em-
bedding:

(cid:40)

(3)

erd =

maxw∈C ewd
minw∈C ewd

if ewd > | minw(cid:48)∈C ew(cid:48)d|
otherwise.

In this equation, d is an index over the dimensions
of the embedding and C is the candidate sentence.

3.2.4 Greedy matching
Greedy matching does not compute a sentence em-
bedding but directly a similarity score between a
candidate C and a reference r (Rus and Lintean,
2012). This similarity score is computed as fol-
lows:

G(C, r) =

(cid:80) w ∈ C max ˆw∈r cos sim(ew, w ˆw)
|C|

GM (C, r) =

G(C, r) + G(r, C)
2

.

(4)

In other words, each word in the candidate sen-
tence is greedily matched to a word in the ref-
erence sentence based on the cosine similarity of

1https://github.com/ryankiros/skip-thoughts

their embeddings. The score is an average of these
similarities over the number of words in the can-
didate sentence. The same score is computed by
reversing the roles of the candidate and reference
sentences and the average of the two scores gives
the ﬁnal similarity score.

4 Response Generation Models

This section presents the different natural lan-
guage generation models that we use in this study.
All of these models take as input a set of dialogue
acts (Austin, 1962) potentially with slot types and
slot values and translate this input into an utter-
ance. An example input is inform(food =
Chinese) and a corresponding output would be
“I am looking for a Chinese restaurant.”. In this
example, the dialogue act is inform, the slot type
is food, and the slot value is Chinese.

4.1 Random

Given a dialogue act with one or more slot types,
the random model ﬁnds all the examples in the
training set with the same dialogue act and slots
(while ignoring slot values) and it randomly se-
lects its output from this set of reference sen-
tences. The datasets that we experiment on have
some special slot values such as “yes”, “no”, and
“don’t care”. Since the model ignores all slot val-
ues, these special cases are not properly handled,
which results in slightly lower performance than
what we could get by spending more time hand-
engineering the model’s behavior for these values.

4.2 LSTM

This model consists of a recurrent LSTM (Hochre-
iter and Schmidhuber, 1997) decoder. The dia-
logue acts and slot types are ﬁrst encoded as a bi-
nary vector whose length is the number of possi-
ble combinations of dialogue acts and slot types
in the dataset. We refer to this binary vector as
the Dialogue Act (DA) vector. The DA vector
for a given set of dialogue acts is a binary vec-
tor over the fused dialogue act-slot types, e.g.,
INFORM-FOOD, INFORM-COUNT, etc.

This binary vector is given as input to the de-
coder at each time-step of the LSTM. The decoder
then outputs a delexicalized sentence. A delexi-
calized sentence contains placeholders for the slot
values. An example is “I am looking for a FOOD
restaurant.”. The values for the delexicalized slots
(the type of food in this example) are then directly

copied from the input.

4.3 delex-sc-LSTM

This model uses the same architecture as the
LSTM model presented in the previous section ex-
cept that it uses sc-LSTM (Wen et al., 2015b) units
in the decoder instead of LSTM units. We call this
model the “delex-sc-LSTM”2. As in the previous
model, the input DA vector only encodes acts and
delexicalized slots. It does not contain any infor-
mation about the slot value.

By providing this model the same DA vector
input as the one given to the LSTM model, we can
directly study if the additional complexity of the
sc-LSTM unit’s reading gate provides signiﬁcant
improvement over the small-sized task-oriented
dialogue datasets which are currently available.

4.4 hierarchical-lex-delex-sc-LSTM

is a variant of the “ld-sc-LSTM”
This model
model proposed by Sharma et al. (2017) which is
based on an encoder-decoder framework. We call
our model “hierarchical-lex-delex-sc-LSTM”3.

Figure 1: Encoder of the hld-scLSTM model

Figure 2: Decoder of the hld-scLSTM model

We present the encoder in Figure 1. The en-
coder consists of a hierarchical LSTM with Ne
time-steps, where Ne is the number of non-zero
entries in the DA vector. Each time-step of the en-
coder encodes one dialogue act’s delexicalized and
lexicalized slot-value pair (e.g. (INFORM-FOOD,

2We will also refer to it as “d-scLSTM”.
3We will also refer to it as“hld-scLSTM”.

Gold
Random
LSTM

B-1
1.00
0.875
0.900
d-scLSTM 0.880
hld-scLSTM 0.909

B-2
1.00
0.843
0.879
0.850
0.890

DSTC2
B-3
1.00
0.822
0.863
0.828
0.878

B-4
1.00
0.807
0.851
0.812
0.870

M
1.00
0.564
0.610
0.578
0.624

R L
1.00
0.852
0.888
0.874
0.899

B-1
1.00
0.872
0.982
0.980
0.985

B-2
1.00
0.813
0.966
0.964
0.978

Restaurants
B-4
B-3
1.00
1.00
0.721
0.765
0.932
0.949
0.931
0.948
0.962
0.970

M
1.00
0.504
0.652
0.654
0.704

R L
1.00
0.796
0.944
0.945
0.965

Table 1: Performance comparison across models on word-overlap based automated metrics

Skip
Thought
1.00
0.906
0.946
0.925
0.932

DSTC2
Embedding
Average
1.00
0.981
0.985
0.984
0.987

Restaurants

Greedy
Vector
Extrema Matching

1.00
0.910
0.935
0.926
0.942

1.00
0.947
0.962
0.957
0.964

Skip
Thought
1.00
0.843
0.945
0.948
0.968

Embedding
Average
1.00
0.957
0.997
0.997
0.997

Greedy
Vector
Extrema Matching

1.00
0.905
0.986
0.986
0.989

1.00
0.930
0.991
0.991
0.993

Gold
Random
LSTM
d-scLSTM
hld-scLSTM

Table 2: Performance comparison across models on sentence-embedding based automated metrics

‘Chinese’)). The delexicalized act-slot part is en-
coded as a one-hot vector which we refer to as
DAt. DAt is constructed by masking all except
the tth dialogue act in the DA vector4. The lexi-
calized value part is encoded by an LSTM encoder
which shares parameters across all time-steps and
operates over the word-embeddings of the lexical-
ized values vt,i. Our model differs from the “ld-sc-
LSTM” model in that we use an LSTM encoder
over the word-embeddings instead of computing
the mean of the word-embeddings. The ﬁnal hid-
den state of this LSTM is concatenated with DAt
and is given as input to the upper LSTM (see Fig-
ure 1). The ﬁnal hidden state of the upper LSTM
is then provided to the decoder as input. This is
another difference from the “ld-sc-LSTM” as that
uses the mean of all the hidden states of the en-
coder instead, which, in our experiments, did not
perform as well as using just the ﬁnal hidden state
E.

The decoder is described in Figure 2. It is the
same as in the “ld-sc-LSTM” model. At each
time-step, it takes as input the encoder output E,
the DA vector, and the word-embedding of the
word generated at the previous time-step. The
DA vector is also additionally provided to the sc-
LSTM cell in order for it to be regulated by its
reading gate as described in Wen et al. (2015b).

5 Experiments

5.1 Decoding

During training, at each time-step, we use the
ground truth word from the previous time-step.

4also referred to as DAt=1:Ne

The model thus learns to generate the next word
given the previous one. On the other hand, to
generate sentences during test time, we use beam
search. The ﬁrst word input to the generator is a
special token < bos > which indicates the begin-
ning of the sequence. Decoding is stopped if we
reach a speciﬁed maximum number of time-steps
or if the model outputs a special token < eos >
which indicates the end of the sequence. We also
use a slot error rate penalty, similarly to Wen et al.
(2015b), to re-rank the sentences generated with
beam search. We use this method for all three of
the LSTM, d-scLSTM, and hld-scLSTM models
for fairness.

Similarly to the LSTM model, the d-scLSTM
and hld-scLSTM generate delexicalized sen-
tences, i.e., they generate slot tokens instead of
slot values directly. These slot tokens are replaced
with slot values in a post-processing step which
is a fairly common step in task-oriented dialogue
NLG literature.

5.2 Evaluation

In NLG tasks, improvements in automated met-
ric scores are most commonly used to demonstrate
improvement in the generation task. However,
these metrics have been shown to only weakly
correlate with human evaluation in the non task-
oriented dialogue setting (Liu et al., 2016) and
hence are not considered reliable measures of im-
provement. Human evaluation is considered the
metric of choice, but human ratings are expensive
to obtain. The ease of computing these automated
metrics and their availability for rapid prototyping
has lead to their widespread adoption.

Metric
Bleu 1
Bleu 2
Bleu 3
Bleu 4
METEOR
ROUGE L
Skip Thought
Embedding Average
Vector Extrema
Greedy Matching
Human

Spearman
-0.317
-0.318
-0.318
-0.318
0.295
0.294
0.528
0.295
0.299
0.295
0.810

DSTC2
p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.583
0.526
0.500
0.461
0.582
0.448
0.086
0.485
0.624
0.572
0.984

p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
0.397
<0.005
<0.005
<0.005
<0.005

Restaurants

Spearman
0.069
0.091
0.109
0.105
0.353
0.346
0.284
0.423
0.446
0.446
0.653

p-value
0.494
0.366
0.280
0.296
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.277
0.166
0.223
0.255
0.489
0.382
0.364
0.260
0.287
0.325
0.857

p-value
0.005
0.099
0.026
0.010
<0.005
<0.005
<0.005
0.009
<0.005
<0.005
<0.005

Table 3: Correlation of automated metrics with human evaluations scores

We evaluate the models described in the pre-
vious section on the DSTC2 (Henderson et al.,
2014) and the Restaurants datasets (Wen et al.,
2015a) using these automated metrics. These
datasets are some of the only available resources
for studying NLG for task-oriented dialogue. The
DSTC2 dataset contains dialogues between human
users and a dialogue system in a restaurant do-
main. The dataset is annotated with dialogue acts,
slot type, and slot values. The NLG component
of the dialogue system used for data collection is
templated. The Restaurants dataset was speciﬁ-
cally proposed for NLG and provides, for a set of
dialogue acts with slot types and slot values, two
sentences generated by humans.

We present the results of our experiments in Ta-
ble 1 and Table 2. The code for our automated
metric evaluation pipeline is available at https:
//github.com/Maluuba/nlg-eval. The
scores of all the models on these automated met-
rics are very high. This indicates that there is sig-
niﬁcant word overlap between the generated and
the reference sentences and that the NLG task on
these datasets can be solved with a simple model
such as the LSTM model. In effect, table 1 shows
that the LSTM model performs comparably to the
d-scLSTM model based on the word-overlap met-
rics. This can be explained by the fact that the
d-scLSTM model has more parameters and might
suffer from overﬁtting issues on these relatively
small datasets.

The hld-scLSTM is considered to consistently
outperform the other models based on the word-
overlap metrics. As explained by Sharma et al.
(2017), this improvement results from the model’s
access to the lexicalized slot values, due to which
it can take into account the grammatical associa-
tions of the generated words near the output to-
kens, thereby generating higher quality sentences.

κ
>0.1
>0.2
>0.3
>0.4
>0.5
>0.6

# pairs % pairs
100.0 %
55/55
72.7 %
40/55
50.9 %
28/55
34.5 %
19/55
14.5 %
8/55
0.0 %
0/55

Table 4: Pairwise Cohen’s kappa scores for the 11
human users

However, Table 2 shows that sentence-embedding
based metrics judge all the models except the ran-
dom one to perform quite similarly with again,
very high performance scores.

In the next section, we add human evaluation

for these models on these datasets.

5.3 Human rating collection

We randomly selected 20 dialogue acts from the
test set of each dataset. For each of these contexts,
we presented 5 sentences to the evaluators:
the
gold response provided in the test set and the re-
sponses generated by the four models described in
Section 4. These sentences were randomly shuf-
ﬂed and not presented in the same order. We in-
vited 18 human users to score each of these 100
sentences on a Likert-type scale of 1 to 5. The
users were asked to rate the responses depending
on how appropriate they were for the speciﬁed di-
alogue acts. A score of 1 was the lowest score,
meaning that the response was not appropriate at
all whereas a score of 5 meant that the sentence
was highly appropriate.

We computed Cohen’s kappa scores (Cohen,
1960) between the human users in pairs of two.
We removed 7 users who had kappa scores less
than 0.1 and used the remaining 11 users for the
correlation study. The kappa scores are presented
in Table 4. Most of the user pairs have a Cohen’s
κ > 0.3 which indicates fair agreement between

(a) DSTC2

(b) Restaurants

Figure 3: Scatter plots for correlation of some automated metrics with human evaluation for (a) the
DSTC2 dataset, and (b) the Restaurants dataset. Random gaussian noise N (0, 0.1) has been added to
data points along the human score axis and N (0, 0.02) has been added to the automated metric score’s
axis to aid visualization of overlapping data points. Transparency has been added for the same effect.

users (Viera et al., 2005).

5.4 Correlation between automated metrics

and human scores

We present the correlation between the automated
metrics and our collected human ratings in Ta-
ble 3. We measure human v.s. human correla-
tion by randomly splitting the human users into
two groups. The results indicate that in most
cases, human scores correlate the best with other
human scores. Except in the case of the Spear-
man correlation for BLEU-N scores, we can see
that there is a positive correlation between the au-
tomated metrics and the human scores for these
task-oriented datasets, which contrasts with the
non task-oriented dialogue setting where Liu et al.
(2016) observed no strong correlation trends.

A likely explanation for the negative Spearman
correlation values for BLEU-N is that there is
only one gold reference per context in the DSTC2
dataset. The Restaurants dataset, on the other
hand, provides two gold references per context.
Having multiple gold references increases the like-
lihood that the generated response will have sig-
niﬁcant word-overlap with one of the reference re-
sponses.

We present scatter plots for some of the metrics
presented in Table 3 in Figure 3. We observe that
all the metrics correlate very well with humans
on high scoring examples. As it can be seen in
the scatter plots, most of the sentences are given
the maximal score of 5 by the human evaluators.
This conﬁrms our previous observation that the
available corpora for task-oriented dialogue NLG
task are not very challenging and a simple LSTM-
based model can output high-quality responses.

Overall, among the word overlap based auto-
mated metrics, METEOR consistently correlates
with human evaluation on both datasets. These
results conﬁrm the original ﬁndings by Banerjee
and Lavie (2005) who showed that METEOR had
good correlation with human evaluation in the ma-
chine translation task. The comparison with ma-
chine translation is highly relevant in the task-
oriented setting because the NLG model essen-
tially learns to translate the abstract representation
of a sentence into a sentence. It is a translation task
contrary to the non task-oriented setting where the
NLG model needs to decide and output a new sen-
tence based on the last sentence typed by a user
and dialogue history. Therefore, automated met-
rics coming from the machine translation literature

are more adequate in our case than in the non-task
oriented case as shown by Liu et al. (2016).

It is interesting to see that METEOR correlates
well with human evaluation consistently. This
can be explained by the fact that even though
METEOR does not rely on word embeddings, it
includes notions of synonymy and paraphrasing
when computing the alignment between the can-
didate and reference utterances.

6 Discussion

We evaluated several natural language generation
models trained on the DSTC2 and the Restaurants
datasets based on several automated metrics. We
also performed human evaluation on the model-
generated responses and our study shows that hu-
man evaluation is a much more reliable metric
compared to the others. Among the word-overlap
based automated metrics, we found that the ME-
TEOR score correlates the most with human judg-
ments and we suggest using METEOR for task-
oriented dialogue natural language generation in-
stead of BLEU. We also observe that these met-
rics are more reliable in the task-oriented dialogue
setting compared to the general, non task-oriented
one due to the limited possible diversity in the
task-oriented setting. Also, as observed by Gal-
ley et al. (2015), we can see that word-overlap
based metrics correlate better with human evalu-
ation when multiple references are provided, as in
the Restaurants dataset. Otherwise, as in the case
of DSTC2 which only provides one reference sen-
tence per example, we observe that all the BLEU-
N metrics negatively correlate with human evalu-
ation on Spearman correlation.

As has been observed in the machine translation
literature, using beam search improves the quality
of generated sentences signiﬁcantly compared to
stochastic sampling. For similar models, our re-
sults show improvement in the automated metrics’
scores compared to Wen et al. (2015b) who used
stochastic sampling for decoding instead of beam
search.

Wen et al. (2015b) did not use the slot er-
ror rate penalty with the vanilla LSTM model
in their experiments. After adding the penalty
in our case, we observe that the vanilla LSTM-
based model performs as well as the delexicalized
semantically-controlled LSTM model. This sug-
gests that the added complexity introduced by the
sc-LSTM unit does not offer a signiﬁcant advan-

tage for these two datasets.

High performance on automated metrics,
achieved by our models on the DSTC2 and the
Restaurants datasets lead us to conclude that these
datasets are not very challenging for the NLG task.
The task-oriented dialogue community should
move towards using larger and more complex
datasets, which have been recently announced,
such as the Frames dataset (El Asri et al., 2017) or
the E2E NLG Challenge dataset (Novikova et al.,
2016).

References

John Langshaw Austin. 1962. How to do things with

words. Oxford University Press.

information system.

Scott Axelrod. 2000. Natural language generation
In Pro-
in the ibm ﬂight
ceedings of the 2000 ANLP/NAACL Workshop on
Conversational Systems - Volume 3. Association
for Computational Linguistics, Stroudsburg, PA,
USA, ANLP/NAACL-ConvSyst ’00, pages 21–26.
https://doi.org/10.3115/1117562.1117567.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proc. ACL
correlation with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and psychological

nominal scales.
measurement 20(1):37–46.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-
to-sequence generation for spoken dialogue via
In Proceed-
deep syntax trees and strings.
ings of
the As-
the 54th Annual Meeting of
sociation for Computational Linguistics (Volume
2:
Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057 .

Michael Elhadad. 1992. Generating coherent argumen-
In In Proceedings of COLING

tative paragraphs.
’92, volume II. pages 638–644.

Gabriel

Forgues,

Jean-Marie
Larcheveque, and Real Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.

Pineau,

Joelle

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for generation

tasks with intrinsically diverse targets. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
http://www.aclweb.org/anthology/P15-
445–450.
2073.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state track-
In Proceedings of the 15th Annual
ing challenge.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL). Association for Compu-
tational Linguistics, Philadelphia, PA, U.S.A., pages
http://www.aclweb.org/anthology/W14-
263–272.
4337.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016.

Ad-
versarial evaluation of dialogue models.
In
NIPS 2016 Workshop on Adversarial Training.
https://arxiv.org/pdf/1701.08198.pdf.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdi-
nov, Richard Zemel, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. 2015.
Skip-
thought vectors.
In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, ed-
itors, Advances in Neural Information Process-
ing Systems 28, Curran Associates,
Inc., pages
3294–3302. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1192–1202.
https://aclweb.org/anthology/D16-
1127.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. CoRR abs/1701.06547.
http://arxiv.org/abs/1701.06547.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2122–2132.
http://aclweb.org/anthology/D16-1230.

Ryan Lowe, Michael Noseworthy,

Iulian V. Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic tur-
ing test: Learning to evaluate dialogue responses.
In Proceedings of
the 5th International Confer-
ence on Learning Representations (ICLR) Work-
shop. Toulon, France.

Ryan Lowe, Nissan Pow, IV Serban, Laurent Charlin,
and Joelle Pineau. 2015. Incorporating unstructured
textual knowledge sources into neural dialogue sys-
In Neural Information Processing Systems
tems.
Workshop on Machine Learning for Spoken Lan-
guage Understanding.

Jekaterina Novikova, Oliver Lemon, and Verena
Crowd-sourcing nlg data: Pic-
Rieser. 2016.
In Proceedings of
tures elicit better data.
the 9th International Natural Language Gen-
eration conference. Association for Computa-
tional Linguistics, Edinburgh, UK, pages 265–273.
http://anthology.aclweb.org/W16-6644.

Alice H. Oh and Alexander

In Proceedings of

I. Rudnicky. 2000.
Stochastic language generation for spoken di-
the 2000
alogue systems.
ANLP/NAACL Workshop
on Conversational
Systems - Volume 3. Association for Compu-
PA, USA,
tational Linguistics,
ANLP/NAACL-ConvSyst
27–32.
pages
https://doi.org/10.3115/1117562.1117568.

Stroudsburg,
’00,

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Verena Rieser and Oliver Lemon. 2009. Natural
language generation as planning under uncertainty
In Proceedings of
for spoken dialogue systems.
the 12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 683–691.
http://www.aclweb.org/anthology/E09-1078.

Vasile Rus and Mihai C. Lintean. 2012. A compari-
son of greedy and optimal assessment of natural lan-
guage student input using word-to-word similarity
metrics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP.

Iulian Vlad Serban, Alessandro Sordoni, Ryan
Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2016.
A hi-
erarchical
latent variable encoder-decoder model
for generating dialogues. CoRR abs/1605.06069.
http://arxiv.org/abs/1605.06069.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2017. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR) Workshop.

Anthony J Viera, Joanne M Garrett, et al. 2005. Under-
standing interobserver agreement: the kappa statis-
tic. Family Medicine 37(5):360–363.

Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007.
Individual and
domain adaptation in sentence planning for
Int. Res. 30(1):413–456.
dialogue.
http://dl.acm.org/citation.cfm?id=1622637.1622648.

J. Artif.

Tsung-Hsien Wen, Milica Gaˇsi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a.
Stochastic Language Generation
in Dialogue using Recurrent Neural Networks with
Convolutional Sentence Reranking. In Proceedings
of the 16th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL). As-
sociation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In The IEEE International Con-
ference on Computer Vision (ICCV).

Relevance of Unsupervised Metrics in Task-Oriented Dialogue for
Evaluating Natural Language Generation

Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer
Microsoft Maluuba
first.last@microsoft.com

7
1
0
2
 
n
u
J
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
9
9
7
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Automated metrics such as BLEU are
widely used in the machine translation lit-
erature. They have also been used recently
in the dialogue community for evaluating
dialogue response generation. However,
previous work in dialogue response gener-
ation has shown that these metrics do not
correlate strongly with human judgment
in the non task-oriented dialogue setting.
Task-oriented dialogue responses are ex-
pressed on narrower domains and exhibit
lower diversity.
It is thus reasonable to
think that these automated metrics would
correlate well with human judgment in the
task-oriented setting where the generation
task consists of translating dialogue acts
into a sentence. We conduct an empirical
study to conﬁrm whether this is the case.
Our ﬁndings indicate that these automated
metrics have stronger correlation with hu-
man judgments in the task-oriented setting
compared to what has been observed in the
non task-oriented setting. We also observe
that these metrics correlate even better for
datasets which provide multiple ground
truth reference sentences. In addition, we
show that some of the currently available
corpora for task-oriented language genera-
tion can be solved with simple models and
advocate for more challenging datasets.

1

Introduction

Rule-based and template-based dialogue response
generation systems have been around for a long
time (Axelrod, 2000; Elhadad, 1992). Even today,
many task-oriented dialogue systems deployed
in production are rule-based and template-based.
These systems do not scale with increasing do-

main complexity and maintaining the increasing
number of templates becomes cumbersome.
In
the past, Oh and Rudnicky (2000) proposed a
corpus-based approach for Natural Language Gen-
eration (NLG) for task-oriented dialogue systems.
Other statistical approaches were proposed us-
ing tree-based models and reinforcement learning
(Walker et al., 2007; Rieser and Lemon, 2009).
Recently, deep-learning based approaches (Wen
et al., 2015b; Sharma et al., 2017; Lowe et al.,
2015; Serban et al., 2016) have shown promising
results for dialogue response generation.

The

automated

of machine-
evaluation
challenging and an
generated language
is
important problem for
language
the natural
processing community. The most widely used au-
tomated metrics currently are word-overlap based
metrics such as BLEU (Papineni et al., 2002),
METEOR (Banerjee and Lavie, 2005) which
were proposed originally for machine translation.
While these metrics were shown to correlate
well with manual human evaluation in machine
translation tasks, previous studies showed that this
is not the case in non-task oriented dialogue (Liu
et al., 2016). This is explained by the fact that for
the same context (e.g. a user utterance), responses
in dialogue have more diversity. Word-overlap
metrics are unable to capture semantics and thus,
can lead to poor scores even for appropriate
responses. Human evaluation in this case is the
most reliable metric. However, human judgments
are expensive to obtain and not readily available
at all times.

Task-oriented dialogue systems are employed in
narrower domains (e.g. booking a restaurant) and
responses do not have as much diversity as in the
non-task oriented setting. Another important dif-
ference is that in the non-task oriented setting, re-
sponse generation is often performed end-to-end,
which means that the model takes as input the last

user utterance and potentially the dialogue history
and it outputs the next system answer. In the task-
oriented setting, on the other hand, the language
generation task is often seen as a translation step
from an abstract representation of a sentence to
the sentence itself. As a consequence, automated
metrics which compare a generated sentence to a
reference sentence might be more appropriate and
In this paper,
correlate with human judgments.
we:

• study the correlation between human judg-
ments and several unsupervised automated
metrics on two popular task-oriented dia-
logue datasets,

• introduce variants of existing models and
evaluate their performance on these metrics

We ﬁnd that the automated metrics have stronger
correlation with human judgments in the task-
oriented setting than what has been observed in
the non task-oriented setting. We also observe that
these metrics correlate even more in the presence
of multiple reference sentences.

2 Related Work

Liu et al. (2016) did an empirical study to eval-
uate the correlation between human scores and
several automated word-overlap metrics as well
as embedding-based metrics for dialogue response
generation. They observed that these metrics,
though widely used in the literature, had only
weak correlation with human judgments in the non
task-oriented dialogue NLG setting.

In terms of supervised NLG evaluation metrics,
Lowe et al. (2017) proposed the ADEM model
which trains a hierarchical recurrent neural net-
work in a supervised manner to predict human-like
scores. This learned score was shown to corre-
late better with human judgments than any other
automated metric. However, the drawback of this
approach is the requirement for expensive human
ratings.

Li et al. (2016) proposed to use reinforcement
learning to train an end-to-end dialogue system.
They simulate a dialogue between two agents and
use a policy gradient algorithm with a reward
function which evaluates speciﬁc properties of the
responses generated by the dialogue system.

In the adversarial setting, Kannan and Vinyals
(2016) train a recurrent neural network discrimi-
nator to differentiate human-generated responses

from model-generated responses. However, an ex-
tensive analysis of the viability and the ease of
standardization of this approach is yet to be con-
ducted. Li et al. (2017), apart from adversari-
ally training dialogue response models, propose an
independent adversarial evaluation metric Adver-
Suc and a measure of the model’s reliability called
evaluator reliability error. Drawbacks of these ap-
proaches are that they are model-dependent. Ad-
versarial methods might be promising for task-
oriented dialogue systems but more research needs
to be conducted on their account.

Most of the work described so far has been
done in the non task-oriented dialogue setting as
there has been prior work indicating that auto-
mated metrics do not correlate well with humans
in that setting. There has not yet been any empiri-
cal validation that these conclusions also apply to
the task oriented setting. Research in the task ori-
ented setting has mostly made use of automated
metrics such as BLEU and human evaluation (Wen
et al., 2015b; Sharma et al., 2017; Duˇsek and Jur-
cicek, 2016).

3 Metrics

This section describes the set of automatic met-
rics whose correlation with human evaluation is
studied. We consider ﬁrst word-overlap metrics
and then embedding-based metrics.
In all that
follows, when multiple references are provided,
we compute the similarity between the prediction
and all the references one-by-one, and then select
the maximum value. We then average the scores
across the entire corpus.

3.1 Word-overlap based metrics

3.1.1 BLEU
The BLEU metric (Papineni et al., 2002) compares
n-grams between the candidate utterance and the
reference utterance. The BLEU score is com-
puted at the corpus-level and relies on the follow-
ing modiﬁed precision:

pn =

(cid:88)

(cid:88)

Ctclip(n − gram)

C∈{Candidates}
(cid:88)

n−gram∈C
(cid:88)

C(cid:48)∈{Candidates(cid:48)}

n−gram(cid:48)∈C(cid:48)

Ctclip(n − gram(cid:48))

(1)

where {Candidates} are the candidate answers
generated by the model and Ctclip is the clipped

count for the n-gram which is the number of times
the n-gram is common to the candidate answer
and the reference answer clipped by the maximum
number of occurrences of the n-gram in the refer-
ence answer. The BLEU-N score is deﬁned as:

BLEU-N = BP exp(

ωn log(pn))

(2)

N
(cid:88)

n

where N is the maximum length of the n-grams
(in this paper, we compute BLEU-1 to BLEU-4),
ω is a weighting that is often uniform and BP is a
brevity penalty. In this paper we report the BLEU
score at the corpus level but we also compute this
score at the sentence level to analyze its correla-
tion with human evaluation.

3.1.2 METEOR
The METEOR metric (Banerjee and Lavie, 2005)
was proposed as a metric which correlates better
at the sentence level with human evaluation. To
compute the METEOR score, ﬁrst, an alignment
between the candidate and the reference sentences
is created by mapping each unigram in the can-
didate sentence to 0 or 1 unigram in the reference
sentence. The alignment is not only based on exact
matches but also stem, synonym, and paraphrase
matches. Based on this alignment, unigram pre-
cision and recall are computed and the METEOR
score is:

METEOR = Fmean(1 − p)

where Fmean is the harmonic mean between preci-
sion and recall with the weight for recall 9 times a
high as the weight for precision, and p is a penalty.

3.1.3 ROUGE
ROUGE (Lin, 2004) is a set of metrics that was
ﬁrst introduced for summarization. We compute
ROUGE-L which is an F-measure based on the
Longest Common Subsequence (LCS) between
the candidate and reference utterances.

3.2 Embedding based metrics

We consider another set of metrics which compute
the cosine similarity between the embeddings of
the predicted and the reference sentence instead of
relying on word overlaps.

3.2.1 Skip-Thought
The Skip-Thought model (Kiros et al., 2015) is
trained in an unsupervised fashion and uses a re-
current network to encode a given sentence into

an embedding and then decode it to predict the
preceding and following sentences. The model
was trained on the BookCorpus dataset (Zhu et al.,
2015). The embeddings produced by the encoder
have a robust performance on semantic relatedness
tasks. We use the pre-trained Skip-Thought en-
coder provided by the authors1.

We also compute other embedding-based meth-
ods which have been used as evaluation metrics
for measuring human correlation in recent litera-
ture (Liu et al., 2016) for non task-oriented dia-
logue in Sections 3.2.2, 3.2.3, and 3.2.4.

3.2.2 Embedding average
This metric computes a sentence-level embedding
by averaging the embeddings of the words com-
posing this sentence:

¯eC =

(cid:80)
| (cid:80)

w∈C ew
w(cid:48)∈C ew(cid:48)|

.

In this equation, the vectors ew are embeddings for
the words w in the candidate sentence C.

3.2.3 Vector extrema
Vector extrema (Forgues et al., 2014) computes a
sentence-level embedding by taking the most ex-
treme value of the embeddings of the words com-
posing the sentence for each dimension of the em-
bedding:

(cid:40)

(3)

erd =

maxw∈C ewd
minw∈C ewd

if ewd > | minw(cid:48)∈C ew(cid:48)d|
otherwise.

In this equation, d is an index over the dimensions
of the embedding and C is the candidate sentence.

3.2.4 Greedy matching
Greedy matching does not compute a sentence em-
bedding but directly a similarity score between a
candidate C and a reference r (Rus and Lintean,
2012). This similarity score is computed as fol-
lows:

G(C, r) =

(cid:80) w ∈ C max ˆw∈r cos sim(ew, w ˆw)
|C|

GM (C, r) =

G(C, r) + G(r, C)
2

.

(4)

In other words, each word in the candidate sen-
tence is greedily matched to a word in the ref-
erence sentence based on the cosine similarity of

1https://github.com/ryankiros/skip-thoughts

their embeddings. The score is an average of these
similarities over the number of words in the can-
didate sentence. The same score is computed by
reversing the roles of the candidate and reference
sentences and the average of the two scores gives
the ﬁnal similarity score.

4 Response Generation Models

This section presents the different natural lan-
guage generation models that we use in this study.
All of these models take as input a set of dialogue
acts (Austin, 1962) potentially with slot types and
slot values and translate this input into an utter-
ance. An example input is inform(food =
Chinese) and a corresponding output would be
“I am looking for a Chinese restaurant.”. In this
example, the dialogue act is inform, the slot type
is food, and the slot value is Chinese.

4.1 Random

Given a dialogue act with one or more slot types,
the random model ﬁnds all the examples in the
training set with the same dialogue act and slots
(while ignoring slot values) and it randomly se-
lects its output from this set of reference sen-
tences. The datasets that we experiment on have
some special slot values such as “yes”, “no”, and
“don’t care”. Since the model ignores all slot val-
ues, these special cases are not properly handled,
which results in slightly lower performance than
what we could get by spending more time hand-
engineering the model’s behavior for these values.

4.2 LSTM

This model consists of a recurrent LSTM (Hochre-
iter and Schmidhuber, 1997) decoder. The dia-
logue acts and slot types are ﬁrst encoded as a bi-
nary vector whose length is the number of possi-
ble combinations of dialogue acts and slot types
in the dataset. We refer to this binary vector as
the Dialogue Act (DA) vector. The DA vector
for a given set of dialogue acts is a binary vec-
tor over the fused dialogue act-slot types, e.g.,
INFORM-FOOD, INFORM-COUNT, etc.

This binary vector is given as input to the de-
coder at each time-step of the LSTM. The decoder
then outputs a delexicalized sentence. A delexi-
calized sentence contains placeholders for the slot
values. An example is “I am looking for a FOOD
restaurant.”. The values for the delexicalized slots
(the type of food in this example) are then directly

copied from the input.

4.3 delex-sc-LSTM

This model uses the same architecture as the
LSTM model presented in the previous section ex-
cept that it uses sc-LSTM (Wen et al., 2015b) units
in the decoder instead of LSTM units. We call this
model the “delex-sc-LSTM”2. As in the previous
model, the input DA vector only encodes acts and
delexicalized slots. It does not contain any infor-
mation about the slot value.

By providing this model the same DA vector
input as the one given to the LSTM model, we can
directly study if the additional complexity of the
sc-LSTM unit’s reading gate provides signiﬁcant
improvement over the small-sized task-oriented
dialogue datasets which are currently available.

4.4 hierarchical-lex-delex-sc-LSTM

is a variant of the “ld-sc-LSTM”
This model
model proposed by Sharma et al. (2017) which is
based on an encoder-decoder framework. We call
our model “hierarchical-lex-delex-sc-LSTM”3.

Figure 1: Encoder of the hld-scLSTM model

Figure 2: Decoder of the hld-scLSTM model

We present the encoder in Figure 1. The en-
coder consists of a hierarchical LSTM with Ne
time-steps, where Ne is the number of non-zero
entries in the DA vector. Each time-step of the en-
coder encodes one dialogue act’s delexicalized and
lexicalized slot-value pair (e.g. (INFORM-FOOD,

2We will also refer to it as “d-scLSTM”.
3We will also refer to it as“hld-scLSTM”.

Gold
Random
LSTM

B-1
1.00
0.875
0.900
d-scLSTM 0.880
hld-scLSTM 0.909

B-2
1.00
0.843
0.879
0.850
0.890

DSTC2
B-3
1.00
0.822
0.863
0.828
0.878

B-4
1.00
0.807
0.851
0.812
0.870

M
1.00
0.564
0.610
0.578
0.624

R L
1.00
0.852
0.888
0.874
0.899

B-1
1.00
0.872
0.982
0.980
0.985

B-2
1.00
0.813
0.966
0.964
0.978

Restaurants
B-4
B-3
1.00
1.00
0.721
0.765
0.932
0.949
0.931
0.948
0.962
0.970

M
1.00
0.504
0.652
0.654
0.704

R L
1.00
0.796
0.944
0.945
0.965

Table 1: Performance comparison across models on word-overlap based automated metrics

Skip
Thought
1.00
0.906
0.946
0.925
0.932

DSTC2
Embedding
Average
1.00
0.981
0.985
0.984
0.987

Restaurants

Greedy
Vector
Extrema Matching

1.00
0.910
0.935
0.926
0.942

1.00
0.947
0.962
0.957
0.964

Skip
Thought
1.00
0.843
0.945
0.948
0.968

Embedding
Average
1.00
0.957
0.997
0.997
0.997

Greedy
Vector
Extrema Matching

1.00
0.905
0.986
0.986
0.989

1.00
0.930
0.991
0.991
0.993

Gold
Random
LSTM
d-scLSTM
hld-scLSTM

Table 2: Performance comparison across models on sentence-embedding based automated metrics

‘Chinese’)). The delexicalized act-slot part is en-
coded as a one-hot vector which we refer to as
DAt. DAt is constructed by masking all except
the tth dialogue act in the DA vector4. The lexi-
calized value part is encoded by an LSTM encoder
which shares parameters across all time-steps and
operates over the word-embeddings of the lexical-
ized values vt,i. Our model differs from the “ld-sc-
LSTM” model in that we use an LSTM encoder
over the word-embeddings instead of computing
the mean of the word-embeddings. The ﬁnal hid-
den state of this LSTM is concatenated with DAt
and is given as input to the upper LSTM (see Fig-
ure 1). The ﬁnal hidden state of the upper LSTM
is then provided to the decoder as input. This is
another difference from the “ld-sc-LSTM” as that
uses the mean of all the hidden states of the en-
coder instead, which, in our experiments, did not
perform as well as using just the ﬁnal hidden state
E.

The decoder is described in Figure 2. It is the
same as in the “ld-sc-LSTM” model. At each
time-step, it takes as input the encoder output E,
the DA vector, and the word-embedding of the
word generated at the previous time-step. The
DA vector is also additionally provided to the sc-
LSTM cell in order for it to be regulated by its
reading gate as described in Wen et al. (2015b).

5 Experiments

5.1 Decoding

During training, at each time-step, we use the
ground truth word from the previous time-step.

4also referred to as DAt=1:Ne

The model thus learns to generate the next word
given the previous one. On the other hand, to
generate sentences during test time, we use beam
search. The ﬁrst word input to the generator is a
special token < bos > which indicates the begin-
ning of the sequence. Decoding is stopped if we
reach a speciﬁed maximum number of time-steps
or if the model outputs a special token < eos >
which indicates the end of the sequence. We also
use a slot error rate penalty, similarly to Wen et al.
(2015b), to re-rank the sentences generated with
beam search. We use this method for all three of
the LSTM, d-scLSTM, and hld-scLSTM models
for fairness.

Similarly to the LSTM model, the d-scLSTM
and hld-scLSTM generate delexicalized sen-
tences, i.e., they generate slot tokens instead of
slot values directly. These slot tokens are replaced
with slot values in a post-processing step which
is a fairly common step in task-oriented dialogue
NLG literature.

5.2 Evaluation

In NLG tasks, improvements in automated met-
ric scores are most commonly used to demonstrate
improvement in the generation task. However,
these metrics have been shown to only weakly
correlate with human evaluation in the non task-
oriented dialogue setting (Liu et al., 2016) and
hence are not considered reliable measures of im-
provement. Human evaluation is considered the
metric of choice, but human ratings are expensive
to obtain. The ease of computing these automated
metrics and their availability for rapid prototyping
has lead to their widespread adoption.

Metric
Bleu 1
Bleu 2
Bleu 3
Bleu 4
METEOR
ROUGE L
Skip Thought
Embedding Average
Vector Extrema
Greedy Matching
Human

Spearman
-0.317
-0.318
-0.318
-0.318
0.295
0.294
0.528
0.295
0.299
0.295
0.810

DSTC2
p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.583
0.526
0.500
0.461
0.582
0.448
0.086
0.485
0.624
0.572
0.984

p-value
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
0.397
<0.005
<0.005
<0.005
<0.005

Restaurants

Spearman
0.069
0.091
0.109
0.105
0.353
0.346
0.284
0.423
0.446
0.446
0.653

p-value
0.494
0.366
0.280
0.296
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005
<0.005

Pearson
0.277
0.166
0.223
0.255
0.489
0.382
0.364
0.260
0.287
0.325
0.857

p-value
0.005
0.099
0.026
0.010
<0.005
<0.005
<0.005
0.009
<0.005
<0.005
<0.005

Table 3: Correlation of automated metrics with human evaluations scores

We evaluate the models described in the pre-
vious section on the DSTC2 (Henderson et al.,
2014) and the Restaurants datasets (Wen et al.,
2015a) using these automated metrics. These
datasets are some of the only available resources
for studying NLG for task-oriented dialogue. The
DSTC2 dataset contains dialogues between human
users and a dialogue system in a restaurant do-
main. The dataset is annotated with dialogue acts,
slot type, and slot values. The NLG component
of the dialogue system used for data collection is
templated. The Restaurants dataset was speciﬁ-
cally proposed for NLG and provides, for a set of
dialogue acts with slot types and slot values, two
sentences generated by humans.

We present the results of our experiments in Ta-
ble 1 and Table 2. The code for our automated
metric evaluation pipeline is available at https:
//github.com/Maluuba/nlg-eval. The
scores of all the models on these automated met-
rics are very high. This indicates that there is sig-
niﬁcant word overlap between the generated and
the reference sentences and that the NLG task on
these datasets can be solved with a simple model
such as the LSTM model. In effect, table 1 shows
that the LSTM model performs comparably to the
d-scLSTM model based on the word-overlap met-
rics. This can be explained by the fact that the
d-scLSTM model has more parameters and might
suffer from overﬁtting issues on these relatively
small datasets.

The hld-scLSTM is considered to consistently
outperform the other models based on the word-
overlap metrics. As explained by Sharma et al.
(2017), this improvement results from the model’s
access to the lexicalized slot values, due to which
it can take into account the grammatical associa-
tions of the generated words near the output to-
kens, thereby generating higher quality sentences.

κ
>0.1
>0.2
>0.3
>0.4
>0.5
>0.6

# pairs % pairs
100.0 %
55/55
72.7 %
40/55
50.9 %
28/55
34.5 %
19/55
14.5 %
8/55
0.0 %
0/55

Table 4: Pairwise Cohen’s kappa scores for the 11
human users

However, Table 2 shows that sentence-embedding
based metrics judge all the models except the ran-
dom one to perform quite similarly with again,
very high performance scores.

In the next section, we add human evaluation

for these models on these datasets.

5.3 Human rating collection

We randomly selected 20 dialogue acts from the
test set of each dataset. For each of these contexts,
we presented 5 sentences to the evaluators:
the
gold response provided in the test set and the re-
sponses generated by the four models described in
Section 4. These sentences were randomly shuf-
ﬂed and not presented in the same order. We in-
vited 18 human users to score each of these 100
sentences on a Likert-type scale of 1 to 5. The
users were asked to rate the responses depending
on how appropriate they were for the speciﬁed di-
alogue acts. A score of 1 was the lowest score,
meaning that the response was not appropriate at
all whereas a score of 5 meant that the sentence
was highly appropriate.

We computed Cohen’s kappa scores (Cohen,
1960) between the human users in pairs of two.
We removed 7 users who had kappa scores less
than 0.1 and used the remaining 11 users for the
correlation study. The kappa scores are presented
in Table 4. Most of the user pairs have a Cohen’s
κ > 0.3 which indicates fair agreement between

(a) DSTC2

(b) Restaurants

Figure 3: Scatter plots for correlation of some automated metrics with human evaluation for (a) the
DSTC2 dataset, and (b) the Restaurants dataset. Random gaussian noise N (0, 0.1) has been added to
data points along the human score axis and N (0, 0.02) has been added to the automated metric score’s
axis to aid visualization of overlapping data points. Transparency has been added for the same effect.

users (Viera et al., 2005).

5.4 Correlation between automated metrics

and human scores

We present the correlation between the automated
metrics and our collected human ratings in Ta-
ble 3. We measure human v.s. human correla-
tion by randomly splitting the human users into
two groups. The results indicate that in most
cases, human scores correlate the best with other
human scores. Except in the case of the Spear-
man correlation for BLEU-N scores, we can see
that there is a positive correlation between the au-
tomated metrics and the human scores for these
task-oriented datasets, which contrasts with the
non task-oriented dialogue setting where Liu et al.
(2016) observed no strong correlation trends.

A likely explanation for the negative Spearman
correlation values for BLEU-N is that there is
only one gold reference per context in the DSTC2
dataset. The Restaurants dataset, on the other
hand, provides two gold references per context.
Having multiple gold references increases the like-
lihood that the generated response will have sig-
niﬁcant word-overlap with one of the reference re-
sponses.

We present scatter plots for some of the metrics
presented in Table 3 in Figure 3. We observe that
all the metrics correlate very well with humans
on high scoring examples. As it can be seen in
the scatter plots, most of the sentences are given
the maximal score of 5 by the human evaluators.
This conﬁrms our previous observation that the
available corpora for task-oriented dialogue NLG
task are not very challenging and a simple LSTM-
based model can output high-quality responses.

Overall, among the word overlap based auto-
mated metrics, METEOR consistently correlates
with human evaluation on both datasets. These
results conﬁrm the original ﬁndings by Banerjee
and Lavie (2005) who showed that METEOR had
good correlation with human evaluation in the ma-
chine translation task. The comparison with ma-
chine translation is highly relevant in the task-
oriented setting because the NLG model essen-
tially learns to translate the abstract representation
of a sentence into a sentence. It is a translation task
contrary to the non task-oriented setting where the
NLG model needs to decide and output a new sen-
tence based on the last sentence typed by a user
and dialogue history. Therefore, automated met-
rics coming from the machine translation literature

are more adequate in our case than in the non-task
oriented case as shown by Liu et al. (2016).

It is interesting to see that METEOR correlates
well with human evaluation consistently. This
can be explained by the fact that even though
METEOR does not rely on word embeddings, it
includes notions of synonymy and paraphrasing
when computing the alignment between the can-
didate and reference utterances.

6 Discussion

We evaluated several natural language generation
models trained on the DSTC2 and the Restaurants
datasets based on several automated metrics. We
also performed human evaluation on the model-
generated responses and our study shows that hu-
man evaluation is a much more reliable metric
compared to the others. Among the word-overlap
based automated metrics, we found that the ME-
TEOR score correlates the most with human judg-
ments and we suggest using METEOR for task-
oriented dialogue natural language generation in-
stead of BLEU. We also observe that these met-
rics are more reliable in the task-oriented dialogue
setting compared to the general, non task-oriented
one due to the limited possible diversity in the
task-oriented setting. Also, as observed by Gal-
ley et al. (2015), we can see that word-overlap
based metrics correlate better with human evalu-
ation when multiple references are provided, as in
the Restaurants dataset. Otherwise, as in the case
of DSTC2 which only provides one reference sen-
tence per example, we observe that all the BLEU-
N metrics negatively correlate with human evalu-
ation on Spearman correlation.

As has been observed in the machine translation
literature, using beam search improves the quality
of generated sentences signiﬁcantly compared to
stochastic sampling. For similar models, our re-
sults show improvement in the automated metrics’
scores compared to Wen et al. (2015b) who used
stochastic sampling for decoding instead of beam
search.

Wen et al. (2015b) did not use the slot er-
ror rate penalty with the vanilla LSTM model
in their experiments. After adding the penalty
in our case, we observe that the vanilla LSTM-
based model performs as well as the delexicalized
semantically-controlled LSTM model. This sug-
gests that the added complexity introduced by the
sc-LSTM unit does not offer a signiﬁcant advan-

tage for these two datasets.

High performance on automated metrics,
achieved by our models on the DSTC2 and the
Restaurants datasets lead us to conclude that these
datasets are not very challenging for the NLG task.
The task-oriented dialogue community should
move towards using larger and more complex
datasets, which have been recently announced,
such as the Frames dataset (El Asri et al., 2017) or
the E2E NLG Challenge dataset (Novikova et al.,
2016).

References

John Langshaw Austin. 1962. How to do things with

words. Oxford University Press.

information system.

Scott Axelrod. 2000. Natural language generation
In Pro-
in the ibm ﬂight
ceedings of the 2000 ANLP/NAACL Workshop on
Conversational Systems - Volume 3. Association
for Computational Linguistics, Stroudsburg, PA,
USA, ANLP/NAACL-ConvSyst ’00, pages 21–26.
https://doi.org/10.3115/1117562.1117567.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
In Proc. ACL
correlation with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and psychological

nominal scales.
measurement 20(1):37–46.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-
to-sequence generation for spoken dialogue via
In Proceed-
deep syntax trees and strings.
ings of
the As-
the 54th Annual Meeting of
sociation for Computational Linguistics (Volume
2:
Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057 .

Michael Elhadad. 1992. Generating coherent argumen-
In In Proceedings of COLING

tative paragraphs.
’92, volume II. pages 638–644.

Gabriel

Forgues,

Jean-Marie
Larcheveque, and Real Tremblay. 2014. Boot-
strapping dialog systems with word embeddings.

Pineau,

Joelle

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for generation

tasks with intrinsically diverse targets. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
http://www.aclweb.org/anthology/P15-
445–450.
2073.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state track-
In Proceedings of the 15th Annual
ing challenge.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL). Association for Compu-
tational Linguistics, Philadelphia, PA, U.S.A., pages
http://www.aclweb.org/anthology/W14-
263–272.
4337.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anjuli Kannan and Oriol Vinyals. 2016.

Ad-
versarial evaluation of dialogue models.
In
NIPS 2016 Workshop on Adversarial Training.
https://arxiv.org/pdf/1701.08198.pdf.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdi-
nov, Richard Zemel, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. 2015.
Skip-
thought vectors.
In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, ed-
itors, Advances in Neural Information Process-
ing Systems 28, Curran Associates,
Inc., pages
3294–3302. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1192–1202.
https://aclweb.org/anthology/D16-
1127.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. CoRR abs/1701.06547.
http://arxiv.org/abs/1701.06547.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2122–2132.
http://aclweb.org/anthology/D16-1230.

Ryan Lowe, Michael Noseworthy,

Iulian V. Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic tur-
ing test: Learning to evaluate dialogue responses.
In Proceedings of
the 5th International Confer-
ence on Learning Representations (ICLR) Work-
shop. Toulon, France.

Ryan Lowe, Nissan Pow, IV Serban, Laurent Charlin,
and Joelle Pineau. 2015. Incorporating unstructured
textual knowledge sources into neural dialogue sys-
In Neural Information Processing Systems
tems.
Workshop on Machine Learning for Spoken Lan-
guage Understanding.

Jekaterina Novikova, Oliver Lemon, and Verena
Crowd-sourcing nlg data: Pic-
Rieser. 2016.
In Proceedings of
tures elicit better data.
the 9th International Natural Language Gen-
eration conference. Association for Computa-
tional Linguistics, Edinburgh, UK, pages 265–273.
http://anthology.aclweb.org/W16-6644.

Alice H. Oh and Alexander

In Proceedings of

I. Rudnicky. 2000.
Stochastic language generation for spoken di-
the 2000
alogue systems.
ANLP/NAACL Workshop
on Conversational
Systems - Volume 3. Association for Compu-
PA, USA,
tational Linguistics,
ANLP/NAACL-ConvSyst
27–32.
pages
https://doi.org/10.3115/1117562.1117568.

Stroudsburg,
’00,

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Verena Rieser and Oliver Lemon. 2009. Natural
language generation as planning under uncertainty
In Proceedings of
for spoken dialogue systems.
the 12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 683–691.
http://www.aclweb.org/anthology/E09-1078.

Vasile Rus and Mihai C. Lintean. 2012. A compari-
son of greedy and optimal assessment of natural lan-
guage student input using word-to-word similarity
metrics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP.

Iulian Vlad Serban, Alessandro Sordoni, Ryan
Lowe, Laurent Charlin, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2016.
A hi-
erarchical
latent variable encoder-decoder model
for generating dialogues. CoRR abs/1605.06069.
http://arxiv.org/abs/1605.06069.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2017. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR) Workshop.

Anthony J Viera, Joanne M Garrett, et al. 2005. Under-
standing interobserver agreement: the kappa statis-
tic. Family Medicine 37(5):360–363.

Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007.
Individual and
domain adaptation in sentence planning for
Int. Res. 30(1):413–456.
dialogue.
http://dl.acm.org/citation.cfm?id=1622637.1622648.

J. Artif.

Tsung-Hsien Wen, Milica Gaˇsi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a.
Stochastic Language Generation
in Dialogue using Recurrent Neural Networks with
Convolutional Sentence Reranking. In Proceedings
of the 16th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL). As-
sociation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In The IEEE International Con-
ference on Computer Vision (ICCV).

