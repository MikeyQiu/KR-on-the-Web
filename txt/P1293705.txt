TALL: Temporal Activity Localization via Language Query

Jiyang Gao1

Chen Sun2
1University of Southern California

Zhenheng Yang1

2Google Research

Ram Nevatia1

{jiyangga, zhenheny, nevatia}@usc.edu, chensun@google.com

7
1
0
2
 
g
u
A
 
3
 
 
]

V
C
.
s
c
[
 
 
2
v
1
0
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

This paper focuses on temporal localization of actions
in untrimmed videos. Existing methods typically train clas-
siﬁers for a pre-deﬁned list of actions and apply them in
a sliding window fashion. However, activities in the wild
consist of a wide combination of actors, actions and ob-
jects; it is difﬁcult to design a proper activity list that meets
users’ needs. We propose to localize activities by natural
language queries. Temporal Activity Localization via Lan-
guage (TALL) is challenging as it requires: (1) suitable de-
sign of text and video representations to allow cross-modal
matching of actions and language queries; (2) ability to lo-
cate actions accurately given features from sliding windows
of limited granularity. We propose a novel Cross-modal
Temporal Regression Localizer (CTRL) to jointly model text
query and video clips, output alignment scores and action
boundary regression results for candidate clips. For evalu-
ation, we adopt TaCoS dataset, and build a new dataset for
this task on top of Charades by adding sentence temporal
annotations, called Charades-STA. We also build complex
sentence queries in Charades-STA for test. Experimental
results show that CTRL outperforms previous methods sig-
niﬁcantly on both datasets.

1. Introduction

Activities in the wild consist of a diverse combination
of actors, actions and objects over various periods of time.
Earlier work focused on classiﬁcation of video clips that
contained a single activity, i.e. where the videos were
trimmed. Recently, there has also been signiﬁcant work in
localizing activities in longer, untrimmed videos [30, 15].
One major limitation of existing action localization meth-
ods is that they are restricted to pre-deﬁned list of actions.
Although the lists of activities can be relatively large [2],
they still face difﬁculty in covering complex activity ques-
tions, for example, “A person runs to the window and then
look out.” , as shown in Figure 1. Hence, it is desirable to
use natural language queries to localize activities. Use of
natural language not only allows for an open set of activi-

Figure 1. Temporal activity localization via language query in an
untrimmed video.

ties but also natural speciﬁcation of additional constraints,
including objects and their properties as well as relations
between the involved entities. We propose the task of Tem-
poral Activity Localization via Language (TALL): given a
temporally untrimmed video and a natural language query,
the goal is to determine the start and end times for the de-
scribed activity inside the video.

For traditional temporal action localization, most current
approaches [30, 15, 26, 34, 35] apply activity classiﬁers
trained with optical ﬂow-based methods [33, 28] or Con-
volutional Neural Networks (CNNs) [29, 32] in a sliding
window fashion. A direct extension to support natural lan-
guage query is to map the queries into a discrete label space.
However, it is non-trivial to design a label space which has
enough coverage for such activities without losing useful
details in users’ queries.

To go beyond discrete activity labels, one possible solu-
tion is to embed visual features and sentence features into
a common space [10, 13, 18]. However, for temporal lo-
calization of activities, it is unclear what a proper visual
model to extract visual features for retrieval is, and how
to achieve high precision of predicted start/end time. Al-
though one could densely sample sliding windows at dif-
ferent scales, doing so is not only computationally expen-
sive but also makes the alignment task more challenging,
as the search space increases. An alternative to dense sam-
pling is to adjust the temporal boundaries of proposals by
learning regression parameters; such an approach has been
successful for object localization, as in [23]. However, tem-

poral regression has not been attempted in the past work
and is more difﬁcult as the activities are characterized by
a spatio-temporal volume, which may lead to more back-
ground noise.

These challenges motivate us to propose a novel Cross-
modal Temporal Regression Localizer (CTRL) model to
jointly model text query, video clip candidates and their
temporal context
information to solve the TALL task.
CTRL generates alignment scores along with location re-
gression results for candidate clips. It utilizes a CNN model
to extract visual features of the clips and a Long Short-
term Memory (LSTM) network to extract sentence embed-
dings. A cross-modal processing module is designed to
jointly model the text and visual features, which calculates
element-wise addition, multiplication and direct concatena-
tion. Finally, multilayer networks are trained for visual-
semantic alignment and clip location regression. We design
the non-parameterized and parameterized location offsets
for temporal coordinate regression. In parameterized set-
ting, the length and the central coordinate of the clip is ﬁrst
parameterized by the ground truth length and coordinate.
In non-parameterized setting, the start and end coordinates
are used directly. We show that the non-parameterized one
works better, unlike the case for object boundary regression.
To facilitate research of TALL, we also generate sen-
tence temporal annotations for Charades [27] dataset.
We name it Charades-STA.We evaluate our methods on
TACoS and Charades-STA datasets by the metric of “R@n,
IoU=m”, which represents the percentage of at least one of
the top-n results ( start and end pairs ) having IoU with the
ground truth larger than m. Experimental results demon-
strate the effectiveness of our proposed CTRL framework.

In summary, our contributions are two-fold:
(1) We propose a novel problem formulation of Temporal
Activity Localization via natural Language (TALL) query.
(2) We introduce an effective Cross-modal Temporal
Regression Localizer (CTRL) which estimates alignment
scores and temporal action boundary by jointly modeling
language query and video clips.1

2. Related Work

Action classiﬁcation and temporal localization. There
have been tremendous explorations about action classiﬁca-
tion in videos using deep convolutional neural networks
(ConvNets). Representative methods include two-stream
ConvNets, C3D (3D ConvNets) and 2D ConvNets with
temporal LSTM or mean pooling. Speciﬁcally, Simonyan
and Zisserman [28] modeled the appearance and motion
information in two separate ConvNets and combined the
scores by late fusion. Tran et al. [32] used 3D convolu-
tional ﬁlters to capture motion information in neighboring

1Source codes are available in https://github.com/jiyanggao/TALL .

frames. [36] [10] proposed to use 2D ConvNets to extract
deep features for one frame and use temporal mean pooling
or LSTM to model temporal information.

For temporal action localization task, Shou et al. [26]
trained C3D [32] with localization loss and achieved state-
of-the-art performance on THUMOS 14. Ma et al.
[15]
used a temporal LSTM to generate frame-wise prediction
scores and then merged the detection intervals based on
the predictions. Singh et al.
[30] extended two-stream
[28] framework with person detection and bi-directional
LSTMs and achieved state-of-the-art performance on MPII-
Cooking dataset [24]. Gao et al. [5] proposed to use tempo-
ral coordinate regression to reﬁne action boundary for tem-
poral localization.

Sentence-based image/video retrieval. Given a set of
candidate videos/images and a sentence query, this task re-
quires retrieving the videos/images that match the query.
Karpathy et al. [9] proposed Deep Visual-Semantic Align-
ment (DVSA) model. DVSA used bidirectional LSTMs to
encode sentence embeddings, and R-CNN object detectors
[7] to extract features from object proposals. Skip-thought
[13] learned a Sent2Vec model by applying skip-gram [19]
on sentence level and achieved top performance in sentence-
based image retrieval task. Sun et al. [31] proposed to dis-
cover visual concepts from image-sentence pairs and apply
the concept detectors for image retrieval. Gao et al.
[4]
proposed to learn verb-object pairs as action concepts from
image-sentence pairs. Hu et al. [8] and Mao et al. [17]
formulated the problem of natural language object retrieval.
[14] parsed the sen-
tence descriptions into a semantic graph, which are then
matched to visual concepts in the videos by generalized bi-
partite matching. Bojanowski et al. [1] tackled the problem
of video-text alignment: given a video and a set of sentences
with temporal ordering, assigning a temporal interval for
each sentence. In our settings, only one sentence query is
input to the system and temporal ordering is not used.

As for video retrieval, Lin et al.

Object detection. Our work is partly inspired by the
success of recent object detection approaches. R-CNN [7]
consists of selective search, CNN feature extraction, SVM
classiﬁcation and bounding box regression. Fast-RCNN [6]
designs RoI pooling layer and the model could be trained
by end-to-end framework. One of the key element shared
in those successful object detection frameworks [21, 23, 6]
is the bounding box regression layer. We show that, un-
like object boundary regression using parameterized offsets,
non-parameterized offsets work better for action boundary
regression.

3. Methods

In this section, we describe our Cross-modal Temporal
Regression Localizer (CTRL) for Temporal Activity Local-
ization via Language (TALL) and training procedure in de-

Figure 2. Cross-modal Temporal Regression Localizer (CTRL) architecture. CTRL contains four modules: a visual encoder to extract
features for video clips, a sentence encoder to extract embeddings, a multi-modal processing network to generate combined representations
for visual and text domain, and a temporal regression network to produce alignment scores and location offsets.

tail. CTRL contains four modules: a visual encoder to ex-
tract features for video clips, a sentence encoder to extract
embeddings, a multi-modal processing network to generate
combined representations for visual and text domain, and
a temporal regression network to produce alignment scores
and location offsets between the input sentence query and
video clips.

3.1. Problem Formulation

We denote a video as V = {ft}T

t=1, T is the frame num-
ber of the video. Each video is associated with temporal
sentence annotations: A = {(sj, τ s
j=1, M is the sen-
tence annotation number of the video V , sj is a natural lan-
guage sentence of a video clip, which has τ s
j as start
and end time in the video. The training data are the sen-
tence and video clip pairs. The task is to predict one or more
(τ s
j ) for the input natural language sentence query.

j and τ e

j )}M

j , τ e

j , τ e

3.2. CTRL Architecture

i , te

i )}H

i and te

Visual Encoder. For a long untrimmed video V , we gen-
erate a set of video clips C = {(ci, ts
i=1 by temporal
sliding windows, where H is the total number of the clips
of the video V , ts
i are the start and end time of video
clip ci. We deﬁne visual encoder as a function Fve(ci) that
maps a certain clip ci and its context to a feature vector fv,
whose dimension is ds.
Inside the visual encoder, a fea-
ture extractor Ev is used to extract clip-level feature vec-
tors, whose input is nf frames and output is a vector with
dimension dv. For one video clip ci, we consider itself (as
the central clip) and its surrounding clips (as context clips)
ci,q, q ∈ [−n, n], j is the clip shift, n is the shift boundary.

(cid:80)−1

We uniformly sample nf frames from each clip (central and
context clips). The feature vector of central clip is denoted
as f ctl
v . For the context clips, we use a pooling layer to cal-
v = 1
culate a pre-context feature f pre
q=−n Ev(ci,q) and
n
(cid:80)n
v = 1
post-context feature f post
q=1 Ev(ci,q). Pre-context
n
feature and post-context feature are pooled separately, as
the end and the start of an activity can be quite different and
, f ctl
both could be critical for temporal localization. f pre
v
and f post
are concatenated and then linearly transformed to
v
the feature vector fv with dimension ds, as the visual repre-
sentation for clip ci.

v

Sentence Encoder. A sentence encoder is a function
Fse(sj) that maps a sentence description sj to a embedding
space, whose dimension is ds( the same as visual feature
space ). Speciﬁcally, a sentence embedding extractor Es is
used to extract a sentence-level embedding f (cid:48)
s and is fol-
lowed by a linear transformation layer, which maps f (cid:48)
s to
fs with dimension ds, the same as visual representation fv.
We experiment two kinds of sentence embedding extractors,
one is a LSTM network which takes a word as input at each
step, and the hidden state of ﬁnal step is used as sentence-
level embedding; the other is an off-the-shelf sentence en-
coder, Skip-thought [13]. More details would be discussed
in Section 4.

Multi-modal Processing Module. The inputs of the
multi-modal processing module are a visual representation
fv and a sentence embedding fs, which have the same di-
mension ds. We use vector element-wise addition (+), vec-
tor element-wise multiplication (×) and vector concatena-
tion ((cid:107)) followed by a Fully Connected (F C) layer to com-
bine the information from both modalities. Addition and

multiplication operation allow additive and multiplicative
interaction between two modalities and don’t change the
feature dimension. The F C layer allows interaction among
all elements. The input dimension of the F C layer is 2 ∗ ds
and the output dimension is ds. The outputs from all three
operations are concatenated to construct a multi-modal rep-
resentation fsv = (fs × fv) (cid:107) (fs + fv) (cid:107) F C(fs (cid:107) fv),
which is the input for our core module, temporal localiza-
tion regression networks.

Temporal Localization Regression Networks. Tempo-
ral localization regression network takes the multi-modal
representation fsv as input, and has two sibling output lay-
ers. The ﬁrst one outputs an alignment score csi,j between
the sentence sj and the video clip ci. The second one out-
puts clip location regression offsets. We design two location
offsets, the ﬁrst one is parameterized offset: t = (tc, tl),
where tc and tl are parameterized central point offset and
length offset respectively. The parameterization is as fol-
lows:

tp = (p − pc)/lc, tl = log(l/lc)
where p and l denote the clip’s center coordinate and clip
length respectively. Variables p, pc are for predicted clip
and test clip (like wise for l). The second offset is non-
parameterized offset: t = (ts, te), where ts and te are the
start and end point offsets.

(1)

ts = s − sc, te = e − ec

(2)

where s and e denote the clip’s start and end coordinate re-
spectively. Temporal coordinate regression can be thought
as clip location regression from a test clip to a nearby
ground-truth clip, as the original clip could be either too
tight or too loose, the regression process tend to ﬁnd better
locations.

3.3. CTRL Training

Multi-task Loss Function. CTRL contains two sibling
output layers, one for alignment and the other for regres-
sion. We design a multi-task loss L on a mini-batch of
training samples to jointly train for visual-semantic align-
ment and clip location regression.

L = Laln + αLreg

(3)

where Laln is for visual-semantic alignment and Lreg is for
clip location regression, and α is a hyper-parameter, which
controls the balance between the two task losses. The align-
ment loss encourages aligned clip-sentence pairs to have
positive scores and misaligned pairs to have negative scores.

Laln =

[αclog(1 + exp(−csi,i))+

1
N

N
(cid:88)

i=0

N
(cid:88)

j=0,j(cid:54)=i

Figure 3. Intersection over Union (IoU) and non-Intersection over
Length (nIoL).

where N is the batch size, csi,j is the alignment score be-
tween sentence sj and video clip ci, αc and αw are the hy-
per parameters which control the weights between positive
( aligned ) and negative ( misaligned ) clip-sentence pairs.

The regression loss Lreg is calculated for the aligned
clip-sentence pairs. A sentence sj annotation contains start
and end time (τ s
j ). The aligned sliding window clip ci
has (ts
i ). The ground truth offsets t∗ are calculated from
i , te
start and end times.

j , τ e

Lreg =

[R(t∗

x,i − tx,i) + R(t∗

y,i − ty,i)]

(5)

1
N

N
(cid:88)

i=0

where x and y indicate p and l for parameterized offsets, or
s and e for non-parameterized offsets. R(t) is smooth L1
function.

h, te

h, te

h, τ e

h, τ e

h)]}NT

h), (ch, ts

Sampling Training Examples. To collect training sam-
ples, we use multi-scale temporal sliding windows with
[64, 128, 256, 512] frames and 80% overlap. (Note that,
at test time, we only use coarsely sampled clips.) We
use the following strategy to collect training samples T =
{[(sh, τ s
h=0. Each training sample con-
tains a sentence description (sh, τ s
h) and a video clip
(ch, ts
h). For a sliding window clip c from C with tem-
poral annotation (ts, te) and a sentence description s with
temporal annotation (τ s, τ e), we align them as a pair of
training samples if they satisfy (1) Intersection over Union
(IoU) is larger than 0.5; (2) non Intersection over Length
(nIoL) is smaller than 0.2 and (3) one sliding window clip
can be aligned with only one sentence description. The rea-
son we use nIoL is that we want the the most part of the
sliding window clip to overlap with the assigned sentence,
and simply increasing IoU threshold would harm regression
layers ( regression aims to move the clip from low IoU to
high IoU). As shown in Figure 3, although the IoU between
c and s1 is about 0.5, if we assign c to s1, then it will disturb
the model ,because c contains information of s2.

αwlog(1 + exp(csi,j))]

(4)

In this section, we describe the evaluation settings and

4. Evaluation

discuss the experiment results

4.1. Datasets

TACoS [22]. This dataset was built on the top of MPII-
Compositive dataset [25] and contains 127 videos. Every
video is associated with two type of annotations. The ﬁrst
one is ﬁne-grained activity labels with temporal location
(start and end time). The second set of annotations is natural
language descriptions with temporal locations. The natu-
ral language descriptions were obtained by crowd-sourcing
annotators, who were asked to describe the content of the
video clips by sentences. In total, there are 17344 pairs of
sentence and video clips. We split it in 50% for training,
25% for validation and 25% for test.

Charades-STA. Charades [27] contains around 10k
videos and each video contains temporal activity annota-
tion (from 157 activity categories) and multiple video-level
descriptions. TALL needs clip-level sentence annotation:
sentence descriptions with start and end time, which are
not provided in the original Charades dataset. We noticed
that the names of activity categories in Charades are parsed
from the video-level descriptions, so many of activity names
appear in descriptions. Another observation we make is
that most descriptions in Charades share a similar syntac-
tic structure: consisting of multiple sub-sentences, which
are connected by comma, period and conjunctions, such as
“then”, “while”, “after”, “and”. For example, “A person is
sitting down by the door. They stand up and start carefully
leaving some dishes in the sink”.

Based on these observations, we designed a semi-
automatic way to generate sentence temporal annotation.
The ﬁrst step is sentence decomposition: a long sentence
is split to sub-sentences by a set of conjunctions (which are
collected by hand ), and for each sub-sentence, the subject (
parsed by Stanford CoreNLP [16] ) of the original long sen-
tence is added to start. The second step is keyword match-
ing: we extract keywords for each activity categories and
match them to sub-sentences, if they are matched, the tem-
poral annotation (start and end time) are assigned to the sub-
sentences. The third step is a human check: for each pair
of sub-sentence and temporal annotation, we (two of the
co-authors) checked whether the sentence made sense and
whether they matched the activity annotation. An example
is shown in Figure 4.

Although TACoS and Charades-STA are challenging,
their lengths of queries are limited to single sentences.
To explore the potential of CTRL framework on handling
longer and more complex sentences, we build a complex
Inside each video, we connect consecutive
sentence set.
sub-sentences to make complex query, each complex query
contains at least two sub-sentences, and is checked to make
sure that the time span is less than half of the video length.
We use them for test purpose only. In total, there are 13898
clip-sentence pairs in Charades-STA training set, 4233 clip-
sentence pairs in test set and 1378 complex sentence quires.

Figure 4. Charades-STA construction.

On average, there are 6.3 words per non-complex sentence,
and 12.4 words per complex sentence.

4.2. Experiment Settings

We will introduce evaluation metric, baseline methods

and our system variants in this part.

4.2.1 Evaluation Metric

We adopted a similar metric used by [8] to compute “R@n,
IoU=m”, which means that the percentage of at least one
of the top-n results having Intersection over Union (IoU)
larger than m. This metric itself is on sentence level, so
the overall performance is the average among all the sen-
tences. R(n, m) = 1
i=1 r(n, m, si), where r(n, m, si)
N
is the recall for a query si, N is total number of queries and
R(n, m) is the averaged overall performance.

(cid:80)N

4.2.2 Baseline Methods

We consider two sentence based image/video retrieval
baseline methods: visual-semantic alignment with LSTM
(VSA-RNN ) [9] and visual-semantic alignment with Skip-
thought vector (VSA-STV) [13]. For these two baseline
methods, we use the same training samples and test sliding
windows as those for CTRL.

VSA-RNN. This baseline method is similar to the model
in DVSA [9]. We use a regular LSTM instead of BRNN
to encode the input description. The size of hidden state
of LSTM is 1024 and the output size is 1000. Video
clips are processed by a C3D network that is pre-trained
on Sports1M [10]. The 4096 dimensional f c6 vector is
extracted and linearly transformed to 1000 dimensional,
which is used as the clip-level feature. Cosine similarity
is used to calculate the conﬁdence score between the clip
and the sentence. Hinge loss is used to train the model.

At test time, we compute the alignment score between in-
put sentence query and all the sliding windows in the video.
VSA-STV: Instead of using RNN to extract sentence em-
bedding, we use an off-the-shelf Skip-thought [13] sentence
embedding extractor. A skip-thought vector is 4800 dimen-
sional, we linearly transform it to 1000 dimensional. Visual
encoder is the same as for VSA-RNN.

Verb and Object Classiﬁers. We also implemented
baseline methods based on annotations of pre-deﬁned ac-
tions and objects. TACoS dataset also contains pre-deﬁned
actions and object annotations at clip-level. These ob-
jects and actions annotations are from the original MPII-
Compositive dataset [25]. 54 categories of actions and 81
categories of objects are involved in training set. We use
the same C3D feature as above to train action classiﬁers and
object classiﬁers. The classiﬁer is based on a 2-layer fully
connected network, the size of ﬁrst layer is 4094 and the
size of second layer is the number of categories. The test
sentences are parsed by Stanford CoreNLP [16], and verb-
object (VO) pairs are extracted using the sentence depen-
dencies. The VO pairs are matched with action and object
annotations based on string matching. The alignment score
between a sentence query and a clip is the score of matched
action and object classiﬁer responses. Verb means that we
only use action classiﬁer; Verb+Obj means that both action
classiﬁers and object classiﬁers are used.

4.2.3 System Variants

We experimented with variants of our system to test the ef-
fectiveness of our method. CTRL(aln): we don’t use re-
gression, train the CTRL with only alignment loss Laln.
train the CTRL with alignment loss Laln
CTRL(reg-p):
and parameterized regression loss Lreg−p. CTRL(reg-np):
context information is considered and CTRL is trained with
alignment loss Laln and non-parameterized regression loss
Lreg−np. CTRL(loc): SCNN [26] proposed to use overlap
loss to improve activity localization performance. Based on
our pure alignment(without regression), we implemented a
similar loss function considering clip overlap as in SCNN.
Lloc = (cid:80)n
−1)), where csi,i and IoUi
are respectively the alignment score and Intersection over
Union (IoU) between the aligned pairs of sentence and clip
in a mini-batch. The major difference is that SCNN solved a
classiﬁcation problem, so they use Softmax score, however
in our case, we consider an alignment problem. The over-
all loss function is Lscnn = Laln + Lloc. For this method,
we use C3D as the visual encoder and Skip-thought as the
sentence encoder.

i (0.5∗( 1/(1+e−csi,i )2

IoUi

4.3. Experiments on TACoS

In this part, we discuss the experiment results on TACoS.
First we compare the performance of different visual en-

Figure 5. Performance comparison of different visual encoders.

coders; second we compare two sentence embedding meth-
ods; third we compare the performance of CTRL variants
and baseline methods. The length of sliding windows for
test is 128 with overlap 0.8, multi-scale windows are not
used. We empirically set the context clip number n as 1 and
the length of context window as 128 frames. The dimension
of fv, fs and fsv are all set to 1000. We set batch size as
64, the networks are optimized by Adam [12] optimizer on
a Nvidia TITAN X GPU.

Comparison of visual features. We consider three clip-
level visual encoders: C3D [32], LRCN [3], VGG+Mean
Pooling [10]. Each of them takes a clip with 16 frames as
input and outputs a 1000-dimensional feature vector. For
C3D, f c6 feature vector is extracted and then linearly trans-
formed to 1000-dimension. For LRCN and VGG poolng,
we extract f c6 of VGG-16 for each frame. The LSTM’s
hidden state size is 256.We use Skip-thought as the sen-
tence embedding extractor and other parts of the model
are the same to CTRL(aln). There are three groups of
curves, which are Recall@10, Recall@5 and Recall@1 re-
spectively, shown in Figure. 5. We can see that C3D per-
forms generally better than other two methods. LRCN’s
performance is inferior, the reason maybe that the dataset is
relatively small, not enough to train the LSTM well.

Comparison of sentence embedding. For sentence
two commonly used methods:
encoder, we consider
word2vec+LSTM [8] and Skip-thought [13]. In our imple-
mentation of word2vec, we train skip-gram model on En-
glish Dump of Wikipedia. The dimension of the word vec-
tor is 500 and the hidden state size of the LSTM is 512. For
Skip-thought vector, we linearly transform it from 4800-
dimension to 1000-dimension. We use C3D as the visual
feature extractor and other parts are the same to CTRL(aln).
From the results, we can see that the performance of Skip-
thought is generally better than word2vec+LSTM. We con-
jecture the reason is that the scale of TACoS is not large
enough to train the LSTM (comparing with the counterpart

Table 1. Comparison of different methods on TACoS

Method

Random
Verb
Verb+Obj
VSA-RNN
VSA-STV
CTRL (aln)
CTRL (loc)
CTRL (reg-p)
CTRL (reg-np)

R@1
IoU=0.5
0.83
1.62
8.25
4.78
7.56
10.67
10.70
11.85
13.30

R@1
IoU=0.3
1.81
2.62
11.24
6.91
10.77
16.53
16.12
17.59
18.32

R@1
IoU=0.1
3.28
6.71
14.69
8.84
15.01
22.29
22.77
23.71
24.32

R@5
IoU=0.5
3.57
3.72
16.46
9.10
15.50
19.44
18.83
23.05
25.42

R@5
IoU=0.3
7.03
6.36
21.50
13.90
23.92
29.09
31.20
33.19
36.69

R@5
IoU=0.1
15.09
11.87
26.60
19.05
32.82
41.05
45.11
47.51
48.73

Method

Table 2. Comparison of different methods on Charades-STA
R@5
IoU=0.7
14.06
20.21
23.58
23.74
24.41
26.61
29.52

Random
VSA-RNN
VSA-STV
CTRL (aln)
CTRL (loc)
CTRL (reg-p)
CTRL (reg-np)

R@5
IoU=0.5
37.12
48.43
53.89
54.29
55.72
57.83
58.92

R@1
IoU=0.5
8.51
10.50
16.91
18.77
20.19
22.27
23.63

R@1
IoU=0.7
3.03
4.32
5.81
6.53
6.92
8.46
8.89

Table 3. Experiments of complex sentence query.

Method

Random
CTRL
CTRL+Fusion

R@1
IoU=0.5
11.83
24.09
25.82

R@1
IoU=0.7
3.21
8.03
8.32

R@5
IoU=0.5
43.28
69.89
69.94

R@5
IoU=0.7
18.17
32.28
32.81

should be ﬁrst normalized to some standard scale, but for
actions, time itself is the standard scale.

Some prediction and regression results are shown in Fig-
ure 7. We can see that the alignment prediction gives
a coarse location, which is limited by the ﬁxed window
length; the regression model helps to reﬁne the clip’s bound-
ing box to a higher IoU location.

4.4. Experiments on Charades-STA

In this part, we evaluate CTRL models and baseline
methods on Charades-STA and report the results for IoU ∈
{0.5, 0.7} and Recall@{1, 5}, which are shown in Table 2.
The lengths of sliding windows for test are 128 and 256,
It can be seen that the results
window’s overlap is 0.8.
are consistent with those in TACoS. CTRL(reg-np) shows
a signiﬁcant improvement over CTRL(aln) and CTRL(loc).
The non-parameterized settings (CTRL(reg-np)) work con-
sistently better than the parameterized settings (CTRL(reg-
p)). Figure 8 shows some prediction and regression results.
We also test complex sentence query on Charades-STA.
As shown in Table.
3, “CTRL” means that we sim-
ply input the whole complex sentence into CTRL model.
“CTRL+fusion” means that we input each sentence of a
complex query separately into CTRL, and then do a late fu-
sion. Speciﬁcally, we compute the average alignment score

Figure 6. Performance comparison of different sentence embed-
ding.

datasets in object detection, like ReferIt [11], Flickr30k En-
tities [20], which contains over 100k sentences).

Comparison with other methods. We test our system
variants and baseline methods on TACoS and report the re-
sult for IoU ∈ {0.1, 0.3, 0.5} and Recall@{1, 5}. The
results are shown in Table 1. “Random” means that we
randomly select n windows from the test sliding windows
and evaluate Recall@n with IoU=m. All methods use the
same C3D features. VSA-RNN uses the end-to-end trained
LSTM as the sentence encoder and all other methods use
pre-trained Skip-thought as sentence embedding extractor.
We can see that visual retrieval baselines (i.e. VSA-
RNN, VSA-STV) lead to inferior performance, even com-
pared with our pure alignment model CTRL(aln). We be-
lieve the major reasons are two-fold: 1) the multilayer align-
ment network learns better alignment than the simple cosine
similarity model, which is trained by hinge loss function; 2)
visual retrieval models do not encode temporal context in-
formation in a video. Pre-deﬁned classiﬁers also produce
inferior results. We think it is mainly because the pre-
deﬁned actions and objects are not precise enough to rep-
resent sentence queries. By comparing Verb and Verb+Obj,
we can see that additional object (such as “knife”, “egg”)
information helps to represent sentence queries.

Temporal action boundary regression As described
before, we implemented a temporal localization loss func-
tion similar to the one in SCNN [26], which consider clip
overlap. Experiment results show that CTRL(loc) does
not bring much improvement over CTRL(aln), perhaps be-
cause CTRL(loc) still relies on clip selection from sliding
windows, which may not overlap with ground truth well.
CTRL(reg-np) outperforms CTRL(aln) and CTRL(loc) sig-
niﬁcantly, showing the effectiveness of temporal regression
model. By comparing CTRL(reg-p) and CTRL(reg-np) in
Table 1, it can be seen that non-parameterized setting helps
the localizer regress the action boundary to a more accurate
location. We think the reason is that unlike objects can be
re-scaled in images due to camera projection, actions’ time
spans can not be easily rescaled in videos (we don’t consider
slow motion and quick motion). Thus, to do the boundary
regression effectively, the object bounding box coordinates

Figure 7. Alignment prediction and regression reﬁnement examples in TACoS. The row with gray background shows the ground truth for
the given query; the row with blue background shows the sliding window alignment results; the row with green background shows the clip
regression results.

Figure 8. Alignment prediction and regression reﬁnement examples in Charades-STA.

over all sentences, take the minimum of all start times and
maximum of all end times as start and end time of the com-
plex query. Although the random performance in Table. 3
(complex) is higher than that in Table 2 (single), the gain
over random performance remains similar, which indicates
that CTRL is able to handle complex query consisting mul-
tiple activities well. Comparing CTRL and CTRL+Fusion,
we can see that CTRL could be an effective ﬁrst step for
complex query, if combined with other fusion methods.

In general, we observed two types of common hard
cases: (1) long query sentences increase chances of failure,
likely because the sentence embeddings are not discrimi-
native enough; (2) videos that contain similar activities but
with different objects (e.g. in TACOS dataset, put a cucum-

ber on chopping board, and put a knife on chopping board)
are hard to distinguish amongst each other.

5. Conclusion

We addressed the problem of Temporal Activity Local-
ization via Language (TALL) and proposed a novel Cross-
modal Temporal Regression Localizer (CTRL) model,
which uses temporal regression for activity location reﬁne-
ment. We showed that non-parameterized offsets works
better than parameterized offsets for temporal boundary re-
gression. Experimental results show the effectiveness of our
method on TACoS and Charades-STA.

[21] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection.
In
CVPR, 2016.

[22] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele,
and M. Pinkal. Grounding action descriptions in videos.
Transactions of the Association for Computational Linguis-
tics, 1:25–36, 2013.

[23] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[24] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A
database for ﬁne grained activity detection of cooking activ-
ities. In CVPR, 2012.

[25] M. Rohrbach, M. Regneri, M. Andriluka, S. Amin,
M. Pinkal, and B. Schiele. Script data for attribute-based
recognition of composite activities. In ECCV, 2012.

[26] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
ization in untrimmed videos via multi-stage cnns. In CVPR,
2016.

[27] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev,
and A. Gupta. Hollywood in homes: Crowdsourcing data
collection for activity understanding. In ECCV, 2016.
[28] K. Simonyan and A. Zisserman. Two-stream convolutional

networks for action recognition in videos. In NIPS, 2014.

[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[30] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A
multi-stream bi-directional recurrent neural network for ﬁne-
grained action detection. In CVPR, 2016.

[31] C. Sun, C. Gan, and R. Nevatia. Automatic concept discov-
ery from parallel text and visual corpora. In ICCV, 2015.
[32] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015.

[33] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action recog-

nition by dense trajectories. In CVPR, 2011.

[34] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In CVPR, 2016.

[35] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal action
localization with pyramid of score distribution features. In
CVPR, 2016.

[36] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In ICCV, 2015.

References

[1] P. Bojanowski, R. Lajugie, E. Grave, F. Bach, I. Laptev,
J. Ponce, and C. Schmid. Weakly-supervised alignment of
video with text. In ICCV, 2015.

[2] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In CVPR, 2015.

[3] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015.

[4] J. Gao, C. Sun, and R. Nevatia. Acd: Action concept discov-
ery from image-sentence corpora. In ICMR. ACM, 2016.
[5] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia. Turn
tap: Temporal unit regression network for temporal action
proposals. arXiv preprint arXiv:1703.06189, 2017.

[6] R. Girshick. Fast r-cnn. In ICCV, 2015.
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[8] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016.
[9] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In CVPR, 2015.

[10] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[11] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014.

[12] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015.

[13] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun,
A. Torralba, and S. Fidler. Skip-thought vectors. In NIPS,
2015.

[14] D. Lin, S. Fidler, C. Kong, and R. Urtasun. Visual semantic
In
search: Retrieving videos via complex textual queries.
CVPR, 2014.

[15] S. Ma, L. Sigal, and S. Sclaroff. Learning activity progres-
sion in lstms for activity detection and early detection.
In
CVPR, 2016.

[16] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel,
S. Bethard, and D. McClosky. The stanford corenlp natu-
ral language processing toolkit. In ACL, 2014.

[17] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016.

[18] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013.

[20] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015.

6. Supplementary Material

To directly compare our the temporal regression method
with previous state-of-the-art methods on traditional ac-
tion detection task, we did additional experiments on
THUMOS-14.

Since THUMOS is a classiﬁcation task with limited
number of action classes, we removed the cross-modal part
and trained the localization network with classiﬁcation loss
(cross-entropy loss) and regression loss. We trained a model
on the validation set (train set only contains trimmed videos
which are not suitable for localization task) and tested it
on the test set. The regression model contains 20*2 out-
puts, corresponding to the 20 categories in the dataset, α
is set to 2.0 and 10.0 for non-parameterized and parame-
terized regression respectively. For each category, we use
NMS to eliminate redundant detections in every video, the
NMS threshold is set to (tIoU - delta), where tIoU = 0.5 and
delta=0.2. We report mAP at tIoU=0.5. For training sam-
ple generation, we use the same procedure as SCNN [24],
we set the high IoU threshold as 0.5 (SCNN used 0.7) and
low IoU threshold as 0.1 (SCNN used 0.3) for generating
training samples. Note that, our method and SCNN both
use C3D features.

Table 4. Temporal action localization experiments on THUMOS-
14

SCNN cls

reg-p reg-np reg-np (p+d)

mAP 19.0

16.3 18.9

19.8

20.5

As shown, “cls” for only using classiﬁcation loss, “reg-
p” for classiﬁcation loss+parameterized regression loss,
“reg-np” for classiﬁcation loss+ non-parameterized regres-
sion loss. For “cls”, “reg-p”,“reg-np”, we use the proposals
generated by SCNN (from their github codes) as input, so
that we can fairly compare the effect of classiﬁcation loss,
localization loss (used in SCNN) and temporal regression
loss. “reg-np (p+d)” means that we apply temporal regres-
sion on both proposal generation and action detection.

Our method (reg-np) outperforms SCNN. Comparing
with “cls” and “reg-np”, we can see the improvement by
the temporal regression. By applying temporal regression
on proposal generation, we can see a further improvement
from 19.8 to 20.5.

TALL: Temporal Activity Localization via Language Query

Jiyang Gao1

Chen Sun2
1University of Southern California

Zhenheng Yang1

2Google Research

Ram Nevatia1

{jiyangga, zhenheny, nevatia}@usc.edu, chensun@google.com

7
1
0
2
 
g
u
A
 
3
 
 
]

V
C
.
s
c
[
 
 
2
v
1
0
1
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

This paper focuses on temporal localization of actions
in untrimmed videos. Existing methods typically train clas-
siﬁers for a pre-deﬁned list of actions and apply them in
a sliding window fashion. However, activities in the wild
consist of a wide combination of actors, actions and ob-
jects; it is difﬁcult to design a proper activity list that meets
users’ needs. We propose to localize activities by natural
language queries. Temporal Activity Localization via Lan-
guage (TALL) is challenging as it requires: (1) suitable de-
sign of text and video representations to allow cross-modal
matching of actions and language queries; (2) ability to lo-
cate actions accurately given features from sliding windows
of limited granularity. We propose a novel Cross-modal
Temporal Regression Localizer (CTRL) to jointly model text
query and video clips, output alignment scores and action
boundary regression results for candidate clips. For evalu-
ation, we adopt TaCoS dataset, and build a new dataset for
this task on top of Charades by adding sentence temporal
annotations, called Charades-STA. We also build complex
sentence queries in Charades-STA for test. Experimental
results show that CTRL outperforms previous methods sig-
niﬁcantly on both datasets.

1. Introduction

Activities in the wild consist of a diverse combination
of actors, actions and objects over various periods of time.
Earlier work focused on classiﬁcation of video clips that
contained a single activity, i.e. where the videos were
trimmed. Recently, there has also been signiﬁcant work in
localizing activities in longer, untrimmed videos [30, 15].
One major limitation of existing action localization meth-
ods is that they are restricted to pre-deﬁned list of actions.
Although the lists of activities can be relatively large [2],
they still face difﬁculty in covering complex activity ques-
tions, for example, “A person runs to the window and then
look out.” , as shown in Figure 1. Hence, it is desirable to
use natural language queries to localize activities. Use of
natural language not only allows for an open set of activi-

Figure 1. Temporal activity localization via language query in an
untrimmed video.

ties but also natural speciﬁcation of additional constraints,
including objects and their properties as well as relations
between the involved entities. We propose the task of Tem-
poral Activity Localization via Language (TALL): given a
temporally untrimmed video and a natural language query,
the goal is to determine the start and end times for the de-
scribed activity inside the video.

For traditional temporal action localization, most current
approaches [30, 15, 26, 34, 35] apply activity classiﬁers
trained with optical ﬂow-based methods [33, 28] or Con-
volutional Neural Networks (CNNs) [29, 32] in a sliding
window fashion. A direct extension to support natural lan-
guage query is to map the queries into a discrete label space.
However, it is non-trivial to design a label space which has
enough coverage for such activities without losing useful
details in users’ queries.

To go beyond discrete activity labels, one possible solu-
tion is to embed visual features and sentence features into
a common space [10, 13, 18]. However, for temporal lo-
calization of activities, it is unclear what a proper visual
model to extract visual features for retrieval is, and how
to achieve high precision of predicted start/end time. Al-
though one could densely sample sliding windows at dif-
ferent scales, doing so is not only computationally expen-
sive but also makes the alignment task more challenging,
as the search space increases. An alternative to dense sam-
pling is to adjust the temporal boundaries of proposals by
learning regression parameters; such an approach has been
successful for object localization, as in [23]. However, tem-

poral regression has not been attempted in the past work
and is more difﬁcult as the activities are characterized by
a spatio-temporal volume, which may lead to more back-
ground noise.

These challenges motivate us to propose a novel Cross-
modal Temporal Regression Localizer (CTRL) model to
jointly model text query, video clip candidates and their
temporal context
information to solve the TALL task.
CTRL generates alignment scores along with location re-
gression results for candidate clips. It utilizes a CNN model
to extract visual features of the clips and a Long Short-
term Memory (LSTM) network to extract sentence embed-
dings. A cross-modal processing module is designed to
jointly model the text and visual features, which calculates
element-wise addition, multiplication and direct concatena-
tion. Finally, multilayer networks are trained for visual-
semantic alignment and clip location regression. We design
the non-parameterized and parameterized location offsets
for temporal coordinate regression. In parameterized set-
ting, the length and the central coordinate of the clip is ﬁrst
parameterized by the ground truth length and coordinate.
In non-parameterized setting, the start and end coordinates
are used directly. We show that the non-parameterized one
works better, unlike the case for object boundary regression.
To facilitate research of TALL, we also generate sen-
tence temporal annotations for Charades [27] dataset.
We name it Charades-STA.We evaluate our methods on
TACoS and Charades-STA datasets by the metric of “R@n,
IoU=m”, which represents the percentage of at least one of
the top-n results ( start and end pairs ) having IoU with the
ground truth larger than m. Experimental results demon-
strate the effectiveness of our proposed CTRL framework.

In summary, our contributions are two-fold:
(1) We propose a novel problem formulation of Temporal
Activity Localization via natural Language (TALL) query.
(2) We introduce an effective Cross-modal Temporal
Regression Localizer (CTRL) which estimates alignment
scores and temporal action boundary by jointly modeling
language query and video clips.1

2. Related Work

Action classiﬁcation and temporal localization. There
have been tremendous explorations about action classiﬁca-
tion in videos using deep convolutional neural networks
(ConvNets). Representative methods include two-stream
ConvNets, C3D (3D ConvNets) and 2D ConvNets with
temporal LSTM or mean pooling. Speciﬁcally, Simonyan
and Zisserman [28] modeled the appearance and motion
information in two separate ConvNets and combined the
scores by late fusion. Tran et al. [32] used 3D convolu-
tional ﬁlters to capture motion information in neighboring

1Source codes are available in https://github.com/jiyanggao/TALL .

frames. [36] [10] proposed to use 2D ConvNets to extract
deep features for one frame and use temporal mean pooling
or LSTM to model temporal information.

For temporal action localization task, Shou et al. [26]
trained C3D [32] with localization loss and achieved state-
of-the-art performance on THUMOS 14. Ma et al.
[15]
used a temporal LSTM to generate frame-wise prediction
scores and then merged the detection intervals based on
the predictions. Singh et al.
[30] extended two-stream
[28] framework with person detection and bi-directional
LSTMs and achieved state-of-the-art performance on MPII-
Cooking dataset [24]. Gao et al. [5] proposed to use tempo-
ral coordinate regression to reﬁne action boundary for tem-
poral localization.

Sentence-based image/video retrieval. Given a set of
candidate videos/images and a sentence query, this task re-
quires retrieving the videos/images that match the query.
Karpathy et al. [9] proposed Deep Visual-Semantic Align-
ment (DVSA) model. DVSA used bidirectional LSTMs to
encode sentence embeddings, and R-CNN object detectors
[7] to extract features from object proposals. Skip-thought
[13] learned a Sent2Vec model by applying skip-gram [19]
on sentence level and achieved top performance in sentence-
based image retrieval task. Sun et al. [31] proposed to dis-
cover visual concepts from image-sentence pairs and apply
the concept detectors for image retrieval. Gao et al.
[4]
proposed to learn verb-object pairs as action concepts from
image-sentence pairs. Hu et al. [8] and Mao et al. [17]
formulated the problem of natural language object retrieval.
[14] parsed the sen-
tence descriptions into a semantic graph, which are then
matched to visual concepts in the videos by generalized bi-
partite matching. Bojanowski et al. [1] tackled the problem
of video-text alignment: given a video and a set of sentences
with temporal ordering, assigning a temporal interval for
each sentence. In our settings, only one sentence query is
input to the system and temporal ordering is not used.

As for video retrieval, Lin et al.

Object detection. Our work is partly inspired by the
success of recent object detection approaches. R-CNN [7]
consists of selective search, CNN feature extraction, SVM
classiﬁcation and bounding box regression. Fast-RCNN [6]
designs RoI pooling layer and the model could be trained
by end-to-end framework. One of the key element shared
in those successful object detection frameworks [21, 23, 6]
is the bounding box regression layer. We show that, un-
like object boundary regression using parameterized offsets,
non-parameterized offsets work better for action boundary
regression.

3. Methods

In this section, we describe our Cross-modal Temporal
Regression Localizer (CTRL) for Temporal Activity Local-
ization via Language (TALL) and training procedure in de-

Figure 2. Cross-modal Temporal Regression Localizer (CTRL) architecture. CTRL contains four modules: a visual encoder to extract
features for video clips, a sentence encoder to extract embeddings, a multi-modal processing network to generate combined representations
for visual and text domain, and a temporal regression network to produce alignment scores and location offsets.

tail. CTRL contains four modules: a visual encoder to ex-
tract features for video clips, a sentence encoder to extract
embeddings, a multi-modal processing network to generate
combined representations for visual and text domain, and
a temporal regression network to produce alignment scores
and location offsets between the input sentence query and
video clips.

3.1. Problem Formulation

We denote a video as V = {ft}T

t=1, T is the frame num-
ber of the video. Each video is associated with temporal
sentence annotations: A = {(sj, τ s
j=1, M is the sen-
tence annotation number of the video V , sj is a natural lan-
guage sentence of a video clip, which has τ s
j as start
and end time in the video. The training data are the sen-
tence and video clip pairs. The task is to predict one or more
(τ s
j ) for the input natural language sentence query.

j and τ e

j )}M

j , τ e

j , τ e

3.2. CTRL Architecture

i , te

i )}H

i and te

Visual Encoder. For a long untrimmed video V , we gen-
erate a set of video clips C = {(ci, ts
i=1 by temporal
sliding windows, where H is the total number of the clips
of the video V , ts
i are the start and end time of video
clip ci. We deﬁne visual encoder as a function Fve(ci) that
maps a certain clip ci and its context to a feature vector fv,
whose dimension is ds.
Inside the visual encoder, a fea-
ture extractor Ev is used to extract clip-level feature vec-
tors, whose input is nf frames and output is a vector with
dimension dv. For one video clip ci, we consider itself (as
the central clip) and its surrounding clips (as context clips)
ci,q, q ∈ [−n, n], j is the clip shift, n is the shift boundary.

(cid:80)−1

We uniformly sample nf frames from each clip (central and
context clips). The feature vector of central clip is denoted
as f ctl
v . For the context clips, we use a pooling layer to cal-
v = 1
culate a pre-context feature f pre
q=−n Ev(ci,q) and
n
(cid:80)n
v = 1
post-context feature f post
q=1 Ev(ci,q). Pre-context
n
feature and post-context feature are pooled separately, as
the end and the start of an activity can be quite different and
, f ctl
both could be critical for temporal localization. f pre
v
and f post
are concatenated and then linearly transformed to
v
the feature vector fv with dimension ds, as the visual repre-
sentation for clip ci.

v

Sentence Encoder. A sentence encoder is a function
Fse(sj) that maps a sentence description sj to a embedding
space, whose dimension is ds( the same as visual feature
space ). Speciﬁcally, a sentence embedding extractor Es is
used to extract a sentence-level embedding f (cid:48)
s and is fol-
lowed by a linear transformation layer, which maps f (cid:48)
s to
fs with dimension ds, the same as visual representation fv.
We experiment two kinds of sentence embedding extractors,
one is a LSTM network which takes a word as input at each
step, and the hidden state of ﬁnal step is used as sentence-
level embedding; the other is an off-the-shelf sentence en-
coder, Skip-thought [13]. More details would be discussed
in Section 4.

Multi-modal Processing Module. The inputs of the
multi-modal processing module are a visual representation
fv and a sentence embedding fs, which have the same di-
mension ds. We use vector element-wise addition (+), vec-
tor element-wise multiplication (×) and vector concatena-
tion ((cid:107)) followed by a Fully Connected (F C) layer to com-
bine the information from both modalities. Addition and

multiplication operation allow additive and multiplicative
interaction between two modalities and don’t change the
feature dimension. The F C layer allows interaction among
all elements. The input dimension of the F C layer is 2 ∗ ds
and the output dimension is ds. The outputs from all three
operations are concatenated to construct a multi-modal rep-
resentation fsv = (fs × fv) (cid:107) (fs + fv) (cid:107) F C(fs (cid:107) fv),
which is the input for our core module, temporal localiza-
tion regression networks.

Temporal Localization Regression Networks. Tempo-
ral localization regression network takes the multi-modal
representation fsv as input, and has two sibling output lay-
ers. The ﬁrst one outputs an alignment score csi,j between
the sentence sj and the video clip ci. The second one out-
puts clip location regression offsets. We design two location
offsets, the ﬁrst one is parameterized offset: t = (tc, tl),
where tc and tl are parameterized central point offset and
length offset respectively. The parameterization is as fol-
lows:

tp = (p − pc)/lc, tl = log(l/lc)
where p and l denote the clip’s center coordinate and clip
length respectively. Variables p, pc are for predicted clip
and test clip (like wise for l). The second offset is non-
parameterized offset: t = (ts, te), where ts and te are the
start and end point offsets.

(1)

ts = s − sc, te = e − ec

(2)

where s and e denote the clip’s start and end coordinate re-
spectively. Temporal coordinate regression can be thought
as clip location regression from a test clip to a nearby
ground-truth clip, as the original clip could be either too
tight or too loose, the regression process tend to ﬁnd better
locations.

3.3. CTRL Training

Multi-task Loss Function. CTRL contains two sibling
output layers, one for alignment and the other for regres-
sion. We design a multi-task loss L on a mini-batch of
training samples to jointly train for visual-semantic align-
ment and clip location regression.

L = Laln + αLreg

(3)

where Laln is for visual-semantic alignment and Lreg is for
clip location regression, and α is a hyper-parameter, which
controls the balance between the two task losses. The align-
ment loss encourages aligned clip-sentence pairs to have
positive scores and misaligned pairs to have negative scores.

Laln =

[αclog(1 + exp(−csi,i))+

1
N

N
(cid:88)

i=0

N
(cid:88)

j=0,j(cid:54)=i

Figure 3. Intersection over Union (IoU) and non-Intersection over
Length (nIoL).

where N is the batch size, csi,j is the alignment score be-
tween sentence sj and video clip ci, αc and αw are the hy-
per parameters which control the weights between positive
( aligned ) and negative ( misaligned ) clip-sentence pairs.

The regression loss Lreg is calculated for the aligned
clip-sentence pairs. A sentence sj annotation contains start
and end time (τ s
j ). The aligned sliding window clip ci
has (ts
i ). The ground truth offsets t∗ are calculated from
i , te
start and end times.

j , τ e

Lreg =

[R(t∗

x,i − tx,i) + R(t∗

y,i − ty,i)]

(5)

1
N

N
(cid:88)

i=0

where x and y indicate p and l for parameterized offsets, or
s and e for non-parameterized offsets. R(t) is smooth L1
function.

h, te

h, te

h, τ e

h, τ e

h)]}NT

h), (ch, ts

Sampling Training Examples. To collect training sam-
ples, we use multi-scale temporal sliding windows with
[64, 128, 256, 512] frames and 80% overlap. (Note that,
at test time, we only use coarsely sampled clips.) We
use the following strategy to collect training samples T =
{[(sh, τ s
h=0. Each training sample con-
tains a sentence description (sh, τ s
h) and a video clip
(ch, ts
h). For a sliding window clip c from C with tem-
poral annotation (ts, te) and a sentence description s with
temporal annotation (τ s, τ e), we align them as a pair of
training samples if they satisfy (1) Intersection over Union
(IoU) is larger than 0.5; (2) non Intersection over Length
(nIoL) is smaller than 0.2 and (3) one sliding window clip
can be aligned with only one sentence description. The rea-
son we use nIoL is that we want the the most part of the
sliding window clip to overlap with the assigned sentence,
and simply increasing IoU threshold would harm regression
layers ( regression aims to move the clip from low IoU to
high IoU). As shown in Figure 3, although the IoU between
c and s1 is about 0.5, if we assign c to s1, then it will disturb
the model ,because c contains information of s2.

αwlog(1 + exp(csi,j))]

(4)

In this section, we describe the evaluation settings and

4. Evaluation

discuss the experiment results

4.1. Datasets

TACoS [22]. This dataset was built on the top of MPII-
Compositive dataset [25] and contains 127 videos. Every
video is associated with two type of annotations. The ﬁrst
one is ﬁne-grained activity labels with temporal location
(start and end time). The second set of annotations is natural
language descriptions with temporal locations. The natu-
ral language descriptions were obtained by crowd-sourcing
annotators, who were asked to describe the content of the
video clips by sentences. In total, there are 17344 pairs of
sentence and video clips. We split it in 50% for training,
25% for validation and 25% for test.

Charades-STA. Charades [27] contains around 10k
videos and each video contains temporal activity annota-
tion (from 157 activity categories) and multiple video-level
descriptions. TALL needs clip-level sentence annotation:
sentence descriptions with start and end time, which are
not provided in the original Charades dataset. We noticed
that the names of activity categories in Charades are parsed
from the video-level descriptions, so many of activity names
appear in descriptions. Another observation we make is
that most descriptions in Charades share a similar syntac-
tic structure: consisting of multiple sub-sentences, which
are connected by comma, period and conjunctions, such as
“then”, “while”, “after”, “and”. For example, “A person is
sitting down by the door. They stand up and start carefully
leaving some dishes in the sink”.

Based on these observations, we designed a semi-
automatic way to generate sentence temporal annotation.
The ﬁrst step is sentence decomposition: a long sentence
is split to sub-sentences by a set of conjunctions (which are
collected by hand ), and for each sub-sentence, the subject (
parsed by Stanford CoreNLP [16] ) of the original long sen-
tence is added to start. The second step is keyword match-
ing: we extract keywords for each activity categories and
match them to sub-sentences, if they are matched, the tem-
poral annotation (start and end time) are assigned to the sub-
sentences. The third step is a human check: for each pair
of sub-sentence and temporal annotation, we (two of the
co-authors) checked whether the sentence made sense and
whether they matched the activity annotation. An example
is shown in Figure 4.

Although TACoS and Charades-STA are challenging,
their lengths of queries are limited to single sentences.
To explore the potential of CTRL framework on handling
longer and more complex sentences, we build a complex
Inside each video, we connect consecutive
sentence set.
sub-sentences to make complex query, each complex query
contains at least two sub-sentences, and is checked to make
sure that the time span is less than half of the video length.
We use them for test purpose only. In total, there are 13898
clip-sentence pairs in Charades-STA training set, 4233 clip-
sentence pairs in test set and 1378 complex sentence quires.

Figure 4. Charades-STA construction.

On average, there are 6.3 words per non-complex sentence,
and 12.4 words per complex sentence.

4.2. Experiment Settings

We will introduce evaluation metric, baseline methods

and our system variants in this part.

4.2.1 Evaluation Metric

We adopted a similar metric used by [8] to compute “R@n,
IoU=m”, which means that the percentage of at least one
of the top-n results having Intersection over Union (IoU)
larger than m. This metric itself is on sentence level, so
the overall performance is the average among all the sen-
tences. R(n, m) = 1
i=1 r(n, m, si), where r(n, m, si)
N
is the recall for a query si, N is total number of queries and
R(n, m) is the averaged overall performance.

(cid:80)N

4.2.2 Baseline Methods

We consider two sentence based image/video retrieval
baseline methods: visual-semantic alignment with LSTM
(VSA-RNN ) [9] and visual-semantic alignment with Skip-
thought vector (VSA-STV) [13]. For these two baseline
methods, we use the same training samples and test sliding
windows as those for CTRL.

VSA-RNN. This baseline method is similar to the model
in DVSA [9]. We use a regular LSTM instead of BRNN
to encode the input description. The size of hidden state
of LSTM is 1024 and the output size is 1000. Video
clips are processed by a C3D network that is pre-trained
on Sports1M [10]. The 4096 dimensional f c6 vector is
extracted and linearly transformed to 1000 dimensional,
which is used as the clip-level feature. Cosine similarity
is used to calculate the conﬁdence score between the clip
and the sentence. Hinge loss is used to train the model.

At test time, we compute the alignment score between in-
put sentence query and all the sliding windows in the video.
VSA-STV: Instead of using RNN to extract sentence em-
bedding, we use an off-the-shelf Skip-thought [13] sentence
embedding extractor. A skip-thought vector is 4800 dimen-
sional, we linearly transform it to 1000 dimensional. Visual
encoder is the same as for VSA-RNN.

Verb and Object Classiﬁers. We also implemented
baseline methods based on annotations of pre-deﬁned ac-
tions and objects. TACoS dataset also contains pre-deﬁned
actions and object annotations at clip-level. These ob-
jects and actions annotations are from the original MPII-
Compositive dataset [25]. 54 categories of actions and 81
categories of objects are involved in training set. We use
the same C3D feature as above to train action classiﬁers and
object classiﬁers. The classiﬁer is based on a 2-layer fully
connected network, the size of ﬁrst layer is 4094 and the
size of second layer is the number of categories. The test
sentences are parsed by Stanford CoreNLP [16], and verb-
object (VO) pairs are extracted using the sentence depen-
dencies. The VO pairs are matched with action and object
annotations based on string matching. The alignment score
between a sentence query and a clip is the score of matched
action and object classiﬁer responses. Verb means that we
only use action classiﬁer; Verb+Obj means that both action
classiﬁers and object classiﬁers are used.

4.2.3 System Variants

We experimented with variants of our system to test the ef-
fectiveness of our method. CTRL(aln): we don’t use re-
gression, train the CTRL with only alignment loss Laln.
train the CTRL with alignment loss Laln
CTRL(reg-p):
and parameterized regression loss Lreg−p. CTRL(reg-np):
context information is considered and CTRL is trained with
alignment loss Laln and non-parameterized regression loss
Lreg−np. CTRL(loc): SCNN [26] proposed to use overlap
loss to improve activity localization performance. Based on
our pure alignment(without regression), we implemented a
similar loss function considering clip overlap as in SCNN.
Lloc = (cid:80)n
−1)), where csi,i and IoUi
are respectively the alignment score and Intersection over
Union (IoU) between the aligned pairs of sentence and clip
in a mini-batch. The major difference is that SCNN solved a
classiﬁcation problem, so they use Softmax score, however
in our case, we consider an alignment problem. The over-
all loss function is Lscnn = Laln + Lloc. For this method,
we use C3D as the visual encoder and Skip-thought as the
sentence encoder.

i (0.5∗( 1/(1+e−csi,i )2

IoUi

4.3. Experiments on TACoS

In this part, we discuss the experiment results on TACoS.
First we compare the performance of different visual en-

Figure 5. Performance comparison of different visual encoders.

coders; second we compare two sentence embedding meth-
ods; third we compare the performance of CTRL variants
and baseline methods. The length of sliding windows for
test is 128 with overlap 0.8, multi-scale windows are not
used. We empirically set the context clip number n as 1 and
the length of context window as 128 frames. The dimension
of fv, fs and fsv are all set to 1000. We set batch size as
64, the networks are optimized by Adam [12] optimizer on
a Nvidia TITAN X GPU.

Comparison of visual features. We consider three clip-
level visual encoders: C3D [32], LRCN [3], VGG+Mean
Pooling [10]. Each of them takes a clip with 16 frames as
input and outputs a 1000-dimensional feature vector. For
C3D, f c6 feature vector is extracted and then linearly trans-
formed to 1000-dimension. For LRCN and VGG poolng,
we extract f c6 of VGG-16 for each frame. The LSTM’s
hidden state size is 256.We use Skip-thought as the sen-
tence embedding extractor and other parts of the model
are the same to CTRL(aln). There are three groups of
curves, which are Recall@10, Recall@5 and Recall@1 re-
spectively, shown in Figure. 5. We can see that C3D per-
forms generally better than other two methods. LRCN’s
performance is inferior, the reason maybe that the dataset is
relatively small, not enough to train the LSTM well.

Comparison of sentence embedding. For sentence
two commonly used methods:
encoder, we consider
word2vec+LSTM [8] and Skip-thought [13]. In our imple-
mentation of word2vec, we train skip-gram model on En-
glish Dump of Wikipedia. The dimension of the word vec-
tor is 500 and the hidden state size of the LSTM is 512. For
Skip-thought vector, we linearly transform it from 4800-
dimension to 1000-dimension. We use C3D as the visual
feature extractor and other parts are the same to CTRL(aln).
From the results, we can see that the performance of Skip-
thought is generally better than word2vec+LSTM. We con-
jecture the reason is that the scale of TACoS is not large
enough to train the LSTM (comparing with the counterpart

Table 1. Comparison of different methods on TACoS

Method

Random
Verb
Verb+Obj
VSA-RNN
VSA-STV
CTRL (aln)
CTRL (loc)
CTRL (reg-p)
CTRL (reg-np)

R@1
IoU=0.5
0.83
1.62
8.25
4.78
7.56
10.67
10.70
11.85
13.30

R@1
IoU=0.3
1.81
2.62
11.24
6.91
10.77
16.53
16.12
17.59
18.32

R@1
IoU=0.1
3.28
6.71
14.69
8.84
15.01
22.29
22.77
23.71
24.32

R@5
IoU=0.5
3.57
3.72
16.46
9.10
15.50
19.44
18.83
23.05
25.42

R@5
IoU=0.3
7.03
6.36
21.50
13.90
23.92
29.09
31.20
33.19
36.69

R@5
IoU=0.1
15.09
11.87
26.60
19.05
32.82
41.05
45.11
47.51
48.73

Method

Table 2. Comparison of different methods on Charades-STA
R@5
IoU=0.7
14.06
20.21
23.58
23.74
24.41
26.61
29.52

Random
VSA-RNN
VSA-STV
CTRL (aln)
CTRL (loc)
CTRL (reg-p)
CTRL (reg-np)

R@5
IoU=0.5
37.12
48.43
53.89
54.29
55.72
57.83
58.92

R@1
IoU=0.5
8.51
10.50
16.91
18.77
20.19
22.27
23.63

R@1
IoU=0.7
3.03
4.32
5.81
6.53
6.92
8.46
8.89

Table 3. Experiments of complex sentence query.

Method

Random
CTRL
CTRL+Fusion

R@1
IoU=0.5
11.83
24.09
25.82

R@1
IoU=0.7
3.21
8.03
8.32

R@5
IoU=0.5
43.28
69.89
69.94

R@5
IoU=0.7
18.17
32.28
32.81

should be ﬁrst normalized to some standard scale, but for
actions, time itself is the standard scale.

Some prediction and regression results are shown in Fig-
ure 7. We can see that the alignment prediction gives
a coarse location, which is limited by the ﬁxed window
length; the regression model helps to reﬁne the clip’s bound-
ing box to a higher IoU location.

4.4. Experiments on Charades-STA

In this part, we evaluate CTRL models and baseline
methods on Charades-STA and report the results for IoU ∈
{0.5, 0.7} and Recall@{1, 5}, which are shown in Table 2.
The lengths of sliding windows for test are 128 and 256,
It can be seen that the results
window’s overlap is 0.8.
are consistent with those in TACoS. CTRL(reg-np) shows
a signiﬁcant improvement over CTRL(aln) and CTRL(loc).
The non-parameterized settings (CTRL(reg-np)) work con-
sistently better than the parameterized settings (CTRL(reg-
p)). Figure 8 shows some prediction and regression results.
We also test complex sentence query on Charades-STA.
As shown in Table.
3, “CTRL” means that we sim-
ply input the whole complex sentence into CTRL model.
“CTRL+fusion” means that we input each sentence of a
complex query separately into CTRL, and then do a late fu-
sion. Speciﬁcally, we compute the average alignment score

Figure 6. Performance comparison of different sentence embed-
ding.

datasets in object detection, like ReferIt [11], Flickr30k En-
tities [20], which contains over 100k sentences).

Comparison with other methods. We test our system
variants and baseline methods on TACoS and report the re-
sult for IoU ∈ {0.1, 0.3, 0.5} and Recall@{1, 5}. The
results are shown in Table 1. “Random” means that we
randomly select n windows from the test sliding windows
and evaluate Recall@n with IoU=m. All methods use the
same C3D features. VSA-RNN uses the end-to-end trained
LSTM as the sentence encoder and all other methods use
pre-trained Skip-thought as sentence embedding extractor.
We can see that visual retrieval baselines (i.e. VSA-
RNN, VSA-STV) lead to inferior performance, even com-
pared with our pure alignment model CTRL(aln). We be-
lieve the major reasons are two-fold: 1) the multilayer align-
ment network learns better alignment than the simple cosine
similarity model, which is trained by hinge loss function; 2)
visual retrieval models do not encode temporal context in-
formation in a video. Pre-deﬁned classiﬁers also produce
inferior results. We think it is mainly because the pre-
deﬁned actions and objects are not precise enough to rep-
resent sentence queries. By comparing Verb and Verb+Obj,
we can see that additional object (such as “knife”, “egg”)
information helps to represent sentence queries.

Temporal action boundary regression As described
before, we implemented a temporal localization loss func-
tion similar to the one in SCNN [26], which consider clip
overlap. Experiment results show that CTRL(loc) does
not bring much improvement over CTRL(aln), perhaps be-
cause CTRL(loc) still relies on clip selection from sliding
windows, which may not overlap with ground truth well.
CTRL(reg-np) outperforms CTRL(aln) and CTRL(loc) sig-
niﬁcantly, showing the effectiveness of temporal regression
model. By comparing CTRL(reg-p) and CTRL(reg-np) in
Table 1, it can be seen that non-parameterized setting helps
the localizer regress the action boundary to a more accurate
location. We think the reason is that unlike objects can be
re-scaled in images due to camera projection, actions’ time
spans can not be easily rescaled in videos (we don’t consider
slow motion and quick motion). Thus, to do the boundary
regression effectively, the object bounding box coordinates

Figure 7. Alignment prediction and regression reﬁnement examples in TACoS. The row with gray background shows the ground truth for
the given query; the row with blue background shows the sliding window alignment results; the row with green background shows the clip
regression results.

Figure 8. Alignment prediction and regression reﬁnement examples in Charades-STA.

over all sentences, take the minimum of all start times and
maximum of all end times as start and end time of the com-
plex query. Although the random performance in Table. 3
(complex) is higher than that in Table 2 (single), the gain
over random performance remains similar, which indicates
that CTRL is able to handle complex query consisting mul-
tiple activities well. Comparing CTRL and CTRL+Fusion,
we can see that CTRL could be an effective ﬁrst step for
complex query, if combined with other fusion methods.

In general, we observed two types of common hard
cases: (1) long query sentences increase chances of failure,
likely because the sentence embeddings are not discrimi-
native enough; (2) videos that contain similar activities but
with different objects (e.g. in TACOS dataset, put a cucum-

ber on chopping board, and put a knife on chopping board)
are hard to distinguish amongst each other.

5. Conclusion

We addressed the problem of Temporal Activity Local-
ization via Language (TALL) and proposed a novel Cross-
modal Temporal Regression Localizer (CTRL) model,
which uses temporal regression for activity location reﬁne-
ment. We showed that non-parameterized offsets works
better than parameterized offsets for temporal boundary re-
gression. Experimental results show the effectiveness of our
method on TACoS and Charades-STA.

[21] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection.
In
CVPR, 2016.

[22] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele,
and M. Pinkal. Grounding action descriptions in videos.
Transactions of the Association for Computational Linguis-
tics, 1:25–36, 2013.

[23] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[24] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A
database for ﬁne grained activity detection of cooking activ-
ities. In CVPR, 2012.

[25] M. Rohrbach, M. Regneri, M. Andriluka, S. Amin,
M. Pinkal, and B. Schiele. Script data for attribute-based
recognition of composite activities. In ECCV, 2012.

[26] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
ization in untrimmed videos via multi-stage cnns. In CVPR,
2016.

[27] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev,
and A. Gupta. Hollywood in homes: Crowdsourcing data
collection for activity understanding. In ECCV, 2016.
[28] K. Simonyan and A. Zisserman. Two-stream convolutional

networks for action recognition in videos. In NIPS, 2014.

[29] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[30] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A
multi-stream bi-directional recurrent neural network for ﬁne-
grained action detection. In CVPR, 2016.

[31] C. Sun, C. Gan, and R. Nevatia. Automatic concept discov-
ery from parallel text and visual corpora. In ICCV, 2015.
[32] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015.

[33] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action recog-

nition by dense trajectories. In CVPR, 2011.

[34] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In CVPR, 2016.

[35] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal action
localization with pyramid of score distribution features. In
CVPR, 2016.

[36] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In ICCV, 2015.

References

[1] P. Bojanowski, R. Lajugie, E. Grave, F. Bach, I. Laptev,
J. Ponce, and C. Schmid. Weakly-supervised alignment of
video with text. In ICCV, 2015.

[2] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In CVPR, 2015.

[3] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015.

[4] J. Gao, C. Sun, and R. Nevatia. Acd: Action concept discov-
ery from image-sentence corpora. In ICMR. ACM, 2016.
[5] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia. Turn
tap: Temporal unit regression network for temporal action
proposals. arXiv preprint arXiv:1703.06189, 2017.

[6] R. Girshick. Fast r-cnn. In ICCV, 2015.
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[8] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016.
[9] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In CVPR, 2015.

[10] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[11] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014.

[12] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015.

[13] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun,
A. Torralba, and S. Fidler. Skip-thought vectors. In NIPS,
2015.

[14] D. Lin, S. Fidler, C. Kong, and R. Urtasun. Visual semantic
In
search: Retrieving videos via complex textual queries.
CVPR, 2014.

[15] S. Ma, L. Sigal, and S. Sclaroff. Learning activity progres-
sion in lstms for activity detection and early detection.
In
CVPR, 2016.

[16] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel,
S. Bethard, and D. McClosky. The stanford corenlp natu-
ral language processing toolkit. In ACL, 2014.

[17] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016.

[18] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep captioning with multimodal recurrent neural networks
(m-rnn). In ICLR, 2015.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013.

[20] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015.

6. Supplementary Material

To directly compare our the temporal regression method
with previous state-of-the-art methods on traditional ac-
tion detection task, we did additional experiments on
THUMOS-14.

Since THUMOS is a classiﬁcation task with limited
number of action classes, we removed the cross-modal part
and trained the localization network with classiﬁcation loss
(cross-entropy loss) and regression loss. We trained a model
on the validation set (train set only contains trimmed videos
which are not suitable for localization task) and tested it
on the test set. The regression model contains 20*2 out-
puts, corresponding to the 20 categories in the dataset, α
is set to 2.0 and 10.0 for non-parameterized and parame-
terized regression respectively. For each category, we use
NMS to eliminate redundant detections in every video, the
NMS threshold is set to (tIoU - delta), where tIoU = 0.5 and
delta=0.2. We report mAP at tIoU=0.5. For training sam-
ple generation, we use the same procedure as SCNN [24],
we set the high IoU threshold as 0.5 (SCNN used 0.7) and
low IoU threshold as 0.1 (SCNN used 0.3) for generating
training samples. Note that, our method and SCNN both
use C3D features.

Table 4. Temporal action localization experiments on THUMOS-
14

SCNN cls

reg-p reg-np reg-np (p+d)

mAP 19.0

16.3 18.9

19.8

20.5

As shown, “cls” for only using classiﬁcation loss, “reg-
p” for classiﬁcation loss+parameterized regression loss,
“reg-np” for classiﬁcation loss+ non-parameterized regres-
sion loss. For “cls”, “reg-p”,“reg-np”, we use the proposals
generated by SCNN (from their github codes) as input, so
that we can fairly compare the effect of classiﬁcation loss,
localization loss (used in SCNN) and temporal regression
loss. “reg-np (p+d)” means that we apply temporal regres-
sion on both proposal generation and action detection.

Our method (reg-np) outperforms SCNN. Comparing
with “cls” and “reg-np”, we can see the improvement by
the temporal regression. By applying temporal regression
on proposal generation, we can see a further improvement
from 19.8 to 20.5.

