Fast-deepKCF Without Boundary Effect

Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, Hanqing Lu
National Lab of Pattern Recognition, Institute of Automation, CAS, Beijing 100190, China
University of Chinese Academy of Sciences, Beijing, China
{linyu.zheng, tangm, yingying.chen, jqwang, luhq}@nlpr.ia.ac.cn

Abstract

Shift ‐100

Shift ‐50

Base Sample

Shift +50

Shift +100

In recent years, correlation ﬁlter based trackers (CF
trackers) have received much attention because of their top
performance. Most CF trackers, however, suffer from low
frame-per-second (fps) in pursuit of higher localization ac-
curacy by relaxing the boundary effect or exploiting the
high-dimensional deep features. In order to achieve real-
time tracking speed while maintaining high localization ac-
curacy, in this paper, we propose a novel CF tracker, fd-
KCF*, which casts aside the popular acceleration tool, i.e.,
fast Fourier transform, employed by all existing CF track-
ers, and exploits the inherent high-overlap among real (i.e.,
noncyclic) and dense samples to efﬁciently construct the k-
ernel matrix. Our fdKCF* enjoys the following three ad-
vantages. (i) It is efﬁciently trained in kernel space and spa-
tial domain without the boundary effect. (ii) Its fps is almost
independent of the number of feature channels. Therefore,
it is almost real-time, i.e., 24 fps on OTB-2015, even though
the high-dimensional deep features are employed. (iii) Its
localization accuracy is state-of-the-art. Extensive exper-
iments on four public benchmarks, OTB-2013, OTB-2015,
VOT2016, and VOT2017, show that the proposed fdKCF*
achieves the state-of-the-art localization performance with
remarkably faster speed than C-COT and ECO.

1. Introduction

Visual object tracking is one of the fundamental prob-
lems in computer vision with many applications.
In the
model free tracking problem, the goal is to estimate the s-
tates (e.g., position and size) of the target in a whole image
sequence only with the initial frame [45, 46]. Model free
tracking is very challenging because the tracker has to learn
the robust appearance model from a very limited training
samples to resist extremely challenging interference, such
as occlusions, large appearance changes, illumination vari-
ation, fast motion, and background clutters. In general, the
key problem of model free tracking is how to construct a
tracker which can not only tolerate appearance variation

virtual
Shift ‐100

virtual
Shift ‐50

real
Base Sample

virtual
Shift +50

virtual
Shift +100

virtual
Learning Region

real
Learning Region

real
Learning Region

real
Learning Region

virtual
Learning Region

real

real

real

real

real

Figure 1: Comparison of sampling methods in KCF [22] (1st row),
BACF [24] (2nd row), and our fdKCF* (last row). Training sam-
ples of KCF come from the cyclic shift of a base sample (i.e. learn-
ing region), and they are all virtual except for the base one. BACF
obtains its training samples with target size (cyan boxes) by clip-
ping the middle parts of all training samples of KCF, and some
of them are virtual. Different from them, in fdKCF*, training
samples with target size (red boxes) are densely sampled from the
learning region in the traditional sliding window way, and they are
all real. We call our sampling method as real and dense sampling.

of target, but also exclude background interference, while
maintaining the processing speed as fast as possible.

In recent years, correlation ﬁlter based trackers (CF
trackers) have received much attention because of their top
performances. Since MOSSE [3], almost all CF trackers
[22, 39, 18, 28, 10, 1, 12, 13, 14, 24, 8, 31] have been rely-
ing on fast Fourier transform (FFT) to accelerate their com-
putations. Unfortunately, while modern CF trackers’ local-
ization accuracies continue to improve, their fps’ become
lower and lower. We believe that the following two reasons
cause this phenomenon. On the one hand, to improve the
robustness of tracking algorithm itself, some representative
CF trackers [12, 24] exploit techniques to relax the bound-
ary effect which is introduced by FFT [3]. These tech-
niques, however, inevitably destroy entire cyclicity of the
training samples, resulting in much slower training speed.
On the other hand, to improve the robustness of features to

4020

Learning Region

Search Region

Region size:

Sample size:

Figure 2: Illustration of redundant computations in the construc-
tion of linear kernel matrix by the brute-force approach. The col-
or bounding boxes are three pairs of real and dense samples, •
denotes dot product, and the red pentagram and the red dot are
shared C-dimensional feature vectors by the solid and dotted line
samples, respectively. The two samples of identical color are con-
sidered as a pair because the relative position of the red pentagram
in the solid sample is the same as that of the red dot in the dotted
sample. We have to calculate the dot product of the red pentagram
and the red dot three times in this example. In practice, we need
to calculate the above dot product h × w times when the dense
and real samples and the brute-force approach are employed. In
contrast, our fCKM calculates it only once.

appearance variations of targets, deep features which are al-
ways high-dimensional are employed by modern CF track-
ers [30, 14, 8, 11, 37, 5]. Although these trackers beneﬁt
from deep features remarkably, their computational costs
increase signiﬁcantly. Particularly, C-COT [14] which not
only relaxes the boundary effect but also employs deep fea-
tures can run at only 0.3 fps on GPU. Further, despite lots
of techniques have been employed to accelerate its compu-
tation, ECO [8] can run at only 6 fps. Naturally, it is asked
whether or not we can design a CF tracker which is able to
efﬁciently relax or even avoid the boundary effect inherent-
ly, i.e., does not employ FFT to accelerate its training, and
efﬁciently exploit deep features at the same time?

To solve the above problem, in this paper, we propose
a novel CF tracker, fdKCF* which not only has not the
boundary effect inherently, but also can run in real-time
even though deep features are employed. First, we in-
troduce the real and dense sampling method to avoid the
boundary effect radically. As shown in Fig. 1, this sampling
method is based on the traditional sliding window where all
training samples are real, and it is different from the cyclic
shift based sampling method used in existing CF trackers,
such as KCF [22] and BACF [24], where training set con-
tains virtual samples, resulting in the negative boundary ef-
fect. Second, we design a novel algorithm, fCKM, to con-
struct the kernel matrix in spatial domain efﬁciently even if
the high-dimensional deep features are employed after in-
vestigating the inherent high-overlap among real and dense
samples. Finally, a Gauss-Seidel based iterative method is
employed to efﬁciently optimize in dual space.

It is observed that there exist vast redundant computa-
tions in the construction of kernel matrix by using the brute-
force approach because of the high-overlap among real and
dense samples. Take the linear kernel as an example. Giv-
en the H × W × C feature maps of learning region and

detection region where C is the number of channels, we
have to calculate the dot product of any two C-dimensional
feature vectors, which come from the two feature maps re-
spectively, K times where K is the number of pairs of sam-
ples which contain the above two feature vectors respec-
tively. Indeed, this dot product need to be calculated only
once. Fig. 2 shows an example.
Inspired by this obser-
vation, we propose a novel algorithm, fCKM, which can
construct the linear kernel matrix efﬁciently by eliminating
redundant calculations, i.e., the dot product of any two C-
dimensional feature vectors is only calculated once, instead
of K times. fCKM conducts the following two steps to con-
struct the matrix of linear kernel: (i) building the dot prod-
uct value table of any two C-dimensional feature vectors
which come from the input two feature maps respectively;
(ii) obtaining each element in the linear kernel matrix by
looking up the table and summing. This way, the redundant
calculations of dot product of any two C-dimensional fea-
ture vectors can be replaced with looking up table with time
complexity O(1) rather than O(C) in brute-force approach.
Consequently, fCKM enjoys the following two advantages:
(i) It is performed in spatial domain without the boundary
effect. (ii) Its running speed is fast and insensitive to the
number of feature channels. In our experiments, only a few
milliseconds are taken to construct the matrix of linear ker-
nel even though the number of feature channels up to 1024.
Additionally, it can also be employed to construct many ma-
trices of typical non-linear kernels with very little increase
in time-consuming by modifying the ﬁrst step and adding
non-linear mapping after the second step.

Experiments are performed on four public benchmark-
s: OTB-2013, OTB-2015, VOT2016, and VOT2017. Our
fdKCF* achieves the state-of-the-art localization perfor-
mance, while running at 24 fps. As a fair comparison, when
C-COT [14] and fdKCF* employ deep features of the same
dimensional, run on same GPU, and do not use any other ac-
celeration techniques, the localization accuracy of fdKCF*
is higher than that of C-COT, while the mean fps of fdKCF*
is about 80 times that of C-COT. To the best of our knowl-
edge, our fdKCF* is the ﬁrst CF tracker which achieves
both high localization accuracy and real-time speed.

2. Related Work

For the ﬁrst time, KCF [21, 22] establishes the relation-
ship between correlation ﬁlters and ridge regression. Com-
pared to MOSSE, KCF is modeled in kernel or dual space
and it can make use of multi-channel features without an
increase in parameters. Besides, another important contri-
bution of KCF is the fast calculation of kernel matrix in
frequency domain. In order to improve the localization per-
formance of KCF, HCF [30] introduces the higher dimen-
sional deep features into KCF, however, the boundary effect
become a bottleneck for its localization performance. Our

4021

fdKCF* is also modeled in kernel or dual space. It is, how-
ever, mainly different from KCF and HCF in two aspects.
First, samples of our fdKCF* are real (i.e., noncyclic) and
dense ones, rather than the cyclic shifts of a real base sam-
ple as in KCF and HCF (See Fig. 1). In other words, there
is no boundary effect inherently in our fdKCF*, whereas
KCF and HCF suffer from it. Therefore, the localization
performance of our fdKCF* exceeds HCF with a large mar-
gin (about 10 percent point on OTB-2013 and OTB-2015).
Second, our fdKCF* is accelerated in the spatial domain by
exploiting the inherent high-overlap among real and dense
samples, rather than in frequency domain as done in KCF
and HCF. It is worth noting that compared to HCF, the track-
ing speed of our fdKCF* is much faster than its (24fps vs.
11fps on GPU) even though the search region of fdKCF* is
larger than HCF (4 vs. 1.8 times target size) and fdKCF*
does not exploit the cyclic samples structure to accelerate.
By exploiting the real and dense samples, LSART [37]
solves for the dual variables of the KCF with linear kernel
through propagating messages forward and backward in a
network. It does not construct the kernel matrix explicitly.
Different from LSART, our fdKCF* constructs the kernel
matrix ﬁrst, and then solve for the dual variables by an iter-
ative method. The efﬁciency of our fdKCF* is remarkably
higher than that of LSART for the following two reasons.
First, in order to solve for the dual variables, every update
for them requires to propagate messages forward and back-
ward in the network of LSART, and this is time-consuming.
Whereas in our fdKCF*, we only construct the kernel ma-
trix once by our fCKM, then solve for the dual variables by
an iterative method. Both steps are efﬁcient. Second, only
the ﬁrst-order convergence method like SGD can be em-
ployed in LSART, while more efﬁcient ones such as Gauss-
Seidel can be used in our fdKCF*. It is worth noting that
LSART can only employ the linear kernel, whereas non-
linear kernels can also be employed in our fdKCF*.

In addition, Siamese networks based trackers [2, 27, 20]
achieved state-of-the-art performance in recent years. They
treat tracking as a similarity learning problem and train their
models by vast ofﬂine data. For completeness, we also com-
pare our fdKCF* with typical ones in the experiment.

3. KCF without Boundary Effect

×

×

W

Let X ∈ RH

C and Z ∈ RH

We will start with the kernel ridge regression problem,
and suggest readers referring to [22, 44] for the relation be-
tween the ridge regression and the kernel ridge regression.
C be the fea-
ture maps of learning region and search region, respec-
tively, where H and W are the height and width of the
feature maps, respectively, C is the number of channels.
All training samples {xi}N
C are
sampled from X, as shown in Fig.3, and all test samples
{zi}N
C are sampled from Z in the

i=1 where xi ∈ Rh

i=1 where zi ∈ Rh

W

×

×

×

×

×

×

w

w

Learning Region

feature map

Feature 
Extraction

sample

xi | i = 1...

Figure 3: Sampling in feature map X (green cuboid). Sam-
W ...N o are obtained by using real and dense
ples n
f
W = W − w + 1 and
sampling method (see Fig. 1) where
N = (H − h + 1) × (W − w + 1). See Sec.3 for details.
f

same way, where h ≤ H and w ≤ W are the height
and width of the feature map of target, respectively, and
N = (H − h + 1) × (W − w + 1). Further, we deﬁne the
kernel matrix KZX as follows:

KZX = 

κ (z1, x1)
...
κ (zN , x1)

· · ·
. . .
· · ·

κ (z1, xN )
...
κ (zN , xN )

,



(1)



where κ (·, ·) is a kernel. KXX is the Gram matrix with
respect to {xi}N
In the rest of this pa-
per, we will use KL
ZX to indicate the linear and
Gaussian kernel matrices with κL (zi, xj) = hzi, xji and
κG (zi, xj) = g(zi, xj) as their elements, respectively,
where g(·, ·) is Gaussian function.

i=1 if Z = X.
ZX and KG




According to Eq.(1), the optimization problem of KCF
without the boundary effect (KCF*) can be formulated in
dual space as

min
α

ky − KXXαk2

2 + λαT KXXα,

(2)

where y = [y1, y2, ..., yN ] is the vector of gaussian labels, λ
is the regularization parameter, and α ∈ RN
1 is the vector
of dual variables. The optimization solution of Problem (2)
can be expressed as

×

α∗ = (KXX + λI)−

(3)
Further, given X and α∗, the process of detection in Z can
be expressed as

1 y.

f (Z) = KZXα∗.
(4)
It is clear that in order to calculate α∗ in Eq.(3), we have
to construct KXX with {xi}N
i=1 ﬁrst. Constructing KXX,
however, is extremely time-consuming when dense samples
and deep features are employed where N and C are gener-
ally large. For example, when KL
XX is constructed, its each
element κ(xi, xj) has to be calculated by the formula

κ(xi, xj) = hxi, xji
w−1

h−1

C−1

X

=

X
m=0

X
n=0

X
d=0

⌊i/

f
W ⌋+m,(i mod

f
W )+n,d

(5)

· X

⌊j/

f
W ⌋+m,(j mod

W )+n,d,
f

4022

f
N 2Chw

W = W − w + 1 and Xp,q,d is the element of X
where
at the p-th row, q-th column and d-th channel. Therefore,
the time complexity of constructing KL
XX with Eq.(5) is
, and so is KL
ZX. Suppose H = βh, H = W ,
O
h = w, and replace N with (H −h+1)×(W −w+1). Then,
1.
the above complexity can be simpliﬁed to O
It is noted that this complexity is extremely high because
h and C often belong to 10 and 103 orders of magnitude,
respectively, when deep features are employed.

Cβ4h6

(cid:0)

(cid:0)

(cid:1)

(cid:1)

4. Fast Calculation of Kernel Matrix (fCKM)

In this section, we ﬁrst introduce our novel algorithm
fCKM for efﬁcient construction of KZX, then show how it
works with linear kernel and Gaussian kernel as two special
cases. Finally, the analysis of complexities of fCKM and its
comparison with the brute-force approach is presented.

4.1. fCKM for General Kernels

Our fCKM can construct the kernel matrix efﬁciently

where kernel κ (·, ·) can be expressed as

h

1

w

1

C

1

−

−

−

φ (zm,n,d, xm,n,d)

,

(6)

κ (z, x) = ψ

 

!

n=0
X

m=0
X

Xd=0
where ψ (·) and φ (·, ·) are two functions with time com-
C and
plexities O (γ) and O (η), respectively, z ∈ Rh
zm,n,d is the element of z at the m-th row, n-th column and
d-th channel. We call such κ(·, ·) as (ψ, φ) kernel.

×

×

w

) and the relative position of Zm,n,

It can be observed from Fig. 2 and Fig. 3 that most
elements of any sample are also elements of its spatial-
ly adjacent ones for real and dense samples. Such large
shared elements will lead to large redundant computations
in constructing KZX of (ψ, φ) kernel with brute-force ap-
proach, i.e., calculating κ(zi, xj)’s with Eq.(6). This is be-
cause there are K pairs of samples, (z, x)’s, which contain
(Zm,n,
in z is
, Xi,j,
the same as that of Xi,j,
in x, where K ∈ [1, h × w] and
Zm,n,
is the C-dimensional feature vector at the m-th row
C
and n-th column of Z, leading to
d=0 φ (Zm,n,d, Xi,j,d)
−
has to calculate K times. In order to reduce these redundant
computations, we design a novel algorithm fCKM to con-
struct KZX of (ψ, φ) kernel efﬁciently. Our fCKM consists
of the following three steps.
(1) Building Base Table. The base table T ∈ RHW
of (ψ, φ) kernel is constructed with expression:

P

HW

×

∗

∗

∗

∗

∗

1

T (i, j) =

φ

Z⌊j/W ⌋,j mod W,d, X⌊i/W ⌋,i mod W,d(cid:1)
(cid:0)

.

(7)

C−1

X
d=0

C
d=0 φ (Zm,n,d, Xi,j,d) for all
Consequently, T contains
−
(m, n, i, j), and any one is calculated only once. The time
P
complexity of this step is O

ηCβ4h4 + Cβ4h4

1

.

1Here, we use β instead of β − 1 for convenience in this paper, and this

is reasonable because β ∈ [4, 5] in general [14, 18, 12, 8, 11, 13].

(cid:1)

(cid:0)

(2) Constructing Summation Matrix. The summation
matrix S ∈ RN
N is constructed through looking up the
base table T and summing. Speciﬁcally,

×

S (i, j) =

T (p, q) ,

(8)

h

1

w

1

−

−

m=0
X

n=0
X

where

p = (⌊i/

W ⌋ + m) × W + (i mod

W ) + n,

W ) + n,
f

(9)

q = (⌊j/

W ⌋ + m) × W + (j mod
f
W = W − w + 1.
f

f
1 (κ (zi, xj)) for all (i, j). The

f

Consequently, S (i, j) = ψ−
time complexity of this step is O
(3) Mapping. KZX ∈ RN
tained by mapping S with function ψ(·). Speciﬁcally,

N of (ψ, φ) kernel can be ob-

β4h6

(cid:0)

(cid:1)

×

.

KZX (i, j) = ψ (S (i, j)) .

(10)

Consequently, KZX (i, j) = κ(zi, xj) for all (i, j). The
time complexity of this step is O
According to above steps,

γβ4h4
1
C
d=0 φ (Zm,n,d, Xi,j,d) for
−
(cid:0)
all (m, n, i, j) is calculated only once, rather than K times,
in constructing KZX of (ψ, φ) kernel, resulting in the high
efﬁciency of our fCKM.

P

(cid:1)

.

Last, we would like to discuss the key difference be-
tween KII [23] and our fCKM. In short, KII needs to val-
idate whether the contribution function satisﬁes the neces-
sary and sufﬁcient condition, and it can only accelerate the
ﬁltering of a single ﬁlter, rather than a group of highly over-
lapping ﬁlters. Whereas, our fCKM focus on accelerating
the ﬁltering of such a group of ﬁlters.

4.2. fCKM for Linear Kernel Matrix

(ψ, φ) kernel is linear if ψ (x) = x and φ (x, y) = xy.
Therefore, the linear kernel matrix KL
ZX can be constructed
efﬁciently with fCKM. Speciﬁcally, according to Sec.4.1,
KL
ZX can be constructed through the following two steps.
(1) Building the base table TL as follows.

TL (i, j) =

Z⌊j/W ⌋,j mod W,dX⌊i/W ⌋,i mod W,d.

(11)

C−1

X
d=0

(2) Constructing the summation matrix SL with Eq.(8),
where T is replaced with TL in Eq. 11. Finally, KL
ZX = SL
because ψ (x) = x.

4.3. fCKM for Gaussian Kernel Matrix

Not only the linear kernel, but also many commonly used
non-linear kernels, such as Gaussian, multi-quadric, and
sigmoid ones, are (ψ, φ) kernel. As the most commonly
used one in CF trackers, Gaussian kernel is used to show
how fCKM works to construct the Gaussian kernel matrix.

4023

− √x
σ2

(ψ, φ) kernel is Gaussian if ψ (x) = exp

and
φ (x, y) = (x − y)2. Therefore, the Gaussian kernel matrix
KG
ZX can be constructed efﬁciently with fCKM. Speciﬁcal-
ly, according to Sec.4.1, KG
ZX can be constructed through
the following three steps.
(1) Building the base table TG as follows.

(cid:16)

(cid:17)

C

1

−

Xd=0

TG (i, j) =

Z

⌊

j/W

⌋

,j mod W,d − X

i/W

,i mod W,d

⌊

⌋

2

.

(cid:0)

(cid:1)
(12)
(2) Constructing the summation matrix SG with Eq.(8),
where T is replaced with TG in Eq. 12.
(3) Mapping SG to KG

ZX with ψ (x) = exp

, i.e.,

− √x
σ2

KG

ZX (i, j) = exp

−

(cid:17)

(cid:16)
SG (i, j)
σ2

.

!

 

p

(13)

4.4. Complexity Analysis

According to Sec. 4.1,

In this section, we analyze and compare the time and
space complexities of our fCKM against those of the brute-
force approach in detail when the linear kernel is employed.
the time complexities of the
brute-force approach with Eq.(6) and our fKCM in con-
structing the kernel matrix KZX of (ψ, φ) kernel are
O(ηCβ4h6 + Cβ4h6 + γβ4h4) and O(ηCβ4h4 + Cβ4h4 +
β4h6 + γβ4h4),
ZX can
be constructed in the time complexities O
and
by the brute-force approach with E-
O
q.(5) and our fKCM, respectively, because the time com-
plexity of φ (·, ·) and ψ (·) are O (1) and O (0), respectively,
i.e., η = 1 and γ = 0. Further, their proportional relation is

Therefore, KL
Cβ4h6

Cβ4h4 + β4h6

respectively.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Cβ4h6
Cβ4h4 + β4h6 =

Ch2
C + h2 =

h2
1 + h2/C

(14)

In practice, when the high-dimensional deep features are
employed, C > h2 ≫ h ≫ 1. Therefore, the time com-
plexities of constructing KL
ZX by brute-force approach is
about h2 times that of our fKCM.

Fig. 4 shows the effect of our fCKM on the reduction of
computational costs compared to the brute-force approach
with common H = W = 60 and h = w = 15 which
are the case if the cell size (stride) of features is 4 × 4. It
can be concluded from the ﬁgure that, when the number of
channels increases, the increment of FLOPs with fCKM is
much slower than that with the brute-force approach, and
the more the number of channels is, the more the accelera-
tion of fCKM is. Therefore, our fCKM can construct KL
ZX
efﬁciently even though the high-dimensional deep features
are exploited. For example, on a TITAN X GPU, when typ-
ically H = W = 60 and h = w = 15, the execution time of
the Step (1) only increases 8µs with C increasing 1, and it

×1011

FLOPs of Linear Kernel

brute-force method
our fCKM

s
P
O
L
F

14

12

10

8

6

4

2

0

0

100

200

300
Number of Feature Channels

400

500

600

Figure 4: Comparison between the amount of computations of our
fCKM and the brute-force method with Eq.(5) in calculating the
linear kernel matrix KL

ZX with H = W = 60 and h = w = 15.

always takes 4ms to perform Step (2) whatever C is. There-
fore, when H = W = 60, h = w = 15, and C = 600, the
time-consuming of our fCKM is about 8.8ms, whereas that
of brute-force approach is about 600ms.
On the other hand, fCKM has O

higher space
complexity than the brute-force approach. This requirement
of extra space is used to store the base table T. However,
it is negligible on the current GPU and RAM. For exam-
ple, fCKM only requires 50MB more than the brute-force
approach does under H = W = 60 and ﬂoat data type.

H 2W 2

(cid:0)

(cid:1)

5. Fast-deepKCF without Boundary Effect

5.1. Fast Training

N 3

In Sec. 3, we have shown how to efﬁciently construc-
t KXtXt by our fCKM, where Xt is the X in frame t.
Achieving optimal α∗t by directly using Eq.(3), however,
is time-consuming, because the time complexity of matrix
. Even though α∗t is achieved through
inversion is O
solving a system of linear equations with Gauss elimination
method, the complexity is still O( 1
3 N 3). In order to achieve
α∗t more efﬁciently, we adopt the iterative approach [12]
based on the Gauss-Seidel. Speciﬁcally, we decompose
KXtXt + λI into a lower triangular Lt and a strictly upper
triangular Ut, i.e., KXtXt + λI = Lt + Ut. Then, α∗t can
be efﬁciently solved by the following iterative expressions:

(cid:1)

(cid:0)

α∗

α∗

(j)

t ← α∗t
−
(j)
t ← Lt \

1,

j = 0,

(15a)

y − Utα∗
t

(j

1)

−

,

j > 0,

(15b)

(cid:16)

(cid:17)

where j indicates the number of iterations, and α∗t
1 is the
optimal solution at frame t − 1. In practice, 5 iterations are
enough for the satisfactory solution α∗t . Note that this it-
erative method is efﬁcient because Eq. (15b) can be solved
efﬁciently with forward substitution, and the time complex-
ity of each iterations is O(N 2).

−

As a comparison to KCF, it is easy to know that the
time complexities of constructing kernel matrix and solving
Cβ2h2 log (βh)
linear system are O
and O (N ), respec-
tively, in KCF, being signiﬁcantly lower than those of our

(cid:0)

(cid:1)

4024

fdKCF*. This is because KCF exploits circulant samples
which cause the negative boundary effect.

5.2. Update

To robustly locating the target object, updating the ap-
pearance model of a tracker is often necessary. Similar to
other CF trackers [18, 38, 22], we update Xt in Sec. 5.1 by
means of the following linear weighting approach, i.e.,

X1 =

X1,

Xt = (1 − δ) Xt
e

−

1 + δ

Xt,

t > 1,

(16)

Xt is the actual sampled feature map of learning re-

where
gion in frame t, and δ is the learning rate.

e

e

5.3. Fast Multi scale Detection

Scale-pyramid based method [28] is employed to locate
the target object and estimate its proper scale simultane-
ously. Speciﬁcally, given X, α∗, and the scale-pyramid
Zi
of feature maps of detection regions, where S is
the level of the scale-pyramid, the fast detection of target
(cid:8)
object on each scale can be expressed as

S
i=1

(cid:9)

f

Zi

= KZiXα∗,

∀i.

(17)

Note that any (ψ, φ) kernel can be used in Eq.(17).

(cid:1)

(cid:0)

Particularly, if (ψ, φ) kernel is linear, the optimal scale
as well as the target location can be achieved with a more
efﬁcient approach. That is,

w =

α∗(i)xi,

N

i=1
X

ˆf

Zi

= ˆZi ⊙ ˆw∗,

∀i,

(18a)

(18b)

(cid:1)

(cid:0)

where α∗(i) is the i-th element of α∗ and ˆ• denotes the
DFT of • in H × W dimension. Before conducting DFT
for •, 0’s are padded to •’s bottom right to make its dimen-
sionality be H × W if the dimensionality of • is less than
H × W . Eq.(18) is more efﬁcient than Eq.(17) in the case
of multi-scale detection with linear (ψ, φ) kernel because
Eq.(18a) is executed only once.

6. Experiments

We evaluate our fdKCF* on four public benchmark-
s, OTB-2013 [45], OTB-2015 [46], VOT2016 [26] and
VOT2017 [25], and compare its performance with the state-
of-the-art and representative trackers. All parameters of fd-
KCF* are kept consistent in all experimental comparisons.

6.1. Implementation Details

Platform. Our fCKM is implemented in C++, and the
rest of our fdKCF* is implemented in PyTorch [33]. Exper-
iments are performed on Linux with single TITAN X GPU.

Features. Similar to C-COT [14], our fdKCF* only em-
ploys deep features to show performances of the algorithm
itself. Speciﬁcally, we adopt the VGG-M-BN [4] trained on
ImageNet [16] for feature extraction. We ﬁrst change orig-
inal strides of Conv-2 and Conv-5 from 2 to 1 to improve
the localization accuracy. Then, the output maps of Conv-1
followed by an average pooling layer with kernel size 2 × 2
and stride 2 × 2 are employed as shallow level features (96
channels), and the output maps of Conv-5 followed by a 2×
bilinear interpolation layer are employed as deep level fea-
tures (512 channels). As a result, both shallow level features
and deep level features are of 4 × 4 cell size (stride).

Parameters. We set different learning rates (Sec. 5.2)
for shallow level features and deep level features. Speciﬁ-
cally, δ = δs = 0.01 for shallow ones and δ = δd = 0.005
for deep ones. The regularization parameter λ in Eq. 2 is set
to 0.01. The maximum number of iterations j in Eq. 15 is
set to 5. Similarly to SRDCF [12], we set the image area of
the square sampling region to 42 times the target area, and
it is re-scaled to the area of 2002 if its area is less than 2002,
and to the area of 2402 if its area is greater than 2402.

Scaling.

In order to balance the localization accuracy
and tracking speed, we set 5 levels scale-pyramid (Sec. 5.3).
Kernel. We only employ the linear kernel in fdKCF* in
our current experiments. The reasons are (1) most of state-
of-the-art CF trackers, such as BACF [18] and ECO [8],
can only employ the linear kernel, (2) fdKCF* with linear
kernel runs slightly faster than that with Gaussian kernel.

6.2. Evaluation on OTB datasets

In our OTB-2013 and OTB-2015 experiments, we com-
pare our fdKCF* with state-of-the-art CF trackers and non-
CF trackers, respectively. When comparing with CF track-
ers, following the standard benchmark protocols in the
OTB-2015 [46], all trackers are quantitatively evaluated
by ﬁve metrics, namely precision plot, success plot, dis-
tance precision (DP), overlap precision (OP), and AUC. In
addition, as pointed out in [32], the deﬁnition of DP in
OTB-2015 is defective because it is sensitive to the size
of bounding boxes, and they propose the normalized preci-
sion, Pnorm, to measure the localization accuracy. Based on
their work, we evaluate all trackers with Pnorm@0.2 which
is computed as the percentage of frames in a video where
Pnorm is smaller than 0.2. When comparing with non-CF
trackers, all trackers are quantitatively evaluated by AUC
metric because they all reported AUCs in their original pa-
pers and there is no way to obtain the detailed tracking re-
sults of some of them to evaluate them with other metrics.

Comparion with CF trackers. We divide state-of-the-
art CF trackers into two groups for a thorough comparison.
The ﬁrst group consists of seven trackers which can run
at real-time speed, i.e. beyond 20 fps. These trackers are
MKCFup [39], BACF [18], ECO-HC [8], LCT [31], Sta-

4025

i

i

n
o
s
c
e
r
P
p
a
l
r
e
v
O

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
p
a
l
r
e
v
O

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
p
a
l
r
e
v
O

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

i

i

n
o
s
c
e
r
P
p
a
l
r
e
v
O

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
p
a
l
r
e
v
O

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
e
c
n
a
t
s
D

 

i

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
e
c
n
a

 

t
s
D

i

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Precision Plot on OTB-2013

Success Plot on OTB-2013

Precision Plot on OTB-2015

Success Plot on OTB-2015

fdKCF* [0.908]
BACF [0.858]
ECO-HC [0.856]
LCT [0.845]
MKCFup [0.837]
CFNet [0.769]
Staple [0.762]
DSST [0.736]

fdKCF* [0.705]
BACF [0.669]
ECO-HC [0.657]
MKCFup [0.640]
LCT [0.602]
Staple [0.592]
CFNet [0.592]
DSST [0.564]

fdKCF* [0.891]
ECO-HC [0.845]
BACF [0.819]
Staple [0.770]
CFNet [0.765]
LCT [0.756]
MKCFup [0.742]
DSST [0.689]

fdKCF* [0.675]
ECO-HC [0.649]
BACF [0.631]
CFNet [0.592]
MKCFup [0.581]
Staple [0.581]
LCT [0.533]
DSST [0.528]

0

5

10 15 20 25 30 35 40 45 50
Center Error Threshold

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

0

5

Overlap Threshold

10 15 20 25 30 35 40 45 50
Center Error Threshold

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Overlap Threshold

(a) Comparison with real-time CF trackers.

Precision Plot on OTB-2013

Success Plot on OTB-2013

Precision Plot on OTB-2015

Success Plot on OTB-2015

ECO [0.910]
fdKCF* [0.908]
C-COT [0.887]
HCF [0.887]
GPRT [0.867]
SRDCFdecon [0.850]
SRDCF [0.835]
deepSRDCF [0.826]

ECO [0.711]
fdKCF* [0.705]
C-COT [0.678]
GPRT [0.677]
SRDCFdecon [0.654]
deepSRDCF [0.641]
SRDCF [0.639]
HCF [0.614]

ECO [0.897]
fdKCF* [0.891]
C-COT [0.890]
GPRT [0.842]
deepSRDCF [0.837]
HCF [0.837]
SRDCFdecon [0.812]
SRDCF [0.782]

ECO [0.697]
C-COT [0.679]
fdKCF* [0.675]
GPRT [0.655]
deepSRDCF [0.640]
SRDCFdecon [0.632]
SRDCF [0.607]
HCF [0.573]

0

5

10 15 20 25 30 35 40 45 50
Center Error Threshold

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

0

5

Overlap Threshold

10 15 20 25 30 35 40 45 50
Center Error Threshold

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Overlap Threshold

(b) Comparison with high localization performance CF trackers.

Figure 5: The mean precision and success plots of our fdKCF* and (a) seven modern real-time CF trackers, (b) seven modern CF trackers
that produce state-of-the-art localization accuracy, on OTB-2013 and OTB-2015, respectively. The mean distance precisions and AUCs are
reported in the legends. fdKCF* outperforms all other real-time CF trackers by large margins.

Precision Plot on Very Hard Sequences

Success Plot on Very Hard Sequences

fdKCF* MKCFup BACF ECO-HC LCT Staple DSST CFNet
0.739 0.721 0.673 0.742
0.815
0.630 0.691 0.615 0.731
0.782
0.773 0.711 0.649 0.725
0.772
0.691 0.720 0.628 0.728
0.762

0.841
0.776
0.816
0.771

0.784
0.689
0.760
0.684

mOP-13 0.884
mOP-15 0.828
mPN-13 0.846
mPN-15 0.820

(a) Comparison with real-time CF trackers.

mOP-13
mOP-15
mPN-13
mPN-15
mFPS-15

fdKCF* C-COT ECO GPRT HCF SRDCF decon deep
0.799 0.779
0.759 0.765
0.772 0.740
0.753 0.755
<1

0.821 0.871 0.841 0.741 0.785
0.816 0.842 0.791 0.661 0.728
0.782 0.832 0.818 0.783 0.748
0.805 0.819 0.793 0.735 0.713
5
0.3

0.884
0.828
0.846
0.820
24

11

1

8

6

(b) Comparison with high localization performance CF trackers.

Table 1: The mean OP (mOP) and mean Pnorm@0.2 (mPN) of
our fdKCF* and (a) seven modern real-time CF trackers, (b) seven
modern CF trackers that produce state-of-the-art localization ac-
curacy, on OTB-2013 and OTB-2015. decon and deep are denote
SRDCFdecon and deepSRDCF, respectively. The best three re-
sults on each line are shown in red, blue and magenta, respectively.
fdKCF* outperforms all other real-time CF trackers by large mar-
gins. In addition, we also report the mean fps’ (mFPS) of trackers
in (b) on OTB-2015, and fdKCF* is visibly faster than them.

ple [1], DSST [9], and CFNet [41]. The second group is
formed by other seven trackers which produce state-of-the-
art localization accuracy but can not run at real-time. These
trackers are ECO [8], C-COT [14], deepSRDCF [11], SRD-
CFdecon [13], SRDCF [12], GPRT [48], and HCF [30].

Fig. 5a and Table 1a show the comparison of our fdKCF*
with the ﬁrst group of CF trackers. As shown in Fig. 5a,
our fdKCF* obtains the mean DP and AUC of 90.8% and
70.5% on OTB-2013, as well as 89.1% and 67.5% on OTB-
2015, respectively, leading the second best trackers with a
signiﬁcant gain of 5.0%, 3.6%, 4.6% and 2.6%, respective-

fdKCF* [0.789]
C-COT [0.751]
ECO [0.717]
HCF [0.676]
deepSRDCF [0.598]
GPRT [0.540]
SRDCFdecon [0.523]
SRDCF [0.442]

fdKCF* [0.495]
C-COT [0.478]
ECO [0.463]
HCF [0.374]
deepSRDCF [0.372]
GPRT [0.358]
SRDCFdecon [0.343]
SRDCF [0.287]

5

10

20

15

25
Center Error Threshold

30

35

40

45

50

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Overlap Threshold

1

Figure 6: The mean precision and success plots of our fdKCF* and
seven modern CF trackers that produce state-of-the-art localization
accuracy on the very hard sequences [19] of OTB-2015. The mean
distance precisions and AUCs are reported in the legends.

ly. In addition, as shown in Table 1a, our fdKCF* obtains
the mean Pnorm@0.2 and mean OP of 84.6% and 88.4%
on OTB-2013, as well as 82.0% and 82.8% on OTB-2015,
respectively, leading the second best trackers with a sig-
niﬁcant gain of 3.0%, 4.3%, 4.9% and 4.6%, respective-
ly. These results show that our fdKCF* signiﬁcantly out-
performs all other state-of-the-art real-time CF trackers in
localization performance. We believe that this is because
our efﬁcient fdKCF* not only avoids the boundary effect
inherently, but also utilizes the powerful deep features.

Fig. 5b and Table 1b show the comparison of our fd-
KCF* with the second group of CF trackers. Judging from
the mean DPs and AUCs as shown in Fig. 5b, the local-
ization performance of our fdKCF* is comparable to that
of C-COT and slightly worse than that of ECO. However,
judging from the mean Pnorm@0.2 and mean OP as shown
in Table 1b, the localization performance of our fdKCF* is
competitive with that of ECO and obviously better than that
of C-COT. Besides, as shown in the last line of Table 1b,
the mean fps of our fdKCF* is visibly higher than those of

i

i

n
o
s
c
e
r
P
e
c
n
a
t
s
D

 

i

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

i

i

n
o
s
c
e
r
P
e
c
n
a

 

t
s
D

i

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P
 
e
c
n
a
t
s
D

i

0

0

4026

where
CVPR2016
SINT+
CVPR2018
SINT++
CVPR2018
RASNet
SASiam
CVPR2018
SiamRPN CVPR2018
DaSiamRPN ECCV2018
StruckSiam ECCV2018
ICCV2017
CREST
ECCV2018
DSLT
NIPS2018
DAT
ICCV2017
PTAV
TRACA
CVPR2018
FlowTrack CVPR2018
CVPR2018
LSART
CVPR2018
VITAL
ours
fdKCF*

AUC-2013 AUC-2015 Real-Time
0.571
0.574
0.642
0.657
0.636
0.658
0.621
0.623
0.660
0.668
0.635
0.603
0.655
0.672
0.682
0.675

0.655
0.624
0.670
0.677
0.658
0.655
0.638
0.673
0.683
0.704
0.663
0.652
0.689
0.701
0.710
0.705

no
no
yes
yes
yes
yes
yes
no
no
no
yes
yes
no
no
no
yes

Table 2: The AUCs of our fdKCF* and other state-of-the-art non-
CF trackers on OTB-2013 and OTB-2015. The best two results are
shown in red and blue, respectively.

all trackers in the second group (including ECO), although
there is not sparse update and feature dimension reduction,
which are employed by ECO to accelerate, in fdKCF*.

We believe that the following three reasons cause the lo-
calization performance of our fdKCF* to be slightly worse
than that of ECO. (1) The clustering algorithm GMM em-
ployed by ECO improves its robust, whereas there is no
similar component in our fdKCF*. (2) The self-adaptive di-
mensionality reduction and weighting different features are
used in ECO, whereas our fdKCF* does not use the similar
components. (3) Both deep features and hand-crafted fea-
tures, i.e., HOG [7] and Color-Names [15], are employed in
ECO, whereas our fdKCF* only employs deep features. In
fact, hand-crafted features can improve the localization per-
formance of trackers on the sequences with easy challenges.
To illustrate this fact, Han, et al. [19] divide the sequences
of OTB-2015 into three sets: easy, hard, and very hard,
according to the localization performance of most state-of-
the-art trackers. It is not hard to ﬁnd that trackers employed
hand-crafted features always achieve high localization per-
formances on the easy sequences. To further illustrate the
above point, we show the comparison of our fdKCF* with
the second group of CF trackers on the very hard sequences
in Fig. 6.
It is seen that the localization performance of
our fdKCF* is obviously better than that of all other CF
trackers (including ECO) on the very hard set. Note that C-
COT, which mainly use deep features, also outperforms oth-
er trackers except fdKCF*. It is concluded that employing
hand-crafted features may not improve but weaken the lo-
calization performance of trackers on very hard sequences.
Comparion with non-CF trackers. We compare
fdKCF* with state-of-the-art non-CF trackers,
includ-
ing SINT+ [40], SINT++ [43], RASNet [42], SASi-
am [20], SiamRPN [27], DaSiamRPN [49], StruckSi-
am [47], CREST [35], DSLT [29], DAT [34], PTAV [17],
TRACA [6], FlowTrack [50], LSART [37], and VI-

EAO on VOT2016

*

fdKCF*: 24 fps

C-COT: 0.3 fps

0.347

0.331

EAO on VOT2017

0.303

CFWCR: 4 fps
CFCF: 1 fps 0.281
ECO: 6 fps
CCOT: 0.3 fps 0.265

0.267

0.286

fdKCF*: 24 fps

*

Figure 7: Expected average overlap on VOT2016 and VOT2017.
Best trackers are closer to the top-right corner.

TAL [36], on OTB-2013 and OTB-2015. Table 2 shows the
results. It is seen that the localization accuracy of our fd-
KCF* outperforms most non-CF trackers, and outperforms
all other real-time ones.

6.3. Evaluation on VOT datasets

We present the evaluation results on VOT2016 [26] and
VOT2017 [25] datasets which contain 60 sequences, re-
spectively. We follow the VOT challenge protocol to com-
pare trackers, where mainly reports the expected average
overlap (EAO) and rank trackers based on it.

Fig. 7 shows the EAO ranking plots where we compare
our fdKCF* against the top-15 CF trackers on VOT2016
and VOT2017, respectively. The performances of these
trackers come from the VOT report. On the whole, the EAO
score of our fdKCF* is competitive to that of C-COT, which
is the winner of VOT2016 challenge, and slightly worse
than that of ECO. However, the tracking speed of our fd-
KCF* is visibly faster than those of C-COT and ECO. These
conclusions are consistent with those obtained on the OTB
datasets. Note that CFWCR and CFCF are improved ver-
sions based on ECO and C-COT, respectively. On the con-
trary, our fdKCF* is a novel tracking framework without
any further improvements and tricks.

7. Conclusions and Future work

A novel CF tracker, fdKCF*, with the state-of-the-art lo-
calization accuracy and real-time speed is proposed in this
fdKCF* achieves the state-of-the-art accuracy be-
paper.
cause there is no boundary effect with it and powerful deep
features are also employed. fdKCF* is able to run at real-
time speed because a novel acceleration method, fCKM, is
developed in spatial domain. Through exploiting the inher-
ent high-overlap among real and dense samples, fCKM is
able to construct the kernel matrix efﬁciently even though
the high-dimensional deep features are employed. Future
work can be found in the supplementary material.
Acknowledgement. This work was supported by National Natural
Science Foundation of China under Grants 61772527, 61806200,
and 61702510.

4027

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip HS Torr. Staple: Complementary learn-
ers for real-time tracking. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1401–1409, 2016.

[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
In European conference on
networks for object tracking.
computer vision, pages 850–865. Springer, 2016.

[3] David S Bolme, J Ross Beveridge, Bruce A Draper, and
Yui Man Lui. Visual object tracking using adaptive corre-
In 2010 IEEE Computer Society Conference
lation ﬁlters.
on Computer Vision and Pattern Recognition, pages 2544–
2550. IEEE, 2010.

[4] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and An-
drew Zisserman. Return of the devil in the details: Delv-
arXiv preprint arX-
ing deep into convolutional nets.
iv:1405.3531, 2014.

[5] Kai Chen and Wenbing Tao. Convolutional regression for
IEEE Transactions on Image Processing,

visual tracking.
27(7):3611–3620, 2018.

[6] Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo
Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, and
Jin Young Choi. Context-aware deep feature compression for
high-speed visual tracking. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
479–488, 2018.

[7] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
In Computer Vision and Pat-
dients for human detection.
tern Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on, volume 1, pages 886–893. IEEE, 2005.
[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan,
Michael Felsberg, et al. Eco: Efﬁcient convolution opera-
tors for tracking. In CVPR, volume 1, page 3, 2017.

[9] Martin Danelljan, Gustav H¨ager, Fahad Khan, and Michael
Felsberg. Accurate scale estimation for robust visual track-
In British Machine Vision Conference, Nottingham,
ing.
September 1-5, 2014. BMVA Press, 2014.

[10] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and
Michael Felsberg. Discriminative scale space tracking. IEEE
transactions on pattern analysis and machine intelligence,
39(8):1561–1575, 2017.

[11] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Convolutional features for correlation ﬁl-
ter based visual tracking. In Proceedings of the IEEE Inter-
national Conference on Computer Vision Workshops, pages
58–66, 2015.

[12] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Learning spatially regularized correlation
ﬁlters for visual tracking. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 4310–4318,
2015.

[13] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Adaptive decontamination of the training
set: A uniﬁed formulation for discriminative visual tracking.

In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1430–1438, 2016.

[14] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation ﬁlters: Learn-
ing continuous convolution operators for visual tracking. In
European Conference on Computer Vision, pages 472–488.
Springer, 2016.

[15] Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg,
and Joost Van de Weijer. Adaptive color attributes for real-
time visual tracking. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1090–
1097, 2014.

[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009.

[17] Heng Fan and Haibin Ling. Parallel tracking and verifying:
A framework for real-time and high accuracy visual track-
In Proceedings of the IEEE International Conference
ing.
on Computer Vision, pages 5486–5494, 2017.

[18] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, pages 1144–1152, 2017.

[19] Bohyung Han, Jack Sim, and Hartwig Adam. Branchout:
Regularization for online ensemble tracking with convolu-
tional neural networks. In Proceedings of IEEE International
Conference on Computer Vision, pages 2217–2224, 2017.

[20] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A
twofold siamese network for real-time object tracking.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4834–4843, 2018.

[21] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. Exploiting the circulant structure of tracking-by-
detection with kernels. In European conference on computer
vision, pages 702–715. Springer, 2012.

[22] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters. IEEE transactions on pattern analysis and machine in-
telligence, 37(3):583–596, 2014.

[23] Mohamed Hussein, Fatih Porikli, and Larry Davis. Kernel
integral images: A framework for fast non-uniform ﬁltering.
In 2008 IEEE Conference on Computer Vision and Pattern
Recognition, pages 1–8. IEEE, 2008.

[24] Hamed Kiani Galoogahi, Terence Sim, and Simon Lucey.
Correlation ﬁlters with limited boundaries. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4630–4638, 2015.

[25] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,
Roman Pﬂugfelder, Luka Cehovin Zajc, Tomas Vojir, Gus-
tav Hager, Alan Lukezic, Abdelrahman Eldesokey, et al. The
visual object tracking vot2017 challenge results. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1949–1972, 2017.

[26] Matej Kristan, Aleˇs Leonardis, Jiri Matas, Michael Felsberg,
Roman Pﬂugfelder, Luka ˇCehovin Zajc, Tomas Vojir, Gustav
H¨ager, Alan Lukeˇziˇc, and Gustavo Fernandez. The visual ob-
ject tracking vot2016 challenge results. Springer, Oct 2016.

4028

[41] Jack Valmadre, Luca Bertinetto, Jo˜ao Henriques, Andrea
Vedaldi, and Philip HS Torr. End-to-end representation
In Computer
learning for correlation ﬁlter based tracking.
Vision and Pattern Recognition (CVPR), 2017 IEEE Confer-
ence on, pages 5000–5008. IEEE, 2017.

[42] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming
Hu, and Stephen Maybank. Learning attentions:
residu-
al attentional siamese network for high performance online
In Proceedings of the IEEE Conference
visual tracking.
on Computer Vision and Pattern Recognition, pages 4854–
4863, 2018.

[43] Xiao Wang, Chenglong Li, Bin Luo, and Jin Tang. Sint++:
robust visual tracking via adversarial positive instance gener-
ation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4864–4873, 2018.

[44] Max Welling. Kernel ridge regression.
[45] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object
In Proceedings of the IEEE con-
tracking: A benchmark.
ference on computer vision and pattern recognition, pages
2411–2418, 2013.

[46] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-
ing benchmark. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(9):1834–1848, 2015.

[47] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 351–
366, 2018.

[48] Linyu Zheng, Ming Tang, and Jinqiao Wang. Learning ro-
bust gaussian process regression for visual tracking. In IJ-
CAI, pages 1219–1225, 2018.

[49] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 101–117, 2018.

[50] Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to-
end ﬂow correlation tracking with spatial-temporal attention.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 548–557, 2018.

[27] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
In Proceedings of the IEEE Conference
posal network.
on Computer Vision and Pattern Recognition, pages 8971–
8980, 2018.

[28] Yang Li and Jianke Zhu. A scale adaptive kernel correlation
ﬁlter tracker with feature integration. In ECCV Workshops
(2), pages 254–265, 2014.

[29] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian
Reid, and Ming-Hsuan Yang. Deep regression tracking with
shrinkage loss. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 353–369, 2018.

[30] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual track-
ing. In Proceedings of the IEEE international conference on
computer vision, pages 3074–3082, 2015.

[31] Chao Ma, Xiaokang Yang, Chongyang Zhang, and Ming-
Hsuan Yang. Long-term correlation tracking. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5388–5396, 2015.

[32] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-
subaihi, and Bernard Ghanem. Trackingnet: A large-scale
dataset and benchmark for object tracking in the wild.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 300–317, 2018.

[33] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory
Chanan. Pytorch: Tensors and dynamic neural networks in
python with strong gpu acceleration, 2017.

[34] Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, and Ming-
Hsuan Yang. Deep attentive tracking via reciprocative learn-
ing. In Advances in Neural Information Processing Systems,
pages 1935–1945, 2018.

[35] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Ryn-
son WH Lau, and Ming-Hsuan Yang. Crest: Convolutional
residual learning for visual tracking. In Proceedings of the
IEEE International Conference on Computer Vision, pages
2555–2564, 2017.

[36] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson WH Lau, and
Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In Proceedings of the IEEE Conference on Comput-
er Vision and Pattern Recognition, pages 8990–8999, 2018.
[37] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan
Yang. Learning spatial-aware regressions for visual track-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 8962–8970, 2018.
[38] Ming Tang and Jiayi Feng. Multi-kernel correlation ﬁlter for
In Proceedings of the IEEE International

visual tracking.
Conference on Computer Vision, pages 3038–3046, 2015.

[39] Ming Tang, Bin Yu, Fan Zhang, and Jinqiao Wang. High-
speed tracking with multi-kernel correlation ﬁlters. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4874–4883, 2018.

[40] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.
Siamese instance search for tracking. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1420–1429, 2016.

4029

