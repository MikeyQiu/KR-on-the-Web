DISCRIMINATIVE SEGMENTAL CASCADES
FOR FEATURE-RICH PHONE RECOGNITION

Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu

Toyota Technological Institute at Chicago
{haotang,weiranwang,kgimpel,klivescu}@ttic.edu

6
1
0
2
 
g
u
A
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
3
7
0
6
0
.
7
0
5
1
:
v
i
X
r
a

ABSTRACT

Discriminative segmental models, such as segmental con-
ditional random ﬁelds (SCRFs) and segmental structured sup-
port vector machines (SSVMs), have had success in speech
recognition via both lattice rescoring and ﬁrst-pass decoding.
However, such models suffer from slow decoding, hampering
the use of computationally expensive features, such as seg-
ment neural networks or other high-order features. A typical
solution is to use approximate decoding, either by beam prun-
ing in a single pass or by beam pruning to generate a lattice
followed by a second pass. In this work, we study discrimina-
tive segmental models trained with a hinge loss (i.e., segmen-
tal structured SVMs). We show that beam search is not suit-
able for learning rescoring models in this approach, though
it gives good approximate decoding performance when the
model is already well-trained. Instead, we consider an ap-
proach inspired by structured prediction cascades, which use
max-marginal pruning to generate lattices. We obtain a high-
accuracy phonetic recognition system with several expensive
feature types: a segment neural network, a second-order lan-
guage model, and second-order phone boundary features.

Index Terms— segmental conditional random ﬁeld,
structured prediction cascades, phone recognition, segment
neural network, beam search

1. INTRODUCTION

Segmental models have been considered for speech recogni-
tion as an alternative to frame-based models such as hidden
Markov models (HMMs), in order to address the shortcom-
ings of the frame-level Markov assumption and introduce
expressive segment-level features. Segmental models in-
clude segmental conditional random ﬁelds (SCRFs) [1],
or semi-Markov conditional random ﬁelds [2]; segmental
structured support vector machines (SSVMs) [3]; and gen-
erative segmental models [4, 5]. Previous work comparing
segmental model training algorithms has shown some ben-
eﬁts of discriminative segmental models trained with hinge
loss (SSVM-type learning) [6], and we consider this type of
model here.

Discriminative segmental models have allowed the explo-
ration of complex features, both at the word level [7] and at

the phone level [8, 9, 6]. These powerful segmental features
are a double-edged sword—on the one hand, the model be-
comes more expressive; on the other, it is computationally
challenging to decode with and train such models. For this
reason, SCRFs [10] and SSVMs [3] were initially applied to
speech recognition in a multi-pass approach, where the seg-
mental model considers only a subset of the hypothesis space
contained in lattices generated by HMMs. Much effort has
been devoted to removing the dependency on HMMs and in-
stead developing ﬁrst-pass segmental models [11, 9, 12].
However, working with the entire hypothesis space imposes
an even larger burden on inference, especially when the fea-
tures are computationally intensive or of high order.

If we wish to consider the entire search space in decod-
ing, we can only afford features of low order or of speciﬁc
types as in [9]. An alternative approach to the problem is to
use approximate decoding. There are two widely used ap-
proximate decoding algorithms: beam search and multi-pass
decoding. In the intuitive and popular beam search, the idea is
to prune as we search along the graph representing the search
space. It has been used for decoding in almost all HMM sys-
tems, and for generating lattices as well. Though popular, it
offers no guarantees about its approximation. In the category
of multi-pass decoding, lattice and n-best list rescoring [13]
are commonly used alternatives.

We focus on a particular type of multi-pass approach
based on structured prediction cascades [14], which we term
discriminative segmental cascades. A cascade is a general
approach for decoding and training complex structured mod-
els, using a multi-pass sequence of models with increasing
order of features, while pruning the hypothesis space by a
multiplicative factor to counteract the growth in feature com-
putation.
In this approach, the hypothesis space in each pass
is pruned with max-marginals, which offers the guarantee
that all paths with scores higher than the pruning threshold
are kept.

Applying the discriminative segmental cascade approach
to speaker-independent phonetic recognition on the TIMIT
data set, we obtain a ﬁrst-pass phone error rate of 21.4%
with a unigram language model, and a two-stage cascade er-
ror rate of 19.9%, which includes a bigram language model,
a segment neural network classiﬁer, and second-order phone

(3)

(4)

boundary features. This is to our knowledge the best result
to date with a segmental model. In the following sections we
deﬁne the discriminative segmental models we consider, de-
scribe how we represent a cascade of hypothesis spaces with
a ﬁnite-state composition-like operation, present discrimina-
tive segmental cascades for decoding and training with max-
marginal pruning, and discuss our experiments.

2. DISCRIMINATIVE SEGMENTAL MODELS

A linear segmental model for input space X and hypothesis
space Y is deﬁned formally as a pair (θ, φ), where θ ∈ Rd
is the parameter vector and φ : X × Y → Rd is the feature
vector. For an input x ∈ X , each hypothesis y ∈ Y is asso-
ciated with a score θ⊤φ(x, y), and the goal of decoding is to
ﬁnd the hypothesis that maximizes the score,

general, the model can be trained with different losses. The
model is an SCRF if we train it with log loss − log p(y|x)
where p(y|x) ∝ exp(θ⊤φ(x, y)). It is a segmental structured
SVM if we use the structured hinge loss:

y′∈Y (cid:20)

ℓhinge(θ) = max

cost(y, y′) − θ⊤φ(x, y) + θ⊤φ(x, y′)
(cid:21)
(2)
where cost : Y × Y → [0, ∞) measures the badness of a
hypothesis path y′ compared with the ground truth y.

,

The loss can be optimized with ﬁrst-order methods,
such as stochastic gradient descent (SGD). The gradient
(or subgradient, in this case) computation typically involves
a forward-backward-like algorithm. For example, the subgra-
dient of the hinge loss is

argmax
y∈Y

θ⊤φ(x, y).

(1)

where computing the cost augmented path

∇θℓhinge(θ) = −φ(x, y) + φ(x, ˜y),

For speech recognition, we formally deﬁne the hypothesis
space Y in terms of ﬁnite-state transducers (FST). Let Σ be
the label set (e.g., the phone set in phone recognition), and
Σ = Σ ∪ {ǫ}, where ǫ is the empty label. Deﬁne a decoding
graph as a standard FST G = (V, E, I, F, w, i, o), where V
is the set of vertices, E ⊆ V × V is the set of edges, I ⊆ V
is the set of initial vertices, F ⊆ V is the set of ﬁnal vertices,
w : E → R is a function that associates a weight to an edge,
i : E → Σ is a function that associates an input label to an
edge, and o : E → Σ is a function that associates an output la-
bel to an edge. In addition to the standard deﬁnition of FSTs,
we equip G with a function t : V → R that maps a vertex to a
time stamp. For any edge (u, v) ∈ E, let tail((u, v)) = u, and
head((u, v)) = v. For convenience, we will use subscripts to
denote components of a particular FST, e.g., EG is the edge
set of G.

For an input utterance, let x be the sequence of acoustic
feature vectors. We construct a decoding graph G from x,
then deﬁne our hypothesis space Y ⊆ 2E to be the subset of
paths that start at an initial vertex in I and end at a ﬁnal vertex
in F . A path y ∈ Y of length m is a sequence of unique edges
{e1, . . . , em}, satisfying head(ei) = tail(ei+1) for i ∈ [m].
Given a model (θ, φ), for each edge e ∈ E, the weight w(e)
is deﬁned as θ⊤φ(x, e). For convenience, for a path y ∈ Y,
e∈y φ(x, e) and
we overload φ and w and deﬁne φ(x, y) =
w(y) = θ⊤φ(x, y) =
P
e∈y w(e), where we treat a path y as
a set of (unique) edges e.
P

If the decoding graph is the full hypothesis space with all
possible segmentations and all possible labels, for example
the graph on the left in Figure 1, then the model is a ﬁrst-pass
segmental model. Otherwise, it is a lattice rescoring model.
By the above deﬁnitions, inference (decoding) in the model
(1) can be solved with a standard shortest-path algorithm.

The model parameters θ can be learned by minimizing the
sum of loss functions on samples (x, y) in a training set. In

˜y = argmax

cost(y, y′) + θ⊤φ(x, y′),

y′∈Y

requires a forward pass over the graph. Compared to comput-
ing the gradient of other losses, which requires more forward
passes and backward passes, hinge loss has computational ad-
vantages, and has been shown to perform well [6], so we will
use hinge loss for the rest of the paper.

3. HIGH-ORDER FEATURES
AND STRUCTURED COMPOSITION

The order of a feature is deﬁned as the number of labels on
which it depends. A feature is said to be a ﬁrst-order feature
if it depends on a single label, a second-order feature if it
depends on a pair of labels, and so on. Features with no label
dependency are called zeroth-order features.

High-order features in sequence prediction can be ex-
tended from low-order ones by increasing the number of
labels considered. Formally for any label set Σ and any fea-
ture vector φ ∈ Rd, the feature vector lexicalized with a label
s ∈ Σ is deﬁned as φ ⊗ 1s, where 1s is a one-hot vector of
length |Σ| for the label s and ⊗ : Rm×n × Rp×q → Rmp×nq
is the outer product. With a slight abuse of notation, we let
φ ⊗ s = φ ⊗ 1s. The resulting vector is of length |Σ|d. Sim-
ilarly, we can lexicalize a feature vector with pairs of labels,
φ ⊗ s1 ⊗ s2 = φ ⊗ 1s1 ⊗ 1s2, giving a vector of length |Σ|2d.
For example, a common type of zeroth-order segmental
feature is of the form ψ(x, t1, t2) where x is the sequence
of acoustic feature vectors, t1 is the start time of the seg-
ment, and t2 is the end time of the segment. To make it
discriminative in a decoding graph H, we can compute the
ﬁrst-order feature φH (x, e) for any edge e by ﬁrst computing
ψ(x, t(tail(e)), t(head(e))) and then lexicalizing it with the
label oH (e).

H1

L2

H2 ◦σ L2

a
b

a

a

c

b

H2

b b
a b

ǫ b
ǫ a

b a
a a

b c
a c

a a

Fig. 1. From left to right: An example of the full hypothesis space H1 with four frames (ﬁve vertices) and three unique labels
{a, b, c} (three edges between every pair of vertices) with segment length up to three frames (actual labels omitted for clarity);
H2, a pruned H1; a graph structure corresponding to a bigram language model L2 over three labels; and H2 σ-composed with
L2, where s1 s2 denotes the bigram s1s2.

(5)

(6)

(7)

(8)
(9)

(10)
(11)

To have a uniﬁed way of extending the order of features,
we deﬁne the concept of FST structured composition, or σ-
composition for short, as follows. For any two FSTs A and
B, the σ-composed FST is deﬁned as

G = A ◦σ B

where

VG = VA × VB

(cid:26)

and

EG =

he1, e2i ∈ EA × EB : oA(e1) = iB(e2)

(cid:27)

iG(he1, e2i) = iA(e1)
oG(he1, e2i) = oB(e2)

tailG(he1, e2i) = htailA(e1), tailB(e2)i
headG(he1, e2i) = hheadA(e1), headB(e2)i

where h·, ·i denotes a tuple. Unlike in classical composition,
we only constrain the structure of G and are free to deﬁne wG
differently. In particular, we let

wG(he1, e2i) = θ⊤

GφG(x, he1, e2i),

(12)

and φG is free to use φA and φB but is not constrained to do
so. In other words, the weight function wG can extract richer
features than wA and wB.

With structured composition, we can easily convert low-
order features to high-order ones. Continuing the above ex-
ample, we can σ-compose the decoding graph H with a bi-
gram language model (LM) L in its FST form [15] with a
slight modiﬁcation. We require the output labels of the LM
FST to include the history labels alongside the current label.
For example, the output labels of a bigram LM are of the
form s1s2 ∈ Σ × Σ, where s1 is the history label (possi-
bly ǫ) and s2 is the current label. Let G = H ◦σ L. We
can deﬁne tG(he1, e2i) = tH (e1). For an edge e ∈ EG,
we can compute ﬁrst-order features ϕ ⊗ s1, and second-order
features ϕ ⊗ s1 ⊗ s2 for s1s2 = oG(e) and s1 6= ǫ, where
If s1 = ǫ, every-
ϕ = ψ(x, tG(tailG(e)), tG(headG(e))).
thing falls back to the previous example. In general, by σ-
composing with high-order n-gram LMs, we can compute
high-order features by lexicalizing low-order ones.

4. DISCRIMINATIVE SEGMENTAL CASCADES

Our approach, which we term a discriminative segmental cas-
cade (DSC), is an instance of multi-pass decoding, consisting
of levels with increasing complexity of features and decreas-
ing size of search space. We start with the full search space
and a “simple” ﬁrst-level discriminative segmental model
using inexpensive features, and use the ﬁrst-level model to
prune the search space. We then apply a model using more
expensive features, and optionally repeat the process for as
many levels as desired. Rather than the typical beam pruning,
we prune with max-marginals [16, 14], which have certain
useful properties and turn out to be important for achieving
good performance with our models. A max-marginal of an
edge e in G is deﬁned as

γ(e) = max
y∋e

θ⊤φ(x, y).

(13)

In words, it is the highest score of a path that passes through
the edge e. We prune the edge if its max-marginal is lower
than a threshold, and keep it otherwise.
In order to prune
a multiplicative factor of edges at each level of the cascade,
Weiss et al. [14] propose to use the threshold

τλ = (1 − λ)

1
|EG| Xe∈EG

γ(e) + λ max
y∈Y

θ⊤φ(x, y),

(14)

which interpolates between the mean of the max-marginals
and the maximum. If λ is set to 1, we only keep the best path.
Lattice generation by max-marginal pruning guarantees
that there is always at least one path left after pruning and
that any y satisfying w(y) > τλ is kept, because for every
e ∈ y, γ(e) ≥ w(y) > τλ. In particular, if the ground truth
has a score higher than the threshold, it will still be in the
search space for the next level of the cascade.

Computing max-marginals in a speciﬁc level of the cas-
cade requires a forward pass and a backward pass through the
graph. Pruning with max-marginals thus takes twice the time
as searching for the best path alone.

Learning the cascade of models is also done level by level.
We start with the entire hypothesis space H1 limited only by
a maximum segment length. A ﬁrst set of computationally
inexpensive features up to ﬁrst order is used for learning. Let

Table 1. A summary of results in terms of phonetic error
rate (%) on the TIMIT test set, for prior ﬁrst-pass segmental
models, a speaker-independent HMM-DNN system given by
a standard Kaldi recipe [18], and our models.

dev
PER (%)

HMM-DNN
ﬁrst ﬁrst-pass SCRF [8]
Boundary-factored SCRF [9]
Deep segmental NN [11]
our ﬁrst-pass model (H1)
DSC 2nd level with bigram LM
+ 2nd-order boundary features
+ 1st-order segment NN
+ 1st-order bi-phone NN bottleneck

22.15
19.80
19.22
18.86
18.77

test
PER (%)
21.4
33.1
26.5
21.87
21.73

19.93

the ﬁrst set of weights learned be θ1. We can use θ1 for ﬁrst-
pass decoding if it is good enough, or we can choose to gen-
erate the next level of the cascade and use more computation-
ally expensive features, such as higher-order ones. Moving
to the next level of the cascade, we compute max-marginals
with θ1 and prune H1 with a threshold, resulting in a lattice
H2. If we wish increase the order of features, we σ-compose
H2 with a bigram LM L2. A second set of features up to sec-
ond order can then be used for learning. Suppose the second
set of weights is θ2. Again, we have the choice either to use
θ2 for decoding or to prune and repeat the process with more
computationally expensive features.

5. EXPERIMENTS

We experiment with segmental models in the context of pho-
netic recognition on the TIMIT corpus [17]. We follow the
standard TIMIT protocol for training and testing. We use 192
randomly selected utterances from the complete test set other
than the core test set as our development set, and will refer to
the core test set simply as the test set. The phone set is col-
lapsed from 61 labels to 48 before training. In addition to the
48 phones, we also keep the glottal stop /q/, sentence start,
and sentence end so that every frame in the training set has
a label. A summary of prior ﬁrst-pass decoding results with
segmental models, along with our results and one from a stan-
dard speaker-independent HMM-DNN, is shown in Table 1.

5.1. First-pass segmental model

First we demonstrate the effectiveness of our ﬁrst-pass de-
coder. The ﬁrst-pass search graph, denoted H1, contains all
possible labels and all possible segmentations up to 30 frames
per segment. Like some prior segmental phonetic recognition
models [11, 9], many of the features in our ﬁrst-pass decoder
are based on averaging and sampling the outputs of a neural
network phonetic frame classiﬁer, speciﬁcally a convolutional

neural network (CNN) [19], which we describe next.

5.1.1. CNN frame classiﬁer

The input to the network is a window of 15 frames of log-
mel ﬁlter outputs. The network has ﬁve convolutional layers,
with 64–256 ﬁlters of size 5 × 5 for the input and 3 × 3 for
others, each of which is followed by a rectiﬁed linear unit
(ReLU) [20] activation, with max pooling layers after the ﬁrst
and the third ReLU layers. The output of the ﬁnal ReLU layer
is concatenated with a window of 15 frames of MFCCs cen-
tered on the current frame, and the resulting vector is passed
through three fully connected ReLU layers with 4096 units
each. The network is trained with SGD for 35 epochs with a
batch size of 100 frames. Fully connected layers and the con-
catenation layer are trained with dropout at a 20% and 50%
rate, respectively. This classiﬁer was tuned on the develop-
ment set and achieves a 22.1% frame error rate (after collaps-
ing to 39 phone labels) on the test set. We will use CNN(x, t)
to denote the log of the ﬁnal softmax layer, corresponding to
the predicted log probabilities of the phones, given as input
[xt−7; . . . ; xt+7].

5.1.2. First-order features

Below we list the features for each edge (u, v). We will use
L = t(v) − t(u) for short.

Average of CNN log probabilities The log of the CNN out-
put layer is averaged over all frames in the segment:

1
L

L−1

Xi=0

CNN(x, t(u) + i)

(15)

Samples of CNN log probabilities The log of the CNN out-
put layer is sampled from the middle frames of three equally
split sub-segments, i.e.,

CNN

x, t(u) +

(cid:18)

[k + (k + 1)]L
3 · 2

(cid:22)

(cid:23)(cid:19)

(16)

for k = 0, 1, 2.

Boundary features The log probabilities i frames before the
left boundary CNN(x, t(u) − i) and i frames after the right
boundary CNN(x, t(v) + i) are used as features. We use the
concatenation of the boundary features for i = 1, 2, 3.

Length indicator 1L=ℓ for ℓ = 0, 1, . . . , 30.

Bias A constant 1.

We lexicalize all of the above features to ﬁrst order,
and include a zeroth-order bias feature. We minimize hinge
loss with the overlap cost function introduced in [6] with
AdaGrad for up to 70 epochs with step sizes tuned in

)

%

(
 
e
t
a
r
 
t
i
h

100
80
60
40
20
0

)

%

(
 
R
E
P
 
v
e
d

25
24
23
22
21
20

)

%

(
 
R
E
P
 
v
e
d

32
30
28
26
24
22

)

%

(
 
R
E
P
 
v
e
d

80
70
60
50
40
30
20
10

0 10 20 30 40 50 60
beam width

0 10 20 30 40 50 60
beam width

0

4

12 16

0

4

12 16

8
epoch

8
epoch

Fig. 2. Beam search on H1 with different beam widths. Left:
Hit rate on the development set. Right: PER on the develop-
ment set. The dashed line is the PER of the exact search.

{0.01, 0.1, 1}. No explicit regularizer is used; instead we
choose the step size and iteration that perform best on the
development set (so-called early stopping). As shown in Ta-
ble 1, our ﬁrst-pass segmental model outperforms all previous
segmental model TIMIT results of which we are aware.

5.2. Higher-order features and segmental cascades

We next explore multi-pass decoding with beam search and
with discriminative segmental cascades. In the second pass
we include features of order two and a bigram LM L2. Back-
off is approximated with ǫ transitions in the bigram LM. Let
G = H ◦σ L2, where H can be H1 or H2, the pruned H1. We
consider the following additional features on edges e ∈ EG.

Bigram LM score The bigram log probability log pLM(s2|s1),
where s1s2 = oG(e) We do not lexicalize this feature because
it is naturally second-order.

5.2.1. Beam search

Before experimenting with the second-order features, we
compare beam search and exact search on the best model
for H1 to give a sense of the approximation quality of beam
search. We measure the quality of approximation via the “hit
rate”, i.e., how often the exact best path is found. Results
are shown in Figure 2. As expected, the hit rate decreases
as the beam width decreases. However, the PER does not
decrease signiﬁcantly, which demonstrates that beam search
is a good approximate decoding algorithm when the model is
well-trained.

Judging from the decoding results, we use beam search
with beam widths {10, 20, 30} for learning. Since the runtime
of beam search is controlled by the beam width when the de-
coding graph is large, we can search directly on H1◦σ L2. The
composition is done on the ﬂy to avoid enumerating all edges
in H1 ◦σ L2. We compare learning on both H1 and H1 ◦σ L2.
For H1 we use the same features as the ﬁrst-pass segmental
model, while for H1 ◦σ L2 we add the bigram LM score and
second-order boundary features. For consistency, we use the
same beam width for decoding. Hinge loss is minimized with
AdaGrad with step sizes tuned in {0.01, 0.1, 1}. Results are

beam=10

Fig. 3. Beam search for learning with different beam widths:
exact. Top:
Learning on H1. Bottom: Learning on H1 ◦σ L2. The
dashed line is the learning curve of the second-level cascade
H2 ◦σ L2.

beam=30

beam=20

shown in Figure 3 for the step size that achieves the lowest
development set PER. When we train the segmental model on
H1 (top of Figure 3), learning with beam search is success-
ful when the beam width is large enough, while for H1 ◦σ L2
(bottom of Figure 3), learning completely fails.

5.2.2. Discriminative segmental cascades (DSC)

We next consider the proposed discriminative structured cas-
cades (DSC) for utilizing the bigram LM and second-order
features. We ﬁrst prune H1 with max-marginal pruning using
our ﬁrst-pass segmental model with weights θ1, resulting in
H2, and σ-compose H2 with L2. Recall that the larger the
pruning parameter λ, the sparser the lattice. We measure the
density of the lattice by the number of edges in H2 divided by
the number of ground-truth (gold) edges. The quality of H2’s
produced with different λ’s is shown in Figure 4 (left). For
the DSC second level, we deﬁne an additional feature:

Lattice score Instead of re-learning all of the weights for
the features in the ﬁrst-pass model, we combine them into
an additional feature from the ﬁrst level of the cascade
θ⊤
1 φH1 (x, e1), which is never lexicalized, where e1 ∈ H
is such that he1, e2i ∈ EG.

To compare with beam search, we use the lattice score, the
bigram LM score, second-order boundary features, ﬁrst-order
length indicators, and ﬁrst-order bias as our features for the
second level of the cascade. Hinge loss is minimized with
AdaGrad for up to 20 epochs with step sizes optimized in
{0.01, 0.1}. Again, no explicit regularizer is used except early
stopping on the development set. Learning results on different
lattices are shown in Figure 4 (right). We see that learning
with the DSC is clearly better than with beam search.

5.2.3. Other expensive features

To add more context information, we use the same CNN ar-
chitecture and training setup to learn a bi-phone frame classi-
ﬁer, but with an added 256-unit bottleneck linear layer before

)

%

(
 
R
E
P
 
e
l
c
a
r
o

4

3

2

1

0

)

%

(
 
R
E
P
 
v
e
d

19.7
19.6
19.5
19.4
19.3
19.2

0 200 400 600 800
lattice density
(edges per gold edge)

0 200 400 600 800
lattice density
(edges per gold edge)

Fig. 4. Quality of H2 for λ’s in {0.8, 0.7, 0.6, 0.5}. Left:
Oracle error rates for different lattice densities. Right: Cor-
responding second-pass development set PERs?

Table 2. TIMIT segment classiﬁcation error rates (ER).
test ER (%)
26.3
22.4
21.0
16.8
15.9
15.0

Gaussian mixture model (GMM) [23]
SVM [23]
Hierarchical GMM [22]
Discriminative hierarchical GMM [24]
SVM with deep scattering spectrum [25]
our CNN ensemble

the softmax [21]. Each frame is labeled with its segment label
and one additional label from a neighboring segment. If the
current frame is in the ﬁrst half of the segment, the additional
label is the previous phone; if it is in the second half, then
the additional label is the next phone. The learned bottleneck
layer outputs are used to deﬁne features (although they do not
correspond to log probabilities) with averaging and sampling
as for the uni-phone case. We refer to the resulting features as
bi-phone NN bottleneck features.

Finally, we also use the same type of CNN to train a
segment classiﬁer. Here the features at the input layer are
the log-mel ﬁlter outputs from a 15-frame window around the
segment’s central frame. The network architecture is the same
as our frame classiﬁer, but instead of concatenation with 15-
frame MFCCs, we concatenate with a segmental feature vec-
tor consisting of the average MFCCs of three sub-segments
in the ratio of 3-4-3, plus two four-frame averages at both
boundaries and length indicators for length 0 to 20 (similar
to the segmental feature vectors of [22, 23]). This CNN is
trained on the ground-truth segments in the training set. Fi-
nally, we build an ensemble of such networks with different
random seeds and a majority vote. This ensemble classiﬁer
has a 15.0% classiﬁcation error on the test set, which is to our
knowledge the best result to date on the task of TIMIT phone
segment classiﬁcation (see Table 2).

It is, however, still too time-consuming to compute the
segment network outputs for every edge in the lattice. We in-
stead compress the best-performing (single) CNN into a shal-
low network with one hidden layer of 512 ReLUs by training
it to predict the log probability outputs of the deep network,
as proposed by [26, 27]. We then use the log probability out-

puts of the shallow network and lexicalize them to ﬁrst order.
We refer to the result as segment NN features.

Results with these additional features are shown in Ta-
ble 1. Adding the second-order features, bigram LM, and the
above NN features gives a 1.8% absolute improvement over
our best ﬁrst-pass system, demonstrating the value of includ-
ing such powerful but expensive features.

6. DISCUSSION

We have presented discriminative segmental cascades (DSC),
an approach for training and decoding with segmental mod-
els that allows us to incorporate high-order and complex fea-
tures in a coarse-to-ﬁne approach, and have applied them to
the task of phone recognition. The DSC approach uses max-
marginal pruning, which outperforms beam search for learn-
ing the second-pass model. Starting from a ﬁrst-pass large-
margin model that outperforms previous segmental model re-
sults and is competitive with HMM-DNNs, the DSC second
pass improves the phone error rate by another 1.8% absolute.
Further analysis may be needed to understand precisely
why learning with beam search is not successful in the context
of our models. One issue is that σ-composing H1 and L2
introduces many dead ends (paths that do not lead to ﬁnal
vertices) in the graph because we have to do the composition
on the ﬂy. Minimizing H1 ◦σ L2 might help, but we would
need to touch the edges of H1 ◦σ L2 at least once, which is
itself expensive. Second, even if we reach the ﬁnal vertices,
the cost-augmented path might still have a lower cost+score
than the ground-truth path, which leads to no gradient update.
This issue has been studied recently, and one possible solution
is “premature updates” [28], but these are intended for the
perceptron loss. Third, the edge weights in our models are
not strictly negative. Beam search would tend to go depth-ﬁrst
when encountering edges with positive weights. On the other
hand, if the edge weights are negative, beam search would
tend to go breadth-ﬁrst, which may explain why greedy search
like beam search may cause problems for segmental models
but works for HMMs.

Additional future work includes considering even more
expressive features, higher-order features and additional cas-
cade levels. There is also much room for exploration with
segment neural network classiﬁers. One concern with our
segment classiﬁers is that they are trained only with ground
truth segments, so it is unclear how they behave when the
input is an incorrect hypothesized segment. Alternatives in-
clude training on all hypothesized segments and allowing the
network to learn to classify non-phones, similarly to the anti-
phone and near-miss modeling of [5].

7. ACKNOWLEDGEMENT

This research was supported by NSF grant IIS-1433485. The
opinions expressed in this work are those of the authors and
do not necessarily reﬂect the views of the funding agency.

8. REFERENCES

[1] Geoffrey Zweig and Patrick Nguyen, “A segmental CRF
approach to large vocabulary continuous speech recog-
nition,” in IEEE Workshop on Automatic Speech Recog-
nition & Understanding, 2009, pp. 152–157.

[2] Sunita Sarawagi and William W Cohen, “Semi-Markov
conditional random ﬁelds for information extraction,”
in Advances in Neural Information Processing Systems,
2004, pp. 1185–1192.

[3] Shi-Xiong Zhang and Mark Gales, “Structured SVMs
for automatic speech recognition,” IEEE Transactions
on Audio, Speech, and Language Processing, vol. 21,
no. 3, pp. 544–555, 2013.

[4] Mari Ostendorf, Vassilios V Digalakis, and Owen Kim-
ball, “From HMM’s to segment models: A uniﬁed view
of stochastic modeling for speech recognition,” IEEE
Transactions on Speech and Audio Processing, vol. 4,
no. 5, pp. 360–378, 1996.

[5] James Glass, “A probabilistic framework for segment-
based speech recognition,” Computer Speech & Lan-
guage, vol. 17, no. 2, pp. 137–152, 2003.

[6] Hao Tang, Kevin Gimpel, and Karen Livescu, “A com-
parison of training approaches for discriminative seg-
mental models,” in Proceedings of the Annual Confer-
ence of International Speech Communication Associa-
tion, 2014.

[7] Geoffrey Zweig, Patrick Nguyen, Dirk Van Com-
pernolle, Kris Demuynck, Les Atlas, Pascal Clark,
Greg Sell, Meihong Wang, Fei Sha, Hynek Herman-
sky, Damianos Karakos, Aren Jansen, Samuel Thomas,
G.S.V.S. Sivaram, Samuel Bowman, and Justine Kao,
“Speech recognition with segmental conditional ran-
dom ﬁelds: A summary of the JHU CLSP 2010 sum-
mer workshop,” in IEEE International Conference on
Acoustics, Speech and Signal Processing, 2011, pp.
5044–5047.

[8] Geoffrey Zweig, “Classiﬁcation and recognition with
direct segment models,” in IEEE International Confer-
ence on Acoustics, Speech and Signal Processing, 2012,
pp. 4161–4164.

[9] Yanzhang He and Eric Fosler-Lussier, “Efﬁcient seg-
mental conditional random ﬁelds for phone recogni-
tion,” in Proceedings of the Annual Conference of the In-
ternational Speech Communication Association, 2012,
pp. 1898–1901.

[10] Geoffrey Zweig and Patrick Nguyen,

“SCARF: A
segmental conditional random ﬁeld toolkit for speech

in Proceedings of the Annual Confer-
recognition,”
ence of International Speech Communication Associa-
tion, 2010, pp. 2858–2861.

[11] Ossama Abdel-Hamid, Li Deng, Dong Yu, and Hui
“Deep segmental neural networks for speech
Jiang,
recognition.,”
in Proceedings of the Annual Confer-
ence of International Speech Communication Associa-
tion, 2013, pp. 1849–1853.

[12] Yanzhang He and Eric Fosler-Lussier, “Segmental con-
ditional random ﬁelds with deep neural networks as
acoustic models for ﬁrst-pass word recognition,” in Pro-
ceedings of the Annual Conference of the International
Speech Communication Association, 2015.

[13] Mari Ostendorf, Ashvin Kannan, Steve Austin, Owen
Kimball, Richard Schwartz, and Jan Rohlicek, “Inte-
gration of diverse recognition methodologies through
reevaluation of n-best sentence hypotheses,” in Proceed-
ings of the Workshop on Speech and Natural Language,
1991, pp. 83–87.

[14] David Weiss, Benjamin Sapp, and Ben Taskar, “Struc-
tured prediction cascades,” arXiv:1208.3279 [stat.ML],
2012.

[15] Cyril Allauzen, Mehryar Mohri, and Brian Roark,
“Generalized algorithms for constructing statistical lan-
guage models,” in Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics, 2003,
pp. 40–47.

[16] A. Sixtus and S. Ortmanns, “High quality word graphs
using forward-backward pruning,”
in IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing, 1999, pp. 593–596.

[17] John S Garofolo, Lori F Lamel, William M Fisher,
Jonathon G Fiscus, and David S Pallett, “Darpa timit
acoustic-phonetic continous speech corpus cd-rom. nist
speech disc 1-1.1,” NASA STI/Recon Technical Report
N, vol. 93, pp. 27403, 1993.

[18] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luk´aˇs
Burget, Ondˇrej Glembek, Nagendra Goel, Mirko Han-
nemann, Petr Motl´ıˇcek, Yanmin Qian, Petr Schwarz,
et al., “The Kaldi speech recognition toolkit,” 2011.

[19] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv:1409.1556 [cs.CV], 2014.

[20] Matthew Zeiler, Marc’Aurelio Ranzato, Rajat Monga,
Min Mao, Kun Yang, Quoc Le, Patrick Nguyen, Alan
Senior, Vincent Vanhoucke, Jeffrey Dean, and Geoff
Hinton, “On rectiﬁed linear units for speech process-
ing,” in IEEE International Conference on Acoustics,
Speech and Signal Processing, 2013, pp. 3517–3521.

[21] Tara Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru
Arisoy, and Bhuvana Ramabhadran,
“Low-rank ma-
trix factorization for deep neural network training with
high-dimensional output targets,” in IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, 2013, pp. 6655–6659.

[22] Andrew Halberstadt, Heterogeneous Acoustic Mea-
surements and Multiple Classiﬁers for Speech Recog-
nition, Ph.D. thesis, Massachusetts Institute of Technol-
ogy, 1998.

[23] Philip Clarkson and Pedro Moreno, “On the use of sup-
in
port vector machines for phonetic classiﬁcation,”
IEEE International Conference on Acoustics, Speech,
and Signal Processing, 1999, vol. 2, pp. 585–588.

[24] Hung-An Chang and James Glass, “Hierarchical large-
margin Gaussian mixture models for phonetic classiﬁca-
tion,” in IEEE Workshop on Automatic Speech Recogni-
tion & Understanding, 2007, pp. 272–277.

[25] Joakim And´en and St´ephane Mallat, “Deep scattering
spectrum,” IEEE Transactions on Signal Processing,
vol. 62, no. 16, pp. 4114–4128, 2014.

[26] Jinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan Gong,
“Learning small-size DNN with output-distribution-
based criteria,” in Proceedings of the Annual Confer-
ence of the International Speech Communication Asso-
ciation, 2014.

[27] Jimmy Ba and Rich Caruana, “Do deep nets really need
to be deep?,” in Advances in Neural Information Pro-
cessing Systems, 2014, pp. 2654–2662.

[28] Liang Huang, Suphan Fayong, and Yang Guo, “Struc-
tured perceptron with inexact search,” in Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, 2012, pp. 142–151.

