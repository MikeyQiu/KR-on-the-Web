Fast Deep Matting for Portrait Animation on Mobile Phone

Bingke Zhu1,2, Yingying Chen1,2, Jinqiao Wang1,2, Si Liu2,3, Bo Zhang4, Ming Tang1,2
1National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China
2University of Chinese Academy of Sciences, Beijing, China
3Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
4North China University of Technology, Beijing, China
{bingke.zhu, yingying.chen, jqwang, tangm}@nlpr.ia.ac.cn
liusi@iie.ac.cn, zhangbo@ncut.edu.cn

7
1
0
2
 
l
u
J
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
9
8
2
8
0
.
7
0
7
1
:
v
i
X
r
a

ABSTRACT
Image matting plays an important role in image and video editing.
However, the formulation of image matting is inherently ill-posed.
Traditional methods usually employ interaction to deal with the
image matting problem with trimaps and strokes, and cannot run on
the mobile phone in real-time. In this paper, we propose a real-time
automatic deep matting approach for mobile devices. By leveraging
the densely connected blocks and the dilated convolution, a light full
convolutional network is designed to predict a coarse binary mask
for portrait image. And a feathering block, which is edge-preserving
and matting adaptive, is further developed to learn the guided
filter and transform the binary mask into alpha matte. Finally, an
automatic portrait animation system based on fast deep matting is
built on mobile devices, which does not need any interaction and
can realize real-time matting with 15 fps. The experiments show
that the proposed approach achieves comparable results with the
state-of-the-art matting solvers.

KEYWORDS
Portrait Matting; Real-time; Automatic; Mobile Phone

1 INTRODUCTION
Image matting plays an important role in computer vision, which
has a number of applications, such as virtual reality, augmented
reality, interactive image editing, and image stylization [27]. As
people are increasingly taking selfies and uploading the edited to
the social network with mobile phones, real-time matting technique
is in demand which can handle real world scenes. However, the
formulation of image matting is still an ill-posed problem. Given an
input image I , image matting problem is equivalent to decomposing
it into foreground F and background B in assumption that I is
blended linearly by F and B:

I = αF + (1 − α)B,
where α is used to evaluate the foreground opacity(alpha matte). In
natural image matting, all quantities on the right-hand side of the

(1)

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’17, October 23–27, 2017, Mountain View, CA, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4906-2/17/10. . . $15.00
https://doi.org/10.1145/3123266.3123286

Figure 1: Examples of portrait animation. (a) The original
images which are the inputs to our system. (b) The fore-
grounds of the original images, which are computed by the
Eq. (1). (c) The portrait animation on mobile phone.

composting Eq. (1) are unknown, which makes the original matting
problem ill-posed.

To alleviate the difficulty of this problem, popular image matting
techniques such as [8, 10, 17, 28] require users to specify foreground
and background color samples with trimaps or strokes, which makes
a rough distinction among foreground, background and regions
with unknown opacity. Benefit from the trimaps and strokes, the
interactive mattings have high accuracy and have the capacity to
segment the elaborate hair. However, despite the high accuracy of
these matting techniques, these methods rely on user involvement
(label the trimaps or strokes) and require extensive computation,
which restricts the application scenario. In order to deal with the
interactive problem, Shen et al. [28] proposed an automatic mat-
ting with the help of semantic segmentation technique. But their
automatic matting have a very high computational complexity and
it is still slow even on the GPU.

In this paper, to solve user involvement and apply matting tech-
nique on mobile phones, we propose a real-time automatic matting
method on mobile phone with a segmentation block and a feather-
ing block. Firstly, we calculate a coarse binary mask by a light full
convolutional network with dense connections. And then we de-
sign a learnable guided filter to obtain the final alpha matte through
a feathering block. Finally, the predicted alpha matte is calculated
through a linear transformation of the coarse binary mask, whose
weights are obtained from the learnt guided filter. The experiments
show that our real-time automatic matting method achieves com-
parable results to the general matting solvers. On the other hand,

Figure 2: Pipeline of our end-to-end real-time image matting framework. It includes a segmentation block and a feathering
block.

we demonstrate that our system possesses adaptive matting ca-
pacity dividing the parts of head and the neck precisely, which is
unprecedented in other alpha matting methods. Thus, our system is
useful in many applications like real-time video matting on mobile
phone. And real-time portrait animation on mobile phone can be
realized using our fast deep matting technique as shown in some
examples in Figure 1. Compared with existing methods, the major
contributions of our work are three-fold:

1. A fast deep matting network based on segmentation block
and feathering block is proposed for mobile devices. By adding the
dialed convolution into the dense block structure, we propose a
light dense network for portrait segmentation.

2. A feathering block is proposed to learn the guided filter and
transform the binary mask into alpha matte, which is edge-preserving
and possesses matting adaptive capacity.

3. The proposed approach can realize real-time matting on the
mobile phones and achieve comparable results with the state-of-
the-art matting solvers.

2 RELATED WORK
Image Matting: For interactive matting, in order to infer the alpha
matte in the unknown regions, a Bayesian matting [7] is proposed
to model background and foreground color samples as Gaussian
mixtures. Levin et al. [17] developed closed matting method to find
the globally optimal alpha matte. He et al. [10] computed the large-
kernel Laplacian to accelerate matting Laplacian computation. Sun
et al. [29] proposed Poisson image mating to solve the homoge-
nous Laplacian matrix. Different from the methods based on the
Laplacian matrix, shared matting [8] presented the first real-time
matting technique on modern GPUs by shared sampling. Benefit
from the performance of deep learning, Xu et al. [34] developed an
encoder-decoder network to predict alpha matte with the help of
trimaps. Recently, Aksoy et al. [1] designed an effective inter-pixel
information flow to predict the alpha matte and reach the state
of the art. Different from the interactive matting, Shen et al. [28]
first proposed an end-to-end convolutional network to train the
automatic matting.

With the rapid development of mobile devices, automatic mat-
ting is a very useful technique for image editing. However, despite
the high efficiency of these techniques, both interactive matting
and automatic matting cannot reach real-time matting on CPUs
and mobile phone. The Laplacian-based methods need to compute

the Laplacian matrix and its inverse, which is an N × N symmetric
matrix, where N is the number of unknowns. Neither non-iterative
methods [17], nor iterative methods [10] can solve the sparse linear
system efficiently due to the high computing cost. The real-time
image matting methods like shared matting can be real-time with
the help of powerful GPUs, but these methods are still slow on
CPUs and mobile phone.
Semantic Segmentation: Long et al. [19] first proposed to solve
the problem of semantic segmentation with fully convolutional
network. ENet [20] is a light full convolutional network for se-
mantic segmentation, which can reach real-time on GPU. Huang
et al. [13] and Bengio et al. [14] proposed a dense network with
several densely connected blocks, which make the receptive filed
of prediction more dense. Specially, Chen et al. [2] developed an
operation called dilated convolution, which makes the receptive
filed more dense and bigger. Similarly, Yu et al. [35] proposed to use
dilated convolution to aggregate the context information. On the
other hand, semantic and multi-scale information is important in
image understanding and semantic segmentation [9, 31, 32, 37]. In
order to absorb more semantic and multi-scale information, Zhao
et al. [36] proposed a pyramid structure with dilated convolution,
Chen et al. [3] proposed an atrous spatial pyramid pooling (ASPP),
and Peng et al. [21] used the large kernel convolutions to extract
features. Based on these observations, we try to combine the works
of Huang [13] and Chen [2] to create a light dense network,which
can predict the segmentation densely in several layers.
Guided Filter: Image filtering plays an important role in computer
vision, especially after the development of convolutional neural
network. He et al. [11] proposed an edge-preserving filter - guided
filter to deal with the problem of gradient drifts, which is a com-
mon problem in image filtering. Despite the powerful performance
of convolutional neural network, it loses the information of edge
when it comes to pixel-wise predicted assignments such as semantic
segmentation and alpha matting due to its piecewise smoothing.

3 OVERVIEW
The pipeline of our system is presented in Figure 2. The input is a
color image I and the output is the alpha matte A . The network
consists of two stages. The first stage is a portrait segmentation
network which takes an image as input and obtains a coarse binary
mask. The second stage is a feathering module that refines the
foreground/background mask to the final alpha matte. The first

The motivation of this architecture is inspired by several popular
full convolutional networks for semantic segmentation. Inspired
by the real-time segmentation architecture of ENet [20] and Incep-
tion network [30], we use the initial block in ENet to down sample
the input, which can maintain more information than that of max-
pooling and bring less computation cost than that of convolution.
Then the down-sampling maps are sent to the dilated dense block,
which is inspired by the densely connected convolution network
[13] and the dilated convolution network [2]. Each layer of the
dilated dense block obtains different field of view, which is impor-
tant in the multi-scale semantic segmentation. The dilated dense
block can seize the foreground in variable sizes, thus obtain a better
segmentation mask by the final classifier.

3.2 Feathering Block
The segmentation block can produce a coarse binary mask to the
alpha matte. However, the binary mask cannot represent the alpha
matte due to the coarse edge. Despite fully convolution networks
have been proved to be effective in the semantic segmentation
[19], the methods like [34] using fully convolutional networks may
suffer from the gradient drifts because of the pixel-wise smoothing
in the convolutional operations. In this section, we will discuss a
feathering block to refine the binary mask and solve the problem
of gradient drifts.

3.2.1 Architecture. The convolutional operations play impor-
tant roles in the classification networks. However, alpha matting is
a pixel-wise predicted task, and it is unsuitable to predict the edge
of the objects with the convolutional operations due to its piecewise
smoothing. The state of the art method [2] relieves this problem
using the conditional random field (CRF) model to refine the edge
of the objects, but it costs too much computing memory and it
cannot solve the problem in the convolutional neural networks
intrinsically. Motivated by the gradient drifts of the convolution
operations, we developed a sub-network to learn the filters of the
coarse binary mask, which does not suffer from the gradient drifts.
The architecture of the feathering block is presented in Figure 4.
The inputs to the feathering block are an image, the corresponding
coarse binary mask, the square of the image as well as the product
of the image and its binary mask. The design of these inputs is
inspired by the guided filter [11], whose weights are designed as
a function of these inputs. We concatenate the inputs and send
the concatenation into the convolutional network which contains
two 3 × 3 convolutional layers. Then we can obtain three maps
corresponding to the weights and bias of the binary mask. The
feathering layer can be represented as a linear transform of the
coarse binary mask in sliding windows centered at each pixel:

αi = ak SF i + bk SBi + ck , ∀i ∈ ωk ,
where α is the output of feathering layer represented as alpha matte,
SF is the foreground score from the coarse binary mask, SB is the
background score, i is the location of the pixel, and (ak , bk , ck ) are
the linear coefficients assumed to be constant in the k-th sliding
window ωk

. Thus, we have:

(2)

qi = ak Fi + bk Bi + ck Ii , ∀i ∈ ωk ,
where qi = αi Ii , Fi = Ii SF i , Bi = Ii SBi , and I is the input image.

(3)

Figure 3: Diagram of our light dense network for portrait
segmentation.

stage provides a coarse binary mask in a fast speed with a light full
convolutional network, while the second stage refines the coarse
binary mask with a single filter, which reduces the error greatly.
We will describe our algorithm with more details in the following
sections.

3.1 Segmentation Block
In order to segment the foreground with a fast speed, we propose a
light dense network in the segmentation block. The architecture
of the light dense network is presented in Figure 3. Output sizes
are reported for an example input image resolution of 128 × 128.
The initial block is a concatenation of a 3 × 3 convolution and a
max-pooling, which is used to down sample the input image. The
dilated dense block contains four convolutional layers in different
dilated rates and four densely connections. The concatenation of
four convolutional layers are sent to the final convolution to obtain
a binary feature maps. Finally, we interpolate the feature maps to
get the score maps which have the same size as the original image.
Specifically, our light dense network has 6 convolutional layers and
1 max-pooling layer.

possible values of αi :

αi =

1
|w |

(cid:213)

k :i ∈wk
= ai SF i + bi SBi + ci ,

ak SF i + bk SBi + ck

(5)

where

ai =

1
|ω |

(cid:213)

k ∈ωi

ak , bi =

bk , ci =

1
|ω |

(cid:213)

k ∈ωi

1
|ω |

(cid:213)

k ∈ωi

ck .

With this modification, we can still have ∇q ≈ a∇F + b∇B + c∇I be-
cause (cid:16)
(cid:17) are the average of the filters, and their gradients
should be much smaller than that of I near strong edges.

ak , bk , ck

To determine the linear coefficients, we design a sub-network
to seek the solution. Specially, our network leverages a loss func-
tion including two parts, which makes the alpha predictions more
accurate. The first loss Lα measures the alpha matte, which is the
absolute difference between the ground truth alpha values and the
predicted alpha values at each pixel. And the second loss Lcolor
is
a compositional loss, which is the L2-norm loss function for the
predicted RGB foreground. Thus, we minimize the following cost
function:

L = Lα + Lcolor ,

(6)

where

(cid:114)

(cid:16)
дt − α i
α i
p

(cid:17)2

Li
α

=

+ ε2,
(cid:114)

Li
color

= (cid:213)

(cid:16)
дt I i
α i
j

− qi
j

(cid:17)2

+ ε2,

j ∈ {R,G, B }
= ak SF i + bk SBi + ck .

α i
p

This loss function is chosen for two reasons. Intuitively, we hope
to obtain an accurate alpha matte through the feathering block in
the end, thus we use the alpha matte loss Lα to learn the parameters.
On the other hand, the second loss Lcolor
is used to maintain the
information of the input image as much as possible. Since the score
maps SF and SB would lose the information of edges due to the
pixel-wise smoothing, we need to use the input image to recover
the lost detail of the edge. Therefore, we leverage the second loss
to guide the learning process. Despite existing deep learning
Lcolor
methods like [6, 28, 34] used the similar loss function to solve the
alpha matting problem, we have a totally different motivation and
a different solution. Their loss functions are used to estimate the
expression of alpha matting without considering the gradient drifts
for the convolution operation, which causes the pixel-wise smooth
alpha matte and loses the gradients of input image, while our loss
function is edge-preserving. Through the gradient propagation,
the parameters of feathering layer can be updated pixel-wise so
as to correct the edges. It is because that the guided filter is an
edge-preserving filter. However, other deep learning methods can
only update the parameters in the fixed windows. As a result, their
approaches would make the results pixel-wise smooth, while our
results are edge-preserving.

3.2.2 Guided Image Filter. The guided filter [11] proposed a

local linear model:

qi = ak Ii + bk , ∀i ∈ ωk ,

(7)

Figure 4: Architecture of the feathering block, which is used
to learn the filter refining the coarse binary mask. I is the
input image, S is the score maps output from the segmenta-
tion network, A is the alpha matte which is the final output
of the whole network, and ⊙ is the hadamard product.

From (3) we can get the derivative

∇q = a∇F + b∇B + c∇I,

(4)

where q = αI , F = ISF , B = ISB . It ensures that the feathering block
possesses the property of edge-preserving and matting adaptive. As
discussed above, both two score maps SF and SB would have strong
responses in the area of edge because of the uncertainty in these
areas. However, it is worth to note that the score maps of foreground
and background are allowed to have inaccurate responses when the
parameters a, b, c are trained well. In this case, we hope that the
parameters a, b become as small as possible, which means that the
inaccurate responses are suppressed. In other word, the feathering
block can preserve the edge as long as the absolute values of a,
b are set to small in the area of edge while c is predominant on
it. Similarly, if we want to segment the neck apart from the head,
the parameters a, b, c should be set to small in the area of neck.
Moreover, it is interesting that the feathering block performs like
ensemble learning because we can treat F , B, I as the classifiers and
the parameters a, b, c as the weights for classification.

When we apply the linear model to all sliding windows in the
entire image, the value αi is not the same in different windows.
We leverage the same strategy as He et al. [11]. After computing
(ak , bk , ck ) for all sliding windows in the image, we average all the

Figure 5: Examples of the filters learnt from the feathering block. (a) The input images. (b) The foregrounds of the original
images, which are calculated by the Eq. (1). (c) The weights ak of feathering block in Eq. (2) and Eq. (3). (d) The weights bk of
feathering block in Eq. (2) and Eq. (3). (e) The weights ck of feathering block in Eq. (2) and Eq. (3).

centered at the
where q is a linear transform of I in a window ωk
pixel k, and (ak , bk ) are some linear coefficients assumed to be
constant in ωk
. In order to determine the linear coefficients, they
minimize the following cost function in the window:

E (ak , bk ) = (cid:213)
i ∈ωk

(cid:16)

(ak Ii + bk − pi )2 + εa

(cid:17)

.

2
k

(8)

Our feathering block does not use the same form like [10] be-
cause we hope to obtain not only an edge-preserving filter, but also
a filter with the capacity of matting adaptive. It means the filters can
suppress the inaccurate response and obtain a finer matting result.
As a result, the guided filter needs to rely on an accurate binary
mask while our filter does not. Therefore, the feathering block can
be viewed as an extension of the guided filter. We extend the linear
transform of the image to the linear transform of the foreground
image, the background image as well as the whole image. More-
over, instead of solving the linear regression to obtain the linear
coefficients, we leverage a sub-network to optimize the cost func-
tion, which should be much faster and reduce the computational
complexity.

Figure 5 shows that our feathering block is closely to the guided
filter in [11]. Both the filter of ours and guided filter have high
response on the high variance regions. However, it is obvious that
there are great distinctions between our filter and guided filter. The
differences between our filter and guided filter can be summarized
into two aspects briefly:

1. Different inputs. The inputs of guided filter are the image as
well as the corresponding binary mask while our inputs are the
image and the score maps of the foreground and the background.
2. Different outputs. The output of guided filter is a feathering
results relied on an accurate binary mask, while our learnt guided
filter outputs an alpha matte or a foreground image, which possesses
the fault tolerance to the binary mask.

In fact, our feathering block is matting oriented, therefore, the
linear transform model of learnt guided filter in Eq. (3) is inferred
from the linear transform model of matting filter in Eq. (2). Thus, our
learnt guided filter has multiple product terms but no constant term.
Though two filters are derived in different process, they have the
same properties such as edge-preserving, which has been proved
in Eq. (4). Moreover, Figure 5 shows that our learnable guided filter
possesses adaptive matting capacity. The general matting methods
like [8, 17] would take the neck as the foreground if we define the
neck in the image as unknown region, because the gradient between
the face and neck is small. However, our method can divide the face
and neck into the foreground and background, respectively.

3.2.3 Attention Mechanism. Our proposed feathering block is
not only an extension of the guided filter, but also can be interpreted
as an attention mechanism. Figure 5 shows several examples of
guided filters learnt from the feathering block. Intuitively, it can
be interpreted as an attention mechanism, which pays different
attention to various parts according to factors. Specially, from the
examples in Figure 5, we can infer that the factor a pays more atten-
tion to the part of object’s body, the factor b pays more attention
to the part of background, and the factor c pays more attention to
the part of object’s head. Consequently, it can be inferred that the
factors a and b emphasize the matting problem locally while the
factor c considers the matting problem globally.

4 EXPERIMENTS
4.1 Dataset and Implementation
Dataset: We collect the primary dataset from [28], which is col-
lected from Flickr. After training on this dataset, we release our
app to obtain more data for training. Furthermore, we hired tens
of well-trained students to accomplish the annotation work. All
the ground truth alpha mattes are labelled with KNN matting [4]
firstly, and refined carefully with Photoshop quick selection.

Specially, we labelled the alpha matte in two ways. The first way
of labelling is same as [28, 34], which labels the whole human as
the foreground. The second way of labelling only labels the heads
of human as the foreground. The head labels help us demonstrate
that our proposed method possesses adaptive matting capacity
dividing the parts of head and neck precisely. After labelling process,
we collect 2,000 images with high-quality mattes, and split them
randomly into training and testing sets with 1,800 and 200 images
respectively.

For data augmentation, we adopt random mirror and random
resize between 0.75 and 1.5 for all images, and additionally add
random rotation between -30 and 30 degrees, and random Gaussian
blur. This comprehensive data augmentation scheme prevents the
network overfitting and greatly improves the performance of our
system to handle new images with possibly different scale, rotation,
noise and intensity.
Implementation details: We setup our model training and testing
experiments on Caffe platform [15]. With the model illustrated in
Figure 2, we use a stochastic gradient descent (SGD) solver with
cross-entropy loss function, batch size 256, momentum 0.99 and
weight decay 0.0005. We train our model without any further post-
processing module nor pre-training. The unknown weights are
initialized with random values. Specially, we train our network
with a three-stage strategy. Firstly, we train the light dense network
with a learning rate of 1e-3 and 1e-4 in the first 10k and the last
10k iterations, respectively. Secondly, the weights in light dense
network is fixed and we only train the feathering block with a
learning rate set to 1e-6 which will be divided by 10 after 10k
iterations. Finally, we train the whole network with a fixed learning
rate of 1e-7 for 20k iterations. We found that the three-stage training
strategy makes the training process more stable. All experiments
on computer are performed on a system of Core E5-2660 @2.60GHz
CPU and a single NVIDIA GeForce GTX TITAN X GPU with 12GB
memory. We also test our approach on the mobile phone with a
Qualcomm Snapdragon 820 MSM8996 CPU and Adreno 530 GPU.

4.2 Head Matting Dataset
Accuracy Measure. We select the gradient error and mean squared
error to measure matting quality, which can be expressed as:

G (cid:0)A , A дt (cid:1) =

1

K

(cid:213)

∇Ai − ∇A дt
i

(cid:13)
(cid:13)
(cid:13),

MSE (cid:0)A , A дt (cid:1) =

(cid:213)

(cid:0)A − A дt (cid:1)2

,

(cid:13)
(cid:13)
(cid:13)

i

i
1

K

(9)

(10)

where A is the predicted matte and A дt is the corresponding
ground truth, K is the number of pixels in A , ∇ is the operator to
compute gradients.

Method Comparison. We compare several automatic schemes
for matting and report the performance of these methods in Table 1.
From the results, we can draw the conclusion that our proposed
method LDN+FB is efficient and effective. Firstly, comparing the
portrait segmentation networks, LDN produces a coarse binary
mask, which increases 3 times gradient error and 3 to 5 times mean
squared error approximately, compared to the semantic segmen-
tation networks. However, LDN is 20 to 60 times faster than the
semantic segmentation networks on CPU. Secondly, taking our

Table 1: Results on Head Matting Dataset. We compare the
components of our proposed system with state-of-the-art se-
mantic segmentation networks Deeplab and PSPN [2, 36].
Light Dense Network (LDN) greatly increases the speed and
Feathering Block (FB) deceases the gradient error(Grad. Er-
ror) and mean squared error(MSE) greatly in our system. Ad-
ditionally, our feathering block has a better performance
than guided filter (GF).

Approaches

Deeplab101 [2]
PSPN101 [36]
PSPN50 [36]
LDN
Deeplab101+FB
PSPN101+FB
PSPN50+FB
LDN+GF
LDN+FB

CPU
(ms)

1243
1289
511
27
1424
1343
548
236
38

GPU
(ms)

73
76
44
11
78
83
52
220
13

Grad.
Error(×10−3)
3.61
3.88
3.92
9.14
3.46
3.37
3.64
4.66
3.85

MSE
(×10−3)
5.84
7.07
8.04
27.49
4.23
3.82
4.67
15.34
7.98

Table 2: Results on Human Matting Dataset of [28]. DAPM
means the approach of Deep Automatic Portrait Matting in
[28]. LDN means the light dense network and FB means the
feathering block. We report our speed on computer(comp.)
and mobile phone(phone.) respectively.

Approaches CPU

/comp.
(ms)

6000
38

GPU
/comp.
(ms)

CPU
/phone.
(ms)

GPU
/phone.
(ms)

600
13

-
140

-
62

Grad.
Error
(×10−3)
3.03
7.40

DAPM
LDN+FB

feathering block into consideration, we find that FB can refine the
portrait segmentation networks with little effort on CPU or GPU.
Moreover, the feathering block decreases the gradient error and
mean squared error of our proposed LDN greatly, which decreases
30% of the errors approximately. Furthermore, when compared to
the guided filter, it deceases 2 times of mean squared error. Thirdly,
our proposed LDN+FB structure have comparable accuracies with
the combinations of segmentation networks and feathering blocks,
while it is 15 to 40 times faster. Visually, we present the results
of binary mask, binary mask with guided filter, and alpha matte
from our feathering block in Figure 6. It is obvious that the binary
masks are very coarse which lose the edge shape and contain lots
of isolated holes. The binary mask with guided filter can fill the
holes thereby enhancing the performance of the mask, however, the
alpha mattes are over transparent due to the coarse binary mask.
In contrast, our proposed system achieves better performance by
making the foreground clearer and remaining the edge shape.

In order to illustrate the efficiency and effectiveness of the light
dense block and feathering block, we discuss them separably. Firstly,
comparing the four portrait segmentation networks, though our

Figure 6: Several results from binary mask, binary mask with guided filter, and alpha matte from our feathering block. (a)
The original images. (b) The ground truth foreground. (c) The foreground calculated by the binary mask. (d) The foreground
calculated by the binary mask with guided filter. (e) The foreground calculated by the binary mask with our feathering block.

light dense network has a low performance, it decreases the com-
puting time greatly, and it can produce a coarse segmentation with
8 convolutional layers. Secondly, the feathering block increases the
performances of four portrait segmentation networks. It decreases
the error of our proposed light dense network greatly, which holds a
comparable results with the best results and keeps a high speed. Ad-
ditionally, comparing the performances between guided filter and
feathering block, we can infer that feathering block outperforms
the guided filter in accuracy while keeping a high speed.

4.3 Human Matting Dataset
Furthermore, in order to demonstrate the efficiency and effective-
ness of our proposed system. We train our system on the dataset of
[28], which contains a training set and a testing set with 1,700 and
300 images respectively, and compare our result with that of their
proposed methods.

We report the performance of these automatic matting methods
in Table 2. From the reported results, we find that our proposed
method increases 4.37 × 10−3 in gradient error, compared to the
error reported in [28]. However, our method decreases the time
cost of 5962ms on CPU and 587ms on GPU. Consequently, it can
be illustrated that our method increases the matting speed greatly
while keeping a comparable result with the general alpha matting
methods. Moreover, the automatic matting approach proposed in
[28] can not run on the CPU of mobile phone because it needs a lot
of time to infer the alpha matte. In contrast, our method can not
only run on the GPU of mobile phone in real-time, but also has the
capacity to run on the CPU of mobile phone.

4.4 Comparison with Fabby
Additionally, we compare our system with Fabby1, which is a real-
time face editing app. The visual results are presented in Figure 7. It
is shown that we have better results than that of Fabby. Moreover,
we found that Fabby cannot locate the objects such as the cases in
former three columns and it fails to process some special cases like
the faults in the last three columns. In contrast, our method can
hold these cases well.

From the former three columns in Figure 7, we can infer that
Fabby needs to detect the objects first, and then computes the alpha
matte. As a result, sometimes it is unable to calculate the alpha
matte of the whole object. Conversely, our method can calculate
the alpha matte of all pixels in an image and thus obtain the opacity
of the whole object. Furthermore, Fabby fails to divide the similar
background from foreground for the case of the fourth column, and
fails to segment the foreground such as collar and shoulder in last
two columns, while our method outperforms Fabby in these cases.
Consequently, it can be inferred that our light matting network has
a more reliable alpha matte than that of Fabby.

5 APPLICATIONS
Since our proposed system is fully automatic and can run fully real-
time on mobile phone, a number of applications are enabled, such
as fast stylization, color transform, background editing and depth-
of-field. Moreover, it can not only process the image matting, but
also real-time video matting due to its fast speed. Thus, we establish
an app for real-time portrait animation on mobile phone, which

1The app of Fabby can be downloaded on the site www.fab.by

Figure 7: The results between our system and real-time editing app Fabby. First row: The input images. Second row: The results
from the app Fabby. Third row: The results from our system.

and matting adaptive capacity. Finally, a portrait animation system
based on fast deep matting is built on mobile devices. Our method
does not need any interaction and can realize real-time matting
on the mobile phone. The experiments show that our real-time
automatic matting method achieves comparable results with the
state-of-the-art matting solvers. However, our system fails to distin-
guish tiny details in the hair areas because we have downsampled
the input image. The downsampling operation would provide a
more complete result but lose the detailed information. We treat
this as the limitation of our method. In future, we will try to improve
our method for higher accuracy by combining multiple technolo-
gies like object detection [5, 18, 24, 25], image retargeting [22, 33]
and face detection [12, 23, 26].

REFERENCES
[1] Yagız Aksoy, Tunç Ozan Aydın, and Marc Pollefeys. 2017. Designing Effective
Inter-Pixel Information Flow for Natural Image Matting. In Computer Vision and
Pattern Recognition (CVPR), 2017.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
Alan L Yuille. 2015. Semantic image segmentation with deep convolutional nets
and fully connected crfs. In International Conference on Learning Representations
(ICLR), 2015.

[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
Alan L. Yuille. 2017. DeepLab: Semantic Image Segmentation with Deep Convo-
lutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE transactions
on pattern analysis and machine intelligence (2017).

[4] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. 2013. KNN matting.

IEEE
transactions on pattern analysis and machine intelligence 35, 9 (2013), 2175–2188.
[5] Yingying Chen, Jinqiao Wang, Min Xu, Xiangjian He, and Hanqing Lu. 2016. A
unified model sharing framework for moving object detection. Signal Processing
124 (2016), 72–80.

[6] Donghyeon Cho, Yu Wing Tai, and Inso Kweon. 2016. Natural Image Matting
Using Deep Convolutional Neural Networks. In European Conference on Computer
Vision (ECCV), 2016. 626–643.

[7] Yung-Yu Chuang, Brian Curless, David H Salesin, and Richard Szeliski. 2001. A
bayesian approach to digital matting. In Computer Vision and Pattern Recognition
(CVPR), 2001. Proceedings of the 2001 IEEE Computer Society Conference on, Vol. 2.
IEEE, II–II.

Figure 8: Examples of our real-time portrait animation app.

is illustrated in Figure 8. Further, we test the speed of our method
on the mobile phone with a Qualcomm Snapdragon 820 MSM8996
CPU and an Adreno 530 GPU. When tests on mobile phone, it runs
140ms on the CPU. Furthermore, after applying render script [16] to
optimize the speed with GPU in Android, our approach takes 62ms
on the GPU. Consequently, we can conclude that our proposed
method is efficient and effective on mobile devices.

6 CONCLUSIONS AND FUTURE WORK
In this paper, we propose a fast and effective method for image
and video matting. Our novel insight is that refining the coarse
segmentation mask can be fast and effective than the general im-
age matting methods. We developed a real-time automatic matting
method with a light dense network and a feathering block. Besides,
the filter learnt by the feathering block possesses edge-preserving

[34] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. 2017. Deep Image Matting.

In Computer Vision and Pattern Recognition (CVPR), 2017.

[35] Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilated
convolutions. In International Conference on Learning Representations (ICLR),
2016.

[36] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. 2017.
Pyramid Scene Parsing Network. In Computer Vision and Pattern Recognition
(CVPR), 2017.

[37] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
Torralba. 2016. Semantic Understanding of Scenes through the ADE20K Dataset.
CoRR abs/1608.05442 (2016).

[8] Eduardo SL Gastal and Manuel M Oliveira. 2010. Shared sampling for real-
time alpha matting. In Computer Graphics Forum, Vol. 29. Wiley Online Library,
575–584.

[9] Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and
Roberto Cipolla. 2015. SceneNet: Understanding Real World Indoor Scenes
With Synthetic Data. CoRR abs/1511.07041 (2015).

[10] Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Fast matting using large kernel
matting laplacian matrices. In Computer Vision and Pattern Recognition (CVPR),
2010 IEEE Conference on. IEEE, 2165–2172.

[11] Kaiming He, Jian Sun, and Xiaoou Tang. 2013. Guided image filtering.

IEEE
transactions on pattern analysis and machine intelligence 35, 6 (2013), 1397–1409.
[12] Peiyun Hu and Deva Ramanan. 2017. Finding Tiny Faces. In Computer Vision

and Pattern Recognition (CVPR), 2017.

[13] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.
2017. Densely connected convolutional networks. In Computer Vision and Pattern
Recognition (CVPR), 2017.

[14] Simon Jégou, Michal Drozdzal, David Vázquez, Adriana Romero, and Yoshua
Bengio. 2017. The One Hundred Layers Tiramisu: Fully Convolutional DenseNets
for Semantic Segmentation. In Workshop on Computer Vision in Vehicle Technology
CVPR, 2017.

[15] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolu-
tional architecture for fast feature embedding. In Proceedings of the 22nd ACM
international conference on Multimedia. ACM, 675–678.

[16] Seyyed Salar Latifi Oskouei, Hossein Golestani, Matin Hashemi, and Soheil Ghiasi.
2016. CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional
Neural Networks on Android. In Proceedings of the 2016 ACM on Multimedia
Conference. 1201–1205.

[17] Anat Levin, Dani Lischinski, and Yair Weiss. 2008. A closed-form solution
to natural image matting. IEEE Transactions on Pattern Analysis and Machine
Intelligence 30, 2 (2008), 228–242.

[18] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed,
Cheng-Yang Fu, and Alexander C. Berg. 2016. SSD: Single Shot MultiBox Detector.
In ECCV.

[19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional
networks for semantic segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 3431–3440.

[20] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. 2016.
Enet: A deep neural network architecture for real-time semantic segmentation.
arXiv preprint arXiv:1606.02147 (2016).

[21] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. 2017. Large Ker-
nel Matters - Improve Semantic Segmentation by Global Convolutional Network.
In Computer Vision and Pattern Recognition (CVPR), 2017.

[22] Shaoyu Qi, Yu-Tseh Chi, Adrian M. Peter, and Jeffrey Ho. 2016. CASAIR: Content
and Shape-Aware Image Retargeting and Its Applications. IEEE Transactions on
Image Processing 25 (2016), 2222–2232.

[23] Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. 2016. Joint Training of Cascaded
CNN for Face Detection. 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2016), 3456–3465.

[24] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: Better, Faster, Stronger. In

Computer Vision and Pattern Recognition (CVPR), 2017.

[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks.
IEEE
Transactions on Pattern Analysis and Machine Intelligence 39 (2015), 1137–1149.
[26] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A
unified embedding for face recognition and clustering. 2015 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2015), 815–823.

[27] Xiaoyong Shen, Aaron Hertzmann, Jiaya Jia, Sylvain Paris, Brian Price, Eli Shecht-
man, and Ian Sachs. 2016. Automatic portrait segmentation for image stylization.
In Computer Graphics Forum, Vol. 35. Wiley Online Library, 93–102.

[28] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and Jiaya Jia. 2016. Deep
Automatic Portrait Matting. In European Conference on Computer Vision. Springer,
92–107.

[29] Jian Sun, Jiaya Jia, Chi-Keung Tang, and Heung-Yeung Shum. 2004. Poisson
matting. In ACM Transactions on Graphics (ToG), Vol. 23. ACM, 315–321.
[30] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex A. Alemi. 2016.
Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learn-
ing. In ICLR 2016 Workshop.

[31] Jinqiao Wang, Ling-Yu Duan, Qingshan Liu, Hanqing Lu, and Jesse S. Jin. 2008. A
Multimodal Scheme for Program Segmentation and Representation in Broadcast
Video Streams. IEEE Trans. Multimedia 10 (2008), 393–408.

[32] Jinqiao Wang, Wei Fu, Hanqing Lu, and Songde Ma. 2014. Bilayer Sparse Topic
Model for Scene Analysis in Imbalanced Surveillance Videos. IEEE Transactions
on Image Processing 23 (2014), 5198–5208.

[33] Jinqiao Wang, Zhan Qu, Yingying Chen, Tao Mei, Min Xu, La Zhang, and Han-
qing Lu. 2016. Adaptive Content Condensation Based on Grid Optimization for
Thumbnail Image Generation. IEEE Trans. Circuits Syst. Video Techn. 26 (2016),
2079–2092.

