Bottleneck Conditional Density Estimation

Rui Shu 1 Hung H. Bui 2 Mohammad Ghavamzadeh 3

7
1
0
2
 
n
u
J
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
8
6
5
8
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We introduce a new framework for training deep
generative models for high-dimensional condi-
tional density estimation. The Bottleneck Con-
ditional Density Estimator (BCDE) is a vari-
ant of the conditional variational autoencoder
(CVAE) that employs layer(s) of stochastic vari-
ables as the bottleneck between the input x
and target y, where both are high-dimensional.
Crucially, we propose a new hybrid training
method that blends the conditional generative
model with a joint generative model. Hybrid
blending is the key to effective training of the
BCDE, which avoids overﬁtting and provides a
novel mechanism for leveraging unlabeled data.
We show that our hybrid training procedure en-
ables models to achieve competitive results in
the MNIST quadrant prediction task in the fully-
supervised setting, and sets new benchmarks in
the semi-supervised regime for MNIST, SVHN,
and CelebA.

1. Introduction

Conditional density estimation (CDE) refers to the prob-
lem of estimating a conditional density p(y|x) for the input
x and target y. In contrast to classiﬁcation where the tar-
get y is simply a discrete class label, y is typically contin-
uous or high-dimensional in CDE. Furthermore, we want
to estimate the full conditional density (as opposed to its
conditional mean in regression), an important task the con-
ditional distribution has multiple modes. CDE problems
in which both x and y are high-dimensional have a wide
range of important applications, including video prediction,
cross-modality prediction (e.g.
image-to-caption), model
estimation in model-based reinforcement learning, and so
on.

1Stanford University 2Adobe Research 3DeepMind (The work
was done when all the authors were with Adobe Research). Cor-
respondence to: Rui Shu <ruishu@stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Classical non-parametric conditional density estimators
typically rely on local Euclidean distance in the original in-
put and target spaces (Holmes et al., 2012). This approach
quickly becomes ineffective in high-dimensions from both
computational and statistical points of view. Recent ad-
vances in deep generative models have led to new para-
metric models for high-dimensional CDE tasks, namely the
conditional variational autoencoders (CVAE) (Sohn et al.,
2015). CVAEs have been applied to a variety of problems,
such as MNIST quadrant prediction, segmentation (Sohn
et al., 2015), attribute-based image generation (Yan et al.,
2015), and machine translation (Zhang et al., 2016).

But CVAEs suffer from two statistical deﬁciencies. First,
they do not learn the distribution of the input x. We ar-
gue that in the case of high-dimensional input x where
there might exist a low-dimensional representation (such
as a low-dimensional manifold) of the data, recovering this
structure is important, even if the task at hand is to learn the
conditional density p(y|x). Otherwise, the model is suscep-
tible to overﬁtting. Second, for many CDE tasks, the ac-
quisition of labeled points is costly, motivating the need for
semi-supervised CDE. A purely conditional model would
not be able to utilize any available unlabeled data.1 We
note that while variational methods (Kingma & Welling,
2013; Rezende et al., 2014) have been applied to semi-
supervised classiﬁcation (where y is a class label) (Kingma
et al., 2014; Maaløe et al., 2016), semi-supervised CDE
(where y is high-dimensional) remains an open problem.

We focus on a set of deep conditional generative mod-
els, which we call bottleneck conditional density estima-
tors (BCDEs). In BCDEs, the input x inﬂuences the target
y via layers of bottleneck stochastic variables z = {zi} in
the generative path. The BCDE naturally has a joint gen-
erative sibling model which we denote the bottleneck joint
density estimator (BJDE), where the bottleneck z gener-
ates x and y independently. Motivated by Lasserre et al.
(2006), we propose a hybrid training framework that regu-
larizes the conditionally-trained BCDE parameters toward
the jointly-trained BJDE parameters. This is the key fea-
ture that enables semi-supervised learning for conditional
density estimation in the BCDEs.

1We deﬁne a “labeled point” to be a paired (x, y) sample, and

an “unlabeled point” to be unpaired x or y.

Bottleneck Conditional Density Estimation

Our BCDE hybrid training framework is a novel approach
for leveraging unlabeled data for conditional density es-
timation. Using our BCDE hybrid training framework,
we establish new benchmarks for the quadrant prediction
task (Sohn et al., 2015) in the semi-supervised regime for
MNIST, SVHN, and CelebA. Our experiments show that
1) hybrid training is competitive for fully-supervised CDE,
2) in semi-supervised CDE, hybrid training helps to avoid
overﬁtting, performs signiﬁcantly better than conditional
training with unlabeled data pre-training, and achieves
state-of-the-art results, and 3) hybrid training encourages
the model to learn better and more robust representations.

2. Background

2.1. Variational Autoencoders

Variational Autoencoder (VAE) is a deep generative model
for density estimation. It consists of a latent variable z with
unit Gaussian prior z ∼ N (0, Ik), which in turn generates
an observable vector x. The observation is usually con-
ditionally Gaussian x|z ∼ N (cid:0)µθ(z), diag(σ2
θ (z)(cid:1), where
µ and σ2 are neural networks whose parameters are rep-
resented by θ.2 VAE can be seen as a non-linear gen-
eralization of the probabilistic PCA (Tipping & Bishop,
1999), and thus, can recover non-linear manifolds in the
data. However, VAE’s ﬂexibility makes posterior inference
of the latent variables intractable. This inference issue is
addressed via a recognition model qφ(z|x), which serves
as an amortized variational approximation of the intractable
posterior pθ(z|x). Learning in VAE’s is done by jointly op-
timizing the parameters of both the generative and recogni-
tion models so as to maximize an objective that resembles
an autoencoder regularized reconstruction loss (Kingma &
Welling, 2013), i.e.,

Eqφ(z|x)

(cid:2) ln pθ(x|z)(cid:3) − DKL

(cid:0)qφ(z|x) || p(z)(cid:1).

(1)

sup
θ,φ

We note that the objective Eq. (1) can be rewritten in the
following form that exposes its connection to the varia-
tional lower bound of the log-likelihood

(cid:16)

sup
θ

ln pθ(x) − inf
φ

(cid:0)qφ(z|x) || pθ(z|x)(cid:1)(cid:17)

DKL

(cid:20)

Eqφ(z|x)

ln

pθ(x, z)
pφ(z|x)

(cid:21)

.

= sup
θ,φ

(2)

We make two remarks regarding the minimization of the
(cid:0)qφ(z|x) || pθ(z|x)(cid:1) in Eq. 2. First, when q(·|·) is
term DKL
a conditionally independent Gaussian, this approximation
is at best as good as the mean-ﬁeld approximation that min-
(cid:0)q || pθ(z|x)(cid:1) over all independent Gaussian
imizes DKL

2For discrete x, one can use a deep network to parameterize a

Bernoulli or a discretized logistic distribution.

q’s. Second, this term serves as a form of amortized pos-
terior regularization that encourages the posterior pθ(z|x)
to be close to an amortized variational family (Dayan et al.,
1995; Ganchev et al., 2010; Hinton et al., 1995). In prac-
tice, both θ and φ are jointly optimized in Eq. (1), and
the reparameterization trick (Kingma & Welling, 2013) is
used to transform the expectation over z ∼ qφ(z|x) into
(cid:15) ∼ N (0, Ik); z = µφ(x) + diag(cid:0)σ2
φ(x)(cid:1)(cid:15), which leads to
an easily obtained stochastic gradient.

2.2. Conditional VAEs (CVAEs)

In Sohn et al. (2015), the authors introduce the conditional
version of variational autoencoders. The conditional gen-
erative model is similar to VAE, except that the latent vari-
able z and the observed vector y are both conditioned on
the input x. The conditional generative path is
z,θ(x)(cid:1)(cid:17)

pθ(z | x) = N

(3)

(cid:16)

z | µz,θ(x), diag(cid:0)σ2
y | µy,θ(x, z), diag(cid:0)σ2

(cid:16)

y,θ(x, z)(cid:1)(cid:17)

,

(4)

pθ(y | x, z) = N

and when we use a Bernoulli decoder is

pθ(y | x, z) = Ber(cid:0)y | µy,θ(x, z)(cid:1).

(5)

Here, θ denotes the parameters of the neural networks used
in the generative path. The CVAE is trained by maximizing
a lower bound of the conditional likelihood

ln pθ(y|x) ≥ Eqφ(z|x,y)

ln

(cid:20)

pθ(z|x)pθ(y|x, z)
qφ(z|x, y)

(cid:21)

,

(6)

but with a recognition network qφ(z|x, y), which is typi-
z|µφ(x, y), diag(cid:0)σ2
cally Gaussian N
, and takes
both x and y as input.

φ(x, y)(cid:1)(cid:17)

(cid:16)

2.3. Blending Generative and Discriminative

It is well-known that a generative model may yield sub-
optimal performance when compared to the same model
trained discriminatively (Ng & Jordan, 2002), a phe-
nomenon attributable to the generative model being mis-
speciﬁed (Lasserre et al., 2006). However, generative mod-
els can easily handle unlabeled data in semi-supervised set-
ting. This is the main motivation behind blending genera-
tive and discriminative models. Lasserre et al. (2006) pro-
posed a principled method for hybrid blending by duplicat-
ing the parameter of the generative model into a discrimi-
natively trained θ and a generatively trained ˜θ, i.e.,

p(Xl, Yl, Xu, ˜θ, θ) = p(˜θ, θ)p(Xu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).
(7)
The discriminatively trained parameter θ is regularized to-
ward the generatively trained parameter ˜θ via a prior p(˜θ, θ)
that prefers small (cid:107)θ − ˜θ(cid:107)2. As a result, in addition to

Bottleneck Conditional Density Estimation

BJDE

z

Regularization

BCDE

z

x

y

x

y

Unpaired Data
{xi} ∪ {yi}

Paired Data
{xi, yi}

Figure 1. The hybrid training procedure that regularizes BCDE towards BJDE. This regularization enables the BCDE to indirectly
leverage unpaired x and y for conditional density estimation.

learning from the labeled data (Xl, Yl), the discrimina-
tive parameter θ can be informed by the unlabeled data
Xu via ˜θ, enabling a form of semi-supervised discrimina-
tively trained generative model. However, this approach is
limited to simple generative models (e.g., naive Bayes and
HMMs), where exact inference of p(y|x, θ) is tractable.

3. Neural Bottleneck Conditional Density

Estimation

While Sohn et al. (2015) has successfully applied the
CVAE to CDE, CVAE suffers from two limitations. First,
the CVAE does not learn the distribution of its input x,
and thus, is far more susceptible to overﬁtting. Second,
it cannot incorporate unlabeled data. To resolve these lim-
itations, we propose a new approach to high-dimensional
CDE that blends the discriminative model that learns the
conditional distribution p(y|x), with a generative model
that learns the joint distribution p(x, y).

3.1. Overview

Figure 1 provides a high-level overview of our approach
that consists of a new architecture and a new training proce-
dure. Our new architecture imposes a bottleneck constraint,
resulting a class of conditional density estimators, we call it
bottleneck conditional density estimators (BCDEs). Unlike
CVAE, the BCDE generative path prevents x from directly
inﬂuencing y. Following the conditional training paradigm
in Sohn et al. (2015), conditional/discriminative training of
the BCDE means maximizing the lower bound of a condi-
tional likelihood similar to (6),i.e.,

ln pθ(y|x) ≥ C(θ, φ; x, y)
(cid:20)

= Eqφ(z|x,y)

ln

pθ(z|x)pθ(y|z)
qφ(z|x, y)

(cid:21)

.

When trained over a dataset of paired (X, Y) samples, the
overall conditional training objective is

C(θ, φ; X, Y) =

C(θ, φ; x, y).

(8)

(cid:88)

x,y∈X,Y

However, this approach suffers from the same limitations as
CVAE and imposes a bottleneck that limits the ﬂexibility of
the generative model. Instead, we propose a hybrid training
framework that takes advantage of the bottleneck architec-
ture to avoid overﬁtting and supports semi-supervision.

One component in our hybrid training procedure tackles the
problem of estimating the joint density p(x, y). To do this,
we use the joint counterpart of the BCDE: the bottleneck
joint density estimator (BJDE). Unlike conditional models,
the BJDE allows us to incorporate unpaired x and y data
during training. Thus, the BJDE can be trained in a semi-
supervised fashion. We will also show that the BJDE is
well-suited to factored inference (see Section 3.4), i.e., a
factorization procedure that makes the parameter space of
the recognition model more compact.

The BJDE also serves as a way to regularize the BCDE,
where the regularization constraint can be viewed as soft-
tying between the parameters of these two models’ gen-
erative and recognition networks. Via this regularization,
BCDE beneﬁts from unpaired x and y for conditional den-
sity estimation.

3.2. Bottleneck Joint Density Estimation

In the BJDE, we wish to learn the joint distribution of x
and y. The bottleneck is introduced in the generative path
via the bottleneck variable z, which points to x and y (see
Figs. 2(a) to 2(c)). Thus, the variational lower bound of the

Bottleneck Conditional Density Estimation

z

y

y

y

z

x

z

x

z

x

(a) Joint: (x)

(b) Joint: (y)

(c) Joint: (x, y)

(d) Conditional: (x, y)

Figure 2. The joint and conditional components of the BCDE. Dotted lines represent recognition models. The conditional model param-
eters are regularized toward the joint model’s. The natural pairing of the conditional and joint parameters is described in Table 1.

Standard

Factored

BJDE:

BCDE:

BJDE:

BCDE:

q ˜φ(z|x, y)
qφ(z|x, y)

−

−

q ˜φ(z|y)
−
ˆ(cid:96) ˜φ(z; y)
ˆ(cid:96) ˜φ(z; y)

q ˜φ(z|x)
pθ(z|x)

p˜θ(y|z)
pθ(y|z)

p˜θ(x|z)
−

q ˜φ(z|x)

p˜θ(y|z)

p˜θ(x|z)

pθ(z|x)

pθ(y|z)

−

Table 1. Soft parameter tying between the BJDE and BCDE. For each network within the BCDE, there is a corresponding network within
the BJDE. We show the correspondence among the networks with and without the application of factored inference. We regularize all
the BCDE networks to their corresponding BJDE network parameters.

joint likelihood is

ln p˜θ(x, y) ≥ Jxy(˜θ, ˜φ; x, y)

(cid:34)

= Eq ˜φ(z|x,y)

ln

p(z)p˜θ(x|z)p˜θ(y|z)
q ˜φ(z|x, y)

(cid:35)

.

(9)

We use {˜θ, ˜φ} to indicate the parameters of the BJDE net-
works and reserve {θ, φ} for the BCDE parameters. For
samples in which x or y is unobserved, we will need to
compute the variational lower bound for the marginal like-
lihoods. Here, the bottleneck plays a critical role.
If x
were to directly inﬂuence y in a non-trivial manner, any
attempt to incorporate unlabeled y would require the recog-
nition model to infer the unobserved x from the observed
y—a conditional density estimation problem which might
be as hard as our original task.
In the bottleneck archi-
tecture, the conditional independence of x and y given z
implies that only the low-dimensional bottleneck needs to
be marginalized. Thus, the usual variational lower bounds
for the marginal likelihoods yield

ln p˜θ(x) ≥ Jx(˜θ, ˜φ; x) = Eq ˜φ(z|x)

ln

ln p˜θ(y) ≥ Jy(˜θ, ˜φ; y) = Eq ˜φ(z|y)

ln

(cid:34)

(cid:34)

p(z)p˜θ(x|z)
q ˜φ(z|x)

p(z)p˜θ(y|z)
q ˜φ(z|y)

(cid:35)

(cid:35)

,

.

Since z takes on the task of reconstructing both x and y, the
BJDE is sensitive to the distributions of x and y and learns
a joint manifold over the two data sources. Thus, the BJDE
provides the following beneﬁts: 1) learning the distribution

of x makes the inference of z given x robust to perturba-
tions in the inputs, 2) z becomes a joint-embedding of x
and y, 3) the model can leverage unlabeled data. Following
the convention in Eq. (8), the joint training objectives is

J (˜θ, ˜φ; Xu, Yu, Xl, Yl) =

(10)

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) + Jxy(˜θ, ˜φ; Xl, Yl),

where (Xl, Yl) is a dataset of paired (x, y) samples, and
Xu and Yu are datasets of unpaired samples.

3.3. Blending Joint and Conditional Deep Models

Because of potential model mis-speciﬁcations, the BJDE is
not expected to yield good performance if applied to the
conditional task. Thus, we aim to blend the BJDE and
BCDE models in the spirit of Lasserre et al. (2006). How-
ever, we note that (7) is not directly applicable since the
BCDE and BJDE are two different models, and not two
different views (discriminative and generative) of the same
model. Therefore, it is not immediately clear how to tie the
BCDE and BJDE parameters together. Further, these mod-
els involve conditional probabilities parameterized by deep
networks and have no closed form for inference.

Any natural prior for the BCDE parameter θ and the BJDE
parameter ˜θ should encourage pBCDE(y|x, θ) to be close to
pBJDE(y|x, ˜θ). In the presence of the latent variable z, it is
then natural to encourage p(z|x, θ) to be close to p(z|x, ˜θ)
and p(y|z, θ) to be close to p(y|z, ˜θ). However, enforc-
ing the former condition is intractable as we do not have a
closed form for pBJDE(z|x, ˜θ). Fortunately, an approxima-

Bottleneck Conditional Density Estimation

tion of pBJDE(z|x, ˜θ) is provided by the recognition model
q(z|x, ˜φ). Thus, we propose to softly tie together the pa-
rameters of networks deﬁning p(z|x, θ) and q(z|x, ˜φ). This
strategy effectively leads to a joint prior over the model net-
work parameters, as well as the recognition network param-
eters p( ˜φ, ˜θ, φ, θ).

As a result, we arrive at the following hybrid blending of
deep stochastic models and its variational lower bound
ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) ≥ ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl).

(11)

We interpret ln p(˜θ, ˜φ, θ, φ) as a (cid:96)2-regularization term that
softly ties the joint parameters (˜θ, ˜φ) and conditional pa-
rameters (θ, φ) in an appropriate way. For the BCDE and
BJDE, there is a natural one-to-one mapping from the con-
ditional parameters to a subset of the joint parameters.
For the joint model described in Fig. 2(c) and conditional
model in Fig. 2(d), the parameter pairings are provided in
Table 1. Formally, we deﬁne γ = {θ, φ} and use the index
γa|b to denote the parameter of the neural network on the
Bayesian network link b → a in the BCDE. For example
γz|x = θz|x, γz|x,y = φz|x,y. Similarly, let ˜γ = {˜θ, ˜φ}.
In the BJDE, the same notation yields ˜γz|x = ˜φz|x. The
hybrid blending regularization term can be written as

ln p(θ, φ, ˜θ, ˜φ) = −

(cid:107)γi − ˜γi(cid:107)2

2 + const,

(12)

λ
2

(cid:88)

i∈I

where I denotes the set of common indices of the joint and
conditional parameters. When the index is z|x, it effec-
tively means that p(z|x, θ) is softly tied to q(z|x, ˜θ), i.e.,

(cid:107)γz|x − ˜γz|x(cid:107)2

2 = (cid:107)θz|x − ˜φz|x(cid:107)2
2 .

Setting λ = 0 unties the BCDE from the BJDE, and effec-
tively yields to a conditionally trained BCDE, while letting
λ → ∞ forces the corresponding parameters of the BCDE
and BJDE to be identical.

Interestingly, Eq. (11) does not contain the term Jxy. Since
explicit training of Jxy may lead to learning a better joint
embedding in the space of z, we note the following general-
ization of Eq. (11) that trades off the contribution between
Jxy and [Jx + C],

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ)

≥ H(˜θ, ˜φ, θ, φ; Xl, Yl, Xu, Yu)
= ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
α · Jxy(˜θ, ˜φ; Xl, Yl) +

Intuitively,
the equation computes the lower bound of
p(Xl, Yl), either using the joint parameters ˜θ, ˜φ or factor-
izes p(Xl, Yl) into p(Xl)p(Yl | Xl) before computing the
lower bound of p(Yl | Xl) with the conditional parame-
ters. A proof that the lower bound holds for any 0 ≤ α ≤ 1
is provided in Appendix B. For simplicity, we set α = 0.5
and do not tune α in our experiments.

3.4. Factored Inference

The inference network qφ(z|x, y) is usually parameterized
as a single neural network that takes both x and y as input.
Using the precision-weighted merging scheme proposed by
Sønderby et al. (2016), we also consider an alternative pa-
rameterization of qφ(z|x, y) that takes a weighted-average
of the Gaussian distribution qφ(z|x) and a Gaussian like-
lihood term ˆ(cid:96)(z; y) (see Appendix A). Doing so offers a
more compact recognition model and more sharing param-
eters between the BCDE and BJDE (e.g., see the bottom
two rows in Table 1), but at the cost of lower ﬂexibility for
the variational family qφ(z|x, y).

4. Experiments

We evaluated the performance of our hybrid training proce-
dure on the permutation-invariant quadrant prediction task
(Sohn et al., 2014; Sohn et al., 2015) for MNIST, SVHN,
and CelebA. The quadrant prediction task is a conditional
density estimation problem where an image data set is par-
tially occluded. The model is given the observed region and
is evaluated by its perplexity on the occluded region. The
quadrant prediction task consists of four sub-tasks depend-
ing on the degree of partial observability. 1-quadrant pre-
diction: the bottom left quadrant is observed. 2-quadrant
prediction: the left half is observed. 3-quadrant prediction:
the bottom right quadrant is not observed. Top-down pre-
diction: the top half is observed.

i=1

In the fully-supervised case, the original MNIST train-
i}50000
ing set {x(cid:48)
is converted into our CDE training set
i=1
{Xl, Yl} = {xi, yi}50000
by splitting each image into
its observed x and unobserved y regions according to the
quadrant prediction task. Note that the training set does not
contain the original class label information. In the nl-label
semi-supervised case, we randomly sub-sampled nl pairs
to create our labeled training set {xi, yi}nl
i=1. The remain-
ing nu paired samples are decoupled and put into our un-
labeled training sets Xu = {xi}nu
i=1. Test
performance is the conditional density estimation perfor-
mance on the entire test set, which is also split into input
x and target y according to the quadrant prediction task.
Analogous procedure is used for SVHN and CelebA.

i=1 , Yu = {yi}nu

(1 − α) ·

(cid:104)

(cid:105)
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl)

. (13)

For comparison against Sohn et al. (2015), we evalu-
ate the performance of our models on the MNIST 1-

Bottleneck Conditional Density Estimation

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

63.91
62.45 ± 0.02
62.00 ± 0.02
62.16 ± 0.03
62.81 ± 0.05

-
64.50 ± 0.03
63.27 ± 0.04
62.90 ± 0.02
63.47 ± 0.02

-
68.23 ± 0.05
65.14 ± 0.05
64.08 ± 0.03
64.16 ± 0.02

-
71.66 ± 0.06
67.13 ± 0.04
65.10 ± 0.03
64.64 ± 0.05

Table 2. MNIST quadrant prediction task: 1-quadrant. We report the test set loss (IW=100) and standard error.

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

44.73
43.91 ± 0.01
43.53 ± 0.02
43.56 ± 0.02
44.07 ± 0.02

-
45.49 ± 0.03
44.42 ± 0.04
44.10 ± 0.02
44.41 ± 0.02

-
48.16 ± 0.02
45.81 ± 0.01
45.23 ± 0.02
45.02 ± 0.04

-
50.83 ± 0.04
47.49 ± 0.06
46.39 ± 0.03
45.86 ± 0.06

Table 3. MNIST quadrant prediction task: 2-quadrant.

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

20.95
20.64 ± 0.01
20.37 ± 0.01
20.31 ± 0.01
20.43 ± 0.01

-
21.27 ± 0.01
20.87 ± 0.02
20.69 ± 0.02
20.56 ± 0.01

-
22.44 ± 0.03
21.65 ± 0.02
21.36 ± 0.02
21.16 ± 0.01

-
23.72 ± 0.04
22.32 ± 0.05
22.27 ± 0.02
21.81 ± 0.03

Table 4. MNIST quadrant prediction task: 3-quadrant.

Models

nl = 10000

nl = 5000

Models

nl = 20000

nl = 10000

BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

4657 ± 48
4547 ± 23
4213 ± 21
4700 ± 146

4845 ± 33
4627 ± 13
4392 ± 13
5030 ± 165

BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

5805 ± 2
5784.8 ± 0.5
5778.6 ± 0.4
5776.1 ± 0.3

5817 ± 3
5793 ± 1
5781.3 ± 0.5
5780.3 ± 0.6

Table 5. SVHN prediction task: Top-Down.

Table 6. CelebA prediction task: Top-Down.

quadrant, 2-quadrant, and 3-quadrant prediction tasks. The
MNIST digits are statically-binarized by sampling from
the Bernoulli distribution according to their pixel values
(Salakhutdinov & Murray, 2008). We use a sigmoid layer
to learn the parameter of the Bernoulli observation model.

We provide the performance on the top-down prediction
task for SVHN and CelebA. We used a discretized logistic
observation model Kingma et al. (2016) to model the pixel
values for SVHN and a Gaussian observation model with
ﬁxed variance for CelebA. For numerical stability, we rely
on the implementation of the discretized logistic distribu-
tion described in Salimans et al. (2017).

In all cases, we extracted a validation set of 10000 samples
for hyperparameter tuning. While our training objective
uses a single (IW=1) importance-weighted sample (Burda
et al., 2015), we measure performance using IW=100 to
get a tighter bound on the test log-likelihood (Sohn et al.,
2015). We run replicates of all experiments and report the
mean performance with standard errors. For a more ex-
pressive variational family (Ranganath et al., 2015), we use
two stochastic layers in the BCDE and perform inference

via top-down inference (Sønderby et al., 2016). We use
multi-layered perceptrons (MLPs) for MNIST and SVHN,
and convolutional neural networks (CNNs) for CelebA. All
neural networks are batch-normalized (Ioffe & Szegedy,
2015) and updated with Adam (Kingma & Ba, 2014). The
number of training epochs is determined based on the val-
idation set. The dimensionality of each stochastic layer is
50, 100, and 300 for MNIST, CelebA, and SVHN respec-
tively. All models were implemented in Python3 using Ten-
sorﬂow (Abadi, 2015).

4.1. Conditional Log-Likelihood Performance

Tables 2 to 6 show the performance comparisons between
the CVAE and the BCDE. For baselines, we use the CVAE,
the BCDE trained with the conditional objective, and the
BCDE initialized via pre-training Jx(·) and Jy(·) using the
available x and y data separately (and then trained condi-
tionally). Against these baselines, we measure the perfor-
mance of the BCDE (with and without factored inference)

3github.com/ruishu/bcde

Bottleneck Conditional Density Estimation

(a) Conditional

(b) Hybrid

Figure 3. Comparison of conditional image generation for the
conditional versus hybrid BCDE on the semi-supervised 1-
quadrant task. Row 1 shows the original images. Rows 2-4 show
three attempts by each model to sample y according to x (the
bottom-left quadrant, indicated in gray). Hybrid training yields a
higher-entropy model that has lower perplexity.

much more resilient against overﬁtting.

trained with the hybrid objective H(·). We tuned the regu-
larization hyperparameter λ = (cid:8)10−3, 10−2, . . . , 103(cid:9) on
the MNIST 2-quadrant semi-supervised tasks and settled
on using λ = 10−2 for all tasks.

Fully-supervised regime. By comparing in the fully-
supervised regime for MNIST (Tables 2 to 4, nl = 50000),
we show that the hybrid BCDE achieves competitive per-
formance against the pretrained BCDE and out-performs
previously reported results for CVAE (Sohn et al., 2015).

Semi-supervised regime. As the labeled training size nl
reduces, the beneﬁt of having the hybrid training procedure
becomes more apparent. The BCDEs trained with the hy-
brid objective function tend to signiﬁcantly improve upon
its conditionally-trained counterparts.

On MNIST, hybrid training of the factored BCDE achieves
the best performance. Both hybrid models achieve over
a 1-nat difference than the pre-trained baseline in some
cases—a signiﬁcant difference for binarized MNIST (Wu
et al., 2016). Conditional BCDE performs very poorly in
the semi-supervised tasks due to overﬁtting.

On CelebA, hybrid training of the factored BCDE also
achieves the best performance. Both hybrid models sig-
niﬁcantly out-perform the conditional baselines and yield
better visual predictions than conditional BCDE (see Ap-
pendix C). The hybrid models also outperform pre-trained
BCDE with only half the amount of labeled data.

On SVHN,
the hybrid BCDE with standard inference
model signiﬁcantly out-performs the conditional baselines.
However, the use of factored inference results in much
poorer performance. Since the decoder is a discretized lo-
gistic distribution with learnable scale, it is possible that the
factored inference model is not expressive enough to model
the posterior distribution.

Model entropy. In Figure 3, we sample from pθ(y|x) for
the conditional BCDE and the hybrid BCDE. We show
that the conditionally-trained BCDE achieves poorer per-
formance because it learns a lower-entropy model. In con-
trast, hybrid training learns a lower perplexity model, re-
sulting in a high-entropy conditional image generator that
spreads the conditional probability mass over the target out-
put space (Theis et al., 2015).

Figure 4. Comparison of the BCDE variants on the 2-quadrant
MNIST prediction task with nl = 10000 labeled points. In con-
trast to conditional training, hybrid training is less susceptible to
overﬁtting.

4.3. Robustness of Representation

Since hybrid training encourages the BCDE to consider the
distribution of x, we can demonstrate that models trained
in a hybrid manner are robust against structured perturba-
tions of the data set. To show this, we experimented with
two variants of the MNIST quadrant task called the shift-
sensitive and shift-invariant top-bottom prediction tasks. In
these experiments, we set λ = 0.1.

4.2. Conditional Training Overﬁts

4.3.1. SHIFT-SENSITIVE ESTIMATION

To demonstrate the hybrid training’s regularization be-
havior, we show the test set performance during training
(Fig. 4) on the 2-quadrant MNIST task (nl = 10000). Even
with pre-trained initialization of parameters, models that
were trained conditionally quickly overﬁt, resulting in poor
test set performance. In contrast, hybrid training regular-
izes the conditional model toward the joint model, which is

In the shift-sensitive task, the objective is to learn to predict
the bottom half of the MNIST digit (y) when given the top
half (x). However, we introduce structural perturbation to
the top and bottom halves of the image in our training, val-
idation, and test sets by randomly shifting each pair (x, y)
horizontally by the same number of pixels (shift varies be-
tween {−4, −3, . . . , 3, 4}). We then train the BCDE us-
ing either the conditional or hybrid objective in the fully-

Bottleneck Conditional Density Estimation

supervised regime. Note that compared to the original top-
down prediction task, the perplexity of the conditional task
remains the same after the perturbation is applied.

continues to achieve better performance than conditional
models and suffer a much smaller performance gap when
structural corruption in x is introduced.

Models

No Shift

Shift

Conditional
Hybrid
Hybrid + Factored

41.59 ± 0.02
41.33 ± 0.01
41.20 ± 0.02

44.02 ± 0.03
43.51 ± 0.01
43.19 ± 0.02

∆

2.43
2.17
1.99

Table 7. Shift-sensitive top-bottom MNIST prediction. Perfor-
mance with and without structural corruption reported, along with
the performance difference. Hybrid training is robust against
structural perturbation of (x, y).

Table 7 shows that hybrid training consistently achieves
better performance than conditional training. Furthermore,
the hybridly trained models were less affected by the in-
troduction of the perturbation, demonstrating a higher de-
gree of robustness. Because of its more compact recogni-
tion model, hybrid + factored is less vulnerable to overﬁt-
ting, resulting in a smaller performance gap between per-
formance on the shifted and original data.

4.3.2. SHIFT-INVARIANT ESTIMATION

The shift-invariant task is similar to the shift-sensitive top-
bottom task, but with one key difference: we only introduce
structural noise to the top half of the image in our training,
validation, and test sets. The goal is thus to learn that the
prediction of y (which is always centered) is invariant to
the shifted position of x.

Models

No Shift

Shift

Conditional
Hybrid
Hybrid + Factored

41.59 ± 0.02
41.33 ± 0.01
41.20 ± 0.02

42.99 ± 0.04
42.53 ± 0.02
42.20 ± 0.02

∆

1.40
1.20
1.00

Table 8. Shift-invariant top-bottom MNIST prediction. Perfor-
mance with and without structural corruption reported, along with
the performance difference. Hybrid training is robust against
structural corruption of x.

Figure 5. Visualization of
space of hybrid and
conditionally-trained BCDEs. PCA plots of the latent space sub-
region for all x’s whose class label = 2 are shown. Fill color
indicates the degree of shift: blue = −4, orange = +4.

the latent

Table 8 shows similar behavior to Table 7. Hybrid training

In Fig. 5, we show the PCA projections of the latent space
sub-region populated by digits 2 and color-coded all points
based on the degree of shift. We observe that hybrid train-
ing versus conditional training of the BCDE result in very
different learned representations in the stochastic layer. Be-
cause of regularization toward the joint model, the hybrid
BCDE’s latent representation retrains information about x
and learns to untangle shift from other features. And as ex-
pected, conditional training does not encourage the BCDE
to be aware of the distribution of x, resulting in a latent
representation that is ignorant of the shift feature of x.

5. Conclusion

We presented a new framework for high-dimensional con-
ditional density estimation. The building blocks of our
framework are a pair of sibling models:
the Bottleneck
Conditional Density Estimator (BCDE) and the Bottleneck
Joint Density Estimator (BJDE). These models use layers
of stochastic neural networks as bottleneck between the in-
put and output data. While the BCDE learns the conditional
distribution p(y|x), the BJDE learns the joint distribution
p(x, y). The bottleneck constraint implies that only the bot-
tleneck needs to be marginalized when either the input x or
the output y are missing during training, thus, enabling the
BJDE to be trained in a semi-supervised fashion.

The key component of our framework is our hybrid objec-
tive function that regularizes the BCDE towards the BJDE.
Our new objective is a novel extension of Lasserre et al.
(2006) that enables the principle of hybrid blending to be
applied to deep variational models. Our framework pro-
vides a new mechanism for the BCDE, a conditional model,
to become more robust and to learn from unlabeled data in
semi-supervised conditional density estimation.

Our experiments showed that hybrid training is compet-
itive in the fully-supervised regime against pre-training,
and achieves superior performance in the semi-supervised
quadrant prediction task in comparison to conditional
models, achieving new state-of-the-art performances on
MNIST, SVHN, and CelebA. Even with pre-trained weight
initializations, the conditional model is still susceptible to
overﬁtting. In contrast, hybrid training is signiﬁcantly more
robust against overﬁtting. Furthermore, hybrid training
transfers the nice embedding properties of the BJDE to the
BCDE, allowing the BCDE to learn better and more robust
representation of the input x. The success of our hybrid
training framework makes it a prime candidate for other
high-dimensional conditional density estimation problems,
especially in semi-supervised settings.

Bottleneck Conditional Density Estimation

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical Vari-
ational Models. ArXiv e-prints, 1511.02386, November
2015.

Rezende, D., Mohamed, S., and Wierstra, D. Stochas-
tic Backpropagation and Approximate Inference in Deep
Generative Models. ArXiv e-prints, 1401.4082, January
2014.

Salakhutdinov, R. and Murray, I. On the quantitative anal-
ysis of deep belief networks. International Conference
on Machine Learning, 2008.

Salimans, Tim, Karpathy, Andrej, Chen, Xi, and Kingma,
Diederik P. Pixelcnn++: Improving the pixelcnn with
discretized logistic mixture likelihood and other modi-
ﬁcations. CoRR, abs/1701.05517, 2017. URL http:
//arxiv.org/abs/1701.05517.

Sohn, K., Shang, W., and H., Lee. Improved multimodal
deep learning with variation of information. Neural In-
formation Processing Systems, 2014.

Sohn, K., Yan, X., and Lee, H. Learning structured output
representation using deep conditional generative models.
Neural Information Processing Systems, 2015.

Sønderby, C. K., Raiko, T., Maaløe, L., Kaae Sønderby,
S., and Winther, O. Ladder Variational Autoencoders.
arXiv:1602.02282, 2016.

Theis, L., van den Oord, A., and Bethge, M. A note on
the evaluation of generative models. arXiv:1511.01844,
2015.

Tipping, M. and Bishop, C. Probabilistic Principal Com-

ponent Analysis. J. R. Statist. Soc. B, 1999.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. arXiv:1611.04273, 2016.

Yan, X., Yang, J., Sohn, K., and Lee, H. Attribute2Image:
Conditional Image Generation from Visual Attributes.
arXiv:1512.00570, 2015.

Zhang, B., Xiong, D., Su, J., Duan, H., and Zhang, M. Vari-
ational Neural Machine Translation. arXiv:1605.07869,
2016.

References

Abadi, Mart´ın, et. al. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. URL http:
//tensorflow.org/. Software available from ten-
sorﬂow.org.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance
Weighted Autoencoders. arXiv preprints:1509.00519,
2015.

Dayan, P., Hinton, G., Neal, R., and Zemel, R. The

Helmholtz Machine. Neural computation, 1995.

Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. Pos-
terior regularization for structured latent variable mod-
els. JMLR, 2010.

Hinton, G., Dayan, P., Frey, B., and Radford, R. The
“wake-sleep” algorithm for unsupervised neural net-
works. Science, 1995.

Holmes, M. P., Gray, A. G.,

and Isbell, C. L.
Fast Nonparametric Conditional Density Estimation.
arXiv:1206.5278, 2012.

Ioffe, S. and Szegedy, C. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Co-
variate Shift. arXiv:1502.03167, 2015.

Kingma, D. and Ba, J. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014.

Kingma, D. P and Welling, M. Auto-Encoding Variational

Bayes. arXiv:1312.6114, 2013.

Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling,
M. Semi-Supervised Learning with Deep Generative
Models. arXiv:1406.5298, 2014.

Kingma, Diederik P., Salimans, Tim, and Welling, Max.
Improving variational inference with inverse autoregres-
sive ﬂow. CoRR, abs/1606.04934, 2016. URL http:
//arxiv.org/abs/1606.04934.

Lasserre, J., Bishop, C., and Minka, T. Principled hybrids
In The IEEE
of generative and discriminative models.
Conference on Computer Vision and Pattern Recogni-
tion, 2006.

Maaløe, L., Kaae Sønderby, C., Kaae Sønderby, S.,
and Winther, O. Auxiliary Deep Generative Models.
arXiv:1602.05473, 2016.

Ng, A. and Jordan, M. On discriminative vs. generative
classiﬁers: A comparison of logistic regression and naive
bayes. Neural Information Processing Systems, 2002.

Bottleneck Conditional Density Estimation

A. Factored Inference

When training the BJDE in the semi-supervised regime,
we introduce a factored inference procedure that reduce the
number of parameters in the recognition model.

In the semi-supervised regime, the 1-layer BJDE recog-
nition model
requires approximating three posteriors:
p(z|x, y) ∝ p(z)p(x, y|z), p(z|x) ∝ p(z)p(x|z), and
p(z|y) ∝ p(z)p(y|z). The standard approach would be to
assign one recognition network for each approximate pos-
terior. This approach, however, does not take advantage
of the fact that these posteriors share the same likelihood
functions, i.e., p(x, y|z) = p(x|z)p(y|z).

Rather than learning the three approximate posteriors in-
dependently, we propose to learn the approximate likeli-
hood functions ˆ(cid:96)(z; x) ≈ p(x|z), ˆ(cid:96)(z; y) ≈ p(y|z) and
let ˆ(cid:96)(z; x, y) = ˆ(cid:96)(z; x)ˆ(cid:96)(z; y). Consequently, this factor-
ization of the recognition model enables parameter sharing
within the joint recognition model (which is beneﬁcial for
semi-supervised learning) and eliminates the need for con-
structing a neural network that takes both x and y as in-
puts. The latter property is especially useful when learning
a joint model over multiple, heterogeneous data types (e.g.
image, text, and audio).

In practice, we directly learn recognition networks for
q(z|x) and ˆ(cid:96)(z; y) and perform factored inference as fol-
lows

Since our model includes unpaired y, we modify Eq. (15)
to include

p(Xl, Yl, Xu, Yu, ˜θ, θ) = p(˜θ, θ)

p(Xu|˜θ)p(Yu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).

(16)

To account for the variational parameters, we include them
in the joint density as well,

p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) = p(˜θ, ˜φ, θ, φ)

p(Xu|˜θ, ˜φ)p(Yu|˜θ, ˜φ)
p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ)

(17)

(18)

By taking the log and replacing the necessary densities with
their variational lower bound,

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) ≥ ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl),

we arrive at Eq. (11). We note, however, that a more gen-
eral hybrid objective Eq. (13) is achievable. To derive the
general objective, we consider an alternative factorization
of the joint density in Eq. (17),

p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) = p(˜θ, ˜φ, θ, φ, )
p(Xl, Yl, Xu, Yu|˜θ, ˜φ, θ, φ).

(19)

q(z|x, y) ∝ q ˜φ(z|x)ˆ(cid:96) ˜φ(z; y), q(z|y) ∝ p(z)ˆ(cid:96) ˜φ(z; y),

We factorize the likelihood term such that Xu and Yu are
always explained by the joint parameters ˜θ, ˜φ,

(14)

(cid:16)

(cid:17)
(cid:104)T (z), η ˜φ(y)(cid:105)

where ˜φ parameterizes the recognition networks. To en-
sure proper normalization in Eq. (14), it is sufﬁcient for ˆ(cid:96)
to be bounded. If the prior p(z) belongs to an exponential
family with sufﬁcient statistics T (z), we can parameterize
ˆ(cid:96) ˜φ(z; y) = exp
, where η ˜φ(y) is a network
such that η ˜φ(y) ∈ {η|{(cid:104)T (z), η(cid:105) ∀z} is upper bounded}.
Then the approximate posterior can be obtained by simple
addition in the natural parameter space of the correspond-
ing exponential family. When the prior and approximate
likelihood are both Gaussians, this is exactly precision-
weighted merging of the means and variances (Sønderby
et al., 2016).

p(Xl, Yl, Xu, Yu|˜θ, ˜φ, θ, φ) = p(Xu|˜θ, ˜φ)p(Yu|˜θ, ˜φ)

p(Xl, Yl|˜θ, ˜φ, θ, φ).

(20)

We then introduce an auxiliary variable s = {0, 1},

p(Xl, Yl|˜θ, ˜φ, θ, φ)

(cid:88)

=

s

p(s)p(Xl, Yl|˜θ, ˜φ, θ, φ, s),

(21)

where

p(Xl, Yl|˜θ, ˜φ, θ, φ, s0) = p(Xl, Yl|˜θ, ˜φ)
p(Xl, Yl|˜θ, ˜φ, θ, φ, s1) = p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ).

(22)

B. Derivation of the Hybrid Objective

We ﬁrst provide the derivation of Eq. (11). We begin with
the factorization proposed in Eq. (7), which we repeat here
for self-containedness,

Jensen’s
Using
ln p(Xl, Yl|˜θ, ˜φ, θ, φ) with

inequality, we

can

lower

bound

p(s0) ln p(Xl, Yl|˜θ, ˜φ) + p(s1) ln p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ).

p(Xl, Yl, Xu, ˜θ, θ) = p(˜θ, θ)

p(Xu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).

(15)

By taking the log of Eq. (19), replacing all remaining
densities with their variational lower bound, and setting

(23)

(24)

Bottleneck Conditional Density Estimation

p(s0) = α,

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ)

≥ H(˜θ, ˜φ, θ, φ; Xl, Yl, Xu, Yu)
= ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
α · Jxy(˜θ, ˜φ; Xl, Yl) +

(25)

(1 − α) ·

(cid:104)

(cid:105)
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl)

, (26)

we arrive at the general hybrid objective. Note that when
α = 0, Eq. (26) reduces to Eq. (18).

We show visualizations of the hybrid BCDE predictions for
CelebA and SVHN on the top-down prediction task in the
nl = 10000 semi-supervised regime. For each data set, we
visualize both the images sampled during reconstruction as
well as prediction using an approximation of the MAP es-
timate by greedily sampling the mode of each conditional
distribution in the generative path.

C. Visualizations for CelebA and SVHN

(a) Conditional Rec.

(b) Conditional Pred.

(c) Pre-train Rec.

(d) Pre-train Pred.

(a) Hybrid Rec.

(b) Hybrid Pred.

Figure 6. Visualization of the reconstructed and predicted bottom
half of SVHN test set images when conditioned on the top half.

(e) Hybrid Rec.

(f) Hybrid Pred.

(g) Hybrid Factored Rec.

(h) Hybrid Factored Pred.

Figure 7. Visualization of the reconstructed and predicted bottom
half of CelebA test set images when conditioned on the top half.

Bottleneck Conditional Density Estimation

Rui Shu 1 Hung H. Bui 2 Mohammad Ghavamzadeh 3

7
1
0
2
 
n
u
J
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
8
6
5
8
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

We introduce a new framework for training deep
generative models for high-dimensional condi-
tional density estimation. The Bottleneck Con-
ditional Density Estimator (BCDE) is a vari-
ant of the conditional variational autoencoder
(CVAE) that employs layer(s) of stochastic vari-
ables as the bottleneck between the input x
and target y, where both are high-dimensional.
Crucially, we propose a new hybrid training
method that blends the conditional generative
model with a joint generative model. Hybrid
blending is the key to effective training of the
BCDE, which avoids overﬁtting and provides a
novel mechanism for leveraging unlabeled data.
We show that our hybrid training procedure en-
ables models to achieve competitive results in
the MNIST quadrant prediction task in the fully-
supervised setting, and sets new benchmarks in
the semi-supervised regime for MNIST, SVHN,
and CelebA.

1. Introduction

Conditional density estimation (CDE) refers to the prob-
lem of estimating a conditional density p(y|x) for the input
x and target y. In contrast to classiﬁcation where the tar-
get y is simply a discrete class label, y is typically contin-
uous or high-dimensional in CDE. Furthermore, we want
to estimate the full conditional density (as opposed to its
conditional mean in regression), an important task the con-
ditional distribution has multiple modes. CDE problems
in which both x and y are high-dimensional have a wide
range of important applications, including video prediction,
cross-modality prediction (e.g.
image-to-caption), model
estimation in model-based reinforcement learning, and so
on.

1Stanford University 2Adobe Research 3DeepMind (The work
was done when all the authors were with Adobe Research). Cor-
respondence to: Rui Shu <ruishu@stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Classical non-parametric conditional density estimators
typically rely on local Euclidean distance in the original in-
put and target spaces (Holmes et al., 2012). This approach
quickly becomes ineffective in high-dimensions from both
computational and statistical points of view. Recent ad-
vances in deep generative models have led to new para-
metric models for high-dimensional CDE tasks, namely the
conditional variational autoencoders (CVAE) (Sohn et al.,
2015). CVAEs have been applied to a variety of problems,
such as MNIST quadrant prediction, segmentation (Sohn
et al., 2015), attribute-based image generation (Yan et al.,
2015), and machine translation (Zhang et al., 2016).

But CVAEs suffer from two statistical deﬁciencies. First,
they do not learn the distribution of the input x. We ar-
gue that in the case of high-dimensional input x where
there might exist a low-dimensional representation (such
as a low-dimensional manifold) of the data, recovering this
structure is important, even if the task at hand is to learn the
conditional density p(y|x). Otherwise, the model is suscep-
tible to overﬁtting. Second, for many CDE tasks, the ac-
quisition of labeled points is costly, motivating the need for
semi-supervised CDE. A purely conditional model would
not be able to utilize any available unlabeled data.1 We
note that while variational methods (Kingma & Welling,
2013; Rezende et al., 2014) have been applied to semi-
supervised classiﬁcation (where y is a class label) (Kingma
et al., 2014; Maaløe et al., 2016), semi-supervised CDE
(where y is high-dimensional) remains an open problem.

We focus on a set of deep conditional generative mod-
els, which we call bottleneck conditional density estima-
tors (BCDEs). In BCDEs, the input x inﬂuences the target
y via layers of bottleneck stochastic variables z = {zi} in
the generative path. The BCDE naturally has a joint gen-
erative sibling model which we denote the bottleneck joint
density estimator (BJDE), where the bottleneck z gener-
ates x and y independently. Motivated by Lasserre et al.
(2006), we propose a hybrid training framework that regu-
larizes the conditionally-trained BCDE parameters toward
the jointly-trained BJDE parameters. This is the key fea-
ture that enables semi-supervised learning for conditional
density estimation in the BCDEs.

1We deﬁne a “labeled point” to be a paired (x, y) sample, and

an “unlabeled point” to be unpaired x or y.

Bottleneck Conditional Density Estimation

Our BCDE hybrid training framework is a novel approach
for leveraging unlabeled data for conditional density es-
timation. Using our BCDE hybrid training framework,
we establish new benchmarks for the quadrant prediction
task (Sohn et al., 2015) in the semi-supervised regime for
MNIST, SVHN, and CelebA. Our experiments show that
1) hybrid training is competitive for fully-supervised CDE,
2) in semi-supervised CDE, hybrid training helps to avoid
overﬁtting, performs signiﬁcantly better than conditional
training with unlabeled data pre-training, and achieves
state-of-the-art results, and 3) hybrid training encourages
the model to learn better and more robust representations.

2. Background

2.1. Variational Autoencoders

Variational Autoencoder (VAE) is a deep generative model
for density estimation. It consists of a latent variable z with
unit Gaussian prior z ∼ N (0, Ik), which in turn generates
an observable vector x. The observation is usually con-
ditionally Gaussian x|z ∼ N (cid:0)µθ(z), diag(σ2
θ (z)(cid:1), where
µ and σ2 are neural networks whose parameters are rep-
resented by θ.2 VAE can be seen as a non-linear gen-
eralization of the probabilistic PCA (Tipping & Bishop,
1999), and thus, can recover non-linear manifolds in the
data. However, VAE’s ﬂexibility makes posterior inference
of the latent variables intractable. This inference issue is
addressed via a recognition model qφ(z|x), which serves
as an amortized variational approximation of the intractable
posterior pθ(z|x). Learning in VAE’s is done by jointly op-
timizing the parameters of both the generative and recogni-
tion models so as to maximize an objective that resembles
an autoencoder regularized reconstruction loss (Kingma &
Welling, 2013), i.e.,

Eqφ(z|x)

(cid:2) ln pθ(x|z)(cid:3) − DKL

(cid:0)qφ(z|x) || p(z)(cid:1).

(1)

sup
θ,φ

We note that the objective Eq. (1) can be rewritten in the
following form that exposes its connection to the varia-
tional lower bound of the log-likelihood

(cid:16)

sup
θ

ln pθ(x) − inf
φ

(cid:0)qφ(z|x) || pθ(z|x)(cid:1)(cid:17)

DKL

(cid:20)

Eqφ(z|x)

ln

pθ(x, z)
pφ(z|x)

(cid:21)

.

= sup
θ,φ

(2)

We make two remarks regarding the minimization of the
(cid:0)qφ(z|x) || pθ(z|x)(cid:1) in Eq. 2. First, when q(·|·) is
term DKL
a conditionally independent Gaussian, this approximation
is at best as good as the mean-ﬁeld approximation that min-
(cid:0)q || pθ(z|x)(cid:1) over all independent Gaussian
imizes DKL

2For discrete x, one can use a deep network to parameterize a

Bernoulli or a discretized logistic distribution.

q’s. Second, this term serves as a form of amortized pos-
terior regularization that encourages the posterior pθ(z|x)
to be close to an amortized variational family (Dayan et al.,
1995; Ganchev et al., 2010; Hinton et al., 1995). In prac-
tice, both θ and φ are jointly optimized in Eq. (1), and
the reparameterization trick (Kingma & Welling, 2013) is
used to transform the expectation over z ∼ qφ(z|x) into
(cid:15) ∼ N (0, Ik); z = µφ(x) + diag(cid:0)σ2
φ(x)(cid:1)(cid:15), which leads to
an easily obtained stochastic gradient.

2.2. Conditional VAEs (CVAEs)

In Sohn et al. (2015), the authors introduce the conditional
version of variational autoencoders. The conditional gen-
erative model is similar to VAE, except that the latent vari-
able z and the observed vector y are both conditioned on
the input x. The conditional generative path is
z,θ(x)(cid:1)(cid:17)

pθ(z | x) = N

(3)

(cid:16)

z | µz,θ(x), diag(cid:0)σ2
y | µy,θ(x, z), diag(cid:0)σ2

(cid:16)

y,θ(x, z)(cid:1)(cid:17)

,

(4)

pθ(y | x, z) = N

and when we use a Bernoulli decoder is

pθ(y | x, z) = Ber(cid:0)y | µy,θ(x, z)(cid:1).

(5)

Here, θ denotes the parameters of the neural networks used
in the generative path. The CVAE is trained by maximizing
a lower bound of the conditional likelihood

ln pθ(y|x) ≥ Eqφ(z|x,y)

ln

(cid:20)

pθ(z|x)pθ(y|x, z)
qφ(z|x, y)

(cid:21)

,

(6)

but with a recognition network qφ(z|x, y), which is typi-
z|µφ(x, y), diag(cid:0)σ2
cally Gaussian N
, and takes
both x and y as input.

φ(x, y)(cid:1)(cid:17)

(cid:16)

2.3. Blending Generative and Discriminative

It is well-known that a generative model may yield sub-
optimal performance when compared to the same model
trained discriminatively (Ng & Jordan, 2002), a phe-
nomenon attributable to the generative model being mis-
speciﬁed (Lasserre et al., 2006). However, generative mod-
els can easily handle unlabeled data in semi-supervised set-
ting. This is the main motivation behind blending genera-
tive and discriminative models. Lasserre et al. (2006) pro-
posed a principled method for hybrid blending by duplicat-
ing the parameter of the generative model into a discrimi-
natively trained θ and a generatively trained ˜θ, i.e.,

p(Xl, Yl, Xu, ˜θ, θ) = p(˜θ, θ)p(Xu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).
(7)
The discriminatively trained parameter θ is regularized to-
ward the generatively trained parameter ˜θ via a prior p(˜θ, θ)
that prefers small (cid:107)θ − ˜θ(cid:107)2. As a result, in addition to

Bottleneck Conditional Density Estimation

BJDE

z

Regularization

BCDE

z

x

y

x

y

Unpaired Data
{xi} ∪ {yi}

Paired Data
{xi, yi}

Figure 1. The hybrid training procedure that regularizes BCDE towards BJDE. This regularization enables the BCDE to indirectly
leverage unpaired x and y for conditional density estimation.

learning from the labeled data (Xl, Yl), the discrimina-
tive parameter θ can be informed by the unlabeled data
Xu via ˜θ, enabling a form of semi-supervised discrimina-
tively trained generative model. However, this approach is
limited to simple generative models (e.g., naive Bayes and
HMMs), where exact inference of p(y|x, θ) is tractable.

3. Neural Bottleneck Conditional Density

Estimation

While Sohn et al. (2015) has successfully applied the
CVAE to CDE, CVAE suffers from two limitations. First,
the CVAE does not learn the distribution of its input x,
and thus, is far more susceptible to overﬁtting. Second,
it cannot incorporate unlabeled data. To resolve these lim-
itations, we propose a new approach to high-dimensional
CDE that blends the discriminative model that learns the
conditional distribution p(y|x), with a generative model
that learns the joint distribution p(x, y).

3.1. Overview

Figure 1 provides a high-level overview of our approach
that consists of a new architecture and a new training proce-
dure. Our new architecture imposes a bottleneck constraint,
resulting a class of conditional density estimators, we call it
bottleneck conditional density estimators (BCDEs). Unlike
CVAE, the BCDE generative path prevents x from directly
inﬂuencing y. Following the conditional training paradigm
in Sohn et al. (2015), conditional/discriminative training of
the BCDE means maximizing the lower bound of a condi-
tional likelihood similar to (6),i.e.,

ln pθ(y|x) ≥ C(θ, φ; x, y)
(cid:20)

= Eqφ(z|x,y)

ln

pθ(z|x)pθ(y|z)
qφ(z|x, y)

(cid:21)

.

When trained over a dataset of paired (X, Y) samples, the
overall conditional training objective is

C(θ, φ; X, Y) =

C(θ, φ; x, y).

(8)

(cid:88)

x,y∈X,Y

However, this approach suffers from the same limitations as
CVAE and imposes a bottleneck that limits the ﬂexibility of
the generative model. Instead, we propose a hybrid training
framework that takes advantage of the bottleneck architec-
ture to avoid overﬁtting and supports semi-supervision.

One component in our hybrid training procedure tackles the
problem of estimating the joint density p(x, y). To do this,
we use the joint counterpart of the BCDE: the bottleneck
joint density estimator (BJDE). Unlike conditional models,
the BJDE allows us to incorporate unpaired x and y data
during training. Thus, the BJDE can be trained in a semi-
supervised fashion. We will also show that the BJDE is
well-suited to factored inference (see Section 3.4), i.e., a
factorization procedure that makes the parameter space of
the recognition model more compact.

The BJDE also serves as a way to regularize the BCDE,
where the regularization constraint can be viewed as soft-
tying between the parameters of these two models’ gen-
erative and recognition networks. Via this regularization,
BCDE beneﬁts from unpaired x and y for conditional den-
sity estimation.

3.2. Bottleneck Joint Density Estimation

In the BJDE, we wish to learn the joint distribution of x
and y. The bottleneck is introduced in the generative path
via the bottleneck variable z, which points to x and y (see
Figs. 2(a) to 2(c)). Thus, the variational lower bound of the

Bottleneck Conditional Density Estimation

z

y

y

y

z

x

z

x

z

x

(a) Joint: (x)

(b) Joint: (y)

(c) Joint: (x, y)

(d) Conditional: (x, y)

Figure 2. The joint and conditional components of the BCDE. Dotted lines represent recognition models. The conditional model param-
eters are regularized toward the joint model’s. The natural pairing of the conditional and joint parameters is described in Table 1.

Standard

Factored

BJDE:

BCDE:

BJDE:

BCDE:

q ˜φ(z|x, y)
qφ(z|x, y)

−

−

q ˜φ(z|y)
−
ˆ(cid:96) ˜φ(z; y)
ˆ(cid:96) ˜φ(z; y)

q ˜φ(z|x)
pθ(z|x)

p˜θ(y|z)
pθ(y|z)

p˜θ(x|z)
−

q ˜φ(z|x)

p˜θ(y|z)

p˜θ(x|z)

pθ(z|x)

pθ(y|z)

−

Table 1. Soft parameter tying between the BJDE and BCDE. For each network within the BCDE, there is a corresponding network within
the BJDE. We show the correspondence among the networks with and without the application of factored inference. We regularize all
the BCDE networks to their corresponding BJDE network parameters.

joint likelihood is

ln p˜θ(x, y) ≥ Jxy(˜θ, ˜φ; x, y)

(cid:34)

= Eq ˜φ(z|x,y)

ln

p(z)p˜θ(x|z)p˜θ(y|z)
q ˜φ(z|x, y)

(cid:35)

.

(9)

We use {˜θ, ˜φ} to indicate the parameters of the BJDE net-
works and reserve {θ, φ} for the BCDE parameters. For
samples in which x or y is unobserved, we will need to
compute the variational lower bound for the marginal like-
lihoods. Here, the bottleneck plays a critical role.
If x
were to directly inﬂuence y in a non-trivial manner, any
attempt to incorporate unlabeled y would require the recog-
nition model to infer the unobserved x from the observed
y—a conditional density estimation problem which might
be as hard as our original task.
In the bottleneck archi-
tecture, the conditional independence of x and y given z
implies that only the low-dimensional bottleneck needs to
be marginalized. Thus, the usual variational lower bounds
for the marginal likelihoods yield

ln p˜θ(x) ≥ Jx(˜θ, ˜φ; x) = Eq ˜φ(z|x)

ln

ln p˜θ(y) ≥ Jy(˜θ, ˜φ; y) = Eq ˜φ(z|y)

ln

(cid:34)

(cid:34)

p(z)p˜θ(x|z)
q ˜φ(z|x)

p(z)p˜θ(y|z)
q ˜φ(z|y)

(cid:35)

(cid:35)

,

.

Since z takes on the task of reconstructing both x and y, the
BJDE is sensitive to the distributions of x and y and learns
a joint manifold over the two data sources. Thus, the BJDE
provides the following beneﬁts: 1) learning the distribution

of x makes the inference of z given x robust to perturba-
tions in the inputs, 2) z becomes a joint-embedding of x
and y, 3) the model can leverage unlabeled data. Following
the convention in Eq. (8), the joint training objectives is

J (˜θ, ˜φ; Xu, Yu, Xl, Yl) =

(10)

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) + Jxy(˜θ, ˜φ; Xl, Yl),

where (Xl, Yl) is a dataset of paired (x, y) samples, and
Xu and Yu are datasets of unpaired samples.

3.3. Blending Joint and Conditional Deep Models

Because of potential model mis-speciﬁcations, the BJDE is
not expected to yield good performance if applied to the
conditional task. Thus, we aim to blend the BJDE and
BCDE models in the spirit of Lasserre et al. (2006). How-
ever, we note that (7) is not directly applicable since the
BCDE and BJDE are two different models, and not two
different views (discriminative and generative) of the same
model. Therefore, it is not immediately clear how to tie the
BCDE and BJDE parameters together. Further, these mod-
els involve conditional probabilities parameterized by deep
networks and have no closed form for inference.

Any natural prior for the BCDE parameter θ and the BJDE
parameter ˜θ should encourage pBCDE(y|x, θ) to be close to
pBJDE(y|x, ˜θ). In the presence of the latent variable z, it is
then natural to encourage p(z|x, θ) to be close to p(z|x, ˜θ)
and p(y|z, θ) to be close to p(y|z, ˜θ). However, enforc-
ing the former condition is intractable as we do not have a
closed form for pBJDE(z|x, ˜θ). Fortunately, an approxima-

Bottleneck Conditional Density Estimation

tion of pBJDE(z|x, ˜θ) is provided by the recognition model
q(z|x, ˜φ). Thus, we propose to softly tie together the pa-
rameters of networks deﬁning p(z|x, θ) and q(z|x, ˜φ). This
strategy effectively leads to a joint prior over the model net-
work parameters, as well as the recognition network param-
eters p( ˜φ, ˜θ, φ, θ).

As a result, we arrive at the following hybrid blending of
deep stochastic models and its variational lower bound
ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) ≥ ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl).

(11)

We interpret ln p(˜θ, ˜φ, θ, φ) as a (cid:96)2-regularization term that
softly ties the joint parameters (˜θ, ˜φ) and conditional pa-
rameters (θ, φ) in an appropriate way. For the BCDE and
BJDE, there is a natural one-to-one mapping from the con-
ditional parameters to a subset of the joint parameters.
For the joint model described in Fig. 2(c) and conditional
model in Fig. 2(d), the parameter pairings are provided in
Table 1. Formally, we deﬁne γ = {θ, φ} and use the index
γa|b to denote the parameter of the neural network on the
Bayesian network link b → a in the BCDE. For example
γz|x = θz|x, γz|x,y = φz|x,y. Similarly, let ˜γ = {˜θ, ˜φ}.
In the BJDE, the same notation yields ˜γz|x = ˜φz|x. The
hybrid blending regularization term can be written as

ln p(θ, φ, ˜θ, ˜φ) = −

(cid:107)γi − ˜γi(cid:107)2

2 + const,

(12)

λ
2

(cid:88)

i∈I

where I denotes the set of common indices of the joint and
conditional parameters. When the index is z|x, it effec-
tively means that p(z|x, θ) is softly tied to q(z|x, ˜θ), i.e.,

(cid:107)γz|x − ˜γz|x(cid:107)2

2 = (cid:107)θz|x − ˜φz|x(cid:107)2
2 .

Setting λ = 0 unties the BCDE from the BJDE, and effec-
tively yields to a conditionally trained BCDE, while letting
λ → ∞ forces the corresponding parameters of the BCDE
and BJDE to be identical.

Interestingly, Eq. (11) does not contain the term Jxy. Since
explicit training of Jxy may lead to learning a better joint
embedding in the space of z, we note the following general-
ization of Eq. (11) that trades off the contribution between
Jxy and [Jx + C],

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ)

≥ H(˜θ, ˜φ, θ, φ; Xl, Yl, Xu, Yu)
= ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
α · Jxy(˜θ, ˜φ; Xl, Yl) +

Intuitively,
the equation computes the lower bound of
p(Xl, Yl), either using the joint parameters ˜θ, ˜φ or factor-
izes p(Xl, Yl) into p(Xl)p(Yl | Xl) before computing the
lower bound of p(Yl | Xl) with the conditional parame-
ters. A proof that the lower bound holds for any 0 ≤ α ≤ 1
is provided in Appendix B. For simplicity, we set α = 0.5
and do not tune α in our experiments.

3.4. Factored Inference

The inference network qφ(z|x, y) is usually parameterized
as a single neural network that takes both x and y as input.
Using the precision-weighted merging scheme proposed by
Sønderby et al. (2016), we also consider an alternative pa-
rameterization of qφ(z|x, y) that takes a weighted-average
of the Gaussian distribution qφ(z|x) and a Gaussian like-
lihood term ˆ(cid:96)(z; y) (see Appendix A). Doing so offers a
more compact recognition model and more sharing param-
eters between the BCDE and BJDE (e.g., see the bottom
two rows in Table 1), but at the cost of lower ﬂexibility for
the variational family qφ(z|x, y).

4. Experiments

We evaluated the performance of our hybrid training proce-
dure on the permutation-invariant quadrant prediction task
(Sohn et al., 2014; Sohn et al., 2015) for MNIST, SVHN,
and CelebA. The quadrant prediction task is a conditional
density estimation problem where an image data set is par-
tially occluded. The model is given the observed region and
is evaluated by its perplexity on the occluded region. The
quadrant prediction task consists of four sub-tasks depend-
ing on the degree of partial observability. 1-quadrant pre-
diction: the bottom left quadrant is observed. 2-quadrant
prediction: the left half is observed. 3-quadrant prediction:
the bottom right quadrant is not observed. Top-down pre-
diction: the top half is observed.

i=1

In the fully-supervised case, the original MNIST train-
i}50000
ing set {x(cid:48)
is converted into our CDE training set
i=1
{Xl, Yl} = {xi, yi}50000
by splitting each image into
its observed x and unobserved y regions according to the
quadrant prediction task. Note that the training set does not
contain the original class label information. In the nl-label
semi-supervised case, we randomly sub-sampled nl pairs
to create our labeled training set {xi, yi}nl
i=1. The remain-
ing nu paired samples are decoupled and put into our un-
labeled training sets Xu = {xi}nu
i=1. Test
performance is the conditional density estimation perfor-
mance on the entire test set, which is also split into input
x and target y according to the quadrant prediction task.
Analogous procedure is used for SVHN and CelebA.

i=1 , Yu = {yi}nu

(1 − α) ·

(cid:104)

(cid:105)
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl)

. (13)

For comparison against Sohn et al. (2015), we evalu-
ate the performance of our models on the MNIST 1-

Bottleneck Conditional Density Estimation

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

63.91
62.45 ± 0.02
62.00 ± 0.02
62.16 ± 0.03
62.81 ± 0.05

-
64.50 ± 0.03
63.27 ± 0.04
62.90 ± 0.02
63.47 ± 0.02

-
68.23 ± 0.05
65.14 ± 0.05
64.08 ± 0.03
64.16 ± 0.02

-
71.66 ± 0.06
67.13 ± 0.04
65.10 ± 0.03
64.64 ± 0.05

Table 2. MNIST quadrant prediction task: 1-quadrant. We report the test set loss (IW=100) and standard error.

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

44.73
43.91 ± 0.01
43.53 ± 0.02
43.56 ± 0.02
44.07 ± 0.02

-
45.49 ± 0.03
44.42 ± 0.04
44.10 ± 0.02
44.41 ± 0.02

-
48.16 ± 0.02
45.81 ± 0.01
45.23 ± 0.02
45.02 ± 0.04

-
50.83 ± 0.04
47.49 ± 0.06
46.39 ± 0.03
45.86 ± 0.06

Table 3. MNIST quadrant prediction task: 2-quadrant.

Models

nl = 50000

nl = 25000

nl = 10000

nl = 5000

CVAE (Sohn et al., 2015)
BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

20.95
20.64 ± 0.01
20.37 ± 0.01
20.31 ± 0.01
20.43 ± 0.01

-
21.27 ± 0.01
20.87 ± 0.02
20.69 ± 0.02
20.56 ± 0.01

-
22.44 ± 0.03
21.65 ± 0.02
21.36 ± 0.02
21.16 ± 0.01

-
23.72 ± 0.04
22.32 ± 0.05
22.27 ± 0.02
21.81 ± 0.03

Table 4. MNIST quadrant prediction task: 3-quadrant.

Models

nl = 10000

nl = 5000

Models

nl = 20000

nl = 10000

BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

4657 ± 48
4547 ± 23
4213 ± 21
4700 ± 146

4845 ± 33
4627 ± 13
4392 ± 13
5030 ± 165

BCDE (conditional)
BCDE (na¨ıve pre-train)
BCDE (hybrid)
BCDE (hybrid + factored)

5805 ± 2
5784.8 ± 0.5
5778.6 ± 0.4
5776.1 ± 0.3

5817 ± 3
5793 ± 1
5781.3 ± 0.5
5780.3 ± 0.6

Table 5. SVHN prediction task: Top-Down.

Table 6. CelebA prediction task: Top-Down.

quadrant, 2-quadrant, and 3-quadrant prediction tasks. The
MNIST digits are statically-binarized by sampling from
the Bernoulli distribution according to their pixel values
(Salakhutdinov & Murray, 2008). We use a sigmoid layer
to learn the parameter of the Bernoulli observation model.

We provide the performance on the top-down prediction
task for SVHN and CelebA. We used a discretized logistic
observation model Kingma et al. (2016) to model the pixel
values for SVHN and a Gaussian observation model with
ﬁxed variance for CelebA. For numerical stability, we rely
on the implementation of the discretized logistic distribu-
tion described in Salimans et al. (2017).

In all cases, we extracted a validation set of 10000 samples
for hyperparameter tuning. While our training objective
uses a single (IW=1) importance-weighted sample (Burda
et al., 2015), we measure performance using IW=100 to
get a tighter bound on the test log-likelihood (Sohn et al.,
2015). We run replicates of all experiments and report the
mean performance with standard errors. For a more ex-
pressive variational family (Ranganath et al., 2015), we use
two stochastic layers in the BCDE and perform inference

via top-down inference (Sønderby et al., 2016). We use
multi-layered perceptrons (MLPs) for MNIST and SVHN,
and convolutional neural networks (CNNs) for CelebA. All
neural networks are batch-normalized (Ioffe & Szegedy,
2015) and updated with Adam (Kingma & Ba, 2014). The
number of training epochs is determined based on the val-
idation set. The dimensionality of each stochastic layer is
50, 100, and 300 for MNIST, CelebA, and SVHN respec-
tively. All models were implemented in Python3 using Ten-
sorﬂow (Abadi, 2015).

4.1. Conditional Log-Likelihood Performance

Tables 2 to 6 show the performance comparisons between
the CVAE and the BCDE. For baselines, we use the CVAE,
the BCDE trained with the conditional objective, and the
BCDE initialized via pre-training Jx(·) and Jy(·) using the
available x and y data separately (and then trained condi-
tionally). Against these baselines, we measure the perfor-
mance of the BCDE (with and without factored inference)

3github.com/ruishu/bcde

Bottleneck Conditional Density Estimation

(a) Conditional

(b) Hybrid

Figure 3. Comparison of conditional image generation for the
conditional versus hybrid BCDE on the semi-supervised 1-
quadrant task. Row 1 shows the original images. Rows 2-4 show
three attempts by each model to sample y according to x (the
bottom-left quadrant, indicated in gray). Hybrid training yields a
higher-entropy model that has lower perplexity.

much more resilient against overﬁtting.

trained with the hybrid objective H(·). We tuned the regu-
larization hyperparameter λ = (cid:8)10−3, 10−2, . . . , 103(cid:9) on
the MNIST 2-quadrant semi-supervised tasks and settled
on using λ = 10−2 for all tasks.

Fully-supervised regime. By comparing in the fully-
supervised regime for MNIST (Tables 2 to 4, nl = 50000),
we show that the hybrid BCDE achieves competitive per-
formance against the pretrained BCDE and out-performs
previously reported results for CVAE (Sohn et al., 2015).

Semi-supervised regime. As the labeled training size nl
reduces, the beneﬁt of having the hybrid training procedure
becomes more apparent. The BCDEs trained with the hy-
brid objective function tend to signiﬁcantly improve upon
its conditionally-trained counterparts.

On MNIST, hybrid training of the factored BCDE achieves
the best performance. Both hybrid models achieve over
a 1-nat difference than the pre-trained baseline in some
cases—a signiﬁcant difference for binarized MNIST (Wu
et al., 2016). Conditional BCDE performs very poorly in
the semi-supervised tasks due to overﬁtting.

On CelebA, hybrid training of the factored BCDE also
achieves the best performance. Both hybrid models sig-
niﬁcantly out-perform the conditional baselines and yield
better visual predictions than conditional BCDE (see Ap-
pendix C). The hybrid models also outperform pre-trained
BCDE with only half the amount of labeled data.

On SVHN,
the hybrid BCDE with standard inference
model signiﬁcantly out-performs the conditional baselines.
However, the use of factored inference results in much
poorer performance. Since the decoder is a discretized lo-
gistic distribution with learnable scale, it is possible that the
factored inference model is not expressive enough to model
the posterior distribution.

Model entropy. In Figure 3, we sample from pθ(y|x) for
the conditional BCDE and the hybrid BCDE. We show
that the conditionally-trained BCDE achieves poorer per-
formance because it learns a lower-entropy model. In con-
trast, hybrid training learns a lower perplexity model, re-
sulting in a high-entropy conditional image generator that
spreads the conditional probability mass over the target out-
put space (Theis et al., 2015).

Figure 4. Comparison of the BCDE variants on the 2-quadrant
MNIST prediction task with nl = 10000 labeled points. In con-
trast to conditional training, hybrid training is less susceptible to
overﬁtting.

4.3. Robustness of Representation

Since hybrid training encourages the BCDE to consider the
distribution of x, we can demonstrate that models trained
in a hybrid manner are robust against structured perturba-
tions of the data set. To show this, we experimented with
two variants of the MNIST quadrant task called the shift-
sensitive and shift-invariant top-bottom prediction tasks. In
these experiments, we set λ = 0.1.

4.2. Conditional Training Overﬁts

4.3.1. SHIFT-SENSITIVE ESTIMATION

To demonstrate the hybrid training’s regularization be-
havior, we show the test set performance during training
(Fig. 4) on the 2-quadrant MNIST task (nl = 10000). Even
with pre-trained initialization of parameters, models that
were trained conditionally quickly overﬁt, resulting in poor
test set performance. In contrast, hybrid training regular-
izes the conditional model toward the joint model, which is

In the shift-sensitive task, the objective is to learn to predict
the bottom half of the MNIST digit (y) when given the top
half (x). However, we introduce structural perturbation to
the top and bottom halves of the image in our training, val-
idation, and test sets by randomly shifting each pair (x, y)
horizontally by the same number of pixels (shift varies be-
tween {−4, −3, . . . , 3, 4}). We then train the BCDE us-
ing either the conditional or hybrid objective in the fully-

Bottleneck Conditional Density Estimation

supervised regime. Note that compared to the original top-
down prediction task, the perplexity of the conditional task
remains the same after the perturbation is applied.

continues to achieve better performance than conditional
models and suffer a much smaller performance gap when
structural corruption in x is introduced.

Models

No Shift

Shift

Conditional
Hybrid
Hybrid + Factored

41.59 ± 0.02
41.33 ± 0.01
41.20 ± 0.02

44.02 ± 0.03
43.51 ± 0.01
43.19 ± 0.02

∆

2.43
2.17
1.99

Table 7. Shift-sensitive top-bottom MNIST prediction. Perfor-
mance with and without structural corruption reported, along with
the performance difference. Hybrid training is robust against
structural perturbation of (x, y).

Table 7 shows that hybrid training consistently achieves
better performance than conditional training. Furthermore,
the hybridly trained models were less affected by the in-
troduction of the perturbation, demonstrating a higher de-
gree of robustness. Because of its more compact recogni-
tion model, hybrid + factored is less vulnerable to overﬁt-
ting, resulting in a smaller performance gap between per-
formance on the shifted and original data.

4.3.2. SHIFT-INVARIANT ESTIMATION

The shift-invariant task is similar to the shift-sensitive top-
bottom task, but with one key difference: we only introduce
structural noise to the top half of the image in our training,
validation, and test sets. The goal is thus to learn that the
prediction of y (which is always centered) is invariant to
the shifted position of x.

Models

No Shift

Shift

Conditional
Hybrid
Hybrid + Factored

41.59 ± 0.02
41.33 ± 0.01
41.20 ± 0.02

42.99 ± 0.04
42.53 ± 0.02
42.20 ± 0.02

∆

1.40
1.20
1.00

Table 8. Shift-invariant top-bottom MNIST prediction. Perfor-
mance with and without structural corruption reported, along with
the performance difference. Hybrid training is robust against
structural corruption of x.

Figure 5. Visualization of
space of hybrid and
conditionally-trained BCDEs. PCA plots of the latent space sub-
region for all x’s whose class label = 2 are shown. Fill color
indicates the degree of shift: blue = −4, orange = +4.

the latent

Table 8 shows similar behavior to Table 7. Hybrid training

In Fig. 5, we show the PCA projections of the latent space
sub-region populated by digits 2 and color-coded all points
based on the degree of shift. We observe that hybrid train-
ing versus conditional training of the BCDE result in very
different learned representations in the stochastic layer. Be-
cause of regularization toward the joint model, the hybrid
BCDE’s latent representation retrains information about x
and learns to untangle shift from other features. And as ex-
pected, conditional training does not encourage the BCDE
to be aware of the distribution of x, resulting in a latent
representation that is ignorant of the shift feature of x.

5. Conclusion

We presented a new framework for high-dimensional con-
ditional density estimation. The building blocks of our
framework are a pair of sibling models:
the Bottleneck
Conditional Density Estimator (BCDE) and the Bottleneck
Joint Density Estimator (BJDE). These models use layers
of stochastic neural networks as bottleneck between the in-
put and output data. While the BCDE learns the conditional
distribution p(y|x), the BJDE learns the joint distribution
p(x, y). The bottleneck constraint implies that only the bot-
tleneck needs to be marginalized when either the input x or
the output y are missing during training, thus, enabling the
BJDE to be trained in a semi-supervised fashion.

The key component of our framework is our hybrid objec-
tive function that regularizes the BCDE towards the BJDE.
Our new objective is a novel extension of Lasserre et al.
(2006) that enables the principle of hybrid blending to be
applied to deep variational models. Our framework pro-
vides a new mechanism for the BCDE, a conditional model,
to become more robust and to learn from unlabeled data in
semi-supervised conditional density estimation.

Our experiments showed that hybrid training is compet-
itive in the fully-supervised regime against pre-training,
and achieves superior performance in the semi-supervised
quadrant prediction task in comparison to conditional
models, achieving new state-of-the-art performances on
MNIST, SVHN, and CelebA. Even with pre-trained weight
initializations, the conditional model is still susceptible to
overﬁtting. In contrast, hybrid training is signiﬁcantly more
robust against overﬁtting. Furthermore, hybrid training
transfers the nice embedding properties of the BJDE to the
BCDE, allowing the BCDE to learn better and more robust
representation of the input x. The success of our hybrid
training framework makes it a prime candidate for other
high-dimensional conditional density estimation problems,
especially in semi-supervised settings.

Bottleneck Conditional Density Estimation

Ranganath, R., Tran, D., and Blei, D. M. Hierarchical Vari-
ational Models. ArXiv e-prints, 1511.02386, November
2015.

Rezende, D., Mohamed, S., and Wierstra, D. Stochas-
tic Backpropagation and Approximate Inference in Deep
Generative Models. ArXiv e-prints, 1401.4082, January
2014.

Salakhutdinov, R. and Murray, I. On the quantitative anal-
ysis of deep belief networks. International Conference
on Machine Learning, 2008.

Salimans, Tim, Karpathy, Andrej, Chen, Xi, and Kingma,
Diederik P. Pixelcnn++: Improving the pixelcnn with
discretized logistic mixture likelihood and other modi-
ﬁcations. CoRR, abs/1701.05517, 2017. URL http:
//arxiv.org/abs/1701.05517.

Sohn, K., Shang, W., and H., Lee. Improved multimodal
deep learning with variation of information. Neural In-
formation Processing Systems, 2014.

Sohn, K., Yan, X., and Lee, H. Learning structured output
representation using deep conditional generative models.
Neural Information Processing Systems, 2015.

Sønderby, C. K., Raiko, T., Maaløe, L., Kaae Sønderby,
S., and Winther, O. Ladder Variational Autoencoders.
arXiv:1602.02282, 2016.

Theis, L., van den Oord, A., and Bethge, M. A note on
the evaluation of generative models. arXiv:1511.01844,
2015.

Tipping, M. and Bishop, C. Probabilistic Principal Com-

ponent Analysis. J. R. Statist. Soc. B, 1999.

Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. On
the Quantitative Analysis of Decoder-Based Generative
Models. arXiv:1611.04273, 2016.

Yan, X., Yang, J., Sohn, K., and Lee, H. Attribute2Image:
Conditional Image Generation from Visual Attributes.
arXiv:1512.00570, 2015.

Zhang, B., Xiong, D., Su, J., Duan, H., and Zhang, M. Vari-
ational Neural Machine Translation. arXiv:1605.07869,
2016.

References

Abadi, Mart´ın, et. al. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. URL http:
//tensorflow.org/. Software available from ten-
sorﬂow.org.

Burda, Y., Grosse, R., and Salakhutdinov, R. Importance
Weighted Autoencoders. arXiv preprints:1509.00519,
2015.

Dayan, P., Hinton, G., Neal, R., and Zemel, R. The

Helmholtz Machine. Neural computation, 1995.

Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. Pos-
terior regularization for structured latent variable mod-
els. JMLR, 2010.

Hinton, G., Dayan, P., Frey, B., and Radford, R. The
“wake-sleep” algorithm for unsupervised neural net-
works. Science, 1995.

Holmes, M. P., Gray, A. G.,

and Isbell, C. L.
Fast Nonparametric Conditional Density Estimation.
arXiv:1206.5278, 2012.

Ioffe, S. and Szegedy, C. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Co-
variate Shift. arXiv:1502.03167, 2015.

Kingma, D. and Ba, J. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014.

Kingma, D. P and Welling, M. Auto-Encoding Variational

Bayes. arXiv:1312.6114, 2013.

Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling,
M. Semi-Supervised Learning with Deep Generative
Models. arXiv:1406.5298, 2014.

Kingma, Diederik P., Salimans, Tim, and Welling, Max.
Improving variational inference with inverse autoregres-
sive ﬂow. CoRR, abs/1606.04934, 2016. URL http:
//arxiv.org/abs/1606.04934.

Lasserre, J., Bishop, C., and Minka, T. Principled hybrids
In The IEEE
of generative and discriminative models.
Conference on Computer Vision and Pattern Recogni-
tion, 2006.

Maaløe, L., Kaae Sønderby, C., Kaae Sønderby, S.,
and Winther, O. Auxiliary Deep Generative Models.
arXiv:1602.05473, 2016.

Ng, A. and Jordan, M. On discriminative vs. generative
classiﬁers: A comparison of logistic regression and naive
bayes. Neural Information Processing Systems, 2002.

Bottleneck Conditional Density Estimation

A. Factored Inference

When training the BJDE in the semi-supervised regime,
we introduce a factored inference procedure that reduce the
number of parameters in the recognition model.

In the semi-supervised regime, the 1-layer BJDE recog-
nition model
requires approximating three posteriors:
p(z|x, y) ∝ p(z)p(x, y|z), p(z|x) ∝ p(z)p(x|z), and
p(z|y) ∝ p(z)p(y|z). The standard approach would be to
assign one recognition network for each approximate pos-
terior. This approach, however, does not take advantage
of the fact that these posteriors share the same likelihood
functions, i.e., p(x, y|z) = p(x|z)p(y|z).

Rather than learning the three approximate posteriors in-
dependently, we propose to learn the approximate likeli-
hood functions ˆ(cid:96)(z; x) ≈ p(x|z), ˆ(cid:96)(z; y) ≈ p(y|z) and
let ˆ(cid:96)(z; x, y) = ˆ(cid:96)(z; x)ˆ(cid:96)(z; y). Consequently, this factor-
ization of the recognition model enables parameter sharing
within the joint recognition model (which is beneﬁcial for
semi-supervised learning) and eliminates the need for con-
structing a neural network that takes both x and y as in-
puts. The latter property is especially useful when learning
a joint model over multiple, heterogeneous data types (e.g.
image, text, and audio).

In practice, we directly learn recognition networks for
q(z|x) and ˆ(cid:96)(z; y) and perform factored inference as fol-
lows

Since our model includes unpaired y, we modify Eq. (15)
to include

p(Xl, Yl, Xu, Yu, ˜θ, θ) = p(˜θ, θ)

p(Xu|˜θ)p(Yu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).

(16)

To account for the variational parameters, we include them
in the joint density as well,

p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) = p(˜θ, ˜φ, θ, φ)

p(Xu|˜θ, ˜φ)p(Yu|˜θ, ˜φ)
p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ)

(17)

(18)

By taking the log and replacing the necessary densities with
their variational lower bound,

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) ≥ ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl),

we arrive at Eq. (11). We note, however, that a more gen-
eral hybrid objective Eq. (13) is achievable. To derive the
general objective, we consider an alternative factorization
of the joint density in Eq. (17),

p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ) = p(˜θ, ˜φ, θ, φ, )
p(Xl, Yl, Xu, Yu|˜θ, ˜φ, θ, φ).

(19)

q(z|x, y) ∝ q ˜φ(z|x)ˆ(cid:96) ˜φ(z; y), q(z|y) ∝ p(z)ˆ(cid:96) ˜φ(z; y),

We factorize the likelihood term such that Xu and Yu are
always explained by the joint parameters ˜θ, ˜φ,

(14)

(cid:16)

(cid:17)
(cid:104)T (z), η ˜φ(y)(cid:105)

where ˜φ parameterizes the recognition networks. To en-
sure proper normalization in Eq. (14), it is sufﬁcient for ˆ(cid:96)
to be bounded. If the prior p(z) belongs to an exponential
family with sufﬁcient statistics T (z), we can parameterize
ˆ(cid:96) ˜φ(z; y) = exp
, where η ˜φ(y) is a network
such that η ˜φ(y) ∈ {η|{(cid:104)T (z), η(cid:105) ∀z} is upper bounded}.
Then the approximate posterior can be obtained by simple
addition in the natural parameter space of the correspond-
ing exponential family. When the prior and approximate
likelihood are both Gaussians, this is exactly precision-
weighted merging of the means and variances (Sønderby
et al., 2016).

p(Xl, Yl, Xu, Yu|˜θ, ˜φ, θ, φ) = p(Xu|˜θ, ˜φ)p(Yu|˜θ, ˜φ)

p(Xl, Yl|˜θ, ˜φ, θ, φ).

(20)

We then introduce an auxiliary variable s = {0, 1},

p(Xl, Yl|˜θ, ˜φ, θ, φ)

(cid:88)

=

s

p(s)p(Xl, Yl|˜θ, ˜φ, θ, φ, s),

(21)

where

p(Xl, Yl|˜θ, ˜φ, θ, φ, s0) = p(Xl, Yl|˜θ, ˜φ)
p(Xl, Yl|˜θ, ˜φ, θ, φ, s1) = p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ).

(22)

B. Derivation of the Hybrid Objective

We ﬁrst provide the derivation of Eq. (11). We begin with
the factorization proposed in Eq. (7), which we repeat here
for self-containedness,

Jensen’s
Using
ln p(Xl, Yl|˜θ, ˜φ, θ, φ) with

inequality, we

can

lower

bound

p(s0) ln p(Xl, Yl|˜θ, ˜φ) + p(s1) ln p(Xl|˜θ, ˜φ)p(Yl|Xl, θ, φ).

p(Xl, Yl, Xu, ˜θ, θ) = p(˜θ, θ)

p(Xu|˜θ)p(Xl|˜θ)p(Yl|Xl, θ).

(15)

By taking the log of Eq. (19), replacing all remaining
densities with their variational lower bound, and setting

(23)

(24)

Bottleneck Conditional Density Estimation

p(s0) = α,

ln p(Xl, Yl, Xu, Yu, ˜θ, ˜φ, θ, φ)

≥ H(˜θ, ˜φ, θ, φ; Xl, Yl, Xu, Yu)
= ln p(˜θ, ˜φ, θ, φ) +

Jx(˜θ, ˜φ; Xu) + Jy(˜θ, ˜φ; Yu) +
α · Jxy(˜θ, ˜φ; Xl, Yl) +

(25)

(1 − α) ·

(cid:104)

(cid:105)
Jx(˜θ, ˜φ; Xl) + C(θ, φ; Xl, Yl)

, (26)

we arrive at the general hybrid objective. Note that when
α = 0, Eq. (26) reduces to Eq. (18).

We show visualizations of the hybrid BCDE predictions for
CelebA and SVHN on the top-down prediction task in the
nl = 10000 semi-supervised regime. For each data set, we
visualize both the images sampled during reconstruction as
well as prediction using an approximation of the MAP es-
timate by greedily sampling the mode of each conditional
distribution in the generative path.

C. Visualizations for CelebA and SVHN

(a) Conditional Rec.

(b) Conditional Pred.

(c) Pre-train Rec.

(d) Pre-train Pred.

(a) Hybrid Rec.

(b) Hybrid Pred.

Figure 6. Visualization of the reconstructed and predicted bottom
half of SVHN test set images when conditioned on the top half.

(e) Hybrid Rec.

(f) Hybrid Pred.

(g) Hybrid Factored Rec.

(h) Hybrid Factored Pred.

Figure 7. Visualization of the reconstructed and predicted bottom
half of CelebA test set images when conditioned on the top half.

