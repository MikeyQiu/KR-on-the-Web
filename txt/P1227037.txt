4
1
0
2
 
r
p
A
 
8
2
 
 
]

G
L
.
s
c
[
 
 
1
v
6
7
8
6
.
4
0
4
1
:
v
i
X
r
a

Conditional Density Estimation
with Dimensionality Reduction via
Squared-Loss Conditional Entropy Minimization

Voot Tangkaratt, Ning Xie, and Masashi Sugiyama

Tokyo Institute of Technology, Japan.

voot@sg., xie@sg., sugi@
}

{

cs.titech.ac.jp

Abstract

Regression aims at estimating the conditional mean of output given input.
However, regression is not informative enough if the conditional density is multi-
modal, heteroscedastic, and asymmetric. In such a case, estimating the conditional
density itself is preferable, but conditional density estimation (CDE) is challenging
in high-dimensional space. A naive approach to coping with high-dimensionality
is to ﬁrst perform dimensionality reduction (DR) and then execute CDE. However,
such a two-step process does not perform well in practice because the error in-
curred in the ﬁrst DR step can be magniﬁed in the second CDE step. In this paper,
we propose a novel single-shot procedure that performs CDE and DR simulta-
neously in an integrated way. Our key idea is to formulate DR as the problem
of minimizing a squared-loss variant of conditional entropy, and this is solved
via CDE. Thus, an additional CDE step is not needed after DR. We demonstrate
the usefulness of the proposed method through extensive experiments on various
datasets including humanoid robot transition and computer art.

Keywords: Conditional density estimation, dimensionality reduction

1 Introduction

Analyzing input-output relationship from samples is one of the central challenges in
machine learning. The most common approach is regression, which estimates the con-
ditional mean of output y given input x. However, just analyzing the conditional mean
x) possesses multimodal-
is not informative enough, when the conditional density p(y
ity, asymmetry, and heteroscedasticity (i.e., input-dependent variance) as a function of
output y. In such cases, it would be more appropriate to estimate the conditional density
itself (Figure 2).

|

The most naive approach to conditional density estimation (CDE) would be ǫ-
neighbor kernel density estimation (ǫ-KDE), which performs standard KDE along y
only with nearby samples in the input domain. However, ǫ-KDE do not work well
in high-dimensional problems because the number of nearby samples is too few. To

1

|

avoid the small sample problem, KDE may be applied twice to estimate p(x, y) and
p(x) separately and the estimated densities may be plugged into the decomposed form
p(y
x) = p(x, y)/p(x) to estimate the conditional density. However, taking the ra-
tio of two estimated densities signiﬁcantly magniﬁes the estimation error and thus is
not reliable. To overcome this problem, an approach to directly estimating the density
ratio p(x, y)/p(x) without separate estimation of densities p(x, y) and p(x) has been
explored [Sugiyama et al., 2010]. This method, called least-squares CDE (LSCDE),
was proved to possess the optimal non-parametric learning rate in the mini-max sense,
and its solution can be efﬁciently and analytically computed. Nevertheless, estimating
conditional densities in high-dimensional problems is still challenging.

A natural idea to cope with the high-dimensionality is to perform dimensionality re-
duction (DR) before CDE. Sufﬁcient DR [Li, 1991, Cook and Ni, 2005] is a framework
of supervised DR aimed at ﬁnding the subspace of input x that contains all informa-
tion on output y, and a method based on conditional-covariance operators in reproduc-
ing kernel Hilbert spaces has been proposed [Fukumizu et al., 2009]. Although this
method possesses superior thoretical properties, it is not easy to use in practice because
no systematic model selection method is available for kernel parameters. To overcome
this problem, an alternative sufﬁcient DR method based on squared-loss mutual infor-
mation (SMI) has been proposed recently [Suzuki and Sugiyama, 2013]. This method
involves non-parametric estimation of SMI that is theoretically guaranteed to achieve
the optimal estimation rate, and all tuning parameters can be systematically chosen in
practice by cross-validation with respect to the SMI approximation error.

Given such state-of-the-art DR methods, performing DR before LSCDE would be a
promising approach to improving the accuracy of CDE in high-dimensional problems.
However, such a two-step approach is not preferable because DR in the ﬁrst step is
performed without regard to CDE in the second step and thus small error incurred in
the DR step can be signiﬁcantly magniﬁed in the CDE step.

In this paper, we propose a single-shot method that integrates DR and CDE. Our key
idea is to formulate the sufﬁcient DR problem in terms of the squared-loss conditional
entropy (SCE) which includes the conditional density in its deﬁnition, and LSCDE is ex-
ecuted when DR is performed. Therefore, when DR is completed, the ﬁnal conditional
density estimator has already been obtained without an additional CDE step (Figure 1).
We demonstrate the usefulness of the proposed method, named least-squares condi-
tional entropy (LSCE), through experiments on benchmark datasets, humanoid robot
control simulations, and computer art.

2 Conditional Density Estimation with Dimensionality

Reduction

In this section, we describe our proposed method for conditional density estimation
with dimensionality reduction.

2

(a) CDE without DR

(b) CDE after DR

(c) CDE with DR (proposed)

Figure 1: Conditional density estimation (CDE) and dimensionality reduction (DR). (a)
CDE without DR performs poorly in high-dimensional problems. (b) CDE after DR can
magnify the small DR error in the CDE step. (c) CDE with DR (proposed) performs
CDE in the DR process in an integrated manner.

2.1 Problem Formulation

⊂

Dy(

Dx(

Rdx) and

Rdy) be the input and output domains with dimensionality dx
Let
and dy, respectively, and let p(x, y) be a joint probability density on
Dx × Dy. Assume
that we are given n independent and identically distributed (i.i.d.) training samples from
the joint density:

⊂

(xi, yi)

{

n
i=1

}

i.i.d.
∼

p(x, y).

The goal is to estimate the conditional density p(y

x) from the samples.

Our implicit assumption is that the input dimensionality dx is large, but its “intrinsic”
dimensionality, denoted by dz, is rather small. More speciﬁcally, let W and W ⊥ be
is an orthogonal matrix.
dz ×
Then we assume that x can be decomposed into the component z = W x and its
perpendicular component z⊥ = W ⊥x so that y and x are conditionally independent
given z:

dx and (dx −

dx matrices such that

W ⊤, W ⊤
⊥

dz)

×

(cid:2)

(cid:3)

|

y

z.

x
|

⊥⊥

(1)

This measn that z is the relevant part of x, and the rest z⊥ does not contain any infor-
mation on y. The problem of ﬁnding W is called sufﬁcient dimensionality reduction
[Li, 1991, Cook and Ni, 2005].

2.2 Sufﬁcient Dimensionality Reduction with SCE

Let us consider a squared-loss variant of conditional entropy named squared-loss CE
(SCE):

SCE(Y

Z) =

|

1
2

−

p(y

z)

1

p(z)dzdy.

|

−

(2)

2

(cid:17)

Z Z (cid:16)

3

By expanding the squared term in Eq.(2), we obtained

SCE(Y

Z) =

|

−

p(y

z)2p(z)dzdy +

p(y

z)p(z)dzdy

p(z)dzdy

1
2

−

Z Z

1
2
1
2

Z Z

=

=

−

Z Z
SCE(Y

|

|

p(y

z)2p(z)dzdy + 1

Z) + 1

|

1
2

−

dy,

Z

|

dy

Z Z

1
2

−

Z

where

SCE(Y

g
Z) is deﬁned as

|

g

SCE(Y

Z) =

|

1
2

−

p(y

z)2p(z)dzdy.

|

Z Z
Then we have the following theorem (its proof is given in Appendix A), which forms
the basis of our proposed method:

g

Theorem 1.

SCE(Y

Z)

SCE(Y

X) =

|

−

|

g

g

1
2
0.

≥

|

≥

if

p(z⊥, y

z)
z)p(y

|

p(z⊥

Z Z (cid:18)

z) −

|

(cid:19)

|

2

1

p(y

z)2p(x)dxdy

This theorem shows

SCE(Y

Z)

SCE(Y

X), and the equality holds if and only

This is equivalent to the conditional independence (1), and therefore sufﬁcient dimen-
sionality reduction can be performed by minimizing

Z) with respect to W :

SCE(Y

|

|

|

g

p(z⊥, y

|

g
z) = p(z⊥

z)p(y

z).

|

|

W ∗ = argmin
(R)

W ∈Gdx
dz

SCE(Y

Z = W X).
g

|

g

(3)

(4)

(5)

Here, Gdx
without overlaps:

dz (R) denotes the Grassmann manifold, which is a set of orthogonal matrices

Gdx

dz (R) =

W

{

∈

Rdz×dx

|

W W ⊤ = I dz}

,
∼
represents the equivalence relation: W and

/

where I denotes the identity matrix and
W ′ are written as W

∼

W ′ if their rows span the same subspace.

|

Since p(y

z) = p(z, y)/p(z), SCE(Y

Z) is equivalent to the negative Pearson di-
|
vergence [Pearson, 1900] from p(z, y) to p(z), which is a member of the f -divergence
class [Ali and Silvey, 1966, Csisz´ar, 1967] with the squared-loss function. On the other
hand, ordinary conditional entropy (CE), deﬁned by

∼

CE(Y

Z) =

p(z, y) log p(y

z)dzdy,

|

−

Z Z

|

4

is the negative Kullback-Leibler divergence [Kullback and Leibler, 1951] from p(z, y)
to p(z). Since the Kullback-Leibler divergence is also a member of the f -divergence
class (with the log-loss function), CE and SCE have similar properties. Indeed, the
above theorem also holds for ordinary CE. However, the Pearson divergence is shown to
be more robust against outliers [Sugiyama et al., 2012], since the log function—which
is very sharp near zero—is not included. Furthermore, as shown below,
SCE can be
approximated analytically and thus its derivative can also be easily computed. This is
a critical property for developing a dimensionality reduction method because we want
SCE with respect to W , where the gradient is highly useful in devising an
to minimize
optimization algorithm. For this reason, we adopt SCE instead of CE below.

g

g
2.3 SCE Approximation

Since
(zi, yi)

{

|
n
zi = W xi
i=1.
The trivial inequality (a

|
g

}

SCE(Y

Z) in Eq.(5) is unknown in practice, we approximate it using samples

b)2/2

0 yields a2/2

ab

b2/2, and thus we have

−

≥

−

a2
2

= max

ab

b

(cid:20)

≥

.

(cid:21)

b2
2

−

(6)

If we set a = p(y

z), we have

|

z)2
p(y
|
2

max
b

≥

(cid:20)

p(y

z)b(z, y)

|

b(z, y)2
2

−

.

(cid:21)

If we multiply both sides of the above inequality with
y, we have

−

p(z), and integrated over z and

SCE(Y

Z)

min
b

≤

|

Z Z (cid:20)

b(z, y)2p(z)
2

−

b(z, y)p(z, y)

dzdy,

(7)

(cid:21)

g

where minimization with respect to b is now performed as a function of z and y. For
more general discussions on divergence bounding, see [Keziou, 2003] and [Nguyen et al.,
2010].

Let us consider a linear-in-parameter model for b:

b(z, y) = α⊤ϕ(z, y),

where α is a parameter vector and ϕ(z, y) is a vector of basis functions. If the expec-
tations over densities p(z) and p(z, y) are approximated by samples averages and the
ℓ2-regularizer λα⊤α/2 (λ

0) is included, the above minimization problem yields

≥

α = argmin

α

α⊤

Gα

⊤

h

α +

−

1
2

(cid:20)

λ
2

,

α⊤α
(cid:21)

b

b

b

5

where

¯Φ(zi),

ϕ(zi, yi),

G =

b
h =

1
n

1
n

n

i=1
X
n

i=1
X

Z

b
¯Φ(z) =

ϕ(z, y)ϕ(z, y)⊤dy.

(8)

The solution

α is analytically given by

b

α =

G + λI

h,

−1

(cid:17)
α⊤ϕ(z, y). Then, from Eq.(7), an approximator of

(cid:16)

b

b

b

b(z, y) =

which yields
is obtained analytically as
b

b

SCE(Y

Z) =

|

1
2

α⊤

G

α

⊤

h

α.

−

We call this method least-squares conditional entropy (LSCE).

d

b

b

b

b

b

2.4 Model Selection by Cross-Validation

SCE(Y

Z)

|

g

The above
SCE approximator depends on the choice of models, i.e., the basis function
ϕ(z, y) and the regularization parameter λ. Such a model can be objectively selected
by cross-validation as follows:

g

1. The training dataset

}
with (approximately) the same size.

S

{

=

(xi, yi)

n
i=1 is divided into K disjoint subsets

K
j=1

j
{S

}

2. For each model M in the candidate set,

(a) For j = 1, . . . , K,

i. For model M, the LSCE solution

b(M,j) is computed from

j (i.e.,

S\S

all samples except

j).

S

ii. Evaluate the upper bound of

SCE obtained by

b(M,j) using the hold-out

b

data

j:

S

CVj(M) =

1

2

j
|S

z∈Sj Z
| X

b

g

b(M,j)(z, y)2dy

b
1

b(M,j)(z, y),

−

j
|S

| X(z,y)∈Sj

b

where

denotes the cardinality of

|
(b) The average score is computed as

j
|S

j.

S

CV(M) =

CVj(M).

1
K

K

j=1
X

6

3. The model that minimizes the average score is chosen:

M = argmin

CV(M).

M

4. For the chosen model
and the approximator

In the experiments, we use K = 5.

c

M, the LSCE solution
SCE(Y
c
d

Z) is computed.
b

|

2.5 Dimensionality Reduction with SCE

b is computed from all samples

S

Now we solve the following optimization problem by gradient descent:

argmin
W ∈Gdx
(R)
dz

SCE(Y

Z = W X).

|

(9)

d
As shown in Appendix B, the gradient of

SCE(Y

Z = W X) is given by

∂
SCE
∂Wl,l′
d

α⊤ ∂

=

G
∂Wl,l′
b

(cid:18)

3
2

d
α
−

where

β =

G + λI

−1

b
α.
G

b

|

β

+

(cid:19)

b

⊤

h
∂
∂Wl,l′
b

(

β

2

α),

−

b

b

In the Euclidean space, the above gradient gives the steepest direction. However, on

(cid:16)

(cid:17)

a manifold, the natural gradient [Amari, 1998] gives the steepest direction.

b

b
The natural gradient

b

∇

∂ d
∂Wl,l′ to the tangent space of Gdx
SCE
d
= 1
canonical metric
[Edelman et al., 1998]:

W , W ′

h

i

b
SCE(W ) at W is the projection of the ordinary gradient
dz (R) at W . If the tangent space is equipped with the
2tr(W ⊤W ′), the natural gradient is given as follows

∇

SCE =

∂
SCE
∂W −
d
d
where W ⊥ is a (dx −
dx matrix such that
Then the geodesic from W to the direction of the natural gradient
Gdx
R as
dz (R) can be expressed using t

∂
SCE
∂W
d
W ⊤, W ⊤
⊥

∂
SCE
∂W
d

W ⊤W =

⊥W ⊥,

W ⊤

dz)

×

(cid:2)

(cid:3)

∈

is an orthogonal matrix.

SCE over

∇

W t =

I dz Odz,(dx−dz)

(cid:2)

exp

t
 −

×

(cid:3)

Odz,dz

⊤

∂ d
SCE
∂W

W ⊥

"

−

∂ d
∂W W ⊤
SCE

⊥

Odx−dz,dx−dz#! (cid:20)

d
W
W ⊥

,

(cid:21)

d′
where “exp” for a matrix denotes the matrix exponential and Od,d′ denotes the d
zero matrix. Note that the derivative ∂tW t at t = 0 coincides with the natural gradient
SCE; see [Edelman et al., 1998] for details. Thus, line search along the geodesic in
.
}
Once W is updated, SCE is re-estimated with the new W and gradient descent is
d
performed again. This entire procedure is repeated until W converges. When SCE

∇
the natural gradient direction is equivalent to ﬁnding the minimizer from

W t

×

≥

{

0

t

|

7

is re-estimated, performing cross-validation in every step is computationally expensive.
In our implementation, we perform cross-validation only once every 5 gradient updates.
Furthermore, to ﬁnd a better local optimal solution, this gradient descent procedure is
executed 20 times with randomly chosen initial solutions and the one achieving the
smallest value of

SCE is chosen.

2.6 Conditional Density Estimation with SCE

d

z) in the current deriva-
Since the maximum of Eq.(6) is attained at b = a and a = p(y
tion, the optimal b(z, y) is actually the conditional density p(y
z) itself. Therefore,
α⊤ϕ(z, y) obtained by LSCE is a conditional density estimator. This actually implies
that the upper-bound minimization procedure described in Section 2.3 is equivalent to
least-squares conditional density estimation (LSCDE) [Sugiyama et al., 2010], which
b
minimizes the squared error:

|

|

1
2

Z Z (cid:16)

b(z, y)

p(y

z)

p(z)dzdy.

−

|

2

(cid:17)

Then, in the same way as the original LSCDE, we may post-process the solution
make the conditional density estimator non-negative and normalized as

α to

p(y

z =

z, y)
z, y′)dy′ ,
e
R
where
αl, 0). Note that, even if the solution is post-processed as Eq.(10),
e
the optimal estimation rate of the LSCDE solution is still maintained [Sugiyama et al.,
2010].

α⊤ϕ(
α⊤ϕ(
e
e

αl = max (

z) =

b

e

|

e

b

b
(10)

2.7 Basis Function Design

In practice, we use the following Gaussian function as the k-th basis:

ϕk(z, y) = exp

z

k

−

uk

y

k

2 +
k
2σ2

vk

2

k

−

,

(cid:19)

−

(cid:18)

(11)

where (uk, vk) denotes the k-th Gaussian center located at (zk, yk). When the sam-
ple size n is too large, we may use only a subset of samples as Gaussian centers. σ
denotes the Gaussian bandwidth, which is chosen by cross-validation as explained in
Section 2.4. We may use different bandwidths for z and y, but this will increase the
computation time for model selection. In our implementation, we normalize each ele-
ment of z and y to have the unit variance in advance and then use the common band-
width for z and y.

A notable advantage of using the Gaussian function is that the integral over y ap-

peared in ¯Φ(z) (see Eq.(8)) can be computed analytically as

¯Φk,k′(z) = (√πσ)dy exp

2

z

k

−

k

uk

2 + 2

z

uk′

2 +

vk

k

−
4σ2

k

k

vk′

2

k

−

.

(cid:19)

−

(cid:18)

8

Similarly, the normalization term in Eq.(10) can also be computed analytically as

α⊤ϕ(z, y)dy = (√2πσ)dy

αk exp

z

k

2

k

uk
−
2σ2

.

(cid:19)

−

(cid:18)

k
X

e

Z

e

2.8 Discussions

We have proposed to minimize SCE for dimensionality reduction:

SCE(Y

Z) =

|

1
2

−

p(z, y)

p(z) −

2

(cid:19)

Z Z (cid:18)

1

p(z)dzdy.

On the other hand, in the previous work [Suzuki and Sugiyama, 2013], squared-loss
mutual information (SMI) was maximized for dimensionality reduction:

SMI(Y , Z) =

p(z)p(y)dzdy.

1
2

Z Z (cid:18)

p(z, y)
p(z)p(y) −

1

2

(cid:19)

This shows that the essential difference is whether p(y) is included in the denominator
of the density ratio. Thus, if p(y) is uniform, the proposed dimensionality reduction
method using SCE is reduced to the existing method using SMI. However, if p(y) is
not uniform, the density ratio function p(z,y)
p(z)p(y) included in SMI may be more ﬂuctuated
than p(z,y)
included in SCE. Since a smoother function can be more accurately estimated
p(z)
from a small number of samples in general, the proposed method using SCE is expected
to work better than the existing method using SMI. We will experimentally demonstrate
this effect in Section 3.

3 Experiments

In this section, we experimentally investigate the practical usefulness of the proposed
method.

3.1 Illustration

We consider the following dimensionality reduction schemes:

None: No dimensionality reduction is performed.

LSMI: Dimension reduction is performed by maximizing an SMI approximator called
least-squares MI (LSMI) using natural gradients over the Grassmann manifold
[Suzuki and Sugiyama, 2013].

LSCE (proposed): Dimension reduction is performed by minimizing the proposed

LSCE using natural gradients over the Grassmann manifold.

True (reference) The “true” subspace is used (only for artiﬁcial data).

After dimension reduction, we execute the following conditional density estimators:

9

 

Data
None/LSCDE
LSCE/LSCDE

3

2

1

0

−1

−2

y

 

Data
Truth
None/LSCDE
LSCE/LSCDE
1.5

1

y

2

6

4

0

 

−2
2

−3

 

−1

−0.5

0

0.5

x(1)

3

4

5

6

7

x(1)

(a) Illustrative data

(b) Bone mineral density

y

3

6

5

4

2

1

 

0
3

 

Data
None/LSCDE
LSCE/LSCDE

4

5

6

7

8

x(1)

(c) Old faithful geyser

Figure 2: Examples of conditional density estimation by plain LSCDE (None/LSCDE)
and the proposed method (LSCE/LSCDE).

ǫ-KDE: ǫ-neighbor kernel density estimation, where ǫ is chosen by least-squares cross-

validation.

LSCDE: Least-squares conditional density estimation [Sugiyama et al., 2010].

Note that the proposed method, which is the combination of LSCE and LSCDE, does
not explicitly require the post-LSCDE step because LSCDE is executed inside LSCE.

First, we illustrate the behavior of the plain LSCDE (None/LSCDE) and the pro-
posed method (LSCE/LSCDE). The datasets illustrated in Figure 2 have dx = 5, dy = 1,
and dz = 1. The ﬁrst dimension of input x and output y of the samples are plotted in the
graphs, and other 4 dimensions of x are just standard normal noise. The results show
that the plain LSCDE does not perform well due to the irrelevant noise dimensions of
x, while the proposed method gives much better estimates.

3.2 Artiﬁcial Datasets

µ, Σ)
0, I 5), and ǫ
For dx = 5, dy = 1, x
denotes the normal distribution with mean µ and covariance matrix Σ, we consider the
following artiﬁcial datasets:

0, 0.252), where

(x
|

∼ N

∼ N

(ǫ
|

N

·|

(

(a) dz = 2 and y = (x(1))2 + (x(2))2 + ǫ.

10

 

0
50

100 150 200 250 300 350 400

Sample size n

0
−2

0

2
y

4

6

50 100 150 200 250 300 350 400
Sample size n

(a) Artiﬁcial data 1

1

0.8

R
D

0.6

r
o
r
r

E

0.4

0.2

0.25

0.2

R
D

0.15

r
o
r
r

E

0.1

0.05

 

 

LSMI
LSCE

LSMI
LSCE

40

30

20

10

y
c
n
e
u
q
e
r
F

200

150

100

y
c
n
e
u
q
e
r
F

50

0
−5

 
 

 
 

None/LSCDE
LSMI/LSCDE
LSCE/LSCDE
True/LSCDE

None/ε−KDE
LSMI/ε−KDE
LSCE/ε−KDE
True/ε−KDE

None/LSCDE
LSMI/LSCDE
LSCE/LSCDE
True/LSCDE

None/ε−KDE
LSMI/ε−KDE
LSCE/ε−KDE
True/ε−KDE

0.1

0

−0.1

E
D
C

−0.2

−0.3

r
o
r
r

E

−0.4

−0.5

−0.6

−0.7

 

 

0.5

1

0

E
D
C

−0.5

r
o
r
r

E

−1

−1.5

−2

−2.5

 

 

 

0
50 100 150 200 250 300 350 400
Sample size n

0

5

10

y

50 100 150 200 250 300 350 400
Sample size n

(b) Artiﬁcial data 2

Figure 3: Left column: The mean and standard error of the dimensionality reduction
400
i=1.
error over 20 runs on the artiﬁcial datasets. Middle column: Histograms of
Right column: The mean and standard error of the conditional density estimation error
over 20 runs.

yi

{

}

(b) dz = 1 and y = x(2) + (x(2))2 + (x(2))3 + ǫ.

The left column of Figure 3 shows the dimensionality reduction error between true

W ∗ and its estimate

W for different sample size n, measured by

ErrorDR =
c

⊤

W

W

k

−

W ∗⊤W ∗

kFrobenius,

{

c

c

k · kFrobenius denotes the Frobenius norm. LSMI and LSCE perform similarly for
where
the dataset (a), while LSCE clearly outperforms LSMI for the datasets (b). To explain
400
this difference, we plot the histograms of
i=1 in the middle column of Figure 3. They
y
}
show that the proﬁle of the histogram (which is a sample approximation of p(y)) in the
dataset (b) is much sharper than that in the dataset (a). As discussed in Section 2.8, the
density ratio p(z,y)
p(z)p(y) used in LSMI contains p(y). Thus, for the dataset (b), the density
ratio p(z,y)
p(z)p(y) would be highly non-smooth and thus is hard to approximate. On the other
hand, the density ratio used in SCE is p(z,y)
p(z) , where p(y) is not included. Therefore,
p(z,y)
is easier to estimate than p(z,y)
p(z) would be smoother than p(z,y)

p(z)
The right column of Figure 3 plots the conditional density estimation error between

p(z)p(y) and p(z,y)

p(z)p(y) .

true p(y

x) and its estimate

p(y

x), evaluated by the squared-loss:

ErrorCDE =

|

n′

1
b
2n′

i=1 Z
X

p(y

xi)2dy

|

1
n′

−

p(

yi|

xi),

n′

i=1
X

n′
where
i=1 is a set of test samples that have not been used for training. We
set n′ = 1000. The graphs show that LSCDE overall outperforms ǫ-KDE for both

yi)

xi,

{

}

(

b

b

e

e

e

|

e

e

11

Figure 4: Simulator of the upper-body part of the humanoid robot CB-i.

datasets. For the dataset (a), LSMI/LSCDE and LSCE/LSCDE perform equally well,
which are much better than no dimension reduction (None/LSCDE) and are comparable
to the method with the true subspace (True/LSCDE). For the dataset (b), LSCE/LSCDE
outperforms LSMI/LSCDE and None/LSCDE, and is comparable to the method with
the true subspace (True/LSCDE).

3.3 Benchmark Datasets

Next, we use the UCI benchmark datasets [Bache and Lichman, 2013]. We randomly
select n samples from each dataset for training, and the rest are used to measure the
conditional density estimation error in the test phase. Since the dimensionality of the
subspace dz is unknown, we chose it by cross-validation. The results are summarized
in Table 1, showing that that the proposed method, LSCE/LSCDE works well overall.
Table 2 describes the dimensionalities selected by cross-validation, showing that both
LSCE and LSMI reduce the dimensionalty signiﬁcantly. For “Housing”, “AutoMPG”,
“Energy”, and “Stock”, LSMI/LSCDE tends to more aggressively reduce the dimen-
sionality than LSCE/LSCDE.

3.4 Humanoid Robot

We evaluate the performance of the proposed method on humanoid robot transition
estimation. We use a simulator of the upper-body part of the humanoid robot CB-i
[Cheng et al., 2007] (see Figure 4). The robot has 9 controllable joints: shoulder pitch,
shoulder roll, elbow pitch of the right arm, shoulder pitch, shoulder roll, elbow pitch of
the left arm, waist yaw, torso roll, and torso pitch joints.

Posture of the robot is described by 18-dimensional real-valued state vector s, which
corresponds to the angle and angular velocity of each joint in radians and radians per
seconds, respectively. We can control the robot by sending the action command a
to the system. The action command a is a 9-dimensional real-valued vector, which

12

1
3

Table 1: Mean and standard error of the conditional density estimation error over 10 runs. The best method in term of the mean error and
comparable methods according to the two-sample paired t-test at the signiﬁcance level 5% are speciﬁed by bold face.

LSCE

LSMI

No reduction

−

−

−

−

−

Dataset

LSCDE

LSCDE

ǫ-KDE
1.62(0.08)

Housing
Auto MPG
Servo
Yacht

5.63(0.26)
0.99(0.02) −1.20(0.01)
2.31(0.01) −2.47(0.15) −2.35(0.02) −2.60(0.12)
1.95(0.17) −2.82(0.03)
1.93(0.17)
6.93(0.02)
6.93(0.03)
6.93(0.04)
1.20(0.06) −1.30(0.03)
1.18(0.04)
3.41(0.49)
6.04(0.47)
4.18(0.22)

(dx, dy) n
LSCDE
ǫ-KDE
(13, 1) 100 −1.73(0.09)
1.57(0.11) −1.91(0.05)
1.41(0.05)
(7, 1) 100 −1.80(0.04) −1.74(0.06) −1.85(0.04) −1.77(0.05) −1.75(0.04)
(4, 1) 50 −2.92(0.18) −3.03(0.14) −2.69(0.18) −2.95(0.11)
2.62(0.09)
(6, 1) 80 −6.46(0.02) −6.23(0.14)
5.47(0.29)
1.72(0.04)
0.97(0.02) −1.19(0.01)
2.06(0.01)
2.03(0.02)
3.40(0.07)
1.11(0.02)
2.12(0.06)
7.35(0.13)
3.95(0.13)
0.83(0.03)
1.60(0.36)
5.98(0.80)
7.69(0.62)
8.98(0.66)

−
8.37(0.53) −9.75(0.37) −9.42(0.50) −10.27(0.33)
7.50(0.54) −8.00(0.84)
2.06(0.25)
1.73(0.14)
2.44(0.17) −9.74(0.63)
1.49(0.78) −6.00(1.28)
9.54(1.31)
2.22(0.97)
18.0(2.61)
6.61(1.25)

Physicochem (9, 1) 500 −1.19(0.01)
White Wine
(11, 1) 400
(11, 1) 300 −2.85(0.02)
Red Wine
(12, 1) 100 −7.18(0.02)
Forest Fires
(8, 1) 300 −1.36(0.03)
Concrete
(8, 2) 200 −7.13(0.04)
Energy
(7, 2) 100
Stock
−
(6, 4) 100 −10.49(0.86)
2 Joints
(12, 8) 200 −2.81(0.21)
4 Joints
(27, 18) 500 −8.37(0.83)
9 Joints
(9, 6) 200 −9.96(1.60)
Sumi-e 1
(9, 6) 250 −16.83(1.70)
Sumi-e 2
(9, 6) 300 −24.92(1.92)
Sumi-e 3

7.44(0.60)
−
1.38(0.16)
−
2.37(0.51)
−
1.24(1.99)
3.12(0.75)
4.47(0.68)

−
−
−
−

−
−
−
−

−
−

−
−

−

−

−

−
−

−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−

ǫ-KDE
1.13(0.01)
1.46(0.04)
2.72(0.06)
2.95(0.02)
0.91(0.01)
1.89(0.01)
1.13(0.04)
6.96(0.02)
0.80(0.03)
1.95(0.14)
9.25(0.14)
3.65(0.14)
0.75(0.01)
0.89(0.02)
0.17(0.44)
0.66(0.13)
1.45(0.43)

−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−

Scale

1
×
1
×
1
×
1
×
1
×
1
×
1
×
1
×
1
×
10
×
1
×
1
×
10
×
100
×
10
×
10
×
10
×

Table 2: Mean and standard error of the chosen dimensionality over 10 runs.

Data set

(dx, dy)

Housing
Auto MPG
Servo
Yacht

(13, 1)
(7, 1)
(4, 1)
(6, 1)
Physicochem (9, 1)
White Wine
(11, 1)
Red Wine
(11, 1)
Forest Fires
(12, 1)
Concrete
(8, 1)
Energy
(8, 2)
Stock
(7, 2)
2 Joints
(6, 4)
4 Joints
(12, 8)
9 Joints
(27, 18)
Sumi-e 1
(9, 6)
Sumi-e 2
(9, 6)
Sumi-e 3
(9, 6)

LSCE

LSMI

LSCDE
3.9(0.74)
3.2(0.66)
1.9(0.35)
1.0(0.00)
6.5(0.58)
1.2(0.13)
1.0(0.00)
1.2(0.20)
1.0(0.00)
5.9(0.10)
3.2(0.83)
2.9(0.31)
5.2(0.68)
13.8(1.28)
5.3(0.72)
4.2(0.55)
3.6(0.50)

ǫ-KDE
2.0(0.79)
1.3(0.15)
2.4(0.40)
1.0(0.00)
1.9(0.28)
1.0(0.00)
1.3(0.15)
4.9(0.99)
1.0(0.00)
3.9(0.80)
2.1(0.59)
2.7(0.21)
6.2(0.63)
15.3(0.94)
2.9(0.85)
4.4(0.85)
2.7(0.76)

LSCDE
2.0(0.39)
2.1(0.67)
2.2(0.33)
1.0(0.00)
6.6(0.58)
1.4(0.31)
1.2(0.20)
1.4(0.22)
1.2(0.13)
2.1(0.10)
2.1(0.60)
2.5(0.31)
5.4(0.67)
11.4(0.75)
4.5(0.45)
4.6(0.87)
2.6(0.40)

ǫ-KDE
1.3(0.15)
1.1(0.10)
1.6(0.31)
1.0(0.00)
2.6(0.86)
1.0(0.00)
1.0(0.00)
6.8(1.23)
1.0(0.00)
2.0(0.30)
2.7(0.67)
2.0(0.00)
4.6(0.43)
13.2(1.02)
3.2(0.76)
2.5(0.78)
1.6(0.27)

corresponds to the target angle of each joint. When the robot is currently at state s and
receives action a, the physical control system of the simulator calculates the amount of
torques to be applied to each joint. These torques are calculated by the proportional-
derivative (PD) controller as

τi = Kpi(ai

si)

Kdi ˙si,

−

−
where si, ˙si, and ai denote the current angle, the current angular velocity, and the re-
ceived target angle of the i-th joint, respectively. Kpi and Kdi denote the position and
velocity gains for the i-th joint, respectively. We set Kpi = 2000 and Kdi = 100 for all
joints except that Kpi = 200 and Kdi = 10 for the elbow pitch joints. After the torques
are applied to the joints, the physical control system update the state of the robot to s′.
In the experiment, we randomly choose the action vector a and simulate a noisy
control system by adding a bimodal Gaussian noise vector. More speciﬁcally, the ac-
tion ai of the i-th joint is ﬁrst drawn from uniform distribution on [si
0.087, si+0.087].
The drawn action is then contaminated by Gaussian noise with mean 0 and standard de-
viation 0.034 with probability 0.6 and Gaussian noise with mean -0.087 and standard
deviation 0.034 with probability 0.4. By repeatedly control the robot n times, we obtain
n
j=1. Our goal is to learn the system dynamic as a
the transition samples
state transition probability p(s′
s, a) from these samples. Thus, as the conditional den-
sity estimation problem, the state-action pair (s⊤, a⊤)⊤ is regarded as input variable x,
while the next state s′ is regarded as output variable y. Such state-transition probabili-
ties are highly useful in model-based reinforcement learning [Sutton and Barto, 1998].
We consider three scenarios: Using only 2 joints (right shoulder pitch and right

(sj, aj, s′
j)

−

}

{

|

14

Figure 5: Three actions of the brush, which is modeled as the footprint on a paper
canvas.

elbow pitch), only 4 joints (in addition, right shoulder roll and waist yaw), and all 9
joints. Thus, dx = 6 and dy = 4 for the 2-joint case, dx = 12 and dy = 8 for the 4-joint
case, and dx = 27 and dy = 18 for the 9-joint case. We generate 500, 1000, and 1500
transition samples for the 2-joint, 4-joint, and 9-joint cases. We then randomly choose
n = 100, 200, and 500 samples for training, and use the rest for evaluating the test
error. The results are summarized also in Table 1, showing that the proposed method
performs well for the all three cases. Table 2 describes the dimensionalities selected by
cross-validation, showing that the humanoid robot’s transition is highly redundant.

3.5 Computer Art

Finally, we consider the transition estimation problem in sumi-e style brush drawings
for non-photorealistic rendering [Xie et al., 2012]. Our aim is to learn the brush dy-
namics as state transition probability p(s′
s, a) from the real artists’ stroke-drawing
samples.

|

From a video of real brush stroks, we extract footprints and identify corresponding
3-dimensional actions (see Figure 5). The state vector consists of six measurements:
the angle of the velocity vector and the heading direction of the footprint relative to
the medial axis of the drawing shape, the ratio of the offset distance from the center of
the footprint to the nearest point on the medial axis over the radius of the footprint, the
relative curvatures of the nearest current point and the next point on the medial axis,
and the binary signal of the reverse driving or not. Thus, the state transition probability
p(s′
s, a) has 9-dimensional input and 6-dimensional output. We collect 722 transition
samples in total. We randomly choose n = 200, 250, and 300 for training and use the
rest for testing.

|

The estimation results summarized at the bottom of Table 1 and Table 2. These
tables show that there exists a low-dimensional sufﬁcient subspace and the proposed
method can successfully ﬁnd it.

4 Conclusion

We proposed a new method for conditional density estimation in high-dimension prob-
lems. The key idea of the proposed method is to perform sufﬁcient dimensionality

15

reduction by minimizing the square-loss conditional entropy (SCE), which can be esti-
mated by least-squares conditional density estimation. Thus, dimensionality reduction
and conditional density estimation are carried out simultaneously in an integrated man-
ner. We have also shown that SCE and the squared-loss mutual information (SMI) are
similar but different in that the output density is included in the denominator of the den-
sity ratio in SMI. This means that estimation of SMI is hard when the output density
is ﬂuctuated, while the proposed method using SCE does not suffer from this prob-
lem. The effectiveness of the proposed method was demonstrated through extensive
experiments including humanoid robot transition and computer art.

16

References

S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from

another. Journal of the Royal Statistical Society, Series B, 28(1):131–142, 1966.

S. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276,

1998.

K. Bache and M. Lichman.

UCI machine learning repository,

2013.

URL

http://archive.ics.uci.edu/ml.

G. Cheng, S. Hyon, J. Morimoto, A. Ude, G.H. Joshua, Glenn Colvin, Wayco Scroggin, and
C. J. Stephen. Cb: A humanoid research platform for exploring neuroscience. Advanced
Robotics, 21(10):1097–1114, 2007.

R. D. Cook and L. Ni. Sufﬁcient dimension reduction via inverse regression. Journal of the

American Statistical Association, 100(470):410–428, 2005.

I. Csisz´ar.

Information-type measures of difference of probability distributions and indirect

observation. Studia Scientiarum Mathematicarum Hungarica, 2:229–318, 1967.

A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogonality

constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303–353, 1998.

K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel dimension reduction in regression. The

Annals of Statistics, 37(4):1871–1905, 2009.

A. Keziou.

Dual representation of φ-divergences and applications.

Comptes Rendus

Math´ematique, 336(10):857–862, 2003.

S. Kullback and R. A. Leibler. On information and sufﬁciency. The Annals of Mathematical

Statistics, 22:79–86, 1951.

K. Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical

Association, 86(414):316–342, 1991.

X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the
likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56
(11):5847–5861, 2010.

K. Pearson. On the criterion that a given system of deviations from the probable in the case of a
correlated system of variables is such that it can be reasonably supposed to have arisen from
random sampling. Philosophical Magazine Series 5, 50(302):157–175, 1900.

M. Sugiyama, I. Takeuchi, T. Kanamori, T. Suzuki, H. Hachiya, and D. Okanohara. Conditional
density estimation via least-squares density ratio estimation. In Y. W. Teh and M. Tiggering-
ton, editors, Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS2010), volume 9 of JMLR Workshop and Conference Proceedings,
pages 781–788, Sardinia, Italy, May 13-15 2010.

M. Sugiyama, T. Suzuki, and T. Kanamori. Density ratio matching under the Bregman diver-
gence: A uniﬁed framework of density ratio estimation. Annals of the Institute of Statistical
Mathematics, 64(5):1009–1044, 2012.

17

R. S. Sutton and G. A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,

MA, USA, 1998.

T. Suzuki and M. Sugiyama. Sufﬁcient dimension reduction via squared-loss mutual information

estimation. Neural Computation, 3(25):725–758, 2013.

N. Xie, H. Hachiya, and M. Sugiyama. Artist agent: A reinforcement learning approach to au-
tomatic stroke generation in oriental ink painting. In J. Langford and J. Pineau, editors, Pro-
ceedings of 29th International Conference on Machine Learning (ICML2012), pages 153–
160, Edinburgh, Scotland, Jun. 26–Jul. 1 2012.

18

A Proof of Theorem 1

The

SCE is deﬁned as

g

SCE(Y

Z) =

|

p(y

z)2p(z)dzdy.

|

1
2

−

Z Z

Then we have

g

SCE(Y

Z)

SCE(Y

X) =

|

−

|

p(y

x)2p(x)dydx

p(y

z)2p(z)dzdy

g

g

=

p(y

x)2p(x)dxdy +

p(y

z)2p(z)dzdy

−

1
2
1
2

Z Z

Z Z

|

|

p(y

z)2p(z)dzdy.

1
2
1
2

Z Z

Z Z

−

Z Z

|

|

|

Let p(x) = p(z, z⊥), and dx = dzdz⊥. Then the ﬁnal term can be expressed as

p(y

z)2p(z)dzdy =

|

Z Z

p(z, y)
p(z)

p(z)dzdy

p(z, y)dzdy

p(z⊥

z, y)p(z, y)dzdz⊥dy

|

p(z, z⊥, y)dzdz⊥dy

p(z, y)
p(z)
p(z, y)
p(z)
p(z, y)
p(z)
p(z, y)
p(z)
p(z, y)
p(z)
p(z, y)
p(z)

=

=

=

=

=

=

Z Z

Z Z

Z Z

Z Z

Z Z

Z Z

Z Z

p(x, y)dxdy

p(x, y)
p(x)

p(x)dxdy

p(y

z)p(y

x)p(x)dxdy,

|

|

where p(z, z⊥, y) = p(x, y), and dzdz⊥ = dx are used. Therefore,

SCE(Y

Z)

SCE(Y

X) =

|

−

|

g

g

1
2

Z Z

−
1
2

=

Z Z

Z Z

p(y

x)2p(x)dxdy +

p(y

z)2p(z)dzdy

1
2

Z Z

|

p(y

z)p(y

x)p(x)dxdy

|

(p(y

x)

|

−

|

p(y

z))2 p(x)dxdy

|

|

19

We can also express p(y

x) in term of p(y

z) as

|

p(y

x) =

|

|
p(x, y)
p(x)
p(x, y)
p(x)

p(z⊥

=

=

=

=

=

p(z, y)
p(z, y)
p(x, y)p(z, y)
z)p(z)p(y
p(z, z⊥, y)p(z, y)
z)p(z)p(y

|

|

z)p(z)

z)p(z)

|

z)p(z, y)
z)p(z)

p(z⊥
|
p(z⊥, y
p(z⊥

|
p(z⊥, y

|
z)p(y
|
z)
z)p(y

|

p(z⊥

|

p(y

z)

|

z)

|

Finally, we obtain

SCE(Y

Z)

SCE(Y

X) =

|

−

|

g

g

which concludes the proof.

B Derivatives of SCE

(p(y

x)

p(y

z))2 p(x)dxdy

Z Z

Z Z (cid:18)

|
p(z⊥, y

|

−

|
z)
z)p(y
|
z)
z)p(y

|

p(z⊥

|
p(z⊥, y

z)

Z Z (cid:18)

p(z⊥

|

z) −

|

(cid:19)

2

p(y

z)

p(y

z)

p(x)dxdy

|

1

−

2

p(y

|

|

(cid:19)
z)2p(x)dxdy

1
2
1
2

1
2
0,

=

=

≥

Here we show the formula of derivatives of
approximation by LSCE estimator is

|

SCE(Y

Z) using LSCE estimator. SCE

Taking its partial derivatives with respect to W and we obtain

SCE(Y

Z) =

|

⊤

d

d
G

α

α⊤

1
2

−

⊤

h

α.

b

b

b

b

b

∂
SCE
∂Wl,l′
d

=

=

=

=

α +

−

∂

α

1
2

b
G

1
2  

α⊤
G
∂Wl,l′ −
b
b
α⊤
∂
∂Wl,l′
b
α⊤
∂
∂Wl,l′
b
α⊤
∂
∂Wl,l′
b

b
b
α +

b
b
α +

1
2

1
2

G

G

h
α
∂
∂Wl,l′
b
b
α)⊤
G
(
∂Wl,l′
b
b
α⊤
∂
1
∂Wl,l′
2
b
α⊤ ∂

G
b
∂Wl,l′
b

b

b

b

⊤

h
∂
∂Wl,l′
b

α

α

! −

b
α +

G

1
2

h

−

α⊤
∂
∂Wl,l′
b
α⊤ ∂

b
G
∂Wl,l′
b

α⊤
∂
b
∂Wl,l′
b

h

b

−

b
α

−

b

20

α

−
⊤

h
∂
b
∂Wl,l′
b

b

α⊤
b
∂
∂Wl,l′
b
α.

h

b

⊤

h
∂
∂Wl,l′
b

−

α

b

(12)

G + λI)−1 ∂

h
∂Wl,l′
b
G + λI)−1.

(

⊤

h
∂
∂Wl,l′
b

b

Next we consider the partial derivatives of

α as follows

b

h

=

=

∂(

∂(

b
h + (

G + λI)−1
∂Wl,l′
b
G + λI)−1
∂Wl,l′
b
G + λI)−1
b
b
∂(
h)⊤ +
∂Wl,l′
b
∂t X −1, we obtain

= (

b

α
∂
∂Wl,l′
b

α⊤
∂
∂Wl,l′
b
X −1 ∂X

h =

(
−

(

G
∂Wl,l′
b
G
∂Wl,l′
b
G + λI)−1.

b
α
−

b

0

Using ∂X−1

∂t =

∂(

−
G + λI)−1
∂Wl,l′
b

b

=

G + λI)−1 ∂
b

(
−
α⊤ ∂
b
−

(

G
∂Wl,l′
b

∂(

(

G + λI)−1
∂Wl,l′
b

h)⊤ =

Substitute Eq.(14) into Eq.(13) to obtain

b

b

b

G + λI)−1 ∂

G + λI)−1

h

(

G + λI)−1 ∂λI
∂Wl,l′

−

G + λI)−1

(

h

b

b

b

b

(13)

(14)

α⊤
∂
∂Wl,l′
b

=

α⊤ ∂
−

G
∂Wl,l′
b

b

b

⊤

h
∂
∂Wl,l′
b

G + λI)−1 +

(

G + λI)−1.

(

(15)

Finally, by substitute Eq.(15) into Eq.(12) and use (

b
G + λI)−1

G

α =

β, we have

∂
SCE
∂Wl,l′
d

=

α⊤ ∂
−

α⊤ ∂
b

+

G
∂Wl,l′
b
G
∂Wl,l′
b
(

β +

b
α

−

b
α

3
2

−

⊤

h
∂
∂Wl,l′
b
⊤
h
∂
∂Wl,l′
b
β) +

b
β +

α⊤ ∂

1
2

h
∂
b
∂Wl,l′
b
β

(

−

b
α

−
⊤

h
∂
b
∂Wl,l′
b

b

b
G
b
∂Wl,l′
b
⊤
α

α

b

b
2

α),

=

G
α⊤ ∂
b
∂Wl,l′
b
G and

b

where the partial derivatives of

b
h depend on the choice of basis function.

b

b

b

Here we consider the Gaussian basis function described in Section 2.4. Their partial

derivatives are given by

b

b

=

1
σ2n

−

=

1
σ2n

−

∂
Gk,k′
∂Wl,l′
b
hk
∂
∂Wl,l′
b

n

i=1
X
n

i=1
X

¯Φk,k′(zi)

ϕk(zi, yi)

(z(l)
(cid:16)

i −

(z(l)
(cid:16)

i −

k )(x(l′)
u(l)

i −

˜u(l′)
k ) + (z(l)

k′ )(x(l′)
u(l)

i −

˜u(l′)
k′ )

i −

(cid:17)

k )(x(l′)
u(l)

i −

˜u(l′)
k )

.

(cid:17)

21

