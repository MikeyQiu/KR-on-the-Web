Agnostic Estimation of Mean and Covariance

Kevin A. Lai∗

Anup B. Rao∗

Santosh Vempala∗

August 16, 2016

Abstract

We consider the problem of estimating the mean and covariance of a distribution from iid
samples in Rn, in the presence of an η fraction of malicious noise; this is in contrast to much
recent work where the noise itself is assumed to be from a distribution of known type. The
agnostic problem includes many interesting special cases, e.g., learning the parameters of a single
Gaussian (or ﬁnding the best-ﬁt Gaussian) when η fraction of data is adversarially corrupted,
agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time
algorithms to estimate the mean and covariance with error guarantees in terms of information-
theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value
Decomposition.

6
1
0
2
 
g
u
A
 
4
1
 
 
]
S
D
.
s
c
[
 
 
2
v
8
6
9
6
0
.
4
0
6
1
:
v
i
X
r
a

∗Georgia Tech. Email: {kevinlai, anup.rao, vempala}@gatech.edu

1

Introduction

The mean and covariance of a probability distribution are its most basic parameters (if they are
bounded). Many families of distributions are deﬁned using only these parameters. Estimating the
mean and covariance from iid samples is thus a fundamental and classical problem in statistics.
The sample mean and sample covariance are generally the best possible estimators (under mild
conditions on the distribution such as their existence). However, they are highly sensitive to noise.
The main goal of this paper is to estimate the mean, covariance and related functions in spite of
arbitrary (adversarial) noise.

Methods for eﬃcient estimation, in terms of sample complexity and time complexity, play
an important role in many algorithms. One such class of problems is unsupervised learning of
generative models. Here the input data is assumed to be iid from an unknown distribution of a
known type. The classical instantiation is Gaussian mixture models, but many other models have
been studied widely. These include topic models, stochastic block models, Independent Component
Analysis (ICA) etc. In all these cases, the problem is to estimate the parameters of the underlying
distribution from samples. For example, for a mixture of k Gaussians in Rn, it is known that
the sample and time complexity are bounded by nO(k) in general [KMV10, MV10, BS10] and by
poly(n, k) under natural separation assumptions [Das99, AK01, VW04, DS07, CR08, BV08, HK13].
For ICA, samples are of the form Ax where A is unknown and x is chosen randomly from an
unknown (non-Gaussian) product distribution; the problem is to estimate the linear transformation
A and thus unravel the underlying product structure [FJK96, NR09, Car98, HKO01, CJ10, BRV13,
AGMS12, BCV13, GVX14, VX15]. These, and other models (see e.g., [KV09]), have been a rich
and active subject of study in recent years and have lead to interesting algorithms and analyses.

The Achilles heel of algorithms for generative models is the assumption that data is exactly from
the model. This is crucial for known guarantees, and relaxations of it are few and specialized, e.g.,
in ICA, data could by noisy, but the noise itself is assumed to be Gaussian. Assumptions about rank
and sparsity are made in a technique that is now called Robust PCA [CSPW11, CLMW11, XCM10].
There have been attempts [Kwa08, MT+11] at achieving robustness by L1 minimization, but they
don’t give any error bounds on the output produced. A natural, important and wide open problem
is estimating the parameters of generative models in the presence of arbitrary, i.e., malicious noise,
a setting usually referred to as agnostic learning. The simplest version of this problem is to estimate
a single Gaussian in the presence of malicious noise. Alternatively, this can be posed as the problem
of ﬁnding a best-ﬁt Gaussian to data or agnostically learning a single Gaussian. We consider the
following generalization:

Problem 1 [Mean and Covariance] Given points in Rn that are each, with probability 1
η
from an unknown distribution with mean µ and covariance Σ, and with probability η completely
arbitrary, estimate µ and Σ.

−

There is a large literature on robust statistics (see e.g., [Hub11, HRRS11, MMY06]), with the
goal of ﬁnding estimators that are stable under perturbations of the data. The classic example for
points on a line is that the sample median is a robust estimator while the sample mean is not (a single
data point can change the mean arbitrarily). One measure for robustness of an estimator is called
breakdown point, which is the minimum fraction of noise that can make the estimator arbitrarily
bad. Robust statistics have been proposed and studied for mean and covariance estimation in high

1

dimension as well (see [Hub64, Tuk74, Mar76, SJD81, Don82, Dav87, HPL91, DG92, MSY92, MZ12,
CGR15] and the references therein). Most commonly used methods (including M-estimators) to
estimate the covariance matrix were shown to have very low break down points [Don82]. The
notion of robustness we consider quantiﬁes how far the estimated value is from the true value. To
the best of our knowledge, all the papers either suﬀer from the diﬃculty that their algorithms are
computationally very expensive, namely exponential time in the dimension, or have poor or no
guarantees for the output. Tukey’s median [Tuk74]) is an example of the former. It is deﬁned as
x i}i. As proven in [CGR15], this is an
the deepest point with respect to a given set of points
{
optimal estimate of the mean. But there is no known polynomial time algorithm to compute this.
Another well-known proposal (see [Sma90]) is the geometric median:

arg min

y

y
k

x ik2.

−

Xi
This has the advantage that it can be computed via a convex program. Unfortunately, as we
observe here (see Proposition 2.1), the error of the mean estimate produced by this method grows
polynomially with the dimension (also see [Bru11]).

This leads to the question, what is the best approximation one can hope for with η arbitrary
(adversarial) noise. From a purely information-theoretic point of view, it is not hard to see that
even for a single Gaussian N (µ, σ2) in one dimension, the best possible estimation of the mean will
have error as large as Ω(ησ), i.e., any estimate ˜µ can be forced to have
= Ω(ησ). For a more
general distribution, this can be slightly worse, namely, Ω(η3/4σ) (see Section 2.1). What about in
Rn? Perhaps surprisingly, but without much diﬃculty, one can show that the information-theoretic
upper bound matches the lower bound in any dimension, with no dependence on the dimension.
This raises a compelling algorithmic question: what are the best estimates for the mean and
covariance that can be computed eﬃciently?

µ
k

−

˜µ

k

In this paper, we give polynomial time algorithms to estimate the mean with error that is close
to the information-theoretically optimal estimator. The dependence on the dimension, of the error
in the estimated mean, is only √log n. To the best of our knowledge, this is the ﬁrst polynomial-
time algorithm with an error dependence on dimension that is less than √n, the bound achieved by
the geometric median. Moreover, as we state precisely later, our techniques extend to very general
input distributions and to estimating higher moments.

Our algorithm is practical. A matlab implementation for mean estimation can be found in
[KRV]. It takes less a couple of seconds to run on a 500-dimensional problem with 5000 samples
on a personal laptop.

Model. We are given points x 1, ..., x m ∈
η
−
probability each x i is independently sampled from a distribution
with mean µ and covariance
Σ, and with η probability it is picked by an adversary. For ease of notation, we will write x i ∼ Dη
when we want to say the x i is picked according to the above rule. The problem we are interested
in is to estimate µ and Σ given the samples. In the following, we will consider mainly two kinds of
distributions.

Rn sampled according to the following rule. With 1

D

Gaussian

= N (µ, Σ) is the Gaussian with mean µ and covariance Σ.

D
Bounded Moments Let

D

is a distribution with mean µ and covariance Σ. We say it has

2

bounded 2k’th moments if there exists a constant C2k such that for every unit vector v ,

E

(x

µ)T v

2k

C2k

E

(x

≤

µ)T v

−

−

(cid:0)

(cid:1)
2

(cid:16)

(cid:0)

k

2

(cid:17)

(cid:1)

x T v

Here Var
used, and for covariance estimation, C8 will be needed.
(cid:1)

=

(cid:0)

(cid:2)

(cid:3)

v T Σv

= C2k(Var

x T v

)k.

(1)

is the variance of x along v . For mean estimation, C4 will be

(cid:2)

(cid:3)

1.1 Main results

All the results we state hold with probability 1
also assume η is a less than a universal constant. We begin with agnostic mean estimation.

1/ poly(n) unless otherwise mentioned. We will

−

Theorem 1.1 (Gaussian mean). Let
algorithm that takes as input m = O
computes

µ such that the error

= N (µ, Σ), µ

D
n(log n+log 1/ǫ) log n
ǫ2

Rn. There exists a poly(n, 1/ǫ)-time
∈
independent samples x 1, ..., x m ∼ Dη and

b

µ
k

µ
k2 is bounded as follows:
(cid:16)
−

(cid:17)

b
η1/2 + ǫ

O (η + ǫ) σ√log n
Σ
k

1/2
2
k

log1/2 n

if Σ = σ2I
otherwise.

O

(cid:0)

We note that the sample complexity is nearly linear, and almost matches the complexity for

(cid:1)

mean estimation with no noise.

Remark 1.2. If we take m = O
samples, and assume that η < c/ log n for a
small enough constant c > 0, then by combining theorems 1.5 and 1.1, we can improve the η depen-
log1/2 n.
dence for the non-spherical Gaussian case in Theorem 1.1 to

µ
k2 = O
(cid:0)
Our next theorem is a similar result for much more general distributions.

1/2
Σ
2
k
k

µ
k

η3/4

−

(cid:17)

(cid:16)

(cid:1)

n2(log n+log 1/η) log n
η2

b

D

be a distribution on Rn with mean µ, covariance Σ, and
Theorem 1.3 (General mean). Let
bounded fourth moments (see Equation 1). There exists a poly(n, 1/ǫ)-time algorithm that takes
independent samples x 1, ..., x m ∼ Dη, and
as input a parameter η and m = O
µ
computes
(cid:16)
k
−
C 1/4
4
η1/2 + C 1/4
(cid:16)

µ
k2 is bounded as follows:
σ√log n
1/2
2
k

(η + ǫ)3/4
b
(η + ǫ)3/4

n(log n+log 1/ǫ) log n
ǫ2

µ such that the error

if Σ = σ2I

otherwise.

log1/2 n

Σ
k

O

O

(cid:17)

(cid:17)

b

4

(cid:16)

(cid:17)

The bounds above are nearly the best possible (up to a factor of O(√log n)) when the covariance

is a multiple of the identity.

Rn and covariance Σ.
Observation 1.4 (Lower Bounds). Let
Any algorithm that takes m (not necessarily O(poly(n))) samples x 1, ..., x m ∼ Dη, and computes a
µ
µ should have with constant probability the error
k

be a distribution with mean µ

−

D

∈

µ
k2 is
= N (µ, Σ)
b
has bounded fourth moments.

D

if

b

Ω(η
Ω(η3/4
p

Σ
k

k2)
k2)
Σ
k

if

D

p

3

−

−
D

, x and (x

be a distribution with mean µ and covariance Σ
D
µ)T have bounded fourth moments with constants C4
is an (unknown) aﬃne transformation of a 4-wise in-

Theorem 1.5 (Covariance Estimation). Let
and that (a) for x
µ)(x
∼ D
and C4,2(see Equation 1) respectively. (b)
dependent distribution. Then, there is an algorithm that takes as input m = O
samples x 1, ...x m ∼ Dη and η and computes in poly(n, 1/ǫ)-time a covariance estimate
η1/2 + C 1/4
(cid:16)

−
b
k · kF denotes the Frobenius norm.
= N (µ, Σ), then it satisﬁes the hypothesis of the above theorem. More generally, it holds
for any 8-wise independent distribution with bounded eighth moments and whose fourth moment
along any direction is at least (1 + c) times the square of the second moment for some c > 0. We
also note that if the distribution is isotropic, then covariance estimation is essentially a 1-d problem
and we get a better bound.

n2(log n+log 1/ǫ) log n
ǫ2
(cid:17)
Σ such that

k2 log1/2 n
Σ

4,2 (η + ǫ)3/4

kF = O

C 1/2
4 k

where

Σ
k

Σ

D

If

(cid:16)

(cid:17)

b

Theorem 1.6 (Agnostic 2-norm). Suppose
centration inequality: there exists a constant γ such that for every unit vector v

is a distribution which satisﬁes the following con-

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

≤

(cid:17)

Then, there is an algorithm that runs in poly(n, log 1

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)
independent samples x 1, ..., x m ∼ Dη, and computes

O

n3(log n/η)2 log n
η2

(cid:16)

(cid:17)

η ) time that takes as input η and m =

λmax such that

(1

O(η))

−

Σ
k

k2 ≤

1 + O(η log2/γ n/η)
(cid:17)

(cid:16)

b
k2.
Σ
k

λmax ≤
b

In independent work, [DKK+16] gave a similar algorithm, which they call a Gaussian ﬁltering
method, for agnostic mean estimation assuming a spherical covariance matrix; while their guar-
antees are speciﬁcally for Gaussians, the error term in their guarantee grows only with log(1/η)
rather than log n. They also give a completely diﬀerent algorithm based on the Ellipsoid method,
for a simple family of distributions including Gaussian and Bernoulli.

As a corollary of Theorem 1.5, we get a guarantee for agnostic SVD.

Theorem 1.7 (Agnostic SVD). Let
Let Σk be the best rank k approximation to Σ in
algorithm that takes as input η and m = poly(n) samples from
such that

is a distribution that satisﬁes the hypothesis of Theorem 1.5.
k · kF norm. There exists a polynomial time
Σk

Dη. It produces a rank k matrix

D

−

−

Σ

Σ

(cid:13)
(cid:13)
(cid:13)

Σk

F ≤ k

ΣkkF + O
Given the wide applicability of SVD to data, we expect the above theorem will have many ap-
plications. As an illustration, we derive a guarantee for agnostic Independent Component Analysis
(ICA). In standard ICA, input data points x are generated as As with a ﬁxed unknown n
n
full-rank matrix A and s generated from an unknown product distribution with non-Gaussian com-
ponents. The problem is to estimate the matrix A (the “basis”) from a polynomial number of
samples in polytime. There is a large literature of algorithms for this problem and its extensions

η log n

Σ
k

k2.

(cid:16)p

(cid:13)
(cid:13)
(cid:13)

×

(cid:17)

b

b

4

[FJK96, NR09, Car98, HKO01, CJ10, BRV13, AGMS12, BCV13, GVX14]. However, all these al-
gorithms rely on no noise or the noise being random (typically Gaussian) and require estimating
singular values to within 1/ poly(n) accuracy, and therefore unable to handle adversarial noise. On
the other hand, the algorithm from [VX15], which gives a sample complexity of ˜O(n), only requires
estimating singular values to within 1/ poly(log n). Our algorithm for agnostic SVD together with
the Recursive Fourier PCA algorithm of [VX15] results in an eﬃcient algorithm for agnostic ICA,
tolerating noise η = O(1/ logc n) for a ﬁxed constant c. To the best of our knowledge, this is the
ﬁrst polynomial-time algorithm that can handle more than an inverse poly(n) amount of noise.

4

∈

−

si|
|

η and be arbitrary with probability η, where A

Theorem 1.8 (Agnostic Standard ICA). Let x
probability 1
components of s are independent,
5
si|
E
|
|
η < ǫ/2, there is an algorithm that, with high probability, ﬁnds vectors
there exist signs ξi =
1 satisfying
poly(n, K, ∆, M, κ, 1
on real symmetric matrices of size n

Rn be given by a noisy ICA model x = As with
Rn×n has condition number κ, the
K√n almost surely, and for each i, Esi = 0, Es2
i = 1,
k ≤
M . Then for any ǫ < ∆3/(108M 2 log3 n), 1/(κ4 log n) and
such that
k2 for each column A(i) of A, using
A
k
ǫ ) samples. The running time is bounded by the time to compute ˜O(n) SVDs

b1, . . . , bn}
{

∆ and maxi E

3
| ≥

A(i)

(cid:13)
(cid:13)
n.

k
≤

ξibi

(cid:13)
(cid:13)

≤

−

−

±

∈

s

ǫ

×

Our results can also be used to estimate the mean and covariance of noisy Bernoulli product
distributions, i.e. distributions in which each coordinate i is 1 with probability pi and 0 with
probability 1
1−p . For a Bernoulli
. Then Theorem 1.3
1
2 , then
If C4 is constant, then by Theorem 1.5, we can get an

pi. In one dimension, C4 for a Bernoulli distribution is (1−p)2
+ p2
i
1−pi
i, pi = p and p

product distribution, C4 will be within a constant of maxi
can be applied to get an estimate
µ
k
estimate for the covariance.

µ for the mean. For instance, if

µ
k2 = O

p + p2

(1−pi)2
pi

≥

−

−

o

n

∀

.

η(1 + √ηp)p log n
b
(cid:1)

(cid:0)p

b

2 Main Ideas

Here we discuss the key ideas of the algorithms. The algorithm AgnosticMean (Algorithm 3)
alternates between an outlier removal step and projection onto the top n/2 principal components;
these steps are repeated.
It is inspired by the work of Brubaker [Bru09] who gave an agnostic
algorithm for learning a mixture of well-separated spherical Gaussians.

For illustration, let us assume for now that the underlying distribution is

= N (µ, σ2I ). We
SN be the points sampled from
are given a set S of m = poly(n) points from
the Gaussian and the adversary respectively. Let us also assume that
. We will use the
|
notation µT for mean of the points in a set T , and ΣT for covariance of the points in T . We then
have

Dη, and S = SG ∪

SN |
|

= η

S
|

D

ΣS = (1

η)σ2I + ηΣSN + η(1

−

η)(µS −

µN )(µS −

−

µN )T .

(2)

If the dimension is n = 1, then we can show that the median of S is an estimate for µ correct up to an
µN ),
additive error of O(ησ). Even if we just knew the direction of the mean shift µS −
µ = η(µG −
µS and then ﬁnding
then we can estimate µ by ﬁrst projecting the sample S on the line along µ
µ
the median. This would give an estimator
k2 = O(ησ). So we can focus on
µ. One would guess that the top principal component of the covariance
ﬁnding the direction of µS −
matrix of S would be a good candidate. But it is easy for the adversary to choose SN to make this
completely useless. Since the noise points SN can be anything, just two points from SN placed far

µ satisfying

µ
k

−

−

b

b

5

away on either side of the mean µ along a particular line passing through µ are suﬃcient to make
the variance in that direction blow up arbitrarily. But we can limit this eﬀect to some extent by
an outlier removal step. By a standard concentration inequality for Gaussians, we know that the
points in SG lie in a ball of radius O(σ√n) around the mean. So, if we can just ﬁnd a point inside
or close to the convex hull of the Gaussian and throw away all the points that lie outside a ball of
radius Cσ√n around this point, we preserve all the points in SG. This will also contain the eﬀect
of noise points on the variance since now they are restricted to be within O(σ√n) distance of µ.
We will see later that we can use coordinate-wise median as the center of the ball. By computing
the variance by projecting onto any direction, we can ﬁgure out σ2 up to a 1
O(η) factor. From
now on, we assume that all points in S lie within a ball of radius O(σ√n) centered at µ.

±

But even after this restriction, the top principal component may not contain any information
about the mean shift direction. By just placing (say) η/10 noise points along the e1 direction
σ√n, and all the remaining noise points perpendicular to this at a single point at a smaller
at
distance, we can make e1 the top principal component. But e1 is perpendicular to the mean shift
direction.

±

The idea to get around this is that even if the top principal component of ΣS may not be along
the mean-shift direction, the span (call it V ) of top n/2 principal components of ΣS will contain a big
projection of the mean-shift vector. This is because, if a big component of the the mean-shift vector
was in the span (say W ) of bottom n/2 principal components of ΣS, by Equation 2 this would mean
that there is a vector in W with a large Rayleigh quotient. This implies that the top n/2 eigenvalues
µN )T ,
of ΣS are all big. Since ΣS = (1
this is possible only if Tr(A) is large. But since the distance of each point in S from µ is O(σ√n),
the trace of A cannot be too large. Therefore, in the space W , we can just compute the sample
mean P W µS and it will be close to P W µ. We still have to ﬁnd the mean in the space V . But we
do this by recursing the above procedure in V . At the end we will be left with a one-dimensional
space, and then we can just ﬁnd the median. This recursive projection onto the top n/2 principal
components is done in Algorithm 3 .

η)σ2I + A, where A = ηΣSN + η(1

µN )(µS −

η)(µS −

−

−

This generalizes to the non-spherical Gaussians with a few modiﬁcations. We use a diﬀerent
outlier removal step. In the non-spherical case, it is not trivial to compute
k2 to be used as
Σ
k
the radius of the ball. We give an algorithm for this later on. To limit the eﬀect of noise, we use
a damping function. Instead of discarding points outside a certain radius, we damp every point
by a weight so that further away points get lower weights. This is done in OutlierDamping
(Algorithm 1). We get the guarantees of Theorem 1.1 by running AgnosticMean (Algorithm 3)
with the outlier removal routine being OutlierDamping. A detailed proof of the whole algorithm
is given in Section 3.1.

We then turn to more general distributions which have bounded fourth moments. We need
bounded fourth moments to ensure that the mean and covariance matrix of the distribution
do not
change much even after conditioning by an event that occurs with probability 1
η. One diﬃculty for
with bounded
general distributions is that the outlier damping doesn’t work. So for distributions
fourth moments, we have another outlier removal routine called OutlierTruncation(
, η). In this
·
routine, we ﬁrst ﬁnd a point analogous to the coordinate-wise median for the Gaussians, and then
η fraction of S. We throw away all the points outside
consider a ball big enough to contain 1
this ball. We get the guarantees of Theorem 1.3 by running AgnosticMean (Algorithm 3) with
the outlier removal routine being OutlierTruncation (Algorithm 2). The complete proof of this
appears in Section 3.3.

−

−

D

D

6

D

is given by ED(x

We now have an algorithm to estimate the mean of very general (with bounded fourth moments)
distributions. To estimate the covariance matrix, we observed that the covariance matrix of a
µ)T . If we knew what µ was, then covariance can be
distribution
computed by estimating the mean of the second moments. To compute the mean of the second
µ)T as a vector in n2 dimensions and run the algorithm for
moments, we can treat (x
mean estimation. Also, we can estimate µ by the same algorithm. Therefore, we get Theorem 1.5
by running CovarianceEstimation (Algorithm 4). Its proof appears in Section 4.2.
Algorithm AgnosticOperatorNorm (Algorithm 5) estimates the 2-norm

µ)(x

µ)(x

−

−

−

−

k2 for general
= N (µ, Σ), and we are given m = poly(n) samples

Σ
k

distributions. For illustration, suppose
x 1, ..., x m ∼ Dη, and the mean µ. We consider the covariance-like matrix

D

Σ(S, µ) =

(x i −

µ)(x i −

µ)T .

1
m

Xi

−

η fraction of the points in S are from the Gaussian, we have Σ(S, µ)

Since 1
η)Σ. Therefore,
(cid:23)
the top eigenvalue σ2 of Σ(S, µ) is at least (1
k2. Let v be the top eigenvector of Σ(S, µ). If
η factor) is much less than σ2, this
the Gaussian variance along v (which can be computed up to 1
should be because there are a lot of noise points in S whose projections onto v are big compared
to the projection of Gaussian points in S. We remove points in S that have big projection and
then iterate the entire procedure. We later show that this procedure terminates in poly(n) steps
and when it terminates the top eigenvalue of Σ(S, µ) is close to that of Σ. A proof of this appears
in Section 5.

Σ
η)
k

(1

−

±

−

Theorem 1.7 follows easily from Theorem 1.5. Let

Σk be the top-k eigenspace of

Σ from

Theorem 1.5. We then have

b

Σ

Σk

−

F
(cid:13)
(cid:13)
(cid:13)

b

(cid:13)
(cid:13)
(cid:13)

b
Σk

−

−

b
Σk

F
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
ΣkkF
(cid:13)

Σ

−

(a)

≤
(b)

≤
(c)

≤
(d)

Σ

Σ

−

F

(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
Σ
(cid:13)

b
Σ

−

Σ

b
−

+

Σ

+

(cid:13)
(cid:13)
(cid:13) b
Σ
(cid:13)
(cid:13)
(cid:13) b
+
k

F

(cid:13)
(cid:13)
b
ΣkkF + O
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)

≤ k

−

η log n

Σ
k

k2.

(cid:17)
(a), (c) follow from triangle inequality, (b) follows from the fact that
imation and (d) from the guarantees of Theorem 1.5.

(cid:16)p

Σk is the best rank-k approx-

Finally we outline the application to agnostic ICA. The algorithm from [VX15]. Proceeds by ﬁrst
estimating the mean and covariance, in order to make the underlying distribution isotropic. Here
we estimate the covariance matrix Σ by ˆΣ and use it to determine a new isotropic transformation
ˆΣ
k2, the
isotropic transformation results in a guarantee of

2 . Since our agnostic SVD algorithm gives a guarantee of

O(√ν log n)
Σ
k

kF ≤

Σ
k

˜Σ

− 1

−

b

− 1

2 Σ ˆΣ

− 1
2

ˆΣ
k

I

k2 ≤

−

O(

Σ
k2
η log n) k
Σ−1
k2
k

p

p

= O(

η log nκ2).

Next the algorithm estimates a weighted covariance matrix W with the weight of a point x pro-
portional to cos(u T x ) for u chosen from a Gaussian distribution; it computes the SVD of W . For

7

this we use our algorithm again (the weights are applied individually to each sample). The main
guarantee is that the eigenvectors of this weighted covariance approximate the columns of A. This
relies on the maximum eigenvalue gap of W being large, and it has to be approximated to within
additive error ǫ = O(1/(log n)3). Theorem 1.7 implies that the additional error in eigenvalues is
k2, and therefore it suﬃces to have √η log n < c/(log n)3 for a suﬃ-
bounded by O(√η log n)
Σ
k
ciently small constant c that depends only on the cumulant and moment bound assumptions (i.e.,
c(log n)−7.
∆, M ). Thus, if suﬃces to have η < ǫ/2

≤

2.1 Lower Bounds: Observation 1.4

In this section we will show the lower bounds stated in Observation 1.4. For Gaussian distributions,
this is a special case of a theorem proved in [CGR15]. We reproduce the relevant part here for
D2 = N (µ2, σ2I ) and
completeness. We will show that there are distributions
µ2k2 = Ω(ησ) and
distributions Q1, Q2 such that
D1 + ηQ1 = (1
η)
−
D2. Let φ1 be p.d.f of
D1,

µ1 −
k
Dη = (1
Dη, no algorithm can distinguish between

D1 = N (µ1, σ2I ),

D1 and φ2 be the

D2 + ηQ2.

(3)

η)

−

D2. Let µ1, µ2 be such that the total variation distance between

D1,

D2 is

So, given
p.d.f of

1
2

η

φ2|
By a standard inequality for the total variation distance of Gaussian distributions, this implies
µ2k2 ≥
φ1)1φ2≥φ1 and Q2 be the
that
distribution with p.d.f 1−η
φ2)1φ1≥φ2. It is now easy to verify that Equation 3 is satisﬁed.
This proves item one of Observation 1.4.

1−η . Let Q1 be the distribution with p.d.f 1−η

η (φ1 −

η (φ2 −

µ1 −
k

φ1 −
|

dx =

2ησ

−

Z

1

η

.

D1 is supported on two points
{−
1/4. It is easy to check that both

For the distributions with bounded fourth moments, consider the following two one-dimensional
distributions.
.
1/2, 1/2
}
{
D2 is supported on three points
respec-
η)/2, η
−
D2 have bounded fourth moments with
tively. Let η
≤
the constant C4 = 8. Furthermore,
D1 by adding η fraction of noise
D2 can be obtained from
points. So no algorithm can distinguish between the two distributions. Since their means diﬀer by
η3/4σ, no algorithm can get an estimate better than this.

with the corresponding probabilities
(1
{

{−
σ, σ, σ/η1/4
}

}
D1 and

with probabilities

η)/2, (1

σ, σ

−

}

We will now show that the geometric median:

arg min

y

x i −
k

y

k2

Xi
has a √n dependence on the dimension. We show this in the Gaussian case even if we have access
to the whole distribution, but with η fraction of noise points placed all at a single point far away
from most of the Gaussian points.

Proposition 2.1 (Geometric Median). Let
= N (0, Σ) be a distribution with diagonal covariance
matrix Σ whose variance along the coordinate direction e 1 is zero, and equal to 1 in all the other
coordinate directions. Assume there is an η fraction of noise at a distance a = n along e 1. Let

D

t0 = arg min

(1

η)Ex ∼D

t

−

t2 + x2

2 + ... + x2
n

+ η(a

t).

−

(cid:19)

(4)

Then, t = Ω(η√n).

(cid:18)q

8

Proof. We have that at the minimizer t0, the derivative with respect to t is zero. Therefore, we
should have

Ex ∼D

t0
0 + x2
t2
2 + ... + x2
n

=

1

η

−

.

η

Consider f (t) = Ex ∼D

t
√t2+x2
2+...+x2
n
t = αη√n for a small enough constant α, then f (t)
x
n/2 with exponential probability. Therefore,
k

2
2 ≥
k

≤

. It is clear from Equation 4 that t0 > 0. We claim that if
p

η
1−η . Suppose t1 = αη√n. Since x

,

∼ D

f (t1)

Ex ∼D

t1
t2
1 + n/2

≤

≤

t1√2π
p
t2
1 + n/2 ≤

αη√2π.

Our algorithms are based on outlier removal and SVD. To simplify the proofs, we use new samples
for each step of the algorithm. The total sample complexity is given in the theorems.

For outlier removal, we use one of the following two simple routines. The ﬁrst, which we call
OutlierDamping, returns a vector of positive weights, one for each sample point.

The claim, and hence the proof follows.

p

2.2 Algorithms

2.2.1 Outlier Removal

Algorithm 1: OutlierDamping(S)

Input: S
⊂
Output: S

Rn with

= m

S
|

|
Rn, w = (w1, ..., wm)

Rm

∈

⊂

1. if n = 1:

Return (S,

1).

−

3. Set wi = exp

4. Return (S, w ).

kx i−aaak2
2
s2

(cid:17)

−

(cid:16)

for every x i ∈

S.

9

The second procedure for outlier removal returns a subset of points. It will be convenient to

view this as a 0/1 weighting of the point set. We call this procedure OutlierTruncation.

2. Let aaa be the coordinate-wise median of S. Let s2 = C Tr(Σ). Estimate Tr(Σ) by esti-

mating 1d variance along n orthogonal directions, see Section 4.1.

Algorithm 2: OutlierTruncation(S, η)

Input: S
Output:

⊂
S

Rn, η

[0, 1]

∈
S, w = 1

Rm

∈

⊂

1. if n = 1:

e

←

∩

2. Let aaa be as in Lemma 3.15.
e

e

fraction of S.

←

∩

e

4.

S

S

B(r, aaa). Return (

S, 1).

2.2.2 Main Algorithm

e

Let [a, b] be the smallest interval containing (1
S

[a, b]. Return (

S, 1).

S

η

−

−

−

ǫ)(1

η) fraction of the points,

3. Let B(r, aaa) = ball of minimum radius r centered at aaa that contains (1

η

ǫ)(1

η)

−

−

−

We are now ready to state the main algorithm for agnostic mean estimation. It uses one of the
above outlier removal procedures and assumes that the output of the procedure is a weighting.

Algorithm 3: AgnosticMean(S)

Input: S
Output:

⊂
µ

Rn.

Rn, and a routine OutlierRemoval(
).
·

∈
S, w ) = OutlierRemoval(S) .

1. Let (
b

2. if n = 1:

e

(a) if w =

1, Return median(

S). //Gaussian case

−
(b) else Return mean(

S). //General case
e

3. Let Σ

eS,w be the weighted covariance matrix of
e

the top n/2 principal components of Σ

4. Set S1 := P V (S) where P V is the projection operation on to V .

5. Let

µV := AgnosticMean(S1) and

µW := mean(P W

S).

Rn be such that P V

µ =

µV and P W

µ =

µW .

e

b

b

b

b

b

µ
b

6. Let

∈
7. Return

b

µ.

b

10

S with weights w , and V be the span of

eS,w , and W be its complement.

e

2.2.3 Estimation of the Covariance Matrix and Operator Norm

For both the tasks in this section, we will assume that the mean of the distribution µ = 0. We
can do this without loss of generality by a standard trick mentioned described in Section 4.2. The
algorithm for estimating the covariance matrix calls AgnosticMean on x x T . Analysis is given in
Section 4.2.

1. Let S(2) =

x ′
{

ix ′
i = 1, ..., m/2
b
i|
}

(see Equation 15)

2. Run the mean estimation algorithm on S(2), where elements of S(2) are viewed as vectors

direction of top variance. The analysis is given in Section 5.

Σ
k

k2 is based on iteratively truncating the samples along the

Algorithm 4: CovarianceEstimation(S)

Input: S
⊂
Output: n

R
Rn, η
∈
n matrix

Σ

×

in Rn2

. Let the output be

Σ.

3. Return

Σ.

b

b
The algorithm for estimating

Algorithm 5: AgnosticOperatorNorm(S)

Input: S
⊂
Output: σ2

Rn, η

∈
R>0.

∈

[0, 1], γ

R

∈

1. Let

S = SafeOutlierTruncation(S, η, γ).

2. Do the following O(n log2/γ n

η ) times

e

3. Let Σ0(

S) := 1
| eS|

i∈ eS x x T .

4. Find v , the top eigenvector of Σ0(

e

S), and its corresponding eigenvalue σ2.

5. Estimate (up to 1

cη factor, see Section 4.1) the variance of

along v and denote it

e

D

P

±

by

σ2
v .

6. if σ2
b

≤
Return σ2.

(1 + c3η log2/γ n
η )

σ2
v

7. Remove all points x

S such that

x T v
|

|

>

c2

σv log1/γ n
η
b
2

.

8. Go to Step (3).

b

∈

e

11

Algorithm 6: SafeOutlierTruncation(S, η, γ)

Input: S
Output:

⊂
S

Rn, η
S

∈

[0, 1], γ

R

∈

⊂
1. Let t =

e

n
i=1

2. Let B(c√t log1/γ n

P

b

3.

S

S

←

∩

B(c√t log1/γ n

η , 0). Return

S.

e

e

σ2
ei be the sum of estimated variances of

D
η , 0) be the ball of radius c√t log1/γ n
η centered at 0.

in n orthogonal directions.

3 Mean Estimation: Theorem 1.1 and Theorem 1.3

b

µ
k

In this section, we will ﬁrst prove Theorem 1.1, which is for Gaussian distributions, and Theorem 1.3,
which is for distributions with bounded fourth moments. All our algorithms will be translationally
is µ = 0. So we will be
invariant. We will assume w.l.o.g that the mean of the distribution
k2. Algorithm 3 has log n levels, we will assume that at each level it uses
proving bounds on
O( n log n

) samples resulting in a total of m = O( n log2 n

ǫ2
ǫ2
At various points in the analysis, to bound the sample complexity we will have to show that
the estimates computed from samples are close to their expectations. We will use the following
two results. Firstly, as an immediate corollary of matrix Bernstien for rectangular matrices (see
Theorem 1.6 in [Tro12]), we get the following concentration result for the sample mean and sample
covariance.
Lemma 3.1. Consider a distribution in Rn with covariance matrix Σ and supported in some
(0, 1). Then the
Σ
Euclidean ball whose radius we denote is
k
following holds with probability at least 1

R. Let ǫ
then

, for some R
k
1/ poly(n): If N
p

∈
R log n
ǫ2

D

R

∈

).

≥

and

Here

µ and

Σ are sample mean and sample covariance matrix.

b

b

Secondly, the functions we estimate will be integrals of low-degree polynomials (degree d at
most 4) restricted to intervals and/or balls. These functions viewed as binary concepts have small
VC-dimension, O(nd) where n is the dimension of space and d is the degree of the polynomial. We
use this to bound the error of estimating integrals via samples, and we can make the error smaller
than any inverse polynomial using a poly(n) size sample.
Proposition 3.2. Let F be a class of real-valued functions from Rn to [
corresponding class of binary concepts, i.e., for each f
f (x)

R, R]. Let CF be the
F , we consider the concepts ht(x) = 1 if
F , and any

t and zero otherwise. Suppose the VC-dimension of CF is d. Then, for any f

−

∈

≥

∈

µ
k

µ

−

k ≤

ǫ

Σ
k

k

p

Σ

−

k ≤

ǫ

Σ
k

.
k

−

b
Σ
k

b

12

distribution
least 1

D
δ satisﬁes

−

over Rn, an iid sample S of size

8
ǫ2 (d log(1/ǫ) + log(1/δ)), with probability at

S
|

| ≥

1
S
|

−

| Xx∈S

2ǫR.

≤

f (x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ex∼D(f (x))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

Proof. By the VC theorem, for any concept in CF , the bound on the size of the sample ensures
that with probability at least 1

δ and any t,

Noting that Ex∼D(f (x)) =

t) dt, we get the claimed bound.

≥

Pr(f (x)
(cid:12)
(cid:12)
(cid:12)
R
−R Pr(f (x)
(cid:12)
R
Tr(Σ) and ǫ2 := kaaak2

≥

x

|{

∈

t)

−

t

≥

}|

S : f (x)
S
|

|

ǫ.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let s2 := 1
ǫ1

η2s2 . We can estimate Tr(Σ) by estimating (1 dimensional)
variances along n orthogonal directions, see Section 4.1. Note that we can arrange 0 < ǫ1, ǫ2 < 1
to be small enough constants. We weight every point x by wx = exp(
= N (0, Σ)
be a Gaussian distribution and S =
SN
be the Gaussian and the noise points repectively, with

−
, x i ∼ Dη be the sample we get. Let S = SG ∪

x 1, ..., x m}
{

D
Rn, let

= ηm. For a set T

kx −aaak2
s2

). Let

2

µT,w :=

wx ix i and ΣT,w :=

1
m

Xi∈T

SN |
|
wi(x i −

1
T
|

| Xi∈T

⊂
µT,w )T

µT,w )(x i −

We use the above notation for T = SG and T = SN . By an abuse of notation, when T = G, we
mean the population version of the above quantities:

µG,w := Ex wx x and ΣG,w := Ex wx (x

µG,w )(x i −

−

µG,w )T .

Note that

We consider the matrix ΣS,w

µS,w = (1

η)µSG,w + ηµSN ,w .

−

ΣS,w =

1
m

= (1

Xi
−

µS,w )(x i −

wx i(x i −
η)ΣSG,w + ηΣSN ,w + η(1

µS,w )T

η)(µSN ,w −

µSG,w )(µSN ,w −

−

µSG,w )T .

3.1 Proof of Theorem 1.1:

Let us assume η < 1/2.1. We then have

Lemma 3.3. Let
we are given x1, ..., xm ∼ Dη, then the median xmed = mediani{
with high probability.

D

xi}

= N (0, σ2) be a one dimensional Gaussian distribution. If m = O

xmed
|

|

satisﬁes

log n
ǫ2

, and
= O((η + ǫ)σ)

(cid:17)

(cid:16)

Proof. Let SG ⊂
Φ−1(1/2 + η + ǫ). Let us bound the probability that the median xmed ≥
xmed ≥
≥
SG|
if
|

S be made up of samples in S that come from the Gaussian, also let c =
c. We ﬁrst note that if
poly(n)

c, then Pr (x > c
x
|
log n
= O
ǫ2

ǫ. By Hoeﬀding’s inequality, we can bound this by 1

∈u SG)

−

.

(cid:16)

(cid:17)

13

We will next consider the multidimensional case. The proof follows by a series of lemmas. We
state the lemmas ﬁrst, conclude the proof of Theorem 1.1 and then prove the lemmas. First, we
observe that by applying Lemma 3.3 in n orthogonal directions and union bound, we get
Lemma 3.4. Suppose v 1, ..., v n ∈
i miv i. Then if m = O
and aaa =
v i’s such that with probability 1

Rn are a set of orthonormal vectors. Suppose mi = medianj{
v t
ix j}
,
, there exists a constant C independent of the choice of

log n
ǫ2
poly(n) ,
(cid:16)

(cid:17)

P

−

By a simple calculation, maxx

bound on the trace.

Lemma 3.5. Suppose A := ηΣSN ,w + η(1
a constant C such that,

−

O(s2). This immediately gives the following

µG,w )(µSN ,w −

µSG,w )T . Then there exists

Cη2 Tr(Σ).

2
aaa
2 ≤
k
k
2e−kx −aaak2/s2
k

x
k

≤

η)(µSN ,w −
Cηs2.

Tr(A)

≤

We will show later

Theorem 3.6.

e−η2ǫ2
1 + ǫ1 −

 

η2ǫ2e2ǫ1

Σ

ΣG,w

(cid:22)

(cid:22)

!

eǫ1Σ.

As will be clear from the proof of Theorem 3.6, when Σ = σ2I is a multiple of identity, then
) samples, we will have

ΣG,w will also be a multiple of I . By Lemma 3.1, if we take m = O( n log n

ǫ2

Suppose, we have

(1

ǫ)ΣG,w

ΣSG,w

(1 + ǫ)ΣG,w .

−

(cid:22)

(cid:22)

(cid:22)

(cid:22)

αΣ

ΣSG,w

βΣ

in the Lowener ordering, for some α, β > 0. By an argument similar to the one sketched in Section
2, we can prove

Lemma 3.7. We will use the notation as deﬁned above. Let W be the bottom n/2 principal
components of the covariance matrix ΣS,w . We have

≤
kmin denotes the least eigenvalue of Σ and δµ := µSN ,w −

Σ
2η ((β + Cη)
k

k2 −

Σ
α
k

kmin) ,

ηPW δµ
k

2
k

µSG,w .

where

Σ
k

By an inductive application of Lemma 3.7, we get the following theorem giving a bound on

µ
k
Theorem 3.8. On input S and the routine OutlierDamping(
), AgnosticMean outputs
·
b
satisfying

.
k
µ

2
µ
k
k

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

b

(cid:0)

b

(cid:1)

14

Theorem 3.6 combined with Theorem 3.8 proves Theorem 1.1. We get a better dependence on
η when Σ = σ2I because we can take α = β in this case. This would lead to the cancellation of
the leading term in the bound in Theorem 3.8 as

Σ
k

k2 =

Σ
k

kmin.

Proof of Lemma 3.7:
have

Recall that Σ denotes the covariance matrix of the Gaussian part. We

ΣS,w = (1
= (1

η)ΣSG,w + ηΣSN ,w + η(1
η)ΣSG,w + A,

−

−

−

η)δµδT
µ

where A = ηΣSN ,w + η(1

η)δµδT

µ. Therefore, we have

−

(1

η)αΣ + A

ΣS,w

(1

η)βΣ + A.

(cid:22)

(cid:22)

−

For a symmetric matrix B, let λk(B ) denote the k’th largest eigenvalue. By Weyl’s inequality,

−

−

we have

Therefore,

By Lemma 3.5 we have

λk((1

η)ΣG,w + A)

λk(A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (ΣS,w )

λn/2 (A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (A)

=

⇒

λn/2(ΣS,w )

Tr(A)
n/2
2C 2η

≤

≤

≤

≤

Σ
k2
k
k2 + 2C 2η
Σ
η)β
(1
k
(β + 2C 2η)
k2.
Σ
k

−

Σ
k

k2

Recall that W is the space spanned by the bottom n/2 eigenvectors of ΣS,w , and P W is the

matrix corresponding to the projection operator on to W . We therefore have

We therefore have

P T

W ΣS,w P W (cid:22)

(β + 2C 2η)
Σ
k

k2I .

αP T

W ΣP W + ηP T

W ΣSN ,w P W + (η

η2)(P W δµ)(P W δµ)T

−

(β + 2C 2η)
Σ
k

k2I .

(cid:22)

Multiplying by the vector P W δµ

kP W δµk and its transpose on either side, we get

Assuming η

1/2, we therefore have

≤

(η

η2)
P W δµ
k

2
k

−

(β + 2C 2η)
Σ
k

≤

k2 −

Σ
α
k

kmin.

ηP W δµ
k

2
k

≤

2η

(β + 2C 2η)
Σ
k

k2 −

Σ
α
k

kmin

.

(cid:0)

(cid:1)

15

Proof of Theorem 3.8:
have

By Equation 6 and Lemma 3.1, since we take O

samples we

n log n
ǫ2

(cid:16)

(cid:17)

µSG,w k
k

2
2 ≤

k2

= O
(cid:0)

η2ǫ2e2ǫ1 + ǫ2
Σ
k
η2 + ǫ2
Σ
k2.
(cid:1)
k
(βη + η2 + ǫ2)
Σ
k

(cid:1)

(cid:0)

(1 + log n) The proof
So it is enough to prove
is by induction. If n = 1, then the conclusion follows from the guarantees of the one dimensional
(cid:0)
median. Now, assume that it holds for all n
1. Let n = k + 1. We have by Lemma
≤
3.7

k for some k

µSG,w k

k2 −

kmin

Σ
k

µ
k

αη

≤

≥

−

O

b

(cid:1)

2

µSN ,w −

ηP W
k
P W µS,w −
(cid:0)

2
µSG,w
k
≤
2
P W µSG,w k
(cid:1)
2 ≤

⇒ k

=

O

O

(βη + η2)
Σ
k
(βη + η2)
Σ
k

k2 −
k2 −

αη

αη

Σ
k
Σ
k

kmin
kmin

(cid:0)

(cid:0)

.

(cid:1)

(cid:1)

By induction hypothesis, since dim(V ) = n/2, we have

µV −
k
Therefore, adding the two, we get

P V µSG,w k

≤

2

(cid:0)

b

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

µSG,w k

−

2

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

(cid:1)

(cid:1)

Proof of Theorem 3.6:

b

(cid:0)

We will ﬁrst consider the second moment

B := Ex exp

x
k

2
k

aaa
−
s2

x x T .

(cid:19)

−

(cid:18)

We have

B =

p

=

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

exp

x T Σ−1x

x x T dx

−

(cid:0)
x T Σ−1x

(cid:1)
x x T dx

=

p

1
(2π)n

p
exp

−

(cid:18)

Z

|

Σ
|
(x

−

−1

2
aaa
s2 +
k
k
1
s2

Σ−1 +

exp

 −

b)T

(cid:18)

1
s4 aaa T

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

I

(x

b)

x x T dx,

(cid:19)

−

(cid:19)

where b = 1
s2

Σ−1 + 1

s2 I

aaa. Therefore, we have

B = exp

(cid:0)

 −

2
aaa
s2 +
k
k

(cid:1)
1
s4 aaaT

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1
Σ−1 + 1

s2 I

Σ−1 +

−1

.

1
s2

I

(cid:19)

!

Σ
|
|

(cid:12)
(cid:12)

(cid:18)

(cid:12)
(cid:12)

16

Now we will look at the scalar term

. Let λi be the eigenvalues of Σ.

Σ
|

|

Σ−1 + 1

s2 I

Σ−1 +

Σ
|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
s2

I

(cid:12)
1
(cid:12)
λi
= Πi (cid:12)
(cid:12)
(cid:12)

+ 1
s2

1
λi

(cid:12)
(cid:12)
(cid:12)

= Πi

1 +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
s2

.

(cid:19)

(cid:18)

We then have

We next bound exp

kaaak2
s2 + 1

s4 aaaT

Σ−1 + 1

s2 I

−1

aaa

−

(cid:16)

Σ
1 + ǫ1 ≤ |

|

Σ−1 +

1
s2

I

(cid:12)
(cid:12)
(cid:12)
(cid:12)

eǫ1.

≤

(cid:12)
(cid:12)
(cid:12)
. We have
(cid:12)

(cid:0)
Σ−1 +

1
s4 aaaT

(cid:18)

1
s2

I

(cid:19)

(cid:17)

(cid:1)

−1

aaa

≤

1
s2 aaa T aaa.

Therefore

Therefore,

exp(

η2ǫ2)

−

exp

≤

 −

2
aaa
s2 +
k
k

1
s4 aaa T

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1.

! ≤

e−η2ǫ2

Σ−1 +

(cid:18)

−1

1
s2

I

(cid:19)

B

(cid:22)

(cid:22)

eǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

Lemma 3.9. We have the following

1
1 + ǫ1

Σ

(cid:22)

(cid:18)

Σ−1 +

−1

1
s2

I

(cid:19)

Σ

(cid:22)

and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors
s2 and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors

Proof. Note that if 1
of Σ−1, then 1
of Σ−1 + 1

λ1 + 1
s2 I . Since,

λ1 , ..., 1
λn
s2 , ..., 1
+ 1
λn

the lemma follows.

From Lemma 3.9, we have

Next we will bound

λi

1 + ǫ1 ≤

1
+ 1

s2 ≤

1
λi

λi

e−η2ǫ2
1 + ǫ1

Σ

B

(cid:22)

(cid:22)

eǫ1Σ.

µG,w = Ex wx x .

17

(5)

µG,w =

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

p

=

exp(

x T Σ−1x )x dx

−

x T Σ−1x

x dx

1
s4 aaaT

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

2
aaa
s2 +
k
k
1
s2

Σ−1 +

=

p

1
(2π)n

p
exp

Z

= exp

−

(cid:18)

 −

exp

 −

|

Σ
|
(x

−

b)T

2
aaa
s2 +
k
k

(cid:18)
1
s4 aaa T

Σ−1 +

(cid:18)

I

(x

b)

x dx

−
−1

(cid:19)

aaa

(cid:19)
1
s2

I

(cid:19)

!

Σ
|

|

1
Σ−1 + 1

s2 I

b,

where b = 1
s2
the two scalars by eǫ1. Therefore, we have

Σ−1 + 1

s2 I

aaa. Recall that ǫ1 = Pi λi
s2

−1

(cid:0)

(cid:1)

. We can, as before, bound the product of

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Therefore, we have

µG,w k
k

2

2 = e2ǫ1 1

s4 aaaT

µG,w k2 ≤
k

eǫ1

Σ−1 +

−1

1
s2

I

(cid:19)

aaa

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−1

1
s2

(cid:18)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−1

(cid:18)

aaa

(cid:19)
−1

Σ−1 +

1
s2

I

Σ−1 +

−1/2

aaa

1
s2

I

(cid:19)

(cid:19)

(cid:18)

Σ−1 +

(cid:18)

Σ−1 +

1
s2

1
s2

I

I

1
s2

I

(cid:18)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1/
Σ
k
k2.
Σ
k

I

1
s2

2
(cid:13)
(cid:13)
−1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
k2 + 1/s2

e2ǫ1 1

s2 aaaT
e2ǫ1 aaaT aaa
s2

η2ǫ2e2ǫ1

= η2ǫ2e2ǫ1

η2ǫ2e2ǫ1

≤

≤

≤

≤

Also, similarly

This implies

µG,w k
k

2
Σ

−1+ 1

s2 I ≤

η2ǫ2e2ǫ1.

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1Σ.

From Lemma 3.9, we have

Combining Equation (6) and Equation 5, we get Theorem 3.6.

18

(6)

3.2

Improving the dependence on η

Now we will show how we can obtain the second part of Theorem 1.1 to get a better dependence
= N (µ, Σ) be a Gaussian with covariance Σ, and
on η by using
c/ log n for a small enough constant c > 0. We ﬁrst use Theorem 1.5 (with ǫ = η) to estimate
η
≤
σ2 =

Σ from Theorem 1.5. Let

σ2 satisfying

D

Σ
k

b
k2. We get a

σ2

σ2

1

O(

η log n)

−

b
(cid:17)
(cid:16)
be the given sample, and let y i ∼
x 1, ..., x m}
Let S =
{
Deﬁne x ′
i = x i + y i. The key thing to note is that if x
′ = N (µ, Σ +
x + y
N (µ, Σ +
σ2I has

D
, and the covariance Σ′ = Σ +

σ2I ). Let

p

≤

≤

∼

(cid:16)

p
N (0,

(cid:17)

b

σ2I ), i = 1, ..., m be i.i.d. samples.
σ2I ), then
N (µ, Σ) and y
∼
∼
′ is same as that of
σ2I ). Note that the mean µ′ of
b
b

N (0,

D

1 + O(

η log n)

σ2.

(7)

D

b
Σ′

λmax

2 + O(

η log n)

b

≤

b
σ2 and λmin

Σ′

We can view x ′
Σ

(cid:16)
(cid:1)
′
η, and we assume η log n
i ∼ D
such that

p

(cid:17)

(cid:0)

′

compute a

≤

1

O(

η log n)

σ2.

(8)

≥

−

(cid:16)

(cid:0)

(cid:1)

p

(cid:17)

c. By Theorem 1.5 and Equation 7, we can

Let α = O

. Therefore,

b
√η log n

′
Σ

Σ′

−

F ≤

(cid:13)
(cid:13)
(cid:13) b

(cid:13)
(cid:13)
(cid:13)

O

η log n

σ2.

(cid:16)p

(cid:17)

(cid:0)

(cid:1)
ασ2I

I

−
1

′

Σ

−

⇒

=
b
=

O

Σ′

(cid:22)
ασ2

Σ

(cid:22)
′−1

′
Σ

+ ασ2I
′−1/2

Σ

b
(cid:22)
η log n
b

′−1/2

Σ′

Σ

′−1/2

I

Σ
b

I + ασ2
′−1/2

(cid:22)
Σ′

Σ

′−1

Σ

(cid:16)

−

⇒

b
(cid:16)p
by Equation 8. Now, if we let x ′′
i =
′′
then we can think of x ′′
η . If we now use Theorem 3.8 with β =
b
µ′′ such that
α =

i ∼ D
on the samples S′′ =

′′ = N (µ′′, Σ′′) = N

√η log n

b
i and

b
(cid:22)
(cid:16)

(cid:17)(cid:17)
′−1/2

(cid:16)p
Σ

x ′

Σ

(cid:22)

D

O

(cid:16)

b

b

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

, we get a

x ′′
i }
{
2 = O(η3/2 log3/2 n).
k

b

µ′′
k

−

µ′′

This implies that

µ =

Σ

′1/2

µ′′ satisﬁes
b

b

b

µ
b
k

−

′

2 = O(
µ
Σ
k
k
Σ
= O(
k
b

η3/2 log3/2 n)
k
k2η3/2 log3/2 n).

b

1 + O

η log n

I

(cid:17)(cid:17)
Σ

µ,

′−1/2

′−1/2

′−1/2

Σ

Σ

1 + O
b
(cid:0)

(cid:0)

√η log n
b

(cid:1)(cid:1)

,

and
(cid:17)

µ with
Remark 3.10. We can use this technique to give a polynomial time algorithm to compute
for any ﬁxed ǫ > 0. This would require estimating
a guarantee
higher order moments by the mean estimation algorithm and then using the above trick to improve
(cid:1)
the η dependence for each of them in sequence. We don’t give a proof of this in this paper.

k2η2−ǫ log2−ǫ n

2 = O
µ
k

Σ
k

µ
k

−

b

(cid:0)

b

19

3.3 Distributions with Bounded Fourth Moments

In this section, we will prove some some useful properties that distributions with bounded fourth
moments satisfy. We will assume that x
for a distribution with mean µ that has bounded
fourth moments, i.e., for every unit vector v

∼ D

E((x

µ)T v )4

C4

E((x

−

≤

−

µ)T v )2

2

,

(9)

for some C4.

Lemma 3.11 (Mean shift). Let X be a random variable with E(X

EX)2 = σ2 and

E(X

EX)4

−

≤

C4

E(X

−

EX)2

2

,

for some C4. Let ǫ

0.5 and A be any event with probability Pr(A) = 1

ǫ. Then

≤

(cid:1)

−

(cid:1)

−

(cid:0)

(cid:0)

Proof. Let a = E(X

A). Then
|

E(X
|

A)
|

−

E(X)

| ≤

4

8C4ǫ3σ.

p

EX = (1

ǫ)a + ǫE(X

−
EX

ǫ)a

−

−

(1
ǫ

Ac)
|
1
=

ǫ

−
ǫ

E(X

Ac) =
|

⇐⇒

(EX

a) + EX

−

The fourth moment of such an X is minimum when its support is just the two-point set
a) + EX

a, 1−ǫ
{

ǫ (EX

−

. Therefore,
}

(1

ǫ)(a

−

−

EX)4 + ǫ

(EX

a)

−

4

≤

(cid:19)

C4σ4

1

ǫ

−
ǫ

C4ǫ3

(cid:18)

=

a

EX

⇒ |

−

4

| ≤

(1

s

−

σ

4

8C4ǫ3σ,

ǫ)(3ǫ2

3ǫ + 1)

≤

−

p

when ǫ

0.5.

≤

Lemma 3.12. Let X be a random variable with EX = µ and E((X

µ)2) = σ2 and let

E(X

µ)4

C4σ2,

−

≤

−

−

for some C4. Then, for every event A that occurs with probability at least 1

ǫ, we have

where 1A is the indicator function of the event A. As an immediate corollary, for ǫ
the following bound on the conditional probability

≤

0.5 we get

E

(X

µ)21A

−

(cid:0)

1

−

≥

(cid:16)

(cid:1)

C4ǫ

σ2,

p

(cid:17)

(10)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

20

Proof. Let dΩ be the probability measure. We can write E(X
following way

−

µ)4

≤

C4(E(X

µ)2)2 in the

−

2

(cid:19)

2

(X

µ)4dΩ +

(X

µ)4dΩ

(X

µ)2dΩ +

(X

µ)2dΩ

ZAc

−

C4

≤

(cid:18)ZA

−

ZAc

−

Using E(Y

EY )4

(E(Y

EY )2)2 for any random variable Y, and Pr(Ac) = ǫ we have

ZA

−

−

≥

−

1
ǫ

2

(X

µ)2dΩ

−

(cid:18)ZAc

≤

ZAc

(cid:19)

(X

µ)4dΩ

−

We therefore have

(cid:0)R

⇐⇒

Ac(X

−
ǫ

µ)2dΩ

2

(cid:1)
µ)2dΩ

(X

−

ZAc
(X

µ)2dΩ

−

(cid:19)
µ)2

E(X

−

≤

≤

+

p
1

(cid:16)

≤

ZA

⇐⇒

C4ǫ

1

−

(cid:16)

p

(cid:17) (cid:18)ZAc
C4ǫ

1

−

(cid:16)

p

(cid:17)

⇐⇒

This proves the inequality (10). Now,

C4

(X

µ)2dΩ +

(X

µ)2dΩ

(cid:18)ZA
C4ǫ

−

(X

−

ZAc
µ)2dΩ +

−

(X

−

(cid:19)
µ)2dΩ

ZAc
µ)2dΩ

(X

−

(cid:19)

−

(X

µ)2dΩ

≤

(cid:19)

ZA

(cid:18)ZA
C4ǫ

−

(X

p

−

(cid:17) (cid:18)ZA

µ)2dΩ

Also,

Therefore, for ǫ

0.5 we get that

≤

E

(X

µ)2

A
|

−

1
µ(A)

1

−

=

≥

(cid:1)

(X

µ)2dΩ

ZA

C4ǫ

−
σ2.

(cid:16)

p

(cid:17)

E

(X

µ)2

A
|

−

=

(X

µ)2dΩ

−

1
µ(A)
1

ZA
σ2.

≤

1

ǫ

−

(cid:1)

(cid:0)

(cid:0)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

As an immediate corollary of Lemma 3.11 and Lemma 3.12, we get for a random variable x

having bounded fourth moments

Corollary 3.13. Let A be an event that happens with probability 1

η. Then,

where Σ

(cid:16)
|A is the conditional covariance matrix Σ

p

(cid:17)

O(

C4η)

Σ

1

−

−

(1 + 2η)Σ,

Σ

|A (cid:22)
(cid:22)
|A := E(x x T

21

A)
|

−

(E(x

A))(E(x
|

A))T .
|

Proof. Let v be any unit vector. Let y be the random variable that is v T x for x
µ. Then
µ = E(y), µA = E(y

. Let

∼ D

A), and d = µA −
|
µA)2
µ
A) = E((y
|

−

E((y

−

By Lemma 3.11 and Lemma 3.12,

d)2

−

A) = E((y
|

= E((y

µ)2
µ)2

A)
|
A)
|

−

−

−

−

2dE(y
d2

µ

A) + d2
|

−

E((y

E((y

µA)2
µA)2

A)
|
A)
|

−

−

−

−

E((y

E((y

−

−

µ)2)
µ)2)

≤

2ηE((y

µ)2)

−
C4ηE((y

≥ −

≥ −

p

C4η +

d2

−

µ)2)

−
8C4η3

E((y

µ)2)

−

(cid:16)p

p

(cid:17)

Finally, by a standard argument as in the proof of Chebyshev’s inequality, we have

Lemma 3.14 (Concentration). For every unit vector v , we have

C4
t4 ,
where σv is the standard deviation of x along the direction v , σ2

x T v
|
(cid:0)

Ex T v

tσv

| ≥

Pr

≤

−

(cid:1)

x T v
v := E
|

2
|

− |

Ex T v

2.
|

3.4 Proof of Theorem 1.3:

3.4.1 One Dimensional Distribution

First we will consider the case when X is a random variable with mean µ and variance σ2 satisfying

E((X

µ)4)

C4σ4.

−

≤

In this case, median need not be a good estimator. Instead, we will consider the interval of minimum
η) fraction of the sample points. Let S be the given sample,
length that contains (1
and let
S) be our estimator. We will show
below that
e

−
S be the points lying in this interval. Let

By the concentration inequality stated in Lemma 3.14, we get that for the distribution, the

(η + ǫ)3/4σ

µ = mean(

ǫ)(1

µ
|

| ≤

−

−

−

O

µ

e

b

η

.

C 1/4
4
(cid:16)

(cid:17)

of the interval around µ consisting of probability mass 1

is bounded by

length r1− η+ǫ
b
2

η+ǫ
2

−

We will refer to this interval by I1− η+ǫ
then with probability 1

2

1/ poly(n) for every interval I

. We note that by VC theorem if

SD|
|

= Ω

log n+log 1/ǫ
ǫ2

,

(cid:16)

(cid:17)

−

Pr (x
∈
|
The length of the smallest interval that contains (1
length of the smallest interval that contains 1

Pr (x

∼ D

−

∈

x

η

I

)

|

|
η

η) fraction of S is at most the
ǫ fraction of SD. This latter quantity is bounded

ǫ)(1

−

−

−

R,

⊂
x ǫu SD)

I

ǫ/2.

| ≤

r1− η+ǫ

2 ≤

C 1/4
4
η+ǫ
2

σ.

1/4

(cid:0)

(cid:1)

−

−

22

by r1−η, since the interval I1− η+ǫ
SD.

2

contains with probability 1

1/ poly(n) a (1

η

ǫ) fraction of

−

−

−

This implies that when we look at the minimum interval containing 1

η

noise points, the extreme points of the interval can be at most at a distance r1− η+ǫ

−

−

ǫ fraction of the non-
from µ. Thus,

2

the distance of all noise points will be within O

. Furthermore, the interval of minimum

η

length with (1
by Lemma 3.11 the mean of
from the true mean.

ǫ)(1

−

−

−

e

η) fraction of S will contain at least 1

S will be within η

r1−η + O

3η
C4(η + ǫ)3σ

−

−

4

ǫ fraction of SD. Therefore,
C 1/4
4
(cid:16)

(η + ǫ)3/4σ

= O

(cid:17)

(cid:17)

(cid:16)

p

1/4
C
4
(η+ǫ)1/4 σ

(cid:19)

(cid:18)

·

3.4.2 Multi-dimensional Case

We will now consider the multidimensional case. Let
random variable that satisﬁes for every direction v

D

be a distribution on Rn and x

is a

∼ D

E(((x

µ)T v )4)

C4

E(((x

−

≤

−

µ)T v )2)

2

,

for some C4.

(cid:0)
For any direction v , let µv = µT v. From the previous section, we know that we can ﬁnd a

(cid:1)

µv

such that

Therefore, by picking n orthogonal directions v 1, ..., v n, we get

µv
|

µv| ≤

−

O(C 1/4
4

(η + ǫ)3/4σv ).

Lemma 3.15. Given O
O(C 1/4
4

(η + ǫ)3/4

(cid:16)
Tr(Σ)).

n log n
ǫ2

b

(cid:17)

samples, we can ﬁnd a vector aaa

Rn such that

∈

aaa
k

µ

k2 =

−

We will now bound the radius of the ball in the outlier removal step (Algorithm 2). We claim

p

the radius of the ball is O

. Suppose we have some x

. Let z = x

||
Using the n orthogonal directions as picked above, let zi = z T v i and let Z 2 =
Consider the following:

∼ D

p

(cid:18)

(cid:19)

1/4
C
4
(η+ǫ)1/4

n

Σ

||2

z2
i =

P

Pr

Z 2

 

≥

C 1/2
Σ
4 n
||
(η + ǫ)1/2 !

||2

= Pr

Z 4

C4n2

Σ
||
η + ǫ

2
2
||

≥

(η + ǫ)E(Z 4)
C4n2

Σ

2
2
||

||

≤

(cid:19)

It suﬃces to bound the right-hand side of (11) by O(η + ǫ), in which case the ball will contain
1

ǫ fraction of the probability mass of

. We have

η

−

−

E(Z 4) = E

z2
i



Xi

Xj

z2
j 

≤

n2 max
i

E

z4
i

C4n2

Σ
k

2
2
k

≤

(cid:0)

(cid:1)


due to the fourth moment condition and the fact that E((z T v i)2)



1/4
C
4
(η+ǫ)1/4

Tr(Σ)

(cid:18)

n

k2. Therefore, a ball of
aaa
k2 =
k
p
, we get that the radius of the ball computed in the outlier removal step

≤ k
ǫ fraction of the points. Since

contains 1

||2

Σ

Σ

−

−

−

(cid:19)

µ

||

η

radius at most O

O

C 1/4
4
(cid:16)
is O

(η + ǫ)3/4
1/4
C
4
(η+ǫ)1/4

n

(cid:18)

p
Σ
k

k2

(cid:19)

p

(cid:17)

. We have proved

(cid:18)

D

23

b

µ.

2
2.
k

−
z

k

(11)

(12)

Lemma 3.16. After the outlier removal step, every remaining point x satisﬁes

x
k

−

µ

k2 ≤

O

 

C 1/4
4
(η + ǫ)1/4

n

Σ
k

k2

.

!

p

Consider the covariance matrix Σ

S (recall that

S be the set of points in

SD ⊂
points sampled by the adversary. Let µ
e
Note that

S that were sampled from the distribution
e
S), µ
SN
e

S := mean(
e

e

e

e

S is the sample after outlier removal). Let
S be the
SD).

and
D
SN ) and µ

:= mean(

SN ⊂
SD := mean(
e
e

e

eS of

µ

S =
e

ηµ

e
+ (1
−

η)µ

SD ,
e

SN
e

e

η = | eSN |
| eS|

is the fraction of noise points after the outlier truncation step. Note that
e

where
≤
η
1−2η−ǫ = O(η). We will therefore pretend that the fraction of noise points is still η after the outlier
is µ = 0. By Lemma 3.11
truncation step. We again assume that the mean of the distribution
applied with X = x T µ eD
and where A is the event that x is not removed by outlier
removal, we have that

kµ eDk for x

∼ D

D

e

e

e

η

e

(13)

Suppose, after the outlier removal step, we had the guarantee that the covariance matrix of the

remaining points from the distribution

, say Σ

D

µ
k

eDk2 = O(C 1/4

4

k2).

(η + ǫ)3/4

Σ
k
p
eD, is between
β(1
Σ

η)Σ

α(1

η)Σ

−

(cid:22)

eD (cid:22)

−

in the Lowener ordering. Corollary 3.13 gives α = 1

by Lemma 3.1 and Lemma 3.16 we have that if

O(

−
= Ω

C4(η + ǫ)) and β = 1 + O(η + ǫ). Also,
n log n
ǫ2

, then

S
|

eD|

p
(cid:16)

ǫ

C 1/4
4
(η + ǫ)1/4

(1

−

)Σ

eD (cid:22)

ΣS eD (cid:22)

(1 +

(cid:17)
C 1/4
4
(η + ǫ)1/4

ǫ

)Σ

eD

We will use the notation as deﬁned above.

Lemma 3.17. Let W be the bottom n/2 principal components of the covariance matrix ΣS. For
some constant C, we have

where δµ = µ

µ

eSN −

ηP W δµ
k
eSD .

2
k

≤

O

(βη + C 1/2
(cid:16)

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

,

(cid:17)

By an inductive application of the above lemma, we can prove

Theorem 3.18. On input (S, n), AgnosticMean outputs

µ satisfying

µ
k

2
k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
b
k

kmin

(1 + log n).

(cid:17)

(cid:16)

b

24

Theorem 3.18 with Corollary 3.13 proves Theorem 1.3.

Proof of Lemma 3.17:
have

Recall that Σ denotes the covariance matrix of the points from

. We

D

Σ

eS = (1
= (1

η)Σ

η)Σ

eSD + ηΣ
eSD + A,

−

−

+ (η

−

eSN

η2)δµδT
µ

where A := ηΣ

+ (η

SN
e

−

η2)δµδT

µ. Therefore, we have

(1

η)αΣ + A

−

(1

η)βΣ + A.

−

Σ

(cid:22)

eS (cid:22)
1/4
C
4
(η+ǫ)1/4

By Lemma 3.16 each x i satisﬁes

x ik
k

= O

(cid:18)
η√C4k
Σ
√η + ǫ

k2n

, so we have

n

Σ
k

k2

(cid:19)

p

O

≤

C4η

Σ
k

k2n

.

Tr(A) = O

(cid:19)
For a symmetric matrix B, let λk(B) denote the k’th largest eigenvalue. By Weyl’s inequality,

(cid:16)p

(cid:18)

(cid:17)

(14)

λk((1

η)Σ

S + A)
e

≤

−

λk(A) + (1

η)β

Σ
k

k2.

−

≤
By Equation (14), there exists a constant

(cid:0)

(cid:1)

C such that

λn/2

Σ
S
e

λn/2 (A) + (1

η)β

Σ
k

k2.

−

we have that

Therefore,

we have,

e
λn/2 (A)

Tr(A)
n/2

≤

≤

C

C4η

Σ
k

k2,

p

e

eS)
Recall that W is the space spanned by the bottom n/2 eigenvectors of Σ
corresponding to the projection operator on to W . We therefore have

λn/2(Σ

k2 +

Σ
k

Σ
k

C4η

η)β

k2

p

(1

−

≤

C

e

eS, and P W is the matrix

(1

η)αP T

W ΣP W + ηP T

W Σ

−
Multiplying by the vector P W δµ

eSN

P T

W Σ

eSP W (cid:22)
P W +(η

((1

−

η)β +

Σ
C4η)
C
k
η2)(P W δµ)(P W δµ)T
e

p

k2I
(cid:22)

−

kP W δµk and its transpose on either side, we get

η

P W δµ
k

2
k

≤

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin.

((1

η)β +

C

−

C4η)
Σ
k

k2I

p

e

where C = eC

1−η . We therefore have

ηP W δµ
k

2
k

≤

η

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin

.

(cid:16)

(cid:17)

p

p
25

2. The proof is by
Proof of Theorem 3.18:
By Equation 13, it is enough to bound
induction on the dimension. If n = 1, then the conclusion follows from the guarantees for the one
1.
dimensional case proven in Section 3.4.1. Now, assume that it holds for all n
Let n = k + 1. We have by Lemma 3.17

k for some k

µ
k

eSD k

≤

−

≥

µ

b

P W
k

µ

µ

eSD

−

2 =
k

(cid:16)

b

(cid:17)

ηP W δµ
k
O

2
k
(βη + C 1/2
(cid:16)

≤

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(cid:17)

Recall that we deﬁned V to be the span of the top n/2 principal components of Σ

S. By
e

induction hypothesis, since dim(V ) = n/2, we have

µV −
k

P V µ

2
eSD k

≤

O

(βη + C 1/2
(cid:16)

4

Therefore, adding the two, we get

b

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

−

µ

2
eSD k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

(cid:17)

(cid:17)

b

(cid:16)

4 Covariance Estimation

4.1 One Dimensional Case

Observation 4.1 (1d Covariance Estimate).

1. Let

be a distribution with mean µ and co-

D
= N (µ, σ2), then there is an algorithm that takes as input m = Ø

log n
ǫ2

variance σ2. If
samples x 1, ...x m ∼ Dη and computes in polynomial time

D

σ2 such that

σ2

σ2

= O(η +ǫ)σ2.
(cid:16)
(cid:17)

−

(cid:12)
(cid:12)

b

(cid:17)

−

−

σ2

(cid:16)
σ2

(cid:12)
(cid:12)b

= O

∼ D

2. If x

log n+log 1/ǫ
ǫ2

has bounded fourth moments with constant C4, and (x

µ)2 has bounded fourth
(cid:12)
(cid:12)b
moments with constant C4,2. Then there is an algorithm that takes as input η and m =
σ2 such that
samples x 1, ...x m ∼ Dη and computes in polynomial time
O
4,2 (η + ǫ)3/4C 1/2
C 1/4
4 σ
(cid:16)

= N (µ, σ), and we are given m = poly(n) samples S =

(cid:12)
is supported on R, we can estimate the variance in the following
Proof. When the distribution
(cid:12)
just having bounded eighth moments separately.
= N (µ, σ) and
way. We will consider the case
Suppose
, xi ∼ Dη. There are
x1, ..., xm}
{
D
several ways to estimate σ, we describe here one of them. First we compute the median, and let
85.1. Let Cσ be the
xmed = mediani{
µ. By Lemma 3.3,
c1’th quantile of S. Then our estimate for the standard deviation is
O(C 1/4
8 ησ2).
µ
= O(ησ). For a similar reason, Cσ = σ
we have
±
is a distribution that has bounded eighth moments, the result follows from the 1d

. Let Φ(x) be the c.d.f. of N (0, 1). Note that c1 = Φ(1)

∼
σ = Cσ −

O(ησ). Therefore,

xi}

D
D

±

D

(cid:17)

b

|

.

mean estimation in Section 3.4 applied (x

µ)2. Note that E(x

σ2 = σ2
b
µ)2 = σ2 and

c

b

µ
|
When
b

−
D

E

(x

µ)2

2

σ2

= E(x

µ)4

−

−

−

(cid:0)

−
σ4

−

−
C4σ4.

(cid:1)

≤

26

From Section 3.4, we therefore have that if m = O
C 1/4
4,2 (η + ǫ)3/4C 1/2
4 σ
(cid:16)

σ2
|

| ≤

σ2

−

O

(cid:17)

.

(cid:16)

b
4.2 Multi-Dimensional Case: Theorem 1.5

log n+log 1/ǫ
ǫ2

(cid:17)

, there is a poly(n) algorithm with

In this section we will prove that CovarianceEstimation (Algorithm 4) gives Theorem 1.5.
is a distribution with mean µ and covariance Σ
Throughout this section, we will assume that
and has bounded fourth moments with parameter C4. We use the following symmetrization trick
to assume that

has mean 0. Given samples S =

, let

D

x 1, ..., x m}
{

D

x ′

i =

x i −

x i+m/2
√2

for i

.
1, ..., m/2
}

∈ {

(15)

Since η fraction of the original samples were corrupted on average, only 2η fraction of the new
samples will be corrupted on average. Moreover, if x , y
are independent random variables,
then we can show that the distribution of x ′ = (x
y)/√2 has bounded fourth moments with
′ the distribution of x ′. CovarianceEstimation
parameter
is just the mean estimation algorithm on S(2) =
, we can appeal to Theorem 1.3.
}
Furthermore, let
Note that

′ be an aﬃne transformation of a 4-wise independent distribution.

C4 + 3/2. We will denote by

x ′x ′T
{

∼ D

x
|

−

≤

D

D

∈

S

Ex ∼D′x x T = Σ.

By Theorem 1.3, we have

Σ
k

Σ

kF = O

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

Σ(2)
k

1/2
2
k

log1/2 n,

b
where Σ(2) is covariance matrix of x x T , x

By Proposition 4.2 , we have

′.

∼ D

Σ
k

kF = O
Σ

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

C 1/2
4 k

Σ

k2 log1/2 n,

which proves Theorem 1.5.

b

(cid:17)

(cid:17)

We will now derive a bound for

Σ(2)
k

k2 when the distribution has bounded fourth moments

and is 4-wise independent. In particular, we will prove

Proposition 4.2. If Σ(2) is the covariance matrix of x x T , x

′, it holds that

∼ D

Σ(2)
k

k2 ≤

O

Σ
C4k

2
2
k

.

(cid:0)

(cid:1)

27

Proof of Proposition 4.2:

Note that E(Y ) = Σ.

E(((Y

E(Y ))

V )2) = E

−

·

2

(Y ij −



Xij

E((Y ij −

Σij)Vij

Σij)(Y kl −

Σkl))VijVkl

E(Y ijY kl −

ΣijΣkl)VijVkl

E(xixjxkxl −

ΣijΣkl)VijVkl.

=

=

=

Xijkl

Xijkl

Xijkl



Next we note that

Therefore,

E(xixjxkxl)

−

ΣijΣkl = 


Σ2

ii if i = j = k = l

E(x4
i )
−
E(x2
i x2
j ) if i = k, j = l or i = l, j = k
0 otherwise.

max
V :kV kF =1

E(((Y

E(Y ))

V )2) = max

−

·

V :kV kF =1

(E(x4
i )

Σ2

ii)V 2

ii + 2

ΣiiΣjjV 2
ij

(E(x4
i )

2Σ2

ii)V 2

ii +

ΣiiΣjjV 2
ij

−

−

Xi<j

Xi,j

2Σ2

ii + max

Σ2
ii.

i

Xi

= max

V :kV kF =1

max
i
O (C4)

Xi
E(x4
i )
−
2
2.
k

Σ
k

≤

≤

5 Estimating

k2: Theorem 1.6
Σ

k

As in Section 4.2, we assume that the true distribution has mean µ = 0.

SN be the given sample, where SD consists of points from some distribution

In this section, we will prove AgnosticOperatorNorm (Algorithm 5) gives Theorem 1.6. Let
with mean
S = SD ∪
µ and covariance Σ and SN consists of points picked by the adversary. Let ΣSD be the sample
has 1D concentration, i.e., there exists a constant γ such that
covariance of SD. We assume that
for every unit vector v

D

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

(cid:17)

S be the remaining sample at the end of the algorithm and let

SD be points in

S sampled from

5.1 Correctness

Let
.

D

e

e

e

28

Deﬁnition 5.1. Given a set of points S

Rn and a vector aaa

Rn, we let

Σaaa (S) :=

(x

aaa)(x

−

−

⊂

1
S
|

x ∈S
| X

∈

aaa)T .

First, we will argue that the covariance of the true distribution is well-approximated by Σµ(

SD).

Lemma 5.2. With probability 1

1/ poly(n),

−

e

Σ
k

−

Σ0(

SD)

η

Σ
k

k

k ≤

Proof. First, note that the t computed in SafeOutlierTruncation is at most O(Tr(Σ)) because
e
σ2
v (namely that the estimated
by an analogous argument as in Section 4.1, we have
v ≤
σv in a direction v is close to the true variance σv in that direction). Then the ball in
variance
SafeOutlierTruncation has radius R = c1
Tr(Σ) log1/γ n
η for some constant c1. We have that
b
deviates from the mean by more than c1σv log1/γ n
b
in any direction v , the probability that x
p
η
∼ D
is 1/ poly( n
η ). Then if we take n orthogonal directions, the probability that any given point is
more than distance R from µ is still 1/ poly( n
η ). Thus, step (1) of the algorithm will remove only
1/ poly( n
η ) fraction of the points sampled from

(1 + O(η))σ2

.

In every direction v , the probability mass of points from

outside an interval of size c2σv log1/γ n
η
around the mean is at most 1/ poly( n
η ), where σv is the variance in the direction v . Let Ci be the
region between the two hyperplanes used for truncation in iteration i. Therefore, if the number of
iterations is O(n log2/η n

1/ poly( n

η ), we will have that Pr (x

) = 1

η ).

ﬁnite k. By Lemma 3.12, we have that the covariance matrix Σ0 (
that of Σ:

Note that 1d concentration implies that the distribution has bounded 2k’th moment for all
D ∩i Ci is close to
(16)

(1 + 1/ poly(

1/ poly(

))Σ

x
∈ ∩iCi |

∼ D

(1

D

−
D ∩i Ci) of
n
))Σ.
η

D ∩i Ci)

(cid:22)

Σ0 (

n
η

(cid:22)

−

D

D ∩i Ci) to Σ0

, we use Proposition 3.2. The concept class we use is
Finally, to relate Σ0 (
all degree two polynomials restricted to convex polytopes with at most O(n) facets, deﬁned by
e
the hyperplanes used for truncation at each iteration of the algorithm. The VC dimension of this
concept class is O(n2 log n). Therefore, by Proposition 3.2 applied with R = c1

Tr(Σ) log1/γ n

SD

(cid:16)

(cid:17)

Σ
c1k

1/2n1/2 log1/γ n
k

η , we get that if we take m = O

n3(log1/γ n
η2

η )2 log n

then

p

(cid:18)

(cid:19)

Σ0
k

SD

Σ0 (

D ∩i Ci)

−

k ≤

Σ
η/2
k

.
k

(cid:16)

(cid:17)

Combining equations 16 and 17 we get the desired result.

e

Theorem 5.3. When the algorithm terminates, we have:

(1

Σ
η)
k

−

k2 ≤ k

Σ0(

S)

k2 ≤

(1 + O(η log2/γ n
η

Σ
))
k

k2.

Proof. First, note that since only an η fraction of

S is noise, we have

e

Σ0(

S)

(1

η)Σ0(
e
−

(cid:23)

SD)

e

29

e

η ≤

(17)

(18)

Therefore, we have that

Σ0(
k
bound. For the upper bound, let v be the top eigenvector of Σ0(
we have

Σ0(
η)
k

k2 ≥

SD)

S)

(1

−

e

e

k2. Lemma 5.2 gives the desired lower
S). When the algorithm terminates,

Σ0(
k

S)

k2 = v T Σ0(

e

S)v
(1 + O(η log2/γ n
e
η
(1 + O(η log2/γ n
η

≤

≤

))v T Σv

Σ
))
k

k2.

e

where the second line follows because of the termination condition and because we can estimate
the variance of

in any direction to within a (1

cη) factor.

D

±

5.2 Termination

In this section, we will show that with high probability, Algorithm 5 terminates in a polynomial
1
number of steps provided that η
C for some constant C that depends only on the estimation in
Step (5).

≤

Every time the algorithm goes through another iteration, it must remove a certain number of
noise points. Suppose in step (7), we remove r noise points. The noise conﬁguration of maximum
r
variance puts r amount of noise at the outlier removal distance d1 = c1

Tr(Σ) log1/γ n

c2

σv log1/γ n
η
b
2

. We can then write an upper

p

η , and η

−

amount of noise at the truncation threshold distance d2 =
bound on σ2.

σ2 =

(1
k

−

η)Σ0(

SD) + ηΣ0(

2
SN )
2 ≤
k

v + rd2
σ2

1 + (η

r)d2
2

−

This implies

e

e

Let us simplify the numerator Z = σ2
(1 + c3η log2/γ n
σ2
η )
v ≤
is less than some constant.

σ2. Here we also assume that η

σ2

r

≥

σ2
v −

−

ηd2
2

σ2
v −
d2
2

−
d2
1 −
ηd2
2. Since we are truncating the sample, we have
1
1−cη

1
C for a suﬃciently large C so that

≤

1

b

Z

σ2

≥

−

σ2
1 + c3η log2/γ n

+ η

2 log2/γ n
c2
η
4

!

1

cη

η  

−
cη + (cη)2 + η

2 log2/γ n
c2
η
4

1 + c3η log2/γ n
η

c3η log2/γ n

η −

(cη)2
η
1 + c3η log2/γ n
η

−

2 log2/γ n
c2
η
4

σ2

σ2

≥

−

1

−





σ2

≥















30

Recall that σ2

(1

Σ
η)
k

k2 by (18). Then as long as c3 is a suﬃciently large constant, we have

−

≥

Then combining Z with the denominator from earlier and using the fact that d1 ≤
we get:

c1

n

Σ
k

k2 log1/γ n
η ,

p

Z

≥

Σ
k2
k
4  

c3η log2/γ n
η
1 + c3η log2/γ n

η !

r

≥

Σ
k

c3η log2/γ n/η
1+c3η log2/γ n

k2
(cid:18)
4c2
Σ
1k

η (cid:19)
k2n log2/γ n
η
c3η
1n(1 + c3η log2/γ n
η )

≥

4c2

Then r

≥
iterations.

O

min

(cid:18)

(cid:26)

η
n ,

1
n log2/γ n

η (cid:27)(cid:19)

Open Questions

, so the algorithm will terminate in a nearly linear number of

An immediate open question is whether the our analysis of the mean estimation algorithm is
tight and the √log n is avoidable. For special distributions including Gaussians, [DKK+16] give
η rather than η√log n or √η log n
an algorithm with higher sample complexity and error η
as in Theorem 1.1. An open question is to give an O(η) approximation. For the more general
distributions considered here, the dependence on η must grow as at least η3/4; it is open to ﬁnd an
algorithm that achieves O(η3/4) error (our guarantee for the general setting has error O(√η log n)).
Other open problems include agnostic learning of a mixture of two arbitrary Gaussians and agnostic
sparse recovery.

log 1

q

We thank Chao Gao and Roman Vershynin for helpful discussions. We would also like to thank the
anonymous reviewers for useful suggestions. This research was supported in part by NSF awards
CCF-1217793 and EAGER-1555447.

Acknowledgment

References

[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with
unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In
NIPS, pages 2384–2392, 2012.

[AK01]

Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Pro-
ceedings of the thirty-third annual ACM symposium on Theory of computing, pages
247–257. ACM, 2001.

31

[BCV13]

[Bru09]

Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of
tensor decompositions with applications to polynomial identiﬁability. arXiv preprint
arXiv:1304.8087, 2013.

S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In Proceedings of
the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2009,
New York, NY, USA, January 4-6, 2009, pages 1078–1087, 2009.

[Bru11]

David Bruce. A multivariate median in banach spaces and applications to robust pca.
http://www-personal.umich.edu/~romanv/students/bruce-REU.pdf, 2011.

[BRV13] Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the

presence of Gaussian noise. In Proc. of COLT, 2013.

[BS10]

[BV08]

[Car98]

Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In 51th
Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 103–112, 2010.

S Charles Brubaker and Santosh S Vempala. Isotropic PCA and aﬃne-invariant clus-
tering. In Building Bridges, pages 241–281. Springer, 2008.

J-F Cardoso. Multidimensional independent component analysis. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference
on, volume 4, pages 1941–1944. IEEE, 1998.

[CGR15] M. Chen, C. Gao, and Z. Ren. Robust Covariance Matrix Estimation via Matrix Depth.

ArXiv e-prints, June 2015.

[CJ10]

Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation.
Academic Press, 2010.

[CLMW11] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal com-

ponent analysis? J. ACM, 58(3):11:1–11:37, June 2011.

[CR08]

Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using
correlations and independence. In Proc. of COLT, 2008.

[CSPW11] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-
SIAM Journal on Optimization,

sparsity incoherence for matrix decomposition.
21(2):572–596, 2011.

[Das99]

[Dav87]

[DG92]

Sanjoy Dasgupta. Learning mixtures of Gaussians. In Foundations of Computer Sci-
ence, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999.

P. L. Davies. Asymptotic behaviour of s-estimates of multivariate location parameters
and dispersion matrices. Ann. Statist., 15(3):1269–1292, 09 1987.

David L. Donoho and Miriam Gasko. Breakdown properties of location estimates based
on halfspace depth and projected outlyingness. Ann. Statist., 20(4):1803–1827, 12 1992.

32

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the computational
intractability. CoRR, abs/1604.06443, 2016.

[Don82]

[DS07]

David L. Donoho. Breakdown Properties of Multivariate Location Estimators. PhD
thesis, Harvard University, 1982.

Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of
separated, spherical Gaussians. The Journal of Machine Learning Research, 8:203–226,
2007.

[FJK96]

Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In
FOCS, pages 359–368, 1996.

[GVX14]

Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decom-
position. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,
pages 584–593. ACM, 2014.

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment
methods and spectral decompositions. In ITCS, pages 11–20, 2013.

[HKO01]

Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis.
Wiley, 2001.

[HPL91]

Peter J. Rousseeuw Hendrik P. Lopuhaa. Breakdown points of aﬃne equivariant es-
timators of multivariate location and covariance matrices. The Annals of Statistics,
19(1):229–248, 1991.

[HRRS11] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.
Robust statistics: the approach based on inﬂuence functions, volume 114. John Wiley
& Sons, 2011.

Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.,
35(1):73–101, 03 1964.

Peter J. Huber.
Statistics, pages 1248–1251. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.

International Encyclopedia of Statistical Science, chapter Robust

[KMV10] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Eﬃciently learning mixtures
of two Gaussians. In Proceedings of the 42nd ACM symposium on Theory of computing,
pages 553–562. ACM, 2010.

[KRV]

https://github.com/kal2000/AgnosticMeanAndCovarianceCode.

[KV09]

Ravi Kannan and Santosh Vempala. Spectral Algorithms. Now Publishers Inc, 2009.

Nojun Kwak. Principal component analysis based on l1-norm maximization. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672–1680, 2008.

Ricardo Antonio Maronna. Robust m-estimators of multivariate location and scatter.
Ann. Statist., 4(1):51–67, 01 1976.

[Hub64]

[Hub11]

[Kwa08]

[Mar76]

33

[MMY06] RARD Maronna, Douglas Martin, and Victor Yohai. Robust statistics. John Wiley &

Sons, Chichester. ISBN, 2006.

[MSY92]

Ricardo A. Maronna, Werner A. Stahel, and Victor J. Yohai. Bias-robust estimators
of multivariate scatter based on projections. J. Multivar. Anal., 42(1):141–161, July
1992.

[MT+11] Michael McCoy, Joel A Tropp, et al. Two proposals for robust pca using semideﬁnite

programming. Electronic Journal of Statistics, 5:1123–1160, 2011.

Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures
of Gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE
Symposium on, pages 93–102. IEEE, 2010.

Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion
for high-dimensional datasets. Technometrics, 2012.

Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH
and NTRU signatures. J. Cryptology, 22(2):139–160, 2009.

J. R. Kettenring S. J. Devlin, R. Gnandesikan. Robust estimation of dispersion ma-
trices and principal components. Journal of the American Statistical Association,
76(374):354–362, 1981.

[Sma90]

Christopher G Small. A survey of multidimensional medians. International Statistical
Review/Revue Internationale de Statistique, pages 263–277, 1990.

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics, 12(4):389–434, 2012.

John W. Tukey. Mathematics and the Picturing of Data. In Ralph D. James, editor,
International Congress of Mathematicians 1974, volume 2, pages 523–532, 1974.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models.
Journal of Computer and System Sciences, 68(4):841–860, 2004.

Santosh Vempala and Ying Xiao. Max vs min: Tensor decomposition and ICA with
nearly linear sample complexity. In Proceedings of The 28th Conference on Learning
Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1710–1723, 2015.

[XCM10] Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis
with contaminated data: The high dimensional case. arXiv preprint arXiv:1002.4658,
2010.

[MV10]

[MZ12]

[NR09]

[SJD81]

[Tro12]

[Tuk74]

[VW04]

[VX15]

34

Agnostic Estimation of Mean and Covariance

Kevin A. Lai∗

Anup B. Rao∗

Santosh Vempala∗

August 16, 2016

Abstract

We consider the problem of estimating the mean and covariance of a distribution from iid
samples in Rn, in the presence of an η fraction of malicious noise; this is in contrast to much
recent work where the noise itself is assumed to be from a distribution of known type. The
agnostic problem includes many interesting special cases, e.g., learning the parameters of a single
Gaussian (or ﬁnding the best-ﬁt Gaussian) when η fraction of data is adversarially corrupted,
agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time
algorithms to estimate the mean and covariance with error guarantees in terms of information-
theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value
Decomposition.

6
1
0
2
 
g
u
A
 
4
1
 
 
]
S
D
.
s
c
[
 
 
2
v
8
6
9
6
0
.
4
0
6
1
:
v
i
X
r
a

∗Georgia Tech. Email: {kevinlai, anup.rao, vempala}@gatech.edu

1

Introduction

The mean and covariance of a probability distribution are its most basic parameters (if they are
bounded). Many families of distributions are deﬁned using only these parameters. Estimating the
mean and covariance from iid samples is thus a fundamental and classical problem in statistics.
The sample mean and sample covariance are generally the best possible estimators (under mild
conditions on the distribution such as their existence). However, they are highly sensitive to noise.
The main goal of this paper is to estimate the mean, covariance and related functions in spite of
arbitrary (adversarial) noise.

Methods for eﬃcient estimation, in terms of sample complexity and time complexity, play
an important role in many algorithms. One such class of problems is unsupervised learning of
generative models. Here the input data is assumed to be iid from an unknown distribution of a
known type. The classical instantiation is Gaussian mixture models, but many other models have
been studied widely. These include topic models, stochastic block models, Independent Component
Analysis (ICA) etc. In all these cases, the problem is to estimate the parameters of the underlying
distribution from samples. For example, for a mixture of k Gaussians in Rn, it is known that
the sample and time complexity are bounded by nO(k) in general [KMV10, MV10, BS10] and by
poly(n, k) under natural separation assumptions [Das99, AK01, VW04, DS07, CR08, BV08, HK13].
For ICA, samples are of the form Ax where A is unknown and x is chosen randomly from an
unknown (non-Gaussian) product distribution; the problem is to estimate the linear transformation
A and thus unravel the underlying product structure [FJK96, NR09, Car98, HKO01, CJ10, BRV13,
AGMS12, BCV13, GVX14, VX15]. These, and other models (see e.g., [KV09]), have been a rich
and active subject of study in recent years and have lead to interesting algorithms and analyses.

The Achilles heel of algorithms for generative models is the assumption that data is exactly from
the model. This is crucial for known guarantees, and relaxations of it are few and specialized, e.g.,
in ICA, data could by noisy, but the noise itself is assumed to be Gaussian. Assumptions about rank
and sparsity are made in a technique that is now called Robust PCA [CSPW11, CLMW11, XCM10].
There have been attempts [Kwa08, MT+11] at achieving robustness by L1 minimization, but they
don’t give any error bounds on the output produced. A natural, important and wide open problem
is estimating the parameters of generative models in the presence of arbitrary, i.e., malicious noise,
a setting usually referred to as agnostic learning. The simplest version of this problem is to estimate
a single Gaussian in the presence of malicious noise. Alternatively, this can be posed as the problem
of ﬁnding a best-ﬁt Gaussian to data or agnostically learning a single Gaussian. We consider the
following generalization:

Problem 1 [Mean and Covariance] Given points in Rn that are each, with probability 1
η
from an unknown distribution with mean µ and covariance Σ, and with probability η completely
arbitrary, estimate µ and Σ.

−

There is a large literature on robust statistics (see e.g., [Hub11, HRRS11, MMY06]), with the
goal of ﬁnding estimators that are stable under perturbations of the data. The classic example for
points on a line is that the sample median is a robust estimator while the sample mean is not (a single
data point can change the mean arbitrarily). One measure for robustness of an estimator is called
breakdown point, which is the minimum fraction of noise that can make the estimator arbitrarily
bad. Robust statistics have been proposed and studied for mean and covariance estimation in high

1

dimension as well (see [Hub64, Tuk74, Mar76, SJD81, Don82, Dav87, HPL91, DG92, MSY92, MZ12,
CGR15] and the references therein). Most commonly used methods (including M-estimators) to
estimate the covariance matrix were shown to have very low break down points [Don82]. The
notion of robustness we consider quantiﬁes how far the estimated value is from the true value. To
the best of our knowledge, all the papers either suﬀer from the diﬃculty that their algorithms are
computationally very expensive, namely exponential time in the dimension, or have poor or no
guarantees for the output. Tukey’s median [Tuk74]) is an example of the former. It is deﬁned as
x i}i. As proven in [CGR15], this is an
the deepest point with respect to a given set of points
{
optimal estimate of the mean. But there is no known polynomial time algorithm to compute this.
Another well-known proposal (see [Sma90]) is the geometric median:

arg min

y

y
k

x ik2.

−

Xi
This has the advantage that it can be computed via a convex program. Unfortunately, as we
observe here (see Proposition 2.1), the error of the mean estimate produced by this method grows
polynomially with the dimension (also see [Bru11]).

This leads to the question, what is the best approximation one can hope for with η arbitrary
(adversarial) noise. From a purely information-theoretic point of view, it is not hard to see that
even for a single Gaussian N (µ, σ2) in one dimension, the best possible estimation of the mean will
have error as large as Ω(ησ), i.e., any estimate ˜µ can be forced to have
= Ω(ησ). For a more
general distribution, this can be slightly worse, namely, Ω(η3/4σ) (see Section 2.1). What about in
Rn? Perhaps surprisingly, but without much diﬃculty, one can show that the information-theoretic
upper bound matches the lower bound in any dimension, with no dependence on the dimension.
This raises a compelling algorithmic question: what are the best estimates for the mean and
covariance that can be computed eﬃciently?

µ
k

−

˜µ

k

In this paper, we give polynomial time algorithms to estimate the mean with error that is close
to the information-theoretically optimal estimator. The dependence on the dimension, of the error
in the estimated mean, is only √log n. To the best of our knowledge, this is the ﬁrst polynomial-
time algorithm with an error dependence on dimension that is less than √n, the bound achieved by
the geometric median. Moreover, as we state precisely later, our techniques extend to very general
input distributions and to estimating higher moments.

Our algorithm is practical. A matlab implementation for mean estimation can be found in
[KRV]. It takes less a couple of seconds to run on a 500-dimensional problem with 5000 samples
on a personal laptop.

Model. We are given points x 1, ..., x m ∈
η
−
probability each x i is independently sampled from a distribution
with mean µ and covariance
Σ, and with η probability it is picked by an adversary. For ease of notation, we will write x i ∼ Dη
when we want to say the x i is picked according to the above rule. The problem we are interested
in is to estimate µ and Σ given the samples. In the following, we will consider mainly two kinds of
distributions.

Rn sampled according to the following rule. With 1

D

Gaussian

= N (µ, Σ) is the Gaussian with mean µ and covariance Σ.

D
Bounded Moments Let

D

is a distribution with mean µ and covariance Σ. We say it has

2

bounded 2k’th moments if there exists a constant C2k such that for every unit vector v ,

E

(x

µ)T v

2k

C2k

E

(x

≤

µ)T v

−

−

(cid:0)

(cid:1)
2

(cid:16)

(cid:0)

k

2

(cid:17)

(cid:1)

x T v

Here Var
used, and for covariance estimation, C8 will be needed.
(cid:1)

=

(cid:0)

(cid:2)

(cid:3)

v T Σv

= C2k(Var

x T v

)k.

(1)

is the variance of x along v . For mean estimation, C4 will be

(cid:2)

(cid:3)

1.1 Main results

All the results we state hold with probability 1
also assume η is a less than a universal constant. We begin with agnostic mean estimation.

1/ poly(n) unless otherwise mentioned. We will

−

Theorem 1.1 (Gaussian mean). Let
algorithm that takes as input m = O
computes

µ such that the error

= N (µ, Σ), µ

D
n(log n+log 1/ǫ) log n
ǫ2

Rn. There exists a poly(n, 1/ǫ)-time
∈
independent samples x 1, ..., x m ∼ Dη and

b

µ
k

µ
k2 is bounded as follows:
(cid:16)
−

(cid:17)

b
η1/2 + ǫ

O (η + ǫ) σ√log n
Σ
k

1/2
2
k

log1/2 n

if Σ = σ2I
otherwise.

O

(cid:0)

We note that the sample complexity is nearly linear, and almost matches the complexity for

(cid:1)

mean estimation with no noise.

Remark 1.2. If we take m = O
samples, and assume that η < c/ log n for a
small enough constant c > 0, then by combining theorems 1.5 and 1.1, we can improve the η depen-
log1/2 n.
dence for the non-spherical Gaussian case in Theorem 1.1 to

µ
k2 = O
(cid:0)
Our next theorem is a similar result for much more general distributions.

1/2
Σ
2
k
k

µ
k

η3/4

−

(cid:17)

(cid:16)

(cid:1)

n2(log n+log 1/η) log n
η2

b

D

be a distribution on Rn with mean µ, covariance Σ, and
Theorem 1.3 (General mean). Let
bounded fourth moments (see Equation 1). There exists a poly(n, 1/ǫ)-time algorithm that takes
independent samples x 1, ..., x m ∼ Dη, and
as input a parameter η and m = O
µ
computes
(cid:16)
k
−
C 1/4
4
η1/2 + C 1/4
(cid:16)

µ
k2 is bounded as follows:
σ√log n
1/2
2
k

(η + ǫ)3/4
b
(η + ǫ)3/4

n(log n+log 1/ǫ) log n
ǫ2

µ such that the error

if Σ = σ2I

otherwise.

log1/2 n

Σ
k

O

O

(cid:17)

(cid:17)

b

4

(cid:16)

(cid:17)

The bounds above are nearly the best possible (up to a factor of O(√log n)) when the covariance

is a multiple of the identity.

Rn and covariance Σ.
Observation 1.4 (Lower Bounds). Let
Any algorithm that takes m (not necessarily O(poly(n))) samples x 1, ..., x m ∼ Dη, and computes a
µ
µ should have with constant probability the error
k

be a distribution with mean µ

−

D

∈

µ
k2 is
= N (µ, Σ)
b
has bounded fourth moments.

D

if

b

Ω(η
Ω(η3/4
p

Σ
k

k2)
k2)
Σ
k

if

D

p

3

−

−
D

, x and (x

be a distribution with mean µ and covariance Σ
D
µ)T have bounded fourth moments with constants C4
is an (unknown) aﬃne transformation of a 4-wise in-

Theorem 1.5 (Covariance Estimation). Let
and that (a) for x
µ)(x
∼ D
and C4,2(see Equation 1) respectively. (b)
dependent distribution. Then, there is an algorithm that takes as input m = O
samples x 1, ...x m ∼ Dη and η and computes in poly(n, 1/ǫ)-time a covariance estimate
η1/2 + C 1/4
(cid:16)

−
b
k · kF denotes the Frobenius norm.
= N (µ, Σ), then it satisﬁes the hypothesis of the above theorem. More generally, it holds
for any 8-wise independent distribution with bounded eighth moments and whose fourth moment
along any direction is at least (1 + c) times the square of the second moment for some c > 0. We
also note that if the distribution is isotropic, then covariance estimation is essentially a 1-d problem
and we get a better bound.

n2(log n+log 1/ǫ) log n
ǫ2
(cid:17)
Σ such that

k2 log1/2 n
Σ

4,2 (η + ǫ)3/4

kF = O

C 1/2
4 k

where

Σ
k

Σ

D

If

(cid:17)

(cid:16)

b

Theorem 1.6 (Agnostic 2-norm). Suppose
centration inequality: there exists a constant γ such that for every unit vector v

is a distribution which satisﬁes the following con-

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

≤

(cid:17)

Then, there is an algorithm that runs in poly(n, log 1

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)
independent samples x 1, ..., x m ∼ Dη, and computes

O

n3(log n/η)2 log n
η2

(cid:16)

(cid:17)

η ) time that takes as input η and m =

λmax such that

(1

O(η))

−

Σ
k

k2 ≤

1 + O(η log2/γ n/η)
(cid:17)

(cid:16)

b
k2.
Σ
k

λmax ≤
b

In independent work, [DKK+16] gave a similar algorithm, which they call a Gaussian ﬁltering
method, for agnostic mean estimation assuming a spherical covariance matrix; while their guar-
antees are speciﬁcally for Gaussians, the error term in their guarantee grows only with log(1/η)
rather than log n. They also give a completely diﬀerent algorithm based on the Ellipsoid method,
for a simple family of distributions including Gaussian and Bernoulli.

As a corollary of Theorem 1.5, we get a guarantee for agnostic SVD.

Theorem 1.7 (Agnostic SVD). Let
Let Σk be the best rank k approximation to Σ in
algorithm that takes as input η and m = poly(n) samples from
such that

is a distribution that satisﬁes the hypothesis of Theorem 1.5.
k · kF norm. There exists a polynomial time
Σk

Dη. It produces a rank k matrix

D

−

−

Σ

Σ

(cid:13)
(cid:13)
(cid:13)

Σk

F ≤ k

ΣkkF + O
Given the wide applicability of SVD to data, we expect the above theorem will have many ap-
plications. As an illustration, we derive a guarantee for agnostic Independent Component Analysis
(ICA). In standard ICA, input data points x are generated as As with a ﬁxed unknown n
n
full-rank matrix A and s generated from an unknown product distribution with non-Gaussian com-
ponents. The problem is to estimate the matrix A (the “basis”) from a polynomial number of
samples in polytime. There is a large literature of algorithms for this problem and its extensions

η log n

Σ
k

k2.

(cid:16)p

(cid:13)
(cid:13)
(cid:13)

×

(cid:17)

b

b

4

[FJK96, NR09, Car98, HKO01, CJ10, BRV13, AGMS12, BCV13, GVX14]. However, all these al-
gorithms rely on no noise or the noise being random (typically Gaussian) and require estimating
singular values to within 1/ poly(n) accuracy, and therefore unable to handle adversarial noise. On
the other hand, the algorithm from [VX15], which gives a sample complexity of ˜O(n), only requires
estimating singular values to within 1/ poly(log n). Our algorithm for agnostic SVD together with
the Recursive Fourier PCA algorithm of [VX15] results in an eﬃcient algorithm for agnostic ICA,
tolerating noise η = O(1/ logc n) for a ﬁxed constant c. To the best of our knowledge, this is the
ﬁrst polynomial-time algorithm that can handle more than an inverse poly(n) amount of noise.

4

∈

−

si|
|

η and be arbitrary with probability η, where A

Theorem 1.8 (Agnostic Standard ICA). Let x
probability 1
components of s are independent,
5
si|
E
|
|
η < ǫ/2, there is an algorithm that, with high probability, ﬁnds vectors
there exist signs ξi =
1 satisfying
poly(n, K, ∆, M, κ, 1
on real symmetric matrices of size n

Rn be given by a noisy ICA model x = As with
Rn×n has condition number κ, the
K√n almost surely, and for each i, Esi = 0, Es2
i = 1,
k ≤
M . Then for any ǫ < ∆3/(108M 2 log3 n), 1/(κ4 log n) and
such that
k2 for each column A(i) of A, using
A
k
ǫ ) samples. The running time is bounded by the time to compute ˜O(n) SVDs

b1, . . . , bn}
{

∆ and maxi E

3
| ≥

A(i)

(cid:13)
(cid:13)
n.

k
≤

ξibi

(cid:13)
(cid:13)

≤

−

−

±

∈

s

ǫ

×

Our results can also be used to estimate the mean and covariance of noisy Bernoulli product
distributions, i.e. distributions in which each coordinate i is 1 with probability pi and 0 with
probability 1
1−p . For a Bernoulli
. Then Theorem 1.3
1
2 , then
If C4 is constant, then by Theorem 1.5, we can get an

pi. In one dimension, C4 for a Bernoulli distribution is (1−p)2
+ p2
i
1−pi
i, pi = p and p

product distribution, C4 will be within a constant of maxi
can be applied to get an estimate
µ
k
estimate for the covariance.

µ for the mean. For instance, if

µ
k2 = O

p + p2

(1−pi)2
pi

≥

−

−

o

n

∀

.

η(1 + √ηp)p log n
b
(cid:1)

(cid:0)p

b

2 Main Ideas

Here we discuss the key ideas of the algorithms. The algorithm AgnosticMean (Algorithm 3)
alternates between an outlier removal step and projection onto the top n/2 principal components;
these steps are repeated.
It is inspired by the work of Brubaker [Bru09] who gave an agnostic
algorithm for learning a mixture of well-separated spherical Gaussians.

For illustration, let us assume for now that the underlying distribution is

= N (µ, σ2I ). We
SN be the points sampled from
are given a set S of m = poly(n) points from
the Gaussian and the adversary respectively. Let us also assume that
. We will use the
|
notation µT for mean of the points in a set T , and ΣT for covariance of the points in T . We then
have

Dη, and S = SG ∪

SN |
|

= η

S
|

D

ΣS = (1

η)σ2I + ηΣSN + η(1

−

η)(µS −

µN )(µS −

−

µN )T .

(2)

If the dimension is n = 1, then we can show that the median of S is an estimate for µ correct up to an
µN ),
additive error of O(ησ). Even if we just knew the direction of the mean shift µS −
µ = η(µG −
µS and then ﬁnding
then we can estimate µ by ﬁrst projecting the sample S on the line along µ
µ
the median. This would give an estimator
k2 = O(ησ). So we can focus on
µ. One would guess that the top principal component of the covariance
ﬁnding the direction of µS −
matrix of S would be a good candidate. But it is easy for the adversary to choose SN to make this
completely useless. Since the noise points SN can be anything, just two points from SN placed far

µ satisfying

µ
k

−

−

b

b

5

away on either side of the mean µ along a particular line passing through µ are suﬃcient to make
the variance in that direction blow up arbitrarily. But we can limit this eﬀect to some extent by
an outlier removal step. By a standard concentration inequality for Gaussians, we know that the
points in SG lie in a ball of radius O(σ√n) around the mean. So, if we can just ﬁnd a point inside
or close to the convex hull of the Gaussian and throw away all the points that lie outside a ball of
radius Cσ√n around this point, we preserve all the points in SG. This will also contain the eﬀect
of noise points on the variance since now they are restricted to be within O(σ√n) distance of µ.
We will see later that we can use coordinate-wise median as the center of the ball. By computing
the variance by projecting onto any direction, we can ﬁgure out σ2 up to a 1
O(η) factor. From
now on, we assume that all points in S lie within a ball of radius O(σ√n) centered at µ.

±

But even after this restriction, the top principal component may not contain any information
about the mean shift direction. By just placing (say) η/10 noise points along the e1 direction
σ√n, and all the remaining noise points perpendicular to this at a single point at a smaller
at
distance, we can make e1 the top principal component. But e1 is perpendicular to the mean shift
direction.

±

The idea to get around this is that even if the top principal component of ΣS may not be along
the mean-shift direction, the span (call it V ) of top n/2 principal components of ΣS will contain a big
projection of the mean-shift vector. This is because, if a big component of the the mean-shift vector
was in the span (say W ) of bottom n/2 principal components of ΣS, by Equation 2 this would mean
that there is a vector in W with a large Rayleigh quotient. This implies that the top n/2 eigenvalues
µN )T ,
of ΣS are all big. Since ΣS = (1
this is possible only if Tr(A) is large. But since the distance of each point in S from µ is O(σ√n),
the trace of A cannot be too large. Therefore, in the space W , we can just compute the sample
mean P W µS and it will be close to P W µ. We still have to ﬁnd the mean in the space V . But we
do this by recursing the above procedure in V . At the end we will be left with a one-dimensional
space, and then we can just ﬁnd the median. This recursive projection onto the top n/2 principal
components is done in Algorithm 3 .

η)σ2I + A, where A = ηΣSN + η(1

µN )(µS −

η)(µS −

−

−

This generalizes to the non-spherical Gaussians with a few modiﬁcations. We use a diﬀerent
outlier removal step. In the non-spherical case, it is not trivial to compute
k2 to be used as
Σ
k
the radius of the ball. We give an algorithm for this later on. To limit the eﬀect of noise, we use
a damping function. Instead of discarding points outside a certain radius, we damp every point
by a weight so that further away points get lower weights. This is done in OutlierDamping
(Algorithm 1). We get the guarantees of Theorem 1.1 by running AgnosticMean (Algorithm 3)
with the outlier removal routine being OutlierDamping. A detailed proof of the whole algorithm
is given in Section 3.1.

We then turn to more general distributions which have bounded fourth moments. We need
bounded fourth moments to ensure that the mean and covariance matrix of the distribution
do not
change much even after conditioning by an event that occurs with probability 1
η. One diﬃculty for
with bounded
general distributions is that the outlier damping doesn’t work. So for distributions
fourth moments, we have another outlier removal routine called OutlierTruncation(
, η). In this
·
routine, we ﬁrst ﬁnd a point analogous to the coordinate-wise median for the Gaussians, and then
η fraction of S. We throw away all the points outside
consider a ball big enough to contain 1
this ball. We get the guarantees of Theorem 1.3 by running AgnosticMean (Algorithm 3) with
the outlier removal routine being OutlierTruncation (Algorithm 2). The complete proof of this
appears in Section 3.3.

−

−

D

D

6

D

is given by ED(x

We now have an algorithm to estimate the mean of very general (with bounded fourth moments)
distributions. To estimate the covariance matrix, we observed that the covariance matrix of a
µ)T . If we knew what µ was, then covariance can be
distribution
computed by estimating the mean of the second moments. To compute the mean of the second
µ)T as a vector in n2 dimensions and run the algorithm for
moments, we can treat (x
mean estimation. Also, we can estimate µ by the same algorithm. Therefore, we get Theorem 1.5
by running CovarianceEstimation (Algorithm 4). Its proof appears in Section 4.2.
Algorithm AgnosticOperatorNorm (Algorithm 5) estimates the 2-norm

µ)(x

µ)(x

−

−

−

−

k2 for general
= N (µ, Σ), and we are given m = poly(n) samples

Σ
k

distributions. For illustration, suppose
x 1, ..., x m ∼ Dη, and the mean µ. We consider the covariance-like matrix

D

Σ(S, µ) =

(x i −

µ)(x i −

µ)T .

1
m

Xi

−

η fraction of the points in S are from the Gaussian, we have Σ(S, µ)

Since 1
η)Σ. Therefore,
(cid:23)
the top eigenvalue σ2 of Σ(S, µ) is at least (1
k2. Let v be the top eigenvector of Σ(S, µ). If
η factor) is much less than σ2, this
the Gaussian variance along v (which can be computed up to 1
should be because there are a lot of noise points in S whose projections onto v are big compared
to the projection of Gaussian points in S. We remove points in S that have big projection and
then iterate the entire procedure. We later show that this procedure terminates in poly(n) steps
and when it terminates the top eigenvalue of Σ(S, µ) is close to that of Σ. A proof of this appears
in Section 5.

Σ
η)
k

(1

−

−

±

Theorem 1.7 follows easily from Theorem 1.5. Let

Σk be the top-k eigenspace of

Σ from

Theorem 1.5. We then have

b

Σ

Σk

−

F
(cid:13)
(cid:13)
(cid:13)

b

(cid:13)
(cid:13)
(cid:13)

b
Σk

−

−

b
Σk

F
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
ΣkkF
(cid:13)

Σ

−

(a)

≤
(b)

≤
(c)

≤
(d)

Σ

Σ

−

F

(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
Σ
(cid:13)

b
Σ

−

Σ

b
−

+

Σ

+

(cid:13)
(cid:13)
(cid:13) b
Σ
(cid:13)
(cid:13)
(cid:13) b
+
k

F

(cid:13)
(cid:13)
b
ΣkkF + O
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)

≤ k

−

η log n

Σ
k

k2.

(cid:17)
(a), (c) follow from triangle inequality, (b) follows from the fact that
imation and (d) from the guarantees of Theorem 1.5.

(cid:16)p

Σk is the best rank-k approx-

Finally we outline the application to agnostic ICA. The algorithm from [VX15]. Proceeds by ﬁrst
estimating the mean and covariance, in order to make the underlying distribution isotropic. Here
we estimate the covariance matrix Σ by ˆΣ and use it to determine a new isotropic transformation
ˆΣ
k2, the
isotropic transformation results in a guarantee of

2 . Since our agnostic SVD algorithm gives a guarantee of

O(√ν log n)
Σ
k

kF ≤

Σ
k

˜Σ

− 1

−

b

− 1

2 Σ ˆΣ

− 1
2

ˆΣ
k

I

k2 ≤

−

O(

Σ
k2
η log n) k
Σ−1
k2
k

p

p

= O(

η log nκ2).

Next the algorithm estimates a weighted covariance matrix W with the weight of a point x pro-
portional to cos(u T x ) for u chosen from a Gaussian distribution; it computes the SVD of W . For

7

this we use our algorithm again (the weights are applied individually to each sample). The main
guarantee is that the eigenvectors of this weighted covariance approximate the columns of A. This
relies on the maximum eigenvalue gap of W being large, and it has to be approximated to within
additive error ǫ = O(1/(log n)3). Theorem 1.7 implies that the additional error in eigenvalues is
k2, and therefore it suﬃces to have √η log n < c/(log n)3 for a suﬃ-
bounded by O(√η log n)
Σ
k
ciently small constant c that depends only on the cumulant and moment bound assumptions (i.e.,
c(log n)−7.
∆, M ). Thus, if suﬃces to have η < ǫ/2

≤

2.1 Lower Bounds: Observation 1.4

In this section we will show the lower bounds stated in Observation 1.4. For Gaussian distributions,
this is a special case of a theorem proved in [CGR15]. We reproduce the relevant part here for
D2 = N (µ2, σ2I ) and
completeness. We will show that there are distributions
µ2k2 = Ω(ησ) and
distributions Q1, Q2 such that
D1 + ηQ1 = (1
η)
−
D2. Let φ1 be p.d.f of
D1,

µ1 −
k
Dη = (1
Dη, no algorithm can distinguish between

D1 = N (µ1, σ2I ),

D1 and φ2 be the

D2 + ηQ2.

(3)

η)

−

D2. Let µ1, µ2 be such that the total variation distance between

D1,

D2 is

So, given
p.d.f of

1
2

η

φ2|
By a standard inequality for the total variation distance of Gaussian distributions, this implies
µ2k2 ≥
φ1)1φ2≥φ1 and Q2 be the
that
distribution with p.d.f 1−η
φ2)1φ1≥φ2. It is now easy to verify that Equation 3 is satisﬁed.
This proves item one of Observation 1.4.

1−η . Let Q1 be the distribution with p.d.f 1−η

η (φ1 −

η (φ2 −

µ1 −
k

φ1 −
|

dx =

2ησ

−

Z

1

η

.

D1 is supported on two points
{−
1/4. It is easy to check that both

For the distributions with bounded fourth moments, consider the following two one-dimensional
distributions.
.
1/2, 1/2
}
{
D2 is supported on three points
respec-
η)/2, η
−
D2 have bounded fourth moments with
tively. Let η
≤
the constant C4 = 8. Furthermore,
D1 by adding η fraction of noise
D2 can be obtained from
points. So no algorithm can distinguish between the two distributions. Since their means diﬀer by
η3/4σ, no algorithm can get an estimate better than this.

with the corresponding probabilities
(1
{

{−
σ, σ, σ/η1/4
}

}
D1 and

with probabilities

η)/2, (1

σ, σ

−

}

We will now show that the geometric median:

arg min

y

x i −
k

y

k2

Xi
has a √n dependence on the dimension. We show this in the Gaussian case even if we have access
to the whole distribution, but with η fraction of noise points placed all at a single point far away
from most of the Gaussian points.

Proposition 2.1 (Geometric Median). Let
= N (0, Σ) be a distribution with diagonal covariance
matrix Σ whose variance along the coordinate direction e 1 is zero, and equal to 1 in all the other
coordinate directions. Assume there is an η fraction of noise at a distance a = n along e 1. Let

D

t0 = arg min

(1

η)Ex ∼D

t

−

t2 + x2

2 + ... + x2
n

+ η(a

t).

−

(cid:19)

(4)

Then, t = Ω(η√n).

(cid:18)q

8

Proof. We have that at the minimizer t0, the derivative with respect to t is zero. Therefore, we
should have

Ex ∼D

t0
0 + x2
t2
2 + ... + x2
n

=

1

η

−

.

η

Consider f (t) = Ex ∼D

t
√t2+x2
2+...+x2
n
t = αη√n for a small enough constant α, then f (t)
x
n/2 with exponential probability. Therefore,
k

2
2 ≥
k

≤

. It is clear from Equation 4 that t0 > 0. We claim that if
p

η
1−η . Suppose t1 = αη√n. Since x

,

∼ D

f (t1)

Ex ∼D

t1
t2
1 + n/2

≤

≤

t1√2π
p
t2
1 + n/2 ≤

αη√2π.

Our algorithms are based on outlier removal and SVD. To simplify the proofs, we use new samples
for each step of the algorithm. The total sample complexity is given in the theorems.

For outlier removal, we use one of the following two simple routines. The ﬁrst, which we call
OutlierDamping, returns a vector of positive weights, one for each sample point.

The claim, and hence the proof follows.

p

2.2 Algorithms

2.2.1 Outlier Removal

Algorithm 1: OutlierDamping(S)

Input: S
⊂
Output: S

Rn with

= m

S
|

|
Rn, w = (w1, ..., wm)

Rm

∈

⊂

1. if n = 1:

Return (S,

1).

−

3. Set wi = exp

4. Return (S, w ).

kx i−aaak2
2
s2

(cid:17)

−

(cid:16)

for every x i ∈

S.

9

The second procedure for outlier removal returns a subset of points. It will be convenient to

view this as a 0/1 weighting of the point set. We call this procedure OutlierTruncation.

2. Let aaa be the coordinate-wise median of S. Let s2 = C Tr(Σ). Estimate Tr(Σ) by esti-

mating 1d variance along n orthogonal directions, see Section 4.1.

Algorithm 2: OutlierTruncation(S, η)

Input: S
Output:

⊂
S

Rn, η

[0, 1]

∈
S, w = 1

Rm

∈

⊂

1. if n = 1:

e

←

∩

2. Let aaa be as in Lemma 3.15.
e

e

fraction of S.

←

∩

e

4.

S

S

B(r, aaa). Return (

S, 1).

2.2.2 Main Algorithm

e

Let [a, b] be the smallest interval containing (1
S

[a, b]. Return (

S, 1).

S

η

−

−

−

ǫ)(1

η) fraction of the points,

3. Let B(r, aaa) = ball of minimum radius r centered at aaa that contains (1

η

ǫ)(1

η)

−

−

−

We are now ready to state the main algorithm for agnostic mean estimation. It uses one of the
above outlier removal procedures and assumes that the output of the procedure is a weighting.

Algorithm 3: AgnosticMean(S)

Input: S
Output:

⊂
µ

Rn.

Rn, and a routine OutlierRemoval(
).
·

∈
S, w ) = OutlierRemoval(S) .

1. Let (
b

2. if n = 1:

e

(a) if w =

1, Return median(

S). //Gaussian case

−
(b) else Return mean(

S). //General case
e

3. Let Σ

eS,w be the weighted covariance matrix of
e

the top n/2 principal components of Σ

4. Set S1 := P V (S) where P V is the projection operation on to V .

5. Let

µV := AgnosticMean(S1) and

µW := mean(P W

S).

Rn be such that P V

µ =

µV and P W

µ =

µW .

e

b

b

b

b

b

µ
b

6. Let

∈
7. Return

b

µ.

b

10

S with weights w , and V be the span of

eS,w , and W be its complement.

e

2.2.3 Estimation of the Covariance Matrix and Operator Norm

For both the tasks in this section, we will assume that the mean of the distribution µ = 0. We
can do this without loss of generality by a standard trick mentioned described in Section 4.2. The
algorithm for estimating the covariance matrix calls AgnosticMean on x x T . Analysis is given in
Section 4.2.

1. Let S(2) =

x ′
{

ix ′
i = 1, ..., m/2
b
i|
}

(see Equation 15)

2. Run the mean estimation algorithm on S(2), where elements of S(2) are viewed as vectors

direction of top variance. The analysis is given in Section 5.

Σ
k

k2 is based on iteratively truncating the samples along the

Algorithm 4: CovarianceEstimation(S)

Input: S
⊂
Output: n

R
Rn, η
∈
n matrix

Σ

×

in Rn2

. Let the output be

Σ.

3. Return

Σ.

b

b
The algorithm for estimating

Algorithm 5: AgnosticOperatorNorm(S)

Input: S
⊂
Output: σ2

Rn, η

∈
R>0.

∈

[0, 1], γ

R

∈

1. Let

S = SafeOutlierTruncation(S, η, γ).

2. Do the following O(n log2/γ n

η ) times

e

3. Let Σ0(

S) := 1
| eS|

i∈ eS x x T .

4. Find v , the top eigenvector of Σ0(

e

S), and its corresponding eigenvalue σ2.

5. Estimate (up to 1

cη factor, see Section 4.1) the variance of

along v and denote it

e

D

P

±

by

σ2
v .

6. if σ2
b

≤
Return σ2.

(1 + c3η log2/γ n
η )

σ2
v

7. Remove all points x

S such that

x T v
|

|

>

c2

σv log1/γ n
η
b
2

.

8. Go to Step (3).

b

∈

e

11

Algorithm 6: SafeOutlierTruncation(S, η, γ)

Input: S
Output:

⊂
S

Rn, η
S

∈

[0, 1], γ

R

∈

⊂
1. Let t =

e

n
i=1

2. Let B(c√t log1/γ n

P

b

3.

S

S

←

∩

B(c√t log1/γ n

η , 0). Return

S.

e

e

σ2
ei be the sum of estimated variances of

D
η , 0) be the ball of radius c√t log1/γ n
η centered at 0.

in n orthogonal directions.

3 Mean Estimation: Theorem 1.1 and Theorem 1.3

b

µ
k

In this section, we will ﬁrst prove Theorem 1.1, which is for Gaussian distributions, and Theorem 1.3,
which is for distributions with bounded fourth moments. All our algorithms will be translationally
is µ = 0. So we will be
invariant. We will assume w.l.o.g that the mean of the distribution
k2. Algorithm 3 has log n levels, we will assume that at each level it uses
proving bounds on
O( n log n

) samples resulting in a total of m = O( n log2 n

ǫ2
ǫ2
At various points in the analysis, to bound the sample complexity we will have to show that
the estimates computed from samples are close to their expectations. We will use the following
two results. Firstly, as an immediate corollary of matrix Bernstien for rectangular matrices (see
Theorem 1.6 in [Tro12]), we get the following concentration result for the sample mean and sample
covariance.
Lemma 3.1. Consider a distribution in Rn with covariance matrix Σ and supported in some
(0, 1). Then the
Σ
Euclidean ball whose radius we denote is
k
following holds with probability at least 1

R. Let ǫ
then

, for some R
k
1/ poly(n): If N
p

∈
R log n
ǫ2

D

R

∈

).

≥

and

Here

µ and

Σ are sample mean and sample covariance matrix.

b

b

Secondly, the functions we estimate will be integrals of low-degree polynomials (degree d at
most 4) restricted to intervals and/or balls. These functions viewed as binary concepts have small
VC-dimension, O(nd) where n is the dimension of space and d is the degree of the polynomial. We
use this to bound the error of estimating integrals via samples, and we can make the error smaller
than any inverse polynomial using a poly(n) size sample.
Proposition 3.2. Let F be a class of real-valued functions from Rn to [
corresponding class of binary concepts, i.e., for each f
f (x)

R, R]. Let CF be the
F , we consider the concepts ht(x) = 1 if
F , and any

t and zero otherwise. Suppose the VC-dimension of CF is d. Then, for any f

−

∈

≥

∈

µ
k

µ

−

k ≤

ǫ

Σ
k

k

p

Σ

−

k ≤

ǫ

Σ
k

.
k

−

b
Σ
k

b

12

distribution
least 1

D
δ satisﬁes

−

over Rn, an iid sample S of size

8
ǫ2 (d log(1/ǫ) + log(1/δ)), with probability at

S
|

| ≥

1
S
|

−

| Xx∈S

2ǫR.

≤

f (x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ex∼D(f (x))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

Proof. By the VC theorem, for any concept in CF , the bound on the size of the sample ensures
that with probability at least 1

δ and any t,

Noting that Ex∼D(f (x)) =

t) dt, we get the claimed bound.

≥

Pr(f (x)
(cid:12)
(cid:12)
(cid:12)
R
−R Pr(f (x)
(cid:12)
R
Tr(Σ) and ǫ2 := kaaak2

2

≥

x

|{

∈

t)

−

t

≥

}|

S : f (x)
S
|

|

ǫ.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let s2 := 1
ǫ1

η2s2 . We can estimate Tr(Σ) by estimating (1 dimensional)
variances along n orthogonal directions, see Section 4.1. Note that we can arrange 0 < ǫ1, ǫ2 < 1
to be small enough constants. We weight every point x by wx = exp(
= N (0, Σ)
be a Gaussian distribution and S =
SN
be the Gaussian and the noise points repectively, with

−
, x i ∼ Dη be the sample we get. Let S = SG ∪

x 1, ..., x m}
{

D
Rn, let

= ηm. For a set T

kx −aaak2
s2

). Let

µT,w :=

wx ix i and ΣT,w :=

1
m

Xi∈T

SN |
|
wi(x i −

1
T
|

| Xi∈T

⊂
µT,w )T

µT,w )(x i −

We use the above notation for T = SG and T = SN . By an abuse of notation, when T = G, we
mean the population version of the above quantities:

µG,w := Ex wx x and ΣG,w := Ex wx (x

µG,w )(x i −

−

µG,w )T .

Note that

We consider the matrix ΣS,w

µS,w = (1

η)µSG,w + ηµSN ,w .

−

ΣS,w =

1
m

= (1

Xi
−

µS,w )(x i −

wx i(x i −
η)ΣSG,w + ηΣSN ,w + η(1

µS,w )T

η)(µSN ,w −

µSG,w )(µSN ,w −

−

µSG,w )T .

3.1 Proof of Theorem 1.1:

Let us assume η < 1/2.1. We then have

Lemma 3.3. Let
we are given x1, ..., xm ∼ Dη, then the median xmed = mediani{
with high probability.

D

xi}

= N (0, σ2) be a one dimensional Gaussian distribution. If m = O

xmed
|

|

satisﬁes

log n
ǫ2

, and
= O((η + ǫ)σ)

(cid:17)

(cid:16)

Proof. Let SG ⊂
Φ−1(1/2 + η + ǫ). Let us bound the probability that the median xmed ≥
xmed ≥
≥
SG|
if
|

S be made up of samples in S that come from the Gaussian, also let c =
c. We ﬁrst note that if
poly(n)

c, then Pr (x > c
x
|
log n
= O
ǫ2

ǫ. By Hoeﬀding’s inequality, we can bound this by 1

∈u SG)

−

.

(cid:16)

(cid:17)

13

We will next consider the multidimensional case. The proof follows by a series of lemmas. We
state the lemmas ﬁrst, conclude the proof of Theorem 1.1 and then prove the lemmas. First, we
observe that by applying Lemma 3.3 in n orthogonal directions and union bound, we get
Lemma 3.4. Suppose v 1, ..., v n ∈
i miv i. Then if m = O
and aaa =
v i’s such that with probability 1

Rn are a set of orthonormal vectors. Suppose mi = medianj{
v t
ix j}
,
, there exists a constant C independent of the choice of

log n
ǫ2
poly(n) ,
(cid:16)

(cid:17)

P

−

By a simple calculation, maxx

bound on the trace.

Lemma 3.5. Suppose A := ηΣSN ,w + η(1
a constant C such that,

−

O(s2). This immediately gives the following

µG,w )(µSN ,w −

µSG,w )T . Then there exists

Cη2 Tr(Σ).

2
aaa
2 ≤
k
k
2e−kx −aaak2/s2
k

x
k

≤

η)(µSN ,w −
Cηs2.

Tr(A)

≤

We will show later

Theorem 3.6.

e−η2ǫ2
1 + ǫ1 −

 

η2ǫ2e2ǫ1

Σ

ΣG,w

(cid:22)

(cid:22)

!

eǫ1Σ.

As will be clear from the proof of Theorem 3.6, when Σ = σ2I is a multiple of identity, then
) samples, we will have

ΣG,w will also be a multiple of I . By Lemma 3.1, if we take m = O( n log n

ǫ2

Suppose, we have

(1

ǫ)ΣG,w

ΣSG,w

(1 + ǫ)ΣG,w .

−

(cid:22)

(cid:22)

(cid:22)

(cid:22)

αΣ

ΣSG,w

βΣ

in the Lowener ordering, for some α, β > 0. By an argument similar to the one sketched in Section
2, we can prove

Lemma 3.7. We will use the notation as deﬁned above. Let W be the bottom n/2 principal
components of the covariance matrix ΣS,w . We have

≤
kmin denotes the least eigenvalue of Σ and δµ := µSN ,w −

Σ
2η ((β + Cη)
k

k2 −

Σ
α
k

kmin) ,

ηPW δµ
k

2
k

µSG,w .

where

Σ
k

By an inductive application of Lemma 3.7, we get the following theorem giving a bound on

µ
k
Theorem 3.8. On input S and the routine OutlierDamping(
), AgnosticMean outputs
·
b
satisfying

.
k
µ

2
µ
k
k

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

b

(cid:0)

b

(cid:1)

14

Theorem 3.6 combined with Theorem 3.8 proves Theorem 1.1. We get a better dependence on
η when Σ = σ2I because we can take α = β in this case. This would lead to the cancellation of
the leading term in the bound in Theorem 3.8 as

Σ
k

k2 =

Σ
k

kmin.

Proof of Lemma 3.7:
have

Recall that Σ denotes the covariance matrix of the Gaussian part. We

ΣS,w = (1
= (1

η)ΣSG,w + ηΣSN ,w + η(1
η)ΣSG,w + A,

−

−

−

η)δµδT
µ

where A = ηΣSN ,w + η(1

η)δµδT

µ. Therefore, we have

−

(1

η)αΣ + A

ΣS,w

(1

η)βΣ + A.

(cid:22)

(cid:22)

−

For a symmetric matrix B, let λk(B ) denote the k’th largest eigenvalue. By Weyl’s inequality,

−

−

we have

Therefore,

By Lemma 3.5 we have

λk((1

η)ΣG,w + A)

λk(A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (ΣS,w )

λn/2 (A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (A)

=

⇒

λn/2(ΣS,w )

Tr(A)
n/2
2C 2η

≤

≤

≤

≤

Σ
k2
k
k2 + 2C 2η
Σ
η)β
(1
k
(β + 2C 2η)
k2.
Σ
k

−

Σ
k

k2

Recall that W is the space spanned by the bottom n/2 eigenvectors of ΣS,w , and P W is the

matrix corresponding to the projection operator on to W . We therefore have

We therefore have

P T

W ΣS,w P W (cid:22)

(β + 2C 2η)
Σ
k

k2I .

αP T

W ΣP W + ηP T

W ΣSN ,w P W + (η

η2)(P W δµ)(P W δµ)T

−

(β + 2C 2η)
Σ
k

k2I .

(cid:22)

Multiplying by the vector P W δµ

kP W δµk and its transpose on either side, we get

Assuming η

1/2, we therefore have

≤

(η

η2)
P W δµ
k

2
k

−

(β + 2C 2η)
Σ
k

≤

k2 −

Σ
α
k

kmin.

ηP W δµ
k

2
k

≤

2η

(β + 2C 2η)
Σ
k

k2 −

Σ
α
k

kmin

.

(cid:0)

(cid:1)

15

Proof of Theorem 3.8:
have

By Equation 6 and Lemma 3.1, since we take O

samples we

n log n
ǫ2

(cid:16)

(cid:17)

µSG,w k
k

2
2 ≤

k2

= O
(cid:0)

η2ǫ2e2ǫ1 + ǫ2
Σ
k
η2 + ǫ2
Σ
k2.
(cid:1)
k
(βη + η2 + ǫ2)
Σ
k

(cid:0)

(cid:1)

(1 + log n) The proof
So it is enough to prove
is by induction. If n = 1, then the conclusion follows from the guarantees of the one dimensional
(cid:0)
median. Now, assume that it holds for all n
1. Let n = k + 1. We have by Lemma
≤
3.7

k for some k

µSG,w k

k2 −

kmin

Σ
k

µ
k

αη

≤

≥

−

O

b

(cid:1)

2

µSN ,w −

ηP W
k
P W µS,w −
(cid:0)

2
µSG,w
k
≤
2
P W µSG,w k
(cid:1)
2 ≤

⇒ k

=

O

O

(βη + η2)
Σ
k
(βη + η2)
Σ
k

k2 −
k2 −

αη

αη

Σ
k
Σ
k

kmin
kmin

(cid:0)

(cid:0)

.

(cid:1)

(cid:1)

By induction hypothesis, since dim(V ) = n/2, we have

µV −
k
Therefore, adding the two, we get

P V µSG,w k

≤

2

(cid:0)

b

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

µSG,w k

−

2

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

(cid:1)

(cid:1)

Proof of Theorem 3.6:

b

(cid:0)

We will ﬁrst consider the second moment

B := Ex exp

x
k

2
k

aaa
−
s2

x x T .

(cid:19)

−

(cid:18)

We have

B =

p

=

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

exp

x T Σ−1x

x x T dx

−

(cid:0)
x T Σ−1x

(cid:1)
x x T dx

=

p

1
(2π)n

p
exp

−

(cid:18)

Z

|

Σ
|
(x

−

−1

2
aaa
s2 +
k
k
1
s2

Σ−1 +

exp

 −

b)T

(cid:18)

1
s4 aaa T

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

I

(x

b)

x x T dx,

(cid:19)

−

(cid:19)

where b = 1
s2

Σ−1 + 1

s2 I

aaa. Therefore, we have

B = exp

(cid:0)

 −

2
aaa
s2 +
k
k

(cid:1)
1
s4 aaaT

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1
Σ−1 + 1

s2 I

Σ−1 +

−1

.

1
s2

I

(cid:19)

!

Σ
|
|

(cid:12)
(cid:12)

(cid:18)

(cid:12)
(cid:12)

16

Now we will look at the scalar term

. Let λi be the eigenvalues of Σ.

Σ
|

|

Σ−1 + 1

s2 I

Σ−1 +

Σ
|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
s2

I

(cid:12)
1
(cid:12)
λi
= Πi (cid:12)
(cid:12)
(cid:12)

+ 1
s2

1
λi

(cid:12)
(cid:12)
(cid:12)

= Πi

1 +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
s2

.

(cid:19)

(cid:18)

We then have

We next bound exp

kaaak2
s2 + 1

s4 aaaT

Σ−1 + 1

s2 I

−1

aaa

−

(cid:16)

Σ
1 + ǫ1 ≤ |

|

Σ−1 +

1
s2

I

(cid:12)
(cid:12)
(cid:12)
(cid:12)

eǫ1.

≤

(cid:12)
(cid:12)
(cid:12)
. We have
(cid:12)

(cid:0)
Σ−1 +

1
s4 aaaT

(cid:18)

1
s2

I

(cid:19)

(cid:17)

(cid:1)

−1

aaa

≤

1
s2 aaa T aaa.

Therefore

Therefore,

exp(

η2ǫ2)

−

exp

≤

 −

2
aaa
s2 +
k
k

1
s4 aaa T

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1.

! ≤

e−η2ǫ2

Σ−1 +

(cid:18)

−1

1
s2

I

(cid:19)

B

(cid:22)

(cid:22)

eǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

Lemma 3.9. We have the following

1
1 + ǫ1

Σ

(cid:22)

(cid:18)

Σ−1 +

−1

1
s2

I

(cid:19)

Σ

(cid:22)

and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors
s2 and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors

Proof. Note that if 1
of Σ−1, then 1
of Σ−1 + 1

λ1 + 1
s2 I . Since,

λ1 , ..., 1
λn
s2 , ..., 1
+ 1
λn

the lemma follows.

From Lemma 3.9, we have

Next we will bound

λi

1 + ǫ1 ≤

1
+ 1

s2 ≤

1
λi

λi

e−η2ǫ2
1 + ǫ1

Σ

B

(cid:22)

(cid:22)

eǫ1Σ.

µG,w = Ex wx x .

17

(5)

µG,w =

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

p

=

exp(

x T Σ−1x )x dx

−

x T Σ−1x

x dx

1
s4 aaaT

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

2
aaa
s2 +
k
k
1
s2

Σ−1 +

=

p

1
(2π)n

p
exp

Z

= exp

−

(cid:18)

 −

exp

 −

|

Σ
|
(x

−

b)T

2
aaa
s2 +
k
k

(cid:18)
1
s4 aaa T

Σ−1 +

(cid:18)

I

(x

b)

x dx

−
−1

(cid:19)

aaa

(cid:19)
1
s2

I

(cid:19)

!

Σ
|

|

1
Σ−1 + 1

s2 I

b,

where b = 1
s2
the two scalars by eǫ1. Therefore, we have

Σ−1 + 1

s2 I

aaa. Recall that ǫ1 = Pi λi
s2

−1

(cid:0)

(cid:1)

. We can, as before, bound the product of

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Therefore, we have

µG,w k
k

2

2 = e2ǫ1 1

s4 aaaT

µG,w k2 ≤
k

eǫ1

Σ−1 +

−1

1
s2

I

(cid:19)

aaa

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−1

1
s2

(cid:18)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−1

(cid:18)

aaa

(cid:19)
−1

Σ−1 +

1
s2

I

Σ−1 +

−1/2

aaa

1
s2

I

(cid:19)

(cid:19)

(cid:18)

Σ−1 +

(cid:18)

Σ−1 +

1
s2

1
s2

I

I

1
s2

I

(cid:18)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1/
Σ
k
k2.
Σ
k

I

1
s2

2
(cid:13)
(cid:13)
−1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
k2 + 1/s2

e2ǫ1 1

s2 aaaT
e2ǫ1 aaaT aaa
s2

η2ǫ2e2ǫ1

= η2ǫ2e2ǫ1

η2ǫ2e2ǫ1

≤

≤

≤

≤

Also, similarly

This implies

µG,w k
k

2
Σ

−1+ 1

s2 I ≤

η2ǫ2e2ǫ1.

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1Σ.

From Lemma 3.9, we have

Combining Equation (6) and Equation 5, we get Theorem 3.6.

18

(6)

3.2

Improving the dependence on η

Now we will show how we can obtain the second part of Theorem 1.1 to get a better dependence
= N (µ, Σ) be a Gaussian with covariance Σ, and
on η by using
c/ log n for a small enough constant c > 0. We ﬁrst use Theorem 1.5 (with ǫ = η) to estimate
η
≤
σ2 =

Σ from Theorem 1.5. Let

σ2 satisfying

D

Σ
k

b
k2. We get a

σ2

σ2

1

O(

η log n)

−

b
(cid:17)
(cid:16)
be the given sample, and let y i ∼
x 1, ..., x m}
Let S =
{
Deﬁne x ′
i = x i + y i. The key thing to note is that if x
′ = N (µ, Σ +
x + y
N (µ, Σ +
σ2I has

D
, and the covariance Σ′ = Σ +

σ2I ). Let

p

≤

≤

∼

(cid:16)

b

p
N (0,

(cid:17)

σ2I ), i = 1, ..., m be i.i.d. samples.
σ2I ), then
N (µ, Σ) and y
∼
∼
′ is same as that of
σ2I ). Note that the mean µ′ of
b
b

N (0,

D

1 + O(

η log n)

σ2.

(7)

D

b
Σ′

λmax

2 + O(

η log n)

b

≤

b
σ2 and λmin

Σ′

We can view x ′
Σ

(cid:16)
(cid:1)
′
η, and we assume η log n
i ∼ D
such that

p

(cid:17)

(cid:0)

′

compute a

≤

1

O(

η log n)

σ2.

(8)

≥

−

(cid:16)

(cid:0)

(cid:1)

p

(cid:17)

c. By Theorem 1.5 and Equation 7, we can

Let α = O

. Therefore,

b
√η log n

′
Σ

Σ′

−

F ≤

(cid:13)
(cid:13)
(cid:13) b

(cid:13)
(cid:13)
(cid:13)

O

η log n

σ2.

(cid:16)p

(cid:17)

(cid:0)

(cid:1)
ασ2I

I

−
1

′

Σ

−

⇒

=
b
=

O

Σ′

(cid:22)
ασ2

Σ

(cid:22)
′−1

′
Σ

+ ασ2I
′−1/2

Σ

b
(cid:22)
η log n
b

′−1/2

Σ′

Σ

′−1/2

I

Σ
b

I + ασ2
′−1/2

(cid:22)
Σ′

Σ

′−1

Σ

(cid:16)

−

⇒

b
(cid:16)p
by Equation 8. Now, if we let x ′′
i =
′′
then we can think of x ′′
η . If we now use Theorem 3.8 with β =
b
µ′′ such that
α =

i ∼ D
on the samples S′′ =

′′ = N (µ′′, Σ′′) = N

, we get a

√η log n

b
i and

b
(cid:22)
(cid:16)

(cid:17)(cid:17)
′−1/2

(cid:16)p
Σ

x ′

Σ

(cid:22)

D

O

(cid:16)

b

b

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

x ′′
i }
{
2 = O(η3/2 log3/2 n).
k

b

µ′′
k

−

µ′′

This implies that

µ =

Σ

′1/2

µ′′ satisﬁes
b

b

b

µ
b
k

−

′

2 = O(
µ
Σ
k
k
Σ
= O(
k
b

η3/2 log3/2 n)
k
k2η3/2 log3/2 n).

b

1 + O

η log n

I

(cid:17)(cid:17)
Σ

µ,

′−1/2

′−1/2

′−1/2

Σ

Σ

1 + O
b
(cid:0)

(cid:0)

√η log n
b

(cid:1)(cid:1)

,

and
(cid:17)

µ with
Remark 3.10. We can use this technique to give a polynomial time algorithm to compute
for any ﬁxed ǫ > 0. This would require estimating
a guarantee
higher order moments by the mean estimation algorithm and then using the above trick to improve
(cid:1)
the η dependence for each of them in sequence. We don’t give a proof of this in this paper.

k2η2−ǫ log2−ǫ n

2 = O
µ
k

Σ
k

µ
k

−

b

(cid:0)

b

19

3.3 Distributions with Bounded Fourth Moments

In this section, we will prove some some useful properties that distributions with bounded fourth
moments satisfy. We will assume that x
for a distribution with mean µ that has bounded
fourth moments, i.e., for every unit vector v

∼ D

E((x

µ)T v )4

C4

E((x

−

≤

−

µ)T v )2

2

,

(9)

for some C4.

Lemma 3.11 (Mean shift). Let X be a random variable with E(X

EX)2 = σ2 and

E(X

EX)4

−

≤

C4

E(X

−

EX)2

2

,

for some C4. Let ǫ

0.5 and A be any event with probability Pr(A) = 1

ǫ. Then

≤

(cid:1)

−

(cid:1)

−

(cid:0)

(cid:0)

Proof. Let a = E(X

A). Then
|

E(X
|

A)
|

−

E(X)

| ≤

4

8C4ǫ3σ.

p

EX = (1

ǫ)a + ǫE(X

−
EX

ǫ)a

−

−

(1
ǫ

Ac)
|
1
=

ǫ

−
ǫ

E(X

Ac) =
|

⇐⇒

(EX

a) + EX

−

The fourth moment of such an X is minimum when its support is just the two-point set
a) + EX

a, 1−ǫ
{

ǫ (EX

−

. Therefore,
}

(1

ǫ)(a

−

−

EX)4 + ǫ

(EX

a)

−

4

≤

(cid:19)

C4σ4

1

ǫ

−
ǫ

C4ǫ3

(cid:18)

=

a

EX

⇒ |

−

4

| ≤

(1

s

−

σ

4

8C4ǫ3σ,

ǫ)(3ǫ2

3ǫ + 1)

≤

−

p

when ǫ

0.5.

≤

Lemma 3.12. Let X be a random variable with EX = µ and E((X

µ)2) = σ2 and let

E(X

µ)4

C4σ2,

−

≤

−

−

for some C4. Then, for every event A that occurs with probability at least 1

ǫ, we have

where 1A is the indicator function of the event A. As an immediate corollary, for ǫ
the following bound on the conditional probability

≤

0.5 we get

E

(X

µ)21A

−

(cid:0)

1

−

≥

(cid:16)

(cid:1)

C4ǫ

σ2,

p

(cid:17)

(10)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

20

Proof. Let dΩ be the probability measure. We can write E(X
following way

−

µ)4

≤

C4(E(X

µ)2)2 in the

−

2

(cid:19)

2

(X

µ)4dΩ +

(X

µ)4dΩ

(X

µ)2dΩ +

(X

µ)2dΩ

ZAc

−

C4

≤

(cid:18)ZA

−

ZAc

−

Using E(Y

EY )4

(E(Y

EY )2)2 for any random variable Y, and Pr(Ac) = ǫ we have

ZA

−

−

≥

−

1
ǫ

2

(X

µ)2dΩ

−

(cid:18)ZAc

≤

ZAc

(cid:19)

(X

µ)4dΩ

−

We therefore have

(cid:0)R

⇐⇒

Ac(X

−
ǫ

µ)2dΩ

2

(cid:1)
µ)2dΩ

(X

−

ZAc
(X

µ)2dΩ

−

(cid:19)
µ)2

E(X

−

≤

≤

+

p
1

(cid:16)

≤

ZA

⇐⇒

C4ǫ

1

−

(cid:16)

p

(cid:17) (cid:18)ZAc
C4ǫ

1

−

(cid:16)

p

(cid:17)

⇐⇒

This proves the inequality (10). Now,

C4

(X

µ)2dΩ +

(X

µ)2dΩ

(cid:18)ZA
C4ǫ

−

(X

−

ZAc
µ)2dΩ +

−

(X

−

(cid:19)
µ)2dΩ

ZAc
µ)2dΩ

(X

−

(cid:19)

−

(X

µ)2dΩ

≤

(cid:19)

ZA

(cid:18)ZA
C4ǫ

−

(X

p

−

(cid:17) (cid:18)ZA

µ)2dΩ

Also,

Therefore, for ǫ

0.5 we get that

≤

E

(X

µ)2

A
|

−

1
µ(A)

1

−

=

≥

(cid:1)

(X

µ)2dΩ

ZA

C4ǫ

−
σ2.

(cid:16)

p

(cid:17)

E

(X

µ)2

A
|

−

=

(X

µ)2dΩ

−

1
µ(A)
1

ZA
σ2.

≤

1

ǫ

−

(cid:1)

(cid:0)

(cid:0)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

As an immediate corollary of Lemma 3.11 and Lemma 3.12, we get for a random variable x

having bounded fourth moments

Corollary 3.13. Let A be an event that happens with probability 1

η. Then,

where Σ

(cid:16)
|A is the conditional covariance matrix Σ

p

(cid:17)

O(

C4η)

Σ

1

−

−

(1 + 2η)Σ,

Σ

|A (cid:22)
(cid:22)
|A := E(x x T

21

A)
|

−

(E(x

A))(E(x
|

A))T .
|

Proof. Let v be any unit vector. Let y be the random variable that is v T x for x
µ. Then
µ = E(y), µA = E(y

. Let

∼ D

A), and d = µA −
|
µA)2
µ
A) = E((y
|

−

E((y

−

By Lemma 3.11 and Lemma 3.12,

d)2

−

A) = E((y
|

= E((y

µ)2
µ)2

A)
|
A)
|

−

−

−

−

2dE(y
d2

µ

A) + d2
|

−

E((y

E((y

µA)2
µA)2

A)
|
A)
|

−

−

−

−

E((y

E((y

−

−

µ)2)
µ)2)

≤

2ηE((y

µ)2)

−
C4ηE((y

≥ −

≥ −

p

C4η +

d2

−

µ)2)

−
8C4η3

E((y

µ)2)

−

(cid:16)p

p

(cid:17)

Finally, by a standard argument as in the proof of Chebyshev’s inequality, we have

Lemma 3.14 (Concentration). For every unit vector v , we have

C4
t4 ,
where σv is the standard deviation of x along the direction v , σ2

x T v
|
(cid:0)

Ex T v

tσv

| ≥

Pr

≤

−

(cid:1)

x T v
v := E
|

2
|

− |

Ex T v

2.
|

3.4 Proof of Theorem 1.3:

3.4.1 One Dimensional Distribution

First we will consider the case when X is a random variable with mean µ and variance σ2 satisfying

E((X

µ)4)

C4σ4.

−

≤

In this case, median need not be a good estimator. Instead, we will consider the interval of minimum
η) fraction of the sample points. Let S be the given sample,
length that contains (1
and let
S) be our estimator. We will show
below that
e

−
S be the points lying in this interval. Let

By the concentration inequality stated in Lemma 3.14, we get that for the distribution, the

(η + ǫ)3/4σ

µ = mean(

ǫ)(1

µ
|

| ≤

−

−

−

O

µ

b

e

η

.

C 1/4
4
(cid:16)

(cid:17)

of the interval around µ consisting of probability mass 1

is bounded by

length r1− η+ǫ
b
2

η+ǫ
2

−

We will refer to this interval by I1− η+ǫ
then with probability 1

2

1/ poly(n) for every interval I

. We note that by VC theorem if

SD|
|

= Ω

log n+log 1/ǫ
ǫ2

,

(cid:16)

(cid:17)

−

Pr (x
∈
|
The length of the smallest interval that contains (1
length of the smallest interval that contains 1

Pr (x

∼ D

−

∈

x

η

I

)

|

|
η

η) fraction of S is at most the
ǫ fraction of SD. This latter quantity is bounded

ǫ)(1

−

−

−

R,

⊂
x ǫu SD)

I

ǫ/2.

| ≤

r1− η+ǫ

2 ≤

C 1/4
4
η+ǫ
2

σ.

1/4

(cid:0)

(cid:1)

−

−

22

by r1−η, since the interval I1− η+ǫ
SD.

2

contains with probability 1

1/ poly(n) a (1

η

ǫ) fraction of

−

−

−

This implies that when we look at the minimum interval containing 1

η

noise points, the extreme points of the interval can be at most at a distance r1− η+ǫ

−

−

ǫ fraction of the non-
from µ. Thus,

2

the distance of all noise points will be within O

. Furthermore, the interval of minimum

η

length with (1
by Lemma 3.11 the mean of
from the true mean.

ǫ)(1

−

−

−

e

η) fraction of S will contain at least 1

S will be within η

r1−η + O

3η
C4(η + ǫ)3σ

−

−

4

ǫ fraction of SD. Therefore,
C 1/4
4
(cid:16)

(η + ǫ)3/4σ

= O

(cid:17)

(cid:17)

(cid:16)

p

1/4
C
4
(η+ǫ)1/4 σ

(cid:19)

(cid:18)

·

3.4.2 Multi-dimensional Case

We will now consider the multidimensional case. Let
random variable that satisﬁes for every direction v

D

be a distribution on Rn and x

is a

∼ D

E(((x

µ)T v )4)

C4

E(((x

−

≤

−

µ)T v )2)

2

,

for some C4.

(cid:0)
For any direction v , let µv = µT v. From the previous section, we know that we can ﬁnd a

(cid:1)

µv

such that

Therefore, by picking n orthogonal directions v 1, ..., v n, we get

µv
|

µv| ≤

−

O(C 1/4
4

(η + ǫ)3/4σv ).

Lemma 3.15. Given O
O(C 1/4
4

(η + ǫ)3/4

(cid:16)
Tr(Σ)).

n log n
ǫ2

b

(cid:17)

samples, we can ﬁnd a vector aaa

Rn such that

∈

aaa
k

µ

k2 =

−

We will now bound the radius of the ball in the outlier removal step (Algorithm 2). We claim

p

the radius of the ball is O

. Suppose we have some x

. Let z = x

||
Using the n orthogonal directions as picked above, let zi = z T v i and let Z 2 =
Consider the following:

∼ D

p

(cid:18)

(cid:19)

1/4
C
4
(η+ǫ)1/4

n

Σ

||2

z2
i =

P

Pr

Z 2

 

≥

C 1/2
Σ
4 n
||
(η + ǫ)1/2 !

||2

= Pr

Z 4

C4n2

Σ
||
η + ǫ

2
2
||

≥

(η + ǫ)E(Z 4)
C4n2

Σ

2
2
||

||

≤

(cid:19)

It suﬃces to bound the right-hand side of (11) by O(η + ǫ), in which case the ball will contain
1

ǫ fraction of the probability mass of

. We have

η

−

−

E(Z 4) = E

z2
i



Xi

Xj

z2
j 

≤

n2 max
i

E

z4
i

C4n2

Σ
k

2
2
k

≤

(cid:0)

(cid:1)


due to the fourth moment condition and the fact that E((z T v i)2)



1/4
C
4
(η+ǫ)1/4

Tr(Σ)

(cid:18)

n

k2. Therefore, a ball of
aaa
k2 =
k
p
, we get that the radius of the ball computed in the outlier removal step

≤ k
ǫ fraction of the points. Since

contains 1

||2

Σ

Σ

−

−

−

(cid:19)

µ

||

η

radius at most O

O

C 1/4
4
(cid:16)
is O

(η + ǫ)3/4
1/4
C
4
(η+ǫ)1/4

n

(cid:18)

p
Σ
k

k2

(cid:19)

p

(cid:17)

. We have proved

(cid:18)

D

23

b

µ.

2
2.
k

−
z

k

(11)

(12)

Lemma 3.16. After the outlier removal step, every remaining point x satisﬁes

x
k

−

µ

k2 ≤

O

 

C 1/4
4
(η + ǫ)1/4

n

Σ
k

k2

.

!

p

Consider the covariance matrix Σ

S (recall that

S be the set of points in

SD ⊂
points sampled by the adversary. Let µ
e
Note that

S that were sampled from the distribution
e
S), µ
SN
e

S := mean(
e

:= mean(

e

e

e

S is the sample after outlier removal). Let
S be the
SD).

and
D
SN ) and µ

SN ⊂
SD := mean(
e
e

e

eS of

µ

S =
e

ηµ

e
+ (1
−

η)µ

SD ,
e

SN
e

e

η = | eSN |
| eS|

is the fraction of noise points after the outlier truncation step. Note that
e

where
≤
η
1−2η−ǫ = O(η). We will therefore pretend that the fraction of noise points is still η after the outlier
is µ = 0. By Lemma 3.11
truncation step. We again assume that the mean of the distribution
applied with X = x T µ eD
and where A is the event that x is not removed by outlier
removal, we have that

kµ eDk for x

∼ D

D

e

e

e

η

e

(13)

Suppose, after the outlier removal step, we had the guarantee that the covariance matrix of the

remaining points from the distribution

, say Σ

D

µ
k

eDk2 = O(C 1/4

4

k2).

(η + ǫ)3/4

Σ
k
p
eD, is between
β(1
Σ

η)Σ

α(1

η)Σ

−

(cid:22)

eD (cid:22)

−

in the Lowener ordering. Corollary 3.13 gives α = 1

by Lemma 3.1 and Lemma 3.16 we have that if

O(

−
= Ω

C4(η + ǫ)) and β = 1 + O(η + ǫ). Also,
n log n
ǫ2

, then

S
|

eD|

p
(cid:16)

ǫ

C 1/4
4
(η + ǫ)1/4

(1

−

)Σ

eD (cid:22)

ΣS eD (cid:22)

(1 +

(cid:17)
C 1/4
4
(η + ǫ)1/4

ǫ

)Σ

eD

We will use the notation as deﬁned above.

Lemma 3.17. Let W be the bottom n/2 principal components of the covariance matrix ΣS. For
some constant C, we have

where δµ = µ

µ

eSN −

ηP W δµ
k
eSD .

2
k

≤

O

(βη + C 1/2
(cid:16)

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

,

(cid:17)

By an inductive application of the above lemma, we can prove

Theorem 3.18. On input (S, n), AgnosticMean outputs

µ satisfying

µ
k

2
k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
b
k

kmin

(1 + log n).

(cid:17)

(cid:16)

b

24

Theorem 3.18 with Corollary 3.13 proves Theorem 1.3.

Proof of Lemma 3.17:
have

Recall that Σ denotes the covariance matrix of the points from

. We

D

Σ

eS = (1
= (1

η)Σ

η)Σ

eSD + ηΣ
eSD + A,

−

−

+ (η

−

eSN

η2)δµδT
µ

where A := ηΣ

+ (η

SN
e

−

η2)δµδT

µ. Therefore, we have

(1

η)αΣ + A

−

(1

η)βΣ + A.

−

Σ

(cid:22)

eS (cid:22)
1/4
C
4
(η+ǫ)1/4

By Lemma 3.16 each x i satisﬁes

x ik
k

= O

(cid:18)
η√C4k
Σ
√η + ǫ

k2n

, so we have

n

Σ
k

k2

(cid:19)

p

O

≤

C4η

Σ
k

k2n

.

Tr(A) = O

(cid:19)
For a symmetric matrix B, let λk(B) denote the k’th largest eigenvalue. By Weyl’s inequality,

(cid:16)p

(cid:18)

(cid:17)

(14)

λk((1

η)Σ

S + A)
e

≤

−

λk(A) + (1

η)β

Σ
k

k2.

−

≤
By Equation (14), there exists a constant

(cid:0)

(cid:1)

C such that

λn/2

Σ
S
e

λn/2 (A) + (1

η)β

Σ
k

k2.

−

we have that

Therefore,

we have,

e
λn/2 (A)

Tr(A)
n/2

≤

≤

C

C4η

Σ
k

k2,

p

e

eS)
Recall that W is the space spanned by the bottom n/2 eigenvectors of Σ
corresponding to the projection operator on to W . We therefore have

λn/2(Σ

k2 +

Σ
k

Σ
k

C4η

η)β

k2

p

(1

−

≤

C

e

eS, and P W is the matrix

(1

η)αP T

W ΣP W + ηP T

W Σ

−
Multiplying by the vector P W δµ

eSN

P T

W Σ

eSP W (cid:22)
P W +(η

((1

−

η)β +

Σ
C4η)
C
k
η2)(P W δµ)(P W δµ)T
e

p

k2I
(cid:22)

−

kP W δµk and its transpose on either side, we get

η

P W δµ
k

2
k

≤

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin.

((1

η)β +

C

−

C4η)
Σ
k

k2I

p

e

where C = eC

1−η . We therefore have

ηP W δµ
k

2
k

≤

η

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin

.

(cid:16)

(cid:17)

p

p
25

2. The proof is by
Proof of Theorem 3.18:
By Equation 13, it is enough to bound
induction on the dimension. If n = 1, then the conclusion follows from the guarantees for the one
1.
dimensional case proven in Section 3.4.1. Now, assume that it holds for all n
Let n = k + 1. We have by Lemma 3.17

k for some k

µ
k

eSD k

≤

−

≥

µ

b

P W
k

µ

µ

eSD

−

2 =
k

(cid:16)

b

(cid:17)

ηP W δµ
k
O

2
k
(βη + C 1/2
(cid:16)

≤

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(cid:17)

Recall that we deﬁned V to be the span of the top n/2 principal components of Σ

S. By
e

induction hypothesis, since dim(V ) = n/2, we have

µV −
k

P V µ

2
eSD k

≤

O

(βη + C 1/2
(cid:16)

4

Therefore, adding the two, we get

b

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

−

µ

2
eSD k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

(cid:17)

(cid:17)

b

(cid:16)

4 Covariance Estimation

4.1 One Dimensional Case

Observation 4.1 (1d Covariance Estimate).

1. Let

be a distribution with mean µ and co-

D
= N (µ, σ2), then there is an algorithm that takes as input m = Ø

log n
ǫ2

variance σ2. If
samples x 1, ...x m ∼ Dη and computes in polynomial time

D

σ2 such that

σ2

σ2

= O(η +ǫ)σ2.
(cid:16)
(cid:17)

−

(cid:12)
(cid:12)

b

(cid:17)

−

−

σ2

(cid:16)
σ2

(cid:12)
(cid:12)b

= O

∼ D

2. If x

log n+log 1/ǫ
ǫ2

has bounded fourth moments with constant C4, and (x

µ)2 has bounded fourth
(cid:12)
(cid:12)b
moments with constant C4,2. Then there is an algorithm that takes as input η and m =
σ2 such that
samples x 1, ...x m ∼ Dη and computes in polynomial time
O
4,2 (η + ǫ)3/4C 1/2
C 1/4
4 σ
(cid:16)

= N (µ, σ), and we are given m = poly(n) samples S =

(cid:12)
is supported on R, we can estimate the variance in the following
Proof. When the distribution
(cid:12)
just having bounded eighth moments separately.
= N (µ, σ) and
way. We will consider the case
Suppose
, xi ∼ Dη. There are
x1, ..., xm}
{
D
several ways to estimate σ, we describe here one of them. First we compute the median, and let
85.1. Let Cσ be the
xmed = mediani{
µ. By Lemma 3.3,
c1’th quantile of S. Then our estimate for the standard deviation is
O(C 1/4
8 ησ2).
µ
= O(ησ). For a similar reason, Cσ = σ
we have
±
is a distribution that has bounded eighth moments, the result follows from the 1d

. Let Φ(x) be the c.d.f. of N (0, 1). Note that c1 = Φ(1)

∼
σ = Cσ −

O(ησ). Therefore,

xi}

D
D

±

D

(cid:17)

b

|

.

mean estimation in Section 3.4 applied (x

µ)2. Note that E(x

σ2 = σ2
b
µ)2 = σ2 and

c

b

µ
|
When
b

−
D

E

(x

µ)2

2

σ2

= E(x

µ)4

−

−

−

(cid:0)

−
σ4

−

−
C4σ4.

(cid:1)

≤

26

From Section 3.4, we therefore have that if m = O
C 1/4
4,2 (η + ǫ)3/4C 1/2
4 σ
(cid:16)

σ2
|

| ≤

σ2

−

O

(cid:17)

.

(cid:16)

b
4.2 Multi-Dimensional Case: Theorem 1.5

log n+log 1/ǫ
ǫ2

(cid:17)

, there is a poly(n) algorithm with

In this section we will prove that CovarianceEstimation (Algorithm 4) gives Theorem 1.5.
is a distribution with mean µ and covariance Σ
Throughout this section, we will assume that
and has bounded fourth moments with parameter C4. We use the following symmetrization trick
to assume that

has mean 0. Given samples S =

, let

D

x 1, ..., x m}
{

D

x ′

i =

x i −

x i+m/2
√2

for i

.
1, ..., m/2
}

∈ {

(15)

Since η fraction of the original samples were corrupted on average, only 2η fraction of the new
samples will be corrupted on average. Moreover, if x , y
are independent random variables,
then we can show that the distribution of x ′ = (x
y)/√2 has bounded fourth moments with
′ the distribution of x ′. CovarianceEstimation
parameter
is just the mean estimation algorithm on S(2) =
, we can appeal to Theorem 1.3.
}
Furthermore, let
Note that

′ be an aﬃne transformation of a 4-wise independent distribution.

C4 + 3/2. We will denote by

x ′x ′T
{

∼ D

x
|

−

≤

D

D

∈

S

Ex ∼D′x x T = Σ.

By Theorem 1.3, we have

Σ
k

Σ

kF = O

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

Σ(2)
k

1/2
2
k

log1/2 n,

b
where Σ(2) is covariance matrix of x x T , x

By Proposition 4.2 , we have

′.

∼ D

Σ
k

kF = O
Σ

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

C 1/2
4 k

Σ

k2 log1/2 n,

which proves Theorem 1.5.

b

(cid:17)

(cid:17)

We will now derive a bound for

Σ(2)
k

k2 when the distribution has bounded fourth moments

and is 4-wise independent. In particular, we will prove

Proposition 4.2. If Σ(2) is the covariance matrix of x x T , x

′, it holds that

∼ D

Σ(2)
k

k2 ≤

O

Σ
C4k

2
2
k

.

(cid:0)

(cid:1)

27

Proof of Proposition 4.2:

Note that E(Y ) = Σ.

E(((Y

E(Y ))

V )2) = E

−

·

2

(Y ij −



Xij

E((Y ij −

Σij)Vij

Σij)(Y kl −

Σkl))VijVkl

E(Y ijY kl −

ΣijΣkl)VijVkl

E(xixjxkxl −

ΣijΣkl)VijVkl.

=

=

=

Xijkl

Xijkl

Xijkl



Next we note that

Therefore,

E(xixjxkxl)

−

ΣijΣkl = 


Σ2

ii if i = j = k = l

E(x4
i )
−
E(x2
i x2
j ) if i = k, j = l or i = l, j = k
0 otherwise.

max
V :kV kF =1

E(((Y

E(Y ))

V )2) = max

−

·

V :kV kF =1

(E(x4
i )

Σ2

ii)V 2

ii + 2

ΣiiΣjjV 2
ij

(E(x4
i )

2Σ2

ii)V 2

ii +

ΣiiΣjjV 2
ij

−

−

Xi<j

Xi,j

2Σ2

ii + max

Σ2
ii.

i

Xi

= max

V :kV kF =1

max
i
O (C4)

Xi
E(x4
i )
−
2
2.
k

Σ
k

≤

≤

5 Estimating

k2: Theorem 1.6
Σ

k

As in Section 4.2, we assume that the true distribution has mean µ = 0.

SN be the given sample, where SD consists of points from some distribution

In this section, we will prove AgnosticOperatorNorm (Algorithm 5) gives Theorem 1.6. Let
with mean
S = SD ∪
µ and covariance Σ and SN consists of points picked by the adversary. Let ΣSD be the sample
has 1D concentration, i.e., there exists a constant γ such that
covariance of SD. We assume that
for every unit vector v

D

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

(cid:17)

S be the remaining sample at the end of the algorithm and let

SD be points in

S sampled from

5.1 Correctness

Let
.

D

e

e

e

28

Deﬁnition 5.1. Given a set of points S

Rn and a vector aaa

Rn, we let

Σaaa (S) :=

(x

aaa)(x

−

−

⊂

1
S
|

x ∈S
| X

∈

aaa)T .

First, we will argue that the covariance of the true distribution is well-approximated by Σµ(

SD).

Lemma 5.2. With probability 1

1/ poly(n),

−

e

Σ
k

−

Σ0(

SD)

η

Σ
k

k

k ≤

Proof. First, note that the t computed in SafeOutlierTruncation is at most O(Tr(Σ)) because
e
σ2
v (namely that the estimated
by an analogous argument as in Section 4.1, we have
v ≤
σv in a direction v is close to the true variance σv in that direction). Then the ball in
variance
SafeOutlierTruncation has radius R = c1
Tr(Σ) log1/γ n
η for some constant c1. We have that
b
deviates from the mean by more than c1σv log1/γ n
b
in any direction v , the probability that x
p
η
∼ D
is 1/ poly( n
η ). Then if we take n orthogonal directions, the probability that any given point is
more than distance R from µ is still 1/ poly( n
η ). Thus, step (1) of the algorithm will remove only
1/ poly( n
η ) fraction of the points sampled from

(1 + O(η))σ2

.

In every direction v , the probability mass of points from

outside an interval of size c2σv log1/γ n
η
around the mean is at most 1/ poly( n
η ), where σv is the variance in the direction v . Let Ci be the
region between the two hyperplanes used for truncation in iteration i. Therefore, if the number of
iterations is O(n log2/η n

1/ poly( n

η ), we will have that Pr (x

) = 1

η ).

ﬁnite k. By Lemma 3.12, we have that the covariance matrix Σ0 (
that of Σ:

Note that 1d concentration implies that the distribution has bounded 2k’th moment for all
D ∩i Ci is close to
(16)

(1 + 1/ poly(

1/ poly(

))Σ

x
∈ ∩iCi |

∼ D

(1

D

−
D ∩i Ci) of
n
))Σ.
η

D ∩i Ci)

(cid:22)

Σ0 (

n
η

(cid:22)

−

D

D ∩i Ci) to Σ0

, we use Proposition 3.2. The concept class we use is
Finally, to relate Σ0 (
all degree two polynomials restricted to convex polytopes with at most O(n) facets, deﬁned by
e
the hyperplanes used for truncation at each iteration of the algorithm. The VC dimension of this
concept class is O(n2 log n). Therefore, by Proposition 3.2 applied with R = c1

Tr(Σ) log1/γ n

SD

(cid:16)

(cid:17)

Σ
c1k

1/2n1/2 log1/γ n
k

η , we get that if we take m = O

n3(log1/γ n
η2

η )2 log n

then

p

(cid:18)

(cid:19)

Σ0
k

SD

Σ0 (

D ∩i Ci)

−

k ≤

Σ
η/2
k

.
k

(cid:16)

(cid:17)

Combining equations 16 and 17 we get the desired result.

e

Theorem 5.3. When the algorithm terminates, we have:

(1

Σ
η)
k

−

k2 ≤ k

Σ0(

S)

k2 ≤

(1 + O(η log2/γ n
η

Σ
))
k

k2.

Proof. First, note that since only an η fraction of

S is noise, we have

e

Σ0(

S)

(1

η)Σ0(
e
−

(cid:23)

SD)

e

29

e

η ≤

(17)

(18)

Therefore, we have that

Σ0(
k
bound. For the upper bound, let v be the top eigenvector of Σ0(
we have

Σ0(
η)
k

k2 ≥

SD)

S)

(1

−

e

e

k2. Lemma 5.2 gives the desired lower
S). When the algorithm terminates,

Σ0(
k

S)

k2 = v T Σ0(

e

S)v
(1 + O(η log2/γ n
e
η
(1 + O(η log2/γ n
η

≤

≤

))v T Σv

Σ
))
k

k2.

e

where the second line follows because of the termination condition and because we can estimate
the variance of

in any direction to within a (1

cη) factor.

D

±

5.2 Termination

In this section, we will show that with high probability, Algorithm 5 terminates in a polynomial
1
number of steps provided that η
C for some constant C that depends only on the estimation in
Step (5).

≤

Every time the algorithm goes through another iteration, it must remove a certain number of
noise points. Suppose in step (7), we remove r noise points. The noise conﬁguration of maximum
r
variance puts r amount of noise at the outlier removal distance d1 = c1

Tr(Σ) log1/γ n

c2

σv log1/γ n
η
b
2

. We can then write an upper

p

η , and η

−

amount of noise at the truncation threshold distance d2 =
bound on σ2.

σ2 =

(1
k

−

η)Σ0(

SD) + ηΣ0(

2
SN )
2 ≤
k

v + rd2
σ2

1 + (η

r)d2
2

−

This implies

e

e

Let us simplify the numerator Z = σ2
(1 + c3η log2/γ n
σ2
η )
v ≤
is less than some constant.

σ2. Here we also assume that η

σ2

r

≥

σ2
v −

−

ηd2
2

σ2
v −
d2
2

−
d2
1 −
ηd2
2. Since we are truncating the sample, we have
1
1−cη

1
C for a suﬃciently large C so that

≤

1

b

Z

σ2

≥

−

σ2
1 + c3η log2/γ n

+ η

2 log2/γ n
c2
η
4

!

1

cη

η  

−
cη + (cη)2 + η

2 log2/γ n
c2
η
4

1 + c3η log2/γ n
η

c3η log2/γ n

η −

(cη)2
η
1 + c3η log2/γ n
η

−

2 log2/γ n
c2
η
4

σ2

σ2

≥

−

1

−





σ2

≥















30

Recall that σ2

(1

Σ
η)
k

k2 by (18). Then as long as c3 is a suﬃciently large constant, we have

−

≥

Then combining Z with the denominator from earlier and using the fact that d1 ≤
we get:

c1

n

Σ
k

k2 log1/γ n
η ,

p

Z

≥

Σ
k2
k
4  

c3η log2/γ n
η
1 + c3η log2/γ n

η !

r

≥

Σ
k

c3η log2/γ n/η
1+c3η log2/γ n

k2
(cid:18)
4c2
Σ
1k

η (cid:19)
k2n log2/γ n
η
c3η
1n(1 + c3η log2/γ n
η )

≥

4c2

Then r

≥
iterations.

O

min

(cid:18)

(cid:26)

η
n ,

1
n log2/γ n

η (cid:27)(cid:19)

Open Questions

, so the algorithm will terminate in a nearly linear number of

An immediate open question is whether the our analysis of the mean estimation algorithm is
tight and the √log n is avoidable. For special distributions including Gaussians, [DKK+16] give
η rather than η√log n or √η log n
an algorithm with higher sample complexity and error η
as in Theorem 1.1. An open question is to give an O(η) approximation. For the more general
distributions considered here, the dependence on η must grow as at least η3/4; it is open to ﬁnd an
algorithm that achieves O(η3/4) error (our guarantee for the general setting has error O(√η log n)).
Other open problems include agnostic learning of a mixture of two arbitrary Gaussians and agnostic
sparse recovery.

log 1

q

We thank Chao Gao and Roman Vershynin for helpful discussions. We would also like to thank the
anonymous reviewers for useful suggestions. This research was supported in part by NSF awards
CCF-1217793 and EAGER-1555447.

Acknowledgment

References

[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with
unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In
NIPS, pages 2384–2392, 2012.

[AK01]

Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Pro-
ceedings of the thirty-third annual ACM symposium on Theory of computing, pages
247–257. ACM, 2001.

31

[BCV13]

[Bru09]

Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of
tensor decompositions with applications to polynomial identiﬁability. arXiv preprint
arXiv:1304.8087, 2013.

S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In Proceedings of
the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2009,
New York, NY, USA, January 4-6, 2009, pages 1078–1087, 2009.

[Bru11]

David Bruce. A multivariate median in banach spaces and applications to robust pca.
http://www-personal.umich.edu/~romanv/students/bruce-REU.pdf, 2011.

[BRV13] Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the

presence of Gaussian noise. In Proc. of COLT, 2013.

[BS10]

[BV08]

[Car98]

Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In 51th
Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 103–112, 2010.

S Charles Brubaker and Santosh S Vempala. Isotropic PCA and aﬃne-invariant clus-
tering. In Building Bridges, pages 241–281. Springer, 2008.

J-F Cardoso. Multidimensional independent component analysis. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference
on, volume 4, pages 1941–1944. IEEE, 1998.

[CGR15] M. Chen, C. Gao, and Z. Ren. Robust Covariance Matrix Estimation via Matrix Depth.

ArXiv e-prints, June 2015.

[CJ10]

Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation.
Academic Press, 2010.

[CLMW11] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal com-

ponent analysis? J. ACM, 58(3):11:1–11:37, June 2011.

[CR08]

Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using
correlations and independence. In Proc. of COLT, 2008.

[CSPW11] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-
SIAM Journal on Optimization,

sparsity incoherence for matrix decomposition.
21(2):572–596, 2011.

[Das99]

[Dav87]

[DG92]

Sanjoy Dasgupta. Learning mixtures of Gaussians. In Foundations of Computer Sci-
ence, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999.

P. L. Davies. Asymptotic behaviour of s-estimates of multivariate location parameters
and dispersion matrices. Ann. Statist., 15(3):1269–1292, 09 1987.

David L. Donoho and Miriam Gasko. Breakdown properties of location estimates based
on halfspace depth and projected outlyingness. Ann. Statist., 20(4):1803–1827, 12 1992.

32

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the computational
intractability. CoRR, abs/1604.06443, 2016.

[Don82]

[DS07]

David L. Donoho. Breakdown Properties of Multivariate Location Estimators. PhD
thesis, Harvard University, 1982.

Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of
separated, spherical Gaussians. The Journal of Machine Learning Research, 8:203–226,
2007.

[FJK96]

Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In
FOCS, pages 359–368, 1996.

[GVX14]

Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decom-
position. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,
pages 584–593. ACM, 2014.

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment
methods and spectral decompositions. In ITCS, pages 11–20, 2013.

[HKO01]

Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis.
Wiley, 2001.

[HPL91]

Peter J. Rousseeuw Hendrik P. Lopuhaa. Breakdown points of aﬃne equivariant es-
timators of multivariate location and covariance matrices. The Annals of Statistics,
19(1):229–248, 1991.

[HRRS11] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.
Robust statistics: the approach based on inﬂuence functions, volume 114. John Wiley
& Sons, 2011.

Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.,
35(1):73–101, 03 1964.

Peter J. Huber.
Statistics, pages 1248–1251. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.

International Encyclopedia of Statistical Science, chapter Robust

[KMV10] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Eﬃciently learning mixtures
of two Gaussians. In Proceedings of the 42nd ACM symposium on Theory of computing,
pages 553–562. ACM, 2010.

[KRV]

https://github.com/kal2000/AgnosticMeanAndCovarianceCode.

[KV09]

Ravi Kannan and Santosh Vempala. Spectral Algorithms. Now Publishers Inc, 2009.

Nojun Kwak. Principal component analysis based on l1-norm maximization. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672–1680, 2008.

Ricardo Antonio Maronna. Robust m-estimators of multivariate location and scatter.
Ann. Statist., 4(1):51–67, 01 1976.

[Hub64]

[Hub11]

[Kwa08]

[Mar76]

33

[MMY06] RARD Maronna, Douglas Martin, and Victor Yohai. Robust statistics. John Wiley &

Sons, Chichester. ISBN, 2006.

[MSY92]

Ricardo A. Maronna, Werner A. Stahel, and Victor J. Yohai. Bias-robust estimators
of multivariate scatter based on projections. J. Multivar. Anal., 42(1):141–161, July
1992.

[MT+11] Michael McCoy, Joel A Tropp, et al. Two proposals for robust pca using semideﬁnite

programming. Electronic Journal of Statistics, 5:1123–1160, 2011.

Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures
of Gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE
Symposium on, pages 93–102. IEEE, 2010.

Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion
for high-dimensional datasets. Technometrics, 2012.

Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH
and NTRU signatures. J. Cryptology, 22(2):139–160, 2009.

J. R. Kettenring S. J. Devlin, R. Gnandesikan. Robust estimation of dispersion ma-
trices and principal components. Journal of the American Statistical Association,
76(374):354–362, 1981.

[Sma90]

Christopher G Small. A survey of multidimensional medians. International Statistical
Review/Revue Internationale de Statistique, pages 263–277, 1990.

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics, 12(4):389–434, 2012.

John W. Tukey. Mathematics and the Picturing of Data. In Ralph D. James, editor,
International Congress of Mathematicians 1974, volume 2, pages 523–532, 1974.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models.
Journal of Computer and System Sciences, 68(4):841–860, 2004.

Santosh Vempala and Ying Xiao. Max vs min: Tensor decomposition and ICA with
nearly linear sample complexity. In Proceedings of The 28th Conference on Learning
Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1710–1723, 2015.

[XCM10] Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis
with contaminated data: The high dimensional case. arXiv preprint arXiv:1002.4658,
2010.

[MV10]

[MZ12]

[NR09]

[SJD81]

[Tro12]

[Tuk74]

[VW04]

[VX15]

34

Agnostic Estimation of Mean and Covariance

Kevin A. Lai∗

Anup B. Rao∗

Santosh Vempala∗

August 16, 2016

Abstract

We consider the problem of estimating the mean and covariance of a distribution from iid
samples in Rn, in the presence of an η fraction of malicious noise; this is in contrast to much
recent work where the noise itself is assumed to be from a distribution of known type. The
agnostic problem includes many interesting special cases, e.g., learning the parameters of a single
Gaussian (or ﬁnding the best-ﬁt Gaussian) when η fraction of data is adversarially corrupted,
agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time
algorithms to estimate the mean and covariance with error guarantees in terms of information-
theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value
Decomposition.

6
1
0
2
 
g
u
A
 
4
1
 
 
]
S
D
.
s
c
[
 
 
2
v
8
6
9
6
0
.
4
0
6
1
:
v
i
X
r
a

∗Georgia Tech. Email: {kevinlai, anup.rao, vempala}@gatech.edu

1

Introduction

The mean and covariance of a probability distribution are its most basic parameters (if they are
bounded). Many families of distributions are deﬁned using only these parameters. Estimating the
mean and covariance from iid samples is thus a fundamental and classical problem in statistics.
The sample mean and sample covariance are generally the best possible estimators (under mild
conditions on the distribution such as their existence). However, they are highly sensitive to noise.
The main goal of this paper is to estimate the mean, covariance and related functions in spite of
arbitrary (adversarial) noise.

Methods for eﬃcient estimation, in terms of sample complexity and time complexity, play
an important role in many algorithms. One such class of problems is unsupervised learning of
generative models. Here the input data is assumed to be iid from an unknown distribution of a
known type. The classical instantiation is Gaussian mixture models, but many other models have
been studied widely. These include topic models, stochastic block models, Independent Component
Analysis (ICA) etc. In all these cases, the problem is to estimate the parameters of the underlying
distribution from samples. For example, for a mixture of k Gaussians in Rn, it is known that
the sample and time complexity are bounded by nO(k) in general [KMV10, MV10, BS10] and by
poly(n, k) under natural separation assumptions [Das99, AK01, VW04, DS07, CR08, BV08, HK13].
For ICA, samples are of the form Ax where A is unknown and x is chosen randomly from an
unknown (non-Gaussian) product distribution; the problem is to estimate the linear transformation
A and thus unravel the underlying product structure [FJK96, NR09, Car98, HKO01, CJ10, BRV13,
AGMS12, BCV13, GVX14, VX15]. These, and other models (see e.g., [KV09]), have been a rich
and active subject of study in recent years and have lead to interesting algorithms and analyses.

The Achilles heel of algorithms for generative models is the assumption that data is exactly from
the model. This is crucial for known guarantees, and relaxations of it are few and specialized, e.g.,
in ICA, data could by noisy, but the noise itself is assumed to be Gaussian. Assumptions about rank
and sparsity are made in a technique that is now called Robust PCA [CSPW11, CLMW11, XCM10].
There have been attempts [Kwa08, MT+11] at achieving robustness by L1 minimization, but they
don’t give any error bounds on the output produced. A natural, important and wide open problem
is estimating the parameters of generative models in the presence of arbitrary, i.e., malicious noise,
a setting usually referred to as agnostic learning. The simplest version of this problem is to estimate
a single Gaussian in the presence of malicious noise. Alternatively, this can be posed as the problem
of ﬁnding a best-ﬁt Gaussian to data or agnostically learning a single Gaussian. We consider the
following generalization:

Problem 1 [Mean and Covariance] Given points in Rn that are each, with probability 1
η
from an unknown distribution with mean µ and covariance Σ, and with probability η completely
arbitrary, estimate µ and Σ.

−

There is a large literature on robust statistics (see e.g., [Hub11, HRRS11, MMY06]), with the
goal of ﬁnding estimators that are stable under perturbations of the data. The classic example for
points on a line is that the sample median is a robust estimator while the sample mean is not (a single
data point can change the mean arbitrarily). One measure for robustness of an estimator is called
breakdown point, which is the minimum fraction of noise that can make the estimator arbitrarily
bad. Robust statistics have been proposed and studied for mean and covariance estimation in high

1

dimension as well (see [Hub64, Tuk74, Mar76, SJD81, Don82, Dav87, HPL91, DG92, MSY92, MZ12,
CGR15] and the references therein). Most commonly used methods (including M-estimators) to
estimate the covariance matrix were shown to have very low break down points [Don82]. The
notion of robustness we consider quantiﬁes how far the estimated value is from the true value. To
the best of our knowledge, all the papers either suﬀer from the diﬃculty that their algorithms are
computationally very expensive, namely exponential time in the dimension, or have poor or no
guarantees for the output. Tukey’s median [Tuk74]) is an example of the former. It is deﬁned as
x i}i. As proven in [CGR15], this is an
the deepest point with respect to a given set of points
{
optimal estimate of the mean. But there is no known polynomial time algorithm to compute this.
Another well-known proposal (see [Sma90]) is the geometric median:

arg min

y

y
k

x ik2.

−

Xi
This has the advantage that it can be computed via a convex program. Unfortunately, as we
observe here (see Proposition 2.1), the error of the mean estimate produced by this method grows
polynomially with the dimension (also see [Bru11]).

This leads to the question, what is the best approximation one can hope for with η arbitrary
(adversarial) noise. From a purely information-theoretic point of view, it is not hard to see that
even for a single Gaussian N (µ, σ2) in one dimension, the best possible estimation of the mean will
have error as large as Ω(ησ), i.e., any estimate ˜µ can be forced to have
= Ω(ησ). For a more
general distribution, this can be slightly worse, namely, Ω(η3/4σ) (see Section 2.1). What about in
Rn? Perhaps surprisingly, but without much diﬃculty, one can show that the information-theoretic
upper bound matches the lower bound in any dimension, with no dependence on the dimension.
This raises a compelling algorithmic question: what are the best estimates for the mean and
covariance that can be computed eﬃciently?

µ
k

−

˜µ

k

In this paper, we give polynomial time algorithms to estimate the mean with error that is close
to the information-theoretically optimal estimator. The dependence on the dimension, of the error
in the estimated mean, is only √log n. To the best of our knowledge, this is the ﬁrst polynomial-
time algorithm with an error dependence on dimension that is less than √n, the bound achieved by
the geometric median. Moreover, as we state precisely later, our techniques extend to very general
input distributions and to estimating higher moments.

Our algorithm is practical. A matlab implementation for mean estimation can be found in
[KRV]. It takes less a couple of seconds to run on a 500-dimensional problem with 5000 samples
on a personal laptop.

Model. We are given points x 1, ..., x m ∈
η
−
probability each x i is independently sampled from a distribution
with mean µ and covariance
Σ, and with η probability it is picked by an adversary. For ease of notation, we will write x i ∼ Dη
when we want to say the x i is picked according to the above rule. The problem we are interested
in is to estimate µ and Σ given the samples. In the following, we will consider mainly two kinds of
distributions.

Rn sampled according to the following rule. With 1

D

Gaussian

= N (µ, Σ) is the Gaussian with mean µ and covariance Σ.

D
Bounded Moments Let

D

is a distribution with mean µ and covariance Σ. We say it has

2

bounded 2k’th moments if there exists a constant C2k such that for every unit vector v ,

E

(x

µ)T v

2k

C2k

E

(x

≤

µ)T v

−

−

(cid:0)

(cid:1)
2

(cid:16)

(cid:0)

k

2

(cid:17)

(cid:1)

x T v

Here Var
used, and for covariance estimation, C8 will be needed.
(cid:1)

=

(cid:0)

(cid:2)

(cid:3)

v T Σv

= C2k(Var

x T v

)k.

(1)

is the variance of x along v . For mean estimation, C4 will be

(cid:2)

(cid:3)

1.1 Main results

All the results we state hold with probability 1
also assume η is a less than a universal constant. We begin with agnostic mean estimation.

1/ poly(n) unless otherwise mentioned. We will

−

Theorem 1.1 (Gaussian mean). Let
algorithm that takes as input m = O
computes

µ such that the error

= N (µ, Σ), µ

D
n(log n+log 1/ǫ) log n
ǫ2

Rn. There exists a poly(n, 1/ǫ)-time
∈
independent samples x 1, ..., x m ∼ Dη and

b

µ
k

µ
k2 is bounded as follows:
(cid:16)
−

(cid:17)

b
η1/2 + ǫ

O (η + ǫ) σ√log n
Σ
k

1/2
2
k

log1/2 n

if Σ = σ2I
otherwise.

O

(cid:0)

We note that the sample complexity is nearly linear, and almost matches the complexity for

(cid:1)

mean estimation with no noise.

Remark 1.2. If we take m = O
samples, and assume that η < c/ log n for a
small enough constant c > 0, then by combining theorems 1.5 and 1.1, we can improve the η depen-
log1/2 n.
dence for the non-spherical Gaussian case in Theorem 1.1 to

µ
k2 = O
(cid:0)
Our next theorem is a similar result for much more general distributions.

1/2
Σ
2
k
k

µ
k

η3/4

−

(cid:17)

(cid:16)

(cid:1)

n2(log n+log 1/η) log n
η2

b

D

be a distribution on Rn with mean µ, covariance Σ, and
Theorem 1.3 (General mean). Let
bounded fourth moments (see Equation 1). There exists a poly(n, 1/ǫ)-time algorithm that takes
independent samples x 1, ..., x m ∼ Dη, and
as input a parameter η and m = O
µ
computes
(cid:16)
k
−
C 1/4
4
η1/2 + C 1/4
(cid:16)

µ
k2 is bounded as follows:
σ√log n
1/2
2
k

(η + ǫ)3/4
b
(η + ǫ)3/4

n(log n+log 1/ǫ) log n
ǫ2

µ such that the error

if Σ = σ2I

otherwise.

log1/2 n

Σ
k

O

O

(cid:17)

(cid:17)

b

4

(cid:16)

(cid:17)

The bounds above are nearly the best possible (up to a factor of O(√log n)) when the covariance

is a multiple of the identity.

Rn and covariance Σ.
Observation 1.4 (Lower Bounds). Let
Any algorithm that takes m (not necessarily O(poly(n))) samples x 1, ..., x m ∼ Dη, and computes a
µ
µ should have with constant probability the error
k

be a distribution with mean µ

−

D

∈

µ
k2 is
= N (µ, Σ)
b
has bounded fourth moments.

D

if

b

Ω(η
Ω(η3/4
p

Σ
k

k2)
k2)
Σ
k

if

D

p

3

−

−
D

, x and (x

be a distribution with mean µ and covariance Σ
D
µ)T have bounded fourth moments with constants C4
is an (unknown) aﬃne transformation of a 4-wise in-

Theorem 1.5 (Covariance Estimation). Let
and that (a) for x
µ)(x
∼ D
and C4,2(see Equation 1) respectively. (b)
dependent distribution. Then, there is an algorithm that takes as input m = O
samples x 1, ...x m ∼ Dη and η and computes in poly(n, 1/ǫ)-time a covariance estimate
η1/2 + C 1/4
(cid:16)

−
b
k · kF denotes the Frobenius norm.
= N (µ, Σ), then it satisﬁes the hypothesis of the above theorem. More generally, it holds
for any 8-wise independent distribution with bounded eighth moments and whose fourth moment
along any direction is at least (1 + c) times the square of the second moment for some c > 0. We
also note that if the distribution is isotropic, then covariance estimation is essentially a 1-d problem
and we get a better bound.

n2(log n+log 1/ǫ) log n
ǫ2
(cid:17)
Σ such that

k2 log1/2 n
Σ

4,2 (η + ǫ)3/4

kF = O

C 1/2
4 k

where

Σ
k

Σ

D

If

(cid:16)

(cid:17)

b

Theorem 1.6 (Agnostic 2-norm). Suppose
centration inequality: there exists a constant γ such that for every unit vector v

is a distribution which satisﬁes the following con-

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

≤

(cid:17)

Then, there is an algorithm that runs in poly(n, log 1

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)
independent samples x 1, ..., x m ∼ Dη, and computes

O

n3(log n/η)2 log n
η2

(cid:16)

(cid:17)

η ) time that takes as input η and m =

λmax such that

(1

O(η))

−

Σ
k

k2 ≤

1 + O(η log2/γ n/η)
(cid:17)

(cid:16)

b
k2.
Σ
k

λmax ≤
b

In independent work, [DKK+16] gave a similar algorithm, which they call a Gaussian ﬁltering
method, for agnostic mean estimation assuming a spherical covariance matrix; while their guar-
antees are speciﬁcally for Gaussians, the error term in their guarantee grows only with log(1/η)
rather than log n. They also give a completely diﬀerent algorithm based on the Ellipsoid method,
for a simple family of distributions including Gaussian and Bernoulli.

As a corollary of Theorem 1.5, we get a guarantee for agnostic SVD.

Theorem 1.7 (Agnostic SVD). Let
Let Σk be the best rank k approximation to Σ in
algorithm that takes as input η and m = poly(n) samples from
such that

is a distribution that satisﬁes the hypothesis of Theorem 1.5.
k · kF norm. There exists a polynomial time
Σk

Dη. It produces a rank k matrix

D

−

−

Σ

Σ

(cid:13)
(cid:13)
(cid:13)

Σk

F ≤ k

ΣkkF + O
Given the wide applicability of SVD to data, we expect the above theorem will have many ap-
plications. As an illustration, we derive a guarantee for agnostic Independent Component Analysis
(ICA). In standard ICA, input data points x are generated as As with a ﬁxed unknown n
n
full-rank matrix A and s generated from an unknown product distribution with non-Gaussian com-
ponents. The problem is to estimate the matrix A (the “basis”) from a polynomial number of
samples in polytime. There is a large literature of algorithms for this problem and its extensions

η log n

Σ
k

k2.

(cid:16)p

(cid:13)
(cid:13)
(cid:13)

×

(cid:17)

b

b

4

[FJK96, NR09, Car98, HKO01, CJ10, BRV13, AGMS12, BCV13, GVX14]. However, all these al-
gorithms rely on no noise or the noise being random (typically Gaussian) and require estimating
singular values to within 1/ poly(n) accuracy, and therefore unable to handle adversarial noise. On
the other hand, the algorithm from [VX15], which gives a sample complexity of ˜O(n), only requires
estimating singular values to within 1/ poly(log n). Our algorithm for agnostic SVD together with
the Recursive Fourier PCA algorithm of [VX15] results in an eﬃcient algorithm for agnostic ICA,
tolerating noise η = O(1/ logc n) for a ﬁxed constant c. To the best of our knowledge, this is the
ﬁrst polynomial-time algorithm that can handle more than an inverse poly(n) amount of noise.

4

∈

−

si|
|

η and be arbitrary with probability η, where A

Theorem 1.8 (Agnostic Standard ICA). Let x
probability 1
components of s are independent,
5
si|
E
|
|
η < ǫ/2, there is an algorithm that, with high probability, ﬁnds vectors
there exist signs ξi =
1 satisfying
poly(n, K, ∆, M, κ, 1
on real symmetric matrices of size n

Rn be given by a noisy ICA model x = As with
Rn×n has condition number κ, the
K√n almost surely, and for each i, Esi = 0, Es2
i = 1,
k ≤
M . Then for any ǫ < ∆3/(108M 2 log3 n), 1/(κ4 log n) and
such that
k2 for each column A(i) of A, using
A
k
ǫ ) samples. The running time is bounded by the time to compute ˜O(n) SVDs

b1, . . . , bn}
{

∆ and maxi E

3
| ≥

A(i)

(cid:13)
(cid:13)
n.

k
≤

ξibi

(cid:13)
(cid:13)

≤

−

−

±

∈

s

ǫ

×

Our results can also be used to estimate the mean and covariance of noisy Bernoulli product
distributions, i.e. distributions in which each coordinate i is 1 with probability pi and 0 with
probability 1
1−p . For a Bernoulli
. Then Theorem 1.3
1
2 , then
If C4 is constant, then by Theorem 1.5, we can get an

pi. In one dimension, C4 for a Bernoulli distribution is (1−p)2
+ p2
i
1−pi
i, pi = p and p

product distribution, C4 will be within a constant of maxi
can be applied to get an estimate
µ
k
estimate for the covariance.

µ for the mean. For instance, if

µ
k2 = O

p + p2

(1−pi)2
pi

≥

−

−

o

n

∀

.

η(1 + √ηp)p log n
b
(cid:1)

(cid:0)p

b

2 Main Ideas

Here we discuss the key ideas of the algorithms. The algorithm AgnosticMean (Algorithm 3)
alternates between an outlier removal step and projection onto the top n/2 principal components;
these steps are repeated.
It is inspired by the work of Brubaker [Bru09] who gave an agnostic
algorithm for learning a mixture of well-separated spherical Gaussians.

For illustration, let us assume for now that the underlying distribution is

= N (µ, σ2I ). We
SN be the points sampled from
are given a set S of m = poly(n) points from
the Gaussian and the adversary respectively. Let us also assume that
. We will use the
|
notation µT for mean of the points in a set T , and ΣT for covariance of the points in T . We then
have

Dη, and S = SG ∪

SN |
|

= η

S
|

D

ΣS = (1

η)σ2I + ηΣSN + η(1

−

η)(µS −

µN )(µS −

−

µN )T .

(2)

If the dimension is n = 1, then we can show that the median of S is an estimate for µ correct up to an
µN ),
additive error of O(ησ). Even if we just knew the direction of the mean shift µS −
µ = η(µG −
µS and then ﬁnding
then we can estimate µ by ﬁrst projecting the sample S on the line along µ
µ
the median. This would give an estimator
k2 = O(ησ). So we can focus on
µ. One would guess that the top principal component of the covariance
ﬁnding the direction of µS −
matrix of S would be a good candidate. But it is easy for the adversary to choose SN to make this
completely useless. Since the noise points SN can be anything, just two points from SN placed far

µ satisfying

µ
k

−

−

b

b

5

away on either side of the mean µ along a particular line passing through µ are suﬃcient to make
the variance in that direction blow up arbitrarily. But we can limit this eﬀect to some extent by
an outlier removal step. By a standard concentration inequality for Gaussians, we know that the
points in SG lie in a ball of radius O(σ√n) around the mean. So, if we can just ﬁnd a point inside
or close to the convex hull of the Gaussian and throw away all the points that lie outside a ball of
radius Cσ√n around this point, we preserve all the points in SG. This will also contain the eﬀect
of noise points on the variance since now they are restricted to be within O(σ√n) distance of µ.
We will see later that we can use coordinate-wise median as the center of the ball. By computing
the variance by projecting onto any direction, we can ﬁgure out σ2 up to a 1
O(η) factor. From
now on, we assume that all points in S lie within a ball of radius O(σ√n) centered at µ.

±

But even after this restriction, the top principal component may not contain any information
about the mean shift direction. By just placing (say) η/10 noise points along the e1 direction
σ√n, and all the remaining noise points perpendicular to this at a single point at a smaller
at
distance, we can make e1 the top principal component. But e1 is perpendicular to the mean shift
direction.

±

The idea to get around this is that even if the top principal component of ΣS may not be along
the mean-shift direction, the span (call it V ) of top n/2 principal components of ΣS will contain a big
projection of the mean-shift vector. This is because, if a big component of the the mean-shift vector
was in the span (say W ) of bottom n/2 principal components of ΣS, by Equation 2 this would mean
that there is a vector in W with a large Rayleigh quotient. This implies that the top n/2 eigenvalues
µN )T ,
of ΣS are all big. Since ΣS = (1
this is possible only if Tr(A) is large. But since the distance of each point in S from µ is O(σ√n),
the trace of A cannot be too large. Therefore, in the space W , we can just compute the sample
mean P W µS and it will be close to P W µ. We still have to ﬁnd the mean in the space V . But we
do this by recursing the above procedure in V . At the end we will be left with a one-dimensional
space, and then we can just ﬁnd the median. This recursive projection onto the top n/2 principal
components is done in Algorithm 3 .

η)σ2I + A, where A = ηΣSN + η(1

µN )(µS −

η)(µS −

−

−

This generalizes to the non-spherical Gaussians with a few modiﬁcations. We use a diﬀerent
outlier removal step. In the non-spherical case, it is not trivial to compute
k2 to be used as
Σ
k
the radius of the ball. We give an algorithm for this later on. To limit the eﬀect of noise, we use
a damping function. Instead of discarding points outside a certain radius, we damp every point
by a weight so that further away points get lower weights. This is done in OutlierDamping
(Algorithm 1). We get the guarantees of Theorem 1.1 by running AgnosticMean (Algorithm 3)
with the outlier removal routine being OutlierDamping. A detailed proof of the whole algorithm
is given in Section 3.1.

We then turn to more general distributions which have bounded fourth moments. We need
bounded fourth moments to ensure that the mean and covariance matrix of the distribution
do not
change much even after conditioning by an event that occurs with probability 1
η. One diﬃculty for
with bounded
general distributions is that the outlier damping doesn’t work. So for distributions
fourth moments, we have another outlier removal routine called OutlierTruncation(
, η). In this
·
routine, we ﬁrst ﬁnd a point analogous to the coordinate-wise median for the Gaussians, and then
η fraction of S. We throw away all the points outside
consider a ball big enough to contain 1
this ball. We get the guarantees of Theorem 1.3 by running AgnosticMean (Algorithm 3) with
the outlier removal routine being OutlierTruncation (Algorithm 2). The complete proof of this
appears in Section 3.3.

−

−

D

D

6

D

is given by ED(x

We now have an algorithm to estimate the mean of very general (with bounded fourth moments)
distributions. To estimate the covariance matrix, we observed that the covariance matrix of a
µ)T . If we knew what µ was, then covariance can be
distribution
computed by estimating the mean of the second moments. To compute the mean of the second
µ)T as a vector in n2 dimensions and run the algorithm for
moments, we can treat (x
mean estimation. Also, we can estimate µ by the same algorithm. Therefore, we get Theorem 1.5
by running CovarianceEstimation (Algorithm 4). Its proof appears in Section 4.2.
Algorithm AgnosticOperatorNorm (Algorithm 5) estimates the 2-norm

µ)(x

µ)(x

−

−

−

−

k2 for general
= N (µ, Σ), and we are given m = poly(n) samples

Σ
k

distributions. For illustration, suppose
x 1, ..., x m ∼ Dη, and the mean µ. We consider the covariance-like matrix

D

Σ(S, µ) =

(x i −

µ)(x i −

µ)T .

1
m

Xi

−

η fraction of the points in S are from the Gaussian, we have Σ(S, µ)

Since 1
η)Σ. Therefore,
(cid:23)
the top eigenvalue σ2 of Σ(S, µ) is at least (1
k2. Let v be the top eigenvector of Σ(S, µ). If
η factor) is much less than σ2, this
the Gaussian variance along v (which can be computed up to 1
should be because there are a lot of noise points in S whose projections onto v are big compared
to the projection of Gaussian points in S. We remove points in S that have big projection and
then iterate the entire procedure. We later show that this procedure terminates in poly(n) steps
and when it terminates the top eigenvalue of Σ(S, µ) is close to that of Σ. A proof of this appears
in Section 5.

Σ
η)
k

(1

−

−

±

Theorem 1.7 follows easily from Theorem 1.5. Let

Σk be the top-k eigenspace of

Σ from

Theorem 1.5. We then have

b

Σ

Σk

−

F
(cid:13)
(cid:13)
(cid:13)

b

(cid:13)
(cid:13)
(cid:13)

b
Σk

−

−

b
Σk

F
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
ΣkkF
(cid:13)

Σ

−

(a)

≤
(b)

≤
(c)

≤
(d)

Σ

Σ

−

F

(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
Σ
(cid:13)

b
Σ

−

Σ

b
−

+

Σ

+

(cid:13)
(cid:13)
(cid:13) b
Σ
(cid:13)
(cid:13)
(cid:13) b
+
k

F

(cid:13)
(cid:13)
b
ΣkkF + O
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)

≤ k

−

η log n

Σ
k

k2.

(cid:17)
(a), (c) follow from triangle inequality, (b) follows from the fact that
imation and (d) from the guarantees of Theorem 1.5.

(cid:16)p

Σk is the best rank-k approx-

Finally we outline the application to agnostic ICA. The algorithm from [VX15]. Proceeds by ﬁrst
estimating the mean and covariance, in order to make the underlying distribution isotropic. Here
we estimate the covariance matrix Σ by ˆΣ and use it to determine a new isotropic transformation
ˆΣ
k2, the
isotropic transformation results in a guarantee of

2 . Since our agnostic SVD algorithm gives a guarantee of

O(√ν log n)
Σ
k

kF ≤

Σ
k

˜Σ

− 1

−

b

− 1

2 Σ ˆΣ

− 1
2

ˆΣ
k

I

k2 ≤

−

O(

Σ
k2
η log n) k
Σ−1
k2
k

p

p

= O(

η log nκ2).

Next the algorithm estimates a weighted covariance matrix W with the weight of a point x pro-
portional to cos(u T x ) for u chosen from a Gaussian distribution; it computes the SVD of W . For

7

this we use our algorithm again (the weights are applied individually to each sample). The main
guarantee is that the eigenvectors of this weighted covariance approximate the columns of A. This
relies on the maximum eigenvalue gap of W being large, and it has to be approximated to within
additive error ǫ = O(1/(log n)3). Theorem 1.7 implies that the additional error in eigenvalues is
k2, and therefore it suﬃces to have √η log n < c/(log n)3 for a suﬃ-
bounded by O(√η log n)
Σ
k
ciently small constant c that depends only on the cumulant and moment bound assumptions (i.e.,
c(log n)−7.
∆, M ). Thus, if suﬃces to have η < ǫ/2

≤

2.1 Lower Bounds: Observation 1.4

In this section we will show the lower bounds stated in Observation 1.4. For Gaussian distributions,
this is a special case of a theorem proved in [CGR15]. We reproduce the relevant part here for
D2 = N (µ2, σ2I ) and
completeness. We will show that there are distributions
µ2k2 = Ω(ησ) and
distributions Q1, Q2 such that
D1 + ηQ1 = (1
η)
−
D2. Let φ1 be p.d.f of
D1,

µ1 −
k
Dη = (1
Dη, no algorithm can distinguish between

D1 = N (µ1, σ2I ),

D1 and φ2 be the

D2 + ηQ2.

(3)

η)

−

D2. Let µ1, µ2 be such that the total variation distance between

D1,

D2 is

So, given
p.d.f of

1
2

η

φ2|
By a standard inequality for the total variation distance of Gaussian distributions, this implies
µ2k2 ≥
φ1)1φ2≥φ1 and Q2 be the
that
distribution with p.d.f 1−η
φ2)1φ1≥φ2. It is now easy to verify that Equation 3 is satisﬁed.
This proves item one of Observation 1.4.

1−η . Let Q1 be the distribution with p.d.f 1−η

η (φ1 −

η (φ2 −

µ1 −
k

φ1 −
|

dx =

2ησ

−

Z

1

η

.

D1 is supported on two points
{−
1/4. It is easy to check that both

For the distributions with bounded fourth moments, consider the following two one-dimensional
distributions.
.
1/2, 1/2
}
{
D2 is supported on three points
respec-
η)/2, η
−
D2 have bounded fourth moments with
tively. Let η
≤
the constant C4 = 8. Furthermore,
D1 by adding η fraction of noise
D2 can be obtained from
points. So no algorithm can distinguish between the two distributions. Since their means diﬀer by
η3/4σ, no algorithm can get an estimate better than this.

with the corresponding probabilities
(1
{

{−
σ, σ, σ/η1/4
}

}
D1 and

with probabilities

η)/2, (1

σ, σ

−

}

We will now show that the geometric median:

arg min

y

x i −
k

y

k2

Xi
has a √n dependence on the dimension. We show this in the Gaussian case even if we have access
to the whole distribution, but with η fraction of noise points placed all at a single point far away
from most of the Gaussian points.

Proposition 2.1 (Geometric Median). Let
= N (0, Σ) be a distribution with diagonal covariance
matrix Σ whose variance along the coordinate direction e 1 is zero, and equal to 1 in all the other
coordinate directions. Assume there is an η fraction of noise at a distance a = n along e 1. Let

D

t0 = arg min

(1

η)Ex ∼D

t

−

t2 + x2

2 + ... + x2
n

+ η(a

t).

−

(cid:19)

(4)

Then, t = Ω(η√n).

(cid:18)q

8

Proof. We have that at the minimizer t0, the derivative with respect to t is zero. Therefore, we
should have

Ex ∼D

t0
0 + x2
t2
2 + ... + x2
n

=

1

η

−

.

η

Consider f (t) = Ex ∼D

t
√t2+x2
2+...+x2
n
t = αη√n for a small enough constant α, then f (t)
x
n/2 with exponential probability. Therefore,
k

2
2 ≥
k

≤

. It is clear from Equation 4 that t0 > 0. We claim that if
p

η
1−η . Suppose t1 = αη√n. Since x

,

∼ D

f (t1)

Ex ∼D

t1
t2
1 + n/2

≤

≤

t1√2π
p
t2
1 + n/2 ≤

αη√2π.

Our algorithms are based on outlier removal and SVD. To simplify the proofs, we use new samples
for each step of the algorithm. The total sample complexity is given in the theorems.

For outlier removal, we use one of the following two simple routines. The ﬁrst, which we call
OutlierDamping, returns a vector of positive weights, one for each sample point.

The claim, and hence the proof follows.

p

2.2 Algorithms

2.2.1 Outlier Removal

Algorithm 1: OutlierDamping(S)

Input: S
⊂
Output: S

Rn with

= m

S
|

|
Rn, w = (w1, ..., wm)

Rm

∈

⊂

1. if n = 1:

Return (S,

1).

−

3. Set wi = exp

4. Return (S, w ).

kx i−aaak2
2
s2

(cid:17)

−

(cid:16)

for every x i ∈

S.

9

The second procedure for outlier removal returns a subset of points. It will be convenient to

view this as a 0/1 weighting of the point set. We call this procedure OutlierTruncation.

2. Let aaa be the coordinate-wise median of S. Let s2 = C Tr(Σ). Estimate Tr(Σ) by esti-

mating 1d variance along n orthogonal directions, see Section 4.1.

Algorithm 2: OutlierTruncation(S, η)

Input: S
Output:

⊂
S

Rn, η

[0, 1]

∈
S, w = 1

Rm

∈

⊂

1. if n = 1:

e

←

∩

2. Let aaa be as in Lemma 3.15.
e

e

fraction of S.

←

∩

e

4.

S

S

B(r, aaa). Return (

S, 1).

2.2.2 Main Algorithm

e

Let [a, b] be the smallest interval containing (1
S

[a, b]. Return (

S, 1).

S

η

−

−

−

ǫ)(1

η) fraction of the points,

3. Let B(r, aaa) = ball of minimum radius r centered at aaa that contains (1

η

ǫ)(1

η)

−

−

−

We are now ready to state the main algorithm for agnostic mean estimation. It uses one of the
above outlier removal procedures and assumes that the output of the procedure is a weighting.

Algorithm 3: AgnosticMean(S)

Input: S
Output:

⊂
µ

Rn.

Rn, and a routine OutlierRemoval(
).
·

∈
S, w ) = OutlierRemoval(S) .

1. Let (
b

2. if n = 1:

e

(a) if w =

1, Return median(

S). //Gaussian case

−
(b) else Return mean(

S). //General case
e

3. Let Σ

eS,w be the weighted covariance matrix of
e

the top n/2 principal components of Σ

4. Set S1 := P V (S) where P V is the projection operation on to V .

5. Let

µV := AgnosticMean(S1) and

µW := mean(P W

S).

Rn be such that P V

µ =

µV and P W

µ =

µW .

e

b

b

b

b

b

µ
b

6. Let

∈
7. Return

b

µ.

b

10

S with weights w , and V be the span of

eS,w , and W be its complement.

e

2.2.3 Estimation of the Covariance Matrix and Operator Norm

For both the tasks in this section, we will assume that the mean of the distribution µ = 0. We
can do this without loss of generality by a standard trick mentioned described in Section 4.2. The
algorithm for estimating the covariance matrix calls AgnosticMean on x x T . Analysis is given in
Section 4.2.

1. Let S(2) =

x ′
{

ix ′
i = 1, ..., m/2
b
i|
}

(see Equation 15)

2. Run the mean estimation algorithm on S(2), where elements of S(2) are viewed as vectors

direction of top variance. The analysis is given in Section 5.

Σ
k

k2 is based on iteratively truncating the samples along the

Algorithm 4: CovarianceEstimation(S)

Input: S
⊂
Output: n

R
Rn, η
∈
n matrix

Σ

×

in Rn2

. Let the output be

Σ.

3. Return

Σ.

b

b
The algorithm for estimating

Algorithm 5: AgnosticOperatorNorm(S)

Input: S
⊂
Output: σ2

Rn, η

∈
R>0.

∈

[0, 1], γ

R

∈

1. Let

S = SafeOutlierTruncation(S, η, γ).

2. Do the following O(n log2/γ n

η ) times

e

3. Let Σ0(

S) := 1
| eS|

i∈ eS x x T .

4. Find v , the top eigenvector of Σ0(

e

S), and its corresponding eigenvalue σ2.

5. Estimate (up to 1

cη factor, see Section 4.1) the variance of

along v and denote it

e

D

P

±

by

σ2
v .

6. if σ2
b

≤
Return σ2.

(1 + c3η log2/γ n
η )

σ2
v

7. Remove all points x

S such that

x T v
|

|

>

c2

σv log1/γ n
η
b
2

.

8. Go to Step (3).

b

∈

e

11

Algorithm 6: SafeOutlierTruncation(S, η, γ)

Input: S
Output:

⊂
S

Rn, η
S

∈

[0, 1], γ

R

∈

⊂
1. Let t =

e

n
i=1

2. Let B(c√t log1/γ n

P

b

3.

S

S

←

∩

B(c√t log1/γ n

η , 0). Return

S.

e

e

σ2
ei be the sum of estimated variances of

D
η , 0) be the ball of radius c√t log1/γ n
η centered at 0.

in n orthogonal directions.

3 Mean Estimation: Theorem 1.1 and Theorem 1.3

b

µ
k

In this section, we will ﬁrst prove Theorem 1.1, which is for Gaussian distributions, and Theorem 1.3,
which is for distributions with bounded fourth moments. All our algorithms will be translationally
is µ = 0. So we will be
invariant. We will assume w.l.o.g that the mean of the distribution
k2. Algorithm 3 has log n levels, we will assume that at each level it uses
proving bounds on
O( n log n

) samples resulting in a total of m = O( n log2 n

ǫ2
ǫ2
At various points in the analysis, to bound the sample complexity we will have to show that
the estimates computed from samples are close to their expectations. We will use the following
two results. Firstly, as an immediate corollary of matrix Bernstien for rectangular matrices (see
Theorem 1.6 in [Tro12]), we get the following concentration result for the sample mean and sample
covariance.
Lemma 3.1. Consider a distribution in Rn with covariance matrix Σ and supported in some
(0, 1). Then the
Σ
Euclidean ball whose radius we denote is
k
following holds with probability at least 1

R. Let ǫ
then

, for some R
k
1/ poly(n): If N
p

∈
R log n
ǫ2

D

R

∈

).

≥

and

Here

µ and

Σ are sample mean and sample covariance matrix.

b

b

Secondly, the functions we estimate will be integrals of low-degree polynomials (degree d at
most 4) restricted to intervals and/or balls. These functions viewed as binary concepts have small
VC-dimension, O(nd) where n is the dimension of space and d is the degree of the polynomial. We
use this to bound the error of estimating integrals via samples, and we can make the error smaller
than any inverse polynomial using a poly(n) size sample.
Proposition 3.2. Let F be a class of real-valued functions from Rn to [
corresponding class of binary concepts, i.e., for each f
f (x)

R, R]. Let CF be the
F , we consider the concepts ht(x) = 1 if
F , and any

t and zero otherwise. Suppose the VC-dimension of CF is d. Then, for any f

−

∈

≥

∈

µ
k

µ

−

k ≤

ǫ

Σ
k

k

p

Σ

−

k ≤

ǫ

Σ
k

.
k

−

b
Σ
k

b

12

distribution
least 1

D
δ satisﬁes

−

over Rn, an iid sample S of size

8
ǫ2 (d log(1/ǫ) + log(1/δ)), with probability at

S
|

| ≥

1
S
|

−

| Xx∈S

2ǫR.

≤

f (x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ex∼D(f (x))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

Proof. By the VC theorem, for any concept in CF , the bound on the size of the sample ensures
that with probability at least 1

δ and any t,

Noting that Ex∼D(f (x)) =

t) dt, we get the claimed bound.

≥

Pr(f (x)
(cid:12)
(cid:12)
(cid:12)
R
−R Pr(f (x)
(cid:12)
R
Tr(Σ) and ǫ2 := kaaak2

≥

x

|{

∈

t)

−

t

≥

}|

S : f (x)
S
|

|

ǫ.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let s2 := 1
ǫ1

η2s2 . We can estimate Tr(Σ) by estimating (1 dimensional)
variances along n orthogonal directions, see Section 4.1. Note that we can arrange 0 < ǫ1, ǫ2 < 1
to be small enough constants. We weight every point x by wx = exp(
= N (0, Σ)
be a Gaussian distribution and S =
SN
be the Gaussian and the noise points repectively, with

−
, x i ∼ Dη be the sample we get. Let S = SG ∪

x 1, ..., x m}
{

D
Rn, let

= ηm. For a set T

kx −aaak2
s2

). Let

2

µT,w :=

wx ix i and ΣT,w :=

1
m

Xi∈T

SN |
|
wi(x i −

1
T
|

| Xi∈T

⊂
µT,w )T

µT,w )(x i −

We use the above notation for T = SG and T = SN . By an abuse of notation, when T = G, we
mean the population version of the above quantities:

µG,w := Ex wx x and ΣG,w := Ex wx (x

µG,w )(x i −

−

µG,w )T .

Note that

We consider the matrix ΣS,w

µS,w = (1

η)µSG,w + ηµSN ,w .

−

ΣS,w =

1
m

= (1

Xi
−

µS,w )(x i −

wx i(x i −
η)ΣSG,w + ηΣSN ,w + η(1

µS,w )T

η)(µSN ,w −

µSG,w )(µSN ,w −

−

µSG,w )T .

3.1 Proof of Theorem 1.1:

Let us assume η < 1/2.1. We then have

Lemma 3.3. Let
we are given x1, ..., xm ∼ Dη, then the median xmed = mediani{
with high probability.

D

xi}

= N (0, σ2) be a one dimensional Gaussian distribution. If m = O

xmed
|

|

satisﬁes

log n
ǫ2

, and
= O((η + ǫ)σ)

(cid:17)

(cid:16)

Proof. Let SG ⊂
Φ−1(1/2 + η + ǫ). Let us bound the probability that the median xmed ≥
xmed ≥
≥
SG|
if
|

S be made up of samples in S that come from the Gaussian, also let c =
c. We ﬁrst note that if
poly(n)

c, then Pr (x > c
x
|
log n
= O
ǫ2

ǫ. By Hoeﬀding’s inequality, we can bound this by 1

∈u SG)

−

.

(cid:16)

(cid:17)

13

We will next consider the multidimensional case. The proof follows by a series of lemmas. We
state the lemmas ﬁrst, conclude the proof of Theorem 1.1 and then prove the lemmas. First, we
observe that by applying Lemma 3.3 in n orthogonal directions and union bound, we get
Lemma 3.4. Suppose v 1, ..., v n ∈
i miv i. Then if m = O
and aaa =
v i’s such that with probability 1

Rn are a set of orthonormal vectors. Suppose mi = medianj{
v t
ix j}
,
, there exists a constant C independent of the choice of

log n
ǫ2
poly(n) ,
(cid:16)

(cid:17)

P

−

By a simple calculation, maxx

bound on the trace.

Lemma 3.5. Suppose A := ηΣSN ,w + η(1
a constant C such that,

−

O(s2). This immediately gives the following

µG,w )(µSN ,w −

µSG,w )T . Then there exists

Cη2 Tr(Σ).

2
aaa
2 ≤
k
k
2e−kx −aaak2/s2
k

x
k

≤

η)(µSN ,w −
Cηs2.

Tr(A)

≤

We will show later

Theorem 3.6.

e−η2ǫ2
1 + ǫ1 −

 

η2ǫ2e2ǫ1

Σ

ΣG,w

(cid:22)

(cid:22)

!

eǫ1Σ.

As will be clear from the proof of Theorem 3.6, when Σ = σ2I is a multiple of identity, then
) samples, we will have

ΣG,w will also be a multiple of I . By Lemma 3.1, if we take m = O( n log n

ǫ2

Suppose, we have

(1

ǫ)ΣG,w

ΣSG,w

(1 + ǫ)ΣG,w .

−

(cid:22)

(cid:22)

(cid:22)

(cid:22)

αΣ

ΣSG,w

βΣ

in the Lowener ordering, for some α, β > 0. By an argument similar to the one sketched in Section
2, we can prove

Lemma 3.7. We will use the notation as deﬁned above. Let W be the bottom n/2 principal
components of the covariance matrix ΣS,w . We have

≤
kmin denotes the least eigenvalue of Σ and δµ := µSN ,w −

Σ
2η ((β + Cη)
k

k2 −

Σ
α
k

kmin) ,

ηPW δµ
k

2
k

µSG,w .

where

Σ
k

By an inductive application of Lemma 3.7, we get the following theorem giving a bound on

µ
k
Theorem 3.8. On input S and the routine OutlierDamping(
), AgnosticMean outputs
·
b
satisfying

.
k
µ

2
µ
k
k

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

b

(cid:0)

b

(cid:1)

14

Theorem 3.6 combined with Theorem 3.8 proves Theorem 1.1. We get a better dependence on
η when Σ = σ2I because we can take α = β in this case. This would lead to the cancellation of
the leading term in the bound in Theorem 3.8 as

Σ
k

k2 =

Σ
k

kmin.

Proof of Lemma 3.7:
have

Recall that Σ denotes the covariance matrix of the Gaussian part. We

ΣS,w = (1
= (1

η)ΣSG,w + ηΣSN ,w + η(1
η)ΣSG,w + A,

−

−

−

η)δµδT
µ

where A = ηΣSN ,w + η(1

η)δµδT

µ. Therefore, we have

−

(1

η)αΣ + A

ΣS,w

(1

η)βΣ + A.

(cid:22)

(cid:22)

−

For a symmetric matrix B, let λk(B ) denote the k’th largest eigenvalue. By Weyl’s inequality,

−

−

we have

Therefore,

By Lemma 3.5 we have

λk((1

η)ΣG,w + A)

λk(A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (ΣS,w )

λn/2 (A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (A)

=

⇒

λn/2(ΣS,w )

Tr(A)
n/2
2C 2η

≤

≤

≤

≤

Σ
k2
k
k2 + 2C 2η
Σ
η)β
(1
k
(β + 2C 2η)
k2.
Σ
k

−

Σ
k

k2

Recall that W is the space spanned by the bottom n/2 eigenvectors of ΣS,w , and P W is the

matrix corresponding to the projection operator on to W . We therefore have

We therefore have

P T

W ΣS,w P W (cid:22)

(β + 2C 2η)
Σ
k

k2I .

αP T

W ΣP W + ηP T

W ΣSN ,w P W + (η

η2)(P W δµ)(P W δµ)T

−

(β + 2C 2η)
Σ
k

k2I .

(cid:22)

Multiplying by the vector P W δµ

kP W δµk and its transpose on either side, we get

Assuming η

1/2, we therefore have

≤

(η

η2)
P W δµ
k

2
k

−

(β + 2C 2η)
Σ
k

≤

k2 −

Σ
α
k

kmin.

ηP W δµ
k

2
k

≤

2η

(β + 2C 2η)
Σ
k

k2 −

Σ
α
k

kmin

.

(cid:0)

(cid:1)

15

Proof of Theorem 3.8:
have

By Equation 6 and Lemma 3.1, since we take O

samples we

n log n
ǫ2

(cid:16)

(cid:17)

µSG,w k
k

2
2 ≤

k2

= O
(cid:0)

η2ǫ2e2ǫ1 + ǫ2
Σ
k
η2 + ǫ2
Σ
k2.
(cid:1)
k
(βη + η2 + ǫ2)
Σ
k

(cid:0)

(cid:1)

(1 + log n) The proof
So it is enough to prove
is by induction. If n = 1, then the conclusion follows from the guarantees of the one dimensional
(cid:0)
median. Now, assume that it holds for all n
1. Let n = k + 1. We have by Lemma
≤
3.7

k for some k

µSG,w k

k2 −

kmin

Σ
k

µ
k

αη

≤

≥

−

O

b

(cid:1)

2

µSN ,w −

ηP W
k
P W µS,w −
(cid:0)

2
µSG,w
k
≤
2
P W µSG,w k
(cid:1)
2 ≤

⇒ k

=

O

O

(βη + η2)
Σ
k
(βη + η2)
Σ
k

k2 −
k2 −

αη

αη

Σ
k
Σ
k

kmin
kmin

(cid:0)

(cid:0)

.

(cid:1)

(cid:1)

By induction hypothesis, since dim(V ) = n/2, we have

µV −
k
Therefore, adding the two, we get

P V µSG,w k

≤

2

(cid:0)

b

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

µSG,w k

−

2

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

(cid:1)

(cid:1)

Proof of Theorem 3.6:

b

(cid:0)

We will ﬁrst consider the second moment

B := Ex exp

x
k

2
k

aaa
−
s2

x x T .

(cid:19)

−

(cid:18)

We have

B =

p

=

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

exp

x T Σ−1x

x x T dx

−

(cid:0)
x T Σ−1x

(cid:1)
x x T dx

=

p

1
(2π)n

p
exp

−

(cid:18)

Z

|

Σ
|
(x

−

−1

2
aaa
s2 +
k
k
1
s2

Σ−1 +

exp

 −

b)T

(cid:18)

1
s4 aaa T

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

I

(x

b)

x x T dx,

(cid:19)

−

(cid:19)

where b = 1
s2

Σ−1 + 1

s2 I

aaa. Therefore, we have

B = exp

(cid:0)

 −

2
aaa
s2 +
k
k

(cid:1)
1
s4 aaaT

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1
Σ−1 + 1

s2 I

Σ−1 +

−1

.

1
s2

I

(cid:19)

!

Σ
|
|

(cid:12)
(cid:12)

(cid:18)

(cid:12)
(cid:12)

16

Now we will look at the scalar term

. Let λi be the eigenvalues of Σ.

Σ
|

|

Σ−1 + 1

s2 I

Σ−1 +

Σ
|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
s2

I

(cid:12)
1
(cid:12)
λi
= Πi (cid:12)
(cid:12)
(cid:12)

+ 1
s2

1
λi

(cid:12)
(cid:12)
(cid:12)

= Πi

1 +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
s2

.

(cid:19)

(cid:18)

We then have

We next bound exp

kaaak2
s2 + 1

s4 aaaT

Σ−1 + 1

s2 I

−1

aaa

−

(cid:16)

Σ
1 + ǫ1 ≤ |

|

Σ−1 +

1
s2

I

(cid:12)
(cid:12)
(cid:12)
(cid:12)

eǫ1.

≤

(cid:12)
(cid:12)
(cid:12)
. We have
(cid:12)

(cid:0)
Σ−1 +

1
s4 aaaT

(cid:18)

1
s2

I

(cid:19)

(cid:17)

(cid:1)

−1

aaa

≤

1
s2 aaa T aaa.

Therefore

Therefore,

exp(

η2ǫ2)

−

exp

≤

 −

2
aaa
s2 +
k
k

1
s4 aaa T

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1.

! ≤

e−η2ǫ2

Σ−1 +

(cid:18)

−1

1
s2

I

(cid:19)

B

(cid:22)

(cid:22)

eǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

Lemma 3.9. We have the following

1
1 + ǫ1

Σ

(cid:22)

(cid:18)

Σ−1 +

−1

1
s2

I

(cid:19)

Σ

(cid:22)

and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors
s2 and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors

Proof. Note that if 1
of Σ−1, then 1
of Σ−1 + 1

λ1 + 1
s2 I . Since,

λ1 , ..., 1
λn
s2 , ..., 1
+ 1
λn

the lemma follows.

From Lemma 3.9, we have

Next we will bound

λi

1 + ǫ1 ≤

1
+ 1

s2 ≤

1
λi

λi

e−η2ǫ2
1 + ǫ1

Σ

B

(cid:22)

(cid:22)

eǫ1Σ.

µG,w = Ex wx x .

17

(5)

µG,w =

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

p

=

exp(

x T Σ−1x )x dx

−

x T Σ−1x

x dx

1
s4 aaaT

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

2
aaa
s2 +
k
k
1
s2

Σ−1 +

=

p

1
(2π)n

p
exp

Z

= exp

−

(cid:18)

 −

exp

 −

|

Σ
|
(x

−

b)T

2
aaa
s2 +
k
k

(cid:18)
1
s4 aaa T

Σ−1 +

(cid:18)

I

(x

b)

x dx

−
−1

(cid:19)

aaa

(cid:19)
1
s2

I

(cid:19)

!

Σ
|

|

1
Σ−1 + 1

s2 I

b,

where b = 1
s2
the two scalars by eǫ1. Therefore, we have

Σ−1 + 1

s2 I

aaa. Recall that ǫ1 = Pi λi
s2

−1

(cid:0)

(cid:1)

. We can, as before, bound the product of

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Therefore, we have

µG,w k
k

2

2 = e2ǫ1 1

s4 aaaT

µG,w k2 ≤
k

eǫ1

Σ−1 +

−1

1
s2

I

(cid:19)

aaa

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−1

1
s2

(cid:18)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−1

(cid:18)

aaa

(cid:19)
−1

Σ−1 +

1
s2

I

Σ−1 +

−1/2

aaa

1
s2

I

(cid:19)

(cid:19)

(cid:18)

Σ−1 +

(cid:18)

Σ−1 +

1
s2

1
s2

I

I

1
s2

I

(cid:18)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1/
Σ
k
k2.
Σ
k

I

1
s2

2
(cid:13)
(cid:13)
−1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
k2 + 1/s2

e2ǫ1 1

s2 aaaT
e2ǫ1 aaaT aaa
s2

η2ǫ2e2ǫ1

= η2ǫ2e2ǫ1

η2ǫ2e2ǫ1

≤

≤

≤

≤

Also, similarly

This implies

µG,w k
k

2
Σ

−1+ 1

s2 I ≤

η2ǫ2e2ǫ1.

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1Σ.

From Lemma 3.9, we have

Combining Equation (6) and Equation 5, we get Theorem 3.6.

18

(6)

3.2

Improving the dependence on η

Now we will show how we can obtain the second part of Theorem 1.1 to get a better dependence
= N (µ, Σ) be a Gaussian with covariance Σ, and
on η by using
c/ log n for a small enough constant c > 0. We ﬁrst use Theorem 1.5 (with ǫ = η) to estimate
η
≤
σ2 =

Σ from Theorem 1.5. Let

σ2 satisfying

D

Σ
k

b
k2. We get a

σ2

σ2

1

O(

η log n)

−

b
(cid:17)
(cid:16)
be the given sample, and let y i ∼
x 1, ..., x m}
Let S =
{
Deﬁne x ′
i = x i + y i. The key thing to note is that if x
′ = N (µ, Σ +
x + y
N (µ, Σ +
σ2I has

D
, and the covariance Σ′ = Σ +

σ2I ). Let

p

≤

≤

∼

(cid:16)

b

p
N (0,

(cid:17)

σ2I ), i = 1, ..., m be i.i.d. samples.
σ2I ), then
N (µ, Σ) and y
∼
∼
′ is same as that of
σ2I ). Note that the mean µ′ of
b
b

N (0,

D

1 + O(

η log n)

σ2.

(7)

D

b
Σ′

λmax

2 + O(

η log n)

b

≤

b
σ2 and λmin

Σ′

We can view x ′
Σ

(cid:16)
(cid:1)
′
η, and we assume η log n
i ∼ D
such that

p

(cid:17)

(cid:0)

′

compute a

≤

1

O(

η log n)

σ2.

(8)

≥

−

(cid:16)

(cid:0)

(cid:1)

p

(cid:17)

c. By Theorem 1.5 and Equation 7, we can

Let α = O

. Therefore,

b
√η log n

′
Σ

Σ′

−

F ≤

(cid:13)
(cid:13)
(cid:13) b

(cid:13)
(cid:13)
(cid:13)

O

η log n

σ2.

(cid:16)p

(cid:17)

(cid:0)

(cid:1)
ασ2I

I

−
1

′

Σ

−

⇒

=
b
=

O

Σ′

(cid:22)
ασ2

Σ

(cid:22)
′−1

′
Σ

+ ασ2I
′−1/2

Σ

b
(cid:22)
η log n
b

′−1/2

Σ′

Σ

′−1/2

I

Σ
b

I + ασ2
′−1/2

(cid:22)
Σ′

Σ

′−1

Σ

(cid:16)

−

⇒

b
(cid:16)p
by Equation 8. Now, if we let x ′′
i =
′′
then we can think of x ′′
η . If we now use Theorem 3.8 with β =
b
µ′′ such that
α =

i ∼ D
on the samples S′′ =

′′ = N (µ′′, Σ′′) = N

√η log n

b
i and

b
(cid:22)
(cid:16)

(cid:17)(cid:17)
′−1/2

(cid:16)p
Σ

x ′

Σ

(cid:22)

D

O

(cid:16)

b

b

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

, we get a

x ′′
i }
{
2 = O(η3/2 log3/2 n).
k

b

µ′′
k

−

µ′′

This implies that

µ =

Σ

′1/2

µ′′ satisﬁes
b

b

b

µ
b
k

−

′

2 = O(
µ
Σ
k
k
Σ
= O(
k
b

η3/2 log3/2 n)
k
k2η3/2 log3/2 n).

b

1 + O

η log n

I

(cid:17)(cid:17)
Σ

µ,

′−1/2

′−1/2

′−1/2

Σ

Σ

1 + O
b
(cid:0)

(cid:0)

√η log n
b

(cid:1)(cid:1)

,

and
(cid:17)

µ with
Remark 3.10. We can use this technique to give a polynomial time algorithm to compute
for any ﬁxed ǫ > 0. This would require estimating
a guarantee
higher order moments by the mean estimation algorithm and then using the above trick to improve
(cid:1)
the η dependence for each of them in sequence. We don’t give a proof of this in this paper.

k2η2−ǫ log2−ǫ n

2 = O
µ
k

Σ
k

µ
k

−

b

(cid:0)

b

19

3.3 Distributions with Bounded Fourth Moments

In this section, we will prove some some useful properties that distributions with bounded fourth
moments satisfy. We will assume that x
for a distribution with mean µ that has bounded
fourth moments, i.e., for every unit vector v

∼ D

E((x

µ)T v )4

C4

E((x

−

≤

−

µ)T v )2

2

,

(9)

for some C4.

Lemma 3.11 (Mean shift). Let X be a random variable with E(X

EX)2 = σ2 and

E(X

EX)4

−

≤

C4

E(X

−

EX)2

2

,

for some C4. Let ǫ

0.5 and A be any event with probability Pr(A) = 1

ǫ. Then

≤

(cid:1)

−

(cid:1)

−

(cid:0)

(cid:0)

Proof. Let a = E(X

A). Then
|

E(X
|

A)
|

−

E(X)

| ≤

4

8C4ǫ3σ.

p

EX = (1

ǫ)a + ǫE(X

−
EX

ǫ)a

−

−

(1
ǫ

Ac)
|
1
=

ǫ

−
ǫ

E(X

Ac) =
|

⇐⇒

(EX

a) + EX

−

The fourth moment of such an X is minimum when its support is just the two-point set
a) + EX

a, 1−ǫ
{

ǫ (EX

−

. Therefore,
}

(1

ǫ)(a

−

−

EX)4 + ǫ

(EX

a)

−

4

≤

(cid:19)

C4σ4

1

ǫ

−
ǫ

C4ǫ3

(cid:18)

=

a

EX

⇒ |

−

4

| ≤

(1

s

−

σ

4

8C4ǫ3σ,

ǫ)(3ǫ2

3ǫ + 1)

≤

−

p

when ǫ

0.5.

≤

Lemma 3.12. Let X be a random variable with EX = µ and E((X

µ)2) = σ2 and let

E(X

µ)4

C4σ2,

−

≤

−

−

for some C4. Then, for every event A that occurs with probability at least 1

ǫ, we have

where 1A is the indicator function of the event A. As an immediate corollary, for ǫ
the following bound on the conditional probability

≤

0.5 we get

E

(X

µ)21A

−

(cid:0)

1

−

≥

(cid:16)

(cid:1)

C4ǫ

σ2,

p

(cid:17)

(10)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

20

Proof. Let dΩ be the probability measure. We can write E(X
following way

−

µ)4

≤

C4(E(X

µ)2)2 in the

−

2

(cid:19)

2

(X

µ)4dΩ +

(X

µ)4dΩ

(X

µ)2dΩ +

(X

µ)2dΩ

ZAc

−

C4

≤

(cid:18)ZA

−

ZAc

−

Using E(Y

EY )4

(E(Y

EY )2)2 for any random variable Y, and Pr(Ac) = ǫ we have

ZA

−

−

≥

−

1
ǫ

2

(X

µ)2dΩ

−

(cid:18)ZAc

≤

ZAc

(cid:19)

(X

µ)4dΩ

−

We therefore have

(cid:0)R

⇐⇒

Ac(X

−
ǫ

µ)2dΩ

2

(cid:1)
µ)2dΩ

(X

−

ZAc
(X

µ)2dΩ

−

(cid:19)
µ)2

E(X

−

≤

≤

+

p
1

(cid:16)

≤

ZA

⇐⇒

C4ǫ

1

−

(cid:16)

p

(cid:17) (cid:18)ZAc
C4ǫ

1

−

(cid:16)

p

(cid:17)

⇐⇒

This proves the inequality (10). Now,

C4

(X

µ)2dΩ +

(X

µ)2dΩ

(cid:18)ZA
C4ǫ

−

(X

−

ZAc
µ)2dΩ +

−

(X

−

(cid:19)
µ)2dΩ

ZAc
µ)2dΩ

(X

−

(cid:19)

−

(X

µ)2dΩ

≤

(cid:19)

ZA

(cid:18)ZA
C4ǫ

−

(X

p

−

(cid:17) (cid:18)ZA

µ)2dΩ

Also,

Therefore, for ǫ

0.5 we get that

≤

E

(X

µ)2

A
|

−

1
µ(A)

1

−

=

≥

(cid:1)

(X

µ)2dΩ

ZA

C4ǫ

−
σ2.

(cid:16)

p

(cid:17)

E

(X

µ)2

A
|

−

=

(X

µ)2dΩ

−

1
µ(A)
1

ZA
σ2.

≤

1

ǫ

−

(cid:1)

(cid:0)

(cid:0)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

As an immediate corollary of Lemma 3.11 and Lemma 3.12, we get for a random variable x

having bounded fourth moments

Corollary 3.13. Let A be an event that happens with probability 1

η. Then,

where Σ

(cid:16)
|A is the conditional covariance matrix Σ

p

(cid:17)

O(

C4η)

Σ

1

−

−

(1 + 2η)Σ,

Σ

|A (cid:22)
(cid:22)
|A := E(x x T

21

A)
|

−

(E(x

A))(E(x
|

A))T .
|

Proof. Let v be any unit vector. Let y be the random variable that is v T x for x
µ. Then
µ = E(y), µA = E(y

. Let

∼ D

A), and d = µA −
|
µA)2
µ
A) = E((y
|

−

E((y

−

By Lemma 3.11 and Lemma 3.12,

d)2

−

A) = E((y
|

= E((y

µ)2
µ)2

A)
|
A)
|

−

−

−

−

2dE(y
d2

µ

A) + d2
|

−

E((y

E((y

µA)2
µA)2

A)
|
A)
|

−

−

−

−

E((y

E((y

−

−

µ)2)
µ)2)

≤

2ηE((y

µ)2)

−
C4ηE((y

≥ −

≥ −

p

C4η +

d2

−

µ)2)

−
8C4η3

E((y

µ)2)

−

(cid:16)p

p

(cid:17)

Finally, by a standard argument as in the proof of Chebyshev’s inequality, we have

Lemma 3.14 (Concentration). For every unit vector v , we have

C4
t4 ,
where σv is the standard deviation of x along the direction v , σ2

x T v
|
(cid:0)

Ex T v

tσv

| ≥

Pr

−

≤

(cid:1)

x T v
v := E
|

2
|

− |

Ex T v

2.
|

3.4 Proof of Theorem 1.3:

3.4.1 One Dimensional Distribution

First we will consider the case when X is a random variable with mean µ and variance σ2 satisfying

E((X

µ)4)

C4σ4.

−

≤

In this case, median need not be a good estimator. Instead, we will consider the interval of minimum
η) fraction of the sample points. Let S be the given sample,
length that contains (1
and let
S) be our estimator. We will show
below that
e

−
S be the points lying in this interval. Let

By the concentration inequality stated in Lemma 3.14, we get that for the distribution, the

(η + ǫ)3/4σ

µ = mean(

ǫ)(1

µ
|

| ≤

−

−

−

O

µ

e

b

η

.

C 1/4
4
(cid:16)

(cid:17)

of the interval around µ consisting of probability mass 1

is bounded by

length r1− η+ǫ
b
2

η+ǫ
2

−

We will refer to this interval by I1− η+ǫ
then with probability 1

2

1/ poly(n) for every interval I

. We note that by VC theorem if

SD|
|

= Ω

log n+log 1/ǫ
ǫ2

,

(cid:16)

(cid:17)

−

Pr (x
∈
|
The length of the smallest interval that contains (1
length of the smallest interval that contains 1

Pr (x

∼ D

−

∈

x

η

I

)

|

|
η

η) fraction of S is at most the
ǫ fraction of SD. This latter quantity is bounded

ǫ)(1

−

−

−

R,

⊂
x ǫu SD)

I

ǫ/2.

| ≤

r1− η+ǫ

2 ≤

C 1/4
4
η+ǫ
2

σ.

1/4

(cid:0)

(cid:1)

−

−

22

by r1−η, since the interval I1− η+ǫ
SD.

2

contains with probability 1

1/ poly(n) a (1

η

ǫ) fraction of

−

−

−

This implies that when we look at the minimum interval containing 1

η

noise points, the extreme points of the interval can be at most at a distance r1− η+ǫ

−

−

ǫ fraction of the non-
from µ. Thus,

2

the distance of all noise points will be within O

. Furthermore, the interval of minimum

η

length with (1
by Lemma 3.11 the mean of
from the true mean.

ǫ)(1

−

−

−

e

η) fraction of S will contain at least 1

S will be within η

r1−η + O

3η
C4(η + ǫ)3σ

−

−

4

ǫ fraction of SD. Therefore,
C 1/4
4
(cid:16)

(η + ǫ)3/4σ

= O

(cid:17)

(cid:17)

(cid:16)

p

1/4
C
4
(η+ǫ)1/4 σ

(cid:19)

(cid:18)

·

3.4.2 Multi-dimensional Case

We will now consider the multidimensional case. Let
random variable that satisﬁes for every direction v

D

be a distribution on Rn and x

is a

∼ D

E(((x

µ)T v )4)

C4

E(((x

−

≤

−

µ)T v )2)

2

,

for some C4.

(cid:0)
For any direction v , let µv = µT v. From the previous section, we know that we can ﬁnd a

(cid:1)

µv

such that

Therefore, by picking n orthogonal directions v 1, ..., v n, we get

µv
|

µv| ≤

−

O(C 1/4
4

(η + ǫ)3/4σv ).

Lemma 3.15. Given O
O(C 1/4
4

(η + ǫ)3/4

(cid:16)
Tr(Σ)).

n log n
ǫ2

b

(cid:17)

samples, we can ﬁnd a vector aaa

Rn such that

∈

aaa
k

µ

k2 =

−

We will now bound the radius of the ball in the outlier removal step (Algorithm 2). We claim

p

the radius of the ball is O

. Suppose we have some x

. Let z = x

||
Using the n orthogonal directions as picked above, let zi = z T v i and let Z 2 =
Consider the following:

∼ D

p

(cid:19)

(cid:18)

1/4
C
4
(η+ǫ)1/4

n

Σ

||2

z2
i =

P

Pr

Z 2

 

≥

C 1/2
Σ
4 n
||
(η + ǫ)1/2 !

||2

= Pr

Z 4

C4n2

Σ
||
η + ǫ

2
2
||

≥

(η + ǫ)E(Z 4)
C4n2

Σ

2
2
||

||

≤

(cid:19)

It suﬃces to bound the right-hand side of (11) by O(η + ǫ), in which case the ball will contain
1

ǫ fraction of the probability mass of

. We have

η

−

−

E(Z 4) = E

z2
i



Xi

Xj

z2
j 

≤

n2 max
i

E

z4
i

C4n2

Σ
k

2
2
k

≤

(cid:0)

(cid:1)


due to the fourth moment condition and the fact that E((z T v i)2)



1/4
C
4
(η+ǫ)1/4

Tr(Σ)

(cid:18)

n

k2. Therefore, a ball of
aaa
k2 =
k
p
, we get that the radius of the ball computed in the outlier removal step

≤ k
ǫ fraction of the points. Since

contains 1

||2

Σ

Σ

−

−

−

(cid:19)

µ

||

η

radius at most O

O

C 1/4
4
(cid:16)
is O

(η + ǫ)3/4
1/4
C
4
(η+ǫ)1/4

n

(cid:18)

p
Σ
k

k2

(cid:19)

p

(cid:17)

. We have proved

(cid:18)

D

23

b

µ.

2
2.
k

−
z

k

(11)

(12)

Lemma 3.16. After the outlier removal step, every remaining point x satisﬁes

x
k

−

µ

k2 ≤

O

 

C 1/4
4
(η + ǫ)1/4

n

Σ
k

k2

.

!

p

Consider the covariance matrix Σ

S (recall that

S be the set of points in

SD ⊂
points sampled by the adversary. Let µ
e
Note that

S that were sampled from the distribution
e
S), µ
SN
e

S := mean(
e

:= mean(

e

e

e

S is the sample after outlier removal). Let
S be the
SD).

and
D
SN ) and µ

SN ⊂
SD := mean(
e
e

e

eS of

µ

S =
e

ηµ

e
+ (1
−

η)µ

SD ,
e

SN
e

e

η = | eSN |
| eS|

is the fraction of noise points after the outlier truncation step. Note that
e

where
≤
η
1−2η−ǫ = O(η). We will therefore pretend that the fraction of noise points is still η after the outlier
is µ = 0. By Lemma 3.11
truncation step. We again assume that the mean of the distribution
applied with X = x T µ eD
and where A is the event that x is not removed by outlier
removal, we have that

kµ eDk for x

∼ D

D

e

e

e

η

e

(13)

Suppose, after the outlier removal step, we had the guarantee that the covariance matrix of the

remaining points from the distribution

, say Σ

D

µ
k

eDk2 = O(C 1/4

4

k2).

(η + ǫ)3/4

Σ
k
p
eD, is between
β(1
Σ

η)Σ

α(1

η)Σ

−

(cid:22)

eD (cid:22)

−

in the Lowener ordering. Corollary 3.13 gives α = 1

by Lemma 3.1 and Lemma 3.16 we have that if

O(

−
= Ω

C4(η + ǫ)) and β = 1 + O(η + ǫ). Also,
n log n
ǫ2

, then

S
|

eD|

p
(cid:16)

ǫ

C 1/4
4
(η + ǫ)1/4

(1

−

)Σ

eD (cid:22)

ΣS eD (cid:22)

(1 +

(cid:17)
C 1/4
4
(η + ǫ)1/4

ǫ

)Σ

eD

We will use the notation as deﬁned above.

Lemma 3.17. Let W be the bottom n/2 principal components of the covariance matrix ΣS. For
some constant C, we have

where δµ = µ

µ

eSN −

ηP W δµ
k
eSD .

2
k

≤

O

(βη + C 1/2
(cid:16)

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

,

(cid:17)

By an inductive application of the above lemma, we can prove

Theorem 3.18. On input (S, n), AgnosticMean outputs

µ satisfying

µ
k

2
k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
b
k

kmin

(1 + log n).

(cid:17)

(cid:16)

b

24

Theorem 3.18 with Corollary 3.13 proves Theorem 1.3.

Proof of Lemma 3.17:
have

Recall that Σ denotes the covariance matrix of the points from

. We

D

Σ

eS = (1
= (1

η)Σ

η)Σ

eSD + ηΣ
eSD + A,

−

−

+ (η

−

eSN

η2)δµδT
µ

where A := ηΣ

+ (η

SN
e

−

η2)δµδT

µ. Therefore, we have

(1

η)αΣ + A

−

(1

η)βΣ + A.

−

Σ

(cid:22)

eS (cid:22)
1/4
C
4
(η+ǫ)1/4

By Lemma 3.16 each x i satisﬁes

x ik
k

= O

(cid:18)
η√C4k
Σ
√η + ǫ

k2n

, so we have

n

Σ
k

k2

(cid:19)

p

O

≤

C4η

Σ
k

k2n

.

Tr(A) = O

(cid:19)
For a symmetric matrix B, let λk(B) denote the k’th largest eigenvalue. By Weyl’s inequality,

(cid:16)p

(cid:18)

(cid:17)

(14)

λk((1

η)Σ

S + A)
e

≤

−

λk(A) + (1

η)β

Σ
k

k2.

−

≤
By Equation (14), there exists a constant

(cid:0)

(cid:1)

C such that

λn/2

Σ
S
e

λn/2 (A) + (1

η)β

Σ
k

k2.

−

we have that

Therefore,

we have,

e
λn/2 (A)

Tr(A)
n/2

≤

≤

C

C4η

Σ
k

k2,

p

e

eS)
Recall that W is the space spanned by the bottom n/2 eigenvectors of Σ
corresponding to the projection operator on to W . We therefore have

λn/2(Σ

k2 +

Σ
k

Σ
k

C4η

η)β

k2

p

(1

−

≤

C

e

eS, and P W is the matrix

(1

η)αP T

W ΣP W + ηP T

W Σ

−
Multiplying by the vector P W δµ

eSN

P T

W Σ

eSP W (cid:22)
P W +(η

((1

−

η)β +

Σ
C4η)
C
k
η2)(P W δµ)(P W δµ)T
e

p

k2I
(cid:22)

−

kP W δµk and its transpose on either side, we get

η

P W δµ
k

2
k

≤

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin.

((1

η)β +

C

−

C4η)
Σ
k

k2I

p

e

where C = eC

1−η . We therefore have

ηP W δµ
k

2
k

≤

η

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin

.

(cid:16)

(cid:17)

p

p
25

2. The proof is by
Proof of Theorem 3.18:
By Equation 13, it is enough to bound
induction on the dimension. If n = 1, then the conclusion follows from the guarantees for the one
1.
dimensional case proven in Section 3.4.1. Now, assume that it holds for all n
Let n = k + 1. We have by Lemma 3.17

k for some k

µ
k

eSD k

≤

−

≥

µ

b

P W
k

µ

µ

eSD

−

2 =
k

(cid:16)

b

(cid:17)

ηP W δµ
k
O

2
k
(βη + C 1/2
(cid:16)

≤

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(cid:17)

Recall that we deﬁned V to be the span of the top n/2 principal components of Σ

S. By
e

induction hypothesis, since dim(V ) = n/2, we have

µV −
k

P V µ

2
eSD k

≤

O

(βη + C 1/2
(cid:16)

4

Therefore, adding the two, we get

b

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

−

µ

2
eSD k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

(cid:17)

(cid:17)

b

(cid:16)

4 Covariance Estimation

4.1 One Dimensional Case

Observation 4.1 (1d Covariance Estimate).

1. Let

be a distribution with mean µ and co-

D
= N (µ, σ2), then there is an algorithm that takes as input m = Ø

log n
ǫ2

variance σ2. If
samples x 1, ...x m ∼ Dη and computes in polynomial time

D

σ2 such that

σ2

σ2

= O(η +ǫ)σ2.
(cid:16)
(cid:17)

−

(cid:12)
(cid:12)

b

(cid:17)

−

−

σ2

(cid:16)
σ2

(cid:12)
(cid:12)b

= O

∼ D

2. If x

log n+log 1/ǫ
ǫ2

has bounded fourth moments with constant C4, and (x

µ)2 has bounded fourth
(cid:12)
(cid:12)b
moments with constant C4,2. Then there is an algorithm that takes as input η and m =
σ2 such that
samples x 1, ...x m ∼ Dη and computes in polynomial time
O
4,2 (η + ǫ)3/4C 1/2
C 1/4
4 σ
(cid:16)

= N (µ, σ), and we are given m = poly(n) samples S =

(cid:12)
is supported on R, we can estimate the variance in the following
Proof. When the distribution
(cid:12)
just having bounded eighth moments separately.
= N (µ, σ) and
way. We will consider the case
Suppose
, xi ∼ Dη. There are
x1, ..., xm}
{
D
several ways to estimate σ, we describe here one of them. First we compute the median, and let
85.1. Let Cσ be the
xmed = mediani{
µ. By Lemma 3.3,
c1’th quantile of S. Then our estimate for the standard deviation is
O(C 1/4
8 ησ2).
µ
= O(ησ). For a similar reason, Cσ = σ
we have
±
is a distribution that has bounded eighth moments, the result follows from the 1d

. Let Φ(x) be the c.d.f. of N (0, 1). Note that c1 = Φ(1)

∼
σ = Cσ −

O(ησ). Therefore,

xi}

D
D

±

D

(cid:17)

b

|

.

mean estimation in Section 3.4 applied (x

µ)2. Note that E(x

σ2 = σ2
b
µ)2 = σ2 and

c

b

µ
|
When
b

−
D

E

(x

µ)2

2

σ2

= E(x

µ)4

−

−

−

(cid:0)

−
σ4

−

−
C4σ4.

(cid:1)

≤

26

From Section 3.4, we therefore have that if m = O
C 1/4
4,2 (η + ǫ)3/4C 1/2
4 σ
(cid:16)

σ2
|

| ≤

σ2

−

O

(cid:17)

.

(cid:16)

b
4.2 Multi-Dimensional Case: Theorem 1.5

log n+log 1/ǫ
ǫ2

(cid:17)

, there is a poly(n) algorithm with

In this section we will prove that CovarianceEstimation (Algorithm 4) gives Theorem 1.5.
is a distribution with mean µ and covariance Σ
Throughout this section, we will assume that
and has bounded fourth moments with parameter C4. We use the following symmetrization trick
to assume that

has mean 0. Given samples S =

, let

D

x 1, ..., x m}
{

D

x ′

i =

x i −

x i+m/2
√2

for i

.
1, ..., m/2
}

∈ {

(15)

Since η fraction of the original samples were corrupted on average, only 2η fraction of the new
samples will be corrupted on average. Moreover, if x , y
are independent random variables,
then we can show that the distribution of x ′ = (x
y)/√2 has bounded fourth moments with
′ the distribution of x ′. CovarianceEstimation
parameter
is just the mean estimation algorithm on S(2) =
, we can appeal to Theorem 1.3.
}
Furthermore, let
Note that

′ be an aﬃne transformation of a 4-wise independent distribution.

C4 + 3/2. We will denote by

x ′x ′T
{

∼ D

x
|

−

≤

D

D

∈

S

Ex ∼D′x x T = Σ.

By Theorem 1.3, we have

Σ
k

Σ

kF = O

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

Σ(2)
k

1/2
2
k

log1/2 n,

b
where Σ(2) is covariance matrix of x x T , x

By Proposition 4.2 , we have

′.

∼ D

Σ
k

kF = O
Σ

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

C 1/2
4 k

Σ

k2 log1/2 n,

which proves Theorem 1.5.

b

(cid:17)

(cid:17)

We will now derive a bound for

Σ(2)
k

k2 when the distribution has bounded fourth moments

and is 4-wise independent. In particular, we will prove

Proposition 4.2. If Σ(2) is the covariance matrix of x x T , x

′, it holds that

∼ D

Σ(2)
k

k2 ≤

O

Σ
C4k

2
2
k

.

(cid:0)

(cid:1)

27

Proof of Proposition 4.2:

Note that E(Y ) = Σ.

E(((Y

E(Y ))

V )2) = E

−

·

2

(Y ij −



Xij

E((Y ij −

Σij)Vij

Σij)(Y kl −

Σkl))VijVkl

E(Y ijY kl −

ΣijΣkl)VijVkl

E(xixjxkxl −

ΣijΣkl)VijVkl.

=

=

=

Xijkl

Xijkl

Xijkl



Next we note that

Therefore,

E(xixjxkxl)

−

ΣijΣkl = 


Σ2

ii if i = j = k = l

E(x4
i )
−
E(x2
i x2
j ) if i = k, j = l or i = l, j = k
0 otherwise.

max
V :kV kF =1

E(((Y

E(Y ))

V )2) = max

−

·

V :kV kF =1

(E(x4
i )

Σ2

ii)V 2

ii + 2

ΣiiΣjjV 2
ij

(E(x4
i )

2Σ2

ii)V 2

ii +

ΣiiΣjjV 2
ij

−

−

Xi<j

Xi,j

2Σ2

ii + max

Σ2
ii.

i

Xi

= max

V :kV kF =1

max
i
O (C4)

Xi
E(x4
i )
−
2
2.
k

Σ
k

≤

≤

5 Estimating

k2: Theorem 1.6
Σ

k

As in Section 4.2, we assume that the true distribution has mean µ = 0.

SN be the given sample, where SD consists of points from some distribution

In this section, we will prove AgnosticOperatorNorm (Algorithm 5) gives Theorem 1.6. Let
with mean
S = SD ∪
µ and covariance Σ and SN consists of points picked by the adversary. Let ΣSD be the sample
has 1D concentration, i.e., there exists a constant γ such that
covariance of SD. We assume that
for every unit vector v

D

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

(cid:17)

S be the remaining sample at the end of the algorithm and let

SD be points in

S sampled from

5.1 Correctness

Let
.

D

e

e

e

28

Deﬁnition 5.1. Given a set of points S

Rn and a vector aaa

Rn, we let

Σaaa (S) :=

(x

aaa)(x

−

−

⊂

1
S
|

x ∈S
| X

∈

aaa)T .

First, we will argue that the covariance of the true distribution is well-approximated by Σµ(

SD).

Lemma 5.2. With probability 1

1/ poly(n),

−

e

Σ
k

−

Σ0(

SD)

η

Σ
k

k

k ≤

Proof. First, note that the t computed in SafeOutlierTruncation is at most O(Tr(Σ)) because
e
σ2
v (namely that the estimated
by an analogous argument as in Section 4.1, we have
v ≤
σv in a direction v is close to the true variance σv in that direction). Then the ball in
variance
SafeOutlierTruncation has radius R = c1
Tr(Σ) log1/γ n
η for some constant c1. We have that
b
deviates from the mean by more than c1σv log1/γ n
b
in any direction v , the probability that x
p
η
∼ D
is 1/ poly( n
η ). Then if we take n orthogonal directions, the probability that any given point is
more than distance R from µ is still 1/ poly( n
η ). Thus, step (1) of the algorithm will remove only
1/ poly( n
η ) fraction of the points sampled from

(1 + O(η))σ2

.

In every direction v , the probability mass of points from

outside an interval of size c2σv log1/γ n
η
around the mean is at most 1/ poly( n
η ), where σv is the variance in the direction v . Let Ci be the
region between the two hyperplanes used for truncation in iteration i. Therefore, if the number of
iterations is O(n log2/η n

1/ poly( n

η ), we will have that Pr (x

) = 1

η ).

ﬁnite k. By Lemma 3.12, we have that the covariance matrix Σ0 (
that of Σ:

Note that 1d concentration implies that the distribution has bounded 2k’th moment for all
D ∩i Ci is close to
(16)

(1 + 1/ poly(

1/ poly(

))Σ

x
∈ ∩iCi |

∼ D

(1

D

−
D ∩i Ci) of
n
))Σ.
η

D ∩i Ci)

(cid:22)

Σ0 (

n
η

(cid:22)

−

D

D ∩i Ci) to Σ0

, we use Proposition 3.2. The concept class we use is
Finally, to relate Σ0 (
all degree two polynomials restricted to convex polytopes with at most O(n) facets, deﬁned by
e
the hyperplanes used for truncation at each iteration of the algorithm. The VC dimension of this
concept class is O(n2 log n). Therefore, by Proposition 3.2 applied with R = c1

Tr(Σ) log1/γ n

SD

(cid:16)

(cid:17)

Σ
c1k

1/2n1/2 log1/γ n
k

η , we get that if we take m = O

n3(log1/γ n
η2

η )2 log n

then

p

(cid:18)

(cid:19)

Σ0
k

SD

Σ0 (

D ∩i Ci)

−

k ≤

Σ
η/2
k

.
k

(cid:16)

(cid:17)

Combining equations 16 and 17 we get the desired result.

e

Theorem 5.3. When the algorithm terminates, we have:

(1

Σ
η)
k

−

k2 ≤ k

Σ0(

S)

k2 ≤

(1 + O(η log2/γ n
η

Σ
))
k

k2.

Proof. First, note that since only an η fraction of

S is noise, we have

e

Σ0(

S)

(1

η)Σ0(
e
−

(cid:23)

SD)

e

29

e

η ≤

(17)

(18)

Therefore, we have that

Σ0(
k
bound. For the upper bound, let v be the top eigenvector of Σ0(
we have

Σ0(
η)
k

k2 ≥

SD)

S)

(1

−

e

e

k2. Lemma 5.2 gives the desired lower
S). When the algorithm terminates,

Σ0(
k

S)

k2 = v T Σ0(

e

S)v
(1 + O(η log2/γ n
e
η
(1 + O(η log2/γ n
η

≤

≤

))v T Σv

Σ
))
k

k2.

e

where the second line follows because of the termination condition and because we can estimate
the variance of

in any direction to within a (1

cη) factor.

D

±

5.2 Termination

In this section, we will show that with high probability, Algorithm 5 terminates in a polynomial
1
number of steps provided that η
C for some constant C that depends only on the estimation in
Step (5).

≤

Every time the algorithm goes through another iteration, it must remove a certain number of
noise points. Suppose in step (7), we remove r noise points. The noise conﬁguration of maximum
r
variance puts r amount of noise at the outlier removal distance d1 = c1

Tr(Σ) log1/γ n

c2

σv log1/γ n
η
b
2

. We can then write an upper

p

η , and η

−

amount of noise at the truncation threshold distance d2 =
bound on σ2.

σ2 =

(1
k

−

η)Σ0(

SD) + ηΣ0(

2
SN )
2 ≤
k

v + rd2
σ2

1 + (η

r)d2
2

−

This implies

e

e

Let us simplify the numerator Z = σ2
(1 + c3η log2/γ n
σ2
η )
v ≤
is less than some constant.

σ2. Here we also assume that η

σ2

r

≥

σ2
v −

−

ηd2
2

σ2
v −
d2
2

−
d2
1 −
ηd2
2. Since we are truncating the sample, we have
1
1−cη

1
C for a suﬃciently large C so that

≤

1

b

Z

σ2

≥

−

σ2
1 + c3η log2/γ n

+ η

2 log2/γ n
c2
η
4

!

1

cη

η  

−
cη + (cη)2 + η

2 log2/γ n
c2
η
4

1 + c3η log2/γ n
η

c3η log2/γ n

η −

(cη)2
η
1 + c3η log2/γ n
η

−

2 log2/γ n
c2
η
4

σ2

σ2

≥

−

1

−





σ2

≥















30

Recall that σ2

(1

Σ
η)
k

k2 by (18). Then as long as c3 is a suﬃciently large constant, we have

−

≥

Then combining Z with the denominator from earlier and using the fact that d1 ≤
we get:

c1

n

Σ
k

k2 log1/γ n
η ,

p

Z

≥

Σ
k2
k
4  

c3η log2/γ n
η
1 + c3η log2/γ n

η !

r

≥

Σ
k

c3η log2/γ n/η
1+c3η log2/γ n

k2
(cid:18)
4c2
Σ
1k

η (cid:19)
k2n log2/γ n
η
c3η
1n(1 + c3η log2/γ n
η )

≥

4c2

Then r

≥
iterations.

O

min

(cid:18)

(cid:26)

η
n ,

1
n log2/γ n

η (cid:27)(cid:19)

Open Questions

, so the algorithm will terminate in a nearly linear number of

An immediate open question is whether the our analysis of the mean estimation algorithm is
tight and the √log n is avoidable. For special distributions including Gaussians, [DKK+16] give
η rather than η√log n or √η log n
an algorithm with higher sample complexity and error η
as in Theorem 1.1. An open question is to give an O(η) approximation. For the more general
distributions considered here, the dependence on η must grow as at least η3/4; it is open to ﬁnd an
algorithm that achieves O(η3/4) error (our guarantee for the general setting has error O(√η log n)).
Other open problems include agnostic learning of a mixture of two arbitrary Gaussians and agnostic
sparse recovery.

log 1

q

We thank Chao Gao and Roman Vershynin for helpful discussions. We would also like to thank the
anonymous reviewers for useful suggestions. This research was supported in part by NSF awards
CCF-1217793 and EAGER-1555447.

Acknowledgment

References

[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with
unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In
NIPS, pages 2384–2392, 2012.

[AK01]

Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Pro-
ceedings of the thirty-third annual ACM symposium on Theory of computing, pages
247–257. ACM, 2001.

31

[BCV13]

[Bru09]

Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of
tensor decompositions with applications to polynomial identiﬁability. arXiv preprint
arXiv:1304.8087, 2013.

S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In Proceedings of
the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2009,
New York, NY, USA, January 4-6, 2009, pages 1078–1087, 2009.

[Bru11]

David Bruce. A multivariate median in banach spaces and applications to robust pca.
http://www-personal.umich.edu/~romanv/students/bruce-REU.pdf, 2011.

[BRV13] Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the

presence of Gaussian noise. In Proc. of COLT, 2013.

[BS10]

[BV08]

[Car98]

Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In 51th
Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 103–112, 2010.

S Charles Brubaker and Santosh S Vempala. Isotropic PCA and aﬃne-invariant clus-
tering. In Building Bridges, pages 241–281. Springer, 2008.

J-F Cardoso. Multidimensional independent component analysis. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference
on, volume 4, pages 1941–1944. IEEE, 1998.

[CGR15] M. Chen, C. Gao, and Z. Ren. Robust Covariance Matrix Estimation via Matrix Depth.

ArXiv e-prints, June 2015.

[CJ10]

Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation.
Academic Press, 2010.

[CLMW11] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal com-

ponent analysis? J. ACM, 58(3):11:1–11:37, June 2011.

[CR08]

Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using
correlations and independence. In Proc. of COLT, 2008.

[CSPW11] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-
SIAM Journal on Optimization,

sparsity incoherence for matrix decomposition.
21(2):572–596, 2011.

[Das99]

[Dav87]

[DG92]

Sanjoy Dasgupta. Learning mixtures of Gaussians. In Foundations of Computer Sci-
ence, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999.

P. L. Davies. Asymptotic behaviour of s-estimates of multivariate location parameters
and dispersion matrices. Ann. Statist., 15(3):1269–1292, 09 1987.

David L. Donoho and Miriam Gasko. Breakdown properties of location estimates based
on halfspace depth and projected outlyingness. Ann. Statist., 20(4):1803–1827, 12 1992.

32

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the computational
intractability. CoRR, abs/1604.06443, 2016.

[Don82]

[DS07]

David L. Donoho. Breakdown Properties of Multivariate Location Estimators. PhD
thesis, Harvard University, 1982.

Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of
separated, spherical Gaussians. The Journal of Machine Learning Research, 8:203–226,
2007.

[FJK96]

Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In
FOCS, pages 359–368, 1996.

[GVX14]

Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decom-
position. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,
pages 584–593. ACM, 2014.

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment
methods and spectral decompositions. In ITCS, pages 11–20, 2013.

[HKO01]

Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis.
Wiley, 2001.

[HPL91]

Peter J. Rousseeuw Hendrik P. Lopuhaa. Breakdown points of aﬃne equivariant es-
timators of multivariate location and covariance matrices. The Annals of Statistics,
19(1):229–248, 1991.

[HRRS11] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.
Robust statistics: the approach based on inﬂuence functions, volume 114. John Wiley
& Sons, 2011.

Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.,
35(1):73–101, 03 1964.

Peter J. Huber.
Statistics, pages 1248–1251. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.

International Encyclopedia of Statistical Science, chapter Robust

[KMV10] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Eﬃciently learning mixtures
of two Gaussians. In Proceedings of the 42nd ACM symposium on Theory of computing,
pages 553–562. ACM, 2010.

[KRV]

https://github.com/kal2000/AgnosticMeanAndCovarianceCode.

[KV09]

Ravi Kannan and Santosh Vempala. Spectral Algorithms. Now Publishers Inc, 2009.

Nojun Kwak. Principal component analysis based on l1-norm maximization. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672–1680, 2008.

Ricardo Antonio Maronna. Robust m-estimators of multivariate location and scatter.
Ann. Statist., 4(1):51–67, 01 1976.

[Hub64]

[Hub11]

[Kwa08]

[Mar76]

33

[MMY06] RARD Maronna, Douglas Martin, and Victor Yohai. Robust statistics. John Wiley &

Sons, Chichester. ISBN, 2006.

[MSY92]

Ricardo A. Maronna, Werner A. Stahel, and Victor J. Yohai. Bias-robust estimators
of multivariate scatter based on projections. J. Multivar. Anal., 42(1):141–161, July
1992.

[MT+11] Michael McCoy, Joel A Tropp, et al. Two proposals for robust pca using semideﬁnite

programming. Electronic Journal of Statistics, 5:1123–1160, 2011.

Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures
of Gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE
Symposium on, pages 93–102. IEEE, 2010.

Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion
for high-dimensional datasets. Technometrics, 2012.

Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH
and NTRU signatures. J. Cryptology, 22(2):139–160, 2009.

J. R. Kettenring S. J. Devlin, R. Gnandesikan. Robust estimation of dispersion ma-
trices and principal components. Journal of the American Statistical Association,
76(374):354–362, 1981.

[Sma90]

Christopher G Small. A survey of multidimensional medians. International Statistical
Review/Revue Internationale de Statistique, pages 263–277, 1990.

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics, 12(4):389–434, 2012.

John W. Tukey. Mathematics and the Picturing of Data. In Ralph D. James, editor,
International Congress of Mathematicians 1974, volume 2, pages 523–532, 1974.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models.
Journal of Computer and System Sciences, 68(4):841–860, 2004.

Santosh Vempala and Ying Xiao. Max vs min: Tensor decomposition and ICA with
nearly linear sample complexity. In Proceedings of The 28th Conference on Learning
Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1710–1723, 2015.

[XCM10] Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis
with contaminated data: The high dimensional case. arXiv preprint arXiv:1002.4658,
2010.

[MV10]

[MZ12]

[NR09]

[SJD81]

[Tro12]

[Tuk74]

[VW04]

[VX15]

34

Agnostic Estimation of Mean and Covariance

Kevin A. Lai∗

Anup B. Rao∗

Santosh Vempala∗

August 16, 2016

Abstract

We consider the problem of estimating the mean and covariance of a distribution from iid
samples in Rn, in the presence of an η fraction of malicious noise; this is in contrast to much
recent work where the noise itself is assumed to be from a distribution of known type. The
agnostic problem includes many interesting special cases, e.g., learning the parameters of a single
Gaussian (or ﬁnding the best-ﬁt Gaussian) when η fraction of data is adversarially corrupted,
agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time
algorithms to estimate the mean and covariance with error guarantees in terms of information-
theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value
Decomposition.

6
1
0
2
 
g
u
A
 
4
1
 
 
]
S
D
.
s
c
[
 
 
2
v
8
6
9
6
0
.
4
0
6
1
:
v
i
X
r
a

∗Georgia Tech. Email: {kevinlai, anup.rao, vempala}@gatech.edu

1

Introduction

The mean and covariance of a probability distribution are its most basic parameters (if they are
bounded). Many families of distributions are deﬁned using only these parameters. Estimating the
mean and covariance from iid samples is thus a fundamental and classical problem in statistics.
The sample mean and sample covariance are generally the best possible estimators (under mild
conditions on the distribution such as their existence). However, they are highly sensitive to noise.
The main goal of this paper is to estimate the mean, covariance and related functions in spite of
arbitrary (adversarial) noise.

Methods for eﬃcient estimation, in terms of sample complexity and time complexity, play
an important role in many algorithms. One such class of problems is unsupervised learning of
generative models. Here the input data is assumed to be iid from an unknown distribution of a
known type. The classical instantiation is Gaussian mixture models, but many other models have
been studied widely. These include topic models, stochastic block models, Independent Component
Analysis (ICA) etc. In all these cases, the problem is to estimate the parameters of the underlying
distribution from samples. For example, for a mixture of k Gaussians in Rn, it is known that
the sample and time complexity are bounded by nO(k) in general [KMV10, MV10, BS10] and by
poly(n, k) under natural separation assumptions [Das99, AK01, VW04, DS07, CR08, BV08, HK13].
For ICA, samples are of the form Ax where A is unknown and x is chosen randomly from an
unknown (non-Gaussian) product distribution; the problem is to estimate the linear transformation
A and thus unravel the underlying product structure [FJK96, NR09, Car98, HKO01, CJ10, BRV13,
AGMS12, BCV13, GVX14, VX15]. These, and other models (see e.g., [KV09]), have been a rich
and active subject of study in recent years and have lead to interesting algorithms and analyses.

The Achilles heel of algorithms for generative models is the assumption that data is exactly from
the model. This is crucial for known guarantees, and relaxations of it are few and specialized, e.g.,
in ICA, data could by noisy, but the noise itself is assumed to be Gaussian. Assumptions about rank
and sparsity are made in a technique that is now called Robust PCA [CSPW11, CLMW11, XCM10].
There have been attempts [Kwa08, MT+11] at achieving robustness by L1 minimization, but they
don’t give any error bounds on the output produced. A natural, important and wide open problem
is estimating the parameters of generative models in the presence of arbitrary, i.e., malicious noise,
a setting usually referred to as agnostic learning. The simplest version of this problem is to estimate
a single Gaussian in the presence of malicious noise. Alternatively, this can be posed as the problem
of ﬁnding a best-ﬁt Gaussian to data or agnostically learning a single Gaussian. We consider the
following generalization:

Problem 1 [Mean and Covariance] Given points in Rn that are each, with probability 1
η
from an unknown distribution with mean µ and covariance Σ, and with probability η completely
arbitrary, estimate µ and Σ.

−

There is a large literature on robust statistics (see e.g., [Hub11, HRRS11, MMY06]), with the
goal of ﬁnding estimators that are stable under perturbations of the data. The classic example for
points on a line is that the sample median is a robust estimator while the sample mean is not (a single
data point can change the mean arbitrarily). One measure for robustness of an estimator is called
breakdown point, which is the minimum fraction of noise that can make the estimator arbitrarily
bad. Robust statistics have been proposed and studied for mean and covariance estimation in high

1

dimension as well (see [Hub64, Tuk74, Mar76, SJD81, Don82, Dav87, HPL91, DG92, MSY92, MZ12,
CGR15] and the references therein). Most commonly used methods (including M-estimators) to
estimate the covariance matrix were shown to have very low break down points [Don82]. The
notion of robustness we consider quantiﬁes how far the estimated value is from the true value. To
the best of our knowledge, all the papers either suﬀer from the diﬃculty that their algorithms are
computationally very expensive, namely exponential time in the dimension, or have poor or no
guarantees for the output. Tukey’s median [Tuk74]) is an example of the former. It is deﬁned as
x i}i. As proven in [CGR15], this is an
the deepest point with respect to a given set of points
{
optimal estimate of the mean. But there is no known polynomial time algorithm to compute this.
Another well-known proposal (see [Sma90]) is the geometric median:

arg min

y

y
k

x ik2.

−

Xi
This has the advantage that it can be computed via a convex program. Unfortunately, as we
observe here (see Proposition 2.1), the error of the mean estimate produced by this method grows
polynomially with the dimension (also see [Bru11]).

This leads to the question, what is the best approximation one can hope for with η arbitrary
(adversarial) noise. From a purely information-theoretic point of view, it is not hard to see that
even for a single Gaussian N (µ, σ2) in one dimension, the best possible estimation of the mean will
have error as large as Ω(ησ), i.e., any estimate ˜µ can be forced to have
= Ω(ησ). For a more
general distribution, this can be slightly worse, namely, Ω(η3/4σ) (see Section 2.1). What about in
Rn? Perhaps surprisingly, but without much diﬃculty, one can show that the information-theoretic
upper bound matches the lower bound in any dimension, with no dependence on the dimension.
This raises a compelling algorithmic question: what are the best estimates for the mean and
covariance that can be computed eﬃciently?

µ
k

−

˜µ

k

In this paper, we give polynomial time algorithms to estimate the mean with error that is close
to the information-theoretically optimal estimator. The dependence on the dimension, of the error
in the estimated mean, is only √log n. To the best of our knowledge, this is the ﬁrst polynomial-
time algorithm with an error dependence on dimension that is less than √n, the bound achieved by
the geometric median. Moreover, as we state precisely later, our techniques extend to very general
input distributions and to estimating higher moments.

Our algorithm is practical. A matlab implementation for mean estimation can be found in
[KRV]. It takes less a couple of seconds to run on a 500-dimensional problem with 5000 samples
on a personal laptop.

Model. We are given points x 1, ..., x m ∈
η
−
probability each x i is independently sampled from a distribution
with mean µ and covariance
Σ, and with η probability it is picked by an adversary. For ease of notation, we will write x i ∼ Dη
when we want to say the x i is picked according to the above rule. The problem we are interested
in is to estimate µ and Σ given the samples. In the following, we will consider mainly two kinds of
distributions.

Rn sampled according to the following rule. With 1

D

Gaussian

= N (µ, Σ) is the Gaussian with mean µ and covariance Σ.

D
Bounded Moments Let

D

is a distribution with mean µ and covariance Σ. We say it has

2

bounded 2k’th moments if there exists a constant C2k such that for every unit vector v ,

E

(x

µ)T v

2k

C2k

E

(x

≤

µ)T v

−

−

(cid:0)

(cid:1)
2

(cid:16)

(cid:0)

k

2

(cid:17)

(cid:1)

x T v

Here Var
used, and for covariance estimation, C8 will be needed.
(cid:1)

=

(cid:0)

(cid:2)

(cid:3)

v T Σv

= C2k(Var

x T v

)k.

(1)

is the variance of x along v . For mean estimation, C4 will be

(cid:2)

(cid:3)

1.1 Main results

All the results we state hold with probability 1
also assume η is a less than a universal constant. We begin with agnostic mean estimation.

1/ poly(n) unless otherwise mentioned. We will

−

Theorem 1.1 (Gaussian mean). Let
algorithm that takes as input m = O
computes

µ such that the error

= N (µ, Σ), µ

D
n(log n+log 1/ǫ) log n
ǫ2

Rn. There exists a poly(n, 1/ǫ)-time
∈
independent samples x 1, ..., x m ∼ Dη and

b

µ
k

µ
k2 is bounded as follows:
(cid:16)
−

(cid:17)

b
η1/2 + ǫ

O (η + ǫ) σ√log n
Σ
k

1/2
2
k

log1/2 n

if Σ = σ2I
otherwise.

O

(cid:0)

We note that the sample complexity is nearly linear, and almost matches the complexity for

(cid:1)

mean estimation with no noise.

Remark 1.2. If we take m = O
samples, and assume that η < c/ log n for a
small enough constant c > 0, then by combining theorems 1.5 and 1.1, we can improve the η depen-
log1/2 n.
dence for the non-spherical Gaussian case in Theorem 1.1 to

µ
k2 = O
(cid:0)
Our next theorem is a similar result for much more general distributions.

1/2
Σ
2
k
k

µ
k

η3/4

−

(cid:17)

(cid:16)

(cid:1)

n2(log n+log 1/η) log n
η2

b

D

be a distribution on Rn with mean µ, covariance Σ, and
Theorem 1.3 (General mean). Let
bounded fourth moments (see Equation 1). There exists a poly(n, 1/ǫ)-time algorithm that takes
independent samples x 1, ..., x m ∼ Dη, and
as input a parameter η and m = O
µ
computes
(cid:16)
k
−
C 1/4
4
η1/2 + C 1/4
(cid:16)

µ
k2 is bounded as follows:
σ√log n
1/2
2
k

(η + ǫ)3/4
b
(η + ǫ)3/4

n(log n+log 1/ǫ) log n
ǫ2

µ such that the error

if Σ = σ2I

otherwise.

log1/2 n

Σ
k

O

O

(cid:17)

(cid:17)

b

4

(cid:16)

(cid:17)

The bounds above are nearly the best possible (up to a factor of O(√log n)) when the covariance

is a multiple of the identity.

Rn and covariance Σ.
Observation 1.4 (Lower Bounds). Let
Any algorithm that takes m (not necessarily O(poly(n))) samples x 1, ..., x m ∼ Dη, and computes a
µ
µ should have with constant probability the error
k

be a distribution with mean µ

−

D

∈

µ
k2 is
= N (µ, Σ)
b
has bounded fourth moments.

D

if

b

Ω(η
Ω(η3/4
p

Σ
k

k2)
k2)
Σ
k

if

D

p

3

−

−
D

, x and (x

be a distribution with mean µ and covariance Σ
D
µ)T have bounded fourth moments with constants C4
is an (unknown) aﬃne transformation of a 4-wise in-

Theorem 1.5 (Covariance Estimation). Let
and that (a) for x
µ)(x
∼ D
and C4,2(see Equation 1) respectively. (b)
dependent distribution. Then, there is an algorithm that takes as input m = O
samples x 1, ...x m ∼ Dη and η and computes in poly(n, 1/ǫ)-time a covariance estimate
η1/2 + C 1/4
(cid:16)

−
b
k · kF denotes the Frobenius norm.
= N (µ, Σ), then it satisﬁes the hypothesis of the above theorem. More generally, it holds
for any 8-wise independent distribution with bounded eighth moments and whose fourth moment
along any direction is at least (1 + c) times the square of the second moment for some c > 0. We
also note that if the distribution is isotropic, then covariance estimation is essentially a 1-d problem
and we get a better bound.

n2(log n+log 1/ǫ) log n
ǫ2
(cid:17)
Σ such that

k2 log1/2 n
Σ

4,2 (η + ǫ)3/4

kF = O

C 1/2
4 k

where

Σ
k

Σ

D

If

(cid:16)

(cid:17)

b

Theorem 1.6 (Agnostic 2-norm). Suppose
centration inequality: there exists a constant γ such that for every unit vector v

is a distribution which satisﬁes the following con-

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

≤

(cid:17)

Then, there is an algorithm that runs in poly(n, log 1

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)
independent samples x 1, ..., x m ∼ Dη, and computes

O

n3(log n/η)2 log n
η2

(cid:16)

(cid:17)

η ) time that takes as input η and m =

λmax such that

(1

O(η))

−

Σ
k

k2 ≤

1 + O(η log2/γ n/η)
(cid:17)

(cid:16)

b
k2.
Σ
k

λmax ≤
b

In independent work, [DKK+16] gave a similar algorithm, which they call a Gaussian ﬁltering
method, for agnostic mean estimation assuming a spherical covariance matrix; while their guar-
antees are speciﬁcally for Gaussians, the error term in their guarantee grows only with log(1/η)
rather than log n. They also give a completely diﬀerent algorithm based on the Ellipsoid method,
for a simple family of distributions including Gaussian and Bernoulli.

As a corollary of Theorem 1.5, we get a guarantee for agnostic SVD.

Theorem 1.7 (Agnostic SVD). Let
Let Σk be the best rank k approximation to Σ in
algorithm that takes as input η and m = poly(n) samples from
such that

is a distribution that satisﬁes the hypothesis of Theorem 1.5.
k · kF norm. There exists a polynomial time
Σk

Dη. It produces a rank k matrix

D

−

−

Σ

Σ

(cid:13)
(cid:13)
(cid:13)

Σk

F ≤ k

ΣkkF + O
Given the wide applicability of SVD to data, we expect the above theorem will have many ap-
plications. As an illustration, we derive a guarantee for agnostic Independent Component Analysis
(ICA). In standard ICA, input data points x are generated as As with a ﬁxed unknown n
n
full-rank matrix A and s generated from an unknown product distribution with non-Gaussian com-
ponents. The problem is to estimate the matrix A (the “basis”) from a polynomial number of
samples in polytime. There is a large literature of algorithms for this problem and its extensions

η log n

Σ
k

k2.

(cid:16)p

(cid:13)
(cid:13)
(cid:13)

×

(cid:17)

b

b

4

[FJK96, NR09, Car98, HKO01, CJ10, BRV13, AGMS12, BCV13, GVX14]. However, all these al-
gorithms rely on no noise or the noise being random (typically Gaussian) and require estimating
singular values to within 1/ poly(n) accuracy, and therefore unable to handle adversarial noise. On
the other hand, the algorithm from [VX15], which gives a sample complexity of ˜O(n), only requires
estimating singular values to within 1/ poly(log n). Our algorithm for agnostic SVD together with
the Recursive Fourier PCA algorithm of [VX15] results in an eﬃcient algorithm for agnostic ICA,
tolerating noise η = O(1/ logc n) for a ﬁxed constant c. To the best of our knowledge, this is the
ﬁrst polynomial-time algorithm that can handle more than an inverse poly(n) amount of noise.

4

∈

−

si|
|

η and be arbitrary with probability η, where A

Theorem 1.8 (Agnostic Standard ICA). Let x
probability 1
components of s are independent,
5
si|
E
|
|
η < ǫ/2, there is an algorithm that, with high probability, ﬁnds vectors
there exist signs ξi =
1 satisfying
poly(n, K, ∆, M, κ, 1
on real symmetric matrices of size n

Rn be given by a noisy ICA model x = As with
Rn×n has condition number κ, the
K√n almost surely, and for each i, Esi = 0, Es2
i = 1,
k ≤
M . Then for any ǫ < ∆3/(108M 2 log3 n), 1/(κ4 log n) and
such that
k2 for each column A(i) of A, using
A
k
ǫ ) samples. The running time is bounded by the time to compute ˜O(n) SVDs

b1, . . . , bn}
{

∆ and maxi E

3
| ≥

A(i)

(cid:13)
(cid:13)
n.

k
≤

ξibi

(cid:13)
(cid:13)

≤

−

−

±

∈

s

ǫ

×

Our results can also be used to estimate the mean and covariance of noisy Bernoulli product
distributions, i.e. distributions in which each coordinate i is 1 with probability pi and 0 with
probability 1
1−p . For a Bernoulli
. Then Theorem 1.3
1
2 , then
If C4 is constant, then by Theorem 1.5, we can get an

pi. In one dimension, C4 for a Bernoulli distribution is (1−p)2
+ p2
i
1−pi
i, pi = p and p

product distribution, C4 will be within a constant of maxi
can be applied to get an estimate
µ
k
estimate for the covariance.

µ for the mean. For instance, if

µ
k2 = O

p + p2

(1−pi)2
pi

≥

−

−

o

n

∀

.

η(1 + √ηp)p log n
b
(cid:1)

(cid:0)p

b

2 Main Ideas

Here we discuss the key ideas of the algorithms. The algorithm AgnosticMean (Algorithm 3)
alternates between an outlier removal step and projection onto the top n/2 principal components;
these steps are repeated.
It is inspired by the work of Brubaker [Bru09] who gave an agnostic
algorithm for learning a mixture of well-separated spherical Gaussians.

For illustration, let us assume for now that the underlying distribution is

= N (µ, σ2I ). We
SN be the points sampled from
are given a set S of m = poly(n) points from
the Gaussian and the adversary respectively. Let us also assume that
. We will use the
|
notation µT for mean of the points in a set T , and ΣT for covariance of the points in T . We then
have

Dη, and S = SG ∪

SN |
|

= η

S
|

D

ΣS = (1

η)σ2I + ηΣSN + η(1

−

η)(µS −

µN )(µS −

−

µN )T .

(2)

If the dimension is n = 1, then we can show that the median of S is an estimate for µ correct up to an
µN ),
additive error of O(ησ). Even if we just knew the direction of the mean shift µS −
µ = η(µG −
µS and then ﬁnding
then we can estimate µ by ﬁrst projecting the sample S on the line along µ
µ
the median. This would give an estimator
k2 = O(ησ). So we can focus on
µ. One would guess that the top principal component of the covariance
ﬁnding the direction of µS −
matrix of S would be a good candidate. But it is easy for the adversary to choose SN to make this
completely useless. Since the noise points SN can be anything, just two points from SN placed far

µ satisfying

µ
k

−

−

b

b

5

away on either side of the mean µ along a particular line passing through µ are suﬃcient to make
the variance in that direction blow up arbitrarily. But we can limit this eﬀect to some extent by
an outlier removal step. By a standard concentration inequality for Gaussians, we know that the
points in SG lie in a ball of radius O(σ√n) around the mean. So, if we can just ﬁnd a point inside
or close to the convex hull of the Gaussian and throw away all the points that lie outside a ball of
radius Cσ√n around this point, we preserve all the points in SG. This will also contain the eﬀect
of noise points on the variance since now they are restricted to be within O(σ√n) distance of µ.
We will see later that we can use coordinate-wise median as the center of the ball. By computing
the variance by projecting onto any direction, we can ﬁgure out σ2 up to a 1
O(η) factor. From
now on, we assume that all points in S lie within a ball of radius O(σ√n) centered at µ.

±

But even after this restriction, the top principal component may not contain any information
about the mean shift direction. By just placing (say) η/10 noise points along the e1 direction
σ√n, and all the remaining noise points perpendicular to this at a single point at a smaller
at
distance, we can make e1 the top principal component. But e1 is perpendicular to the mean shift
direction.

±

The idea to get around this is that even if the top principal component of ΣS may not be along
the mean-shift direction, the span (call it V ) of top n/2 principal components of ΣS will contain a big
projection of the mean-shift vector. This is because, if a big component of the the mean-shift vector
was in the span (say W ) of bottom n/2 principal components of ΣS, by Equation 2 this would mean
that there is a vector in W with a large Rayleigh quotient. This implies that the top n/2 eigenvalues
µN )T ,
of ΣS are all big. Since ΣS = (1
this is possible only if Tr(A) is large. But since the distance of each point in S from µ is O(σ√n),
the trace of A cannot be too large. Therefore, in the space W , we can just compute the sample
mean P W µS and it will be close to P W µ. We still have to ﬁnd the mean in the space V . But we
do this by recursing the above procedure in V . At the end we will be left with a one-dimensional
space, and then we can just ﬁnd the median. This recursive projection onto the top n/2 principal
components is done in Algorithm 3 .

η)σ2I + A, where A = ηΣSN + η(1

µN )(µS −

η)(µS −

−

−

This generalizes to the non-spherical Gaussians with a few modiﬁcations. We use a diﬀerent
outlier removal step. In the non-spherical case, it is not trivial to compute
k2 to be used as
Σ
k
the radius of the ball. We give an algorithm for this later on. To limit the eﬀect of noise, we use
a damping function. Instead of discarding points outside a certain radius, we damp every point
by a weight so that further away points get lower weights. This is done in OutlierDamping
(Algorithm 1). We get the guarantees of Theorem 1.1 by running AgnosticMean (Algorithm 3)
with the outlier removal routine being OutlierDamping. A detailed proof of the whole algorithm
is given in Section 3.1.

We then turn to more general distributions which have bounded fourth moments. We need
bounded fourth moments to ensure that the mean and covariance matrix of the distribution
do not
change much even after conditioning by an event that occurs with probability 1
η. One diﬃculty for
with bounded
general distributions is that the outlier damping doesn’t work. So for distributions
fourth moments, we have another outlier removal routine called OutlierTruncation(
, η). In this
·
routine, we ﬁrst ﬁnd a point analogous to the coordinate-wise median for the Gaussians, and then
η fraction of S. We throw away all the points outside
consider a ball big enough to contain 1
this ball. We get the guarantees of Theorem 1.3 by running AgnosticMean (Algorithm 3) with
the outlier removal routine being OutlierTruncation (Algorithm 2). The complete proof of this
appears in Section 3.3.

−

−

D

D

6

D

is given by ED(x

We now have an algorithm to estimate the mean of very general (with bounded fourth moments)
distributions. To estimate the covariance matrix, we observed that the covariance matrix of a
µ)T . If we knew what µ was, then covariance can be
distribution
computed by estimating the mean of the second moments. To compute the mean of the second
µ)T as a vector in n2 dimensions and run the algorithm for
moments, we can treat (x
mean estimation. Also, we can estimate µ by the same algorithm. Therefore, we get Theorem 1.5
by running CovarianceEstimation (Algorithm 4). Its proof appears in Section 4.2.
Algorithm AgnosticOperatorNorm (Algorithm 5) estimates the 2-norm

µ)(x

µ)(x

−

−

−

−

k2 for general
= N (µ, Σ), and we are given m = poly(n) samples

Σ
k

distributions. For illustration, suppose
x 1, ..., x m ∼ Dη, and the mean µ. We consider the covariance-like matrix

D

Σ(S, µ) =

(x i −

µ)(x i −

µ)T .

1
m

Xi

−

η fraction of the points in S are from the Gaussian, we have Σ(S, µ)

Since 1
η)Σ. Therefore,
(cid:23)
the top eigenvalue σ2 of Σ(S, µ) is at least (1
k2. Let v be the top eigenvector of Σ(S, µ). If
η factor) is much less than σ2, this
the Gaussian variance along v (which can be computed up to 1
should be because there are a lot of noise points in S whose projections onto v are big compared
to the projection of Gaussian points in S. We remove points in S that have big projection and
then iterate the entire procedure. We later show that this procedure terminates in poly(n) steps
and when it terminates the top eigenvalue of Σ(S, µ) is close to that of Σ. A proof of this appears
in Section 5.

Σ
η)
k

(1

−

−

±

Theorem 1.7 follows easily from Theorem 1.5. Let

Σk be the top-k eigenspace of

Σ from

Theorem 1.5. We then have

b

Σ

Σk

−

F
(cid:13)
(cid:13)
(cid:13)

b

(cid:13)
(cid:13)
(cid:13)

b
Σk

−

−

b
Σk

F
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
ΣkkF
(cid:13)

Σ

−

(a)

≤
(b)

≤
(c)

≤
(d)

Σ

Σ

−

F

(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
Σ
(cid:13)

b
Σ

−

Σ

b
−

+

Σ

+

(cid:13)
(cid:13)
(cid:13) b
Σ
(cid:13)
(cid:13)
(cid:13) b
+
k

F

(cid:13)
(cid:13)
b
ΣkkF + O
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

(cid:13)
(cid:13)
Σ
(cid:13)

≤ k

−

η log n

Σ
k

k2.

(cid:17)
(a), (c) follow from triangle inequality, (b) follows from the fact that
imation and (d) from the guarantees of Theorem 1.5.

(cid:16)p

Σk is the best rank-k approx-

Finally we outline the application to agnostic ICA. The algorithm from [VX15]. Proceeds by ﬁrst
estimating the mean and covariance, in order to make the underlying distribution isotropic. Here
we estimate the covariance matrix Σ by ˆΣ and use it to determine a new isotropic transformation
ˆΣ
k2, the
isotropic transformation results in a guarantee of

2 . Since our agnostic SVD algorithm gives a guarantee of

O(√ν log n)
Σ
k

kF ≤

Σ
k

˜Σ

− 1

−

b

− 1

2 Σ ˆΣ

− 1
2

ˆΣ
k

I

k2 ≤

−

O(

Σ
k2
η log n) k
Σ−1
k2
k

p

p

= O(

η log nκ2).

Next the algorithm estimates a weighted covariance matrix W with the weight of a point x pro-
portional to cos(u T x ) for u chosen from a Gaussian distribution; it computes the SVD of W . For

7

this we use our algorithm again (the weights are applied individually to each sample). The main
guarantee is that the eigenvectors of this weighted covariance approximate the columns of A. This
relies on the maximum eigenvalue gap of W being large, and it has to be approximated to within
additive error ǫ = O(1/(log n)3). Theorem 1.7 implies that the additional error in eigenvalues is
k2, and therefore it suﬃces to have √η log n < c/(log n)3 for a suﬃ-
bounded by O(√η log n)
Σ
k
ciently small constant c that depends only on the cumulant and moment bound assumptions (i.e.,
c(log n)−7.
∆, M ). Thus, if suﬃces to have η < ǫ/2

≤

2.1 Lower Bounds: Observation 1.4

In this section we will show the lower bounds stated in Observation 1.4. For Gaussian distributions,
this is a special case of a theorem proved in [CGR15]. We reproduce the relevant part here for
D2 = N (µ2, σ2I ) and
completeness. We will show that there are distributions
µ2k2 = Ω(ησ) and
distributions Q1, Q2 such that
D1 + ηQ1 = (1
η)
−
D2. Let φ1 be p.d.f of
D1,

µ1 −
k
Dη = (1
Dη, no algorithm can distinguish between

D1 = N (µ1, σ2I ),

D1 and φ2 be the

D2 + ηQ2.

(3)

η)

−

D2. Let µ1, µ2 be such that the total variation distance between

D1,

D2 is

So, given
p.d.f of

1
2

η

φ2|
By a standard inequality for the total variation distance of Gaussian distributions, this implies
µ2k2 ≥
φ1)1φ2≥φ1 and Q2 be the
that
distribution with p.d.f 1−η
φ2)1φ1≥φ2. It is now easy to verify that Equation 3 is satisﬁed.
This proves item one of Observation 1.4.

1−η . Let Q1 be the distribution with p.d.f 1−η

η (φ1 −

η (φ2 −

µ1 −
k

φ1 −
|

dx =

2ησ

−

Z

1

η

.

D1 is supported on two points
{−
1/4. It is easy to check that both

For the distributions with bounded fourth moments, consider the following two one-dimensional
distributions.
.
1/2, 1/2
}
{
D2 is supported on three points
respec-
η)/2, η
−
D2 have bounded fourth moments with
tively. Let η
≤
the constant C4 = 8. Furthermore,
D1 by adding η fraction of noise
D2 can be obtained from
points. So no algorithm can distinguish between the two distributions. Since their means diﬀer by
η3/4σ, no algorithm can get an estimate better than this.

with the corresponding probabilities
(1
{

{−
σ, σ, σ/η1/4
}

}
D1 and

with probabilities

η)/2, (1

σ, σ

−

}

We will now show that the geometric median:

arg min

y

x i −
k

y

k2

Xi
has a √n dependence on the dimension. We show this in the Gaussian case even if we have access
to the whole distribution, but with η fraction of noise points placed all at a single point far away
from most of the Gaussian points.

Proposition 2.1 (Geometric Median). Let
= N (0, Σ) be a distribution with diagonal covariance
matrix Σ whose variance along the coordinate direction e 1 is zero, and equal to 1 in all the other
coordinate directions. Assume there is an η fraction of noise at a distance a = n along e 1. Let

D

t0 = arg min

(1

η)Ex ∼D

t

−

t2 + x2

2 + ... + x2
n

+ η(a

t).

−

(cid:19)

(4)

Then, t = Ω(η√n).

(cid:18)q

8

Proof. We have that at the minimizer t0, the derivative with respect to t is zero. Therefore, we
should have

Ex ∼D

t0
0 + x2
t2
2 + ... + x2
n

=

1

η

−

.

η

Consider f (t) = Ex ∼D

t
√t2+x2
2+...+x2
n
t = αη√n for a small enough constant α, then f (t)
x
n/2 with exponential probability. Therefore,
k

2
2 ≥
k

≤

. It is clear from Equation 4 that t0 > 0. We claim that if
p

η
1−η . Suppose t1 = αη√n. Since x

,

∼ D

f (t1)

Ex ∼D

t1
t2
1 + n/2

≤

≤

t1√2π
p
t2
1 + n/2 ≤

αη√2π.

Our algorithms are based on outlier removal and SVD. To simplify the proofs, we use new samples
for each step of the algorithm. The total sample complexity is given in the theorems.

For outlier removal, we use one of the following two simple routines. The ﬁrst, which we call
OutlierDamping, returns a vector of positive weights, one for each sample point.

The claim, and hence the proof follows.

p

2.2 Algorithms

2.2.1 Outlier Removal

Algorithm 1: OutlierDamping(S)

Input: S
⊂
Output: S

Rn with

= m

S
|

|
Rn, w = (w1, ..., wm)

Rm

∈

⊂

1. if n = 1:

Return (S,

1).

−

3. Set wi = exp

4. Return (S, w ).

kx i−aaak2
2
s2

(cid:17)

−

(cid:16)

for every x i ∈

S.

9

The second procedure for outlier removal returns a subset of points. It will be convenient to

view this as a 0/1 weighting of the point set. We call this procedure OutlierTruncation.

2. Let aaa be the coordinate-wise median of S. Let s2 = C Tr(Σ). Estimate Tr(Σ) by esti-

mating 1d variance along n orthogonal directions, see Section 4.1.

Algorithm 2: OutlierTruncation(S, η)

Input: S
Output:

⊂
S

Rn, η

[0, 1]

∈
S, w = 1

Rm

∈

⊂

1. if n = 1:

e

←

∩

2. Let aaa be as in Lemma 3.15.
e

e

fraction of S.

←

∩

e

4.

S

S

B(r, aaa). Return (

S, 1).

2.2.2 Main Algorithm

e

Let [a, b] be the smallest interval containing (1
S

[a, b]. Return (

S, 1).

S

η

−

−

−

ǫ)(1

η) fraction of the points,

3. Let B(r, aaa) = ball of minimum radius r centered at aaa that contains (1

η

ǫ)(1

η)

−

−

−

We are now ready to state the main algorithm for agnostic mean estimation. It uses one of the
above outlier removal procedures and assumes that the output of the procedure is a weighting.

Algorithm 3: AgnosticMean(S)

Input: S
Output:

⊂
µ

Rn.

Rn, and a routine OutlierRemoval(
).
·

∈
S, w ) = OutlierRemoval(S) .

1. Let (
b

2. if n = 1:

e

(a) if w =

1, Return median(

S). //Gaussian case

−
(b) else Return mean(

S). //General case
e

3. Let Σ

eS,w be the weighted covariance matrix of
e

the top n/2 principal components of Σ

4. Set S1 := P V (S) where P V is the projection operation on to V .

5. Let

µV := AgnosticMean(S1) and

µW := mean(P W

S).

Rn be such that P V

µ =

µV and P W

µ =

µW .

e

b

b

b

b

b

µ
b

6. Let

∈
7. Return

b

µ.

b

10

S with weights w , and V be the span of

eS,w , and W be its complement.

e

2.2.3 Estimation of the Covariance Matrix and Operator Norm

For both the tasks in this section, we will assume that the mean of the distribution µ = 0. We
can do this without loss of generality by a standard trick mentioned described in Section 4.2. The
algorithm for estimating the covariance matrix calls AgnosticMean on x x T . Analysis is given in
Section 4.2.

1. Let S(2) =

x ′
{

ix ′
i = 1, ..., m/2
b
i|
}

(see Equation 15)

2. Run the mean estimation algorithm on S(2), where elements of S(2) are viewed as vectors

direction of top variance. The analysis is given in Section 5.

Σ
k

k2 is based on iteratively truncating the samples along the

Algorithm 4: CovarianceEstimation(S)

Input: S
⊂
Output: n

R
Rn, η
∈
n matrix

Σ

×

in Rn2

. Let the output be

Σ.

3. Return

Σ.

b

b
The algorithm for estimating

Algorithm 5: AgnosticOperatorNorm(S)

Input: S
⊂
Output: σ2

Rn, η

∈
R>0.

∈

[0, 1], γ

R

∈

1. Let

S = SafeOutlierTruncation(S, η, γ).

2. Do the following O(n log2/γ n

η ) times

e

3. Let Σ0(

S) := 1
| eS|

i∈ eS x x T .

4. Find v , the top eigenvector of Σ0(

e

S), and its corresponding eigenvalue σ2.

5. Estimate (up to 1

cη factor, see Section 4.1) the variance of

along v and denote it

e

D

P

±

by

σ2
v .

6. if σ2
b

≤
Return σ2.

(1 + c3η log2/γ n
η )

σ2
v

7. Remove all points x

S such that

x T v
|

|

>

c2

σv log1/γ n
η
b
2

.

8. Go to Step (3).

b

∈

e

11

Algorithm 6: SafeOutlierTruncation(S, η, γ)

Input: S
Output:

⊂
S

Rn, η
S

∈

[0, 1], γ

R

∈

⊂
1. Let t =

e

n
i=1

2. Let B(c√t log1/γ n

P

b

3.

S

S

←

∩

B(c√t log1/γ n

η , 0). Return

S.

e

e

σ2
ei be the sum of estimated variances of

D
η , 0) be the ball of radius c√t log1/γ n
η centered at 0.

in n orthogonal directions.

3 Mean Estimation: Theorem 1.1 and Theorem 1.3

b

µ
k

In this section, we will ﬁrst prove Theorem 1.1, which is for Gaussian distributions, and Theorem 1.3,
which is for distributions with bounded fourth moments. All our algorithms will be translationally
is µ = 0. So we will be
invariant. We will assume w.l.o.g that the mean of the distribution
k2. Algorithm 3 has log n levels, we will assume that at each level it uses
proving bounds on
O( n log n

) samples resulting in a total of m = O( n log2 n

ǫ2
ǫ2
At various points in the analysis, to bound the sample complexity we will have to show that
the estimates computed from samples are close to their expectations. We will use the following
two results. Firstly, as an immediate corollary of matrix Bernstien for rectangular matrices (see
Theorem 1.6 in [Tro12]), we get the following concentration result for the sample mean and sample
covariance.
Lemma 3.1. Consider a distribution in Rn with covariance matrix Σ and supported in some
(0, 1). Then the
Σ
Euclidean ball whose radius we denote is
k
following holds with probability at least 1

R. Let ǫ
then

, for some R
k
1/ poly(n): If N
p

∈
R log n
ǫ2

D

R

∈

).

≥

and

Here

µ and

Σ are sample mean and sample covariance matrix.

b

b

Secondly, the functions we estimate will be integrals of low-degree polynomials (degree d at
most 4) restricted to intervals and/or balls. These functions viewed as binary concepts have small
VC-dimension, O(nd) where n is the dimension of space and d is the degree of the polynomial. We
use this to bound the error of estimating integrals via samples, and we can make the error smaller
than any inverse polynomial using a poly(n) size sample.
Proposition 3.2. Let F be a class of real-valued functions from Rn to [
corresponding class of binary concepts, i.e., for each f
f (x)

R, R]. Let CF be the
F , we consider the concepts ht(x) = 1 if
F , and any

t and zero otherwise. Suppose the VC-dimension of CF is d. Then, for any f

−

∈

≥

∈

µ
k

µ

−

k ≤

ǫ

Σ
k

k

p

Σ

−

k ≤

ǫ

Σ
k

.
k

−

b
Σ
k

b

12

distribution
least 1

D
δ satisﬁes

−

over Rn, an iid sample S of size

8
ǫ2 (d log(1/ǫ) + log(1/δ)), with probability at

S
|

| ≥

1
S
|

−

| Xx∈S

2ǫR.

≤

f (x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ex∼D(f (x))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

Proof. By the VC theorem, for any concept in CF , the bound on the size of the sample ensures
that with probability at least 1

δ and any t,

Noting that Ex∼D(f (x)) =

t) dt, we get the claimed bound.

≥

Pr(f (x)
(cid:12)
(cid:12)
(cid:12)
R
−R Pr(f (x)
(cid:12)
R
Tr(Σ) and ǫ2 := kaaak2

≥

x

|{

∈

t)

−

t

≥

}|

S : f (x)
S
|

|

ǫ.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Let s2 := 1
ǫ1

η2s2 . We can estimate Tr(Σ) by estimating (1 dimensional)
variances along n orthogonal directions, see Section 4.1. Note that we can arrange 0 < ǫ1, ǫ2 < 1
to be small enough constants. We weight every point x by wx = exp(
= N (0, Σ)
be a Gaussian distribution and S =
SN
be the Gaussian and the noise points repectively, with

−
, x i ∼ Dη be the sample we get. Let S = SG ∪

x 1, ..., x m}
{

D
Rn, let

= ηm. For a set T

kx −aaak2
s2

). Let

2

µT,w :=

wx ix i and ΣT,w :=

1
m

Xi∈T

SN |
|
wi(x i −

1
T
|

| Xi∈T

⊂
µT,w )T

µT,w )(x i −

We use the above notation for T = SG and T = SN . By an abuse of notation, when T = G, we
mean the population version of the above quantities:

µG,w := Ex wx x and ΣG,w := Ex wx (x

µG,w )(x i −

−

µG,w )T .

Note that

We consider the matrix ΣS,w

µS,w = (1

η)µSG,w + ηµSN ,w .

−

ΣS,w =

1
m

= (1

Xi
−

µS,w )(x i −

wx i(x i −
η)ΣSG,w + ηΣSN ,w + η(1

µS,w )T

η)(µSN ,w −

µSG,w )(µSN ,w −

−

µSG,w )T .

3.1 Proof of Theorem 1.1:

Let us assume η < 1/2.1. We then have

Lemma 3.3. Let
we are given x1, ..., xm ∼ Dη, then the median xmed = mediani{
with high probability.

D

xi}

= N (0, σ2) be a one dimensional Gaussian distribution. If m = O

xmed
|

|

satisﬁes

log n
ǫ2

, and
= O((η + ǫ)σ)

(cid:17)

(cid:16)

Proof. Let SG ⊂
Φ−1(1/2 + η + ǫ). Let us bound the probability that the median xmed ≥
xmed ≥
≥
SG|
if
|

S be made up of samples in S that come from the Gaussian, also let c =
c. We ﬁrst note that if
poly(n)

c, then Pr (x > c
x
|
log n
= O
ǫ2

ǫ. By Hoeﬀding’s inequality, we can bound this by 1

∈u SG)

−

.

(cid:16)

(cid:17)

13

We will next consider the multidimensional case. The proof follows by a series of lemmas. We
state the lemmas ﬁrst, conclude the proof of Theorem 1.1 and then prove the lemmas. First, we
observe that by applying Lemma 3.3 in n orthogonal directions and union bound, we get
Lemma 3.4. Suppose v 1, ..., v n ∈
i miv i. Then if m = O
and aaa =
v i’s such that with probability 1

Rn are a set of orthonormal vectors. Suppose mi = medianj{
v t
ix j}
,
, there exists a constant C independent of the choice of

log n
ǫ2
poly(n) ,
(cid:16)

(cid:17)

P

−

By a simple calculation, maxx

bound on the trace.

Lemma 3.5. Suppose A := ηΣSN ,w + η(1
a constant C such that,

−

O(s2). This immediately gives the following

µG,w )(µSN ,w −

µSG,w )T . Then there exists

Cη2 Tr(Σ).

2
aaa
2 ≤
k
k
2e−kx −aaak2/s2
k

x
k

≤

η)(µSN ,w −
Cηs2.

Tr(A)

≤

We will show later

Theorem 3.6.

e−η2ǫ2
1 + ǫ1 −

 

η2ǫ2e2ǫ1

Σ

ΣG,w

(cid:22)

(cid:22)

!

eǫ1Σ.

As will be clear from the proof of Theorem 3.6, when Σ = σ2I is a multiple of identity, then
) samples, we will have

ΣG,w will also be a multiple of I . By Lemma 3.1, if we take m = O( n log n

ǫ2

Suppose, we have

(1

ǫ)ΣG,w

ΣSG,w

(1 + ǫ)ΣG,w .

−

(cid:22)

(cid:22)

(cid:22)

(cid:22)

αΣ

ΣSG,w

βΣ

in the Lowener ordering, for some α, β > 0. By an argument similar to the one sketched in Section
2, we can prove

Lemma 3.7. We will use the notation as deﬁned above. Let W be the bottom n/2 principal
components of the covariance matrix ΣS,w . We have

≤
kmin denotes the least eigenvalue of Σ and δµ := µSN ,w −

Σ
2η ((β + Cη)
k

k2 −

Σ
α
k

kmin) ,

ηPW δµ
k

2
k

µSG,w .

where

Σ
k

By an inductive application of Lemma 3.7, we get the following theorem giving a bound on

µ
k
Theorem 3.8. On input S and the routine OutlierDamping(
), AgnosticMean outputs
·
b
satisfying

.
k
µ

2
µ
k
k

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

b

(cid:0)

b

(cid:1)

14

Theorem 3.6 combined with Theorem 3.8 proves Theorem 1.1. We get a better dependence on
η when Σ = σ2I because we can take α = β in this case. This would lead to the cancellation of
the leading term in the bound in Theorem 3.8 as

Σ
k

k2 =

Σ
k

kmin.

Proof of Lemma 3.7:
have

Recall that Σ denotes the covariance matrix of the Gaussian part. We

ΣS,w = (1
= (1

η)ΣSG,w + ηΣSN ,w + η(1
η)ΣSG,w + A,

−

−

−

η)δµδT
µ

where A = ηΣSN ,w + η(1

η)δµδT

µ. Therefore, we have

−

(1

η)αΣ + A

ΣS,w

(1

η)βΣ + A.

(cid:22)

(cid:22)

−

For a symmetric matrix B, let λk(B ) denote the k’th largest eigenvalue. By Weyl’s inequality,

−

−

we have

Therefore,

By Lemma 3.5 we have

λk((1

η)ΣG,w + A)

λk(A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (ΣS,w )

λn/2 (A) + (1

≤

η)β

Σ
k

k2.

−

λn/2 (A)

=

⇒

λn/2(ΣS,w )

Tr(A)
n/2
2C 2η

≤

≤

≤

≤

Σ
k2
k
k2 + 2C 2η
Σ
η)β
(1
k
(β + 2C 2η)
k2.
Σ
k

−

Σ
k

k2

Recall that W is the space spanned by the bottom n/2 eigenvectors of ΣS,w , and P W is the

matrix corresponding to the projection operator on to W . We therefore have

We therefore have

P T

W ΣS,w P W (cid:22)

(β + 2C 2η)
Σ
k

k2I .

αP T

W ΣP W + ηP T

W ΣSN ,w P W + (η

η2)(P W δµ)(P W δµ)T

−

(β + 2C 2η)
Σ
k

k2I .

(cid:22)

Multiplying by the vector P W δµ

kP W δµk and its transpose on either side, we get

Assuming η

1/2, we therefore have

≤

(η

η2)
P W δµ
k

2
k

−

(β + 2C 2η)
Σ
k

≤

k2 −

Σ
α
k

kmin.

ηP W δµ
k

2
k

≤

2η

(β + 2C 2η)
Σ
k

k2 −

Σ
α
k

kmin

.

(cid:0)

(cid:1)

15

Proof of Theorem 3.8:
have

By Equation 6 and Lemma 3.1, since we take O

samples we

n log n
ǫ2

(cid:16)

(cid:17)

µSG,w k
k

2
2 ≤

k2

= O
(cid:0)

η2ǫ2e2ǫ1 + ǫ2
Σ
k
η2 + ǫ2
Σ
k2.
(cid:1)
k
(βη + η2 + ǫ2)
Σ
k

(cid:0)

(cid:1)

(1 + log n) The proof
So it is enough to prove
is by induction. If n = 1, then the conclusion follows from the guarantees of the one dimensional
(cid:0)
median. Now, assume that it holds for all n
1. Let n = k + 1. We have by Lemma
≤
3.7

k for some k

µSG,w k

k2 −

kmin

Σ
k

µ
k

αη

≥

≤

−

O

b

(cid:1)

2

µSN ,w −

ηP W
k
P W µS,w −
(cid:0)

2
µSG,w
k
≤
2
P W µSG,w k
(cid:1)
2 ≤

⇒ k

=

O

O

(βη + η2)
Σ
k
(βη + η2)
Σ
k

k2 −
k2 −

αη

αη

Σ
k
Σ
k

kmin
kmin

(cid:0)

(cid:0)

.

(cid:1)

(cid:1)

By induction hypothesis, since dim(V ) = n/2, we have

µV −
k
Therefore, adding the two, we get

P V µSG,w k

≤

2

(cid:0)

b

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

µSG,w k

−

2

≤

O

(βη + η2 + ǫ2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

(cid:1)

(cid:1)

Proof of Theorem 3.6:

b

(cid:0)

We will ﬁrst consider the second moment

B := Ex exp

x
k

2
k

aaa
−
s2

x x T .

(cid:19)

−

(cid:18)

We have

B =

p

=

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

exp

x T Σ−1x

x x T dx

−

(cid:0)
x T Σ−1x

(cid:1)
x x T dx

=

p

1
(2π)n

p
exp

−

(cid:18)

Z

|

Σ
|
(x

−

−1

2
aaa
s2 +
k
k
1
s2

Σ−1 +

exp

 −

b)T

(cid:18)

1
s4 aaa T

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

I

(x

b)

x x T dx,

(cid:19)

−

(cid:19)

where b = 1
s2

Σ−1 + 1

s2 I

aaa. Therefore, we have

B = exp

(cid:0)

 −

2
aaa
s2 +
k
k

(cid:1)
1
s4 aaaT

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1
Σ−1 + 1

s2 I

Σ−1 +

−1

.

1
s2

I

(cid:19)

!

Σ
|
|

(cid:12)
(cid:12)

(cid:18)

(cid:12)
(cid:12)

16

Now we will look at the scalar term

. Let λi be the eigenvalues of Σ.

Σ
|

|

Σ−1 + 1

s2 I

Σ−1 +

Σ
|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
s2

I

(cid:12)
1
(cid:12)
λi
= Πi (cid:12)
(cid:12)
(cid:12)

+ 1
s2

1
λi

(cid:12)
(cid:12)
(cid:12)

= Πi

1 +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
s2

.

(cid:19)

(cid:18)

We then have

We next bound exp

kaaak2
s2 + 1

s4 aaaT

Σ−1 + 1

s2 I

−1

aaa

−

(cid:16)

Σ
1 + ǫ1 ≤ |

|

Σ−1 +

1
s2

I

(cid:12)
(cid:12)
(cid:12)
(cid:12)

eǫ1.

≤

(cid:12)
(cid:12)
(cid:12)
. We have
(cid:12)

(cid:0)
Σ−1 +

1
s4 aaaT

(cid:18)

1
s2

I

(cid:19)

(cid:17)

(cid:1)

−1

aaa

≤

1
s2 aaa T aaa.

Therefore

Therefore,

exp(

η2ǫ2)

−

exp

≤

 −

2
aaa
s2 +
k
k

1
s4 aaa T

Σ−1 +

(cid:18)

−1

aaa

1
s2

I

(cid:19)

1.

! ≤

e−η2ǫ2

Σ−1 +

(cid:18)

−1

1
s2

I

(cid:19)

B

(cid:22)

(cid:22)

eǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

Lemma 3.9. We have the following

1
1 + ǫ1

Σ

(cid:22)

(cid:18)

Σ−1 +

−1

1
s2

I

(cid:19)

Σ

(cid:22)

and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors
s2 and v 1, ..., v n are the eigenvalues and the corresponding eigenvectors

Proof. Note that if 1
of Σ−1, then 1
of Σ−1 + 1

λ1 + 1
s2 I . Since,

λ1 , ..., 1
λn
s2 , ..., 1
+ 1
λn

the lemma follows.

From Lemma 3.9, we have

Next we will bound

λi

1 + ǫ1 ≤

1
+ 1

s2 ≤

1
λi

λi

e−η2ǫ2
1 + ǫ1

Σ

B

(cid:22)

(cid:22)

eǫ1Σ.

µG,w = Ex wx x .

17

(5)

µG,w =

1
(2π)n
1
(2π)n

Σ
|

| Z

Σ
|

| Z

x
k

x
k

aaa

2
k

aaa

2
k

−
s2

−
s2

(cid:19)

−

exp

exp

−

(cid:18)

−

(cid:18)

p

=

exp(

x T Σ−1x )x dx

−

x T Σ−1x

x dx

1
s4 aaaT

(cid:18)

Σ−1 +

−1

(cid:19)
1
s2

I

(cid:19)

aaa

!

2
aaa
s2 +
k
k
1
s2

Σ−1 +

=

p

1
(2π)n

p
exp

Z

= exp

−

(cid:18)

 −

exp

 −

|

Σ
|
(x

−

b)T

2
aaa
s2 +
k
k

(cid:18)
1
s4 aaa T

Σ−1 +

(cid:18)

I

(x

b)

x dx

−
−1

(cid:19)

aaa

(cid:19)
1
s2

I

(cid:19)

!

Σ
|

|

1
Σ−1 + 1

s2 I

b,

where b = 1
s2
the two scalars by eǫ1. Therefore, we have

Σ−1 + 1

s2 I

aaa. Recall that ǫ1 = Pi λi
s2

−1

(cid:0)

(cid:1)

. We can, as before, bound the product of

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Therefore, we have

µG,w k
k

2

2 = e2ǫ1 1

s4 aaaT

µG,w k2 ≤
k

eǫ1

Σ−1 +

−1

1
s2

I

(cid:19)

aaa

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−1

1
s2

(cid:18)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−1

(cid:18)

aaa

(cid:19)
−1

Σ−1 +

1
s2

I

Σ−1 +

−1/2

aaa

1
s2

I

(cid:19)

(cid:19)

(cid:18)

Σ−1 +

(cid:18)

Σ−1 +

1
s2

1
s2

I

I

1
s2

I

(cid:18)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
Σ−1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1/
Σ
k
k2.
Σ
k

I

1
s2

2
(cid:13)
(cid:13)
−1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
k2 + 1/s2

e2ǫ1 1

s2 aaaT
e2ǫ1 aaaT aaa
s2

η2ǫ2e2ǫ1

= η2ǫ2e2ǫ1

η2ǫ2e2ǫ1

≤

≤

≤

≤

Also, similarly

This implies

µG,w k
k

2
Σ

−1+ 1

s2 I ≤

η2ǫ2e2ǫ1.

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1

Σ−1 +

(cid:18)

−1

.

1
s2

I

(cid:19)

µG,w µT

G,w (cid:22)

η2ǫ2e2ǫ1Σ.

From Lemma 3.9, we have

Combining Equation (6) and Equation 5, we get Theorem 3.6.

18

(6)

3.2

Improving the dependence on η

Now we will show how we can obtain the second part of Theorem 1.1 to get a better dependence
= N (µ, Σ) be a Gaussian with covariance Σ, and
on η by using
c/ log n for a small enough constant c > 0. We ﬁrst use Theorem 1.5 (with ǫ = η) to estimate
η
≤
σ2 =

Σ from Theorem 1.5. Let

σ2 satisfying

D

Σ
k

b
k2. We get a

σ2

σ2

1

O(

η log n)

−

b
(cid:17)
(cid:16)
be the given sample, and let y i ∼
x 1, ..., x m}
Let S =
{
Deﬁne x ′
i = x i + y i. The key thing to note is that if x
′ = N (µ, Σ +
x + y
N (µ, Σ +
σ2I has

D
, and the covariance Σ′ = Σ +

σ2I ). Let

p

≤

≤

∼

(cid:16)

p
N (0,

(cid:17)

b

σ2I ), i = 1, ..., m be i.i.d. samples.
σ2I ), then
N (µ, Σ) and y
∼
∼
′ is same as that of
σ2I ). Note that the mean µ′ of
b
b

N (0,

D

1 + O(

η log n)

σ2.

(7)

D

b
Σ′

λmax

2 + O(

η log n)

b

≤

b
σ2 and λmin

Σ′

We can view x ′
Σ

(cid:16)
(cid:1)
′
η, and we assume η log n
i ∼ D
such that

p

(cid:17)

(cid:0)

′

compute a

≤

1

O(

η log n)

σ2.

(8)

≥

−

(cid:16)

(cid:0)

(cid:1)

p

(cid:17)

c. By Theorem 1.5 and Equation 7, we can

Let α = O

. Therefore,

b
√η log n

′
Σ

Σ′

−

F ≤

(cid:13)
(cid:13)
(cid:13) b

(cid:13)
(cid:13)
(cid:13)

O

η log n

σ2.

(cid:16)p

(cid:17)

(cid:0)

(cid:1)
ασ2I

I

−
1

′

Σ

−

⇒

=
b
=

O

Σ′

(cid:22)
ασ2

Σ

(cid:22)
′−1

′
Σ

+ ασ2I
′−1/2

Σ

b
(cid:22)
η log n
b

′−1/2

Σ′

Σ

′−1/2

I

Σ
b

I + ασ2
′−1/2

(cid:22)
Σ′

Σ

′−1

Σ

(cid:16)

−

⇒

b
(cid:16)p
by Equation 8. Now, if we let x ′′
i =
′′
then we can think of x ′′
η . If we now use Theorem 3.8 with β =
b
µ′′ such that
α =

i ∼ D
on the samples S′′ =

′′ = N (µ′′, Σ′′) = N

√η log n

b
i and

b
(cid:22)
(cid:16)

(cid:17)(cid:17)
′−1/2

(cid:16)p
Σ

x ′

Σ

(cid:22)

D

O

(cid:16)

b

b

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

, we get a

x ′′
i }
{
2 = O(η3/2 log3/2 n).
k

b

µ′′
k

−

µ′′

This implies that

µ =

Σ

′1/2

µ′′ satisﬁes
b

b

b

µ
b
k

−

′

2 = O(
µ
Σ
k
k
Σ
= O(
k
b

η3/2 log3/2 n)
k
k2η3/2 log3/2 n).

b

1 + O

η log n

I

(cid:17)(cid:17)
Σ

µ,

′−1/2

′−1/2

′−1/2

Σ

Σ

1 + O
b
(cid:0)

(cid:0)

√η log n
b

(cid:1)(cid:1)

,

and
(cid:17)

µ with
Remark 3.10. We can use this technique to give a polynomial time algorithm to compute
for any ﬁxed ǫ > 0. This would require estimating
a guarantee
higher order moments by the mean estimation algorithm and then using the above trick to improve
(cid:1)
the η dependence for each of them in sequence. We don’t give a proof of this in this paper.

k2η2−ǫ log2−ǫ n

2 = O
µ
k

Σ
k

µ
k

−

b

(cid:0)

b

19

3.3 Distributions with Bounded Fourth Moments

In this section, we will prove some some useful properties that distributions with bounded fourth
moments satisfy. We will assume that x
for a distribution with mean µ that has bounded
fourth moments, i.e., for every unit vector v

∼ D

E((x

µ)T v )4

C4

E((x

−

≤

−

µ)T v )2

2

,

(9)

for some C4.

Lemma 3.11 (Mean shift). Let X be a random variable with E(X

EX)2 = σ2 and

E(X

EX)4

−

≤

C4

E(X

−

EX)2

2

,

for some C4. Let ǫ

0.5 and A be any event with probability Pr(A) = 1

ǫ. Then

≤

(cid:1)

−

(cid:1)

−

(cid:0)

(cid:0)

Proof. Let a = E(X

A). Then
|

E(X
|

A)
|

−

E(X)

| ≤

4

8C4ǫ3σ.

p

EX = (1

ǫ)a + ǫE(X

−
EX

ǫ)a

−

−

(1
ǫ

Ac)
|
1
=

ǫ

−
ǫ

E(X

Ac) =
|

⇐⇒

(EX

a) + EX

−

The fourth moment of such an X is minimum when its support is just the two-point set
a) + EX

a, 1−ǫ
{

ǫ (EX

−

. Therefore,
}

(1

ǫ)(a

−

−

EX)4 + ǫ

(EX

a)

−

4

≤

(cid:19)

C4σ4

1

ǫ

−
ǫ

C4ǫ3

(cid:18)

=

a

EX

⇒ |

−

4

| ≤

(1

s

−

σ

4

8C4ǫ3σ,

ǫ)(3ǫ2

3ǫ + 1)

≤

−

p

when ǫ

0.5.

≤

Lemma 3.12. Let X be a random variable with EX = µ and E((X

µ)2) = σ2 and let

E(X

µ)4

C4σ2,

−

≤

−

−

for some C4. Then, for every event A that occurs with probability at least 1

ǫ, we have

where 1A is the indicator function of the event A. As an immediate corollary, for ǫ
the following bound on the conditional probability

≤

0.5 we get

E

(X

µ)21A

−

(cid:0)

1

−

≥

(cid:16)

(cid:1)

C4ǫ

σ2,

p

(cid:17)

(10)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

20

Proof. Let dΩ be the probability measure. We can write E(X
following way

−

µ)4

≤

C4(E(X

µ)2)2 in the

−

2

(cid:19)

2

(X

µ)4dΩ +

(X

µ)4dΩ

(X

µ)2dΩ +

(X

µ)2dΩ

ZAc

−

C4

≤

(cid:18)ZA

−

ZAc

−

Using E(Y

EY )4

(E(Y

EY )2)2 for any random variable Y, and Pr(Ac) = ǫ we have

ZA

−

−

≥

−

1
ǫ

2

(X

µ)2dΩ

−

(cid:18)ZAc

≤

ZAc

(cid:19)

(X

µ)4dΩ

−

We therefore have

(cid:0)R

⇐⇒

Ac(X

−
ǫ

µ)2dΩ

2

(cid:1)
µ)2dΩ

(X

−

ZAc
(X

µ)2dΩ

−

(cid:19)
µ)2

E(X

−

≤

≤

+

p
1

(cid:16)

≤

ZA

⇐⇒

C4ǫ

1

−

(cid:16)

p

(cid:17) (cid:18)ZAc
C4ǫ

1

−

(cid:16)

p

(cid:17)

⇐⇒

This proves the inequality (10). Now,

C4

(X

µ)2dΩ +

(X

µ)2dΩ

(cid:18)ZA
C4ǫ

−

(X

−

ZAc
µ)2dΩ +

−

(X

−

(cid:19)
µ)2dΩ

ZAc
µ)2dΩ

(X

−

(cid:19)

−

(X

µ)2dΩ

≤

(cid:19)

ZA

(cid:18)ZA
C4ǫ

−

(X

p

−

(cid:17) (cid:18)ZA

µ)2dΩ

Also,

Therefore, for ǫ

0.5 we get that

≤

E

(X

µ)2

A
|

−

1
µ(A)

1

−

=

≥

(cid:1)

(X

µ)2dΩ

ZA

C4ǫ

−
σ2.

(cid:16)

p

(cid:17)

E

(X

µ)2

A
|

−

=

(X

µ)2dΩ

−

1
µ(A)
1

ZA
σ2.

≤

1

ǫ

−

(cid:1)

(cid:0)

(cid:0)

C4ǫσ2

E

(X

≤

µ)2

A
|

−

σ2

−

≤

2ǫσ2.

(cid:0)

(cid:1)

−

p

As an immediate corollary of Lemma 3.11 and Lemma 3.12, we get for a random variable x

having bounded fourth moments

Corollary 3.13. Let A be an event that happens with probability 1

η. Then,

where Σ

(cid:16)
|A is the conditional covariance matrix Σ

p

(cid:17)

O(

C4η)

Σ

1

−

−

(1 + 2η)Σ,

Σ

|A (cid:22)
(cid:22)
|A := E(x x T

21

A)
|

−

(E(x

A))(E(x
|

A))T .
|

Proof. Let v be any unit vector. Let y be the random variable that is v T x for x
µ. Then
µ = E(y), µA = E(y

. Let

∼ D

A), and d = µA −
|
µA)2
µ
A) = E((y
|

−

E((y

−

By Lemma 3.11 and Lemma 3.12,

d)2

−

A) = E((y
|

= E((y

µ)2
µ)2

A)
|
A)
|

−

−

−

−

2dE(y
d2

µ

A) + d2
|

−

E((y

E((y

µA)2
µA)2

A)
|
A)
|

−

−

−

−

E((y

E((y

−

−

µ)2)
µ)2)

≤

2ηE((y

µ)2)

−
C4ηE((y

≥ −

≥ −

p

C4η +

d2

−

µ)2)

−
8C4η3

E((y

µ)2)

−

(cid:16)p

p

(cid:17)

Finally, by a standard argument as in the proof of Chebyshev’s inequality, we have

Lemma 3.14 (Concentration). For every unit vector v , we have

C4
t4 ,
where σv is the standard deviation of x along the direction v , σ2

x T v
|
(cid:0)

Ex T v

tσv

| ≥

Pr

−

≤

(cid:1)

x T v
v := E
|

2
|

− |

Ex T v

2.
|

3.4 Proof of Theorem 1.3:

3.4.1 One Dimensional Distribution

First we will consider the case when X is a random variable with mean µ and variance σ2 satisfying

E((X

µ)4)

C4σ4.

−

≤

In this case, median need not be a good estimator. Instead, we will consider the interval of minimum
η) fraction of the sample points. Let S be the given sample,
length that contains (1
and let
S) be our estimator. We will show
below that
e

−
S be the points lying in this interval. Let

By the concentration inequality stated in Lemma 3.14, we get that for the distribution, the

(η + ǫ)3/4σ

µ = mean(

ǫ)(1

µ
|

| ≤

−

−

−

O

µ

b

e

η

.

C 1/4
4
(cid:16)

(cid:17)

of the interval around µ consisting of probability mass 1

is bounded by

length r1− η+ǫ
b
2

η+ǫ
2

−

We will refer to this interval by I1− η+ǫ
then with probability 1

2

1/ poly(n) for every interval I

. We note that by VC theorem if

SD|
|

= Ω

log n+log 1/ǫ
ǫ2

,

(cid:16)

(cid:17)

−

Pr (x
∈
|
The length of the smallest interval that contains (1
length of the smallest interval that contains 1

Pr (x

∼ D

−

∈

x

η

I

)

|

|
η

η) fraction of S is at most the
ǫ fraction of SD. This latter quantity is bounded

ǫ)(1

−

−

−

R,

⊂
x ǫu SD)

I

ǫ/2.

| ≤

r1− η+ǫ

2 ≤

C 1/4
4
η+ǫ
2

σ.

1/4

(cid:0)

(cid:1)

−

−

22

by r1−η, since the interval I1− η+ǫ
SD.

2

contains with probability 1

1/ poly(n) a (1

η

ǫ) fraction of

−

−

−

This implies that when we look at the minimum interval containing 1

η

noise points, the extreme points of the interval can be at most at a distance r1− η+ǫ

−

−

ǫ fraction of the non-
from µ. Thus,

2

the distance of all noise points will be within O

. Furthermore, the interval of minimum

η

length with (1
by Lemma 3.11 the mean of
from the true mean.

ǫ)(1

−

−

−

e

η) fraction of S will contain at least 1

S will be within η

r1−η + O

3η
C4(η + ǫ)3σ

−

−

4

ǫ fraction of SD. Therefore,
C 1/4
4
(cid:16)

(η + ǫ)3/4σ

= O

(cid:17)

(cid:17)

(cid:16)

p

1/4
C
4
(η+ǫ)1/4 σ

(cid:19)

(cid:18)

·

3.4.2 Multi-dimensional Case

We will now consider the multidimensional case. Let
random variable that satisﬁes for every direction v

D

be a distribution on Rn and x

is a

∼ D

E(((x

µ)T v )4)

C4

E(((x

−

≤

−

µ)T v )2)

2

,

for some C4.

(cid:0)
For any direction v , let µv = µT v. From the previous section, we know that we can ﬁnd a

(cid:1)

µv

such that

Therefore, by picking n orthogonal directions v 1, ..., v n, we get

µv
|

µv| ≤

−

O(C 1/4
4

(η + ǫ)3/4σv ).

Lemma 3.15. Given O
O(C 1/4
4

(η + ǫ)3/4

(cid:16)
Tr(Σ)).

n log n
ǫ2

b

(cid:17)

samples, we can ﬁnd a vector aaa

Rn such that

∈

aaa
k

µ

k2 =

−

We will now bound the radius of the ball in the outlier removal step (Algorithm 2). We claim

p

the radius of the ball is O

. Suppose we have some x

. Let z = x

||
Using the n orthogonal directions as picked above, let zi = z T v i and let Z 2 =
Consider the following:

∼ D

p

(cid:19)

(cid:18)

1/4
C
4
(η+ǫ)1/4

n

Σ

||2

z2
i =

P

Pr

Z 2

 

≥

C 1/2
Σ
4 n
||
(η + ǫ)1/2 !

||2

= Pr

Z 4

C4n2

Σ
||
η + ǫ

2
2
||

≥

(η + ǫ)E(Z 4)
C4n2

Σ

2
2
||

||

≤

(cid:19)

It suﬃces to bound the right-hand side of (11) by O(η + ǫ), in which case the ball will contain
1

ǫ fraction of the probability mass of

. We have

η

−

−

E(Z 4) = E

z2
i



Xi

Xj

z2
j 

≤

n2 max
i

E

z4
i

C4n2

Σ
k

2
2
k

≤

(cid:0)

(cid:1)


due to the fourth moment condition and the fact that E((z T v i)2)



1/4
C
4
(η+ǫ)1/4

Tr(Σ)

(cid:18)

n

k2. Therefore, a ball of
aaa
k2 =
k
p
, we get that the radius of the ball computed in the outlier removal step

≤ k
ǫ fraction of the points. Since

contains 1

||2

Σ

Σ

−

−

−

(cid:19)

µ

||

η

radius at most O

O

C 1/4
4
(cid:16)
is O

(η + ǫ)3/4
1/4
C
4
(η+ǫ)1/4

n

(cid:18)

p
Σ
k

k2

(cid:19)

p

(cid:17)

. We have proved

(cid:18)

D

23

b

µ.

2
2.
k

−
z

k

(11)

(12)

Lemma 3.16. After the outlier removal step, every remaining point x satisﬁes

x
k

−

µ

k2 ≤

O

 

C 1/4
4
(η + ǫ)1/4

n

Σ
k

k2

.

!

p

Consider the covariance matrix Σ

S (recall that

S be the set of points in

SD ⊂
points sampled by the adversary. Let µ
e
Note that

S that were sampled from the distribution
e
S), µ
SN
e

S := mean(
e

e

e

e

S is the sample after outlier removal). Let
S be the
SD).

and
D
SN ) and µ

:= mean(

SN ⊂
SD := mean(
e
e

e

eS of

µ

S =
e

ηµ

e
+ (1
−

η)µ

SD ,
e

SN
e

e

η = | eSN |
| eS|

is the fraction of noise points after the outlier truncation step. Note that
e

where
≤
η
1−2η−ǫ = O(η). We will therefore pretend that the fraction of noise points is still η after the outlier
is µ = 0. By Lemma 3.11
truncation step. We again assume that the mean of the distribution
applied with X = x T µ eD
and where A is the event that x is not removed by outlier
removal, we have that

kµ eDk for x

∼ D

D

e

e

e

η

e

(13)

Suppose, after the outlier removal step, we had the guarantee that the covariance matrix of the

remaining points from the distribution

, say Σ

D

µ
k

eDk2 = O(C 1/4

4

k2).

(η + ǫ)3/4

Σ
k
p
eD, is between
β(1
Σ

η)Σ

α(1

η)Σ

−

(cid:22)

eD (cid:22)

−

in the Lowener ordering. Corollary 3.13 gives α = 1

by Lemma 3.1 and Lemma 3.16 we have that if

O(

−
= Ω

C4(η + ǫ)) and β = 1 + O(η + ǫ). Also,
n log n
ǫ2

, then

S
|

eD|

p
(cid:16)

ǫ

C 1/4
4
(η + ǫ)1/4

(1

−

)Σ

eD (cid:22)

ΣS eD (cid:22)

(1 +

(cid:17)
C 1/4
4
(η + ǫ)1/4

ǫ

)Σ

eD

We will use the notation as deﬁned above.

Lemma 3.17. Let W be the bottom n/2 principal components of the covariance matrix ΣS. For
some constant C, we have

where δµ = µ

µ

eSN −

ηP W δµ
k
eSD .

2
k

≤

O

(βη + C 1/2
(cid:16)

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

,

(cid:17)

By an inductive application of the above lemma, we can prove

Theorem 3.18. On input (S, n), AgnosticMean outputs

µ satisfying

µ
k

2
k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
b
k

kmin

(1 + log n).

(cid:17)

(cid:16)

b

24

Theorem 3.18 with Corollary 3.13 proves Theorem 1.3.

Proof of Lemma 3.17:
have

Recall that Σ denotes the covariance matrix of the points from

. We

D

Σ

eS = (1
= (1

η)Σ

η)Σ

eSD + ηΣ
eSD + A,

−

−

+ (η

−

eSN

η2)δµδT
µ

where A := ηΣ

+ (η

SN
e

−

η2)δµδT

µ. Therefore, we have

(1

η)αΣ + A

−

(1

η)βΣ + A.

−

Σ

(cid:22)

eS (cid:22)
1/4
C
4
(η+ǫ)1/4

By Lemma 3.16 each x i satisﬁes

x ik
k

= O

(cid:18)
η√C4k
Σ
√η + ǫ

k2n

, so we have

n

Σ
k

k2

(cid:19)

p

O

≤

C4η

Σ
k

k2n

.

Tr(A) = O

(cid:19)
For a symmetric matrix B, let λk(B) denote the k’th largest eigenvalue. By Weyl’s inequality,

(cid:16)p

(cid:18)

(cid:17)

(14)

λk((1

η)Σ

S + A)
e

≤

−

λk(A) + (1

η)β

Σ
k

k2.

−

≤
By Equation (14), there exists a constant

(cid:0)

(cid:1)

C such that

λn/2

Σ
S
e

λn/2 (A) + (1

η)β

Σ
k

k2.

−

we have that

Therefore,

we have,

e
λn/2 (A)

Tr(A)
n/2

≤

≤

C

C4η

Σ
k

k2,

p

e

eS)
Recall that W is the space spanned by the bottom n/2 eigenvectors of Σ
corresponding to the projection operator on to W . We therefore have

λn/2(Σ

k2 +

Σ
k

Σ
k

C4η

η)β

k2

p

(1

−

≤

C

e

eS, and P W is the matrix

(1

η)αP T

W ΣP W + ηP T

W Σ

−
Multiplying by the vector P W δµ

eSN

P T

W Σ

eSP W (cid:22)
P W +(η

((1

−

η)β +

Σ
C4η)
C
k
η2)(P W δµ)(P W δµ)T
e

p

k2I
(cid:22)

−

kP W δµk and its transpose on either side, we get

η

P W δµ
k

2
k

≤

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin.

((1

η)β +

C

−

C4η)
Σ
k

k2I

p

e

where C = eC

1−η . We therefore have

ηP W δµ
k

2
k

≤

η

(β + C

Σ
C4η)
k

k2 −

Σ
α
k

kmin

.

(cid:16)

(cid:17)

p

p
25

2. The proof is by
Proof of Theorem 3.18:
By Equation 13, it is enough to bound
induction on the dimension. If n = 1, then the conclusion follows from the guarantees for the one
1.
dimensional case proven in Section 3.4.1. Now, assume that it holds for all n
Let n = k + 1. We have by Lemma 3.17

k for some k

µ
k

eSD k

≤

−

≥

µ

b

P W
k

µ

µ

eSD

−

2 =
k

(cid:16)

b

(cid:17)

ηP W δµ
k
O

2
k
(βη + C 1/2
(cid:16)

≤

4 η3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(cid:17)

Recall that we deﬁned V to be the span of the top n/2 principal components of Σ

S. By
e

induction hypothesis, since dim(V ) = n/2, we have

µV −
k

P V µ

2
eSD k

≤

O

(βη + C 1/2
(cid:16)

4

Therefore, adding the two, we get

b

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n/2).

µ
k

−

µ

2
eSD k

≤

O

(βη + C 1/2

4

(η + ǫ)3/2)
Σ
k

k2 −

αη

Σ
k

kmin

(1 + log n).

(cid:17)

(cid:17)

b

(cid:16)

4 Covariance Estimation

4.1 One Dimensional Case

Observation 4.1 (1d Covariance Estimate).

1. Let

be a distribution with mean µ and co-

D
= N (µ, σ2), then there is an algorithm that takes as input m = Ø

log n
ǫ2

variance σ2. If
samples x 1, ...x m ∼ Dη and computes in polynomial time

D

σ2 such that

σ2

σ2

= O(η +ǫ)σ2.
(cid:16)
(cid:17)

−

(cid:12)
(cid:12)

b

(cid:17)

−

−

σ2

(cid:16)
σ2

(cid:12)
(cid:12)b

= O

∼ D

2. If x

log n+log 1/ǫ
ǫ2

has bounded fourth moments with constant C4, and (x

µ)2 has bounded fourth
(cid:12)
(cid:12)b
moments with constant C4,2. Then there is an algorithm that takes as input η and m =
σ2 such that
samples x 1, ...x m ∼ Dη and computes in polynomial time
O
4,2 (η + ǫ)3/4C 1/2
C 1/4
4 σ
(cid:16)

= N (µ, σ), and we are given m = poly(n) samples S =

(cid:12)
is supported on R, we can estimate the variance in the following
Proof. When the distribution
(cid:12)
just having bounded eighth moments separately.
= N (µ, σ) and
way. We will consider the case
Suppose
, xi ∼ Dη. There are
x1, ..., xm}
{
D
several ways to estimate σ, we describe here one of them. First we compute the median, and let
85.1. Let Cσ be the
xmed = mediani{
µ. By Lemma 3.3,
c1’th quantile of S. Then our estimate for the standard deviation is
O(C 1/4
8 ησ2).
µ
= O(ησ). For a similar reason, Cσ = σ
we have
±
is a distribution that has bounded eighth moments, the result follows from the 1d

. Let Φ(x) be the c.d.f. of N (0, 1). Note that c1 = Φ(1)

∼
σ = Cσ −

O(ησ). Therefore,

xi}

D
D

±

D

(cid:17)

b

|

.

mean estimation in Section 3.4 applied (x

µ)2. Note that E(x

σ2 = σ2
b
µ)2 = σ2 and

c

b

µ
|
When
b

−
D

E

(x

µ)2

2

σ2

= E(x

µ)4

−

−

−

(cid:0)

−
σ4

−

−
C4σ4.

(cid:1)

≤

26

From Section 3.4, we therefore have that if m = O
C 1/4
4,2 (η + ǫ)3/4C 1/2
4 σ
(cid:16)

σ2
|

| ≤

σ2

−

O

(cid:17)

.

(cid:16)

b
4.2 Multi-Dimensional Case: Theorem 1.5

log n+log 1/ǫ
ǫ2

(cid:17)

, there is a poly(n) algorithm with

In this section we will prove that CovarianceEstimation (Algorithm 4) gives Theorem 1.5.
is a distribution with mean µ and covariance Σ
Throughout this section, we will assume that
and has bounded fourth moments with parameter C4. We use the following symmetrization trick
to assume that

has mean 0. Given samples S =

, let

D

x 1, ..., x m}
{

D

x ′

i =

x i −

x i+m/2
√2

for i

.
1, ..., m/2
}

∈ {

(15)

Since η fraction of the original samples were corrupted on average, only 2η fraction of the new
samples will be corrupted on average. Moreover, if x , y
are independent random variables,
then we can show that the distribution of x ′ = (x
y)/√2 has bounded fourth moments with
′ the distribution of x ′. CovarianceEstimation
parameter
is just the mean estimation algorithm on S(2) =
, we can appeal to Theorem 1.3.
}
Furthermore, let
Note that

′ be an aﬃne transformation of a 4-wise independent distribution.

C4 + 3/2. We will denote by

x ′x ′T
{

∼ D

x
|

−

≤

D

D

∈

S

Ex ∼D′x x T = Σ.

By Theorem 1.3, we have

Σ
k

Σ

kF = O

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

Σ(2)
k

1/2
2
k

log1/2 n,

b
where Σ(2) is covariance matrix of x x T , x

By Proposition 4.2 , we have

′.

∼ D

Σ
k

kF = O
Σ

−

η1/2 + C 1/4
(cid:16)

4,2 (η + ǫ)3/4

C 1/2
4 k

Σ

k2 log1/2 n,

which proves Theorem 1.5.

b

(cid:17)

(cid:17)

We will now derive a bound for

Σ(2)
k

k2 when the distribution has bounded fourth moments

and is 4-wise independent. In particular, we will prove

Proposition 4.2. If Σ(2) is the covariance matrix of x x T , x

′, it holds that

∼ D

Σ(2)
k

k2 ≤

O

Σ
C4k

2
2
k

.

(cid:0)

(cid:1)

27

Proof of Proposition 4.2:

Note that E(Y ) = Σ.

E(((Y

E(Y ))

V )2) = E

−

·

2

(Y ij −



Xij

E((Y ij −

Σij)Vij

Σij)(Y kl −

Σkl))VijVkl

E(Y ijY kl −

ΣijΣkl)VijVkl

E(xixjxkxl −

ΣijΣkl)VijVkl.

=

=

=

Xijkl

Xijkl

Xijkl



Next we note that

Therefore,

E(xixjxkxl)

−

ΣijΣkl = 


Σ2

ii if i = j = k = l

E(x4
i )
−
E(x2
i x2
j ) if i = k, j = l or i = l, j = k
0 otherwise.

max
V :kV kF =1

E(((Y

E(Y ))

V )2) = max

−

·

V :kV kF =1

(E(x4
i )

Σ2

ii)V 2

ii + 2

ΣiiΣjjV 2
ij

(E(x4
i )

2Σ2

ii)V 2

ii +

ΣiiΣjjV 2
ij

−

−

Xi<j

Xi,j

2Σ2

ii + max

Σ2
ii.

i

Xi

= max

V :kV kF =1

max
i
O (C4)

Xi
E(x4
i )
−
2
2.
k

Σ
k

≤

≤

5 Estimating

k2: Theorem 1.6
Σ

k

As in Section 4.2, we assume that the true distribution has mean µ = 0.

SN be the given sample, where SD consists of points from some distribution

In this section, we will prove AgnosticOperatorNorm (Algorithm 5) gives Theorem 1.6. Let
with mean
S = SD ∪
µ and covariance Σ and SN consists of points picked by the adversary. Let ΣSD be the sample
has 1D concentration, i.e., there exists a constant γ such that
covariance of SD. We assume that
for every unit vector v

D

D

Pr

(x

µ)T v

> t√v T Σv

e−tγ

.

−

(cid:16)(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

(cid:17)

S be the remaining sample at the end of the algorithm and let

SD be points in

S sampled from

5.1 Correctness

Let
.

D

e

e

e

28

Deﬁnition 5.1. Given a set of points S

Rn and a vector aaa

Rn, we let

Σaaa (S) :=

(x

aaa)(x

−

−

⊂

1
S
|

x ∈S
| X

∈

aaa)T .

First, we will argue that the covariance of the true distribution is well-approximated by Σµ(

SD).

Lemma 5.2. With probability 1

1/ poly(n),

−

e

Σ
k

−

Σ0(

SD)

η

Σ
k

k

k ≤

Proof. First, note that the t computed in SafeOutlierTruncation is at most O(Tr(Σ)) because
e
σ2
v (namely that the estimated
by an analogous argument as in Section 4.1, we have
v ≤
σv in a direction v is close to the true variance σv in that direction). Then the ball in
variance
SafeOutlierTruncation has radius R = c1
Tr(Σ) log1/γ n
η for some constant c1. We have that
b
deviates from the mean by more than c1σv log1/γ n
b
in any direction v , the probability that x
p
η
∼ D
is 1/ poly( n
η ). Then if we take n orthogonal directions, the probability that any given point is
more than distance R from µ is still 1/ poly( n
η ). Thus, step (1) of the algorithm will remove only
1/ poly( n
η ) fraction of the points sampled from

(1 + O(η))σ2

.

In every direction v , the probability mass of points from

outside an interval of size c2σv log1/γ n
η
around the mean is at most 1/ poly( n
η ), where σv is the variance in the direction v . Let Ci be the
region between the two hyperplanes used for truncation in iteration i. Therefore, if the number of
iterations is O(n log2/η n

1/ poly( n

η ), we will have that Pr (x

) = 1

η ).

ﬁnite k. By Lemma 3.12, we have that the covariance matrix Σ0 (
that of Σ:

Note that 1d concentration implies that the distribution has bounded 2k’th moment for all
D ∩i Ci is close to
(16)

(1 + 1/ poly(

1/ poly(

))Σ

x
∈ ∩iCi |

∼ D

(1

D

−
D ∩i Ci) of
n
))Σ.
η

D ∩i Ci)

(cid:22)

Σ0 (

n
η

(cid:22)

−

D

D ∩i Ci) to Σ0

, we use Proposition 3.2. The concept class we use is
Finally, to relate Σ0 (
all degree two polynomials restricted to convex polytopes with at most O(n) facets, deﬁned by
e
the hyperplanes used for truncation at each iteration of the algorithm. The VC dimension of this
concept class is O(n2 log n). Therefore, by Proposition 3.2 applied with R = c1

Tr(Σ) log1/γ n

SD

(cid:17)

(cid:16)

Σ
c1k

1/2n1/2 log1/γ n
k

η , we get that if we take m = O

n3(log1/γ n
η2

η )2 log n

then

p

(cid:18)

(cid:19)

Σ0
k

SD

Σ0 (

D ∩i Ci)

−

k ≤

Σ
η/2
k

.
k

(cid:16)

(cid:17)

Combining equations 16 and 17 we get the desired result.

e

Theorem 5.3. When the algorithm terminates, we have:

(1

Σ
η)
k

−

k2 ≤ k

Σ0(

S)

k2 ≤

(1 + O(η log2/γ n
η

Σ
))
k

k2.

Proof. First, note that since only an η fraction of

S is noise, we have

e

Σ0(

S)

(1

η)Σ0(
e
−

(cid:23)

SD)

e

29

e

η ≤

(17)

(18)

Therefore, we have that

Σ0(
k
bound. For the upper bound, let v be the top eigenvector of Σ0(
we have

Σ0(
η)
k

k2 ≥

SD)

S)

(1

−

e

e

k2. Lemma 5.2 gives the desired lower
S). When the algorithm terminates,

Σ0(
k

S)

k2 = v T Σ0(

e

S)v
(1 + O(η log2/γ n
e
η
(1 + O(η log2/γ n
η

≤

≤

))v T Σv

Σ
))
k

k2.

e

where the second line follows because of the termination condition and because we can estimate
the variance of

in any direction to within a (1

cη) factor.

D

±

5.2 Termination

In this section, we will show that with high probability, Algorithm 5 terminates in a polynomial
1
number of steps provided that η
C for some constant C that depends only on the estimation in
Step (5).

≤

Every time the algorithm goes through another iteration, it must remove a certain number of
noise points. Suppose in step (7), we remove r noise points. The noise conﬁguration of maximum
r
variance puts r amount of noise at the outlier removal distance d1 = c1

Tr(Σ) log1/γ n

c2

σv log1/γ n
η
b
2

. We can then write an upper

p

η , and η

−

amount of noise at the truncation threshold distance d2 =
bound on σ2.

σ2 =

(1
k

−

η)Σ0(

SD) + ηΣ0(

2
SN )
2 ≤
k

v + rd2
σ2

1 + (η

r)d2
2

−

This implies

e

e

Let us simplify the numerator Z = σ2
(1 + c3η log2/γ n
σ2
η )
v ≤
is less than some constant.

σ2. Here we also assume that η

σ2

r

≥

σ2
v −

−

ηd2
2

σ2
v −
d2
2

−
d2
1 −
ηd2
2. Since we are truncating the sample, we have
1
1−cη

1
C for a suﬃciently large C so that

≤

1

b

Z

σ2

≥

−

σ2
1 + c3η log2/γ n

+ η

2 log2/γ n
c2
η
4

!

1

cη

η  

−
cη + (cη)2 + η

2 log2/γ n
c2
η
4

1 + c3η log2/γ n
η

c3η log2/γ n

η −

(cη)2
η
1 + c3η log2/γ n
η

−

2 log2/γ n
c2
η
4

σ2

σ2

≥

−

1

−





σ2

≥















30

Recall that σ2

(1

Σ
η)
k

k2 by (18). Then as long as c3 is a suﬃciently large constant, we have

−

≥

Then combining Z with the denominator from earlier and using the fact that d1 ≤
we get:

c1

n

Σ
k

k2 log1/γ n
η ,

p

Z

≥

Σ
k2
k
4  

c3η log2/γ n
η
1 + c3η log2/γ n

η !

r

≥

Σ
k

c3η log2/γ n/η
1+c3η log2/γ n

k2
(cid:18)
4c2
Σ
1k

η (cid:19)
k2n log2/γ n
η
c3η
1n(1 + c3η log2/γ n
η )

≥

4c2

Then r

≥
iterations.

O

min

(cid:18)

(cid:26)

η
n ,

1
n log2/γ n

η (cid:27)(cid:19)

Open Questions

, so the algorithm will terminate in a nearly linear number of

An immediate open question is whether the our analysis of the mean estimation algorithm is
tight and the √log n is avoidable. For special distributions including Gaussians, [DKK+16] give
η rather than η√log n or √η log n
an algorithm with higher sample complexity and error η
as in Theorem 1.1. An open question is to give an O(η) approximation. For the more general
distributions considered here, the dependence on η must grow as at least η3/4; it is open to ﬁnd an
algorithm that achieves O(η3/4) error (our guarantee for the general setting has error O(√η log n)).
Other open problems include agnostic learning of a mixture of two arbitrary Gaussians and agnostic
sparse recovery.

log 1

q

We thank Chao Gao and Roman Vershynin for helpful discussions. We would also like to thank the
anonymous reviewers for useful suggestions. This research was supported in part by NSF awards
CCF-1217793 and EAGER-1555447.

Acknowledgment

References

[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with
unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In
NIPS, pages 2384–2392, 2012.

[AK01]

Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Pro-
ceedings of the thirty-third annual ACM symposium on Theory of computing, pages
247–257. ACM, 2001.

31

[BCV13]

[Bru09]

Aditya Bhaskara, Moses Charikar, and Aravindan Vijayaraghavan. Uniqueness of
tensor decompositions with applications to polynomial identiﬁability. arXiv preprint
arXiv:1304.8087, 2013.

S. Charles Brubaker. Robust PCA and clustering in noisy mixtures. In Proceedings of
the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2009,
New York, NY, USA, January 4-6, 2009, pages 1078–1087, 2009.

[Bru11]

David Bruce. A multivariate median in banach spaces and applications to robust pca.
http://www-personal.umich.edu/~romanv/students/bruce-REU.pdf, 2011.

[BRV13] Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the

presence of Gaussian noise. In Proc. of COLT, 2013.

[BS10]

[BV08]

[Car98]

Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In 51th
Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pages 103–112, 2010.

S Charles Brubaker and Santosh S Vempala. Isotropic PCA and aﬃne-invariant clus-
tering. In Building Bridges, pages 241–281. Springer, 2008.

J-F Cardoso. Multidimensional independent component analysis. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference
on, volume 4, pages 1941–1944. IEEE, 1998.

[CGR15] M. Chen, C. Gao, and Z. Ren. Robust Covariance Matrix Estimation via Matrix Depth.

ArXiv e-prints, June 2015.

[CJ10]

Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation.
Academic Press, 2010.

[CLMW11] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal com-

ponent analysis? J. ACM, 58(3):11:1–11:37, June 2011.

[CR08]

Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using
correlations and independence. In Proc. of COLT, 2008.

[CSPW11] Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-
SIAM Journal on Optimization,

sparsity incoherence for matrix decomposition.
21(2):572–596, 2011.

[Das99]

[Dav87]

[DG92]

Sanjoy Dasgupta. Learning mixtures of Gaussians. In Foundations of Computer Sci-
ence, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999.

P. L. Davies. Asymptotic behaviour of s-estimates of multivariate location parameters
and dispersion matrices. Ann. Statist., 15(3):1269–1292, 09 1987.

David L. Donoho and Miriam Gasko. Breakdown properties of location estimates based
on halfspace depth and projected outlyingness. Ann. Statist., 20(4):1803–1827, 12 1992.

32

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the computational
intractability. CoRR, abs/1604.06443, 2016.

[Don82]

[DS07]

David L. Donoho. Breakdown Properties of Multivariate Location Estimators. PhD
thesis, Harvard University, 1982.

Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of
separated, spherical Gaussians. The Journal of Machine Learning Research, 8:203–226,
2007.

[FJK96]

Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In
FOCS, pages 359–368, 1996.

[GVX14]

Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decom-
position. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,
pages 584–593. ACM, 2014.

[HK13]

Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment
methods and spectral decompositions. In ITCS, pages 11–20, 2013.

[HKO01]

Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis.
Wiley, 2001.

[HPL91]

Peter J. Rousseeuw Hendrik P. Lopuhaa. Breakdown points of aﬃne equivariant es-
timators of multivariate location and covariance matrices. The Annals of Statistics,
19(1):229–248, 1991.

[HRRS11] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.
Robust statistics: the approach based on inﬂuence functions, volume 114. John Wiley
& Sons, 2011.

Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.,
35(1):73–101, 03 1964.

Peter J. Huber.
Statistics, pages 1248–1251. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.

International Encyclopedia of Statistical Science, chapter Robust

[KMV10] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Eﬃciently learning mixtures
of two Gaussians. In Proceedings of the 42nd ACM symposium on Theory of computing,
pages 553–562. ACM, 2010.

[KRV]

https://github.com/kal2000/AgnosticMeanAndCovarianceCode.

[KV09]

Ravi Kannan and Santosh Vempala. Spectral Algorithms. Now Publishers Inc, 2009.

Nojun Kwak. Principal component analysis based on l1-norm maximization. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 30(9):1672–1680, 2008.

Ricardo Antonio Maronna. Robust m-estimators of multivariate location and scatter.
Ann. Statist., 4(1):51–67, 01 1976.

[Hub64]

[Hub11]

[Kwa08]

[Mar76]

33

[MMY06] RARD Maronna, Douglas Martin, and Victor Yohai. Robust statistics. John Wiley &

Sons, Chichester. ISBN, 2006.

[MSY92]

Ricardo A. Maronna, Werner A. Stahel, and Victor J. Yohai. Bias-robust estimators
of multivariate scatter based on projections. J. Multivar. Anal., 42(1):141–161, July
1992.

[MT+11] Michael McCoy, Joel A Tropp, et al. Two proposals for robust pca using semideﬁnite

programming. Electronic Journal of Statistics, 5:1123–1160, 2011.

Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures
of Gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE
Symposium on, pages 93–102. IEEE, 2010.

Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion
for high-dimensional datasets. Technometrics, 2012.

Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH
and NTRU signatures. J. Cryptology, 22(2):139–160, 2009.

J. R. Kettenring S. J. Devlin, R. Gnandesikan. Robust estimation of dispersion ma-
trices and principal components. Journal of the American Statistical Association,
76(374):354–362, 1981.

[Sma90]

Christopher G Small. A survey of multidimensional medians. International Statistical
Review/Revue Internationale de Statistique, pages 263–277, 1990.

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics, 12(4):389–434, 2012.

John W. Tukey. Mathematics and the Picturing of Data. In Ralph D. James, editor,
International Congress of Mathematicians 1974, volume 2, pages 523–532, 1974.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models.
Journal of Computer and System Sciences, 68(4):841–860, 2004.

Santosh Vempala and Ying Xiao. Max vs min: Tensor decomposition and ICA with
nearly linear sample complexity. In Proceedings of The 28th Conference on Learning
Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1710–1723, 2015.

[XCM10] Huan Xu, Constantine Caramanis, and Shie Mannor. Principal component analysis
with contaminated data: The high dimensional case. arXiv preprint arXiv:1002.4658,
2010.

[MV10]

[MZ12]

[NR09]

[SJD81]

[Tro12]

[Tuk74]

[VW04]

[VX15]

34

