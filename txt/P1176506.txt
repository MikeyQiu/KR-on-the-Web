Reluplex: An Eﬃcient SMT Solver for Verifying
Deep Neural Networks(cid:63)

Guy Katz, Clark Barrett, David Dill, Kyle Julian and Mykel Kochenderfer

Stanford University, USA
guyk, clarkbarrett, dill, kjulian3, mykel

{

@stanford.edu

}

Abstract. Deep neural networks have emerged as a widely used and
eﬀective means for tackling complex, real-world problems. However, a
major obstacle in applying them to safety-critical systems is the great dif-
ﬁculty in providing formal guarantees about their behavior. We present
a novel, scalable, and eﬃcient technique for verifying properties of deep
neural networks (or providing counter-examples). The technique is based
on the simplex method, extended to handle the non-convex Rectiﬁed Lin-
ear Unit (ReLU ) activation function, which is a crucial ingredient in
many modern neural networks. The veriﬁcation procedure tackles neu-
ral networks as a whole, without making any simplifying assumptions.
We evaluated our technique on a prototype deep neural network imple-
mentation of the next-generation airborne collision avoidance system for
unmanned aircraft (ACAS Xu). Results show that our technique can
successfully prove properties of networks that are an order of magnitude
larger than the largest networks veriﬁed using existing methods.

1

Introduction

Artiﬁcial neural networks [7, 31] have emerged as a promising approach for cre-
ating scalable and robust systems. Applications include speech recognition [9],
image classiﬁcation [22], game playing [32], and many others. It is now clear that
software that may be extremely diﬃcult for humans to implement can instead
be created by training deep neural networks (DNN s), and that the performance
of these DNNs is often comparable to, or even surpasses, the performance of
manually crafted software. DNNs are becoming widespread, and this trend is
likely to continue and intensify.

Great eﬀort is now being put into using DNNs as controllers for safety-critical
systems such as autonomous vehicles [4] and airborne collision avoidance systems
for unmanned aircraft (ACAS Xu) [13]. DNNs are trained over a ﬁnite set of in-
puts and outputs and are expected to generalize, i.e. to behave correctly for
previously-unseen inputs. However, it has been observed that DNNs can react in
unexpected and incorrect ways to even slight perturbations of their inputs [33].
This unexpected behavior of DNNs is likely to result in unsafe systems, or re-
strict the usage of DNNs in safety-critical applications. Hence, there is an urgent

(cid:63) This is the extended version of a paper with the same title that appeared at CAV

2017.

7
1
0
2
 
y
a
M
 
9
1
 
 
]
I

A
.
s
c
[
 
 
2
v
5
3
1
1
0
.
2
0
7
1
:
v
i
X
r
a

need for methods that can provide formal guarantees about DNN behavior. Un-
fortunately, manual reasoning about large DNNs is impossible, as their structure
renders them incomprehensible to humans. Automatic veriﬁcation techniques are
thus sorely needed, but here, the state of the art is a severely limiting factor.

Verifying DNNs is a diﬃcult problem. DNNs are large, non-linear, and non-
convex, and verifying even simple properties about them is an NP-complete
problem (see Section I of the appendix). DNN veriﬁcation is experimentally
beyond the reach of general-purpose tools such as linear programming (LP )
solvers or existing satisﬁability modulo theories (SMT ) solvers [3, 10, 30], and
thus far, dedicated tools have only been able to handle very small networks (e.g.
a single hidden layer with only 10 to 20 hidden nodes [29, 30]).

The diﬃculty in proving properties about DNNs is caused by the presence
of activation functions. A DNN is comprised of a set of layers of nodes, and the
value of each node is determined by computing a linear combination of values
from nodes in the preceding layer and then applying an activation function to
the result. These activation functions are non-linear and render the problem
non-convex. We focus here on DNNs with a speciﬁc kind of activation func-
tion, called a Rectiﬁed Linear Unit (ReLU ) [26]. When the ReLU function is
applied to a node with a positive value, it returns the value unchanged (the
active case), but when the value is negative, the ReLU function returns 0 (the
inactive case). ReLUs are very widely used [22, 24], and it has been suggested
that their piecewise linearity allows DNNs to generalize well to previously un-
seen inputs [6,7,11,26]. Past eﬀorts at verifying properties of DNNs with ReLUs
have had to make signiﬁcant simplifying assumptions [3, 10] — for instance, by
considering only small input regions in which all ReLUs are ﬁxed at either the
active or inactive state [3], hence making the problem convex but at the cost of
being able to verify only an approximation of the desired property.

We propose a novel, scalable, and eﬃcient algorithm for verifying properties
of DNNs with ReLUs. We address the issue of the activation functions head-
on, by extending the simplex algorithm — a standard algorithm for solving
LP instances — to support ReLU constraints. This is achieved by leveraging
the piecewise linear nature of ReLUs and attempting to gradually satisfy the
constraints that they impose as the algorithm searches for a feasible solution.
We call the algorithm Reluplex, for “ReLU with Simplex”.

The problem’s NP-completeness means that we must expect the worst-case
performance of the algorithm to be poor. However, as is often the case with
SAT and SMT solvers, the performance in practice can be quite reasonable; in
particular, our experiments show that during the search for a solution, many
of the ReLUs can be ignored or even discarded altogether, reducing the search
space by an order of magnitude or more. Occasionally, Reluplex will still need
to split on a speciﬁc ReLU constraint — i.e., guess that it is either active or
inactive, and possibly backtrack later if the choice leads to a contradiction.

We evaluated Reluplex on a family of 45 real-world DNNs, developed as an
early prototype for the next-generation airborne collision avoidance system for
unmanned aircraft ACAS Xu [13]. These fully connected DNNs have 8 layers

and 300 ReLU nodes each, and are intended to be run onboard aircraft. They
take in sensor data indicating the speed and present course of the aircraft (the
ownship) and that of any nearby intruder aircraft, and issue appropriate naviga-
tion advisories. These advisories indicate whether the aircraft is clear-of-conﬂict,
in which case the present course can be maintained, or whether it should turn
to avoid collision. We successfully proved several properties of these networks,
e.g. that a clear-of-conﬂict advisory will always be issued if the intruder is suﬃ-
ciently far away or that it will never be issued if the intruder is suﬃciently close
and on a collision course with the ownship. Additionally, we were able to prove
certain robustness properties [3] of the networks, meaning that small adversarial
perturbations do not change the advisories produced for certain inputs.

Our contributions can be summarized as follows. We (i) present Reluplex, an
SMT solver for a theory of linear real arithmetic with ReLU constraints; (ii) show
how DNNs and properties of interest can be encoded as inputs to Reluplex;
(iii) discuss several implementation details that are crucial to performance and
scalability, such as the use of ﬂoating-point arithmetic, bound derivation for
ReLU variables, and conﬂict analysis; and (iv) conduct a thorough evaluation
on the DNN implementation of the prototype ACAS Xu system, demonstrating
the ability of Reluplex to scale to DNNs that are an order of magnitude larger
than those that can be analyzed using existing techniques.

The rest of the paper is organized as follows. We begin with some background
on DNNs, SMT, and simplex in Section 2. The abstract Reluplex algorithm is
described in Section 3, with key implementation details highlighted in Section 4.
We then describe the ACAS Xu system and its prototype DNN implementation
that we used as a case-study in Section 5, followed by experimental results in
Section 6. Related work is discussed in Section 7, and we conclude in Section 8.

2 Background

Neural Networks. Deep neural networks (DNNs) are comprised of an input
layer, an output layer, and multiple hidden layers in between. A layer is com-
prised of multiple nodes, each connected to nodes from the preceding layer using
a predetermined set of weights (see Fig. 1). Weight selection is crucial, and is
performed during a training phase (see, e.g., [7] for an overview). By assigning
values to inputs and then feeding them forward through the network, values
for each layer can be computed from the values of the previous layer, ﬁnally
resulting in values for the outputs.

The value of each hidden node in the network is determined by calculating
a linear combination of node values from the previous layer, and then applying
a non-linear activation function [7]. Here, we focus on the Rectiﬁed Linear Unit
(ReLU) activation function [26]. When a ReLU activation function is applied to
a node, that node’s value is calculated as the maximum of the linear combination
of nodes from the previous layer and 0. We can thus regard ReLUs as the function
ReLU(x) = max (0, x).

Input #1

Input #2

Input #3

Input #4

Input #5

Output #1

Output #2

Output #3

Output #4

Output #5

Fig. 1: A fully connected DNN with 5 input nodes (in green), 5 output nodes (in
red), and 4 hidden layers containing a total of 36 hidden nodes (in blue).

Formally, for a DNN N , we use n to denote the number of layers and si to
denote the size of layer i (i.e., the number of its nodes). Layer 1 is the input
layer, layer n is the output layer, and layers 2, . . . , n − 1 are the hidden layers.
The value of the j-th node of layer i is denoted vi,j and the column vector
[vi,1, . . . , vi,si]T is denoted Vi. Evaluating N entails calculating Vn for a given
assignment V1 of the input layer. This is performed by propagating the input
values through the network using predeﬁned weights and biases, and applying
the activation functions — ReLUs, in our case. Each layer 2 ≤ i ≤ n has a
weight matrix Wi of size si × si−1 and a bias vector Bi of size si, and its values
are given by Vi = ReLU(WiVi−1 + Bi), with the ReLU function being applied
element-wise. This rule is applied repeatedly for each layer until Vn is calculated.
When the weight matrices W1, . . . Wn do not have any zero entries, the network
is said to be fully connected (see Fig. 1 for an illustration).

Fig. 2 depicts a small network that we will use as a running example. The
network has one input node, one output node and a single hidden layer with two
nodes. The bias vectors are set to 0 and are ignored, and the weights are shown
for each edge. The ReLU function is applied to each of the hidden nodes. It is
possible to show that, due to the eﬀect of the ReLUs, the network’s output is
always identical to its input: v31 ≡ v11.

Input
layer

Hidden
layer

Output
layer

1.0

v21

1.0

v11

v31

1.0

v22

1.0

−

Fig. 2: A small neural network.

Satisﬁability Modulo Theories. We present our algorithm as a theory solver
in the context of satisﬁability modulo theories (SMT).1 A theory is a pair T =
(Σ, I) where Σ is a signature and I is a class of Σ-interpretations, the models
of T , that is closed under variable reassignment. A Σ-formula ϕ is T -satisﬁable
(resp., T -unsatisﬁable) if it is satisﬁed by some (resp., no) interpretation in I. In
this paper, we consider only quantiﬁer-free formulas. The SMT problem is the
problem of determining the T -satisﬁability of a formula for a given theory T .

Given a theory T with signature Σ, the DPLL(T ) architecture [27] provides a
generic approach for determining the T -satisﬁability of Σ-formulas. In DPLL(T ),
a Boolean satisﬁability (SAT) engine operates on a Boolean abstraction of the
formula, performing Boolean propagation, case-splitting, and Boolean conﬂict
resolution. The SAT engine is coupled with a dedicated theory solver, which
checks the T -satisﬁability of the decisions made by the SAT engine. Splitting-
on-demand [1] extends DPLL(T ) by allowing theory solvers to delegate case-
splitting to the SAT engine in a generic and modular way. In Section 3, we
present our algorithm as a deductive calculus (with splitting rules) operating on
conjunctions of literals. The DPLL(T ) and splitting-on-demand mechanisms can
then be used to obtain a full decision procedure for arbitrary formulas.

Linear Real Arithmetic and Simplex. In the context of DNNs, a particu-
larly relevant theory is that of real arithmetic, which we denote as TR. TR con-
sists of the signature containing all rational number constants and the symbols
{+, −, ·, ≤, ≥}, paired with the standard model of the real numbers. We focus on
linear formulas: formulas over TR with the additional restriction that the mul-
tiplication symbol · can only appear if at least one of its operands is a rational
constant. Linear atoms can always be rewritten into the form (cid:80)
xi∈X cixi (cid:46)(cid:47) d,
for (cid:46)(cid:47) ∈ {=, ≤, ≥}, where X is a set of variables and ci, d are rational constants.
The simplex method [5] is a standard and highly eﬃcient decision procedure
for determining the TR-satisﬁability of conjunctions of linear atoms.2 Our algo-
rithm extends simplex, and so we begin with an abstract calculus for the original
algorithm (for a more thorough description see, e.g., [34]). The rules of the cal-
culus operate over data structures we call conﬁgurations. For a given set of vari-
ables X = {x1, . . . , xn}, a simplex conﬁguration is either one of the distinguished
symbols {SAT, UNSAT} or a tuple (cid:104)B, T, l, u, α(cid:105), where: B ⊆ X is a set of basic
variables; T , the tableau, contains for each xi ∈ B an equation xi = (cid:80)
xj /∈B cjxj;
l, u are mappings that assign each variable x ∈ X a lower and an upper bound,
respectively; and α, the assignment, maps each variable x ∈ X to a real value.
The initial conﬁguration (and in particular the initial tableau T0) is derived
from a conjunction of input atoms as follows: for each atom (cid:80)
xi∈X cixi (cid:46)(cid:47) d, a
new basic variable b is introduced, the equation b = (cid:80)
xi∈X cixi is added to the

1 Consistent with most treatments of SMT, we assume many-sorted ﬁrst-order logic

with equality as our underlying formalism (see, e.g., [2] for details).

2 There exist SMT-friendly extensions of simplex (see e.g. [16]) which can handle

R-
satisﬁability of arbitrary literals, including strict inequalities and disequalities, but
we omit these extensions here for simplicity (and without loss of generality).

T

Pivot1

Pivot2

xi ∈ B, α(xi) < l(xi), xj ∈ slack+(xi)
T := pivot(T, i, j), B := B ∪ {xj } \ {xi}

xi ∈ B, α(xi) > u(xi), xj ∈ slack−(xi)
T := pivot(T, i, j), B := B ∪ {xj } \ {xi}

xj /∈ B, α(xj ) < l(xj ) ∨ α(xj ) > u(xj ),

l(xj ) ≤ α(xj ) + δ ≤ u(xj )

xi ∈ B,

(α(xi) < l(xi) ∧ slack+(xi) = ∅) ∨ (α(xi) > u(xi) ∧ slack−(xi) = ∅)

Update

Failure

α := update(α, xj , δ)

UNSAT

Success

∀xi ∈ X . l(xi) ≤ α(xi) ≤ u(xi)
SAT

Fig. 3: Derivation rules for the abstract simplex algorithm.

tableau, and d is added as a bound for b (either upper, lower, or both, depend-
ing on (cid:46)(cid:47)). The initial assignment is set to 0 for all variables, ensuring that all
tableau equations hold (though variable bounds may be violated).

The tableau T can be regarded as a matrix expressing each of the basic
variables (variables in B) as a linear combination of non-basic variables (variables
in X \ B). The rows of T correspond to the variables in B and its columns to
those of X \ B. For xi ∈ B and xj /∈ B we denote by Ti,j the coeﬃcient cj of
xj in the equation xi = (cid:80)
xj /∈B cjxj. The tableau is changed via pivoting: the
switching of a basic variable xi (the leaving variable) with a non-basic variable
xj (the entering variable) for which Ti,j (cid:54)= 0. A pivot(T, i, j) operation returns
a new tableau in which the equation xi = (cid:80)
xk /∈B ckxk has been replaced by
− (cid:80)
the equation xj = xi
xk, and in which every occurrence of xj in
cj
each of the other equations has been replaced by the right-hand side of the new
equation (the resulting expressions are also normalized to retain the tableau
form). The variable assignment α is changed via update operations that are
applied to non-basic variables: for xj /∈ B, an update(α, xj, δ) operation returns
an updated assignment α(cid:48) identical to α, except that α(cid:48)(xj) = α(xj) + δ and for
every xi ∈ B, we have α(cid:48)(xi) = α(xi) + δ · Ti,j. To simplify later presentation we
also denote:

xk /∈B,k(cid:54)=j

ck
cj

slack+(xi) = {xj /∈ B | (Ti,j > 0 ∧ α(xj) < u(xj)) ∨ (Ti,j < 0 ∧ α(xj) > l(xj))
slack−(xi) = {xj /∈ B | (Ti,j < 0 ∧ α(xj) < u(xj)) ∨ (Ti,j > 0 ∧ α(xj) > l(xj))

The rules of the simplex calculus are provided in Fig. 3 in guarded assignment
form. A rule applies to a conﬁguration S if all of the rule’s premises hold for S. A
rule’s conclusion describes how each component of S is changed, if at all. When
S(cid:48) is the result of applying a rule to S, we say that S derives S(cid:48). A sequence of
conﬁgurations Si where each Si derives Si+1 is called a derivation.

The Update rule (with appropriate values of δ) is used to enforce that non-
basic variables satisfy their bounds. Basic variables cannot be directly updated.
Instead, if a basic variable xi is too small or too great, either the Pivot1 or the
Pivot2 rule is applied, respectively, to pivot it with a non-basic variable xj. This

makes xi non-basic so that its assignment can be adjusted using the Update
rule. Pivoting is only allowed when xj aﬀords slack, that is, the assignment for
xj can be adjusted to bring xi closer to its bound without violating its own
bound. Of course, once pivoting occurs and the Update rule is used to bring xi
within its bounds, other variables (such as the now basic xj) may be sent outside
their bounds, in which case they must be corrected in a later iteration. If a basic
variable is out of bounds, but none of the non-basic variables aﬀords it any slack,
then the Failure rule applies and the problem is unsatisﬁable. Because the tableau
is only changed by scaling and adding rows, the set of variable assignments that
satisfy its equations is always kept identical to that of T0. Also, the update
operation guarantees that α continues to satisfy the equations of T . Thus, if all
variables are within bounds then the Success rule can be applied, indicating that
α constitutes a satisfying assignment for the original problem.

It is well-known that the simplex calculus is sound [34] (i.e. if a derivation
ends in SAT or UNSAT, then the original problem is satisﬁable or unsatisﬁable,
respectively) and complete (there always exists a derivation ending in either SAT
or UNSAT from any starting conﬁguration). Termination can be guaranteed if cer-
tain strategies are used in applying the transition rules — in particular in picking
the leaving and entering variables when multiple options exist [34]. Variable se-
lection strategies are also known to have a dramatic eﬀect on performance [34].
We note that the version of simplex described above is usually referred to as
phase one simplex, and is usually followed by a phase two in which the solution
is optimized according to a cost function. However, as we are only considering
satisﬁability, phase two is not required.

3 From Simplex to Reluplex

The simplex algorithm described in Section 2 is an eﬃcient means for solving
problems that can be encoded as a conjunction of atoms. Unfortunately, while
the weights, biases, and certain properties of DNNs can be encoded this way,
the non-linear ReLU functions cannot.

When a theory solver operates within an SMT solver, input atoms can be
embedded in arbitrary Boolean structure. A na¨ıve approach is then to encode
ReLUs using disjunctions, which is possible because ReLUs are piecewise linear.
However, this encoding requires the SAT engine within the SMT solver to enu-
merate the diﬀerent cases. In the worst case, for a DNN with n ReLU nodes,
the solver ends up splitting the problem into 2n sub-problems, each of which
is a conjunction of atoms. As observed by us and others [3, 10], this theoretical
worst-case behavior is also seen in practice, and hence this approach is practi-
cal only for very small networks. A similar phenomenon occurs when encoding
DNNs as mixed integer problems (see Section 6).

We take a diﬀerent route and extend the theory TR to a theory TRR of reals
and ReLUs. TRR is almost identical to TR, except that its signature additionally
includes the binary predicate ReLU with the interpretation: ReLU(x, y) iﬀ y =

max (0, x). Formulas are then assumed to contain atoms that are either linear
inequalities or applications of the ReLU predicate to linear terms.

DNNs and their (linear) properties can be directly encoded as conjunctions
of TRR-atoms. The main idea is to encode a single ReLU node v as a pair of
variables, vb and vf , and then assert ReLU(vb, vf ). vb, the backward-facing vari-
able, is used to express the connection of v to nodes from the preceding layer;
whereas vf , the forward-facing variable, is used for the connections of x to the
following layer (see Fig. 4). The rest of this section is devoted to presenting an
eﬃcient algorithm, Reluplex, for deciding the satisﬁability of a conjunction of
such atoms.

Input
layer

v11

Hidden
Layer

ReLU

1.0

vb
21

vf
21

1.0

1.0

vb
22

−

ReLU

vf
22

1.0

Output
layer

v31

Fig. 4: The network from Fig. 2, with ReLU nodes split into backward- and
forward-facing variables.

The Reluplex Procedure. As with simplex, Reluplex allows variables to tem-
porarily violate their bounds as it iteratively looks for a feasible variable assign-
ment. However, Reluplex also allows variables that are members of ReLU pairs
to temporarily violate the ReLU semantics. Then, as it iterates, Reluplex re-
peatedly picks variables that are either out of bounds or that violate a ReLU,
and corrects them using Pivot and Update operations.

For a given set of variables X = {x1, . . . , xn}, a Reluplex conﬁguration is
either one of the distinguished symbols {SAT, UNSAT} or a tuple (cid:104)B, T, l, u, α, R(cid:105),
where B, T, l, u and α are as before, and R ⊂ X × X is the set of ReLU con-
nections. The initial conﬁguration for a conjunction of atoms is also obtained
as before except that (cid:104)x, y(cid:105) ∈ R iﬀ ReLU(x, y) is an atom. The simplex transi-
tion rules Pivot1, Pivot2 and Update are included also in Reluplex, as they are
designed to handle out-of-bounds violations. We replace the Success rule with
the ReluSuccess rule and add rules for handling ReLU violations, as depicted in
Fig. 5. The Updateb and Updatef rules allow a broken ReLU connection to be
corrected by updating the backward- or forward-facing variables, respectively,
provided that these variables are non-basic. The PivotForRelu rule allows a basic
variable appearing in a ReLU to be pivoted so that either Updateb or Updatef can
be applied (this is needed to make progress when both variables in a ReLU are
basic and their assignments do not satisfy the ReLU semantics). The ReluSplit

Updateb

xi /∈ B,

(cid:104)xi, xj (cid:105) ∈ R, α(xj ) (cid:54)= max (0, α(xi)), α(xj ) ≥ 0

α := update(α, xi, α(xj ) − α(xi))

Updatef

xj /∈ B,

(cid:104)xi, xj (cid:105) ∈ R, α(xj ) (cid:54)= max (0, α(xi))

α := update(α, xj , max (0, α(xi)) − α(xj ))

PivotForRelu

xi ∈ B, ∃xl. (cid:104)xi, xl(cid:105) ∈ R ∨ (cid:104)xl, xi(cid:105) ∈ R, xj /∈ B, Ti,j (cid:54)= 0
T := pivot(T, i, j), B := B ∪ {xj } \ {xi}

ReluSplit

(cid:104)xi, xj (cid:105) ∈ R,

u(xi) := 0

l(xi) < 0, u(xi) > 0
l(xi) := 0

ReluSuccess

∀x ∈ X . l(x) ≤ α(x) ≤ u(x), ∀(cid:104)xb, xf (cid:105) ∈ R. α(xf ) = max (0, α(xb))
SAT

Fig. 5: Additional derivation rules for the abstract Reluplex algorithm.

rule is used for splitting on certain ReLU connections, guessing whether they
are active (by setting l(xi) := 0) or inactive (by setting u(xi) := 0).

Introducing splitting means that derivations are no longer linear. Using the
notion of derivation trees, we can show that Reluplex is sound and complete
(see Section II of the appendix). In practice, splitting can be managed by a SAT
engine with splitting-on-demand [1]. The na¨ıve approach mentioned at the begin-
ning of this section can be simulated by applying the ReluSplit rule eagerly until
it no longer applies and then solving each derived sub-problem separately (this
reduction trivially guarantees termination just as do branch-and-cut techniques
in mixed integer solvers [28]). However, a more scalable strategy is to try to
ﬁx broken ReLU pairs using the Updateb and Updatef rules ﬁrst, and split only
when the number of updates to a speciﬁc ReLU pair exceeds some threshold.
Intuitively, this is likely to limit splits to “problematic” ReLU pairs, while still
guaranteeing termination (see Section III of the appendix). Additional details
appear in Section 6.

Example. To illustrate the use of the derivation rules, we use Reluplex to solve
a simple example. Consider the network in Fig. 4, and suppose we wish to check
whether it is possible to satisfy v11 ∈ [0, 1] and v31 ∈ [0.5, 1]. As we know that
the network outputs its input unchanged (v31 ≡ v11), we expect Reluplex to be
able to derive SAT. The initial Reluplex conﬁguration is obtained by introducing
new basic variables a1, a2, a3, and encoding the network with the equations:

a1 = −v11 + vb
21

a2 = v11 + vb
22

a3 = −vf

21 − vf

22 + v31

22, vf

21, vf

21(cid:105), (cid:104)vb

The equations above form the initial tableau T0, and the initial set of ba-
sic variables is B = {a1, a2, a3}. The set of ReLU connections is R =
{(cid:104)vb
22(cid:105)}. The initial assignment of all variables is set to 0. The
lower and upper bounds of the basic variables are set to 0, in order to enforce the
equalities that they represent. The bounds for the input and output variables are
set according to the problem at hand; and the hidden variables are unbounded,
except that forward-facing variables are, by deﬁnition, non-negative:

v11 vb

21 vf
variable
22 v31 a1 a2 a3
lower bound 0 −∞ 0 −∞ 0 0.5 0 0 0
assignment
0 0 0 0
0
upper bound 1 ∞ ∞ ∞ ∞ 1 0 0 0

22 vf

21 vb

0

0

0

0

Starting from this initial conﬁguration, our search strategy is to ﬁrst ﬁx any
out-of-bounds variables. Variable v31 is non-basic and is out of bounds, so we
perform an Update step and set it to 0.5. As a result, a3, which depends on v31,
is also set to 0.5. a3 is now basic and out of bounds, so we pivot it with vf
21, and
then update a3 back to 0. The tableau now consists of the equations:

a1 = −v11 + vb
21

a2 = v11 + vb
22

21 = −vf
vf

22 + v31 − a3

And the assignment is α(vf
21) = 0.5, α(v31) = 0.5, and α(v) = 0 for all other vari-
ables v. At this point, all variables are within their bounds, but the ReluSuccess
rule does not apply because α(vf

21) = 0.5 (cid:54)= 0 = max (0, α(vb
21, vf

21)).
21(cid:105). Since vb

The next step is to ﬁx the broken ReLU pair (cid:104)vb

21 is non-
basic, we use Updateb to increase its value by 0.5. The assignment becomes
α(vb
21) = 0.5, α(v31) = 0.5, α(a1) = 0.5, and α(v) = 0 for all other
variables v. All ReLU constraints hold, but a1 is now out of bounds. This is ﬁxed
by pivoting a1 with v11 and then updating it. The resulting tableau is:

21) = 0.5, α(vf

v11 = vb

21 − a1

a2 = vb

21 + vb

22 − a1

21 = −vf
vf

22 + v31 − a3

Observe that because v11 is now basic, it was eliminated from the equation for
a2 and replaced with vb
21 − a1. The non-zero assignments are now α(v11) = 0.5,
21) = 0.5, α(vf
α(vb
21) = 0.5, α(v31) = 0.5, α(a2) = 0.5. Variable a2 is now too
large, and so we have a ﬁnal round of pivot-and-update: a2 is pivoted with vb
22
and then updated back to 0. The ﬁnal tableau and assignments are:

21 − a1

v11 = vb
vb
22 = −vb
vf
21 = −vf

21 + a1 + a2
22 + v31 − a3

v11 vb

21 vf
22 v31 a1 a2 a3
variable
lower bound 0 −∞ 0 −∞ 0 0.5 0 0 0
assignment
0.5 0.5 0.5 −0.5 0 0.5 0 0 0
upper bound 1 ∞ ∞ ∞ ∞ 1 0 0 0

22 vf

21 vb

and the algorithm halts with the feasible solution it has found. A key observation
is that we did not ever split on any of the ReLU connections. Instead, it was
suﬃcient to simply use updates to adjust the ReLU variables as needed.

4 Eﬃciently Implementing Reluplex

We next discuss three techniques that signiﬁcantly boost the performance of
Reluplex: use of tighter bound derivation, conﬂict analysis and ﬂoating point
arithmetic. A fourth technique, under-approximation, is discussed in Section IV
of the appendix.

Tighter Bound Derivation. The simplex and Reluplex procedures naturally
lend themselves to deriving tighter variable bounds as the search progresses [16].
Consider a basic variable xi ∈ B and let pos(xi) = {xj /∈ B | Ti,j > 0} and
neg(xi) = {xj /∈ B | Ti,j < 0}. Throughout the execution, the following rules can
be used to derive tighter bounds for xi, regardless of the current assignment:

deriveLowerBound

deriveUpperBound

xi ∈ B,

l(xi) < (cid:80)

xj ∈pos(xi) Ti,j · l(xj) + (cid:80)

xj ∈neg(xi) Ti,j · u(xj)

l(xi) := (cid:80)

xj ∈pos(xi) Ti,j · l(xj) + (cid:80)

xj ∈neg(xi) Ti,j · u(xj)

xi ∈ B, u(xi) > (cid:80)
u(xi) := (cid:80)

xj ∈pos(xi) Ti,j · u(xj) + (cid:80)

xj ∈neg(xi) Ti,j · l(xj)

xj ∈pos(xi) Ti,j · u(xj) + (cid:80)

xj ∈neg(xi) Ti,j · l(xj)

The derived bounds can later be used to derive additional, tighter bounds.

When tighter bounds are derived for ReLU variables, these variables can
sometimes be eliminated, i.e., ﬁxed to the active or inactive state, without split-
ting. For a ReLU pair xf = ReLU(xb), discovering that either l(xb) or l(xf ) is
strictly positive means that in any feasible solution this ReLU connection will
be active. Similarly, discovering that u(xb) < 0 implies inactivity.

Bound tightening operations incur overhead, and simplex implementations
often use them sparsely [16]. In Reluplex, however, the beneﬁts of eliminating
ReLUs justify the cost. The actual amount of bound tightening to perform can
be determined heuristically; we describe the heuristic that we used in Section 6.

Derived Bounds and Conﬂict Analysis. Bound derivation can lead to situ-
ations where we learn that l(x) > u(x) for some variable x. Such contradictions
allow Reluplex to immediately undo a previous split (or answer UNSAT if no pre-
vious splits exist). However, in many cases more than just the previous split can
be undone. For example, if we have performed 8 nested splits so far, it may be
that the conﬂicting bounds for x are the direct result of split number 5 but have
only just been discovered. In this case we can immediately undo splits number
8, 7, and 6. This is a particular case of conﬂict analysis, which is a standard
technique in SAT and SMT solvers [25].

Floating Point Arithmetic. SMT solvers typically use precise (as opposed
to ﬂoating point) arithmetic to avoid roundoﬀ errors and guarantee soundness.
Unfortunately, precise computation is usually at least an order of magnitude
slower than its ﬂoating point equivalent. Invoking Reluplex on a large DNN can
require millions of pivot operations, each of which involves the multiplication and
division of rational numbers, potentially with large numerators or denominators
— making the use of ﬂoating point arithmetic important for scalability.

There are standard techniques for keeping the roundoﬀ error small when
implementing simplex using ﬂoating point, which we incorporated into our im-
plementation. For example, one important practice is trying to avoid Pivot op-
erations involving the inversion of extremely small numbers [34].

To provide increased conﬁdence that any roundoﬀ error remained within an
acceptable range, we also added the following safeguards: (i) After a certain

number of Pivot steps we would measure the accumulated roundoﬀ error; and
(ii) If the error exceeded a threshold M , we would restore the coeﬃcients of the
current tableau T using the initial tableau T0.

Cumulative roundoﬀ error can be measured by plugging the current assign-
ment values for the non-basic variables into the equations of the initial tableau
T0, using them to calculate the values for every basic variable xi, and then mea-
suring by how much these values diﬀer from the current assignment α(xi). We
deﬁne the cumulative roundoﬀ error as:

(cid:88)

xi∈B0

(cid:88)

xj /∈B0

|α(xi) −

T0i,j · α(xj)|

T is restored by starting from T0 and performing a short series of Pivot steps
that result in the same set of basic variables as in T . In general, the shortest
sequence of pivot steps to transform T0 to T is much shorter than the series of
steps that was followed by Reluplex — and hence, although it is also performed
using ﬂoating point arithmetic, it incurs a smaller roundoﬀ error.

The tableau restoration technique serves to increase our conﬁdence in the
algorithm’s results when using ﬂoating point arithmetic, but it does not guar-
antee soundness. Providing true soundness when using ﬂoating point arithmetic
remains a future goal (see Section 8).

5 Case Study: The ACAS Xu System

Airborne collision avoidance systems are critical for ensuring the safe opera-
tion of aircraft. The Traﬃc Alert and Collision Avoidance System (TCAS ) was
developed in response to midair collisions between commercial aircraft, and is
currently mandated on all large commercial aircraft worldwide [23]. Recent work
has focused on creating a new system, known as Airborne Collision Avoidance
System X (ACAS X ) [18, 19]. This system adopts an approach that involves
solving a partially observable Markov decision process to optimize the alerting
logic and further reduce the probability of midair collisions, while minimizing
unnecessary alerts [18, 19, 21].

The unmanned variant of ACAS X, known as ACAS Xu, produces horizontal
maneuver advisories. So far, development of ACAS Xu has focused on using a
large lookup table that maps sensor measurements to advisories [13]. However,
this table requires over 2GB of memory. There is concern about the memory
requirements for certiﬁed avionics hardware. To overcome this challenge, a DNN
representation was explored as a potential replacement for the table [13]. Initial
results show a dramatic reduction in memory requirements without compromis-
ing safety. In fact, due to its continuous nature, the DNN approach can some-
times outperform the discrete lookup table [13]. Recently, in order to reduce
lookup time, the DNN approach was improved further, and the single DNN was
replaced by an array of 45 DNNs. As a result, the original 2GB table can now
be substituted with eﬃcient DNNs that require less than 3MB of memory.

A DNN implementation of ACAS Xu presents new certiﬁcation challenges.
Proving that a set of inputs cannot produce an erroneous alert is paramount
for certifying the system for use in safety-critical settings. Previous certiﬁcation
methodologies included exhaustively testing the system in 1.5 million simulated
encounters [20], but this is insuﬃcient for proving that faulty behaviors do not
exist within the continuous DNNs. This highlights the need for verifying DNNs
and makes the ACAS Xu DNNs prime candidates on which to apply Reluplex.

Network Functionality. The ACAS Xu system maps input variables to action
advisories. Each advisory is assigned a score, with the lowest score corresponding
to the best action. The input state is composed of seven dimensions (shown in
Fig. 6) which represent information determined from sensor measurements [19]:
(i) ρ: Distance from ownship to intruder; (ii) θ: Angle to intruder relative to
ownship heading direction; (iii) ψ: Heading angle of intruder relative to ownship
heading direction; (iv) vown: Speed of ownship; (v) vint: Speed of intruder; (vi) τ :
Time until loss of vertical separation; and (vii) aprev: Previous advisory. There
are ﬁve outputs which represent the diﬀerent horizontal advisories that can be
given to the ownship: Clear-of-Conﬂict (COC), weak right, strong right, weak
left, or strong left. Weak and strong mean heading rates of 1.5 ◦/s and 3.0 ◦/s,
respectively.

Fig. 6: Geometry for ACAS Xu Horizontal Logic Table

The array of 45 DNNs was produced by discretizing τ and aprev, and produc-
ing a network for each discretized combination. Each of these networks thus has
ﬁve inputs (one for each of the other dimensions) and ﬁve outputs. The DNNs
are fully connected, use ReLU activation functions, and have 6 hidden layers
with a total of 300 ReLU nodes each.

Network Properties. It is desirable to verify that the ACAS Xu networks
assign correct scores to the output advisories in various input domains. Fig. 7
illustrates this kind of property by showing a top-down view of a head-on en-
counter scenario, in which each pixel is colored to represent the best action if
the intruder were at that location. We expect the DNN’s advisories to be con-
sistent in each of these regions; however, Fig. 7 was generated from a ﬁnite set

of input samples, and there may exist other inputs for which a wrong advisory
is produced, possibly leading to collision. Therefore, we used Reluplex to prove
properties from the following categories on the DNNs: (i) The system does not
give unnecessary turning advisories; (ii) Alerting regions are uniform and do
not contain inconsistent alerts; and (iii) Strong alerts do not appear for high τ
values.

Fig. 7: Advisories for a head-on encounter with aprev = COC, τ = 0 s.

6 Evaluation

We used a proof-of-concept implementation of Reluplex to check realistic prop-
erties on the 45 ACAS Xu DNNs. Our implementation consists of three main
logical components: (i) A simplex engine for providing core functionality such as
tableau representation and pivot and update operations; (ii) A Reluplex engine
for driving the search and performing bound derivation, ReLU pivots and ReLU
updates; and (iii) A simple SMT core for providing splitting-on-demand services.
For the simplex engine we used the GLPK open-source LP solver3 with some
modiﬁcations, for instance in order to allow the Reluplex core to perform bound
tightening on tableau equations calculated by GLPK. Our implementation, to-
gether with the experiments described in this section, is available online [14].

Our search strategy was to repeatedly ﬁx any out-of-bounds violations ﬁrst,
and only then correct any violated ReLU constraints (possibly introducing new
out-of-bounds violations). We performed bound tightening on the entering vari-
able after every pivot operation, and performed a more thorough bound tight-
ening on all the equations in the tableau once every few thousand pivot steps.
Tighter bound derivation proved extremely useful, and we often observed that
after splitting on about 10% of the ReLU variables it led to the elimination of all
remaining ReLUs. We counted the number of times a ReLU pair was ﬁxed via
Updateb or Updatef or pivoted via PivotForRelu, and split only when this number
reached 5 (a number empirically determined to work well). We also implemented
conﬂict analysis and back-jumping. Finally, we checked the accumulated round-
oﬀ error (due to the use of double-precision ﬂoating point arithmetic) after every

3 www.gnu.org/software/glpk/

5000 Pivot steps, and restored the tableau if the error exceeded 10−6. Most ex-
periments described below required two tableau restorations or fewer.

We began by comparing our implementation of Reluplex to state-of-the-art
solvers: the CVC4, Z3, Yices and MathSat SMT solvers and the Gurobi LP solver
(see Table 1). We ran all solvers with a 4 hour timeout on 2 of the ACAS Xu
networks (selected arbitrarily), trying to solve for 8 simple satisﬁable properties
ϕ1, . . . , ϕ8, each of the form x ≥ c for a ﬁxed output variable x and a constant
c. The SMT solvers generally performed poorly, with only Yices and MathSat
successfully solving two instances each. We attribute the results to these solvers’
lack of direct support for encoding ReLUs, and to their use of precise arithmetic.
Gurobi solved 3 instances quickly, but timed out on all the rest. Its logs indicated
that whenever Gurobi could solve the problem without case-splitting, it did so
quickly; but whenever the problem required case-splitting, Gurobi would time
out. Reluplex was able to solve all 8 instances. See Section V of the appendix
for the SMT and LP encodings that we used.

Table 1: Comparison to SMT and LP solvers. Entries indicate solution time (in
seconds).

ϕ1

ϕ2 ϕ3 ϕ4 ϕ5 ϕ6 ϕ7 ϕ8

-
-
1

CVC4
Z3
Yices

-
-
37
MathSat 2040 9780
1
Gurobi
2
Reluplex

1
8

-
-
-
-
1
7

-
-
-
-
-
-
-
-
-
-
7 93

-
-
-
-
-
4

-
-
-
-
-
7

-
-
-
-
-
9

Next, we used Reluplex to test a set of 10 quantitative properties φ1, . . . , φ10.
The properties, described below, are formally deﬁned in Section VI of the ap-
pendix. Table 2 depicts for each property the number of tested networks (spec-
iﬁed as part of the property), the test results and the total duration (in sec-
onds). The Stack and Splits columns list the maximal depth of nested case-splits
reached (averaged over the tested networks) and the total number of case-splits
performed, respectively. For each property, we looked for an input that would
violate it; thus, an UNSAT result indicates that a property holds, and a SAT result
indicates that it does not hold. In the SAT case, the satisfying assignment is an
example of an input that violates the property.

Property φ1 states that if the intruder is distant and is signiﬁcantly slower
than the ownship, the score of a COC advisory will always be below a certain
ﬁxed threshold (recall that the best action has the lowest score). Property φ2
states that under similar conditions, the score for COC can never be maximal,
meaning that it can never be the worst action to take. This property was discov-
ered not to hold for 35 networks, but this was later determined to be acceptable
behavior: the DNNs have a strong bias for producing the same advisory they

Table 2: Verifying properties of the ACAS Xu networks.

Networks Result Time Stack

Splits

394517

47 1522384

φ1

φ2

φ3
φ4
φ5
φ6
φ7
φ8
φ9
φ10

UNSAT

UNSAT
SAT
UNSAT
UNSAT
UNSAT
UNSAT

41
4 TIMEOUT
1
35
42
42
1
1
1 TIMEOUT
1
1
1

SAT
UNSAT
UNSAT

463
82419
28156
12475
19355
180288

40102
99634
19944

55
44
22
21
46
50

69
48
49

88388
284515
52080
23940
58914
548496

116697
227002
88520

had previously produced, and this can result in advisories other than COC even
for far-away intruders if the previous advisory was also something other than
COC. Properties φ3 and φ4 deal with situations where the intruder is directly
ahead of the ownship, and state that the DNNs will never issue a COC advisory.
Properties φ5 through φ10 each involve a single network, and check for con-
sistent behavior in a speciﬁc input region. For example, φ5 states that if the in-
truder is near and approaching from the left, the network advises “strong right”.
Property φ7, on which we timed out, states that when the vertical separation is
large the network will never advise a strong turn. The large input domain and
the particular network proved diﬃcult to verify. Property φ8 states that for a
large vertical separation and a previous “weak left” advisory, the network will
either output COC or continue advising “weak left”. Here, we were able to ﬁnd
a counter-example, exposing an input on which the DNN was inconsistent with
the lookup table. This conﬁrmed the existence of a discrepancy that had also
been seen in simulations, and which will be addressed by retraining the DNN.
We observe that for all properties, the maximal depth of nested splits was al-
ways well below the total number of ReLU nodes, 300, illustrating the fact that
Reluplex did not split on many of them. Also, the total number of case-splits
indicates that large portions of the search space were pruned.

Another class of properties that we tested is adversarial robustness proper-
ties. DNNs have been shown to be susceptible to adversarial inputs [33]: correctly
classiﬁed inputs that an adversary slightly perturbs, leading to their misclassi-
ﬁcation by the network. Adversarial robustness is thus a safety consideration,
and adversarial inputs can be used to train the network further, making it more
robust [8]. There exist approaches for ﬁnding adversarial inputs [3, 8], but the
ability to verify their absence is limited.

We say that a network is δ-locally-robust at input point x if for every x(cid:48) such
that (cid:107)x − x(cid:48)(cid:107)∞ ≤ δ, the network assigns the same label to x and x(cid:48). In the case
of the ACAS Xu DNNs, this means that the same output has the lowest score

Table 3: Local adversarial robustness tests. All times are in seconds.

δ = 0.1

Total
Result Time Result Time Result Time Result Time Result Time Time

δ = 0.075

δ = 0.025

δ = 0.05

δ = 0.01

239 SAT

24 UNSAT
Point 1 SAT
285 UNSAT
Point 2 UNSAT
99 UNSAT
Point 3 UNSAT
Point 4 SAT
1168 UNSAT
Point 5 UNSAT 14560 UNSAT 4344 UNSAT 1331 UNSAT

135 SAT
5880 UNSAT 1167 UNSAT
436 UNSAT
863 UNSAT
977 SAT
2 SAT

609 UNSAT
57 UNSAT
53 UNSAT
656 UNSAT
221 UNSAT

1064
57
7394
5
1452
1
7
2810
6 20462

for both x and x(cid:48). Reluplex can be used to prove local robustness for a given
x and δ, as depicted in Table 3. We used one of the ACAS Xu networks, and
tested combinations of 5 arbitrary points and 5 values of δ. SAT results show
that Reluplex found an adversarial input within the prescribed neighborhood,
and UNSAT results indicate that no such inputs exist. Using binary search on
values of δ, Reluplex can thus be used for approximating the optimal δ value
up to a desired precision: for example, for point 4 the optimal δ is between
0.025 and 0.05. It is expected that diﬀerent input points will have diﬀerent local
robustness, and the acceptable thresholds will thus need to be set individually.
Finally, we mention an additional variant of adversarial robustness which
we term global adversarial robustness, and which can also be solved by Relu-
plex. Whereas local adversarial robustness is measured for a speciﬁc x, global
adversarial robustness applies to all inputs simultaneously. This is expressed by
encoding two side-by-side copies of the DNN in question, N1 and N2, operating
on separate input variables x1 and x2, respectively, such that x2 represents an
adversarial perturbation of x1. We can then check whether (cid:107)x1 − x2(cid:107)∞ ≤ δ
implies that the two copies of the DNN produce similar outputs. Formally, we
require that if N1 and N2 assign output a values p1 and p2 respectively, then
|p1 − p2| ≤ (cid:15). If this holds for every output, we say that the network is (cid:15)-globally-
robust. Global adversarial robustness is harder to prove than the local variant,
because encoding two copies of the network results in twice as many ReLU nodes
and because the problem is not restricted to a small input domain. We were able
to prove global adversarial robustness only on small networks; improving the
scalability of this technique is left for future work.

7 Related Work

In [29], the authors propose an approach for verifying properties of neural net-
works with sigmoid activation functions. They replace the activation functions
with piecewise linear approximations thereof, and then invoke black-box SMT
solvers. When spurious counter-examples are found, the approximation is re-
ﬁned. The authors highlight the diﬃculty in scaling-up this technique, and are
able to tackle only small networks with at most 20 hidden nodes [30].

The authors of [3] propose a technique for ﬁnding local adversarial examples
in DNNs with ReLUs. Given an input point x, they encode the problem as a
linear program and invoke a black-box LP solver. The activation function issue
is circumvented by considering a suﬃciently small neighborhood of x, in which
all ReLUs are ﬁxed at the active or inactive state, making the problem convex.
Thus, it is unclear how to address an x for which one or more ReLUs are on the
boundary between active and inactive states. In contrast, Reluplex can be used
on input domains for which ReLUs can have more than one possible state.

In a recent paper [10], the authors propose a method for proving the local
adversarial robustness of DNNs. For a speciﬁc input point x, the authors attempt
to prove consistent labeling in a neighborhood of x by means of discretization:
they reduce the inﬁnite neighborhood into a ﬁnite set of points, and check that
the labeling of these points is consistent. This process is then propagated through
the network, layer by layer. While the technique is general in the sense that
it is not tailored for a speciﬁc activation function, the discretization process
means that any UNSAT result only holds modulo the assumption that the ﬁnite
sets correctly represent their inﬁnite domains. In contrast, our technique can
guarantee that there are no irregularities hiding between the discrete points.

Finally, in [12], the authors employ hybrid techniques to analyze an ACAS
X controller given in lookup-table form, seeking to identify safe input regions
in which collisions cannot occur. It will be interesting to combine our technique
with that of [12], in order to verify that following the advisories provided by the
DNNs indeed leads to collision avoidance.

8 Conclusion and Next Steps

We presented a novel decision algorithm for solving queries on deep neural net-
works with ReLU activation functions. The technique is based on extending the
simplex algorithm to support the non-convex ReLUs in a way that allows their
inputs and outputs to be temporarily inconsistent and then ﬁxed as the algo-
rithm progresses. To guarantee termination, some ReLU connections may need
to be split upon — but in many cases this is not required, resulting in an eﬃcient
solution. Our success in verifying properties of the ACAS Xu networks indicates
that the technique holds much potential for verifying real-world DNNs.

In the future, we plan to increase the technique’s scalability. Apart from mak-
ing engineering improvements to our implementation, we plan to explore better
strategies for the application of the Reluplex rules, and to employ advanced con-
ﬂict analysis techniques for reducing the amount of case-splitting required. An-
other direction is to provide better soundness guarantees without harming per-
formance, for example by replaying ﬂoating-point solutions using precise arith-
metic [17], or by producing externally-checkable correctness proofs [15]. Finally,
we plan to extend our approach to handle DNNs with additional kinds of layers.
We speculate that the mechanism we applied to ReLUs can be applied to other
piecewise linear layers, such as max-pooling layers.

Acknowledgements. We thank Neal Suchy from the Federal Aviation Admin-
istration, Lindsey Kuper from Intel and Tim King from Google for their valuable
comments and support. This work was partially supported by a grant from Intel.

References

1. C. Barrett, R. Nieuwenhuis, A. Oliveras, and C. Tinelli. Splitting On Demand
In Proc. 13th Int. Conf. on Logic for Programming,

in SAT Modulo Theories.
Artiﬁcial Intelligence, and Reasoning (LPAR), pages 512–526, 2006.

2. C. Barrett, R. Sebastiani, S. Seshia, and C. Tinelli. Satisﬁability Modulo Theories.
In A. Biere, M. J. H. Heule, H. van Maaren, and T. Walsh, editors, Handbook of
Satisﬁability, volume 185 of Frontiers in Artiﬁcial Intelligence and Applications,
chapter 26, pages 825–885. IOS Press, 2009.

3. O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi.
Measuring Neural Net Robustness with Constraints. In Proc. 30th Conf. on Neural
Information Processing Systems (NIPS), 2016.

4. M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
L. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba.
End to End Learning for Self-Driving Cars, 2016. Technical Report. http:
//arxiv.org/abs/1604.07316.

5. G. Dantzig. Linear Programming and Extensions. Princeton University Press,

1963.

6. X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse Rectiﬁer Neural Networks. In
Proc. 14th Int. Conf. on Artiﬁcial Intelligence and Statistics (AISTATS), pages
315–323, 2011.

7. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
8. I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial

Examples, 2014. Technical Report. http://arxiv.org/abs/1412.6572.

9. G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Van-
houcke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep Neural Networks for
Acoustic Modeling in Speech Recognition: The Shared Views of Four Research
Groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

10. X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety Veriﬁcation of Deep
Neural Networks, 2016. Technical Report. http://arxiv.org/abs/1610.06940.
11. K. Jarrett, K. Kavukcuoglu, and Y. LeCun. What is the Best Multi-Stage Ar-
chitecture for Object Recognition? In Proc. 12th IEEE Int. Conf. on Computer
Vision (ICCV), pages 2146–2153, 2009.

12. J.-B. Jeannin, K. Ghorbal, Y. Kouskoulas, R. Gardner, A. Schmidt, E. Zawadzki,
and A. Platzer. A Formally Veriﬁed Hybrid System for the Next-Generation Air-
borne Collision Avoidance System. In Proc. 21st Int. Conf. on Tools and Algo-
rithms for the Construction and Analysis of Systems (TACAS), pages 21–36, 2015.
13. K. Julian, J. Lopez, J. Brush, M. Owen, and M. Kochenderfer. Policy Compression
for Aircraft Collision Avoidance Systems. In Proc. 35th Digital Avionics Systems
Conf. (DASC), pages 1–10, 2016.

14. G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex, 2017.

https://github.com/guykatzz/ReluplexCav2017.

15. G. Katz, C. Barrett, C. Tinelli, A. Reynolds, and L. Hadarean. Lazy Proofs for
In Proc. 16th Int. Conf. on Formal Methods in

DPLL(T)-Based SMT Solvers.
Computer-Aided Design (FMCAD), pages 93–100, 2016.

16. T. King. Eﬀective Algorithms for the Satisﬁability of Quantiﬁer-Free Formulas

Over Linear Real and Integer Arithmetic. PhD Thesis, 2014.

17. T. King, C. Barret, and C. Tinelli. Leveraging Linear and Mixed Integer Program-
ming for SMT. In Proc. 14th Int. Conf. on Formal Methods in Computer-Aided
Design (FMCAD), pages 139–146, 2014.

18. M. Kochenderfer. Decision Making Under Uncertainty: Theory and Application,
chapter Optimized Airborne Collision Avoidance, pages 259–276. MIT, 2015.
19. M. Kochenderfer and J. Chryssanthacopoulos. Robust Airborne Collision Avoid-
ance through Dynamic Programming. Project Report ATC-371, Massachusetts
Institute of Technology, Lincoln Laboratory, 2011.

20. M. Kochenderfer, M. Edwards, L. Espindle, J. Kuchar, and J. Griﬃth. Airspace
Encounter Models for Estimating Collision Risk. AIAA Journal on Guidance,
Control, and Dynamics, 33(2):487–499, 2010.

21. M. Kochenderfer, J. Holland, and J. Chryssanthacopoulos. Next Generation Air-
borne Collision Avoidance System. Lincoln Laboratory Journal, 19(1):17–33, 2012.
Imagenet Classiﬁcation with Deep
Convolutional Neural Networks. Advances in Neural Information Processing Sys-
tems, pages 1097–1105, 2012.

22. A. Krizhevsky, I. Sutskever, and G. Hinton.

23. J. Kuchar and A. Drumm. The Traﬃc Alert and Collision Avoidance System.

Lincoln Laboratory Journal, 16(2):277–296, 2007.

24. A. Maas, A. Hannun, and A. Ng. Rectiﬁer Nonlinearities improve Neural Network
Acoustic Models. In Proc. 30th Int. Conf. on Machine Learning (ICML), 2013.
25. J. Marques-Silva and K. Sakallah. GRASP: A Search Algorithm for Propositional

Satisﬁability. IEEE Transactions on Computers, 48(5):506–521, 1999.

26. V. Nair and G. Hinton. Rectiﬁed Linear Units Improve Restricted Boltzmann
Machines. In Proc. 27th Int. Conf. on Machine Learning (ICML), pages 807–814,
2010.

27. R. Nieuwenhuis, A. Oliveras, and C. Tinelli.

Solving SAT and SAT Mod-
ulo Theories: From an abstract Davis–Putnam–Logemann–Loveland procedure to
DPLL(T ). Journal of the ACM (JACM), 53(6):937–977, 2006.

28. M. Padberg and G. Rinaldi. A Branch-And-Cut Algorithm for the Resolution
of Large-Scale Symmetric Traveling Salesman Problems. IEEE Transactions on
Computers, 33(1):60–100, 1991.

29. L. Pulina and A. Tacchella. An Abstraction-Reﬁnement Approach to Veriﬁca-
tion of Artiﬁcial Neural Networks. In Proc. 22nd Int. Conf. on Computer Aided
Veriﬁcation (CAV), pages 243–257, 2010.

30. L. Pulina and A. Tacchella. Challenging SMT Solvers to Verify Neural Networks.

AI Communications, 25(2):117–135, 2012.

31. M. Riesenhuber and P. Tomaso. Hierarchical Models of Object Recognition in

Cortex. Nature Neuroscience, 2(11):1019–1025, 1999.

32. D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, and S. Dieleman.
Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature,
529(7587):484–489, 2016.

33. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and
Intriguing Properties of Neural Networks, 2013. Technical Report.

R. Fergus.
http://arxiv.org/abs/1312.6199.

34. R. Vanderbei. Linear Programming: Foundations and Extensions. Springer, 1996.

Appendix: Supplementary Material

I Verifying Properties in DNNs with ReLUs is

NP-Complete

Let N be a DNN with ReLUs and let ϕ denote a property that is a conjunction of
linear constraints on the inputs x and outputs y of N , i.e. ϕ = ϕ1(x)∧ϕ2(y). We
say that ϕ is satisﬁable on N if there exists an assignment α for the variables x
and y such that α(y) is the result of propagating α(x) through N and α satisﬁes
ϕ.

Claim. The problem of determining whether ϕ is satisﬁable on N for a given
DNN N and a property ϕ is NP-complete.

Proof. We ﬁrst show that the problem is in NP. A satisﬁability witness is simply
an assignment α(x) for the input variables x. This witness can be checked by
feeding the values for the input variables forward through the network, obtaining
the assignment α(y) for the output values, and checking whether ϕ1(x) ∧ ϕ2(y)
holds under the assignment α.

Next, we show that the problem is NP-hard, using a reduction from the 3-
SAT problem. We will show how any 3-SAT formula ψ can be transformed into
a DNN with ReLUs N and a property ϕ, such that ϕ is satisﬁable on N if and
only if ψ is satisﬁable.

Let ψ = C1 ∧ C2 ∧ . . . ∧ Cn denote a 3-SAT formula over variable set X =
{x1, . . . , xk}, i.e. each Ci is a disjunction of three literals q1
i where the q’s
are variables from X or their negations. The question is to determine whether
there exists an assignment a : X → {0, 1} that satisﬁes ψ, i.e. that satisﬁes all
the clauses simultaneously.

i ∨q2

i ∨q3

For simplicity, we ﬁrst show the construction assuming that the input nodes
take the discrete values 0 or 1. Later we will explain how this limitation can be
relaxed, so that the only limitation on the input nodes is that they be in the
range [0, 1].

We begin by introducing the disjunction gadget which, given nodes q1, q2, q3 ∈
{0, 1}, outputs a node yi that is 1 if q1 + q2 + q3 ≥ 1 and 0 otherwise. The gadget
is shown below for the case that the qi literals are all variables (i.e. not negations
of variables):

q1
i

q2
i

q3
i

1

1

1

−

−

−

1

ReLU

ti

1

−

yi

1

1

The disjunction gadget can be regarded as calculating the expression

yi = 1 − max (0, 1 −

3
(cid:88)

j=1

qj
i )

If there is at least one input variable set to 1, yi will be equal to 1. If all inputs
are 0, yi will be equal to 0. The crux of this gadget is that the ReLU operator
allows us to guarantee that even if there are multiple inputs set to 1, the output
yi will still be precisely 1.

In order to handle any negative literals qj

i ≡ ¬xj, before feeding the literal

into the disjunction gadget we ﬁrst use a negation gadget:

This gadget simply calculates 1 − xj, and then we continue as before.

The last part of the construction involves a conjunction gadget:

1

−
1

qj
i

xj

1

y1

...

yn

1

1

y

Assuming all nodes y1, . . . , yn are in the domain {0, 1}, we require that node y
be in the range [n, n]. Clearly this holds only if yi = 1 for all i.

Finally, in order to check whether all clauses C1, . . . , Cn are simultaneously
satisﬁed, we construct a disjunction gadget for each of the clauses (using nega-
tion gadgets for their inputs as needed), and combine them using a conjunction
gadget:

x1

x2

...

1

xn

1

−
1

y1

...

1

−
1

yn

ReLU

t1

ReLU

tn

1

1

1

1

y

where the input variables are mapped to each ti node according to the deﬁnition
of clause Ci. As we discussed before, node yi will be equal to 1 if clause Ci is
satisﬁed, and will be 0 otherwise. Therefore, node y will be in the range [n, n]
if and only if all clauses are simultaneously satisﬁed. Consequently, an input
assignment a : X → {0, 1} satisﬁes the input and output constraints on the
network if and only if it also satisﬁes the original ψ, as needed.

The construction above is based on the assumption that we can require that
the input nodes take values in the discrete set {0, 1}, which does not ﬁt our
assumption that ϕ1(x) is a conjunction of linear constraints. We show now how
this requirement can be relaxed.

Let (cid:15) > 0 be a very small number. We set the input range for each variable
xi to be [0, 1], but we will ensure that any feasible solution has xi ∈ [0, (cid:15)] or
xi ∈ [1 − (cid:15), 1]. We do this by adding to the network for each xi an auxiliary
gadget that uses ReLU nodes to compute the expression

max (0, (cid:15) − x) + max (0, x − 1 + (cid:15)),

and requiring that the output node of this gadget be in the range [0, (cid:15)]. It is
straightforward to show that this holds for x ∈ [0, 1] if and only if x ∈ [0, (cid:15)] or
x ∈ [1 − (cid:15), 1].

The disjunction gadgets in our construction then change accordingly. The
yi nodes at the end of each gadget will no longer take just the discrete values
{0, 1}, but instead be in the range [0, 3 · (cid:15)] if all inputs were in the range [0, (cid:15)],
or in the range [1 − (cid:15), 1] if at least one input was in the range [1 − (cid:15), (cid:15)].

If every input clause has at least one node in the range [1 − (cid:15), 1] then all
yi nodes will be in the range [1 − (cid:15), 1], and consequently y will be in the range
[n(1 − (cid:15)), n]. However, if at least one clause does not have a node in the range
[1 − (cid:15), 1] then y will be smaller than n(1 − (cid:15)) (for (cid:15) < 1
n+3 ). Thus, by requiring
that y ∈ [n(1 − (cid:15)), n], the input and output constraints will be satisﬁable on
the network if and only if ψ is satisﬁable; and the satisfying assignment can be
(cid:117)(cid:116)
constructed by treating every xi ∈ [0, (cid:15)] as 0 and every xi ∈ [1 − (cid:15), 1] as 1.

II The Reluplex Calculus is Sound and Complete

We deﬁne a derivation tree as a tree where each node is a conﬁguration whose
children (if any) are obtained by applying to it one of the derivation rules. A
derivation tree D derives a derivation tree D(cid:48) if D(cid:48) is obtained from D by
applying exactly one derivation rule to one of D’s leaves. A derivation is a
sequence Di of derivation trees such that D0 has only a single node and each Di
derives Di+1. A refutation is a derivation ending in a tree, all of whose leaves
are UNSAT. A witness is a derivation ending in a tree, at least one of whose leaves
is SAT. If φ is a conjunction of atoms, we say that D is a derivation from φ if
the initial tree in D contains the conﬁguration initialized from φ. A calculus is
sound if, whenever a derivation D from φ is either a refutation or a witness, φ is
correspondingly unsatisﬁable or satisﬁable, respectively. A calculus is complete
if there always exists either a refutation or a witness starting from any φ.

In order to prove that the Reluplex calculus is sound, we ﬁrst prove the

following lemmas:

Lemma 1. Let D denote a derivation starting from a derivation tree D0 with
a single node s0 = (cid:104)B0, T0, l0, u0, α0, R0(cid:105). Then, for every derivation tree Di
appearing in D, and for each node s = (cid:104)B, T, l, u, α, R(cid:105) appearing in Di (except
for the distinguished nodes SAT and UNSAT), the following properties hold:

(i) an assignment satisﬁes T0 if and only if it satisﬁes T ; and
(ii) the assignment α satisﬁes T (i.e., α satisﬁes all equations in T ).

Proof. The proof is by induction on i. For i = 0, the claim holds trivially (recall
that α0 assigns every variable to 0). Now, suppose the claim holds for some i and
consider Di+1. Di+1 is equivalent to Di except for the addition of one or more
nodes added by the application of a single derivation rule d to some node s with
tableau T . Because s appears in Di, we know by the induction hypothesis that
an assignment satisﬁes T0 iﬀ it satisﬁes T and that α satisﬁes T . Let s(cid:48) be a new
node (not a distinguished node SAT or UNSAT) with tableau T (cid:48) and assignment
α(cid:48), introduced by the rule d. Note that d cannot be ReluSuccess or Failure as
these introduce only distinguished nodes, and that if d is deriveLowerBound,
deriveUpperBound, or ReluSplit, then both the tableau and the assigment are
unchanged, so both properties are trivially preserved.

Suppose d is Pivot1, Pivot2 or PivotForRelu. For any of these rules, α(cid:48) = α
and T (cid:48) = pivot(T, i, j) for some i and j. Observe that by deﬁnition of the pivot
operation, the equations of T logically entail those of T (cid:48) and vice versa, and so
they are satisﬁed by exactly the same assignments. From this observation, both
properties follow easily.

The remaining cases are when d is Update, Updateb or Updatef . For these
rules, T (cid:48) = T , from which property (i) follows trivially. For property (ii), we
ﬁrst note that α(cid:48) = update(α, xi, δ) for some i and δ. By deﬁnition of the update
operation, because α satisﬁed the equations of T , α(cid:48) continues to satisfy these
equations and so (because T (cid:48) = T ) α(cid:48) also satisﬁes T (cid:48).
(cid:117)(cid:116)

Lemma 2. Let D denote a derivation starting from a derivation tree D0 with
a single node s0 = (cid:104)B0, T0, l0, u0, α0, R0(cid:105). If there exists an assignment α∗ (not
necessarily α0) such that α∗ satisﬁes T0 and l0(xi) ≤ α∗(xi) ≤ u0(xi) for all
i, then for each derivation tree Di appearing in D at least one of these two
properties holds:

(i) Di has a SAT leaf.
(ii) Di has a leaf s = (cid:104)B, T, l, u, α, R(cid:105) (that is not a distinguished node SAT or

UNSAT) such that l(xi) ≤ α∗(xi) ≤ u(xi) for all i.

Proof. The proof is again by induction on i. For i = 0, property (ii) holds
trivially. Now, suppose the claim holds for some i and consider Di+1. Di+1 is
equivalent to Di except for the addition of one or more nodes added by the
application of a single derivation rule d to a leaf s of Di.

Due to the induction hypothesis, we know that Di has a leaf ¯s that is either a
SAT leaf or that satisﬁes property (ii). If ¯s (cid:54)= s, then ¯s also appears in Di+1, and
the claim holds. We will show that the claim also holds when ¯s = s. Because none
of the derivation rules can be applied to a SAT or UNSAT node, we know that node
s is not a distinguished SAT or UNSAT node, and we denote s = (cid:104)B, T, l, u, α, R(cid:105).
If d is ReluSuccess, Di+1 has a SAT leaf and property (i) holds. Suppose d is
Pivot1, Pivot2, PivotForRelu, Update, Updateb or Updatef . In any of these cases,
node s has a single child in Di+1, which we denote s(cid:48) = (cid:104)B(cid:48), T (cid:48), l(cid:48), u(cid:48), α(cid:48), R(cid:48)(cid:105).
By deﬁnition of these derivation rules, l(cid:48)(xj) = l(xj) and u(cid:48)(xj) = u(xj) for all
j. Because node s satisﬁes property (ii), we get that s(cid:48) is a leaf that satisﬁes
property (ii), as needed.

Suppose that d is ReluSplit, applied to a pair (cid:104)xi, xj(cid:105) ∈ R. Node s has two
children in Di+1: a state s+ in which the lower bound for xi is 0, and a state s−
in which the upper bound for xi is 0. All other lower and upper bounds in s+
and s− are identical to those of s. It is straightforward to see that if α∗(xi) ≥ 0
then property (ii) holds for s+, and if α∗(xi) ≤ 0 then property (ii) holds for s−.
Either way, Di+1 has a leaf for which property (ii) holds, as needed.

Next, consider the case where d is deriveLowerBound (the deriveUpperBound
case is symmetrical and is omitted). Node s has a single child in Di+1, which
we denote s(cid:48) = (cid:104)B(cid:48), T (cid:48), l(cid:48), u(cid:48), α(cid:48), R(cid:48)(cid:105). Let xi denote the variable to which
deriveLowerBound was applied. By deﬁnition, l(cid:48)(xi) ≥ l(xi), and all other vari-
able bounds are unchanged between s and s(cid:48). Thus, it suﬃces to show that
α∗(xi) ≥ l(cid:48)(xi). Because α∗ satisﬁes T0, it follows from Lemma 1 that it sat-
isﬁes T . By the induction hypothesis, l(xj) ≤ α∗(xj) ≤ u(xj) for all j. The
fact that α∗(xi) ≥ l(cid:48)(xi) then follows directly from the guard condition of
deriveLowerBound.

The only remaining case is when d is the Failure rule. We explain why this
case is impossible. Suppose towards contradiction that in node s the Failure
rule is applicable to variable xi, and suppose (without loss of generality) that
α(xi) < l(xi). By the inductive hypothesis, we know that l(xj) ≤ α∗(xj) ≤ u(xj)
for all j, and by Lemma 1 we know that α∗ satisﬁes T . Consequently, there must
be a variable xk such that (Ti,k > 0 ∧ α(xk) < α∗(xk)), or (Ti,k < 0 ∧ α(xk) >

α∗(xk)). But because all variables under α∗ are within their bounds, this means
that slack+(xi) (cid:54)= ∅, which is contradictory to the fact that the Failure rule was
(cid:117)(cid:116)
applicable in s.

Lemma 3. Let D denote a derivation starting from a derivation tree D0 with
a single node s0 = (cid:104)B0, T0, l0, u0, α0, R0(cid:105). Then, for every derivation tree Di
appearing in D, and for each node s = (cid:104)B, T, l, u, α, R(cid:105) appearing in Di (except
for the distinguished nodes SAT and UNSAT), the following properties hold:

(i) R = R0; and
(ii) l(xi) ≥ l0(xi) and u(xi) ≤ u0(xi) for all i.

Proof. Property (i) follows from the fact that none of the derivation rules (except
for ReluSuccess and Failure) changes the set R. Property (ii) follows from the fact
that the only rules (except for ReluSuccess and Failure) that update lower and
upper variable bounds are deriveLowerBound and deriveUpperBound, respectively,
and that these rules can only increase lower bounds or decrease upper bounds.

We are now ready to prove that the Reluplex calculus is sound and complete.

Claim. The Reluplex calculus is sound.

Proof. We begin with the satisﬁable case. Let D denote a witness for φ. By
deﬁnition, the ﬁnal tree D in D has a SAT leaf. Let s0 = (cid:104)B0, T0, l0, u0, α0, R0(cid:105)
denote the initial state of D0 and let s = (cid:104)B, T, l, u, α, R(cid:105) denote a state in D in
which the ReluSuccess rule was applied (i.e., a predecessor of a SAT leaf).

By Lemma 1, α satisﬁes T0. Also, by the guard conditions of the ReluSuccess
rule, l(xi) ≤ α(xi) ≤ u(xi) for all i. By property (ii) of Lemma 3, this implies
that l0(xi) ≤ α(xi) ≤ u0(xi) for all i. Consequently, α satisﬁes every linear
inequality in φ.

Finally, we observe that by the conditions of the ReluSuccess rule, α satisﬁes
all ReLU constraints of s. From property (i) of Lemma 3, it follows that α also
satisﬁes the ReLU constraints of s0, which are precisely the ReLU constraints in
φ. We conclude that α satisﬁes every constraint in φ, and hence φ is satisﬁable,
as needed.

For the unsatisﬁable case, it suﬃces to show that if φ is satisﬁable then
there cannot exist a refutation for it. This is a direct result of Lemma 2: if φ is
satisﬁable, then there exists an assignment α∗ that satisﬁes the initial tableau
T0, and for which all variables are within bounds. Hence, Lemma 2 implies that
any derivation tree in any derivation D from φ must have a leaf that is not the
distinguished UNSAT leaf. It follows that there cannot exist a refutation for φ. (cid:117)(cid:116)

Claim. The Reluplex calculus is complete.

Proof. Having shown that the Reluplex calculus is sound, it suﬃces to show
a strategy for deriving a witness or a refutation for every φ within a ﬁnite
number of steps. As mentioned in Section 3, one such strategy involves two
steps: (i) Eagerly apply the ReluSplit rule until it no longer applies; and (ii) For

every leaf of the resulting derivation tree, apply the simplex rules Pivot1, Pivot2,
Update, and Failure, and the Reluplex rule ReluSuccess, in a way that guarantees
a SAT or an UNSAT conﬁguration is reached within a ﬁnite number of steps.

Let D denote the derivation tree obtained after step (i). In every leaf s of D,
all ReLU connections have been eliminated, meaning that the variable bounds
force each ReLU connection to be either active or inactive. This means that
every such s can be regarded as a pure simplex problem, and that any solution
to that simplex problem is guaranteed to satisfy also the ReLU constraints in s.
The existence of a terminating simplex strategy for deciding the satisﬁability
of each leaf of D follows from the completeness of the simplex calculus [34]. One
such widely used strategy is Bland’s Rule [34]. We observe that although the
simplex Success rule does not exist in Reluplex, it can be directly substituted
with the ReluSuccess rule. This is so because, having applied the ReluSplit rule to
completion, any assignment that satisﬁes the variable bounds in s also satisﬁes
the ReLU constraints in s.

It follows that for every φ we can produce a witness or a refutation, as needed.
(cid:117)(cid:116)

III A Reluplex Strategy that Guarantees Termination

As discussed in Section 6, our strategy for applying the Reluplex rules was to re-
peatedly ﬁx any out-of-bounds violations ﬁrst (using the original simplex rules),
and only afterwards to correct any violated ReLU constraints using the Updateb,
Updatef and PivotForRelu rules. If correcting a violated ReLU constraint intro-
duced new out-of-bounds violations, these were again ﬁxed using the simplex
rules, and so on.

As mentioned above, there exist well known strategies for applying the sim-
plex rules in a way that guarantees that within a ﬁnite number of steps, either
all variables become assigned to values within their bounds, or the Failure rule is
applicable (and is applied) [34]. By using such a strategy for ﬁxing out-of-bounds
violations, and by splitting on a ReLU pair whenever the Updateb, Updatef or
PivotForRelu rules are applied to it more some ﬁxed number of times, termination
is guaranteed.

IV Under-Approximations

Under-approximation can be integrated into the Reluplex algorithm in a straight-
forward manner. Consider a variable x with lower and upper bounds l(x)
and u(x), respectively. Since we are searching for feasible solutions for which
x ∈ [l(x), u(x)], an under-approximation can be obtained by restricting this
range, and only considering feasible solutions for which x ∈ [l(x) + (cid:15), u(x) − (cid:15)]
for some small (cid:15) > 0.

Applying under-approximations can be particularly useful when it eﬀectively
eliminates a ReLU constraint (consequently reducing the potential number of

case splits needed). Speciﬁcally, observe a ReLU pair xf = ReLU(xb) for which
we have l(xb) ≥ −(cid:15) for a very small positive (cid:15). We can under-approximate this
range and instead set l(xb) = 0; and, as previously discussed, we can then ﬁx
the ReLU pair to the active state. Symmetrical measures can be employed when
learning a very small upper bound for xf , in this case leading to the ReLU pair
being ﬁxed in the inactive state.

Any feasible solution that is found using this kind of under-approximation
will be a feasible solution for the original problem. However, if we determine that
the under-approximated problem is infeasible, the original may yet be feasible.

V Encoding ReLUs for SMT and LP Solvers

We demonstrate the encoding of ReLU nodes that we used for the evaluation
conducted using SMT and LP solvers. Let y = ReLU(x). In the SMTLIB format,
used by all SMT solvers that we tested, ReLUs were encoded using an if-then-else
construct:

(assert (= y (ite (>= x 0) x 0)))

In LP format this was encoded using mixed integer programming. Using
Gurobi’s built-in Boolean type, we deﬁned for every ReLU connection a pair of
Boolean variables, bon and boﬀ, and used them to encode the two possible states
of the connection. Taking M to be a very large positive constant, we used the
following assertions:

bon + boff = 1
y >= 0
x - y - M*boff <= 0
x - y + M*boff >= 0
y - M*bon <= 0
x - M*bon <= 0

When bon= 1 and boﬀ= 0, the ReLU connection is in the active state; and

otherwise, when bon= 0 and boﬀ= 1, it is in the inactive state.

In the active case, because boﬀ = 0 the third and fourth equations imply
that x = y (observe that y is always non-negative). M is very large, and can be
regarded as ∞; hence, because bon= 1, the last two equations merely imply that
x, y ≤ ∞, and so pose no restriction on the solution.

In the inactive case, bon = 0, and so the last two equations force y = 0
and x ≤ 0. In this case boﬀ= 1 and so the third and fourth equations pose no
restriction on the solution.

VI Formal Deﬁnitions for Properties φ1,. . . ,φ10

The units for the ACAS Xu DNNs’ inputs are:

– ρ: feet.
– θ, ψ: radians.
– vown, vint: feet per second.
– τ : seconds.

θ and ψ are measured counter clockwise, and are always in the range [−π, π].

In line with the discussion in Section 5, the family of 45 ACAS Xu DNNs
are indexed according to the previous action aprev and time until loss of vertical
separation τ . The possible values are for these two indices are:

1. aprev: [Clear-of-Conﬂict, weak left, weak right, strong left, strong right].
2. τ : [0, 1, 5, 10, 20, 40, 60, 80, 100].

We use Nx,y to denote the network trained for the x-th value of aprev and y-th
value of τ . For example, N2,3 is the network trained for the case where aprev =
weak left and τ = 5. Using this notation, we now give the formal deﬁnition of
each of the properties φ1, . . . , φ10 that we tested.

Property φ1.

Property φ2.

Property φ3.

– Description: If the intruder is distant and is signiﬁcantly slower than the
ownship, the score of a COC advisory will always be below a certain ﬁxed
threshold.

– Tested on: all 45 networks.
– Input constraints: ρ ≥ 55947.691, vown ≥ 1145, vint ≤ 60.
– Desired output property: the score for COC is at most 1500.

– Description: If the intruder is distant and is signiﬁcantly slower than the

ownship, the score of a COC advisory will never be maximal.

– Tested on: Nx,y for all x ≥ 2 and for all y.
– Input constraints: ρ ≥ 55947.691, vown ≥ 1145, vint ≤ 60.
– Desired output property: the score for COC is not the maximal score.

– Description: If the intruder is directly ahead and is moving towards the

ownship, the score for COC will not be minimal.
– Tested on: all networks except N1,7, N1,8, and N1,9.
– Input constraints: 1500 ≤ ρ ≤ 1800, −0.06 ≤ θ ≤ 0.06, ψ ≥ 3.10, vown ≥ 980,

vint ≥ 960.

– Desired output property: the score for COC is not the minimal score.

Property φ4.

– Description: If the intruder is directly ahead and is moving away from the
ownship but at a lower speed than that of the ownship, the score for COC
will not be minimal.

– Tested on: all networks except N1,7, N1,8, and N1,9.
– Input constraints: 1500 ≤ ρ ≤ 1800, −0.06 ≤ θ ≤ 0.06, ψ = 0, vown ≥ 1000,

700 ≤ vint ≤ 800.

– Desired output property: the score for COC is not the minimal score.

Property φ5.

advises “strong right”.

– Description: If the intruder is near and approaching from the left, the network

– Tested on: N1,1.
– Input constraints: 250 ≤ ρ ≤ 400, 0.2 ≤ θ ≤ 0.4, −3.141592 ≤ ψ ≤

−3.141592 + 0.005, 100 ≤ vown ≤ 400, 0 ≤ vint ≤ 400.

– Desired output property: the score for “strong right” is the minimal score.

Property φ6.

– Description: If the intruder is suﬃciently far away, the network advises COC.
– Tested on: N1,1.
– Input constraints: 12000 ≤ ρ ≤ 62000, (0.7 ≤ θ ≤ 3.141592) ∨ (−3.141592 ≤
θ ≤ −0.7), −3.141592 ≤ ψ ≤ −3.141592 + 0.005, 100 ≤ vown ≤ 1200,
0 ≤ vint ≤ 1200.

– Desired output property: the score for COC is the minimal score.

Property φ7.

– Description: If vertical separation is large, the network will never advise a

strong turn.
– Tested on: N1,9.
– Input constraints: 0 ≤ ρ ≤ 60760, −3.141592 ≤ θ ≤ 3.141592, −3.141592 ≤

ψ ≤ 3.141592, 100 ≤ vown ≤ 1200, 0 ≤ vint ≤ 1200.

– Desired output property: the scores for “strong right” and “strong left” are

never the minimal scores.

Property φ8.

– Description: For a large vertical separation and a previous “weak left” advi-
sory, the network will either output COC or continue advising “weak left”.

– Tested on: N2,9.
– Input constraints: 0 ≤ ρ ≤ 60760, −3.141592 ≤ θ ≤ −0.75 · 3.141592, −0.1 ≤

ψ ≤ 0.1, 600 ≤ vown ≤ 1200, 600 ≤ vint ≤ 1200.

– Desired output property: the score for “weak left” is minimal or the score

for COC is minimal.

Property φ9.

– Description: Even if the previous advisory was “weak right”, the presence of
a nearby intruder will cause the network to output a “strong left” advisory
instead.

– Tested on: N3,3.
– Input constraints: 2000 ≤ ρ ≤ 7000, −0.4 ≤ θ ≤ −0.14, −3.141592 ≤ ψ ≤

−3.141592 + 0.01, 100 ≤ vown ≤ 150, 0 ≤ vint ≤ 150.

– Desired output property: the score for “strong left” is minimal.

Property φ10.

– Description: For a far away intruder, the network advises COC.
– Tested on: N4,5.
– Input constraints: 36000 ≤ ρ ≤ 60760, 0.7 ≤ θ ≤ 3.141592, −3.141592 ≤

ψ ≤ −3.141592 + 0.01, 900 ≤ vown ≤ 1200, 600 ≤ vint ≤ 1200.

– Desired output property: the score for COC is minimal.

