VegFru: A Domain-Speciﬁc Dataset for Fine-grained Visual Categorization

Saihui Hou, Yushan Feng and Zilei Wang
Department of Automation, University of Science and Technology of China
{saihui, fyushan}@mail.ustc.edu.cn, zlwang@ustc.edu.cn

Abstract

In this paper, we propose a novel domain-speciﬁc data-
set named VegFru for ﬁne-grained visual categorization
(FGVC). While the existing datasets for FGVC are mainly
focused on animal breeds or man-made objects with limit-
ed labelled data, VegFru is a larger dataset consisting of
vegetables and fruits which are closely associated with the
daily life of everyone. Aiming at domestic cooking and food
management, VegFru categorizes vegetables and fruits ac-
cording to their eating characteristics, and each image con-
tains at least one edible part of vegetables or fruits with the
same cooking usage. Particularly, all the images are la-
belled hierarchically. The current version covers vegetables
and fruits of 25 upper-level categories and 292 subordinate
classes. And it contains more than 160,000 images in total
and at least 200 images for each subordinate class. Accom-
panying the dataset, we also propose an effective framework
called HybridNet to exploit the label hierarchy for FGVC.
Speciﬁcally, multiple granularity features are ﬁrst extracted
by dealing with the hierarchical labels separately. And then
they are fused through explicit operation, e.g., Compact Bi-
linear Pooling, to form a uniﬁed representation for the ul-
timate recognition. The experimental results on the novel
VegFru, the public FGVC-Aircraft and CUB-200-2011 in-
dicate that HybridNet achieves one of the top performance
on these datasets. The dataset and code are available at
https://github.com/ustc-vim/vegfru.

1. Introduction

In computer vision, ﬁne-grained visual categorization
(FGVC) refers to categorizing objects into subordinate
classes, e.g., breeds of birds or dogs. Compared to gener-
ic classiﬁcation [6], FGVC needs to handle more subtle
inter-class difference and larger intra-class variation of ob-
jects, thus requiring more discriminative and robust im-
age representation. Recent years have witnessed the res-
urrection of deep convolutional neural network (DCNN),
which holds state-of-the-art performance of various vi-
sual tasks [8, 25, 5]. The top-performing methods for

Figure 1. Sample images in VegFru. Top: vegetable images. Bot-
tom: fruit images. Best viewed electronically.

FGVC [9, 36, 37] are also built upon DCNN and the training
is data-hungry. However, the data with ﬁne-grained labels
is usually insufﬁcient, e.g., in CUB-200-2011 [28] there are
only about 30 training images for each class. And the ex-
isting datasets for FGVC are mainly focused on domains of
animal breeds, e.g., birds [28] and dogs [11], or man-made
objects, e.g., cars [13] and aircrafts [18]. As the saying
goes, hunger breeds discontent. In modern life, increasing
attention has been paid to how to go on a balanced and nu-
tritious diet. However, to the best of our knowledge, there is
still no public dataset specially designed for recognizing the
raw food materials and recommending appropriate recipes
for individuals.

In this work, aiming at domestic cooking and food
management, we introduce a novel domain-speciﬁc data-
set named VegFru, which consists of vegetables and fruits
that are closely associated with people’s diet. In VegFru,
vegetables and fruits are categorized according to their eat-
ing characteristics, e.g., different edible parts of a certain
vegetable or fruit, such as leaf and root, are classiﬁed in-
to separate subordinate classes. And the objects in each
image are the raw food materials, while the images that
contain cooked food whose raw materials are indistinguish-
able are ﬁltered out. Currently, the dataset covers vegeta-
bles and fruits of 25 upper-level categories and 292 subor-
dinate classes1, which has taken in all species in common.
And it contains more than 160,000 images in total and at

1The upper-level category and subordinate class are respectively denot-

ed as sup-class and sub-class in the following.

541

(a)           

(b)          

(c)            

(d)

Figure 2. (a) chive (b) shallot (c) leek (d) green Chinese onion.
These images belong to different sub-classes but with subtle inter-
class difference.

(a)           

(b)          

(c)            

(d)

Figure 3. (a) lotus root (b) lotus root (c) lotus root (d) lotus root.
These images belong to the same sub-classes but with large intra-
class variation.

least 200 images for each sub-class, which is much larger
than the previous ﬁne-grained datasets [11, 28, 13, 18]. Par-
ticularly, besides the ﬁne-grained annotation, the images in
VegFru are assigned hierarchical labels. And compared to
the vegetable and fruit subsets of ImageNet [6], the taxono-
my adopted by VegFru is more popular for domestic cook-
ing and food management, and the image collection strictly
serves this purpose, making each image in VegFru contain
at least one edible part of vegetables or fruits with the same
cooking usage. Some sample images are shown in Figure 1.
Our VegFru has the potential to be applied to the follow-

ing aspects, but is not limited to them:

* Fine-grained Visual Categorization (FGVC). The sub-
classes in VegFru all belong to vegetables or fruits, and
there exist subtle inter-class difference (Figure 2) and
large intra-class variation (Figure 3). So it can be con-
sidered as a ﬁne-grained dataset of novel domain, with
more images available for each sub-class.

* Hybrid-granularity methods for FGVC. The label hi-
erarchy has proved to be helpful for image recogni-
tion [33, 30, 32, 37]. With all images labelled hier-
archically, VegFru is naturally well-suited for research
on exploiting the hybrid-granularity information, i.e.,
label hierarchy, for the challenging FGVC.

* Practical applications for domestic cooking and food
management. VegFru collects enormous vegetable and
fruit images of raw food materials and categorizes
them by the eating characteristics. It is closely related
to the daily life of everyone, and thus can promote the
applications of computer vision in the Smart Home [4],
e.g., personalized recipe recommendation.

To verify the application value of VegFru, we also pro-
pose an effective framework named HybridNet with the aim

of utilizing the hybrid-granularity information for FGVC,
which aspect VegFru is well-suited for due to the label hi-
erarchy. We take DCNN model to deal with the issue. In
practice, DCNN is trained in a top-down manner, i.e., the
training is driven by the loss generated at the highest lay-
er according to back propagation. When categorizing the
coarse-grained sup-classes, the network only needs to han-
dle generic attributes (e.g., bird outline), but subtle charac-
teristics (e.g., bird eye, foot) become necessary when dis-
tinguishing the ﬁne-grained sub-classes. Our HybridNet
is exactly motivated by exploiting the complementarity be-
tween the coarse-grained and ﬁne-grained features. Specif-
ically, two-stream DCNNs are ﬁrst trained by feeding the
sup-class and sub-class labels separately, whose features
are then fused to form a uniﬁed representation for the ul-
timate recognition. As for the fusion method, we adopt
the advanced Compact Bilinear Pooling proposed by [7],
in which the model is currently the best for FGVC without
utilizing parts or external data. The experimental results on
VegFru, FGVC-Aircraft [18] and CUB-200-2011 [28] show
the robustness and superiority of HybridNet.

In summary, the main contributions of this work lie in
two folds: VegFru and HybridNet. Speciﬁcally, the contri-
bution of VegFru is highlighted in four aspects: novel do-
main, large scale, label hierarchy and application prospects.
And HybridNet outperforms the model in [7] and achieves
one of the top performance on the three datasets by exploit-
ing the label hierarchy.

The rest of the paper is organized as follows. In Sec-
tion 2, we introduce the construction of VegFru and perform
detailed comparison of VegFru with the vegetable and fruit
subsets of ImageNet and the existing ﬁne-grained datasets.
HybridNet and its related works are presented in Section 3.
In Section 4, we set baselines on VegFru and experimentally
evaluate the proposed HybridNet. Finally, the whole work
is concluded in Section 5.

2. VegFru

2.1. Overview

We build the hierarchical structure of VegFru in accor-
dance with the ofﬁcial literatures [21, 38]. Speciﬁcally, the
vegetable hierarchy is constructed according to the Agricul-
tural Biological Taxonomy described in [21]2, which is the
most reasonable for the cooking purpose and arranges veg-
etables into root vegetable, cabbage, leafy vegetable, etc.
Consequently, we obtain 15 sup-classes vegetables with
200 sub-classes. For fruits, similarly, we adopt the Horti-
cultural Taxonomy in [38] to organize fruits into 10 sup-

2In fact, three are three taxonomies for vegetables in [21]. Besides
the Agricultural Biological Taxonomy, the Botanical Taxonomy divides
vegetables into two categories, namely monocotyledon and dicotyledon,
and the Edible Organ Taxonomy groups vegetables into ﬁve categories,
i.e., root, stem, leaf, ﬂower, and fruit.

542

Table 1. The structure of VegFru. #Sub-the number of sub-
classes included. Perennial∗-Perennial-miscellaneous vegetable.
Persimmons∗-Persimmons-jujubes fruit.

Sup-class
Aquatic vegetable
Brassia oleracea
Bud seedling
Green-leafy vegetable
Perennial∗
Tuber vegetable
Wild vegetable

Root vegetable

Total

Berry fruit
Citrus fruit
Persimmons∗
Pome

Collective fruit

Cucurbites

Total

#Sub
13
9
4
31
13
10
32

Sup-class
Alliaceous
Beans
Cabbage
Eggplant
Melon
Mushroom
Mustard

#Sub
10
15
5
7
14
24
2
11 (beetroot, black salsify,
burdock root, carrot, celeriac,
green radish, kohlrabi, parsnip,
red radish, wasabi, white radish)
15 sup-classes and
200 sub-classes for Veg200

22
13
6
11

Drupe
Litchies
Nut fruit
Other fruit

13
3
11
2
5 (breadfruit, pineapple,
sweetsop, annona muricata,
artocarpus heterophyllus)
6 (golden melon, muskmelon,
honey dew melon, papaya,
netted melon, Hami melon)
10 sup-classes and
92 sub-classes for Fru92

classes and 92 sub-classes. In the current version, there are
91,117 images for vegetables and 69,614 images for fruits.
The number of images for each sub-class varies from 200
to 2000.

VegFru can be naturally divided into two subsets, i.e.,
Veg200 for vegetables and Fru92 for fruits. Table 1 shows
the structure of VegFru, where the sup-classes of Veg200
and Fru92 are listed along with the corresponding number
of sub-classes included. And the sub-classes of Root veg-
etable, Collective fruit and Cucurbites are also listed3.

2.2. VegFru Details

This section presents the details of VegFru. Speciﬁcally,
we will respectively introduce the principles for building
VegFru, process of collecting images, and dataset splits for
training and test.

2.2.1 Building Principles

Aiming at domestic cooking and food management, VegFru
is constructed according to the following principles.

3The detailed sub-classes for each sup-class are provided in the sup-

plementary material.

(a)                        (b)                           

(c)                        (d)

Figure 4. (a) soybean (b) soybean (c) soybean seed (d) soybean
seed. Although (a)-(d) all belong to the seeds of soybean (in d-
ifferent growth periods), they are cooked in disparate ways, thus
being classiﬁed into separate sub-classes.

(a)                        (b)                          (c)                        (d)(cid:3)
Figure 5. (a) potato (b) watermelon (c) cucumber (d) pimen-
to. These images are dropped because the raw food materials are
almost indistinguishable.

* The objects in the images of each sub-class have the

same cooking usage. (Figure 3)

* Each image contains at least one edible part of vegeta-

bles or fruits. (Figure 1)

* The images that contain different edible parts of a cer-
tain vegetable or fruit, e.g., leaf, ﬂower, stem, root, are
classiﬁed into separate sub-classes.

* Even for the images that contain the same edible part
of given vegetable or fruit, if the objects are different
in cooking, we also classify them into different sub-
classes. (Figure 4)

* The objects in each image should be the raw food ma-
terials. If the raw materials of cooked food can not be
made out, the images will be removed. (Figure 5)

2.2.2 Collecting Images

The above principles guide the construction of VegFru, as
well as the process of image collection, which is a really
challenging project.

The ﬁrst step is to collect candidate images for each sub-
class. The images are obtained by searching on the Inter-
net, which is widely used to generate ImageNet [6] and
Microsoft COCO [16]. The sources include Google, Im-
ageNet, Flicker, Bing, Baidu, and so on. The retrieval key-
words are the synonym sets of sub-class names in both Chi-
nese and English. As a consequence, a large number of can-
didate images are collected. Speciﬁcally, over 800 images
are gathered for each sub-class.

Then, to make the dataset highly reliable, the candidate
images are further carefully processed through manual se-
lection. In practice, the images of each sub-class are ﬁltered

543

Figure 6. Sample images of hyacinth bean. Left: Two images
without edible part in ImageNet; Right: Two images with edible
part in VegFru.

(a)

(b)

(c)
Figure 7. Potatoes in the vegetable subsets of ImageNet. Left
To Right: (a) Baked potato (b) French fries (c) Home fries (d)
Mashed potato (e) Uruguay potato. Only the images of Uruguay
potato contain the raw food materials.

(d)

(e)

by ten people on the basis of the class description provided
in [21, 38] and the true positives, following the principles
described in Section 2.2.1. Only images afﬁrmed by more
than eight are reserved. The faded, binary, blurry and dupli-
cated ones are all ﬁltered out.

So far we have completed the construction of 25 sup-
classes and 292 sub-classes, with more than 160, 000 im-
ages totally. Figure 1 displays some sample images collect-
ed by VegFru4.

2.2.3 Dataset Splits

In VegFru, each sub-class contains at least 200 images,
which are divided into training, validation and test set (de-
noted as train, val and test set in the following). An alterna-
tive split way is to ﬁrst arrange the images in each sub-class
randomly. Then the top 100 are selected for train, the fol-
lowing 50 for val, and the rest for test. Finally, a slight
adjustment is applied to the split to ensure that each set is
representative for the variability such as object numbers and
background. The image list for each set is released attached
in the dataset.

2.3. VegFru vs. ImageNet subsets

In this section, we compare VegFru with the vegetable
and fruit subsets of ImageNet from three aspects, i.e., tax-
onomy, image selection and dataset structure. Through the
comparison, the construction and usage of VegFru are fur-
ther motivated.
Taxonomy.

ImageNet [6] constructs its hierarchical
structure based on WordNet [19], which organizes all the

4More sample images are provided in the supplementary material.

Table 2. VegFru vs. ImageNet subsets on dataset structure.
#Sup-the number of sup-classes. #Sub-the number of sub-classes.
Min/Max-the minimum/maximum number of images in each sub-
class. #Sub<200-the number of sub-classes that consist of less
than 200 images.

#Sup

#Sub Min

Max

Vegetables in ImageNet
Vegetables in VegFru
Fruits in ImageNet
Fruits in VegFru

25
15
75
10

175
200
196
92

3
202
0
202

1500+
1807
1500+
1615

#Sub
<200
27
0
50
0

words according to the semantics. For vegetables and fruit-
s, however, we tend to concentrate more on their eating
characteristics in daily life. Actually the taxonomy adopt-
ed by ImageNet for vegetables and fruits is quite unpopular
for domestic cooking and food management, and even con-
tains many repeated categories. For example, turnip and
radish simultaneously belong to root vegetable and crucif-
erous vegetable, and potato is grouped into root vegetable
In fact,
but is also on the list of solanaceous vegetable.
according to [21], potato should be categorized into tuber
vegetable. Moreover, some vegetables which are common
in diet are not included in ImageNet, e.g., water spinach,
shepherd’s purse, basella rubra.

By contrast, in the construction of VegFru, we remove
the rare categories, e.g. woad and ottelia, while many reg-
ular categories are added, e.g., Chinese pumpkin and sug-
arcane. And some categories are grouped into ﬁner class-
es, e.g., radish are divided into white radish, red radish and
green radish. The taxonomy adopted by VegFru specially
serves the purpose of domestic cooking and food manage-
ment in daily life.

Image Selection. All images in VegFru contain the edi-
ble part of a certain vegetable or fruit, which is not included
in lots of images in ImageNet, as shown in Figure 6. Be-
sides, some categories in ImageNet do not cover any raw
food materials. For example, Figure 7 displays the images
of ﬁve potato subordinate classes in the vegetable subsets
of ImageNet, i.e., baked potato, French fries, home fries,
mashed potato and Uruguay potato. Only Uruguay potato
belongs to the raw food materials.

Dataset Structure. Table 2 shows some statistics of
VegFru and the ImageNet subsets. In particular, there are 50
fruit sub-classes and 27 vegetable sub-classes whose num-
ber of images is less than 200 in ImageNet, while VegFru
is comprised of 292 popular sub-classes of vegetables and
fruits with more than 200 images for each sub-classes. And
the taxonomy tree, i.e., the distribution of sup-classes and
sub-classes, is reasonably reorganized for vegetables and
fruits in VegFru.

544

Figure 8. Sample images in FGVC-Aircraft. The airplanes oc-
cupy a large fraction of the whole images. And there exists only
one airplane in each image with relatively clean background.

3. HybridNet

Table 3. VegFru vs. Fine-grained Datasets. #Sup-the number of
sup-classes. #Sub-the number of sub-classes. #Image-the number
of images in total. #Train/#Val/#Test-the number of images in
train/val/test set. #Train+Val(avg)-the average number of images
in each sub-classes for model training (include train and val set).

Dataset

#Sup

#Sub

#Image

#Train

#Val

Birds
Dogs
Cars
Aircrafts

VegFru

none
none
none
70

25

200
120
196
100

292

11788
20580
16185
10000

160731

5994
12000
8144
3334

29200

none
none
none
3333

14600

#Train+
Val(avg)

˜30
100
˜42
˜67

150

#Test

5794
8580
8041
3333

116931

2.4. VegFru vs. Fine grained Datasets

In this section we further compare VegFru with four rep-
resentative ﬁne-gained datasets, i.e., CUB-200-2011 [28]
(Birds), Stanford Dogs [11] (Dogs), Stanford Cars [13]
(Cars) and FGVC-Aircraft [18] (Aircrafts)5, which are
widely used in previous works [35, 12, 17, 7]. The detailed
comparison is shown in Table 3. More ﬁne-grained dataset-
s, such as Oxford Flowers [20] and Pets [23], are not listed
here out of the consideration of simplicity.

Compare to these existing datasets, the domain of Veg-
Fru is novel and more associated with people’s daily life,
which contributes to its broad application prospects. And
VegFru is larger in scale, which has up to 150 images avail-
able in each sub-classes for model training. Particularly,
all the images in VegFru are hierarchically categorized in-
to sup-classes and sub-classes, while the images in these
datasets, except FGVC-Aircraft, are only assigned with
ﬁne-grained labels. So VegFru is well-suited for the hybrid-
granularity research on FGVC. We noticed that the previ-
ous works [30, 39] declared to annotate some ﬁne-grained
datasets, e.g., CUB-200-2011, with extra labels to get the
label hierarchy. However, to the best of our knowledge, the
annotation is not publicly available until the submission,
and the labelling process is labor-intensive. Furthermore,
though FGVC-Aircraft is with hierarchical labels, the ob-
jects of interest, i.e., aircrafts, usually occupy a large frac-
tion of the whole images, and each image only contains one
aircraft with relatively clean background (Figure 8). In con-

5For FGVC-Aircraft, airplane variants are chosen as the labels of sub-
classes, and the 70 sup-classes in Tabel 3 is the number of airplane fami-
lies, which are the upper-level annotaions of airplane variants. Please refer
to [18] for details of this dataset.

Coarse Network 

coarse label

coarse features

Coarse
Extractor

Coarse
Classifier 

Input 
Images

Fusion

fused features

Fused
Classifier

fine label

fine features

Fine
Extractor

Fine
Classifier

Fine Network 

Figure 9. Illustration of the proposed HybridNet. Two-stream
features which deal with the hierarchical labels are ﬁrst extracted
separately, and then sent through the Fusion module to train the
Fused Classiﬁer for overall classiﬁcation.

trast, the images in VegFru are with cluttered background
and vary in number and scale of the objects (Figure 1).

Accompanying the dataset, we also propose an effective
framework called HybridNet to conduct the image classiﬁ-
cation, illustrated in Figure 9. The motivation is to exploit
the label hierarchy for FGVC, which can further verify the
application value of VegFru.

Speciﬁcally, the input images with sup-class and sub-
class labels (denoted as coarse label and ﬁne label in Fig-
ure 9) are ﬁrstly sent into two DCNNs for separate clas-
siﬁcation. Here an end-to-end DCNN is logically divided
into two functional parts, i.e., feature extractor and image
classiﬁer. The division can theoretically occur at any lay-
er, e.g., pool5 in VGGNet. Secondly, the features output
by each extractor, i.e., coarse features and ﬁne features, are
sent through the Fusion module to form a uniﬁed represen-
tation, i.e., fused features. The advanced Compact Bilinear
Pooling [7] is chosen as the fusion method. Finally, the
Fused Classiﬁer plays as the key component to aggregate
two-level features for the ultimate recognition. Actually the
Fused Classiﬁer can handle either coarse-grained or ﬁne-
grained categorization, and the latter one which is more
challenging is evaluated in our experiments. The training
strategy of HybridNet will be elaborated in Section 4.2.1.

The design of HybridNet comes from the following phi-
losophy. Since DCNN is trained in a top-down manner, the
coarse features and ﬁne features tend to deal with different
aspects of the objects, with the condition of being fed with
the coarse label and ﬁne label separately. After the Fusion,
the fused features has synthesized the hybrid-granularity in-
formation, so it is expected to be richer and more accurate
than the ﬁne features, thus resulting in higher accuracy for
FGVC. From another perspective, the optimization of Hy-
bridNet is comprised of three tasks, i.e.,

* Categorizing sup-classes according to the coarse fea-

tures in the Coarse Classiﬁer

545

* Categorizing sub-classes according to the ﬁne features

in the Fine Classiﬁer

* Categorizing sub-classes according to the fused fea-

tures in the Fused Classiﬁer.

Such multi-task optimization is beneﬁcial to learn more dis-
criminative image representation [22, 37, 34].

In the existing literatures, there are considerable interest-
s to enhance DCNN with greater capacity for FGVC, e.g.,
leveraging parts of objects [35, 30, 9, 36], embedding dis-
tance metric learning [24, 29, 31, 37]. Since HybridNet is
motivated by exploiting the label hierarchy, here we only
focus on the related works [30, 37, 32] that make use of la-
bel hierarchy for FGVC and clarify their differences with
our work. In [30], the features of multiple granularity are
concatenated before the linear SVM, whose training is ind-
ependent from DCNN. While in HybridNet, the Compact
Bilinear Pooling are adopted as fusion method and the mod-
In [37], the label
el is trained in an end-to-end manner.
hierarchy is used to construct the input triplets for jointly
optimizing both classiﬁcation and similarity constraints. In
our opinions, the hybrid-granularity information is not ful-
ly utilized in this way. Our HybridNet shares similar ideas
with [32]. However, in [32], the training set is augmented
by the external data annotated with hyper-classes, while the
images in original dataset are still only with ﬁne-grained la-
bels. In contrast, HybridNet is applied to the input images
that are annotated with hierarchical labels, e.g., VegFru, and
the multiple granularity features are separately learned and
fused through explicit operation.

Furthermore, the architecture of HybridNet is intuitive-
ly similar to the Bilinear CNN in [17], where the Bilinear
Pooling is ﬁrst proposed to aggregate the two-stream DCNN
features for FGVC. Actually they differ from each other in
three aspects. Firstly and most importantly, in [17], the two-
stream DCNNs both deal with ﬁne-grained categorization
and much efforts are taken to break the symmetry of two
networks. But in HybridNet, the two DCNNs are natural-
ly asymmetric and complementary since they are fed with
the coarse-grained and ﬁne-grained labels separately. Sec-
ondly, the network architectures are actually dissimilar. The
Bilinear CNN is eventually implemented by a single DCNN
due to weight sharing, while HybridNet holds two DCNNs
which do not share weights. Thirdly, the training process
is quite different. Compared to single-task optimization of
the Bilinear CNN, the training of HybridNet is made up of
multiple tasks. In practice, we adopt the Compact Bilinear
Pooling [7] as fusion method, which inherits the discrimina-
tive power of the Bilinear Pooling and meanwhile reduces
the computation cost. The model in [7] is denoted as CBP-
CNN in the following.

Table 4. Baselines on VegFru. The typical CaffeNet, VGGNet
and GoogLeNet are chosen to set benchmarks on VegFru. All re-
sults are evaluated on the test set and reported in the top-1 mean
accuracy.

Dataset

Category

CaffeNet

VGGNet

GoogLeNet

Veg200

Fru92

VegFru

15 sup-classes
200 sub-classes

10 sup-classes
92 sub-classes

25 sup-classes
292 sub-classes

74.92%
67.21%

79.86%
71.60%

72.87%
66.40%

83.81%
78.50%

86.81%
79.80%

82.45%
77.12%

83.50%
80.17%

87.54%
81.79%

82.52%
79.22%

4. Experiment

In the experiments, we ﬁrst set benchmarks on Veg-
Fru, and then compare HybridNet with the correspond-
ing baselines on VegFru, FGVC-Aircraft [18] and CUB-
200-2011 [28]. All the networks are implemented with
Caffe [10].

4.1. VegFru Baselines

4.1.1 Experimental Setup

The choice of features is usually treated as the most im-
portant design in image recognition, and so far DCNN is
considered to be the most competitive method for feature
extraction. To comprehensively evaluate VegFru, therefore,
we adopt the representative DCNN architectures including
CaffeNet [14], VGGNet [26], and GoogLeNet [27] (avail-
able in the Caffe Model Zoo [1]) to set benchmarks.

All the networks are pretrained on ImageNet and then
ﬁnetuned on VegFru. The images are randomly ﬂipped be-
fore passing into the networks and no other data augmen-
tation is used. The base learning rate is set to 0.001 and
reduced by a factor of 10 when the loss plateaus. The test is
done with one center crop of the input images. Finally the
top-1 mean accuracy is taken to measure the classiﬁcation
performance. It is worth mentioning that the dataset split
way follows the description in Section 2.2.3. The train set
is used for the ﬁnetuning and the evaluation is performed on
the test set. The val set is taken for error analysis here6.

4.1.2 Quantitative Results

The experiments are carried on sup-classes and sub-classes
of VegFru as well as its subsets, i.e., Veg200 and Fru92, and
the quantitative results are shown in Table 4. The three net-
works all achieve reasonable performance for the task of im-
age classiﬁcation, which validates the reliability of VegFru.
However, even the best top-1 accuracy with GoogLeNet

6The top-1 mean accuracy on val set with CaffeNet, VGGNet, and

GoogLeNet is provided in the supplementary material.

546

)

%

(
 
c
c
A
 
1
-
p
o
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0

80

60

40

20

100
Veg200 sub-classes
Figure 10. The top-1 accuracy of GoogLeNet on each sub-
classes of Veg200. The results are evaluated on the val set and
the lowest accuracy lies in the 67th sub-class, i.e., dandelion.

200

180

120

140

160

Figure 11. Sample images of dandelion

Figure 12. Left: shepherd’s purse; Right: prickly lettuce.

(79.22% on sub-classes of VegFru) is still not satisfying e-
nough for real-world applications, indicating that it is still
vital and necessary to develop more advanced models for
the recognition.

4.1.3 Error Analysis

Along with reporting the top-1 mean accuracy, we also ana-
lyze the classiﬁcation performance on each sub-class. Here
GoogLeNet is taken to illustrate the proof and evaluated on
Veg200. The val set is chosen for the evaluation since it has
equal number of images for each sub-classes. The analy-
sis results are shown in Figure 10, and the lowest accuracy
(46%) lies in the sub-class of dandelion (Figure 11). We
further look into the result and ﬁnd that lots of misclassiﬁed
images are predicted to be shepherd’s purse and prickly let-
tuce (Figure 12). It can be seen that the images in Figure 11
and Figure 12 are of subtle difference, and thus more robust
image representation is required to discriminate them.

4.2. HybridNet Performance

4.2.1

Implementation Details

The DCNN in HybridNet can be any existing model, e.g.,
CaffeNet, VGGNet or GoogLeNet. Here the 16-layer VG-
GNet [26] is selected to construct the HybridNet as in CBP-
CNN [7]. Speciﬁcally, the feature extractor of Hybrid-
Net is comprised of the layers of VGGNet before pool5.
And the image classiﬁer includes the layers of compact bi-
linear pooling, signed square-root, l2-normalization, fully-
connection and softmax.

The Coarse Network and Fine Network in HybridNet
are the variants of CBP-CNN. So before introducing the
training of HybridNet, we ﬁrst review the training pro-
cess of CBP-CNN, which consists of two stages denoted
as ft last layer and ft all [2]. Speciﬁcally, ft last layer is
used to train the layers after the Compact Bilinear Pooling
starting with a high learning rate (e.g., 1), and ft all means
global ﬁnetuning with a relatively low learning rate (e.g.,
0.001). The training strategy of HybridNet is illustrated
in Figure 13. Firstly, the Coarse Network and Fine Net-
work are trained in parallel by feeding the coarse label and
ﬁne label separately (each including ft last layer and ft all
shown in Figure 13(a)(b)). Secondly, the Fused Classiﬁer
is optimized based on the fused features with the rest ﬁxed
(Figure 13(c)). Finally, the whole network is globally ﬁne-
tuned (Figure 13(d)). The Coarse Classiﬁer and Fine Clas-
siﬁer are removed in the global ﬁnetuning (Figure 13(d)),
since the jointly ﬁnetuning strategy does not help in this
case, which will be further discussed in Section 4.2.3. The
detailed parameters for the training and test is released with
the dataset and code.

4.2.2 Performance Comparison

The baseline of HybridNet is set by replacing the coarse
label with the ﬁne label in Figure 9. In that condition, the
two DCNNs with the same architectures are symmetrical-
ly initialized and remain symmetric after ﬁnetuning since
the gradients for two networks are identical [17]. Thus the
model can be implemented with just a singe DCNN. So it is
natural to treat CBP-CNN as the baseline of HybridNet.

The performance comparison for HybridNet on VegFru,
FGVC-Aircraft and CUB-200-2011 is shown in Table 5,
where the results are reported in the top-1 mean accuracy.
For HybridNet, the output of Fused Classiﬁer is taken for
the evaluation. As far as we know, CBP-CNN is the exist-
ing state-of-the-art method for FGVC without utilizing part-
s or external data. Our HybridNet outperforms CBP-CNN
by more than 1.3% on VegFru and FGVC-Aircraft, which
both contain hierarchical labels. For CUB-200-2011, extra
efforts are ﬁrst taken to construct the label hierarchy ac-
cording to the taxonomy in North American Birds [3], and

547

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

coarse label
Input 
Images

fine label
f
Input 
Images

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

(a) ft_last_layer of Coarse/Fine Classifier

(b) ft_all of Coarse/Fine Classifier

5. Conclusion

share some shallow layers as the model in [32], which has
the potential to reduce the GPU memory consumption.

coarse label
Input 
Images

fine label
f
Input 
Images

fine label
Input 
Images

Coarse
Extractor

Fine
Extractor

Fused
Classifier

fine label
Input 
Images

Fused
Classifier

Coarse
Extractor

Fine
Extractor

(c) ft_last_layer of Fused_Classifier

(d) ft_all of Fused_Classifier

Figure 13. Training strategy of HybridNet. Inspired by the train-
ing of CBP-CNN, the training of the Fused Classiﬁer is also di-
vided into two stages following the same denotations ((c)(d)). The
Fusion moduels in (c)(d) are omitted for simplicity. In each stage,
only the components surrounded by the red rectangle are ﬁnetuned
with the rest ﬁxed. Best viewed electronically.

Table 5. Performance comparison for HybridNet. To keep the
experiments consistent, HybridNet is trained on the train set of
VegFru. And it is trained on the trainval set of FGVC-Aircraft [18]
and train set of CUB-200-2011 [28]. Finally, all results are evalu-
ated on the test set and reported in the top-1 mean accuracy.

Dataset

VGGNet [26]
CBP-CNN [7]
HybridNet (ours)

VegFru
(292 sub-classes)
77.12%
82.21%
83.51%

Aircrafts [18]
(100 sub-classes)
84.46%
87.49%
88.84%

CUB [28]
(200 sub-classes)
72.32%
84.91%
85.78%

then HybridNet is applied to obtain 85.78% on this dataset
which is higher than that with CBP-CNN (84.91%). The
improvement on CUB-200-2011 is less signiﬁcant, which
is probably due to the small size of training set. The experi-
mental results indicate that the label hierarchy does help for
FGVC7.

4.2.3 Discussion

In the global ﬁnetuning of HybridNet (Figure 13 (d)), we
have tried to add the Coarse Classiﬁer and Fine Classiﬁ-
er as regulations, for the sake of making the features sepa-
rately learned by each extractor discriminative alone in the
training process [15, 5]. However, our preliminary experi-
ments indicate that this jointly ﬁnetuning strategy does not
suit for this case and instead brings performance degrada-
tion, which is probably caused by the complexity of the
Compact Bilinear Pooling for optimizing. Actually, it has
been proved in [34] that the jointly ﬁnetuning strategy does
not always work. The training strategy of HybridNet is e-
quivalent to the iterative switchable learning scheme adopt-
ed in [34], i.e., multiple tasks are optimized by turns (Fig-
ure 13 (a)-(d)). Besides, there still exists room to improve
HybridNet, e.g., the Coarse Network and Fine Network can

In this work, we construct a domain-speciﬁc dataset,
namely VegFru, in the ﬁeld of FGVC. The novelty of Veg-
Fru is that it aims at domestic cooking and food manage-
ment, and categorizes vegetables and fruits according to
their eating characteristics. In VegFru, there are at least 200
images for each subordinate class with hierarchical label-
s, and each image contains at least one edible part of veg-
etables or fruits with the same cooking usage. It is close-
ly associated with the daily life of everyone and has broad
application prospects. Besides, HybridNet is proposed ac-
companying the dataset to exploit the label hierarchy for
FGVC. In HybridNet, multiple granularity features are ﬁrst
separately learned and then fused through explicit opera-
tion, i.e., Compact Bilinear Pooling, to form a uniﬁed image
representation for overall classiﬁcation. The results on Veg-
Fru, FGVC-Aircraft and CUB-200-2011 demonstrate that
HybridNet achieves one of the top performance on these
datasets. We believe that our VegFru and HybridNet would
inspire more advanced research on FGVC.

Acknowledgment

This work is supported partially by the National Natu-
ral Science Foundation of China under Grant 61673362 and
61233003, Youth Innovation Promotion Association CAS,
and the Fundamental Research Funds for the Central Uni-
versities. Many thanks to Dequan Wang for offering the
label hierarchy of CUB-200-2011. And we are grateful for
the generous donation of Tesla GPU K40 from the NVIDIA
corporation.

References

[1] https://github.com/BVLC/caffe/wiki/

Model-Zoo. 6

[2] https://github.com/gy20073/compact_

bilinear_pooling. 7

[3] https://birdsna.org/Species-Account/bna/

home. 7

[4] M. Chan, D. Est`eve, C. Escriba, and E. Campo. A review of
smart homes - present state and future challenges. Computer
Methods and Programs in Biomedicine, 91:55–81, 2008. 2

[5] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In CVPR, 2016. 1, 8

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2, 3, 4

548

7We also provide the results of evaluating HybridNet on the coarse-

[7] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact

grained categorization in the supplementary material.

bilinear pooling. In CVPR, 2016. 2, 5, 6, 7, 8

[28] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 1, 2, 5, 6, 8

[29] C. Wah, G. V. Horn, S. Branson, S. Maji, P. Perona, and S. J.
Belongie. Similarity comparisons for interactive ﬁne-grained
categorization. In CVPR, 2014. 6

[30] D. Wang, Z. Shen, J. Shao, W. Zhang, X. Xue, and Z. Zhang.
Multiple granularity descriptors for ﬁne-grained categoriza-
tion. In ICCV, 2015. 2, 5, 6

[31] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In CVPR, 2014. 6

[32] S. Xie, T. Yang, X. Wang, and Y. Lin. Hyper-class aug-
mented and regularized deep learning for ﬁne-grained image
classiﬁcation. In CVPR, 2015. 2, 6, 8

[33] Z. Yan, H. Zhang, R. Piramuthu, V. Jagadeesh, D. DeCoste,
W. Di, and Y. Yu. Hd-cnn: Hierarchical deep convolutional
neural networks for large scale visual recognition. In ICCV,
2015. 2

[34] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
counting via deep convolutional neural networks. In CVPR,
2015. 6, 8

[35] N. Zhang, J. Donahue, R. B. Girshick, and T. Darrell. Part-
based r-cnns for ﬁne-grained category detection. In ECCV,
2014. 5, 6

[36] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian. Picking
deep ﬁlter responses for ﬁne-grained image recognition. In
CVPR, 2016. 1, 6

[37] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
structures for ﬁne-grained feature representation. In CVPR,
2016. 1, 2, 6

[38] Z. Zheng, S. Zhang, and Z. Zhang. Fruit Cultivation (In
Chinese). Beijing: Press of Agricultural Science and Tech-
nology of China, 2011. 2, 4

[39] F. Zhou and Y. Lin. Fine-grained image classiﬁcation by

exploring bipartite-graph labels. In CVPR, 2016. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 1

[9] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for
ﬁne-grained visual categorization. In CVPR, 2016. 1, 6
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint arX-
iv:1408.5093, 2014. 6

[11] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In CVPR Workshop, 2011. 1, 2, 5

[12] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In CVPR, 2015. 5
[13] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shop, 2013. 1, 2, 5

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[15] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 8

[16] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 3

[17] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In ICCV, 2015. 5, 6,
7

[18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedal-
di. Fine-grained visual classiﬁcation of aircraft. Technical
report, 2013. 1, 2, 5, 6, 8

[19] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 38(11):39–41, 1995. 4

[20] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing, 2008. 5

[21] Chinese Academy of Agricultural Sciences. Vegetable Cul-
tivation (Second Edition, In Chinese). Beijing: China Agri-
culture Press, 2010. 2, 4

[22] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6

[23] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.

Cats and dogs. In CVPR, 2012. 5

[24] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual cat-
egorization via multi-stage metric learning. In CVPR, 2015.
6

[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7, 8

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6

549

VegFru: A Domain-Speciﬁc Dataset for Fine-grained Visual Categorization

Saihui Hou, Yushan Feng and Zilei Wang
Department of Automation, University of Science and Technology of China
{saihui, fyushan}@mail.ustc.edu.cn, zlwang@ustc.edu.cn

Abstract

In this paper, we propose a novel domain-speciﬁc data-
set named VegFru for ﬁne-grained visual categorization
(FGVC). While the existing datasets for FGVC are mainly
focused on animal breeds or man-made objects with limit-
ed labelled data, VegFru is a larger dataset consisting of
vegetables and fruits which are closely associated with the
daily life of everyone. Aiming at domestic cooking and food
management, VegFru categorizes vegetables and fruits ac-
cording to their eating characteristics, and each image con-
tains at least one edible part of vegetables or fruits with the
same cooking usage. Particularly, all the images are la-
belled hierarchically. The current version covers vegetables
and fruits of 25 upper-level categories and 292 subordinate
classes. And it contains more than 160,000 images in total
and at least 200 images for each subordinate class. Accom-
panying the dataset, we also propose an effective framework
called HybridNet to exploit the label hierarchy for FGVC.
Speciﬁcally, multiple granularity features are ﬁrst extracted
by dealing with the hierarchical labels separately. And then
they are fused through explicit operation, e.g., Compact Bi-
linear Pooling, to form a uniﬁed representation for the ul-
timate recognition. The experimental results on the novel
VegFru, the public FGVC-Aircraft and CUB-200-2011 in-
dicate that HybridNet achieves one of the top performance
on these datasets. The dataset and code are available at
https://github.com/ustc-vim/vegfru.

1. Introduction

In computer vision, ﬁne-grained visual categorization
(FGVC) refers to categorizing objects into subordinate
classes, e.g., breeds of birds or dogs. Compared to gener-
ic classiﬁcation [6], FGVC needs to handle more subtle
inter-class difference and larger intra-class variation of ob-
jects, thus requiring more discriminative and robust im-
age representation. Recent years have witnessed the res-
urrection of deep convolutional neural network (DCNN),
which holds state-of-the-art performance of various vi-
sual tasks [8, 25, 5]. The top-performing methods for

Figure 1. Sample images in VegFru. Top: vegetable images. Bot-
tom: fruit images. Best viewed electronically.

FGVC [9, 36, 37] are also built upon DCNN and the training
is data-hungry. However, the data with ﬁne-grained labels
is usually insufﬁcient, e.g., in CUB-200-2011 [28] there are
only about 30 training images for each class. And the ex-
isting datasets for FGVC are mainly focused on domains of
animal breeds, e.g., birds [28] and dogs [11], or man-made
objects, e.g., cars [13] and aircrafts [18]. As the saying
goes, hunger breeds discontent. In modern life, increasing
attention has been paid to how to go on a balanced and nu-
tritious diet. However, to the best of our knowledge, there is
still no public dataset specially designed for recognizing the
raw food materials and recommending appropriate recipes
for individuals.

In this work, aiming at domestic cooking and food
management, we introduce a novel domain-speciﬁc data-
set named VegFru, which consists of vegetables and fruits
that are closely associated with people’s diet. In VegFru,
vegetables and fruits are categorized according to their eat-
ing characteristics, e.g., different edible parts of a certain
vegetable or fruit, such as leaf and root, are classiﬁed in-
to separate subordinate classes. And the objects in each
image are the raw food materials, while the images that
contain cooked food whose raw materials are indistinguish-
able are ﬁltered out. Currently, the dataset covers vegeta-
bles and fruits of 25 upper-level categories and 292 subor-
dinate classes1, which has taken in all species in common.
And it contains more than 160,000 images in total and at

1The upper-level category and subordinate class are respectively denot-

ed as sup-class and sub-class in the following.

541

(a)           

(b)          

(c)            

(d)

Figure 2. (a) chive (b) shallot (c) leek (d) green Chinese onion.
These images belong to different sub-classes but with subtle inter-
class difference.

(a)           

(b)          

(c)            

(d)

Figure 3. (a) lotus root (b) lotus root (c) lotus root (d) lotus root.
These images belong to the same sub-classes but with large intra-
class variation.

least 200 images for each sub-class, which is much larger
than the previous ﬁne-grained datasets [11, 28, 13, 18]. Par-
ticularly, besides the ﬁne-grained annotation, the images in
VegFru are assigned hierarchical labels. And compared to
the vegetable and fruit subsets of ImageNet [6], the taxono-
my adopted by VegFru is more popular for domestic cook-
ing and food management, and the image collection strictly
serves this purpose, making each image in VegFru contain
at least one edible part of vegetables or fruits with the same
cooking usage. Some sample images are shown in Figure 1.
Our VegFru has the potential to be applied to the follow-

ing aspects, but is not limited to them:

* Fine-grained Visual Categorization (FGVC). The sub-
classes in VegFru all belong to vegetables or fruits, and
there exist subtle inter-class difference (Figure 2) and
large intra-class variation (Figure 3). So it can be con-
sidered as a ﬁne-grained dataset of novel domain, with
more images available for each sub-class.

* Hybrid-granularity methods for FGVC. The label hi-
erarchy has proved to be helpful for image recogni-
tion [33, 30, 32, 37]. With all images labelled hier-
archically, VegFru is naturally well-suited for research
on exploiting the hybrid-granularity information, i.e.,
label hierarchy, for the challenging FGVC.

* Practical applications for domestic cooking and food
management. VegFru collects enormous vegetable and
fruit images of raw food materials and categorizes
them by the eating characteristics. It is closely related
to the daily life of everyone, and thus can promote the
applications of computer vision in the Smart Home [4],
e.g., personalized recipe recommendation.

To verify the application value of VegFru, we also pro-
pose an effective framework named HybridNet with the aim

of utilizing the hybrid-granularity information for FGVC,
which aspect VegFru is well-suited for due to the label hi-
erarchy. We take DCNN model to deal with the issue. In
practice, DCNN is trained in a top-down manner, i.e., the
training is driven by the loss generated at the highest lay-
er according to back propagation. When categorizing the
coarse-grained sup-classes, the network only needs to han-
dle generic attributes (e.g., bird outline), but subtle charac-
teristics (e.g., bird eye, foot) become necessary when dis-
tinguishing the ﬁne-grained sub-classes. Our HybridNet
is exactly motivated by exploiting the complementarity be-
tween the coarse-grained and ﬁne-grained features. Specif-
ically, two-stream DCNNs are ﬁrst trained by feeding the
sup-class and sub-class labels separately, whose features
are then fused to form a uniﬁed representation for the ul-
timate recognition. As for the fusion method, we adopt
the advanced Compact Bilinear Pooling proposed by [7],
in which the model is currently the best for FGVC without
utilizing parts or external data. The experimental results on
VegFru, FGVC-Aircraft [18] and CUB-200-2011 [28] show
the robustness and superiority of HybridNet.

In summary, the main contributions of this work lie in
two folds: VegFru and HybridNet. Speciﬁcally, the contri-
bution of VegFru is highlighted in four aspects: novel do-
main, large scale, label hierarchy and application prospects.
And HybridNet outperforms the model in [7] and achieves
one of the top performance on the three datasets by exploit-
ing the label hierarchy.

The rest of the paper is organized as follows. In Sec-
tion 2, we introduce the construction of VegFru and perform
detailed comparison of VegFru with the vegetable and fruit
subsets of ImageNet and the existing ﬁne-grained datasets.
HybridNet and its related works are presented in Section 3.
In Section 4, we set baselines on VegFru and experimentally
evaluate the proposed HybridNet. Finally, the whole work
is concluded in Section 5.

2. VegFru

2.1. Overview

We build the hierarchical structure of VegFru in accor-
dance with the ofﬁcial literatures [21, 38]. Speciﬁcally, the
vegetable hierarchy is constructed according to the Agricul-
tural Biological Taxonomy described in [21]2, which is the
most reasonable for the cooking purpose and arranges veg-
etables into root vegetable, cabbage, leafy vegetable, etc.
Consequently, we obtain 15 sup-classes vegetables with
200 sub-classes. For fruits, similarly, we adopt the Horti-
cultural Taxonomy in [38] to organize fruits into 10 sup-

2In fact, three are three taxonomies for vegetables in [21]. Besides
the Agricultural Biological Taxonomy, the Botanical Taxonomy divides
vegetables into two categories, namely monocotyledon and dicotyledon,
and the Edible Organ Taxonomy groups vegetables into ﬁve categories,
i.e., root, stem, leaf, ﬂower, and fruit.

542

Table 1. The structure of VegFru. #Sub-the number of sub-
classes included. Perennial∗-Perennial-miscellaneous vegetable.
Persimmons∗-Persimmons-jujubes fruit.

Sup-class
Aquatic vegetable
Brassia oleracea
Bud seedling
Green-leafy vegetable
Perennial∗
Tuber vegetable
Wild vegetable

Root vegetable

Total

Berry fruit
Citrus fruit
Persimmons∗
Pome

Collective fruit

Cucurbites

Total

#Sub
13
9
4
31
13
10
32

Sup-class
Alliaceous
Beans
Cabbage
Eggplant
Melon
Mushroom
Mustard

#Sub
10
15
5
7
14
24
2
11 (beetroot, black salsify,
burdock root, carrot, celeriac,
green radish, kohlrabi, parsnip,
red radish, wasabi, white radish)
15 sup-classes and
200 sub-classes for Veg200

22
13
6
11

Drupe
Litchies
Nut fruit
Other fruit

13
3
11
2
5 (breadfruit, pineapple,
sweetsop, annona muricata,
artocarpus heterophyllus)
6 (golden melon, muskmelon,
honey dew melon, papaya,
netted melon, Hami melon)
10 sup-classes and
92 sub-classes for Fru92

classes and 92 sub-classes. In the current version, there are
91,117 images for vegetables and 69,614 images for fruits.
The number of images for each sub-class varies from 200
to 2000.

VegFru can be naturally divided into two subsets, i.e.,
Veg200 for vegetables and Fru92 for fruits. Table 1 shows
the structure of VegFru, where the sup-classes of Veg200
and Fru92 are listed along with the corresponding number
of sub-classes included. And the sub-classes of Root veg-
etable, Collective fruit and Cucurbites are also listed3.

2.2. VegFru Details

This section presents the details of VegFru. Speciﬁcally,
we will respectively introduce the principles for building
VegFru, process of collecting images, and dataset splits for
training and test.

2.2.1 Building Principles

Aiming at domestic cooking and food management, VegFru
is constructed according to the following principles.

3The detailed sub-classes for each sup-class are provided in the sup-

plementary material.

(a)                        (b)                           

(c)                        (d)

Figure 4. (a) soybean (b) soybean (c) soybean seed (d) soybean
seed. Although (a)-(d) all belong to the seeds of soybean (in d-
ifferent growth periods), they are cooked in disparate ways, thus
being classiﬁed into separate sub-classes.

(a)                        (b)                          (c)                        (d)(cid:3)
Figure 5. (a) potato (b) watermelon (c) cucumber (d) pimen-
to. These images are dropped because the raw food materials are
almost indistinguishable.

* The objects in the images of each sub-class have the

same cooking usage. (Figure 3)

* Each image contains at least one edible part of vegeta-

bles or fruits. (Figure 1)

* The images that contain different edible parts of a cer-
tain vegetable or fruit, e.g., leaf, ﬂower, stem, root, are
classiﬁed into separate sub-classes.

* Even for the images that contain the same edible part
of given vegetable or fruit, if the objects are different
in cooking, we also classify them into different sub-
classes. (Figure 4)

* The objects in each image should be the raw food ma-
terials. If the raw materials of cooked food can not be
made out, the images will be removed. (Figure 5)

2.2.2 Collecting Images

The above principles guide the construction of VegFru, as
well as the process of image collection, which is a really
challenging project.

The ﬁrst step is to collect candidate images for each sub-
class. The images are obtained by searching on the Inter-
net, which is widely used to generate ImageNet [6] and
Microsoft COCO [16]. The sources include Google, Im-
ageNet, Flicker, Bing, Baidu, and so on. The retrieval key-
words are the synonym sets of sub-class names in both Chi-
nese and English. As a consequence, a large number of can-
didate images are collected. Speciﬁcally, over 800 images
are gathered for each sub-class.

Then, to make the dataset highly reliable, the candidate
images are further carefully processed through manual se-
lection. In practice, the images of each sub-class are ﬁltered

543

Figure 6. Sample images of hyacinth bean. Left: Two images
without edible part in ImageNet; Right: Two images with edible
part in VegFru.

(a)

(b)

(c)
Figure 7. Potatoes in the vegetable subsets of ImageNet. Left
To Right: (a) Baked potato (b) French fries (c) Home fries (d)
Mashed potato (e) Uruguay potato. Only the images of Uruguay
potato contain the raw food materials.

(d)

(e)

by ten people on the basis of the class description provided
in [21, 38] and the true positives, following the principles
described in Section 2.2.1. Only images afﬁrmed by more
than eight are reserved. The faded, binary, blurry and dupli-
cated ones are all ﬁltered out.

So far we have completed the construction of 25 sup-
classes and 292 sub-classes, with more than 160, 000 im-
ages totally. Figure 1 displays some sample images collect-
ed by VegFru4.

2.2.3 Dataset Splits

In VegFru, each sub-class contains at least 200 images,
which are divided into training, validation and test set (de-
noted as train, val and test set in the following). An alterna-
tive split way is to ﬁrst arrange the images in each sub-class
randomly. Then the top 100 are selected for train, the fol-
lowing 50 for val, and the rest for test. Finally, a slight
adjustment is applied to the split to ensure that each set is
representative for the variability such as object numbers and
background. The image list for each set is released attached
in the dataset.

2.3. VegFru vs. ImageNet subsets

In this section, we compare VegFru with the vegetable
and fruit subsets of ImageNet from three aspects, i.e., tax-
onomy, image selection and dataset structure. Through the
comparison, the construction and usage of VegFru are fur-
ther motivated.
Taxonomy.

ImageNet [6] constructs its hierarchical
structure based on WordNet [19], which organizes all the

4More sample images are provided in the supplementary material.

Table 2. VegFru vs. ImageNet subsets on dataset structure.
#Sup-the number of sup-classes. #Sub-the number of sub-classes.
Min/Max-the minimum/maximum number of images in each sub-
class. #Sub<200-the number of sub-classes that consist of less
than 200 images.

#Sup

#Sub Min

Max

Vegetables in ImageNet
Vegetables in VegFru
Fruits in ImageNet
Fruits in VegFru

25
15
75
10

175
200
196
92

3
202
0
202

1500+
1807
1500+
1615

#Sub
<200
27
0
50
0

words according to the semantics. For vegetables and fruit-
s, however, we tend to concentrate more on their eating
characteristics in daily life. Actually the taxonomy adopt-
ed by ImageNet for vegetables and fruits is quite unpopular
for domestic cooking and food management, and even con-
tains many repeated categories. For example, turnip and
radish simultaneously belong to root vegetable and crucif-
erous vegetable, and potato is grouped into root vegetable
In fact,
but is also on the list of solanaceous vegetable.
according to [21], potato should be categorized into tuber
vegetable. Moreover, some vegetables which are common
in diet are not included in ImageNet, e.g., water spinach,
shepherd’s purse, basella rubra.

By contrast, in the construction of VegFru, we remove
the rare categories, e.g. woad and ottelia, while many reg-
ular categories are added, e.g., Chinese pumpkin and sug-
arcane. And some categories are grouped into ﬁner class-
es, e.g., radish are divided into white radish, red radish and
green radish. The taxonomy adopted by VegFru specially
serves the purpose of domestic cooking and food manage-
ment in daily life.

Image Selection. All images in VegFru contain the edi-
ble part of a certain vegetable or fruit, which is not included
in lots of images in ImageNet, as shown in Figure 6. Be-
sides, some categories in ImageNet do not cover any raw
food materials. For example, Figure 7 displays the images
of ﬁve potato subordinate classes in the vegetable subsets
of ImageNet, i.e., baked potato, French fries, home fries,
mashed potato and Uruguay potato. Only Uruguay potato
belongs to the raw food materials.

Dataset Structure. Table 2 shows some statistics of
VegFru and the ImageNet subsets. In particular, there are 50
fruit sub-classes and 27 vegetable sub-classes whose num-
ber of images is less than 200 in ImageNet, while VegFru
is comprised of 292 popular sub-classes of vegetables and
fruits with more than 200 images for each sub-classes. And
the taxonomy tree, i.e., the distribution of sup-classes and
sub-classes, is reasonably reorganized for vegetables and
fruits in VegFru.

544

Figure 8. Sample images in FGVC-Aircraft. The airplanes oc-
cupy a large fraction of the whole images. And there exists only
one airplane in each image with relatively clean background.

3. HybridNet

Table 3. VegFru vs. Fine-grained Datasets. #Sup-the number of
sup-classes. #Sub-the number of sub-classes. #Image-the number
of images in total. #Train/#Val/#Test-the number of images in
train/val/test set. #Train+Val(avg)-the average number of images
in each sub-classes for model training (include train and val set).

Dataset

#Sup

#Sub

#Image

#Train

#Val

Birds
Dogs
Cars
Aircrafts

VegFru

none
none
none
70

25

200
120
196
100

292

11788
20580
16185
10000

160731

5994
12000
8144
3334

29200

none
none
none
3333

14600

#Train+
Val(avg)

˜30
100
˜42
˜67

150

#Test

5794
8580
8041
3333

116931

2.4. VegFru vs. Fine grained Datasets

In this section we further compare VegFru with four rep-
resentative ﬁne-gained datasets, i.e., CUB-200-2011 [28]
(Birds), Stanford Dogs [11] (Dogs), Stanford Cars [13]
(Cars) and FGVC-Aircraft [18] (Aircrafts)5, which are
widely used in previous works [35, 12, 17, 7]. The detailed
comparison is shown in Table 3. More ﬁne-grained dataset-
s, such as Oxford Flowers [20] and Pets [23], are not listed
here out of the consideration of simplicity.

Compare to these existing datasets, the domain of Veg-
Fru is novel and more associated with people’s daily life,
which contributes to its broad application prospects. And
VegFru is larger in scale, which has up to 150 images avail-
able in each sub-classes for model training. Particularly,
all the images in VegFru are hierarchically categorized in-
to sup-classes and sub-classes, while the images in these
datasets, except FGVC-Aircraft, are only assigned with
ﬁne-grained labels. So VegFru is well-suited for the hybrid-
granularity research on FGVC. We noticed that the previ-
ous works [30, 39] declared to annotate some ﬁne-grained
datasets, e.g., CUB-200-2011, with extra labels to get the
label hierarchy. However, to the best of our knowledge, the
annotation is not publicly available until the submission,
and the labelling process is labor-intensive. Furthermore,
though FGVC-Aircraft is with hierarchical labels, the ob-
jects of interest, i.e., aircrafts, usually occupy a large frac-
tion of the whole images, and each image only contains one
aircraft with relatively clean background (Figure 8). In con-

5For FGVC-Aircraft, airplane variants are chosen as the labels of sub-
classes, and the 70 sup-classes in Tabel 3 is the number of airplane fami-
lies, which are the upper-level annotaions of airplane variants. Please refer
to [18] for details of this dataset.

Coarse Network 

coarse label

coarse features

Coarse
Extractor

Coarse
Classifier 

Input 
Images

Fusion

fused features

Fused
Classifier

fine label

fine features

Fine
Extractor

Fine
Classifier

Fine Network 

Figure 9. Illustration of the proposed HybridNet. Two-stream
features which deal with the hierarchical labels are ﬁrst extracted
separately, and then sent through the Fusion module to train the
Fused Classiﬁer for overall classiﬁcation.

trast, the images in VegFru are with cluttered background
and vary in number and scale of the objects (Figure 1).

Accompanying the dataset, we also propose an effective
framework called HybridNet to conduct the image classiﬁ-
cation, illustrated in Figure 9. The motivation is to exploit
the label hierarchy for FGVC, which can further verify the
application value of VegFru.

Speciﬁcally, the input images with sup-class and sub-
class labels (denoted as coarse label and ﬁne label in Fig-
ure 9) are ﬁrstly sent into two DCNNs for separate clas-
siﬁcation. Here an end-to-end DCNN is logically divided
into two functional parts, i.e., feature extractor and image
classiﬁer. The division can theoretically occur at any lay-
er, e.g., pool5 in VGGNet. Secondly, the features output
by each extractor, i.e., coarse features and ﬁne features, are
sent through the Fusion module to form a uniﬁed represen-
tation, i.e., fused features. The advanced Compact Bilinear
Pooling [7] is chosen as the fusion method. Finally, the
Fused Classiﬁer plays as the key component to aggregate
two-level features for the ultimate recognition. Actually the
Fused Classiﬁer can handle either coarse-grained or ﬁne-
grained categorization, and the latter one which is more
challenging is evaluated in our experiments. The training
strategy of HybridNet will be elaborated in Section 4.2.1.

The design of HybridNet comes from the following phi-
losophy. Since DCNN is trained in a top-down manner, the
coarse features and ﬁne features tend to deal with different
aspects of the objects, with the condition of being fed with
the coarse label and ﬁne label separately. After the Fusion,
the fused features has synthesized the hybrid-granularity in-
formation, so it is expected to be richer and more accurate
than the ﬁne features, thus resulting in higher accuracy for
FGVC. From another perspective, the optimization of Hy-
bridNet is comprised of three tasks, i.e.,

* Categorizing sup-classes according to the coarse fea-

tures in the Coarse Classiﬁer

545

* Categorizing sub-classes according to the ﬁne features

in the Fine Classiﬁer

* Categorizing sub-classes according to the fused fea-

tures in the Fused Classiﬁer.

Such multi-task optimization is beneﬁcial to learn more dis-
criminative image representation [22, 37, 34].

In the existing literatures, there are considerable interest-
s to enhance DCNN with greater capacity for FGVC, e.g.,
leveraging parts of objects [35, 30, 9, 36], embedding dis-
tance metric learning [24, 29, 31, 37]. Since HybridNet is
motivated by exploiting the label hierarchy, here we only
focus on the related works [30, 37, 32] that make use of la-
bel hierarchy for FGVC and clarify their differences with
our work. In [30], the features of multiple granularity are
concatenated before the linear SVM, whose training is ind-
ependent from DCNN. While in HybridNet, the Compact
Bilinear Pooling are adopted as fusion method and the mod-
In [37], the label
el is trained in an end-to-end manner.
hierarchy is used to construct the input triplets for jointly
optimizing both classiﬁcation and similarity constraints. In
our opinions, the hybrid-granularity information is not ful-
ly utilized in this way. Our HybridNet shares similar ideas
with [32]. However, in [32], the training set is augmented
by the external data annotated with hyper-classes, while the
images in original dataset are still only with ﬁne-grained la-
bels. In contrast, HybridNet is applied to the input images
that are annotated with hierarchical labels, e.g., VegFru, and
the multiple granularity features are separately learned and
fused through explicit operation.

Furthermore, the architecture of HybridNet is intuitive-
ly similar to the Bilinear CNN in [17], where the Bilinear
Pooling is ﬁrst proposed to aggregate the two-stream DCNN
features for FGVC. Actually they differ from each other in
three aspects. Firstly and most importantly, in [17], the two-
stream DCNNs both deal with ﬁne-grained categorization
and much efforts are taken to break the symmetry of two
networks. But in HybridNet, the two DCNNs are natural-
ly asymmetric and complementary since they are fed with
the coarse-grained and ﬁne-grained labels separately. Sec-
ondly, the network architectures are actually dissimilar. The
Bilinear CNN is eventually implemented by a single DCNN
due to weight sharing, while HybridNet holds two DCNNs
which do not share weights. Thirdly, the training process
is quite different. Compared to single-task optimization of
the Bilinear CNN, the training of HybridNet is made up of
multiple tasks. In practice, we adopt the Compact Bilinear
Pooling [7] as fusion method, which inherits the discrimina-
tive power of the Bilinear Pooling and meanwhile reduces
the computation cost. The model in [7] is denoted as CBP-
CNN in the following.

Table 4. Baselines on VegFru. The typical CaffeNet, VGGNet
and GoogLeNet are chosen to set benchmarks on VegFru. All re-
sults are evaluated on the test set and reported in the top-1 mean
accuracy.

Dataset

Category

CaffeNet

VGGNet

GoogLeNet

Veg200

Fru92

VegFru

15 sup-classes
200 sub-classes

10 sup-classes
92 sub-classes

25 sup-classes
292 sub-classes

74.92%
67.21%

79.86%
71.60%

72.87%
66.40%

83.81%
78.50%

86.81%
79.80%

82.45%
77.12%

83.50%
80.17%

87.54%
81.79%

82.52%
79.22%

4. Experiment

In the experiments, we ﬁrst set benchmarks on Veg-
Fru, and then compare HybridNet with the correspond-
ing baselines on VegFru, FGVC-Aircraft [18] and CUB-
200-2011 [28]. All the networks are implemented with
Caffe [10].

4.1. VegFru Baselines

4.1.1 Experimental Setup

The choice of features is usually treated as the most im-
portant design in image recognition, and so far DCNN is
considered to be the most competitive method for feature
extraction. To comprehensively evaluate VegFru, therefore,
we adopt the representative DCNN architectures including
CaffeNet [14], VGGNet [26], and GoogLeNet [27] (avail-
able in the Caffe Model Zoo [1]) to set benchmarks.

All the networks are pretrained on ImageNet and then
ﬁnetuned on VegFru. The images are randomly ﬂipped be-
fore passing into the networks and no other data augmen-
tation is used. The base learning rate is set to 0.001 and
reduced by a factor of 10 when the loss plateaus. The test is
done with one center crop of the input images. Finally the
top-1 mean accuracy is taken to measure the classiﬁcation
performance. It is worth mentioning that the dataset split
way follows the description in Section 2.2.3. The train set
is used for the ﬁnetuning and the evaluation is performed on
the test set. The val set is taken for error analysis here6.

4.1.2 Quantitative Results

The experiments are carried on sup-classes and sub-classes
of VegFru as well as its subsets, i.e., Veg200 and Fru92, and
the quantitative results are shown in Table 4. The three net-
works all achieve reasonable performance for the task of im-
age classiﬁcation, which validates the reliability of VegFru.
However, even the best top-1 accuracy with GoogLeNet

6The top-1 mean accuracy on val set with CaffeNet, VGGNet, and

GoogLeNet is provided in the supplementary material.

546

)

%

(
 
c
c
A
 
1
-
p
o
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0

80

60

40

20

100
Veg200 sub-classes
Figure 10. The top-1 accuracy of GoogLeNet on each sub-
classes of Veg200. The results are evaluated on the val set and
the lowest accuracy lies in the 67th sub-class, i.e., dandelion.

120

140

160

180

200

Figure 11. Sample images of dandelion

Figure 12. Left: shepherd’s purse; Right: prickly lettuce.

(79.22% on sub-classes of VegFru) is still not satisfying e-
nough for real-world applications, indicating that it is still
vital and necessary to develop more advanced models for
the recognition.

4.1.3 Error Analysis

Along with reporting the top-1 mean accuracy, we also ana-
lyze the classiﬁcation performance on each sub-class. Here
GoogLeNet is taken to illustrate the proof and evaluated on
Veg200. The val set is chosen for the evaluation since it has
equal number of images for each sub-classes. The analy-
sis results are shown in Figure 10, and the lowest accuracy
(46%) lies in the sub-class of dandelion (Figure 11). We
further look into the result and ﬁnd that lots of misclassiﬁed
images are predicted to be shepherd’s purse and prickly let-
tuce (Figure 12). It can be seen that the images in Figure 11
and Figure 12 are of subtle difference, and thus more robust
image representation is required to discriminate them.

4.2. HybridNet Performance

4.2.1

Implementation Details

The DCNN in HybridNet can be any existing model, e.g.,
CaffeNet, VGGNet or GoogLeNet. Here the 16-layer VG-
GNet [26] is selected to construct the HybridNet as in CBP-
CNN [7]. Speciﬁcally, the feature extractor of Hybrid-
Net is comprised of the layers of VGGNet before pool5.
And the image classiﬁer includes the layers of compact bi-
linear pooling, signed square-root, l2-normalization, fully-
connection and softmax.

The Coarse Network and Fine Network in HybridNet
are the variants of CBP-CNN. So before introducing the
training of HybridNet, we ﬁrst review the training pro-
cess of CBP-CNN, which consists of two stages denoted
as ft last layer and ft all [2]. Speciﬁcally, ft last layer is
used to train the layers after the Compact Bilinear Pooling
starting with a high learning rate (e.g., 1), and ft all means
global ﬁnetuning with a relatively low learning rate (e.g.,
0.001). The training strategy of HybridNet is illustrated
in Figure 13. Firstly, the Coarse Network and Fine Net-
work are trained in parallel by feeding the coarse label and
ﬁne label separately (each including ft last layer and ft all
shown in Figure 13(a)(b)). Secondly, the Fused Classiﬁer
is optimized based on the fused features with the rest ﬁxed
(Figure 13(c)). Finally, the whole network is globally ﬁne-
tuned (Figure 13(d)). The Coarse Classiﬁer and Fine Clas-
siﬁer are removed in the global ﬁnetuning (Figure 13(d)),
since the jointly ﬁnetuning strategy does not help in this
case, which will be further discussed in Section 4.2.3. The
detailed parameters for the training and test is released with
the dataset and code.

4.2.2 Performance Comparison

The baseline of HybridNet is set by replacing the coarse
label with the ﬁne label in Figure 9. In that condition, the
two DCNNs with the same architectures are symmetrical-
ly initialized and remain symmetric after ﬁnetuning since
the gradients for two networks are identical [17]. Thus the
model can be implemented with just a singe DCNN. So it is
natural to treat CBP-CNN as the baseline of HybridNet.

The performance comparison for HybridNet on VegFru,
FGVC-Aircraft and CUB-200-2011 is shown in Table 5,
where the results are reported in the top-1 mean accuracy.
For HybridNet, the output of Fused Classiﬁer is taken for
the evaluation. As far as we know, CBP-CNN is the exist-
ing state-of-the-art method for FGVC without utilizing part-
s or external data. Our HybridNet outperforms CBP-CNN
by more than 1.3% on VegFru and FGVC-Aircraft, which
both contain hierarchical labels. For CUB-200-2011, extra
efforts are ﬁrst taken to construct the label hierarchy ac-
cording to the taxonomy in North American Birds [3], and

547

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

coarse label
Input 
Images

fine label
f
Input 
Images

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

(a) ft_last_layer of Coarse/Fine Classifier

(b) ft_all of Coarse/Fine Classifier

5. Conclusion

share some shallow layers as the model in [32], which has
the potential to reduce the GPU memory consumption.

coarse label
Input 
Images

fine label
f
Input 
Images

fine label
Input 
Images

Coarse
Extractor

Fine
Extractor

Fused
Classifier

fine label
Input 
Images

Fused
Classifier

Coarse
Extractor

Fine
Extractor

(c) ft_last_layer of Fused_Classifier

(d) ft_all of Fused_Classifier

Figure 13. Training strategy of HybridNet. Inspired by the train-
ing of CBP-CNN, the training of the Fused Classiﬁer is also di-
vided into two stages following the same denotations ((c)(d)). The
Fusion moduels in (c)(d) are omitted for simplicity. In each stage,
only the components surrounded by the red rectangle are ﬁnetuned
with the rest ﬁxed. Best viewed electronically.

Table 5. Performance comparison for HybridNet. To keep the
experiments consistent, HybridNet is trained on the train set of
VegFru. And it is trained on the trainval set of FGVC-Aircraft [18]
and train set of CUB-200-2011 [28]. Finally, all results are evalu-
ated on the test set and reported in the top-1 mean accuracy.

Dataset

VGGNet [26]
CBP-CNN [7]
HybridNet (ours)

VegFru
(292 sub-classes)
77.12%
82.21%
83.51%

Aircrafts [18]
(100 sub-classes)
84.46%
87.49%
88.84%

CUB [28]
(200 sub-classes)
72.32%
84.91%
85.78%

then HybridNet is applied to obtain 85.78% on this dataset
which is higher than that with CBP-CNN (84.91%). The
improvement on CUB-200-2011 is less signiﬁcant, which
is probably due to the small size of training set. The experi-
mental results indicate that the label hierarchy does help for
FGVC7.

4.2.3 Discussion

In the global ﬁnetuning of HybridNet (Figure 13 (d)), we
have tried to add the Coarse Classiﬁer and Fine Classiﬁ-
er as regulations, for the sake of making the features sepa-
rately learned by each extractor discriminative alone in the
training process [15, 5]. However, our preliminary experi-
ments indicate that this jointly ﬁnetuning strategy does not
suit for this case and instead brings performance degrada-
tion, which is probably caused by the complexity of the
Compact Bilinear Pooling for optimizing. Actually, it has
been proved in [34] that the jointly ﬁnetuning strategy does
not always work. The training strategy of HybridNet is e-
quivalent to the iterative switchable learning scheme adopt-
ed in [34], i.e., multiple tasks are optimized by turns (Fig-
ure 13 (a)-(d)). Besides, there still exists room to improve
HybridNet, e.g., the Coarse Network and Fine Network can

In this work, we construct a domain-speciﬁc dataset,
namely VegFru, in the ﬁeld of FGVC. The novelty of Veg-
Fru is that it aims at domestic cooking and food manage-
ment, and categorizes vegetables and fruits according to
their eating characteristics. In VegFru, there are at least 200
images for each subordinate class with hierarchical label-
s, and each image contains at least one edible part of veg-
etables or fruits with the same cooking usage. It is close-
ly associated with the daily life of everyone and has broad
application prospects. Besides, HybridNet is proposed ac-
companying the dataset to exploit the label hierarchy for
FGVC. In HybridNet, multiple granularity features are ﬁrst
separately learned and then fused through explicit opera-
tion, i.e., Compact Bilinear Pooling, to form a uniﬁed image
representation for overall classiﬁcation. The results on Veg-
Fru, FGVC-Aircraft and CUB-200-2011 demonstrate that
HybridNet achieves one of the top performance on these
datasets. We believe that our VegFru and HybridNet would
inspire more advanced research on FGVC.

Acknowledgment

This work is supported partially by the National Natu-
ral Science Foundation of China under Grant 61673362 and
61233003, Youth Innovation Promotion Association CAS,
and the Fundamental Research Funds for the Central Uni-
versities. Many thanks to Dequan Wang for offering the
label hierarchy of CUB-200-2011. And we are grateful for
the generous donation of Tesla GPU K40 from the NVIDIA
corporation.

References

[1] https://github.com/BVLC/caffe/wiki/

Model-Zoo. 6

[2] https://github.com/gy20073/compact_

bilinear_pooling. 7

[3] https://birdsna.org/Species-Account/bna/

home. 7

[4] M. Chan, D. Est`eve, C. Escriba, and E. Campo. A review of
smart homes - present state and future challenges. Computer
Methods and Programs in Biomedicine, 91:55–81, 2008. 2

[5] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In CVPR, 2016. 1, 8

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2, 3, 4

548

7We also provide the results of evaluating HybridNet on the coarse-

[7] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact

grained categorization in the supplementary material.

bilinear pooling. In CVPR, 2016. 2, 5, 6, 7, 8

[28] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 1, 2, 5, 6, 8

[29] C. Wah, G. V. Horn, S. Branson, S. Maji, P. Perona, and S. J.
Belongie. Similarity comparisons for interactive ﬁne-grained
categorization. In CVPR, 2014. 6

[30] D. Wang, Z. Shen, J. Shao, W. Zhang, X. Xue, and Z. Zhang.
Multiple granularity descriptors for ﬁne-grained categoriza-
tion. In ICCV, 2015. 2, 5, 6

[31] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In CVPR, 2014. 6

[32] S. Xie, T. Yang, X. Wang, and Y. Lin. Hyper-class aug-
mented and regularized deep learning for ﬁne-grained image
classiﬁcation. In CVPR, 2015. 2, 6, 8

[33] Z. Yan, H. Zhang, R. Piramuthu, V. Jagadeesh, D. DeCoste,
W. Di, and Y. Yu. Hd-cnn: Hierarchical deep convolutional
neural networks for large scale visual recognition. In ICCV,
2015. 2

[34] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
counting via deep convolutional neural networks. In CVPR,
2015. 6, 8

[35] N. Zhang, J. Donahue, R. B. Girshick, and T. Darrell. Part-
based r-cnns for ﬁne-grained category detection. In ECCV,
2014. 5, 6

[36] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian. Picking
deep ﬁlter responses for ﬁne-grained image recognition. In
CVPR, 2016. 1, 6

[37] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
structures for ﬁne-grained feature representation. In CVPR,
2016. 1, 2, 6

[38] Z. Zheng, S. Zhang, and Z. Zhang. Fruit Cultivation (In
Chinese). Beijing: Press of Agricultural Science and Tech-
nology of China, 2011. 2, 4

[39] F. Zhou and Y. Lin. Fine-grained image classiﬁcation by

exploring bipartite-graph labels. In CVPR, 2016. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 1

[9] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for
ﬁne-grained visual categorization. In CVPR, 2016. 1, 6
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint arX-
iv:1408.5093, 2014. 6

[11] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In CVPR Workshop, 2011. 1, 2, 5

[12] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In CVPR, 2015. 5
[13] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shop, 2013. 1, 2, 5

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[15] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 8

[16] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 3

[17] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In ICCV, 2015. 5, 6,
7

[18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedal-
di. Fine-grained visual classiﬁcation of aircraft. Technical
report, 2013. 1, 2, 5, 6, 8

[19] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 38(11):39–41, 1995. 4

[20] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing, 2008. 5

[21] Chinese Academy of Agricultural Sciences. Vegetable Cul-
tivation (Second Edition, In Chinese). Beijing: China Agri-
culture Press, 2010. 2, 4

[22] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6

[23] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.

Cats and dogs. In CVPR, 2012. 5

[24] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual cat-
egorization via multi-stage metric learning. In CVPR, 2015.
6

[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7, 8

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6

549

VegFru: A Domain-Speciﬁc Dataset for Fine-grained Visual Categorization

Saihui Hou, Yushan Feng and Zilei Wang
Department of Automation, University of Science and Technology of China
{saihui, fyushan}@mail.ustc.edu.cn, zlwang@ustc.edu.cn

Abstract

In this paper, we propose a novel domain-speciﬁc data-
set named VegFru for ﬁne-grained visual categorization
(FGVC). While the existing datasets for FGVC are mainly
focused on animal breeds or man-made objects with limit-
ed labelled data, VegFru is a larger dataset consisting of
vegetables and fruits which are closely associated with the
daily life of everyone. Aiming at domestic cooking and food
management, VegFru categorizes vegetables and fruits ac-
cording to their eating characteristics, and each image con-
tains at least one edible part of vegetables or fruits with the
same cooking usage. Particularly, all the images are la-
belled hierarchically. The current version covers vegetables
and fruits of 25 upper-level categories and 292 subordinate
classes. And it contains more than 160,000 images in total
and at least 200 images for each subordinate class. Accom-
panying the dataset, we also propose an effective framework
called HybridNet to exploit the label hierarchy for FGVC.
Speciﬁcally, multiple granularity features are ﬁrst extracted
by dealing with the hierarchical labels separately. And then
they are fused through explicit operation, e.g., Compact Bi-
linear Pooling, to form a uniﬁed representation for the ul-
timate recognition. The experimental results on the novel
VegFru, the public FGVC-Aircraft and CUB-200-2011 in-
dicate that HybridNet achieves one of the top performance
on these datasets. The dataset and code are available at
https://github.com/ustc-vim/vegfru.

1. Introduction

In computer vision, ﬁne-grained visual categorization
(FGVC) refers to categorizing objects into subordinate
classes, e.g., breeds of birds or dogs. Compared to gener-
ic classiﬁcation [6], FGVC needs to handle more subtle
inter-class difference and larger intra-class variation of ob-
jects, thus requiring more discriminative and robust im-
age representation. Recent years have witnessed the res-
urrection of deep convolutional neural network (DCNN),
which holds state-of-the-art performance of various vi-
sual tasks [8, 25, 5]. The top-performing methods for

Figure 1. Sample images in VegFru. Top: vegetable images. Bot-
tom: fruit images. Best viewed electronically.

FGVC [9, 36, 37] are also built upon DCNN and the training
is data-hungry. However, the data with ﬁne-grained labels
is usually insufﬁcient, e.g., in CUB-200-2011 [28] there are
only about 30 training images for each class. And the ex-
isting datasets for FGVC are mainly focused on domains of
animal breeds, e.g., birds [28] and dogs [11], or man-made
objects, e.g., cars [13] and aircrafts [18]. As the saying
goes, hunger breeds discontent. In modern life, increasing
attention has been paid to how to go on a balanced and nu-
tritious diet. However, to the best of our knowledge, there is
still no public dataset specially designed for recognizing the
raw food materials and recommending appropriate recipes
for individuals.

In this work, aiming at domestic cooking and food
management, we introduce a novel domain-speciﬁc data-
set named VegFru, which consists of vegetables and fruits
that are closely associated with people’s diet. In VegFru,
vegetables and fruits are categorized according to their eat-
ing characteristics, e.g., different edible parts of a certain
vegetable or fruit, such as leaf and root, are classiﬁed in-
to separate subordinate classes. And the objects in each
image are the raw food materials, while the images that
contain cooked food whose raw materials are indistinguish-
able are ﬁltered out. Currently, the dataset covers vegeta-
bles and fruits of 25 upper-level categories and 292 subor-
dinate classes1, which has taken in all species in common.
And it contains more than 160,000 images in total and at

1The upper-level category and subordinate class are respectively denot-

ed as sup-class and sub-class in the following.

541

(a)           

(b)          

(c)            

(d)

Figure 2. (a) chive (b) shallot (c) leek (d) green Chinese onion.
These images belong to different sub-classes but with subtle inter-
class difference.

(a)           

(b)          

(c)            

(d)

Figure 3. (a) lotus root (b) lotus root (c) lotus root (d) lotus root.
These images belong to the same sub-classes but with large intra-
class variation.

least 200 images for each sub-class, which is much larger
than the previous ﬁne-grained datasets [11, 28, 13, 18]. Par-
ticularly, besides the ﬁne-grained annotation, the images in
VegFru are assigned hierarchical labels. And compared to
the vegetable and fruit subsets of ImageNet [6], the taxono-
my adopted by VegFru is more popular for domestic cook-
ing and food management, and the image collection strictly
serves this purpose, making each image in VegFru contain
at least one edible part of vegetables or fruits with the same
cooking usage. Some sample images are shown in Figure 1.
Our VegFru has the potential to be applied to the follow-

ing aspects, but is not limited to them:

* Fine-grained Visual Categorization (FGVC). The sub-
classes in VegFru all belong to vegetables or fruits, and
there exist subtle inter-class difference (Figure 2) and
large intra-class variation (Figure 3). So it can be con-
sidered as a ﬁne-grained dataset of novel domain, with
more images available for each sub-class.

* Hybrid-granularity methods for FGVC. The label hi-
erarchy has proved to be helpful for image recogni-
tion [33, 30, 32, 37]. With all images labelled hier-
archically, VegFru is naturally well-suited for research
on exploiting the hybrid-granularity information, i.e.,
label hierarchy, for the challenging FGVC.

* Practical applications for domestic cooking and food
management. VegFru collects enormous vegetable and
fruit images of raw food materials and categorizes
them by the eating characteristics. It is closely related
to the daily life of everyone, and thus can promote the
applications of computer vision in the Smart Home [4],
e.g., personalized recipe recommendation.

To verify the application value of VegFru, we also pro-
pose an effective framework named HybridNet with the aim

of utilizing the hybrid-granularity information for FGVC,
which aspect VegFru is well-suited for due to the label hi-
erarchy. We take DCNN model to deal with the issue. In
practice, DCNN is trained in a top-down manner, i.e., the
training is driven by the loss generated at the highest lay-
er according to back propagation. When categorizing the
coarse-grained sup-classes, the network only needs to han-
dle generic attributes (e.g., bird outline), but subtle charac-
teristics (e.g., bird eye, foot) become necessary when dis-
tinguishing the ﬁne-grained sub-classes. Our HybridNet
is exactly motivated by exploiting the complementarity be-
tween the coarse-grained and ﬁne-grained features. Specif-
ically, two-stream DCNNs are ﬁrst trained by feeding the
sup-class and sub-class labels separately, whose features
are then fused to form a uniﬁed representation for the ul-
timate recognition. As for the fusion method, we adopt
the advanced Compact Bilinear Pooling proposed by [7],
in which the model is currently the best for FGVC without
utilizing parts or external data. The experimental results on
VegFru, FGVC-Aircraft [18] and CUB-200-2011 [28] show
the robustness and superiority of HybridNet.

In summary, the main contributions of this work lie in
two folds: VegFru and HybridNet. Speciﬁcally, the contri-
bution of VegFru is highlighted in four aspects: novel do-
main, large scale, label hierarchy and application prospects.
And HybridNet outperforms the model in [7] and achieves
one of the top performance on the three datasets by exploit-
ing the label hierarchy.

The rest of the paper is organized as follows. In Sec-
tion 2, we introduce the construction of VegFru and perform
detailed comparison of VegFru with the vegetable and fruit
subsets of ImageNet and the existing ﬁne-grained datasets.
HybridNet and its related works are presented in Section 3.
In Section 4, we set baselines on VegFru and experimentally
evaluate the proposed HybridNet. Finally, the whole work
is concluded in Section 5.

2. VegFru

2.1. Overview

We build the hierarchical structure of VegFru in accor-
dance with the ofﬁcial literatures [21, 38]. Speciﬁcally, the
vegetable hierarchy is constructed according to the Agricul-
tural Biological Taxonomy described in [21]2, which is the
most reasonable for the cooking purpose and arranges veg-
etables into root vegetable, cabbage, leafy vegetable, etc.
Consequently, we obtain 15 sup-classes vegetables with
200 sub-classes. For fruits, similarly, we adopt the Horti-
cultural Taxonomy in [38] to organize fruits into 10 sup-

2In fact, three are three taxonomies for vegetables in [21]. Besides
the Agricultural Biological Taxonomy, the Botanical Taxonomy divides
vegetables into two categories, namely monocotyledon and dicotyledon,
and the Edible Organ Taxonomy groups vegetables into ﬁve categories,
i.e., root, stem, leaf, ﬂower, and fruit.

542

Table 1. The structure of VegFru. #Sub-the number of sub-
classes included. Perennial∗-Perennial-miscellaneous vegetable.
Persimmons∗-Persimmons-jujubes fruit.

Sup-class
Aquatic vegetable
Brassia oleracea
Bud seedling
Green-leafy vegetable
Perennial∗
Tuber vegetable
Wild vegetable

Root vegetable

Total

Berry fruit
Citrus fruit
Persimmons∗
Pome

Collective fruit

Cucurbites

Total

#Sub
13
9
4
31
13
10
32

Sup-class
Alliaceous
Beans
Cabbage
Eggplant
Melon
Mushroom
Mustard

#Sub
10
15
5
7
14
24
2
11 (beetroot, black salsify,
burdock root, carrot, celeriac,
green radish, kohlrabi, parsnip,
red radish, wasabi, white radish)
15 sup-classes and
200 sub-classes for Veg200

22
13
6
11

Drupe
Litchies
Nut fruit
Other fruit

13
3
11
2
5 (breadfruit, pineapple,
sweetsop, annona muricata,
artocarpus heterophyllus)
6 (golden melon, muskmelon,
honey dew melon, papaya,
netted melon, Hami melon)
10 sup-classes and
92 sub-classes for Fru92

classes and 92 sub-classes. In the current version, there are
91,117 images for vegetables and 69,614 images for fruits.
The number of images for each sub-class varies from 200
to 2000.

VegFru can be naturally divided into two subsets, i.e.,
Veg200 for vegetables and Fru92 for fruits. Table 1 shows
the structure of VegFru, where the sup-classes of Veg200
and Fru92 are listed along with the corresponding number
of sub-classes included. And the sub-classes of Root veg-
etable, Collective fruit and Cucurbites are also listed3.

2.2. VegFru Details

This section presents the details of VegFru. Speciﬁcally,
we will respectively introduce the principles for building
VegFru, process of collecting images, and dataset splits for
training and test.

2.2.1 Building Principles

Aiming at domestic cooking and food management, VegFru
is constructed according to the following principles.

3The detailed sub-classes for each sup-class are provided in the sup-

plementary material.

(a)                        (b)                           

(c)                        (d)

Figure 4. (a) soybean (b) soybean (c) soybean seed (d) soybean
seed. Although (a)-(d) all belong to the seeds of soybean (in d-
ifferent growth periods), they are cooked in disparate ways, thus
being classiﬁed into separate sub-classes.

(a)                        (b)                          (c)                        (d)(cid:3)
Figure 5. (a) potato (b) watermelon (c) cucumber (d) pimen-
to. These images are dropped because the raw food materials are
almost indistinguishable.

* The objects in the images of each sub-class have the

same cooking usage. (Figure 3)

* Each image contains at least one edible part of vegeta-

bles or fruits. (Figure 1)

* The images that contain different edible parts of a cer-
tain vegetable or fruit, e.g., leaf, ﬂower, stem, root, are
classiﬁed into separate sub-classes.

* Even for the images that contain the same edible part
of given vegetable or fruit, if the objects are different
in cooking, we also classify them into different sub-
classes. (Figure 4)

* The objects in each image should be the raw food ma-
terials. If the raw materials of cooked food can not be
made out, the images will be removed. (Figure 5)

2.2.2 Collecting Images

The above principles guide the construction of VegFru, as
well as the process of image collection, which is a really
challenging project.

The ﬁrst step is to collect candidate images for each sub-
class. The images are obtained by searching on the Inter-
net, which is widely used to generate ImageNet [6] and
Microsoft COCO [16]. The sources include Google, Im-
ageNet, Flicker, Bing, Baidu, and so on. The retrieval key-
words are the synonym sets of sub-class names in both Chi-
nese and English. As a consequence, a large number of can-
didate images are collected. Speciﬁcally, over 800 images
are gathered for each sub-class.

Then, to make the dataset highly reliable, the candidate
images are further carefully processed through manual se-
lection. In practice, the images of each sub-class are ﬁltered

543

Figure 6. Sample images of hyacinth bean. Left: Two images
without edible part in ImageNet; Right: Two images with edible
part in VegFru.

(a)

(b)

(c)
Figure 7. Potatoes in the vegetable subsets of ImageNet. Left
To Right: (a) Baked potato (b) French fries (c) Home fries (d)
Mashed potato (e) Uruguay potato. Only the images of Uruguay
potato contain the raw food materials.

(d)

(e)

by ten people on the basis of the class description provided
in [21, 38] and the true positives, following the principles
described in Section 2.2.1. Only images afﬁrmed by more
than eight are reserved. The faded, binary, blurry and dupli-
cated ones are all ﬁltered out.

So far we have completed the construction of 25 sup-
classes and 292 sub-classes, with more than 160, 000 im-
ages totally. Figure 1 displays some sample images collect-
ed by VegFru4.

2.2.3 Dataset Splits

In VegFru, each sub-class contains at least 200 images,
which are divided into training, validation and test set (de-
noted as train, val and test set in the following). An alterna-
tive split way is to ﬁrst arrange the images in each sub-class
randomly. Then the top 100 are selected for train, the fol-
lowing 50 for val, and the rest for test. Finally, a slight
adjustment is applied to the split to ensure that each set is
representative for the variability such as object numbers and
background. The image list for each set is released attached
in the dataset.

2.3. VegFru vs. ImageNet subsets

In this section, we compare VegFru with the vegetable
and fruit subsets of ImageNet from three aspects, i.e., tax-
onomy, image selection and dataset structure. Through the
comparison, the construction and usage of VegFru are fur-
ther motivated.
Taxonomy.

ImageNet [6] constructs its hierarchical
structure based on WordNet [19], which organizes all the

4More sample images are provided in the supplementary material.

Table 2. VegFru vs. ImageNet subsets on dataset structure.
#Sup-the number of sup-classes. #Sub-the number of sub-classes.
Min/Max-the minimum/maximum number of images in each sub-
class. #Sub<200-the number of sub-classes that consist of less
than 200 images.

#Sup

#Sub Min

Max

Vegetables in ImageNet
Vegetables in VegFru
Fruits in ImageNet
Fruits in VegFru

25
15
75
10

175
200
196
92

3
202
0
202

1500+
1807
1500+
1615

#Sub
<200
27
0
50
0

words according to the semantics. For vegetables and fruit-
s, however, we tend to concentrate more on their eating
characteristics in daily life. Actually the taxonomy adopt-
ed by ImageNet for vegetables and fruits is quite unpopular
for domestic cooking and food management, and even con-
tains many repeated categories. For example, turnip and
radish simultaneously belong to root vegetable and crucif-
erous vegetable, and potato is grouped into root vegetable
In fact,
but is also on the list of solanaceous vegetable.
according to [21], potato should be categorized into tuber
vegetable. Moreover, some vegetables which are common
in diet are not included in ImageNet, e.g., water spinach,
shepherd’s purse, basella rubra.

By contrast, in the construction of VegFru, we remove
the rare categories, e.g. woad and ottelia, while many reg-
ular categories are added, e.g., Chinese pumpkin and sug-
arcane. And some categories are grouped into ﬁner class-
es, e.g., radish are divided into white radish, red radish and
green radish. The taxonomy adopted by VegFru specially
serves the purpose of domestic cooking and food manage-
ment in daily life.

Image Selection. All images in VegFru contain the edi-
ble part of a certain vegetable or fruit, which is not included
in lots of images in ImageNet, as shown in Figure 6. Be-
sides, some categories in ImageNet do not cover any raw
food materials. For example, Figure 7 displays the images
of ﬁve potato subordinate classes in the vegetable subsets
of ImageNet, i.e., baked potato, French fries, home fries,
mashed potato and Uruguay potato. Only Uruguay potato
belongs to the raw food materials.

Dataset Structure. Table 2 shows some statistics of
VegFru and the ImageNet subsets. In particular, there are 50
fruit sub-classes and 27 vegetable sub-classes whose num-
ber of images is less than 200 in ImageNet, while VegFru
is comprised of 292 popular sub-classes of vegetables and
fruits with more than 200 images for each sub-classes. And
the taxonomy tree, i.e., the distribution of sup-classes and
sub-classes, is reasonably reorganized for vegetables and
fruits in VegFru.

544

Figure 8. Sample images in FGVC-Aircraft. The airplanes oc-
cupy a large fraction of the whole images. And there exists only
one airplane in each image with relatively clean background.

3. HybridNet

Table 3. VegFru vs. Fine-grained Datasets. #Sup-the number of
sup-classes. #Sub-the number of sub-classes. #Image-the number
of images in total. #Train/#Val/#Test-the number of images in
train/val/test set. #Train+Val(avg)-the average number of images
in each sub-classes for model training (include train and val set).

Dataset

#Sup

#Sub

#Image

#Train

#Val

Birds
Dogs
Cars
Aircrafts

VegFru

none
none
none
70

25

200
120
196
100

292

11788
20580
16185
10000

160731

5994
12000
8144
3334

29200

none
none
none
3333

14600

#Train+
Val(avg)

˜30
100
˜42
˜67

150

#Test

5794
8580
8041
3333

116931

2.4. VegFru vs. Fine grained Datasets

In this section we further compare VegFru with four rep-
resentative ﬁne-gained datasets, i.e., CUB-200-2011 [28]
(Birds), Stanford Dogs [11] (Dogs), Stanford Cars [13]
(Cars) and FGVC-Aircraft [18] (Aircrafts)5, which are
widely used in previous works [35, 12, 17, 7]. The detailed
comparison is shown in Table 3. More ﬁne-grained dataset-
s, such as Oxford Flowers [20] and Pets [23], are not listed
here out of the consideration of simplicity.

Compare to these existing datasets, the domain of Veg-
Fru is novel and more associated with people’s daily life,
which contributes to its broad application prospects. And
VegFru is larger in scale, which has up to 150 images avail-
able in each sub-classes for model training. Particularly,
all the images in VegFru are hierarchically categorized in-
to sup-classes and sub-classes, while the images in these
datasets, except FGVC-Aircraft, are only assigned with
ﬁne-grained labels. So VegFru is well-suited for the hybrid-
granularity research on FGVC. We noticed that the previ-
ous works [30, 39] declared to annotate some ﬁne-grained
datasets, e.g., CUB-200-2011, with extra labels to get the
label hierarchy. However, to the best of our knowledge, the
annotation is not publicly available until the submission,
and the labelling process is labor-intensive. Furthermore,
though FGVC-Aircraft is with hierarchical labels, the ob-
jects of interest, i.e., aircrafts, usually occupy a large frac-
tion of the whole images, and each image only contains one
aircraft with relatively clean background (Figure 8). In con-

5For FGVC-Aircraft, airplane variants are chosen as the labels of sub-
classes, and the 70 sup-classes in Tabel 3 is the number of airplane fami-
lies, which are the upper-level annotaions of airplane variants. Please refer
to [18] for details of this dataset.

Coarse Network 

coarse label

coarse features

Coarse
Extractor

Coarse
Classifier 

Input 
Images

Fusion

fused features

Fused
Classifier

fine label

fine features

Fine
Extractor

Fine
Classifier

Fine Network 

Figure 9. Illustration of the proposed HybridNet. Two-stream
features which deal with the hierarchical labels are ﬁrst extracted
separately, and then sent through the Fusion module to train the
Fused Classiﬁer for overall classiﬁcation.

trast, the images in VegFru are with cluttered background
and vary in number and scale of the objects (Figure 1).

Accompanying the dataset, we also propose an effective
framework called HybridNet to conduct the image classiﬁ-
cation, illustrated in Figure 9. The motivation is to exploit
the label hierarchy for FGVC, which can further verify the
application value of VegFru.

Speciﬁcally, the input images with sup-class and sub-
class labels (denoted as coarse label and ﬁne label in Fig-
ure 9) are ﬁrstly sent into two DCNNs for separate clas-
siﬁcation. Here an end-to-end DCNN is logically divided
into two functional parts, i.e., feature extractor and image
classiﬁer. The division can theoretically occur at any lay-
er, e.g., pool5 in VGGNet. Secondly, the features output
by each extractor, i.e., coarse features and ﬁne features, are
sent through the Fusion module to form a uniﬁed represen-
tation, i.e., fused features. The advanced Compact Bilinear
Pooling [7] is chosen as the fusion method. Finally, the
Fused Classiﬁer plays as the key component to aggregate
two-level features for the ultimate recognition. Actually the
Fused Classiﬁer can handle either coarse-grained or ﬁne-
grained categorization, and the latter one which is more
challenging is evaluated in our experiments. The training
strategy of HybridNet will be elaborated in Section 4.2.1.

The design of HybridNet comes from the following phi-
losophy. Since DCNN is trained in a top-down manner, the
coarse features and ﬁne features tend to deal with different
aspects of the objects, with the condition of being fed with
the coarse label and ﬁne label separately. After the Fusion,
the fused features has synthesized the hybrid-granularity in-
formation, so it is expected to be richer and more accurate
than the ﬁne features, thus resulting in higher accuracy for
FGVC. From another perspective, the optimization of Hy-
bridNet is comprised of three tasks, i.e.,

* Categorizing sup-classes according to the coarse fea-

tures in the Coarse Classiﬁer

545

* Categorizing sub-classes according to the ﬁne features

in the Fine Classiﬁer

* Categorizing sub-classes according to the fused fea-

tures in the Fused Classiﬁer.

Such multi-task optimization is beneﬁcial to learn more dis-
criminative image representation [22, 37, 34].

In the existing literatures, there are considerable interest-
s to enhance DCNN with greater capacity for FGVC, e.g.,
leveraging parts of objects [35, 30, 9, 36], embedding dis-
tance metric learning [24, 29, 31, 37]. Since HybridNet is
motivated by exploiting the label hierarchy, here we only
focus on the related works [30, 37, 32] that make use of la-
bel hierarchy for FGVC and clarify their differences with
our work. In [30], the features of multiple granularity are
concatenated before the linear SVM, whose training is ind-
ependent from DCNN. While in HybridNet, the Compact
Bilinear Pooling are adopted as fusion method and the mod-
In [37], the label
el is trained in an end-to-end manner.
hierarchy is used to construct the input triplets for jointly
optimizing both classiﬁcation and similarity constraints. In
our opinions, the hybrid-granularity information is not ful-
ly utilized in this way. Our HybridNet shares similar ideas
with [32]. However, in [32], the training set is augmented
by the external data annotated with hyper-classes, while the
images in original dataset are still only with ﬁne-grained la-
bels. In contrast, HybridNet is applied to the input images
that are annotated with hierarchical labels, e.g., VegFru, and
the multiple granularity features are separately learned and
fused through explicit operation.

Furthermore, the architecture of HybridNet is intuitive-
ly similar to the Bilinear CNN in [17], where the Bilinear
Pooling is ﬁrst proposed to aggregate the two-stream DCNN
features for FGVC. Actually they differ from each other in
three aspects. Firstly and most importantly, in [17], the two-
stream DCNNs both deal with ﬁne-grained categorization
and much efforts are taken to break the symmetry of two
networks. But in HybridNet, the two DCNNs are natural-
ly asymmetric and complementary since they are fed with
the coarse-grained and ﬁne-grained labels separately. Sec-
ondly, the network architectures are actually dissimilar. The
Bilinear CNN is eventually implemented by a single DCNN
due to weight sharing, while HybridNet holds two DCNNs
which do not share weights. Thirdly, the training process
is quite different. Compared to single-task optimization of
the Bilinear CNN, the training of HybridNet is made up of
multiple tasks. In practice, we adopt the Compact Bilinear
Pooling [7] as fusion method, which inherits the discrimina-
tive power of the Bilinear Pooling and meanwhile reduces
the computation cost. The model in [7] is denoted as CBP-
CNN in the following.

Table 4. Baselines on VegFru. The typical CaffeNet, VGGNet
and GoogLeNet are chosen to set benchmarks on VegFru. All re-
sults are evaluated on the test set and reported in the top-1 mean
accuracy.

Dataset

Category

CaffeNet

VGGNet

GoogLeNet

Veg200

Fru92

VegFru

15 sup-classes
200 sub-classes

10 sup-classes
92 sub-classes

25 sup-classes
292 sub-classes

74.92%
67.21%

79.86%
71.60%

72.87%
66.40%

83.81%
78.50%

86.81%
79.80%

82.45%
77.12%

83.50%
80.17%

87.54%
81.79%

82.52%
79.22%

4. Experiment

In the experiments, we ﬁrst set benchmarks on Veg-
Fru, and then compare HybridNet with the correspond-
ing baselines on VegFru, FGVC-Aircraft [18] and CUB-
200-2011 [28]. All the networks are implemented with
Caffe [10].

4.1. VegFru Baselines

4.1.1 Experimental Setup

The choice of features is usually treated as the most im-
portant design in image recognition, and so far DCNN is
considered to be the most competitive method for feature
extraction. To comprehensively evaluate VegFru, therefore,
we adopt the representative DCNN architectures including
CaffeNet [14], VGGNet [26], and GoogLeNet [27] (avail-
able in the Caffe Model Zoo [1]) to set benchmarks.

All the networks are pretrained on ImageNet and then
ﬁnetuned on VegFru. The images are randomly ﬂipped be-
fore passing into the networks and no other data augmen-
tation is used. The base learning rate is set to 0.001 and
reduced by a factor of 10 when the loss plateaus. The test is
done with one center crop of the input images. Finally the
top-1 mean accuracy is taken to measure the classiﬁcation
performance. It is worth mentioning that the dataset split
way follows the description in Section 2.2.3. The train set
is used for the ﬁnetuning and the evaluation is performed on
the test set. The val set is taken for error analysis here6.

4.1.2 Quantitative Results

The experiments are carried on sup-classes and sub-classes
of VegFru as well as its subsets, i.e., Veg200 and Fru92, and
the quantitative results are shown in Table 4. The three net-
works all achieve reasonable performance for the task of im-
age classiﬁcation, which validates the reliability of VegFru.
However, even the best top-1 accuracy with GoogLeNet

6The top-1 mean accuracy on val set with CaffeNet, VGGNet, and

GoogLeNet is provided in the supplementary material.

546

)

%

(
 
c
c
A
 
1
-
p
o
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0

80

60

40

20

100
Veg200 sub-classes
Figure 10. The top-1 accuracy of GoogLeNet on each sub-
classes of Veg200. The results are evaluated on the val set and
the lowest accuracy lies in the 67th sub-class, i.e., dandelion.

120

140

160

180

200

Figure 11. Sample images of dandelion

Figure 12. Left: shepherd’s purse; Right: prickly lettuce.

(79.22% on sub-classes of VegFru) is still not satisfying e-
nough for real-world applications, indicating that it is still
vital and necessary to develop more advanced models for
the recognition.

4.1.3 Error Analysis

Along with reporting the top-1 mean accuracy, we also ana-
lyze the classiﬁcation performance on each sub-class. Here
GoogLeNet is taken to illustrate the proof and evaluated on
Veg200. The val set is chosen for the evaluation since it has
equal number of images for each sub-classes. The analy-
sis results are shown in Figure 10, and the lowest accuracy
(46%) lies in the sub-class of dandelion (Figure 11). We
further look into the result and ﬁnd that lots of misclassiﬁed
images are predicted to be shepherd’s purse and prickly let-
tuce (Figure 12). It can be seen that the images in Figure 11
and Figure 12 are of subtle difference, and thus more robust
image representation is required to discriminate them.

4.2. HybridNet Performance

4.2.1

Implementation Details

The DCNN in HybridNet can be any existing model, e.g.,
CaffeNet, VGGNet or GoogLeNet. Here the 16-layer VG-
GNet [26] is selected to construct the HybridNet as in CBP-
CNN [7]. Speciﬁcally, the feature extractor of Hybrid-
Net is comprised of the layers of VGGNet before pool5.
And the image classiﬁer includes the layers of compact bi-
linear pooling, signed square-root, l2-normalization, fully-
connection and softmax.

The Coarse Network and Fine Network in HybridNet
are the variants of CBP-CNN. So before introducing the
training of HybridNet, we ﬁrst review the training pro-
cess of CBP-CNN, which consists of two stages denoted
as ft last layer and ft all [2]. Speciﬁcally, ft last layer is
used to train the layers after the Compact Bilinear Pooling
starting with a high learning rate (e.g., 1), and ft all means
global ﬁnetuning with a relatively low learning rate (e.g.,
0.001). The training strategy of HybridNet is illustrated
in Figure 13. Firstly, the Coarse Network and Fine Net-
work are trained in parallel by feeding the coarse label and
ﬁne label separately (each including ft last layer and ft all
shown in Figure 13(a)(b)). Secondly, the Fused Classiﬁer
is optimized based on the fused features with the rest ﬁxed
(Figure 13(c)). Finally, the whole network is globally ﬁne-
tuned (Figure 13(d)). The Coarse Classiﬁer and Fine Clas-
siﬁer are removed in the global ﬁnetuning (Figure 13(d)),
since the jointly ﬁnetuning strategy does not help in this
case, which will be further discussed in Section 4.2.3. The
detailed parameters for the training and test is released with
the dataset and code.

4.2.2 Performance Comparison

The baseline of HybridNet is set by replacing the coarse
label with the ﬁne label in Figure 9. In that condition, the
two DCNNs with the same architectures are symmetrical-
ly initialized and remain symmetric after ﬁnetuning since
the gradients for two networks are identical [17]. Thus the
model can be implemented with just a singe DCNN. So it is
natural to treat CBP-CNN as the baseline of HybridNet.

The performance comparison for HybridNet on VegFru,
FGVC-Aircraft and CUB-200-2011 is shown in Table 5,
where the results are reported in the top-1 mean accuracy.
For HybridNet, the output of Fused Classiﬁer is taken for
the evaluation. As far as we know, CBP-CNN is the exist-
ing state-of-the-art method for FGVC without utilizing part-
s or external data. Our HybridNet outperforms CBP-CNN
by more than 1.3% on VegFru and FGVC-Aircraft, which
both contain hierarchical labels. For CUB-200-2011, extra
efforts are ﬁrst taken to construct the label hierarchy ac-
cording to the taxonomy in North American Birds [3], and

547

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

coarse label
Input 
Images

fine label
f
Input 
Images

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

(a) ft_last_layer of Coarse/Fine Classifier

(b) ft_all of Coarse/Fine Classifier

5. Conclusion

share some shallow layers as the model in [32], which has
the potential to reduce the GPU memory consumption.

coarse label
Input 
Images

fine label
f
Input 
Images

fine label
Input 
Images

Coarse
Extractor

Fine
Extractor

Fused
Classifier

fine label
Input 
Images

Fused
Classifier

Coarse
Extractor

Fine
Extractor

(c) ft_last_layer of Fused_Classifier

(d) ft_all of Fused_Classifier

Figure 13. Training strategy of HybridNet. Inspired by the train-
ing of CBP-CNN, the training of the Fused Classiﬁer is also di-
vided into two stages following the same denotations ((c)(d)). The
Fusion moduels in (c)(d) are omitted for simplicity. In each stage,
only the components surrounded by the red rectangle are ﬁnetuned
with the rest ﬁxed. Best viewed electronically.

Table 5. Performance comparison for HybridNet. To keep the
experiments consistent, HybridNet is trained on the train set of
VegFru. And it is trained on the trainval set of FGVC-Aircraft [18]
and train set of CUB-200-2011 [28]. Finally, all results are evalu-
ated on the test set and reported in the top-1 mean accuracy.

Dataset

VGGNet [26]
CBP-CNN [7]
HybridNet (ours)

VegFru
(292 sub-classes)
77.12%
82.21%
83.51%

Aircrafts [18]
(100 sub-classes)
84.46%
87.49%
88.84%

CUB [28]
(200 sub-classes)
72.32%
84.91%
85.78%

then HybridNet is applied to obtain 85.78% on this dataset
which is higher than that with CBP-CNN (84.91%). The
improvement on CUB-200-2011 is less signiﬁcant, which
is probably due to the small size of training set. The experi-
mental results indicate that the label hierarchy does help for
FGVC7.

4.2.3 Discussion

In the global ﬁnetuning of HybridNet (Figure 13 (d)), we
have tried to add the Coarse Classiﬁer and Fine Classiﬁ-
er as regulations, for the sake of making the features sepa-
rately learned by each extractor discriminative alone in the
training process [15, 5]. However, our preliminary experi-
ments indicate that this jointly ﬁnetuning strategy does not
suit for this case and instead brings performance degrada-
tion, which is probably caused by the complexity of the
Compact Bilinear Pooling for optimizing. Actually, it has
been proved in [34] that the jointly ﬁnetuning strategy does
not always work. The training strategy of HybridNet is e-
quivalent to the iterative switchable learning scheme adopt-
ed in [34], i.e., multiple tasks are optimized by turns (Fig-
ure 13 (a)-(d)). Besides, there still exists room to improve
HybridNet, e.g., the Coarse Network and Fine Network can

In this work, we construct a domain-speciﬁc dataset,
namely VegFru, in the ﬁeld of FGVC. The novelty of Veg-
Fru is that it aims at domestic cooking and food manage-
ment, and categorizes vegetables and fruits according to
their eating characteristics. In VegFru, there are at least 200
images for each subordinate class with hierarchical label-
s, and each image contains at least one edible part of veg-
etables or fruits with the same cooking usage. It is close-
ly associated with the daily life of everyone and has broad
application prospects. Besides, HybridNet is proposed ac-
companying the dataset to exploit the label hierarchy for
FGVC. In HybridNet, multiple granularity features are ﬁrst
separately learned and then fused through explicit opera-
tion, i.e., Compact Bilinear Pooling, to form a uniﬁed image
representation for overall classiﬁcation. The results on Veg-
Fru, FGVC-Aircraft and CUB-200-2011 demonstrate that
HybridNet achieves one of the top performance on these
datasets. We believe that our VegFru and HybridNet would
inspire more advanced research on FGVC.

Acknowledgment

This work is supported partially by the National Natu-
ral Science Foundation of China under Grant 61673362 and
61233003, Youth Innovation Promotion Association CAS,
and the Fundamental Research Funds for the Central Uni-
versities. Many thanks to Dequan Wang for offering the
label hierarchy of CUB-200-2011. And we are grateful for
the generous donation of Tesla GPU K40 from the NVIDIA
corporation.

References

[1] https://github.com/BVLC/caffe/wiki/

Model-Zoo. 6

[2] https://github.com/gy20073/compact_

bilinear_pooling. 7

[3] https://birdsna.org/Species-Account/bna/

home. 7

[4] M. Chan, D. Est`eve, C. Escriba, and E. Campo. A review of
smart homes - present state and future challenges. Computer
Methods and Programs in Biomedicine, 91:55–81, 2008. 2

[5] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In CVPR, 2016. 1, 8

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2, 3, 4

548

7We also provide the results of evaluating HybridNet on the coarse-

[7] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact

grained categorization in the supplementary material.

bilinear pooling. In CVPR, 2016. 2, 5, 6, 7, 8

[28] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 1, 2, 5, 6, 8

[29] C. Wah, G. V. Horn, S. Branson, S. Maji, P. Perona, and S. J.
Belongie. Similarity comparisons for interactive ﬁne-grained
categorization. In CVPR, 2014. 6

[30] D. Wang, Z. Shen, J. Shao, W. Zhang, X. Xue, and Z. Zhang.
Multiple granularity descriptors for ﬁne-grained categoriza-
tion. In ICCV, 2015. 2, 5, 6

[31] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In CVPR, 2014. 6

[32] S. Xie, T. Yang, X. Wang, and Y. Lin. Hyper-class aug-
mented and regularized deep learning for ﬁne-grained image
classiﬁcation. In CVPR, 2015. 2, 6, 8

[33] Z. Yan, H. Zhang, R. Piramuthu, V. Jagadeesh, D. DeCoste,
W. Di, and Y. Yu. Hd-cnn: Hierarchical deep convolutional
neural networks for large scale visual recognition. In ICCV,
2015. 2

[34] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
counting via deep convolutional neural networks. In CVPR,
2015. 6, 8

[35] N. Zhang, J. Donahue, R. B. Girshick, and T. Darrell. Part-
based r-cnns for ﬁne-grained category detection. In ECCV,
2014. 5, 6

[36] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian. Picking
deep ﬁlter responses for ﬁne-grained image recognition. In
CVPR, 2016. 1, 6

[37] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
structures for ﬁne-grained feature representation. In CVPR,
2016. 1, 2, 6

[38] Z. Zheng, S. Zhang, and Z. Zhang. Fruit Cultivation (In
Chinese). Beijing: Press of Agricultural Science and Tech-
nology of China, 2011. 2, 4

[39] F. Zhou and Y. Lin. Fine-grained image classiﬁcation by

exploring bipartite-graph labels. In CVPR, 2016. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 1

[9] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for
ﬁne-grained visual categorization. In CVPR, 2016. 1, 6
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint arX-
iv:1408.5093, 2014. 6

[11] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In CVPR Workshop, 2011. 1, 2, 5

[12] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In CVPR, 2015. 5
[13] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shop, 2013. 1, 2, 5

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[15] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 8

[16] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 3

[17] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In ICCV, 2015. 5, 6,
7

[18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedal-
di. Fine-grained visual classiﬁcation of aircraft. Technical
report, 2013. 1, 2, 5, 6, 8

[19] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 38(11):39–41, 1995. 4

[20] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing, 2008. 5

[21] Chinese Academy of Agricultural Sciences. Vegetable Cul-
tivation (Second Edition, In Chinese). Beijing: China Agri-
culture Press, 2010. 2, 4

[22] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6

[23] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.

Cats and dogs. In CVPR, 2012. 5

[24] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual cat-
egorization via multi-stage metric learning. In CVPR, 2015.
6

[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7, 8

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6

549

VegFru: A Domain-Speciﬁc Dataset for Fine-grained Visual Categorization

Saihui Hou, Yushan Feng and Zilei Wang
Department of Automation, University of Science and Technology of China
{saihui, fyushan}@mail.ustc.edu.cn, zlwang@ustc.edu.cn

Abstract

In this paper, we propose a novel domain-speciﬁc data-
set named VegFru for ﬁne-grained visual categorization
(FGVC). While the existing datasets for FGVC are mainly
focused on animal breeds or man-made objects with limit-
ed labelled data, VegFru is a larger dataset consisting of
vegetables and fruits which are closely associated with the
daily life of everyone. Aiming at domestic cooking and food
management, VegFru categorizes vegetables and fruits ac-
cording to their eating characteristics, and each image con-
tains at least one edible part of vegetables or fruits with the
same cooking usage. Particularly, all the images are la-
belled hierarchically. The current version covers vegetables
and fruits of 25 upper-level categories and 292 subordinate
classes. And it contains more than 160,000 images in total
and at least 200 images for each subordinate class. Accom-
panying the dataset, we also propose an effective framework
called HybridNet to exploit the label hierarchy for FGVC.
Speciﬁcally, multiple granularity features are ﬁrst extracted
by dealing with the hierarchical labels separately. And then
they are fused through explicit operation, e.g., Compact Bi-
linear Pooling, to form a uniﬁed representation for the ul-
timate recognition. The experimental results on the novel
VegFru, the public FGVC-Aircraft and CUB-200-2011 in-
dicate that HybridNet achieves one of the top performance
on these datasets. The dataset and code are available at
https://github.com/ustc-vim/vegfru.

1. Introduction

In computer vision, ﬁne-grained visual categorization
(FGVC) refers to categorizing objects into subordinate
classes, e.g., breeds of birds or dogs. Compared to gener-
ic classiﬁcation [6], FGVC needs to handle more subtle
inter-class difference and larger intra-class variation of ob-
jects, thus requiring more discriminative and robust im-
age representation. Recent years have witnessed the res-
urrection of deep convolutional neural network (DCNN),
which holds state-of-the-art performance of various vi-
sual tasks [8, 25, 5]. The top-performing methods for

Figure 1. Sample images in VegFru. Top: vegetable images. Bot-
tom: fruit images. Best viewed electronically.

FGVC [9, 36, 37] are also built upon DCNN and the training
is data-hungry. However, the data with ﬁne-grained labels
is usually insufﬁcient, e.g., in CUB-200-2011 [28] there are
only about 30 training images for each class. And the ex-
isting datasets for FGVC are mainly focused on domains of
animal breeds, e.g., birds [28] and dogs [11], or man-made
objects, e.g., cars [13] and aircrafts [18]. As the saying
goes, hunger breeds discontent. In modern life, increasing
attention has been paid to how to go on a balanced and nu-
tritious diet. However, to the best of our knowledge, there is
still no public dataset specially designed for recognizing the
raw food materials and recommending appropriate recipes
for individuals.

In this work, aiming at domestic cooking and food
management, we introduce a novel domain-speciﬁc data-
set named VegFru, which consists of vegetables and fruits
that are closely associated with people’s diet. In VegFru,
vegetables and fruits are categorized according to their eat-
ing characteristics, e.g., different edible parts of a certain
vegetable or fruit, such as leaf and root, are classiﬁed in-
to separate subordinate classes. And the objects in each
image are the raw food materials, while the images that
contain cooked food whose raw materials are indistinguish-
able are ﬁltered out. Currently, the dataset covers vegeta-
bles and fruits of 25 upper-level categories and 292 subor-
dinate classes1, which has taken in all species in common.
And it contains more than 160,000 images in total and at

1The upper-level category and subordinate class are respectively denot-

ed as sup-class and sub-class in the following.

541

(a)           

(b)          

(c)            

(d)

Figure 2. (a) chive (b) shallot (c) leek (d) green Chinese onion.
These images belong to different sub-classes but with subtle inter-
class difference.

(a)           

(b)          

(c)            

(d)

Figure 3. (a) lotus root (b) lotus root (c) lotus root (d) lotus root.
These images belong to the same sub-classes but with large intra-
class variation.

least 200 images for each sub-class, which is much larger
than the previous ﬁne-grained datasets [11, 28, 13, 18]. Par-
ticularly, besides the ﬁne-grained annotation, the images in
VegFru are assigned hierarchical labels. And compared to
the vegetable and fruit subsets of ImageNet [6], the taxono-
my adopted by VegFru is more popular for domestic cook-
ing and food management, and the image collection strictly
serves this purpose, making each image in VegFru contain
at least one edible part of vegetables or fruits with the same
cooking usage. Some sample images are shown in Figure 1.
Our VegFru has the potential to be applied to the follow-

ing aspects, but is not limited to them:

* Fine-grained Visual Categorization (FGVC). The sub-
classes in VegFru all belong to vegetables or fruits, and
there exist subtle inter-class difference (Figure 2) and
large intra-class variation (Figure 3). So it can be con-
sidered as a ﬁne-grained dataset of novel domain, with
more images available for each sub-class.

* Hybrid-granularity methods for FGVC. The label hi-
erarchy has proved to be helpful for image recogni-
tion [33, 30, 32, 37]. With all images labelled hier-
archically, VegFru is naturally well-suited for research
on exploiting the hybrid-granularity information, i.e.,
label hierarchy, for the challenging FGVC.

* Practical applications for domestic cooking and food
management. VegFru collects enormous vegetable and
fruit images of raw food materials and categorizes
them by the eating characteristics. It is closely related
to the daily life of everyone, and thus can promote the
applications of computer vision in the Smart Home [4],
e.g., personalized recipe recommendation.

To verify the application value of VegFru, we also pro-
pose an effective framework named HybridNet with the aim

of utilizing the hybrid-granularity information for FGVC,
which aspect VegFru is well-suited for due to the label hi-
erarchy. We take DCNN model to deal with the issue. In
practice, DCNN is trained in a top-down manner, i.e., the
training is driven by the loss generated at the highest lay-
er according to back propagation. When categorizing the
coarse-grained sup-classes, the network only needs to han-
dle generic attributes (e.g., bird outline), but subtle charac-
teristics (e.g., bird eye, foot) become necessary when dis-
tinguishing the ﬁne-grained sub-classes. Our HybridNet
is exactly motivated by exploiting the complementarity be-
tween the coarse-grained and ﬁne-grained features. Specif-
ically, two-stream DCNNs are ﬁrst trained by feeding the
sup-class and sub-class labels separately, whose features
are then fused to form a uniﬁed representation for the ul-
timate recognition. As for the fusion method, we adopt
the advanced Compact Bilinear Pooling proposed by [7],
in which the model is currently the best for FGVC without
utilizing parts or external data. The experimental results on
VegFru, FGVC-Aircraft [18] and CUB-200-2011 [28] show
the robustness and superiority of HybridNet.

In summary, the main contributions of this work lie in
two folds: VegFru and HybridNet. Speciﬁcally, the contri-
bution of VegFru is highlighted in four aspects: novel do-
main, large scale, label hierarchy and application prospects.
And HybridNet outperforms the model in [7] and achieves
one of the top performance on the three datasets by exploit-
ing the label hierarchy.

The rest of the paper is organized as follows. In Sec-
tion 2, we introduce the construction of VegFru and perform
detailed comparison of VegFru with the vegetable and fruit
subsets of ImageNet and the existing ﬁne-grained datasets.
HybridNet and its related works are presented in Section 3.
In Section 4, we set baselines on VegFru and experimentally
evaluate the proposed HybridNet. Finally, the whole work
is concluded in Section 5.

2. VegFru

2.1. Overview

We build the hierarchical structure of VegFru in accor-
dance with the ofﬁcial literatures [21, 38]. Speciﬁcally, the
vegetable hierarchy is constructed according to the Agricul-
tural Biological Taxonomy described in [21]2, which is the
most reasonable for the cooking purpose and arranges veg-
etables into root vegetable, cabbage, leafy vegetable, etc.
Consequently, we obtain 15 sup-classes vegetables with
200 sub-classes. For fruits, similarly, we adopt the Horti-
cultural Taxonomy in [38] to organize fruits into 10 sup-

2In fact, three are three taxonomies for vegetables in [21]. Besides
the Agricultural Biological Taxonomy, the Botanical Taxonomy divides
vegetables into two categories, namely monocotyledon and dicotyledon,
and the Edible Organ Taxonomy groups vegetables into ﬁve categories,
i.e., root, stem, leaf, ﬂower, and fruit.

542

Table 1. The structure of VegFru. #Sub-the number of sub-
classes included. Perennial∗-Perennial-miscellaneous vegetable.
Persimmons∗-Persimmons-jujubes fruit.

Sup-class
Aquatic vegetable
Brassia oleracea
Bud seedling
Green-leafy vegetable
Perennial∗
Tuber vegetable
Wild vegetable

Root vegetable

Total

Berry fruit
Citrus fruit
Persimmons∗
Pome

Collective fruit

Cucurbites

Total

#Sub
13
9
4
31
13
10
32

Sup-class
Alliaceous
Beans
Cabbage
Eggplant
Melon
Mushroom
Mustard

#Sub
10
15
5
7
14
24
2
11 (beetroot, black salsify,
burdock root, carrot, celeriac,
green radish, kohlrabi, parsnip,
red radish, wasabi, white radish)
15 sup-classes and
200 sub-classes for Veg200

22
13
6
11

Drupe
Litchies
Nut fruit
Other fruit

13
3
11
2
5 (breadfruit, pineapple,
sweetsop, annona muricata,
artocarpus heterophyllus)
6 (golden melon, muskmelon,
honey dew melon, papaya,
netted melon, Hami melon)
10 sup-classes and
92 sub-classes for Fru92

classes and 92 sub-classes. In the current version, there are
91,117 images for vegetables and 69,614 images for fruits.
The number of images for each sub-class varies from 200
to 2000.

VegFru can be naturally divided into two subsets, i.e.,
Veg200 for vegetables and Fru92 for fruits. Table 1 shows
the structure of VegFru, where the sup-classes of Veg200
and Fru92 are listed along with the corresponding number
of sub-classes included. And the sub-classes of Root veg-
etable, Collective fruit and Cucurbites are also listed3.

2.2. VegFru Details

This section presents the details of VegFru. Speciﬁcally,
we will respectively introduce the principles for building
VegFru, process of collecting images, and dataset splits for
training and test.

2.2.1 Building Principles

Aiming at domestic cooking and food management, VegFru
is constructed according to the following principles.

3The detailed sub-classes for each sup-class are provided in the sup-

plementary material.

(a)                        (b)                           

(c)                        (d)

Figure 4. (a) soybean (b) soybean (c) soybean seed (d) soybean
seed. Although (a)-(d) all belong to the seeds of soybean (in d-
ifferent growth periods), they are cooked in disparate ways, thus
being classiﬁed into separate sub-classes.

(a)                        (b)                          (c)                        (d)(cid:3)
Figure 5. (a) potato (b) watermelon (c) cucumber (d) pimen-
to. These images are dropped because the raw food materials are
almost indistinguishable.

* The objects in the images of each sub-class have the

same cooking usage. (Figure 3)

* Each image contains at least one edible part of vegeta-

bles or fruits. (Figure 1)

* The images that contain different edible parts of a cer-
tain vegetable or fruit, e.g., leaf, ﬂower, stem, root, are
classiﬁed into separate sub-classes.

* Even for the images that contain the same edible part
of given vegetable or fruit, if the objects are different
in cooking, we also classify them into different sub-
classes. (Figure 4)

* The objects in each image should be the raw food ma-
terials. If the raw materials of cooked food can not be
made out, the images will be removed. (Figure 5)

2.2.2 Collecting Images

The above principles guide the construction of VegFru, as
well as the process of image collection, which is a really
challenging project.

The ﬁrst step is to collect candidate images for each sub-
class. The images are obtained by searching on the Inter-
net, which is widely used to generate ImageNet [6] and
Microsoft COCO [16]. The sources include Google, Im-
ageNet, Flicker, Bing, Baidu, and so on. The retrieval key-
words are the synonym sets of sub-class names in both Chi-
nese and English. As a consequence, a large number of can-
didate images are collected. Speciﬁcally, over 800 images
are gathered for each sub-class.

Then, to make the dataset highly reliable, the candidate
images are further carefully processed through manual se-
lection. In practice, the images of each sub-class are ﬁltered

543

Figure 6. Sample images of hyacinth bean. Left: Two images
without edible part in ImageNet; Right: Two images with edible
part in VegFru.

(a)

(b)

(c)
Figure 7. Potatoes in the vegetable subsets of ImageNet. Left
To Right: (a) Baked potato (b) French fries (c) Home fries (d)
Mashed potato (e) Uruguay potato. Only the images of Uruguay
potato contain the raw food materials.

(d)

(e)

by ten people on the basis of the class description provided
in [21, 38] and the true positives, following the principles
described in Section 2.2.1. Only images afﬁrmed by more
than eight are reserved. The faded, binary, blurry and dupli-
cated ones are all ﬁltered out.

So far we have completed the construction of 25 sup-
classes and 292 sub-classes, with more than 160, 000 im-
ages totally. Figure 1 displays some sample images collect-
ed by VegFru4.

2.2.3 Dataset Splits

In VegFru, each sub-class contains at least 200 images,
which are divided into training, validation and test set (de-
noted as train, val and test set in the following). An alterna-
tive split way is to ﬁrst arrange the images in each sub-class
randomly. Then the top 100 are selected for train, the fol-
lowing 50 for val, and the rest for test. Finally, a slight
adjustment is applied to the split to ensure that each set is
representative for the variability such as object numbers and
background. The image list for each set is released attached
in the dataset.

2.3. VegFru vs. ImageNet subsets

In this section, we compare VegFru with the vegetable
and fruit subsets of ImageNet from three aspects, i.e., tax-
onomy, image selection and dataset structure. Through the
comparison, the construction and usage of VegFru are fur-
ther motivated.
Taxonomy.

ImageNet [6] constructs its hierarchical
structure based on WordNet [19], which organizes all the

4More sample images are provided in the supplementary material.

Table 2. VegFru vs. ImageNet subsets on dataset structure.
#Sup-the number of sup-classes. #Sub-the number of sub-classes.
Min/Max-the minimum/maximum number of images in each sub-
class. #Sub<200-the number of sub-classes that consist of less
than 200 images.

#Sup

#Sub Min

Max

Vegetables in ImageNet
Vegetables in VegFru
Fruits in ImageNet
Fruits in VegFru

25
15
75
10

175
200
196
92

3
202
0
202

1500+
1807
1500+
1615

#Sub
<200
27
0
50
0

words according to the semantics. For vegetables and fruit-
s, however, we tend to concentrate more on their eating
characteristics in daily life. Actually the taxonomy adopt-
ed by ImageNet for vegetables and fruits is quite unpopular
for domestic cooking and food management, and even con-
tains many repeated categories. For example, turnip and
radish simultaneously belong to root vegetable and crucif-
erous vegetable, and potato is grouped into root vegetable
In fact,
but is also on the list of solanaceous vegetable.
according to [21], potato should be categorized into tuber
vegetable. Moreover, some vegetables which are common
in diet are not included in ImageNet, e.g., water spinach,
shepherd’s purse, basella rubra.

By contrast, in the construction of VegFru, we remove
the rare categories, e.g. woad and ottelia, while many reg-
ular categories are added, e.g., Chinese pumpkin and sug-
arcane. And some categories are grouped into ﬁner class-
es, e.g., radish are divided into white radish, red radish and
green radish. The taxonomy adopted by VegFru specially
serves the purpose of domestic cooking and food manage-
ment in daily life.

Image Selection. All images in VegFru contain the edi-
ble part of a certain vegetable or fruit, which is not included
in lots of images in ImageNet, as shown in Figure 6. Be-
sides, some categories in ImageNet do not cover any raw
food materials. For example, Figure 7 displays the images
of ﬁve potato subordinate classes in the vegetable subsets
of ImageNet, i.e., baked potato, French fries, home fries,
mashed potato and Uruguay potato. Only Uruguay potato
belongs to the raw food materials.

Dataset Structure. Table 2 shows some statistics of
VegFru and the ImageNet subsets. In particular, there are 50
fruit sub-classes and 27 vegetable sub-classes whose num-
ber of images is less than 200 in ImageNet, while VegFru
is comprised of 292 popular sub-classes of vegetables and
fruits with more than 200 images for each sub-classes. And
the taxonomy tree, i.e., the distribution of sup-classes and
sub-classes, is reasonably reorganized for vegetables and
fruits in VegFru.

544

Figure 8. Sample images in FGVC-Aircraft. The airplanes oc-
cupy a large fraction of the whole images. And there exists only
one airplane in each image with relatively clean background.

3. HybridNet

Table 3. VegFru vs. Fine-grained Datasets. #Sup-the number of
sup-classes. #Sub-the number of sub-classes. #Image-the number
of images in total. #Train/#Val/#Test-the number of images in
train/val/test set. #Train+Val(avg)-the average number of images
in each sub-classes for model training (include train and val set).

Dataset

#Sup

#Sub

#Image

#Train

#Val

Birds
Dogs
Cars
Aircrafts

VegFru

none
none
none
70

25

200
120
196
100

292

11788
20580
16185
10000

160731

5994
12000
8144
3334

29200

none
none
none
3333

14600

#Train+
Val(avg)

˜30
100
˜42
˜67

150

#Test

5794
8580
8041
3333

116931

2.4. VegFru vs. Fine grained Datasets

In this section we further compare VegFru with four rep-
resentative ﬁne-gained datasets, i.e., CUB-200-2011 [28]
(Birds), Stanford Dogs [11] (Dogs), Stanford Cars [13]
(Cars) and FGVC-Aircraft [18] (Aircrafts)5, which are
widely used in previous works [35, 12, 17, 7]. The detailed
comparison is shown in Table 3. More ﬁne-grained dataset-
s, such as Oxford Flowers [20] and Pets [23], are not listed
here out of the consideration of simplicity.

Compare to these existing datasets, the domain of Veg-
Fru is novel and more associated with people’s daily life,
which contributes to its broad application prospects. And
VegFru is larger in scale, which has up to 150 images avail-
able in each sub-classes for model training. Particularly,
all the images in VegFru are hierarchically categorized in-
to sup-classes and sub-classes, while the images in these
datasets, except FGVC-Aircraft, are only assigned with
ﬁne-grained labels. So VegFru is well-suited for the hybrid-
granularity research on FGVC. We noticed that the previ-
ous works [30, 39] declared to annotate some ﬁne-grained
datasets, e.g., CUB-200-2011, with extra labels to get the
label hierarchy. However, to the best of our knowledge, the
annotation is not publicly available until the submission,
and the labelling process is labor-intensive. Furthermore,
though FGVC-Aircraft is with hierarchical labels, the ob-
jects of interest, i.e., aircrafts, usually occupy a large frac-
tion of the whole images, and each image only contains one
aircraft with relatively clean background (Figure 8). In con-

5For FGVC-Aircraft, airplane variants are chosen as the labels of sub-
classes, and the 70 sup-classes in Tabel 3 is the number of airplane fami-
lies, which are the upper-level annotaions of airplane variants. Please refer
to [18] for details of this dataset.

Coarse Network 

coarse label

coarse features

Coarse
Extractor

Coarse
Classifier 

Input 
Images

Fusion

fused features

Fused
Classifier

fine label

fine features

Fine
Extractor

Fine
Classifier

Fine Network 

Figure 9. Illustration of the proposed HybridNet. Two-stream
features which deal with the hierarchical labels are ﬁrst extracted
separately, and then sent through the Fusion module to train the
Fused Classiﬁer for overall classiﬁcation.

trast, the images in VegFru are with cluttered background
and vary in number and scale of the objects (Figure 1).

Accompanying the dataset, we also propose an effective
framework called HybridNet to conduct the image classiﬁ-
cation, illustrated in Figure 9. The motivation is to exploit
the label hierarchy for FGVC, which can further verify the
application value of VegFru.

Speciﬁcally, the input images with sup-class and sub-
class labels (denoted as coarse label and ﬁne label in Fig-
ure 9) are ﬁrstly sent into two DCNNs for separate clas-
siﬁcation. Here an end-to-end DCNN is logically divided
into two functional parts, i.e., feature extractor and image
classiﬁer. The division can theoretically occur at any lay-
er, e.g., pool5 in VGGNet. Secondly, the features output
by each extractor, i.e., coarse features and ﬁne features, are
sent through the Fusion module to form a uniﬁed represen-
tation, i.e., fused features. The advanced Compact Bilinear
Pooling [7] is chosen as the fusion method. Finally, the
Fused Classiﬁer plays as the key component to aggregate
two-level features for the ultimate recognition. Actually the
Fused Classiﬁer can handle either coarse-grained or ﬁne-
grained categorization, and the latter one which is more
challenging is evaluated in our experiments. The training
strategy of HybridNet will be elaborated in Section 4.2.1.

The design of HybridNet comes from the following phi-
losophy. Since DCNN is trained in a top-down manner, the
coarse features and ﬁne features tend to deal with different
aspects of the objects, with the condition of being fed with
the coarse label and ﬁne label separately. After the Fusion,
the fused features has synthesized the hybrid-granularity in-
formation, so it is expected to be richer and more accurate
than the ﬁne features, thus resulting in higher accuracy for
FGVC. From another perspective, the optimization of Hy-
bridNet is comprised of three tasks, i.e.,

* Categorizing sup-classes according to the coarse fea-

tures in the Coarse Classiﬁer

545

* Categorizing sub-classes according to the ﬁne features

in the Fine Classiﬁer

* Categorizing sub-classes according to the fused fea-

tures in the Fused Classiﬁer.

Such multi-task optimization is beneﬁcial to learn more dis-
criminative image representation [22, 37, 34].

In the existing literatures, there are considerable interest-
s to enhance DCNN with greater capacity for FGVC, e.g.,
leveraging parts of objects [35, 30, 9, 36], embedding dis-
tance metric learning [24, 29, 31, 37]. Since HybridNet is
motivated by exploiting the label hierarchy, here we only
focus on the related works [30, 37, 32] that make use of la-
bel hierarchy for FGVC and clarify their differences with
our work. In [30], the features of multiple granularity are
concatenated before the linear SVM, whose training is ind-
ependent from DCNN. While in HybridNet, the Compact
Bilinear Pooling are adopted as fusion method and the mod-
In [37], the label
el is trained in an end-to-end manner.
hierarchy is used to construct the input triplets for jointly
optimizing both classiﬁcation and similarity constraints. In
our opinions, the hybrid-granularity information is not ful-
ly utilized in this way. Our HybridNet shares similar ideas
with [32]. However, in [32], the training set is augmented
by the external data annotated with hyper-classes, while the
images in original dataset are still only with ﬁne-grained la-
bels. In contrast, HybridNet is applied to the input images
that are annotated with hierarchical labels, e.g., VegFru, and
the multiple granularity features are separately learned and
fused through explicit operation.

Furthermore, the architecture of HybridNet is intuitive-
ly similar to the Bilinear CNN in [17], where the Bilinear
Pooling is ﬁrst proposed to aggregate the two-stream DCNN
features for FGVC. Actually they differ from each other in
three aspects. Firstly and most importantly, in [17], the two-
stream DCNNs both deal with ﬁne-grained categorization
and much efforts are taken to break the symmetry of two
networks. But in HybridNet, the two DCNNs are natural-
ly asymmetric and complementary since they are fed with
the coarse-grained and ﬁne-grained labels separately. Sec-
ondly, the network architectures are actually dissimilar. The
Bilinear CNN is eventually implemented by a single DCNN
due to weight sharing, while HybridNet holds two DCNNs
which do not share weights. Thirdly, the training process
is quite different. Compared to single-task optimization of
the Bilinear CNN, the training of HybridNet is made up of
multiple tasks. In practice, we adopt the Compact Bilinear
Pooling [7] as fusion method, which inherits the discrimina-
tive power of the Bilinear Pooling and meanwhile reduces
the computation cost. The model in [7] is denoted as CBP-
CNN in the following.

Table 4. Baselines on VegFru. The typical CaffeNet, VGGNet
and GoogLeNet are chosen to set benchmarks on VegFru. All re-
sults are evaluated on the test set and reported in the top-1 mean
accuracy.

Dataset

Category

CaffeNet

VGGNet

GoogLeNet

Veg200

Fru92

VegFru

15 sup-classes
200 sub-classes

10 sup-classes
92 sub-classes

25 sup-classes
292 sub-classes

74.92%
67.21%

79.86%
71.60%

72.87%
66.40%

83.81%
78.50%

86.81%
79.80%

82.45%
77.12%

83.50%
80.17%

87.54%
81.79%

82.52%
79.22%

4. Experiment

In the experiments, we ﬁrst set benchmarks on Veg-
Fru, and then compare HybridNet with the correspond-
ing baselines on VegFru, FGVC-Aircraft [18] and CUB-
200-2011 [28]. All the networks are implemented with
Caffe [10].

4.1. VegFru Baselines

4.1.1 Experimental Setup

The choice of features is usually treated as the most im-
portant design in image recognition, and so far DCNN is
considered to be the most competitive method for feature
extraction. To comprehensively evaluate VegFru, therefore,
we adopt the representative DCNN architectures including
CaffeNet [14], VGGNet [26], and GoogLeNet [27] (avail-
able in the Caffe Model Zoo [1]) to set benchmarks.

All the networks are pretrained on ImageNet and then
ﬁnetuned on VegFru. The images are randomly ﬂipped be-
fore passing into the networks and no other data augmen-
tation is used. The base learning rate is set to 0.001 and
reduced by a factor of 10 when the loss plateaus. The test is
done with one center crop of the input images. Finally the
top-1 mean accuracy is taken to measure the classiﬁcation
performance. It is worth mentioning that the dataset split
way follows the description in Section 2.2.3. The train set
is used for the ﬁnetuning and the evaluation is performed on
the test set. The val set is taken for error analysis here6.

4.1.2 Quantitative Results

The experiments are carried on sup-classes and sub-classes
of VegFru as well as its subsets, i.e., Veg200 and Fru92, and
the quantitative results are shown in Table 4. The three net-
works all achieve reasonable performance for the task of im-
age classiﬁcation, which validates the reliability of VegFru.
However, even the best top-1 accuracy with GoogLeNet

6The top-1 mean accuracy on val set with CaffeNet, VGGNet, and

GoogLeNet is provided in the supplementary material.

546

)

%

(
 
c
c
A
 
1
-
p
o
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0

80

60

40

20

100
Veg200 sub-classes
Figure 10. The top-1 accuracy of GoogLeNet on each sub-
classes of Veg200. The results are evaluated on the val set and
the lowest accuracy lies in the 67th sub-class, i.e., dandelion.

120

140

160

180

200

Figure 11. Sample images of dandelion

Figure 12. Left: shepherd’s purse; Right: prickly lettuce.

(79.22% on sub-classes of VegFru) is still not satisfying e-
nough for real-world applications, indicating that it is still
vital and necessary to develop more advanced models for
the recognition.

4.1.3 Error Analysis

Along with reporting the top-1 mean accuracy, we also ana-
lyze the classiﬁcation performance on each sub-class. Here
GoogLeNet is taken to illustrate the proof and evaluated on
Veg200. The val set is chosen for the evaluation since it has
equal number of images for each sub-classes. The analy-
sis results are shown in Figure 10, and the lowest accuracy
(46%) lies in the sub-class of dandelion (Figure 11). We
further look into the result and ﬁnd that lots of misclassiﬁed
images are predicted to be shepherd’s purse and prickly let-
tuce (Figure 12). It can be seen that the images in Figure 11
and Figure 12 are of subtle difference, and thus more robust
image representation is required to discriminate them.

4.2. HybridNet Performance

4.2.1

Implementation Details

The DCNN in HybridNet can be any existing model, e.g.,
CaffeNet, VGGNet or GoogLeNet. Here the 16-layer VG-
GNet [26] is selected to construct the HybridNet as in CBP-
CNN [7]. Speciﬁcally, the feature extractor of Hybrid-
Net is comprised of the layers of VGGNet before pool5.
And the image classiﬁer includes the layers of compact bi-
linear pooling, signed square-root, l2-normalization, fully-
connection and softmax.

The Coarse Network and Fine Network in HybridNet
are the variants of CBP-CNN. So before introducing the
training of HybridNet, we ﬁrst review the training pro-
cess of CBP-CNN, which consists of two stages denoted
as ft last layer and ft all [2]. Speciﬁcally, ft last layer is
used to train the layers after the Compact Bilinear Pooling
starting with a high learning rate (e.g., 1), and ft all means
global ﬁnetuning with a relatively low learning rate (e.g.,
0.001). The training strategy of HybridNet is illustrated
in Figure 13. Firstly, the Coarse Network and Fine Net-
work are trained in parallel by feeding the coarse label and
ﬁne label separately (each including ft last layer and ft all
shown in Figure 13(a)(b)). Secondly, the Fused Classiﬁer
is optimized based on the fused features with the rest ﬁxed
(Figure 13(c)). Finally, the whole network is globally ﬁne-
tuned (Figure 13(d)). The Coarse Classiﬁer and Fine Clas-
siﬁer are removed in the global ﬁnetuning (Figure 13(d)),
since the jointly ﬁnetuning strategy does not help in this
case, which will be further discussed in Section 4.2.3. The
detailed parameters for the training and test is released with
the dataset and code.

4.2.2 Performance Comparison

The baseline of HybridNet is set by replacing the coarse
label with the ﬁne label in Figure 9. In that condition, the
two DCNNs with the same architectures are symmetrical-
ly initialized and remain symmetric after ﬁnetuning since
the gradients for two networks are identical [17]. Thus the
model can be implemented with just a singe DCNN. So it is
natural to treat CBP-CNN as the baseline of HybridNet.

The performance comparison for HybridNet on VegFru,
FGVC-Aircraft and CUB-200-2011 is shown in Table 5,
where the results are reported in the top-1 mean accuracy.
For HybridNet, the output of Fused Classiﬁer is taken for
the evaluation. As far as we know, CBP-CNN is the exist-
ing state-of-the-art method for FGVC without utilizing part-
s or external data. Our HybridNet outperforms CBP-CNN
by more than 1.3% on VegFru and FGVC-Aircraft, which
both contain hierarchical labels. For CUB-200-2011, extra
efforts are ﬁrst taken to construct the label hierarchy ac-
cording to the taxonomy in North American Birds [3], and

547

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

coarse label
Input 
Images

fine label
f
Input 
Images

Coarse
Extractor

Coarse
Classifier 

Fine
Extractor

Fine
Classifier

(a) ft_last_layer of Coarse/Fine Classifier

(b) ft_all of Coarse/Fine Classifier

5. Conclusion

share some shallow layers as the model in [32], which has
the potential to reduce the GPU memory consumption.

coarse label
Input 
Images

fine label
f
Input 
Images

fine label
Input 
Images

Coarse
Extractor

Fine
Extractor

Fused
Classifier

fine label
Input 
Images

Fused
Classifier

Coarse
Extractor

Fine
Extractor

(c) ft_last_layer of Fused_Classifier

(d) ft_all of Fused_Classifier

Figure 13. Training strategy of HybridNet. Inspired by the train-
ing of CBP-CNN, the training of the Fused Classiﬁer is also di-
vided into two stages following the same denotations ((c)(d)). The
Fusion moduels in (c)(d) are omitted for simplicity. In each stage,
only the components surrounded by the red rectangle are ﬁnetuned
with the rest ﬁxed. Best viewed electronically.

Table 5. Performance comparison for HybridNet. To keep the
experiments consistent, HybridNet is trained on the train set of
VegFru. And it is trained on the trainval set of FGVC-Aircraft [18]
and train set of CUB-200-2011 [28]. Finally, all results are evalu-
ated on the test set and reported in the top-1 mean accuracy.

Dataset

VGGNet [26]
CBP-CNN [7]
HybridNet (ours)

VegFru
(292 sub-classes)
77.12%
82.21%
83.51%

Aircrafts [18]
(100 sub-classes)
84.46%
87.49%
88.84%

CUB [28]
(200 sub-classes)
72.32%
84.91%
85.78%

then HybridNet is applied to obtain 85.78% on this dataset
which is higher than that with CBP-CNN (84.91%). The
improvement on CUB-200-2011 is less signiﬁcant, which
is probably due to the small size of training set. The experi-
mental results indicate that the label hierarchy does help for
FGVC7.

4.2.3 Discussion

In the global ﬁnetuning of HybridNet (Figure 13 (d)), we
have tried to add the Coarse Classiﬁer and Fine Classiﬁ-
er as regulations, for the sake of making the features sepa-
rately learned by each extractor discriminative alone in the
training process [15, 5]. However, our preliminary experi-
ments indicate that this jointly ﬁnetuning strategy does not
suit for this case and instead brings performance degrada-
tion, which is probably caused by the complexity of the
Compact Bilinear Pooling for optimizing. Actually, it has
been proved in [34] that the jointly ﬁnetuning strategy does
not always work. The training strategy of HybridNet is e-
quivalent to the iterative switchable learning scheme adopt-
ed in [34], i.e., multiple tasks are optimized by turns (Fig-
ure 13 (a)-(d)). Besides, there still exists room to improve
HybridNet, e.g., the Coarse Network and Fine Network can

In this work, we construct a domain-speciﬁc dataset,
namely VegFru, in the ﬁeld of FGVC. The novelty of Veg-
Fru is that it aims at domestic cooking and food manage-
ment, and categorizes vegetables and fruits according to
their eating characteristics. In VegFru, there are at least 200
images for each subordinate class with hierarchical label-
s, and each image contains at least one edible part of veg-
etables or fruits with the same cooking usage. It is close-
ly associated with the daily life of everyone and has broad
application prospects. Besides, HybridNet is proposed ac-
companying the dataset to exploit the label hierarchy for
FGVC. In HybridNet, multiple granularity features are ﬁrst
separately learned and then fused through explicit opera-
tion, i.e., Compact Bilinear Pooling, to form a uniﬁed image
representation for overall classiﬁcation. The results on Veg-
Fru, FGVC-Aircraft and CUB-200-2011 demonstrate that
HybridNet achieves one of the top performance on these
datasets. We believe that our VegFru and HybridNet would
inspire more advanced research on FGVC.

Acknowledgment

This work is supported partially by the National Natu-
ral Science Foundation of China under Grant 61673362 and
61233003, Youth Innovation Promotion Association CAS,
and the Fundamental Research Funds for the Central Uni-
versities. Many thanks to Dequan Wang for offering the
label hierarchy of CUB-200-2011. And we are grateful for
the generous donation of Tesla GPU K40 from the NVIDIA
corporation.

References

[1] https://github.com/BVLC/caffe/wiki/

Model-Zoo. 6

[2] https://github.com/gy20073/compact_

bilinear_pooling. 7

[3] https://birdsna.org/Species-Account/bna/

home. 7

[4] M. Chan, D. Est`eve, C. Escriba, and E. Campo. A review of
smart homes - present state and future challenges. Computer
Methods and Programs in Biomedicine, 91:55–81, 2008. 2

[5] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At-
tention to scale: Scale-aware semantic image segmentation.
In CVPR, 2016. 1, 8

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2, 3, 4

548

7We also provide the results of evaluating HybridNet on the coarse-

[7] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact

grained categorization in the supplementary material.

bilinear pooling. In CVPR, 2016. 2, 5, 6, 7, 8

[28] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 1, 2, 5, 6, 8

[29] C. Wah, G. V. Horn, S. Branson, S. Maji, P. Perona, and S. J.
Belongie. Similarity comparisons for interactive ﬁne-grained
categorization. In CVPR, 2014. 6

[30] D. Wang, Z. Shen, J. Shao, W. Zhang, X. Xue, and Z. Zhang.
Multiple granularity descriptors for ﬁne-grained categoriza-
tion. In ICCV, 2015. 2, 5, 6

[31] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In CVPR, 2014. 6

[32] S. Xie, T. Yang, X. Wang, and Y. Lin. Hyper-class aug-
mented and regularized deep learning for ﬁne-grained image
classiﬁcation. In CVPR, 2015. 2, 6, 8

[33] Z. Yan, H. Zhang, R. Piramuthu, V. Jagadeesh, D. DeCoste,
W. Di, and Y. Yu. Hd-cnn: Hierarchical deep convolutional
neural networks for large scale visual recognition. In ICCV,
2015. 2

[34] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd
counting via deep convolutional neural networks. In CVPR,
2015. 6, 8

[35] N. Zhang, J. Donahue, R. B. Girshick, and T. Darrell. Part-
based r-cnns for ﬁne-grained category detection. In ECCV,
2014. 5, 6

[36] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian. Picking
deep ﬁlter responses for ﬁne-grained image recognition. In
CVPR, 2016. 1, 6

[37] X. Zhang, F. Zhou, Y. Lin, and S. Zhang. Embedding label
structures for ﬁne-grained feature representation. In CVPR,
2016. 1, 2, 6

[38] Z. Zheng, S. Zhang, and Z. Zhang. Fruit Cultivation (In
Chinese). Beijing: Press of Agricultural Science and Tech-
nology of China, 2011. 2, 4

[39] F. Zhou and Y. Lin. Fine-grained image classiﬁcation by

exploring bipartite-graph labels. In CVPR, 2016. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 1

[9] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn for
ﬁne-grained visual categorization. In CVPR, 2016. 1, 6
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint arX-
iv:1408.5093, 2014. 6

[11] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In CVPR Workshop, 2011. 1, 2, 5

[12] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In CVPR, 2015. 5
[13] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In ICCV Work-
shop, 2013. 1, 2, 5

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[15] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In AISTATS, 2015. 8

[16] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014. 3

[17] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In ICCV, 2015. 5, 6,
7

[18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedal-
di. Fine-grained visual classiﬁcation of aircraft. Technical
report, 2013. 1, 2, 5, 6, 8

[19] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 38(11):39–41, 1995. 4

[20] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing, 2008. 5

[21] Chinese Academy of Agricultural Sciences. Vegetable Cul-
tivation (Second Edition, In Chinese). Beijing: China Agri-
culture Press, 2010. 2, 4

[22] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6

[23] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.

Cats and dogs. In CVPR, 2012. 5

[24] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual cat-
egorization via multi-stage metric learning. In CVPR, 2015.
6

[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 1

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7, 8

[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6

549

