BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

Hasso Plattner Institute (HPI), University of Potsdam, Germany
P.O. Box 900460
Potsdam D-14480
{haojin.yang,christian.bartz,meinel}@hpi.de
{martin.fritzsche}@student.hpi.de

7
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
4
6
8
9
0
.
5
0
7
1
:
v
i
X
r
a

ABSTRACT
Binary Neural Networks (BNNs) can drastically reduce memory size
and accesses by applying bit-wise operations instead of standard
arithmetic operations. Therefore it could significantly improve the
efficiency and lower the energy consumption at runtime, which
enables the application of state-of-the-art deep learning models on
low power devices. BMXNet is an open-source BNN library based
on MXNet, which supports both XNOR-Networks and Quantized
Neural Networks. The developed BNN layers can be seamlessly
applied with other standard library components and work in both
GPU and CPU mode. BMXNet is maintained and developed by the
multimedia research group at Hasso Plattner Institute and released
under Apache license. Extensive experiments validate the efficiency
and effectiveness of our implementation. The BMXNet library,
several sample projects, and a collection of pre-trained binary deep
models are available for download at https://github.com/hpi-xnor

CCS CONCEPTS
•Software and its engineering → Software libraries and repos-
itories; •Computer systems organization → Neural networks;
•Computing methodologies → Computer vision;

KEYWORDS
Open Source, Computer Vision, Binary Neural Networks, Machine
Learning

ACM Reference format:
Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel. 2016.
BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet. In Proceedings of ACM Conference, Wash-
ington, DC, USA, July 2017 (Conference’17), 4 pages.
DOI: 10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
In recent years, deep learning technologies achieved excellent per-
formance and many breakthroughs in both academia and industry.
However the state-of-the-art deep models are computational ex-
pensive and consume large storage space. Deep learning is also
strongly demanded by numerous applications from areas such as
mobile platforms, wearable devices, autonomous robots and IoT
devices. How to efficiently apply deep models on such low power

Conference’17, Washington, DC, USA
2016. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
DOI: 10.1145/nnnnnnn.nnnnnnn

devices becomes a challenging research problem. The recently intro-
duced Binary Neural Networks (BNNs) could be one of the possible
solutions for this problem.

Several approaches [4, 7, 14, 15] introduce the usage of BNNs.
These BNNs have the capability of decreasing the memory con-
sumption and computational complexity of the neural network.
This is achieved by on the one hand storing the weights, that are
typically stored as 32 bit floating point values, as binary values, by
binarizing the floating point values with the sign function, to be of
either {0, 1} or {−1, 1}, and storing several of them in a single 32
bit float or integer. Computational complexity, on the other hand,
is reduced by using xnor and popcount for performing matrix mul-
tiplications used in convolutional and fully connected layers. Most
of the publicly available implementations of BNN do not store the
weights in their binarized form [4, 7, 14, 15], nor use xnor and
popcount [7, 15] while performing the matrix multiplications in
convolutional and fully connected layers.

The deep learning library Tensorflow [8] tries to decrease the
memory consumption and computational complexity of deep neural
networks, by quantizing the 32 bit floating point weights and inputs
into 8 bit integers. Together with the minimum and maximum
value of the weight/input matrix, 4× less memory usage and also
decreased computational complexity is achieved, as all operations
only need to be performed on 8 bit values rather than 32 bit values.
BMXNet stores the weights of convolutional and fully connected
layers in their binarized format, which enables us to store 32/64
weights in a single 32/64 bit float/integer and use 32× less mem-
ory. During training and inference we binarize the input to each
binary convolution and fully connected layer in the same way as
the weights get binarized, and perform matrix multiplication using
bit-wise operations (xnor and popcount). Our implementation is
also prepared to use networks that store weights and use inputs
with arbitrary bit widths as proposed by Zhou et al. [15].

The deep learning library MXNet [3] serves as a base for our
code. MXNet is a high performance and modular deep learning
library, that is written in C++. MXNet provides Bindings for other
popular programming languages like Python, R, Scala and Go, and
is used by a wide range of researchers and companies.

2 FRAMEWORK
BMXNet provides activation, convolution and fully connected lay-
ers that support quantization and binarization of input data and
weights. These layers are designed as drop-in replacements for
the corresponding MXNet variants and are called QActivation,

Conference’17, July 2017, Washington, DC, USA

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

QConvolution and QFullyConnected. They provide an additional
parameter, act bit, which controls the bit width the layers calcu-
late with.

A Python example usage of our framework in comparison to
MXNet is shown in Listing 1 and 2. We do not use binary layers for
the first and last layer in the network, as we have confirmed the
experiments of [14] showing that this greatly decreases accuracy.
The standard block structure of a BNN in BMXNet is conducted as:
QActivation-QConv/QFC-BatchNorm-Pooling as shown in Listing 2.

Listing 1: LeNet

def get_lenet () :

data = mx . symbol . Variable ( ' data ')
# first conv layer
conv1 = mx . sym . Convolution (...)
tanh1 = mx . sym . Activation (...)
pool1 = mx . sym . Pooling (...)
bn1 = mx . sym . BatchNorm (...)
# second conv layer
conv2 = mx . sym . Convolution (...)
bn2 = mx . sym . BatchNorm (...)
tanh2 = mx . sym . Activation (...)
pool2 = mx . sym . Pooling (...)
# first fullc layer
flatten = mx . sym . Flatten (...)
fc1 = mx . symbol . FullyConnected (..)
bn3 = mx . sym . BatchNorm (...)
tanh3 = mx . sym . Activation (...)
# second fullc
fc2 = mx . sym . FullyConnected (...)
# softmax loss
lenet = mx . sym . SoftmaxOutput (...)
return lenet

Listing 2: Binary LeNet

def get_binary_lenet () :

data = mx . symbol . Variable ( ' data ')
# first conv layer
conv1 = mx . sym . Convolution (...)
tanh1 = mx . sym . Activation (...)
pool1 = mx . sym . Pooling (...)
bn1 = mx . sym . BatchNorm (...)
# second conv layer
ba1 = mx.sym.QActivation(...)
conv2 = mx.sym.QConvolution(...)
bn2 = mx . sym . BatchNorm (...)
pool2 = mx . sym . Pooling (...)
# first fullc layer
flatten = mx . sym . Flatten (...)
ba2 = mx.symbol.QActivation(..)
fc1 = mx.symbol.QFullyConnected(..)
bn3 = mx . sym . BatchNorm (...)
tanh3 = mx . sym . Activation (...)
# second fullc
fc2 = mx . sym . FullyConnected (...)
# softmax loss
lenet = mx . sym . SoftmaxOutput (...)
return lenet

2.1 Quantization
The quantization on bit widths ranging from 2 to 31 bit is available
for experiments with training and prediction, using low precision
weights and inputs. The quantized data is still stored in the default
32 bit float values and the standard MXNet dot product operations
are applied.

We quantize the weights following the linear quantization as
shown by [15]. Equation 1 will quantize a real number input in the
range [0, 1] to a number in the same range representable with a bit
width of k bit.

quantize(input, k) = round((2k − 1) ∗ input)

(1)

2k − 1

2.2 Binarization
The extreme case of quantizing to 1 bit wide values is the bina-
rization. Working with binarized weights and input data allows
for highly performant matrix multiplications by utilizing the CPU
instructions xnor and popcount.

2.2.1 Dot Product with xnor and popcount. Fully connected
and convolutional layers heavily rely on dot products of matri-
ces, which in turn require massive floating point operations. Most
modern CPUs are optimized for these types of operations. But
especially for real time applications on embedded or less powerful
devices (cell phones, IoT devices) there are optimizations that im-
prove performance, reduce memory and I/O footprint, and lower
power consumption [2].

To calculate the dot product of two binary matrices A ◦ B, no
multiplication operation is required. The element-wise multiplica-
tion and summation of each row of A with each column of B can
be approximated by first combining them with the xnor operation
and then counting the number of bits set to 1 in the result which is
the population count [14].

Listing 3: Baseline xnor GEMM Kernel

void xnor_gemm_baseline_no_omp ( int M , int N , int K ,

BINARY_WORD *A , int lda ,
BINARY_WORD *B , int ldb ,
float *C , int ldc ){

for ( int m = 0; m < M; ++ m) {

for ( int k = 0; k < K; k ++) {

BINARY_WORD A_PART = A[m* lda +k ];
for ( int n = 0; n < N; ++ n) {

C[m* ldc +n] += __builtin_popcountl (∼( A_PART ∧ B[k* ldb +n ]) );

}

}

}

}

We can approximate the multiplication and addition of two times
64 matrix elements in just a few processor instructions on x64
CPUs and two times 32 elements on x86 and ARMv7 processors.
This is enabled by hardware support for the xnor and popcount
operations. They translate directly into a single assembly command.
The population count instruction is available on x86 and x64 CPUs
supporting SSE4.2, while on ARM architecture it is included in the
NEON instruction set.

An unoptimized GEMM (General Matrix Multiplication) imple-
mentation utilizing these instructions is shown in Listing 3. The
compiler intrinsic builtin popcount is supported by both gcc
and clang compilers and translates into the machine instruction on
supported hardware. BINARY WORD is the packed data type storing
32 (x86 and ARMv7) or 64 (x64) matrix elements, each represented
by a single bit.

We have implemented several optimized versions of the xnor
GEMM kernel. We leverage processor cache hierarchies by blocking
and packing the data, use unrolling techniques and OpenMP for
parallelization.

2.2.2 Training. We carefully designed the binarized layers (uti-
lizing xnor and population count operations) to exactly match the
output of the built-in layers of MXNet (computing with BLAS dot
product operations) when limiting those to the discrete values -1
and +1. This enables massively parallel training with GPU support
by utilizing CuDNN on high performance clusters. The trained
model can then be used on less powerful devices where the forward
pass for prediction will calculate the dot product with the xnor and
popcount operations instead of multiplication and addition.

(m×n)

◦ B

The possible values after performing an xnor and popcount
are in the range [0, +n] with the
matrix multiplication A
step size 1, whereas a normal dot product of matrices limited to
discrete values -1 and +1 will be in the range [−n, +n] with the step
size 2. To enable GPU supported training we modify the training
process. After calculation of the dot product we map the result back
to the range [0, +n] to match the xnor dot product, as in Equation 2.

(n×k )

outputx nor dot

= outputdot

+ n

2

(2)

BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet

Conference’17, July 2017, Washington, DC, USA

Figure 1: Processing time comparison of GEMM methods

Figure 3: Speedup comparison based on naive gemm method
by varying kernel size of the convolution layer. The input
channel size, batch size and filter number are set to 256, 200
and 64 respectively.

In the current deep neural network implementations, most of
the fully connected and convolution layers are implemented us-
ing GEMM. According to the evaluation result from [9], over 90%
of the processing time of the Caffe-AlexNet [10] model is spent
on such layers. We thus conducted experiments to measure the
efficiency of different GEMM methods. The measurements were
performed within a convolution layer, where we fixed the parame-
ters as follows: filter number=64, kernel size=5×5, batch size=200,
and the matrix sizes M, N, K are 64, 12800, kernelw × kernelh ×
inputChannelSize, respectively. Figure 1 shows the evaluation re-
sults. The colored columns denote the processing time in millisec-
onds across varying input channel size; xnor 32 and xnor 64 de-
note the xnor gemm operator in 32 bit and 64 bit; xnor 64 omp
denotes the 64 bit xnor gemm accelerated by using the OpenMP2
parallel programming library; binarize input and xnor 64 omp
further accumulated the processing time of input data binariza-
tion. From the results we can determine that xnor 64 omp achieved
about 50× and 125× acceleration in comparison to Cblas(Atlas3)
and naive gemm kernel, respectively. By accumulating the bina-
rization time of input data we still achieved about 13× acceleration
compared with Cblas method.

Figures 2 and 3 illustrate the speedup achieved by varying filter

number and kernel size based on the naive gemm method.

3.2 Classification Accuracy
We further conduted experiments with our BNNs on the MNIST,
CIFAR-10 and ImageNet datasets. The experiments were performed
on a work station which has an Intel(R) Core(TM) i7-6900K CPU,
64 GB RAM and 4 TITAN X (Pascal) GPUs.

By following the same strategy as applied in [7, 14, 15] we al-
ways avoid binarization at the first convolution layer and the last
fully connected layer. Table 1 depicts the classification test ac-
curacy of our binary, as well as full precision models trained on
MNIST and CIFAR-10. The table shows that the size of binary
models is significantly reduced, while the accuracy is still competi-
tive. Table 2 demonstrates the validation accuracy of our binary,

Figure 2: Speedup comparison based on naive gemm method
by varying filter number of the convolution layer. The input
channel size is fixed to 256 while the kernel size and batch
size are set to 5×5 and 200 respectively.

2.2.3 Model Converter. After training a network with BMXNet,
the weights are stored in 32 bit float variables. This is also the case
for networks trained with a bit width of 1 bit. We provide a model
converter1 that reads in a binary trained model file and packs the
weights of QConvolution and QFullyConnected layers. After this
conversion only 1 bit of storage and runtime memory is used per
weight. A ResNet-18 network with full precision weights has a size
of 44.7MB. The conversion with our model converter achieves 29×
compression resulting in a file size of 1.5MB (cf. Table 1).

3 EVALUATION
In this section we report the evaluation results of both efficiency
analysis and classification accuracy over MNIST [13], CIFAR-10 [11]
and ImageNet [5] datasets using BMXNet.

3.1 Efficiency Analysis
All the experiments in this section have been performed on Ubuntu
16.04/64-bit platform with Intel 2.50GHz × 4 CPU with popcnt in-
struction (SSE4.2) and 8G RAM.

1https://github.com/hpi-xnor/BMXNet/tree/master/smd hpi/tools/model-converter

2http://www.openmp.org/
3http://math-atlas.sourceforge.net/

Conference’17, July 2017, Washington, DC, USA

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

Architecture Test Accuracy (Binary/Full Precision) Model Size (Binary/Full Precision)

MNIST
CIFAR-10

Lenet
ResNet-18

0.97/0.99
0.86/0.90

206kB/4.6MB
1.5MB/44.7MB

Table 1: Classification test accuracy of binary and full precision models trained on MNIST and CIFAR-10 dataset. No pre-
training or data augmentation was used.

GEMM computation. In order to demonstrate the applicability we
developed sample applications for image classification on Android
as well as iOS using a binarized ResNet-18 model. Source code, doc-
umentation, pre-trained models and sample projects are published
on GitHub [1].

REFERENCES
[1] 2017. BMXNet: an open-source binary neural network library. https://github.

com/hpi-xnor. (2017).

[2] Renzo Andri, Lukas Cavigelli, Davide Rossi, and Luca Benini. 2016. YodaNN:
An ultra-low power convolutional neural network accelerator based on binary
weights. In VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on.
IEEE, 236–241.

[5]

[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible
and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
CoRR abs/1512.01274 (2015).

[4] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryCon-
nect: Training Deep Neural Networks with binary weights during propagations.
In Advances in Neural Information Processing Systems 28. 3123–3131.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A
Large-Scale Hierarchical Image Database. In CVPR09.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770–778.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized Neural Networks. In Advances in Neural Information
Processing Systems 29. 4107–4115.

[7]

[8] Google Inc. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Systems. (2015). http://tensorflow.org/ Software available from tensorflow.org.
[9] Yangqing Jia. 2014. Learning Semantic Image Representations at a Large Scale.
Ph.D. Dissertation. EECS Department, University of California, Berkeley.
[10] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolu-
tional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093
(2014).

[11] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2014. CIFAR-10 (Canadian

Institute for Advanced Research). (2014).

[12] Yann LeCun and Yoshua Bengio. 1998. The Handbook of Brain Theory and Neural
Networks. MIT Press, Cambridge, MA, USA, Chapter Convolutional Networks
for Images, Speech, and Time Series, 255–258.

[13] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database. (2010).

http://yann.lecun.com/exdb/mnist/

[14] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.
XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Net-
works. In Computer Vision - ECCV 2016. 525–542.

[15] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
2016. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
Low Bitwidth Gradients. arXiv:1606.06160 [cs] (2016).

Val-acc-top-1 Val-acc-top-5 Model Size

Full Precision
Stage
none
1st
2nd
3rd
4th
1st, 2nd
All

0.42
0.48
0.44
0.49
0.47
0.49
0.61

0.66
0.73
0.69
0.73
0.71
0.73
0.84

3.6MB
4.1MB
5.6MB
11.3MB
36MB
6.2MB
47MB

Table 2: Classification test accuracy of binary, partially
binarized and full precision models trained on ImageNet.
ResNet-18 architecture was used in the experiment.

partially-binarized and full precision models trained on ImageNet.
The ResNet implementation in MXNet consists of 4 ResUnit stages,
we thus also report the results of a partially-binarized model with
specific full precision stages. The partially-binarized model with the
first full precision stage shows a great accuracy improvement with
very minor model size increase, compared to the fully binarized
model.

4 EXAMPLE APPLICATIONS
4.1 Python Scripts
The BMXNet repository [1] contains python scripts that can train
and validate binarized neural networks. The script smd hpi/examples/
binary mnist/mnist cnn.py will train a binary LeNet [12] with
the MNIST [13] data set. To train a network with the CIFAR-
10 [11] or ImageNet [5] data set there is a python script based
on the ResNet-18 [6] architecture. Find it at smd hpi/examples/
binary-imagenet1k/train cifar10/train [dataset].py. For
further information and example invocation see the corresponding
README.md

4.2 Mobile Applications

4.2.1

Image Classification. The Android application android

-image-classification and iOS application ios-image-
classification can classify the live camera feed based on a bina-
rized ResNet-18 model trained on the ImageNet dataset.

4.2.2 Handwritten Digit Detection. The iOS application ios-mnist

can classify handwritten numbers based on a binarized LeNet model
trained on the MNIST dataset.

5 CONCLUSION
We introduced BMXNet, an open-source binary neural network
implementation in C/C++ based on MXNet. The evaluation results
show up to 29× model size saving and much more efficient xnor

BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

Hasso Plattner Institute (HPI), University of Potsdam, Germany
P.O. Box 900460
Potsdam D-14480
{haojin.yang,christian.bartz,meinel}@hpi.de
{martin.fritzsche}@student.hpi.de

7
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
4
6
8
9
0
.
5
0
7
1
:
v
i
X
r
a

ABSTRACT
Binary Neural Networks (BNNs) can drastically reduce memory size
and accesses by applying bit-wise operations instead of standard
arithmetic operations. Therefore it could significantly improve the
efficiency and lower the energy consumption at runtime, which
enables the application of state-of-the-art deep learning models on
low power devices. BMXNet is an open-source BNN library based
on MXNet, which supports both XNOR-Networks and Quantized
Neural Networks. The developed BNN layers can be seamlessly
applied with other standard library components and work in both
GPU and CPU mode. BMXNet is maintained and developed by the
multimedia research group at Hasso Plattner Institute and released
under Apache license. Extensive experiments validate the efficiency
and effectiveness of our implementation. The BMXNet library,
several sample projects, and a collection of pre-trained binary deep
models are available for download at https://github.com/hpi-xnor

CCS CONCEPTS
•Software and its engineering → Software libraries and repos-
itories; •Computer systems organization → Neural networks;
•Computing methodologies → Computer vision;

KEYWORDS
Open Source, Computer Vision, Binary Neural Networks, Machine
Learning

ACM Reference format:
Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel. 2016.
BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet. In Proceedings of ACM Conference, Wash-
ington, DC, USA, July 2017 (Conference’17), 4 pages.
DOI: 10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
In recent years, deep learning technologies achieved excellent per-
formance and many breakthroughs in both academia and industry.
However the state-of-the-art deep models are computational ex-
pensive and consume large storage space. Deep learning is also
strongly demanded by numerous applications from areas such as
mobile platforms, wearable devices, autonomous robots and IoT
devices. How to efficiently apply deep models on such low power

Conference’17, Washington, DC, USA
2016. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
DOI: 10.1145/nnnnnnn.nnnnnnn

devices becomes a challenging research problem. The recently intro-
duced Binary Neural Networks (BNNs) could be one of the possible
solutions for this problem.

Several approaches [4, 7, 14, 15] introduce the usage of BNNs.
These BNNs have the capability of decreasing the memory con-
sumption and computational complexity of the neural network.
This is achieved by on the one hand storing the weights, that are
typically stored as 32 bit floating point values, as binary values, by
binarizing the floating point values with the sign function, to be of
either {0, 1} or {−1, 1}, and storing several of them in a single 32
bit float or integer. Computational complexity, on the other hand,
is reduced by using xnor and popcount for performing matrix mul-
tiplications used in convolutional and fully connected layers. Most
of the publicly available implementations of BNN do not store the
weights in their binarized form [4, 7, 14, 15], nor use xnor and
popcount [7, 15] while performing the matrix multiplications in
convolutional and fully connected layers.

The deep learning library Tensorflow [8] tries to decrease the
memory consumption and computational complexity of deep neural
networks, by quantizing the 32 bit floating point weights and inputs
into 8 bit integers. Together with the minimum and maximum
value of the weight/input matrix, 4× less memory usage and also
decreased computational complexity is achieved, as all operations
only need to be performed on 8 bit values rather than 32 bit values.
BMXNet stores the weights of convolutional and fully connected
layers in their binarized format, which enables us to store 32/64
weights in a single 32/64 bit float/integer and use 32× less mem-
ory. During training and inference we binarize the input to each
binary convolution and fully connected layer in the same way as
the weights get binarized, and perform matrix multiplication using
bit-wise operations (xnor and popcount). Our implementation is
also prepared to use networks that store weights and use inputs
with arbitrary bit widths as proposed by Zhou et al. [15].

The deep learning library MXNet [3] serves as a base for our
code. MXNet is a high performance and modular deep learning
library, that is written in C++. MXNet provides Bindings for other
popular programming languages like Python, R, Scala and Go, and
is used by a wide range of researchers and companies.

2 FRAMEWORK
BMXNet provides activation, convolution and fully connected lay-
ers that support quantization and binarization of input data and
weights. These layers are designed as drop-in replacements for
the corresponding MXNet variants and are called QActivation,

Conference’17, July 2017, Washington, DC, USA

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

QConvolution and QFullyConnected. They provide an additional
parameter, act bit, which controls the bit width the layers calcu-
late with.

A Python example usage of our framework in comparison to
MXNet is shown in Listing 1 and 2. We do not use binary layers for
the first and last layer in the network, as we have confirmed the
experiments of [14] showing that this greatly decreases accuracy.
The standard block structure of a BNN in BMXNet is conducted as:
QActivation-QConv/QFC-BatchNorm-Pooling as shown in Listing 2.

Listing 1: LeNet

def get_lenet () :

data = mx . symbol . Variable ( ' data ')
# first conv layer
conv1 = mx . sym . Convolution (...)
tanh1 = mx . sym . Activation (...)
pool1 = mx . sym . Pooling (...)
bn1 = mx . sym . BatchNorm (...)
# second conv layer
conv2 = mx . sym . Convolution (...)
bn2 = mx . sym . BatchNorm (...)
tanh2 = mx . sym . Activation (...)
pool2 = mx . sym . Pooling (...)
# first fullc layer
flatten = mx . sym . Flatten (...)
fc1 = mx . symbol . FullyConnected (..)
bn3 = mx . sym . BatchNorm (...)
tanh3 = mx . sym . Activation (...)
# second fullc
fc2 = mx . sym . FullyConnected (...)
# softmax loss
lenet = mx . sym . SoftmaxOutput (...)
return lenet

Listing 2: Binary LeNet

def get_binary_lenet () :

data = mx . symbol . Variable ( ' data ')
# first conv layer
conv1 = mx . sym . Convolution (...)
tanh1 = mx . sym . Activation (...)
pool1 = mx . sym . Pooling (...)
bn1 = mx . sym . BatchNorm (...)
# second conv layer
ba1 = mx.sym.QActivation(...)
conv2 = mx.sym.QConvolution(...)
bn2 = mx . sym . BatchNorm (...)
pool2 = mx . sym . Pooling (...)
# first fullc layer
flatten = mx . sym . Flatten (...)
ba2 = mx.symbol.QActivation(..)
fc1 = mx.symbol.QFullyConnected(..)
bn3 = mx . sym . BatchNorm (...)
tanh3 = mx . sym . Activation (...)
# second fullc
fc2 = mx . sym . FullyConnected (...)
# softmax loss
lenet = mx . sym . SoftmaxOutput (...)
return lenet

2.1 Quantization
The quantization on bit widths ranging from 2 to 31 bit is available
for experiments with training and prediction, using low precision
weights and inputs. The quantized data is still stored in the default
32 bit float values and the standard MXNet dot product operations
are applied.

We quantize the weights following the linear quantization as
shown by [15]. Equation 1 will quantize a real number input in the
range [0, 1] to a number in the same range representable with a bit
width of k bit.

quantize(input, k) = round((2k − 1) ∗ input)

(1)

2k − 1

2.2 Binarization
The extreme case of quantizing to 1 bit wide values is the bina-
rization. Working with binarized weights and input data allows
for highly performant matrix multiplications by utilizing the CPU
instructions xnor and popcount.

2.2.1 Dot Product with xnor and popcount. Fully connected
and convolutional layers heavily rely on dot products of matri-
ces, which in turn require massive floating point operations. Most
modern CPUs are optimized for these types of operations. But
especially for real time applications on embedded or less powerful
devices (cell phones, IoT devices) there are optimizations that im-
prove performance, reduce memory and I/O footprint, and lower
power consumption [2].

To calculate the dot product of two binary matrices A ◦ B, no
multiplication operation is required. The element-wise multiplica-
tion and summation of each row of A with each column of B can
be approximated by first combining them with the xnor operation
and then counting the number of bits set to 1 in the result which is
the population count [14].

Listing 3: Baseline xnor GEMM Kernel

void xnor_gemm_baseline_no_omp ( int M , int N , int K ,

BINARY_WORD *A , int lda ,
BINARY_WORD *B , int ldb ,
float *C , int ldc ){

for ( int m = 0; m < M; ++ m) {

for ( int k = 0; k < K; k ++) {

BINARY_WORD A_PART = A[m* lda +k ];
for ( int n = 0; n < N; ++ n) {

C[m* ldc +n] += __builtin_popcountl (∼( A_PART ∧ B[k* ldb +n ]) );

}

}

}

}

We can approximate the multiplication and addition of two times
64 matrix elements in just a few processor instructions on x64
CPUs and two times 32 elements on x86 and ARMv7 processors.
This is enabled by hardware support for the xnor and popcount
operations. They translate directly into a single assembly command.
The population count instruction is available on x86 and x64 CPUs
supporting SSE4.2, while on ARM architecture it is included in the
NEON instruction set.

An unoptimized GEMM (General Matrix Multiplication) imple-
mentation utilizing these instructions is shown in Listing 3. The
compiler intrinsic builtin popcount is supported by both gcc
and clang compilers and translates into the machine instruction on
supported hardware. BINARY WORD is the packed data type storing
32 (x86 and ARMv7) or 64 (x64) matrix elements, each represented
by a single bit.

We have implemented several optimized versions of the xnor
GEMM kernel. We leverage processor cache hierarchies by blocking
and packing the data, use unrolling techniques and OpenMP for
parallelization.

2.2.2 Training. We carefully designed the binarized layers (uti-
lizing xnor and population count operations) to exactly match the
output of the built-in layers of MXNet (computing with BLAS dot
product operations) when limiting those to the discrete values -1
and +1. This enables massively parallel training with GPU support
by utilizing CuDNN on high performance clusters. The trained
model can then be used on less powerful devices where the forward
pass for prediction will calculate the dot product with the xnor and
popcount operations instead of multiplication and addition.

(m×n)

◦ B

The possible values after performing an xnor and popcount
are in the range [0, +n] with the
matrix multiplication A
step size 1, whereas a normal dot product of matrices limited to
discrete values -1 and +1 will be in the range [−n, +n] with the step
size 2. To enable GPU supported training we modify the training
process. After calculation of the dot product we map the result back
to the range [0, +n] to match the xnor dot product, as in Equation 2.

(n×k )

outputx nor dot

= outputdot

+ n

2

(2)

BMXNet: An Open-Source Binary Neural Network
Implementation Based on MXNet

Conference’17, July 2017, Washington, DC, USA

Figure 1: Processing time comparison of GEMM methods

Figure 3: Speedup comparison based on naive gemm method
by varying kernel size of the convolution layer. The input
channel size, batch size and filter number are set to 256, 200
and 64 respectively.

In the current deep neural network implementations, most of
the fully connected and convolution layers are implemented us-
ing GEMM. According to the evaluation result from [9], over 90%
of the processing time of the Caffe-AlexNet [10] model is spent
on such layers. We thus conducted experiments to measure the
efficiency of different GEMM methods. The measurements were
performed within a convolution layer, where we fixed the parame-
ters as follows: filter number=64, kernel size=5×5, batch size=200,
and the matrix sizes M, N, K are 64, 12800, kernelw × kernelh ×
inputChannelSize, respectively. Figure 1 shows the evaluation re-
sults. The colored columns denote the processing time in millisec-
onds across varying input channel size; xnor 32 and xnor 64 de-
note the xnor gemm operator in 32 bit and 64 bit; xnor 64 omp
denotes the 64 bit xnor gemm accelerated by using the OpenMP2
parallel programming library; binarize input and xnor 64 omp
further accumulated the processing time of input data binariza-
tion. From the results we can determine that xnor 64 omp achieved
about 50× and 125× acceleration in comparison to Cblas(Atlas3)
and naive gemm kernel, respectively. By accumulating the bina-
rization time of input data we still achieved about 13× acceleration
compared with Cblas method.

Figures 2 and 3 illustrate the speedup achieved by varying filter

number and kernel size based on the naive gemm method.

3.2 Classification Accuracy
We further conduted experiments with our BNNs on the MNIST,
CIFAR-10 and ImageNet datasets. The experiments were performed
on a work station which has an Intel(R) Core(TM) i7-6900K CPU,
64 GB RAM and 4 TITAN X (Pascal) GPUs.

By following the same strategy as applied in [7, 14, 15] we al-
ways avoid binarization at the first convolution layer and the last
fully connected layer. Table 1 depicts the classification test ac-
curacy of our binary, as well as full precision models trained on
MNIST and CIFAR-10. The table shows that the size of binary
models is significantly reduced, while the accuracy is still competi-
tive. Table 2 demonstrates the validation accuracy of our binary,

Figure 2: Speedup comparison based on naive gemm method
by varying filter number of the convolution layer. The input
channel size is fixed to 256 while the kernel size and batch
size are set to 5×5 and 200 respectively.

2.2.3 Model Converter. After training a network with BMXNet,
the weights are stored in 32 bit float variables. This is also the case
for networks trained with a bit width of 1 bit. We provide a model
converter1 that reads in a binary trained model file and packs the
weights of QConvolution and QFullyConnected layers. After this
conversion only 1 bit of storage and runtime memory is used per
weight. A ResNet-18 network with full precision weights has a size
of 44.7MB. The conversion with our model converter achieves 29×
compression resulting in a file size of 1.5MB (cf. Table 1).

3 EVALUATION
In this section we report the evaluation results of both efficiency
analysis and classification accuracy over MNIST [13], CIFAR-10 [11]
and ImageNet [5] datasets using BMXNet.

3.1 Efficiency Analysis
All the experiments in this section have been performed on Ubuntu
16.04/64-bit platform with Intel 2.50GHz × 4 CPU with popcnt in-
struction (SSE4.2) and 8G RAM.

1https://github.com/hpi-xnor/BMXNet/tree/master/smd hpi/tools/model-converter

2http://www.openmp.org/
3http://math-atlas.sourceforge.net/

Conference’17, July 2017, Washington, DC, USA

Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel

Architecture Test Accuracy (Binary/Full Precision) Model Size (Binary/Full Precision)

MNIST
CIFAR-10

Lenet
ResNet-18

0.97/0.99
0.86/0.90

206kB/4.6MB
1.5MB/44.7MB

Table 1: Classification test accuracy of binary and full precision models trained on MNIST and CIFAR-10 dataset. No pre-
training or data augmentation was used.

GEMM computation. In order to demonstrate the applicability we
developed sample applications for image classification on Android
as well as iOS using a binarized ResNet-18 model. Source code, doc-
umentation, pre-trained models and sample projects are published
on GitHub [1].

REFERENCES
[1] 2017. BMXNet: an open-source binary neural network library. https://github.

com/hpi-xnor. (2017).

[2] Renzo Andri, Lukas Cavigelli, Davide Rossi, and Luca Benini. 2016. YodaNN:
An ultra-low power convolutional neural network accelerator based on binary
weights. In VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on.
IEEE, 236–241.

[5]

[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible
and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
CoRR abs/1512.01274 (2015).

[4] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryCon-
nect: Training Deep Neural Networks with binary weights during propagations.
In Advances in Neural Information Processing Systems 28. 3123–3131.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A
Large-Scale Hierarchical Image Database. In CVPR09.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770–778.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized Neural Networks. In Advances in Neural Information
Processing Systems 29. 4107–4115.

[7]

[8] Google Inc. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Systems. (2015). http://tensorflow.org/ Software available from tensorflow.org.
[9] Yangqing Jia. 2014. Learning Semantic Image Representations at a Large Scale.
Ph.D. Dissertation. EECS Department, University of California, Berkeley.
[10] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolu-
tional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093
(2014).

[11] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2014. CIFAR-10 (Canadian

Institute for Advanced Research). (2014).

[12] Yann LeCun and Yoshua Bengio. 1998. The Handbook of Brain Theory and Neural
Networks. MIT Press, Cambridge, MA, USA, Chapter Convolutional Networks
for Images, Speech, and Time Series, 255–258.

[13] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database. (2010).

http://yann.lecun.com/exdb/mnist/

[14] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.
XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Net-
works. In Computer Vision - ECCV 2016. 525–542.

[15] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
2016. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
Low Bitwidth Gradients. arXiv:1606.06160 [cs] (2016).

Val-acc-top-1 Val-acc-top-5 Model Size

Full Precision
Stage
none
1st
2nd
3rd
4th
1st, 2nd
All

0.42
0.48
0.44
0.49
0.47
0.49
0.61

0.66
0.73
0.69
0.73
0.71
0.73
0.84

3.6MB
4.1MB
5.6MB
11.3MB
36MB
6.2MB
47MB

Table 2: Classification test accuracy of binary, partially
binarized and full precision models trained on ImageNet.
ResNet-18 architecture was used in the experiment.

partially-binarized and full precision models trained on ImageNet.
The ResNet implementation in MXNet consists of 4 ResUnit stages,
we thus also report the results of a partially-binarized model with
specific full precision stages. The partially-binarized model with the
first full precision stage shows a great accuracy improvement with
very minor model size increase, compared to the fully binarized
model.

4 EXAMPLE APPLICATIONS
4.1 Python Scripts
The BMXNet repository [1] contains python scripts that can train
and validate binarized neural networks. The script smd hpi/examples/
binary mnist/mnist cnn.py will train a binary LeNet [12] with
the MNIST [13] data set. To train a network with the CIFAR-
10 [11] or ImageNet [5] data set there is a python script based
on the ResNet-18 [6] architecture. Find it at smd hpi/examples/
binary-imagenet1k/train cifar10/train [dataset].py. For
further information and example invocation see the corresponding
README.md

4.2 Mobile Applications

4.2.1

Image Classification. The Android application android

-image-classification and iOS application ios-image-
classification can classify the live camera feed based on a bina-
rized ResNet-18 model trained on the ImageNet dataset.

4.2.2 Handwritten Digit Detection. The iOS application ios-mnist

can classify handwritten numbers based on a binarized LeNet model
trained on the MNIST dataset.

5 CONCLUSION
We introduced BMXNet, an open-source binary neural network
implementation in C/C++ based on MXNet. The evaluation results
show up to 29× model size saving and much more efficient xnor

