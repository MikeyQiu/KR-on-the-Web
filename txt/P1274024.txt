Merging and Evolution: Improving Convolutional
Neural Networks for Mobile Applications

Zheng Qin, Zhaoning Zhang∗, Shiqing Zhang, Hao Yu, Yuxing Peng
Science and Technology on Parallel and Distributed Laboratory
National University of Defense Technology
Changsha, China
qinzheng12@nudt.edu.cn; zzningxp@gmail.com; {zhangshiqing12, yuhao12, pengyuxing}@nudt.edu.cn

8
1
0
2
 
r
a

M
 
4
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
2
1
9
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Compact neural networks are inclined to exploit
“sparsely-connected” convolutions such as depthwise convolution
and group convolution for employment in mobile applications.
Compared with standard “fully-connected” convolutions, these
convolutions are more computationally economical. However,
“sparsely-connected” convolutions block the inter-group informa-
tion exchange, which induces severe performance degradation.
To address this issue, we present two novel operations named
merging and evolution to leverage the inter-group information.
Our key idea is encoding the inter-group information with a
narrow feature map, then combining the generated features with
the original network for better representation. Taking advantage
of the proposed operations, we then introduce the Merging-and-
Evolution (ME) module, an architectural unit speciﬁcally designed
for compact networks. Finally, we propose a family of compact
neural networks called MENet based on ME modules. Extensive
experiments on ILSVRC 2012 dataset and PASCAL VOC 2007
dataset demonstrate that MENet consistently outperforms other
state-of-the-art compact networks under different computational
budgets. For instance, under the computational budget of 140
MFLOPs, MENet surpasses ShufﬂeNet by 1% and MobileNet
by 1.95% on ILSVRC 2012 top-1 accuracy, while by 2.3% and
4.1% on PASCAL VOC 2007 mAP, respectively.

Index Terms—Convolutional neural networks; deep learning;

model acceleration

I. INTRODUCTION

Convolutional neural networks (CNNs) have achieved sig-
niﬁcant progress in computer vision tasks such as image
classiﬁcation [1]–[5], object detection [6]–[9] and semantic
segmentation [10]. However, state-of-the-art CNNs require
computation at billions of FLOPs, which prevents them from
being utilized in mobile or embedded applications. For in-
stance, ResNet-101 [3], which is broadly used in detection
tasks [6], [9], has a complexity of 7.8 GFLOPs and fails to
achieve real-time detection even with a powerful GPU.

In view of the huge computational cost of modern CNNs,
compact neural networks [11]–[13] have been proposed to
deploy both accurate and efﬁcient networks on mobile or
embedded devices. Compact networks can achieve relatively
high accuracy under a tight computational budget. For bet-
ter computational efﬁciency, these networks are inclined to
utilize “sparsely-connected” convolutions such as depthwise
convolution and group convolution rather than standard “fully-
connected” convolutions. For instance, ShufﬂeNet [13] uti-

∗Corresponding author.

Fig. 1. Channel shufﬂe operation with 9 channels and 3 channel groups. Each
group in the second convolution receives only 1 channel from each group in
the ﬁrst convolution. This leads to severe inter-group information loss.

lizes a lightweight version of the bottleneck unit [3] termed
the original 3 × 3
ShufﬂeNet unit. In a ShufﬂeNet unit,
convolution is replaced with a 3 × 3 depthwise convolution,
while the 1 × 1 convolutions are substituted with pointwise
group convolutions. This modiﬁcation signiﬁcantly reduces the
computational cost, but blocks the information ﬂow between
channel groups and leads to severe performance degradation.
For this reason, ShufﬂeNet introduces the channel shufﬂe ope-
ration to enable inter-group information exchange. As shown
in Fig. 1, a channel shufﬂe operation permutes the channels
so each group in the second convolutional
layer contains
channels from every group in the ﬁrst convolutional layer.
Beneﬁting from the channel shufﬂe operation, ShufﬂeNet
achieves 65.9% top-1 accuracy on ILSVRC 2012 dataset
[14] with 140 MFLOPs, and 70.9% top-1 accuracy with 524
MFLOPs, which is state-of-the-art.

However, the channel shufﬂe operation fails to eliminate
the performance degradation and ShufﬂeNet still suffers from
the loss of inter-group information. Fig. 1 illustrates a channel
shufﬂe operation with 9 channels and 3 channel groups. Each
group in the second convolutional layer receives only 1 chan-
nel from every group in the ﬁrst convolutional layer, whereas
there are 2 other channels in each group being ignored. As
a result, a large portion of the inter-group information cannot
be leveraged. This problem is aggravated given more channel
groups. Although there are more channels in total given more
groups, the number of channels in each group is smaller, which
increases the loss of inter-group information. Consequently,

when the computational budget is relatively larger, ShufﬂeNet
architectures with more channel groups perform worse than
the narrower ones which have less groups. This indicates that
it is difﬁcult for ShufﬂeNet to gain performance increase by
increasing the number of channels directly.

To address this issue, we propose two novel operations
named merging and evolution to directly fuse features across
all channels in a group convolution and alleviate the loss of
inter-group information. For a feature map generated from a
group convolution, a merging operation aggregates the features
at the same spatial position across all channels and encodes
the inter-group information into a narrow feature map. An
evolution operation is performed afterwards to extract spatial
information from the feature map. Then, based on the proposed
operations, we introduce the Merging-and-Evolution (ME)
module, a powerful and efﬁcient architectural unit speciﬁcally
for compact networks. For computational efﬁciency, ME mo-
dules exploit depthwise convolutions and group convolutions
to reduce the computational cost. For better representation,
ME modules utilize merging and evolution operations to
leverage the inter-group information. Finally, we present a
new family of compact neural networks called MENet which
is built with ME modules. Compared with ShufﬂeNet [13],
MENet alleviates the loss of inter-group information and gains
substantial improvements as the group number increases.

We conduct extensive experiments to evaluate the effec-
tiveness of MENet. Firstly, we compare MENet with other
state-of-the-art network structures on the ILSVRC 2012 clas-
siﬁcation dataset [14]. Then, we examine the generalization
ability of MENet on the PASCAL VOC 2007 detection dataset
[15]. Experiments show that MENet consistently outperforms
other state-of-the-art compact networks under different com-
putational budgets. For instance, under a complexity of 140
MFLOPs, MENet achieves improvements of 1% over Shuf-
ﬂeNet and 1.95% over MobileNet on ILSVRC 2012 top-1
accuracy, while 2.3% and 4.1% on PASCAL VOC 2007 mAP,
respectively. Our models have been made publicly available at
https://github.com/clavichord93/MENet.

II. RELATED WORK

As deep neural networks suffer from heavy computational
cost and large model size, the inference-time compression and
acceleration of neural networks has become an attractive topic
in deep learning community. Commonly, the related work can
be categorized into four groups.

Tensor decomposition factorizes a convolution into a se-
quence of smaller convolutions with fewer parameters and
less computational cost. Jaderberg et al. [16] proposed to
decompose a k × k convolution into a k × 1 convolution
and a 1 × k convolution, reporting 4.5× speedup with 1%
accuracy loss. Denton et al. [17] proposed a method exploiting
a low-rank decomposition to estimate the original convolution.
Recently, Zhang et al. [18] proposed a method based on
generalized singular value decomposition without the need of
stochastic gradient descent, which achieved 4× speedup on
VGG-16 [2] with a graceful accuracy degradation.

Parameter quantization is proposed to utilize low-bit para-
meters in neural networks. Vanhoucke et al. [19] proposed to
use 8-bit ﬁxed-point parameters and achieved 3× speedup.
Gong et al. [20] applied k-means clustering on network
parameters and provided 20× compression with only 1% accu-
racy drop. Binarization methods [21]–[23] attempted to train
networks directly with 1-bit weights. Quantization methods
provide signiﬁcant memory savings and enormous theoretical
speedup. However, current hardware is mainly optimized for
half-/single-/double-precision computation, so it is difﬁcult for
quantization methods to achieve the theoretical speedup.

Network pruning attempts to recognize the structure redun-
dancy in network architectures and cut off the redundant
parameters. Han et al. [24] proposed a method to remove all
connections with small weights, reporting 10× reduction in
model size. Network slimming [25] applied sparsity-induced
penalty on the scaling factors in batch normalization layers
and removes the channels with small scaling factors. He et
al. [26] proposed a LASSO regression based method to prune
redundant channels, achieving 5× speedup with comparable
accuracy. Yu et al. [27] proposed a group-wise 2D-ﬁlter
pruning approach and provided 4× speedup on VGG-16.
However, iterative pruning strategy is commonly utilized in
network pruning, which slows down the training procedure.

Compact networks are designed for mobile or embedded
applications speciﬁcally. SqueezeNet [11] proposed ﬁre mo-
dules, where a 1 × 1 convolutional layer is ﬁrst applied to
“squeeze” the width of the network, followed by a layer
mixing 3 × 3 and 1 × 1 convolutional kernels to reduce
parameters. MobileNet [12] exploited depthwise separable
convolutions as its building unit, which decompose a standard
convolution into a combination of a depthwise convolution and
a pointwise convolution. ShufﬂeNet [13] utilized depthwise
convolutions and pointwise group convolutions into the bot-
tleneck unit [3], and proposed the channel shufﬂe operation to
enable inter-group information exchange. Compact networks
can be trained from the scratch, so the training procedure is
very fast. Moreover, compact networks are orthogonal to the
aforementioned methods and can be further compressed.

III. MERGING-AND-EVOLUTION NETWORKS

In this section, we ﬁrst analyze the loss of inter-group
information in ShufﬂeNet and introduce merging and evolution
operations for alleviating the performance degradation. Next,
we describe the structure of the ME module. At last, the details
about MENet architecture are introduced.

A. Merging and Evolution Operations

As ﬁgured out in Section I, ShufﬂeNet suffers from the
severe inter-group information loss. The loss of inter-group
information can be measured with the number of inter-group
connections. Speciﬁcally, for two consecutive convolutional
layers with C output channels and G channel groups, each
group contains C
G channels, and there are totally

Ntotal =

· C · (C −

) =

(1)

C
G

C 2(G − 1)
2G

1
2

connections

inter-group
“fully-
connected”. After a channel shufﬂe operation, each group in
the later convolutional layer receives C
G2 channels from every
group in the former layer, so there are

channels were

the

if

Nactual =

· C · (

−

1
2

C
G

C
G2 ) =

C 2(G − 1)
2G2

actual inter-group connections. This means a ratio of

1 −

Nactual
Ntotal

= 1 −

C 2(G − 1)/(2G2)
C 2(G − 1)/(2G)

=

G − 1
G

(2)

(3)

of the inter-group connections are lost, which induces severe
loss of inter-group information. This signiﬁcantly weakens
the representation capability and leads to serious performance
degradation. The problem is aggravated when there are more
channel groups. The ratio of the inter-group connections lost is
66.7% when there are three groups, but the number increases
to 87.5% given eight groups. This explains why ShufﬂeNet
with three groups outperforms the one with eight groups.

To address this issue, we design two operations termed
merging and evolution to leverage inter-group information. As
shown in Fig. 2, the proposed operations encode the inter-
group information with a narrow feature map, and combine it
with the original network for more discriminative features.

1) Merging Operation: The merging operation is designed
to fuse features across all channels and encode the inter-group
information into a narrow feature map. Given the feature map
X ∈ RC×H×W generated from a group convolution, a merging
transformation f : RC×H×W → R ˜C×H×W is applied to
aggregate features over all channels, where C is the number of
channels in the original feature map, H and W are the spatial
dimensions, and ˜C is the number of channels in the produced
feature map. A small ˜C is chosen to make the merging
operations computationally economical. As C is relatively
large, it is difﬁcult to integrate the spatial information without
harming the computational efﬁciency. So we aggregate only
the features on the same spatial position along all channels
in a merging operation. A single pointwise convolution is
exploited as the merging transformation, followed by a batch
normalization [28] and a ReLU activation. Formally, the output
feature map of a merging operation is calculated as

Z = δ(BN(f (X))) = δ(BN(W ∗ X))

(4)

where δ indicates the ReLU function, ∗ represents the convolu-
tion operator, and W ∈ R ˜C×C×1×1 is the convolutional kernel.
By this means, each channel in Z contains information from
every channel in the previous group convolutional layer.

2) Evolution Operation: After a merging operation, an
evolution operation is performed to obtain more discriminative
features. An evolution operation is deﬁned in two steps. In
the ﬁrst step, an evolution transformation ge : R ˜C×H×W →
R ˜C×H×W is applied to the feature map from the previous
merging operation. The number of channels is kept unchanged.
In this step, we intend to leverage more spatial information
so a 3 × 3 standard convolution is selected as the evolution
transformation, followed by a batch normalization and a ReLU

Fig. 2. Merging and evolution operations. A merging operation applies a
merging transformation and encodes the inter-group information into a narrow
feature map. An evolution operation consists of an evolution transformation
and a matching transformation and leverages spatial information.

activation. In the second step, a matching transformation
gm : R ˜C×H×W → RC×H×W is performed to match the size
of the output feature map with the original network. As in the
merging operation, a single pointwise convolution is chosen
as gm to maintain the computational efﬁciency. Another batch
normalization and a sigmoid activation are added afterwards.
The whole process is formally written as

Ze = δ(BN(ge(Z))) = δ(BN(We ∗ Z))
Zm = σ(BN(gm(Ze))) = σ(BN(Wm ∗ Ze))
= σ(BN(Wm ∗ δ(BN(We ∗ Z))))

(5)

(6)

where We ∈ R ˜C× ˜C×3×3 and Wm ∈ RC× ˜C×1×1 are the
convolutional kernels, and σ indicates the sigmoid function.

At last, the features generated from evolution operations are
regarded as neuron-wise scaling factors and combined with the
original network using an element-wise product to improve the
representation capability of the features in the network:

˜X = h(X) ◦ Zm

= h(X) ◦ σ(BN(Wm ∗ δ(BN(We ∗ Z))))

(7)

where h : RC×H×W → RC×H×W is the transformation in the
original network, and ◦ represents element-wise product. As
Zm encodes information from every channel in the previous
convolution, each channel in ˜X also contains information from
all channels. This alleviates the loss of inter-group information.

B. Merging-and-Evolution Module

Taking advantage of the proposed merging and evolution
operations, we present the Merging-and-Evolution (ME) mo-
dule, an architectural unit speciﬁcally designed for compact
neural networks.

The ME module is a variant of the conventional residual
block [3]. An ME module consists of three branches: an
identity branch, a residual branch and a fusion branch, as
illustrated from left to right in Fig.3(a). For computational
efﬁciency, the residual branch adopts a bottleneck design [3]
and exploits “sparsely-connected” convolutions. It consists of
three layers, a pointwise group convolution to squeeze the
channel dimension, a 3 × 3 depthwise convolution to leverage
spatial information, and another pointwise group convolution
to recover the channel dimension. A channel shufﬂe operation

Fig. 3. The structure of ME module. (a): Standard ME module. (b): Downsampling ME module. GConv: Group convolution. DWConv: Depthwise convolution.

(a)

(b)

[13] is applied after the ﬁrst pointwise group convolution for
inter-group information exchange. The utilization of merging
and evolution operations introduces the fusion branch. A
merging operation is performed after the channel shufﬂe, with
an evolution operation following. Then the fusion branch is
combined with the residual branch before the second pointwise
group convolutional layer. This design helps alleviate the loss
of inter-group information in the second group convolutional
layer. The merging and evolution operations are applied to
the bottleneck channels to reduce the overall computational
cost. Additionally, as described in Section III-A, the number
of channels in the fusion branch is kept small to maintain
computational efﬁciency.

For the downsampling version of ME modules, two more
modiﬁcations are performed. (i) The strides of the depthwise
convolution in the residual branch and the 3 × 3 convolution
in the fusion branch are altered to 2. (ii) Inspired by [13],
a 3 × 3 average pooling with a stride of 2 is applied in the
identity branch, and the element-wise addition is substituted
with a concatenation to combine the identity branch and the
residual branch. After a downsampling ME module, the spatial
dimensions of the feature map are halved, while the channel
dimension is doubled. Fig. 3(b) describes the structure of the
downsampling ME module.

C. MENet Architecture

Based on ME modules, we propose MENet, a new family of
compact neural networks. The overall architecture of MENet
for ImageNet classiﬁcation is demonstrated in Table I.

MENet begins with a 3 × 3 convolutional layer and a max
pooling layer, both with strides of 2. A batch normalization
and a ReLU activation are applied after the convolutional layer.
These two layers perform 4× downsampling to reduce the
overall computational cost. Then there follow a sequence of

ME modules, which are grouped into three stages (Stage 2 to
4). In each stage, the ﬁrst building block is a downsampling
ME module, while the rest building blocks are standard ME
modules. The number of output channels is kept the same
within a stage and is doubled in the next stage. Furthermore,
the number of bottleneck channels in the residual branch is set
to 1/4 of the output channels in the same ME module, and we
do not apply group convolution on the ﬁrst pointwise layer in
Stage 2. We build MENet with three group numbers g: g = 3,
g = 4 and g = 8. Increasing the group number aggravates the
connection sparsity in the residual branch, but contributes to
wider feature maps. The inﬂuence of the group number on the
performance of MENet is discussed in the next section.

We furthermore introduce three hyper-parameters for cus-
tomizing MENet to ﬁt different computational budgets. The
ﬁrst two hyper-parameters are the fusion width k and the
expansion factor α, which control the complexity of the fusion
branch. The fusion width is deﬁned as the number of channels
in the fusion branch of Stage 2, and the expansion factor
represents the ratio of the channels in the fusion branch
between two consecutive stages. The number of channels in
the fusion branch of Stage i (i ≥ 2) is calculated as αi−2k.
We ﬁgure that intuitively it is beneﬁcial for generating more
discriminative features to have wider fusion branches, but it
also leads to more computational cost. The effects of the fusion
width and the expansion factor on the performance of MENet
is discussed in the next section. The third hyper-parameter is
the residual width w, which is deﬁned as the number of output
channels in the residual branch of Stage 2. The residual width
controls the computational cost in the residual branch.

Finally, we deﬁne a notation “w-MENet-k × α” to represent
a network with a residual width w, a fusion width k and an
expansion factor α. For example, the network in Table I with
g = 3 can be denoted as “228-MENet-12 × 1”.

TABLE I
MENET ARCHITECTURE FOR IMAGENET UNDER THE COMPUTATIONAL BUDGET OF 140 MFLOPS

Stage

Stage 1

Output Size
224 × 224
112 × 112
56 × 56

Stage 2

28 × 28

Stage 3

14 × 14

Stage 4

Classiﬁer
FLOPs

7 × 7

1 × 1

228-MENet-12×1 (g = 3)

352-MENet-12×1 (g = 8)

256-MENet-12×1 (g = 4)
Image
3 × 3 conv, 24, /2
3 × 3 max pool, /2
ME module, 256, /2
ME module, 256, ×3
ME module, 512, /2
ME module, 512, ×7
ME module, 1024, /2
ME module, 1024, ×3
global average pool, 1000-d fc, softmax
140 × 106

ME module, 352, /2
ME module, 352, ×3
ME module, 704, /2
ME module, 704, ×7
ME module, 1408, /2
ME module, 1408, ×3

144 × 106

ME module, 228, /2
ME module, 228, ×3
ME module, 456, /2
ME module, 456, ×7
ME module, 912, /2
ME module, 912, ×3

144 × 106

The number after the layer/module type is the number of output channels. “×3” and “×7” indicate the ME module repeats
3 or 7 times respectively. “/2” represents the stride of the layer is 2. The ME modules with “/2” perform downsampling.

TABLE II
ILSVRC 2012 ACCURACY (%) COMPARISON WITH STATE-OF-THE-ART
NETWORK STRUCTURES

TABLE IV
ILSVRC 2012 ACCURACY (%) COMPARISON WITH SHUFFLENET AND
MOBILENET

Models
VGG-16 [2]
GoogLeNet [4]
456-MENet-24×1 (g = 3, ours)
ResNet-15 [3]
Xception-15 [29]
RexNeXt-15 [30]
352-MENet-12×1 (g = 8, ours)
ResNet-15 [3]
Xception-15 [29]
RexNeXt-15 [30]
108-MENet-8×1 (g = 3, ours)

MFLOPs Top-1 Acc. Top-5 Acc.
71.5
69.8
71.6
61.3
64.9
65.7
66.7
51.1
53.9
53.7
56.1

15300
1550
551
140
140
140
144
38
38
38
38

89.8
89.6
90.2
-
-
-
86.9
-
-
-
79.2

The results that surpasses all competing networks are bold. Larger number
in top-1 and top-5 accuracy represents better performance.

Models
1× MobileNet-224 [12]
ShufﬂeNet 2× (g = 3) [13]
456-MENet-24×1 (g = 3, ours)
0.75× MobileNet-224 [12]
ShufﬂeNet 1.5× (g = 3) [13]
348-MENet-12×1 (g = 3, ours)
0.5× MobileNet-224 [12]
ShufﬂeNet 1× (g = 3) [13]
228-MENet-12×1 (g = 3, ours)
352-MENet-12×1 (g = 8, ours)
0.25× MobileNet-224 [12]
ShufﬂeNet 0.5× (g = 3) [13]
108-MENet-8×1 (g = 3, ours)

MFLOPs Top-1 Acc. Top-5 Acc.
70.73
70.79
71.60
68.60
68.85
69.91
64.74
65.69
66.43
66.69
53.71
55.40
56.08

89.72
89.80
90.07
88.33
88.41
89.08
85.63
86.29
86.72
86.92
77.04
78.70
79.24

569
524
551
325
292
299
149
137
144
144
41
38
38

TABLE III
ILSVRC 2012 ACCURACY (%) COMPARISON WITH SHUFFLENET

Models
ShufﬂeNet 1× (g = 3) [13]
ShufﬂeNet 1× (g = 4) [13]
ShufﬂeNet 1× (g = 8) [13]
ShufﬂeNet 1× (g = 3, re-impl.) [13]
ShufﬂeNet 1× (g = 4, re-impl.) [13]
ShufﬂeNet 1× (g = 8, re-impl.) [13]
228-MENet-12×1 (g = 3, ours)
256-MENet-12×1 (g = 4, ours)
352-MENet-12×1 (g = 8, ours)

MFLOPs Top-1 Acc. Top-5 Acc.
65.9
65.7
65.3
65.7
65.5
65.4
66.4
66.6
66.7

-
-
-
86.3
86.2
86.3
86.7
86.7
86.9

137
134
138
137
134
138
144
140
144

IV. EXPERIMENTS

We conduct extensive experiments to examine the effective-
ness of MENet with two benchmarks. We ﬁrst evaluate MENet
on the ILSVRC 2012 classiﬁcation dataset [14] and compare
MENet with other state-of-the-art networks. The inﬂuence
of different model choices is then investigated. At last, we
conduct experiments on the PASCAL VOC 2007 detection
dataset [15] to examine the generalization ability of MENet.

A. ImageNet Classiﬁcation

The ILSVRC 2012 dataset is composed of a training set
of 1.28 million images and a validation set of 50,000 images,
which are categorized into 1,000 classes. We train the networks

on the training set and report the top-1 and the top-5 accuracy
rates on the validation set using center-crop evaluations.

1) Implementation Details: All our experiments are con-
ducted using PyTorch [31] with four GPUs. We utilize syn-
chronous stochastic gradient descent to train the models for
120 epochs with a batch size of 256 and a momentum of
0.9. Following [13], a relatively small weight decay of 4e-5 is
used to avoid underﬁtting. The learning rate starts from 0.1,
and is divided by 10 every 30 epochs. Because our models
are relatively small, we use less aggressive multi-scale data
augmentation. Color jittering is not adopted because we ﬁnd it
can lead to underﬁtting. On evaluation, each validation image
is ﬁrst resized with its shorter edge to 256 pixels, and then
evaluated using the center 224 × 224 pixels crop.

2) Comparison with Other State-of-the-art Networks:
Table II demonstrates the comparison of MENet and some
state-of-the-art network structures on ILSVRC 2012 dataset.
We ﬁrst compare MENet with two popular networks,
GoogLeNet [4] and VGG-16 [2]. GoogLeNet provides 69.8%
top-1 accuracy and 89.6% top-5 accuracy, while VGG-16
produces remarkably better top-1 accuracy of 71.5%. However,
they are all computationally intensive. In comparison, 456-
MENet-24×1 achieves 71.6% top-1 accuracy and 90.2% top-5
accuracy under a complexity of about 550 MFLOPs. MENet
signiﬁcantly surpasses GoogLeNet by 1.8% on top-1 accuracy

TABLE V
ILSVRC 2012 ACCURACY (%) OF DIFFERENT FUSION WIDTHS

Models
228-MENet-10×1 (g = 3)
228-MENet-12×1 (g = 3)
228-MENet-14×1 (g = 3)
228-MENet-16×1 (g = 3)

MFLOPs
140
144
148
153

Top-1 Acc.
66.37
66.43
66.56
66.86

Top-5 Acc.
86.70
86.72
86.95
87.19

TABLE VI
ILSVRC 2012 ACCURACY (%) OF DIFFERENT EXPANSION RATES

Models
228-MENet-12×1 (g = 3)
228-MENet-12×1.5 (g = 3)
228-MENet-12×2 (g = 3)
228-MENet-12×2.5 (g = 3)

MFLOPs
144
152
163
179

Top-1 Acc.
66.43
66.71
67.25
67.51

Top-5 Acc.
86.72
87.14
87.30
87.66

TABLE VII
ILSVRC 2012 ACCURACY (%) OF ELEMENT-WISE PRODUCT AND
ELEMENT-WISE ADDITION

Models
228-MENet-12×1 (g = 3)
228-MENet-12×1 (add, g = 3)
256-MENet-12×1 (g = 4)
256-MENet-12×1 (add, g = 4)

66.43
66.27
66.53
65.48

86.77
86.25
86.82
86.30

Top-1 Acc. (%) Top-5 Acc. (%)

in Section III-A,

In ShufﬂeNet, the top-1 accuracy decreases as the number
the
of groups increases. As ﬁgured out
ratio of the inter-group connections lost is G−1
G when there
are G channel groups. Increasing G makes more inter-group
connections lost, which aggravates the loss of inter-group in-
formation. More speciﬁcally, although there are more channels
in total in the residual branch when the group number is larger,
the number of channels within each channel group become
smaller, which harms the representation capability. However,
the results are opposite for MENet: the classiﬁcation accuracy
rises given more channel groups. This is another advantage that
MENet brings: it can gain accuracy improvement by directly
increasing the width of the network and the number of groups.
The merging and evolution operations fuse the features from
all channels simultaneously, thus alleviates the loss of inter-
group information. Consequently, MENet beneﬁts from the
wider feature maps and generates more discriminative features.
These improvements are consistent with our initial motivation
to design ME modules.

We further compare the three compact networks under
four computational budgets. The results are demonstrated in
Table IV. The number of output channels in the ﬁrst convolu-
tion in MENet is adjusted to ﬁt the computational budget.
According to the table, MENet signiﬁcantly outperforms Shuf-
ﬂeNet and MobileNet under all the computational budgets.
Under a budget of 140 MFLOPs, MENet surpasses ShufﬂeNet
by 0.74% on top-1 accuracy with the same group number
(g = 3), and by 1% with more groups (g = 8). Meanwhile,
MENet surpasses MobileNet by 1.95%. We do not tune too
much on group numbers for MENet but simply set g = 3 under
other computational budgets. For smaller networks with only
40 MFLOPs, MENet provides improvements of 0.68% over

Fig. 4. Comparison of MENet with other network structures. MENet surpasses
all competing structures under all four computational budgets.

with 2.8× fewer FLOPs, and slightly outperforms VGG-16
(∼0.1%) with 27× fewer FLOPs.

We further compare the ME module with the building
structures of three state-of-the-art networks, ResNet [3], Xcep-
tion [29] and ResNeXt [30]. Following [13], we replace
the ME modules with other structures in the architecture
shown in Table I and adapting the number of channels to
the computational budgets. These networks are referred to as
ResNet-15, Xception-15 and RexNeXt-151 and use the results
reported in [13] for comparison. As shown in Table II, 352-
MENet-12×1 achieves signiﬁcant improvements of 5.4% over
ResNet-15, 1.8% over Xception-15 and 1% over ResNeXt-
15 under a complexity of 140 MFLOPs, while 108-MENet-
8×1 achieves improvements of 5.0%, 2.2% and 2.3% under 40
MFLOPs, respectively. These improvements have proven the
effectiveness of ME modules in building compact networks.
3) Comparison with Other Compact Networks: We also
compare the performance of MENet with two state-of-the-art
compact networks: ShufﬂeNet [13] and MobileNet [12]. For
fair comparison, we re-implement ShufﬂeNet and MobileNet
with the same settings as described in Section IV-A1.

Table III demonstrates the comparison of the results between
MENet and ShufﬂeNet with different group numbers. When
the group number is kept the same, MENet surpasses Shuf-
ﬂeNet by a large margin. Considering there are fewer channels
than in ShufﬂeNet, we
in the residual branch in MENet
attribute this improvement to the effectiveness of the proposed
merging and evolution operations. Although ShufﬂeNet has
more channels, it suffers from the loss of inter-group infor-
mation. On the other side, MENet leverages the inter-group
information through the merging and evolution operations.
Consequently, MENet generates more discriminative features
than ShufﬂeNet and overcomes the performance degradation.
This is the ﬁrst advantage of MENet: it can achieve better
performance with fewer channels.

1The number 15 indicates the number of building blocks in the network.

TABLE VIII
COMPARISON OF MAP (%) AND AP (%) ON PASCAL VOC 2007 TEST SET

mAP areo bike bird boat bottle bus
Backbone
0.5× MobileNet-224
54.8
ShufﬂeNet 1× (g = 3)
56.6
352-MENet-12×1 (g = 8) 58.9
1× MobileNet-224
62.4
ShufﬂeNet 2× (g = 3)
63.5
456-MENet-24×1 (g = 3) 65.5

59.4 65.4 52.7 36.3
57.4 64.6 55.4 37.7
64.2 64.1 59.0 44.6
65.1 69.0 59.9 51.4
68.4 69.7 58.8 49.7
69.2 72.6 66.5 52.1

cat
car
57.2 71.3 64.4
60.1 72.1 66.8
64.1 73.8 70.0
66.2 75.7 76.7
70.5 76.4 77.0
70.8 79.4 78.9

30.1
30.4
34.6
40.1
38.2
42.3

chair cow table dog horse mbike person plant sheep sofa train
31.1
29.9
32.8
40.0
38.7
41.3

64.4 47.5 62.5
64.0 50.3 65.7
69.1 52.5 64.3
69.3 52.9 72.9
73.0 54.7 73.8
75.7 56.3 78.0

tv
51.8 61.8 54.9
50.0 65.7 60.0
50.6 66.9 60.5
58.1 70.0 66.8
60.2 71.4 64.7
58.2 72.2 67.4

70.8
73.5
75.3
75.2
79.0
77.4

64.2
66.9
67.7
71.2
72.0
74.4

27.6
34.2
35.3
35.3
36.1
38.0

68.8
68.6
69.4
68.3
71.1
71.9

53.2
58.5
59.5
64.8
65.8
66.8

ShufﬂeNet and 2.37% over MobileNet. For larger networks
under a complexity of 300 MFLOPs, MENet performs 1.06%
better than ShufﬂeNet and 1.31% better than MobileNet. When
the complexity is 550 MFLOPs, MENet surpasses ShufﬂeNet
and MobileNet by 0.81% and 0.87% respectively. Similar
results are observed on the top-5 accuracy. More detailed
comparison results are illustrated in Fig. 4. These results have
proven that MENet has stronger representation capability and
is both efﬁcient and accurate for various scenarios.

4) Model Choices: We furthermore conduct experiments to
examine the inﬂuences of several design choices on the per-
formance of MENet, including the fusion width, the expansion
rate, and the function which combines the fusion branch and
the residual branch.

Fusion Width. The fusion width is the hyper-parameter
which controls the initial number of channels in the fusion
branch. We evaluate the effects of the fusion width using four
models: 228-MENet-10×1, 228-MENet-12×1, 228-MENet-
14×1 and 228-MENet-16×1, all with g = 3. Table V shows
the comparison of these networks. Substantial improvements
in accuracy are observed as the fusion width increases. In
ME modules, we set the fusion branch to be relatively narrow
for computational efﬁciency. This limits the representation
capability of the features generated from the fusion branch.
Increasing the fusion width improves the information capacity
of the fusion branch, which allows more inter-group informa-
tion to be encoded and improves the representation capability.
Expansion Rate. The expansion rate controls the “growth”
of the channels in the fusion branch between stages. We
also select four MENet models to examine the effect of the
expansion rate: 228-MENet-12×1, 228-MENet-12×1.5, 228-
MENet-12×2, 228-MENet-12×2.5, all with g = 3. The results
are shown in Table VI. It is observed that the networks with
larger expansion rates are inclined to have higher accuracy.
The model with an expansion rate of 2.5 achieves an im-
provement above 1% on top-1 accuracy over the model whose
expansion rate is 1. It is conjectured that as the width of the
residual branch increases from stage to stage, the inter-group
information becomes increasingly complicated. This makes it
difﬁcult to encode all the information within a ﬁxed number
of channels in the fusion branch for all stages. By applying
a large expansion rate, different number of channels are used
to fuse the features in each stage, which helps improve the
representation capability in the later stages.

Element-wise Product vs. Element-wise Addition. It is a
conventional practice to learn residual information (element-
wise addition) in state-of-the-art deep networks [3], [5], [30].

However, we choose to learn neuron-wise scaling information
(element-wise product) instead in MENet. We evaluate the
effects of these two choices using two MENet models with
different group numbers (g = 3 and g = 4). For the
networks using element-wise addition, we simply make two
modiﬁcations: (i) The element-wise product is replaced by an
element-wise addition. (ii) The sigmoid activation after the
second pointwise convolution in the fusion branch is removed.
The results are demonstrated in Table VII. It is clear that
learning scaling information signiﬁcantly outperforms residual
information. The model with element-wise product is 0.16%
better when g = 3, and 1.05% better when g = 4. Notice
that the model learning residual information provides a worse
result than its ShufﬂeNet counterpart when g = 4. These
results indicate that residual information is not effective for
inter-group feature fusion. This difference may be potentially
induced by the narrow feature maps in the fusion branch,
which cannot encode adequate residual information. We are
planning to further examine this in our future work.

B. Object Detection on PASCAL VOC

To investigate the generalization ability of MENet, we
conduct comparative experiments on PASCAL VOC 2007
detection dataset [15]. PASCAL VOC 2007 dataset consists
of about 10,000 images split into three (train/val/test) sets.
We train the models on VOC 2007 trainval set and report the
single-model results on VOC 2007 test set.

We adopt Faster R-CNN [6] detection pipeline and com-
pare the performance of MENet, ShufﬂeNet and MobileNet
on 600× resolution under two computational budgets (140
MFLOPs and 550 MFLOPs). The pre-trained models on
ILSVRC 2012 dataset are used for transfer learning. For the
MobileNet-based detectors, we use the ﬁrst 28 layers as the
R-CNN base network and the remaining 4 layers as the R-
CNN subnet. For the ShufﬂeNet-based and the MENet-based
detectors, the ﬁrst three stages are used as the base network,
and the last stage is used as the R-CNN subnet. All strides in
R-CNN subnets are set 1 to obtain larger feature maps. RoI
align [9] is used to encode RoIs instead of RoI pooling [32].
During testing, 300 region proposals are sent to the R-CNN
subnet to generate the ﬁnal predictions.

Table VIII demonstrates the comparison of the three com-
pact networks on VOC 2007 test set. According to the results,
MENet signiﬁcantly outperforms MobileNet and ShufﬂeNet
under both computational budgets. Under the computational
budget of 140 MFLOPs, the MENet-based detector achieves
the mAP of 58.9%, while the mAP of the ShufﬂeNet-based

and the MobileNet-based detectors is 56.6% and 54.8%,
respectively. MENet achieves improvements of 2.3% mAP
over ShufﬂeNet and 4.1% over MobileNet. More speciﬁcally,
MENet provides better results on most classes, with the
improvements from 0.5% (tv) to 6.9% (boat). On the classes
which are difﬁcult for ShufﬂeNet and MobileNet, such as boat,
bottle, table and plant, the MENet-based detector increases
the AP by 6.9%, 4.2%, 2.2% and 1.1%, respectively. Under
a complexity of 550 MFLOPs,
the MENet-based detector
surpasses the ShufﬂeNet-based one by 2% and the MobileNet-
based one by 3.1% on mAP. Additionally, MENet also out-
performs ShufﬂeNet and MobileNet on single-class results.
These results have proven that the proposed MENet has strong
generalization ability and can beneﬁt various tasks.

V. CONCLUSION

In this paper, we propose two novel operations, merging and
evolution, to perform feature fusion across all channels in a
group convolution and alleviate the performance degradation
induced by the loss of inter-group information. Based on the
proposed operations, we introduce an architectural unit named
ME module specially designed for compact networks. Finally,
we propose MENet, a family of compact neural networks.
Compared with ShufﬂeNet, the proposed MENet leverages
inter-group information and generates more discriminative
features. Extensive experiments show that MENet consistently
outperforms other state-of-the-art compact neural networks
under different computational budgets. Experiments on object
detection show that MENet has strong generalization ability
for transfer learning. For future work, we consider to further
evaluate MENet on other tasks such as semantic segmentation.

ACKNOWLEDGMENT

This work is supported by the National Key Research and

Development Program of China (2016YFB1000100).

REFERENCES

[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural
information processing systems, 2012, pp. 1097–1105.

[2] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2015, pp. 1–9.

[5] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4,
inception-resnet and the impact of residual connections on learning.” in
AAAI, 2017, pp. 4278–4284.

[6] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in Advances in neural
information processing systems, 2015, pp. 91–99.

[7] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg, “Ssd: Single shot multibox detector,” in European conference on
computer vision. Springer, 2016, pp. 21–37.

[8] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” arXiv

[9] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” arXiv

preprint arXiv:1612.08242, 2016.

preprint arXiv:1703.06870, 2017.

[10] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[11] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and < 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[12] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.

[13] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely efﬁ-
cient convolutional neural network for mobile devices,” arXiv preprint
arXiv:1707.01083, 2017.

[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International Journal of Computer
Vision, vol. 115, no. 3, pp. 211–252, 2015.

[15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.

[16] M. Jaderberg, A. Vedaldi, and A. Zisserman, “Speeding up convo-
lutional neural networks with low rank expansions,” arXiv preprint
arXiv:1405.3866, 2014.

[17] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus,
“Exploiting linear structure within convolutional networks for efﬁcient
evaluation,” in Advances in Neural Information Processing Systems,
2014, pp. 1269–1277.

[18] X. Zhang, J. Zou, K. He, and J. Sun, “Accelerating very deep convolu-
tional networks for classiﬁcation and detection,” IEEE transactions on
pattern analysis and machine intelligence, vol. 38, no. 10, pp. 1943–
1955, 2016.

[19] V. Vanhoucke, A. Senior, and M. Z. Mao, “Improving the speed of neural
networks on cpus,” in Proc. Deep Learning and Unsupervised Feature
Learning NIPS Workshop, vol. 1, 2011, p. 4.

[20] Y. Gong, L. Liu, M. Yang, and L. Bourdev, “Compressing deep
convolutional networks using vector quantization,” arXiv preprint
arXiv:1412.6115, 2014.

[21] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Ben-
gio, “Binarized neural networks: Training deep neural networks with
-1,” arXiv preprint
weights and activations constrained to +1 or
arXiv:1602.02830, 2016.

[22] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-net:
Training low bitwidth convolutional neural networks with low bitwidth
gradients,” arXiv preprint arXiv:1606.06160, 2016.

[23] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks,” in
European Conference on Computer Vision.
Springer, 2016, pp. 525–
542.

[24] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights
and connections for efﬁcient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.

[25] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning
efﬁcient convolutional networks through network slimming,” arxiv
preprint, vol. 1708, 2017.

[26] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very
deep neural networks,” arXiv preprint arXiv:1707.06168, 2017.
[27] N. Yu, S. Qiu, X. Hu, and J. Li, “Accelerating convolutional neural
networks by group-wise 2d-ﬁlter pruning,” in Neural Networks (IJCNN),
2017 International Joint Conference on.
IEEE, 2017, pp. 2502–2509.
[28] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in International
Conference on Machine Learning, 2015, pp. 448–456.

[29] F. Chollet, “Xception: Deep learning with depthwise separable convolu-

tions,” arXiv preprint arXiv:1610.02357, 2016.

[30] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
IEEE, 2017, pp.
5987–5995.

[31] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like
environment for machine learning,” in BigLearn, NIPS Workshop, no.
EPFL-CONF-192376, 2011.

[32] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international

conference on computer vision, 2015, pp. 1440–1448.

