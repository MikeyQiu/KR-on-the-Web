Large-scale Cloze Test Dataset Created by Teachers

Qizhe Xie∗ , Guokun Lai∗ , Zihang Dai, Eduard Hovy
Language Technologies Institute, Carnegie Melon University
{qizhex, guokun, dzihang, hovy}@cs.cmu.edu

8
1
0
2
 
g
u
A
 
8
2
 
 
]
L
C
.
s
c
[
 
 
3
v
5
2
2
3
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Cloze tests are widely adopted in language
exams to evaluate students’ language proﬁ-
In this paper, we propose the ﬁrst
ciency.
large-scale human-created cloze test dataset
CLOTH 1 2, containing questions used in
middle-school and high-school language ex-
ams. With missing blanks carefully created
by teachers and candidate choices purposely
designed to be nuanced, CLOTH requires a
deeper language understanding and a wider
attention span than previously automatically-
generated cloze datasets. We test the perfor-
mance of dedicatedly designed baseline mod-
els including a language model trained on the
One Billion Word Corpus and show humans
outperform them by a signiﬁcant margin. We
investigate the source of the performance gap,
trace model deﬁciencies to some distinct prop-
erties of CLOTH, and identify the limited abil-
ity of comprehending the long-term context to
be the key bottleneck.

1

Introduction

Being a classic language exercise,
the cloze
test (Taylor, 1953) is an accurate assessment of
language proﬁciency (Fotos, 1991; Jonz, 1991;
Tremblay, 2011) and has been widely employed
in language examinations. Under a typical set-
ting, a cloze test requires examinees to ﬁll in miss-
ing words (or sentences) to best ﬁt the surround-
ing context. To facilitate natural language under-
standing, automatically-generated cloze datasets
are introduced to measure the ability of machines
in reading comprehension (Hermann et al., 2015;
In these
Hill et al., 2016; Onishi et al., 2016).
datasets, each cloze question typically consists of

∗ Equal contribution.
1CLOTH (CLOze test by TeacHers) is available at
http://www.cs.cmu.edu/˜glai1/data/cloth/.
at http://www.
qizhexie.com/data/CLOTH_leaderboard.html

leaderboard is

available

2The

a context paragraph and a question sentence. By
randomly replacing a particular word in the ques-
tion sentence with a blank symbol, a single test
case is created. For instance, CNN/Daily Mail
datasets (Hermann et al., 2015) use news articles
as contexts and summary bullet points as the ques-
tion sentence. Only named entities are removed
when creating the blanks. Similarly, in Children’s
Books test (CBT) (Hill et al., 2016), cloze ques-
tions are obtained by removing a word in the last
sentence of every consecutive 21 sentences, with
the ﬁrst 20 sentences being the context. Different
from CNN/Daily Mail datasets, CBT also provides
each question with a candidate answer set, con-
sisting of randomly sampled words with the same
part-of-speech tag from the context as that of the
correct answer.

Thanks to the automatic generation process,
these datasets can be very large in size, leading
to signiﬁcant research progresses. However, com-
pared to how humans would create cloze ques-
tions and evaluate reading comprehension ability,
the automatic generation process bears some in-
evitable issues. Firstly, blanks are chosen uni-
formly without considering which aspect of the
language phenomenon that questions will test.
Hence, quite a portion of automatically-generated
questions can be purposeless or even trivial to an-
swer. Another issue involves the ambiguity of
answers. Given a context and a sentence with a
blank, there can be multiple words that ﬁt almost
equally well into the blank. A possible solution is
to include a candidate option set, as done by CBT,
to get rid of the ambiguity. However, automati-
cally generating the candidate option set can be
problematic since it cannot guarantee the ambigu-
ity is removed. More importantly, automatically-
generated candidates can be totally irrelevant or
simply grammatically unsuitable for the blank, re-
sulting in again purposeless or trivial questions.

Probably due to these unsatisfactory issues, neu-
ral models have achieved comparable results to
the human-level performance within a very short
time (Chen et al., 2016; Dhingra et al., 2016; Seo
et al., 2016). While there have been works try-
ing to incorporate human design into cloze ques-
tion generation (Zweig and Burges, 2011; Paperno
et al., 2016), due to the expensive labeling process,
the MSR Sentence Completion Challenge created
by this effort has 1, 040 questions and the LAM-
BADA (Paperno et al., 2016) dataset has 10, 022
questions, limiting the possibility of developing
powerful neural models on it. As a result of
the small size, human-created questions are only
used to compose development sets and test sets.
Motivated by the aforementioned drawbacks, we
propose CLOTH, a large-scale cloze test dataset
collected from English exams. Questions in the
dataset are designed by middle-school and high-
school teachers to prepare Chinese students for
entrance exams. To design a cloze test, teachers
ﬁrstly determine the words that can test students’
knowledge of vocabulary, reasoning or grammar;
then replace those words with blanks and provide
other three candidate options for each blank. If a
question does not speciﬁcally test grammar usage,
all of the candidate options would complete the
sentence with correct grammar, leading to highly
nuanced questions. As a result, human-created
questions are usually harder and are a better as-
sessment of language proﬁciency. A general cloze
test evaluates several aspects of language proﬁ-
ciency including vocabulary, reasoning and gram-
mar, which are key components of comprehending
natural language.

To verify if human-created cloze questions are
difﬁcult for current models, we train and evaluate
the state-of-the-art language model (LM) and ma-
chine comprehension models on this dataset, in-
cluding a language model trained on the One Bil-
lion Word Corpus. We ﬁnd that the state-of-the-
art model lags behind human performance even if
the model is trained on a large external corpus.
We analyze where the model fails compared to
humans who perform well. After conducting er-
ror analysis, we assume the performance gap re-
sults from the model’s inability to use a long-term
context. To examine this assumption, we eval-
uate human-level performance when the human
subjects are only allowed to see one sentence as
the context. Our assumption is conﬁrmed by the

matched performances of the models and human
In addition, we
when given only one sentence.
demonstrate that human-created data is more dif-
ﬁcult than automatically-generated data. Speciﬁ-
cally, it is much easier for the same model to per-
form well on automatically-generated data.

We hope that CLOTH provides a valuable
testbed for both the language modeling commu-
nity and the machine comprehension community.
Speciﬁcally, the language modeling community
can use CLOTH to evaluate their models’ abil-
ities in modeling long contexts, while the ma-
chine comprehension community can use CLOTH
to test machine’s understanding of language phe-
nomena.

2 Related Work

automatically-generated

Large-scale
cloze
tests (Hermann et al., 2015; Hill et al., 2016;
Onishi et al., 2016) lead to signiﬁcant research ad-
vancements. However, generated questions do not
consider language phenomenon to be tested and
are relatively easy to solve. Recently proposed
reading comprehension datasets are all labeled by
humans to ensure a high quality (Rajpurkar et al.,
2016; Joshi et al., 2017; Trischler et al., 2016;
Nguyen et al., 2016).

Perhaps the closet work to CLOTH is the LAM-
BADA dataset
(Paperno et al., 2016). LAM-
BADA also targets at ﬁnding challenging words to
test LM’s ability in comprehending a longer con-
text. However, LAMBADA does not provide a
candidate set for each question, which can cause
ambiguities when multiple words can ﬁt in. Fur-
thermore, only test set and development set are la-
beled manually. The provided training set is the
unlabeled Book Corpus (Zhu et al., 2015). Such
unlabeled data do not emphasize long-dependency
questions and have a mismatched distribution with
the test set, as showed in Section 5. Further, the
Book Corpus is too large to allow rapid algorithm
development for researchers who do not have ac-
cess to a huge amount of computational power.

Aiming to evaluate machines under the same
conditions that the humans are evaluated, there is
a growing interest in obtaining data from exam-
inations. NTCIR QA Lab (Shibuki et al., 2014)
contains a set of real-world college entrance exam
questions. The Entrance Exams task at CLEF QA
Track (Pe˜nas et al., 2014; Rodrigo et al., 2015)
evaluates machine’s reading comprehension abil-

ity. The AI2 Reasoning Challenge (Clark et al.,
2018; Schoenick et al., 2017) contains approx-
imately eight thousand scientiﬁc questions used
in middle school. Lai et al. (2017) proposes the
ﬁrst large-scale machine comprehension dataset
obtained from exams. They show that questions
designed by teachers have a signiﬁcantly larger
proportion of reasoning questions. Our dataset fo-
cuses on evaluating both language proﬁciency and
reasoning abilities.

3 CLOTH Dataset

In this section, we introduce the CLOTH dataset
that is collected from English examinations, and
study its abilities of assessment.

3.1 Data Collection and Statistics

We collect the raw data from three free and pub-
lic websites in China that gather exams created
by English teachers to prepare students for col-
lege/high school entrance exams3. Before clean-
ing, there are 20, 605 passages and 332, 755 ques-
tions. We perform the following processes to en-
sure the validity of data: Firstly, we remove ques-
tions with an inconsistent format such as questions
with more than four options. Then we ﬁlter all
questions whose validity relies on external infor-
mation such as pictures or tables. Further, we ﬁnd
that half of the total passages are duplicates and we
delete those passages. Lastly, on one of the web-
sites, the answers are stored as images. We use two
OCR software programs4 to extract the answers
from images. We discard the questions when re-
sults from the two software are different. After
the cleaning process, we obtain a clean dataset of
7, 131 passages and 99, 433 questions.

Since high school questions are more difﬁ-
cult than middle school questions, we divide the
datasets into CLOTH-M and CLOTH-H, which
stand for the middle school part and the high
school part. We split 11% of the data for both
the test set and the development set. The detailed
statistics of the whole dataset and two subsets are
presented in Table 1. Note that the questions were
created to test non-native speakers, hence the vo-
cabulary size is not very large.

3 The three websites include http://www.21cnjy.com/;
http://5utk.ks5u.com/; http://zujuan.xkw.com/. We checked
that CLOTH does not contain sentence completion example
questions from GRE, SAT and PSAT.

4tesseract:

https://github.com/tesseract-ocr; ABBYY

FineReader: https://www.abbyy.com/en-us/ﬁnereader/

3.2 Question Type Analysis

In order to evaluate students’ mastery of a lan-
guage, teachers usually design tests in a way that
questions cover different aspects of a language.
Speciﬁcally, they ﬁrst identify words in the pas-
sage that can examine students’ knowledge in vo-
cabulary, logic, or grammar. Then, they replace
the words with blanks and prepare three incorrect
but nuanced candidate options to make the test
non-trivial. A sample passage is presented in Ta-
ble 2.

To understand the abilities of assessment on this
dataset, we divide questions into several types and
label the proportion of each type. According to
English teachers who regularly create cloze test
questions for English exams in China, there are
largely three types: grammar, vocabulary and rea-
soning. Grammar questions are easily differen-
tiated from other two categories. However, the
teachers themselves cannot specify a clear distinc-
tion between reasoning questions and vocabulary
questions since all questions require comprehend-
ing the words within the context and conducting
some level of reasoning by recognizing incom-
plete information or conceptual overlap.

Hence, we divided the questions except gram-
mar questions based on the difﬁculty level
for a machine to answer the question, follow-
ing works on analyzing machine comprehension
datasets (Chen et al., 2016; Trischler et al., 2016).
In particular, we divide them in terms of their de-
pendency ranges, since questions that only involve
a single sentence are easier to answer than ques-
tions involving evidence distributed in multiple
sentences. Further, we divided questions involving
long-term dependency into matching/paraphrasing
questions and reasoning questions since matching
questions are easier. The four types include:

• Grammar: The question is about grammar us-
age,
involving tense, preposition usage, ac-
tive/passive voices, subjunctive mood and so on.

• Short-term-reasoning: The question is about
content words and can be answered based on
the information within the same sentence. Note
that the content words can evaluate knowledge
of both vocabulary and reasoning.

• Matching/paraphrasing: The question is an-
swered by copying/paraphrasing a word in the
context.

Dataset

# passages
# questions
Vocab. size

Avg. # sentence
Avg. # words

Train

2,341
22,056

CLOTH-M
Dev

355
3,273
15,096

16.26
242.88

Test

Train

335
3,198

3,172
54,794

Test

Train

478
8,318

5,513
76,850

CLOTH-H
Dev

450
7,794
32,212

18.92
365.1

CLOTH (Total)
Dev

Test

813
11,516

805
11,067
37,235

17.79
313.16

Table 1: The statistics of the training, development and test sets of CLOTH-M (middle school questions),
CLOTH-H (high school questions) and CLOTH

• Long-term-reasoning: The answer must be
inferred from synthesizing information dis-
tributed across multiple sentences.

We sample 100 passages in the high school cat-
egory and the middle school category respectively
with totally 3, 000 questions. The types of these
questions are labeled on Amazon Turk. We pay
$1 and $0.5 for high school passages and middle
school passages respectively. We refer readers to
Appendix A.1 for details of the labeling processes
and the labeled sample passage.

The proportion of different questions is shown
in Table 3. The majority of questions are short-
term-reasoning questions while approximately
22.4% of the data needs long-term information,
in which the long-term-reasoning questions con-
stitute a large proportion.

4 Exploring Models’ Limits

In this section, we investigate if human-created
cloze test is a challenging problem for state-of-
the-art models. We ﬁnd that LM trained on the
One Billion Word Corpus can achieve a remark-
able score but cannot solve the cloze test. After
conducting an error analysis, we hypothesize that
the model is not able to deal with long-term de-
pendencies. We verify the hypothesis by compar-
ing the model’s performance with the human per-
formance when the information humans obtain is
limited to one sentence.

4.1 Human and Model Performance

LSTM To test the performance of RNN-based
supervised models, we train a bidirectional
LSTM (Hochreiter and Schmidhuber, 1997) to
predict the missing word given the context with
only labeled data. The implementation details are
in Appendix A.3.

ment the supervised LSTM model with the at-
tention mechanism (Bahdanau et al., 2014), so
that the representation at the blank is used as a
query to ﬁnd the relevant context in the docu-
ment and a blank-speciﬁc representation of the
document is used to score each candidate an-
swer. Speciﬁcally, we adapt the Stanford Atten-
tive Reader (Chen et al., 2016) and the position-
aware attention model (Zhang et al., 2017) to the
cloze test problem. With the position-aware atten-
tion model, the attention scores are based on both
the context match and the distance from a context
to the blank. Both attention models are trained
only with human-created blanks just as the LSTM
model.

LM In cloze test, the context on both sides may
be enough to determine the correct answer. Sup-
pose xi is the missing word and x1, · · · , xi−1,
xi+1, · · · , xn are the context, we choose xi that
maximizes the joint probability p(x1, · · · , xn),
which essentially maximizes
the conditional
likelihood p(xi
| x1, · · · , xi−1, xi+1, · · · , xn).
Therefore, LM can be naturally adapted to cloze
test.

In essence, LM treats each word as a possible
blank and learns to predict it. As a result, it re-
ceives more supervision than the LSTM trained
on human-labeled questions. Besides training a
neural LM on our dataset, interested in whether
the state-of-the-art LM can solve cloze test, we
also test the LM trained on the One Billion Word
Benchmark (Chelba et al., 2013) (referred as 1B-
LM) that achieves a perplexity of 30.0 (Jozefow-
icz et al., 2016)5. To make the evaluation time
tractable, we limit the context length to one sen-
tence or three sentences. Note that the One Billion
Word Corpus does not overlap with the CLOTH
corpus.

Attentive Readers To enable the model
to
gather information from a longer context, we aug-

5The

pre-trained

model

is

obtained

from

https://github.com/tensorﬂow/models/tree/master/research/lm 1b

Passage: Nancy had just got a job as a secretary in a com-
pany. Monday was the ﬁrst day she went to work, so she
was very 1 and arrived early. She 2 the door open and
found nobody there. ”I am the 3 to arrive.” She thought and
came to her desk. She was surprised to ﬁnd a bunch of 4
on it. They were fresh. She 5 them and they were sweet.
She looked around for a 6 to put them in. ”Somebody has
sent me ﬂowers the very ﬁrst day!” she thought 7 . ” But
who could it be?” she began to 8 . The day passed quickly
and Nancy did everything with 9 interest. For the follow-
ing days of the 10 , the ﬁrst thing Nancy did was to change
water for the followers and then set about her work.
Then came another Monday.
11 she came near her desk
she was overjoyed to see a(n) 12 bunch of ﬂowers there.
She quickly put them in the vase, 13 the old ones. The
same thing happened again the next Monday. Nancy began
to think of ways to ﬁnd out the 14 . On Tuesday afternoon,
she was sent to hand in a plan to the 15 . She waited for
his directives at his secretary’s 16 . She happened to see on
the desk a half-opened notebook, which 17 : ”In order to
keep the secretaries in high spirits, the company has decided
that every Monday morning a bunch of fresh ﬂowers should
be put on each secretarys desk.” Later, she was told that their
general manager was a business management psychologist.

Questions:

D. ﬁrst

D. held
D. bottle

B. pushed
B. second
B. grapes

1. A. depressed B. encouraged C. excited D. surprised
2. A. turned
3. A. last
4. A. keys
5. A. smelled B. ate
6. A. vase
7. A. angrily
8. A. seek
9. A. low
10. A. month
11. A. Unless
12. A. old
13. A. covering B. demanding C. replacing D. forbidding
14. A. sender
15. A. assistant B. colleague C. employee D. manager
16. A. notebook B. desk
17. A. said

C. knocked D. forced
C. third
C. ﬂowers D. bananas
C. took
C. glass
C. strangely D. happily
C. work
C. great
C. year
C. Since
C. blue

B. room
B. quietly
B. wonder
B. little
B. period
B. When
B. red

D. ask
D. general
D. week
D. Before
D. new

C. secretary D. waiter

C. ofﬁce
C. printed

D. house
D. signed

B. receiver

B. written

Table 2: A Sample passage from our dataset. Bold
faces highlight the correct answers. There is only
one best answer among four candidates, although
several candidates may seem correct.

Human performance We measure the perfor-
mance of Amazon Mechanical Turkers on 3, 000
sampled questions when the whole passage is
given.

Results The comparison is shown in Table 4.
Both attentive readers achieve similar accuracy
to the LSTM. We hypothesize that the reason of
the attention model’s unsatisfactory performance
is that the evidence of a question cannot be simply
found by matching the context. Similarly, on read-
ing comprehension, though attention-based mod-
els (Wang et al., 2017; Seo et al., 2016; Dhingra
et al., 2016) have reached human performance on

Short-term

Long-term

Dataset

GM

STR

MP

LTR

O

CLOTH
0.265
CLOTH-M 0.330
CLOTH-H
0.240

0.503
0.413
0.539

0.044
0.068
0.035

0.180
0.174
0.183

0.007
0.014
0.004

Table 3: The question type statistics of 3000 sam-
pled questions where GM, STR, MP, LTR and
O denotes grammar, short-term-reasoning, match-
ing/paraphrasing, long-term-reasoning and others
respectively.

Model

CLOTH CLOTH-M CLOTH-H

LSTM
Stanford AR
Position-aware AR

LM
1B-LM (one sent.)
1B-LM (three sent.)

Human performance

0.484
0.487
0.485

0.548
0.695
0.707

0.859

0.518
0.529
0.523

0.646
0.723
0.745

0.897

0.471
0.471
0.471

0.506
0.685
0.693

0.845

Table 4: Models’ performance and human-level
performance on CLOTH. LSTM, Stanford Atten-
tive Reader and Attentive Reader with position-
aware attention shown in the top part only use
supervised data labelled by human. LM out-
performs LSTM since it receives more supervi-
sions in learning to predict each word. Training
on large external corpus further signiﬁcantly en-
hances LM’s accuracy.

the SQuAD dataset (Rajpurkar et al., 2016), their
performance is still not comparable to human per-
formance on datasets that focus more on reason-
ing where the evidence cannot be simply found by
a matching behavior (Lai et al., 2017; Xu et al.,
2017). Since the focus of this paper is to analyze
the proposed dataset, we leave the design of rea-
soning oriented attention models for future work.
The LM achieves much better performance than
LSTM. The gap is larger when the LM is trained
indicating that
on the 1 Billion Word Corpus,
more training data results in a better generaliza-
tion. Speciﬁcally, the accuracy of 1B-LM is 0.695
when one sentence is used as the context. It in-
dicates that LM can learn sophisticated language
regularities when given sufﬁcient data. The same
conclusion can also be drawn from the success of
a concurrent work ELMo which uses LM repre-
sentations as word vectors and achieves state-of-
the-art results on six language tasks (Peters et al.,
2018). However, if we increase the context length

to three sentences, the accuracy of 1B-LM only
has a marginal improvement. In contrast, humans
outperform 1B-LM by a signiﬁcant margin, which
demonstrates that deliberately designed questions
in CLOTH are not completely solved even for
state-of-the-art models.

4.2 Analyzing 1B-LM’s Strengths and

Weaknesses

In this section, we would like to understand why
1B-LM lags behind human performance. We ﬁnd
that most of the errors involve long-term reason-
ing. Additionally, in a lot of cases, the depen-
dency is within the context of three sentences. We
show several errors made by the 1B-LM in Table
5. In the ﬁrst example, the model does not know
that Nancy found nobody in the company means
that Nancy was the ﬁrst one to arrive at the com-
pany. In the second and third example, the model
fails probably because of not recognizing “they”
referred to “ﬂowers”. The dependency in the last
case is longer. It depends on the fact that Nancy
was alone in the company.

Based on the case study, we hypothesize that
the LM is not able to take long-term information
into account, although it achieves a surprisingly
good overall performance. Additionally, the 1B-
LM is trained on the sentence level, which might
also result in the inability to track paragraph level
information. However, to investigate the differ-
ences between training on sentence level and on
paragraph level, a prohibitive amount of computa-
tional resource is required to train a large model
on the 1 Billion Word Corpus.

On the other hand, a practical comparison is to
test the model’s performance on different types
of questions. We ﬁnd that the model’s accu-
racy is 0.591 on long-term-reasoning questions of
CLOTH-H while it achieves 0.693 on short-term-
reasoning (a comprehensive type-speciﬁc perfor-
mance is available in Appendix A.3), which par-
tially conﬁrms that long-term-reasoning is harder.
However, we could not completely rely on the per-
formance on speciﬁc questions types, partly due to
a large variance caused by the small sample size.
Another reason is that the reliability of question
type labels depends on whether turkers are careful
enough. For example, in the error analysis shown
in Table 5, a careless turker would label the second
example as short-term-reasoning without noticing
that the meaning of “they” relies on a long context.

To objectively verify if the LM’s strengths lie
in dealing with short-term information, we obtain
the ceiling performance of only utilizing short-
term information. Showing only one sentence as
the context, we ask the Turkers to select an option
based on their best guesses given the insufﬁcient
information. By limiting the context span man-
ually, the ceiling performance with the access to
only a short context is estimated accurately.

As shown in Table 6, The performance of 1B-
LM using one sentence as the context can almost
match the human ceiling performance of only us-
ing short-term information. Hence we conclude
that the LM can almost perfectly solve all short-
term cloze questions. However, the performance
of LM is not improved signiﬁcantly when a long-
term context is given, indicating that the perfor-
mance gap is due to the inability of long-term rea-
soning.

5 Comparing Human-created Data and

Automatically-generated Data

In this section, we demonstrate that human-
created data is a better testbed than automatically-
generated cloze test since it results in a larger gap
between model’s performance and human perfor-
mance.

A casual observation is that a cloze test can
be created by randomly deleting words and ran-
In fact, to
domly sampling candidate options.
generate large-scale data, similar generation pro-
cesses have been introduced and widely used in
machine comprehension (Hermann et al., 2015;
Hill et al., 2016; Onishi et al., 2016). However,
research on cloze test design (Sachs et al., 1997)
shows that tests created by deliberately deleting
words are more reliable than tests created by ran-
domly or periodically deleting words. To design
accurate language proﬁciency assessment, teach-
ers usually deliberately select words in order to ex-
amine students’ proﬁciency in grammar, vocabu-
lary and reasoning. Moreover, in order to make the
question non-trivial, three incorrect options pro-
vided by teachers are usually grammatically cor-
rect and relevant to the context. For instance, in
the fourth problem of the sample passage shown
in Table 2, “grapes”, “ﬂowers” and “bananas” all
ﬁt the description of being fresh.

Hence we naturally hypothesize that human-
generated data has distinct characteristics when
compared with automatically-generated data. To

Context

Options

She pushed the door open and found nobody there. ”I am the
thought and came to her desk.

to arrive.” She

A. last

B. second

C. third

D. ﬁrst

They were fresh. She
to put them in.

them and they were sweet. She looked around for a vase

A. smelled

B. ate

C. took

D. held

She smelled them and they were sweet. She looked around for a
”Somebody has sent me ﬂowers the very ﬁrst day!”

to put them in.

A. vase

B. room

C. glass

D. bottle

”But who could it be?” she began to
everything with great interest.

. The day passed quickly and Nancy did

A. seek

B. wonder

C. work

D. ask

Table 5: Error analysis of 1-billion-language-model with three sentences as the context. The questions are
sampled from the sample passage shown in Table 2. The correct answer is in bold text. The incorrectly
selected options are in italics.

Model

CLOTH CLOTH-M CLOTH-H

Short context

Long context

1B-LM
Human

1B-LM
Human

0.695
0.713

0.707
0.859

0.723
0.771

0.745
0.897

0.685
0.691

0.693
0.845

Table 6: Humans’ performance compared with 1-
billion-language-model. In the short context part,
both 1B-LM and humans only use information of
one sentence.
In the long context part, humans
have the whole passage as the context, while 1B-
LM uses contexts of three sentences.

verify this assumption, we compare the LSTM
model’s performance when given different propor-
tions of the two types of data. Speciﬁcally, to train
a model with α percent of automatically-generated
data, we randomly replace a percent blanks with
blanks at random positions, while keeping the re-
maining 1 − α percent questions the same. The
candidate options for the generated blanks are ran-
dom words sampled from the unigram distribu-
tion. We test models obtained with varying α on
human-created data and automatically-generated
data respectively.

spectively, as shown in Tab. 4, leading to a gap
of 0.376. In comparison, the performance gap on
the automatically-generated data is at most 0.185
since the model’s performance reaches an accu-
racy of 0.815 when fully trained on generated data.
(2) Although human-created data may provide
more information in distinguishing similar words,
the distributional mismatch between two types of
data makes it non-trivial to transfer the knowl-
edge gained from human-created data to tackle
automatically-generated data.
the
model’s performance on automatically-generated
data monotonically decreases when given a higher
ratio of human-created data.

Speciﬁcally,

6 Combining Human-created Data with

Automatically-generated Data

In Section 4.1, we show that LM is able to take
advantage of more supervision since it predicts
each word based on the context. At the same
time, we also show that human-created data and
the automatically-generated data are quite differ-
ent in Section 5.
In this section, we propose a
model that takes advantage of both sources.

(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)

Test

α%

0% 25% 50% 75% 100%

6.1 Representative-based Model

human-created
Generated

0.484 0.475 0.469 0.423 0.381
0.422 0.699 0.757 0.785 0.815

Table 7: The model’s performance when trained
on α percent of automatically-generated data and
100 − α percent of human-created data

From the comparison in Table 7, we have the
following observations:
(1) human-created data
leads to a larger gap between model’s perfor-
mance and the ceiling/human performance. The
model’s performance and human’s performance
on the human-created data are 0.484 and 0.859 re-

Speciﬁcally, for each question, regardless of be-
ing human-created or automatically-generated, we
can compute the negative log likelihood of the
correct answer as the loss function. Suppose JH
is the average negative log likelihood loss for
human-created questions and JR is the loss func-
tion on generated questions, we combine losses on
human-created questions and generated questions
by simply adding them together, i.e., JR + JH is
used as the ﬁnal loss function. We will introduce
the deﬁnition of JR in the following paragraphs.

Although automatically-generated data has a
large quantity and is valuable to the model

training, as shown in the previous Section,
automatically-generated questions are quite dif-
ferent from human-created questions.
Ideally, a
large amount of human-created questions is more
desirable than a large amount of automatically-
generated questions. A possible avenue towards
having large-scale human-created data is to au-
tomatically pick out a large number of generated
questions which are representative of or similar to
human-created questions. In other words, we train
a network to predict whether a question is a gener-
ated question or a human-created question. A gen-
erated question is representative of human-created
questions if it has a high probability of being a
human-created question. Then we can give higher
weights to questions that resemble human-created
question.

We ﬁrst introduce our method to obtain the rep-
resentativeness information. Let x denote the pas-
sage and z denote whether a word is selected as
a question by human, i.e., z is 1 if this word is
selected to be ﬁlled in the original passage or 0
otherwise. Suppose hi is the representation of i-th
word given by a bidirectional LSTM. The network
computes the probability pi of xi being a human-
created question as follows:

li = hT

i wxi;

pi = Sigmoid(li)

where li is the logit which will be used as in the
ﬁnal model and wxi is the the word embedding.
We train the network to minimize the binary cross
entropy between p and ground-truth labels at each
token.

After obtaining the representativeness informa-
tion, we deﬁne the representativeness weighted
loss function as

JR =

Softmaxi(

, · · · ,

)Ji

l1
α

ln
α

(cid:88)

i(cid:54)∈H

where Ji denotes the negative log likelihood loss
for the i−th question and let li be the output rep-
resentativeness of the i-th question and H is the
set of all human-generated questions and α is the
temperature of the Softmax function. The model
degenerates into assigning a uniform weight to all
questions when the temperature is +∞. We set α
to 2 based on the performance on the dev set. 6.

6The code is available at https://github.com/qizhex/Large-

scale-Cloze-Test-Dataset-Created-by-Teachers

Model

Ex.

CLOTH CLOTH-M CLOTH-H

Our model
LM
LSTM
Stanford AR

1B-LM

Human

No

Yes

0.583
0.548
0.484
0.487

0.707

0.859

0.673
0.646
0.518
0.529

0.745

0.897

0.549
0.506
0.471
0.471

0.693

0.845

Table 8: Overall results on CLOTH. Ex. denotes
external data.

Model

CLOTH CLOTH-M CLOTH-H

Our model
w.o. rep.
w.o. hum.
w.o. rep. or hum.

0.583
0.566
0.565
0.543

0.673
0.662
0.665
0.643

0.549
0.528
0.526
0.505

Table 9: Ablation study on using the representa-
tiveness information (denoted as rep.)
and the
human-created data (denoted as hum.)

6.2 Results

We summarize performances of all models in Ta-
ble 8. Our representativeness model outperforms
all other models that do not use external data on
CLOTH, CLOTH-H and CLOTH-M.

6.3 Analysis

In this section, we verify the effectiveness of
the representativeness-based averaging by abla-
tion studies. When we remove the representative-
ness information by setting α to inﬁnity, the ac-
curacy drops from 0.583 to 0.566. When we fur-
ther remove the human-created data so that only
generated data is employed, the accuracy drops to
0.543, similar to the performance of LM. The re-
sults further conﬁrm that it is beneﬁcial to incor-
porate human-created questions into training.

A sample of the predicted representativeness is
shown in Figure 17. Clearly, words that are too ob-
vious have low scores, such as punctuation marks,
simple words “a” and “the”. In contrast, content
words whose semantics are directly related to the
context have a higher score, e.g., “same”, “simi-
lar”, “difference” have a high score when the dif-
ference between two objects is discussed and “se-
crets” has a high score since it is related to the sub-
sequent sentence “does not want to share with oth-
ers”. Our prediction model achieves an F1 score of
36.5 on the test set, which is understandable since

7The

script

to generate

is obtained
https://gist.github.com/ihsgnef/

the Figure

at
f13c35cd46624c8f458a4d23589ac768

Figure 1: Representativeness prediction for each word. Lighter color means less representative. The
words deleted by human as blanks are in bold text.

there are many plausible questions within a pas-
sage.

It has been shown that features such as morphol-
ogy information and readability are beneﬁcial in
cloze test prediction (Skory and Eskenazi, 2010;
Correia et al., 2012, 2010; Kurtasov, 2013). We
leave investigating the advanced approaches of au-
tomatically designing cloze test to future work.

7 Conclusion and Discussion

In this paper, we propose a large-scale cloze test
dataset CLOTH that is designed by teachers. With
missing blanks and candidate options carefully
created by teachers to test different aspects of lan-
guage phenomena, CLOTH requires a deep lan-
guage understanding and better captures the com-
plexity of human language. We ﬁnd that hu-
man outperforms 1B-LM by a signiﬁcant mar-
gin. After detailed analysis, we ﬁnd that the
performance gap is due to the model’s inability
to understanding a long context. We also show
that, compared to automatically-generated ques-
tions, human-created questions are more difﬁcult
and lead to a larger margin between human per-
formance and the model’s performance.

Despite the excellent performance of 1B-LM
when compared with models trained only on
CLOTH, it is still important to investigate and cre-
ate more effective models and algorithms which
provide complementary advantages to having a
large amount of data. For rapid algorithm devel-
opments, we suggest training models only on the
training set of CLOTH and comparing with mod-
els that do not utilize external data.

We hope our dataset provides a valuable testbed
to the language modeling community and the
machine comprehension community.
In partic-
ular, the language modeling community can use

CLOTH to evaluate their models’ abilities in mod-
eling a long context.
In addition, the machine
comprehension community may also ﬁnd CLOTH
useful in evaluating machine’s understanding of
language phenomena including vocabulary, rea-
soning and grammar, which are key components
of comprehending natural language.

In our future work, we would like to design al-
gorithms to better model a long context, to utilize
external knowledge, and to explore more effec-
tive semi-supervised learning approaches. Firstly,
we would like to investigate efﬁcient ways of uti-
lizing external knowledge such as paraphrasing
and semantic concepts like prior works (Dong
et al., 2017; Dasigi et al., 2017). In comparison,
training on a large external dataset is actually a
time-consuming way of utilizing external knowl-
edge. Secondly, to use the generated questions
more effectively, the representative-based semi-
supervised approach might be improved by tech-
niques studied in active learning and hard exam-
ple mining (Settles, 2009; Shrivastava et al., 2016;
Chang et al., 2017).

Acknowledgement

We thank Yulun Du, Kaiyu Shi and Zhilin Yang
for insightful discussions and suggestions on the
draft. We thank Shi Feng for the script to high-
light representative words. This research was sup-
ported in part by DARPA grant FA8750-12-2-
0342 funded under the DEFT program.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Haw-Shiuan Chang, Erik Learned-Miller, and Andrew
McCallum. 2017. Active bias: Training more accu-
rate neural networks by emphasizing high variance
In Advances in Neural Information Pro-
samples.
cessing Systems, pages 1003–1013.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for mea-
suring progress in statistical lming. arXiv preprint
arXiv:1312.3005.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016.
the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858.

A thorough examination of

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.

Rui Correia, Jorge Baptista, Maxine Eskenazi, and
Nuno J Mamede. 2012. Automatic generation of
cloze question stems. In PROPOR, pages 168–178.
Springer.

Rui Correia, Jorge Baptista, Nuno Mamede, Isabel
Trancoso, and Maxine Eskenazi. 2010. Automatic
In Pro-
generation of cloze question distractors.
ceedings of the Interspeech 2010 Satellite Workshop
on Second Language Studies: Acquisition, Learn-
ing, Education and Technology, Waseda University,
Tokyo, Japan.

Pradeep Dasigi, Waleed Ammar, Chris Dyer, and Ed-
uard Hovy. 2017. Ontology-aware token embed-
dings for prepositional phrase attachment. arXiv
preprint arXiv:1705.02925.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549.

Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella
Lapata. 2017. Learning to paraphrase for question
answering. arXiv preprint arXiv:1708.06022.

Sandra S Fotos. 1991. The cloze test as an integra-
tive measure of eﬂ proﬁciency: A substitute for es-
says on college entrance examinations? Language
Learning, 41(3):313–336.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In NIPS.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Jon Jonz. 1991. Cloze item types and second language

comprehension. Language testing, 8(1):1–22.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of lming. arXiv preprint arXiv:1602.02410.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Andrey Kurtasov. 2013. A system for generating cloze
test items from russian-language text. In Proceed-
ings of the Student Research Workshop associated
with RANLP 2013, pages 107–112.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017.
Race: Large-scale
reading comprehension dataset from examinations.
EMNLP.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268.

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457.

Denis Paperno, Germ´an Kruszewski, Angeliki Lazari-
dou, Quan Ngoc Pham, Raffaella Bernardi, San-
dro Pezzelle, Marco Baroni, Gemma Boleda, and
Raquel Fern´andez. 2016.
The lambada dataset:
Word prediction requiring a broad discourse context.
arXiv preprint arXiv:1606.06031.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS-W.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes), pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2016. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. ICLR.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen,
and Xiaodong Liu. 2017. Towards human-level ma-
chine reading comprehension: Reasoning and in-
arXiv preprint
ference with multiple strategies.
arXiv:1711.04964.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot ﬁll-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 35–45.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
In Proceedings of the IEEE
and reading books.
international conference on computer vision, pages
19–27.

Geoffrey Zweig and Christopher JC Burges. 2011. The
microsoft research sentence completion challenge.
Technical report, Technical Report MSR-TR-2011-
129, Microsoft.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Squad: 100,000+ questions
Percy Liang. 2016.
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

J Sachs, P Tung, and RYH Lam. 1997. How to con-
struct a cloze test: Lessons from testing measure-
ment theory models. Perspectives.

Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter
Turney, and Oren Etzioni. 2017. Moving beyond the
turing test with the allen ai science challenge. Com-
munications of the ACM, 60(9):60–64.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
ﬂow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Burr Settles. 2009. Active learning literature survey.

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-
shick. 2016. Training region-based object detectors
with online hard example mining. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 761–769.

Adam Skory and Maxine Eskenazi. 2010. Predicting
In Pro-
cloze task quality for vocabulary training.
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 49–56. Association for Compu-
tational Linguistics.

Wilson L Taylor. 1953.

a new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.

cloze procedure:

Annie Tremblay. 2011. Proﬁciency assessment stan-
dards in second language acquisition research. Stud-
ies in Second Language Acquisition, 33(3):339–372.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

(a) Middle school group (CLOTH-M)

(b) High school group (CLOTH-H)

Figure 2: Model and human’s performance on questions with different types. Our model will be intro-
duced in Sec. 6.

A Appendix

A.1 Question Type Labeling

To label the questions, we provided the deﬁnition and an example for each question category to the
Amazon Mechanical Turkers. To ensure quality, we limited the workers to master Turkers who are expe-
rienced and maintain a high acceptance rate. However, we did not restrict the backgrounds of the Turkers
since master Turkers should have a reasonable amount of knowledge about English to conduct previous
tasks. In addition, the vocabulary used in CLOTH are usually not difﬁcult since they are constructed to
test non-native speakers in middle school or high school. To get a concrete idea of the nature of question
types, please refer to examples shown in Tab. 10.

A.2 Type-speciﬁc Performance Analysis

We can also further verify the strengths and weaknesses of the 1B-LM by studying the performance of
models and human on different question categories. Note that the performance presented here may be
subject to a high variance due to the limited number of samples in each category. From the comparison
shown in Figure 2, we see that 1B-LM is indeed good at short-term questions. Speciﬁcally, when the
human only has access to the context of one sentence, 1B-LM is close to human’s performance on
almost all categories. Further, comparing LM and 1B-LM, we ﬁnd that training on the large corpus leads
to improvements on all categories, showing that training on a large amount of data leads to a substantial
improvement in learning complex language regularities.

A.3

Implementation Details

We implement our models using PyTorch (Paszke et al., 2017). We train our model on all questions in
CLOTH and test it on CLOTH-M and CLOTH-H separately. For our ﬁnal model, we use Adam (Kingma
and Ba, 2014) with the learning rate of 0.001. The hidden dimension is set to 650 and we initialize
the word embedding by 300-dimensional Glove word vector (Pennington et al., 2014). The temperature
α is set to 2. We tried to increase the dimensionality of the model but do not observe performance
improvement.

When we train the small LM on CLOTH, we largely follow the recommended hyperparameters in the
Pytorch LM example8. Speciﬁcally, we employ a 2-layer LSTM with hidden dimension as 1024. The
input embedding and output weight matrix are tied. We set the dropout rate to 0.5. The initial learning
rate is set to 10 and divided by 4 whenever the PPL stops improving on the dev set.

We predict the answer for each blank independently for all of the models mentioned in this paper,
since we do not observe signiﬁcant performance improvements in our preliminary experiments when an
auto-regressive approach is employed, i.e., when we ﬁll all previous blanks with predicted answers. We
hypothesize that, regardless of whether there exist inter-blank dependencies, since blanks are usually

8https://github.com/pytorch/examples/tree/master/word language model

distributed far away from each other, LSTM is not able to capture such long dependencies. When testing
language models, we use the longest text spans that do not contain blanks.

Passage: Nancy had just got a job as a secretary in a company. Monday was the ﬁrst day she went
to work, so she was very 1 and arrived early. She 2 the door open and found nobody there. ”I
am the 3 to arrive.” She thought and came to her desk. She was surprised to ﬁnd a bunch of 4
on it. They were fresh. She 5 them and they were sweet. She looked around for a 6 to put
them in. ”Somebody has sent me ﬂowers the very ﬁrst day!” she thought 7 . ” But who could it
be?” she began to 8 . The day passed quickly and Nancy did everything with 9 interest. For
the following days of the 10 , the ﬁrst thing Nancy did was to change water for the followers and
then set about her work.
Then came another Monday. 11 she came near her desk she was overjoyed to see a(n) 12 bunch
of ﬂowers there. She quickly put them in the vase, 13 the old ones. The same thing happened
again the next Monday. Nancy began to think of ways to ﬁnd out the 14 . On Tuesday afternoon,
she was sent to hand in a plan to the 15 . She waited for his directives at his secretary’s 16
. She happened to see on the desk a half-opened notebook, which 17 : ”In order to keep the
secretaries in high spirits, the company has decided that every Monday morning a bunch of fresh
ﬂowers should be put on each secretarys desk.” Later, she was told that their general manager was
a business management psychologist.

A. depressed
A. turned
A. last
A. keys
A. smelled
A. vase
A. angrily
A. seek
A. low

1.
2.
3.
4.
5.
6.
7.
8.
9.
10. A. month
11. A. Unless
12. A. old
13. A. covering
14. A. sender
15. A. assistant
16. A. notebook
17. A. said

Questions

D. surprised
D. forced
D. ﬁrst
D. bananas
D. held
D. bottle
D. happily
D. ask
D. general
D. week
D. Before
D. new

B. encouraged C. excited
B. pushed
C. knocked
C. third
B. second
C. ﬂowers
B. grapes
C. took
B. ate
C. glass
B. room
C. strangely
B. quietly
B. wonder
C. work
C. great
B. little
C. year
B. period
B. When
C. Since
C. blue
B. red
C. replacing D. forbidding
B. demanding
C. secretary
B. receiver
C. employee
B. colleague
B. desk
C. ofﬁce
C. printed
B. written

D. waiter
D. manager
D. house
D. signed

Question type
short-term reasoning
short-term reasoning
long-term reasoning
matching
short-term reasoning
long-term reasoning
short-term reasoning
long-term reasoning
long-term reasoning
long-term reasoning
grammar
long-term reasoning
long-term reasoning
long-term reasoning
matching
matching
grammar

Table 10: An Amazon Turker’s label for the sample passage

Large-scale Cloze Test Dataset Created by Teachers

Qizhe Xie∗ , Guokun Lai∗ , Zihang Dai, Eduard Hovy
Language Technologies Institute, Carnegie Melon University
{qizhex, guokun, dzihang, hovy}@cs.cmu.edu

8
1
0
2
 
g
u
A
 
8
2
 
 
]
L
C
.
s
c
[
 
 
3
v
5
2
2
3
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Cloze tests are widely adopted in language
exams to evaluate students’ language proﬁ-
In this paper, we propose the ﬁrst
ciency.
large-scale human-created cloze test dataset
CLOTH 1 2, containing questions used in
middle-school and high-school language ex-
ams. With missing blanks carefully created
by teachers and candidate choices purposely
designed to be nuanced, CLOTH requires a
deeper language understanding and a wider
attention span than previously automatically-
generated cloze datasets. We test the perfor-
mance of dedicatedly designed baseline mod-
els including a language model trained on the
One Billion Word Corpus and show humans
outperform them by a signiﬁcant margin. We
investigate the source of the performance gap,
trace model deﬁciencies to some distinct prop-
erties of CLOTH, and identify the limited abil-
ity of comprehending the long-term context to
be the key bottleneck.

1

Introduction

Being a classic language exercise,
the cloze
test (Taylor, 1953) is an accurate assessment of
language proﬁciency (Fotos, 1991; Jonz, 1991;
Tremblay, 2011) and has been widely employed
in language examinations. Under a typical set-
ting, a cloze test requires examinees to ﬁll in miss-
ing words (or sentences) to best ﬁt the surround-
ing context. To facilitate natural language under-
standing, automatically-generated cloze datasets
are introduced to measure the ability of machines
in reading comprehension (Hermann et al., 2015;
In these
Hill et al., 2016; Onishi et al., 2016).
datasets, each cloze question typically consists of

∗ Equal contribution.
1CLOTH (CLOze test by TeacHers) is available at
http://www.cs.cmu.edu/˜glai1/data/cloth/.
at http://www.
qizhexie.com/data/CLOTH_leaderboard.html

leaderboard is

available

2The

a context paragraph and a question sentence. By
randomly replacing a particular word in the ques-
tion sentence with a blank symbol, a single test
case is created. For instance, CNN/Daily Mail
datasets (Hermann et al., 2015) use news articles
as contexts and summary bullet points as the ques-
tion sentence. Only named entities are removed
when creating the blanks. Similarly, in Children’s
Books test (CBT) (Hill et al., 2016), cloze ques-
tions are obtained by removing a word in the last
sentence of every consecutive 21 sentences, with
the ﬁrst 20 sentences being the context. Different
from CNN/Daily Mail datasets, CBT also provides
each question with a candidate answer set, con-
sisting of randomly sampled words with the same
part-of-speech tag from the context as that of the
correct answer.

Thanks to the automatic generation process,
these datasets can be very large in size, leading
to signiﬁcant research progresses. However, com-
pared to how humans would create cloze ques-
tions and evaluate reading comprehension ability,
the automatic generation process bears some in-
evitable issues. Firstly, blanks are chosen uni-
formly without considering which aspect of the
language phenomenon that questions will test.
Hence, quite a portion of automatically-generated
questions can be purposeless or even trivial to an-
swer. Another issue involves the ambiguity of
answers. Given a context and a sentence with a
blank, there can be multiple words that ﬁt almost
equally well into the blank. A possible solution is
to include a candidate option set, as done by CBT,
to get rid of the ambiguity. However, automati-
cally generating the candidate option set can be
problematic since it cannot guarantee the ambigu-
ity is removed. More importantly, automatically-
generated candidates can be totally irrelevant or
simply grammatically unsuitable for the blank, re-
sulting in again purposeless or trivial questions.

Probably due to these unsatisfactory issues, neu-
ral models have achieved comparable results to
the human-level performance within a very short
time (Chen et al., 2016; Dhingra et al., 2016; Seo
et al., 2016). While there have been works try-
ing to incorporate human design into cloze ques-
tion generation (Zweig and Burges, 2011; Paperno
et al., 2016), due to the expensive labeling process,
the MSR Sentence Completion Challenge created
by this effort has 1, 040 questions and the LAM-
BADA (Paperno et al., 2016) dataset has 10, 022
questions, limiting the possibility of developing
powerful neural models on it. As a result of
the small size, human-created questions are only
used to compose development sets and test sets.
Motivated by the aforementioned drawbacks, we
propose CLOTH, a large-scale cloze test dataset
collected from English exams. Questions in the
dataset are designed by middle-school and high-
school teachers to prepare Chinese students for
entrance exams. To design a cloze test, teachers
ﬁrstly determine the words that can test students’
knowledge of vocabulary, reasoning or grammar;
then replace those words with blanks and provide
other three candidate options for each blank. If a
question does not speciﬁcally test grammar usage,
all of the candidate options would complete the
sentence with correct grammar, leading to highly
nuanced questions. As a result, human-created
questions are usually harder and are a better as-
sessment of language proﬁciency. A general cloze
test evaluates several aspects of language proﬁ-
ciency including vocabulary, reasoning and gram-
mar, which are key components of comprehending
natural language.

To verify if human-created cloze questions are
difﬁcult for current models, we train and evaluate
the state-of-the-art language model (LM) and ma-
chine comprehension models on this dataset, in-
cluding a language model trained on the One Bil-
lion Word Corpus. We ﬁnd that the state-of-the-
art model lags behind human performance even if
the model is trained on a large external corpus.
We analyze where the model fails compared to
humans who perform well. After conducting er-
ror analysis, we assume the performance gap re-
sults from the model’s inability to use a long-term
context. To examine this assumption, we eval-
uate human-level performance when the human
subjects are only allowed to see one sentence as
the context. Our assumption is conﬁrmed by the

matched performances of the models and human
In addition, we
when given only one sentence.
demonstrate that human-created data is more dif-
ﬁcult than automatically-generated data. Speciﬁ-
cally, it is much easier for the same model to per-
form well on automatically-generated data.

We hope that CLOTH provides a valuable
testbed for both the language modeling commu-
nity and the machine comprehension community.
Speciﬁcally, the language modeling community
can use CLOTH to evaluate their models’ abil-
ities in modeling long contexts, while the ma-
chine comprehension community can use CLOTH
to test machine’s understanding of language phe-
nomena.

2 Related Work

automatically-generated

Large-scale
cloze
tests (Hermann et al., 2015; Hill et al., 2016;
Onishi et al., 2016) lead to signiﬁcant research ad-
vancements. However, generated questions do not
consider language phenomenon to be tested and
are relatively easy to solve. Recently proposed
reading comprehension datasets are all labeled by
humans to ensure a high quality (Rajpurkar et al.,
2016; Joshi et al., 2017; Trischler et al., 2016;
Nguyen et al., 2016).

Perhaps the closet work to CLOTH is the LAM-
BADA dataset
(Paperno et al., 2016). LAM-
BADA also targets at ﬁnding challenging words to
test LM’s ability in comprehending a longer con-
text. However, LAMBADA does not provide a
candidate set for each question, which can cause
ambiguities when multiple words can ﬁt in. Fur-
thermore, only test set and development set are la-
beled manually. The provided training set is the
unlabeled Book Corpus (Zhu et al., 2015). Such
unlabeled data do not emphasize long-dependency
questions and have a mismatched distribution with
the test set, as showed in Section 5. Further, the
Book Corpus is too large to allow rapid algorithm
development for researchers who do not have ac-
cess to a huge amount of computational power.

Aiming to evaluate machines under the same
conditions that the humans are evaluated, there is
a growing interest in obtaining data from exam-
inations. NTCIR QA Lab (Shibuki et al., 2014)
contains a set of real-world college entrance exam
questions. The Entrance Exams task at CLEF QA
Track (Pe˜nas et al., 2014; Rodrigo et al., 2015)
evaluates machine’s reading comprehension abil-

ity. The AI2 Reasoning Challenge (Clark et al.,
2018; Schoenick et al., 2017) contains approx-
imately eight thousand scientiﬁc questions used
in middle school. Lai et al. (2017) proposes the
ﬁrst large-scale machine comprehension dataset
obtained from exams. They show that questions
designed by teachers have a signiﬁcantly larger
proportion of reasoning questions. Our dataset fo-
cuses on evaluating both language proﬁciency and
reasoning abilities.

3 CLOTH Dataset

In this section, we introduce the CLOTH dataset
that is collected from English examinations, and
study its abilities of assessment.

3.1 Data Collection and Statistics

We collect the raw data from three free and pub-
lic websites in China that gather exams created
by English teachers to prepare students for col-
lege/high school entrance exams3. Before clean-
ing, there are 20, 605 passages and 332, 755 ques-
tions. We perform the following processes to en-
sure the validity of data: Firstly, we remove ques-
tions with an inconsistent format such as questions
with more than four options. Then we ﬁlter all
questions whose validity relies on external infor-
mation such as pictures or tables. Further, we ﬁnd
that half of the total passages are duplicates and we
delete those passages. Lastly, on one of the web-
sites, the answers are stored as images. We use two
OCR software programs4 to extract the answers
from images. We discard the questions when re-
sults from the two software are different. After
the cleaning process, we obtain a clean dataset of
7, 131 passages and 99, 433 questions.

Since high school questions are more difﬁ-
cult than middle school questions, we divide the
datasets into CLOTH-M and CLOTH-H, which
stand for the middle school part and the high
school part. We split 11% of the data for both
the test set and the development set. The detailed
statistics of the whole dataset and two subsets are
presented in Table 1. Note that the questions were
created to test non-native speakers, hence the vo-
cabulary size is not very large.

3 The three websites include http://www.21cnjy.com/;
http://5utk.ks5u.com/; http://zujuan.xkw.com/. We checked
that CLOTH does not contain sentence completion example
questions from GRE, SAT and PSAT.

4tesseract:

https://github.com/tesseract-ocr; ABBYY

FineReader: https://www.abbyy.com/en-us/ﬁnereader/

3.2 Question Type Analysis

In order to evaluate students’ mastery of a lan-
guage, teachers usually design tests in a way that
questions cover different aspects of a language.
Speciﬁcally, they ﬁrst identify words in the pas-
sage that can examine students’ knowledge in vo-
cabulary, logic, or grammar. Then, they replace
the words with blanks and prepare three incorrect
but nuanced candidate options to make the test
non-trivial. A sample passage is presented in Ta-
ble 2.

To understand the abilities of assessment on this
dataset, we divide questions into several types and
label the proportion of each type. According to
English teachers who regularly create cloze test
questions for English exams in China, there are
largely three types: grammar, vocabulary and rea-
soning. Grammar questions are easily differen-
tiated from other two categories. However, the
teachers themselves cannot specify a clear distinc-
tion between reasoning questions and vocabulary
questions since all questions require comprehend-
ing the words within the context and conducting
some level of reasoning by recognizing incom-
plete information or conceptual overlap.

Hence, we divided the questions except gram-
mar questions based on the difﬁculty level
for a machine to answer the question, follow-
ing works on analyzing machine comprehension
datasets (Chen et al., 2016; Trischler et al., 2016).
In particular, we divide them in terms of their de-
pendency ranges, since questions that only involve
a single sentence are easier to answer than ques-
tions involving evidence distributed in multiple
sentences. Further, we divided questions involving
long-term dependency into matching/paraphrasing
questions and reasoning questions since matching
questions are easier. The four types include:

• Grammar: The question is about grammar us-
age,
involving tense, preposition usage, ac-
tive/passive voices, subjunctive mood and so on.

• Short-term-reasoning: The question is about
content words and can be answered based on
the information within the same sentence. Note
that the content words can evaluate knowledge
of both vocabulary and reasoning.

• Matching/paraphrasing: The question is an-
swered by copying/paraphrasing a word in the
context.

Dataset

# passages
# questions
Vocab. size

Avg. # sentence
Avg. # words

Train

2,341
22,056

CLOTH-M
Dev

355
3,273
15,096

16.26
242.88

Test

Train

335
3,198

3,172
54,794

Test

Train

478
8,318

5,513
76,850

CLOTH-H
Dev

450
7,794
32,212

18.92
365.1

CLOTH (Total)
Dev

Test

813
11,516

805
11,067
37,235

17.79
313.16

Table 1: The statistics of the training, development and test sets of CLOTH-M (middle school questions),
CLOTH-H (high school questions) and CLOTH

• Long-term-reasoning: The answer must be
inferred from synthesizing information dis-
tributed across multiple sentences.

We sample 100 passages in the high school cat-
egory and the middle school category respectively
with totally 3, 000 questions. The types of these
questions are labeled on Amazon Turk. We pay
$1 and $0.5 for high school passages and middle
school passages respectively. We refer readers to
Appendix A.1 for details of the labeling processes
and the labeled sample passage.

The proportion of different questions is shown
in Table 3. The majority of questions are short-
term-reasoning questions while approximately
22.4% of the data needs long-term information,
in which the long-term-reasoning questions con-
stitute a large proportion.

4 Exploring Models’ Limits

In this section, we investigate if human-created
cloze test is a challenging problem for state-of-
the-art models. We ﬁnd that LM trained on the
One Billion Word Corpus can achieve a remark-
able score but cannot solve the cloze test. After
conducting an error analysis, we hypothesize that
the model is not able to deal with long-term de-
pendencies. We verify the hypothesis by compar-
ing the model’s performance with the human per-
formance when the information humans obtain is
limited to one sentence.

4.1 Human and Model Performance

LSTM To test the performance of RNN-based
supervised models, we train a bidirectional
LSTM (Hochreiter and Schmidhuber, 1997) to
predict the missing word given the context with
only labeled data. The implementation details are
in Appendix A.3.

ment the supervised LSTM model with the at-
tention mechanism (Bahdanau et al., 2014), so
that the representation at the blank is used as a
query to ﬁnd the relevant context in the docu-
ment and a blank-speciﬁc representation of the
document is used to score each candidate an-
swer. Speciﬁcally, we adapt the Stanford Atten-
tive Reader (Chen et al., 2016) and the position-
aware attention model (Zhang et al., 2017) to the
cloze test problem. With the position-aware atten-
tion model, the attention scores are based on both
the context match and the distance from a context
to the blank. Both attention models are trained
only with human-created blanks just as the LSTM
model.

LM In cloze test, the context on both sides may
be enough to determine the correct answer. Sup-
pose xi is the missing word and x1, · · · , xi−1,
xi+1, · · · , xn are the context, we choose xi that
maximizes the joint probability p(x1, · · · , xn),
which essentially maximizes
the conditional
likelihood p(xi
| x1, · · · , xi−1, xi+1, · · · , xn).
Therefore, LM can be naturally adapted to cloze
test.

In essence, LM treats each word as a possible
blank and learns to predict it. As a result, it re-
ceives more supervision than the LSTM trained
on human-labeled questions. Besides training a
neural LM on our dataset, interested in whether
the state-of-the-art LM can solve cloze test, we
also test the LM trained on the One Billion Word
Benchmark (Chelba et al., 2013) (referred as 1B-
LM) that achieves a perplexity of 30.0 (Jozefow-
icz et al., 2016)5. To make the evaluation time
tractable, we limit the context length to one sen-
tence or three sentences. Note that the One Billion
Word Corpus does not overlap with the CLOTH
corpus.

Attentive Readers To enable the model
to
gather information from a longer context, we aug-

5The

pre-trained

model

is

obtained

from

https://github.com/tensorﬂow/models/tree/master/research/lm 1b

Passage: Nancy had just got a job as a secretary in a com-
pany. Monday was the ﬁrst day she went to work, so she
was very 1 and arrived early. She 2 the door open and
found nobody there. ”I am the 3 to arrive.” She thought and
came to her desk. She was surprised to ﬁnd a bunch of 4
on it. They were fresh. She 5 them and they were sweet.
She looked around for a 6 to put them in. ”Somebody has
sent me ﬂowers the very ﬁrst day!” she thought 7 . ” But
who could it be?” she began to 8 . The day passed quickly
and Nancy did everything with 9 interest. For the follow-
ing days of the 10 , the ﬁrst thing Nancy did was to change
water for the followers and then set about her work.
Then came another Monday.
11 she came near her desk
she was overjoyed to see a(n) 12 bunch of ﬂowers there.
She quickly put them in the vase, 13 the old ones. The
same thing happened again the next Monday. Nancy began
to think of ways to ﬁnd out the 14 . On Tuesday afternoon,
she was sent to hand in a plan to the 15 . She waited for
his directives at his secretary’s 16 . She happened to see on
the desk a half-opened notebook, which 17 : ”In order to
keep the secretaries in high spirits, the company has decided
that every Monday morning a bunch of fresh ﬂowers should
be put on each secretarys desk.” Later, she was told that their
general manager was a business management psychologist.

Questions:

D. ﬁrst

D. held
D. bottle

B. pushed
B. second
B. grapes

1. A. depressed B. encouraged C. excited D. surprised
2. A. turned
3. A. last
4. A. keys
5. A. smelled B. ate
6. A. vase
7. A. angrily
8. A. seek
9. A. low
10. A. month
11. A. Unless
12. A. old
13. A. covering B. demanding C. replacing D. forbidding
14. A. sender
15. A. assistant B. colleague C. employee D. manager
16. A. notebook B. desk
17. A. said

C. knocked D. forced
C. third
C. ﬂowers D. bananas
C. took
C. glass
C. strangely D. happily
C. work
C. great
C. year
C. Since
C. blue

B. room
B. quietly
B. wonder
B. little
B. period
B. When
B. red

D. ask
D. general
D. week
D. Before
D. new

C. secretary D. waiter

C. ofﬁce
C. printed

D. house
D. signed

B. receiver

B. written

Table 2: A Sample passage from our dataset. Bold
faces highlight the correct answers. There is only
one best answer among four candidates, although
several candidates may seem correct.

Human performance We measure the perfor-
mance of Amazon Mechanical Turkers on 3, 000
sampled questions when the whole passage is
given.

Results The comparison is shown in Table 4.
Both attentive readers achieve similar accuracy
to the LSTM. We hypothesize that the reason of
the attention model’s unsatisfactory performance
is that the evidence of a question cannot be simply
found by matching the context. Similarly, on read-
ing comprehension, though attention-based mod-
els (Wang et al., 2017; Seo et al., 2016; Dhingra
et al., 2016) have reached human performance on

Short-term

Long-term

Dataset

GM

STR

MP

LTR

O

CLOTH
0.265
CLOTH-M 0.330
CLOTH-H
0.240

0.503
0.413
0.539

0.044
0.068
0.035

0.180
0.174
0.183

0.007
0.014
0.004

Table 3: The question type statistics of 3000 sam-
pled questions where GM, STR, MP, LTR and
O denotes grammar, short-term-reasoning, match-
ing/paraphrasing, long-term-reasoning and others
respectively.

Model

CLOTH CLOTH-M CLOTH-H

LSTM
Stanford AR
Position-aware AR

LM
1B-LM (one sent.)
1B-LM (three sent.)

Human performance

0.484
0.487
0.485

0.548
0.695
0.707

0.859

0.518
0.529
0.523

0.646
0.723
0.745

0.897

0.471
0.471
0.471

0.506
0.685
0.693

0.845

Table 4: Models’ performance and human-level
performance on CLOTH. LSTM, Stanford Atten-
tive Reader and Attentive Reader with position-
aware attention shown in the top part only use
supervised data labelled by human. LM out-
performs LSTM since it receives more supervi-
sions in learning to predict each word. Training
on large external corpus further signiﬁcantly en-
hances LM’s accuracy.

the SQuAD dataset (Rajpurkar et al., 2016), their
performance is still not comparable to human per-
formance on datasets that focus more on reason-
ing where the evidence cannot be simply found by
a matching behavior (Lai et al., 2017; Xu et al.,
2017). Since the focus of this paper is to analyze
the proposed dataset, we leave the design of rea-
soning oriented attention models for future work.
The LM achieves much better performance than
LSTM. The gap is larger when the LM is trained
indicating that
on the 1 Billion Word Corpus,
more training data results in a better generaliza-
tion. Speciﬁcally, the accuracy of 1B-LM is 0.695
when one sentence is used as the context. It in-
dicates that LM can learn sophisticated language
regularities when given sufﬁcient data. The same
conclusion can also be drawn from the success of
a concurrent work ELMo which uses LM repre-
sentations as word vectors and achieves state-of-
the-art results on six language tasks (Peters et al.,
2018). However, if we increase the context length

to three sentences, the accuracy of 1B-LM only
has a marginal improvement. In contrast, humans
outperform 1B-LM by a signiﬁcant margin, which
demonstrates that deliberately designed questions
in CLOTH are not completely solved even for
state-of-the-art models.

4.2 Analyzing 1B-LM’s Strengths and

Weaknesses

In this section, we would like to understand why
1B-LM lags behind human performance. We ﬁnd
that most of the errors involve long-term reason-
ing. Additionally, in a lot of cases, the depen-
dency is within the context of three sentences. We
show several errors made by the 1B-LM in Table
5. In the ﬁrst example, the model does not know
that Nancy found nobody in the company means
that Nancy was the ﬁrst one to arrive at the com-
pany. In the second and third example, the model
fails probably because of not recognizing “they”
referred to “ﬂowers”. The dependency in the last
case is longer. It depends on the fact that Nancy
was alone in the company.

Based on the case study, we hypothesize that
the LM is not able to take long-term information
into account, although it achieves a surprisingly
good overall performance. Additionally, the 1B-
LM is trained on the sentence level, which might
also result in the inability to track paragraph level
information. However, to investigate the differ-
ences between training on sentence level and on
paragraph level, a prohibitive amount of computa-
tional resource is required to train a large model
on the 1 Billion Word Corpus.

On the other hand, a practical comparison is to
test the model’s performance on different types
of questions. We ﬁnd that the model’s accu-
racy is 0.591 on long-term-reasoning questions of
CLOTH-H while it achieves 0.693 on short-term-
reasoning (a comprehensive type-speciﬁc perfor-
mance is available in Appendix A.3), which par-
tially conﬁrms that long-term-reasoning is harder.
However, we could not completely rely on the per-
formance on speciﬁc questions types, partly due to
a large variance caused by the small sample size.
Another reason is that the reliability of question
type labels depends on whether turkers are careful
enough. For example, in the error analysis shown
in Table 5, a careless turker would label the second
example as short-term-reasoning without noticing
that the meaning of “they” relies on a long context.

To objectively verify if the LM’s strengths lie
in dealing with short-term information, we obtain
the ceiling performance of only utilizing short-
term information. Showing only one sentence as
the context, we ask the Turkers to select an option
based on their best guesses given the insufﬁcient
information. By limiting the context span man-
ually, the ceiling performance with the access to
only a short context is estimated accurately.

As shown in Table 6, The performance of 1B-
LM using one sentence as the context can almost
match the human ceiling performance of only us-
ing short-term information. Hence we conclude
that the LM can almost perfectly solve all short-
term cloze questions. However, the performance
of LM is not improved signiﬁcantly when a long-
term context is given, indicating that the perfor-
mance gap is due to the inability of long-term rea-
soning.

5 Comparing Human-created Data and

Automatically-generated Data

In this section, we demonstrate that human-
created data is a better testbed than automatically-
generated cloze test since it results in a larger gap
between model’s performance and human perfor-
mance.

A casual observation is that a cloze test can
be created by randomly deleting words and ran-
In fact, to
domly sampling candidate options.
generate large-scale data, similar generation pro-
cesses have been introduced and widely used in
machine comprehension (Hermann et al., 2015;
Hill et al., 2016; Onishi et al., 2016). However,
research on cloze test design (Sachs et al., 1997)
shows that tests created by deliberately deleting
words are more reliable than tests created by ran-
domly or periodically deleting words. To design
accurate language proﬁciency assessment, teach-
ers usually deliberately select words in order to ex-
amine students’ proﬁciency in grammar, vocabu-
lary and reasoning. Moreover, in order to make the
question non-trivial, three incorrect options pro-
vided by teachers are usually grammatically cor-
rect and relevant to the context. For instance, in
the fourth problem of the sample passage shown
in Table 2, “grapes”, “ﬂowers” and “bananas” all
ﬁt the description of being fresh.

Hence we naturally hypothesize that human-
generated data has distinct characteristics when
compared with automatically-generated data. To

Context

Options

She pushed the door open and found nobody there. ”I am the
thought and came to her desk.

to arrive.” She

A. last

B. second

C. third

D. ﬁrst

They were fresh. She
to put them in.

them and they were sweet. She looked around for a vase

A. smelled

B. ate

C. took

D. held

She smelled them and they were sweet. She looked around for a
”Somebody has sent me ﬂowers the very ﬁrst day!”

to put them in.

A. vase

B. room

C. glass

D. bottle

”But who could it be?” she began to
everything with great interest.

. The day passed quickly and Nancy did

A. seek

B. wonder

C. work

D. ask

Table 5: Error analysis of 1-billion-language-model with three sentences as the context. The questions are
sampled from the sample passage shown in Table 2. The correct answer is in bold text. The incorrectly
selected options are in italics.

Model

CLOTH CLOTH-M CLOTH-H

Short context

Long context

1B-LM
Human

1B-LM
Human

0.695
0.713

0.707
0.859

0.723
0.771

0.745
0.897

0.685
0.691

0.693
0.845

Table 6: Humans’ performance compared with 1-
billion-language-model. In the short context part,
both 1B-LM and humans only use information of
one sentence.
In the long context part, humans
have the whole passage as the context, while 1B-
LM uses contexts of three sentences.

verify this assumption, we compare the LSTM
model’s performance when given different propor-
tions of the two types of data. Speciﬁcally, to train
a model with α percent of automatically-generated
data, we randomly replace a percent blanks with
blanks at random positions, while keeping the re-
maining 1 − α percent questions the same. The
candidate options for the generated blanks are ran-
dom words sampled from the unigram distribu-
tion. We test models obtained with varying α on
human-created data and automatically-generated
data respectively.

spectively, as shown in Tab. 4, leading to a gap
of 0.376. In comparison, the performance gap on
the automatically-generated data is at most 0.185
since the model’s performance reaches an accu-
racy of 0.815 when fully trained on generated data.
(2) Although human-created data may provide
more information in distinguishing similar words,
the distributional mismatch between two types of
data makes it non-trivial to transfer the knowl-
edge gained from human-created data to tackle
automatically-generated data.
the
model’s performance on automatically-generated
data monotonically decreases when given a higher
ratio of human-created data.

Speciﬁcally,

6 Combining Human-created Data with

Automatically-generated Data

In Section 4.1, we show that LM is able to take
advantage of more supervision since it predicts
each word based on the context. At the same
time, we also show that human-created data and
the automatically-generated data are quite differ-
ent in Section 5.
In this section, we propose a
model that takes advantage of both sources.

(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)

Test

α%

0% 25% 50% 75% 100%

6.1 Representative-based Model

human-created
Generated

0.484 0.475 0.469 0.423 0.381
0.422 0.699 0.757 0.785 0.815

Table 7: The model’s performance when trained
on α percent of automatically-generated data and
100 − α percent of human-created data

From the comparison in Table 7, we have the
following observations:
(1) human-created data
leads to a larger gap between model’s perfor-
mance and the ceiling/human performance. The
model’s performance and human’s performance
on the human-created data are 0.484 and 0.859 re-

Speciﬁcally, for each question, regardless of be-
ing human-created or automatically-generated, we
can compute the negative log likelihood of the
correct answer as the loss function. Suppose JH
is the average negative log likelihood loss for
human-created questions and JR is the loss func-
tion on generated questions, we combine losses on
human-created questions and generated questions
by simply adding them together, i.e., JR + JH is
used as the ﬁnal loss function. We will introduce
the deﬁnition of JR in the following paragraphs.

Although automatically-generated data has a
large quantity and is valuable to the model

training, as shown in the previous Section,
automatically-generated questions are quite dif-
ferent from human-created questions.
Ideally, a
large amount of human-created questions is more
desirable than a large amount of automatically-
generated questions. A possible avenue towards
having large-scale human-created data is to au-
tomatically pick out a large number of generated
questions which are representative of or similar to
human-created questions. In other words, we train
a network to predict whether a question is a gener-
ated question or a human-created question. A gen-
erated question is representative of human-created
questions if it has a high probability of being a
human-created question. Then we can give higher
weights to questions that resemble human-created
question.

We ﬁrst introduce our method to obtain the rep-
resentativeness information. Let x denote the pas-
sage and z denote whether a word is selected as
a question by human, i.e., z is 1 if this word is
selected to be ﬁlled in the original passage or 0
otherwise. Suppose hi is the representation of i-th
word given by a bidirectional LSTM. The network
computes the probability pi of xi being a human-
created question as follows:

li = hT

i wxi;

pi = Sigmoid(li)

where li is the logit which will be used as in the
ﬁnal model and wxi is the the word embedding.
We train the network to minimize the binary cross
entropy between p and ground-truth labels at each
token.

After obtaining the representativeness informa-
tion, we deﬁne the representativeness weighted
loss function as

JR =

Softmaxi(

, · · · ,

)Ji

l1
α

ln
α

(cid:88)

i(cid:54)∈H

where Ji denotes the negative log likelihood loss
for the i−th question and let li be the output rep-
resentativeness of the i-th question and H is the
set of all human-generated questions and α is the
temperature of the Softmax function. The model
degenerates into assigning a uniform weight to all
questions when the temperature is +∞. We set α
to 2 based on the performance on the dev set. 6.

6The code is available at https://github.com/qizhex/Large-

scale-Cloze-Test-Dataset-Created-by-Teachers

Model

Ex.

CLOTH CLOTH-M CLOTH-H

Our model
LM
LSTM
Stanford AR

1B-LM

Human

No

Yes

0.583
0.548
0.484
0.487

0.707

0.859

0.673
0.646
0.518
0.529

0.745

0.897

0.549
0.506
0.471
0.471

0.693

0.845

Table 8: Overall results on CLOTH. Ex. denotes
external data.

Model

CLOTH CLOTH-M CLOTH-H

Our model
w.o. rep.
w.o. hum.
w.o. rep. or hum.

0.583
0.566
0.565
0.543

0.673
0.662
0.665
0.643

0.549
0.528
0.526
0.505

Table 9: Ablation study on using the representa-
tiveness information (denoted as rep.)
and the
human-created data (denoted as hum.)

6.2 Results

We summarize performances of all models in Ta-
ble 8. Our representativeness model outperforms
all other models that do not use external data on
CLOTH, CLOTH-H and CLOTH-M.

6.3 Analysis

In this section, we verify the effectiveness of
the representativeness-based averaging by abla-
tion studies. When we remove the representative-
ness information by setting α to inﬁnity, the ac-
curacy drops from 0.583 to 0.566. When we fur-
ther remove the human-created data so that only
generated data is employed, the accuracy drops to
0.543, similar to the performance of LM. The re-
sults further conﬁrm that it is beneﬁcial to incor-
porate human-created questions into training.

A sample of the predicted representativeness is
shown in Figure 17. Clearly, words that are too ob-
vious have low scores, such as punctuation marks,
simple words “a” and “the”. In contrast, content
words whose semantics are directly related to the
context have a higher score, e.g., “same”, “simi-
lar”, “difference” have a high score when the dif-
ference between two objects is discussed and “se-
crets” has a high score since it is related to the sub-
sequent sentence “does not want to share with oth-
ers”. Our prediction model achieves an F1 score of
36.5 on the test set, which is understandable since

7The

script

to generate

is obtained
https://gist.github.com/ihsgnef/

the Figure

at
f13c35cd46624c8f458a4d23589ac768

Figure 1: Representativeness prediction for each word. Lighter color means less representative. The
words deleted by human as blanks are in bold text.

there are many plausible questions within a pas-
sage.

It has been shown that features such as morphol-
ogy information and readability are beneﬁcial in
cloze test prediction (Skory and Eskenazi, 2010;
Correia et al., 2012, 2010; Kurtasov, 2013). We
leave investigating the advanced approaches of au-
tomatically designing cloze test to future work.

7 Conclusion and Discussion

In this paper, we propose a large-scale cloze test
dataset CLOTH that is designed by teachers. With
missing blanks and candidate options carefully
created by teachers to test different aspects of lan-
guage phenomena, CLOTH requires a deep lan-
guage understanding and better captures the com-
plexity of human language. We ﬁnd that hu-
man outperforms 1B-LM by a signiﬁcant mar-
gin. After detailed analysis, we ﬁnd that the
performance gap is due to the model’s inability
to understanding a long context. We also show
that, compared to automatically-generated ques-
tions, human-created questions are more difﬁcult
and lead to a larger margin between human per-
formance and the model’s performance.

Despite the excellent performance of 1B-LM
when compared with models trained only on
CLOTH, it is still important to investigate and cre-
ate more effective models and algorithms which
provide complementary advantages to having a
large amount of data. For rapid algorithm devel-
opments, we suggest training models only on the
training set of CLOTH and comparing with mod-
els that do not utilize external data.

We hope our dataset provides a valuable testbed
to the language modeling community and the
machine comprehension community.
In partic-
ular, the language modeling community can use

CLOTH to evaluate their models’ abilities in mod-
eling a long context.
In addition, the machine
comprehension community may also ﬁnd CLOTH
useful in evaluating machine’s understanding of
language phenomena including vocabulary, rea-
soning and grammar, which are key components
of comprehending natural language.

In our future work, we would like to design al-
gorithms to better model a long context, to utilize
external knowledge, and to explore more effec-
tive semi-supervised learning approaches. Firstly,
we would like to investigate efﬁcient ways of uti-
lizing external knowledge such as paraphrasing
and semantic concepts like prior works (Dong
et al., 2017; Dasigi et al., 2017). In comparison,
training on a large external dataset is actually a
time-consuming way of utilizing external knowl-
edge. Secondly, to use the generated questions
more effectively, the representative-based semi-
supervised approach might be improved by tech-
niques studied in active learning and hard exam-
ple mining (Settles, 2009; Shrivastava et al., 2016;
Chang et al., 2017).

Acknowledgement

We thank Yulun Du, Kaiyu Shi and Zhilin Yang
for insightful discussions and suggestions on the
draft. We thank Shi Feng for the script to high-
light representative words. This research was sup-
ported in part by DARPA grant FA8750-12-2-
0342 funded under the DEFT program.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Haw-Shiuan Chang, Erik Learned-Miller, and Andrew
McCallum. 2017. Active bias: Training more accu-
rate neural networks by emphasizing high variance
In Advances in Neural Information Pro-
samples.
cessing Systems, pages 1003–1013.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for mea-
suring progress in statistical lming. arXiv preprint
arXiv:1312.3005.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016.
the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858.

A thorough examination of

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.

Rui Correia, Jorge Baptista, Maxine Eskenazi, and
Nuno J Mamede. 2012. Automatic generation of
cloze question stems. In PROPOR, pages 168–178.
Springer.

Rui Correia, Jorge Baptista, Nuno Mamede, Isabel
Trancoso, and Maxine Eskenazi. 2010. Automatic
In Pro-
generation of cloze question distractors.
ceedings of the Interspeech 2010 Satellite Workshop
on Second Language Studies: Acquisition, Learn-
ing, Education and Technology, Waseda University,
Tokyo, Japan.

Pradeep Dasigi, Waleed Ammar, Chris Dyer, and Ed-
uard Hovy. 2017. Ontology-aware token embed-
dings for prepositional phrase attachment. arXiv
preprint arXiv:1705.02925.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549.

Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella
Lapata. 2017. Learning to paraphrase for question
answering. arXiv preprint arXiv:1708.06022.

Sandra S Fotos. 1991. The cloze test as an integra-
tive measure of eﬂ proﬁciency: A substitute for es-
says on college entrance examinations? Language
Learning, 41(3):313–336.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In NIPS.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Jon Jonz. 1991. Cloze item types and second language

comprehension. Language testing, 8(1):1–22.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of lming. arXiv preprint arXiv:1602.02410.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Andrey Kurtasov. 2013. A system for generating cloze
test items from russian-language text. In Proceed-
ings of the Student Research Workshop associated
with RANLP 2013, pages 107–112.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017.
Race: Large-scale
reading comprehension dataset from examinations.
EMNLP.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268.

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457.

Denis Paperno, Germ´an Kruszewski, Angeliki Lazari-
dou, Quan Ngoc Pham, Raffaella Bernardi, San-
dro Pezzelle, Marco Baroni, Gemma Boleda, and
Raquel Fern´andez. 2016.
The lambada dataset:
Word prediction requiring a broad discourse context.
arXiv preprint arXiv:1606.06031.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS-W.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes), pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2016. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. ICLR.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen,
and Xiaodong Liu. 2017. Towards human-level ma-
chine reading comprehension: Reasoning and in-
arXiv preprint
ference with multiple strategies.
arXiv:1711.04964.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot ﬁll-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 35–45.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
In Proceedings of the IEEE
and reading books.
international conference on computer vision, pages
19–27.

Geoffrey Zweig and Christopher JC Burges. 2011. The
microsoft research sentence completion challenge.
Technical report, Technical Report MSR-TR-2011-
129, Microsoft.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Squad: 100,000+ questions
Percy Liang. 2016.
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

J Sachs, P Tung, and RYH Lam. 1997. How to con-
struct a cloze test: Lessons from testing measure-
ment theory models. Perspectives.

Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter
Turney, and Oren Etzioni. 2017. Moving beyond the
turing test with the allen ai science challenge. Com-
munications of the ACM, 60(9):60–64.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
ﬂow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Burr Settles. 2009. Active learning literature survey.

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-
shick. 2016. Training region-based object detectors
with online hard example mining. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 761–769.

Adam Skory and Maxine Eskenazi. 2010. Predicting
In Pro-
cloze task quality for vocabulary training.
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 49–56. Association for Compu-
tational Linguistics.

Wilson L Taylor. 1953.

a new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.

cloze procedure:

Annie Tremblay. 2011. Proﬁciency assessment stan-
dards in second language acquisition research. Stud-
ies in Second Language Acquisition, 33(3):339–372.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

(a) Middle school group (CLOTH-M)

(b) High school group (CLOTH-H)

Figure 2: Model and human’s performance on questions with different types. Our model will be intro-
duced in Sec. 6.

A Appendix

A.1 Question Type Labeling

To label the questions, we provided the deﬁnition and an example for each question category to the
Amazon Mechanical Turkers. To ensure quality, we limited the workers to master Turkers who are expe-
rienced and maintain a high acceptance rate. However, we did not restrict the backgrounds of the Turkers
since master Turkers should have a reasonable amount of knowledge about English to conduct previous
tasks. In addition, the vocabulary used in CLOTH are usually not difﬁcult since they are constructed to
test non-native speakers in middle school or high school. To get a concrete idea of the nature of question
types, please refer to examples shown in Tab. 10.

A.2 Type-speciﬁc Performance Analysis

We can also further verify the strengths and weaknesses of the 1B-LM by studying the performance of
models and human on different question categories. Note that the performance presented here may be
subject to a high variance due to the limited number of samples in each category. From the comparison
shown in Figure 2, we see that 1B-LM is indeed good at short-term questions. Speciﬁcally, when the
human only has access to the context of one sentence, 1B-LM is close to human’s performance on
almost all categories. Further, comparing LM and 1B-LM, we ﬁnd that training on the large corpus leads
to improvements on all categories, showing that training on a large amount of data leads to a substantial
improvement in learning complex language regularities.

A.3

Implementation Details

We implement our models using PyTorch (Paszke et al., 2017). We train our model on all questions in
CLOTH and test it on CLOTH-M and CLOTH-H separately. For our ﬁnal model, we use Adam (Kingma
and Ba, 2014) with the learning rate of 0.001. The hidden dimension is set to 650 and we initialize
the word embedding by 300-dimensional Glove word vector (Pennington et al., 2014). The temperature
α is set to 2. We tried to increase the dimensionality of the model but do not observe performance
improvement.

When we train the small LM on CLOTH, we largely follow the recommended hyperparameters in the
Pytorch LM example8. Speciﬁcally, we employ a 2-layer LSTM with hidden dimension as 1024. The
input embedding and output weight matrix are tied. We set the dropout rate to 0.5. The initial learning
rate is set to 10 and divided by 4 whenever the PPL stops improving on the dev set.

We predict the answer for each blank independently for all of the models mentioned in this paper,
since we do not observe signiﬁcant performance improvements in our preliminary experiments when an
auto-regressive approach is employed, i.e., when we ﬁll all previous blanks with predicted answers. We
hypothesize that, regardless of whether there exist inter-blank dependencies, since blanks are usually

8https://github.com/pytorch/examples/tree/master/word language model

distributed far away from each other, LSTM is not able to capture such long dependencies. When testing
language models, we use the longest text spans that do not contain blanks.

Passage: Nancy had just got a job as a secretary in a company. Monday was the ﬁrst day she went
to work, so she was very 1 and arrived early. She 2 the door open and found nobody there. ”I
am the 3 to arrive.” She thought and came to her desk. She was surprised to ﬁnd a bunch of 4
on it. They were fresh. She 5 them and they were sweet. She looked around for a 6 to put
them in. ”Somebody has sent me ﬂowers the very ﬁrst day!” she thought 7 . ” But who could it
be?” she began to 8 . The day passed quickly and Nancy did everything with 9 interest. For
the following days of the 10 , the ﬁrst thing Nancy did was to change water for the followers and
then set about her work.
Then came another Monday. 11 she came near her desk she was overjoyed to see a(n) 12 bunch
of ﬂowers there. She quickly put them in the vase, 13 the old ones. The same thing happened
again the next Monday. Nancy began to think of ways to ﬁnd out the 14 . On Tuesday afternoon,
she was sent to hand in a plan to the 15 . She waited for his directives at his secretary’s 16
. She happened to see on the desk a half-opened notebook, which 17 : ”In order to keep the
secretaries in high spirits, the company has decided that every Monday morning a bunch of fresh
ﬂowers should be put on each secretarys desk.” Later, she was told that their general manager was
a business management psychologist.

A. depressed
A. turned
A. last
A. keys
A. smelled
A. vase
A. angrily
A. seek
A. low

1.
2.
3.
4.
5.
6.
7.
8.
9.
10. A. month
11. A. Unless
12. A. old
13. A. covering
14. A. sender
15. A. assistant
16. A. notebook
17. A. said

Questions

D. surprised
D. forced
D. ﬁrst
D. bananas
D. held
D. bottle
D. happily
D. ask
D. general
D. week
D. Before
D. new

B. encouraged C. excited
B. pushed
C. knocked
C. third
B. second
C. ﬂowers
B. grapes
C. took
B. ate
C. glass
B. room
C. strangely
B. quietly
B. wonder
C. work
C. great
B. little
C. year
B. period
B. When
C. Since
C. blue
B. red
C. replacing D. forbidding
B. demanding
C. secretary
B. receiver
C. employee
B. colleague
B. desk
C. ofﬁce
C. printed
B. written

D. waiter
D. manager
D. house
D. signed

Question type
short-term reasoning
short-term reasoning
long-term reasoning
matching
short-term reasoning
long-term reasoning
short-term reasoning
long-term reasoning
long-term reasoning
long-term reasoning
grammar
long-term reasoning
long-term reasoning
long-term reasoning
matching
matching
grammar

Table 10: An Amazon Turker’s label for the sample passage

