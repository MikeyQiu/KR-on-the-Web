7
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
4
0
0
.
6
0
7
1
:
v
i
X
r
a

NMTPY: A FLEXIBLE TOOLKIT FOR ADVANCED
NEURAL MACHINE TRANSLATION SYSTEMS

Ozan Caglayan, Mercedes García-Martínez, Adrien Bardet, Walid Aransa,
Fethi Bougares, Loïc Barrault
Laboratoire d’Informatique de l’Université du Maine (LIUM)
Language and Speech Technology (LST) Team
Le Mans, France

ABSTRACT

In this paper, we present nmtpy, a ﬂexible Python toolkit based on Theano for
training Neural Machine Translation and other neural sequence-to-sequence ar-
chitectures. nmtpy decouples the speciﬁcation of a network from the training and
inference utilities to simplify the addition of a new architecture and reduce the
amount of boilerplate code to be written. nmtpy has been used for LIUM’s top-
ranked submissions to WMT Multimodal Machine Translation and News Trans-
lation tasks in 2016 and 2017.

1 OVERVIEW

nmtpy is a refactored, extended and Python 3 only version of dl4mt-tutorial 1, a Theano (Theano De-
velopment Team, 2016) implementation of attentive Neural Machine Translation (NMT) (Bahdanau
et al., 2014).

The development of nmtpy project which has been open-sourced2 under MIT license in March 2017,
started in March 2016 as an effort to adapt dl4mt-tutorial to multimodal translation models. nmtpy
has now become a powerful toolkit where adding a new model is as simple as deriving from an
abstract base class to ﬁll in a set of fundamental methods and (optionally) implementing a custom
data iterator. The training and inference utilities are as model-agnostic as possible allowing one
to use them for different sequence generation networks such as multimodal NMT and image cap-
tioning to name a few. This ﬂexibility and the rich set of provided architectures (Section 3) is
what differentiates nmtpy from Nematus (Sennrich et al., 2017) another NMT software derived from
dl4mt-tutorial.

2 WORKFLOW

Figure 1 describes the general workﬂow of a training session. An experiment in nmtpy is described
with a conﬁguration ﬁle (Appendix A) to ensure reusability and reproducibility. A training ex-
periment can be simply launched by providing this conﬁguration ﬁle to nmt-train which sets up
the environment and starts the training. Speciﬁcally nmt-train automatically selects a free GPU,
sets the seed for all random number generators and ﬁnally creates a model (model_type option)
instance. Architecture-speciﬁc steps like data loading, weight initialization and graph construc-
tion are delegated to the model instance. The corresponding log ﬁle and model checkpoints are
named in a way to reﬂect the experiment options determined by the conﬁguration ﬁle (Example:
model_type-e<embdim>-r<rnndim>-<opt>_<lrate>...).

Once everything is ready, nmt-train starts consuming mini-batches of data from the model’s iterator
to perform forward/backward passes along with the weight updates. A validation on held-out corpus
is periodically performed to evaluate the generalization performance of the model. Speciﬁcally, after
each valid_freq updates, nmt-train calls the nmt-translate utility which will perform beam-

1https://github.com/nyu-dl/dl4mt-tutorial
2https://github.com/lium-lst/nmtpy

1

Figure 1: The components of nmtpy.

search decoding, compute the requested metrics and return the results back so that nmt-train can
track the progress and save best checkpoints to disk.

Several examples regarding the usage of the utilities are given in Appendix B.

2.1 ADDING NEW ARCHITECTURES

New architectures can be deﬁned by creating a new ﬁle under nmtpy/models/ using a copy of
an existing architecture and modifying the following predeﬁned methods:

• __init__(): Instantiates a model. Keyword arguments can be used to add options
speciﬁc to the architecture that will be automatically gathered from the conﬁguration ﬁle
by nmt-train.

• init_params(): Initializes the layers and weights.

• build(): Deﬁnes the Theano computation graph that will be used during training.

• build_sampler(): Deﬁnes the Theano computation graph that will be used during
beam-search. This is generally very similar to build() but with sequential RNN steps
and non-masked tensors.

• load_valid_data(): Loads the validation data for perplexity computation.

• load_data(): Loads the training data.

2.2 BUILDING BLOCKS

In this section, we introduce the currently available components and features of nmtpy that one can
use to design their architecture.

Training nmtpy provides Theano implementations of stochastic gradient descent (SGD) and its
adaptive variants RMSProp (Tieleman & Hinton, 2012), Adadelta (Zeiler, 2012) and Adam (Kingma
& Ba, 2014) to optimize the weights of the trained network. A preliminary support for gradient noise
(Neelakantan et al., 2015) is available for Adam. Gradient norm clipping (Pascanu et al., 2013)
is enabled by default with a threshold of 5 to avoid exploding gradients. Although the provided
architectures all use the cross-entropy objective by their nature, any arbitrary differentiable objective
function can be used since the training loop is agnostic to the architecture being trained.

Regularization A dropout (Srivastava et al., 2014) layer which can be placed after any arbitrary
feed-forward layer in the architecture is available. This layer works in inverse mode where the
magnitudes are scaled during training instead of testing. Additionally, L2 regularization loss with a
scalar factor deﬁned by decay_c option in the conﬁguration can be added to the training loss.

2

Initialization The weight initialization is governed by the weight_init option and supports
Xavier (Glorot & Bengio, 2010) and He (He et al., 2015) initialization methods besides orthogonal
(Saxe et al., 2013) and random normal.

Layers The following layers are available in the latest version of nmtpy:

• Feed-forward and highway layer (Srivastava et al., 2015)
• Gated Recurrent Unit (GRU) (Chung et al., 2014)
• Conditional GRU (CGRU) (Firat & Cho, 2016)
• Multimodal CGRU (Caglayan et al., 2016a;b)

Layer normalization (Ba et al., 2016), a method that adaptively learns to scale and shift the incoming
activations of a neuron, can be enabled for GRU and CGRU blocks.

Iteration Parallel and monolingual text iterators with compressed (.gz, .bz2, .xz) ﬁle support are
available under the names TextIterator and BiTextIterator. Additionally, the multimodal WMTItera-
tor allows using image features and source/target sentences at the same time for multimodal NMT
(Section 3.3). We recommend using shuffle_mode:trglen when implemented to speed up
the training by efﬁciently batching same-length sequences.

Post-processing All decoded translations will be post-processed if filter option is given in the
conﬁguration ﬁle. This is useful in the case where one would like to compute automatic metrics on
surface forms instead of segmented. Currently available ﬁlters are bpe and compound for cleaning
subword BPE (Sennrich et al., 2016) and German compound-splitting (Sennrich & Haddow, 2015)
respectively.

Metrics nmt-train performs a patience based early-stopping using either validation perplexity or
one of the following external evaluation metrics:

• bleu: Wrapper around Moses multi-bleu BLEU (Papineni et al., 2002)
• bleu_v13a: A Python reimplementation of Moses mteval-v13a.pl BLEU
• meteor: Wrapper around METEOR (Lavie & Agarwal, 2007)

The above metrics are also available for nmt-translate to immediately score the produced hypothe-
ses. Other metrics can be easily added and made available as early-stopping metrics.

3 ARCHITECTURES

3.1 NMT

The default NMT architecture (attention) is based on the original dl4mt-tutorial implementation
which differs from Bahdanau et al. (2014) in the following major aspects:

• CGRU decoder which consists of two GRU layers interleaved with attention mechanism.
• The hidden state of the decoder is initialized with a non-linear transformation applied to

mean bi-directional encoder state in contrast to last bi-directional encoder state.

• The Maxout (Goodfellow et al., 2013) hidden layer before the softmax operation is re-

moved.

In addition, nmtpy offers the following conﬁgurable options for this NMT:

• layer_norm Enables/disables layer normalization for bi-directional GRU encoder.
• init_cgru Allows initializing CGRU with all-zeros instead of mean encoder state.
• n_enc_layers Number of additional unidirectional GRU encoders to stack on top of bi-

directional encoder.

3

• tied_emb Allows sharing feedback embeddings and output embeddings (2way) or all em-

beddings in the network (3way) (Inan et al., 2016; Press & Wolf, 2016).

• *_dropout Dropout probabilities for three dropout layers placed after source embeddings
(emb_dropout), encoder hidden states (ctx_dropout) and pre-softmax activations
(out_dropout).

3.2 FACTORED NMT

Factored NMT (FNMT) is an extension of NMT which is able to generate two output symbols. The
architecture of such a model is presented in Figure 2. In contrast to multi-task architectures, FNMT
outputs share the same recurrence and output symbols are generated in a synchronous fashion3.

Figure 2: Global architecture of the Factored NMT system.

Two FNMT variants which differ in how they handle the output layer are currently available:

• attention_factors: the lemma and factor embeddings are concatenated to form a single

feedback embedding.

• attention_factors_seplogits: the output path for lemmas and factors are kept separate with

different pre-softmax transformations applied for specialization.

FNMT with lemmas and linguistic factors has been successfully used for
English→French (García-Martínez
English→Czech evaluation campaigns.

IWSLT’16
and WMT’174 English→Latvian and

al., 2016)

et

3.3 MULTIMODAL NMT & CAPTIONING

We provide several multimodal architectures (Caglayan et al., 2016a;b) where the probability of
a target word is conditioned on source sentence representations and convolutional image features
(Figure 3). More speciﬁcally, these architectures extends monomodal CGRU into a multimodal one
where the attention mechanism can be shared or separate between input modalities. A late fusion of
attended context vectors are done using either by summing or concatenating the modality-speciﬁc
representations.

Our attentive multimodal system for Multilingual Image Description Generation track of WMT’16
Multimodal Machine Translation surpassed the baseline architecture (Elliott et al., 2015) by +1.1
METEOR and +3.4 BLEU and ranked ﬁrst among multimodal submissions (Specia et al., 2016).

3.4 LANGUAGE MODELING

A GRU-based language model architecture (rnnlm) is available in the repository which can be used
with nmt-test-lm to obtain language model scores.

3FNMT currently uses a dedicated nmt-translate-factors utility though it will probably be merged in the

near future.

4http://matrix.statmt.org/

4

Figure 3: The architecture of multimodal attention (Caglayan et al., 2016b).

3.5

IMAGE CAPTIONING

A GRU-based reimplementation of Show, Attend and Tell architecture (Xu et al., 2015) which learns
to generate a natural language description by applying soft attention over convolutional image fea-
tures is available under the name img2txt. This architecture is recently used 5 as a baseline system
for the Multilingual Image Description Generation track of WMT’17 Multimodal Machine Trans-
lation shared task.

In this section we present translation and rescoring utilities nmt-translate and nmt-rescore. Other
auxiliary utilities are brieﬂy described in Appendix C.

4 TOOLS

4.1 NMT-TRANSLATE

nmt-translate is responsible for translation decoding using the beam-search method deﬁned by NMT
architecture. This default beam-search supports single and ensemble decoding for both monomodal
and multimodal translation models. If a given architecture reimplements the beam-search method in
its class, that one will be used instead.

Since the number of CPUs in a single machine is 2x-4x higher than the number of GPUs and we
mainly reserve the GPUs for training, nmt-translate makes use of CPU workers for maximum ef-
ﬁciency. More speciﬁcally, each worker receives a model instance (or instances when ensembling)
and performs the beam-search on samples that it continuously fetches from a shared queue. This
queue is ﬁlled by the master process using the iterator provided by the model.

One thing to note for parallel CPU decoding is that if the numpy is linked against a BLAS implemen-
tation with threading support enabled (as in the case with Anaconda & Intel MKL), each spawned
process attempts to use all available threads in the machine leading to a resource conﬂict. In order
for nmt-translate to beneﬁt correctly from parallelism, the number of threads per process is thus
limited 6 to 1. The impact of this setting and the overall decoding speed in terms of words/sec (wps)
are reported in (Table 1) for a medium-sized En→Tr NMT with ∼10M parameters.

4.2 NMT-RESCORE

A 1-best plain text or n-best hypotheses ﬁle can be rescored with nmt-rescore using either a single or
an ensemble of models. Since rescoring of a given hypothesis simply means computing the negative

5http://www.statmt.org/wmt17/multimodal-task.html
6This

is achieved by setting X_NUM_THREADS=1 environment variable where X is one of

OPENBLAS,OMP,MKL depending on the numpy installation.

5

# BLAS Threads Tesla K40

4 CPU

8 CPU 16 CPU

Default
Set to 1

185 wps
185 wps

26 wps
109 wps

25 wps
198 wps

25 wps
332 wps

Table 1: Median beam-search speed over 3 runs with beam size 12: decoding on a single Tesla K40
GPU is rougly equivalent to using 8 CPUs (Intel Xeon E5-2687v3).

log-likelihood of it given the source sentence, nmt-rescore uses a single GPU to efﬁciently compute
the scores in batched mode. See Appendix B for examples.

5 CONCLUSION

We have presented nmtpy, an open-source sequence-to-sequence framework based on dl4mt-tutorial
and reﬁned in many ways to ease the task of integrating new architectures. The toolkit has been inter-
nally used in our team for tasks ranging from monomodal, multimodal and factored NMT to image
captioning and language modeling to help achieving top-ranked submissions during campaigns like
IWSLT and WMT.

This work was supported by the French National Research Agency (ANR) through the CHIST-ERA
M2CR project, under the contract number ANR-15-CHR2-0006-017.

ACKNOWLEDGMENTS

REFERENCES

arXiv:1607.06450, 2016.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes García-Martínez, Fethi
Bougares, Loïc Barrault, and Joost van de Weijer. Does multimodality help human and machine
for translation and image captioning? In Proceedings of the First Conference on Machine Trans-
lation, pp. 627–633, Berlin, Germany, August 2016a. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/W/W16/W16-2358.

Ozan Caglayan, Loïc Barrault, and Fethi Bougares. Multimodal attention for neural machine trans-

lation. arXiv preprint arXiv:1609.03976, 2016b.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325, 2015.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. URL
http://arxiv.org/abs/1412.3555.

Desmond Elliott, Stella Frank, and Eva Hasler. Multi-language image description with neural
sequence models. CoRR, abs/1510.04709, 2015. URL http://arxiv.org/abs/1510.
04709.

Orhan Firat and Kyunghyun Cho. Conditional gated recurrent unit with attention mechanism.
github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf, 2016.

Mercedes García-Martínez, Loïc Barrault, and Fethi Bougares. Factored neural machine transla-
In Proceedings of the International Workshop on Spoken Language Trans-
tion architectures.
lation, IWSLT’16, Seattle, USA, 2016. URL http://workshop2016.iwslt.org/
downloads/IWSLT_2016_paper_2.pdf.

7http://m2cr.univ-lemans.fr

6

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neu-
In In Proceedings of the International Conference on Artiﬁcial Intelligence and

ral networks.
Statistics (AISTATS’10). Society for Artiﬁcial Intelligence and Statistics, 2010.

Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp.
1319–1327, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.
mlr.press/v28/goodfellow13.html.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
In Computer Vision (ICCV), 2015 IEEE

human-level performance on imagenet classiﬁcation.
International Conference on, pp. 1026–1034. IEEE, 2015.

Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A

loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for mt evaluation with high levels
In Proceedings of the Second Workshop on Statistical
of correlation with human judgments.
Machine Translation, StatMT ’07, pp. 228–231, Stroudsburg, PA, USA, 2007. Association for
Computational Linguistics.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.

In Stan Szpakowicz
Marie-Francine Moens (ed.), Text Summarization Branches Out: Proceedings of the ACL-04
Workshop, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807, 2015.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ’02, pp. 311–318, Stroudsburg, PA, USA, 2002.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of The 30th International Conference on Machine Learning, pp. 1310–
1318, 2013.

Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint

arXiv:1608.05859, 2016.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-

ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Rico Sennrich and Barry Haddow. A joint dependency model of morphological and syntactic struc-
In Proceedings of the 2015 Conference on Empirical
ture for statistical machine translation.
Methods in Natural Language Processing, pp. 114–121. Association for Computational Linguis-
tics, 2015.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
In Proceedings of the 54th Annual Meeting of the Association for Com-
with subword units.
putational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
P16-1162.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch-Mayne, Barry Haddow, Julian
Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli, Antonio Miceli Barone, Jozef Mokry, and
Maria Nadejde. Nematus: a Toolkit for Neural Machine Translation, pp. 65–68. Association for
Computational Linguistics (ACL), 4 2017. ISBN 978-1-945626-34-0.

7

Lucia Specia, Stella Frank, Khalil Sima’an, and Desmond Elliott. A shared task on multimodal
machine translation and crosslingual image description. In Proceedings of the First Conference
on Machine Translation, Berlin, Germany. Association for Computational Linguistics, 2016.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint

arXiv:1505.00387, 2015.

Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. arXiv e-prints, abs/1605.02688, 2016. URL http://arxiv.org/abs/1605.
02688.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4566–4575, 2015.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Proceedings of The 32nd International Conference on Machine Learning, pp. 2048–
2057, 2015.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

2012.

8

A CONFIGURATION FILE EXAMPLE

# Options in this section are consumed by nmt-train
[training]
model_type: attention
patience: 20
valid_freq: 1000
valid_metric: meteor
valid_start: 2
valid_beam: 3
valid_njobs: 16
valid_save_hyp: True
decay_c: 1e-5
clip_c: 5
seed: 1235
save_best_n: 2
device_id: auto
snapshot_freq: 10000
max_epochs: 100

# Model type without .py
# early-stopping patience
# Compute metrics each 1000 updates
# Use meteor during validations
# Start validations after 2nd epoch
# Decode with beam size 3
# Use 16 processes for beam-search
# Save validation hypotheses
# L2 regularization factor
# Gradient clip threshold
# Seed for numpy and Theano RNG
# Keep 2 best models on-disk
# Pick 1st available GPU
# Save a resumeable snapshot

# weight-tying mode (False,2way,3way)
# layer norm in GRU encoder
# Shuffled/length-ordered batches
# post-processing filter(s)
# limit src vocab if > 0
# limit trg vocab if > 0
# Where to store checkpoints
# Encoder and decoder RNN dim
# All embedding dim

# Options below are passed to model instance
[model]
tied_emb: 2way
layer_norm: True
shuffle_mode: trglen
filter: bpe
n_words_src: 0
n_words_trg: 0
save_path: ~/models
rnn_dim: 100
embedding_dim: 100
weight_init: xavier
batch_size: 32
optimizer: adam
lrate: 0.0004
emb_dropout: 0.2
ctx_dropout: 0.4
out_dropout: 0.4

# Set dropout rates

# Dictionary files produced by nmt-build-dict
[model.dicts]
src: ~/data/train.norm.max50.tok.lc.bpe.en.pkl
trg: ~/data/train.norm.max50.tok.lc.bpe.de.pkl

# Training and validation data
[model.data]
train_src
train_trg
valid_src
valid_trg
valid_trg_orig: ~/data/val.norm.tok.lc.de

: ~/data/train.norm.max50.tok.lc.bpe.en
: ~/data/train.norm.max50.tok.lc.bpe.de
: ~/data/val.norm.tok.lc.bpe.en
: ~/data/val.norm.tok.lc.bpe.de

9

B USAGE EXAMPLES

# Launch an experiment
$ nmt-train -c wmt-en-de.conf

# Launch an experiment with different architecture
$ nmt-train -c wmt-en-de.conf ’model_type:my_amazing_nmt’

# Change dimensions
$ nmt-train -c wmt-en-de.conf ’rnn_dim:500’ ’embedding_dim:300’

# Force specific GPU device
$ nmt-train -c wmt-en-de.conf ’device_id:gpu5’

Listing 1: Example usage patterns for nmt-train.

# Decode on 30 CPUs with beam size 10, compute BLEU/METEOR
# Language for METEOR is set through source file suffix (.en)
$ nmt-translate -j 30 -m best_model.npz -S val.tok.bpe.en \

-R val.tok.de -o out.tok.de -M bleu meteor -b 10

# Generate n-best list with an ensemble of checkpoints
$ nmt-translate -m model*npz -S val.tok.de \

-o out.tok.50best.de -b 50 -N 50

# Generate json file with alignment weights (-e)
$ nmt-translate -m best_model.npz -S val.tok.bpe.en \

-R val.tok.de -o out.tok.de -e

Listing 2: Example usage patterns for nmt-translate.

# Rescore 50-best list with ensemble of models
$ nmt-rescore -m model*npz -s val.tok.bpe.en \

-t out.tok.50best.de \
-o out.tok.50best.rescored.de

Listing 3: Example usage patterns for nmt-rescore.

C DESCRIPTION OF THE PROVIDED TOOLS

nmt-build-dict Generates .pkl vocabulary ﬁles from preprocessed corpus. A single/combined vo-
cabulary for two or more languages can be created with -s ﬂag.

nmt-extract Extracts arbitrary weights from a model snapshot which can further be used as pre-
trained weights of a new experiment or analyzed using visualization techniques (especially for em-
beddings).

nmt-coco-metrics A stand-alone utility which computes multi-reference BLEU, METEOR,
CIDE-r (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) using MSCOCO evaluation tools (Chen
et al., 2015). Multiple systems can be given with -s ﬂag to produce a table of scores.

nmt-bpe-(learn,apply) Copy of subword utilities 8 (Sennrich et al., 2016) which are used to ﬁrst
learn a BPE segmentation model over a given corpus ﬁle and then apply it to new sentences.

nmt-test-lm Computes language model perplexity of a given corpus.

8https://github.com/rsennrich/subword-nmt

10

7
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
4
0
0
.
6
0
7
1
:
v
i
X
r
a

NMTPY: A FLEXIBLE TOOLKIT FOR ADVANCED
NEURAL MACHINE TRANSLATION SYSTEMS

Ozan Caglayan, Mercedes García-Martínez, Adrien Bardet, Walid Aransa,
Fethi Bougares, Loïc Barrault
Laboratoire d’Informatique de l’Université du Maine (LIUM)
Language and Speech Technology (LST) Team
Le Mans, France

ABSTRACT

In this paper, we present nmtpy, a ﬂexible Python toolkit based on Theano for
training Neural Machine Translation and other neural sequence-to-sequence ar-
chitectures. nmtpy decouples the speciﬁcation of a network from the training and
inference utilities to simplify the addition of a new architecture and reduce the
amount of boilerplate code to be written. nmtpy has been used for LIUM’s top-
ranked submissions to WMT Multimodal Machine Translation and News Trans-
lation tasks in 2016 and 2017.

1 OVERVIEW

nmtpy is a refactored, extended and Python 3 only version of dl4mt-tutorial 1, a Theano (Theano De-
velopment Team, 2016) implementation of attentive Neural Machine Translation (NMT) (Bahdanau
et al., 2014).

The development of nmtpy project which has been open-sourced2 under MIT license in March 2017,
started in March 2016 as an effort to adapt dl4mt-tutorial to multimodal translation models. nmtpy
has now become a powerful toolkit where adding a new model is as simple as deriving from an
abstract base class to ﬁll in a set of fundamental methods and (optionally) implementing a custom
data iterator. The training and inference utilities are as model-agnostic as possible allowing one
to use them for different sequence generation networks such as multimodal NMT and image cap-
tioning to name a few. This ﬂexibility and the rich set of provided architectures (Section 3) is
what differentiates nmtpy from Nematus (Sennrich et al., 2017) another NMT software derived from
dl4mt-tutorial.

2 WORKFLOW

Figure 1 describes the general workﬂow of a training session. An experiment in nmtpy is described
with a conﬁguration ﬁle (Appendix A) to ensure reusability and reproducibility. A training ex-
periment can be simply launched by providing this conﬁguration ﬁle to nmt-train which sets up
the environment and starts the training. Speciﬁcally nmt-train automatically selects a free GPU,
sets the seed for all random number generators and ﬁnally creates a model (model_type option)
instance. Architecture-speciﬁc steps like data loading, weight initialization and graph construc-
tion are delegated to the model instance. The corresponding log ﬁle and model checkpoints are
named in a way to reﬂect the experiment options determined by the conﬁguration ﬁle (Example:
model_type-e<embdim>-r<rnndim>-<opt>_<lrate>...).

Once everything is ready, nmt-train starts consuming mini-batches of data from the model’s iterator
to perform forward/backward passes along with the weight updates. A validation on held-out corpus
is periodically performed to evaluate the generalization performance of the model. Speciﬁcally, after
each valid_freq updates, nmt-train calls the nmt-translate utility which will perform beam-

1https://github.com/nyu-dl/dl4mt-tutorial
2https://github.com/lium-lst/nmtpy

1

Figure 1: The components of nmtpy.

search decoding, compute the requested metrics and return the results back so that nmt-train can
track the progress and save best checkpoints to disk.

Several examples regarding the usage of the utilities are given in Appendix B.

2.1 ADDING NEW ARCHITECTURES

New architectures can be deﬁned by creating a new ﬁle under nmtpy/models/ using a copy of
an existing architecture and modifying the following predeﬁned methods:

• __init__(): Instantiates a model. Keyword arguments can be used to add options
speciﬁc to the architecture that will be automatically gathered from the conﬁguration ﬁle
by nmt-train.

• init_params(): Initializes the layers and weights.

• build(): Deﬁnes the Theano computation graph that will be used during training.

• build_sampler(): Deﬁnes the Theano computation graph that will be used during
beam-search. This is generally very similar to build() but with sequential RNN steps
and non-masked tensors.

• load_valid_data(): Loads the validation data for perplexity computation.

• load_data(): Loads the training data.

2.2 BUILDING BLOCKS

In this section, we introduce the currently available components and features of nmtpy that one can
use to design their architecture.

Training nmtpy provides Theano implementations of stochastic gradient descent (SGD) and its
adaptive variants RMSProp (Tieleman & Hinton, 2012), Adadelta (Zeiler, 2012) and Adam (Kingma
& Ba, 2014) to optimize the weights of the trained network. A preliminary support for gradient noise
(Neelakantan et al., 2015) is available for Adam. Gradient norm clipping (Pascanu et al., 2013)
is enabled by default with a threshold of 5 to avoid exploding gradients. Although the provided
architectures all use the cross-entropy objective by their nature, any arbitrary differentiable objective
function can be used since the training loop is agnostic to the architecture being trained.

Regularization A dropout (Srivastava et al., 2014) layer which can be placed after any arbitrary
feed-forward layer in the architecture is available. This layer works in inverse mode where the
magnitudes are scaled during training instead of testing. Additionally, L2 regularization loss with a
scalar factor deﬁned by decay_c option in the conﬁguration can be added to the training loss.

2

Initialization The weight initialization is governed by the weight_init option and supports
Xavier (Glorot & Bengio, 2010) and He (He et al., 2015) initialization methods besides orthogonal
(Saxe et al., 2013) and random normal.

Layers The following layers are available in the latest version of nmtpy:

• Feed-forward and highway layer (Srivastava et al., 2015)
• Gated Recurrent Unit (GRU) (Chung et al., 2014)
• Conditional GRU (CGRU) (Firat & Cho, 2016)
• Multimodal CGRU (Caglayan et al., 2016a;b)

Layer normalization (Ba et al., 2016), a method that adaptively learns to scale and shift the incoming
activations of a neuron, can be enabled for GRU and CGRU blocks.

Iteration Parallel and monolingual text iterators with compressed (.gz, .bz2, .xz) ﬁle support are
available under the names TextIterator and BiTextIterator. Additionally, the multimodal WMTItera-
tor allows using image features and source/target sentences at the same time for multimodal NMT
(Section 3.3). We recommend using shuffle_mode:trglen when implemented to speed up
the training by efﬁciently batching same-length sequences.

Post-processing All decoded translations will be post-processed if filter option is given in the
conﬁguration ﬁle. This is useful in the case where one would like to compute automatic metrics on
surface forms instead of segmented. Currently available ﬁlters are bpe and compound for cleaning
subword BPE (Sennrich et al., 2016) and German compound-splitting (Sennrich & Haddow, 2015)
respectively.

Metrics nmt-train performs a patience based early-stopping using either validation perplexity or
one of the following external evaluation metrics:

• bleu: Wrapper around Moses multi-bleu BLEU (Papineni et al., 2002)
• bleu_v13a: A Python reimplementation of Moses mteval-v13a.pl BLEU
• meteor: Wrapper around METEOR (Lavie & Agarwal, 2007)

The above metrics are also available for nmt-translate to immediately score the produced hypothe-
ses. Other metrics can be easily added and made available as early-stopping metrics.

3 ARCHITECTURES

3.1 NMT

The default NMT architecture (attention) is based on the original dl4mt-tutorial implementation
which differs from Bahdanau et al. (2014) in the following major aspects:

• CGRU decoder which consists of two GRU layers interleaved with attention mechanism.
• The hidden state of the decoder is initialized with a non-linear transformation applied to

mean bi-directional encoder state in contrast to last bi-directional encoder state.

• The Maxout (Goodfellow et al., 2013) hidden layer before the softmax operation is re-

moved.

In addition, nmtpy offers the following conﬁgurable options for this NMT:

• layer_norm Enables/disables layer normalization for bi-directional GRU encoder.
• init_cgru Allows initializing CGRU with all-zeros instead of mean encoder state.
• n_enc_layers Number of additional unidirectional GRU encoders to stack on top of bi-

directional encoder.

3

• tied_emb Allows sharing feedback embeddings and output embeddings (2way) or all em-

beddings in the network (3way) (Inan et al., 2016; Press & Wolf, 2016).

• *_dropout Dropout probabilities for three dropout layers placed after source embeddings
(emb_dropout), encoder hidden states (ctx_dropout) and pre-softmax activations
(out_dropout).

3.2 FACTORED NMT

Factored NMT (FNMT) is an extension of NMT which is able to generate two output symbols. The
architecture of such a model is presented in Figure 2. In contrast to multi-task architectures, FNMT
outputs share the same recurrence and output symbols are generated in a synchronous fashion3.

Figure 2: Global architecture of the Factored NMT system.

Two FNMT variants which differ in how they handle the output layer are currently available:

• attention_factors: the lemma and factor embeddings are concatenated to form a single

feedback embedding.

• attention_factors_seplogits: the output path for lemmas and factors are kept separate with

different pre-softmax transformations applied for specialization.

FNMT with lemmas and linguistic factors has been successfully used for
English→French (García-Martínez
English→Czech evaluation campaigns.

IWSLT’16
and WMT’174 English→Latvian and

al., 2016)

et

3.3 MULTIMODAL NMT & CAPTIONING

We provide several multimodal architectures (Caglayan et al., 2016a;b) where the probability of
a target word is conditioned on source sentence representations and convolutional image features
(Figure 3). More speciﬁcally, these architectures extends monomodal CGRU into a multimodal one
where the attention mechanism can be shared or separate between input modalities. A late fusion of
attended context vectors are done using either by summing or concatenating the modality-speciﬁc
representations.

Our attentive multimodal system for Multilingual Image Description Generation track of WMT’16
Multimodal Machine Translation surpassed the baseline architecture (Elliott et al., 2015) by +1.1
METEOR and +3.4 BLEU and ranked ﬁrst among multimodal submissions (Specia et al., 2016).

3.4 LANGUAGE MODELING

A GRU-based language model architecture (rnnlm) is available in the repository which can be used
with nmt-test-lm to obtain language model scores.

3FNMT currently uses a dedicated nmt-translate-factors utility though it will probably be merged in the

near future.

4http://matrix.statmt.org/

4

Figure 3: The architecture of multimodal attention (Caglayan et al., 2016b).

3.5

IMAGE CAPTIONING

A GRU-based reimplementation of Show, Attend and Tell architecture (Xu et al., 2015) which learns
to generate a natural language description by applying soft attention over convolutional image fea-
tures is available under the name img2txt. This architecture is recently used 5 as a baseline system
for the Multilingual Image Description Generation track of WMT’17 Multimodal Machine Trans-
lation shared task.

In this section we present translation and rescoring utilities nmt-translate and nmt-rescore. Other
auxiliary utilities are brieﬂy described in Appendix C.

4 TOOLS

4.1 NMT-TRANSLATE

nmt-translate is responsible for translation decoding using the beam-search method deﬁned by NMT
architecture. This default beam-search supports single and ensemble decoding for both monomodal
and multimodal translation models. If a given architecture reimplements the beam-search method in
its class, that one will be used instead.

Since the number of CPUs in a single machine is 2x-4x higher than the number of GPUs and we
mainly reserve the GPUs for training, nmt-translate makes use of CPU workers for maximum ef-
ﬁciency. More speciﬁcally, each worker receives a model instance (or instances when ensembling)
and performs the beam-search on samples that it continuously fetches from a shared queue. This
queue is ﬁlled by the master process using the iterator provided by the model.

One thing to note for parallel CPU decoding is that if the numpy is linked against a BLAS implemen-
tation with threading support enabled (as in the case with Anaconda & Intel MKL), each spawned
process attempts to use all available threads in the machine leading to a resource conﬂict. In order
for nmt-translate to beneﬁt correctly from parallelism, the number of threads per process is thus
limited 6 to 1. The impact of this setting and the overall decoding speed in terms of words/sec (wps)
are reported in (Table 1) for a medium-sized En→Tr NMT with ∼10M parameters.

4.2 NMT-RESCORE

A 1-best plain text or n-best hypotheses ﬁle can be rescored with nmt-rescore using either a single or
an ensemble of models. Since rescoring of a given hypothesis simply means computing the negative

5http://www.statmt.org/wmt17/multimodal-task.html
6This

is achieved by setting X_NUM_THREADS=1 environment variable where X is one of

OPENBLAS,OMP,MKL depending on the numpy installation.

5

# BLAS Threads Tesla K40

4 CPU

8 CPU 16 CPU

Default
Set to 1

185 wps
185 wps

26 wps
109 wps

25 wps
198 wps

25 wps
332 wps

Table 1: Median beam-search speed over 3 runs with beam size 12: decoding on a single Tesla K40
GPU is rougly equivalent to using 8 CPUs (Intel Xeon E5-2687v3).

log-likelihood of it given the source sentence, nmt-rescore uses a single GPU to efﬁciently compute
the scores in batched mode. See Appendix B for examples.

5 CONCLUSION

We have presented nmtpy, an open-source sequence-to-sequence framework based on dl4mt-tutorial
and reﬁned in many ways to ease the task of integrating new architectures. The toolkit has been inter-
nally used in our team for tasks ranging from monomodal, multimodal and factored NMT to image
captioning and language modeling to help achieving top-ranked submissions during campaigns like
IWSLT and WMT.

This work was supported by the French National Research Agency (ANR) through the CHIST-ERA
M2CR project, under the contract number ANR-15-CHR2-0006-017.

ACKNOWLEDGMENTS

REFERENCES

arXiv:1607.06450, 2016.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes García-Martínez, Fethi
Bougares, Loïc Barrault, and Joost van de Weijer. Does multimodality help human and machine
for translation and image captioning? In Proceedings of the First Conference on Machine Trans-
lation, pp. 627–633, Berlin, Germany, August 2016a. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/W/W16/W16-2358.

Ozan Caglayan, Loïc Barrault, and Fethi Bougares. Multimodal attention for neural machine trans-

lation. arXiv preprint arXiv:1609.03976, 2016b.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325, 2015.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. URL
http://arxiv.org/abs/1412.3555.

Desmond Elliott, Stella Frank, and Eva Hasler. Multi-language image description with neural
sequence models. CoRR, abs/1510.04709, 2015. URL http://arxiv.org/abs/1510.
04709.

Orhan Firat and Kyunghyun Cho. Conditional gated recurrent unit with attention mechanism.
github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf, 2016.

Mercedes García-Martínez, Loïc Barrault, and Fethi Bougares. Factored neural machine transla-
In Proceedings of the International Workshop on Spoken Language Trans-
tion architectures.
lation, IWSLT’16, Seattle, USA, 2016. URL http://workshop2016.iwslt.org/
downloads/IWSLT_2016_paper_2.pdf.

7http://m2cr.univ-lemans.fr

6

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neu-
In In Proceedings of the International Conference on Artiﬁcial Intelligence and

ral networks.
Statistics (AISTATS’10). Society for Artiﬁcial Intelligence and Statistics, 2010.

Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp.
1319–1327, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL http://proceedings.
mlr.press/v28/goodfellow13.html.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
In Computer Vision (ICCV), 2015 IEEE

human-level performance on imagenet classiﬁcation.
International Conference on, pp. 1026–1034. IEEE, 2015.

Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A

loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for mt evaluation with high levels
In Proceedings of the Second Workshop on Statistical
of correlation with human judgments.
Machine Translation, StatMT ’07, pp. 228–231, Stroudsburg, PA, USA, 2007. Association for
Computational Linguistics.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.

In Stan Szpakowicz
Marie-Francine Moens (ed.), Text Summarization Branches Out: Proceedings of the ACL-04
Workshop, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807, 2015.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ’02, pp. 311–318, Stroudsburg, PA, USA, 2002.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of The 30th International Conference on Machine Learning, pp. 1310–
1318, 2013.

Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint

arXiv:1608.05859, 2016.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-

ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Rico Sennrich and Barry Haddow. A joint dependency model of morphological and syntactic struc-
In Proceedings of the 2015 Conference on Empirical
ture for statistical machine translation.
Methods in Natural Language Processing, pp. 114–121. Association for Computational Linguis-
tics, 2015.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
In Proceedings of the 54th Annual Meeting of the Association for Com-
with subword units.
putational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
P16-1162.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch-Mayne, Barry Haddow, Julian
Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli, Antonio Miceli Barone, Jozef Mokry, and
Maria Nadejde. Nematus: a Toolkit for Neural Machine Translation, pp. 65–68. Association for
Computational Linguistics (ACL), 4 2017. ISBN 978-1-945626-34-0.

7

Lucia Specia, Stella Frank, Khalil Sima’an, and Desmond Elliott. A shared task on multimodal
machine translation and crosslingual image description. In Proceedings of the First Conference
on Machine Translation, Berlin, Germany. Association for Computational Linguistics, 2016.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint

arXiv:1505.00387, 2015.

Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. arXiv e-prints, abs/1605.02688, 2016. URL http://arxiv.org/abs/1605.
02688.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4566–4575, 2015.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Proceedings of The 32nd International Conference on Machine Learning, pp. 2048–
2057, 2015.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,

2012.

8

A CONFIGURATION FILE EXAMPLE

# Options in this section are consumed by nmt-train
[training]
model_type: attention
patience: 20
valid_freq: 1000
valid_metric: meteor
valid_start: 2
valid_beam: 3
valid_njobs: 16
valid_save_hyp: True
decay_c: 1e-5
clip_c: 5
seed: 1235
save_best_n: 2
device_id: auto
snapshot_freq: 10000
max_epochs: 100

# Model type without .py
# early-stopping patience
# Compute metrics each 1000 updates
# Use meteor during validations
# Start validations after 2nd epoch
# Decode with beam size 3
# Use 16 processes for beam-search
# Save validation hypotheses
# L2 regularization factor
# Gradient clip threshold
# Seed for numpy and Theano RNG
# Keep 2 best models on-disk
# Pick 1st available GPU
# Save a resumeable snapshot

# weight-tying mode (False,2way,3way)
# layer norm in GRU encoder
# Shuffled/length-ordered batches
# post-processing filter(s)
# limit src vocab if > 0
# limit trg vocab if > 0
# Where to store checkpoints
# Encoder and decoder RNN dim
# All embedding dim

# Options below are passed to model instance
[model]
tied_emb: 2way
layer_norm: True
shuffle_mode: trglen
filter: bpe
n_words_src: 0
n_words_trg: 0
save_path: ~/models
rnn_dim: 100
embedding_dim: 100
weight_init: xavier
batch_size: 32
optimizer: adam
lrate: 0.0004
emb_dropout: 0.2
ctx_dropout: 0.4
out_dropout: 0.4

# Set dropout rates

# Dictionary files produced by nmt-build-dict
[model.dicts]
src: ~/data/train.norm.max50.tok.lc.bpe.en.pkl
trg: ~/data/train.norm.max50.tok.lc.bpe.de.pkl

# Training and validation data
[model.data]
train_src
train_trg
valid_src
valid_trg
valid_trg_orig: ~/data/val.norm.tok.lc.de

: ~/data/train.norm.max50.tok.lc.bpe.en
: ~/data/train.norm.max50.tok.lc.bpe.de
: ~/data/val.norm.tok.lc.bpe.en
: ~/data/val.norm.tok.lc.bpe.de

9

B USAGE EXAMPLES

# Launch an experiment
$ nmt-train -c wmt-en-de.conf

# Launch an experiment with different architecture
$ nmt-train -c wmt-en-de.conf ’model_type:my_amazing_nmt’

# Change dimensions
$ nmt-train -c wmt-en-de.conf ’rnn_dim:500’ ’embedding_dim:300’

# Force specific GPU device
$ nmt-train -c wmt-en-de.conf ’device_id:gpu5’

Listing 1: Example usage patterns for nmt-train.

# Decode on 30 CPUs with beam size 10, compute BLEU/METEOR
# Language for METEOR is set through source file suffix (.en)
$ nmt-translate -j 30 -m best_model.npz -S val.tok.bpe.en \

-R val.tok.de -o out.tok.de -M bleu meteor -b 10

# Generate n-best list with an ensemble of checkpoints
$ nmt-translate -m model*npz -S val.tok.de \

-o out.tok.50best.de -b 50 -N 50

# Generate json file with alignment weights (-e)
$ nmt-translate -m best_model.npz -S val.tok.bpe.en \

-R val.tok.de -o out.tok.de -e

Listing 2: Example usage patterns for nmt-translate.

# Rescore 50-best list with ensemble of models
$ nmt-rescore -m model*npz -s val.tok.bpe.en \

-t out.tok.50best.de \
-o out.tok.50best.rescored.de

Listing 3: Example usage patterns for nmt-rescore.

C DESCRIPTION OF THE PROVIDED TOOLS

nmt-build-dict Generates .pkl vocabulary ﬁles from preprocessed corpus. A single/combined vo-
cabulary for two or more languages can be created with -s ﬂag.

nmt-extract Extracts arbitrary weights from a model snapshot which can further be used as pre-
trained weights of a new experiment or analyzed using visualization techniques (especially for em-
beddings).

nmt-coco-metrics A stand-alone utility which computes multi-reference BLEU, METEOR,
CIDE-r (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) using MSCOCO evaluation tools (Chen
et al., 2015). Multiple systems can be given with -s ﬂag to produce a table of scores.

nmt-bpe-(learn,apply) Copy of subword utilities 8 (Sennrich et al., 2016) which are used to ﬁrst
learn a BPE segmentation model over a given corpus ﬁle and then apply it to new sentences.

nmt-test-lm Computes language model perplexity of a given corpus.

8https://github.com/rsennrich/subword-nmt

10

