Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural
network

Charis Lanarasa,∗, Jos´e Bioucas-Diasb, Silvano Galliania, Emmanuel Baltsaviasa, Konrad Schindlera

aPhotogrammetry and Remote Sensing, ETH Zurich, Zurich, Switzerland
bInstituto de Telecomunicac¸ ˜oes, Instituto Superior T´ecnico, Universidade de Lisboa, Portugal

8
1
0
2
 
t
c
O
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
7
2
4
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial
resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance
– GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-
of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower
resolution, i.e., from 40→20 m, respectively 360→60 m GSD. In this way, one has access to a virtually inﬁnite amount of
training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical
locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve
arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth
is available), our network, which we call DSen2, outperforms the best competing approach by almost 50% in RMSE, while
better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD.

Keywords: Sentinel-2; super-resolution; sharpening of bands; convolutional neural network; deep learning

1. Introduction

Several widely used satellite imagers record multiple
spectral bands with different spatial resolutions. Such in-
struments have the considerable advantage that the different
spectral bands are recorded (quasi-) simultaneously, thus
with similar illumination and atmospheric conditions, and
without multi-temporal changes. Furthermore, the view-
ing directions are (almost) the same for all bands, and
the co-registration between bands is typically very pre-
cise. Examples of such multi-spectral, multi-resolution
sensors include: MODIS, VIIRS, ASTER, Worldview-3
and Sentinel-2. The resolutions between the spectral bands
of any single instrument typically differ by a factor of about
2–6. Reasons for recording at varying spatial resolution in-
clude: storage and transmission bandwidth restrictions, im-
proved signal-to-noise ratio (SNR) in some bands through
larger pixels, and bands designed for speciﬁc purposes that
do not require high spatial resolution (e.g. atmospheric cor-
rections). Still, it is often desired to have all bands avail-
able at the highest spatial resolution, and the question arises
whether it is possible to computationally super-resolve the

∗Corresponding author
Email addresses: charis.lanaras@geod.baug.ethz.ch

(Charis Lanaras), bioucas@lx.it.pt (Jos´e Bioucas-Dias),
silvano.galliani@geod.baug.ethz.ch (Silvano Galliani),
emmanuel.baltsavias@geod.baug.ethz.ch (Emmanuel
Baltsavias), konrad.schindler@geod.baug.ethz.ch (Konrad
Schindler)

lower-resolution bands, so as to support more detailed and
accurate information extraction. Such a high-quality super-
resolution, beyond naive interpolation or pan-sharpening,
is the topic of this paper. We focus speciﬁcally on super-
resolution of Sentinel-2 images.

Sentinel-2 (S2) consists of two identical satellites, 2A
and 2B, which use identical sensors and ﬂy on the same or-
bit with a phase difference of 180 degrees, decreasing thus
the repeat and revisit periods. The sensor acquires 13 spec-
tral bands with 10 m, 20 m and 60 m resolution, with high
spatial, spectral, radiometric and temporal resolution, com-
pared to other, similar instruments. More details on the S2
mission and data are given in Section 3. Despite its re-
cency, S2 data have been already extensively used. Beyond
conventional thematic and land-cover mapping, the sensor
characteristics also favour applications like hydrology and
water resource management, or monitoring of dynamically
changing geophysical variables.

E.g., Mura et al. (2018) exploit S2 to predict growing
stock volume in forest ecosystems. Castillo et al. (2017)
compute the Leaf Area Index (LAI) as a proxy for above-
ground biomass of mangrove forests in the Philippines.
Similarly, Clevers et al. (2017) retrieve LAI and leaf and
canopy chlorophyll content of a potato crop. Delloye et al.
(2018) estimate nitrogen uptake in intensive winter wheat
cropping systems by retrieval of the canopy chlorophyll
content. Paul et al. (2016) map the extent of glaciers, while
Toming et al. (2016) map lake water quality.
Immitzer
et al. (2016) have demonstrated the use of S2 data for crop

ISPRS Journal of Photogrammetry and Remote Sensing, Volume 146, Pages 305–319, 2018

and tree species classiﬁcation, and Pesaresi et al. (2016)
for detecting built-up areas. The quality, free availability
and world-wide coverage make S2 an important tool for
(current and) future earth observation, which motivates this
work.

Obviously,

low-resolution images can be upsampled
with simple and fast, but naive methods like bilinear or
bicubic interpolation. However, such methods return blurry
images with little additional information content. More
sophisticated methods, including ours, attempt to do bet-
ter and recover as much as possible of the spatial de-
tail, through a “smarter” upsampling that is informed by
the available high-resolution bands. Here, we propose a
(deep) machine learning approach to multi-spectral super-
resolution, using convolutional neural networks (CNNs).
The goal is to surpass the current state-of-the-art in terms of
reconstruction accuracy, while at the same time to preserve
the spectral information of the original bands. Moreover,
the method shall be computationally efﬁcient enough for
large-area practical use. We train two CNNs, one for super-
resolving 20 m bands to 10 m, and one for super-resolving
60 m bands to 10 m. Our method, termed DSen2, implic-
itly captures the statistics of all bands and their correla-
tions, and jointly super-resolves the lower-resolution bands
to 10 m GSD. See an example in Fig. 1. True to the statis-
tical learning paradigm, we learn an end-to-end-mapping
from raw S2 imagery to super-resolved bands purely from
the statistics over a large amount of image data. Our ap-
proach is based on one main assumption, namely that the
spectral correlation of the image texture is self-similar over
a (limited) range of scales. I.e., we postulate that upsam-
pling from 20 m to 10 m GSD, by transferring high reso-
lution (10 m) details across spectral bands, can be learned
from ground truth images at 40 m and 20 m GSD; and sim-
ilarly for the 60 m to 10 m case. Under this assumption,
creating training data for supervised learning is simple and
cheap: we only need to synthetically downsample original
S2 images by the desired factor, use the downsampled ver-
sion as input to generate original data as output.

In this way, one gains access to large amounts of train-
ing data, as required for deep learning: S2 data are avail-
able free of charge, covering all continents, climate zones,
biomes and land-cover types. Moreover, we assert that the
high-capacity of modern deep neural networks is sufﬁcient
to encode a super-resolution mapping which is valid across
the globe. Fig. 2 and 3 show various land-cover types and
geographical/climatic areas used for training and testing.
It is likely that even better results could be achieved, if a
user focusing on a speciﬁc task and geographic region re-
trains the proposed networks with images from that par-
ticular environment. In that case, one can start from our
trained network and ﬁne-tune the network weights with ap-
propriate training sites. However, our experiments show
that even a single model, trained on a selected set of rep-
resentative sites world-wide, achieves much better super-
resolution than prior state-of-the-art methods for indepen-

Figure 1: Top:
Input Sentinel-2 bands at 10 m, 20 m and 60 m GSD,
Bottom: Super-Resolved bands to 10 m GSD, with the proposed method
(DSen2).

dent test sites, also sampled globally. That is, our network
is not overﬁtted to a particular context (as often the case
with discriminative statistical learning), but can be applied
worldwide.

Extensive experimental tests at reduced scale (where S2
ground truth is available) show that our single, globally ap-
plicable network yields greatly improved super-resolution
of all S2 bands to 10 m GSD. We compare our method
to four other methods both quantitatively and qualitatively.
Our approach achieves almost 50% lower RMSE than the
best competing methods, as well as > 5 dB higher signal-
to-reconstruction-error ratio and >30% improvement in
spectral angle mapping. The performance difference is par-
ticularly pronounced for the Short-Wave Infrared (SWIR)
bands and the 60 m ones, which are particularly challeng-
ing for super-resolution. For completeness, we also pro-
vide results for three “classical” pan-sharpening methods
on the 20 m bands, which conﬁrm that pan-sharpening can-
not compete with true multi-band super-resolution meth-
ods, including ours. Importantly, we also train a version
of our network at half resolution (80→40 m) and evaluate
its performance on 40→20 m test data. While there is of
course some loss in performance, the CNN trained in this
way still performs signiﬁcantly better than all other meth-
ods. This supports our assertion that the mapping is to a
large extent scale-invariant and can be learned from train-
ing data at reduced resolution – which is important for ma-
chine learning approaches in general, beyond our speciﬁc
implementation.

Summarising our contributions, we have developed a

2

Figure 2: A selection of the images used for training and testing.

CNN-based super-resolution algorithm optimised for (but
conceptually not limited to) S2, with the following char-
acteristics: (i) signiﬁcantly higher accuracy of all super-
resolved bands, (ii) better preservation of spectral charac-
teristics, (iii) favourable computational speed when run on
modern GPUs, (iv) global applicability for S2 data without
retraining, according to our (necessarily limited) tests, (v)
generic end-to-end system that can, if desired, be retrained
for speciﬁc geographical locations and land-covers, simply
by running additional training iterations, (vi) free, publicly
available source code and pre-trained network weights, en-
abling out-of-the-box super-resolution of S2 data.

2. Related work

Enhancing the spatial resolution of remotely sensed
multi-resolution images has been addressed for vari-
ous types of images and sensors,
including for exam-
ple ASTER (Tonooka, 2005; Fasbender et al., 2008),
MODIS (Trishchenko et al., 2006; Sirguey et al., 2008), and
In the following, we differ-
VIIRS (Picaro et al., 2016).
entiate three types of methods: pan-sharpening per band,
inverting an explicit imaging model, and machine learn-
ing approaches. The ﬁrst group increases the spatial res-
olution independently for each target band, by blending in
information from a spectrally overlapping high-resolution
band. It is therefore essentially equivalent to classical pan-
sharpening, applied separately to the spectral region around
each high-resolution band. Such an approach relies on
the assumption that for each relevant portion of the spec-
trum there is one high-resolution band (in classical pan-
sharpening the “panchromatic” one), which overlaps, at
least partially, with the lower-resolution bands to be en-
hanced. That view leads directly to the inverse problem
of undoing the spatial blur from the panchromatic to the
lower-resolution texture. A number of computational tools
have been applied ranging from straight-forward compo-
nent substitution to multiresolution analysis, Bayesian in-
ference and variational regularisation. For a few repre-
sentative examples we refer to (Choi et al., 2011), (Lee
and Lee, 2010) and (Garzelli et al., 2008). A recent re-
view and comparison of pan-sharpening methods can be
found in Vivone et al. (2015). The pan-sharpening strat-
egy has also been applied directly to Sentinel-2, although

the sensor does not meet the underlying assumptions: as
opposed to a number of other earth observation satellites
(e.g., Landsat 8) it does not have a panchromatic band
that covers most of the sensor’s spectral range. In a com-
parative study Vaiopoulos and Karantzalos (2016) evaluate
21 pan-sharpening algorithms to enhance the 20 m visible
and near infrared (VNIR) and short wave infrared (SWIR)
bands of Sentinel-2, using heuristics to select or synthe-
sise the “panchromatic” input from the (in most cases non-
overlapping) 10 m bands. Wang et al. (2016) report some of
the best results in the literature for their ATPRK (Area-To-
Point Regression Kriging) method, which includes a sim-
ilar band selection, performs regression analysis between
bands at low resolution, and applies the estimated regres-
sion coefﬁcients to the high-resolution input, with appro-
priate normalisation. Park et al. (2017) propose a number
of modiﬁcations to optimise the band selection and syn-
thesis, which is then used for pan-sharpening with com-
ponent substitution and multiresolution analysis. Du et al.
(2016), having in mind the monitoring of open water bod-
ies, have tested four popular pan-sharpening methods to
sharpen the B11 SWIR band of S2, in order to compute
a high-resolution the normalized differential water index
(NDWI). Further in this direction, Gasparovic and Jogun
(2018) used ﬁve different pan-sharpening methods to en-
hance the resolution of the 20 m bands. Their goal was to
investigate the effect of the sharpened images on a land-
cover classiﬁcation compared to naive nearest neighbor up-
sampling. Interestingly, the classiﬁcation results improved
for most of the methods.

The second group of methods attacks super-resolution as
an inverse imaging problem under the variational, respec-
tively Bayesian, inference frameworks. These model-based
methods are conceptually appealing in that they put forward
an explicit observation model, which describes the assumed
blurring, downsampling, and noise processes. As the in-
verse problem is ill-posed by deﬁnition, they also add an
explicit regulariser (in Bayesian terms an “image prior”).
The high-resolution image is then obtained by minimising
the residual error w.r.t. the model (respectively, the nega-
tive log-likelihood of the predicted image) in a single op-
timisation for all bands simultaneously. Brodu (2017) in-
troduced a method that separates band-dependent spectral
information from information that is common across all

3

Table 1: The 13 Sentinel-2 bands.

Band

Center wavelength [nm]
Bandwidth [nm]
Spatial Resolution [m]

B1

443
20
60

B2

490
65
10

B3

560
35
10

B4

665
30
10

B5

705
15
20

B6

740
15
20

B7

783
20
20

B8 B8a

B10

B11

B12

842
115
10

865
20
20

1380
30
60

1610
90
20

2190
180
20

B9

945
20
60

bands, termed “geometry of scene elements”. The model
then super-resolves the low-resolution bands such that they
are consistent with those scene elements, while preserving
their overall reﬂectance. Lanaras et al. (2017) adopt an ob-
servation model with per-band point spread functions that
account for convolutional blur, downsampling, and noise.
The regularisation consists of two parts, a dimensionality
reduction that implies correlation between the bands, and
thus lower intrinsic dimension of the signal; and a spatially
varying, contrast-dependent penalisation of the (quadratic)
gradients, which is learned from the 10 m bands. SMUSH,
introduced in Paris et al. (2017), adopts an observation
model similar to Lanaras et al. (2017), but employs a differ-
ent, patch-based regularisation that promotes self-similarity
of the images. The method proceeds hierarchically, ﬁrst
sharpening the 20 m bands, then the coarse 60 m ones.

The third group of super-resolution methods casts the
prediction of the high-resolution data cube as a supervised
machine learning problem.
In contrast to the two pre-
vious groups, the relation between lower-resolution input
to higher-resolution output is not explicitly speciﬁed, but
learned from example data. Learning methods (and in par-
ticular, deep neural networks) can thus capture much more
complex and general relations, but in turn require massive
amounts of training data, and large computational resources
to solve the underlying, extremely high-dimensional and
complex optimisation. We note that the methods de-
scribed in the following were designed with the classic
pan-sharpening problem in mind. Due to the generic na-
ture of end-to-end machine learning, this does not consti-
tute a conceptual problem: in principle, they could be re-
trained with different input and output dimensions. Obvi-
ously, their current weights are not suitable for Sentinel-2
upsampling. To the best of our knowledge, we are the ﬁrst
to apply deep learning to that problem. Masi et al. (2016)
adapt a comparatively shallow three-layer CNN architec-
ture originally designed for single-image (blind) super-
resolution. They train pan-sharpening networks for Ikonos,
GeoEye-1 and WorldWiew-2. Yang et al. (2017) introduced
PanNet, based on the high-performance ResNet architec-
ture (He et al., 2016). PanNet starts by upsampling the
low-resolution inputs with naive interpolation. The actual
network is fed with high-pass ﬁltered versions of the raw
inputs and learns a correction that is added to the naively
upsampled images.1 PanNet was trained for Worldview-

2, Worldview-3, and Ikonos. More recently, this concept
has been further exploited in Scarpa et al. (2018). Learning
based pan-sharpening networks are trained with relatively
small amounts of data, presumably because of the high data
cost. In this context, we point out that with deep learning
one need not specify sensor characteristics like for instance
spectral response functions. Rather, the sensor properties
are implicit in the training data.

Example-based super-resolution has been investigated
in computer vision and image processing (e.g., Freeman
et al., 2002), but mainly for single-image super-resolution.
I.e., enhancing the spatial resolution of a single (RGB)
image with the help of a prior learned from a suitable
training set. The rise of deep learning has also advanced
single-image super-resolution (Lim et al., 2017; Kim et al.,
2016). Moreover, such super-resolution has been applied
to Sentinel-2 and Landsat-8 images (Pouliot et al., 2018).
All these works have in common that they predict im-
ages of higher spatial resolution, meaning that what is
learned is a generic prior on the local structure of high-
resolution images; whereas our method increases resolu-
tion of particular bands in a more informed and more ac-
curate manner, by transferring the texture from available
high-resolution bands; effectively learning a prior on the
correlations across the spectrum (or, equivalently, on the
high-resolution structure of some bands conditioned on the
known high-resolution structure of other bands).

3. Input data

We use data from the ESA/Copernicus satellites Sen-
tinel 2A and 2B2. They were launched on June 23, 2015
and March 7, 2017, respectively, with a design lifetime
of 7.25 years, potentially extendible up to 5 additional
years. The two satellites are identical and have the same
sun-synchronous, quasi-circular, near-polar, low-earth orbit
with a phase difference of 180 degrees. This allows the re-
duction of the repeat (and revisit) periods from 10 to 5 days
at the equator. The satellites systematically cover all land
masses except Antarctica, including all major and some
smaller islands. The main sensor on the satellites is a mul-
tispectral imager with 13 bands. Their spectral character-
istics and GSDs are shown in Table 1. Applications of the

1In CNN terminology, adding the upsampled input constitutes a “skip

connection”.

2See

details

at
eoportal/satellite-missions/c-missions/
copernicus-sentinel-2

https://eoportal.org/web/

4

10 m and 20 m bands include: general land-cover mapping,
agriculture, forestry, mapping of biophysical variables (for
instance, leaf chlorophyll content, leaf water content, leaf
area index), monitoring of coastal and inland waters, and
risk and disaster mapping. The three bands with 60 m GSD
are intended mainly for water vapour, aerosol corrections
and cirrus clouds estimation. In actual fact they are cap-
tured at 20 m GSD and are downsampled in software to
60 m, thus increasing the SNR. The ﬁrst 10 bands cover
the VNIR spectrum and are acquired by a CMOS detec-
tor for two bands (B3 and B4) with 2-line TDI (time delay
and integration) for better signal quality. The last 3 bands
cover the SWIR spectrum and are acquired by passively
cooled HgCdTe detectors. Bands B11 and B12 also have
staggered-row, 2-line TDI. The swath width is 290 km. In-
tensities are quantised to 12 bit and compressed by a fac-
tor ≈2.9 with a lossy wavelet method (depending on the
band). Empirical data quality has been quantiﬁed as: abso-
lute geolocation accuracy (without ground control) of 11 m
at 95.5% conﬁdence, absolute radiometric uncertainty (ex-
cept B10) <5%, and SNR values comply to the speciﬁca-
tions with > 27% margin.

Clerc and MPC Team (2018) report on further aspects of
S2 data quality. The mean pairwise co-registration errors
between spectral bands are 0.14–0.21 pixels (at the lower of
the two resolutions) for S2A and 0.07–0.18 pixels for S2B,
99.7% conﬁdence. This parameter is important for our ap-
plication: good band-to-band co-registration is important
for super-resolution, and S2 errors are low enough to ignore
them and proceed without correcting band-to-band offsets.
Moreover, data quality is very similar for Sentinel-2A and
2B, so that no separate treatment is required. B10 (in an
atmospheric absorption window, included for cirrus clouds
detection) has comparatively poor radiometric quality and
exhibits across-track striping artifacts, and is excluded from
many aspects of quality control. For that reason we also ex-
clude it.

Potential sensor issues that could impair super-resolution
would mainly be band-to-band misregistration (which is
very low for S2), radiometric or geometric misalignments
within a band (which do not seem to occur), and moving
objects such as airplanes (which are very rare). The data
thus fulﬁlls the preconditions for super-resolution, and we
did not notice any effects in our results that we attribute to
sensor anomalies.

S2 data can be downloaded from the Copernicus Ser-
vices Data Hub, free of charge. The data comes in tiles
(granules) of 110×110 km2 (≈800MB per tile). For pro-
cessing, we use the Level-1C top-of-atmosphere (TOA) re-
ﬂectance product, which includes the usual radiometric and
geometric corrections. The images are geocoded and or-
thorectiﬁed using the 90m DEM grid (PlanetDEM3) with
a height (LE95) and planimetric (CE95) accuracy of 14 m

3https://www.planetobserver.com/products/

planetdem/planetdem-30/

Figure 3: Locations of Sentinel-2 tiles used for training and testing. Image
source: meteoblue.com

and 10 m, respectively. We note that a reﬁnement step for
the Level-1C processing chain is planned, which shall bring
the geocoding accuracy between different passes to <0.3
pixels at 95% conﬁdence, which will allow high-accuracy
multi-temporal analysis.

In this study, we use data from both Sentinel 2A and 2B,
acquired between December 2016 and November 2017, re-
spectively July 2017 and November 2017. Fig. 3 shows
the locations of the tiles used. They have been picked ran-
domly, aiming for a roughly even distribution on the globe
and for variety in terms of climate zone, land-cover and
biome type. To simplify implementation and testing, we
chose only tiles with no undeﬁned (“black background”)
pixels. Pointers to the exact tiles are included in our pub-
licly available implementation (see below). Using this wide
variety of scenes, we aim to train a globally applicable
super-resolution network that can be applied to any S2
scene.

4. Method

We adopt a deep learning approach to Sentinel-2 super-
resolution. The rationale is that the relation between the
multi-resolution input and a uniform, high-resolution out-
put data cube is a complex mixture of correlations across
many (perhaps all) spectral bands, over a potentially large
It is
spatial context, respectively texture neighbourhood.
thus not obvious how to design a suitable prior (regulariser)
for the mapping. On the contrary, the underlying statistics
can be assumed to be the same across different Sentinel-
2 images. We therefore use a CNN to directly learn it
from data. In other words, the network serves as a big re-
gression engine from raw multi-resolution input patches to
high-resolution patches of the bands that need to be upsam-
pled. We found that it is sufﬁcient to train two separate
networks for the 20 m and 60 m bands. I.e., the 60 m reso-
lution bands, unsurprisingly, do not contribute information
to the upsampling from 20 to 10 m.

We point out that the machine learning approach is
generic, and not limited to a speciﬁc sensor. For our ap-
plication the network is speciﬁcally tailored to the image

5

statistics of Sentinel-2. But the sensor-speciﬁc information
is encoded only in the network weights, so it can be readily
retrained for a different multi-resolution sensor.

4.1. Simulation process

CNNs are fully supervised and need (a lot of) training
data, i.e., patches for which both the multi-resolution in-
put and the true high-resolution output are known. Thus, a
central issue in our approach is how to construct the train-
ing, validation and testing datasets, given that ground truth
with 10 m resolution is not available for the 20 m and 60 m
bands. Even with great effort, e.g., using aerial hyper-
spectral data and sophisticated simulation technology, it
is at present impossible to synthesise such data with the
degree of realism necessary for faithful super-resolution.
Hence, to become practically viable, our approach there-
fore requires one fundamental assumption: we posit that
the transfer of spatial detail from high-resolution to low-
resolution bands is scale-invariant and that it depends only
on the relative resolution difference, but not on the abso-
lute GSD of the images. I.e. the relations between bands
of different resolutions are self-similar within the relevant
scale range. Note however, we require only a weak form of
self-similarity: it is not necessary for our network to learn
a “blind” generative mapping from lower to higher reso-
lution. Rather, it only needs to learn how to transfer high
frequency details from existing high-resolution bands. The
literature on self-similarity in image analysis supports such
an assumption (e.g., Shechtman and Irani, 2007; Glasner
et al., 2009), at least over a certain scale range. We empha-
sise that for our case, the assumption must hold only over a
limited range up to 6× resolution differences, i.e., less than
In this way, virtually unlimited
one order of magnitude.
amounts of training data can be generated by synthetically
downsampling raw Sentinel-2 images as required.

For our purposes, the scale-invariance means that the
mappings between, say, 20→10 m and 40→20 m are
roughly equivalent. We can therefore train our CNN on the
latter and apply it to the former. If the assumed invariance
holds, the learned spatial-spectral correlations will be cor-
rect. To generate training data with a desired scale ratio s,
we downsample the original S2 data, by ﬁrst blurring it with
a Gaussian ﬁlter of standard deviation σ = 1/s pixels, em-
ulating the modulation transfer function (mtf ) of S2. From
the Data Quality Report (Clerc and MPC Team, 2018) we
get a range of 0.44–0.55 for the point spread function (psf )
of the bands, given the relation psf = (cid:112)−2 log(mtf )/π2.
Then we downsample by averaging over s × s windows,
with s = 2 respectively s = 6. The process of generating
the training data is schematised in Fig. 4. In this way, we
obtain two datasets for training, validation and testing. The
ﬁrst dataset consists of “high-resolution” images at 20 m
GSD and “low-resolution” images of 40 m GSD, created by
downsampling the original 10 m and 20 m bands by a factor
of 2. It serves to train a network for 2× super-resolution.
The second one consists of images with 60 m, 120 m and

Figure 4: The downsampling process used for simulating the data for train-
ing and testing.

360 m GSD, downsampled from the original 10 m, 20 m
and 60 m data. This dataset is used to learn a network for
6× super-resolution. We note that, due to unavailability of
10 m ground truth, quantitative analysis of the results must
also be conducted at the reduced resolution. We chose the
following strategy: to validate the self-similarity assump-
tion, we train a network at quarter-resolution 80→40 m
as well as one at half-resolution 40→20 m and verify that
both achieve satisfactory performance on the ground truth
20 m images. To test the actual application scenario, we
then apply the 40→20 m network to real S2 data to get
20→10 m super-resolution. However, the resulting 10 m
super-resolved bands can only be checked by visual inspec-
tion.

4.2. 20m and 60m resolution networks

To avoid confusion between bands and simplify notation,
we collect bands that share the same GSD into three sets
A = {B2, B3, B4, B8} (GSD=10 m), B = {B5, B6,
B7, B8a, B11, B12} (GSD=20 m) and C = {B1, B9}
(GSD=60 m). The spatial dimensions of the high-resolution
bands in A are W × H. Further, let yA ∈ RW ×H×4,
yB ∈ RW/2×H/2×6, and yC ∈ RW/6×H/6×2 denote, re-
spectively, the observed intensities of all bands contained
in sets A, B and C. As mentioned above, we train two sep-
arate networks for different super-resolution factors. This
reﬂects our belief that self-similarity may progressively de-
grade with increasing scale difference, such that 120→60 m
is probably a worse proxy for 20→10 m than the less distant
40→20 m.

The ﬁrst network upsamples the bands in B using infor-

mation from A and B:

T2× : RW ×H×4 × RW/2×H/2×6 → RW ×H×6

(yA, yB) (cid:55)→ xB,

where xB ∈ RW ×H×6 denotes the super-resolved 6-band
image with GSD 10 m. The second network upsamples C,
unsing information from A, B and C:

S6× : RW ×H×4 × RW/2×H/2×6 × RW/6×H/6×2

→ RW ×H×2
(yA, yB, yC) (cid:55)→ xC,

(1a)

(1b)

(2a)

(2b)

with xC ∈ RW ×H×2 again the super-resolved 2-band im-
age of GSD 10 m.

6

4.3. Basic architecture

Our network design was inspired by EDSR (Lim et al.,
2017), state-of-the-art
in single-image super-resolution.
EDSR follows the well-known ResNet architecture (He
et al., 2016) for image classiﬁcation, which enables the use
of very deep networks by using the so called “skip con-
nections”. These long-range connections bypass portions
of the network and are added again later, such that skipped
layers only need to estimate the residual w.r.t. their input
state. In this way the average effective path length through
the network is reduced, which alleviates the vanishing gra-
dient problem and greatly accelerates the learning.

Our problem however, is different from classical single-
image super-resolution. In the case of Sentinel-2, the net-
work does not need to hallucinate the high-resolution tex-
ture only on the basis of previously seen images. Rather, it
has access to the high-resolution bands to guide the super-
resolution, i.e., it must learn to transfer the high-frequency
content to the low-resolution input bands, and do so in
such a way that the resulting (high-resolution) pixels have
plausible spectra. Contrary to EDSR, where the upsam-
pling takes place at the end, we prefer to work with the
high (10 m) resolution from the beginning, since some in-
put bands already have that resolution. We thus start by up-
sampling the low-resolution bands yB and yC to the target
resolution (10 m) with simple bilinear interpolation, to ob-
tain (cid:101)yB ∈ RW ×H×6 and (cid:101)yC ∈ RW ×H×2. The inputs and
outputs depend on whether the network T2× or S6× is used.
To avoid confusion we deﬁne the set k of low-resolution
bands as either k = {B} or k = {B, C}. Such that the
input is yk, and the addition (skip connection) to the output
is (cid:101)yB for T2×, respectively (cid:101)yC for S6×. The proposed net-
work architecture consists mainly of convolutional layers,
ReLU non-linearities and skip connections. A graphical
overview of the network is given in Fig. 5 and 6, pseudo-
code for the network speciﬁcation is given in Algorithm 1.
The operator conv(x, fout) represents a single convolu-
tion layer, i.e., a multi-dimensional convolution of image z
with kernel w, followed by an additive bias b:

v = conv(x, fout) := w ∗ z + b
w : (fout × fin × k × k), b : (fout × 1 × 1 × 1)
z : (fin × w × h), v : (fout × w × h)

(3)

where ∗ is the convolution operator. The convolved image
v has the same spatial dimensions (w × h) as the input, as
we use zero-padded convolution. The convolution kernels
w have dimensions (k × k). We always use k = 3, in line
with the recent literature, which suggests that many layers
of small kernels are preferable. The output feature dimen-
sion fout of the convolution (number of ﬁlters) depends
only on w and is required as an input. fout can be cho-
sen for each convolutional layer, and constitutes a hyper-
parameter of the network. Its selection is further discussed
in Sec. 4.4. The input feature dimensions fin (depth of the
ﬁlters) depend only on the input image z. The weights w

Algorithm 1 DSen2. Network architecture.
Require: high-resolution bands (A): yA, low-resolution
bands (B, C): yk, feature dimensions f , number of Res-
Blocks: d
# Cubic interpolation of low resolution:
Upsample yk to (cid:101)yk
# Concatenation:
x0 := [yA, (cid:101)yk]
# First Convolution and ReLU:
x1 := max(conv(x0, f ), 0)
# Repeat the ResBlock module d times:
for i = 1 to d do

xi = ResBlock(xi−1, f )

end for
# Last Convolution to match the output dimensions:
# where blast is either 6 (T2×) or 2 (S6×)
xd+1 := conv(xd, blast)
# Skip connection:
x := xd+1 + (cid:101)yB
return x

x := xd+1 + (cid:101)yC

(cid:0)T2×

or

(cid:1)

(cid:0)S6×

(cid:1)

Figure 5: The proposed networks T2× and S6×, with multiple ResBlock
modules. The two networks differ only regarding the inputs.

and b are the free parameters learned during training and
ultimately what the network has to learn.

The rectiﬁed linear unit (ReLU ) is a simple non-linear
function that truncates all negative responses in the output
to 0:

v = max(z, 0).

(4)

A residual block v = ResBlock(z, f ) is deﬁned as a se-
ries of layers that operate on an input image z to generate an

7

rather than the ﬁnal output image, helps to preserve the ra-
diometry of the input image.

4.4. Deep and very deep networks

Finding the right size and capacity for a CNN is largely
an empirical choice. Conveniently, the CNN framework
makes it possible to explore a range of depths with the
same network design, thus providing an easy way of ex-
ploring the trade-off between small, efﬁcient models and
larger, more powerful ones. Also in our case, it is hard
to know in advance how complex the network must be to
adequately encode the super-resolution mapping. We in-
troduce two conﬁgurations of our ResNet architecture, a
deep (DSen2) and a very deep one (VDSen2). The names
are derived from Deep Sentinel-2 and Very Deep Sentinel-
2, respectively. For the deep version we use d = 6 and
f = 128, corresponding to 14 convolutional layers, respec-
tively 1.8 million tunable weights. For the very deep one
we set d = 32 and f = 256, leading to 66 convolutional
layers and a total of 37.8 million tunable weights. DSen2
is comparatively small for a modern CNN. The design goal
here was a light network that is fast in training and pre-
diction, but still reaches good accuracy. VDSen2 has a lot
higher capacity, and was designed with maximum accuracy
in mind. It is closer in terms of size and training time to
modern high-end CNNs for other image analysis tasks (Si-
monyan and Zisserman, 2015; He et al., 2016; Huang et al.,
2017), but is approximately two times slower and ﬁve times
slower in both training and prediction respectively, com-
pared to its shallower counterpart (DSen2). Naturally, one
can easily construct intermediate versions by changing the
corresponding parameters d and f . The optimal choice will
depend on the application task as well as available compu-
tational resources. On the one hand, the very deep variant is
consistently a bit better, while training and applying it is not
more difﬁcult, if adequate resources (i.e., high-end GPUs)
are available. However, the gains are small compared to
the 20× increase in free parameters, and it is unlikely that
going even deeper will bring much further improvement.

4.5. Training details

As loss function we use the mean absolute pixel error (L1
norm) between the true and the predicted high-resolution
Interestingly, we found the L1 norm to converge
image.
faster and deliver better results than the L2 norm, even
though the latter serves as error metric during evaluation.
Most likely this is due to the L1 norm’s greater robustness
of absolute deviations to outliers. We did observe that some
Sentinel-2 images contain a small number of pixels with
very high reﬂectance, and due to the high dynamic range
these reach extreme values without saturating.

Our learning procedure is standard: the network weights
are initialised to small random values with the HeUniform
method (He et al., 2015), and optimised with stochastic gra-
dient descent (where each gradient step consists of a for-
ward pass to compute the current loss over a small random

Figure 6: Expanded view of the Residual Block.

output z4, then adds that output to the input image (Fig. 6):

z1 = conv(z, f )
z2 = max(z1, 0)
z3 = conv(z2, f )
z4 = λ · z3
v = z4 + z

#convolution

#ReLU layer

#convolution

#residual scaling

#skip connection

(5a)

(5b)

(5c)

(5d)

(5e)

λ is a custom layer (5d) that multiplies its input acti-
vations (multi-dimensional images) with a constant. This
is also termed residual scaling and greatly speeds up the
training of very deep networks (Szegedy et al., 2017). In
our experience residual scaling is crucial and we always
use λ = 0.1. As a alternative, we also tested the more com-
mon Batch Normalization (BN), but found that it did not
improve accuracy or training time, while increasing the pa-
rameters of the network. Also, Lim et al. (2017) report that
BN normalises the features and thus reduces the range ﬂex-
ibility (the actual reﬂectance) of the images. Within each
ResBlock module we only include a ReLU after the ﬁrst
convolution, but not after the second, since our network
shall learn corrections to the bilinearly upsampled image,
which can be negative. Within our network design, the Res-
Block module can be repeated as often as desired. We show
experiments with two different numbers d of ResBlocks:
6 and 32. The ﬁnal convolution at the head of the net-
work, after all ResBlocks, reduces the output dimension to
blast, such that it matches the number of the required out-
put bands (xB and xC). So fout = blast = 6 for T2×, and
fout = blast = 2 for S6× is used.

A particularity of our network architecture is a long, ad-
ditive skip connection directly from the rescaled input to
the output (Fig. 5). This means that the complete network
in fact learns the additive correction from the bilinearly up-
sampled image to the desired output. The strategy to predict
the differences from a simple, robust bilinear interpolation,

8

batch of image patches, followed by back-propagation of
the error signal through the network). In detail, we use the
Adam variant of SGD (Kingma and Ba, 2014) with Nes-
terov momentum (Dozat, 2015). Empirically, the proposed
network architecture converges faster than other ones we
experimented with, due to the ResNet-style skip connec-
tions.

Sentinel-2 images are too big to ﬁt them into GPU mem-
ory for training and testing, and in fact it is unlikely that
long-range context over distances of a kilometer or more
plays any signiﬁcant role for super-resolution at the 10 m
level. With this in mind, we train the network on small
patches of w×h = (32×32) for T2×, respectively (96×96)
pixels for S6×. We note that this corresponds to a recep-
tive ﬁeld of several hundred metres on the ground, sufﬁ-
cient to capture the local low-level texture and potentially
also small semantic structures such as individual buildings
or small waterbodies, but not large-scale topographic fea-
tures. We do not expect the latter to hold much information
about the local pixel values, instead there is a certain dan-
ger that the large-scale layout of a limited training set it is
too unique to generalise to unseen locations.

As our network is fully convolutional, it can process in-
put images of arbitrary spatial extent w × h (after padding
to a multiple of the patch size). The tile size in the pre-
diction step is limited only by the on-board memory on the
GPU. To avoid boundary artifacts from tiling, adjacent tiles
are cropped with an overlap of 2 low-resolution input pix-
els, corresponding to 40 m for T2×, respectively 120 m for
S6×.

5. Experimental results

5.1. Implementation details

As mentioned before, we aim for global coverage. We
therefore sample 60 representative scenes from around the
globe, 45 for training and 15 for testing. For T2× we sam-
ple 8000 random patches per training image, for a total of
360,000 patches. For S6×, we sample 500 patches per im-
age for a total of 22,500 (note that each patch covers a 9×
larger area in object space and has 9× more high-resolution
pixels than for T2×). Out of these patches 90% are used for
training the weights, the remaining 10% serve as valida-
tion set, see Table 2. To test the networks, we run both
on the 15 test images, each with a size of 110×110 km2,
which corresponds to 5,490×5,490 pixels at 20 m GSD, or
1,830×1,830 pixels at 60 m GSD.

Each network is implemented in the Keras frame-
work (Chollet et al., 2015), with TensorFlow as back-end.
Training is run on a NVIDIA Titan Xp GPU, with 12 GB
of RAM, for approximately 3 days. The mini-batch size
for SGD is set to 128 to ﬁt into GPU memory. The initial
learning rate is lr = 1e-4 and it is reduced by a factor of
2 whenever the validation loss does not decrease for 5 con-
secutive epochs. For numerical stability we divide the raw
0 − 10,000 reﬂectance values by 2000 before processing.

Table 2: Training and testing split.

Images

45

15

45

15

Training
Validation
Test

Training
Validation
Test

Patches

Split
90% 324,000 × 322
36,000 × 322
10%
15 × 5,4902
20,250 × 962
2,250 × 962
15 × 1,8302

90%
10%

T2×

S6×

5.2. Baselines and evaluation metrics

As baselines, we use the methods of Lanaras et al. (2017)
– termed SupReME, Wang et al. (2016) – termed ATPRK,
and Brodu (2017) – termed Superres. Moreover, as ele-
mentary baseline we use bicubic interpolation, to illustrate
naive upsampling without considering spectral correlations.
Note, this also directly shows the effect of our network,
which is trained to reﬁne a bilinearly upsampled image.
The input image sizes for the baselines were chosen to ob-
tain the best possible results. SupReME showed the best
performance when run with patches of 256, respectively
240 for T2× and S6×. We speculate that this may be due
to the subspace projection used within SupReME, which
can better adapt to the local image content with moderate
tile size. The remaining baselines performed best on full
images. The parameters for all baselines were set as sug-
gested in the original publications. This lead to rather con-
sistent results across the test set.

The main evaluation metric of our quantitative compar-
ison is the root mean squared error (RMSE), estimated in-
dependently per spectral band:
(cid:114) 1
n

(ˆx − x)2 ,

RMSE =

(cid:88)

(6)

where ˆx is each reconstructed band (vectorised), x is the
vectorised ground truth band and n the number of pixels
in x. The unit of the Sentinel-2 images is reﬂectance mul-
tiplied by 10,000, however, some pixels on specularities,
clouds, snow etc. exceed 10,000. Therefore, we did not ap-
ply any kind of normalisation, and report RMSE values in
the original ﬁles’ value range, meaning that a residual of 1
corresponds to a reﬂectance error of 10−4.

Depending on the scene content, some images have
higher reﬂectance values than others, and typically also
higher absolute reﬂectance errors. To compensate for this
effect, we also compute the signal to reconstruction error
ratio (SRE) as additional error metric, which measures the
error relative to the power of the signal. It is computed as:

SRE = 10 log10

µ2
x
(cid:107)ˆx − x(cid:107)2/n

,

(7)

where µx is the average value of x. The values of SRE
are given in decibels (dB). We point out that using SRE,

9

which measures errors relative to the mean image intensity,
is better suited to make errors comparable between images
of varying brightness. Whereas the popular peak signal to
noise ratio (PSNR) would not achieve the same effect, since
the peak intensity is constant. Moreover, we also compute
the spectral angle mapper (SAM), i.e., the angular devia-
tion between true and estimated spectral signatures (Yuhas
et al., 1992). We compute the SAM for each pixel and
then average over the whole image. The values of SAM
are given in degrees. This metric is complimentary to the
two previous ones, and quite useful for some applications,
in that it measures how faithful the relative spectral distri-
bution of a pixel is reconstructed, while ignoring absolute
brightness. Finally, we report the universal image quality
index (UIQ) (Wang and Bovik, 2002). This metric evalu-
ates the reconstructed image in terms of luminance, con-
trast, and structure. UIQ is unitless and its maximum value
is 1.

5.3. Evaluation at lower scale

Quantitative evaluation on Sentinel-2 images is only pos-
sible at the lower scale at which the models are trained. I.e.,
T2× is evaluated on the task to super-resolve 40→20 m,
where the 40 m low-resolution and 20 m high-resolution
bands are generated by synthetically degrading the origi-
nal data – for details see Sec. 4.1. In the same way, S6×
is evaluated on the super-resolution task from 360→60 m.
Furthermore, to support the claim that the upsampling func-
tion is to a sufﬁcient degree scale-invariant, we also run
a test where we train T2× on the upsampling task from
80→40 m, and then test that network to the 40→20 m up-
sampling task. In the following, we separately discuss the
T2× and S6× networks.

T2× — 20 m bands. We start with results for the T2× net-
work, trained for super-resolution of actual S2 data to 10 m.
Average results over all 15 test images and all bands in
B = {B5,B6,B7,B8a,B11,B12} are displayed in Table 3.
The state-of-the-art methods SupReME and Superres per-
form similar, with Superres slightly better in all error met-
rics. DSen2 reduces the RMSE by 48% compared to the
previous state-of-the-art. The other error measures conﬁrm
this gulf in performance (>5 dB higher SRE, 24% lower
SAM). VDSen2 further improves the results, consistently
over all error measures (except UIQ, where their scores are
exactly the same). Relative to the leap from the best base-
line to DSen2 the differences may seem small, but note that
0.3 dB would still be considered a marked improvement
in many image enhancement tasks. Interestingly, ATPRK
and SupReME yield rather poor results for SAM (relative
spectral ﬁdelity). Among the baselines, only Superres beats
bicubic upsampling. Our method again wins comfortably,
more than doubling the margin between the strongest com-
petitor Superres and the simplistic baseline of bicubic up-
sampling.

Table 3: Aggregate results for 2× upsampling of the bands in set B, eval-
uated at lower scale (input 40 m, output 20 m). Best results in bold.

Training RMSE SRE SAM UIQ

Bicubic
ATPRK
SupReME
Superres
DSen2 (ours) 40→20
VDSen2 (ours) 40→20

DSen2 (ours) 80→40
VDSen2 (ours) 80→40

123.5
116.2
69.7
66.2
34.5
33.7

51.7
51.6

25.3
25.7
29.7
30.4
36.0
36.3

32.6
32.7

1.24
1.68
1.26
1.02
0.78
0.76

0.89
0.88

0.821
0.855
0.887
0.915
0.941
0.941

0.924
0.925

In the second test, we train an auxiliary T2× network on
80→40 m instead of the 40→20 m, but nevertheless evalu-
ate it on the 20 m ground truth (while the model has never
seen a 20 m GSD image). Of course this causes some
drop in performance, but the performance stays well above
I.e., the learned mapping
all baselines, across all bands.
is indeed sufﬁciently scale-invariant to beat state-of-the-
art model-based approaches, which by construction should
not depend on the absolute scale. For our actual setting,
train on 40→20 m then use for 20→10 m, one would ex-
pect even a smaller performance drop (compared to train
on 80→40 m then use for 40→20 m), because of the well-
documented inverse relation between spatial frequency and
contrast in image signals (e.g., Ruderman, 1994; van der
Schaaf and van Hateren, 1996; Srivastava et al., 2003). This
experiment justiﬁes our assumption, at 2× reduced resolu-
tion, that training 40→20 m super-resolution on syntheti-
cally degraded images is a reasonable proxy for the actual
20→10 m upsampling of real Sentinel-2 images. We note
that this result has potential implications beyond our spe-
ciﬁc CNN approach. It validates the general procedure to
train on lower-resolution imagery, that has been synthesised
from the original sensor data. That procedure is in no way
speciﬁc to our technical implementation, and in all likeli-
hood also not to the sensor characteristics of Sentinel-2.

Tables 4 and Fig. 7 show detailed per-band results. The
large advantage for our method is consistent across all
bands, and in fact particularly pronounced for the challeng-
ing extrapolation to B11 and B12. We point out that the
RMSE values for B6, B7 and B8a are higher than for the
In these bands also the
other bands (with all methods).
reﬂectance is higher. The relative errors, as measured by
SRE, are very similar. Among our two networks, VDSen2
holds a moderate, but consistent beneﬁt over its shallower
counterpart across all bands, in both RMSE and SRE. In
terms of UIQ, they both rank well above the competition,
but there is no clear winner. We attribute this to limitations
of the UIQ metric, which is a product of three terms and
thus not overly stable near its maximum of 1.

It is interesting to note that the baselines exhibit a marked
drop in accuracy for bands B11 and B12, whereas our net-

10

Figure 7: Per-band error metrics for 2× upsampling.

Table 4: Per-band values of RMSE, SRE and UIQ, for 2× upsampling. Values are averages over all test images. Evaluation at lower scale (input 40 m,
output 20 m). Best results in bold.

B5

B6

B7

B8a

B11

B12

Bicubic
ATPRK
SupReME
Superres
DSen2
VDSen2

Bicubic
ATPRK
SupReME
Superres
DSen2
VDSen2

105.0
89.4
48.1
50.2
27.7
27.1

138.1
119.1
70.2
66.6
37.6
37.0

25.1
26.6
31.2
31.3
36.2
36.5

25.6
26.9
31.0
31.7
36.5
36.8

Bicubic
ATPRK
SupReME
Superres
DSen2 (ours)
VDSen2 (ours)

0.811
0.889
0.889
0.918
0.943
0.939

0.801
0.881
0.890
0.920
0.942
0.944

RMSE

159.3
136.5
78.6
76.8
42.8
42.2

SRE

25.4
26.7
31.0
31.4
36.5
36.7

UIQ

0.802
0.891
0.894
0.921
0.942
0.938

168.3
147.4
82.9
82.0
43.8
43.0

92.4
113.3
76.5
66.9
29.0
28.0

25.5
26.6
31.2
31.4
36.9
37.1

26.3
24.7
27.9
29.1
36.3
36.7

78.0
91.7
61.7
54.5
26.2
25.1

24.0
22.7
26.1
27.2
33.6
34.0

0.806
0.883
0.894
0.919
0.935
0.943

0.857
0.789
0.878
0.904
0.943
0.946

0.847
0.795
0.879
0.905
0.940
0.935

works reconstruct B11 as well as other bands and show only
a slight drop in relative accuracy for B12. These two bands
lie in the SWIR (>1.6µm) spectrum, far outside the spectral
range covered by the high-resolution bands (0.4–0.9µm).
Especially ATPRK performs poorly on B11 and B12. The
issue is further discussed in Sec. 5.5.

In Fig. 8, we compare reconstructed images to ground
truth for one of the test images. Yellow denotes high resid-
ual errors, dark blue means zero error. For bands B6,
B7, B8a and B11 all baselines exhibit errors along high-
contrast edges (the residual images resemble a high-pass

ﬁltering), meaning that they either blur edges or exaggerate
the contrast. Our method shows only traces of this common
behaviour, and has visibly lower residuals in all spectral
bands.

S6× — 60 m bands. For 6× super-resolution we train a
separate network, using synthetically downgraded images
with 60 m GSD as ground truth. The baselines are run with
the same settings as before (i.e., jointly super-resolving all
input bands), but only the 60 m bands C = {B1, B9}
are displayed. Overall and per-band results are given in

11

Bicubic

ATPRK

SupReME

Superres

DSen2 (ours)

VDSen2 (ours)

5
B

6
B

7
B

a
8
B

1
1
B

2
1
B

Figure 8: Absolute differences between ground truth and 2× upsampled result at 20 m GSD. The images show (absolute) reﬂectance differences on a
reﬂectance scale from 0 to 10, 000. Top, left to right: RGB (B2, B3, B4) image, color composites of bands (B5, B6, B7), and of bands (B8a, B11, B12).
The image depicts the Siberian tundra near the mouth of the Pur River.

12

Table 5: Full results and detailed RMSE, SRE and UIQ values per spectral band. The results are averaged over all images for the 6× upsampling, with
evaluation at lower scale (input 360 m, output 60 m). Best results in bold.

RMSE SRE

UIQ RMSE SRE

UIQ RMSE SRE

UIQ

Bicubic
ATPRK
SupReME
Superres
DSen2
VDSen2

171.8
162.9
114.9
107.5
33.6
27.6

0.404
0.745
0.667
0.566
0.912
0.921

148.7
127.4
56.4
92.9
30.9
24.4

0.368
0.711
0.819
0.657
0.886
0.899

160.2
145.1
85.7
100.2
32.2
26.0

19.7
20.4
24.8
22.8
32.8
35.1

0.386
0.728
0.743
0.612
0.899
0.910

B1

22.3
22.8
25.2
24.8
35.6
37.9

B9

17.1
18.0
24.5
20.8
29.9
32.3

Average

SAM

1.79
1.62
0.98
1.42
0.41
0.34

Bicubic

ATPRK

SupReME

Superres

DSen2 (ours)

VDSen2 (ours)

1
B

9
B

Figure 9: Absolute differences between ground truth and 6× upsampled result at 60 m GSD. The images show (absolute) reﬂectance differences on a
reﬂectance scale from 0 to 10, 000. Top: True scene RGB (B2, B3, B4), and false color composite of B1 and B9. This image depicts Berg River Dam in
the rocky Hotentots Holland, east of Cape Town, South Africa.

Table 5. Once again, our DSen2 network outperforms the
previous state-of-the-art by a large margin, reducing the
RMSE by a factor of ≈3. For the larger upsampling fac-
tor, the very deep VDSen2 beats the shallower DSen2 by
a solid margin, reaching about 20% lower RMSE, respec-
tively 2.3 dB higher SRE.

Among the baselines, SupReME this time exhibits bet-
ter overall numbers than Superres, thanks to it clearly su-
perior performance on the B9 band. Contrary to the 2×
super-resolution, all baselines improve SAM compared to
simple bicubic interpolation. Our method again is the run-
away winner, with VDSen2 reaching 65% lower error than
the nearest competitor SupReME. Looking at the individual

bands, all methods perform better (relative to average radi-
ance) on B1 than on B9. The latter is the most challenging
band for super-resolution, and the only one for which our
SRE drops below 33 dB, and our UIQ below 0.9. It is worth
noticing, that in this more challenging 6× super-resolution,
our method brings a bigger improvement compared to the
state-of-the-art baselines in 2× super-resolution.

We also present a qualitative comparison to ground truth,
again plotting absolute residuals in Fig. 9. As for 20 m, the
visual impression conﬁrms that DSen2 and VDSen2 clearly
dominate the competition, with much lower and less struc-
tured residuals.

13

Figure 10: Results of DSen2 on real Sentinel-2 data, for 2× upsampling. From left to right: True scene RGB in 10 m GSD (B2, B3, B4), Initial 20 m
bands, Super-resolved bands (B12, B8a and B5 as RGB) to 10 m GSD with DSen2. Top: An agricultural area close to Malm¨o in Sweden. Middle: A
coastal area at the Shark Bay, Australia. Bottom: Central Park at Manhattan, New York, USA. Best viewed on computer screen.

5.4. Evaluation at the original scale

To verify that our method can be applied to true scale
Sentinel-2 data, we super-resolve the same test images
as before, but feed the original images, without synthetic
downsampling, to our networks. As said before, we see no
way to obtain ground truth data for a quantitative compar-
ison, and therefore have to rely on visual inspection. We
plot the upsampled results next to the low-resolution in-
puts, in Fig. 10 for 2× upsampling and in Fig. 11 for 6×
upsampling. For each upsampling rate, the ﬁgures show 3
different test locations with varying land cover. Since vi-
sualisation is limited to 3 bands at a time, we pick bands

(B5, B8a, B12) for 2× upsampling. For 6× upsampling we
show both bands (B1,B9). In all cases the super-resolved
image is clearly sharper and brings out additional details
compared to the respective input bands. At least visually,
the perceptual quality of the super-resolved images matches
that of the RGB bands, which have native 10 m resolution.

5.5. Suitability of pan-sharpening methods

As discussed earlier,

there is a conceptual differ-
ence between multi-spectral super-resolution and classical
pan-sharpening, in that the latter simply “copies” high-
frequency information from an overlapping or nearby high-

14

Figure 11: Results of DSen2 on real Sentinel-2 data, for 6× upsampling. From left to right: True scene RGB (B2, B3, B4), Initial 60 m bands, Super-
resolved bands (B9, B9 and B1 as RGB) with DSen2. Top: London Heathrow airport and surroundings. Middle: The foot of Mt. Aso, on Kyushu island,
Japan. Bottom: A glacier in Greenland. Best viewed on computer screen.

15

Table 6: Results of well-known pan-sharpening methods. RMSE, SRE and UIQ values per spectral band averaged over all images for the 2× upsampling,
with evaluation at lower scale (input 40 m, output 20 m). Best results in bold.

B5

B6

B7

B8a

B11

B12

Average

Bicubic
PRACS
MTF-GLP-HPM-PP
BDSD

105.0
99.3
91.0
64.7

138.1
148.1
66.5
84.2

159.3
99.2
77.6
76.0

Bicubic
PRACS
MTF-GLP-HPM-PP
BDSD

25.1
24.0
28.0
28.3

25.6
24.2
30.7
29.2

25.4
28.7
30.5
31.1

Bicubic
PRACS
MTF-GLP-HPM-PP
BDSD

0.811
0.836
0.893
0.866

0.801
0.858
0.898
0.892

0.802
0.882
0.909
0.909

RMSE

168.3
104.2
82.7
78.8

SRE

25.5
29.0
30.7
31.5

UIQ

0.806
0.881
0.909
0.908

92.4
290.0
78.7
93.4

78.0
320.0
240.6
79.4

26.3
19.5
28.0
26.3

24.0
14.4
23.0
23.9

0.857
0.791
0.877
0.858

0.847
0.773
0.881
0.848

123.5
176.8
106.2
79.4

25.3
23.3
28.5
28.4

0.821
0.837
0.895
0.880

resolution band, but cannot exploit the overall reﬂectance
distribution across the spectrum. Still, it is a-priori not clear
how much of a practical impact this has, therefore we also
test three of the best-performing pan-sharpening methods
in the literature, namely PRACS (Choi et al., 2011), MTF-
GLP-HPM-PP (Lee and Lee, 2010) and BDSD (Garzelli
et al., 2008). Quantitative error measures for the 2× case
are given in Table 6. Pan-sharpening requires a single
“panchromatic” band as high-resolution input. The combi-
nations that empirically worked best for our data were the
following: For the near-infrared bands B6, B7 and B8a, we
use the broad high-resolution NIR band B8. As panchro-
matic band for B5 we use B2, which surprisingly worked
better than the spectrally closer B8, and also slightly better
than other visual bands. While for the SWIR bands there is
no spectrally close high-resolution band, and the best com-
promise appears to be the average of the three visible bands,
1
3 (B2+B3+B4).

For bands B5, B6, B7 and B8 the results are reasonable:
the errors are higher than those of the best super-resolution
baseline (and consequently 2-3× higher than with our net-
works, c.f . Table 4), but lower than naive bicubic upsam-
pling. This conﬁrms that there is a beneﬁt from using all
bands together, rather than the high-frequency data from
only one, arbitrarily deﬁned “panchromatic” band.

On the contrary, for the SWIR bands B11 and B12 the
performance of pan-sharpening drops signiﬁcantly, to a
point that the RMSE drops below that of bicubic interpo-
lation (and similar for SRE). As was to be expected, suc-
cessful pan-sharpening is not possible with a spectrally dis-
tant band that has very different image statistics and local
appearance. Moreover, pan-sharpening is very sensitive to

the choice of the “panchromatic” band. We empirically
picked the one that worked best on average, but found that,
for all tested methods, there isn’t one that performs con-
sistently across all test images. This is particularly evident
for MTF-GLP-HPM-PP. Even with the best pan-band we
found (the average of the visible bands), it reconstructed
reasonable SWIR bands for some images, but completely
failed on others, leading to excessive residuals. 4

While it may be possible to improve pan-sharpening
performance with some sophisticated, perhaps non-linear
combination for the pan-band, determining that combina-
tion is a research problem on its own, and beyond the scope
of this paper.

For readability, the pan-sharpening results are displayed
in a separate table. We note for completeness that, among
the super-resolution baselines (Tables 3 and 4), ATPRK is
technically also a pan-sharpening method, but includes a
mechanism to automatically select one ut of several high-
resolution channels as its the “panchromatic” input. We
categorise it as super-resolution, since its creators also in-
tend and apply it for that purpose. It can be seen in Table 4
that ATPRK actually also exhibits a distinct performance
drop for bands B11 and B12.

Overall, we conclude that pan-sharpening cannot sub-
stitute qualiﬁed super-resolution, and is not suitable for
Sentinel-2. Nevertheless, we point out that in the literature,
the difﬁculties it has especially with bands B11 and B12 is
sometimes masked, because many papers do not show the

4 Actually, for MTF-GLP-HPM-PP we had to exclude one of the 15
images from the evaluation, since the method did not produce a valid out-
put.

16

individual per-band errors.

6. Discussion

6.1. Different network conﬁgurations

The behaviour of our two tested network conﬁgurations
is in line with the recent literature: networks of moder-
ate size (by today’s standards), like DSen2, already per-
form fairly well. Across a wide range of image analysis
tasks from denoising to instance-level semantic segmen-
tation and beyond, CNNs with around 10–20 layers have
redeﬁned the state-of-the-art. Over the last few years, im-
provements to the network architecture have enabled train-
ing of very deep networks with even more (in some cases
>100) layers, like VDSen2. Empirically, these models tend
to raise the bar even further, but the gains are less dra-
matic, as adding more and more layers faces diminishing
returns. Somewhat surprisingly, even the very deep mod-
els with tens of millions of free parameters do not show
a strong tendency to overﬁt, if designed correctly. We
note that our networks differ from the prevalent design for
high-level analysis (semantic segmentation, depth estima-
tion, etc.). These normally have an “hourglass” structure
with an encoder part that successively increases the recep-
tive ﬁeld (respectively, reduces the spatial resolution) via
pooling operations, followed by a decoder part that restores
the original resolution via transposed convolutions. We re-
frain from pooling, since it carries the danger of degrading
local detail, while conversely a fairly small neighbourhood
is, in our view, sufﬁcient to determine the local spectral re-
lations.

What is the “right” depth for super-resolution? As usual
in such cases, there is no single answer, since this de-
pends on the speciﬁc application (e.g., variability of the
land-cover, available computational resources, update fre-
quency, etc.). As a general guideline, we ﬁnd that, with ad-
equate hardware at hand, there is no disadvantage in using
VDSen2. It is neither more difﬁcult to use nor more brittle
to train from the perspective of the user. While it does con-
sistently produce super-resolved images with lower residu-
als, especially for the challenging 6× upsampling. If hard-
ware resources (especially GPU memory) are limited, or
very large interest regions must be processed in a short
time, it may nevertheless be better to work with DSen2.
The results are still very good, and in certain cases, e.g.,
if only 2× upsampling is needed and/or the spectral vari-
ability in the interest region is not too high will probably
match the performance of a deeper architecture.
Impor-
tantly, intermediate variants are also possible: if one aims
for the highest possible quality under limited resources, it
may make sense to chose a number of ResBlocks between
the d = 6 of DSen2 and the d = 32 of VDSen2. In fact,
for “easy” land-cover or if maximal accuracy is not needed
(e.g., for visualisation) it may well be possible to remove
another 1 or 2 ResBlocks from DSen2 and still obtain sat-
isfactory results.

Table 7: Runtimes for super-resolving the six 20 m bands of a standard
Sentinel-2 tile (10980 × 10980 pixels, ≈120 Mpix).

Method

CPU time GPU time

Bicubic
ATPRK
SupReME
Superres
DSen2 (ours)
VDSen2 (ours)

(cid:28)1 min
149 min
123 min
315 min
130 min
≈ 30 h

–
–
–
–
3 min
14 min

6.2. Timing

As in general for deep learning, training a network is
computationally demanding and takes time (often several
days, see sec. 4.5), but the single forward pass to super-
resolve a new image is very fast. We note that long training
times are usually required only once, when training from
scratch. Reﬁning/adapting an existing network with further
training data is a lot less costly. Our pretrained networks
can serve as a starting point.

In Table 7 the runtimes of all tested methods are pre-
sented for super-resolving all 20 m bands of a complete
Sentinel-2 tile (10,980×10,980 pixels). The baselines are
only available as CPU code, and in some cases not easy to
parallelise, whereas CNNs are almost always run on GPUs
– in fact, their current revival was, to a large part, triggered
by the advent of parallel computing on GPUs. We therefore
show both processing times. The comparison is indicative
and not meant to claim our method is a lot faster than the
baselines: modern CNN frameworks are highly optimised,
whereas the baselines are research implementations with
much potential for speed-ups. Still, the numbers are use-
ful to show that CNN-based super-resolution is fairly efﬁ-
cient, and clearly fast enough to be used in practice with-
out much further code optimisation. For the Comparison,
we used an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz,
respectively an NVIDIA Titan Xp GPU. On a desktop com-
puter with a single GPU, DSen2 super-resolves a complete
Sentinel-2 tile to 10 m in 3 minutes, and VDSen2 in 14 min-
utes. We note that hardware producers are working on spe-
cialised tensor processing hardware that is optimised for
deep learning (rather than gaming and computer graphics),
and can be expected to further speed up CNNs. We do
point out that if no powerful GPU is available, very deep
networks are not viable. On the contrary, DSen2 takes ≈2
hours of CPU time and is comparable with the fastest base-
line method.

6.3. Open-source publication of our models

The publication of this paper includes open, publicly
available implementations of our models, at: https://
github.com/lanha/DSen2. We provide the python
source ﬁles (in Keras format) for the network speciﬁca-
tions as well as the training procedure. Moreover, we also

17

provide the already trained weights used in all our experi-
ments. These shall enable out-of-the-box super-resolution
of Sentinel-2 images world-wide, with minimal knowledge
of neural network tools. Of course, if a study is focussed
only in a speciﬁc geographic location, biome or land-cover
type, even better result can be expected by training the net-
work only with images showing those speciﬁc conditions.
The literature suggests that in that case, it may be best
to start from our globally trained network and ﬁne-tune it
through further training iterations on task-speciﬁc imagery.
In the future, we hope to also integrate our method into
the SNAP toolbox for Sentinel-2 processing, so as to use
our super-resolution instead of naive upsampling within the
processing pipeline. A word of caution: our weights are
trained only on real Sentinel-2 images, and their excellent
performance is to a large part due to the fact that they are
optimised speciﬁcally for the image statistics of the input
data. They are therefore not suitable for processing data
from other sensors, or other processing levels of Sentinel-2.

7. Conclusions

We have described a tool to super-resolve (“sharpen”)
the lower-resolution (20 m and 60 m) bands of Sentinel-
2 to a uniform 10 m GSD data cube. Our method uses
two deep convolutional neural networks to jointly learn the
mapping from all input bands to the 2×, respectively 6×
super-resolved output bands. To train the network, we make
the empirically plausible assumption that the correct way
of transferring high-frequency information across spectral
bands is invariant over a range of scales. In this way, we can
synthesise arbitrary amounts of training data with known
ground truth from the Sentinel-2 archive. We sample a
large and varied global dataset that, according to our ex-
periments, yields a super-resolution tool that generalises to
unseen locations in different parts of the world.

The super-resolution network shows excellent perfor-
mance, reducing the RMSE of the prediction by 50% com-
pared to the best competing methods; respectively, increas-
ing the SRE by almost 6 dB. Qualitative results from differ-
ent land-cover types, biomes and climate zones conﬁrm the
good performance also on full-resolution S2 images. More-
over, the method is also fast enough for practical large-scale
applications, computation times are on the order of a few
minutes for a complete, 120 MPix Sentinel-2 tile.

While in our work we have focussed on Sentinel-2, the
networks are learned end-to-end from image data and thus
completely generic. We are conﬁdent that they can be
retrained for super-resolution of different multi-resolution
multi-spectral sensors. We make our software and models
available as open-source tools for the remote sensing com-
munity.

Acknowledgments

The authors acknowledge ﬁnancial support from the
Swiss National Science Foundation (SNSF), project No.
200021 162998, Fundac¸ ˜ao para a Ciˆencia e a Tecnolo-
gia, Portuguese Ministry of Science, Technology and
Higher Education, projects UID/EEA/50008/2013 and ER-
ANETMED/0001/2014.

References

Brodu, N., Aug 2017. Super-resolving multiresolution images with band-
independent geometry of multispectral pixels. IEEE Transactions on
Geoscience and Remote Sensing 55 (8), 4610–4617.

Castillo, J. A. A., Apan, A. A., Maraseni, T. N., Salmo III, S. G., 2017.
Estimation and mapping of above-ground biomass of mangrove forests
and their replacement land uses in the philippines using Sentinel im-
agery. ISPRS Journal of Photogrammetry and Remote Sensing 134, 70
– 85.

Choi, J., Yu, K., Kim, Y., 2011. A new adaptive component-substitution-
based satellite image fusion by using partial replacement. IEEE Trans-
actions on Geoscience and Remote Sensing 49 (1), 295–309.

Chollet, F., et al., 2015. Keras. https://github.com/fchollet/

keras.

Clerc, S., MPC Team,

reference S2-PDGS-MPC-DQR,

2018. S2 MPC - Data Quality Re-
[On-
http://earth.esa.int/documents/247904/
accessed

port. ESA,
line:
685211/Sentinel-2-Data-Quality-Report,
01 Feb. 2018].

issue

23.

Clevers, J., Kooistra, L., Van Den Brande, M., 2017. Using sentinel-2 data
for retrieving lai and leaf and canopy chlorophyll content of a potato
crop. Remote Sensing 9 (5), 405.

Delloye, C., Weiss, M., Defourny, P., 2018. Retrieval of the canopy chloro-
phyll content from sentinel-2 spectral bands to estimate nitrogen uptake
in intensive winter wheat cropping systems. Remote Sensing of Envi-
ronment 216, 245–261.

Dozat, T., 2015.

rep., Stanford University, Tech. Rep.

Incorporating Nesterov momentum into Adam.
http:
Tech.
//cs229.stanford.edu/proj2015/054_report.pdf, ac-
cessed 06 Feb. 2018].

[Online:

Du, Y., Zhang, Y., Ling, F., Wang, Q., Li, W., Li, X., 2016. Water bod-
ies mapping from Sentinel-2 imagery with modiﬁed normalized differ-
ence water index at 10-m spatial resolution produced by sharpening the
SWIR band. Remote Sensing 8 (4), 354.

Fasbender, D., Tuia, D., Bogaert, P., Kanevski, M., 2008. Support-based
implementation of Bayesian data fusion for spatial enhancement: Ap-
plications to ASTER thermal images. IEEE Geoscience and Remote
Sensing Letters 5 (4), 598–602.

Freeman, W. T., Jones, T. R., Pasztor, E. C., 2002. Example-based super-
resolution. IEEE Computer graphics and Applications 22 (2), 56–65.
Garzelli, A., Nencini, F., Capobianco, L., 2008. Optimal mmse pan sharp-
ening of very high resolution multispectral images. IEEE Transactions
on Geoscience and Remote Sensing 46 (1), 228–236.

Gasparovic, M., Jogun, T., 2018. The effect of fusing Sentinel-2 bands
on land-cover classiﬁcation. International Journal of Remote Sensing
39 (3), 822–841.

Glasner, D., Bagon, S., Irani, M., 2009. Super-resolution from a single
image. In: IEEE International Conference on Computer Vision. pp.
349–356.

He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving deep into rectiﬁers: Sur-
passing human-level performance on imagenet classiﬁcation. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). pp.
1026–1034.

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for im-
age recognition. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 770–778.

18

Huang, G., Liu, Z., van der Maaten, L., Weinberger, K. Q., 2017. Densely
connected convolutional networks. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 2261–2269.

Immitzer, M., Vuolo, F., Atzberger, C., 2016. First experience with
Sentinel-2 data for crop and tree species classiﬁcations in central Eu-
rope. Remote Sensing 8 (3), 166.

Kim, J., Kwon Lee, J., Mu Lee, K., 2016. Accurate image super-resolution
using very deep convolutional networks. In: IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). pp. 1646–1654.

Kingma, D., Ba, J., 2014. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980.

Lanaras, C., Bioucas-Dias, J., Baltsavias, E., Schindler, K., 2017. Super-
resolution of multispectral multiresolution images from a single sen-
sor. In: IEEE Conference on Computer Vision and Pattern Recognition
Workshops (CVPRW). pp. 1505–1513.

Lee, J., Lee, C., 2010. Fast and efﬁcient panchromatic sharpening. IEEE
Transactions on Geoscience and Remote Sensing 48 (1), 155–163.
Lim, B., Son, S., Kim, H., Nah, S., Lee, K. M., 2017. Enhanced deep resid-
ual networks for single image super-resolution. In: IEEE Conference
on Computer Vision and Pattern Recognition Workshops (CVPRW).
pp. 1132–1140.

Toming, K., Kutser, T., Laas, A., Sepp, M., Paavel, B., Nges, T.,
2016. First experiences in mapping lake water quality parameters with
Sentinel-2 MSI imagery. Remote Sensing 8 (8), 640.

Tonooka, H., 2005. Resolution enhancement of ASTER shortwave and
thermal infrared bands based on spectral similarity. In: Proc. SPIE.
Vol. 5657. pp. 9–19.

Trishchenko, A. P., Luo, Y., Khlopenkov, K. V., 2006. A method for
downscaling MODIS land channels to 250-m spatial resolution using
adaptive regression and normalization. In: Proc. SPIE. Vol. 6366. pp.
636607–636607–8.

Vaiopoulos, A., Karantzalos, K., 2016. Pansharpening on the narrow
VNIR and SWIR spectral bands of Sentinel-2. ISPRS-International
Archives of the Photogrammetry, Remote Sensing and Spatial Infor-
mation Sciences XLI-B7, 723–730.

van der Schaaf, A., van Hateren, J. H., 1996. Modelling the power spectra
of natural images: Statistics and information. Vision Research 36 (17),
2759–2770.

Vivone, G., Alparone, L., Chanussot, J., Dalla Mura, M., Garzelli, A.,
Licciardi, G. A., Restaino, R., Wald, L., 2015. A critical comparison
among pansharpening algorithms. IEEE Transactions on Geoscience
and Remote Sensing 53 (5), 2565–2586.

Masi, G., Cozzolino, D., Verdoliva, L., Scarpa, G., 2016. Pansharpening

Wang, Q., Shi, W., Li, Z., Atkinson, P. M., 2016. Fusion of Sentinel-2

by convolutional neural networks. Remote Sensing 8 (7), 594.

images. Remote Sensing of Environment 187, 241–252.

Wang, Z., Bovik, A. C., March 2002. A universal image quality index.

IEEE Signal Processing Letters 9 (3), 81–84.

Yang, J., Fu, X., Hu, Y., Huang, Y., Ding, X., Paisley, J., 2017. Pannet: A
deep network architecture for pan-sharpening. In: IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 5449–5457.
Yuhas, R. H., Goetz, A. F., Boardman, J. W., 1992. Discrimination
among semi-arid landscape endmembers using the spectral angle map-
per (SAM) algorithm. In: Summaries of the Third Annual JPL Air-
borne Geoscience Workshop. Vol. 1. pp. 147–149.

Mura, M., Bottalico, F., Giannetti, F., Bertani, R., Giannini, R., Mancini,
M., Orlandini, S., Travaglini, D., Chirici, G., 2018. Exploiting the capa-
bilities of the Sentinel-2 multi spectral instrument for predicting grow-
ing stock volume in forest ecosystems. International Journal of Applied
Earth Observation and Geoinformation 66, 126 – 134.

Paris, C., Bioucas-Dias, J., Bruzzone, L., July 2017. A hierarchical ap-
proach to superresolution of multispectral images with different spatial
resolutions. In: IEEE International Geoscience and Remote Sensing
Symposium (IGARSS). pp. 2589–2592.

Park, H., Choi, J., Park, N., Choi, S., 2017. Sharpening the VNIR and
SWIR bands of Sentinel-2A imagery through modiﬁed selected and
synthesized band schemes. Remote Sensing 9 (10), 1080.

Paul, F., Winsvold, S. H., Kb, A., Nagler, T., Schwaizer, G., 2016. Glacier
remote sensing using Sentinel-2. Part II: Mapping glacier extents and
surface facies, and comparison to Landsat 8. Remote Sensing 8 (7).
Pesaresi, M., Corbane, C., Julea, A., Florczyk, A. J., Syrris, V., Soille, P.,
2016. Assessment of the added-value of Sentinel-2 for detecting built-
up areas. Remote Sensing 8 (4), 299.

Picaro, G., Addesso, P., Restaino, R., Vivone, G., Picone, D., Dalla Mura,
M., 2016. Thermal sharpening of VIIRS data. In: IEEE International
Geoscience and Remote Sensing Symposium (IGARSS). pp. 7260–
7263.

Pouliot, D., Latifovic, R., Pasher,

J., 2018. Landsat
super-resolution enhancement using convolution neural networks and
Sentinel-2 for training. Remote Sensing 10 (3), 394.

J., Duffe,

Ruderman, D. L., 1994. The statistics of natural images. Network: Com-

putation in Neural Systems 5 (4), 517–548.

Scarpa, G., Vitale, S., Cozzolino, D., 2018. Target-adaptive cnn-based
pansharpening. IEEE Transactions on Geoscience and Remote Sens-
ing (99), 1–15.

Shechtman, E., Irani, M., 2007. Matching local self-similarities across im-
ages and videos. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for
large-scale image recognition. In: International Conference on Learn-
ing Representations.

Sirguey, P., Mathieu, R., Arnaud, Y., Khan, M. M., Chanussot, J., 2008.
Improving MODIS spatial resolution for snow mapping using wavelet
fusion and ARSIS concept. IEEE Geoscience and Remote Sensing Let-
ters 5 (1), 78–82.

Srivastava, A., Lee, A. B., Simoncelli, E. P., Zhu, S.-C., 2003. On ad-
vances in statistical modeling of natural images. Journal of Mathemat-
ical Imaging and Vision 18 (1), 17–33.

Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A. A., 2017. Inception-v4,
inception-resnet and the impact of residual connections on learning. In:
Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelli-
gence (AAAI-17).

19

